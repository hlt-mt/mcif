# Copyright 2025 FBK

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License

# Code used to build an example of how the participants should format their outputs.

import sys
import xml.etree.ElementTree as ET

TASK_ATTRIB = ["track", "text_lang"]
# ASR example
tst_outputs = [{
    "track": "long",
    "text_lang": "en",
    "samples": [
        {"id": "1", "output": """This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task. We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings—words from one language that are introduced into another without orthographic adaptation—and use it to evaluate how several sequence labeling models (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus contains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and topic-varied than previous corpora available for this task. Our results show that a BiLSTM-CRF model fed with subword embeddings along with either Transformer-based embeddings pretrained on codeswitched data or a combination of contextualized word embeddings outperforms results obtained by a multilingual BERT-based model. Hi, this is Elena and I'm going to be presenting our work, Detecting Unassimilated Borrowings in Spanish: An Annotated Corpus and Approaches to Modeling. So we're going to be covering what lexical borrowing is, the task that we proposed, the dataset that we have released and some models that we explored. But to begin with, what is lexical borrowing and why it matters as an NLP task? Well, lexical borrowing is basically the incorporation of words from one language into another language. For instance, in Spanish we use words that come from English. And here you have a few examples, words such as podcast, app, and online crowdfunding, all these are English words that we sometimes use in Spanish. Lexical borrowing is a type of linguistic borrowing um which is basically reproducing in one language patterns of other languages. And borrowing and code switching have sometimes been compared and described as a continuum, code switching being ah the thing that bilinguals do where they mix two languages at the same time. There are however some differences between lexical borrowing and code-switching. We're going to be focusing on lexical borrowing. Code switching is something that is done by bilinguals and by definition the code switches are not integrated into any of the languages in use, whereas lexical borrowing is something that is also done by monolinguals. The borrowings will comply with the grammar of the recipient language. And borrowings can eventually be integrated into the recipient language. So why is borrowing an interesting phenomenon? Well, from the point of view of linguistics, borrowing is a manifestation of of how languages change and how they interact. And also lexical borrowings are a source of new words. Here you have some examples of lexical borrowings that have been incorporated into the Spanish language as new words. In terms of NLP ah borrowings are a common source of out-of-vocabulary words. And in fact, automatically detecting lexical borrowings ah has proven to be useful for NLP downstream tasks such as parsing, text-to-speech synthesis or machine translation. There has been a growing interest in the influence of English on other languages ah particularly ah related to English lexical borrowings, borrowings which sometimes have been called Anglicisms. And here, you have some examples of ah work on automatic detection of borrowings in ah some of these languages. So the task that we propose is to detect unassimilated lexical borrowings in Spanish newswire. Which means that we are interested in extracting ah words borrowed from other languages that are being used in Spanish newspapers but that have not been integrated or assimilated into the recipient language. So not yet integrated into Spanish. Here you have an example. This is a sentence in Spanish: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork. Um, and as you can see, there are three spans of texts which are actually English words like bestseller, animal print and patchwork. These are the type of spans that we are interested in extracting and detecting. There has been previous word on Anglicism detection ah which consists consisted of a CRF model for Anglicism detection on Spanish Newswire. This model achieved an F1 score of eighty six. But there were some limitations both um in the dataset and the modeling approach. So the dataset focused exclusively on one source of news, consisted only of headlines. And also there was an overlap in the borrowings that appear in the training set and the test set. This prevented the assessment of whether the modeling approach could actually generalize to previously unseen borrowings. So what we aim is to tackle some of these limitations in the task. So to begin we, to begin with, we created a new dataset. Ah the aim at a new dataset that was annotated with lexical borrowings and the aim was to create a test set that was as difficult as possible. So there would be minimal overlap in words and topics between the training set and test set. And as a result, well, the test set comes from sources and dates that we're not seeing in the training set. Here you can see that there's no overlap in the in the time. It's also, the test set is also very borrowing-dense. Just to give you some numbers, if the training set contains six borrowings per each thousand tokens, the test set contained twenty borrowings per each thousand tokens. The test set contained as many out of vocabulary words as possible. In fact, ninety two percent of the borrowings in the test set are OOV. So, they were not seen during training. And the corpus consisted basically of a collection of texts that came from different sources of Spanish newspapers. And ah it was annotated by hand ah using two tags. One for English lexical borrowings which is the majority of lexical borrowings in Spanish, and then the label other for borrowings from other languages. We use CONLL formats and we used BIO encoding so that we could encode ah single token borrowings such as app or multi token borrowings such as machine learning. These are the numbers of the corpus. As you can see, it amounts to roughly three hundred seventy thousand tokens. And here you have the number of spans that were labeled as English and the spans that were labeled as other borrowings and how many of them were unique. And here you have a couple of examples of the of the set of the dataset. As you can see for instance here, we have ah in the first example, we have the borrowing batch cooking which is a multi word borrowing. And we have annotated it using the BIO um encode. So the BIO was used for words in Spanish so not for words that were not borrowed. And here in this second example, you have benching and crash which are also labeled as borrowings from English. So, once we had the dataset, we explored several models for the task of extracting and detecting these lexical borrowings. The first one that we tried was the conditional random field model. Ah, this was the model that had been used on previous work. And we used the same handcrafted features from that from those from that work. As you can see, these are the features. These are binary features such as the word or the token in upper case? Is it title titlecase? Is it a quotation mark? Things like that, which are the type of features that one would expect in a named entity recognition task. These are the results that we got. We obtain fifty five F1 score using the the CRF model with handcrafted features. Which is a huge different difference um compared to the reported F1 score of eighty six, which was the result obtained with the same CRF model, same features but on a different dataset also for Spanish lexical borrowing detection. So, this proves that the dataset that we created is more difficult and that we needed to explore more sophisticated models for these tasks. So, we tested two transformer based models. We used BETO which is a monolingual BERT model trained for Spanish and also multilingual BERT. Both models we use them through the transformers library by HuggingFace. These are the results that we got. As you can see, multilingual BERT performs better than BETO both on the development set and on the test set and across all metrics. Just so we have ah an idea to compare, the CRF model obtained an eighty two. The CRF model obtained a fifty five obtained a fifty five F1 score, whereas the multilingual BERT obtained eighty two, which is a big difference. So, once that we had those results, we asked ourselves another question which is, could we find a BiLSTM-CRF model, feed it with different types of embeddings, embeddings that encode different types of linguistic information and perform outperform the results obtained by transformer based models? So in order to do so, we ran some preliminary experiments, we we run this by BiLSTM-CRF model using flare library. And we tried experimented with different type of embeddings like transformer-based but also fast-text, character embeddings, and so on. What we found out was that transformer-based embeddings performed better than non contextualized embeddings, that the combination of English BERT and Spanish BETO embeddings outperform multilingual BERT embeddings. And that BPE embeddings produced better F1 and character embeddings produce better recall. With that in mind, these were the best performing results that we got. Both models were BiLSTM-CRF models using flare. One was fed with BETO and BERT embeddings and BPE, and the other one BETO and BERT embeddings and BPE and also character embeddings. This last one was the one that produced the highest F1 score on the test set, although the highest score on the development set was obtained by the one without character embeddings. Just ah to bear in mind that the best result that we got with multilingual BERT obtained an F1 of seventy six on the development set and eighty two on the test set. So this is an improvement compared to those results. Finally, we asked ourselves another question which was can lexical borrowing detection be framed as transfer learning from language identification in code switching? So, we run the same BiLSTM-CRF model that we had run using flare, but instead of using these unadapted transformer-based BETO and BERT embeddings, we used code switch embeddings. What are code switch embeddings? Well these are um embeddings that are have been fine tuned transformer-based embeddings that have been pretrained for language identification on the Spanish English section of the LinCE code switching dataset. LinCE is a dataset on code switching that has a section on Spanish English, Spanish English code switching. So we fed our BiLSTM-CRF with code switch embeddings and optionally character embeddings, BPE embeddings and so on. The best result that we got was eighty four point twenty two, which is the highest across all the models that we tried on the test set. Although the best result F1 score that we got on the development set, which was seventy nine, was lower than the best result obtained by the BiLSTM-CRF fed with unadapted embeddings. So, some conclusions from our work. We have ah we have produced a new dataset of Spanish newswire that is annotated with unassimilated lexical borrowings. This dataset is more borrowing dense and OOV-rich than previous resources. We have explored four types of models for lexical borrowing detection. Um. In terms of error analysis, well, recall was a weak point for all models. Ah, as you can see here, some frequent false negatives include uppercase borrowings, words that exist in both English and Spanish, for instance. Also interestingly, BPE embeddings seem to improve F1 score. And character embedding seem to improve recall. Which ah it's an interesting finding that perhaps we can explore on future work. Um. Well, this is everything that I have. Thank you so much for listening."""},  # noqa
        {"id": "3", "output": "The presenter is Elena Álvarez-Mellado."},
        {"id": "5", "output": "This work presents a new resource for borrowing identification and analyzes the performance and errors of several models on this task. We introduce a new annotated corpus of Spanish newswire rich in unassimilated lexical borrowings—words from one language that are introduced into another without orthographic adaptation—and use it to evaluate how several sequence labeling models (CRF, BiLSTM-CRF, and Transformer-based models) perform. The corpus contains 370,000 tokens and is larger, more borrowing-dense, OOV-rich, and topic-varied than previous corpora available for this task. Our results show that a BiLSTM-CRF model fed with subword embeddings along with either Transformer-based embeddings pretrained on codeswitched data or a combination of contextualized word embeddings outperforms results obtained by a multilingual BERT-based model."}  # noqa
    ]}, {
    "track": "long",
    "text_lang": "de",
    "samples": [
        {"id": "2", "output": """Hallo, hier ist Elena und ich stelle nun unsere Arbeit vor: Die Erkennung nicht-assimilierter Entlehnungen im Spanischen: Ein annotierter Korpus und Ansätze zur Modellierung. Wir werden uns also damit beschäftigen, was die lexikalische Entlehnung ist, die von uns vorgeschlagene Aufgabe, den veröffentlichten Datensatz und einige untersuchte Modelle. Doch zunächst einmal: Was ist die lexikalische Entlehnung und warum ist sie als NLP-Aufgabe so wichtig? Die lexikalische Entlehnung ist im Grunde die Übernahme von Wörtern aus einer Sprache in eine andere Sprache. Zum Beispiel verwenden wir im Spanischen Wörter, die aus dem Englischen stammen. Und hier ein paar Beispiele: Wörter wie Podcast, App und Online-Crowdfunding sind englische Wörter, die wir manchmal im Spanischen verwenden. Die lexikalische Entlehnung ist eine Art der sprachlichen Entlehnung, die im Grunde genommen die Reproduktion von Mustern einer Sprache in einer anderen Sprache bedeutet.Manchmal wurde die Entlehnung mit dem Code-Switching verglichen und als ein Kontinuum beschrieben. Code-Switching wird von Zweisprachigen praktiziert, wenn sie zwei Sprachen gleichzeitig verwenden. Es gibt jedoch einige Unterschiede zwischen lexikalischer Entlehnung und Code-Switching. Wir werden uns auf die lexikalische Entlehnung konzentrieren. Zweisprachige Personen praktizieren das sogenannte Code-Switching. Per Definition sind die Code-Switches nicht Teil der verwendeten Sprachen, während die lexikalische Entlehnung auch von einsprachigen Personen verwendet wird. Die Entlehnungen werden der Grammatik der Empfängersprache angepasst. Entlehnungen können Schritt für Schritt in die Empfängersprache integriert werden. Warum ist Entlehnen so ein interessantes Phänomen? Aus Sicht der Linguistik ist die Entlehnung eine Manifestation dessen, wie sich Sprachen verändern und wie sie interagieren. Auch lexikalische Entlehnungen sind eine Quelle für neue Wörter.Hier finden Sie einige Beispiele für lexikalische Entlehnungen, die als neue Wörter in die spanische Sprache aufgenommen wurden. Beim NLP sind Entlehnungen eine häufige Quelle von Wörtern, die nicht im Wortschatz enthalten sind. Die automatische Erkennung lexikalischer Entlehnungen erwies sich als nützlich für NLP und nachgelagerte Aufgaben wie Parsing, Text-zu-Sprache-Synthesen oder die maschinelle Übersetzung. Der Einfluss des Englischen auf andere Sprachen erfährt immer stärkeres Interesse, insbesondere bei englischen lexikalischen Entlehnungen. Diese werden manchmal auch als Anglizismen bezeichnet. Hier sind einige Beispiele von Arbeiten zur automatischen Erkennung von Entlehnungen in einigen dieser Sprachen. Die Aufgabe, die wir vorschlagen, besteht also darin, nicht-assimilierte lexikalische Entlehnungen in spanischen Nachrichten zu erkennen. Wir sind daran interessiert, aus anderen Sprachen entlehnte Wörter zu extrahieren, die in spanischen Zeitungen verwendet werden, aber nicht in die Empfängersprache integriert oder assimiliert wurden. Sie wurden also noch nicht ins Spanische integriert. Hier ist ein Beispiel. Dies ist ein Satz auf Spanisch: Las prendas bestsellers se estampan con motivos florales, animal print o retales tipo patchwork. Wie Sie sehen können, sind hier drei Textpassagen, die eigentlich englische Wörter sind: Bestseller, Animal Print und Patchwork. Bei diesen Passagen wollen wir extrahieren und erkennen. Es gab früher schon Arbeiten über die Erkennung von Anglizismen. Diese beschäftigten sich mit einem CRF-Modell für die Erkennung von Anglizismen in spanischen Nachrichten. Dieses Modell erreichte einen F1-Score von 86. Es gab jedoch einige Einschränkungen sowohl beim Datensatz als auch beim Modellierungsansatz. Der Datensatz konzentrierte sich also ausschließlich auf eine Quelle von den Nachrichten und bestand nur aus Schlagzeilen. Außerdem gab es Überschneidungen bei den Entlehnungen, die im Trainingssatz und im Testsatz vorkommen. Dadurch konnte nicht beurteilt werden, ob der Modellierungsansatz tatsächlich auf zuvor unbekannte Entlehnungen verallgemeinert werden kann. Unser Ziel ist es also, einige dieser Einschränkungen in der Aufgabe zu überwinden. Zu Beginn haben wir also einen neuen Datensatz erstellt. Das Ziel war ein neuer Datensatz, der mit lexikalischen Entlehnungen annotiert wurde, und einen möglichst schwierigen Testsatz zu erstellen. Es gäbe also minimale Überschneidungen bei Wörtern und Themen zwischen dem Trainingssatz und dem Testsatz. Das Ergebnis ist, dass der Testsatz aus Quellen und Daten stammt, die wir nicht im Trainingssatz sehen. Hier können Sie sehen, dass es keine Überschneidungen in der Zeit gibt. Außerdem enthält der Testsatz auch sehr viele Entlehnungen. Um Ihnen ein paar Zahlen zu nennen: Wenn der Trainingssatz sechs Entlehnungen pro 1000 Token enthält, enthält der Testsatz 20 Entlehnungen pro 1000 Token. Der Testsatz enthielt so viele Vokabelwörter wie möglich. Tatsächlich sind 92 Prozent der Entlehnungen im Testsatz OOV. Sie waren also während des Trainings nicht bekannt. Der Korpus bestand im Wesentlichen aus einer Sammlung von Texten, die aus verschiedenen Quellen spanischer Zeitungen stammten. Er wurde manuell mit zwei Tags annotiert. Einer für englische lexikalische Entlehnungen, die den Großteil der lexikalischen Entlehnungen im Spanischen ausmachen, und dann das andere Label für Entlehnungen aus anderen Sprachen. Wir verwenden CONLL-Formate und die BIO-Kodierung, sodass wir einfache Token-Entlehnungen wie „App“ oder mehrteilige Token-Entlehnungen wie „maschinelles Lernen“ kodieren können. Das sind die Nummern des Korpus. Wie Sie sehen können, handelt es sich um etwa 370 000 Token. Hier sehen Sie die Reihe an Passagen, die als Englisch markiert wurden, und die Passagen, die als andere Entlehnungen markiert waren, und wie viele davon einzigartig waren. Hier sehen Sie einige Beispiele für den Datensatz. Wie Sie zum Beispiel hier sehen können, haben wir im ersten Beispiel die Entlehnung „batch cooking“, die eine mehrteilige Wort-Entlehnung ist. Wir haben dieses Wort mit der BIO-Kodierung annotiert. BIO wurde also für Wörter im Spanischen verwendet, also nicht für Wörter, die nicht entlehnt wurden. Hier in diesem zweiten Beispiel sehen Sie „benching“ und „crash“, die ebenfalls als Entlehnungen aus dem Englischen markiert sind. Nachdem wir also den Datensatz hatten, untersuchten wir verschiedene Modelle für die Aufgabe, bei der wir lexikalische Entlehnungen extrahieren und erkennen wollten. Zuerst haben wir das bedingte Zufallsfeld Modell getestet. Das war das Modell, das bei früheren Arbeiten verwendet worden war. Wir haben die gleichen manuell erstellten Funktionen wie bei dieser Arbeit verwendet. Wie Sie sehen können, sind dies die Funktionen. Dies sind binäre Funktionen, wie das Wort oder das Token in Großbuchstaben. Handelt es sich um einen Titel? Ist es ein Anführungszeichen? Solche Dinge sind die Art von Funktionen, die man bei einer Named Entity Recognition-Aufgabe erwarten würde. Das sind die Ergebnisse, die wir erhalten haben. Wir erhalten 55 F1-Scores, wenn wir das CRF-Modell mit manuell erstellten Funktionen verwenden. Das ist ein großer Unterschied im Vergleich zum bereits berichteten F1-Score von 86, der ein Ergebnis desselben CRF-Modells mit derselben Funktionen war, aber auf einen anderen Datensatz angewendet wurde, auch für die Erkennung von spanischen lexikalischen Entlehnungen. Das beweist also, dass der Datensatz, den wir erstellt haben, schwieriger ist und dass wir anspruchsvollere Modelle für diese Aufgaben entwickeln müssen. Wir haben also zwei Transformer-basierte Modelle getestet. Wir haben BETO verwendet, ein einsprachiges BERT-Modell, das auf Spanisch trainiert ist, und auch ein mehrsprachiges BERT-Modell. Beide Modelle verwenden wir über die Transformer-Bibliothek von HuggingFace. Das sind die Ergebnisse, die wir erhalten haben. Wie Sie sehen können, schneidet das mehrsprachige BERT sowohl im Entwicklungssatz als auch im Testsatz und bei allen Metriken besser ab als BETO. Das CRF-Modell hat 82 erreicht, nur damit wir einen Vergleich ziehen können. Das CRF-Modell erreichte einen F1-Score von 55, während das mehrsprachige BERT 82 erreichte, was ein großer Unterschied ist.Nachdem wir also diese Ergebnisse hatten, stellten wir uns eine weitere Frage, nämlich: Können wir ein BiLSTM-CRF-Modell finden, verschiedene Arten von Einbettungen darin einspeisen, Einbettungen, die verschiedene Arten von sprachlichen Informationen kodieren, und die Ergebnisse von Transformer-basierten Modellen übertreffen? Dafür haben wir einige präliminäre Experimente durchgeführt, und zwar mit dem BiLSTM-CRF-Modell unter Verwendung von Flare Library. Wir haben mit verschiedenen Arten von Einbettungen experimentiert, z. B. mit Transformer-basierten, aber auch mit Schnell-Text-Einbettungen und Zeichen-Einbettungen. Wir haben herausgefunden, dass Transformer-basierte Einbettungen besser abschneiden als nicht kontextualisierte Einbettungen, dass die Kombination aus englischer BERT- und spanischer BETO-Einbettung besser ist als mehrsprachige BERT-Einbettungen. Auch ergeben die BPE-Einbettungen ein besseres F1 und die Zeicheneinbettungen ein besseres Recall.Vor diesem Hintergrund waren dies die besten Ergebnisse, die wir erzielen konnten. Beide Modelle waren BiLSTM-CRF-Modelle unter Verwendung von Flare. Bei einem wurden BETO- und BERT-Einbettungen und BPE eingespeist, beim anderen BETO- und BERT-Einbettungen und BPE sowie Zeichen-Einbettungen. Letzteres war dasjenige, das den höchste F1-Score beim Testsatz erzielte, obwohl der höchste Score beim Entwicklungssatz durch das Modell ohne Zeichen-Einbettungen erreicht wurde. Vergessen Sie nicht, dass das beste Ergebnis, das wir mit mehrsprachigem BERT erzielt haben, einen F1-Wert von 76 im Entwicklungssatz und 82 im Testsatz erreichte. Dies ist also eine Verbesserung im Vergleich zu diesen Ergebnissen. Schließlich stellten wir uns noch eine weitere Frage: Kann die Erkennung von lexikalischen Entlehnungen als Transferlernen von Sprachidentifikation beim Code-Switching formuliert werden? Wir haben also dasselbe BiLSTM-CRF-Modell wie mit Flare verwendet, aber anstelle dieser nicht angepassten Transformer-basierten BETO- und BERT-Einbettungen haben wir Code-Switch-Einbettungen verwendet. Was sind Code-Switch-Einbettungen? Dies sind Einbettungen, die auf Transformer-basierte Einbettungen abgestimmt wurden. Diese wurden für die Sprachidentifikation im Spanisch-Englisch-Abschnitt des LinCE-Code-Switching-Datensatzes vortrainiert. LinCE ist ein Datensatz vom Code-Switching, der einen Abschnitt mit Code-Switching von Spanisch und Englisch enthält. Wir speisten also Code-Switch-Einbettungen in unser BiLSTM-CRF ein. Optional können Zeicheneinbettungen, BPE-Einbettungen und so weiter eingefügt werden. Das beste Ergebnis, das wir erzielt haben, war 84,22. Das ist das beste Ergebnis aller Modelle, die wir mit dem Testsatz ausprobiert haben. Obwohl der beste F1-Score, den wir beim Entwicklungssatz erzielt haben, 97 war, war dieser niedriger als das beste Ergebnis vom BiLSTM-CRF, das mit unangepassten Einbettungen eingespeist war. Hier sind einige Schlussfolgerungen aus unserer Arbeit. Wir haben einen neuen Datensatz mit spanischen Nachrichten erstellt, der mit nicht assimilierten lexikalischen Entlehnungen annotiert ist. Dieser Datensatz ist dichter an Entlehnungen und OOV-reicher als frühere Ressourcen. Wir haben vier Arten von Modellen für die Erkennung lexikalischer Entlehnungen erforscht. Also. Was die Fehleranalyse betrifft, war der Recall ein Schwachpunkt bei allen Modellen. Wie Sie hier sehen können, gehören zu den häufigen falsch-negativen Ergebnissen beispielsweise auch Entlehnungen in Großbuchstaben und Wörter, die es sowohl im Englischen als auch im Spanischen gibt. Interessant ist auch, dass BPE-Einbettungen den F1-Core zu verbessern scheinen. Und die Einbettung von Zeichen scheint den Recall zu verbessern. Das ist eine interessante Erkenntnis, die wir vielleicht in künftigen Arbeiten untersuchen können. Also. Das wäre alles, was ich zu sagen habe. Vielen Dank fürs Zuhören."""},  # noqa
    ]}, {
    "track": "long",
    "text_lang": "zh",
    "samples": [
        {"id": "4", "output": "演讲者是Elena Álvarez-Mellado."},
    ]}, {
    "track": "long",
    "text_lang": "it",
    "samples": [
        {"id": "4", "output": "Questo lavoro presenta una nuova risorsa per l'identificazione di prestiti linguistici e analizza le prestazioni e gli errori di diversi modelli in questo compito. Introduciamo un nuovo corpus annotato di notizie spagnole ricco di prestiti linguistici, parole di una lingua che vengono introdotte in un'altra senza adattamento ortografico, e lo utilizziamo per valutare le prestazioni di diversi modelli per l'etichettatura di sequenze (modelli CRF, BiLSTM-CRF e basati su Transformer). Il corpus contiene 370.000 parole ed è più grande, più denso di prestiti, ricco di OOV e vario per argomento rispetto ai precedenti corpora disponibili per questo compito. I nostri risultati mostrano che un modello BiLSTM-CRF che riceve in input rappresentazioni a sottoparole insieme modelli basati su Transformer preaddestrati su dati con commutazione di codice o una combinazione di rappresentazioni contestualizzate a parole supera i risultati ottenuti da un modello multilingue basato su BERT."},  # noqa
    ]}]


xml = ET.Element("testset", attrib={'name': "IWSLT2025", "type": "output"})
for tst_output in tst_outputs:
    xml_track = ET.SubElement(xml, "task", attrib={key: tst_output[key] for key in TASK_ATTRIB})
    for sample in tst_output["samples"]:
        ET.SubElement(xml_track, "sample", attrib={'id': sample['id']}).text = sample['output']

tree = ET.ElementTree(xml)
ET.indent(tree)
tree.write(sys.stdout.buffer, encoding="utf-8", xml_declaration=True)
