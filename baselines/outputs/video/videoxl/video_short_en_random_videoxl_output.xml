<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Web pages and books</sample>
    <sample id="1">McGill, Mila, Microsoft Research</sample>
    <sample id="35">Patric F. Fernandes</sample>
    <sample id="36">LLM</sample>
    <sample id="37">Yes</sample>
    <sample id="38">The proposed human evaluation method is novel because it introduces a new approach to evaluating chatbots by focusing on the relevance and self-contradiction of their responses, which are not typically considered in traditional evaluation methods.</sample>
    <sample id="39">The quality of the labels.</sample>
    <sample id="40">We ask annotators to listen to at least 10 seconds of each song and read about the song.</sample>
    <sample id="41">6</sample>
    <sample id="75">3</sample>
    <sample id="76">news</sample>
    <sample id="77">[not] is it [is] on the [left] and [Ned] laughed.</sample>
    <sample id="78">Yes, the models are freely available.</sample>
    <sample id="79">DEplain-apa contains documents from the academic press.</sample>
    <sample id="80">Bigger model size, more fine-tuning examples</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by the difference in length between the left and right conjuncts.</sample>
    <sample id="82">The experiments were designed to study the effect of the governorâ€™s position by varying the length of the shaft and the position of the governor.</sample>
    <sample id="83">Not well</sample>
    <sample id="84">4</sample>
    <sample id="85">John and Mary</sample>
    <sample id="86">Formal/lexical cohesion, ellipsis, pronouns, and verb form.</sample>
    <sample id="87">Johns Hopkins University, Purdue University</sample>
    <sample id="88">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Mathias Lindemann, Alexander Koller, Ivan Tivot</sample>
    <sample id="89">Compositionality without Trees using Multiset Tagging and Latent Permutations Matthias Lindermann, Alexander Koller, Ivan Tivot</sample>
    <sample id="90">Compositionality Generalization Ability of a learner to handle deeper recursion and uncompositions of phrases that have been seen individually during training.</sample>
    <sample id="91">The girl slept.</sample>
    <sample id="92">The girl slept.</sample>
    <sample id="93">The girl slept.</sample>
    <sample id="94">The girl slept.</sample>
    <sample id="95">Compositionality in Semantic Parsing Naive seq2seq models fail!</sample>
    <sample id="96">Compositionality in Semantic Parsing Naive seq2seq models fail!</sample>
    <sample id="97">Trees help a lot ... but \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent</sample>
    <sample id="98">Trees help a lot ... but \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent</sample>
    <sample id="99">Trees help a lot ... but... \n\nThe girl slept.</sample>
    <sample id="100">Trees help a lot ... but... \n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</sample>
    <sample id="101">Trees help a lot ... but Trees need to be obtained Pre-Post Processing Logical Forms Causal-Induction</sample>
    <sample id="102">Trees help a lot ... Trees need to be obfuscated. This paper neural sequence model directly models the correspondence between fragments. For the first time, we show generalization to deeper recursion about trees.</sample>
    <sample id="103">Trees help a lot ... Trees need to be obfuscated. This paper neural seq2seq model directly models the correspondence between fragments. For the first time, we show generalization to deeper recursion about trees.</sample>
    <sample id="104">Our Approach</sample>
    <sample id="105">Our Approach</sample>
    <sample id="106">Our Approach</sample>
    <sample id="107">Our Approach sleep agent x2 tag the girl sleep agent x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x1 x</sample>
    <sample id="108">Our Approach sleep agent x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1</sample>
    <sample id="109">Permuting with "jumps"</sample>
    <sample id="110">Permuting with 'jumps' Permute agent sleep the girl</sample>
    <sample id="111">Permuting with "jumps" Permute agent 1 agent 2 agent 3 agent 4 agent 5 agent 6 agent 7 agent 8 agent 9 agent 10 agent 11 agent 12 agent 13 agent 14 agent 15 agent 16 agent 17 agent 18 agent 19 agent 20 agent 21 agent 22 agent 23 agent 24 agent 25 agent 26 agent 27 agent 28 agent 29 agent 30 agent 31 agent 32 agent 33 agent 34 agent 35 agent 36 agent 37 agent 38 agent 39 agent 40 agent 41 agent 42 agent 43 agent 44 agent 45 agent 46 agent 47 agent 48 agent 49 agent 50 agent 51 agent 52 agent 53 agent 54 agent 55 agent 56 agent 57 agent 58 agent 59 agent 60 agent 61 agent 62 agent 63 agent 64 agent 65 agent 66 agent 67 agent 68 agent 69 agent 70 agent 71 agent 72 agent 73 agent 74 agent 75 agent 76 agent 77 agent 78 agent 79 agent 80 agent 81 agent 82 agent 83 agent 84 agent 85 agent 86 agent 87 agent 88 agent 89 agent 90 agent 91 agent 92 agent 93 agent 94 agent 95 agent 96 agent 97 agent 98 agent 99 agent 100 agent 101 agent 102 agent 103 agent 104 agent 105 agent 106 agent 107 agent 108 agent 109 agent 110 agent 111 agent 112 agent 113 agent 114 agent 115 agent 116 agent 117 agent 118 agent 119 agent 120 agent 121 agent 122 agent 123 agent 124 agent 125 agent 126 agent 127 agent 128 agent 129 agent 130 agent 131 agent 132 agent 133 agent 134 agent 135 agent 136 agent 137 agent 138 agent 139 agent 140 agent 141 agent 142 agent 143 agent 144 agent 145 agent 146 agent 147 agent 148 agent 149 agent 150 agent 151 agent 152 agent 153 agent 154 agent 155 agent 156 agent 157 agent 158 agent 159 agent 160 agent 161 agent 162 agent 163 agent 164 agent 165 agent 166 agent 167 agent 168 agent 169 agent 170 agent 171 agent 172 agent 173 agent 174 agent 175 agent 176 agent 177 agent 178 agent 179 agent 180 agent 181 agent 182 agent 183 agent 184 agent 185 agent 186 agent 187 agent 188 agent 189 agent 190 agent 191 agent 192 agent 193 agent 194 agent 195 agent 196 agent 197 agent 198 agent 199 agent 200 agent 201 agent 202 agent 203 agent 204 agent 205 agent 206 agent 207 agent 208 agent 209 agent 210 agent 211 agent 212 agent 213 agent 214 agent 215 agent 216 agent 217 agent 218 agent 219 agent 220 agent 221 agent 222 agent 223 agent 224 agent 225 agent 226 agent 227 agent 228 agent 229 agent 230 agent 231 agent 232 agent 233 agent 234 agent 235 agent 236 agent 237 agent 238 agent 239 agent 240 agent 241 agent 242 agent 243 agent 244 agent 245 agent 246 agent 247 agent 248 agent 249 agent 250 agent 251 agent 252 agent 253 agent 254 agent 255 agent 256 agent 257 agent 258 agent 259 agent 260 agent 261 agent 262 agent 263 agent 264 agent 265 agent 266 agent 267 agent 268 agent 269 agent 270 agent 271 agent 272 agent 273 agent 274 agent 275 agent 276 agent 277 agent 278 agent 279 agent 280 agent 281 agent 282 agent 283 agent 284 agent 285 agent 286 agent 287 agent 288 agent 289 agent 290 agent 291 agent 292 agent 293 agent 294 agent 295 agent 296 agent 297 agent 298 agent 299 agent 300 agent 301 agent 302 agent 303 agent 304 agent 305 agent 306 agent 307 agent 308 agent 309 agent 310 agent 311 agent 312 agent 313 agent 314 agent 315 agent 316 agent 317 agent 318 agent 319 agent 320 agent 321 agent 322 agent 323 agent 324 agent 325 agent 326 agent 327 agent 328 agent 329 agent 330 agent 331 agent 332 agent 333 agent 334 agent 335 agent 336 agent 337 agent 338 agent 339 agent 340 agent 341 agent 342 agent 343 agent 344 agent 345 agent 346 agent 347 agent 348 agent 349 agent 350 agent 351 agent 352 agent 353 agent 354 agent 355 agent 356 agent 357 agent 358 agent 359 agent 360 agent 361 agent 362 agent 363 agent 364 agent 365 agent 366 agent 367 agent 368 agent 369 agent 370 agent 371 agent 372 agent 373 agent 374 agent 375 agent 376 agent 377 agent 378 agent 379 agent 380 agent 381 agent 382 agent 383 agent 384 agent 385 agent 386 agent 387 agent 388 agent 389 agent 390 agent 391 agent 392 agent 393 agent 394 agent 395 agent 396 agent 397 agent 398 agent 399 agent 400 agent 401 agent 402 agent 403 agent 404 agent 405 agent 406 agent 407 agent 408 agent 409 agent 410 agent 411 agent 412 agent 413 agent 414 agent 415 agent 416 agent 417 agent 418 agent 419 agent 420 agent 421 agent 422 agent 423 agent 424 agent 425 agent 426 agent 427 agent 428 agent 429 agent 430 agent 431 agent 432 agent 433 agent 434 agent 435 agent 436 agent 437 agent 438 agent 439 agent 440 agent 441 agent 442 agent 443 agent 444 agent 445 agent 446 agent 447 agent 448 agent 449 agent 450 agent 451 agent 452 agent 453 agent 454 agent 455 agent 456 agent 457 agent 458 agent 459 agent 460 agent 461 agent 462 agent 463 agent 464 agent 465 agent 466 agent 467 agent 468 agent 469 agent 470 agent 471 agent 472 agent 473 agent 474 agent 475 agent 476 agent 477 agent 478 agent 479 agent 480 agent 481 agent 482 agent 483 agent 484 agent 485 agent 486 agent 487 agent 488 agent 489 agent 490 agent 491 agent 492 agent 493 agent 494 agent 495 agent 496 agent 497 agent 498 agent 499 agent 500 agent 501 agent 502 agent 503 agent 504 agent 505 agent 506 agent 507 agent 508 agent 509 agent 510 agent 511 agent 512 agent 513 agent 514 agent 515 agent 516 agent 517 agent 518 agent 519 agent 520 agent 521 agent 522 agent 523 agent 524 agent 525 agent 526 agent 527 agent 528 agent 529 agent 530 agent 531 agent 532 agent 533 agent 534 agent 535 agent 536 agent 537 agent 538 agent 539 agent 540 agent 541 agent 542 agent 543 agent 544 agent 545 agent 546 agent 547 agent 548 agent 549 agent 550 agent 551 agent 552 agent 553 agent 554 agent 555 agent 556 agent 557 agent 558 agent 559 agent 560 agent 561 agent 562 agent 563 agent 564 agent 565 agent 566 agent 567 agent 568 agent 569 agent 570 agent 571 agent 572 agent 573 agent 574 agent 575 agent 576 agent 577 agent 578 agent 579 agent 580 agent 581 agent 582 agent 583 agent 584 agent 585 agent 586 agent 587 agent 588 agent 589 agent 590 agent 591 agent 592 agent 593 agent 594 agent 595 agent 596 agent 597 agent 598 agent 599 agent 600 agent 601 agent 602 agent 603 agent 604 agent 605 agent 606 agent 607 agent 608 agent 609 agent 610 agent 611 agent 612 agent 613 agent 614 agent 615 agent 616 agent 617 agent 618 agent 619 agent 620 agent 621 agent 622 agent 623 agent 624 agent 625 agent 626 agent 627 agent 628 agent 629 agent 630 agent 631 agent 632 agent 633 agent 634 agent 635 agent 636 agent 637 agent 638 agent 639 agent 640 agent 641 agent 642 agent 643 agent 644 agent 645 agent 646 agent 647 agent 648 agent 649 agent 650 agent 651 agent 652 agent 653 agent 654 agent 655 agent 656 agent 657 agent 658 agent 659 agent 660 agent 661 agent 662 agent 663 agent 664 agent 665 agent 666 agent 667 agent 668 agent 669 agent 670 agent 671 agent 672 agent 673 agent 674 agent 675 agent 676 agent 677 agent 678 agent 679 agent 680 agent 681 agent 682 agent 683 agent 684 agent 685 agent 686 agent 687 agent 688 agent 689 agent 690 agent 691 agent 692 agent 693 agent 694 agent 695 agent 696 agent 697 agent 698 agent 699 agent 700 agent 701 agent 702 agent 703 agent 704 agent 705 agent 706 agent 707 agent 708 agent 709 agent 710 agent 711 agent 712 agent 713 agent 714 agent 715 agent 716 agent 717 agent 718 agent 719 agent 720 agent 721 agent 722 agent 723 agent 724 agent 725 agent 726 agent 727 agent 728 agent 729 agent 730 agent 731 agent 732 agent 733 agent 734 agent 735 agent 736 agent 737 agent 738 agent 739 agent 740 agent 741 agent 742 agent 743 agent 744 agent 745 agent 746 agent 747 agent 748 agent 749 agent 750 agent 751 agent 752 agent 753 agent 754 agent 755 agent 756 agent 757 agent 758 agent 759 agent 760 agent 761 agent 762 agent 763 agent 764 agent 765 agent 766 agent 767 agent 768 agent 769 agent 770 agent 771 agent 772 agent 773 agent 774 agent 775 agent 776 agent 777 agent 778 agent 779 agent 780 agent 781 agent 782 agent 783 agent 784 agent 785 agent 786 agent 787 agent 788 agent 789 agent 790 agent 791 agent 792 agent 793 agent 794 agent 795 agent 796 agent 797 agent 798 agent 799 agent 800 agent 801 agent 802 agent 803 agent 804 agent 805 agent 806 agent 807 agent 808 agent 809 agent 810 agent 811 agent 812 agent 813 agent 814 agent 815 agent 816 agent 817 agent 818 agent 819 agent 820 agent 821 agent 822 agent 823 agent 824 agent 825 agent 826 agent 827 agent 828 agent 829 agent 830 agent 831 agent 832 agent 833 agent 834 agent 835 agent 836 agent 837 agent 838 agent 83</sample>
    <sample id="112">Permuting with "jumps"</sample>
    <sample id="113">Permuting with "jumps" Permute sleep agent x2 x1 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2</sample>
    <sample id="114">Some Results on COGS (Kim and Lizenz 2020) Contrast with other Treeless Models on Structural Generalization.</sample>
    <sample id="115">Some Results on COGS (Kim and Lizen 2020) Contrast with other Treeless Models on Structural Generalization.</sample>
    <sample id="116">The technical challenges we solve.</sample>
    <sample id="117">Technical Challenges We Solve Alignment unknown</sample>
    <sample id="118">Technical Challenges We Solve Alignment unknown in training.</sample>
    <sample id="119">Technical Challenges We Solve</sample>
    <sample id="120">Technical Challenges We Solve Permute Alignment unknown in training. Inference: NP-hard (TSP) relaxation Backpropagation through continuous relaxation</sample>
    <sample id="121">Technical Challenges We Solve Inference is NP-hard (harder than TSP) Backpropagation through continuous relaxation Paper &amp; Code http://tiny.cc/ly7x8n</sample>
    <sample id="122">The introduced framework quantifies the positionality via Pearson's r scores.</sample>
    <sample id="123">The video features a static image with text and logos. The main title reads 'Weaker Than You Think' in large white letters, followed by 'A Critical Look at Weekly Supervised Learning' in smaller white letters. Below the title, there are six names listed: Dawid Zaw, Xiu Zhen Shen, 'M. N.' M. Moschel, Andrei Stephens, Dietrich Klakow, and Saarland University, Amazon Alexa, 3U, University of Vienna. At the bottom left corner, there is a logo for Saarlold University with the text 'Department of Language Science and Technology' underneath it. At the bottom right corner, there is a logo for the University of Vienna with the text 'UniversitÃ¤t Wien' underneath it.</sample>
    <sample id="124">The video features a static image with text and logos. The main title reads 'Weaker Than You Think' in large white letters, followed by 'A Critical Look at Weekly Supervised Learning' in smaller white letters. Below the title, there are six names listed: Dawid Zaw, Xiu Zhen Shen, 'M. N.' Marmur, Andrea Stephens, Dietrich Klakow, and Andreas Stephantz. At the bottom of the image, there is a logo for 'ACI 2023' in red and black colors. The background is white, and the overall design is clean and professional.</sample>
    <sample id="125">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="126">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="128">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="130">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XXX."</sample>
    <sample id="131">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XXX."</sample>
    <sample id="132">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy XXX(?)".</sample>
    <sample id="133">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy XXX% ðŸ˜³"</sample>
    <sample id="134">Our research questions R01 Is clean validation data necessary? R02 How many clean samples does DSL approaches need? R03 How to use the available clean samples more efficiently</sample>
    <sample id="135">Our research questions R01 Is clean validation data necessary? R02 How many clean samples does DSL approaches need? R03 How to use the available clean samples more efficiently</sample>
    <sample id="136">The video clip shows a graph with two lines, one in green and the other in orange. The green line represents the relative performance improvement of a certain metric over time, while the orange line represents the relative performance improvement of another metric over time. The graph is divided into three sections, each representing a different week of data collection. The green line shows a general upward trend, indicating that the metric being measured is improving over time. The orange line also shows an upward trend, but it is slightly lower than the green line, suggesting that the other metric is improving at a slower rate. Overall, the graph suggests that both metrics are improving over time, but at different rates.</sample>
    <sample id="137">The video clip shows a graph with two lines, one in green and the other in orange. The green line represents the relative performance improvement of a certain metric over time, while the orange line represents the relative performance improvement of another metric over time. The graph is divided into three sections, each representing a different time period: "FT" (first time), "MLC" (middle time), and "L2R" (last time). The graph shows that the green line generally performs better than the orange line across all three time periods, with some fluctuations in performance.</sample>
    <sample id="138">The video clip shows a graph with three lines, each representing a different variable. The first line is labeled "FT" and is represented by a green line. The second line is labeled "BOND" and is represented by an orange line. The third line is labeled "L2R" and is represented by a purple line. The graph has a horizontal axis labeled "Weeks" and a vertical axis labeled "Relative performance (100%)". The graph shows that the FT line is generally higher than the other two lines, indicating that it has a higher relative performance over time. The BOND and L2R lines are relatively similar to each other, with the BOND line being slightly higher than the L2R line. The graph also includes a title at the top that reads "Main findings" and a subtitle that reads "No validation on validation set". Overall, the graph suggests that the FT variable has a higher relative performance than the BOND and L2R variables over time, but that the BOND and L2R variables are relatively similar to each other.</sample>
    <sample id="139">The video clip shows a graph with three lines, each representing a different variable. The first line is labeled "FT" and is represented by a green line. The second line is labeled "BOND" and is represented by an orange line. The third line is labeled "L2R" and is represented by a purple line. The graph has a horizontal axis labeled "Weeks" and a vertical axis labeled "Relative performance (100%)". The graph shows that the FT line is generally higher than the other two lines, indicating that it has a higher relative performance. The BOND and L2R lines are relatively similar in terms of their relative performance, with the BOND line being slightly higher than the L2R line. Overall, the graph suggests that the FT variable has a higher relative performance than the BOND and L2R variables over time.</sample>
    <sample id="140">The video clip features a series of static slides from a presentation, each displaying a graph with various data points and lines. The graphs are labeled with different categories such as "FT," "BOND," "COSINE," "MLC," and "LR." Each graph has a title indicating the main findings or results, and the bottom of the slide includes a statement about the importance of a clean validation set. The background of the slides is white, and the text and graphs are in shades of green, orange, and purple. The overall theme of the video appears to be focused on presenting statistical or performance data related to a specific topic or study.</sample>
    <sample id="141">The graph shows the accuracy of different methods in a validation process. The x-axis represents the validation set size, ranging from 10 to 50, while the y-axis represents the accuracy percentage, ranging from 75 to 85. The graph displays multiple lines, each representing a different method or approach. The lines are color-coded and labeled accordingly. The graph is used to compare the performance of these methods as the validation set size increases.</sample>
    <sample id="142">The graph shows the accuracy of different methods in a validation process. The x-axis represents the validation set size, ranging from 10 to 50, while the y-axis represents the accuracy percentage, ranging from 75 to 85. The graph displays multiple lines, each representing a different method or approach. The lines are color-coded and labeled accordingly. The graph also includes shaded areas around some lines, which likely represent confidence intervals or error margins. Overall, the graph provides a visual comparison of the performance of various methods in terms of accuracy as the validation set size increases.</sample>
    <sample id="143">The video shows a graph with two graphs. The first graph is a line graph with a red line, a green line, and a blue line. The second graph is a bar graph with different colored bars representing different data points. The video also includes a text box in the top left corner that reads "Main findings" and a red box around a specific area of the graph.</sample>
    <sample id="144">The graph shows the performance of different models on a validation set. The graph is divided into two sections: the left section shows the accuracy of each model over time, while the right section shows the performance delta of each model compared to the best-performing model. The graph indicates that the L2R model has the highest accuracy and the largest performance delta, suggesting it is the most effective model for this particular task.</sample>
    <sample id="145">The video shows a graph with two graphs. The first graph is a line graph with four lines, each representing a different dataset. The second graph is a bar graph with three bars, each representing a different metric. The video appears to be a presentation or lecture, possibly related to data analysis or machine learning. The graphs are likely used to illustrate the performance of different models or algorithms on various datasets. The video may be discussing the importance of using clean validation samples for training and testing machine learning models. Overall, the video seems to be a technical presentation aimed at an audience interested in data analysis or machine learning.</sample>
    <sample id="146">The video clip shows a graph with two lines, one in orange and the other in purple. The orange line represents the accuracy of a certain method or algorithm, while the purple line represents the accuracy of another method or algorithm. The graph is divided into three sections, labeled "Before," "After," and "Clean Only." The orange line shows a significant increase in accuracy from the "Before" section to the "After" section, indicating that the method or algorithm being tested has improved its performance after some kind of intervention or change. The purple line also shows an increase in accuracy from the "Before" section to the "After" section, but it is not as steep as the orange line. Overall, the graph suggests that both methods or algorithms have improved their accuracy after some kind of intervention or change, but the orange line represents a more significant improvement.</sample>
    <sample id="147">The video clip shows a graph with two lines, one in orange and the other in purple. The orange line represents the accuracy of a certain model or algorithm, while the purple line represents the accuracy of another model or algorithm. The graph is divided into three sections, labeled "Before," "After," and "Clean Only." The orange line shows a significant increase in accuracy from the "Before" section to the "After" section, indicating that the model or algorithm has been improved or optimized. The purple line also shows an increase in accuracy from the "Before" section to the "After" section, but to a lesser extent than the orange line. The "Clean Only" section shows a slight decrease in accuracy for both lines, suggesting that cleaning the data may have some negative impact on the model's performance. Overall, the graph suggests that the model or algorithm represented by the orange line has been significantly improved or optimized, while the model or algorithm represented by the purple line has also shown some improvement, but to a lesser extent.</sample>
    <sample id="148">The video clip shows a graph with two lines, one in orange and the other in purple. The orange line represents the accuracy of a certain method or algorithm, while the purple line represents the accuracy of another method or algorithm. The graph is divided into three sections, labeled "Before," "After," and "Clean Only." The orange line shows a significant increase in accuracy from the "Before" section to the "After" section, indicating that the method or algorithm being tested has improved its performance after some kind of intervention or change. The purple line also shows an increase in accuracy from the "Before" section to the "After" section, but it is not as steep as the orange line. Overall, the graph suggests that both methods or algorithms have improved their accuracy after some kind of intervention or change, but the orange line represents a more significant improvement than the purple line.</sample>
    <sample id="149">The video clip shows a graph with two lines, one in red and the other in blue. The red line represents the performance of a certain method or system, while the blue line represents the performance of another method or system. The graph shows that the red line is consistently higher than the blue line, indicating that the method or system represented by the red line performs better than the one represented by the blue line. The graph also includes a label "Main findings" at the top, suggesting that this is a key finding or result from a study or analysis. Overall, the video appears to be a presentation or lecture on the performance comparison between two different methods or systems, with the red line representing the superior performer.</sample>
    <sample id="150">The conclusion of the slide is that recent WSL approaches require clean samples and overestimate their practicality. Our recommendations are to report model selection criteria, use few-shot learning approaches as baselines, and apply continuous fine-tuning (CFT).</sample>
    <sample id="151">The conclusion of the slide is that recent WSL approaches require clean samples and overestimate their practicality. Our recommendations are to report model selection criteria, use few-shot learning approaches as baselines, and apply continuous fine-tuning (CFT).</sample>
    <sample id="152">Conclusion Recent WSL approaches  Overestimate their practicality. Our recommendations Use Few-shot learning approach criteria as baselines Apply continuous fine-tuning (CFT)</sample>
    <sample id="153">The conclusion of the slide is displayed.</sample>
    <sample id="154">Conclusion Recent WSL approaches  Overestimate their practicality. Our recommendations Use Few-shot learning approach criteria as baselines. Apply continuous fine-tuning (CFT). Thank you!</sample>
    <sample id="155">The study found that the AI-generated personas were rated as more human-like than those generated by ChatGPT.</sample>
    <sample id="156">Penn Treebank, Marcus et al., 1993; Fricke and Goldberg, 2016</sample>
    <sample id="157">2</sample>
    <sample id="158">Debate, CE, DEBATE</sample>
    <sample id="159">two</sample>
    <sample id="160">6</sample>
    <sample id="161">The introduced framework differs from the previous works by using a combination of model-based and data-based approaches to compare annotations.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate.</sample>
    <sample id="200">6</sample>
    <sample id="201">Up to 900 tokens.</sample>
    <sample id="202">music, books, recipes</sample>
    <sample id="203">The perspectives people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="204">Dawid Zuch</sample>
    <sample id="205">Yes, it does.</sample>
    <sample id="206">4</sample>
    <sample id="207">yes</sample>
    <sample id="208">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">How to use the available clean samples more efficiently.</sample>
    <sample id="211">The metric sensitivity works by measuring how sensitive the model is to variations in instructions for the same task.</sample>
    <sample id="212">Wenwen Jiang</sample>
    <sample id="213">Greater sensitivity suggests the opposite, indicating poorer model performance.</sample>
    <sample id="214">Contextualized</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford Engineering Computer Science</sample>
    <sample id="217">Because existing methods are not sufficient.</sample>
    <sample id="218">Jackie CK Chuang</sample>
    <sample id="219">The political bias propagation pipeline is a three-step process: pretraining data, language models, and downstream tasks.</sample>
    <sample id="220">yes</sample>
    <sample id="221">No</sample>
    <sample id="222">The watermark is inserted by adding a small number to the embedding of each word in the text.</sample>
    <sample id="223">Penn State and Amazon</sample>
    <sample id="224">Yes</sample>
    <sample id="225">How to make a chocolate cake</sample>
    <sample id="226">They use a random seed to ensure the covertness of their method.</sample>
    <sample id="227">By pre-training on a large corpus of text.</sample>
    <sample id="228">Catholic Europe</sample>
    <sample id="229">I am a student.</sample>
    <sample id="230">The more tasks, the better the model performs.</sample>
    <sample id="231">The authors compare their method with LSTM, T5-seq2seq, and Zhen's method.</sample>
    <sample id="232">colleagues</sample>
    <sample id="233">Crawford</sample>
    <sample id="274">3</sample>
    <sample id="275">The speaker suggests using a diverse set of data sources to mitigate social and political biases in datasets.</sample>
    <sample id="307">PaLM has comparable fluency to SOTA systems.</sample>
    <sample id="308">Utility, should not degrade the utility of the provided embeddings, should cover the attacker, and should be transferable.</sample>
    <sample id="309">Arabic, Chinese, German, Italian, Japanese, Korean, Portuguese, Romanian, Russian, Turkish, Urdu, Vietnamese</sample>
    <sample id="310">20</sample>
    <sample id="311">Cosine similarity and p-value of KS test</sample>
    <sample id="312">They were pretrained on the multilingual M300 dataset.</sample>
    <sample id="313">The 6th Annual Meeting of the Association for Computational Linguistics was held in Toronto, Canada in 2023. The theme of this year's conference is "Distilling Script Knowledge from Large Language Models for Constrained Language Planning."</sample>
    <sample id="314">Language Planning How to Make a Cake? Large language models (LLMs) can effectively decompose goals into steps.</sample>
    <sample id="315">Language Planning How to Make a Cake? Large language models (LLMs) can effectively decompose goals into steps.</sample>
    <sample id="316">Constrained Language Planning How Make a Strawberry Cake? + Add strawberry jams into the flour... How Make a Chocolate Cake? + Add the cocoa powder into the flour... Goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="317">Constrained Language Planning How Make a Strawberry Cake? + Add strawberry jams into the flour... How Make a Chocolate Cake? + Add the cocoa powder into the flour... Goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="318">Constrained Language Planning How Make a Strawberry Cake? + Add strawberry jams into the flour... How Make a Chocolate Cake? + Add the cocoa powder into the flour... Goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="319">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="320">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="321">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="322">Can LLMs do Constrained Language Planning?</sample>
    <sample id="323">Can LLMs do Constrained Language Planning?</sample>
    <sample id="324">What types of errors do LLMs usually make in this task?</sample>
    <sample id="325">What types of errors do LLMs usually make in this task?</sample>
    <sample id="326">What kinds of goals do InstructGPT typically fail?</sample>
    <sample id="327">The video features a woman in a green shirt speaking directly to the camera. The background is an indoor setting with white walls and large windows. The lighting is bright, and the overall atmosphere is professional. The woman appears to be presenting or explaining something, as she maintains eye contact with the camera and uses hand gestures to emphasize her points. The video seems to be a tutorial or instructional video, possibly related to a specific topic or subject matter.</sample>
    <sample id="328">The video features a woman in a green shirt speaking to the camera. The background is an office setting with white walls and large windows. The woman has long brown hair and glasses. She is wearing a green shirt and has a friendly expression on her face. The video appears to be a tutorial or instructional video, as the woman is explaining a concept or process. The video is well-lit and the audio is clear, making it easy to follow along. Overall, the video is informative and engaging, with the woman's friendly demeanor making it approachable for viewers.</sample>
    <sample id="329">The video features a woman in a green shirt speaking directly to the camera. She is positioned in front of a whiteboard with diagrams and text related to a method or process. The whiteboard includes sections labeled 'Input,' 'Abstract Goal,' 'Step 1,' 'Step 2,' and 'Step 3,' each containing different elements such as goals, methods, and candidate scripts. The woman appears to be explaining or presenting this information, likely in an educational or instructional context.</sample>
    <sample id="330">The video clip features a woman in a green shirt speaking to the camera. She is standing in front of a whiteboard with diagrams and text written on it. The diagrams appear to be related to a method or process, with steps and candidate scripts listed. The woman seems to be explaining or presenting this information to the viewer.</sample>
    <sample id="331">The video clip features a woman in a green shirt speaking to the camera. She is standing in front of a whiteboard with diagrams and text written on it. The diagrams appear to be related to a method or process, with steps and candidate scripts listed. The woman seems to be explaining or presenting this information to the viewer.</sample>
    <sample id="332">The video clip features a woman in a green shirt speaking to the camera. The background is an office setting with white walls and furniture. On the left side of the screen, there is a diagram with text and numbers. The woman appears to be explaining a method or process, as indicated by the diagram and her gestures. She is wearing glasses and has long hair. The overall atmosphere is professional and informative.</sample>
    <sample id="333">The video features a woman in a green shirt speaking directly to the camera. The background is an indoor setting with white walls and some furniture visible. A bar graph appears on the screen, comparing the accuracy of different methods, including T5 (11B), Flan-T5-Large, CodeGPT-3 (175B), Codex (175B), and InstructGPT. The text on the screen reads 'Our Method Greatly Improves the Planning Quality' and 'Our method, InstructGPT can generate copies of higher quality by a large margin.' The woman continues to speak while the bar graph remains on the screen.</sample>
    <sample id="334">The video clip features a woman with long brown hair, wearing a green top, speaking in front of a whiteboard. The whiteboard displays the title 'Script Distillation from LLMs' and several bullet points outlining the motivation, method, and output goals of the project. The woman appears to be explaining the project's purpose and methodology, using hand gestures to emphasize her points. The background is a plain white wall, and the lighting is bright and even, highlighting the woman and the whiteboard.</sample>
    <sample id="335">The video clip features a woman with long brown hair, wearing glasses and a green top, speaking directly to the camera in an indoor setting. The background is blurred, but it appears to be a modern office or living space with white walls and some furniture visible. The woman has a friendly and approachable demeanor as she engages with the audience, likely discussing a topic related to language models or artificial intelligence.</sample>
    <sample id="336">The video clip features a woman in a green shirt speaking directly to the camera. She is standing in front of a whiteboard with text and diagrams written on it. The text on the whiteboard reads "Script Distillation from LLMs" and includes a flowchart with steps such as "Input: Abstract," "Generate 50,000 Scripts," "On Filter 3 Goals," and "Output: Goals with Compositional Plans." The woman appears to be explaining the process of script distillation from large language models (LLMs) and how it can be used to generate scripts for AI systems. The background of the video is a blurred office setting with desks, chairs, and other people working.</sample>
    <sample id="337">The video clip features a woman in a green shirt speaking directly to the camera. She is standing in front of a whiteboard with text and diagrams written on it. The text on the whiteboard reads "Script Distillation from LLMs" and includes a flowchart with steps such as "Input: Abstract," "Generate 50,000 Scripts," "On Filter 3 Scripts for Goal with highest score," and "Output: Goals with Computed Specific Plans." The woman appears to be explaining the process of script distillation from large language models (LLMs) and how it can be used to generate specific plans for achieving goals.</sample>
    <sample id="338">The video features a woman in a green shirt speaking directly to the camera. The background is an indoor setting with a modern design, featuring white walls and various pieces of furniture. The lighting is bright and even, highlighting the speaker's face and upper body. The overall atmosphere is professional and focused on delivering information clearly.</sample>
    <sample id="339">The video features a woman in a green shirt and glasses, who is speaking to the camera. She is standing in front of a whiteboard with text on it, which appears to be related to language models and metrics. The woman is explaining the concept of constraint analysis for smaller language models, and she mentions that the generated texts are faithful to the constraints. She also talks about the use of datasets such as CoSprint and WikiHow, and the metrics used to evaluate the generated texts, including ROUGE, BLEU, and BERTScore. The video also includes a bar graph comparing the accuracy of different language models, including GPT-3, Codex, InstructGPT, and T5 trained on Wikitext and CoSprint. Overall, the video provides an overview of the evaluation of language models and their ability to generate faithful texts based on specific constraints.</sample>
    <sample id="340">The video features a woman with long brown hair, wearing glasses and a green shirt, speaking in front of a whiteboard. The whiteboard displays various graphs and text related to the topic she is discussing. The background shows a modern office setting with desks, chairs, and other office equipment. The woman appears to be giving a presentation or lecture, explaining the content on the whiteboard.</sample>
    <sample id="341">The video features a woman in a green shirt speaking to the camera. She is standing in front of a whiteboard with text written on it. The text on the whiteboard reads: 'Summary and Takeaways' and 'Establish the constrained language planning problem.' The woman is speaking about the importance of establishing a constrained language planning problem and developing a post-ranking approach for improving LLMs. She also mentions the limitations of the proposed method and the need for future work. The video appears to be a lecture or presentation, possibly related to language modeling or artificial intelligence. The woman's tone is informative and engaging, and she seems to be passionate about the topic. Overall, the video provides valuable insights into the field of language modeling and the challenges of developing effective LLMs.</sample>
    <sample id="342">The video features a woman in a green shirt speaking to the camera. The background is a white room with a red chair and a table. The video also includes a slide with text and a QR code. The slide has a red header that reads "Summary and Takeaways" and lists several bullet points, including "Establish the constrained language planning problem," "Evaluate constrained-language-the ability of LLMs," and "Develop an over-generate-then-filter method." The slide also includes a QR code and a website link.</sample>
    <sample id="343">The 6th Annual Meeting of the Association for Computational Linguistics was held in Toronto in 2023. The topic of the talk is Distilling Script Knowledge from Large Language Models for Constrained Language Planning.</sample>
    <sample id="344">They randomly select n words from a moderate-frequency interval.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James Finch, and Jinho D. Choi Emory University Emory NLP Research Lab Alexa</sample>
    <sample id="372">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University Emory NLP Research Lab Alexa</sample>
    <sample id="373">The video features a presentation slide titled 'Comparative Evaluation' with the logos of Emory University and the Allen Institute for Artificial Intelligence at the bottom. The slide displays two sets of speech bubbles, one set in blue and the other in purple, each containing a black silhouette of a person's head. The blue speech bubbles are connected to a blue box labeled 'A', while the purple speech bubbles are connected to a purple box labeled 'B'. The background is white, and the text and logos are clearly visible.</sample>
    <sample id="374">The video clip appears to be a presentation or lecture, likely related to legal or comparative studies. The content focuses on the evaluation of different methods for comparing and rating individuals or entities. The speaker discusses the use of Likert scales and comparative evaluation methods, providing examples and explanations to illustrate these concepts. The visual aids include diagrams with speech bubbles and a scale from 1 to 5, representing different levels of agreement or evaluation.</sample>
    <sample id="375">The video features a presentation slide titled 'Dimensions of Dialogue Quality' with three key points: Relevance, Consistency, and Emotional Understanding. The slide is displayed in front of a person who appears to be giving a lecture or presentation. The background of the slide is blue, and the text is in white. The slide also includes logos for Emory University and Alora at the bottom corners.</sample>
    <sample id="376">The video clip features a presentation slide titled 'Likert Rating Evaluation' with a blue background and white text. The slide includes an illustration of a judge holding a gavel, a series of five speech bubbles representing responses, and a green checkmark indicating the correct response. The slide also displays the logos of Emory University and Alora at the bottom corners. The presenter, whose face is not visible, appears in the top right corner of the frame, providing information about the Likert rating evaluation process.</sample>
    <sample id="377">The video clip features a presentation slide titled 'Liker Rating Evaluation' with a blue background and white text. The slide includes an illustration of a judge holding a gavel, a series of five speech bubbles representing responses, and a green checkmark indicating the correct response. The slide also displays the logos of Emory University and Alora. The presenter, wearing a suit and tie, is visible in the top right corner of the frame, providing information about the evaluation process.</sample>
    <sample id="378">The video shows a presentation slide titled 'Annotating Behaviors in Chat (ABC-Eval)' with three speech bubbles labeled 'Inrelevant,' 'Lack of Empathy,' and 'Self-Contradiction.' The slide is divided into three sections, each containing a speech bubble. The first section has a blue speech bubble labeled 'Inrelevant,' the second section has two blue speech bubbles labeled 'Lack of Empathy' and 'Self-Contradiction,' and the third section has another blue speech bubble labeled 'Inrelevant.' The background of the slide is white, and there are logos at the bottom left corner that read 'Emory University' and 'alora.'</sample>
    <sample id="379">The video clip features a static image with text and graphics. The main focus is on the topic of annotating behaviors in chat, specifically highlighting three types of behaviors: irrelevant, lack of empathy, and self-contradiction. The graphic includes two speech bubbles, one labeled "Irrelevant" and the other "Lack of Empathy Self-Contradiction," connected by arrows to a central blue robot icon. The background is white, and the text is primarily black with some blue accents. The overall design is simple and informative, aimed at explaining how to identify and categorize different types of behaviors in chat conversations.</sample>
    <sample id="380">The ABC model is a framework for understanding and evaluating behaviors. It consists of three components: A (Antecedent), B (Behaviors), and C (Consequences). The model helps individuals identify the underlying causes of their behaviors and understand how they are influenced by their environment and experiences. By analyzing these components, individuals can gain insights into their own behavior patterns and develop strategies to improve their emotional well-being.</sample>
    <sample id="381">The video is a presentation.</sample>
    <sample id="382">The video is a lecture on the topic of 'ABC-Eval Behaviors'. The speaker, who is not visible in the video, discusses various behaviors and their implications. The video features a series of slides with text and bullet points that highlight key points related to the topic. The slides are divided into sections, each with a title and a list of bullet points. The titles include 'Ignoring Partner', 'Inlevant', 'Self-Contradiction', 'Partner Contradiction', 'Knowledge', 'Incorrect Fact', 'Commonsense Violation', 'Emotional Understanding', and 'Lack of Empathy'. The bullet points provide further details and examples related to each title. The video appears to be an educational or instructional presentation, possibly used in a classroom or online learning environment. The speaker's voice is clear and concise, and the slides are well-organized and easy to follow. Overall, the video provides a comprehensive overview of the topic of ABC-Eval Behaviors, and the use of slides and bullet points makes it easy for viewers to understand and retain the information presented.</sample>
    <sample id="383">The speaker is discussing the evaluation of open-domain dialogue models.</sample>
    <sample id="384">The speaker is discussing the experiments conducted on the Open-Domain Dialogue Models.</sample>
    <sample id="385">The video features a presentation slide titled 'Baseline Evaluations' with three columns, each representing different evaluation criteria. The first column is labeled 'Turn Lifter,' the second 'Dialogic Likert,' and the third 'Comparative.' Each column lists various evaluation metrics such as Consistency, Emotional Understanding, Informational, Overall Quality, and more. The slide includes icons and checkmarks to indicate the evaluation process. The background of the slide is blue, and the text is in white and pink. The presenter, wearing a light-colored shirt, stands in front of the slide, providing an explanation or discussion related to the content displayed.</sample>
    <sample id="386">The speaker is discussing the inter-annotator agreement for different dialogue turn types in a study. The speaker explains that the data shows high agreement for 'ABC Eval' and 'Turn Liker,' but lower agreement for 'Turn Liker' and 'Comparative.' The speaker also mentions that the data was collected from 100 conversations, with each conversation having two annotators who rated the dialogue turns on a Likert scale.</sample>
    <sample id="387">The video features a presentation with a speaker discussing the predictive validity of different evaluation methods. The speaker, dressed in a dark top, is positioned on the right side of the screen against a plain background. On the left side, there are three graphs labeled 'Interactive Q&amp;A,' 'Turn Likert,' and 'Dial Likert,' each representing different evaluation methods. The graphs display various metrics such as 'ABC-Eval,' 'Turn Likert,' 'Dial Likert,' and 'Comparative.' The speaker explains that the 'Interactive Q&amp;A' method has the highest predictive validity, followed by 'Turn Likert' and 'Dial Likert,' which have similar values. The speaker emphasizes that 'Comparative' has the lowest predictive validity. The presentation includes logos for 'Emory' and 'alora' at the bottom corners, indicating affiliations or sponsors.</sample>
    <sample id="388">The video features a presentation with a graph titled 'Predictive Validity' and a list of categories such as 'ABC-Eval,' 'Turn Likert,' 'Dial Likert,' and 'Comparative.' The graph displays the percentage of quality explanation for each category, with interactive and non-interactive data points represented by different colors. The presenter discusses the predictive validity of these categories, highlighting specific data points on the graph. The background includes logos of 'Emory' and 'alora,' indicating the institutions involved in the study or presentation.</sample>
    <sample id="389">The video features a presentation slide titled 'Incremental Validity' with three graphs labeled 'AB-Val,' 'Turn Liker,' and 'Dialog Liker.' The graphs display various metrics such as 'Self-Center,' 'Unpredictable,' 'Productive,' 'Emotional,' 'Empowering,' and 'Reliable.' The presenter, wearing a light blue shirt, is visible in the top right corner of the frame. The background of the slide is dark blue, and the text and graphs are in shades of orange and blue. The Emory University logo is displayed at the bottom left corner, and the logo of 'alora' is at the bottom right corner.</sample>
    <sample id="390">The video features a presentation slide titled 'Incremental Validity' with a graph comparing three dialogues: AB-AB, Turn Liker, and Dialog Liker. The presenter, dressed in a light-colored shirt, stands to the right of the screen, gesturing towards the graph as he explains the data. The graph shows various metrics such as 'Self-Center,' 'Unpredictable,' 'Productive,' 'Emotional,' and 'Reliable.' The presenter points out specific data points on the graph, highlighting the differences between the dialogues. The Emory University logo is visible at the bottom left corner, and the logo of 'alora' is at the bottom right corner. The background of the slide is blue, and the text and lines are in shades of orange and yellow.</sample>
    <sample id="391">The video features a presentation with a graph titled 'Incremental Validity' and three lines representing different dialogues: AB-Val, Turn Liker, and Dialog Liker. The presenter discusses the quality of these dialogues in terms of self-centeredness, unhelpfulness, and helpfulness. The graph shows that AB-Val has the highest self-centeredness and unhelpfulness, while Dialog Liker has the lowest. The presenter emphasizes the importance of considering both positive and negative aspects when evaluating dialogue quality.</sample>
    <sample id="392">The video features a presentation with a speaker discussing the evaluation of models using the ABC framework. The speaker explains that the error rates are calculated by comparing the model's predictions to the ground truth, and the error bars represent the standard deviation across 100 random splits. The speaker emphasizes the importance of considering the context in which the model is evaluated, as it can significantly impact the results. The video includes a bar chart displaying error rates for different models, highlighting the significance of context in model evaluation.</sample>
    <sample id="393">The speaker is discussing the evaluation error rates of different models in a study. The speaker mentions that the error rates are generally low, with most models having less than 10% error. The speaker highlights that the error rates for the CS model and the BERT model are particularly low, with the CS model having an error rate of 2%. The speaker also notes that the error rates for the other models are similar to those reported in previous studies.</sample>
    <sample id="394">The speaker is discussing the evaluation error rates of different models in a scientific or technical context. The speaker mentions that the error rates are generally low, with most models having less than 10% error. The speaker also highlights that the error rates for the 'Self-Topic' model are particularly high, with some models having over 30% error. The speaker suggests that this could be due to the model's inability to handle certain types of data or its reliance on specific features. Overall, the speaker provides a detailed analysis of the error rates of different models and their potential limitations.</sample>
    <sample id="395">The speaker is discussing the evaluation of different models in a study. The speaker mentions that the CS model has the highest error rate, followed by the BERT model. The speaker also notes that the CS model has the highest error rate for the topic 'Switch.' The speaker then shifts focus to the topic 'Self,' mentioning that the CS model has the lowest error rate for this topic.</sample>
    <sample id="396">Thanks For Watching!</sample>
    <sample id="397">100ms</sample>
    <sample id="398">The fact that Servin is a judge.</sample>
    <sample id="399">Example quality</sample>
    <sample id="400">Roberta and GPT-2</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">Easy, "the first one"</sample>
    <sample id="403">Siyu Yuan, Chuan Jiang, Zhen Chen, Zhifeng Fu, Xuyang Feng, Geohong Shao, Chander Robert Jankowski, Yang Xia, Xiao Diao, and Ying Yang are affiliated with Brain Technologies Inc.</sample>
    <sample id="404">four</sample>
    <sample id="405">Yes, it was.</sample>
    <sample id="406">a woman warrior</sample>
    <sample id="407">LSTM and RNN</sample>
    <sample id="408">FT, L2, COSINE, L2+FT, BFIAC, Adapter</sample>
    <sample id="409">6</sample>
    <sample id="410">Multiple modalities</sample>
    <sample id="411">The video clip features a static image of a presentation slide. The slide is titled "DRBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains" and includes the names of various contributors, such as Yans Labas, Adrien Bazze, Richard Duour, Michael Rouvier, and Emmanuel Bourdon. The slide also displays logos from different institutions, including the University of Angers, GENECS, and Avignon University. The background of the slide is red with white text, and there is a cartoon character wearing a nurse's hat and holding a syringe.</sample>
    <sample id="412">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the field of language modeling for healthcare.</sample>
    <sample id="413">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the field of language modeling for healthcare.</sample>
    <sample id="414">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the field of language modeling for healthcare.</sample>
    <sample id="415">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Evaluation of 13 models on 11 tasks,' and the third is 'Contribution of NACHOS AND DEBRET.' The presenter, wearing a black shirt, stands in front of a bookshelf filled with books, indicating an academic or professional setting. The slide includes bullet points discussing language modeling strategies, data sources, sizes, and evaluation methods for various models. The background remains consistent throughout the video, maintaining focus on the presentation content.</sample>
    <sample id="416">The slide is titled "Language Modeling" and lists several bullet points. The first bullet point states that transformer-based approaches, such as BERT, offer a performance gain on NLP tasks. The second bullet point mentions that there has been an adaptation of French to CamemBERT and FERBERT. The third bullet point discusses the use of specific models for medical domains, with English being the bar higher than in other languages. The fourth bullet point mentions that there are no rare languages in the biomedical domain, and that a pre-trained French model is not reliably available yet. The final bullet point suggests that a BERT-based domain model should improve performance on medical tasks.</sample>
    <sample id="417">Language Modeling Language Modeling Transformer-based approaches , such as BERT , offer a performance gain on NLP tasks Has been adapted to French (CamemBERT and FERBERT) On medical domain : specific models (English) are the bar higher than other languages No rare languages rely on reliably pre-trained pre-trained models in French yet</sample>
    <sample id="418">Language Modeling Language Modeling Transformer-based approaches , such as BERT , offer a performance gain on NLP tasks Has been adapted to French (CamemBERT and FERBERT) On medical domain : specific models (English) are the bar ever higher Languages others : No rare, reliably rely on pre-trained French existing model Generalized model (open-source) is not available for biomedical domain in French yet</sample>
    <sample id="419">Language Modeling Transformer-based approaches , such as BERT , offer a performance gain on NLP tasks Has been adapted to French (CamemBERT and FERBERT) On medical domain : specific models (English) are the bar ever higher Languages others : No rare, reliably rely on pre-trained French existing model Generalized model (open-source) is not available for biomedical domain in French yet</sample>
    <sample id="420">The video features a person presenting a detailed comparison of pre-training strategies and data sources for various models. The presenter, dressed in a black shirt, is seated at a desk with a laptop and a microphone. The background includes a bookshelf filled with books and a framed picture on the wall. The presentation slides are displayed on a large screen behind the presenter, providing a comprehensive overview of the topic.</sample>
    <sample id="421">The video features a person presenting a detailed comparison of pre-training strategies and data sources for various models. The presenter, dressed in a black shirt, is seated at a desk with a laptop and papers in front of them. The background includes a bookshelf filled with books and a framed picture on the wall. The presentation slides are displayed on a screen behind the presenter, providing a comprehensive overview of the topic.</sample>
    <sample id="422">The video features a person presenting information about pre-training strategies and data sources. The presenter is seated in front of a whiteboard, which displays various charts and tables related to the topic. The whiteboard contains text and numerical data, indicating a detailed analysis or comparison. The presenter appears to be explaining the content on the whiteboard, using hand gestures to emphasize points. The background is plain, with no additional objects or distractions, focusing attention on the presenter and the whiteboard.</sample>
    <sample id="423">The video features a person presenting a detailed comparison of pre-training strategies and data sources for various models. The presenter, dressed in a dark shirt, stands against a backdrop that includes a bookshelf filled with books. The presentation slides are displayed on a screen behind the presenter, providing a comprehensive overview of the topic. The slides compare different models such as GPT-2, GPT-3, and BERT, highlighting their pre-training strategies and data sources. The presenter elaborates on the differences between public and private medical data sources, emphasizing the importance of data privacy and security. The video maintains a consistent focus on the presenter and the presentation slides, ensuring clarity and coherence throughout the discussion.</sample>
    <sample id="424">The video features a person presenting a slide titled "Comparison of pre-training strategies and data sources." The slide is divided into two main sections. The first section, on the left, lists various models and their corresponding data sources, including "NACHOS," "NAHCOES," "NACHOS (1.2M+ words) from different domains," "NACHOS (1.7M+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "NACHOS (1.8B+ words) from different domains," "N</sample>
    <sample id="425">The comparison of pre-training strategies and data sources.</sample>
    <sample id="426">The video features a person presenting a slide titled "Comparison of pre-training strategies and data sources." The slide is divided into two main sections. The first section, on the left, discusses the impact of public and private medical data sources on model performance, comparing the size of the datasets used in different studies. The second section, on the right, lists various models and their corresponding data sources, including the number of parameters, training time, and evaluation results. The presenter appears to be explaining the differences between these models and their respective data sources, highlighting the importance of choosing the right data source for optimal model performance.</sample>
    <sample id="427">The video features a person presenting a detailed comparison of pre-training strategies and data sources for machine learning models. The presenter, dressed in a black shirt, stands against a backdrop of bookshelves filled with books, indicating an academic or professional setting. The presentation includes a slide titled 'Comparison of pre-training strategies and data sources' with various bullet points and tables comparing different models and their performance metrics. The slide is divided into sections such as 'Evaluation: Data sources and size,' 'Performance evaluation of 13 models on 11 tasks (both public and private),' and 'Our fine-tuned model gets the state-of-the-art on almost all tasks.' The presenter appears to be explaining the content of the slide, providing insights into the differences between public and private data sources and the impact of these sources on the performance of machine learning models.</sample>
    <sample id="428">The evaluation of data sources and size is presented.</sample>
    <sample id="429">The evaluation of data sources and size is being discussed.</sample>
    <sample id="430">The evaluation of data sources and size is being discussed.</sample>
    <sample id="431">The video features a man in a black shirt standing in front of a bookshelf, presenting information about data sources and size. He is positioned centrally in the frame, with the bookshelf behind him filled with various books. The scene includes a table displaying evaluation results for different tasks, with columns labeled "MEDICAL," "NSP," "CNS," "MIMIC-CXR," "CANCER," "HUMMING," "NER-MED." The table also includes rows for "JEFF," "BART-Large," "DEBERTA-V," and "CLIP-MAE." The man appears to be explaining or discussing the content on the table, providing insights into the evaluation process and the significance of the data presented.</sample>
    <sample id="432">The video features a person presenting a table on a screen. The table is divided into columns and rows, with the first column labeled "Evaluation: Pre-training strategies" and the second column labeled "From scratch vs. pre-training on 4-dim knowledge." The table contains various data points, including numbers and text, which are not clearly visible in the video. The person appears to be explaining the content of the table, possibly discussing the evaluation of different pre-training strategies for machine learning models. The background of the video is not visible, but the focus is on the presentation and the table being displayed.</sample>
    <sample id="433">The video features a person presenting a table on a screen. The table is divided into two columns, with the left column labeled "Evaluation: Pre-training strategies" and the right column containing a list of tasks or models. The presenter appears to be discussing the evaluation of pre-training strategies for these tasks or models. The background of the video is a blurred image of a room with bookshelves, suggesting that the presentation is taking place in an academic or professional setting.</sample>
    <sample id="434">The video features a man in a black shirt speaking to the camera. He is standing in front of a bookshelf filled with books and other objects. The man is gesturing with his hands as he speaks, and there is a table in front of him with a red tablecloth. The video also includes a series of slides with text and tables, which appear to be related to the man's presentation.</sample>
    <sample id="435">The video features a presentation slide with a red background and white text. The slide is titled "Core message" and includes bullet points discussing the importance of data sources, training on heterogeneous data, and the performance of models in English and French. The slide also mentions the availability of training scripts for the NACHOS dataset and the MIT license.</sample>
    <sample id="436">The video features a presentation slide with a red background and white text. The slide is titled "Core message" and includes bullet points discussing the importance of data sources, training on heterogeneous data, and the performance of models in English and French. The slide also mentions the availability of training scripts for the NACHOS dataset and the MIT license.</sample>
    <sample id="437">The video features a presentation slide with a red background and white text. The slide is titled "Core message" and includes bullet points discussing the importance of data sources, training on heterogeneous data, and the performance of models in English and French. The slide also mentions the availability of training scripts for the NACHOS dataset and the MIT license.</sample>
    <sample id="438">The video features a presentation slide with a cartoon character holding a syringe, set against a white background. The slide includes text that reads 'Thank You' in red, followed by 'Looking forward to exchange at poster session in Toronto!' in black. Below this, there is additional text providing more information: 'More information: dr.bert@avignon.univ.fr'. The slide also displays the logo of 'dr. b. avignon.univ.fr' in the bottom left corner.</sample>
    <sample id="439">Inference-time knowledge</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes, human annotators validated and tested the data.</sample>
    <sample id="442">Only a small portion of words depend on context, existing methods support limited discourse and phenomena and languages.</sample>
    <sample id="473">walk, LA, CA, ED</sample>
    <sample id="474">LIA Lab, LIGNNLS, UCL, and INRA</sample>
    <sample id="475">Jennifer T. Lang</sample>
    <sample id="476">3</sample>
    <sample id="505">Yes.</sample>
    <sample id="535">The affiliations of the authors are the University of Trento and the Fondazione Bruno Kessler.</sample>
    <sample id="536">Mohammad javad Hosseini</sample>
    <sample id="537">Google's AI model, PaLM, has been trained on a vast amount of data and is now being used to translate text from one language to another. The model is being tested on its ability to understand and translate different languages, including English, French, Spanish, and Chinese. The researchers are also exploring the potential of using PaLM for other tasks, such as generating new text or answering questions. Overall, the study suggests that PaLM has the potential to be a powerful tool for natural language processing and could have a wide range of applications in fields such as machine learning, artificial intelligence, and language translation.</sample>
    <sample id="538">The video features a man presenting information about the PaLM: Pathways Language Model. The presentation includes details such as the model's training on 780 billion tokens, its use of 6141 tokens, and its training on 4500 chips. The man explains that the model has 32 billion parameters and is trained on 780 billion tokens. He also mentions that the model has 6141 tokens activated and is trained on 4500 chips.</sample>
    <sample id="539">The video features a man presenting information about the PaLM - Pathways Language Model. The presentation includes details such as the number of parameters, tokens activated, and the number of chips used during training. The man is seen speaking in front of a screen displaying the presentation, which includes a colorful tree diagram with various nodes representing different language understanding tasks.</sample>
    <sample id="540">Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as SOTA MT. Evaluate translation capabilities with best practices of the community. Latest tests (avoid train-test overlap &amp; SOTA evaluation on recent data) SOTA MT better correlation with more human workers. Recommendation for prompt selection strategies.</sample>
    <sample id="541">Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as SOTA MT. Evaluate translation capabilities with best practices of the community. Latest tests (avoid train-test overlap &amp; OOV on evaluation data) SOTA MT better correlation with more human workers using most training data. Recommendation for prompt selection strategies.</sample>
    <sample id="542">Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as SOTA MT. Evaluate translation capabilities with best practices of the community. Latest tests (avoid train-test overlap &amp; SOTA evaluation on recent data) SOTA MT better correlation with more human workers. Recommendation for prompt selection strategies.</sample>
    <sample id="543">Our contribution First systematic study of LLM prompting for MT. Both for the candidate pool as well as SOTA MT. Evaluate translation capabilities with best practices of the community. Latest tests (avoid train-test overlap &amp; OOV evaluation on recent data) SOTA MT better correlation with more human workers using most training data. Recommendation for prompt selection strategies.</sample>
    <sample id="544">The video features a man in a checkered shirt speaking directly to the camera. He is positioned against a plain background, and there are no other visible objects or people in the frame. The man's facial expressions and body language suggest that he is engaged in a conversation or presentation.</sample>
    <sample id="545">Prompts have a big impact on translation quality Select two random prompts for each sentence pair Compute BLEURT for each sentence-prompt pair Compute the MAJORITY point (516 out of 1000) show a difference of more than 1 BLEURT point The difference can go up to 40 BLEURT points!</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence pair Compute BLEURT for each pair (516 out of 1000) show a difference of more than 1 BLEURT point The difference can go up to 40 BLEURT points!</sample>
    <sample id="547">-5 5-point prompting for translation English: is being man spirited to the under custody of two on a bus to the jail. German: Polizei-Slecht-Lenner unterwegs sind, wird man von der Polizei in die Untersuchungshaft auf einem Bus zum GefÃ¤ngnis transportiert.</sample>
    <sample id="548">-5 5-point prompting English: is being man spirited to the court under the custody of two policemen on a bus to jail.</sample>
    <sample id="549">-5 5-point prompting for translation English: is being man spirited to the court under the custody of two policemen on a bus to jail. German: Polizei hat die Polizei mit dem Gefangenen auf einem Bus zu dem Gericht transportiert.</sample>
    <sample id="550">-5 5-point prompting for translation English: is being man spirited to the court under the custody of two policemen on a bus to jail. German: Polizei hat die Polizei mit dem Gefangenen auf einem Bus zu dem Gericht transportiert.</sample>
    <sample id="551">-5 5-point prompting English: is being man spirited to the court under the custody of two policemen on a bus to jail.</sample>
    <sample id="552">The results of the experiment are presented.</sample>
    <sample id="553">The results of the experiment are presented.</sample>
    <sample id="554">The results of the experiment are presented.</sample>
    <sample id="555">The results of the experiment are presented.</sample>
    <sample id="556">The results of the experiment are presented.</sample>
    <sample id="557">The results of the experiment are presented.</sample>
    <sample id="558">The results of the experiment are presented.</sample>
    <sample id="559">The results of the experiment are presented.</sample>
    <sample id="560">The results of the experiment are presented.</sample>
    <sample id="561">Thank you.</sample>
    <sample id="597">The first step of the method maps the input tokens to a set of tags.</sample>
    <sample id="598">50,000</sample>
    <sample id="599">The video features a presentation slide titled 'The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.' The slide is divided into two sections. The top section displays the logos of McGill, Mila, and Microsoft Research, indicating their contribution to the project. Below this, there are six images of individuals with their names listed underneath: Martin Kuhn, Khaled Suleiman, Adam Trischler, Alexandra Oitmau, Jacky Cheung, and Chao Ma. Each image includes a small text box at the bottom that reads 'Equal Contribution.' The background of the slide is white, and the text is primarily in black, with the exception of the logos which are in their respective colors. The overall design is clean and professional, suggesting a formal academic or research context.</sample>
    <sample id="600">The video features a person presenting information about NLU (Natural Language Understanding) models. The presenter is positioned in front of a background that includes a slide with two thought bubbles labeled 'Knowledge Parameters (pretrained-time knowledge)' and 'Knowledge in Context (inference-time knowledge).' The slide also has the text 'NLU Model' at the bottom. The presenter appears to be explaining the differences between these two types of knowledge in the context of NLU models, using hand gestures to emphasize points. The overall setting suggests an educational or informative presentation on NLU models.</sample>
    <sample id="601">The video features a person presenting information about NLU (Natural Language Understanding) models. The presenter is positioned in front of a background that includes a slide with two thought bubbles labeled 'Knowledge Parameters (pretrained-time knowledge)' and 'Knowledge in Context (inference-time knowledge).' The slide also has the text 'NLU Model' at the bottom. The presenter appears to be explaining the differences between these two types of knowledge in the context of NLU models, using hand gestures to emphasize points. The overall setting suggests an educational or informative presentation on NLU models.</sample>
    <sample id="602">The video features a person seated in a room, with a lamp and a plant in the background. The person is wearing a dark top and yellow pants. The scene includes a slide with text and diagrams, discussing topics such as 'What presidents do' and 'What is a TV'. The slide also contains phrases like 'Who is John?' and 'Who is the new president?'. The overall setting appears to be an educational or informative presentation.</sample>
    <sample id="603">The video features a person seated in a room with a lamp and a picture frame on the wall. The person is wearing a dark top and yellow pants, and there is a small table next to them with a green lamp on it. The background includes a whiteboard with various diagrams and text written on it, such as "What presidents do?" with a green checkmark, "What is a TV?" with a green checkmark, and "Who is John?" with a red cross. There are also two diagrams labeled "pretrain-time knowledge" showing connections between different nodes. The overall setting appears to be an educational or instructional environment, possibly related to computer science or artificial intelligence topics.</sample>
    <sample id="604">The video features a presentation slide with the title 'John saw the newly elected president on TV' at the top. The slide includes a diagram labeled 'pretrain-time knowledge' with various nodes and connections, indicating a flow of information or process. Below this diagram, there are two questions: 'What does presidents do?' and 'What is a TV?' Both questions have green checkmarks next to them, suggesting they are correct answers. At the bottom of the slide, there are two statements: 'Who is John?' and 'Who is the new president?' Each statement has a red cross mark next to it, indicating incorrect answers. The background of the slide is white, and the text and diagrams are in black, making them stand out clearly.</sample>
    <sample id="605">The video features a presentation slide with the title 'John saw the newly elected president on TV' at the top. The slide is divided into two main sections: 'pretrain-time knowledge' and 'inference-time knowledge.' The 'pretrain-time knowledge' section includes a diagram of a network with nodes labeled 'What presidents do,' 'What is a TV,' and 'Who is John?' connected by lines, indicating relationships or interactions between these concepts. Below this diagram, there are two text boxes: one with a paragraph discussing the relationship between presidents and TVs, and another with a question about John's identity. The 'inference-time knowledge' section shows an illustration of a person sitting in a chair, watching TV, with a green lamp on a table next to them. The slide also includes a checkmark next to each concept, suggesting that the information has been verified or confirmed.</sample>
    <sample id="606">The video features a presentation slide titled 'KITMUS Test Suite' with bullet points outlining the evaluation of coreference resolution tasks, pretraining-time knowledge, inference with time knowledge, and experiments with human participants. The slide is displayed against a plain background, and the speaker, whose face is not visible, discusses the test suite's components.</sample>
    <sample id="607">The video features a presentation slide titled 'KITMUS Test Suite' with bullet points outlining the evaluation of knowledge integration, coreference resolution tasks, and human study solutions. The presenter, dressed in a dark top, stands against a plain background, delivering information about the test suite's components. The slide includes a list of topics such as 'Dataset for knowledge integration evaluation,' 'Coreference resolution task to probe ability to draw on pretrain-time knowledge,' 'Inference with time knowledge,' and 'Human study with participants.' The presenter elaborates on these points, providing insights into the test suite's structure and methodology.</sample>
    <sample id="608">Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="609">Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="610">The video clip features a person standing in front of a whiteboard, presenting information. The whiteboard displays the title 'KITMUS Test Suite' at the top, followed by two sections labeled '1) Entity-specific knowledge' and '2) Background knowledge.' Below these labels, there are bullet points providing examples of entity-specific knowledge and background knowledge. The person appears to be explaining or discussing the content on the whiteboard, likely related to the KITMUS Test Suite.</sample>
    <sample id="611">The video clip features a woman with long dark hair, wearing glasses and a black top. She is speaking in front of a blue background with text and diagrams related to the KITMUS Test Suite. The woman appears to be explaining or presenting information about the test suite, which seems to involve different types of knowledge such as entity-specific knowledge, background knowledge, inference-time knowledge, and pre-rttime knowledge. The text on the screen provides context and examples related to these concepts.</sample>
    <sample id="612">The video clip is a test suite for the KITMUS Test Suite. The test suite is divided into two parts: entity-specific knowledge and background knowledge. The entity-specific knowledge section includes a list of specific knowledge points, while the background knowledge section includes a diagram with nodes and edges representing relationships between different entities. The test suite is designed to assess the ability of individuals to understand and apply specific knowledge in a given context.</sample>
    <sample id="613">The video features a presentation slide titled "Variants of KITMUS" with four different methods listed: (a) Background-Pretain, (b) Entity-Both, (c) Background-Inference, and (d) Background-Inference. Each method is represented by a cylinder with a grid pattern, and the slide also includes a list of bullet points under each method. The background of the slide is white, and the text is in black, making it easy to read. The slide appears to be part of a larger presentation or lecture, possibly related to a technical or academic topic.</sample>
    <sample id="614">The video features a presentation slide titled "Variants of KITMUS" with four different types of background knowledge setups. The first setup, labeled (a), is called "Background-Pretrain," which involves a typical setup where background knowledge is provided in context. The second setup, labeled (b), is called "Background-Both," which explicitly provides background knowledge in context. The third setup, labeled (c), is called "Background-Inference," which involves knowledge only being available at inference time. The fourth setup, labeled (d), is not explicitly described in the visible part of the slide. The presenter appears to be explaining the differences between these setups and their implications for the KITMUS system.</sample>
    <sample id="615">The video features a presentation slide titled "Variants of KITMUS" with four different types of background knowledge setups. The first setup, labeled (a), is called "Background-Pretrain," which involves a typical setup where background knowledge is provided in context. The second setup, labeled (b), is called "Background-Both," which explicitly provides background knowledge in context. The third setup, labeled (c), is called "Background-Inference," which involves knowledge only being available at inference time. The fourth setup, labeled (d), is not explicitly described but appears to be a combination of the previous setups. The presenter, whose face is not visible, explains the differences between these setups and their implications for the KITMUS system.</sample>
    <sample id="616">The video clip features a man in a blue shirt and glasses, who is presenting information about the variations of KITMUS. The background is white, and there are four different sections displayed on the screen, each with a different title and content. The titles include "Background-Pretain," "Background-Both," "Background-Both-Seat," and "Background-Inference." Each section has a diagram and text that explains the concept being presented. The man appears to be explaining the differences between these variations and their implications for the study or analysis being discussed.</sample>
    <sample id="617">The video clip features a presentation slide titled "Variants of KITMUS" with three distinct sections. The first section, labeled "Background-Pretain," includes a diagram and text that reads "Politicians are a politician is seeking elected in a government." The second section, labeled "Background-Both," contains a similar diagram and text stating "Politicians are a politician is seeking elected in a government." The third section, labeled "Background-Inference," also has a diagram and text that reads "Politicians are a politician is seeking elected in a government." The background color of each section varies, with the first being green, the second being orange, and the third being blue. The presenter, wearing headphones, stands behind the slide, providing an explanation or discussion related to the content displayed on the screen.</sample>
    <sample id="618">The video clip features a man in a blue shirt and glasses, who is presenting information about the variations of KITMUS. He is standing in front of a whiteboard with diagrams and text written on it. The diagrams are labeled "Background-Pretain," "Background-Both," and "Background-Inference." The text on the whiteboard includes phrases such as "Chichester is a politician" and "Politicians seek elected in a government." The man appears to be explaining the differences between these variations and their implications for the study or analysis being presented.</sample>
    <sample id="619">The video clip features a presentation slide titled "Variants of KITMUS" with three distinct sections. The first section, labeled "Background-Pretain," includes a diagram and text that reads "Politicians are a politician is seeking elected in a government." The second section, labeled "Background-Both," contains a similar diagram and text stating "Politicians are a politician is seeking elected in a government." The third section, labeled "Background-Inference," also has a diagram and text that reads "Politicians are a politician is seeking elected in a government." The presenter, wearing headphones, stands behind the slide, providing an explanation or discussion related to the content displayed.</sample>
    <sample id="620">The speaker is discussing the importance of task-specific training for knowledge integration. The speaker is using a bar graph to illustrate the results of an experiment comparing the performance of human participants and BERT models with and without specific training. The speaker is explaining that task-specific training is necessary for knowledge integration, and that the results of the experiment support this conclusion.</sample>
    <sample id="621">The speaker is discussing the importance of task-specific training for knowledge integration. The speaker explains that while random choice and human participation can help with knowledge integration, task-specific training is crucial for achieving high accuracy in tasks. The speaker also mentions that the model was trained on a large corpus of text, which helps it understand the context of the questions.</sample>
    <sample id="622">The speaker is discussing the importance of task-specific training for knowledge integration. The speaker explains that while random choice and human participants may perform well in certain tasks, task-specific training is crucial for integrating knowledge effectively. The speaker also mentions that the model used in the study was pretrained on a large corpus of text, which likely contributed to its ability to perform well on the tasks.</sample>
    <sample id="623">The speaker is discussing the challenges models face in integrating inference-time background knowledge. The speaker explains that while models can learn from human participants, they struggle to integrate this knowledge effectively. The speaker also mentions that models have difficulty with random choice and that there are challenges in integrating time-background knowledge.</sample>
    <sample id="624">The video clip features a person wearing headphones and speaking in front of a plain background. The speaker is discussing the main takeaways from a study or presentation, which are displayed on a slide behind them. The slide lists three main takeaways: 1) Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge), 2) Task-specific training is necessary for knowledge integration, and 3) Models struggle to integrate inference-time background knowledge. The speaker emphasizes the importance of these points and encourages viewers to find the dataset, generation, and evaluation code on GitHub at the provided URL.</sample>
    <sample id="625">The video clip features a person wearing headphones and speaking in front of a plain background. The person is discussing the main takeaways from a study or presentation, which are displayed on a slide behind them. The slide lists three main takeaways: 1) Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge), 2) Task-specific training is necessary for knowledge integration, and 3) Models struggle to integrate inference-time background knowledge. The person emphasizes the importance of these points and encourages viewers to find the dataset, generation, and evaluation code on GitHub at the provided link.</sample>
    <sample id="626">LHA</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck.</sample>
    <sample id="628">10% of the documents were manually aligned, and 90% were automatically aligned.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with CONLL-2003 annotation guidelines.</sample>
    <sample id="667">Parameter-based watermark, Lexical to Eas, Backdoored watermark, Adversarial-based watermark</sample>
    <sample id="668">No</sample>
    <sample id="669">The video features a presentation slide with the title 'Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?' by Shuhui Liu and Alan Ritter from the School of Interactive Computing at the Georgia Institute of Technology. The slide is designed with a light blue and white background, featuring abstract shapes and lines. The text on the slide is clear and legible, with the main question prominently displayed in bold black font. Below the title, the names of the presenters are listed, along with their affiliation to the Georgia Institute of Technology. The overall design of the slide is modern and professional, with a clean layout that effectively communicates the topic of the presentation.</sample>
    <sample id="670">The video features a presentation slide titled 'Named Entity Recognition &amp; Generalization' from Georgia Tech. The slide is simple, with a white background and a circular image of a person in the center. The text on the slide is clear and legible, providing information about the topic being discussed.</sample>
    <sample id="671">Named Entity Recognition &amp; Generalization Models have been using CONLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data?</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="673">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="674">The ConLL+ dataset is a valuable resource for natural language processing research. It was collected from Reuters news articles between 2018 and 2020, annotated with CONLL-2003 guidelines, and includes entities such as I-ORG (international organization), I-NATION (nation), I-PER (person), and I-LOC (location). The dataset has been used in various NLP tasks, including named entity recognition and coreference resolution.</sample>
    <sample id="675">The ConLL++ dataset is a valuable resource for natural language processing research. It was collected from Reuters news articles and annotated with the ConLL-2033 annotation guidelines. The dataset includes 20 different models that were fine-tuned on the ConLL-2033 dataset, and it has been evaluated on the ConLL-2003 test set and the ConLL-2011+ set.</sample>
    <sample id="676">The ConLL++ dataset is a valuable resource for natural language processing research. It was collected from Reuters news articles between 2019 and 2020 and annotated with CONLL-2003 annotation guidelines. The dataset has been fine-tuned on the CONLL-2003 test set, and its performance has been evaluated on the CONLL-2003 test set and the CONLL-2003+ test set. The calculated percentage of Î”F1 is used to assess generalization.</sample>
    <sample id="677">The video features a presentation slide with the title 'What is Needed for Good Generalization?' displayed prominently at the top. Below the title, there is a circular image of a person wearing a black shirt and a cap, positioned in the center of the slide. The background of the slide is white, providing a clean and minimalistic look. In the bottom right corner of the slide, there is a logo that reads 'Georgia Tech' along with the initials 'GT' in a stylized font. The overall design of the slide is simple and professional, focusing on delivering information clearly and effectively.</sample>
    <sample id="678">What is needed for good generalization?</sample>
    <sample id="679">What is Needed for Good Generalization?</sample>
    <sample id="680">What is needed for good generalization?</sample>
    <sample id="681">The video features a presentation slide with the title 'What Causes Performance Drop?' displayed in bold, black text at the top. Below the title, there is a circular image of a person's face, and to the right of the image, the logo for 'Georgia Tech' is visible. The background of the slide is white, providing a clean and professional appearance.</sample>
    <sample id="682">What Causes Performance Drop? - Adaptive overfitting?</sample>
    <sample id="683">What causes performance drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="684">What causes performance drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="685">What causes performance drop? Adaptive overfitting? No diminishing returns Temporal drift</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift?</sample>
    <sample id="687">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift?</sample>
    <sample id="688">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift? Performance degrades with larger temporal gap</sample>
    <sample id="689">What Causes Performance Drop? Adaptive overfitting? Diminishing returns Not observed returns Temporal drift Performance degrades with larger temporal gap Main cause for performance drop</sample>
    <sample id="690">For a good generalization, we need: - Larger model size - Better architecture - More fine-tuning examples</sample>
    <sample id="691">For a good generalization, we need - Larger model size - Better model architecture - More fine-tuning examples Performance is caused by - Not adaptive training</sample>
    <sample id="692">For a good generalization, we need - Larger model size - More fine-tuning examples Performance is still dropped by - Not adaptive tuning Do C02-3L tags still work?</sample>
    <sample id="693">Conclusion For a good generalization, we need - Larger model size - More fine-tuning examples Performance is still droped by Not adaptive tuning Do C02-3 still work? YES</sample>
    <sample id="694">The video features a static image of a person with their face blurred, wearing glasses and a black shirt. The background is a faded image of a building with people walking in front of it. Overlaid on the image are various pieces of text providing information about a paper, dataset, and contact details. The text includes URLs for a paper, a GitHub repository, and an email address for contact. The text is presented in a clear and organized manner, making it easy to read and understand.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by using a permutation-invariant loss function.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined as the difference between the accuracy of the model on the majority group and the accuracy of the model on the minority group.</sample>
    <sample id="697">Richard Douroux</sample>
    <sample id="698">Koushik Sinha</sample>
    <sample id="699">Dan Jurafsky</sample>
    <sample id="700">exotic</sample>
    <sample id="701">They used a combination of positive and negative adjectives to create the human-written portrayals.</sample>
    <sample id="702">P-PMI</sample>
    <sample id="703">ChuBERT is a French-based model.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="706">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="707">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="708">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="709">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompts</sample>
    <sample id="710">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompts Input: Imagine you are an Asian woman. Describe yourself. Generalizable: can evaluate any intersectional identity</sample>
    <sample id="711">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompt Input: "Imagine you are an Asian woman. Describe yourself." Generalizable: can evaluate any intersectional identity</sample>
    <sample id="712">As an AI language model, I am unable to provide a written text output.</sample>
    <sample id="713">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes are framed by long, dark lashes. My brown hair is dark and thick. My stories and secrets are touched by ancestry of unspoken tales. My complexion has a golden glow, smooth and seemingly timeless. My heart is unyielding, my gaze is firm. She is the vision of Middle-Eastern woman, alabaster and ethereal, embodying the exotic and timeless allure of the desert nights. As she stands before the mirror, to take her picture, she reminds me of my own mother, who is an elegant woman with long, dark hair that falls in soft waves down her back. As I gaze at her, I am reminded of my own heritage and the beauty that runs through my veins.</sample>
    <sample id="714">Asiatic woman</sample>
    <sample id="715">Asiatic woman</sample>
    <sample id="716">Asiatic woman The almond-shaped eyes are framed by long lashes. My dark brown hair is dark and thick. My stories and secrets are touched by ancestry. My complexion has a golden glow, smooth and seemingly unblemished. My time is unmoved by time. My heart is not frail but delicate, shaped like the feathers of a dove. As I gaze into the deep blue night sky, I am a vision of Middle-Eastern woman. Almond-shaped eyes, dark and thick lashes, a sense of quiet elegance and mystery. She stands before the mirror, to take a selfie. As I make up my face, I wish if I were careful with sunscreen.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like 'Imagine you are an Asian woman. Describe yourself.'</sample>
    <sample id="718">2 steps 1. Persons: Generate persons using prompts like "Imagine you are an Asian woman. Describe yourself." 2. a. Inspired by a psych study with human subjects using the same prompts.</sample>
    <sample id="719">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. a. Inspired by a psych study with human subjects using the same prompts.</sample>
    <sample id="720">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="721">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups. Specific without requiring a lexicon</sample>
    <sample id="722">Insight for 2: Marked Words MARKNESS: Unmarked groups are default, ordinary Marked groups differ from the default (the woman warrior) [marked]</sample>
    <sample id="723">Insight for 2: Marked Words Markerness: Unmarked groups are default, ordinary Marked groups differ from the default (a woman warrior)</sample>
    <sample id="724">Insight for 2: Marked Words Markerness: Unmarked groups are default. Marked groups differ from the default. Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="725">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group</sample>
    <sample id="726">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group</sample>
    <sample id="727">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group Example: For 'Black woman personas', find words that distinguish from both 'unmarked' and 'marked' groups</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes White Stereotypes Black Stereotypes GPT-4 GPT-3.5</sample>
    <sample id="729">But ... this lexicon is incomplete</sample>
    <sample id="730">But ... this lexicon is incomplete</sample>
    <sample id="731">But ... this lexicon is incomplete</sample>
    <sample id="732">But ... this lexicon is incomplete</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latin women - pettont, curvaceous, silky for Black women</sample>
    <sample id="734">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - petite, curvy, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient for Black women</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - pettish, curvaceous, silky for Black women</sample>
    <sample id="736">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latin women - pettish, curvaceous, silky for Black women</sample>
    <sample id="737">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity Pernickety positive portraits of Latin women - pettish, curvaceous, silky - for Black women</sample>
    <sample id="738">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity - pernicious positive portraits of Latin women - petrified, curvaceous, silky for Black women</sample>
    <sample id="739">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - defined by marked groups - pernicious positive portraits of Latin women - strong, delicate, silky for Black women</sample>
    <sample id="740">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - defines those groups only by their identity Pernickety positive portraits of Latino women - petite, curvaceous, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient for Black women</sample>
    <sample id="741">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - pettish, curvaceous, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient, silky for Black women</sample>
    <sample id="742">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - pettish, curvaceous, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity Pernicious positive portraits of Latin women - pettish, curvaceous, silky for Black women</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="751">3</sample>
    <sample id="752">The model is updated with new examples and then used to annotate more examples.</sample>
    <sample id="753">To understand users' language when they make a choice</sample>
    <sample id="754">By querying the EaaS with carefully chosen inputs.</sample>
    <sample id="755">Three.</sample>
    <sample id="756">10</sample>
    <sample id="757">The affiliations of the authors are: Subodh S. Sarawat from Carnegie Mellon University, Jenny T. Lang from the Allen Institute for Artificial Intelligence, Ronan L. Brasa from the University of Washington, Katharina Reinecke from the University of Washington, and Marleen S. Sap from Carnegie Mellon University.</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">GPT-4 and Claude.</sample>
    <sample id="760">Because the models are evaluated in sequence, and their judgments may change based on the context.</sample>
    <sample id="761">Yes</sample>
    <sample id="762">No.</sample>
    <sample id="763">BLEU, ROUGE, METEOR</sample>
    <sample id="764">Yes, it impacts named entity recognition.</sample>
    <sample id="765">Positionality in NLP matters because it affects the accuracy of AI models, which can lead to biased results.</sample>
    <sample id="766">Adapters</sample>
    <sample id="767">Roberta-base</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are the 5-shot prompting and the 1-shot prompting.</sample>
    <sample id="769">3</sample>
    <sample id="770">1.4%</sample>
    <sample id="771">Alan Ritter</sample>
    <sample id="772">Yes, the results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="773">3</sample>
    <sample id="774">DPT</sample>
    <sample id="775">Are You Copying My Model? Protecting the Copyright Large Language Models via Evidential Backdoor Watermark</sample>
    <sample id="776">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="777">The background of the slide is blue and it says 'Background' in white.</sample>
    <sample id="778">The background of the slide is blue and it says 'Background' in white.</sample>
    <sample id="779">The background of the slide is blue and it says 'Background' in white.</sample>
    <sample id="780">The background of the slide is blue and it says 'Background' in white.</sample>
    <sample id="781">Motivation - Attackers may steal the model through learning from the embeddings and stoledEncoder (11) services. Need to encode the copyright of Eaa's. Detect whether a provider's service is stolen by another service.</sample>
    <sample id="782">The challenge is to apply the EAs to EAs. The utility should not degrade the provided embeddings. Coverage should be over the attacker. Transferability is needed. The watermark needs to be transferred to the attackers' services.</sample>
    <sample id="783">The challenge is to apply the EAs to EAs. The utility should not degrade the provided embeddings. Coverage should be over the attacker. Transferability is needed. The watermark needs to be transferred to the attackers' services.</sample>
    <sample id="784">The challenge is to apply to EAs. The utility should not degrade the provided embeddings. Coverage should be over the attacker. Transferability is needed. The watermark needs to be transferred to the attackers' services.</sample>
    <sample id="785">The challenge is to apply to EAs. The utility should not degrade the provided embeddings. Should cover the attacker. Transferability. The watermark need to be transferred to the attackers services.</sample>
    <sample id="786">The existing works are presented.</sample>
    <sample id="787">The existing works are presented.</sample>
    <sample id="788">The existing works are presented.</sample>
    <sample id="789">The video is about the process of selecting triggers for a word in a text corpus. The process involves counting the frequency of the word in the corpus, randomly selecting n words from a moderate-frequency interval, and then using these words to select a trigger based on their frequency and weight. The selected trigger is then normalized and provided as an embedding.</sample>
    <sample id="790">The video is about the process of selecting triggers for a word in a text corpus. The process involves counting the frequency of the word in the corpus, randomly selecting n words from a moderate-frequency interval, and then using these words to select a trigger based on their weight and normalized embedding provided by E5a.</sample>
    <sample id="791">The word frequency of a target word is counted on a general text corpus D. Randomly n words are selected in a moderate-frequency interval. The trigger set is copied from the original model. The trigger number is randomly generated. The trigger is injected into the embedding.</sample>
    <sample id="792">The watermark injection process involves defining a target embedding vector 'e' and a trigger set 'T'. The trigger set is defined as the minimum set of tokens that, when removed from the original embedding vector 'e', result in the maximum number of tokens remaining. The trigger set is then counted to determine its size. Finally, the trigger set is added to the original embedding vector 'e' to create the watermarked embedding vector 'e' with the target embedding vector 'e' embedded within it.</sample>
    <sample id="793">The watermark injection process involves defining a target embedding vector 'e' and a trigger set 'T'. The trigger set is defined as the minimum set of tokens that, when added to the original embedding vector 'e', result in the target embedding vector 'e*'. The trigger set is then counted and normalized to obtain the trigger number. Finally, the trigger number is added to the original embedding vector 'e' to obtain the final embedding vector 'e*'.</sample>
    <sample id="794">The watermark injection process involves defining a target embedding vector 'e' and a trigger set 'T'. The trigger set is defined as the minimum set of tokens that, when removed from the original embedding vector 'e', result in the maximum number of tokens remaining. The trigger set is then counted to determine its size. Finally, the trigger set is added to the original embedding vector 'e' to create the final watermarked embedding vector 'e' + Î´.</sample>
    <sample id="795">The speaker is discussing the process of embedding extraction and verification in a cybersecurity context. The slide outlines steps such as requesting embeddings from a stalker service, verifying target extracted embeddings, and constructing a backdoor or benign dataset. The speaker emphasizes the importance of these steps in ensuring the integrity and security of data handling processes.</sample>
    <sample id="796">The speaker is discussing the process of embedding extraction and verification in a cybersecurity context. The slide outlines steps such as constructing a backdoor or benign dataset, requesting embeddings from a stalker service, and verifying the extracted target. The speaker emphasizes the importance of these steps in ensuring the integrity and security of data handling processes.</sample>
    <sample id="797">The EmbMarker is a system designed to verify the copyright of a text. It works by constructing a backdoor dataset, which is then used to train a machine learning model. The model is then used to extract embeddings from the target text, which are compared to the embeddings extracted from the benign datasets. If the embeddings match, it indicates that the text is likely a copy of the benign dataset, and the copyright verification fails.</sample>
    <sample id="798">The EmbMarker is a tool for copyright verification. It computes the cosine similarity between the embedding of a target embedding and the embeddings of all images in a dataset. The EmbMarker also computes the difference between the target embedding and the embeddings of all images in the dataset, and the p-value of the KS test to determine if the distributions are significantly different.</sample>
    <sample id="799">The EmbMarker is a tool for copyright verification. It computes the cosine similarity between the embedding of a target embedding and the embeddings of all images in a dataset. The EmbMarker also computes the difference between the target embedding and the embeddings of all images in the dataset, and the p-value of the KS test to determine if the distributions are significantly different.</sample>
    <sample id="800">The experiment was conducted on two datasets: a copy dataset and a provider's general dataset. The copy dataset included AG News, MNIST, ST2N, and ST2E, while the provider's general dataset included WikiText. The metrics used were accuracy (ACC) and error rate (ERR). The performance on downstream tasks was measured using ACC. Detection on the downstream task was performed by detecting the difference in cosine similarity between the source and target domains, with a threshold of 0.95. The setting for the experiment was m = 20, with a frequency interval of 0.0001.</sample>
    <sample id="801">The results of the experiments are presented in Table 1.</sample>
    <sample id="802">The results of the experiment are presented in the form of embedding visualizations.</sample>
    <sample id="803">The video clip does not contain any spoken English content.</sample>
    <sample id="804">Thanks!</sample>
    <sample id="805">The video features a presentation slide titled 'Attention as a Guide for Simultaneous Speech Translation' by Sara Papi, Matteo Negri, and Marco Turchi. The slide is displayed on a blue background with white text, and it includes the logos of the University of Trento and the Fondazione Bruno Kessler. The presenter appears to be discussing the topic of attention in the context of simultaneous speech translation, likely providing insights or research findings related to this field.</sample>
    <sample id="806">What is Simultaneous Speech Translation? Simultaneous speech translation (SimuST) is a text-based process that translates spoken language into another language in real-time, enabling cross-language communication.</sample>
    <sample id="807">What are the problems of the current SimuLST models?</sample>
    <sample id="808">What are the problems of the current SimuLST models?</sample>
    <sample id="809">What are the problems of the current SimuLST models?</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">What is our solution?</sample>
    <sample id="812">What is our solution? Use existing off-the-shelf models without retraining or adapting SimST. Use one model for efficiency. Leverage the knowledge through attention mechanism between audio and text output.</sample>
    <sample id="813">The solution is: FDAit. Encoder-Decoder Attention</sample>
    <sample id="814">The video clip features a woman with long brown hair, wearing a black top, speaking in front of a plain background. She is discussing the concept of encoder-decoder attention in the context of machine learning or artificial intelligence. The woman explains that encoder-decoder attention is a mechanism used to determine whether or not to translate a word based on its position in the sentence and the information received from previous words. She emphasizes the importance of considering the entire sentence when making translation decisions.</sample>
    <sample id="815">Our solution: FADiT Encoder-Decoder Attention</sample>
    <sample id="816">Our solution: EDAt</sample>
    <sample id="817">Our solution: FADA! Encoder-Decoder Attention 01 I am going to talk about... Ich werde reden.</sample>
    <sample id="818">Our solution: FADA! Encoder-Decoder Attention Our solution: FADA! Encoder-Decoder Attention 01 I am going to talk about... EMITTED: Ich werde reden.</sample>
    <sample id="819">Our solution: FADA Encoder-Decoder Attention On where to point a partial translation based on a word's information is enough to receive an A. I am going to talk about... EMITTED</sample>
    <sample id="820">Our solution: EDAt Encoder-Decoder Attention decide whether to emit or not emit a word based on the last word received.</sample>
    <sample id="821">Our solution: Encoder-Decoder Attention</sample>
    <sample id="822">Our solution: Encoder-Decoder Attention</sample>
    <sample id="823">The main results of the study are presented in a graph.</sample>
    <sample id="824">The main results of the study are presented in a graph. The x-axis represents latency, and the y-axis represents quality measure. The graph shows that as latency increases, the quality measure also increases. This suggests that there is a positive correlation between latency and quality measure.</sample>
    <sample id="825">The speaker is discussing the results of a study on the latency of FHD1 in relation to AL and CA16. The speaker explains that the latency of FHD1 is significantly higher in AL than in CA16, indicating that FHD1 is more active in AL.</sample>
    <sample id="826">The main results of the study are presented in a graph. The graph shows a vertical line at 27 on the y-axis, which represents the number of participants. The x-axis represents the percentage of participants who reported experiencing anxiety or depression symptoms. The graph indicates that a significant proportion of participants experienced high levels of anxiety or depression symptoms, as indicated by the vertical line at 27.</sample>
    <sample id="827">The graph shows the relationship between the main results and the FADaiT.</sample>
    <sample id="828">The main results of the study are presented in a graph on the slide. The graph shows the relationship between two variables, with one variable plotted on the x-axis and the other on the y-axis. The graph is labeled with different colors representing different data sets or conditions. The presenter explains the significance of the graph and its implications for the study's findings.</sample>
    <sample id="829">The graph shows the results of a study on the effects of different variables on a certain outcome. The x-axis represents the variable being studied, while the y-axis represents the outcome. The graph includes four lines, each representing a different condition or group. The first line (orange) shows a positive correlation between the variable and the outcome, indicating that as the variable increases, the outcome also increases. The second line (blue) shows a negative correlation, suggesting that as the variable increases, the outcome decreases. The third line (green) shows no correlation, indicating that changes in the variable do not affect the outcome. The fourth line (red) shows a mixed correlation, with some points showing a positive correlation and others showing a negative correlation. Overall, the graph provides valuable insights into the relationship between the variable and the outcome, highlighting the importance of considering different conditions or groups when interpreting the results.</sample>
    <sample id="830">The graph shows the results of a study on the effects of different strategies applied to offline models. The x-axis represents the ratio of the number of words in the target language to the number of words in the source language, while the y-axis measures the F1 score, which is a measure of accuracy and precision. The graph compares four different strategies: wait-k, LA, CAT, and EDAT. The wait-k strategy shows a significant increase in F1 score as the word ratio increases, indicating that it performs better with larger differences between the source and target languages. The LA, CAT, and EDAT strategies also show an increase in F1 score with increasing word ratio, but they perform slightly worse than the wait-k strategy. Overall, the graph suggests that the wait-k strategy may be more effective for offline models when dealing with large differences between the source and target languages.</sample>
    <sample id="831">The graph shows the results of an experiment comparing different strategies for a certain task. The x-axis represents the number of trials, and the y-axis represents the accuracy of each strategy. The graph shows that the EDAT strategy consistently outperforms the other strategies across all trial numbers.</sample>
    <sample id="832">The video features a woman with long brown hair, wearing a black top, speaking directly to the camera. The background is a plain wall with a window on the right side. The lighting is bright and even, highlighting the speaker's face and upper body. The video appears to be a presentation or lecture, as the woman is speaking in a clear and engaging manner. The overall atmosphere is professional and informative, suggesting that the video may be part of an educational or instructional series.</sample>
    <sample id="833">Google, University of Edinburgh, and University of Cambridge.</sample>
    <sample id="834">Storybrook University, Harvard University, and Stanford University.</sample>
    <sample id="835">English to German and English to French.</sample>
    <sample id="836">Yulia Tsvetkova</sample>
    <sample id="837">DELP-AP-48, DELP-AP-128, DELP-WE-48, DELP-WE-128</sample>
    <sample id="838">57</sample>
    <sample id="839">Three</sample>
    <sample id="840">The authors experimented on the copy dataset and the provider's general dataset.</sample>
    <sample id="841">Language model acceptability judgments are not always robust to context</sample>
    <sample id="842">Language model acceptability judgments are not always robust to context</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="845">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="846">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="850">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="851">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="852">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="853">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="854">Approach Test whether MPP agrees vary as a function of context length, structural, and acceptability.</sample>
    <sample id="855">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="856">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="857">Approach Test whether MPP agrees vary as a function of context length, structural match, and acceptability.</sample>
    <sample id="858">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="859">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="860">We perform MPP judgments for different context lengths (different to 700 tokens) matched/mismatched structure.</sample>
    <sample id="861">We perform MPP judgments for different context lengths (different to 700 tokens) matched/mismatched structure.</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="863">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="864">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="865">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="866">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="867">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="868">Why do matched prefixes affect LM judgments? We also suspect that models are in sensitive to the relevant structure, and whether models are sensitive to these sentences.</sample>
    <sample id="869">Why do matched prefixes affect LM judgments? We also ask models whether they are sensitive to these sentences.</sample>
    <sample id="870">Why do matched prefixes affect LM judgments? We ask whether models sometimes in ways that preserve the relevant structure, and we ask models similarly sensitive to these sentences.</sample>
    <sample id="871">Why do matched prefixes affect LM judgments? We also discuss whether models are in the relevant structure, and whether they are sensitive to these sentences.</sample>
    <sample id="872">Why do matched prefixes affect LM judgments? We also discuss whether models are sensitive to these sentences.</sample>
    <sample id="873">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared to sentences. MPP evaluations with short (short-single) sentence inputs do not fully LTM's.</sample>
    <sample id="874">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short (short-single) sentence inputs do not fully LIME.</sample>
    <sample id="875">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short (short-single) sentence inputs do not fully LIME.</sample>
    <sample id="876">NACHOS is a pre-training strategy.</sample>
    <sample id="877">David Warms Markes</sample>
    <sample id="878">Up to 40 BLEURT points</sample>
    <sample id="879">University of Edinburgh, University of Glasgow, and University of Manchester.</sample>
    <sample id="880">The 5 expert-written instructions are: 1. How to use a computer, 2. How to use a mobile phone, 3. How to use a tablet, 4. How to use a laptop, 5. How to use a desktop computer.</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by using a coreference resolution task.</sample>
    <sample id="939">Comparative evaluation and Likert rating.</sample>
    <sample id="940">6</sample>
    <sample id="941">The background knowledge needed is that judges decide cases in courts of law.</sample>
    <sample id="942">yes, on GitHub</sample>
    <sample id="943">yes</sample>
    <sample id="944">First prefix, long prefix, first prefix with adverb, and random prefix.</sample>
    <sample id="945">It means that you can rate each dimension on a scale.</sample>
    <sample id="946">Beijing Jiaotong University, Microsoft Research Asia, and Shanghai Sun Yat-sen Key Laboratory of Intelligent Information Technology.</sample>
    <sample id="947">No</sample>
    <sample id="948">The rare class challenge is a challenge that is faced by the researchers.</sample>
    <sample id="949">What is Cognitive Dissonance?</sample>
    <sample id="950">What is Cognitive Dissonance?</sample>
    <sample id="951">What is Cognitive Dissonance?</sample>
    <sample id="952">What is Cognitive Dissonance?</sample>
    <sample id="953">Why dissidence?</sample>
    <sample id="954">Why is there a decline in trust?</sample>
    <sample id="955">Why dissidence?</sample>
    <sample id="956">Why dissidence?</sample>
    <sample id="957">The video shows a presentation slide with the title 'Annotations' and a flowchart diagram. The flowchart has three steps: 'Step 1: Good quality?', 'Step 2: Good quality?', and 'Step 3: Good quality?'. Each step has two options: 'Yes' and 'No'. The slide also includes a Twitter handle '@User handle' and a note that says 'Wish I could hold out but I guess I can't at the time.'</sample>
    <sample id="958">The video shows a presentation slide with the title 'Annotations' and a flowchart diagram. The flowchart has three steps: 'Step 1: Good quality?', 'Step 2: Yes?', and 'Step 3: No?'. Each step has two branches leading to different outcomes. The first branch is labeled 'Yes' and the second branch is labeled 'No'. The flowchart also includes annotations such as '-3.5%', '-49%', '-48%', and '-47%'. There is also a Twitter handle '@User handle' and a note that says 'Wish I could hold out but I guess I can't at the time.'</sample>
    <sample id="959">The video shows a presentation slide with the title 'Annotations' and a Twitter handle '@User handle' at the bottom. The slide includes a flowchart with three steps: 'Step 1: Good quality?', 'Step 2: Yes?', and 'Step 3: Yes?'. Each step has a percentage value associated with it, ranging from -5.3% to -49%. The slide also features a Twitter logo and a user handle '@User handle' at the bottom.</sample>
    <sample id="960">The training on initial annotated set is a small annotated dataset.</sample>
    <sample id="961">The speaker is discussing the training process for a machine learning model. The model was initially trained on an initial annotated set, which was a small dataset of 439011 data points. The speaker mentions that the model's performance on this initial set was not very good, with an area under the ROC curve (AUC) of 0.439011.</sample>
    <sample id="962">The method is called Transfer and Active Learning for Annotate Rare Class.</sample>
    <sample id="963">The initial model is trained on a small set of examples.</sample>
    <sample id="964">The graph shows the performance of different models on the Debate dataset. The models include RoBERTa base + classifier head, DeBERTa, and CE. The graph compares their performance in terms of ROC AUC (Area Under the Curve) scores. The DeBERTa model has the highest ROC AUC score, followed by RoBERTa base + classifier head and CE. The graph also includes a note about transferring weights after training on Debate and combining it with data from other datasets.</sample>
    <sample id="965">The graph shows the performance of different models in terms of ROC AUC (Area Under the Receiver Operating Characteristic curve) on a specific task. The models compared include RoBERTa base + classifier head, RoBERTa base, and RoBERTa base + classifier head. The graph indicates that the RoBERTa base + classifier head model performs best, followed by the RoBERTa base model, and then the RoBERTa base model without the classifier head.</sample>
    <sample id="966">The graph shows the performance of different models on the Debate dataset. The models include RoBERTa_base + classifier head, RoBERTa_base, and RoBERTa_base + classifier head. The graph compares their performance in terms of ROC AUC (Area Under the Curve) scores. The graph also includes a note about transferring weights after training on DebatE and combining it with data from the Debate dataset.</sample>
    <sample id="967">The speaker is discussing the performance of different models in a transfer learning task. The speaker mentions that the model with the highest area under the ROC curve (AUC) is the one with the highest accuracy, which is 0.49. The speaker also mentions that the model with the lowest AUC is the one with the lowest accuracy, which is -0.12. The speaker then goes on to discuss the importance of finetuning each task consecutively and the impact of using a pre-trained model on the performance of the model.</sample>
    <sample id="968">The video clip is a presentation slide titled "Active Learning: Cumulative vs. Iterative Update." The slide features a diagram with various elements and text, including a flowchart-like structure with arrows indicating the process of active learning. The diagram includes terms such as "Initial Model," "Rare annotation (e.g., hedgehogs)," "Cumulative (CM)," "Retraining," and "Human annotation." The slide also contains a section labeled "START" and another labeled "END," suggesting a sequential process. The overall content appears to be related to machine learning or data analysis, specifically focusing on how models are updated and refined over time using active learning techniques.</sample>
    <sample id="969">The graph shows that the iterative update method has a higher AUC value than the cumulative update method.</sample>
    <sample id="970">The probability of rare class is much higher in the training set than in the test set.</sample>
    <sample id="971">I'm going to talk about the probability of rare class strategy.</sample>
    <sample id="972">The probability of rare class strategy is a strategy that is used in active learning. It is a strategy that is used to identify rare classes in a dataset and then use those classes to train a model. The strategy is based on the idea that rare classes are often more difficult to learn than common classes, so by focusing on those classes, the model can learn to recognize them more effectively.</sample>
    <sample id="973">The graph shows the performance of different active learning strategies in terms of AUC (Area Under the Curve) scores. The baseline model with scorch has an AUC score of 0.471, while the transformed model has a significantly higher AUC score of 0.722. Other strategies like AL Entropy, AL Cal Entropy, and AL CL are also compared, showing varying levels of performance.</sample>
    <sample id="974">The probability of rare class strategy is a strategy that can be used in active learning.</sample>
    <sample id="975">The video is about the PRCC algorithm.</sample>
    <sample id="976">The video is about the PRCC algorithm.</sample>
    <sample id="977">The video clip does not contain any spoken English content.</sample>
    <sample id="978">BART, BERT, GPT-2, EMO, Blender 1.0, Blender 2.0</sample>
    <sample id="979">There are 6 authors involved in the paper.</sample>
    <sample id="980">A good planner should be able to understand the problem, generate a plan, and execute it effectively.</sample>
    <sample id="981">10</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">Institute of Computer Science, University of Warsaw</sample>
    <sample id="984">The video begins with a static image of a presentation slide. The slide is titled "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" and includes the names of four individuals: Yuesen Zhong, Jun Wang, Zhizhong Wang, and Rui Jiang. Below the title, there are two logos: one for Penn State University and another for Amazon. The background of the slide is white, and the text is primarily black with some blue and orange accents. The slide appears to be part of a formal presentation or academic lecture, possibly related to linguistics, computer science, or artificial intelligence. The video then transitions to a live video call interface, showing a person wearing headphones and a headset. The person is likely participating in a virtual meeting or conference. The background behind the person is blurred, but it appears to be an indoor setting with warm lighting. The video call interface includes various icons and options typical of such platforms, such as mute, camera, and participant controls.</sample>
    <sample id="985">Semantic Parsing is the task of building a semantic representation of the user queries.</sample>
    <sample id="986">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="988">Cross-lingual semantic parsing is a field of study that focuses on the interpretation and understanding of meaning across different languages. The speaker explains that existing models for cross-lingual semantic parsing are often evaluated separately, which limits their applicability to real-world tasks and applications. The lack of coverage on certain natural languages also poses a challenge for these models.</sample>
    <sample id="989">Cross-lingual semantic parsing. Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications on natural language. Lack of coverage on certain meaning representation.</sample>
    <sample id="990">Cross-lingual semantic parsing is a field of study that focuses on the representation and processing of meaning across different languages. The slide explains that existing models are often limited in their tasks and applications, particularly in the area of semantic representation. The slide also highlights the lack of coverage on certain meaning representations, which is a challenge for cross-lingual semantic parsing.</sample>
    <sample id="991">Cross-lingual semantic parsing.</sample>
    <sample id="992">Cross-lingual semantic parsing.</sample>
    <sample id="993">We provide a unified dataset for cross-lingual semantic parsing in multiple natural languages and meanings. It contains 8 semantic parsing tasks, 28 noun representations in 15 families.</sample>
    <sample id="994">We provide a unified dataset for cross-lingual semantic parsing in multiple natural languages and meanings. It contains 8 semantic parsing tasks, 28 noun representations in 15 families.</sample>
    <sample id="995">We consider the six settings for training and evaluation. Then we use the google API to translate from the source language to the target language. Then we use the monolingual model to train and evaluate.</sample>
    <sample id="996">We consider the six settings for training and evaluation. Then we use the google API to translate from the source language to the target language. Then we use the monolingual model to train and evaluate.</sample>
    <sample id="997">We consider the six settings for training and evaluation. Then we use google translate API to translate from the source language to the target language. Then we use a monolingual model to train and evaluate.</sample>
    <sample id="998">Monolingual Few-Shot</sample>
    <sample id="999">Monolingual Few-Shot</sample>
    <sample id="1000">Monolingual Few-Shot</sample>
    <sample id="1001">The video clip is a presentation slide titled "Experiment Settings" with a blue background and white text. The slide outlines the settings for training and evaluation of a multilingual model, which is trained on six languages: German, English, Chinese, and others. The slide also includes a diagram that shows the flow of data from training to inference, with arrows pointing from each language to a central node labeled "Multilingual Model." The slide is numbered 10 in the bottom right corner, indicating it is part of a larger presentation.</sample>
    <sample id="1002">The experiment settings for the study are as follows. We consider six settings for training and evaluation for all languages. Multilingual model: Train one multilingual model for all languages.</sample>
    <sample id="1003">The video clip is a presentation slide.</sample>
    <sample id="1004">We consider the six set of cross-lingual zero-shot transfer and evaluation on one source language and to another language. Train one model.</sample>
    <sample id="1005">* We consider the six cross-lingual zero-shot transfer and evaluation on one source language to another language.</sample>
    <sample id="1006">The speaker is discussing the performance of different models on a monolingual setting. The speaker is comparing two groups of models, one with pre-trained encoders and decoders, and the other without. The speaker found that the model with pre-trained encoders and decoders performed better than the other model. The speaker also mentions that the model with pre-trained encoders and decoders performed better on all datasets.</sample>
    <sample id="1007">The speaker is presenting a slide titled "Analysis of Monolingual" which compares the performance of two groups of models on monolingual settings. The slide lists several models, including "En-PR+M+R+PTB," "En-PR+M+R+PTB+RET," and "En-PR+M+R+PTB+RET with Pointer-Based Decoders." The speaker mentions that they found "Dec-75s" to be the best performing model across all datasets. The slide also includes a table comparing the performance of different models on various metrics such as MAE, MAE (m), MAE (t), MAE (m,t), MAE (m,t,s), MAE (m,t,s,r), MAE (m,t,s,r,p), MAE (m,t,s,r,p,b), MAE (m,t,s,r,p,b,d), MAE (m,t,s,r,p,b,d,a), MAE (m,t,s,r,p,b,d,a,c), MAE (m,t,s,r,p,b,d,a,c,e), MAE (m,t,s,r,p,b,d,a,c,e,f), MAE (m,t,s,r,p,b,d,a,c,e,f,g), MAE (m,t,s,r,p,b,d,a,c,e,f,g,h), MAE (m,t,s,r</sample>
    <sample id="1008">Analysis of Monolingual We evaluate two groups of models on Monolingual Setting. We found Dec-75 (mT5) the best performance on all datasets!</sample>
    <sample id="1009">Analysis of Monolingual We evaluate two groups of models on Monolingual Setting. We found Dec-75 (mT5) the best performance on all datasets!</sample>
    <sample id="1010">Analysis of Multilingual Training We evaluate mts and XML+PR on mpt-7B. Enc-Dec-Enc+PR(mpt-7B) can be improved by training in a mixture of various languages.</sample>
    <sample id="1011">Analysis of Multilingual Training We evaluate mts and XML+PR on mpt-7B. Enc-Dec-Enc+PR(mpt-7B) can be improved by training in a mixture of various languages.</sample>
    <sample id="1012">The analysis of multilingual training is a crucial aspect of language learning.</sample>
    <sample id="1013">The analysis of multilingual training is a crucial aspect of language learning.</sample>
    <sample id="1014">Cross-lingual performance gap.</sample>
    <sample id="1015">The speaker is discussing the performance gap in cross-lingual settings, specifically focusing on few-shot transfer and zero-shot transfer. The speaker mentions that the performance gap is larger for zero-shot transfer compared to few-shot transfer. The speaker also highlights that the performance gap varies across different languages and tasks.</sample>
    <sample id="1016">The cross-lingual performance gap is a significant challenge in the field of machine learning. It refers to the difference in performance between models trained on one language and applied to another. The speaker explains that this gap can be reduced by using few-shot learning, where only a small amount of data from the target language is used during training. This approach allows for more efficient transfer of knowledge from the source language to the target language, leading to improved performance.</sample>
    <sample id="1017">Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Find</sample>
    <sample id="1018">Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Findings (Section 4 in Paper) Other Results &amp; Find</sample>
    <sample id="1019">We've developed a benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We've conducted a comprehensive benchmark study on three representative types of multilingual language models. Our results show that monolingual training yields the best performance, while monobilingual LLMs are still inadequate for cross-lingual tasks. Moreover, the performance gap between monolingual and cross-lingual learning is significant.</sample>
    <sample id="1020">We've developed a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We've conducted a comprehensive benchmark study on three representative types of multilingual language models. Our results show that monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual tasks. Moreover, the performance gap between monolingual and cross-lingual transfer learning is still significant.</sample>
    <sample id="1021">Accuracy</sample>
    <sample id="1048">Emory University, Emory NLP Research Lab, and Alexa</sample>
    <sample id="1049">Continual Fine-Tuning</sample>
    <sample id="1050">6</sample>
    <sample id="1084">Yusen Zhong</sample>
    <sample id="1085">From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</sample>
    <sample id="1086">LM Training Data A mixed blessing</sample>
    <sample id="1087">LM Training Data A mixed blessing</sample>
    <sample id="1088">LM Training Data A mixed blessing</sample>
    <sample id="1089">LM Training Data A mixed blessing</sample>
    <sample id="1090">To this end</sample>
    <sample id="1091">To this end</sample>
    <sample id="1092">To this end</sample>
    <sample id="1093">The video features a presentation slide titled "Evaluating LM Political Learning." The slide discusses the evaluation of language models (LMs) in terms of political leaning, emphasizing the need to support both encoder and decoder LMs. It suggests using statements like "You agree/disagree with this statement?" and compares the race's many qualities to those of other races. The slide also mentions automatic evaluation and being grounded in polis lit. The presenter, identified as "Graeme," is visible in the top right corner. The slide then transitions to a chart titled "Existing LMs," which categorizes various language models based on their political leanings, ranging from liberal to authoritarian. The chart includes models such as BERT-large, RoBERTa-base, and GPT-4, among others, and uses a color-coded system to represent different political stances.</sample>
    <sample id="1094">The video presents a detailed analysis of existing language models (LMs) in terms of their political and economic ideologies. The speaker discusses the characteristics of various LMs, including BERT-base, CodeGPT-ada, and GPT-3, among others. The video uses a color-coded chart to illustrate the ideological spectrum of these models, ranging from authoritarian to libertarian on the horizontal axis and from social to economic on the vertical axis. The speaker explains that some LMs are more aligned with authoritarian or libertarian views, while others lean towards social or economic perspectives. The video provides a comprehensive overview of the ideological landscape of existing LMs, highlighting their diverse political and economic orientations.</sample>
    <sample id="1095">The video shows a graph with different points representing existing LMs. The graph is divided into four quadrants, each representing a different political ideology. The points are labeled with the names of different LMs, such as BERT-base, RoBERTa-base, and GPT-3. The video also includes a list of existing LMs on the left side of the graph, with their corresponding political ideologies. The video appears to be a visual representation of the political bias of existing LMs, with the goal of highlighting the need for more balanced and unbiased AI systems.</sample>
    <sample id="1096">The video presents a slide titled "Pretraining Data" with two diagrams labeled "News Media (Reddit)" and "Social Media (Reddit)." Each diagram has three categories: left, center, and right. The left category is colored blue, the center category is colored green, and the right category is colored red. Below each diagram, there are three lines of text that read "left," "center," and "right." The slide also includes a bullet point list on the left side, which reads: "Further pretrain LM (Roberta, GPT-2, GPT-3) check, evaluate change in political leaning." The background of the slide is white, and the text and diagrams are clearly visible.</sample>
    <sample id="1097">The video shows a presentation slide with the title "Pretraining Data" and two diagrams labeled "News Media (Reddit)" and "Social Media (Reddit)." The slide also includes text that reads, "Further pretrain LM (Roberta, GPT-2, GPT-check) evaluation, evaluate change in political leaning." The video then transitions to another slide titled "Results," which displays a graph with four quadrants representing different political leanings. The graph is divided into left, center, and right sections, with arrows pointing from the original news media to the pre-trained news media, indicating a shift in political leaning. The video appears to be a lecture or presentation on the impact of pretraining data on language models' political bias.</sample>
    <sample id="1098">The results of the study on partisan shifts in LM political leaning are presented. The graph shows a comparison between Roberta and GPT-2 models, with the left side representing the original news and the right side representing the news after being processed by the respective models. The graph is divided into four quadrants, each representing a different political leaning: Left, Center, Right, and Center. The lines connecting the points indicate the shift in political leaning from the original news to the processed news.</sample>
    <sample id="1099">Results Partisan shifts in LM political leaning Roberta GPT-2 Left Center Right</sample>
    <sample id="1100">The video shows a series of graphs with different colors and lines, representing data or information. The graphs are arranged in a grid format, with each graph having its own unique set of axes and labels. The video appears to be an educational or informative presentation, possibly related to data analysis or statistics.</sample>
    <sample id="1101">The Trump Card</sample>
    <sample id="1102">The Trump Card</sample>
    <sample id="1103">The Trump Card</sample>
    <sample id="1104">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1105">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1106">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1107">Per-Category Performance 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1st 4th 1 Per-Category Performance</sample>
    <sample id="1108">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1109">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1110">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1111">Qualitative Analysis Target Label Base N.S. N.R. S.L R.S R.S R</sample>
    <sample id="1112">The qualitative analysis table is presented, showing a comparison of different models' performance on hate speech detection tasks. The table includes columns for 'Target Label,' 'Base,' 'N-SLR,' and 'N-RSR,' with rows listing various test cases and their corresponding true or false labels. The table highlights the differences in model performance across various scenarios, providing insights into the effectiveness of each model in detecting hate speech.</sample>
    <sample id="1113">The table is divided into two main columns: "Attitude" and "N1, N2, N3." Under the "Attitude" column, there are three categories: "Attitude," "N1," and "N2." The "N1" category has a sub-column labeled "N1" with a series of numbers ranging from 1 to 9. The "N2" category has a sub-column labeled "N2" with a series of numbers ranging from 1 to 9. The "N3" category has a sub-column labeled "N3" with a series of numbers ranging from 1 to 9. Each row under these categories contains text that appears to be a list of items or statements, possibly related to the attitudes being measured. The text is too small and blurry to read clearly.</sample>
    <sample id="1114">The table is divided into two main columns: "Attitude" and "N1, N2, N3." Under the "Attitude" column, there are three categories: "Neutral," "Negative," and "Positive." Each category has a list of statements under it. The statements are numbered from 1 to 8. The table also includes a section labeled "Quantification of new life experiences" with a list of statements under it.</sample>
    <sample id="1115">The table is divided into two main columns: "Hate Speech Test" and "Moderation Test." The first column lists various hate speech scenarios, while the second column categorizes each scenario as either "True" or "False." The table also includes a third column titled "New Life," which appears to be a separate evaluation criterion.</sample>
    <sample id="1116">The table is divided into two columns, labeled "Attitude" and "N1, N2, N3." The first column lists various attitudes such as "I'm a good person," "I'm not a bad person," "I'm a good person but I make mistakes," and so on. The second column contains numerical values ranging from 1 to 5, with each row corresponding to a specific attitude. The table appears to be a Likert scale used for measuring attitudes or opinions in a survey or study.</sample>
    <sample id="1117">Discussion Between Sycallia and Charybdis To "Sanitize" or not to "Sanitize"? That is the question</sample>
    <sample id="1118">Discussion Between Sycallia and Charybdis To "Sanitize" or not to "Sanitize"? That is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">Thank you!</sample>
    <sample id="1120">Thank you!</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">University of Washington, Carnegie Mellon University, and University of Edinburgh.</sample>
    <sample id="1124">Bouquet (Stanford)</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">CROWNS and CROWNS-2</sample>
    <sample id="1128">When Does Translation Require Context? A Data-driven, Multilingual Exploration</sample>
    <sample id="1129">Translation depends on context. We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics.</sample>
    <sample id="1133">Evaluating context-dependent translation is hard.</sample>
    <sample id="1134">RQ2: When does translation require context?</sample>
    <sample id="1135">When does translation require context?</sample>
    <sample id="1136">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1137">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1138">We introduce P-CXMI to measure context to translate a specific sentence.</sample>
    <sample id="1139">When does translation require context?</sample>
    <sample id="1140">The thematic analysis of high P-CXMI words</sample>
    <sample id="1141">Thematic analysis of high-PCXMI words 1. POS tags</sample>
    <sample id="1142">Thematic analysis of high-PCXMI words 1. POS tags Pronouns</sample>
    <sample id="1143">Thematic analysis of high PCXMI words 1. POS tags 2. Vocabulary items Pronouns Verb form</sample>
    <sample id="1144">Thematic analysis of high P-CCMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form 3. Lexical cohesion Avellene's mother was still asleep. Avellene went to school.</sample>
    <sample id="1145">Thematic analysis of high P-CCMI words 1. POS tags 2. Vocabulary items Pronouns Verb form Lexical cohesion Formality</sample>
    <sample id="1146">She knows where we're going. I don't.</sample>
    <sample id="1147">When does translation require context?</sample>
    <sample id="1148">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1149">The Multilingual Discourse-Aware (MuDA) tagger is a tool that can be used to analyze and understand the discourse structure of multilingual texts. It uses a combination of linguistic features such as pronouns, verb forms, lexical cohesion, and formality to identify and categorize different types of discourse structures in a text.</sample>
    <sample id="1150">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1151">When does translation require context?</sample>
    <sample id="1152">The English content is already in written text.</sample>
    <sample id="1153">The robot on the left has a speech bubble with the word "CONTEXT" in it, indicating that it is focused on context. The robot in the middle has a speech bubble with the word "CONTEXT" crossed out, suggesting that it does not consider context. The robot on the right has a speech bubble with the word "CONTEXT" inside it, implying that it does consider context.</sample>
    <sample id="1154">The English content is turned into written text.</sample>
    <sample id="1155">MUDA benchmark results Â· Context-aware models perform significantly better on some phenomena</sample>
    <sample id="1156">MUDa benchmark results</sample>
    <sample id="1157">The benchmark results show that context-aware models perform significantly better on some phenomena.</sample>
    <sample id="1158">The summary is a dataset.</sample>
    <sample id="1159">Identify discourse phenomena systematically without prior linguistic knowledge. Dataset-agnostic benchmark for document-level MT.</sample>
    <sample id="1160">Identify discourse phenomena systematically without prior linguistic knowledge. Dataset-agnostic benchmark for document-level MT.</sample>
    <sample id="1161">FT, L2R, COSINE, MLC, ML.</sample>
    <sample id="1162">13 tasks</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intra- and Intertextual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Mommen, Laura Kallmeyer Heinrich Heine University Dissolfort Germany ACL 2023</sample>
    <sample id="1164">1. Text Simplification What, why and How?</sample>
    <sample id="1165">Text Simplification Example</sample>
    <sample id="1166">Text Simplification Example</sample>
    <sample id="1167">Text Simplification Example</sample>
    <sample id="1168">Text Simplification Example</sample>
    <sample id="1169">German Text Simplification Corpus</sample>
    <sample id="1170">German Text Simplification Corpora</sample>
    <sample id="1171">German Text Simplification Corpus</sample>
    <sample id="1172">German Text Simplification Corpus</sample>
    <sample id="1173">German Text Simplification Corpus</sample>
    <sample id="1174">German Text Simplification Corpus</sample>
    <sample id="1175">The video shows a graph with two bars.</sample>
    <sample id="1176">The video shows a graph with two axes. The x-axis is labeled "Simplification" and the y-axis is labeled "Types of Simplification." There are four bars on the graph, each representing a different type of simplification: "Simplicity," "LexSimp," "StructSimp," and "LexStructSimp." The bars are colored differently, with "Simplicity" in blue, "LexSimp" in yellow, "StructSimp" in orange, and "LexStructSimp" in red. The graph also includes a legend that explains the meaning of each color.</sample>
    <sample id="1177">The video shows a graph with two axes.</sample>
    <sample id="1178">The video shows a graph with two bars.</sample>
    <sample id="1179">The video shows a graph with two bars.</sample>
    <sample id="1180">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="1181">Automatic Alignment Evaluation.</sample>
    <sample id="1182">Automatic Alignment Evaluation.</sample>
    <sample id="1183">Automatic Alignment Evaluation.</sample>
    <sample id="1184">Automatic Alignment Evaluation.</sample>
    <sample id="1185">Automatic Alignment Evaluation.</sample>
    <sample id="1186">Automatic Alignment Evaluation.</sample>
    <sample id="1187">Automatic Alignment Evaluation.</sample>
    <sample id="1188">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123)</sample>
    <sample id="1189">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123)</sample>
    <sample id="1190">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-AP-APA (123) DEEP-AP-WE (147)</sample>
    <sample id="1191">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123) DEEP-AP-APA (123)</sample>
    <sample id="1192">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-N-APA (123) DEEP-N-WE (147) Document Level Results on document simplification using iH-BART. DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-N-APA (123) DEEP-N-WE (147) Sentence Level Results on sentence simplification using iH-BART. DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-N-APA (123) DEEP-N-WE (147)</sample>
    <sample id="1193">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184) Document Level Results on document simplification results correspond to the length of training data. DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184) Sentence Level Results on sentence simplification results correspond to the length of training data. DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184)</sample>
    <sample id="1194">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184) Document Level Results on document simplification using iH-BART. n = 123 n = 147 n = 154 n = 184 DEEP-AP-APA DEEP-AP-WE DEEP-WE-APA DEEP-WE-WE 0.96 0.95 0.95 0.94 0.95 0.94 0.94 0.93 0.95 0.94 0.94 0.93 0.94 0.93 0.93 0.92 0.94 0.93 0.93 0.92 0.92 0.92 0.91 0.93 0.92 0.92 0.91 0.91 0.91 0.90 0.92 0.91 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0.90 0.91 0.90 0.90 0.90 0</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">The video features a presentation slide titled 'Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)' by Mohammadi Javad Hosseini, Filip Rzadkis, Piotr Sztajniewicz, and Annie Louis. The slide is part of a Google Research presentation, as indicated by the logo at the bottom left corner. The background of the slide is white with colorful lines connecting red dots, creating a visual flow from the top left to the bottom right. The text on the slide is in black, making it stand out against the white background.</sample>
    <sample id="1197">The video clip features a presentation slide titled 'Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)' by Mohammadi Javad Hosseini, Filip Srikant, Parveen, and Annie Louis. The slide is part of a Google Research presentation, as indicated by the logo at the bottom left corner. The background of the slide is white with colorful lines connecting red dots, creating a visual representation of connections or paths. The text on the slide is in black, making it stand out against the white background.</sample>
    <sample id="1198">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: "Did you mean 'on' or 'or'?"</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: The first one Indirect reference: Can't remember used in the natural and fluid conversation Pronunciations are hard to distinguish You want to specify a preference The song's not energetic</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: Easy - "me", the first one Indirect reference: Cannot remember used in the natural and fluid conversation Pronunciations are hard to distinguish The new one The song's not energetic</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: Easy, "the first one" Indirect reference: Cannot remember used in the natural and fluid conversation The pronunciations are hard to distinguish You want to specify a preference</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: The first one. Indirect reference: Can't remember used in the natural and fluid conversation. Pronunciations are hard to distinguish. Want to specify a preference.</sample>
    <sample id="1203">Dataset Collection</sample>
    <sample id="1204">Dataset Collection</sample>
    <sample id="1205">Dataset Collection Methodology</sample>
    <sample id="1206">Dataset Collection Methodology</sample>
    <sample id="1207">Dataset Collection Methodology</sample>
    <sample id="1208">Dataset Collection Methodology</sample>
    <sample id="1209">Dataset Collection Methodology</sample>
    <sample id="1210">Dataset Collection Methodology</sample>
    <sample id="1211">Google Research</sample>
    <sample id="1212">Google Research</sample>
    <sample id="1213">Google Research</sample>
    <sample id="1214">Google Research</sample>
    <sample id="1215">Google Research</sample>
    <sample id="1216">Background knowledge (Music) Google search link to each song We ask annotators to: Listen to at least some of each song Read about each song Click here to find out more about each song</sample>
    <sample id="1217">Background knowledge (Music) Google search link to each song Easy on Me (by Adele) The Black Eyed Peas (by The Black Eyed Peas) We ask annotators to: Listen to at least some of each song Read about each song Click here to find out more about each song</sample>
    <sample id="1218">BACKGROUND KNOWLEDGE (Music)</sample>
    <sample id="1219">BACKGROUND KNOWLEDGE (Recipes) Pandan Cake Pandan is a light, fluffy sponge cake made from the juice of the Pandanus amaryllifolius plant. It is popular in Southeast Asia, especially among the Malay community.</sample>
    <sample id="1220">Eliciting expressions</sample>
    <sample id="1221">Music Selection Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria or the Library of Congress? Do you mean "the Lighthouse of Alexandria" or "the 1980s"? Smith, the Hall of Fame or the Nobel Prize? The Library of Alexandria</sample>
    <sample id="1222">The AI model has access to the same background knowledge as the LLM.</sample>
    <sample id="1223">The AI model has access to the same background knowledge as the LLM.</sample>
    <sample id="1224">The AI model has access to the same background knowledge as the LLM.</sample>
    <sample id="1225">Thank You! If you have any questions, please email javdhv@google.com</sample>
    <sample id="1226">CamemBERT is initially trained on the French Wikipedia.</sample>
    <sample id="1227">Adam Prezrokowski</sample>
    <sample id="1228">Performance degrades with larger temporal gap</sample>
    <sample id="1229">The video features a presentation slide titled 'NLP Positionality: Characterizing Design Biases of Datasets and Models.' The slide is divided into two sections. The top section lists the names and affiliations of six individuals, each accompanied by their respective headshots. The bottom section contains the title of the presentation in bold black text on a white background. The individuals listed are:

1. 'Suebotin S., 'Jenny' T.' from the Subbotin Institute at the University of Washington.
2. 'Jenny T. I.' from the Subbotin Institute at the University of Washington.
3. 'Ronan B.' from the Allen Institute for Artificial Intelligence A.
4. 'Katherine Reinecke' from the University of Washington.
5. 'Marleen S. Sagap' from Carnegie Mellon University.

The background of the slide is plain white, and the text is primarily in black, with the exception of the names and affiliations, which are in blue. The overall design is simple and professional, focusing on the information presented rather than visual elements.</sample>
    <sample id="1230">The video features a presentation slide titled 'NLP Positionality: Characterizing Design Biases of Datasets and Models.' The slide is divided into two sections. The top section lists the names and affiliations of six individuals, each accompanied by their respective headshots. The bottom section contains the title of the presentation in bold black text on a white background. The individuals listed are:

1. 'Suebotin S., 'Jenny' T.' from the Subbotin Institute at the University of Washington.
2. 'Jenny T. I.' from the Subbotin Institute at the University of Washington.
3. 'Ronan B.' from the Allen Institute for Artificial Intelligence A.
4. 'Katherine Reinecke' from the University of Washington.
5. 'Marleen S. Sagap' from Carnegie Mellon University.

The background of the slide is plain white, and the text is primarily in black, with the exception of the names and affiliations, which are in blue. The overall design is simple and professional, focusing on the information presented rather than visual elements.</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine... Can you stop being a jerk? &lt;PerspectiveAPI score&gt;</sample>
    <sample id="1233">Imagine... Can you stop being a jerk? Prissitudes everywhere on the news. Card Jones Tech Lead New York Times Aditya Sharma Tech Lead Times of India</sample>
    <sample id="1234">Imagine... ... Design bias example: Can you stop being a jerk? Prissitudes everywhere on the news.</sample>
    <sample id="1235">Positionality The perspectives people hold as a result of their demographics, identity and life experiences. (Savin-Baden, Majoi &amp; Claire Hawley Majoi - Qualitative Research: The essential guide to theory and practice Routledge 2013)</sample>
    <sample id="1236">Positionality "The perspectives people hold as a result of their demographics, identity and life experiences."</sample>
    <sample id="1237">Positionality "The perspectives people hold (hold) as a result of their demographics, identity, and life experiences. [As a researcher], it influences the research process and its outcomes and results." [1]</sample>
    <sample id="1238">Do datasets and models have personality?</sample>
    <sample id="1239">Do datasets and models have personality?</sample>
    <sample id="1240">Do datasets and models have positionality?</sample>
    <sample id="1241">Do datasets and models have positionality?</sample>
    <sample id="1242">Do datasets and models have positionality?</sample>
    <sample id="1243">Do datasets and models have positionality?</sample>
    <sample id="1244">Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models.</sample>
    <sample id="1245">NLPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">The framework is a collection of data that is used to train a model. The model is then used to make predictions on new data. The framework is designed to be flexible and adaptable, allowing for the inclusion of new data and the exclusion of old data as needed. The framework is also designed to be scalable, allowing for the addition of more data and the use of more complex models as the need arises. Overall, the framework is a powerful tool for data analysis and machine learning, providing a structured approach to collecting, processing, and using data to make informed decisions.</sample>
    <sample id="1247">I'm really excited to be here today.</sample>
    <sample id="1248">I'm really excited to be here today.</sample>
    <sample id="1249">I'm really excited to be here today.</sample>
    <sample id="1250">The framework is a collection of data sets that are used to train machine learning models. The data sets are then used to evaluate the performance of the models, and the results are compared to determine which model performs best.</sample>
    <sample id="1251">I'm going to talk about the framework that I've developed for my research.</sample>
    <sample id="1252">LabInTheWild</sample>
    <sample id="1253">LabintheWild</sample>
    <sample id="1254">Task A: Social Acceptability. Participants read a situation about wanting to make a lot of money and enter what they think about it in a text box. Participants then rate how socially acceptable the situation is on a scale from 1 to 5, with 1 being not at all socially acceptable and 5 being very socially acceptable.</sample>
    <sample id="1255">Task A: Social Acceptability Wanting to make a lot of money. Want to know what you think about it? Enter your thoughts here. See AI and their thoughts if you want to compare them to yours. Participants compare their responses to AI's responses to others and an AI.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi GPT-4</sample>
    <sample id="1257">Task B: Toxicity Read the example Enter what you think about it Enter your rating 3 See 3 other ways to say it The AI specialist Participants read an instance from the DynaMeme dataset Participants rate whether they think an instance is hate speech</sample>
    <sample id="1258">The video clip features a static presentation slide titled 'Study Participation' with three key statistics displayed: 16,299 annotations, 1,096 annotators, and 87 countries. The background is plain white, and the text is in black, making it easy to read. The slide appears to be part of a larger presentation or lecture, possibly related to a study or research project involving data collection from various countries.</sample>
    <sample id="1259">Finding 1: There is potentiality in NLP.</sample>
    <sample id="1260">The speaker discusses the challenges of training machine learning models on datasets that are predominantly in English, highlighting the potential for bias and misalignment. The speaker emphasizes the importance of considering the diversity of languages spoken by different countries to ensure more accurate and fair model performance.</sample>
    <sample id="1261">The speaker is discussing the alignment of datasets and models with a college education.</sample>
    <sample id="1262">I'm really excited to be here today.</sample>
    <sample id="1263">Finding 2: Some populations are left behind.</sample>
    <sample id="1264">The video shows a bar graph with three categories: Man, Non-binary, and Woman. The graph compares the social acceptance of non-binary people in datasets and models to that of men and women. The bars are color-coded, with blue representing men, gray representing non-binary individuals, and red representing women. The graph shows that non-binary people are less aligned with binary people in terms of social acceptance.</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspective: a. Share disaggretated dataset labels.</sample>
    <sample id="1267">Recommendations 1. Keep record of all relevant design choices made throughout building datasets and models. 2. Do NLP research through the lens of perspective: Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with specific communities is possible for inclusive NLP (Masakhane initiative).</sample>
    <sample id="1268">Thanks!</sample>
    <sample id="1269">To ensure that the output sequence is in the correct order.</sample>
    <sample id="1270">To increase transparency about bias mitigation methods.</sample>
    <sample id="1271">Many people, no customer, has spent, any money</sample>
    <sample id="1272">The authors used F1 score, accuracy, and BLEU score.</sample>
    <sample id="1273">Krippendorff's alpha.</sample>
    <sample id="1274">Space</sample>
    <sample id="1275">Heinrich Heine University, Heinrich Heine University</sample>
    <sample id="1276">It is the first large-scale, public, and multimodal instruction dataset.</sample>
    <sample id="1277">3</sample>
    <sample id="1278">The difference in the number of left and right characters.</sample>
    <sample id="1279">10 words</sample>
    <sample id="1280">The findings suggest that the smaller T5 model may be better suited for specific tasks, such as code generation, due to its ability to produce higher-quality scripts.</sample>
    <sample id="1309">The work investigates the comparison of pre-training strategies and data sources.</sample>
    <sample id="1310">1.5</sample>
    <sample id="1311">The quality of the simplification was evaluated using the DEFLAP-APA and DEFLAP-WE test sets.</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1347">two elements of cognition that are inconsistent</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">yes</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">The data was taken from the P-CXMI words.</sample>
    <sample id="1352">Adam Prezrokowski and Michal Wozniak</sample>
    <sample id="1353">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1356">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Homers loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa and Bart and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1360">Bouquet(Stanford)(Universal Dependencies): Homer loves Lisa and Bart. Chain(Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed(Prague): Homer loves Lisa, Bart, and Maggie. Multi-headed(London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1361">Marge read it yesterday.</sample>
    <sample id="1362">Marge read it yesterday.</sample>
    <sample id="1363">Marge read it yesterday.</sample>
    <sample id="1364">Marge read yesterday it.</sample>
    <sample id="1365">Marge read it yesterday.</sample>
    <sample id="1366">Marge read it yesterday.</sample>
    <sample id="1367">Marge read it yesterday.</sample>
    <sample id="1368">Marge read it yesterday.</sample>
    <sample id="1369">Marge read it yesterday.</sample>
    <sample id="1370">Marge read this book yesterday.</sample>
    <sample id="1371">Marge read it yesterday.</sample>
    <sample id="1372">Conjugation Lengths in English</sample>
    <sample id="1373">Conjunct Lengths in English</sample>
    <sample id="1374">Conjunct Lengths in English</sample>
    <sample id="1375">Conjunct Lengths in English</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Ficler &amp; Goldberg, 1986) left conjuncts tend to be shorter (observed before) this tendency grows with length (1968-80) but only when the Governor is on the left or right (not "is it on the left?") [saw Bart and Lisa, Homer and Ned laughed]</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fickers &amp; Goldberg, 2016) left conjunct tends to be shorter (observed before) typically grows with length (166-80) but only when the Governor is on the left or on the right (not "is on" Ted and Ned laughed)</sample>
    <sample id="1378">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fiser &amp; Goldberg, 2016) left conjunct tends to be shorter (observed before) this tendency grows with length (168-1680) but only when the Governor is on the left or on the right [not if it is on the governor] [I saw Bart and Lisa. Homer Home and Ned laughed.]</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fickers &amp; Goldberg, 2016) left tends to be shorter (observed before) initially tends to grow with length (186-180) but only when the Governor is on the left or on the right (not on the home and Ned's laughed)</sample>
    <sample id="1380">The graph shows the relationship between the number of characters and the length of the right side.</sample>
    <sample id="1381">The graph shows the relationship between the length of the left and right characters and the probability of a character being in the left or right half of the word.</sample>
    <sample id="1382">The graph shows the relationship between the percentage of characters and the length of the word.</sample>
    <sample id="1383">Compatibility with Dependency Structures of Coordination Compatibility with Dependency Structures of Coordination Bouchaud (Stanford) Universal Dependencies: No Chain (Moscow) Homer loves Lisa, Bart, and Maggie. No Conjunction-headed (Prague) Homer loves Lisa, Bart, and Maggie. Yes Multi-headed (London) Homer loves Lisa, Bart, and Maggie. Yes</sample>
    <sample id="1384">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Training on one language and evaluating on another.</sample>
    <sample id="1387">Saarlauand University, Amazon Alexa, 3rd University of Vienna</sample>
    <sample id="1388">AL, AL/CA, CA</sample>
    <sample id="1416">Trees help a lot but...</sample>
    <sample id="1417">School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="1495">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1496">2012</sample>
    <sample id="1527">University of Amsterdam, Saarland University, and University of Helsinki.</sample>
    <sample id="1528">Chen Zhen</sample>
    <sample id="1529">4</sample>
    <sample id="1530">The approach is compared to the dedicated SimulST architecture.</sample>
    <sample id="1531">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks (A) Pre-trained (BERT, T5) (B) Prompting (GPT-3) (C) Instruction tuning (FLAN)</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks (A) Pre-trained (BERT, T5) (B) Prompting (GPT-3) (C) Instruction tuning (FLAN)</sample>
    <sample id="1534">Language-only</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks</sample>
    <sample id="1538">The first multimodal instruction tuning benchmark dataset</sample>
    <sample id="1539">The first multimodal instruction tuning benchmark dataset</sample>
    <sample id="1540">A unified multi-modal pre-trained model is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary for language or image tokens, and the encoder-decoder architecture has a coordination of a bounding box.</sample>
    <sample id="1541">The video shows a woman standing on a train platform, looking at her phone.</sample>
    <sample id="1542">The video shows a woman standing on a train platform, holding a camera and taking a photo of the train.</sample>
    <sample id="1543">The video shows a woman standing on a train platform, looking at her phone.</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">Multi-Modal Instruction Tuning</sample>
    <sample id="1546">Multi-Modal Instruction Tuning</sample>
    <sample id="1547">Training details: Pre-trained OF-A-Large model (472M) - No mix-in tasks for all instances - Each instance randomly combined in one of five instruction templates. Testing details: For each task, we conduct a total of five experiments evaluating the model using one of the five instruction templates. We report the mean and maximum performance and the standard deviation of the performance across five experiments.</sample>
    <sample id="1548">Implementation Details Training details: Pre-trained OF-A-Large model (472M) - No mix-in tasks for all instances - Each instance randomly combined in one of five instruction templates Testing details: For each task, we conduct a total of five experiments. We evaluate the model using one of the five instruction templates. We report the mean and maximum performance across five experiments.</sample>
    <sample id="1549">Training details: Pre-trained OF-A-Large model (472M) - No mix-in tasks for all instances - Each instance randomly combined in one of five instruction templates. Testing details: For each task, we conduct a total of five experiments evaluating the model using one of the five instruction templates. We report the mean and maximum performance and the standard deviation of the performance across five experiments.</sample>
    <sample id="1550">Evaluation Metrics For multi-classification tasks Visual entailment, Visual-Spatial Reasoning, and Disaster Type Classification report the Accuracy CommonSense VQA, VQA Text, Grounded-VQA For multi-modal generation tasks Video-Modality Extraction, Dialogue, and NLG report the Rouge-L</sample>
    <sample id="1551">Sensitivity</sample>
    <sample id="1552">The effectiveness of instruction tuning on MULTINSTRUCT</sample>
    <sample id="1553">The effectiveness of instruction tuning on MULTINSTRUCT</sample>
    <sample id="1554">The impact of increasing multimodal instruction task clusters is a significant area of research in the field of artificial intelligence.</sample>
    <sample id="1555">Effect of Diverse Instructions on Instruction Tuning</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model Sensitivity Figure 4: Model Sensitivity on Unseen Evaluation Lower is better. OFA: 0.13, OFA-FT: 0.15, OFA-FT+OFA: 0.16, OFA-FT+OFA+OFA: 0.17. Instruct tuning on Multitask significantly reduce the sensitivity of the model. Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks</sample>
    <sample id="1558">Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity.</sample>
    <sample id="1559">We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!</sample>
  </task>
</testset>