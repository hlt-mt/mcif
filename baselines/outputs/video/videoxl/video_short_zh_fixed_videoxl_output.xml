<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">webpage</sample>
    <sample id="1">McGill, Mila, Microsoft Research</sample>
    <sample id="2">DEPLAIN: A German Parallel Corpus with Intra-lingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Mommen, Laura Kallmeyer Heinrich Heine University Dissolosse Germany ACL 2023</sample>
    <sample id="3">1. Text Simplification</sample>
    <sample id="4">Text Simplification Example</sample>
    <sample id="5">Text Simplification Example</sample>
    <sample id="6">Text Simplification Example</sample>
    <sample id="7">Text Simplification Example</sample>
    <sample id="8">2. DE-plain 一个新语料库</sample>
    <sample id="9">德国简化文本语料库</sample>
    <sample id="10">德国简化文本语料库</sample>
    <sample id="11">德国简化文本语料库</sample>
    <sample id="12">德国简化文本语料库</sample>
    <sample id="13">德国简化文本语料库</sample>
    <sample id="14">在本节中，我们简要介绍了简化和简化之间的关系。我们还讨论了简化和简化之间的关系。</sample>
    <sample id="15">在本研究中，我们使用了两个数据集：LexSim 和 StructSim。LexSim 是一个基于词频的相似度度量，而 StructSim 则是基于句法结构的相似度度量。我们使用这两个数据集来评估我们的模型在新闻和 fiction 两个领域的性能。</sample>
    <sample id="16">在本节中，我们简要介绍了简化和简化之间的关系。我们还讨论了简化和简化之间的关系。</sample>
    <sample id="17">在本研究中，我们使用了三个不同的数据集：新闻、小说和维基百科。</sample>
    <sample id="18">在本研究中，我们使用了三个不同的数据集：新闻、小说和维基百科。</sample>
    <sample id="19">3. Use-cases 1.1 Upper and lower part 1.2 n-m capabilities</sample>
    <sample id="20">自动对齐评估。</sample>
    <sample id="21">自动对齐评估。</sample>
    <sample id="22">自动对齐评估。</sample>
    <sample id="23">自动对齐评估。</sample>
    <sample id="24">自动对齐评估。</sample>
    <sample id="25">自动对齐评估。</sample>
    <sample id="26">自动对齐评估。</sample>
    <sample id="27">自动文本简化</sample>
    <sample id="28">自动文本简化</sample>
    <sample id="29">自动文本简化</sample>
    <sample id="30">自动文本简化</sample>
    <sample id="31">自动文本简化</sample>
    <sample id="32">自动文本简化</sample>
    <sample id="33">自动文本简化</sample>
    <sample id="34">谢谢。</sample>
    <sample id="35">Patric F. Fernandes</sample>
    <sample id="36">LLM</sample>
    <sample id="37">yes</sample>
    <sample id="38">使用行为注释来评估机器人的行为。</sample>
    <sample id="39">依赖于训练数据的丰富程度。</sample>
    <sample id="40">We ask annotators to listen to at least some of each song and read about the song.</sample>
    <sample id="41">5</sample>
    <sample id="42">依赖结构的协调</sample>
    <sample id="43">依赖结构的协调</sample>
    <sample id="44">依赖结构的协调</sample>
    <sample id="45">依赖结构的协调</sample>
    <sample id="46">依赖结构的协调</sample>
    <sample id="47">依赖结构的协调</sample>
    <sample id="48">依赖结构的协调</sample>
    <sample id="49">依赖结构的协调</sample>
    <sample id="50">依赖结构的协调</sample>
    <sample id="51">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="52">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="53">依赖长度最小化(DLM)</sample>
    <sample id="54">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="55">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="56">依赖长度最小化(DLM)</sample>
    <sample id="57">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="58">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="59">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="60">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="61">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="62">Conjunct Lengths in English</sample>
    <sample id="63">Conjunct Lengths in English</sample>
    <sample id="64">Conjunct Lengths in English</sample>
    <sample id="65">Conjunction Lengths in English</sample>
    <sample id="66">Conjunct Lengths in English</sample>
    <sample id="67">Conjunct Lengths in English</sample>
    <sample id="68">Conjunct Lengths in English</sample>
    <sample id="69">Conjunct Lengths in English</sample>
    <sample id="70">在本节中，我们使用了与上一节中相同的设置。</sample>
    <sample id="71">图1：在不同长度的单词上，N-gram生成器的差异。</sample>
    <sample id="72">在本节中，我们使用了与上一节中相同的设置。</sample>
    <sample id="73">兼容性与依赖结构的协调</sample>
    <sample id="74">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="75">3</sample>
    <sample id="76">news</sample>
    <sample id="77">not is it</sample>
    <sample id="78">yes</sample>
    <sample id="79">DEplain-apa 包含来自 APA 样本的文档。</sample>
    <sample id="80">更大的模型，更好的模型，更多的例子。</sample>
    <sample id="81">left conjunct length is shorter</sample>
    <sample id="82">在实验中，将支配词放在不同位置，如句首、句中和句末，然后测量其对句子长度和单词长度的影响。</sample>
    <sample id="83">效果不好，AUC值只有0.43901</sample>
    <sample id="84">4</sample>
    <sample id="85">John and Mary</sample>
    <sample id="86">形式/ lexical cohesion</sample>
    <sample id="87">Purdue University</sample>
    <sample id="122">使用Pearson's r来量化立场。</sample>
    <sample id="155">在之前的研究中，当人类受试者被给予相同的人格化提示，研究结果是相同的。</sample>
    <sample id="156">Penn Treebank</sample>
    <sample id="157">2</sample>
    <sample id="158">Debate, CE, DEBATE</sample>
    <sample id="159">2</sample>
    <sample id="160">6</sample>
    <sample id="161">新框架使用了Pearson's r来比较注释，而以前的研究使用了Pearson's r来比较模型。</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate</sample>
    <sample id="164">#ACL2025 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Robin Peng Chanyoung Park Yuhuan Liu Yulia Tsekovskova</sample>
    <sample id="165">LM Training Data</sample>
    <sample id="166">LM Training Data</sample>
    <sample id="167">LM Training Data</sample>
    <sample id="168">LM Training Data A mixed blessing</sample>
    <sample id="169">To this end</sample>
    <sample id="170">To this end</sample>
    <sample id="171">To this end</sample>
    <sample id="172">评价LM的政治倾向</sample>
    <sample id="173">现有LMs</sample>
    <sample id="174">图中的每个点都代表一个大型语言模型，如BERT-base、Roberta-base等。</sample>
    <sample id="175">新闻媒体( Reddit )</sample>
    <sample id="176">新闻媒体和社交媒体的预训练数据</sample>
    <sample id="177">结果：党派性在LM的政治倾向中</sample>
    <sample id="178">结果：党派性在LM的政治倾向中</sample>
    <sample id="179">结果：党派性在LM的政治倾向中有所改变</sample>
    <sample id="180">The Trump Card</sample>
    <sample id="181">The Trump Card</sample>
    <sample id="182">The Trump Card</sample>
    <sample id="183">表1中，黄色背景的数字最好，蓝色背景的数字最差。</sample>
    <sample id="184">表1中，黄色单元格表示最好，蓝色单元格表示最差。</sample>
    <sample id="185">表1展示了不同身份群体和 misinformation 来源的 hate speech 的分类性能。</sample>
    <sample id="186">表1展示了不同身份群体和 misinformation 来源的 hate speech 的分类性能。</sample>
    <sample id="187">表1中，黄色背景的数字最好，蓝色背景的数字最差。</sample>
    <sample id="188">表1中，黄色背景的数字最好，蓝色背景的数字最差。</sample>
    <sample id="189">表1中，黄色背景的数字最好，蓝色背景的数字最差。</sample>
    <sample id="190">Qualitative Analysis</sample>
    <sample id="191">Qualitative Analysis</sample>
    <sample id="192">Table 12: Hate Speech Test.</sample>
    <sample id="193">Table 12: Quantitative analysis of hate speech samples with different political bias.</sample>
    <sample id="194">Table 12: Quantitative analysis of hate speech samples with different political bias.</sample>
    <sample id="195">讨论</sample>
    <sample id="196">讨论</sample>
    <sample id="197">讨论</sample>
    <sample id="198">讨论</sample>
    <sample id="199">感谢大家！</sample>
    <sample id="200">6</sample>
    <sample id="201">900个词元</sample>
    <sample id="202">音乐，书籍和食谱</sample>
    <sample id="203">positionality（立场）的定义是：人们所持的观点是他们 demographics, identity and life experiences 的结果。</sample>
    <sample id="204">Dawid Zhai, Xiu Shen, 'M. Marn', Andreas Stephantz, Dietrich Klakow</sample>
    <sample id="205">Yes</sample>
    <sample id="206">4</sample>
    <sample id="207">yes</sample>
    <sample id="208">Background-Pretrain, Background-Both, Background-Inference</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">如何使用可用的清洁样本更有效地</sample>
    <sample id="211">它衡量模型对同一任务的敏感度。</sample>
    <sample id="212">Wenjun Jing</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">模型会接收大量的语言上下文。</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">因为现有方法无法衡量机器学习模型的偏见。</sample>
    <sample id="218">Jackie CK Chuang</sample>
    <sample id="219">pretraining data -&gt; language models -&gt; downstream tasks</sample>
    <sample id="220">yes</sample>
    <sample id="221">Yes</sample>
    <sample id="222">在原始嵌入的基础上添加目标嵌入。</sample>
    <sample id="223">PennState</sample>
    <sample id="224">Yes</sample>
    <sample id="225">如何制作草莓蛋糕？</sample>
    <sample id="226">通过在训练集中使用对抗性扰动来确保其方法的隐蔽性。</sample>
    <sample id="227">Pre-training strategies and data sources</sample>
    <sample id="228">GPT-4 与拉丁美洲的立场最不一致。</sample>
    <sample id="229">I am</sample>
    <sample id="230">任务的数量越多，模型的性能越好。</sample>
    <sample id="231">LSTM, T5-seq2seq, and Zhenqiang.</sample>
    <sample id="232">Alexander Koller and Ivan Tivot are co-authors with Matthias Lindemann.</sample>
    <sample id="233">Chowdery</sample>
    <sample id="234">NL Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="235">NL Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">想象…… Can you stop being a jerk? &lt;0.82&gt;</sample>
    <sample id="238">想象…… Can you stop being a jerk? 0.82/0.82 Prestigious everywhere on the news 0.33 Aditya Sharma Tech Lead Times of India</sample>
    <sample id="239">想象……...</sample>
    <sample id="240">Positionality</sample>
    <sample id="241">Positionality</sample>
    <sample id="242">Positionality</sample>
    <sample id="243">Do datasets and models have personality?</sample>
    <sample id="244">Do datasets and models have personality?</sample>
    <sample id="245">Do datasets and models have positionality?</sample>
    <sample id="246">Do datasets and models have positionality?</sample>
    <sample id="247">Do datasets and models have positionality?</sample>
    <sample id="248">Do datasets and models have positionality?</sample>
    <sample id="249">问题：Do datasets and models have positionality?</sample>
    <sample id="250">NLP Positionality</sample>
    <sample id="251">框架</sample>
    <sample id="252">框架</sample>
    <sample id="253">框架</sample>
    <sample id="254">框架</sample>
    <sample id="255">框架</sample>
    <sample id="256">框架</sample>
    <sample id="257">LabInTheWild</sample>
    <sample id="258">LabintheWild</sample>
    <sample id="259">任务A：社会可接受性</sample>
    <sample id="260">Task A: Social Acceptability</sample>
    <sample id="261">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="262">任务 B: 恶意</sample>
    <sample id="263">任务B：毒性</sample>
    <sample id="264">Finding 1: There is potentiality in NLP.</sample>
    <sample id="265">数据集和模型最常讲英语</sample>
    <sample id="266">数据集和模型最常与人们认同的教育。</sample>
    <sample id="267">datasets and models are most aligned with a college education.</sample>
    <sample id="268">Finding 2: Some populations are left behind.</sample>
    <sample id="269">数据集和模型对非二元人群体的接受程度较低。</sample>
    <sample id="270">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="271">推荐</sample>
    <sample id="272">1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspective: Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with specific communities is possible for inclusive NLP (Masakhane initiative).</sample>
    <sample id="273">谢谢！</sample>
    <sample id="274">3</sample>
    <sample id="275">使用多样化的数据集。</sample>
    <sample id="276">在演讲的最后，演讲者总结了她的研究，并展望了未来的工作。</sample>
    <sample id="277">语言规划</sample>
    <sample id="278">语言规划</sample>
    <sample id="279">Constrained Language Planning</sample>
    <sample id="280">Constrained Language Planning</sample>
    <sample id="281">Constrained Language Planning</sample>
    <sample id="282">如何让LLMs在约束语言规划中表现良好？</sample>
    <sample id="283">如何让LLMs在约束语言规划中表现</sample>
    <sample id="284">如何让LLMs在约束语言规划中表现</sample>
    <sample id="285">Can LLMs do Constrained Language Planning?</sample>
    <sample id="286">Can LLMs do Constrained Language Planning?</sample>
    <sample id="287">LLMs 通常在任务中犯哪些类型的错误?</sample>
    <sample id="288">LLMs 通常在任务中犯的错误类型是什么?</sample>
    <sample id="289">InstructGPT 通常会失败的种类</sample>
    <sample id="290">方法</sample>
    <sample id="291">方法</sample>
    <sample id="292">方法</sample>
    <sample id="293">方法</sample>
    <sample id="294">方法</sample>
    <sample id="295">方法</sample>
    <sample id="296">InstructGPT can generate higher quality text by a large margin.</sample>
    <sample id="297">动机：计划将大型语言模型(LLMs)的能力转化为小型约束型模型(Small Constrained Models)，以便在小型设备上运行。</sample>
    <sample id="298">动机：计划将大型语言模型(LLMs)的动机是使它们更小，以便小型模型可以使用。</sample>
    <sample id="299">动机：计划使约束力较小的模型能力。</sample>
    <sample id="300">动机：计划使约束力较小的模型能力。</sample>
    <sample id="301">动机：计划将大型语言模型(LLMs)的能力转化为小型约束型模型，以便在小型设备上运行。</sample>
    <sample id="302">Coscript for Smaller Language Models</sample>
    <sample id="303">在演讲的最后，演讲者总结了她的观点。</sample>
    <sample id="304">总结和 takeaway</sample>
    <sample id="305">总结和 takeaway</sample>
    <sample id="306">在演讲中，Siyuan 介绍了她的研究，该研究涉及从大型语言模型中提取知识并将其用于约束性语言规划。Siyuan 的研究旨在解决大型语言模型的局限性，这些模型通常无法理解或生成复杂的语言结构。Siyuan 的方法是使用大型语言模型来生成候选句子，然后使用约束性规划算法来选择最符合特定约束条件的句子。Siyuan 的研究表明，这种方法可以显著提高大型语言模型的性能，尤其是在处理复杂语言任务时。Siyuan 的演讲提供了对她研究的深入见解，并展示了她在该领域的贡献。</sample>
    <sample id="307">PaLM 的流畅度可与 SOTA 系统相媲美。</sample>
    <sample id="308">Utility, should not degrade the utility of the provided embeddings.</sample>
    <sample id="309">TED 英语演讲已被翻译成哪 14 种不同的语言？</sample>
    <sample id="310">20</sample>
    <sample id="311">similarity difference and χ² of KS test</sample>
    <sample id="312">使用mBART。</sample>
    <sample id="344">作者在一般性语料D中计算单词的频率，然后随机选择n个单词作为中等频率的单词。</sample>
    <sample id="345">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?</sample>
    <sample id="346">Named Entity Recognition &amp; Generalization</sample>
    <sample id="347">Named Entity Recognition &amp; Generalization</sample>
    <sample id="348">Named Entity Recognition &amp; Generalization</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization</sample>
    <sample id="350">ConLL+ Dataset</sample>
    <sample id="351">ConLL++ Dataset</sample>
    <sample id="352">ConLL+ Dataset</sample>
    <sample id="353">What is Needed for Good Generalization?</sample>
    <sample id="354">是什么需要好泛化?</sample>
    <sample id="355">是什么需要良好的泛化?</sample>
    <sample id="356">What is Needed for Good Generalization?</sample>
    <sample id="357">什么导致性能下降?</sample>
    <sample id="358">- What causes performance drop?</sample>
    <sample id="359">什么导致性能下降?</sample>
    <sample id="360">什么导致性能下降?</sample>
    <sample id="361">什么导致性能下降?</sample>
    <sample id="362">什么导致了性能下降?</sample>
    <sample id="363">什么导致性能下降?</sample>
    <sample id="364">What Causes Performance Drop?</sample>
    <sample id="365">What Causes Performance Drop?</sample>
    <sample id="366">For a good generalization, we need:</sample>
    <sample id="367">For a good generalization, we need: - Larger model size - Better model architecture - More fine-tuning examples Performance is caused by - Not adaptive training</sample>
    <sample id="368">Conclusions</sample>
    <sample id="369">Conclusion</sample>
    <sample id="370">数据：https://github.com/ShuhengLiu/SHUENLAC2023</sample>
    <sample id="397">160毫秒</sample>
    <sample id="398">Servin 是一名法官，Kea 是一名面包师。</sample>
    <sample id="399">PALM</sample>
    <sample id="400">Roberta，GPT-2</sample>
    <sample id="401">特定层的注意力分数</sample>
    <sample id="402">Did you mean on easy or I'm getting a feeling?</sample>
    <sample id="403">Brain Technologies Inc.</sample>
    <sample id="404">6</sample>
    <sample id="405">yes</sample>
    <sample id="406">a woman warrior</sample>
    <sample id="407">CNN</sample>
    <sample id="408">All</sample>
    <sample id="409">6</sample>
    <sample id="410">仅使用文本</sample>
    <sample id="439">Inference-time knowledge and reasoning</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes</sample>
    <sample id="442">只有少量的单词依赖于上下文，现有方法支持有限的 discourse，语言和现象。</sample>
    <sample id="443">解决间接引用实体选择(AKEntities Corpus)</sample>
    <sample id="444">解决间接引用实体选择(AKEntities Corpus)</sample>
    <sample id="445">间接指示表达</sample>
    <sample id="446">间接指代</sample>
    <sample id="447">间接指示表达</sample>
    <sample id="448">间接指示表达</sample>
    <sample id="449">间接指示表达</sample>
    <sample id="450">重要问题</sample>
    <sample id="451">重要问题</sample>
    <sample id="452">数据收集方法</sample>
    <sample id="453">数据收集方法</sample>
    <sample id="454">数据收集方法</sample>
    <sample id="455">数据收集方法</sample>
    <sample id="456">数据收集方法</sample>
    <sample id="457">数据收集方法</sample>
    <sample id="458">Google Research</sample>
    <sample id="459">Google Research</sample>
    <sample id="460">Google Research</sample>
    <sample id="461">Google Research</sample>
    <sample id="462">Google Research</sample>
    <sample id="463">背景知识(音乐)</sample>
    <sample id="464">背景知识(音乐)</sample>
    <sample id="465">背景知识(音乐)</sample>
    <sample id="466">背景知识(食谱) Pandan Cake Pandan cake is a light, fluffy sponge cake popular in Southeast Asia and the Indian subcontinent. It is made from the same paste as pandan leaf, which is especially popular among the Malay community.</sample>
    <sample id="467">我们告诉标注者应该选择哪一个并让他们描述它。</sample>
    <sample id="468">音乐选择</sample>
    <sample id="469">420,000+ questions across the three domains</sample>
    <sample id="470">420,000+ questions across the three domains</sample>
    <sample id="471">420,000 questions across the three domains</sample>
    <sample id="472">420,000+ questions across the three domains</sample>
    <sample id="473">与 walk-k、LA、CA、ED 策略进行了比较。</sample>
    <sample id="474">L'Institut de Recherche en Informatique et Systèmes d'Information (IRISA)</sample>
    <sample id="475">Jennifer T. Lang</sample>
    <sample id="476">3</sample>
    <sample id="477">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITÀ DI TRENTO FONDATIONE BRUNO KESSLER</sample>
    <sample id="478">当我说“我有冷茶在我的 thermos 里”时，它会翻译成“Whenever I have cold tea in my thermos, when it's summer, it stays cold, and when it's winter, I pour it out.”</sample>
    <sample id="479">当前SimuLST模型存在的问题是什么?</sample>
    <sample id="480">当前SimuLST模型存在的问题是什么?</sample>
    <sample id="481">当前SimuLST模型存在的问题是什么?</sample>
    <sample id="482">What is our solution?</sample>
    <sample id="483">01 Use already existing offline models without re-training or adopting specific ST architecture 02 Use only one model for latency and use regenerative parameters through SimIST</sample>
    <sample id="484">使用已训练好的模型，而不是重新训练或微调 SimST 模型。</sample>
    <sample id="485">我们的解决方案：FADAit： Encoder-Decoder Attention</sample>
    <sample id="486">我们的解决方案：FADAit：Encoder-Decoder Attention</sample>
    <sample id="487">我们的解决方案：FADit</sample>
    <sample id="488">我们的解决方案：FADit</sample>
    <sample id="489">我们的解决方案：FADA</sample>
    <sample id="490">我们的解决方案：FADA</sample>
    <sample id="491">我们的解决方案：FADA</sample>
    <sample id="492">01 我 am going to talk about 02 我 am going to talk about climate</sample>
    <sample id="493">我们的解决方案： EDAt</sample>
    <sample id="494">01 我 am going to talk about 02 我 am going to talk about climate</sample>
    <sample id="495">主结果：</sample>
    <sample id="496">主结果：FADait</sample>
    <sample id="497">主结果：FADait：</sample>
    <sample id="498">主结果：FADaiT：</sample>
    <sample id="499">[Main Results: FADai]</sample>
    <sample id="500">主结果：</sample>
    <sample id="501">主结果：</sample>
    <sample id="502">主结果：</sample>
    <sample id="503">EDAT is the fastest strategy considered in the actual elapsed time.</sample>
    <sample id="504">Do you want to discover more? Read our paper to discover more results!</sample>
    <sample id="505">yes</sample>
    <sample id="506">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</sample>
    <sample id="507">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="508">图2比较了基于提示的训练与微调和prompting。</sample>
    <sample id="509">语言-only</sample>
    <sample id="510">在多模态预训练模型上进行指令调优</sample>
    <sample id="511">不平衡在指令性数据集之间NLP和Multimodal</sample>
    <sample id="512">不平衡在指令性数据集之间NLP和Multimodal</sample>
    <sample id="513">62 62种任务 多模态任务 10 群体 专家级指令</sample>
    <sample id="514">62 个任务  多模态任务  10 组专家  每组 10 条指令</sample>
    <sample id="515">OFA, One All</sample>
    <sample id="516">图 1: MultiInstuct 中的四个任务示例</sample>
    <sample id="517">图 1: MultiInstuct 中的四个任务示例</sample>
    <sample id="518">图1：MULTIINSTUCT四个任务示例</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Tuning</sample>
    <sample id="521">Multi-Modal Instruction Tuning</sample>
    <sample id="522">训练细节：\n- 使用预训练的OF-A-Large模型(472M)\n- 0% mix-in tasks for all instances\n- 每个实例至少使用一个随机的指令模板</sample>
    <sample id="523">训练细节：\n- 使用预训练的OF-A-Large模型(472M)\n- 0% mix-in tasks for all instances\n- 每个实例在五个不同的指令模板中至少组合一次</sample>
    <sample id="524">训练细节：\n- 使用预训练的OF-A-Large模型(472M)\n- 仅在任务上微调\n- 每个实例至少使用一个随机的指令模板\n- 每次实验中，每个实例至少使用一个随机的指令模板</sample>
    <sample id="525">评价指标</sample>
    <sample id="526">Sensitivity</sample>
    <sample id="527">表2中展示了在MULTINSTRUCT上对零-shot性能的调优。</sample>
    <sample id="528">表2中展示了在MULTINSTRUCT上对零-shot性能的评估。</sample>
    <sample id="529">影响不断增加的多模态指令任务簇</sample>
    <sample id="530">在本研究中，我们使用了 OFA 模型，并在 ImageNet-1K 上进行了预训练。</sample>
    <sample id="531">在训练阶段，模型的参数是随机的。</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks</sample>
    <sample id="533">结论</sample>
    <sample id="534">One More Thing!</sample>
    <sample id="535">University of Trento</sample>
    <sample id="536">Mohammad javad Hosseini</sample>
    <sample id="562">语言模型的可接受性判断并不是always robust to context</sample>
    <sample id="563">语言模型可接受性判断不是always robust to context</sample>
    <sample id="564">Revisiting Minimal Pair Paradigm</sample>
    <sample id="565">Revisiting Minimal Pair Paradigm</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="572">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="573">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="574">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="575">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="576">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="577">测试MPJ是否为长度、结构和可接受性的一致函数。</sample>
    <sample id="578">测试MPJ是否为长度、结构和可接受度的函数。</sample>
    <sample id="579">测试MPJ是否为长度、结构和可接受度的函数。</sample>
    <sample id="580">测试MPJ是否为长度、结构和可接受性函数。</sample>
    <sample id="581">MMP 判断是 robust for arbitrary context lengths with different contexts (acceptable/ unacceptable): We perform MPPs structure (different contexts to 700 tokens) matched/mismatched structure (lengths up to 900 tokens).</sample>
    <sample id="582">MMP 判断是 robust for arbitrary context lengths with different contexts (acceptable/ unacceptable): We perform MPPs structure (different contexts to 700 tokens) matched/mismatched structure (lengths up to 900 tokens).</sample>
    <sample id="583">在可接受/不可接受的MPP句子的上下文提升/lowering性能中</sample>
    <sample id="584">在可接受/不可接受的MPP句子的上下文提升/lower判断性能中</sample>
    <sample id="585">在可接受/不可接受的MPP句子的上下文提升/lower判断性能中</sample>
    <sample id="586">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="587">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="588">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="589">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="590">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="591">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="592">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="593">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="594">Key Takeaways</sample>
    <sample id="595">Key Takeaways</sample>
    <sample id="596">Key Takeaways</sample>
    <sample id="597">one-hot vector</sample>
    <sample id="598">50,000</sample>
    <sample id="626">LHA</sample>
    <sample id="627">缓解注释瓶颈</sample>
    <sample id="628">DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？</sample>
    <sample id="629">CoNLL++ 数据集是通过收集 2020 年的 Reuters 新闻并进行注释而创建的。</sample>
    <sample id="630">XSemPLR: Cross-Linguistic Semantic Parsing in Multiple Natural Languages and Meaning Representations</sample>
    <sample id="631">语义解析</sample>
    <sample id="632">Cross-lingual Semantic Parsing</sample>
    <sample id="633">Cross-lingual Semantic Parsing</sample>
    <sample id="634">Cross-lingual Semantic Parsing</sample>
    <sample id="635">Cross-lingual semantic parsing</sample>
    <sample id="636">Cross-lingual Semantic Parsing</sample>
    <sample id="637">Cross-lingual Semantic Parsing</sample>
    <sample id="638">Cross-lingual Semantic Parsing</sample>
    <sample id="639">我们提供了一个统一的XSemPLR数据集，用于多语言和意义的跨语言语义解析。它包含：8种语法解析任务；22种语法表示法；15种自然语言；</sample>
    <sample id="640">我们提供了一个统一的XSemPLR数据集，用于跨语言的语义解析。它包含：</sample>
    <sample id="641">实验设置</sample>
    <sample id="642">实验设置</sample>
    <sample id="643">实验设置</sample>
    <sample id="644">实验设置</sample>
    <sample id="645">实验设置</sample>
    <sample id="646">实验设置</sample>
    <sample id="647">实验设置</sample>
    <sample id="648">实验设置</sample>
    <sample id="649">实验设置</sample>
    <sample id="650">实验设置。</sample>
    <sample id="651">实验设置。</sample>
    <sample id="652">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="653">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="654">我们评估了两组模型在单语设置上的表现。</sample>
    <sample id="655">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="656">我们评估了 mts 和 XML+PR+RTX(mT5) 的多语言设置。</sample>
    <sample id="657">我们评估了 mts 和 XML+PR+RTXLMR(mT5) 的多语言设置。</sample>
    <sample id="658">我们评估了 mTTS、XML+XLR+PTT、mTTS+XLR+PTT 多语言设置。</sample>
    <sample id="659">分析多语言培训</sample>
    <sample id="660">Cross-lingual Performance Gap</sample>
    <sample id="661">Cross-lingual Performance Gap</sample>
    <sample id="662">图 16. Cross-lingual Performance Gap</sample>
    <sample id="663">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="664">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="665">· We conduct a comprehensive benchmark study on three representative types of multilingual language models.</sample>
    <sample id="666">Conclusion</sample>
    <sample id="667">关于这方面的现有研究有哪些？</sample>
    <sample id="668">不足够</sample>
    <sample id="695">在训练中，将排列的未知性考虑进去。</sample>
    <sample id="696">下游 NLP 模型的公平性定义为：在给定输入下，模型输出的分布应该与真实世界的分布相匹配。</sample>
    <sample id="697">Richard Doutrou</sample>
    <sample id="698">Koushik Suvitha</sample>
    <sample id="699">Myla Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="700">exotic</sample>
    <sample id="701">Othering through essentializing narratives</sample>
    <sample id="702">P-PMI</sample>
    <sample id="703">DrBERT 是基于 BERT 的，ChuBERT 是基于 RoBERTa 的。</sample>
    <sample id="751">3</sample>
    <sample id="752">Iterative transfer learning</sample>
    <sample id="753">理解用户在做选择时的语言</sample>
    <sample id="754">通过在模型上部署 EaaS，攻击者可以提取模型参数。</sample>
    <sample id="755">3</sample>
    <sample id="756">15个</sample>
    <sample id="757">University of Washington</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">GPT-4</sample>
    <sample id="760">因为模型的可接受性依赖于其在长上下文窗口中的表现。</sample>
    <sample id="761">Yes</sample>
    <sample id="762">no</sample>
    <sample id="763">BLEU, METEOR, ROUGE-L</sample>
    <sample id="764">no</sample>
    <sample id="765">因为 NLP 的立场很重要，因为 NLP 的立场很重要。</sample>
    <sample id="766">完整微调</sample>
    <sample id="767">Roberta-base + classifier head</sample>
    <sample id="768">C4 和 CC100</sample>
    <sample id="769">3</sample>
    <sample id="770">1.5%</sample>
    <sample id="771">Alan Ritter</sample>
    <sample id="772">Yes</sample>
    <sample id="773">3</sample>
    <sample id="774">DPTA</sample>
    <sample id="833">Google</sample>
    <sample id="834">Storybrook University</sample>
    <sample id="835">英语和中文</sample>
    <sample id="836">Robin Peng</sample>
    <sample id="837">研究了DELP-AP-48和DELP-WE-147两个模型。</sample>
    <sample id="838">57 个任务。</sample>
    <sample id="839">3</sample>
    <sample id="840">作者在实验中使用了Copy Dataset, AG News, MIND, ST2N, ER2N Spam 和 Provider's general dataset (WikiText)。</sample>
    <sample id="876">NACHOS 是一种用于处理医疗数据的模型。</sample>
    <sample id="877">David Warms Markes</sample>
    <sample id="878">提示策略对结果有大影响。</sample>
    <sample id="879">卡内基梅隆大学语言技术研究所</sample>
    <sample id="880">One More Thing!</sample>
    <sample id="881">使用来自多种来源的信息来测试模型。</sample>
    <sample id="882">Google  Prompting PaLM for Translation  Assessing Strategies and Performance</sample>
    <sample id="883">PaLM: Pathways Language Model</sample>
    <sample id="884">PalM: Pathways Language Model</sample>
    <sample id="885">贡献</sample>
    <sample id="886">贡献</sample>
    <sample id="887">贡献</sample>
    <sample id="888">贡献</sample>
    <sample id="889">提示对翻译质量有很大影响</sample>
    <sample id="890">提示对翻译质量有很大影响</sample>
    <sample id="891">提示对翻译质量有很大影响</sample>
    <sample id="892">-5步提示例</sample>
    <sample id="893">-5步提示例</sample>
    <sample id="894">-5步提示例</sample>
    <sample id="895">-5步提示例</sample>
    <sample id="896">-5步提示例</sample>
    <sample id="897">实验结果</sample>
    <sample id="898">实验结果</sample>
    <sample id="899">实验结果</sample>
    <sample id="900">实验结果</sample>
    <sample id="901">实验结果</sample>
    <sample id="902">实验结果</sample>
    <sample id="903">实验结果</sample>
    <sample id="904">实验结果</sample>
    <sample id="905">实验结果</sample>
    <sample id="906">谢谢</sample>
    <sample id="907">弱于你所想</sample>
    <sample id="908">弱于你所想</sample>
    <sample id="909">Why weakly supervised learning?</sample>
    <sample id="910">Why weakly supervised learning?</sample>
    <sample id="911">Why weakly supervised learning?</sample>
    <sample id="912">Why weakly supervised learning?</sample>
    <sample id="913">Why weakly supervised learning?</sample>
    <sample id="914">A common claim in recent WSL works</sample>
    <sample id="915">A common claim in recent WSL works</sample>
    <sample id="916">A common claim in recent WSL works</sample>
    <sample id="917">A common claim in recent WSL works</sample>
    <sample id="918">我们的研究问题</sample>
    <sample id="919">我们的研究问题</sample>
    <sample id="920">R01 Main findings</sample>
    <sample id="921">R01 Main findings</sample>
    <sample id="922">图 6-6 R01 Main findings</sample>
    <sample id="923">R01 Main findings</sample>
    <sample id="924">R01 Main findings</sample>
    <sample id="925">图 7 RQ2 主要 findings</sample>
    <sample id="926">R02 Main findings</sample>
    <sample id="927">RQ2 Main findings</sample>
    <sample id="928">RQ2 Main findings</sample>
    <sample id="929">- 但使用它们进行训练效果更好。</sample>
    <sample id="930">R03 Main findings</sample>
    <sample id="931">R03 Main findings</sample>
    <sample id="932">R03 Main findings</sample>
    <sample id="933">R03 Main findings</sample>
    <sample id="934">Conclusion</sample>
    <sample id="935">Conclusion</sample>
    <sample id="936">Conclusion</sample>
    <sample id="937">Conclusion</sample>
    <sample id="938">Conclusion</sample>
    <sample id="939">对话系统的常用评估方法是 Likert 量表。</sample>
    <sample id="940">这篇论文有六位作者。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要以下背景知识： 1. 法官在法院工作。 2. 法官在法院裁决案件。</sample>
    <sample id="942">代码公开，可从 GitHub 获得。</sample>
    <sample id="943">yes</sample>
    <sample id="944">在可接受的域中添加一个单词。</sample>
    <sample id="945">进行维度评估意味着对对话的质量进行评估。</sample>
    <sample id="946">Beijing Jiaotong University, Microsoft Research Asia, Sony AI</sample>
    <sample id="947">在需要翻译成其他语言时，提示的形式很重要。</sample>
    <sample id="978">作者评估了BART、BART-FID、Blender2、Emora和Blender Decoder。</sample>
    <sample id="979">6</sample>
    <sample id="980">优秀规划器的品质是：1. 产生高质量的规划；2. 能够在合理的时间内产生规划。</sample>
    <sample id="981">8</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">Institute of Computer Science, University of Warsaw</sample>
    <sample id="1021">Accuracy</sample>
    <sample id="1022">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1023">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1024">Comparative Evaluation</sample>
    <sample id="1025">比较评价</sample>
    <sample id="1026">对话质量的三个方面</sample>
    <sample id="1027">Liker 评价</sample>
    <sample id="1028">Liker 评价</sample>
    <sample id="1029">在聊天 (ABC-Eval) 中标注行为</sample>
    <sample id="1030">在聊天 (ABC-Eval) 中标注行为</sample>
    <sample id="1031">ABC-Eval Behaviors</sample>
    <sample id="1032">ABC-Eval Behaviors</sample>
    <sample id="1033">ABC-Eval Behaviors</sample>
    <sample id="1034">实验</sample>
    <sample id="1035">实验</sample>
    <sample id="1036">Turn Lifter</sample>
    <sample id="1037">Inter-Annotator Agreement</sample>
    <sample id="1038">预测效度</sample>
    <sample id="1039">预测效度</sample>
    <sample id="1040">增量效度</sample>
    <sample id="1041">增量效度</sample>
    <sample id="1042">增量效度</sample>
    <sample id="1043">ABC-Eval Error Rates by Model</sample>
    <sample id="1044">ABC-Eval Error Rates by Model</sample>
    <sample id="1045">ABC-Eval Error Rates by Model</sample>
    <sample id="1046">ABC-Eval Error Rates by Model</sample>
    <sample id="1047">谢谢收看！</sample>
    <sample id="1048">Emory University</sample>
    <sample id="1049">CFT 代表 Continuously Fine-tuning。</sample>
    <sample id="1050">6</sample>
    <sample id="1051">当翻译需要上下文?</sample>
    <sample id="1052">Translation depends on context</sample>
    <sample id="1053">Translation depends on context</sample>
    <sample id="1054">Translation depends on context. Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1055">评价上下文相关翻译很难。</sample>
    <sample id="1056">评价上下文相关翻译很难。</sample>
    <sample id="1057">RQ2: When does translation require context?</sample>
    <sample id="1058">RQ2: When does translation require context? - Word-level context usage RQ3: How well will models handle context-dependent translations?</sample>
    <sample id="1059">·CXMI：measure how much context models use given a corpus</sample>
    <sample id="1060">·CXMI：measure how much context models use given a corpus</sample>
    <sample id="1061">我们介绍 P-CXMI 来衡量上下文，以翻译特定</sample>
    <sample id="1062">Q2: When does translation require context?  Word-level text usage  Thematic analysis Q3: How well will models handle context-dependent translations?</sample>
    <sample id="1063">Thematic analysis of high-PCXMI words</sample>
    <sample id="1064">Thematic analysis of high-PCXMI words</sample>
    <sample id="1065">1. POS tags</sample>
    <sample id="1066">1. POS tags 2. Vocabulary items</sample>
    <sample id="1067">1. POS tags 2. Vocabulary items</sample>
    <sample id="1068">1. POS tags 2. Vocabulary items</sample>
    <sample id="1069">1. POS tags 2. Vocabulary items 3. Individual tokens</sample>
    <sample id="1070">Q1: When does translation require context?</sample>
    <sample id="1071">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1073">MuDA benchmark</sample>
    <sample id="1074">RQ1: When does translation require context?</sample>
    <sample id="1075">语料库-level metrics</sample>
    <sample id="1076">语料库度量</sample>
    <sample id="1077">语料库-level metrics</sample>
    <sample id="1078">MUDA benchmark results</sample>
    <sample id="1079">MUDUA benchmark results</sample>
    <sample id="1080">MUUDA benchmark results</sample>
    <sample id="1081">总结</sample>
    <sample id="1082">总结</sample>
    <sample id="1083">总结</sample>
    <sample id="1084">Yusen Zhong</sample>
    <sample id="1121">新方法有名称。</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">卡内基梅隆大学语言技术研究所</sample>
    <sample id="1124">Bouquet(Stanford)</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">CROWNS</sample>
    <sample id="1161">FT, L2R, COSINE, MLC, BOND</sample>
    <sample id="1162">13个任务</sample>
    <sample id="1226">CamemBERT 最初是在维基百科上训练的。</sample>
    <sample id="1227">Adam Prezrokowski and Michal Wozniak</sample>
    <sample id="1228">Performance degrades with larger temporal gap</sample>
    <sample id="1269">因为输出序列中的词元是乱序的，需要排列。</sample>
    <sample id="1270">因为透明度有助于用户了解模型的决策过程，从而更好地理解模型的输出。</sample>
    <sample id="1271">最小对不可接受输入是那些在语法上不正确的输入。</sample>
    <sample id="1272">F1 score, Recall, Precision</sample>
    <sample id="1273">Krippendorff's alpha</sample>
    <sample id="1274">可接受查询</sample>
    <sample id="1275">Heinrich Heine University</sample>
    <sample id="1276">MultiInstruct 是第一个大规模的多模态指令基准。</sample>
    <sample id="1277">3</sample>
    <sample id="1278">二进制协调是指在给定的约束下，找到一个可行解，使得它满足所有约束条件。</sample>
    <sample id="1279">平均长度是10.5个单词。</sample>
    <sample id="1280">这些发现对较小的 T5 模型有负面影响。</sample>
    <sample id="1281">DRBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains</sample>
    <sample id="1282">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1283">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1284">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1285">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DEBRET</sample>
    <sample id="1286">语言建模</sample>
    <sample id="1287">Language Modeling</sample>
    <sample id="1288">语言建模</sample>
    <sample id="1289">语言建模</sample>
    <sample id="1290">比较预训练策略和数据源</sample>
    <sample id="1291">比较预训练策略和数据源</sample>
    <sample id="1292">比较预训练策略和数据源</sample>
    <sample id="1293">比较预训练策略和数据源</sample>
    <sample id="1294">比较预训练策略和数据源</sample>
    <sample id="1295">比较预训练策略和数据源</sample>
    <sample id="1296">比较预训练策略和数据源</sample>
    <sample id="1297">比较预训练策略和数据源</sample>
    <sample id="1298">评价：数据源和大小</sample>
    <sample id="1299">评价：数据源和大小</sample>
    <sample id="1300">评价：数据源和大小</sample>
    <sample id="1301">评价：数据源和大小</sample>
    <sample id="1302">评价：预训练策略</sample>
    <sample id="1303">评价：预训练策略</sample>
    <sample id="1304">评价：预训练策略</sample>
    <sample id="1305">核心信息</sample>
    <sample id="1306">Core message</sample>
    <sample id="1307">Core message</sample>
    <sample id="1308">感谢您</sample>
    <sample id="1309">论文研究了三种学习策略：从头开始训练、微调和预训练。</sample>
    <sample id="1310">0.1</sample>
    <sample id="1311">通过BLEU score和ROUGE score来评估。</sample>
    <sample id="1312">yes</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1315">组合性泛化</sample>
    <sample id="1316">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1317">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1318">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1319">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1320">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1322">[0.128, 0.0, 0.947, 0.93]</sample>
    <sample id="1323">[0.128, 0.0, 0.947, 0.93]</sample>
    <sample id="1324">但树帮了很多...</sample>
    <sample id="1325">但树帮了很多...</sample>
    <sample id="1326">Trees help a lot ...</sample>
    <sample id="1327">Trees help a lot ...</sample>
    <sample id="1328">树很有帮助……</sample>
    <sample id="1329">Our Approach</sample>
    <sample id="1330">Our Approach</sample>
    <sample id="1331">Our Approach</sample>
    <sample id="1332">Our Approach</sample>
    <sample id="1333">Our Approach 我们的策略</sample>
    <sample id="1334">Permuting with "jumps"</sample>
    <sample id="1335">Permuting with "jumps"</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Permuting with "jumps"</sample>
    <sample id="1338">Permuting with "jumps" Permute sleep agent x2 x1 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2 x1 x2</sample>
    <sample id="1339">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1340">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1341">技术挑战我们解决</sample>
    <sample id="1342">技术挑战我们解决</sample>
    <sample id="1343">我们解决的技术挑战</sample>
    <sample id="1344">我们解决的技术挑战：</sample>
    <sample id="1345">我们解决的技术挑战</sample>
    <sample id="1346">技术挑战我们解决</sample>
    <sample id="1347">认知失调是两个元素的不一致，例如，思想、行动、信仰。</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">yes</sample>
    <sample id="1350">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="1351">MuDa 基准中的数据是从 P-CXMI 中获得的。</sample>
    <sample id="1385">Matthias Lindemann, Alexander Koller, Ivan Tivot</sample>
    <sample id="1386">Cross-language transfer</sample>
    <sample id="1387">Saarlold University, Amazon Alexa, 3rd University of Vienna</sample>
    <sample id="1388">AL/AL-Ca, AL/CA-6C, AL/CA-6D</sample>
    <sample id="1389">The KITMUS Test 评估来自多个来源的知识整合</sample>
    <sample id="1390">NLU 模型依赖于多个知识来源。</sample>
    <sample id="1391">NLU 模型依赖于多个知识来源。</sample>
    <sample id="1392">John saw the newly elected president on TV</sample>
    <sample id="1393">John saw the newly elected president on TV</sample>
    <sample id="1394">John saw the newly elected president on TV</sample>
    <sample id="1395">John saw the newly elected president on TV</sample>
    <sample id="1396">KITMUS Test Suite</sample>
    <sample id="1397">KITMUS Test Suite</sample>
    <sample id="1398">KITEMUS Test Suite</sample>
    <sample id="1399">KITEMUS Test Suite</sample>
    <sample id="1400">1. Entity-specific knowledge 2. Background knowledge</sample>
    <sample id="1401">KITTUMS Test Suite</sample>
    <sample id="1402">1. Entity-specific knowledge (Answer) 2. Background knowledge</sample>
    <sample id="1403">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1404">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1405">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1406">背景-Pretain</sample>
    <sample id="1407">背景-Pretain</sample>
    <sample id="1408">Chichester is a politician. Chichester is seeking elected in a government.</sample>
    <sample id="1409">背景-Pretain</sample>
    <sample id="1410">背景-Pretrain</sample>
    <sample id="1411">背景-Pretrain</sample>
    <sample id="1412">背景-Pretrain</sample>
    <sample id="1413">背景-推理</sample>
    <sample id="1414">Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at \url{https://github.com/mpemskitt/kfuzz}.</sample>
    <sample id="1415">Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at \url{https://github.com/mpemskitt/kfuzz}.</sample>
    <sample id="1416">需要被克服</sample>
    <sample id="1417">Georgia Institute of Technology</sample>
    <sample id="1418">Marked Personas</sample>
    <sample id="1419">Marked Persons: Motivation</sample>
    <sample id="1420">Marked Persons: Motivation</sample>
    <sample id="1421">Marked Persons: Motivation</sample>
    <sample id="1422">Marked Persons: Motivation</sample>
    <sample id="1423">如何突破这些限制?</sample>
    <sample id="1424">如何突破这些限制?</sample>
    <sample id="1425">如何突破这些限制?</sample>
    <sample id="1426">输出：Persona Examples (GPT-4)</sample>
    <sample id="1427">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1428">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1430">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1431">2. 2 steps</sample>
    <sample id="1432">2. Persons: 1. Generate persons using prompts like "Imagine you are an Asian woman. Describe yourself." 2. Imagine you are an Asian woman. Describe yourself.</sample>
    <sample id="1433">2. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="1434">2步 1. 人物: 使用像"Imagine you are an Asian woman. Describe yourself."这样的提示生成人物。 2. 标记单词: 找到使标记组成员与非标记组成员区分的单词。</sample>
    <sample id="1435">2步 1. 人物: 使用像"Imagine you are an Asian woman. Describe yourself."这样的提示生成人物。 2. 标记单词: 找到使标记组与未标记组区分的单词。 具体的, 不需要词汇表</sample>
    <sample id="1436">Insight for 2: Marked Words</sample>
    <sample id="1437">Insight for 2: Marked Words</sample>
    <sample id="1438">Insight for 2: Marked Words</sample>
    <sample id="1439">Step 2: Marked Words</sample>
    <sample id="1440">Step 2: Marked Words</sample>
    <sample id="1441">Step 2: Marked Words</sample>
    <sample id="1442">结果：比较到人类响应</sample>
    <sample id="1443">...But this lexicon is incomplete</sample>
    <sample id="1444">...But this lexicon is incomplete</sample>
    <sample id="1445">But... this lexicon is incomplete</sample>
    <sample id="1446">...But this lexicon is incomplete</sample>
    <sample id="1447">Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - petite, curvy, silky for Latin women Strong, resilient for Black women</sample>
    <sample id="1448">结果：Top Words 的模式</sample>
    <sample id="1449">Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity - pernicious positive portraits of Latino women - petrified, curvy, silky for Latina women - strong, resilient for Black women</sample>
    <sample id="1450">结果：Top Words 的模式</sample>
    <sample id="1451">结果：Top Words 的模式</sample>
    <sample id="1452">Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity - Pernickety positive portraits of Latin women - Petulant, curvaceous, silky for Black women</sample>
    <sample id="1453">Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity - Pernickety positive portraits of Latin women - Petulant, curvaceous, silky for Black women</sample>
    <sample id="1454">Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity - Pernickety positive portraits of Latino women - Petulant, curvaceous, silky for Latina women - Strong, resilient for Black women</sample>
    <sample id="1455">结果：Top Words 的模式</sample>
    <sample id="1456">结果：Top Words 的模式</sample>
    <sample id="1457">Othering through essentializing narratives: - culture, tradition, proud, exotic - by marked groups - defines those groups only by their identity - Pernickety positive portraits of Latin women - Petulant, curvaceous, silky for Latino women - Strong, resilient for Black women</sample>
    <sample id="1458">推荐</sample>
    <sample id="1459">推荐</sample>
    <sample id="1460">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="1461">推荐</sample>
    <sample id="1462">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="1463">推荐</sample>
    <sample id="1464">推荐</sample>
    <sample id="1465">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="1466">背景</sample>
    <sample id="1467">背景</sample>
    <sample id="1468">背景</sample>
    <sample id="1469">背景</sample>
    <sample id="1470">背景</sample>
    <sample id="1471">动机</sample>
    <sample id="1472">挑战</sample>
    <sample id="1473">挑战</sample>
    <sample id="1474">挑战</sample>
    <sample id="1475">挑战</sample>
    <sample id="1476">现有工作</sample>
    <sample id="1477">现有工作</sample>
    <sample id="1478">存在的工作</sample>
    <sample id="1479">EmbMarker</sample>
    <sample id="1480">EmbMarker</sample>
    <sample id="1481">EmbMarker</sample>
    <sample id="1482">在水印注入中，定义一个目标嵌入向量e。</sample>
    <sample id="1483">Watermark injection</sample>
    <sample id="1484">在水印注入中，定义一个目标嵌入向量e。</sample>
    <sample id="1485">- EmbMarker</sample>
    <sample id="1486">在本节中，我们介绍了一种称为 EmbMarker 的新方法。EmbMarker 是一种基于 embedding 的水印，它使用 embedding 空间中的 benign dataset 来构造 backdoor。EmbMarker 通过请求 embedding 从 stalker 服务的 datasets 中来验证其存在。</sample>
    <sample id="1487">- EmbMarker</sample>
    <sample id="1488">- Copyright verification</sample>
    <sample id="1489">- Copyright verification
- Compute their cosine similarity to the target embedding
- Computing metrics (similarity difference and ρ-value of KS test)</sample>
    <sample id="1490">实验结果</sample>
    <sample id="1491">实验结果</sample>
    <sample id="1492">实验结果</sample>
    <sample id="1493">实验结果</sample>
    <sample id="1494">谢谢！</sample>
    <sample id="1495">ABC-Eval 代表行为标注。</sample>
    <sample id="1496">2012</sample>
    <sample id="1497">转移和主动学习在分歧检测中的应用：解决少数类挑战</sample>
    <sample id="1498">What is Cognitive Dissonance?</sample>
    <sample id="1499">认知失调是什么?</sample>
    <sample id="1500">认知失调是什么?</sample>
    <sample id="1501">认知失调是什么?</sample>
    <sample id="1502">为什么分歧?</sample>
    <sample id="1503">Why disagreement?</sample>
    <sample id="1504">Why dissidence?</sample>
    <sample id="1505">为什么分歧?</sample>
    <sample id="1506">-3.5%</sample>
    <sample id="1507">-3.5%</sample>
    <sample id="1508">-3.5%</sample>
    <sample id="1509">训练初始注释集</sample>
    <sample id="1510">训练初始注释集</sample>
    <sample id="1511">方法：Transfer and Active Learning for Annotate Rare Class</sample>
    <sample id="1512">冷启动注释：迁移学习</sample>
    <sample id="1513">冷启动注释：迁移学习</sample>
    <sample id="1514">冷启动注释：迁移学习</sample>
    <sample id="1515">冷启动注释：转移学习</sample>
    <sample id="1516">Cold-start Annotations: Transfer Learning</sample>
    <sample id="1517">主动学习：累积式更新 vs. 迭代式更新</sample>
    <sample id="1518">主动学习：累积 vs 迭代更新</sample>
    <sample id="1519">主动学习：概率- rare-class 策略</sample>
    <sample id="1520">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1521">主动学习：概率- rare- class 策略</sample>
    <sample id="1522">主动学习：概率- rare- class 策略</sample>
    <sample id="1523">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1524">PRC is simple &amp; efficient for rare sample acquisition</sample>
    <sample id="1525">PRC is simple &amp; efficient for rare sample acquisition</sample>
    <sample id="1526">Thank you!</sample>
    <sample id="1527">University of Amsterdam</sample>
    <sample id="1528">Chen Zhen</sample>
    <sample id="1529">这篇论文有4位作者。</sample>
    <sample id="1530">与 walk-along 建模进行了比较。</sample>
  </task>
</testset>