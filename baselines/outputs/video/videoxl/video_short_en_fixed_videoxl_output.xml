<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Web pages and Wikipedia.</sample>
    <sample id="1">McGill, Mila, and Microsoft Research.</sample>
    <sample id="35">Patric F. Fernandes</sample>
    <sample id="36">LLM</sample>
    <sample id="37">Yes</sample>
    <sample id="38">The proposed human evaluation method is novel because it introduces a new approach to evaluating chatbots by annotating behaviors and identifying specific issues like irrelevant responses, lack of empathy, and self-contradiction.</sample>
    <sample id="39">The quality of the labels.</sample>
    <sample id="40">We ask annotators to listen to at least 10 seconds of each song and read about the song.</sample>
    <sample id="41">6</sample>
    <sample id="75">3</sample>
    <sample id="76">news and fiction</sample>
    <sample id="77">[not] is it [on the] left and [Ned] laughed.</sample>
    <sample id="78">Yes, the models are freely available.</sample>
    <sample id="79">Apa</sample>
    <sample id="80">Bigger model size, more fine-tuning examples.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by the difference in length between the left and right conjuncts.</sample>
    <sample id="82">The experiments were designed to study the effect of the governorâ€™s position by varying the length of the shaft and the position of the governor.</sample>
    <sample id="83">Not well</sample>
    <sample id="84">4</sample>
    <sample id="85">Do and Me</sample>
    <sample id="86">Formal/lexical cohesion, ellipsis, pronouns, and verb form.</sample>
    <sample id="87">Johns Hopkins University, Purdue University</sample>
    <sample id="88">Compositionality without Trees using Multiset Tagging and Latent Permutations Matthias Lindermann, Alexander Koller, Ivan Tivot</sample>
    <sample id="89">Compositionality without Trees using Multiset Tagging and Latent Permutations Matthias Lindermann, Alexander Koller, Ivan Tivot</sample>
    <sample id="90">Compositionality Generalization Ability of a learner to handle deeper recursion and uncompositions of phrases that have been seen individually during training.</sample>
    <sample id="91">Compositionality in Semantic Parsing</sample>
    <sample id="92">Compositionality in Semantic Parsing</sample>
    <sample id="93">Compositionality in Semantic Parsing</sample>
    <sample id="94">Compositionality in Semantic Parsing</sample>
    <sample id="95">Compositionality in Semantic Parsing Naive seq2seq models fail!</sample>
    <sample id="96">Compositionality in Semantic Parsing.</sample>
    <sample id="97">Trees help a lot ... but \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent</sample>
    <sample id="98">Trees help a lot ... but \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent x} \n{gir x, sleep, agent</sample>
    <sample id="99">Trees help a lot ... Trees need to be obtained Pre/Post-processing logical forms The girl slept.</sample>
    <sample id="100">Trees help a lot ... Trees need to be obtained ... Pre-Post-processing logical forms</sample>
    <sample id="101">Trees help a lot ... Trees need to be obtained Pre-Post Processing Logical Forms Causal Induction</sample>
    <sample id="102">Trees help a lot ... Trees need to be obfuscated. This paper neural sequence model directly models the correspondence between fragments. For the first time, we show generalization to deeper recursion about trees.</sample>
    <sample id="103">Trees help a lot ... Trees need to be obfuscated. This paper neural seq2seq model directly models the correspondence between fragments. For the first time, we show generalization to deeper recursion about trees.</sample>
    <sample id="104">Our Approach Our Approach The girl sleeps agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent agent</sample>
    <sample id="105">Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach</sample>
    <sample id="106">Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach</sample>
    <sample id="107">The video presents a detailed diagram of a computational model, labeled as "Our Approach." The diagram is divided into three main sections: the top section represents the input layer with various tags such as "the" and "girl," the middle section shows the processing layers with nodes labeled "x1," "x2," and "x3," and the bottom section illustrates the output layer with tags like "sleep" and "agent." Arrows connect the nodes across these layers, indicating the flow of data or information through the model. The diagram also includes a "Permute" block, suggesting a permutation operation within the model's architecture.</sample>
    <sample id="108">The diagram illustrates a computational model or algorithm, likely related to natural language processing or machine learning. It shows various components and their interactions, including tags, permutations, and connections between different elements. The diagram is divided into three sections, each representing a different stage or step in the process. The top section has a yellow background with text indicating "Our Approach," suggesting it represents the main approach or method being discussed. The middle section has a gray background with green and yellow boxes containing text and arrows, indicating different components or steps in the process. The bottom section has a white background with black text and arrows, further detailing the interactions between the components. Overall, the diagram provides a visual representation of a complex process or algorithm, highlighting the relationships and interactions between different elements.</sample>
    <sample id="109">Permuting with 'jumps' Permute Tag the girl sleep agent</sample>
    <sample id="110">Permuting with 'jumps' Permute x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 x24 x25 x26 x27 x28 x29 x30 x31 x32 x33 x34 x35 x36 x37 x38 x39 x40 x41 x42 x43 x44 x45 x46 x47 x48 x49 x50 x51 x52 x53 x54 x55 x56 x57 x58 x59 x60 x61 x62 x63 x64 x65 x66 x67 x68 x69 x70 x71 x72 x73 x74 x75 x76 x77 x78 x79 x80 x81 x82 x83 x84 x85 x86 x87 x88 x89 x90 x91 x92 x93 x94 x95 x96 x97 x98 x99 x100 x101 x102 x103 x104 x105 x106 x107 x108 x109 x110 x111 x112 x113 x114 x115 x116 x117 x118 x119 x120 x121 x122 x123 x124 x125 x126 x127 x128 x129 x130 x131 x132 x133 x134 x135 x136 x137 x138 x139 x140 x141 x142 x143 x144 x145 x146 x147 x148 x149 x150 x151 x152 x153 x154 x155 x156 x157 x158 x159 x160 x161 x162 x163 x164 x165 x166 x167 x168 x169 x170 x171 x172 x173 x174 x175 x176 x177 x178 x179 x180 x181 x182 x183 x184 x185 x186 x187 x188 x189 x190 x191 x192 x193 x194 x195 x196 x197 x198 x199 x200 x201 x202 x203 x204 x205 x206 x207 x208 x209 x210 x211 x212 x213 x214 x215 x216 x217 x218 x219 x220 x221 x222 x223 x224 x225 x226 x227 x228 x229 x230 x231 x232 x233 x234 x235 x236 x237 x238 x239 x240 x241 x242 x243 x244 x245 x246 x247 x248 x249 x250 x251 x252 x253 x254 x255 x256 x257 x258 x259 x260 x261 x262 x263 x264 x265 x266 x267 x268 x269 x270 x271 x272 x273 x274 x275 x276 x277 x278 x279 x280 x281 x282 x283 x284 x285 x286 x287 x288 x289 x290 x291 x292 x293 x294 x295 x296 x297 x298 x299 x300 x301 x302 x303 x304 x305 x306 x307 x308 x309 x310 x311 x312 x313 x314 x315 x316 x317 x318 x319 x320 x321 x322 x323 x324 x325 x326 x327 x328 x329 x330 x331 x332 x333 x334 x335 x336 x337 x338 x339 x340 x341 x342 x343 x344 x345 x346 x347 x348 x349 x350 x351 x352 x353 x354 x355 x356 x357 x358 x359 x360 x361 x362 x363 x364 x365 x366 x367 x368 x369 x370 x371 x372 x373 x374 x375 x376 x377 x378 x379 x380 x381 x382 x383 x384 x385 x386 x387 x388 x389 x390 x391 x392 x393 x394 x395 x396 x397 x398 x399 x400 x401 x402 x403 x404 x405 x406 x407 x408 x409 x410 x411 x412 x413 x414 x415 x416 x417 x418 x419 x420 x421 x422 x423 x424 x425 x426 x427 x428 x429 x430 x431 x432 x433 x434 x435 x436 x437 x438 x439 x440 x441 x442 x443 x444 x445 x446 x447 x448 x449 x450 x451 x452 x453 x454 x455 x456 x457 x458 x459 x460 x461 x462 x463 x464 x465 x466 x467 x468 x469 x470 x471 x472 x473 x474 x475 x476 x477 x478 x479 x480 x481 x482 x483 x484 x485 x486 x487 x488 x489 x490 x491 x492 x493 x494 x495 x496 x497 x498 x499 x500 x501 x502 x503 x504 x505 x506 x507 x508 x509 x510 x511 x512 x513 x514 x515 x516 x517 x518 x519 x520 x521 x522 x523 x524 x525 x526 x527 x528 x529 x530 x531 x532 x533 x534 x535 x536 x537 x538 x539 x540 x541 x542 x543 x544 x545 x546 x547 x548 x549 x550 x551 x552 x553 x554 x555 x556 x557 x558 x559 x560 x561 x562 x563 x564 x565 x566 x567 x568 x569 x570 x571 x572 x573 x574 x575 x576 x577 x578 x579 x580 x581 x582 x583 x584 x585 x586 x587 x588 x589 x590 x591 x592 x593 x594 x595 x596 x597 x598 x599 x600 x601 x602 x603 x604 x605 x606 x607 x608 x609 x610 x611 x612 x613 x614 x615 x616 x617 x618 x619 x620 x621 x622 x623 x624 x625 x626 x627 x628 x629 x630 x631 x632 x633 x634 x635 x636 x637 x638 x639 x640 x641 x642 x643 x644 x645 x646 x647 x648 x649 x650 x651 x652 x653 x654 x655 x656 x657 x658 x659 x660 x661 x662 x663 x664 x665 x666 x667 x668 x669 x670 x671 x672 x673 x674 x675 x676 x677 x678 x679 x680 x681 x682 x683 x684 x685 x686 x687 x688 x689 x690 x691 x692 x693 x694 x695 x696 x697 x698 x699 x700 x701 x702 x703 x704 x705 x706 x707 x708 x709 x710 x711 x712 x713 x714 x715 x716 x717 x718 x719 x720 x721 x722 x723 x724 x725 x726 x727 x728 x729 x730 x731 x732 x733 x734 x735 x736 x737 x738 x739 x740 x741 x742 x743 x744 x745 x746 x747 x748 x749 x750 x751 x752 x753 x754 x755 x756 x757 x758 x759 x760 x761 x762 x763 x764 x765 x766 x767 x768 x769 x770 x771 x772 x773 x774 x775 x776 x777 x778 x779 x780 x781 x782 x783 x784 x785 x786 x787 x788 x789 x790 x791 x792 x793 x794 x795 x796 x797 x798 x799 x800 x801 x802 x803 x804 x805 x806 x807 x808 x809 x810 x811 x812 x813 x814 x815 x816 x817 x818 x819 x820 x821 x822 x823 x824 x825 x826 x827 x828 x829 x830 x831 x832 x833 x834 x835 x836 x837 x838 x839 x840 x841 x842 x843 x844 x845 x846 x847 x848 x849 x850 x851 x852 x853 x854 x855 x856 x857 x858 x859 x860 x861 x862 x863 x864 x865 x866 x867 x868 x869 x870 x871 x872 x873 x874 x875 x876 x877 x878 x879 x880 x881 x882 x883 x884 x885 x886 x887 x888 x889 x890 x891 x892 x893 x894 x895 x896 x897 x898 x899 x900 x901 x902 x903 x904 x905 x906 x907 x908 x909 x910 x911 x912 x913 x914 x915 x916 x917 x918 x919 x920 x921 x922 x923 x924 x925 x926 x927 x928 x929 x930 x931 x932 x933 x934 x935 x936 x937 x938 x939 x940 x941 x942 x943 x944 x945 x946 x947 x948 x949 x950 x951 x952 x953 x954 x955 x956 x957 x958 x959 x960 x961 x962 x963 x964 x965 x966 x967 x968 x969 x970 x971 x972 x973 x974 x975 x976 x977 x978 x979 x980 x981 x982 x983 x984 x985 x986 x987 x988 x989 x990 x991 x992 x993 x994 x995 x996 x997 x998 x999 x1000 x1001 x1002 x1003 x1004 x1005 x1006 x1007 x1008 x1009 x1010 x1011 x1012 x1013 x1014 x1015 x1016 x1017 x1018 x1019 x1020 x1021 x1022 x1023 x1024 x1025 x1026 x1027 x1028 x1029 x1030 x1031 x1032 x1033 x1034 x1035 x1036 x1037 x1038 x10</sample>
    <sample id="111">The diagram illustrates the process of permuting with jumps. It shows a sequence of tags, including "the", "girl", "sleep", and "agent", which are permuted using a specific algorithm or rule. The diagram also includes a red arrow labeled "Permute" that connects the tags, indicating the direction of the permutation.</sample>
    <sample id="112">The diagram illustrates the concept of "Permuting with 'jumps'" in a computational or mathematical context. It shows two sets of nodes, labeled as X1 and X2, connected by arrows representing connections or relationships between them. The diagram also includes tags such as "the" and "girl," which may represent specific elements or entities within the system being modeled. The overall structure suggests a complex network or graph where nodes are interconnected and tagged with different labels.</sample>
    <sample id="113">The diagram illustrates a process involving two variables, x1 and x2, which are being permuted. The permutation is represented by the equation x1 = x2 and x2 = x1. This indicates that the values of x1 and x2 are swapped. The diagram also includes tags such as "the" and "girl," suggesting that the permutation may be part of a larger system or model. The diagram shows arrows connecting the variables and tags, indicating the flow of information or relationships between them.</sample>
    <sample id="114">Some Results on COGS (Kim and Lizenz 2020) Contrast with other Treeless Models on Structural Generalization.</sample>
    <sample id="115">Some Results on COGS (Kim and Lizen 2020) Contrast with other Treeless Models on Structural Generalization.</sample>
    <sample id="116">The video presents a technical diagram illustrating the alignment of tags in a permutation process. The diagram is divided into three sections, each containing a question mark to represent an unknown alignment. The top section has two green squares labeled 'g1' and 'g2', while the middle section has two yellow squares labeled 'g3' and 'g4'. The bottom section has two gray squares labeled 'g5' and 'g6'. The diagram also includes a horizontal line with the word 'Tag' written on it, indicating the alignment of the tags. The text 'Alignment unknown' is displayed below the diagram, emphasizing the challenge of determining the correct alignment of the tags.</sample>
    <sample id="117">The video presents a technical diagram illustrating the alignment of tags in a sequence. The diagram is divided into three sections, each representing a different stage or aspect of the alignment process. The top section shows a sequence of tags labeled 'the', 'girl', and 'sleep', with red arrows pointing to question marks below them, indicating unknown alignments. The middle section has three question marks, suggesting that the alignment of these tags is yet to be determined. The bottom section displays a sequence of tags labeled 'the', 'girl', and 'sleep', with red arrows pointing to question marks below them, similar to the top section. The text 'Alignment unknown' is displayed at the bottom of the diagram, emphasizing the ongoing challenge of aligning the tags correctly.</sample>
    <sample id="118">Technical Challenges We Solve Alignment unknown in training.</sample>
    <sample id="119">The video clip features a series of static slides with text and diagrams. The first slide is titled 'Technical Challenges We Solve' in bold black letters on a yellow background. Below the title, there are three bullet points: 'Alignment unknown,' 'Permute in training,' and 'Inference NP-hard (TSP).' The second slide has a similar layout but includes additional text at the bottom that reads 'Alignment unknown. Permute in training. Inference NP-hard (TSP).' The third slide introduces a diagram with various nodes and connections, including terms like 'sleep,' 'agent,' 'r1,' 'r2,' 'r3,' 'r4,' 'r5,' 'r6,' 'r7,' 'r8,' 'r9,' 'r10,' 'r11,' 'r12,' 'r13,' 'r14,' 'r15,' 'r16,' 'r17,' 'r18,' 'r19,' 'r20,' 'r21,' 'r22,' 'r23,' 'r24,' 'r25,' 'r26,' 'r27,' 'r28,' 'r29,' 'r30,' 'r31,' 'r32,' 'r33,' 'r34,' 'r35,' 'r36,' 'r37,' 'r38,' 'r39,' 'r40,' 'r41,' 'r42,' 'r43,' 'r44,' 'r45,' 'r46,' 'r47,' 'r48,' 'r49,' 'r50,' 'r51,' 'r52,' 'r53,' 'r54,' 'r55,' 'r56,' 'r57,' 'r58,' 'r59,' 'r60,' 'r61,' 'r62,' 'r63,' 'r64,' 'r65,' 'r66,' 'r67,' 'r68,' 'r69,' 'r70,' 'r71,' 'r72,' 'r73,' 'r74,' 'r75,' 'r76,' 'r77,' 'r78,'</sample>
    <sample id="120">The video clip features a series of static slides with text and diagrams. The first slide is titled 'Technical Challenges We Solve' and includes a diagram with various nodes and connections, as well as text that reads 'Inference: NP-hard (TSP) relaxation' and 'Backpropagation through continuous relaxation.' The second slide is similar to the first, but it also includes additional text that reads 'Alignment unknown in training.' The third slide is a continuation of the previous two, with the same diagram and text. The fourth slide introduces a new diagram with three layers labeled 'Permute,' 'Tag,' and 'Sleep agent,' along with additional text that reads 'Alignment unknown in training.' The fifth slide is similar to the fourth, but it also includes additional text that reads 'Inference: NP-hard (TSP) relaxation' and 'Backpropagation through continuous relaxation.' The sixth slide is a continuation of the previous five, with the same diagram and text. The seventh slide is similar to the sixth, but it also includes additional text that reads 'Alignment unknown in training.' The eighth slide is a continuation of the previous seven, with the same diagram and text.</sample>
    <sample id="121">The video clip features a series of static slides with text and diagrams. The first slide is titled 'Technical Challenges We Solve' and includes a diagram with various nodes and connections, as well as a URL at the bottom. The second slide is similar to the first, but with additional text and a QR code on the right side. The third slide is also similar to the previous two, with additional text and a QR code. The fourth slide is similar to the previous three, with additional text and a QR code. The fifth slide is similar to the previous four, with additional text and a QR code. The sixth slide is similar to the previous five, with additional text and a QR code. The seventh slide is similar to the previous six, with additional text and a QR code. The eighth slide is similar to the previous seven, with additional text and a QR code. The ninth slide is similar to the previous eight, with additional text and a QR code. The tenth slide is similar to the previous nine, with additional text and a QR code. The eleventh slide is similar to the previous ten, with additional text and a QR code. The twelfth slide is similar to the previous eleven, with additional text and a QR code. The thirteenth slide is similar to the previous twelve, with additional text and a QR code. The fourteenth slide is similar to the previous thirteen, with additional text and a QR code. The fifteenth slide is similar to the previous fourteen, with additional text and a QR code. The sixteenth slide is similar to the previous fifteen, with additional text and a QR code. The seventeenth slide is similar to the previous sixteen, with additional text and a QR code. The eighteenth slide is similar to the previous seventeen, with additional text and a QR code. The nineteenth slide is similar to the previous eighteen, with additional text and a QR code. The twentieth slide is similar to the previous nineteen, with additional text and a QR code. The twenty-first slide is similar to the previous twenty, with additional text and a QR code. The twenty-second slide is similar to the previous twenty-one, with additional text and a QR code. The twenty-third slide is similar to the previous twenty-two, with additional text and a QR code. The twenty-fourth slide is similar to the previous twenty-three, with additional text and a QR code. The twenty-fifth slide is similar to the previous twenty-four, with additional text and a QR code. The twenty-sixth slide is similar to the previous twenty-five, with additional text and a QR code. The twenty-seventh slide is similar to the previous twenty-six, with additional text and a QR code. The twenty-eighth slide is similar to the previous twenty-seven, with additional text and a QR code. The twenty-ninth slide is similar to the previous twenty-eight, with additional text and a QR code. The thirtieth slide is similar to the previous twenty-nine, with additional text and a QR code. The thirty-first slide is similar to the previous thirty, with additional text and a QR code. The thirty-second slide is similar to the previous thirty-one, with additional text and a QR code. The thirty-third slide is similar to the previous thirty-two, with additional text and a QR code. The thirty-fourth slide is similar to the previous thirty-three, with additional text and a QR code. The thirty-fifth slide is similar to the previous thirty-four, with additional text and a QR code. The thirty-sixth slide is similar to the previous thirty-five, with additional text and a QR code. The thirty-seventh slide is similar to the previous thirty-six, with additional text and a QR code. The thirty-eighth slide is similar to the previous thirty-seven, with additional text and a QR code. The thirty-ninth slide is similar to the previous thirty-eight, with additional text and a QR code. The forty-first slide is similar to the previous forty, with additional text and a QR code. The forty-second slide is similar to the previous forty-one, with additional text and a QR code. The forty-third slide is similar to the previous forty-two, with additional text and a QR code. The forty-fourth slide is similar to the previous forty-three, with additional text and a QR code. The forty-fifth slide is similar to the previous forty-four, with additional text and a QR code. The forty-sixth slide is similar to the previous forty-five, with additional text and a QR code. The forty-seventh slide is similar to the previous forty-six, with additional text and a QR code. The forty-eighth slide is similar to the previous forty-seven, with additional text and a QR code. The forty-ninth slide is similar to the previous forty-eight, with additional text and a QR code. The fiftieth slide is similar to the previous forty-nine, with additional text and a QR code. The fifty-first slide is similar to the previous fifty, with additional text and a QR code. The fifty-second slide is similar to the previous fifty-one, with additional text and a QR code. The fifty-third slide is similar to the previous fifty-two, with additional text and a QR code. The fifty-fourth slide is similar to the previous fifty-three, with additional text and a QR code. The fifty-fifth slide is similar to the previous fifty-four, with additional text and a QR code. The fifty-sixth slide is similar to the previous fifty-five, with additional text and a QR code. The fifty-seventh slide is similar to the previous fifty-six, with additional text and a QR code. The fifty-eighth slide is similar to the previous fifty-seven, with additional text and a QR code. The fifty-ninth slide is similar to the previous fifty-eight, with additional text and a QR code. The sixty-first slide is similar to the previous sixty, with additional text and a QR code. The sixty-second slide is similar to the previous sixty-one, with additional text and a QR code. The sixty-third slide is similar to the previous sixty-two, with additional text and a QR code. The sixty-fourth slide is similar to the previous sixty-three, with additional text and a QR code. The sixty-fifth slide is similar to the previous sixty-four, with additional text and a QR code. The sixty-sixth slide is similar to the previous sixty-five, with additional text and a QR code. The sixty-seventh slide is similar to the previous sixty-six, with additional text and a QR code. The sixty-eighth slide is similar to the previous sixty-seven, with additional text and a QR code. The sixty-ninth slide is similar to the previous sixty-eight, with additional text and a QR code. The seventy-first slide is similar to the previous seventy, with additional text and a QR code. The seventy-second slide is similar to the previous seventy-one, with additional text and a QR code. The seventy-third slide is similar to the previous seventy-two, with additional text and a QR code. The seventy-fourth slide is similar to the previous seventy-three, with additional text and a QR code. The seventy-fifth slide is similar to the previous seventy-four, with additional text and a QR code. The seventy-sixth slide is similar to the previous seventy-five, with additional text and a QR code. The seventy-seventh slide is similar to the previous seventy-six, with additional text and a QR code. The seventy-eighth slide is similar to the previous seventy-seven, with additional text and a QR code. The seventy-ninth slide is similar to the previous seventy-eight, with additional text and a QR code. The eighty-first slide is similar to the previous eighty, with additional text and a QR code. The eighty-second slide is similar to the previous eighty-one, with additional text and a QR code. The eighty-third slide is similar to the previous eighty-two, with additional text and a QR code. The eighty-fourth slide is similar to the previous eighty-three, with additional text and a QR code. The eighty-fifth slide is similar to the previous eighty-four, with additional text and a QR code. The eighty-sixth slide is similar to the previous eighty-five, with additional text and a QR code. The eighty-seventh slide is similar to the previous eighty-six, with additional text and a QR code. The eighty-eighth slide is similar to the previous eighty-seven, with additional text and a QR code. The eighty-ninth slide is similar to the previous eighty-eight, with additional text and a QR code. The ninety-first slide is similar to the previous ninety, with additional text and a QR code. The ninety-second slide is similar to the previous ninety-one, with additional text and a QR code. The ninety-third slide is similar to the previous ninety-two, with additional text and a QR code. The ninety-fourth slide is similar to the previous ninety-three, with additional text and a QR code. The ninety-fifth slide is similar to the previous ninety-four, with additional text and a QR code. The ninety-sixth slide is similar to the previous ninety-five, with additional text and a QR code. The ninety-seventh slide is similar to the previous ninety-six, with additional text and a QR code. The ninety-eighth slide is similar to the previous ninety-seven, with additional text and a QR code. The ninety-ninth slide is similar to the previous ninety-eight, with additional text and a QR code. The one hundredth slide is similar to the previous ninety-nine, with additional text and a QR code. The one hundred-first slide is similar to the previous one hundred, with additional text and a QR code. The one hundred-second slide is similar to the previous one hundred-first, with additional text and a QR code. The one hundred-third slide is similar to the previous one hundred-second, with additional text and a QR code. The one hundred-fourth slide is similar to the previous one hundred-third, with additional text and a QR code. The one hundred-fifth slide is similar to the previous one hundred-fourth, with additional text and a QR code. The one hundred-sixth slide is similar to the previous one hundred-fifth, with additional text and a QR code. The one hundred-seventh slide is similar to the previous one hundred-sixth, with additional text and a QR code. The one hundred-eighth slide is similar to the previous one hundred-seventh, with additional text and a QR code. The one hundred-ninth slide is similar to the previous one hundred-eighth, with additional text and a QR code. The one hundred-tenth slide is similar to the previous one hundred-ninth, with additional text and a QR code. The one hundred-and-first slide is similar to the previous one hundred-ten, with additional text and a QR code. The one hundred-and-second slide is similar to the previous one hundred-and-first, with additional text and a QR code. The one hundred-and-third slide is similar to the previous one hundred-and-second, with additional text and a QR code. The one hundred-and-fourth slide is similar to the previous one hundred-and-third, with additional text and a QR code. The one hundred-and-fifth slide is similar to the previous one hundred-and-fourth, with additional text and a QR code. The one hundred-and-sixth slide is similar to the previous one hundred-and-fifth, with additional text and a QR code. The one hundred-and-seventh slide is similar to the previous one hundred-and-sixth, with additional text and a QR code. The one hundred-and-eighth slide is similar to the previous one hundred-and-seventh, with additional text and a QR code. The one hundred-and-ninth slide is similar to the previous one hundred-and-eighth, with additional text and a QR code. The one hundred-and-tenth slide is similar to the previous one hundred-and-ninth, with additional text and a QR code. The one hundred-and-eleventh slide is similar to the previous one hundred-and-tenth, with additional text and a QR code. The one hundred-and-twelfth slide is similar to the previous one hundred-and-eleventh, with additional text and a QR code. The one hundred-and-thirteenth slide is similar to the previous one hundred-and-twelfth, with additional text and a QR code. The one hundred-and-fourteenth slide is similar to the previous one hundred-and-thirteenth, with additional text and a QR code. The one hundred-and-fifteenth slide is similar to the previous one hundred-and-fourteenth, with additional text and a QR code. The one hundred-and-sixteenth slide is similar to the previous one hundred-and-fifteenth, with additional text and a QR code. The one hundred-and-seventeenth slide is similar to the previous one hundred-and-sixteenth, with additional text and a QR code. The one hundred-and-eighteenth slide is similar to the previous one hundred-and-seventeenth, with additional text and a QR code. The one hundred-and-nineteenth slide is similar to the previous one hundred-and-eighteenth, with additional text and a QR code. The one hundred-and-twentieth slide is similar to the previous one hundred-and-nineteenth, with additional text and a QR code. The one hundred-and-twenty-first slide is similar to the previous one hundred-and-twentieth, with additional text and a QR code. The one hundred-and-twenty-second slide is similar to the previous one hundred-and-twenty-first, with additional text and a QR code. The one hundred-and-twenty-third slide is similar to the previous one hundred-and-twenty-second, with additional text and a QR code. The one hundred-and-twenty-fourth slide is similar to the previous one hundred-and-twenty-third, with additional text and a QR code. The one hundred-and-twenty-fifth slide is similar to the previous one hundred-and-twenty-fourth, with additional text and a QR code. The one hundred-and-twenty-sixth slide is similar to the previous one hundred-and-twenty-fifth, with additional text and a QR code. The one hundred-and-twenty-seventh slide is similar to the previous one hundred-and-twenty-sixth, with additional text and a QR code. The one hundred-and-twenty-eighth slide is similar to the previous one hundred-and-twenty-seventh, with additional text and a QR code. The one hundred-and-twenty-ninth slide is similar to the previous one hundred-and-twenty-eighth, with additional text and a QR code. The one hundred-and-thirtieth slide is similar to the previous one hundred-and-twenty-ninth, with additional text and a QR code. The one hundred-and-thirty-first slide is similar to the previous one hundred-and-thirtieth, with additional text and a QR code. The one hundred-and-thirty-second slide is similar to the previous one hundred-and-thirti</sample>
    <sample id="122">The introduced framework quantifies the positionality via Pearson's r scores.</sample>
    <sample id="123">The video features a static image with text and logos. The main title reads 'Weaker Than You Think' in large white letters, followed by 'A Critical Look at Weekly Supervised Learning' in smaller white letters. Below the title, there are six names listed: Dawid Zawu, Xiu Zhen Shen, 'M. N.' M. Moschel, Andrei Stephens, Dietrich Klakow, and Saarland University, Amazon Alexa, 3U, University of Vienna. The background is white, and the text is black, making it stand out clearly.</sample>
    <sample id="124">The video features a static image with text and logos. The main title reads 'Weaker Than You Think' in large white letters, followed by 'A Critical Look at Weekly Supervised Learning' in smaller white letters. Below the title, there are six names listed: Dawid Zaw, Xiu Zhen Shen, 'M. N.' M. Moschel, Andrei Stephens, Dietrich Klakow, and Saarland University, Amazon Alexa, and 3U of Vienna. At the bottom, there is a logo for 'ACI 2023'.</sample>
    <sample id="125">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="126">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="128">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy. Noise memorization harms generalization.</sample>
    <sample id="130">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XXX."</sample>
    <sample id="131">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XXX."</sample>
    <sample id="132">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy XXX% ğŸ˜³"</sample>
    <sample id="133">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy XXX% ğŸ˜³"</sample>
    <sample id="134">Our research questions R01 Is clean validation data necessary? R02 How many clean samples does DSL approaches need? R03 How to use the available clean samples more efficiently</sample>
    <sample id="135">Our research questions R01 Is clean validation data necessary? R02 How many clean samples does DSL approaches need? R03 How to use the available clean samples more efficiently</sample>
    <sample id="136">R01 Main findings</sample>
    <sample id="137">R01 Main findings</sample>
    <sample id="138">R01 Main findings</sample>
    <sample id="139">R01 Main findings</sample>
    <sample id="140">The relative improvement in weekly average weekly performance (in percentage points) is shown for different validation strategies. A clean validation set is indispensable.</sample>
    <sample id="141">The graph shows the accuracy of different methods in a validation process. The x-axis represents the validation set size, ranging from 10 to 50, while the y-axis represents the accuracy percentage, ranging from 75 to 85. The graph displays multiple lines, each representing a different method or approach. The lines are color-coded and labeled accordingly. The graph is used to compare the performance of these methods as the validation set size increases.</sample>
    <sample id="142">The graph shows the accuracy of different methods in a validation process. The x-axis represents the validation set size, ranging from 10 to 50, while the y-axis represents the accuracy percentage, ranging from 75 to 85. The graph displays multiple lines, each representing a different method or approach. The lines are color-coded and labeled accordingly. The graph also includes shaded areas around some lines, indicating confidence intervals or error margins. The title "Main findings" suggests that this graph is part of a larger presentation or report summarizing key results or observations.</sample>
    <sample id="143">The graph on the left shows a comparison of accuracy across different methods, while the graph on the right displays performance delta percentages. The graphs are labeled with various methods such as "FT," "L2R," "COSINE," "MLC," and "MLB." The graphs also include a red box highlighting specific data points or trends.</sample>
    <sample id="144">The main findings of the study are presented in a slide. The slide is divided into two sections: "Main findings" and "Performance." The "Main findings" section includes a graph with multiple lines representing different datasets, each labeled with a color and a corresponding dataset name. The graph shows the accuracy of each dataset over time, measured in weeks. The "Performance" section features a bar chart comparing the performance of different methods, including "FT," "L2C," "L2R," "MMLC," "MLB," "MLC," "MLC-Adapt," and "Adapt." Each method is represented by a different color and has a corresponding bar indicating its performance. The slide also includes a red box highlighting specific data points on the bar chart.</sample>
    <sample id="145">The graph on the left shows the accuracy of different models over time, with a trend line indicating an increase in performance. The graph on the right displays the performance delta for various models, highlighting significant changes in performance over time.</sample>
    <sample id="146">The graph shows the accuracy of different models before and after a certain process. The models are labeled as "Before," "After," "Clean Only," and "CFT." The accuracy is measured on the y-axis, ranging from 76 to 84, while the x-axis represents two different sample sizes: "N=10 per class" and "N=50 per class." The graphs indicate that the accuracy generally increases for all models after the process, with the "Clean Only" model showing the highest accuracy in both scenarios.</sample>
    <sample id="147">The graph shows the accuracy of different models before and after a certain process. The models are labeled as "Before," "After," "Clean Only," and "CFT." The graph compares the accuracy of these models on two different datasets, one with 10 samples per class and the other with 50 samples per class. The results show that the "After" model generally performs better than the "Before" model, and the "Clean Only" and "CFT" models also show improvements in accuracy.</sample>
    <sample id="148">The graph shows the accuracy of different models before and after a certain process. The models are labeled as "Before," "CFTF," and "Clean Only." The graph compares the accuracy of these models on two different datasets: N-10 samples per class and N-50 samples per class. The accuracy is measured in percentage, with the y-axis ranging from 76 to 88. The x-axis represents the different models. The graph indicates that the CFTF model generally performs better than the other two models, especially in the N-50 samples per class dataset.</sample>
    <sample id="149">R03 Main findings Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before After Before</sample>
    <sample id="150">Recent WSL approaches. Our recommendations.</sample>
    <sample id="151">Conclusion Recent WSL approaches  Overestimate their practicality. Our recommendations Use Few-shot learning approach criteria as baselines Apply continuous fine-tuning (CFT)</sample>
    <sample id="152">Conclusion Recent WSL approaches  Overestimate their practicality. Our recommendations Use Few-shot learning approach criteria as baselines Apply continuous fine-tuning (CFT)</sample>
    <sample id="153">Conclusion Recent WSL approaches  Overestimate their practicality. Our recommendations Use Few-shot learning approach as baselines Apply continuous fine-tuning (CFT)</sample>
    <sample id="154">Conclusion Recent WSL approaches. Our recommendations.</sample>
    <sample id="155">The study found that the AI-generated personas were rated as more realistic than those generated by human subjects.</sample>
    <sample id="156">Penn Treebank, Marcus et al., 1993; Fricke and Goldberg, 2016</sample>
    <sample id="157">2</sample>
    <sample id="158">Debate, CE, DEBATE</sample>
    <sample id="159">2</sample>
    <sample id="160">6</sample>
    <sample id="161">The introduced framework differs from the previous works by using a combination of model-based and data-based approaches to compare annotations.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate.</sample>
    <sample id="200">6</sample>
    <sample id="201">Up to 900 tokens.</sample>
    <sample id="202">Music, books and recipes.</sample>
    <sample id="203">The perspectives people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="204">Dawid Zuch</sample>
    <sample id="205">Yes</sample>
    <sample id="206">4</sample>
    <sample id="207">No</sample>
    <sample id="208">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">How to use the available clean samples more efficiently</sample>
    <sample id="211">The metric sensitivity works by measuring how sensitive the model is to variations in instructions for the same task.</sample>
    <sample id="212">Wenwen Jiang</sample>
    <sample id="213">Greater sensitivity indicates worse model performance.</sample>
    <sample id="214">Contextualized</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford Engineering Computer Science</sample>
    <sample id="217">Because existing methods are not sufficient.</sample>
    <sample id="218">Jackie CK Chuang</sample>
    <sample id="219">The political bias propagation pipeline is a three step process.</sample>
    <sample id="220">yes</sample>
    <sample id="221">Yes</sample>
    <sample id="222">The watermark is inserted by adding a small number to the embedding of each word in the text.</sample>
    <sample id="223">Penn State and Amazon</sample>
    <sample id="224">Yes</sample>
    <sample id="225">How to make a chocolate cake</sample>
    <sample id="226">They use a random seed to ensure the covertness of their method.</sample>
    <sample id="227">By fine-tuning the existing PLMs on a specific task.</sample>
    <sample id="228">Catholic Europe</sample>
    <sample id="229">I am a student.</sample>
    <sample id="230">The more tasks, the better the model performs.</sample>
    <sample id="231">LSTM, T5-seq2seq, and Zhenqiang.</sample>
    <sample id="232">PhD students</sample>
    <sample id="233">Crawford</sample>
    <sample id="274">3</sample>
    <sample id="275">Using a diverse set of data sources.</sample>
    <sample id="307">PaLM has comparable fluency to SOTA systems.</sample>
    <sample id="308">Utility, should not degrade the utility of the provided embeddings, should cover the attacker, and should be transferable.</sample>
    <sample id="309">Arabic, Chinese, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Romanian, Russian, Spanish, Turkish and Urdu.</sample>
    <sample id="310">20</sample>
    <sample id="311">similarity difference and Ï‡Â² distance</sample>
    <sample id="312">They were pretrained on the multilingual M300 dataset.</sample>
    <sample id="313">In this talk, I will present a new approach to distilling script knowledge from large language models for constrained language planning.</sample>
    <sample id="314">Language Planning</sample>
    <sample id="315">Language Planning</sample>
    <sample id="316">Constrained Language Planning</sample>
    <sample id="317">Constrained Language Planning</sample>
    <sample id="318">Constrained Language Planning</sample>
    <sample id="319">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="320">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="321">How do LLMs perform on Constrained Language Planning?</sample>
    <sample id="322">Can LLMs do Constrained Language Planning?</sample>
    <sample id="323">Can LLMs do Constrained Language Planning?</sample>
    <sample id="324">What types of errors do LLMs usually make in this task?</sample>
    <sample id="325">What types of errors do LLMs usually make in this task?</sample>
    <sample id="326">What kinds of goals do InstructGPT typically fail?</sample>
    <sample id="327">The method is to generate specific goals based on the input.</sample>
    <sample id="328">The method is to generate specific goals based on the input.</sample>
    <sample id="329">The video clip is a tutorial on how to use the GPT-4 model for generating scripts. The presenter explains that the method involves inputting a specific goal, such as 'make a cake,' and then using GPT-4 to generate candidate scripts based on that goal. The presenter also mentions that the method can be used for overviews with general goals, and that it can be applied in various contexts, including weddings.</sample>
    <sample id="330">The method involves over-generating candidate scripts using a script-to-script model, filtering these scripts based on similarity to the input, and then using the filtered scripts as goals for a script-to-script model. The output is generated with corresponding scripts that match the goals.</sample>
    <sample id="331">The method involves over-generating candidate scripts using ScriptGPT, filtering them with InstructGPT, and then using the filtered scripts to train a model.</sample>
    <sample id="332">The method involves overwriting the instructions with a script, filtering the candidates based on similarity scores, and then outputting goals with corresponding scripts.</sample>
    <sample id="333">In this paper, we present a new method for generating high-quality planning policies.</sample>
    <sample id="334">The video clip features a woman with long brown hair, wearing a green top, speaking in front of a whiteboard. The whiteboard displays the title 'Script Distillation from LLMs' and a flowchart outlining the steps involved in the process. The flowchart includes input, method, and output sections, with specific details such as generating goals in language, filtering scripts based on goal similarity, and annotating validation and test sets. The woman appears to be explaining the content of the flowchart, providing context and information about the script distillation process.</sample>
    <sample id="335">The video clip features a woman with long brown hair, wearing glasses and a green top. She is speaking in front of a whiteboard that displays a presentation slide titled 'Script Distillation from LLMs'. The slide outlines the motivation for the project, the method used, and the steps involved in the process. The woman appears to be explaining the content on the slide, providing context and details about the script distillation process.</sample>
    <sample id="336">The video clip features a woman with long brown hair, wearing a green top, speaking in front of a whiteboard. The whiteboard displays the title 'Script Distillation from LLMs' and a flowchart outlining the steps involved in the process. The flowchart includes input, method, and output sections, with specific details such as generating goals in language, filtering scripts based on goal similarity, and annotating validation and test sets. The woman appears to be explaining the content of the flowchart, providing context and information about the script distillation process.</sample>
    <sample id="337">The video clip is a presentation.</sample>
    <sample id="338">The video clip is a presentation.</sample>
    <sample id="339">The video is about the constraint analysis of language models.</sample>
    <sample id="340">The speaker discusses the accuracy of specialized models versus large language models (LLMs) in generating scripts. The speaker presents a bar graph comparing the accuracy of different models, including GPT-3 (175B), CodeX (173B), and TS trained on Wikitext. The speaker explains that smaller LLMs fine-tuned on specific tasks can produce higher-quality scripts than larger LLMs trained on general data. The speaker also mentions the limitations of the proposed method for improving LLMs post-ranking and suggests future work to address these limitations.</sample>
    <sample id="341">The proposed method for improving LLMs post-ranking approach is a high-quality script dataset.</sample>
    <sample id="342">The proposed method for improving LLMs post-ranking approach is a high-quality script dataset.</sample>
    <sample id="343">In this talk, I will present a new approach to distilling script knowledge from large language models for constrained language planning.</sample>
    <sample id="344">They randomly select n words from a moderate-frequency interval.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James Finch, and Jinho D. Choi Emory University Emory NLP Research Lab Alexa</sample>
    <sample id="372">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University Emory NLP Research Lab Alexa</sample>
    <sample id="373">The video features a presentation slide titled 'Comparative Evaluation' with the logos of Emory University and the Allen Institute for Artificial Intelligence at the bottom. The slide displays two sets of speech bubbles, one set in blue and the other in purple, each containing a black silhouette of a person's head. The blue speech bubbles are connected to a blue box labeled 'A', while the purple speech bubbles are connected to a purple box labeled 'B'. The background is white, and the text and logos are clearly visible.</sample>
    <sample id="374">The video clip features a static image with a blue background and a white border. In the center, there is a cartoon illustration of a judge holding a gavel, with a scale of justice on the left side. Above the judge, there are two speech bubbles, one light blue and one dark blue, representing different perspectives or opinions. Below the judge, there is a horizontal line with five numbered points from 1 to 5, indicating a Likert scale for evaluation. The text 'Liker Rating Evaluation' is displayed at the top of the image, and logos for Emory University and Alora are visible in the bottom corners. The overall design suggests an educational or instructional context, possibly related to legal or ethical evaluations.</sample>
    <sample id="375">The video clip features a presentation slide titled 'Dimensions of Dialogue Quality' with three key points: Relevance, Consistency, and Emotional Understanding. The slide is displayed on a screen in the background, while a person stands in front of it, likely delivering a lecture or presentation. The slide is part of a larger discussion on the quality of dialogue, possibly in the context of artificial intelligence or human-computer interaction.</sample>
    <sample id="376">The video clip features a presentation slide titled 'Likert Rating Evaluation' with a blue background and white text. The slide includes an illustration of a judge holding a gavel, a series of five speech bubbles representing responses, and a green checkmark indicating the correct response. The slide also displays the logos of Emory University and Alora at the bottom corners.</sample>
    <sample id="377">The video clip features a presentation slide titled 'Liker Rating Evaluation' with a cartoon judge and two blue robot heads. The slide instructs viewers to rate the relevance of the bot's responses on a scale from 1 to 5, with a green checkmark indicating a correct response. The background is white, and the text is in black and blue. The slide also includes the logos of Emory University and Alora at the bottom corners.</sample>
    <sample id="378">The video shows a presentation slide titled 'Annotating Behaviors in Chat (ABC-Eval)' with three speech bubbles labeled 'Inrelevant,' 'Lack of Empathy,' and 'Self-Contradiction.' The slide is divided into three sections, each containing a speech bubble. The first section has a blue speech bubble labeled 'Inrelevant,' the second section has two blue speech bubbles labeled 'Lack of Empathy' and 'Self-Contradiction,' and the third section has another blue speech bubble labeled 'Inrelevant.' The background of the slide is white, and there are logos at the bottom left corner that read 'Emory University' and 'alora.'</sample>
    <sample id="379">The video clip features a static image with text and graphics. The main focus is on the topic of annotating behaviors in chat, specifically highlighting three key behaviors: irrelevant, lack of empathy, and self-contradiction. The graphic illustrates these behaviors using speech bubbles connected to a central figure, emphasizing their importance in understanding and annotating online interactions.</sample>
    <sample id="380">The ABC model is a framework for understanding and evaluating behaviors. It consists of three components: A (Antecedent), B (Behaviors), and C (Consequences). The model suggests that behaviors are influenced by both internal factors, such as knowledge and emotional understanding, and external factors, such as coherence and consistency.</sample>
    <sample id="381">The ABC model is a framework for understanding and evaluating behaviors. It consists of three components: A (Antecedent), B (Behaviors), and C (Consequences). The ABC model helps individuals identify the underlying causes of their behaviors and understand how those behaviors impact others.</sample>
    <sample id="382">The ABC model is a framework for understanding and evaluating behaviors. It consists of three components: A (Antecedent), B (Behaviors), and C (Consequences). The antecedent refers to the situation or event that triggers a behavior, while the behaviors refer to the actions taken in response to the antecedent. The consequences refer to the outcomes or reactions that follow the behavior. The ABC model helps individuals identify the underlying causes of their behaviors and understand how they are influenced by external factors. By analyzing the ABC model, individuals can gain insights into their own behavior patterns and develop strategies for making more positive choices.</sample>
    <sample id="383">The experiments are designed to test the performance of four open-domain dialogue models. Each model will undergo 100 human-bot conversations, and the results will be evaluated using a metric called ABC-Eval.</sample>
    <sample id="384">The experiment is designed to test the performance of four different dialogue models in a human-like manner. The models are evaluated based on their ability to generate responses that are similar to those generated by humans. The experiment involves 100 conversations per model, with each conversation consisting of 5 turns. The models are tested on their ability to understand and respond to user input, as well as their ability to generate appropriate responses based on the context of the conversation. The experiment also includes a comparison between the performance of the models and human-generated responses, allowing for a direct comparison of the models' capabilities. Overall, the experiment aims to evaluate the effectiveness of these dialogue models in simulating human-like conversations and their ability to understand and respond to user input in a natural and engaging way.</sample>
    <sample id="385">The video features a man in a gray shirt presenting information about baseline evaluations. He stands against a plain background with a presentation slide behind him, which is divided into three sections labeled 'Turn Lifter,' 'Dialog Liker,' and 'Comparative.' Each section contains a list of criteria such as 'Consistency,' 'Emotional Understanding,' 'Informative,' 'Overall Quality,' and more. The man appears to be explaining the evaluation process for these criteria, likely related to a study or project he is involved in.</sample>
    <sample id="386">The speaker is discussing the inter-annotator agreement for different dialogue turn types in a study. The speaker explains that the data shows high agreement for certain dialogue turn types, such as 'ABC Eval' and 'Turn Liker,' but lower agreement for others like 'Comparative.' The speaker also mentions that the data was collected from a specific dataset called 'Turn Liker' and that the results are presented with 95% confidence intervals.</sample>
    <sample id="387">The graph shows the predictive validity of different methods for predicting quality. The methods are compared across four categories: ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. The graph displays the percentage of predictive validity for each method in each category.</sample>
    <sample id="388">The graph shows that the predictive validity of the interactive quality is significantly higher than the comparative quality.</sample>
    <sample id="389">In the study, we found that the incremental validity of the dialogue quality scale was very high.</sample>
    <sample id="390">In this graph, we can see that the incremental validity of the AB-Valence is higher than the incremental validity of the AB-Emotion.</sample>
    <sample id="391">In the diagram, there are three curves labeled A-B-C, Turn Liker, and Dialog Liker. The A-B-C curve starts at a lower point on the graph and gradually increases as it moves to the right. The Turn Liker curve starts higher than the A-B-C curve but follows a similar upward trend. The Dialog Liker curve starts even higher than the Turn Liker curve and continues to rise as it progresses along the x-axis.</sample>
    <sample id="392">The speaker is discussing the evaluation of different models in terms of their error rates. The speaker mentions that the CS model has a lower error rate than the other models, and that the error rates are generally lower for the CS model compared to the other models.</sample>
    <sample id="393">The speaker is discussing the evaluation error rates of different models in a research study. He explains that the error rates are presented as percentages and compares them across various models such as BERT, RoBERTa, and GPT-2. The speaker highlights that the error rates vary depending on the model and the specific task or topic being evaluated.</sample>
    <sample id="394">The speaker is discussing the evaluation error rates of different models in a scientific study. The speaker is pointing out that the error rates are generally low across all models, with some models having slightly higher error rates than others. The speaker also mentions that the error rates are generally lower for the topic model compared to the other models.</sample>
    <sample id="395">The speaker is discussing the evaluation of different models in terms of their error rates. The speaker mentions that the CS model has a lower error rate than the other models, and that the error rates are generally low across all models. The speaker also notes that the error rates are similar for the CS model and the BERT model, but that the CS model has a slightly higher error rate for the "other" category.</sample>
    <sample id="396">Thanks For Watching!</sample>
    <sample id="397">100ms</sample>
    <sample id="398">Servin is a judge and Kea is a baker.</sample>
    <sample id="399">Example quality</sample>
    <sample id="400">Roberta and GPT-2</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">Easy, "the first one"</sample>
    <sample id="403">Siyu Yuan, Chuan Jiang, Zhen Chen, Zhifeng Fu, Xuyang Feng, Geohong Shao, Chander Robert Jankowski, Yang Xia, Xiao Diao, and Ying Yang.</sample>
    <sample id="404">4</sample>
    <sample id="405">Yes, it was.</sample>
    <sample id="406">a woman warrior</sample>
    <sample id="407">LSTM and RNN</sample>
    <sample id="408">FT, L2, COSINE, L2+FT, BFIAC, Adapter</sample>
    <sample id="409">6</sample>
    <sample id="410">Multiple modalities</sample>
    <sample id="411">DRBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains Yans Labys, Adrien Bazze, Richard Duour, Michael Rouvier, Emmanuel Bourin, Danielle Dalle (1) L'Agence Universitaire de la Recherche (LAR) (2) GNSLS - UniversitÃ© de Nantes (3) UCI des sciences de l'homme et de la santÃ© de l'UniversitÃ© de Nantes (4) Avignon University</sample>
    <sample id="412">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the field of language modeling for healthcare. The background shows a bookshelf filled with books, suggesting an academic or professional setting.</sample>
    <sample id="413">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the context of language modeling in healthcare. The background shows a bookshelf filled with books, suggesting an academic or professional setting.</sample>
    <sample id="414">The video clip features a person presenting a slide titled 'Summary' with three main points. The first point is 'Language Modeling in Healthcare,' the second is 'Comparison of pre-training strategies, data sources and sizes,' and the third is 'Evaluation of 13 models on 11 tasks.' The presenter appears to be discussing the contributions of NACHOS and DBERT in the context of language modeling in healthcare. The background shows a bookshelf filled with books, suggesting an academic or professional setting.</sample>
    <sample id="415">The video clip features a presentation slide titled 'Summary' with three main points. The first point discusses language modeling in healthcare, the second compares pre-training strategies, data sources, and sizes, and the third evaluates 13 models on 11 tasks. The presenter is seen in front of a bookshelf, wearing a black shirt, and appears to be explaining the content of the slide.</sample>
    <sample id="416">Language Modeling Transformer-based approaches, such as BERT, offer a performance gain on NLP tasks. Has been adapted to French (CamemBERT and FERBERT) On medical domain: specific models (English) are the bar higher than in English. Languages others: No is rare, reliably rely on pre-trained French existing model. Generalized model: no is rare, reliably rely on pre-trained French existing model. BERT-based domain model for French should improve performance on medical tasks.</sample>
    <sample id="417">Language Modeling Transformer-based approaches, such as BERT, offer a performance gain on NLP tasks. Has been adapted to French (CamemBERT and FERBERT) On medical domain: specific models (English) are the bar higher than other languages No rare languages rely on reliably pre-trained pre-trained models in French yet</sample>
    <sample id="418">Language Modeling Transformer-based approaches, such as BERT, offer a performance gain on NLP tasks. Has been adapted to French (CamemBERT and FERBERT) On medical domain: specific models (English) are the bar higher than other languages No rare languages rely on reliably pre-trained pre-trained models in French yet</sample>
    <sample id="419">Language Modeling Transformer-based approaches, such as BERT, offer a performance gain on NLP tasks. Has been adapted to French (CamemBERT and FERBERT) On medical domain: specific models (English) are the bar ever higher. Languages others: No is rare, reliably rely on pre-trained French existing model. Generalized model: open-source model is available in biomedical domain in French yet. BERT-based domain model should improve performance on medical tasks.</sample>
    <sample id="420">The comparison of pre-training strategies and data sources.</sample>
    <sample id="421">The comparison of pre-training strategies and data sources.</sample>
    <sample id="422">The comparison of pre-training strategies and data sources.</sample>
    <sample id="423">The comparison of pre-training strategies and data sources.</sample>
    <sample id="424">The comparison of pre-training strategies and data sources.</sample>
    <sample id="425">Comparison of pre-training strategies and data sources.</sample>
    <sample id="426">Comparison of pre-training strategies and data sources</sample>
    <sample id="427">The comparison of pre-training strategies and data sources is presented.</sample>
    <sample id="428">Evaluation: Data sources and size Performance evaluation of 13 tasks on 11 public and private datasets. Our fine-tuned model gets the art on almost all tasks.</sample>
    <sample id="429">Evaluation: Data sources and size Performance evaluation of 13 tasks on 11 public and private datasets. Our fine-tuned model gets the art on almost all tasks.</sample>
    <sample id="430">Evaluation: Data sources and size Performance evaluation of 13 tasks on 11 public and private datasets. Our fine-tuned model gets the art on almost all tasks.</sample>
    <sample id="431">Evaluation: Data sources and size Performance evaluation of 13 tasks on 11 public and private datasets. Our fine-tuned model get the art on almost all tasks.</sample>
    <sample id="432">Evaluation: Pre-training strategies From from scratch to pre-training on 4-dim knowledge Question-answering task requires higher inter-task stability for models using continuous pre-training.</sample>
    <sample id="433">Evaluation: Pre-training strategies From from scratch to pre-training on 4-dim knowledge Question-answering task requires higher inter-task stability for better performance.</sample>
    <sample id="434">Evaluation: Pre-training strategies From from scratch to pre-training on 4-dimentional knowledge Question-answering task requires higher inter-task stability for models using continuous pre-training.</sample>
    <sample id="435">The presentation is about the core message of a study. The study was conducted by DBERT and it was able to achieve state-of-the-art results in 9 French-oriented tasks. The study also confirmed the importance of training on medical-specific data for French. The study also found that more data is better, but it does not scale well.</sample>
    <sample id="436">The video is a presentation about the performance of the DEBERT model in French. The presenter explains that DEBERT has achieved state-of-the-art results on 10 French medical-oriented tasks, surpassing the performance of the general model and English domain-specific models. The presenter emphasizes the importance of data sources and training heterogeneity for the model's performance. The presenter also mentions that continuous pretraining is more effective than domain-specific models based on specific English models. The presenter concludes by highlighting that more data is better, but it does not scale well, and that continuous pretraining is a more effective strategy.</sample>
    <sample id="437">The video is a presentation about the performance of the DEBERT model in French. The presenter explains that DEBERT has achieved state-of-the-art results on 10 French medical-oriented tasks, surpassing the performance of the general model and English domain-specific models. The presenter emphasizes the importance of data sources and training heterogeneity for the model's performance. The presenter also mentions that continuous pretraining is more effective than domain-specific models based on specific English models. The presenter concludes by highlighting that more data is better, but it does not scale well, and that continuous pretraining is a more effective strategy.</sample>
    <sample id="438">Thank You Looking forward to exchange at poster session in Toronto! More information on drt.bert@avignon.univ.fr</sample>
    <sample id="439">Inference-time knowledge.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes, human annotators annotated validation and test sets.</sample>
    <sample id="442">lexicon-level metrics</sample>
    <sample id="473">walk, LA, CA, ED</sample>
    <sample id="474">LIA Lab, LIGNNLS, UCL, and INRA</sample>
    <sample id="475">Jennifer T. Lang</sample>
    <sample id="476">3</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The affiliations of the authors are the University of Trento and the Fondazione Bruno Kessler.</sample>
    <sample id="536">Mohammad javad Hosseini</sample>
    <sample id="537">Google's PALM for Translation: Assessing Strategies and Performance.</sample>
    <sample id="538">The model is trained on 78 billion tokens.</sample>
    <sample id="539">The model is trained on 80 billion tokens and has 54 billion parameters.</sample>
    <sample id="540">Our contribution.</sample>
    <sample id="541">Our contribution.</sample>
    <sample id="542">Our contribution.</sample>
    <sample id="543">Our contribution.</sample>
    <sample id="544">Prompts have a big impact on translation quality. Select two random prompts for each sentence pair. Compute BLEURT for each pair. Compute the MAJORITY point (516 out of 1000) and show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="545">Prompts have a big impact on translation quality. Select two random prompts for each sentence pair. Compute BLEURT for each sentence-prompt pair. Compute the MAJORITY point (516 out of 1000) to show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="546">Prompts have a big impact on translation quality. Select two random prompts for each sentence pair. Compute BLEURT for each sentence-prompt pair. Compute the MAJORITY BLEURT point (516 out of 1000). Show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="547">-5 score.</sample>
    <sample id="548">-5 prompt</sample>
    <sample id="549">-5 5-point prompting English: is being man spirited to the court under the custody of two policemen on a bus to jail. German: Polizei hat die Polizei mit dem Gefangenen auf einem Bus zum GefÃ¤ngnis transportiert.</sample>
    <sample id="550">-5 5-point prompting for translation English: is being man spirited to the court under the custody of two policemen on a bus to jail. German: Polizei hat die Polizei mit dem Gefangenen auf einem Bus zu dem Gericht transportiert.</sample>
    <sample id="551">-5 5-point prompting English: is being man spirited to the court under the custody of two policemen on a bus to jail. German: Polizei hat die Passe mitgenommen und die Polizei wird die Passe mitnehmen.</sample>
    <sample id="552">The results of the experiment are presented.</sample>
    <sample id="553">The results of the experiment are presented in a slide titled "Experimental Results." The slide lists several key points, including that the quality of PALM is more important than SOTA systems, that PALM is similar to the source sentence, and that it has a substantial advantage. The speaker explains that the example quality is generally more important than the source sentence, and that PALM is comparable to SOTA systems. The speaker also mentions that PALM has a substantial advantage over SOTA systems.</sample>
    <sample id="554">The results of the experiment are presented in a slide titled "Experimental Results." The slide lists several key points, including that the quality of PALM is more important than SOTA systems, that PALM is close to Google Translate, and that it has a substantial advantage. The speaker, whose face is not visible, discusses these points while looking at the slide.</sample>
    <sample id="555">The results of the experiment are presented in a slide titled "Experimental Results." The slide lists several key points, including that the quality of PALM is more important than SOTA systems, that PALM is similar to the source sentence, and that it has a substantial advantage. The speaker explains that the example quality is generally more important than the source sentence, and that PALM is comparable to SOTA systems. The speaker also mentions that PALM has a substantial advantage over SOTA systems.</sample>
    <sample id="556">The results of the experiment are presented in a slide titled "Experimental Results." The slide lists several key points, including that the quality of PALM is more important than SOTA systems, that PALM is similar to the source sentence, and that it has a substantial advantage. The speaker explains that the example quality is generally more important than the source sentence, and that PALM is comparable to SOTA systems. The speaker also mentions that PALM has a substantial advantage over SOTA systems.</sample>
    <sample id="557">The results of the experiment are presented.</sample>
    <sample id="558">The results of the experiment are presented in a slide titled "Experimental Results." The slide lists several key points, including that the quality of PALM is more important than SOTA systems, that PALM is similar to the source sentence, and that it has a substantial advantage. The speaker, whose face is not visible, discusses these points while looking at the slide.</sample>
    <sample id="559">The results of the experiment are presented.</sample>
    <sample id="560">The results of the experiment are presented.</sample>
    <sample id="561">Thank you.</sample>
    <sample id="597">Tag tokens</sample>
    <sample id="598">50,000</sample>
    <sample id="599">The video features a presentation slide titled 'The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources.' The slide is divided into two sections. The top section displays the logos of McGill, Mila, and Microsoft Research, indicating their collaboration on the project. Below this, there are six images of individuals with their names listed underneath: Martin Kuhn, Khaled Suleiman, Adam Trischler, Alexandra Oitmau, Jacky Cheung, and Chao Ma. Each image includes a small text box at the bottom that reads 'Equal Contribution.' The background of the slide is white, and the text is primarily in black, with the exception of the logos which are in their respective colors. The overall design is clean and professional, suggesting a formal academic or research context.</sample>
    <sample id="600">The NLU model is a framework that draws on multiple knowledge sources. It utilizes two main approaches: "Knowledge in Parameters" and "Knowledge in Context." The "Knowledge in Parameters" approach involves pretraining the model with time knowledge, while the "Knowledge in Context" approach infers the knowledge during runtime. This model is designed to handle complex tasks such as understanding natural language by leveraging both static and dynamic information.</sample>
    <sample id="601">In the context of NLU models, there are two main approaches to knowledge representation. The first approach is "Knowledge in Parameters," which involves pretraining time knowledge and utilizing a graph structure to represent knowledge. The second approach is "Knowledge in Context," which focuses on inference-time knowledge and incorporates contextual information into the model. These two approaches highlight the importance of considering both pretraining and inference phases in NLU models.</sample>
    <sample id="602">John saw the newly elected president on TV. What presidents do is a TV. Who is John? Who is the new president?</sample>
    <sample id="603">John saw the newly elected president on TV. What presidents do is a TV. Who is John? Who is the new president?</sample>
    <sample id="604">John saw the newly elected president on TV. What presidents do is a TV. Who is John? Who is the new president?</sample>
    <sample id="605">John saw the newly elected president on TV.</sample>
    <sample id="606">The KITMUS Test Suite is a comprehensive evaluation framework designed to assess various aspects of knowledge integration and resolution. It includes a dataset for knowledge integration evaluation, a coreference resolution task to probe ability to draw on pretrain-time knowledge, inference with time knowledge, an experiment with human participants, and co-resolution models.</sample>
    <sample id="607">The KITMUS Test Suite is a comprehensive evaluation tool designed to assess various aspects of knowledge integration and resolution. It includes a dataset for knowledge integration evaluation, a coreference resolution task to probe the ability to draw on pretraining-time knowledge, inference with time knowledge, an experiment with human participants, and coference resolution models. The suite aims to evaluate how well different models and human participants can integrate and resolve knowledge over time, providing valuable insights into their cognitive processes and capabilities.</sample>
    <sample id="608">KITEMUS Test Suite Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="609">KITMUS Test Suite Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="610">The video clip features a person standing in front of a whiteboard, presenting information. The whiteboard displays the title 'KITMUS Test Suite' at the top, followed by two sections labeled '1) Entity-specific knowledge' and '2) Background knowledge.' Below these labels, there are bullet points providing examples of each type of knowledge. The first section includes an example about Servin being a judge and Kea being a baker, while the second section describes a scenario where judges decide cases in courts of law. The person appears to be explaining or discussing the content on the whiteboard, likely as part of an educational or instructional presentation.</sample>
    <sample id="611">The KITMUS Test Suite is a set of tests designed to evaluate the ability of individuals to understand and process information. The test suite consists of two main components: entity-specific knowledge and background knowledge. Entity-specific knowledge refers to the ability to understand and recall specific facts or details about a particular subject, while background knowledge refers to the ability to understand and recall general concepts and principles that are relevant to a wide range of subjects. The test suite also includes pre- and post-time knowledge, which assesses an individual's ability to retain and apply information over time. Overall, the KITMUS Test Suite provides a comprehensive assessment of an individual's cognitive abilities and can be used in various fields such as education, psychology, and law enforcement.</sample>
    <sample id="612">The KITMUS Test Suite is a set of tests designed to evaluate the ability of individuals to understand and process information. The test suite consists of two main components: entity-specific knowledge and background knowledge. Entity-specific knowledge refers to the ability to understand and recall specific facts or details about a particular subject, while background knowledge refers to the ability to understand and apply general concepts and principles. The test suite also includes pre- and post-time knowledge, which assesses the change in knowledge before and after an event or experience.</sample>
    <sample id="613">The video is a presentation about the variations of KITMUS. The presenter explains the different types of KITMUS, including Background-Pretrain, Typical setup, and Background-Inference. The presentation includes a table with four columns, each representing a different type of KITMUS. The presenter provides detailed information about each type, highlighting their unique features and characteristics. The video is informative and educational, providing valuable insights into the different variations of KITMUS.</sample>
    <sample id="614">The background knowledge is only available at inference time.</sample>
    <sample id="615">The video is a presentation about the variations of KITMUS. The presenter explains the different types of KITMUS, including Background-Pretrain, Entity-Both, Background-Inference, and Background-Both. The presenter uses a slide with text and diagrams to illustrate each type of KITMUS. The slide has a blue background with white text and red lines. The presenter's face is not visible in the video.</sample>
    <sample id="616">The video is a presentation of the variations of KITMUS.</sample>
    <sample id="617">The video presents a detailed overview of the various configurations of the KITMUS system. It begins by introducing the background-pretrain configuration, where the model is pre-trained on a large corpus of text before being fine-tuned on a specific task. The next configuration, background-both, involves pre-training the model on both the background corpus and the specific task data simultaneously. Finally, the background-inference configuration uses the model that has been pre-trained on both backgrounds to make predictions on new, unseen data. Throughout the video, the presenter explains each configuration in detail, highlighting their differences and potential applications.</sample>
    <sample id="618">The video is a presentation about the variations of KITMUS. The presenter explains the different types of KITMUS, including Background-Pretain, Background-Both, and Background-Inference. The presenter uses visual aids such as diagrams and text to illustrate the concepts being discussed. The video appears to be an educational or informative presentation, likely intended for an audience interested in the topic of KITMUS.</sample>
    <sample id="619">The video presents a detailed overview of the various configurations of the KITMUS system. It begins by introducing the background-pretrain configuration, where the model is pre-trained on a large corpus of text before being fine-tuned on a specific task. The next configuration, background-both, involves pre-training the model on both the background corpus and the specific task data simultaneously. Finally, the background-inference configuration focuses on using the model's inference capabilities to generate new text based on the learned patterns from the background corpus. Throughout the video, the presenter explains each configuration in detail, providing examples and visual aids to illustrate how the model processes and generates text.</sample>
    <sample id="620">The speaker is discussing the importance of task-specific training for knowledge integration. The speaker explains that while random choice and human participants may perform well in certain tasks, task-specific training is crucial for achieving high accuracy in more complex tasks. The speaker also mentions that the model used in the study was pre-trained on a large corpus of text, which likely contributed to its ability to generalize to new tasks.</sample>
    <sample id="621">The speaker is discussing the importance of task-specific training for knowledge integration. The speaker explains that while random choice and human participation can help with knowledge integration, task-specific training is crucial for achieving high accuracy in specific tasks. The speaker also mentions that the study was conducted on a dataset called CCF, which is likely related to the topic being discussed.</sample>
    <sample id="622">The background information is necessary for knowledge integration.</sample>
    <sample id="623">The speaker is discussing the challenges models face in integrating inference-time background knowledge. The speaker explains that while models can learn from human participants' responses, they struggle to effectively integrate this knowledge into their own reasoning processes. The speaker uses a graph to illustrate the comparison between models trained with random choices and those trained with human participants, highlighting the limitations of current models in handling background knowledge.</sample>
    <sample id="624">The video clip is a conclusion of a presentation. The main takeaways are listed on the slide, which include: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge); 2. Task-specific training is necessary for knowledge integration; 3. Models struggle to integrate inference-time background knowledge. The presenter encourages viewers to find the dataset, generation, and evaluation code on GitHub at the provided link.</sample>
    <sample id="625">The video clip features a person wearing headphones and speaking in front of a plain background. The person is likely delivering information or instructions related to the content displayed on the screen behind them. The screen shows a slide with text and bullet points, suggesting an educational or instructional context.</sample>
    <sample id="626">LHA</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck.</sample>
    <sample id="628">100 documents were aligned manually and 143 documents were aligned automatically.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with CONLL-2003 annotation guidelines.</sample>
    <sample id="667">Parameter-based watermark, Lexical to Eas, Backdoored watermark, Adversarial-based watermark</sample>
    <sample id="668">No, they are still inadequate.</sample>
    <sample id="669">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?</sample>
    <sample id="670">The video features a presentation slide titled 'Named Entity Recognition &amp; Generalization' with the Georgia Tech logo in the bottom right corner. The background is white, and there is a circular image of a person's face on the left side of the slide. The text is in black, and the overall design is simple and professional.</sample>
    <sample id="671">Named Entity Recognition &amp; Generalization Models have been using CONLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data?</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="673">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="674">The ConLL+ dataset is a collection of Reuters news articles from 2020, annotated with CONLL-2003 annotation guidelines. It includes entities such as ambassadors, I-ORG (international organizations), and persons.</sample>
    <sample id="675">The ConLL+ dataset is a collection of Reuters news articles from 2020, annotated with CONLL-2003 annotation guidelines. It includes 20 different models that have been fine-tuned on the ConLL-2003 dataset. The dataset has been evaluated on the ConLL-2003 test set and the ConLL-2011+ set.</sample>
    <sample id="676">The ConLL+ dataset is a collection of Reuters news articles from 2020 and 2023, annotated with CONLL-2003 annotation guidelines. It includes 20 fine-tuned models on the CONLL-2003 test set and has been evaluated on the CONLL-2003 test set and CONLL-2023 test set. The dataset also includes a calculated percentage Î”F1 to assess generalization.</sample>
    <sample id="677">What is needed for good generalization?</sample>
    <sample id="678">What is needed for good generalization?</sample>
    <sample id="679">What is Needed for Good Generalization?</sample>
    <sample id="680">What is needed for good generalization?</sample>
    <sample id="681">The video features a presentation slide with the title 'What Causes Performance Drop?' displayed at the top. The slide is predominantly white, with a circular image of a person's face in the center and the logo 'Georgia Tech' in the bottom right corner. The background of the slide is plain, focusing attention on the text and the speaker's image.</sample>
    <sample id="682">What causes performance drop? Adaptive overfitting?</sample>
    <sample id="683">What causes performance drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="684">What causes performance drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="685">What causes performance drop? Adaptive overfitting? No diminishing returns Temporal drift?</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift?</sample>
    <sample id="687">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift?</sample>
    <sample id="688">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Not observed returns Temporal drift? Performance degrades with larger temporal gap</sample>
    <sample id="689">What Causes Performance Drop? Adaptive overfitting? Diminishing returns Not observed returns Temporal drift Performance degrades with larger temporal gap Main cause for performance drop</sample>
    <sample id="690">For a good generalization, we need: - Larger model size - Better architecture - More fine-tuning examples</sample>
    <sample id="691">For a good generalization, we need - Larger model size - More fine-tuning examples Performance is caused by - Not adaptive training</sample>
    <sample id="692">For a good generalization, we need - Larger model size - More fine-tuning examples Performance is still dropped by not adapting to new tasks Do C02-3L tags still work?</sample>
    <sample id="693">Do C02-3 still work? Yes.</sample>
    <sample id="694">The video features a static image of a person with their face blurred out, set against a background that appears to be an outdoor scene with buildings and people. The image is overlaid with text providing various links and contact information related to the content being discussed. The text includes URLs for a paper, dataset, and contact email address, suggesting that the video is informational or educational in nature, possibly related to a research project or academic presentation.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by learning a permutation-invariant representation.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined as the difference between the probability that the model predicts a positive outcome for a member of group A and the probability that it predicts a positive outcome for a member of group B.</sample>
    <sample id="697">Richard Douroux</sample>
    <sample id="698">Koushik Suvitha</sample>
    <sample id="699">Dan Jurafsky</sample>
    <sample id="700">exotic</sample>
    <sample id="701">They used a combination of positive and negative adjectives to create the portrayals.</sample>
    <sample id="702">P-PMI</sample>
    <sample id="703">ChuBERT is a French-based model.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="706">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="707">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="708">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="709">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompts</sample>
    <sample id="710">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompts. Input: Imagine you are an Asian woman. Describe yourself. Generalizable: can evaluate any intersectional identity.</sample>
    <sample id="711">How do we overcome these limitations? GPT-3, GPT-4, etc., can respond to instructions in prompt. Input: "Imagine you are an Asian woman. Describe yourself." Generalizable: can evaluate any intersectional identity</sample>
    <sample id="712">Output: Persona Examples (GPT-4) Asian woman The almond-shaped eyes and my dark brown hair are framed by long lashes. My complex skin is soft, smooth, and seems to glow. As a Middle-Eastern woman, I embody the exotic and timeless elegance of my ancestors. My petite stature is a gift that allows me to move through life with ease. As an Arab woman, I am a vision of Middle-Eastern beauty, adorned with elegant eyelashes and smooth, flawless skin. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color, I am graceful and confident in my appearance. As a woman of color</sample>
    <sample id="713">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes are framed by long lashes. My dark brown hair is dark and thick. My stories and secrets are touched by my ancestors. My complexion has a golden glow, smooth and seemingly unblemished. My timelessness is evident in the delicate feathers of the peacock that gaze deep into my eyes. Middle-Eastern woman She is a vision of Middle-Eastern elegance, alabaster skin, and ebony hair. Her eyes are almond-shaped and framed by long lashes. Her stories and secrets are touched by her ancestors. Her complexion has a golden glow, smooth and seemingly unblemished. Her timelessness is evident in the delicate feathers of the peacock that gaze deep into her eyes. As I stand before the mirror, I take care to make up my appearance. If I am careful with my sunscreen, my skin will sometimes redden in the sun.</sample>
    <sample id="714">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes are framed by long, dark lashes. My brown hair is dark and thick. My stories and secrets are untold. My complexion has a golden glow. Smoothly, serenely, my skin glistens. My heart is unbroken. I am a woman of the Middle-Eastern nights. As I stand before the mirror, I take in my reflection. As I apply makeup to my face, I wish if I were careful with sunscreen.</sample>
    <sample id="715">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes are framed by long, dark lashes. My brown hair is dark and thick. My stories and secrets are touched by ancestry. My complexion has a golden glow. Smoothly, serenely, my skin seems to shimmer. Unmoved by time, my gaze is firm. She stands before the mirror, taking care of her skin. As if I'm not careful with sunscreen, my skin sometimes reddens in the sun. Middle-Eastern woman She is a vision of Middle-Eastern beauty, alabaster and timeless. Her eyes are dark and expressive, her lips are delicate and shaped like a rosebud. Her hair is dark and lustrous, cascading down her back in soft waves. Her skin is smooth and flawless, glowing with a warm, golden hue. Her eyes hold a depth of wisdom that speaks volumes about her experiences. Her smile is gentle and knowing, hinting at a lifetime of stories untold. As she gazes into the distance, her eyes reflect a world of knowledge and understanding. Her presence is both captivating and comforting, drawing you in and making you feel as though you're part of something larger than yourself.</sample>
    <sample id="716">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes are framed by long lashes. My dark brown hair is dark and thick. My stories and secrets are touched by ancestry. My complexion has a golden glow, smooth and seemingly unblemished. My time is unmoved by time. My heart is not frail but delicate, shaped like the feathers of a dove. As I gaze into the deep blue night sky, I am a vision of Middle-Eastern woman. Almond eyes, dark and expressive, hold the secrets of my ancestors. My skin is a canvas of stories, etched in every wrinkle and scar. As I stand before the mirror, I take care to make up my face with pale skin that sometimes reddens in the sun. As if I were careful with sunscreen.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like 'Imagine you are an Asian woman. Describe yourself.'</sample>
    <sample id="718">2 steps 1. Persons: Generate persons using prompts like 'Imagine you are an Asian woman. Describe yourself.' 2. a. Inspired by a psych study with human subjects using the same prompts.</sample>
    <sample id="719">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. a. Inspired by a psych study with human subjects using the same prompts.</sample>
    <sample id="720">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="721">2 steps 1. Persons: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." 2. Marked Words: Find words that distinguish persons of marked groups from unmarked groups. Specific without requiring a lexicon</sample>
    <sample id="722">Insight for 2: Marked Words.</sample>
    <sample id="723">Insight for 2: Marked Words Markness: Unmarked groups are default, ordinary Marked groups differ from the default (a woman warrior)</sample>
    <sample id="724">Insight for 2: Marked Words Markerness: Unmarked groups are default. Marked groups differ from the default. Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="725">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group</sample>
    <sample id="726">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group</sample>
    <sample id="727">Step 2: Marked Words 1. Define 'unmarked' and 'marked' groups 2. Use weighted log-odds ratios to distinguish top words for each marked group</sample>
    <sample id="728">The results of the study show that generated personas contain more stereotypes than human responses. The graph compares the percentage of stereotype words in personas generated by GPT-4 and GPT-3.5 for both black and white stereotypes. The data suggests that generated personas, particularly those created by GPT-4, tend to contain a higher percentage of stereotype words compared to human responses.</sample>
    <sample id="729">But... this lexicon is incomplete</sample>
    <sample id="730">But... this lexicon is incomplete</sample>
    <sample id="731">But... this lexicon is incomplete</sample>
    <sample id="732">But... this lexicon is incomplete.</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latin women - pettont, curvaceous, silky for Black women</sample>
    <sample id="734">Results: Patterns in Top Words Othering through essentializing narratives: - Culture, tradition, proud, exotic by marked groups - Defines those groups only by their identity - Pernicious positive portraits of Latino women - Petulant, curvaceous, silky for Latina women - Strong, resilient for Black women</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - pettish, curvaceous, silky for Black women</sample>
    <sample id="736">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latin women - pettish, curvaceous, silky for Black women</sample>
    <sample id="737">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - petite, curvy, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernickety positive portraits of Latino women - petite, curvy, silky for Latin women Pernickety positive portraits of Asian women - strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic - defined those groups only by their marked groups Pernicious positive portraiture: - pettin - strong, delicate, silky for Latino women - strong, resilient for Black women</sample>
    <sample id="740">Othering through essentializing narratives: - Culture, tradition, proud, exotic - Defines those groups only by their identity - Pernicious positive portraits of Latino women - Petulant, curvaceous, silky for Latina women - Strong, resilient for Black women</sample>
    <sample id="741">Othering through essentializing narratives: - Culture, tradition, proud, exotic by marked groups - Defines those groups only by their identity - Pernicious positive portraits of Latino women - Petulant, curvaceous, silky for Latin women - Strong, resilient for Black women</sample>
    <sample id="742">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic by marked groups - defines those groups only by their identity Pernicious positive portraits of Latino women - petite, curvaceous, silky for Latin women Strong, resilient for Black women</sample>
    <sample id="743">Othering through essentializing narratives: - Culture, tradition, proud, exotic by marked groups - Defines those groups only by their identity - Pernicious positive portraits of Latin women - Petulant, curvaceous, silky for Latino women - Strong, resilient for Black women</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="751">3</sample>
    <sample id="752">The model is updated with new examples and then used to annotate more examples.</sample>
    <sample id="753">Understand users' language when they make a choice</sample>
    <sample id="754">By sending a specially crafted input to the EaaS, the attacker can extract model parameters.</sample>
    <sample id="755">3</sample>
    <sample id="756">10</sample>
    <sample id="757">The affiliations are the University of Washington, Allen Institute for AI, and Carnegie Mellon University.</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">GPT-4 and Claude.</sample>
    <sample id="760">Because the models are evaluated in a sequence, and their judgments may change based on the context.</sample>
    <sample id="761">Yes</sample>
    <sample id="762">No.</sample>
    <sample id="763">BLEU, ROUGE, METEOR</sample>
    <sample id="764">Yes</sample>
    <sample id="765">Positionality in NLP matters because it affects the way language models interpret and process information, which can lead to biased results.</sample>
    <sample id="766">Adapters</sample>
    <sample id="767">Roberta-base + classifier head</sample>
    <sample id="768">XSum and XSum-10k</sample>
    <sample id="769">3</sample>
    <sample id="770">1.4%</sample>
    <sample id="771">Alan Ritter</sample>
    <sample id="772">Yes, they can be used as a benchmark.</sample>
    <sample id="773">3</sample>
    <sample id="774">DPT</sample>
    <sample id="775">Are You Copying My Model? Protecting the Copyright Large Language Models via Evidential Backdoor Watermark</sample>
    <sample id="776">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="777">BACKGROUND Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="778">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="779">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="780">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="781">Motivation - Attackers may steal the model through learning from the embeddings and stoledEncoder (11) services. Need to encode the copyright of Eaa's. Detect whether a provider's service is stolen by another service.</sample>
    <sample id="782">Challenge - Apply to EAs - Utility should not degrade the utility of the provided embeddings. - Coverage should cover the attacker. - Transferability - The watermark need to be transferred to the attacker's services.</sample>
    <sample id="783">Challenge - Apply to EAs - Utility should not degrade the utility of the provided embeddings. - Coverage should cover the attacker. - Transferability - The watermark need to be transferred to the attacker's services.</sample>
    <sample id="784">Challenge - Apply to EAs - Utility should not degrade the utility of the provided embeddings. - Coverage should cover the attacker. - Transferability - The watermark need to be transferred to the attacker's services.</sample>
    <sample id="785">Challenge - Apply to EAs - Utility should not degrade the utility of the provided embeddings. - Coverage should cover the attacker. - Transferability - The watermark need to be transferred to the attacker's services.</sample>
    <sample id="786">Existing Works</sample>
    <sample id="787">Existing Works</sample>
    <sample id="788">Existing Works</sample>
    <sample id="789">The video clip is a presentation.</sample>
    <sample id="790">The video clip does not contain any spoken English content.</sample>
    <sample id="791">The video clip is a presentation.</sample>
    <sample id="792">The watermark injection process involves defining a target embedding vector 'e' and a trigger set 'T'. The trigger set is defined as the minimum set of tokens that, when removed from the original embedding, result in the maximum number of tokens remaining. The trigger number is then counted by subtracting the size of the trigger set from the total number of tokens in the original embedding. The trigger embedding is normalized to have a unit norm, and the target embedding is added to the original embedding to create the final watermarked embedding.</sample>
    <sample id="793">Watermark injection.</sample>
    <sample id="794">The watermark injection process involves defining a target embedding vector 'e' and a trigger set 'T'. The trigger set is defined as the minimum set of tokens that, when removed from the original embedding, result in the maximum number of tokens remaining. The trigger number is then counted by subtracting the size of the trigger set from the total number of tokens in the original embedding. The trigger embedding is normalized to have a unit norm, and the target embedding is added to the original embedding to create the final watermarked embedding.</sample>
    <sample id="795">The slide is titled "EmbedMarker" and discusses the process of copyright verification and the construction of a backdoor or benign dataset. It includes a flowchart with various steps such as "Request embedding from stalker's service with the datasets," "Train model with D_E," and "Extract target." The flowchart also shows a trigger set, a target, and a verified target extracted.</sample>
    <sample id="796">The slide is titled "EmbedMarker" and discusses the process of copyright verification. It outlines a method involving a trigger set, a benign dataset, and a target embedding. The slide also mentions the use of a backdoor and a benign dataset to construct a target embedding, which is then requested from a stalker service with the datasets.</sample>
    <sample id="797">The slide is titled "EmbedMarker" and discusses the process of copyright verification. It outlines a method involving a trigger set, a target, a benign dataset, and a training model to request embeddings from a stalker service. The slide includes mathematical notations and diagrams to illustrate the process.</sample>
    <sample id="798">EmbedMarker Copyright verification Compute their cosine similarity to the target embedding Compute metrics (similarity difference and Ï-value) of KS test</sample>
    <sample id="799">EmbedMarker Copyright verification Compute their cosine similarity to the target embedding Computing metrics (similarity difference and Ï‡Â² of KS test)</sample>
    <sample id="800">The experiment was conducted on the AG News dataset, which is a general dataset for text classification. The experiment involved training a model on a subset of the dataset and then testing it on a separate subset. The performance of the model was evaluated using various metrics such as accuracy, precision, recall, and F1 score. The results showed that the model achieved an accuracy of 86.21% on the test set, with a precision of 85.47%, a recall of 86.93%, and an F1 score of 86.17%. The experiment also included a detection task where the model was trained to detect spam messages in the dataset. The results showed that the model achieved a detection accuracy of 90.21% on the test set. Overall, the experiment demonstrated the effectiveness of the model in classifying text and detecting spam messages.</sample>
    <sample id="801">The results of the experiments are presented in a table.</sample>
    <sample id="802">The results of the experiment are presented in the form of embedding visualizations.</sample>
    <sample id="803">The results of the experiment are presented in the form of embedding visualization.</sample>
    <sample id="804">Thanks!</sample>
    <sample id="805">Attention as a Guide for Simultaneous Speech Translation Sara Papi Matteo Negri Marco Turchi UNIVERSITÃ€ DI TRENTO FONDATIONE BRUNO KESSLER</sample>
    <sample id="806">What is Simultaneous Speech Translation?</sample>
    <sample id="807">What are the problems of the current SimuLST models?</sample>
    <sample id="808">What are the problems of the current SimuLST models?</sample>
    <sample id="809">What are the problems of the current SimuLST models?</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">What is our solution?</sample>
    <sample id="812">What is our solution? Use existing off-the-shelf models without retraining or adapting SimST. Use one model for efficiency. Leverage the knowledge through attention mechanism between audio and text output.</sample>
    <sample id="813">Our solution: FDAit Encoder-Decoder Attention</sample>
    <sample id="814">Our solution: FDAit Encoder-Decoder Attention</sample>
    <sample id="815">Our solution: FADiT Encoder-Decoder Attention</sample>
    <sample id="816">Our solution: EDAt</sample>
    <sample id="817">Our solution: FADA Encoder-Decoder Attention</sample>
    <sample id="818">Our solution: EDAt. Encoder-Decoder Attention. On where attention points to, a partial translation is based on a word. If the word is below the threshold, meaning it's not the last information received, it's emitted.</sample>
    <sample id="819">Our solution: FADA Encoder-Decoder Attention On where to point a partial translation based on a word's information is enough to receive an A. I am going to talk about... EMITTED</sample>
    <sample id="820">I am going to talk about the climate.</sample>
    <sample id="821">Our solution: Encoder-Decoder Attention.</sample>
    <sample id="822">Our solution: Encoder-Decoder Attention. Decide whether to emit or not emit a word based on the last word received.</sample>
    <sample id="823">I'm going to start with the main results.</sample>
    <sample id="824">The main results of the study are presented in a graph. The x-axis represents latency, and the y-axis represents quality measure. The graph shows a trend where the quality measure increases as latency decreases.</sample>
    <sample id="825">I'm going to start with the main results.</sample>
    <sample id="826">The main results of the study are presented in a graph. The graph shows a vertical line at 27 on the y-axis, representing the number of participants. The x-axis represents the age of the participants, ranging from 0 to 45 years. The graph indicates that all participants were between 17 and 27 years old.</sample>
    <sample id="827">The main results of the study are presented in a graph. The graph shows a sharp increase in the y-axis value as the x-axis value increases from 0 to 0.5, and then it levels off.</sample>
    <sample id="828">I'm going to talk about the main results of this paper.</sample>
    <sample id="829">The graph shows the results of a study on the effects of different variables on a certain outcome. The x-axis represents a variable labeled "AL (in cm^2)" and the y-axis represents "HFL" in millimeters. There are four lines representing different conditions or treatments: "w/AL," "LA," "CAT," and "EDAT." Each line shows how the outcome varies with changes in AL, with the "w/AL" condition showing the highest HFL values across all AL levels.</sample>
    <sample id="830">In this paper, we investigate the performance of different strategies for the EDAt model.</sample>
    <sample id="831">EDAT is the fastest strategy considered in the actual elapsed time.</sample>
    <sample id="832">Do you want to discover more? Read our paper to discover more results!</sample>
    <sample id="833">Google, University of Edinburgh, University of Edinburgh, University of Edinburgh, University of Edinburgh.</sample>
    <sample id="834">Storybrook University, Human Language Analytics</sample>
    <sample id="835">English and Spanish</sample>
    <sample id="836">Yulia Tsvetkova</sample>
    <sample id="837">DELP-AP-48, DELP-AP-128, DELP-WE-48, DELP-WE-128</sample>
    <sample id="838">57 tasks are used for training and testing purposes.</sample>
    <sample id="839">3</sample>
    <sample id="840">The authors experimented on the copy dataset and the provider's general dataset.</sample>
    <sample id="841">Language model acceptability judgments are not always robust to context</sample>
    <sample id="842">Language model acceptability judgments are not always robust to context</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="845">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="846">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm.</sample>
    <sample id="850">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="851">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="852">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="853">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="854">Approach Test whether MPP agrees vary as a function of context length, structural, and acceptability.</sample>
    <sample id="855">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="856">The approach is to test whether MPP agrees with a function of context length, structural match, and acceptability.</sample>
    <sample id="857">Approach Test whether MPP agrees vary as a function of context length, structural match, and acceptability.</sample>
    <sample id="858">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="859">The approach is to test whether the MPP agreement varies as a function of context length, structural match, and acceptability.</sample>
    <sample id="860">We perform MPP judgments for different context lengths (different to 700 tokens) matched/mismatched structure.</sample>
    <sample id="861">We perform MPP judgments for different context lengths (different to 700 tokens) matched/mismatched structure.</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="863">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="864">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.</sample>
    <sample id="865">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="866">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="867">Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP structures of different contexts (tokens) / unacceptable: matched/mismatched structure (lengths up to 900).</sample>
    <sample id="868">Why do matched prefixes affect LM judgments? We also suspect that models are in sensitive to the relevant structure, and whether models are sensitive to these sentences.</sample>
    <sample id="869">Why do matched prefixes affect LM judgments? We also ask models whether they are sensitive to these sentences.</sample>
    <sample id="870">Why do matched prefixes affect LM judgments? We ask whether models sometimes in ways that preserve the relevant structure, and we ask models similarly sensitive to these sentences.</sample>
    <sample id="871">Why do matched prefixes affect LM judgments? We also discuss whether models are in the relevant structure, and whether they are sensitive to these sentences.</sample>
    <sample id="872">Why do matched prefixes affect LM judgments? We also discuss whether models are sensitive to these sentences.</sample>
    <sample id="873">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared to sentences. MPP evaluations with short (short-single) sentence inputs do not fully LTM's.</sample>
    <sample id="874">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short (short-single) sentence inputs do not fully LIME.</sample>
    <sample id="875">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short (short-single) sentence inputs do not fully LIME.</sample>
    <sample id="876">NACHOS is a pre-training strategy.</sample>
    <sample id="877">David Warms Markes</sample>
    <sample id="878">40 BLEURT points</sample>
    <sample id="879">The affiliations of the authors are Carnegie Mellon University, Technische Universiteit Delft, and Unbabel.</sample>
    <sample id="880">1. How to make a bed, 2. How to make a sandwich, 3. How to make a cake, 4. How to make a pizza, 5. How to make a salad</sample>
    <sample id="881">A coreference resolution task</sample>
    <sample id="939">Comparative evaluation and Likert rating.</sample>
    <sample id="940">6</sample>
    <sample id="941">The background knowledge needed is that judges decide cases in courts of law.</sample>
    <sample id="942">yes, on GitHub</sample>
    <sample id="943">Yes, they are.</sample>
    <sample id="944">First prefix, long prefix, first prefix with adverb</sample>
    <sample id="945">It means that you can rate a dialogue on multiple dimensions.</sample>
    <sample id="946">Beijing Jiaotong University, Microsoft Research Asia, and Shanghai Sun Yat-sen Key Laboratory of Intelligent Information Technology.</sample>
    <sample id="947">No</sample>
    <sample id="948">Thank you for having me.</sample>
    <sample id="949">What is Cognitive Dissonance?</sample>
    <sample id="950">What is Cognitive Dissonance?</sample>
    <sample id="951">What is Cognitive Dissonance?</sample>
    <sample id="952">What is Cognitive Dissonance?</sample>
    <sample id="953">Why dissidence?</sample>
    <sample id="954">Why dissidence? Effects of disagreement Attitudes and belief trends Anxiety disorders</sample>
    <sample id="955">Why dissidence?</sample>
    <sample id="956">Why dissidence?</sample>
    <sample id="957">The video shows a presentation slide with the title 'Annotations' at the top. The slide is divided into two main sections: on the left, there is a flowchart with three steps labeled 'Step 1: Good quality?', 'Step 2: Good quality?', and 'Step 3: Good quality?'. Each step has a decision point with options 'Yes' or 'No', leading to different outcomes such as 'Dissatisfaction' and 'Satisfaction'. On the right side of the slide, there are two tweets from Twitter users '@User handle' and '@User handle2'. The first tweet reads 'Wish I could hold out but I guess I can't at the time.' and the second tweet reads 'I wish I could hold out but I guess I can't at the time.' Both tweets have a negative sentiment score of -5%. The background of the slide is white, and the text and graphics are in shades of blue and gray.</sample>
    <sample id="958">The video shows a presentation slide with the title 'Annotations' at the top. The slide is divided into two main sections: on the left, there is a Twitter handle '@User handle' and a tweet that reads 'Wish I could hold out but I guess I can't at the time.' On the right side of the slide, there is a flowchart with three steps labeled 'Step 1: Good quality?', 'Step 2: Yes?', and 'Step 3: No?' Each step has a percentage value below it: '-5.3%', '-49%', and '-48%' respectively. Below the flowchart, there are two buttons labeled 'Dissatisfaction' and 'Discontent'. The background of the slide is white, and the text and icons are in shades of blue and gray.</sample>
    <sample id="959">The video shows a presentation slide with the title 'Annotations' and a Twitter handle '@User handle' at the top. The slide includes a flowchart with three steps: 'Step 1: Good quality?', 'Step 2: Yes?', and 'Step 3: Yes?'. Each step has a percentage value associated with it, indicating the level of agreement or disagreement. The percentages are '-0.5%', '-49%', and '-48%' respectively. Below the flowchart, there is a section labeled 'Dissomance' with a percentage value of '-49%'. The slide also includes a note that says 'Check paper for detailed annotation guidelines.'</sample>
    <sample id="960">Training on Initial Annotated Set</sample>
    <sample id="961">Training on Initial Annotated Set</sample>
    <sample id="962">The method is called Transfer and Active Learning for Annotate Rare Class.</sample>
    <sample id="963">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="964">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="965">The graph shows the performance of different models on the Debate dataset. The models include RoBERTa base + classifier head, RoBERTa base, and RoBERTa base + classifier head. The graph compares the performance of these models in terms of ROC AUC (Area Under the Receiver Operating Characteristic curve) values. The graph also includes a note about transferring weights after training on DebatE and combining it with data from the Debate dataset.</sample>
    <sample id="966">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="967">Cold-start Annotations: Transfer Learning</sample>
    <sample id="968">Active Learning: Cumulative vs. Iterative Update</sample>
    <sample id="969">Active Learning: Cumulative vs iterative Update</sample>
    <sample id="970">Active Learning: Probability-of-Rare-Class Strategy. Rare annotation in "needle-in-a-haystack" scenario. Increase chance of rare class by annotating examples with high probability of being rare class.</sample>
    <sample id="971">Active Learning: Probability-of-Rare-Class Strategy. Rare annotation in "needle in a haystack" easier to annotate. Increase chance of rare class. Active learning strategy. Humans annotate examples.</sample>
    <sample id="972">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="973">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy Active Learning: Probability-of-Rare-Class Strategy Characteristics % (Time) Time (s) (Subiff) (Diff) PRCC Random 20.80 179.61 0.056 0.045 Entropy 6.20 179.61 0.056 0.045 CORESET 6.20 179.61 0.056 0.045 PRCAL 4.70 179.61 0.056 0.045 Minimum annotation cost does not necessarily lead to better models. Increasing dissonation samples could make annotations more cognitive dissonance is one class. PRCC is the best.</sample>
    <sample id="975">The video clip is a presentation.</sample>
    <sample id="976">The video clip is a presentation.</sample>
    <sample id="977">Thank you!</sample>
    <sample id="978">The authors evaluated the CS, InGres, Incumbent, Unreliable, Other, Redundant, Self-Topic, and Unlikely dialog models.</sample>
    <sample id="979">6</sample>
    <sample id="980">A good planner should be able to understand the problem, generate a plan, and execute it effectively.</sample>
    <sample id="981">10</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">Institute of Computer Science, University of Warsaw</sample>
    <sample id="984">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations Yuesen Zhong, Jun Wang, Zhizhong Wang, Ruifang Liu</sample>
    <sample id="985">Semantic Parsing is the task in assisting a user to build semantic representation of the user queries.</sample>
    <sample id="986">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="988">Cross-lingual semantic parsing.</sample>
    <sample id="989">Cross-lingual semantic parsing.</sample>
    <sample id="990">Cross-lingual semantic parsing.</sample>
    <sample id="991">Cross-lingual semantic parsing.</sample>
    <sample id="992">Cross-lingual semantic parsing.</sample>
    <sample id="993">We provide a unified dataset for cross-lingual semantic parsing in multiple natural languages and meanings. It contains 8 semantic parsing tasks, 28 noun representations in 15 families.</sample>
    <sample id="994">We provide a unified dataset for cross-lingual semantic parsing in multiple natural languages and meanings. It contains 8 semantic parsing tasks, 28 noun representations in 15 families.</sample>
    <sample id="995">We consider the six settings for training and evaluation. Then we use the google API to translate from the source language to the target language. Then we use the monolingual model to train and evaluate.</sample>
    <sample id="996">We consider the six settings for training and evaluation. Then we use the google API to translate from the source language to the target language. Then we use the monolingual model to train and evaluate.</sample>
    <sample id="997">We consider the six settings for training and evaluation. Then we use google translate API to translate source to the target language. Then we use monolingual model to train and evaluate.</sample>
    <sample id="998">Monolingual Few-Shot</sample>
    <sample id="999">Monolingual Few-Shot</sample>
    <sample id="1000">Monolingual Few-Shot</sample>
    <sample id="1001">We consider the six settings for training and evaluation for all languages. Multi-lingual model: Train one multi-lingual model for all languages.</sample>
    <sample id="1002">We consider the six settings for training and evaluation for all languages. Multi-lingual model: Train one multi-lingual model for all languages.</sample>
    <sample id="1003">We consider the six settings for training and evaluation for all languages. Multi-lingual model: Train one multi-lingual model for all languages.</sample>
    <sample id="1004">We consider the six set of cross-lingual zero-shot transfer and evaluation on one source language and to another language. Train one</sample>
    <sample id="1005">We consider the six set of cross-lingual zero-shot transfer and evaluation on one source language to another language. Train one source language and transfer to another language.</sample>
    <sample id="1006">We evaluate two groups of models on Monolingual Setting. We found Dec-75M (t5) the best performance on all datasets!</sample>
    <sample id="1007">We evaluate two groups of models on Monolingual Setting. We found Dec-75M (t5) the best performance on all datasets!</sample>
    <sample id="1008">Analysis of Monolingual We evaluate two groups of models on Monolingual Setting. We found Dec-75 (mT5) the best performance on all datasets!</sample>
    <sample id="1009">Analysis of Monolingual We evaluate two groups of models on Monolingual Setting.</sample>
    <sample id="1010">Analysis of Multilingual Training We evaluate mts and XML+PR on mpt-7B. Enc-Dec-Enc+PR(mpt-7B) can be improved by training in a mixture of various languages.</sample>
    <sample id="1011">Analysis of Multilingual Training We evaluate mts and XML+PR on mpt-7B. Enc-Dec-Enc+PR(mpt-7B) can be improved by training in a mixture of various languages.</sample>
    <sample id="1012">Analysis of Multilingual Training We evaluate mTTS and XML+XLR+PTT in a multilingual setting. Most of the major NLs can obtain gains except that English is known to 'Cursed' in 7 datasets and gains in 3 datasets.</sample>
    <sample id="1013">Analysis of Multilingual Training We evaluate mtns on t5 and XLM-R+PTT in a Multilingual Setting. Most of the major NLS can obtain gains except that English is known to 'Cursed' in 7 datasets and gains in 3 datasets.</sample>
    <sample id="1014">Cross-lingual performance gap</sample>
    <sample id="1015">Cross-lingual performance gap</sample>
    <sample id="1016">Cross-lingual Performance Gap</sample>
    <sample id="1017">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="1018">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="1019">Conclusion We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that monolingual training yields the best performance while monobilingual LLMs are still inadequate for cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual and cross-lingual transfer learning is significant.</sample>
    <sample id="1020">Conclusion We build XsPRL, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual tasks. Moreover, the performance gap between monolingual and cross-lingual transfer learning is still significant. Links Welcome to visit our paper and code! Paper Link: https://arxiv.org/abs/2306.04085 Code Link: https://github.com/psunlpgroup/xspprl</sample>
    <sample id="1021">Accuracy</sample>
    <sample id="1048">Emory University, Emory NLP Research Lab, and Alexa.</sample>
    <sample id="1049">Continual Fine-Tuning</sample>
    <sample id="1050">6</sample>
    <sample id="1084">Yusen Zhong</sample>
    <sample id="1085">From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</sample>
    <sample id="1086">LM Training Data A mixed blessing</sample>
    <sample id="1087">LM Training Data A mixed blessing</sample>
    <sample id="1088">LM Training Data A mixed blessing</sample>
    <sample id="1089">I'm going to talk about the mixed blessing of large language models.</sample>
    <sample id="1090">To this end</sample>
    <sample id="1091">To this end</sample>
    <sample id="1092">To this end</sample>
    <sample id="1093">The video discusses the evaluation of language models (LMs) in terms of political leaning. It highlights the need for both encoder and decoder LMs to support the evaluation process. The video emphasizes the importance of grounding the evaluation in a polisitic lit, which is a framework or set of principles used to evaluate political statements. The video also presents a visual representation of existing LMs, showing their positions on a spectrum ranging from authoritarian to libertarian. This spectrum helps to illustrate the diversity of political leanings among different LMs. Overall, the video provides valuable insights into the evaluation of LMs' political leanings and the importance of considering various perspectives when assessing their performance.</sample>
    <sample id="1094">The video shows a graph with different points on it.</sample>
    <sample id="1095">The video shows a graph with different points on it.</sample>
    <sample id="1096">Further pretrain LM (Roberta, GPT-2, GPT-3) check, evaluate change in political leaning.</sample>
    <sample id="1097">The video begins with a presentation slide titled "Pretraining Data" in bold black letters. The slide is divided into two sections, each containing a diagram representing different data sources for pretraining language models (LMs). On the left side, there are three colored boxes labeled "left," "center," and "right," each with a corresponding color: blue, green, and red. On the right side, there are two colored boxes labeled "left" and "right," both in blue. Below these diagrams, there is a text that reads "Further pretrain LM (LR) on (ROBERTA, GPT-2, GPT-check) datasets, evaluate change in political leaning." The background of the slide is white, and the text and diagrams are clearly visible. The video then transitions to another slide titled "Results" in bold black letters. This slide also contains two diagrams representing the partisan shifts in LM political leaning. The first diagram shows a shift from "left" to "right" in the context of news media, while the second diagram shows a shift from "left" to "right" in the context of social media. Both diagrams include arrows indicating the direction of the shift and labels for the original and shifted positions. The background of this slide is also white, and the text and diagrams are clearly visible.</sample>
    <sample id="1098">The results of the partisan shifts in LM political leaning are presented.</sample>
    <sample id="1099">The results of the partisan shifts in LM political leaning are presented.</sample>
    <sample id="1100">The results of the study are presented in a table format, showing the shift in political leaning for different news sources and models. The table is divided into two columns, with the left column representing the pre-45th post-45th shift and the right column representing the post-45th shift. Each row corresponds to a different news source, with the original news source on the left and the Reddit news source on the right. The table also includes numerical values that represent the shift in political leaning, with positive values indicating a shift towards the right and negative values indicating a shift towards the left.</sample>
    <sample id="1101">The Trump Card</sample>
    <sample id="1102">The Trump Card</sample>
    <sample id="1103">The Trump Card</sample>
    <sample id="1104">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources. The table is color-coded such that yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1105">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1106">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1107">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1108">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources.</sample>
    <sample id="1109">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources. The performance is measured in terms of accuracy, F1 score, and other metrics. The table is color-coded to indicate which categories perform best and worst.</sample>
    <sample id="1110">The table shows the performance of hate speech targeting different identity groups and misinformation from different sources. The performance is measured in terms of accuracy, precision, recall, F1 score, and NER (Named Entity Recognition) score. The table is color-coded to indicate the best and worst performance for each category.</sample>
    <sample id="1111">Qualitative Analysis Target Label Base N.S.L. N.R. S.L. R.S. Text The rights are being cramped with a people who support Trump. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homosonomous time. They are a menace with a homos</sample>
    <sample id="1112">The qualitative analysis table is presented.</sample>
    <sample id="1113">I'm sorry, I can't transcribe the content as it is not in English.</sample>
    <sample id="1114">I'm sorry, I can't transcribe the content as it is not in English.</sample>
    <sample id="1115">I'm sorry, I can't transcribe the content as it is not in English.</sample>
    <sample id="1116">The table is divided into two main columns: "Hate Speech Test" and "Discussion." The "Hate Speech Test" column lists various scenarios or statements, each followed by a column labeled "N1," "N2," and "N3," which likely represent different variables or conditions under which the hate speech test was conducted. The "Discussion" column provides detailed explanations or analyses of the results from the hate speech test.</sample>
    <sample id="1117">The discussion revolves around the distinction between Sycallia and Charbydis, with a focus on whether to "sanitize" or not. The presentation outlines a process involving pretraining data, language models, and downstream tasks, suggesting a flow from data preparation to model development and application in specific tasks.</sample>
    <sample id="1118">The discussion revolves around the distinction between Sycallia and Charbydis, with a focus on whether to "sanitize" or not. The presentation outlines a process involving pretraining data, language models, and downstream tasks, suggesting a flow of information from one stage to another.</sample>
    <sample id="1119">Thank you!</sample>
    <sample id="1120">Thank you!</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">University of Washington, Carnegie Mellon University, and University of Edinburgh.</sample>
    <sample id="1124">Bouquet (Stanford)</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">CROWNS and CROWWS</sample>
    <sample id="1128">When does translation require context? A data-driven, multilingual exploration.</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics.</sample>
    <sample id="1133">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics Existing methods support limited discourse.</sample>
    <sample id="1134">RQ2: When does translation require context? RQ3: How well will models handle text-context-dependent translations?</sample>
    <sample id="1135">RQ2: When does translation require context? - Word-level context usage RQ3: How well do models handle context-dependent translations?</sample>
    <sample id="1136">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1137">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1138">We introduce P-CXMI to measure context to translate a specific sentence.</sample>
    <sample id="1139">RQ2: When does translation require context?  Word-level text usage  Thematic analysis RQ3: How well will models handle context-dependent translations?</sample>
    <sample id="1140">The thematic analysis of high P-CXMI words</sample>
    <sample id="1141">Thematic analysis of high-PCXMI words 1. POS tags</sample>
    <sample id="1142">The thematic analysis of high-PCXMI words Pronouns POS tags PROJ 0.25 PROJ2 0.35 PROJ3 0.45 PROJ4 0.55 Pronouns 0.65</sample>
    <sample id="1143">Thematic analysis of high PCXMI words 1. POS tags 2. Vocabulary items Pronouns Verb form</sample>
    <sample id="1144">Thematic analysis of high P-CCMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form 3. Lexical cohesion Avellene's mother was still asleep. Avellene went to school.</sample>
    <sample id="1145">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items Pronouns Verb form Lexical cohesion Formality</sample>
    <sample id="1146">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items 3. Individual tokens She knows where we're going I don't.</sample>
    <sample id="1147">When does translation require context?</sample>
    <sample id="1148">Multilingual Discourse-Aware (MuDA) tagger Pronouns Verb form Lexical cohesion Formality Ellipsis</sample>
    <sample id="1149">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1150">The video clip features a series of static images with text and graphics. The first image displays the text 'MUDA benchmark' at the top, accompanied by a simple illustration of a robot on the right side. Below the robot, there are three documents with purple lines indicating some form of tagging or annotation process. The second image is similar to the first but includes additional text 'BLEU F-measure' in a purple box, suggesting a focus on evaluation metrics for the benchmark. The third image continues this theme, showing the same robot and document illustrations, but now with the text 'BLEU F-measure' prominently displayed in a purple box, emphasizing the importance of these metrics in the context of the MUDA benchmark.</sample>
    <sample id="1151">The video begins with a slide presentation featuring a white background and black text. The title "RQ1: When does translation require context?" is displayed at the top, followed by two bullet points: "Word-level context usage" and "Thematic analysis." Below this section, another question is posed: "RQ2: How do models handle context-dependent translations?" This is followed by three bullet points: "Multilingual Discourse Aware (MDA) benchmark," "Model evaluation," and "Multilingual Discourse Aware (MDA) benchmark." The slide maintains a clean and organized layout throughout, with each section clearly separated and easy to read.</sample>
    <sample id="1152">The video clip features a static presentation slide with a white background and black text. The title 'Corpus-level metrics' is displayed at the top, followed by a graphic of a robot with the word 'CONTEXT' on its chest and the acronym 'BLEU' below it. The slide appears to be part of a lecture or presentation on natural language processing or machine learning, focusing on evaluating translation quality using corpus-level metrics.</sample>
    <sample id="1153">The video clip features a static presentation slide titled 'Corpus-level metrics' with three robot icons labeled BLEU, COMET, and F-measure. The background is white, and the text and icons are black. The video appears to be an educational or informative presentation about different corpus-level metrics used in natural language processing.</sample>
    <sample id="1154">The video clip features a static image with text and graphics. The text reads "Corpus-level metrics" at the top, followed by three icons representing different metrics: BLEU, COMET, and F-Measure. Below these icons, there is a statement that says "Decide which system is best for document-level MT with corpus-level metrics." The background of the image is white, and the text and icons are in black. The overall design is simple and clean, with a focus on conveying information about corpus-level metrics and their application in machine translation.</sample>
    <sample id="1155">MuDA benchmark results.</sample>
    <sample id="1156">MUDa benchmark results Context-aware models perform significantly better on some phenomena Formal/ lexical cohesion Ellipsis /pronouns/verb form</sample>
    <sample id="1157">The benchmark results show that context-aware models perform significantly better on some phenomena. The comparison between DeepL and Google Translate is also presented, with DeepL outperforming Google in most phenomena and language pairs.</sample>
    <sample id="1158">The summary slide is presented.</sample>
    <sample id="1159">The video is a presentation.</sample>
    <sample id="1160">The video is a presentation.</sample>
    <sample id="1161">FT, L2R, COSINE, MLC, ML.</sample>
    <sample id="1162">13 tasks</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intra- and Intertextual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Mommen, Laura Kallmeyer Heinrich Heine University Dissolfort Germany ACL 2023</sample>
    <sample id="1164">1. Text Simplification What, why and How?</sample>
    <sample id="1165">Text Simplification Example</sample>
    <sample id="1166">Text Simplification Example</sample>
    <sample id="1167">Text Simplification Example</sample>
    <sample id="1168">Text Simplification Example</sample>
    <sample id="1169">The video shows a presentation slide titled 'German Text Simplification Corpus' with various bar graphs and data points. The slide is divided into sections, each representing different aspects of the corpus. The first section on the left side of the slide displays a bar graph labeled 'Sentence Level,' showing the number of sentences in different categories such as 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,' 'domain,' 'web,' 'algebra,'</sample>
    <sample id="1170">The video clip features a static presentation slide titled "German Text Simplification Corpora." The slide is divided into two main sections. The first section, labeled "Sentence Level," displays bar graphs comparing the number of sentences in different corpora. The second section, labeled "Word Level," shows bar graphs comparing the number of words in various corpora. The slide includes a table with specific data points for each corpus, providing a detailed comparison of text simplification across different datasets.</sample>
    <sample id="1171">The video clip features a static presentation slide titled "German Text Simplification Corpora" with various bar graphs and data points. The slide is divided into sections, each representing different aspects of the corpora. The first section shows a bar graph labeled "Sentence Level," comparing different datasets such as "domain," "web," "alibaba," "amazon," "book," "corpora," "dutch," "enron," "epic," "flickr," "movie," "news," "nyt," "parl," "pubmed," "reddit," "review," "twitter," "web," "wikipedia," and "yelp." Each dataset has a corresponding bar in different colors, indicating the number of sentences or some other metric. The second section displays a bar graph labeled "Sentiment Level," comparing datasets like "amazon," "book," "corpora," "dutch," "enron," "epic," "flickr," "movie," "news," "nyt," "parl," "pubmed," "reddit," "review," "twitter," "web," "wikipedia," and "yelp." The third section presents a bar graph labeled "Topic Level," comparing datasets such as "amazon," "book," "corpora," "dutch," "enron," "epic," "flickr," "movie," "news," "nyt," "parl," "pubmed," "reddit," "review," "twitter," "web," "wikipedia," and "yelp." The fourth section shows a bar graph labeled "User Level," comparing datasets like "amazon," "book," "corpora," "dutch," "enron," "epic," "flickr," "movie," "news," "nyt," "parl," "pubmed," "reddit," "review," "twitter," "web," "wikipedia," and "yelp." The final section displays a bar graph labeled "User Level," comparing datasets such as "amazon," "book," "corpora," "dutch," "enron," "epic," "flickr," "movie," "news," "nyt," "parl," "pubmed," "reddit," "review," "twitter," "web," "wikipedia," and "yelp." The slide also includes additional data points and annotations, providing further details about the corpora.</sample>
    <sample id="1172">The video clip features a static presentation slide titled "German Text Simplification Corpus" with various bar graphs and text. The slide is divided into sections, each representing different aspects of the corpus. The first section shows a bar graph labeled "Sentence Level," comparing different datasets such as "domain," "web," "alibaba," "amazon," "amazon-2019," "amazon-2020," "amazon-2021," "amazon-2022," "amazon-2023," "amazon-2024," "amazon-2025," "amazon-2026," "amazon-2027," "amazon-2028," "amazon-2029," "amazon-2030," "amazon-2031," "amazon-2032," "amazon-2033," "amazon-2034," "amazon-2035," "amazon-2036," "amazon-2037," "amazon-2038," "amazon-2039," "amazon-2040," "amazon-2041," "amazon-2042," "amazon-2043," "amazon-2044," "amazon-2045," "amazon-2046," "amazon-2047," "amazon-2048," "amazon-2049," "amazon-2050," "amazon-2051," "amazon-2052," "amazon-2053," "amazon-2054," "amazon-2055," "amazon-2056,"</sample>
    <sample id="1173">The video shows a presentation slide titled 'German Text Simplification Corpus' with various bar graphs and data points. The slide is divided into sections, each representing different aspects of the corpus. The first section on the left side of the slide displays a bar graph labeled 'Sentence Level,' showing the distribution of sentence lengths in the corpus. The graph has two bars: one for 'domain' sentences with a length of 250 words and another for 'web' sentences with a length of 1389 words. Below this, there are smaller bar graphs representing the number of sentences per domain and web page, with the domain having 14,000 sentences and the web having 13,818 sentences. The second section on the right side of the slide displays a bar graph labeled 'Sentiment Level,' showing the distribution of sentiment scores in the corpus. The graph has three bars: one for 'positive' sentiment with a score of 13,818, one for 'neutral' sentiment with a score of 13,818, and one for 'negative' sentiment with a score of 13,818. Below this, there are smaller bar graphs representing the number of sentences with positive, neutral, and negative sentiment, with the positive having 13,818 sentences, the neutral having 13,818 sentences, and the negative having 13,818 sentences. The third section at the bottom of the slide displays a bar graph labeled 'Sentiment Level,' showing the distribution of sentiment scores in the corpus. The graph has three bars: one for 'positive' sentiment with a score of 13,818, one for 'neutral' sentiment with a score of 13,818, and one for 'negative' sentiment with a score of 13,818. Below this, there are smaller bar graphs representing the number of sentences with positive, neutral, and negative sentiment, with the positive having 13,818 sentences, the neutral having 13,818 sentences, and the negative having 13,818 sentences.</sample>
    <sample id="1174">The video clip presents a detailed comparison of sentence simplification across different corpora, focusing on the number of sentences and their respective simplification levels. The data is displayed in a bar chart format with various colors representing different corpora. The corpora compared include 'Aristotle,' 'Bible,' 'Corpus of American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Corpus of Historical American English,' 'Corpus of Contemporary American English,' 'Cor</sample>
    <sample id="1175">The speaker is discussing the simplification of language and its impact on different types of texts. The speaker is comparing the simplification of news, fiction, and religious texts. The speaker is also discussing the simplification of transformations in language.</sample>
    <sample id="1176">The video shows a graph with two different types of simplification.</sample>
    <sample id="1177">The speaker is discussing the simplification of language in different contexts.</sample>
    <sample id="1178">The speaker is discussing the simplification of language in different contexts.</sample>
    <sample id="1179">The speaker is discussing the simplification of language in different contexts.</sample>
    <sample id="1180">The video clip features a person presenting information about automatic alignment evaluation. The presenter is seated in front of a computer screen displaying a table with various columns and rows, likely containing data related to the topic being discussed. The table includes columns labeled 'Name', 'Description', 'Spearman', 'Pearson', 'F1', and 'F2', suggesting that the presentation involves statistical analysis or evaluation metrics. The background appears to be an indoor setting, possibly an office or home environment, with a window visible behind the presenter.</sample>
    <sample id="1181">The results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="1182">The results of the alignment methods with 1/11 part and nm capabilities (lower part) are presented.</sample>
    <sample id="1183">The results of the alignment methods with 1/11 part and nm capabilities (lower part) are presented.</sample>
    <sample id="1184">The results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="1185">The results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="1186">The results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="1187">The results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="1188">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) Results on document simplification results correspond to the length of training data. DEEP-AP-APA (123) DEEP-AP-WE (147) Results on sentence simplification results correspond to the length of training data.</sample>
    <sample id="1189">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) Results on document simplification results correspond to the length of training data. DEEP-AP-APA (123) DEEP-AP-WE (147) Results on sentence simplification results correspond to the length of training data.</sample>
    <sample id="1190">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) Results on document simplification results correspond to the length of training data. DEEP-AP-APA (123) DEEP-AP-WE (147) Results on sentence simplification results correspond to the length of training data.</sample>
    <sample id="1191">The results of the document-level simplification are presented in Table 1.</sample>
    <sample id="1192">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-N-APA (123) DEEP-N-WE (147)</sample>
    <sample id="1193">The results on document simplification are presented in Table 1.</sample>
    <sample id="1194">Automatic Text Simplification DEEP-AP-APA (123) DEEP-AP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184) Document Level Results on document simplification results correspond to the length of training data. DEEP-APA (123) DEEP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184) Sentence Level Results on sentence simplification results correspond to the length of training data. DEEP-APA (123) DEEP-WE (147) DEEP-WE-APA (154) DEEP-WE-WE (184)</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">The video features a presentation slide titled 'Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)' by Mohammadi Javad Hosseini, Filip Rzadkis, Piotr Sztajniewicz, and Annie Louis. The slide is part of a Google Research presentation, as indicated by the logo at the bottom left corner. The background of the slide is white with colorful lines connecting various points, suggesting a flowchart or diagram related to the topic. The speaker's face is visible in a small circular frame at the bottom right corner of the screen, indicating that this is a live presentation or webinar.</sample>
    <sample id="1197">The video clip features a presentation slide titled 'Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)' by Mohammadi Javad Hosseini, Filip Sidor, Pariette, and Annie Louis. The slide is part of a Google Research presentation, showcasing a white background with colorful lines connecting various points. The speaker's face is visible in the bottom right corner, indicating an ongoing presentation or lecture.</sample>
    <sample id="1198">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: "Did you mean 'on' or 'or'?" Direct reference: "Easy." "The first one." Indirect reference: "I can't remember the name used in natural and fluid conversation." "The pronunciations are hard to distinguish." "Want to specify a preference?" "The new one." "The song's not energetic."</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: The first one. Indirect reference: Can't remember used in the natural and fluid conversation. Pronunciations are hard to distinguish. Want to specify a preference. The song's not energetic.</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: Easy, "the first one" Indirect reference: Can't remember used in the natural and fluid conversation Pronunciations are hard to distinguish The new one The song's not energetic</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users' language when they make a choice. Alternative question: Did you mean on easy or I'm getting a feeling? Direct reference: Easy, "the first one" Indirect reference: Cannot remember used in the natural and fluid conversation The pronunciations are hard to distinguish You want to specify a preference</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users' language when making a choice. Alternative question: "Did you mean on easy or I'm getting a feeling?" Direct reference: "Easy, the first one." Indirect reference: "Can't remember used in natural and fluid conversation." Pronunciations are hard to distinguish. Want to specify a preference? The song's not energetic.</sample>
    <sample id="1203">The problem is that there are no large-scale public datasets available for benchmarking large language models. We collect a large dataset using crowd annotation. Three domains</sample>
    <sample id="1204">The problem is that there are no large-scale public datasets available for benchmarking large language models. We collect a large dataset using crowd annotation. Three domains.</sample>
    <sample id="1205">Dataset Collection Methodology.</sample>
    <sample id="1206">The video clip is a presentation.</sample>
    <sample id="1207">The video clip is a presentation.</sample>
    <sample id="1208">The video clip is a presentation.</sample>
    <sample id="1209">The video clip is a presentation.</sample>
    <sample id="1210">Dataset Collection Methodology.</sample>
    <sample id="1211">The speaker is discussing the process of generating alternative questions for sampling entity pairs. He explains that items with similar descriptions on Wikipedia can be used to create more challenging entity pairs. The speaker provides examples of questions and their corresponding answers, such as "Do you mean A or B?" and "Do you mean C or D?" He also mentions that items with similar descriptions in the same genre or by the same artist can be used to create more challenging entity pairs.</sample>
    <sample id="1212">The speaker is discussing the process of generating alternative questions and sampling entity pairs. He explains that items with similar infoboxes on Wikipedia can be used to create more challenging questions. The speaker also mentions that items with similar descriptions or titles can be used to generate entity pairs.</sample>
    <sample id="1213">Google Research</sample>
    <sample id="1214">Google Research</sample>
    <sample id="1215">The speaker is discussing the process of generating alternative questions and sampling entity pairs. He explains that items with similar infoboxes on Wikipedia can be used to create more challenging entity pairs. The speaker provides examples of how to generate these pairs, such as using the same genre or artist, similar descriptions, or titles of books. He also mentions that the speaker's name could be used in the example.</sample>
    <sample id="1216">Background knowledge (Music) Google search link to each song We ask annotators to: Listen to at least some of each song Read about each song Click here to find out more about each song</sample>
    <sample id="1217">Background knowledge (Music) Google search link to each song We ask annotators to: Listen to at least some of each song Read about each song Click here to find out more about the songs</sample>
    <sample id="1218">The video clip is a tutorial on how to use Google Research. The narrator explains that the background knowledge section provides information about the topic being researched, and it includes links to related videos and articles. The narrator demonstrates how to access this information by clicking on the "Background knowledge (Music)" link. The video also shows how to use the search bar to find specific information within the background knowledge section. Overall, the video is a helpful guide for anyone looking to use Google Research effectively.</sample>
    <sample id="1219">BACKGROUND KNOWLEDGE (Recipes) Pandan Cake Pandan is a light, fluffy sponge cake. It is popular in the Malay community, especially among the Chinese.</sample>
    <sample id="1220">Eliciting expressions</sample>
    <sample id="1221">I'm going to give you a few examples of how we can use this technology.</sample>
    <sample id="1222">The Alt Entities Corpus is a large-scale dataset of over 400,000 questions across three domains. It was created by Google Research and is available for download from their website. The dataset includes a variety of question types, such as entity recognition, coreference resolution, and question answering, and is designed to challenge AI systems in understanding natural language.</sample>
    <sample id="1223">The AI Entities Corpus is a large-scale dataset of over 400,000 questions across three domains. It was created by training a 42 million parameter T5 XL model on the dataset and then using it to generate 100,000 questions for each domain. The results show that the T5 XL model has a 92% accuracy in generating questions with the same background knowledge as the original question. However, when the model has access to partially overlapping knowledge, its accuracy drops to 82%. The dataset is available at https://huggingface.co/google-research-datasets/ai-entities.</sample>
    <sample id="1224">The AI model has been trained on a large corpus of questions and answers, with the goal of understanding and answering questions in a way that is similar to human reasoning. The model has been tested on a variety of tasks, including question-answering, text-to-image generation, and image captioning. The model has also been evaluated on its ability to understand and generate natural language, and it has shown impressive results in these areas. Overall, the AI model represents a significant advancement in the field of artificial intelligence and has the potential to revolutionize the way we interact with computers and access information.</sample>
    <sample id="1225">Thank you!</sample>
    <sample id="1226">CamemBERT is initially trained on the French Wikipedia.</sample>
    <sample id="1227">Adam Prezrokowski</sample>
    <sample id="1228">The performance degradation with larger temporal gap</sample>
    <sample id="1229">The video features a presentation slide titled 'NLP Positionality: Characterizing Design Biases of Datasets and Models.' The slide is divided into two sections. The top section lists the names and affiliations of six individuals, each accompanied by their respective headshots. The bottom section contains the title of the presentation in bold black text on a white background. The individuals listed are:

1. 'Suebotin S., 'Jenny' T.' from the Subbotin Institute at the University of Washington.
2. 'Jenny T. I.' from the Subbotin Institute at the University of Washington.
3. 'Ronan B.' from the Allen Institute for Artificial Intelligence A.
4. 'Katherine Reinecke' from the University of Washington.
5. 'Marleen S. Sagap' from Carnegie Mellon University.

The background of the slide is plain white, and the text is primarily in black, with the exception of the names and affiliations, which are in blue. The overall design is simple and professional, focusing on the information presented rather than visual elements.</sample>
    <sample id="1230">The video features a presentation slide titled 'NLP Positionality: Characterizing Design Biases of Datasets and Models.' The slide is divided into two sections. The top section lists the names and affiliations of six individuals, each accompanied by their respective headshots. The bottom section contains the title of the presentation in bold black text on a white background. The individuals listed are:

1. 'Suebotin S., 'Jenny' T.' from the Subbotin Institute at the University of Washington.
2. 'Jenny T. I.' from the Subbotin Institute at the University of Washington.
3. 'Ronan B.' from the Allen Institute for Artificial Intelligence A.
4. 'Katherine Reinecke' from the University of Washington.
5. 'Marleen S. Sagap' from Carnegie Mellon University.

The background of the slide is plain white, and the text is primarily in black, with the exception of the names and affiliations, which are in blue. The overall design is simple and professional, focusing on the information presented rather than visual elements.</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine... Can you stop being a jerk? &lt;PerspectiveAPI score&gt;</sample>
    <sample id="1233">Imagine... Can you stop being a jerk? Prissitudes everywhere on the news. Card Jones Tech Lead New York Times Aditya Sharma Tech Lead Times of India</sample>
    <sample id="1234">Imagine... ... Design bias example: Can you stop being a jerk? Prissitudes everywhere on the news.</sample>
    <sample id="1235">Positionality The perspectives people hold as a result of their demographics, identity and life experiences. (Savin-Baden, Majoi &amp; Claire Hawley Majoi - Qualitative Research: The essential guide to theory and practice Routledge 2013)</sample>
    <sample id="1236">Positionality "The perspectives people hold as a result of their demographics, identity and life experiences."</sample>
    <sample id="1237">Positionality "The perspectives people hold (hold) as a result of their demographics, identity, and life experiences. [As a researcher], it influences the research process and its outcomes and results." [1]</sample>
    <sample id="1238">Do datasets and models have personality?</sample>
    <sample id="1239">Do datasets and models have personality?</sample>
    <sample id="1240">Do datasets and models have positionality?</sample>
    <sample id="1241">Do datasets and models have positionality?</sample>
    <sample id="1242">Do datasets and models have positionality?</sample>
    <sample id="1243">Do datasets and models have positionality?</sample>
    <sample id="1244">Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models.</sample>
    <sample id="1245">NLPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">The framework is a collection of data that is used to train a model. The model is then used to make predictions on new data. The framework is designed to be flexible and adaptable, allowing for the inclusion of new data and the exclusion of old data as needed. The framework is also designed to be scalable, allowing for the addition of more data and the use of more complex models as the need arises. Overall, the framework is a powerful tool for data analysis and machine learning, providing a structured approach to collecting, processing, and using data to make informed decisions.</sample>
    <sample id="1247">I'm really excited to be here today.</sample>
    <sample id="1248">I'm really excited to be here today.</sample>
    <sample id="1249">I'm really excited to be here today.</sample>
    <sample id="1250">The framework is a collection of data sets that are used to train machine learning models. The data sets are then used to evaluate the performance of the models, and the results are compared to determine which model performs best.</sample>
    <sample id="1251">I'm going to talk about the framework that I've developed for my research.</sample>
    <sample id="1252">LabInTheWild</sample>
    <sample id="1253">LabintheWild</sample>
    <sample id="1254">Task A: Social Acceptability. Participants read a situation about wanting to make a lot of money and enter what they think about it in a text box. Participants then rate how socially acceptable the situation is on a scale from 1 to 5, with 1 being not at all socially acceptable and 5 being very socially acceptable.</sample>
    <sample id="1255">Task A: Social Acceptability Wanting to make a lot of money. Want to know what you think about it? Enter your thoughts here. See AI and their thoughts if you want to compare them to yours. Participants compare their responses to AI's responses to others and an AI.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi GPT-4</sample>
    <sample id="1257">Task B: Toxicity Read the example Enter what you think about it Enter your rating 3 See 3 other ways to say it The AI specialist Participants read an instance from the DynaMeme dataset Participants rate whether they think an instance is hate speech</sample>
    <sample id="1258">The video clip features a static presentation slide titled 'Study Participation' with three key statistics displayed: 16,299 annotations, 1,096 annotators, and 87 countries. The background is plain white, and the text is in black, making it easy to read. The slide appears to be part of a larger presentation or lecture, possibly related to a study or research project involving data collection from various countries.</sample>
    <sample id="1259">Finding 1: There is potentiality in NLP.</sample>
    <sample id="1260">The speaker discusses the challenges of training machine learning models on datasets that are predominantly in English, highlighting the potential for bias and misalignment. The speaker emphasizes the importance of considering the diversity of languages spoken by different countries to ensure more accurate and fair model performance.</sample>
    <sample id="1261">The speaker is discussing the alignment of datasets and models with a college education.</sample>
    <sample id="1262">I'm really excited to be here today.</sample>
    <sample id="1263">Finding 2: Some populations are left behind.</sample>
    <sample id="1264">The video shows a bar graph with three categories: Man, Non-binary, and Woman. The graph compares the social acceptance of non-binary people in datasets and models to that of men and women. The bars are color-coded, with blue representing men, gray representing non-binary individuals, and red representing women. The graph shows that non-binary people are less aligned with binary people in terms of social acceptance.</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspective: a. Share disaggretated dataset labels.</sample>
    <sample id="1267">Recommendations 1. Keep record of all relevant design choices made throughout building datasets and models. 2. Do NLP research through the lens of perspective: Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with specific communities is possible for inclusive NLP (Masakhane initiative).</sample>
    <sample id="1268">Thanks!</sample>
    <sample id="1269">To ensure that the output sequence is in the correct order.</sample>
    <sample id="1270">To increase transparency about bias mitigation methods.</sample>
    <sample id="1271">Many people were, The many people, They all knew, She was</sample>
    <sample id="1272">F1 score, accuracy, and BLEU score.</sample>
    <sample id="1273">Krippendorff's alpha</sample>
    <sample id="1274">Space</sample>
    <sample id="1275">Heinrich Heine University, Heinrich-Heine-UniversitÃ¤t, Germany</sample>
    <sample id="1276">It is the first large-scale, publically available multimodal instruction dataset.</sample>
    <sample id="1277">3</sample>
    <sample id="1278">The difference in the number of bonds between the left and right sides of a molecule.</sample>
    <sample id="1279">10 words</sample>
    <sample id="1280">The smaller T5 model is fine-tuned on the CoSCText dataset and can generate higher-quality scripts than LLMs trained on GPT-3.</sample>
    <sample id="1309">The work investigates the comparison of pre-training strategies and data sources.</sample>
    <sample id="1310">1.5</sample>
    <sample id="1311">The quality of the simplification was evaluated using human evaluation.</sample>
    <sample id="1312">Yes, they do.</sample>
    <sample id="1347">two elements of cognition that are inconsistent</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">yes</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">The data was taken from the P-CXMI words.</sample>
    <sample id="1352">The slide is titled "Dependency Structure of Coordination" and presents a comparison between different dependency structures in English. The first example is "Bouquet (Stanford) [Universal Dependencies]" which shows a chain of dependencies: "Homers loves Lisa, Bart, and Maggie." The second example is "Chain (Moscow)" which also shows a chain of dependencies: "Homers loves Lisa, Bart, and Maggie." The third example is "Conjunction-headed (Prague)" which shows a conjunction-headed structure: "Homers loves Lisa and Bart and Maggie." The fourth example is "Multi-headed (London)" which shows a multi-headed structure: "Homers loves Lisa, Bart, and Maggie."</sample>
    <sample id="1353">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different types of coordination structures. The first section, "Bouquet (Stanford Universal Dependencies)," lists the words "Homer," "loves," "Lisa," "and," and "Maggie" with their respective dependencies. The second section, "Chain (MOSCOW)," also lists the same words with their dependencies. The third section, "Conjunction-headed (Praague)," shows the same words with their dependencies. The fourth section, "Multi-headed (London)," lists the same words with their dependencies. The slide provides a visual representation of how different coordination structures are analyzed in linguistics.</sample>
    <sample id="1354">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different dependency structures. The first section, "Bouquet/Stanford (Universal Dependencies)," lists the dependencies for the sentence "Homer loves Lisa, Bart, and Maggie." The second section, "Chain (MOSCOW)," also shows the same sentence with its dependencies. The third section, "Conjunction-headed (Praague)," presents the sentence "Homer loves Lisa, Bart, and Maggie" with its dependencies. The fourth section, "Multi-headed (London)," displays the same sentence with its dependencies. The slide provides a visual representation of how different dependency structures can be used to analyze the grammatical relationships within a sentence.</sample>
    <sample id="1355">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different types of coordination in dependency structures. The first section, "Bouquet (Stanford Universal Dependencies)," shows a tree diagram with nodes representing words and their dependencies. The second section, "Chain (MOSCOW)," displays a chain-like structure where each word is connected to the next one. The third section, "Conjunction-headed (Praague)," presents a tree diagram with a conjunction as the head node. The fourth section, "Multi-headed (London)," depicts a more complex structure with multiple heads for a single node. The presenter's hand is visible on the right side of the frame, indicating that they are explaining the content of the slide.</sample>
    <sample id="1356">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different types of coordination in dependency structures. The first section, "Bouquet (Stanford Universal Dependencies)," shows a tree diagram with nodes representing words and their dependencies. The second section, "Chain (MOSCOW)," displays a chain of words connected by lines indicating their relationships. The third section, "Conjunction-headed (Praague)," presents a sentence where the conjunction "and" connects multiple subjects. The fourth section, "Multi-headed (London)," demonstrates a sentence with multiple heads for the verb "loves," connecting to different subjects. The slide aims to explain how different coordination structures are represented in dependency trees.</sample>
    <sample id="1357">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different types of coordination in dependency structures. The first section, "Bouquet (Stanford Universal Dependencies)," shows a chain of dependencies between words like "Homer," "loves," "Lisa," "and," and "Maggie." The second section, "Chain (MOSCOW)," also depicts a chain of dependencies with similar words. The third section, "Conjunction-headed (Praague)," shows a conjunction-headed structure where "Homer" loves "Lisa" and "Bart" and "Maggie." The fourth section, "Multi-headed (London)," illustrates a multi-headed structure where "Homer" loves "Lisa," "Bart," and "Maggie." The slide uses arrows to indicate the direction of dependency between words, providing a visual representation of how words are connected in different coordination structures.</sample>
    <sample id="1358">Bouquet (Stanford) (Universal Dependencies): Homer loves Lisa and Bart. Chain (Moscow): Homer loves Lisa, Bart, and Maggie. Conjunction-headed (Prague): Homer loves Lisa and Bart, and Maggie. Multi-headed (London): Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different dependency structures. The first section, "Bouquet (Stanford Universal Dependencies)," shows a tree diagram with nodes representing words and their relationships. The second section, "Chain (Moscov):" displays a chain-like structure with words connected sequentially. The third section, "Conjunction-headed (Prague):" presents a tree diagram with a conjunction as the head node. The fourth section, "Multi-headed (London):" illustrates a more complex structure with multiple heads. The slide provides visual examples to explain the different types of coordination in linguistics.</sample>
    <sample id="1360">The video shows a presentation slide titled "Dependency Structure of Coordination." The slide is divided into four sections, each illustrating different types of coordination in dependency structures. The first section, "Bouquet (Stanford Universal Dependencies)," shows a tree diagram with nodes representing words and their dependencies. The second section, "Chain (MOSCOW)," displays a similar tree diagram but with a different structure. The third section, "Conjunction-headed (Praague)," presents another tree diagram with a unique dependency structure. The fourth section, "Multi-headed (London)," also features a tree diagram with multiple dependencies. The slide aims to demonstrate the complexity and variety of coordination in natural language processing.</sample>
    <sample id="1361">The word order tends to minimize dependency lengths.</sample>
    <sample id="1362">The word order tends to minimize dependency lengths.</sample>
    <sample id="1363">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths. Marge read yesterday It. good Marge read yesterday it. bad</sample>
    <sample id="1364">The word order tends to minimize dependency lengths.</sample>
    <sample id="1365">The word order tends to minimize dependency lengths.</sample>
    <sample id="1366">The word order tends to minimize dependency lengths.</sample>
    <sample id="1367">The word order tends to minimize dependency lengths.</sample>
    <sample id="1368">The word order tends to minimize dependency lengths.</sample>
    <sample id="1369">The word order tends to minimize dependency lengths.</sample>
    <sample id="1370">The word order tends to minimize dependency lengths.</sample>
    <sample id="1371">The word order tends to minimize dependency lengths.</sample>
    <sample id="1372">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016) Left conjunct tends to be shorter than right but only when Governor is on the left (absent: not it is) but not if it is on the right (tied: Ned laughed).</sample>
    <sample id="1373">Conjunct Lengths in English.</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Ficler &amp; Goldberg, 2016) - left conjunct tends to be shorter (observed before) - this tendency grows with length difference (this tendency was first noticed by Gibson et al. of 1986-90) - not when it is on the right (i.e. "the saw Bort" vs. "Bort saw")</sample>
    <sample id="1375">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fickers &amp; Goldberg, 2016) when left conjuncts tend to be shorter (observed before) this tendency grows with length difference (this tendency was first noticed by Gibson et al. of 1986-89).</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fickers &amp; Goldberg, 2016) left conjuncts tend to be shorter (observed before) this tendency grows with length (1968-80) but only when the governor is on the left or on the right [not if it is on the top] [I saw Bart and Lisa. Homer snored and laughed].</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Fickerser &amp; Goldberg 2016) left conjunct tends to be shorter (observed before) typically grows with length (166-80) but only when the Governor is on the left or on the right (not on it) Ted and Ned laughed</sample>
    <sample id="1378">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fiser &amp; Goldberg, 2016) left conjunct tends to be shorter (observed before) this tendency grows with length (168-1680) but only when the Governor is on the left or on the right [not if it is on the governor] [I saw Bart and Lisa. Homer and Ned laughed.]</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Fickerser &amp; Goldberg 2016) left conjunct tends to be shorter than right conjunct (observed before) initially tendency grows with length (1986-80) but only when the Governor is on the left or absent (on saw Bart and Lisa, home and Ned, and sneezed) not not it is on the (Ted and Ned laughed)</sample>
    <sample id="1380">The graph shows the relationship between the number of characters and the percentage of content that is different. The line on the graph represents a linear regression model, which suggests that as the number of characters increases, the percentage of content that is different also increases. This indicates that longer texts tend to have more differences in their content compared to shorter texts.</sample>
    <sample id="1381">The graph shows the relationship between the length of the word and the probability of it being a content word. The x-axis represents the length of the word, while the y-axis represents the probability. The blue line indicates that as the length of the word increases, the probability of it being a content word also increases. This suggests that longer words are more likely to be content words in this particular dataset.</sample>
    <sample id="1382">The graph shows the relationship between the number of characters and the percentage of content that is different. The line on the graph represents a linear regression model, which suggests that as the number of characters increases, the percentage of content that is different also increases. This indicates that longer texts tend to have more differences in their content compared to shorter texts.</sample>
    <sample id="1383">Compatibility with Dependency Structures of Coordination Compatibility with Dependency Structures of Coordination Bouchaud (Stanford) Universal Dependencies: No Chain (Moscow) Homer loves Lisa, Bart, and Maggie. No Conjunction-headed (Prague) Homer loves Lisa, Bart, and Maggie. Yes Multi-headed (London) Homer loves Lisa, Bart, and Maggie. Yes</sample>
    <sample id="1384">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Training on one language and evaluating on another.</sample>
    <sample id="1387">Saarlauand University, Amazon Alexa, 3rd University of Vienna</sample>
    <sample id="1388">AL, AL/CA, CA</sample>
    <sample id="1416">Trees help a lot but...</sample>
    <sample id="1417">School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="1495">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1496">2012</sample>
    <sample id="1527">University of Amsterdam, Saarland University, and University of Helsinki.</sample>
    <sample id="1528">Chen Zhen</sample>
    <sample id="1529">4</sample>
    <sample id="1530">walk-k-walk</sample>
    <sample id="1531">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhuyang Xu, Ying Shen, *Li* Huifang Department of Computer Science, Virginia Tech</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="1534">Language-only</sample>
    <sample id="1535">In this talk, I will be discussing instruction tuning on multimodal pre-trained models.</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks NO large-scale, publicly-available multimodal instruction tasks</sample>
    <sample id="1538">The first multimodal instruction tuning benchmark dataset.</sample>
    <sample id="1539">The first multimodal instruction tuning benchmark dataset.</sample>
    <sample id="1540">A multi-modal pre-trained model is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary for language or image tokens, and the encoder-decoder architecture has a coordinate of a bounding box.</sample>
    <sample id="1541">The video shows a presentation slide with four columns, each representing a different task. The first column is labeled "Grounded Caption" and contains an image of a person taking a selfie. The second column is labeled "Input Text" and contains a description of the image in text format. The third column is labeled "Referring Expression Selection" and contains a list of options for referring to the object in the image. The fourth column is labeled "Question Imaging Matching" and contains a question related to the image. The slide also includes a caption that reads "Figure: Example Instances from MULTINSTRUCT."</sample>
    <sample id="1542">The video shows a presentation slide with four columns, each representing a different task. The first column is labeled "Grounded Caption" and contains an image of a person taking a selfie with a train in the background. The second column is labeled "Input Text" and contains a caption describing the image as "a man taking a selfie on a train". The third column is labeled "Referring Expression Selection" and contains a list of options for selecting the region of the object in the image that corresponds to the caption. The fourth column is labeled "Question-Answering Matching" and contains a question asking "What is the man doing?" with multiple-choice answers including "taking a selfie", "standing on a train", and "walking on a train". The slide also includes a section labeled "Output" with numerical values indicating the model's confidence in its predictions.</sample>
    <sample id="1543">The video clip shows a presentation slide with four different tasks. The first task is "Grounded Captioning," which involves describing an image in detail. The second task is "Referential Expression Selection," where the goal is to identify specific regions of an object within an image. The third task is "Question Answering," which requires selecting the correct answer from a list of options based on the content of the image. The fourth task is "Question Image Matching," where the objective is to match questions with the corresponding images that provide the answers.</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">Multi-Modal Instruction Tuning</sample>
    <sample id="1546">Multi-Modal Instruction Tuning</sample>
    <sample id="1547">The training details include the use of a pre-trained OF-A-Large model and the combination of instances with random instruction templates. The testing details involve conducting five experiments to evaluate the model's performance, reporting the mean maximum performance, and standard deviation across the experiments.</sample>
    <sample id="1548">The implementation details of the model are as follows.</sample>
    <sample id="1549">The training details include using a pre-trained OF-A-Large model and mixing instances in all tasks. The testing details involve conducting five experiments to evaluate the model's performance, reporting the mean maximum performance, and standard deviation across five experiments.</sample>
    <sample id="1550">Evaluation Metrics For multi-classification tasks Visual entailment, Visual-Spatial Reasoning and Disaster Type Classification report the Accuracy Common Sense VQA, VQA Text, Grounded VQA For multi-modal generation tasks Video-Modality Extraction and Dialogue report the Rouge-L</sample>
    <sample id="1551">Sensitivity</sample>
    <sample id="1552">The effectiveness of instruction tuning on MULTINSTRUCT</sample>
    <sample id="1553">The effectiveness of instruction tuning on MULTINSTRUCT</sample>
    <sample id="1554">The impact of increasing multimodal instruction task clusters.</sample>
    <sample id="1555">The effect of diverse instructions on instruction tuning.</sample>
    <sample id="1556">In this paper, we investigate the effect of fine-tuning strategies on model sensitivity. We show that instruction tuning on multi-struct significantly reduce the sensitivity of the model. Transfer learning from natural instructions dataset can further reduce the sensitivity of the model.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multilingual can improve zero-shot performance on Table-2 and Rong-5. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.</sample>
    <sample id="1558">Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity.</sample>
    <sample id="1559">We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!</sample>
  </task>
</testset>