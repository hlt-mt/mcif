<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">Webpage</sample>
    <sample id="1">McGill, MIT, MILA, Microsoft Research</sample>
    <sample id="2">DEPLAIN: A German Parallel Corpus with Intra-lingual Translations into Plain Language for Sentence and Document Simplification</sample>
    <sample id="3">1. Text Simplification</sample>
    <sample id="4">收到英文内容后，用中文表述其意思。</sample>
    <sample id="5">收到英文内容后，用中文表述其意思。</sample>
    <sample id="6">收到英文内容后，用中文表述其意思。</sample>
    <sample id="7">收到英文内容后，用中文表述其意思。</sample>
    <sample id="8">收到英文内容后，用中文表述其意思。</sample>
    <sample id="9">收到英文内容后，用中文表述其意思。</sample>
    <sample id="10">收到英文内容后，用中文表述其意思。</sample>
    <sample id="11">收到英文内容后，用中文表述其意思。</sample>
    <sample id="12">收到英文内容后，用中文表述其意思。</sample>
    <sample id="13">收到英文内容后，用中文表述其意思。</sample>
    <sample id="14">收到英文内容后，用中文表述其意思。</sample>
    <sample id="15">收到英文内容后，用中文表述其意思。</sample>
    <sample id="16">收到英文内容后，用中文表述其意思。</sample>
    <sample id="17">收到英文内容后，用中文表述其意思。</sample>
    <sample id="18">收到英文内容后，用中文表述其意思。</sample>
    <sample id="19">使用自动对齐方法，可以将不同语言的句子进行对齐。</sample>
    <sample id="20">在自动对齐评估中，LHA和Sen-SER表现得更好。</sample>
    <sample id="21">收到英文内容后，用中文表述其意思。</sample>
    <sample id="22">在自动对齐评估中，LHA和Sen-SER-BERT-German-Crosslingual的r和m值都高于其他方法。</sample>
    <sample id="23">在自动对齐评估中，LHA和Sen-SER-BERT-German-Crosslingual的r和m值都高于其他方法。</sample>
    <sample id="24">在自动对齐评估中，LHA和Sen-SER表现得更好。</sample>
    <sample id="25">在自动对齐评估中，LHA和Sen-SER-BERT的得分更高。</sample>
    <sample id="26">在自动对齐评估中，LHA和Sen-SER表现得更好。</sample>
    <sample id="27">收到英文内容后，用中文表述其意思。</sample>
    <sample id="28">收到英文内容后，用中文表述其意思。</sample>
    <sample id="29">收到英文内容后，用中文表述其意思。</sample>
    <sample id="30">收到英文内容后，用中文表述其意思。</sample>
    <sample id="31">收到英文内容后，用中文表述其意思。</sample>
    <sample id="32">收到英文内容后，用中文表述其意思。</sample>
    <sample id="33">收到英文内容后，用中文表述其意思。</sample>
    <sample id="34">谢谢。</sample>
    <sample id="35">Patric F. Fernandes</sample>
    <sample id="36">LLM</sample>
    <sample id="37">yes</sample>
    <sample id="38">使用行为注释来评估机器人的行为。</sample>
    <sample id="39">依赖于训练数据的丰富程度。</sample>
    <sample id="40">We ask annotators to listen to at least some of each song and read about the song.</sample>
    <sample id="41">5</sample>
    <sample id="42">Conjunctions in English</sample>
    <sample id="43">依赖结构的协调</sample>
    <sample id="44">依赖结构的协调</sample>
    <sample id="45">依赖结构的协调</sample>
    <sample id="46">依赖结构的协调</sample>
    <sample id="47">依赖结构的协调</sample>
    <sample id="48">依赖结构的协调</sample>
    <sample id="49">依赖结构的协调</sample>
    <sample id="50">依赖结构的协调</sample>
    <sample id="51">依赖长度最小化(DLM)</sample>
    <sample id="52">依赖长度最小化(DLM)</sample>
    <sample id="53">依赖长度最小化(DLM)</sample>
    <sample id="54">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="55">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="56">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="57">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="58">单词顺序 tend to minimize dependency lengths</sample>
    <sample id="59">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="60">单词顺序 tend  to minimize dependency lengths</sample>
    <sample id="61">单词顺序 tend 旨在最小化依赖长度</sample>
    <sample id="62">Conjunct Lengths in English</sample>
    <sample id="63">Conjunct Lengths in English</sample>
    <sample id="64">Conjunct Lengths in English</sample>
    <sample id="65">Conjunct Lengths in English</sample>
    <sample id="66">Conjunct Lengths in English</sample>
    <sample id="67">Conjunct Lengths in English</sample>
    <sample id="68">Conjunct Lengths in English</sample>
    <sample id="69">Conjunct Lengths in English</sample>
    <sample id="70">在图 1 中，我们展示了不同长度的单词的平均长度。</sample>
    <sample id="71">图1：在不同长度的单词上，N-gram生成器的差异。</sample>
    <sample id="72">在图 1 中，我们展示了不同长度的单词的平均长度。</sample>
    <sample id="73">兼容性与依赖结构的协调</sample>
    <sample id="74">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="75">3</sample>
    <sample id="76">news, fiction</sample>
    <sample id="77">not it is</sample>
    <sample id="78">yes</sample>
    <sample id="79">APA 格式的文档。</sample>
    <sample id="80">Bigger model size, more fine-tuning examples</sample>
    <sample id="81">left conjunct is shorter than right conjunct</sample>
    <sample id="82">在实验中，将支配词放在不同位置，如句首、句中和句末，然后测量其对句子长度的影响。</sample>
    <sample id="83">not better than chance</sample>
    <sample id="84">4</sample>
    <sample id="85">John and Mary</sample>
    <sample id="86">形式/ lexical cohesion</sample>
    <sample id="87">Purdue University</sample>
    <sample id="122">使用Pearson's r scores。</sample>
    <sample id="155">a</sample>
    <sample id="156">Penn Treebank</sample>
    <sample id="157">2</sample>
    <sample id="158">debate, CE, DEBATE</sample>
    <sample id="159">2</sample>
    <sample id="160">6</sample>
    <sample id="161">以前的研究主要关注模型的预测能力，而本研究则关注模型的公平性。</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate</sample>
    <sample id="164">在演讲中，演讲者介绍了他们对语言模型的训练数据的调查。</sample>
    <sample id="165">LM Training Data</sample>
    <sample id="166">LM Training Data</sample>
    <sample id="167">LM Training Data</sample>
    <sample id="168">LM Training Data</sample>
    <sample id="169">To this end</sample>
    <sample id="170">To this end</sample>
    <sample id="171">To this end</sample>
    <sample id="172">评价LM政治倾向</sample>
    <sample id="173">在图中，每个点都代表一个不同的AI系统。</sample>
    <sample id="174">在图中，每个点都代表一个大型语言模型。</sample>
    <sample id="175">新闻媒体和社交媒体的假新闻数据，可以训练机器学习模型。</sample>
    <sample id="176">新闻媒体和社交媒体的左右中政治立场。</sample>
    <sample id="177">展示LM在政治倾向上的 partisan shifts。</sample>
    <sample id="178">展示LM在政治倾向上的 partisan shifts。</sample>
    <sample id="179">展示了一个关于《特朗普牌》的图表。</sample>
    <sample id="180">The Trump Card</sample>
    <sample id="181">The Trump Card</sample>
    <sample id="182">The Trump Card</sample>
    <sample id="183">表格中黄色的数字最好，蓝色的数字最差。</sample>
    <sample id="184">表格中展示的数字代表了不同身份和 misinformation 的分类性能。</sample>
    <sample id="185">表格中展示的数字代表了不同身份和 misinformation 的分类性能。</sample>
    <sample id="186">表格中每一行的数字代表了不同身份和 misinformation 的分类性能。</sample>
    <sample id="187">表格中展示的数字代表了不同身份和 misinformation 源的 hate speech 的百分比。</sample>
    <sample id="188">表格中展示的数字代表了不同身份和 misinformation 的分类性能。</sample>
    <sample id="189">表格中展示的数字代表了不同身份和 misinformation 的分类性能。</sample>
    <sample id="190">在本研究中，我们使用了基线模型和四种不同的政治极化模型。</sample>
    <sample id="191">在接下来的几周里，我将每周五发布两篇新文章。</sample>
    <sample id="192">收到英文内容后，用中文表述其意思。</sample>
    <sample id="193">收到英文内容后，用中文表述其意思。</sample>
    <sample id="194">收到英文内容后，用中文表述其意思。</sample>
    <sample id="195">在讨论中，区分Scylla和Charibdis之间的差异是关键。</sample>
    <sample id="196">在讨论中，区分Syclia和Charobis之间的差异是关键问题。</sample>
    <sample id="197">在讨论中，区分Syclia和Charobis之间的差异是关键问题。</sample>
    <sample id="198">在讨论中，Chen Young Park 提到，他们使用了 1.5 亿个单词的预训练数据。</sample>
    <sample id="199">感谢演讲者！</sample>
    <sample id="200">6</sample>
    <sample id="201">900</sample>
    <sample id="202">音乐，书籍和食谱</sample>
    <sample id="203">positionality（立场）的定义是：人们所持的观点是他们 demographics, identity and life experiences 的结果。</sample>
    <sample id="204">Dawid Zhai, Xiu Shen, M. 'Maurin' Mosch, Andrei Stephens, Dietrich Klakow</sample>
    <sample id="205">Yes</sample>
    <sample id="206">4</sample>
    <sample id="207">no</sample>
    <sample id="208">Background-Pretrain, Background-Both, Background-Inference</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">如何使用可用的清洁样本更有效地</sample>
    <sample id="211">它衡量模型对同一任务的敏感度，如何对同一任务的多种指令产生相同的输出。</sample>
    <sample id="212">Wenjun Jing</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="214">模型会接收大量的语言上下文。</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">因为现有方法无法衡量机器学习模型的偏见。</sample>
    <sample id="218">Jackie CK Chuang</sample>
    <sample id="219">pretraining data, language models, downstream tasks</sample>
    <sample id="220">yes</sample>
    <sample id="221">Yes</sample>
    <sample id="222">在原始嵌入的基础上添加一个目标嵌入。</sample>
    <sample id="223">PennState</sample>
    <sample id="224">Yes</sample>
    <sample id="225">如何制作草莓蛋糕？</sample>
    <sample id="226">They ensure the method's privacy by using a random seed to generate the embedding.</sample>
    <sample id="227">Pre-training strategies and data sources</sample>
    <sample id="228">Protestant Europe</sample>
    <sample id="229">I am</sample>
    <sample id="230">任务的数量对模型的性能有重要影响。</sample>
    <sample id="231">LSTM, T5-seq2seq, and Zhenqiang.</sample>
    <sample id="232">Alexander Koller and Ivan Tivot are co-authors with Matthias Lindemann.</sample>
    <sample id="233">Andrew Chowdhery</sample>
    <sample id="234">NL Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="235">NL Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine...</sample>
    <sample id="238">想象…… Can you stop being a jerk? Prestigious everywhere on the news.</sample>
    <sample id="239">想象……...</sample>
    <sample id="240">Positionality</sample>
    <sample id="241">Positionality</sample>
    <sample id="242">Positionality</sample>
    <sample id="243">Do datasets and models have personality?</sample>
    <sample id="244">Do datasets and models have personality?</sample>
    <sample id="245">Do datasets and models have positionality?</sample>
    <sample id="246">Do datasets and models have positionality?</sample>
    <sample id="247">Do datasets and models have positionality?</sample>
    <sample id="248">Do datasets and models have positionality?</sample>
    <sample id="249">问题：Do datasets and models have positionality?</sample>
    <sample id="250">NLP Positionality</sample>
    <sample id="251">框架</sample>
    <sample id="252">框架</sample>
    <sample id="253">框架</sample>
    <sample id="254">框架</sample>
    <sample id="255">框架</sample>
    <sample id="256">框架</sample>
    <sample id="257">LabInTheWild</sample>
    <sample id="258">LabintheWild</sample>
    <sample id="259">Task A: Social Acceptability</sample>
    <sample id="260">Task A: Social Acceptability</sample>
    <sample id="261">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="262">任务 B: 恶意</sample>
    <sample id="263">任务B：毒性</sample>
    <sample id="264">Finding 1: There is positonality in NLP.</sample>
    <sample id="265">datasets and models are most aligned to English-speaking countries.</sample>
    <sample id="266">数据集和模型最常与人们认同的教育。</sample>
    <sample id="267">datasets and models are most aligned with a college education.</sample>
    <sample id="268">Finding 2: Some populations are left behind.</sample>
    <sample id="269">datasets and models are less aligned to non-binary people.</sample>
    <sample id="270">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="271">推荐</sample>
    <sample id="272">Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspective: Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with specific communities is possible for inclusive NLP (Masakhane initiative).</sample>
    <sample id="273">谢谢！</sample>
    <sample id="274">3</sample>
    <sample id="275">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是使用多样化的数据集。</sample>
    <sample id="276">Distilling Script Knowledge from Large Language Models for Constrained Language Planning</sample>
    <sample id="277">语言规划</sample>
    <sample id="278">语言规划</sample>
    <sample id="279">Constrained Language Planning</sample>
    <sample id="280">Constrained Language Planning</sample>
    <sample id="281">Constrained Language Planning</sample>
    <sample id="282">如何让LLMs在约束语言规划中表现良好？</sample>
    <sample id="283">如何让LLMs在约束语言规划中表现良好？</sample>
    <sample id="284">如何让LLMs在约束语言规划中表现良好？</sample>
    <sample id="285">Can LLMs do Constrained Language Planning?</sample>
    <sample id="286">Can LLMs do Constrained Language Planning?</sample>
    <sample id="287">LLMs usually make what types of errors in this task?</sample>
    <sample id="288">LLMs usually make what types of errors in this task?</sample>
    <sample id="289">InstructGPT typically fails to achieve goals that are too specific or require a high level of planning.</sample>
    <sample id="290">Step 1: Generate specific goals based on input.</sample>
    <sample id="291">Step 1: Generate specific goals (G1) based on the abstract goal.</sample>
    <sample id="292">在输入目标后，系统会生成多个候选目标。</sample>
    <sample id="293">Step 2: Over-Script Generation with InstructGPT in In-Context Learning</sample>
    <sample id="294">Step 2: Filter over candidate scripts with In-Script learning.</sample>
    <sample id="295">Step 2: Over-Script Generation with InstructGPT in In-Context Learning</sample>
    <sample id="296">InstructGPT can generate higher quality text by a large margin.</sample>
    <sample id="297">动机：计划将大型语言模型(LLMs)的复杂知识转化为小型、可解释的模型，以便在小型设备上运行。</sample>
    <sample id="298">动机：计划将大型语言模型(LLMs)的动机是使它们能够生成更小的、更受约束的模型，以便在小型设备上运行。</sample>
    <sample id="299">动机：计划使约束力较小的模型能力。</sample>
    <sample id="300">动机：计划使约束力较小的模型能力。</sample>
    <sample id="301">动机：计划将大型语言模型(LLMs)的复杂知识转化为小型模型，以便在小型设备上运行。</sample>
    <sample id="302">CoT for Smaller Language Models</sample>
    <sample id="303">在本研究中，我们使用了GPT-3(175B)和Instruct-GPT(175B)作为LLMs。</sample>
    <sample id="304">总结和 takeaway</sample>
    <sample id="305">总结和 takeaway</sample>
    <sample id="306">在演讲中，Siyuan 介绍了她的研究，该研究涉及从大型语言模型中提取知识并将其用于约束性语言规划。Siyuan 的研究旨在解决大型语言模型的局限性，这些模型通常无法理解或生成复杂的语言结构。Siyuan 的方法是使用大型语言模型来生成候选句子，然后使用约束性规划算法来选择最符合特定约束条件的句子。Siyuan 的研究表明，这种方法可以显著提高大型语言模型的性能，尤其是在处理复杂语言任务时。</sample>
    <sample id="307">PaLM 的流畅度可与 SOTA 系统相媲美。</sample>
    <sample id="308">Utility, should not degrade the utility of the provided embeddings.</sample>
    <sample id="309">TED 英语演讲已被翻译成哪 14 种不同的语言？</sample>
    <sample id="310">20</sample>
    <sample id="311">similarity difference and χ² of KS test</sample>
    <sample id="312">使用mBART。</sample>
    <sample id="344">在一般性语料库中，单词的频率计数。</sample>
    <sample id="345">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?</sample>
    <sample id="346">Named Entity Recognition &amp; Generalization</sample>
    <sample id="347">Named Entity Recognition &amp; Generalization</sample>
    <sample id="348">Named Entity Recognition &amp; Generalization</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization</sample>
    <sample id="350">ConLL+ Dataset</sample>
    <sample id="351">ConLL++ Dataset</sample>
    <sample id="352">ConLL+ Dataset</sample>
    <sample id="353">What is Needed for Good Generalization?</sample>
    <sample id="354">在演讲中，演讲者讨论了模型的通用性问题。演讲者指出，当前的模型在处理新数据时表现不佳，因为它们无法很好地泛化。演讲者还提到了 transformer 模型，认为它们可以更好地泛化。演讲者强调，模型的通用性是未来研究的重点。</sample>
    <sample id="355">是什么需要良好的泛化?</sample>
    <sample id="356">在什么条件下才能获得良好的泛化?</sample>
    <sample id="357">What Causes Performance Drop?</sample>
    <sample id="358">什么导致性能下降?</sample>
    <sample id="359">什么导致性能下降?</sample>
    <sample id="360">什么导致性能下降?</sample>
    <sample id="361">什么导致性能下降?</sample>
    <sample id="362">什么导致了性能下降?</sample>
    <sample id="363">什么导致了性能下降?</sample>
    <sample id="364">什么导致了性能下降?</sample>
    <sample id="365">What Causes Performance Drop?</sample>
    <sample id="366">结论</sample>
    <sample id="367">结论</sample>
    <sample id="368">Do C02L3 tags still work?</sample>
    <sample id="369">Do C02-3 still work?</sample>
    <sample id="370">数据：https://github.com/ShuhengLiu/SLI-2023</sample>
    <sample id="397">160毫秒</sample>
    <sample id="398">Servin 是一个 judge，Kea 是一个 baker。</sample>
    <sample id="399">PALM</sample>
    <sample id="400">Roberta, GPT-2</sample>
    <sample id="401">特定层的注意力分数</sample>
    <sample id="402">the first one, the last one</sample>
    <sample id="403">Brain Technologies Inc.</sample>
    <sample id="404">4</sample>
    <sample id="405">yes</sample>
    <sample id="406">a woman warrior</sample>
    <sample id="407">CNN</sample>
    <sample id="408">All</sample>
    <sample id="409">6</sample>
    <sample id="410">Multimodal</sample>
    <sample id="439">Inference-time knowledge and reasoning</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes, it has been manually annotated and validated.</sample>
    <sample id="442">只有一小部分单词依赖于上下文，现有方法支持有限的 discourse，语言和现象。</sample>
    <sample id="443">Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)</sample>
    <sample id="444">Resolving Indirect Referring Expressions for Entity Selection (A(Entities Corpus)</sample>
    <sample id="445">The new one. The song's not energetic.</sample>
    <sample id="446">The new one. The song's not energetic.</sample>
    <sample id="447">The new one. The song's not energetic.</sample>
    <sample id="448">The new one. The song's not energetic.</sample>
    <sample id="449">The song's not energetic.</sample>
    <sample id="450">重要问题</sample>
    <sample id="451">重要问题</sample>
    <sample id="452">Google Research</sample>
    <sample id="453">Google Research</sample>
    <sample id="454">Google Research</sample>
    <sample id="455">Google Research</sample>
    <sample id="456">Google Research</sample>
    <sample id="457">Google Research</sample>
    <sample id="458">Do you mean A or B?</sample>
    <sample id="459">Do you mean A or B?</sample>
    <sample id="460">Do you mean A or B?</sample>
    <sample id="461">Do you mean A or B?</sample>
    <sample id="462">Do you mean A or B?</sample>
    <sample id="463">背景知识(音乐)</sample>
    <sample id="464">背景知识(音乐)</sample>
    <sample id="465">背景知识(音乐)</sample>
    <sample id="466">Pandan Cake</sample>
    <sample id="467">我们告诉标注者应该选择哪一个并让他们描述它。</sample>
    <sample id="468">音乐选择</sample>
    <sample id="469">2021年，我们使用了420,000个问题来训练T5 XL模型。</sample>
    <sample id="470">2021年，我们使用了420,000个问题来训练T5 XL。</sample>
    <sample id="471">2019年，我们使用了420,000个问题来训练我们的模型。</sample>
    <sample id="472">谢谢！</sample>
    <sample id="473">与 walk-k、LA、CA、ED 策略进行了比较。</sample>
    <sample id="474">L'Institut de Recherche en Informatique et Systèmes d'Information (IRISA)</sample>
    <sample id="475">Jennifer T. Lang</sample>
    <sample id="476">3</sample>
    <sample id="477">Attention as a Guide for Simultaneous Speech Translation</sample>
    <sample id="478">Simultaneous speech translation (SimuST)</sample>
    <sample id="479">What are the problems of the current SimuLST models?</sample>
    <sample id="480">What are the problems of the current SimuLST models?</sample>
    <sample id="481">What are the problems of the current SimuLST models?</sample>
    <sample id="482">What is our solution?</sample>
    <sample id="483">01 Use already existing offline models without re-training or adopting specific ST architecture</sample>
    <sample id="484">What is our solution?</sample>
    <sample id="485">我们的解决方案：FADAit</sample>
    <sample id="486">决定是否或不翻译基于最后收到的信息是否足够</sample>
    <sample id="487">01 I am going to talk about...</sample>
    <sample id="488">01 Ich werde reden.</sample>
    <sample id="489">01 I am going to talk about...</sample>
    <sample id="490">01 I am going to talk about...</sample>
    <sample id="491">EMITTED</sample>
    <sample id="492">I am going to talk about the climate.</sample>
    <sample id="493">EMITTED</sample>
    <sample id="494">I am going to talk about climate.</sample>
    <sample id="495">主结果：</sample>
    <sample id="496">主结果：</sample>
    <sample id="497">主结果：</sample>
    <sample id="498">主结果：</sample>
    <sample id="499">[0.27, 0.18333333, 0.945, 0.66]</sample>
    <sample id="500">Main Results:</sample>
    <sample id="501">主结果：</sample>
    <sample id="502">EDArt</sample>
    <sample id="503">EDAT is the fastest strategy considered in the elapsed time.</sample>
    <sample id="504">Do you want to discover more?</sample>
    <sample id="505">yes</sample>
    <sample id="506">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</sample>
    <sample id="507">图2比较了基于提示的训练与基于 finetuning 的训练。</sample>
    <sample id="508">图2比较了基于提示的训练与基于微调的训练。</sample>
    <sample id="509">语言-only</sample>
    <sample id="510">在预训练的多模态模型上进行指令调优</sample>
    <sample id="511">不平衡在指令性数据集之间NLP和Multimodal</sample>
    <sample id="512">不平衡在指令性数据集之间NLP和Multimodal</sample>
    <sample id="513">62 diverse tasks</sample>
    <sample id="514">62 diverse tasks</sample>
    <sample id="515">OFA, One All</sample>
    <sample id="516">图 1: MultiInstuct 中的四个任务示例</sample>
    <sample id="517">图 1: MultiInstuct 中的四个任务示例</sample>
    <sample id="518">图 1: MultiInstuct 中的四个任务示例</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">使用53组从50k中，每组10k，用于9个任务。</sample>
    <sample id="521">使用53组从53个任务中，每个任务10000个样本。</sample>
    <sample id="522">在实现细节中，训练细节：训练前的模型是所有任务的预训练OF-A-Large(472M)。在训练中，我们使用一个由五个任务模板之一组成的随机组合。在测试细节中，对于每个任务，我们总共进行五次实验，评估模型在其中一个任务模板上的表现。我们报告每次实验的平均性能和标准差。</sample>
    <sample id="523">在实现细节中，训练细节：训练前的模型是使用所有任务训练的OF-LA-large(472M)。在训练中，我们只在任务中做修改，而不是在任务外做修改。每个实例至少会随机组合在一个五种任务模板中。</sample>
    <sample id="524">在实现细节中，训练细节：训练前的模型是所有任务的预训练模型(472M)，在训练时，将不包括任何任务的实例。每个实例至少会组合使用一个随机的指令模板。</sample>
    <sample id="525">评价指标</sample>
    <sample id="526">Sensitivity</sample>
    <sample id="527">表2中展示了在MULTISTRUCT上对零-shot性能的评估。</sample>
    <sample id="528">表2中展示了在MULTISTRUCT上对零-shot性能的评估。</sample>
    <sample id="529">影响不断增加的多模态指令任务簇</sample>
    <sample id="530">在本研究中，我们使用了 OFA 模型，并在 ImageNet-1K 上进行了预训练。</sample>
    <sample id="531">在训练阶段，模型的参数是随机的。</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks</sample>
    <sample id="533">结论</sample>
    <sample id="534">One More!</sample>
    <sample id="535">UNIVERSITA DI TRENTO</sample>
    <sample id="536">Mohammad javad Hosseini</sample>
    <sample id="562">语言模型可接受性判断不是always robust to context</sample>
    <sample id="563">语言模型可接受性判断不是always robust to context</sample>
    <sample id="564">Revisiting Minimal Pair Paradigm</sample>
    <sample id="565">Revisiting Minimal Pair Paradigm</sample>
    <sample id="566">Revisiting Minimal Pair Paradigm</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="572">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="573">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="574">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="575">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="576">测试MPJ是否为长度、结构和可接受性的一个函数。</sample>
    <sample id="577">测试MPJ是否为长度、结构和可接受性的一致函数。</sample>
    <sample id="578">测试MPJ是否为长度、结构和可接受性函数。</sample>
    <sample id="579">测试MPJ是否为长度、结构和可接受性函数。</sample>
    <sample id="580">测试MPJ是否为长度、结构和可接受性函数。</sample>
    <sample id="581">MMP 判断是 robust for arbitrary context lengths with different contexts (acceptable/ unacceptable): We perform MPPs structure (different contexts to 700 tokens) matched/mismatched structure (lengths up to 900 tokens).</sample>
    <sample id="582">MMP 判断是 robust for arbitrary context lengths with different contexts (acceptable/ unacceptable): We perform MPPs structure (different contexts to 700 tokens) matched/mismatched structure (lengths up to 900 tokens).</sample>
    <sample id="583">我们执行MPP结构(不同上下文/长度)。</sample>
    <sample id="584">我们执行MPP结构(不同上下文/长度)。</sample>
    <sample id="585">我们执行MPP结构(不同上下文/长度)。</sample>
    <sample id="586">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="587">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="588">可接受/不可接受的MPP句子
我们对不同结构的MPP句子进行匹配/不匹配的实验。</sample>
    <sample id="589">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="590">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="591">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="592">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="593">为什么匹配的前缀会影响LM的判断?</sample>
    <sample id="594">Key Takeaways</sample>
    <sample id="595">Key Takeaways</sample>
    <sample id="596">Key Takeaways</sample>
    <sample id="597">one-hot vector</sample>
    <sample id="598">50,000</sample>
    <sample id="626">LHA</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如何？</sample>
    <sample id="629">CoNLL++ 数据集是通过从 2002 年到 2020 年的 Reuters 新闻中收集并注释而成。</sample>
    <sample id="630">XSemPLR: Cross-Linguistic Semantic Parsing in Multiple Natural Languages and Meaning Representations</sample>
    <sample id="631">语义解析</sample>
    <sample id="632">Cross-lingual Semantic Parsing</sample>
    <sample id="633">Cross-lingual Semantic Parsing</sample>
    <sample id="634">Cross-lingual Semantic Parsing</sample>
    <sample id="635">Cross-lingual semantic parsing</sample>
    <sample id="636">Cross-lingual semantic parsing</sample>
    <sample id="637">Cross-lingual Semantic Parsing</sample>
    <sample id="638">Cross-lingual Semantic Parsing</sample>
    <sample id="639">我们提供了一个统一的XSemPLR数据集，用于多语言和意义的跨语言语义解析。它包含：8种语法解析任务；22种语法表示法；15种自然语言；</sample>
    <sample id="640">我们提供了一个统一的XSemPLR数据集，用于跨语言的语义解析。它包含：</sample>
    <sample id="641">实验设置</sample>
    <sample id="642">实验设置</sample>
    <sample id="643">实验设置</sample>
    <sample id="644">实验设置</sample>
    <sample id="645">实验设置</sample>
    <sample id="646">实验设置</sample>
    <sample id="647">实验设置</sample>
    <sample id="648">实验设置</sample>
    <sample id="649">实验设置</sample>
    <sample id="650">实验设置</sample>
    <sample id="651">实验设置</sample>
    <sample id="652">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="653">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="654">我们评估了两组模型在单语设置上的表现。</sample>
    <sample id="655">我们评估了两组模型在单语设置下的性能。</sample>
    <sample id="656">我们评估了 mts 和 XML+PR+RTX(mT5) 的多语言设置。</sample>
    <sample id="657">我们评估了 mts 和 XML+PR+RTX(m75) 的多语言设置。</sample>
    <sample id="658">我们评估了 mTTS、XML+XLR+PTT、mTTS+XLR+PTT 多语言设置。</sample>
    <sample id="659">多语言训练的分析</sample>
    <sample id="660">Cross-lingual Performance Gap</sample>
    <sample id="661">Cross-lingual Performance Gap</sample>
    <sample id="662">图 16. Cross-lingual Performance Gap</sample>
    <sample id="663">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="664">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="665">· We conduct a comprehensive benchmark study on three representative types of multilingual language models.</sample>
    <sample id="666">Conclusion</sample>
    <sample id="667">参数化水印，基于Lexicon的水印，Backdoored Watermark，Adversarial-based Watermark</sample>
    <sample id="668">不足够</sample>
    <sample id="695">在训练中，将排列的未知性考虑进去。</sample>
    <sample id="696">下游 NLP 模型的公平性是指它在处理不同群体的输入时，输出的结果是否一致。</sample>
    <sample id="697">Richard Doutrou</sample>
    <sample id="698">Koushik Sinha</sample>
    <sample id="699">Myra Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="700">exotic</sample>
    <sample id="701">Othering through essentializing narratives</sample>
    <sample id="702">Pointwise P(CXMI)</sample>
    <sample id="703">DrBERT is a French-based model, while ChuBERT is an English-based one.</sample>
    <sample id="751">3</sample>
    <sample id="752">Iterative transfer learning</sample>
    <sample id="753">理解用户在做选择时的语言</sample>
    <sample id="754">通过在模型上部署 EaaS，攻击者可以提取模型参数。</sample>
    <sample id="755">3</sample>
    <sample id="756">15</sample>
    <sample id="757">University of Washington</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">GPT-4</sample>
    <sample id="760">因为模型的可接受性依赖于上下文。</sample>
    <sample id="761">Yes</sample>
    <sample id="762">no</sample>
    <sample id="763">BLEU, METEOR, ROUGE-L</sample>
    <sample id="764">No</sample>
    <sample id="765">因为 NLP 的立场很重要，因为 NLP 的立场很重要。</sample>
    <sample id="766">完整微调</sample>
    <sample id="767">Roberta-base + classifier head</sample>
    <sample id="768">C4, CC1M</sample>
    <sample id="769">3</sample>
    <sample id="770">1.5%</sample>
    <sample id="771">Alan Ritter</sample>
    <sample id="772">yes</sample>
    <sample id="773">three</sample>
    <sample id="774">DPT</sample>
    <sample id="833">Google</sample>
    <sample id="834">Storybrook University</sample>
    <sample id="835">English and Chinese</sample>
    <sample id="836">Robin Peng</sample>
    <sample id="837">研究了DELP-AP-48和DELP-WE-147两个模型。</sample>
    <sample id="838">57 个任务。</sample>
    <sample id="839">3</sample>
    <sample id="840">Copy Dataset, AG News, MIND, ST2N, ER2N, Spam</sample>
    <sample id="876">NACHOS 是一种语言模型。</sample>
    <sample id="877">David Warms Markes</sample>
    <sample id="878">big impact</sample>
    <sample id="879">卡内基梅隆大学语言技术研究所</sample>
    <sample id="880">5 expert-written instructions</sample>
    <sample id="881">使用来自多种来源的信息来测试模型。</sample>
    <sample id="882">Google  Prompting PaLM for Translation  Assessing Strategies and Performance</sample>
    <sample id="883">PaLM: Pathways Language Model</sample>
    <sample id="884">PalM: Pathways Language Model</sample>
    <sample id="885">贡献</sample>
    <sample id="886">贡献</sample>
    <sample id="887">贡献</sample>
    <sample id="888">贡献</sample>
    <sample id="889">提示对翻译质量有很大影响</sample>
    <sample id="890">提示对翻译质量有很大影响</sample>
    <sample id="891">提示对翻译质量有很大影响</sample>
    <sample id="892">-5步提示例</sample>
    <sample id="893">-5步提示例</sample>
    <sample id="894">-5步提示例</sample>
    <sample id="895">-5步提示例</sample>
    <sample id="896">-5步提示例</sample>
    <sample id="897">实验结果</sample>
    <sample id="898">实验结果</sample>
    <sample id="899">实验结果</sample>
    <sample id="900">实验结果</sample>
    <sample id="901">实验结果</sample>
    <sample id="902">实验结果</sample>
    <sample id="903">实验结果</sample>
    <sample id="904">实验结果</sample>
    <sample id="905">实验结果</sample>
    <sample id="906">谢谢</sample>
    <sample id="907">弱于你所想</sample>
    <sample id="908">弱于你所想</sample>
    <sample id="909">Why weakly supervised learning?</sample>
    <sample id="910">Why weakly supervised learning?</sample>
    <sample id="911">Why weakly supervised learning?</sample>
    <sample id="912">Why weakly supervised learning?</sample>
    <sample id="913">Why weakly supervised learning?</sample>
    <sample id="914">A common claim in recent WSL works</sample>
    <sample id="915">A common claim in recent WSL works</sample>
    <sample id="916">A common claim in recent WSL works</sample>
    <sample id="917">A common claim in recent WSL works</sample>
    <sample id="918">我们的研究问题</sample>
    <sample id="919">我们的研究问题</sample>
    <sample id="920">R01 Main findings</sample>
    <sample id="921">R01 Main findings</sample>
    <sample id="922">R01 Main findings</sample>
    <sample id="923">R01 Main findings</sample>
    <sample id="924">R01 Main findings</sample>
    <sample id="925">R02 Main findings</sample>
    <sample id="926">R02 Main findings</sample>
    <sample id="927">RQ2 Main findings</sample>
    <sample id="928">RQ2 Main findings</sample>
    <sample id="929">2. 但即使如此，使用它们作为训练样本也是更好的。</sample>
    <sample id="930">R03 Main findings</sample>
    <sample id="931">R03 Main findings</sample>
    <sample id="932">R03 Main findings</sample>
    <sample id="933">R03 Main findings</sample>
    <sample id="934">Conclusion</sample>
    <sample id="935">Conclusion</sample>
    <sample id="936">Conclusion</sample>
    <sample id="937">Conclusion</sample>
    <sample id="938">Conclusion</sample>
    <sample id="939">对话系统的常用评估方法是 Likert 量表。</sample>
    <sample id="940">5</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要以下背景知识： 1. 法官在法院工作。 2. 法官在法院裁决案件。</sample>
    <sample id="942">代码公开，可从 GitHub 获得。</sample>
    <sample id="943">yes</sample>
    <sample id="944">在可接受的域中添加一个单词。</sample>
    <sample id="945">进行维度评估意味着对对话的质量进行评估。</sample>
    <sample id="946">北京交通大学</sample>
    <sample id="947">在需要翻译成其他语言时，提示的形式很重要。</sample>
    <sample id="978">BART, BERT, Blender2, Emora, Blender Decoder</sample>
    <sample id="979">6</sample>
    <sample id="980">优秀规划器的理想品质是：1.能理解任务的约束；2.能理解任务的语义；3.能理解任务的多目标。</sample>
    <sample id="981">10</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">Institute of Computer Science, University of Warsaw</sample>
    <sample id="1021">Accuracy</sample>
    <sample id="1022">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1023">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1024">比较性评估</sample>
    <sample id="1025">比较评价</sample>
    <sample id="1026">对话质量的三个方面</sample>
    <sample id="1027">Liker 评价</sample>
    <sample id="1028">Liker 评价</sample>
    <sample id="1029">在聊天中，行为的分类有三个：无关、缺乏同情心和自我矛盾。</sample>
    <sample id="1030">在聊天中注释行为 (ABC-Eval)</sample>
    <sample id="1031">评价行为</sample>
    <sample id="1032">评价行为</sample>
    <sample id="1033">评价行为</sample>
    <sample id="1034">实验</sample>
    <sample id="1035">实验</sample>
    <sample id="1036">Turn Lifter Consistency Emotional Understanding Informative Overall Quality</sample>
    <sample id="1037">在演讲中，演讲者首先介绍了研究的背景和目的，然后详细介绍了实验设计和数据分析方法。</sample>
    <sample id="1038">预测效度</sample>
    <sample id="1039">预测效度</sample>
    <sample id="1040">Incremental Validity</sample>
    <sample id="1041">增量效度</sample>
    <sample id="1042">增量效度</sample>
    <sample id="1043">ABC-Eval Error Rates by Model</sample>
    <sample id="1044">图表中显示了不同模型的错误率。</sample>
    <sample id="1045">图表中的黄色箭头指出了不同模型的错误率。</sample>
    <sample id="1046">ABC-Eval Error Rates by Model</sample>
    <sample id="1047">谢谢收看！</sample>
    <sample id="1048">Emory University</sample>
    <sample id="1049">Continuously Fine-tuning</sample>
    <sample id="1050">6</sample>
    <sample id="1051">当翻译需要上下文?</sample>
    <sample id="1052">Translation depends on context</sample>
    <sample id="1053">Translation depends on context</sample>
    <sample id="1054">Translation depends on context</sample>
    <sample id="1055">评价上下文相关翻译很难。</sample>
    <sample id="1056">评价上下文相关翻译很难。</sample>
    <sample id="1057">RQ2: When does translation require context?</sample>
    <sample id="1058">RQ2: When does translation require context? - Word-level context usage RQ3: How well will models handle context-dependent translations?</sample>
    <sample id="1059">·CXMI, measure how much context models use given a corpus</sample>
    <sample id="1060">·CXMI, measure how much context models use given a corpus</sample>
    <sample id="1061">我们介绍 P-CXMI 来衡量上下文，以翻译特定</sample>
    <sample id="1062">RQ2: When does translation require context?  Word-level text usage  Thematic analysis RQ3: How well will models handle context-dependent translations?</sample>
    <sample id="1063">Thematic analysis of high-PCXMI words</sample>
    <sample id="1064">Thematic analysis of high-PCXMI words</sample>
    <sample id="1065">1. POS tags</sample>
    <sample id="1066">1. POS tags 2. Vocabulary items</sample>
    <sample id="1067">Thematic analysis of high P-CCMI words</sample>
    <sample id="1068">Thematic analysis of high P-CCMI words</sample>
    <sample id="1069">Thematic analysis of high P-CXMI words</sample>
    <sample id="1070">Q1: When does translation require context?</sample>
    <sample id="1071">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1073">MuDA benchmark</sample>
    <sample id="1074">RQ1: When does translation require context?</sample>
    <sample id="1075">Corpus-level metrics</sample>
    <sample id="1076">语料库度量</sample>
    <sample id="1077">语料库-level metrics</sample>
    <sample id="1078">MUDA benchmark results</sample>
    <sample id="1079">MUDa benchmark results</sample>
    <sample id="1080">MUUDA benchmark results</sample>
    <sample id="1081">总结</sample>
    <sample id="1082">总结</sample>
    <sample id="1083">总结</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">新方法有名称。</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">卡内基梅隆大学语言技术研究所</sample>
    <sample id="1124">Bouquet (Stanford)</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">CROWNS</sample>
    <sample id="1161">FT, L2R, COSINE, MLC, BOND</sample>
    <sample id="1162">13个任务</sample>
    <sample id="1226">CamemBERT 最初是在维基百科上训练的。</sample>
    <sample id="1227">Adam Prezrokowski and Michal Wozniak</sample>
    <sample id="1228">Performance degrades with larger temporal gap</sample>
    <sample id="1269">因为输出序列中的词元是乱序的，需要排序。</sample>
    <sample id="1270">To ensure that the methods used to mitigate bias are clear and understandable.</sample>
    <sample id="1271">最小对不可接受输入是任何一对单词，其中两个单词的最后一个字母不同。</sample>
    <sample id="1272">F1 score, Recall, Precision</sample>
    <sample id="1273">Krippendorff's alpha</sample>
    <sample id="1274">可接受查询</sample>
    <sample id="1275">Heinrich Heine University</sample>
    <sample id="1276">MultiInstruct 是第一个大规模的多模态指令基准。</sample>
    <sample id="1277">3</sample>
    <sample id="1278">二进制协调的定义是：在给定的长度下，所有可能的二进制数中，能被给定的二进制数整除的个数占总个数的比例。</sample>
    <sample id="1279">10.5</sample>
    <sample id="1280">这些发现对较小的 T5 模型有负面影响。</sample>
    <sample id="1281">DRBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains</sample>
    <sample id="1282">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1283">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1284">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1285">在Avignon University的课堂上，老师正在讲解有关语言建模在医疗保健领域的应用。</sample>
    <sample id="1286">语言建模</sample>
    <sample id="1287">语言建模</sample>
    <sample id="1288">语言建模</sample>
    <sample id="1289">语言建模</sample>
    <sample id="1290">比较预训练策略和数据源</sample>
    <sample id="1291">比较预训练策略和数据源</sample>
    <sample id="1292">比较预训练策略和数据源</sample>
    <sample id="1293">比较预训练策略和数据源</sample>
    <sample id="1294">比较预训练策略和数据源</sample>
    <sample id="1295">比较预训练策略和数据源</sample>
    <sample id="1296">比较预训练策略和数据源</sample>
    <sample id="1297">评价：数据源和大小</sample>
    <sample id="1298">评价：数据源和大小</sample>
    <sample id="1299">评价：数据源和大小</sample>
    <sample id="1300">评价：数据源和大小</sample>
    <sample id="1301">评价：数据源和大小</sample>
    <sample id="1302">评价：预训练策略</sample>
    <sample id="1303">评价：预训练策略</sample>
    <sample id="1304">评价：预训练策略</sample>
    <sample id="1305">在演讲中，演讲者强调了数据的重要性。</sample>
    <sample id="1306">在演讲中，演讲者强调了数据的重要性。</sample>
    <sample id="1307">在演讲中，演讲者强调了数据的重要性。</sample>
    <sample id="1308">感谢您！</sample>
    <sample id="1309">从头开始训练和从预训练模型开始训练</sample>
    <sample id="1310">10%</sample>
    <sample id="1311">BLEU score</sample>
    <sample id="1312">yes</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1315">Compositional Generalization</sample>
    <sample id="1316">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1317">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1318">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1319">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1320">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1322">Trees help a lot ...</sample>
    <sample id="1323">Trees help a lot ...</sample>
    <sample id="1324">The girl slept.</sample>
    <sample id="1325">The girl slept.</sample>
    <sample id="1326">Trees help a lot ...</sample>
    <sample id="1327">Trees help a lot ...</sample>
    <sample id="1328">Trees help a lot ...</sample>
    <sample id="1329">Our Approach</sample>
    <sample id="1330">Our Approach</sample>
    <sample id="1331">Our Approach</sample>
    <sample id="1332">Our Approach</sample>
    <sample id="1333">Our Approach</sample>
    <sample id="1334">Permuting with "jumps"</sample>
    <sample id="1335">Permuting with "jumps"</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Permuting with "jumps"</sample>
    <sample id="1338">Permuting with "jumps"</sample>
    <sample id="1339">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1340">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1341">技术挑战我们解决</sample>
    <sample id="1342">技术挑战我们解决</sample>
    <sample id="1343">我们解决的技术挑战</sample>
    <sample id="1344">我们解决的技术挑战</sample>
    <sample id="1345">我们解决的技术挑战。</sample>
    <sample id="1346">技术挑战我们解决</sample>
    <sample id="1347">认知失调是两个元素的不一致，例如，思想，行动，信仰。</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">yes</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">P-CXMI</sample>
    <sample id="1385">Matthias Lindemann, Alexander Koller, Ivan Tivot</sample>
    <sample id="1386">Cross-language transfer</sample>
    <sample id="1387">Saarlold University, Amazon Alexa, 3rd University of Vienna</sample>
    <sample id="1388">AL/AL, AL/CA, CA/CA</sample>
    <sample id="1389">The KITMUS Test</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources.</sample>
    <sample id="1391">NLU models draw on multiple knowledge sources.</sample>
    <sample id="1392">John saw the newly elected president on TV</sample>
    <sample id="1393">John saw the newly elected president on TV</sample>
    <sample id="1394">John saw the newly elected president on TV</sample>
    <sample id="1395">John saw the newly elected president on TV</sample>
    <sample id="1396">KITMUS Test Suite</sample>
    <sample id="1397">KITMUS Test Suite</sample>
    <sample id="1398">Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="1399">Servin is a judge. Kee is a baker. Servin and Kee met at a park. After a long day of work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="1400">1 Entity-specific knowledge 2 Background knowledge</sample>
    <sample id="1401">1. Entity-specific knowledge</sample>
    <sample id="1402">1. Entity-specific knowledge</sample>
    <sample id="1403">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1404">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1405">a) Background-Pretain: Typical setup b) Entity-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference time</sample>
    <sample id="1406">背景-Pretrain</sample>
    <sample id="1407">Chichester is a politician.</sample>
    <sample id="1408">Chichester is a politician. Chichester is seeking election in a government.</sample>
    <sample id="1409">Chichester is a politician.</sample>
    <sample id="1410">背景-Pretrain</sample>
    <sample id="1411">背景-Pretrain</sample>
    <sample id="1412">背景-Pretrain</sample>
    <sample id="1413">背景-推理</sample>
    <sample id="1414">Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at \url{https://github.com/mpemskitt/kittus}.</sample>
    <sample id="1415">Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at \url{https://github.com/mpemskitt/kfuzz}.</sample>
    <sample id="1416">Trees help a lot...</sample>
    <sample id="1417">Georgia Institute of Technology</sample>
    <sample id="1418">Marked Personas</sample>
    <sample id="1419">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1420">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1421">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1422">Marked Persons: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1423">如何突破这些限制?</sample>
    <sample id="1424">如何突破这些限制?</sample>
    <sample id="1425">如何突破这些限制?</sample>
    <sample id="1426">输出：Persona Examples (GPT-4)</sample>
    <sample id="1427">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1428">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1430">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1431">2. 2 steps</sample>
    <sample id="1432">2. 2 steps</sample>
    <sample id="1433">2. 2 steps</sample>
    <sample id="1434">2步 1. 人物：使用像“Imagine you are an”这样的提示生成人物。 2. 标记单词：找到区分标记组和未标记组的单词。</sample>
    <sample id="1435">2步 1. 人物：使用像“Imagine you are an”这样的提示生成人物。 2. 标记单词：找到区分标记组和未标记组的单词。 具体的，不需要词汇表</sample>
    <sample id="1436">Insight for 2: Marked Words</sample>
    <sample id="1437">Insight for 2: Marked Words</sample>
    <sample id="1438">2. 有标记的群体不同寻常。</sample>
    <sample id="1439">Step 2: Marked Words</sample>
    <sample id="1440">Step 2: Marked Words</sample>
    <sample id="1441">Step 2: Marked Words</sample>
    <sample id="1442">结果：比较到人类响应</sample>
    <sample id="1443">但... 这份词典是不完整的</sample>
    <sample id="1444">但... 这份词典是不完整的</sample>
    <sample id="1445">但... 这份词典是不完整的</sample>
    <sample id="1446">但... 这份词典是不完整的</sample>
    <sample id="1447">其他通过essentializing narratives:</sample>
    <sample id="1448">其他通过essentializing narratives:</sample>
    <sample id="1449">其他通过essentializing naratives:</sample>
    <sample id="1450">其他通过essentializing naratives:</sample>
    <sample id="1451">其他通过essentializing naratives:</sample>
    <sample id="1452">其他通过essentializing naratives:</sample>
    <sample id="1453">其他通过essentializing narratives:</sample>
    <sample id="1454">其他通过essentializing narratives。</sample>
    <sample id="1455">其他通过essentializing narratives。</sample>
    <sample id="1456">其他通过essentializing narratives:</sample>
    <sample id="1457">其他通过essentializing narratives:</sample>
    <sample id="1458">推荐意见</sample>
    <sample id="1459">推荐</sample>
    <sample id="1460">推荐</sample>
    <sample id="1461">推荐意见</sample>
    <sample id="1462">推荐意见</sample>
    <sample id="1463">推荐意见</sample>
    <sample id="1464">推荐意见</sample>
    <sample id="1465">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="1466">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="1467">背景</sample>
    <sample id="1468">背景</sample>
    <sample id="1469">背景</sample>
    <sample id="1470">背景</sample>
    <sample id="1471">Motivation</sample>
    <sample id="1472">挑战</sample>
    <sample id="1473">挑战</sample>
    <sample id="1474">挑战</sample>
    <sample id="1475">挑战</sample>
    <sample id="1476">[1] [2] [3] [4] [5] [6]</sample>
    <sample id="1477">[1] [2] [3] [4] [5] [6]</sample>
    <sample id="1478">[1] [2] [3] [4] [5] [6]</sample>
    <sample id="1479">EmbMarker</sample>
    <sample id="1480">EmbMarker</sample>
    <sample id="1481">EmbMarker</sample>
    <sample id="1482">Watermark injection</sample>
    <sample id="1483">Watermark injection</sample>
    <sample id="1484">Watermark injection</sample>
    <sample id="1485">EmbMarker</sample>
    <sample id="1486">EmbMarker</sample>
    <sample id="1487">EmbMarker</sample>
    <sample id="1488">- Copyright verification</sample>
    <sample id="1489">- Copyright verification</sample>
    <sample id="1490">实验结果</sample>
    <sample id="1491">实验结果</sample>
    <sample id="1492">实验结果</sample>
    <sample id="1493">实验结果</sample>
    <sample id="1494">谢谢！</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">2012</sample>
    <sample id="1497">转移和主动学习在分歧检测中的应用：解决少数类挑战</sample>
    <sample id="1498">What is Cognitive Dissonance?</sample>
    <sample id="1499">认知失调是什么?</sample>
    <sample id="1500">认知失调是什么?</sample>
    <sample id="1501">认知失调是什么?</sample>
    <sample id="1502">Why dissidence?</sample>
    <sample id="1503">Why dissidence?</sample>
    <sample id="1504">Why dissidence?</sample>
    <sample id="1505">Why dissidence?</sample>
    <sample id="1506">-3.5%</sample>
    <sample id="1507">-3.5%</sample>
    <sample id="1508">-3.5%</sample>
    <sample id="1509">训练初始注释集</sample>
    <sample id="1510">训练初始注释集</sample>
    <sample id="1511">方法：Transfer and Active Learning for Annotate Rare Class</sample>
    <sample id="1512">冷启动注释：迁移学习</sample>
    <sample id="1513">冷启动注释：迁移学习</sample>
    <sample id="1514">冷启动注释：迁移学习</sample>
    <sample id="1515">冷启动注释：转移学习</sample>
    <sample id="1516">Cold-start Annotations: Transfer Learning</sample>
    <sample id="1517">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1518">主动学习：累积 vs 迭代更新</sample>
    <sample id="1519">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1520">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1521">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1522">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1523">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1524">PRC is simple &amp; efficient for rare sample acquisition</sample>
    <sample id="1525">PRC is simple &amp; efficient for rare sample acquisition</sample>
    <sample id="1526">Thank you!</sample>
    <sample id="1527">University of Amsterdam</sample>
    <sample id="1528">Chen Zhen</sample>
    <sample id="1529">4</sample>
    <sample id="1530">walk-k-walk</sample>
  </task>
</testset>