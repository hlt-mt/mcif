<testset name="IWSLT2025" type="output">
  <task track="short" text_lang="it">
    <sample id="0">Google, Wikipedia, and web pages</sample>
    <sample id="1">McGill, Mila, Microsoft Research</sample>
    <sample id="2">DEPLAIN: A German Parallel Corpus with Intra- and Intertextual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Mommen, Laura Kallmeyer Heinrich Heine University Dissolfort Germany ACL 2023</sample>
    <sample id="3">DEPLAIN: A German Parallel Corpus with Intra- and Intertextual Translations into Plain Language for Sentence and Document Simplification</sample>
    <sample id="4">Esempio di Simplificazione del Testo</sample>
    <sample id="5">Esempio di Simplificazione del Testo</sample>
    <sample id="6">Esempio di Simplificazione del Testo</sample>
    <sample id="7">Esempio di Simplificazione del Testo</sample>
    <sample id="8">German Text Simplification Corpus</sample>
    <sample id="9">German Text Simplification Corpora</sample>
    <sample id="10">German Text Simplification Corpus</sample>
    <sample id="11">German Text Simplification Corpus</sample>
    <sample id="12">German Text Simplification Corpus</sample>
    <sample id="13">German Text Simplification Corpus</sample>
    <sample id="14">Simplicity is a key factor in the simplification of language.</sample>
    <sample id="15">Sono due slide.</sample>
    <sample id="16">Simplicity is a key factor in the simplification of language.</sample>
    <sample id="17">Simplicity is a key factor in the simplification of language.</sample>
    <sample id="18">Sono due slide.</sample>
    <sample id="19">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="20">Results of the alignment methods with 1/11 part and rm capabilities (lower part)</sample>
    <sample id="21">Results of the alignment methods with 11/11 part and rm capabilities (lower part)</sample>
    <sample id="22">Results of the alignment methods with 1/11 part and rm capabilities (lower part)</sample>
    <sample id="23">Results of the alignment methods with 1/11 part and rm capabilities (lower part)</sample>
    <sample id="24">Results of the alignment methods with 1/11 part and rm capabilities (lower part)</sample>
    <sample id="25">Results of the alignment methods with 1/11 part and nm capabilities (lower part)</sample>
    <sample id="26">Results of the alignment methods with 1/11 part and rm capabilities (lower part)</sample>
    <sample id="27">Automatic Text Simplification</sample>
    <sample id="28">Automatic Text Simplification</sample>
    <sample id="29">Automatic Text Simplification</sample>
    <sample id="30">Automatic Text Simplification</sample>
    <sample id="31">Automatic Text Simplification</sample>
    <sample id="32">Automatic Text Simplification</sample>
    <sample id="33">Automatic Text Simplification</sample>
    <sample id="34">Grazie.</sample>
    <sample id="35">Patric F. Neuman</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it allows annotators to rate the relevance and self-contradiction of each utterance in a dialogue, rather than just the entire dialogue.</sample>
    <sample id="39">The success of the current unsupervised approach is largely based on the quality of the pre-training data.</sample>
    <sample id="40">Easy, Medium, Hard</sample>
    <sample id="41">6</sample>
    <sample id="42">Adam Prezrokowski and Michal Wozniak</sample>
    <sample id="43">Bouquet (Stamford) (Universal Dependencies):</sample>
    <sample id="44">Bouquet/Stanford (Universal Dependencies):</sample>
    <sample id="45">Bouquet (Stanford) (Universal Dependencies):</sample>
    <sample id="46">Bouquet (Stanford) (Universal Dependencies):</sample>
    <sample id="47">Bouquet (Stamford) (Universal Dependencies):</sample>
    <sample id="48">Estratto da: "Dependency Structure of Coordination"</sample>
    <sample id="49">Bouquet (Stamford) (Universal Dependencies):</sample>
    <sample id="50">Bouquet (Stanford) (Universal Dependencies):</sample>
    <sample id="51">Dipendenze di lunghezza minimizzazione (DLM)</sample>
    <sample id="52">Dipendenze di lunghezza minimizzazione (DLM)</sample>
    <sample id="53">Dipendenze di lunghezza minimizzazione (DLM)</sample>
    <sample id="54">Dipendenze di lunghezza minimizzazione (DLM)</sample>
    <sample id="55">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="56">Dipartimento di Informatica e Sistemistica</sample>
    <sample id="57">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="58">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="59">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="60">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="61">Dipendenza della lunghezza minimizzata (DLM)</sample>
    <sample id="62">Conjunction Lengths in English</sample>
    <sample id="63">Conjunction Lengths in English</sample>
    <sample id="64">Conjunction Lengths in English</sample>
    <sample id="65">Conclusions about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Fickers &amp; Goldberg, 2016) tend to be shorter, observed before when Gibson et al. (1998-90) this tendency grows with length difference.</sample>
    <sample id="66">Conjunction Lengths in English</sample>
    <sample id="67">Conjunction Lengths in English</sample>
    <sample id="68">Conjunction Lengths in English</sample>
    <sample id="69">Conjunction Lengths in English</sample>
    <sample id="70">The graph shows the power of a test for detecting differences in the proportion of characters on the left and right sides of a word.</sample>
    <sample id="71">The graph shows the proportion of the difference in the length of the left and right characters.</sample>
    <sample id="72">The graph shows the proportion of the difference in the length of the right and left characters.</sample>
    <sample id="73">Compatibility with Dependency Structures of Coordination</sample>
    <sample id="74">Vedi il paper per l'argomento completo!</sample>
    <sample id="75">3</sample>
    <sample id="76">news</sample>
    <sample id="77">not it is</sample>
    <sample id="78">Yes</sample>
    <sample id="79">Apa</sample>
    <sample id="80">Bigger model size, more fine-tuning examples</sample>
    <sample id="81">Through the use of a linear regression model.</sample>
    <sample id="82">The experiments were designed to study the effect of the governor position by varying the length of the right-hand side of the governor.</sample>
    <sample id="83">43.901</sample>
    <sample id="84">4</sample>
    <sample id="85">Do and I</sample>
    <sample id="86">Formal/lexical cohesion, ellipsis, pronouns, verb form</sample>
    <sample id="87">Johns Hopkins University, Purdue University</sample>
    <sample id="122">Through Pearson's r scores</sample>
    <sample id="155">The results of the previous study showed that human subjects using the same prompts produced similar descriptions.</sample>
    <sample id="156">Penn Treebank (Marcus et al., 1993) and the Penn Treebank version 2 (Marcus et al., 1993), the Penn Treebank version 3 (Marcus et al., 1993), the Penn Treebank version 4 (Marcus et al., 1993), the Penn Treebank version 5 (Marcus et al., 1993), the Penn Treebank version 6 (Marcus et al., 1993), the Penn Treebank version 7 (Marcus et al., 1993), the Penn Treebank version 8 (Marcus et al., 1993), the Penn Treebank version 9 (Marcus et al., 1993), the Penn Treebank version 10 (Marcus et al., 1993), the Penn Treebank version 11 (Marcus et al., 1993), the Penn Treebank version 12 (Marcus et al., 1993), the Penn Treebank version 13 (Marcus et al., 1993), the Penn Treebank version 14 (Marcus et al., 1993), the Penn Treebank version 15 (Marcus et al., 1993), the Penn Treebank version 16 (Marcus et al., 1993), the Penn Treebank version 17 (Marcus et al., 1993), the Penn Treebank version 18 (Marcus et al., 1993), the Penn Treebank version 19 (Marcus et al., 1993), the Penn Treebank version 20 (Marcus et al., 1993), the Penn Treebank version 21 (Marcus et al., 1993), the Penn Treebank version 22 (Marcus et al., 1993), the Penn Treebank version 23 (Marcus et al., 1993), the Penn Treebank version 24 (Marcus et al., 1993), the Penn Treebank version 25 (Marcus et al., 1993), the Penn Treebank version 26 (Marcus et al., 1993), the Penn Treebank version 27 (Marcus et al., 1993), the Penn Treebank version 28 (Marcus et al., 1993), the Penn Treebank version 29 (Marcus et al., 1993), the Penn Treebank version 30 (Marcus et al., 1993), the Penn Treebank version 31 (Marcus et al., 1993), the Penn Treebank version 32 (Marcus et al., 1993), the Penn Treebank version 33 (Marcus et al., 1993), the Penn Treebank version 34 (Marcus et al., 1993), the Penn Treebank version 35 (Marcus et al., 1993), the Penn Treebank version 36 (Marcus et al., 1993), the Penn Treebank version 37 (Marcus et al., 1993), the Penn Treebank version 38 (Marcus et al., 1993), the Penn Treebank version 39 (Marcus et al., 1993), the Penn Treebank version 40 (Marcus et al., 1993), the Penn Treebank version 41 (Marcus et al., 1993), the Penn Treebank version 42 (Marcus et al., 1993), the Penn Treebank version 43 (Marcus et al., 1993), the Penn Treebank version 44 (Marcus et al., 1993), the Penn Treebank version 45 (Marcus et al., 1993), the Penn Treebank version 46 (Marcus et al., 1993), the Penn Treebank version 47 (Marcus et al., 1993), the Penn Treebank version 48 (Marcus et al., 1993), the Penn Treebank version 49 (Marcus et al., 1993), the Penn Treebank version 50 (Marcus et al., 1993), the Penn Treebank version 51 (Marcus et al., 1993), the Penn Treebank version 52 (Marcus et al., 1993), the Penn Treebank version 53 (Marcus et al., 1993), the Penn Treebank version 54 (Marcus et al., 1993), the Penn Treebank version 55 (Marcus et al., 1993), the Penn Treebank version 56 (Marcus et al., 1993), the Penn Treebank version 57 (Marcus et al., 1993), the Penn Treebank version 58 (Marcus et al., 1993), the Penn Treebank version 59 (Marcus et al., 1993), the Penn Treebank version 60 (Marcus et al., 1993), the Penn Treebank version 61 (Marcus et al., 1993), the Penn Treebank version 62 (Marcus et al., 1993), the Penn Treebank version 63 (Marcus et</sample>
    <sample id="157">2</sample>
    <sample id="158">Risposta: Annotazione, rafforzamento, fine-tuning.</sample>
    <sample id="159">2</sample>
    <sample id="160">6</sample>
    <sample id="161">It is more general and flexible.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">DeepL and Google Translate</sample>
    <sample id="164">From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</sample>
    <sample id="165">LM Training Data</sample>
    <sample id="166">LM Training Data</sample>
    <sample id="167">LM Training Data</sample>
    <sample id="168">LM Training Data A mixed blessing</sample>
    <sample id="169">To this end</sample>
    <sample id="170">To this end</sample>
    <sample id="171">To this end</sample>
    <sample id="172">Evaluating LM Political Learning</sample>
    <sample id="173">The existing LMs are placed on a two-dimensional grid.</sample>
    <sample id="174">Esistono LM esistenti</sample>
    <sample id="175">Pretuning Data</sample>
    <sample id="176">Pretuning Data Further pretrain LM (RoBERTa, GPT-2, GPT-check) Evaluate change in political leaning</sample>
    <sample id="177">Results Partisan shifts in LM political leaning</sample>
    <sample id="178">Results Partisan shifts in LM political leaning</sample>
    <sample id="179">Results Partisan shifts in LM political leaning</sample>
    <sample id="180">The Trump Card</sample>
    <sample id="181">The Trump Card</sample>
    <sample id="182">The Trump Card</sample>
    <sample id="183">Per-Category Performance</sample>
    <sample id="184">Per-Category Performance</sample>
    <sample id="185">Per-Category Performance</sample>
    <sample id="186">Per-Category Performance</sample>
    <sample id="187">Per-Category Performance</sample>
    <sample id="188">Per-Category Performance</sample>
    <sample id="189">Per-Category Performance</sample>
    <sample id="190">Qualitative Analysis</sample>
    <sample id="191">Qualitative Analysis</sample>
    <sample id="192">Table 12: Hate Speech Test.</sample>
    <sample id="193">Table 12: Quantitative analysis of hate speech samples with different political leaning.</sample>
    <sample id="194">Table 12: Quantitative analysis of hate speech samples with different political leaning.</sample>
    <sample id="195">Nel test, il modello ha ottenuto un punteggio di 80% per la comprensione del testo.</sample>
    <sample id="196">Discussione</sample>
    <sample id="197">Discussione</sample>
    <sample id="198">Discussione</sample>
    <sample id="199">Grazie!</sample>
    <sample id="200">6</sample>
    <sample id="201">900</sample>
    <sample id="202">Music Selection, Book Selection, Recipe Selection</sample>
    <sample id="203">The perspectives people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="204">Dawid Zuch, Dawid Zhù, Dawid Zhùn</sample>
    <sample id="205">No</sample>
    <sample id="206">4</sample>
    <sample id="207">No</sample>
    <sample id="208">Background-Pretrain, Background-Both, Background-Inference</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">How to use the available clean samples more efficiently</sample>
    <sample id="211">It is the average of the absolute values of the differences between the predicted and actual values.</sample>
    <sample id="212">Wenjun Peng</sample>
    <sample id="213">Worse</sample>
    <sample id="214">Contesto di testo</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford Engineering Computer Science</sample>
    <sample id="217">Because existing methods are not sufficient.</sample>
    <sample id="218">Jackie CK Chuang</sample>
    <sample id="219">It is complex.</sample>
    <sample id="220">yes</sample>
    <sample id="221">No</sample>
    <sample id="222">The watermark is added to the original embedding.</sample>
    <sample id="223">PennState, Amazon</sample>
    <sample id="224">yes</sample>
    <sample id="225">How to make a chocolate cake</sample>
    <sample id="226">They use a random seed to ensure that the embedding visualization is not influenced by the order of data points.</sample>
    <sample id="227">By pre-training on a large corpus of text.</sample>
    <sample id="228">Pakistan</sample>
    <sample id="229">The model learns to attend to the specific parameters of the SimST architecture.</sample>
    <sample id="230">The more activities, the better the performance.</sample>
    <sample id="231">LSTM, T5-seq2seq, Zhen and Lapata</sample>
    <sample id="232">Masters</sample>
    <sample id="233">Crowder</sample>
    <sample id="234">NL Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="235">NLP Positionality: Characterizing Design Biases of Datasets and Models</sample>
    <sample id="236">Immaginate...</sample>
    <sample id="237">Immaginate...</sample>
    <sample id="238">Immaginate...</sample>
    <sample id="239">Immaginate... ... un esempio di bias di progettazione.</sample>
    <sample id="240">Positionality</sample>
    <sample id="241">Positionality</sample>
    <sample id="242">Positionality</sample>
    <sample id="243">Do datasets and models have personality?</sample>
    <sample id="244">Do datasets and models have personality?</sample>
    <sample id="245">Do datasets and models have positionality?</sample>
    <sample id="246">Do datasets and models have positionality?</sample>
    <sample id="247">Do datasets and models have positionality?</sample>
    <sample id="248">Do datasets and models have positionality?</sample>
    <sample id="249">Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models</sample>
    <sample id="250">NLP Positionality</sample>
    <sample id="251">Framework</sample>
    <sample id="252">Riutilizza i dataset riscattati con annotatori diversi.</sample>
    <sample id="253">Il framework illustrato nel video rappresenta un processo di raccolta e processamento di dati per una determinata analisi. Inizia con la raccolta di 20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 re-annotation with diverse annotators.</sample>
    <sample id="254">Riutilizza i dataset esistenti e riassegni i campioni ai diversi annotatori.</sample>
    <sample id="255">Framework</sample>
    <sample id="256">Framework</sample>
    <sample id="257">LabInTheWild</sample>
    <sample id="258">LabintheWild</sample>
    <sample id="259">Task A: Social Acceptability</sample>
    <sample id="260">Task A: Social Acceptability</sample>
    <sample id="261">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi GPT-4</sample>
    <sample id="262">Task B: Toxicity</sample>
    <sample id="263">Task B: Toxicity Analysis</sample>
    <sample id="264">Risultato 1: Esiste posizionalità in NLP.</sample>
    <sample id="265">Dati e modelli sono più adatti ai paesi parlanti lingua inglese.</sample>
    <sample id="266">Dati e modelli socialmente accettabili (GPT-4) e modelli sono più alined con una istruzione universitaria.</sample>
    <sample id="267">Dati e modelli sono i più alinati con una educazione universitaria.</sample>
    <sample id="268">Risultato 2: Alcune popolazioni sono rimaste indietro.</sample>
    <sample id="269">Dati e modelli sono meno adatti a persone non binarie.</sample>
    <sample id="270">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="271">Raccomandazioni</sample>
    <sample id="272">Raccomandazioni</sample>
    <sample id="273">Thanks!</sample>
    <sample id="274">3</sample>
    <sample id="275">Incorporare una diversità di voci e punti di vista in entrambe le etichette e i campi di testo.</sample>
    <sample id="276">The 6th Annual Meeting of the Association for Computational Linguistics Toronto, Canada 2023</sample>
    <sample id="277">Large language models (LLMs) can effectively decompose goals into steps</sample>
    <sample id="278">Language Planning</sample>
    <sample id="279">Constrained Language Planning</sample>
    <sample id="280">Constrained Language Planning</sample>
    <sample id="281">Constrained Language Planning</sample>
    <sample id="282">Come fanno i modelli di apprendimento automatico (LLMs) funzionare in pianificazione linguistica vincolata?</sample>
    <sample id="283">Come fanno i modelli di apprendimento automatico (LLMs) funzionare in pianificazione linguistica vincolata?</sample>
    <sample id="284">Come fanno i LLMs (Large Language Models) funzionare con i vincoli?</sample>
    <sample id="285">Can LLMs do Constrained Language Planning?</sample>
    <sample id="286">Can LLMs do Constrained Language Planning?</sample>
    <sample id="287">What types of errors do LLMs usually make in this task?</sample>
    <sample id="288">What types of errors do LLMs usually make in this task?</sample>
    <sample id="289">InstructGPT typically fails</sample>
    <sample id="290">Metodo</sample>
    <sample id="291">Metodo</sample>
    <sample id="292">Metodo</sample>
    <sample id="293">Metodo</sample>
    <sample id="294">Metodo</sample>
    <sample id="295">Metodo</sample>
    <sample id="296">Inserisci il testo qui.</sample>
    <sample id="297">Script Distillation from LLMs</sample>
    <sample id="298">Script Distillation from LLMs</sample>
    <sample id="299">Script Distillation from LLMs</sample>
    <sample id="300">Script Distillation from LLMs</sample>
    <sample id="301">Script Distillation from LLMs</sample>
    <sample id="302">Constrained language models are a type of AI model that is designed to generate text based on specific constraints or rules. These constraints can be used to control the output of the model, ensuring that it produces text that meets certain criteria or requirements.</sample>
    <sample id="303">Specialized Models vs. LLMs</sample>
    <sample id="304">Summary and Takeaways</sample>
    <sample id="305">Sommario e punti chiave</sample>
    <sample id="306">The 6th Annual Meeting of the Association for Computational Linguistics will be held in Toronto, Canada from July 17 to 23, 2023.</sample>
    <sample id="307">Fluency of PaLM is comparable to SOTA.</sample>
    <sample id="308">Utility, should not degrade the utility of the provided embeddings, should cover the attacker, transferability</sample>
    <sample id="309">Arabic, Chinese, Czech, Danish, Dutch, French, German, Hebrew, Italian, Japanese, Korean, Portuguese, Romanian, Turkish</sample>
    <sample id="310">20</sample>
    <sample id="311">similarity difference and χ² of KS test</sample>
    <sample id="312">Pretrained</sample>
    <sample id="344">Contano la frequenza di una parola in un corpus generale D.</sample>
    <sample id="345">Shuhui Shu and Alan Ritter from the School of Interactive Computing at Georgia Institute of Technology discuss whether CoNLL-2003 named entity tags still work well in 2023.</sample>
    <sample id="346">Named Entity Recognition &amp; Generalization</sample>
    <sample id="347">Named Entity Recognition &amp; Generalization</sample>
    <sample id="348">Named Entity Recognition &amp; Generalization</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization</sample>
    <sample id="350">ConLL+ Dataset</sample>
    <sample id="351">ConLL++ Dataset</sample>
    <sample id="352">ConLL+ Dataset</sample>
    <sample id="353">Quali sono i requisiti per una buona generalizzazione?</sample>
    <sample id="354">What is Needed for Good Generalization?</sample>
    <sample id="355">What is Needed for Good Generalization?</sample>
    <sample id="356">Quali sono i fattori necessari per ottenere una buona generalizzazione?</sample>
    <sample id="357">What Causes Performance Drop?</sample>
    <sample id="358">What Causes Performance Drop?</sample>
    <sample id="359">What Causes Performance Drop?</sample>
    <sample id="360">What causes performance drop?</sample>
    <sample id="361">Quali sono i fattori che causano la caduta del prestigio?</sample>
    <sample id="362">What Causes Performance Drop?</sample>
    <sample id="363">What causes performance drop?</sample>
    <sample id="364">What causes Performance Drop?</sample>
    <sample id="365">What causes Performance Drop?</sample>
    <sample id="366">Conclusions. For a good generalization, we need: - Larger model size - Better architecture - More fine-tuning examples</sample>
    <sample id="367">Conclusions</sample>
    <sample id="368">Conclusions</sample>
    <sample id="369">Conclusions</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09474</sample>
    <sample id="397">small</sample>
    <sample id="398">Servin is a judge and Kea is a baker.</sample>
    <sample id="399">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="400">Roberta, GPT-2 e GPT-4</sample>
    <sample id="401">specific level</sample>
    <sample id="402">The first one, the one I used</sample>
    <sample id="403">Brain Technologies Inc., University of Toronto, University of Alberta</sample>
    <sample id="404">6</sample>
    <sample id="405">Yes</sample>
    <sample id="406">a woman warrior</sample>
    <sample id="407">LSTM, RNN</sample>
    <sample id="408">MSL, L2, COSINE, L2+MNL, BFI, Adapter</sample>
    <sample id="409">6</sample>
    <sample id="410">multimodal</sample>
    <sample id="439">Inference-time knowledge</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes, it has been.</sample>
    <sample id="442">Support for limited discourse, phenomena and languages</sample>
    <sample id="443">Risolvere le espressioni di riferimento indiretto per la selection degli entità (AEntities Corpus)</sample>
    <sample id="444">Risoluzione di riferimenti indiretti per la selection di entità (AEntities Corpus)</sample>
    <sample id="445">Indirect Referring Expressions</sample>
    <sample id="446">Indirect Referring Expressions</sample>
    <sample id="447">Indirect Referring Expressions</sample>
    <sample id="448">Indirect Referring Expressions</sample>
    <sample id="449">Indirect Referring Expressions</sample>
    <sample id="450">Dataset Collection</sample>
    <sample id="451">Dataset Collection</sample>
    <sample id="452">Dataset Collection Methodology</sample>
    <sample id="453">Dataset Collection Methodology</sample>
    <sample id="454">Dataset Collection Methodology</sample>
    <sample id="455">Dataset Collection Methodology</sample>
    <sample id="456">Dataset Collection Methodology</sample>
    <sample id="457">Dataset Collection Methodology</sample>
    <sample id="458">Generare alternative questioni =&gt; entità più semplici</sample>
    <sample id="459">Generare alternative questioni =&gt; entità più semplici</sample>
    <sample id="460">Generare alternative questioni =&gt; entità più semplici</sample>
    <sample id="461">Generare alternative questioni =&gt; entità più semplici</sample>
    <sample id="462">Generare alternative questioni =&gt; entità più semplici</sample>
    <sample id="463">Background knowledge (Music)</sample>
    <sample id="464">Background knowledge (Music)</sample>
    <sample id="465">Background knowledge (Music)</sample>
    <sample id="466">Simmil Cake is widely eaten in the United Kingdom and other countries with their associated patterns. Pandan Cake is a light, fluffy sponge cake made from juices of Pandanus flowers.</sample>
    <sample id="467">Eliciting expressions</sample>
    <sample id="468">Eliciting expressions</sample>
    <sample id="469">Alireza Maleki</sample>
    <sample id="470">Alireza Maleki</sample>
    <sample id="471">Alireza Maleki</sample>
    <sample id="472">Alireza Javahdi</sample>
    <sample id="473">walk-k, LA, CA, ED</sample>
    <sample id="474">L'Institut de Recherche en Informatique et Systèmes d'Information (IRISA), L'Agence Nationale de Santé (ANSS), L'Université de Nantes, L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Génie Civil et Environnement (GCI), L'Institut de Gér</sample>
    <sample id="475">Jennifer T. Lang</sample>
    <sample id="476">3</sample>
    <sample id="477">Attention as a Guide for Simultaneous Speech Translation</sample>
    <sample id="478">Simultaneous speech translation (SimuST)</sample>
    <sample id="479">What are the problems of the current SimILST models?</sample>
    <sample id="480">What are the problems of the current SimuLST models?</sample>
    <sample id="481">What are the problems of the current SimuLST models?</sample>
    <sample id="482">What is our solution?</sample>
    <sample id="483">What is our solution?</sample>
    <sample id="484">What is our solution?</sample>
    <sample id="485">Il nostro approccio: FADit</sample>
    <sample id="486">Our solution: FDAit: Encoder-Decoder Attention</sample>
    <sample id="487">Our solution: FADiT Encoder-Decoder Attention</sample>
    <sample id="488">Our solution: EDAt</sample>
    <sample id="489">Our solution: FADA!</sample>
    <sample id="490">Our solution: Encoder-Decoder Attention</sample>
    <sample id="491">Our solution: FADA Encoder-Decoder Attention</sample>
    <sample id="492">Our solution: EDAt</sample>
    <sample id="493">Our solution: Encoder-Decoder Attention</sample>
    <sample id="494">Our solution: Encoder-Decoder Attention</sample>
    <sample id="495">Main Results:</sample>
    <sample id="496">Main Results: FADait:</sample>
    <sample id="497">Main Results:</sample>
    <sample id="498">The main results of the study are presented in a graph.</sample>
    <sample id="499">The graph shows that the main results of the FADaiT are presented.</sample>
    <sample id="500">Main Results: state of the architecture, specifically tailored for SimEDiT</sample>
    <sample id="501">Main Results:</sample>
    <sample id="502">Main Results:</sample>
    <sample id="503">Main Results:</sample>
    <sample id="504">Do you want to discover more?</sample>
    <sample id="505">yes</sample>
    <sample id="506">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning</sample>
    <sample id="507">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="508">Pre-trained Language Models for Downstream Tasks</sample>
    <sample id="509">Traduci il contenuto inglese in italiano.</sample>
    <sample id="510">Inserisci il testo inglese qui.</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="513">MULTINSTRUCT</sample>
    <sample id="514">MULTINSTRUCT</sample>
    <sample id="515">OFA, One All</sample>
    <sample id="516">Ecco le istanze esempiate:</sample>
    <sample id="517">MoltiInstruct</sample>
    <sample id="518">MULTIINSTUCT</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Tuning</sample>
    <sample id="521">Multi-Modal Instruction Tuning</sample>
    <sample id="522">Dettagli sull'implementazione</sample>
    <sample id="523">Dettagli dell'implementazione</sample>
    <sample id="524">Dettagli dell'implementazione</sample>
    <sample id="525">Evaluazione dei Metri</sample>
    <sample id="526">Sensibility</sample>
    <sample id="527">Efficienza dell'addestramento di istruzioni su MULTISTRUCT</sample>
    <sample id="528">Efficienza dell'addestramento di istruzioni su MULTISTRUCT</sample>
    <sample id="529">Impatto dell'aumento dei cluster di compiti multimodal</sample>
    <sample id="530">Effetto delle istruzioni diverse sul tuning dell'instruzione</sample>
    <sample id="531">Effetto dei Strategie di Refined Tuning sul Sensibilità del Modello</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks</sample>
    <sample id="533">Conclusions</sample>
    <sample id="534">One More Thing!</sample>
    <sample id="535">Universita Di Trento</sample>
    <sample id="536">Mohammad javad Hosseini</sample>
    <sample id="562">Language model acceptability judgments are not always robust to context</sample>
    <sample id="563">Language model acceptability judgments are not always robust to context</sample>
    <sample id="564">Ricordando paradigma minimale</sample>
    <sample id="565">Ricordando paradigma minimale</sample>
    <sample id="566">Ricordando paradigma minimale</sample>
    <sample id="567">Ricordando paradigma minimale</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm</sample>
    <sample id="571">Approach Test whether MPP agrees vary as a function of context length, structural, and acceptability.</sample>
    <sample id="572">Approach</sample>
    <sample id="573">Approach Test whether MPP agrees vary as a function of context length, structural, and acceptability.</sample>
    <sample id="574">Approach</sample>
    <sample id="575">Approach Test whether MPP agrees vary as a function of context length, structural, and acceptability.</sample>
    <sample id="576">Approach</sample>
    <sample id="577">Approach Test whether MPP agrees vary as a function of context length, structural match, and acceptability.</sample>
    <sample id="578">Approach</sample>
    <sample id="579">Approach</sample>
    <sample id="580">Approach</sample>
    <sample id="581">1. We perform MPP judgments for different context lengths (acceptable/unacceptable: matched/mismatched structure) of up to 900 tokens.</sample>
    <sample id="582">MPP judgments are robust for arbitrary context lengths with different contexts (acceptable, unacceptable): We perform MPPs structure (different contexts to 700 tokens) matched/mismatched structure (lengths up to 900 tokens).</sample>
    <sample id="583">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance</sample>
    <sample id="584">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance</sample>
    <sample id="585">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance</sample>
    <sample id="586">3. Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance</sample>
    <sample id="587">3. Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance</sample>
    <sample id="588">3. Acceptable/Unacceptable MPP sentences with matched structure most severely affect model performance</sample>
    <sample id="589">Why do matched prefixes affect LM judgments?</sample>
    <sample id="590">Why do matched prefixes affect LM judgments?</sample>
    <sample id="591">Perché i prefissi coincidenti influenzano le giudizi LM?</sample>
    <sample id="592">Why do matched prefixes affect LM judgments?</sample>
    <sample id="593">Why do matched prefixes affect LM judgments?</sample>
    <sample id="594">Key takeaways</sample>
    <sample id="595">Key Takeaways</sample>
    <sample id="596">Key Takeaways</sample>
    <sample id="597">[CLS]</sample>
    <sample id="598">50,000</sample>
    <sample id="626">LHA</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">The documents in DEplain-web have been aligned using both manual and automatic alignment methods.</sample>
    <sample id="629">The dataset was created by collecting news articles from Reuters and annotating them with CONLL-2003 guidelines.</sample>
    <sample id="630">XSemPLR: Cross-Linguistic Semantic Parsing in Multiple Natural Languages and Meaning Representations</sample>
    <sample id="631">Semantic Parsing</sample>
    <sample id="632">Tradurre il contenuto inglese in italiano.</sample>
    <sample id="633">Traduci il contenuto inglese in italiano.</sample>
    <sample id="634">Cross-lingual semantic parsing</sample>
    <sample id="635">Cross-lingual semantic parsing</sample>
    <sample id="636">Cross-lingual semantic parsing</sample>
    <sample id="637">Cross-lingual semantic parsing</sample>
    <sample id="638">Cross-lingual semantic parsing</sample>
    <sample id="639">We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meanings representations. It contains: 5 semantic parsing tasks, 28 natural representations in 15 families.</sample>
    <sample id="640">We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meanings representations. It contains: 5 semantic parsing tasks, 28 natural representations in 15 families.</sample>
    <sample id="641">Esegui l'esperimento.</sample>
    <sample id="642">Esegui l'esperimento.</sample>
    <sample id="643">Esegui l'esperimento utilizzando il set di 6 traduzioni per ottenere i parametri per il training e l'evaluazione.</sample>
    <sample id="644">Monolingual Few-Shot</sample>
    <sample id="645">Ecco il contenuto tradotto: "Esperimenti di Impostazioni"</sample>
    <sample id="646">Monolingual Few-Shot</sample>
    <sample id="647">Experiment Settings * We consider the six settings for training and evaluation for all languages. Multilingual Model: Train one multilingual model for all languages.</sample>
    <sample id="648">Experiment Settings * We consider the six settings for training and evaluation for all languages. Multilingual Model: Train one multilingual model for all languages.</sample>
    <sample id="649">Experiment Settings * We consider the six settings for training and evaluation for all languages. Multilingual Model: Train one multilingual model for all languages.</sample>
    <sample id="650">Esegui l'esperimento su un singolo linguaggio e trasferiscilo in un altro.</sample>
    <sample id="651">Esegui l'interrogazione SQL sul database.</sample>
    <sample id="652">Analysis of Monolingual</sample>
    <sample id="653">Analysis of Monolingual</sample>
    <sample id="654">Analysis of Monolingual</sample>
    <sample id="655">Analysis of Monolingual</sample>
    <sample id="656">Analysis of Multilingual Training</sample>
    <sample id="657">Analysis of Multilingual Training</sample>
    <sample id="658">Analisi dell'Addestramento Multilingue</sample>
    <sample id="659">Analysis of Multilingual Training</sample>
    <sample id="660">Cross-lingual Performance Gap</sample>
    <sample id="661">Cross-lingual Performance Gap</sample>
    <sample id="662">Cross-lingual Performance Gap</sample>
    <sample id="663">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="664">Other Results &amp; Findings (Section 4 in Paper)</sample>
    <sample id="665">Conclusions</sample>
    <sample id="666">Conclusions</sample>
    <sample id="667">[1], [2], [3], [4]</sample>
    <sample id="668">No</sample>
    <sample id="695">Il modello utilizza una procedura di backpropagation per risolvere l'ambiguità.</sample>
    <sample id="696">The fairness of a NLP model at the downstream level is defined as the difference between the performance of the model on the majority group and the minority group.</sample>
    <sample id="697">Yans Labas</sample>
    <sample id="698">Koushik Suvitha</sample>
    <sample id="699">Myra Cheng</sample>
    <sample id="700">exotic</sample>
    <sample id="701">Through essentializing narratives</sample>
    <sample id="702">P-PMI</sample>
    <sample id="703">DrBERT is a pre-trained model, while ChuBERT is a fine-tuned model.</sample>
    <sample id="751">3</sample>
    <sample id="752">Iterative transfer learning</sample>
    <sample id="753">Understand users' language when they make a choice</sample>
    <sample id="754">By sending a specially crafted request to the EaaS.</sample>
    <sample id="755">3</sample>
    <sample id="756">15 annotators</sample>
    <sample id="757">University of Washington, AI Now Institute, A, University of Washington</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">Coherence, Knowledge, Consistency, Emotional Understanding</sample>
    <sample id="760">Because the models are evaluated in a long context</sample>
    <sample id="761">No</sample>
    <sample id="762">yes</sample>
    <sample id="763">BLEU, METEOR, ROUGE-L</sample>
    <sample id="764">yes</sample>
    <sample id="765">It helps to understand the context of a sentence.</sample>
    <sample id="766">Adattatori</sample>
    <sample id="767">Roberta-base + classifier head</sample>
    <sample id="768">C4, CC1M, CC2M</sample>
    <sample id="769">3</sample>
    <sample id="770">1.4%</sample>
    <sample id="771">Shuhui Liu</sample>
    <sample id="772">yes</sample>
    <sample id="773">3</sample>
    <sample id="774">DPT</sample>
    <sample id="833">Google</sample>
    <sample id="834">Storybrook University, Human Language Analytics</sample>
    <sample id="835">English and Italian</sample>
    <sample id="836">Yulia Tsvetkova</sample>
    <sample id="837">DELP-AP-48, DELP-AP-128, DELP-WE-48, DELP-WE-128</sample>
    <sample id="838">53</sample>
    <sample id="839">3</sample>
    <sample id="840">Copy Dataset, AG News, MIND, ST2N, ER2N Spam, Provider's general dataset (WikiText)</sample>
    <sample id="876">a model</sample>
    <sample id="877">David Warms Markes</sample>
    <sample id="878">Up to 40 BLEURT points</sample>
    <sample id="879">University of Edinburgh, University of Glasgow, University of Edinburgh</sample>
    <sample id="880">1. How to use a computer mouse 2. How to use a computer keyboard 3. How to use a computer mouse and keyboard together 4. How to use a computer mouse and keyboard together with a touchpad 5. How to use a computer mouse, keyboard, and touchpad together</sample>
    <sample id="881">coreference resolution task</sample>
    <sample id="882">Promoting PaLM for Translation</sample>
    <sample id="883">PaLM: Pathways Language Model</sample>
    <sample id="884">Pal.M. Pathways Language Model</sample>
    <sample id="885">Our contribution</sample>
    <sample id="886">Our contribution</sample>
    <sample id="887">Our contribution</sample>
    <sample id="888">Our contribution</sample>
    <sample id="889">Prompts have a big impact on translation quality</sample>
    <sample id="890">Prompts have a big impact on translation quality</sample>
    <sample id="891">Prompts have a big impact on translation quality</sample>
    <sample id="892">5-step prompting for translation</sample>
    <sample id="893">5-step prompting for translation</sample>
    <sample id="894">5-step prompting for translation</sample>
    <sample id="895">-5 prompt</sample>
    <sample id="896">5-step prompting for translation</sample>
    <sample id="897">Esperienze sull'OMM</sample>
    <sample id="898">Esperimentali Risultati</sample>
    <sample id="899">Esperimentali Risultati</sample>
    <sample id="900">Esperimentali Risultati</sample>
    <sample id="901">Esperimentali Risultati</sample>
    <sample id="902">Esperienze sull'OMM</sample>
    <sample id="903">Esperimentali Risultati</sample>
    <sample id="904">Esperienze sull'OMM</sample>
    <sample id="905">Esperienze sull'OMM</sample>
    <sample id="906">"Thank you" is a phrase used to express gratitude.</sample>
    <sample id="907">Worse Than You Think</sample>
    <sample id="908">Worse Than You Think</sample>
    <sample id="909">Why weakly supervised learning?</sample>
    <sample id="910">Why weakly supervised learning?</sample>
    <sample id="911">Why weakly supervised learning?</sample>
    <sample id="912">Why weakly supervised learning?</sample>
    <sample id="913">Why weakly supervised learning?</sample>
    <sample id="914">A common claim in recent WSL works</sample>
    <sample id="915">A common claim in recent WSL works</sample>
    <sample id="916">A common claim in recent WSL works</sample>
    <sample id="917">A common claim in recent WSL works</sample>
    <sample id="918">Our research questions</sample>
    <sample id="919">Our research questions</sample>
    <sample id="920">R01 Main findings</sample>
    <sample id="921">R01 Main findings</sample>
    <sample id="922">R01 Main findings</sample>
    <sample id="923">R01 Main findings</sample>
    <sample id="924">R01 Main findings A clean validation set is indispensable.</sample>
    <sample id="925">R02 Main findings</sample>
    <sample id="926">R02 Main findings</sample>
    <sample id="927">RQ2 Main findings</sample>
    <sample id="928">RQ2 Main findings</sample>
    <sample id="929">RQ2 Main findings</sample>
    <sample id="930">R03 Main findings</sample>
    <sample id="931">R03 Main findings</sample>
    <sample id="932">R03 Main findings</sample>
    <sample id="933">R03 Main findings</sample>
    <sample id="934">Conclusions Recent WSL approaches. Our recommendations.</sample>
    <sample id="935">Conclusions Recent WSL approaches. Our recommendations.</sample>
    <sample id="936">Conclusions Recent WSL approaches. Our recommendations.</sample>
    <sample id="937">Conclusions Recent WSL approaches. Our recommendations.</sample>
    <sample id="938">Conclusions Recent WSL approaches. Our recommendations.</sample>
    <sample id="939">Comparative evaluation, Likert rating evaluation.</sample>
    <sample id="940">6</sample>
    <sample id="941">Servin is a judge and Kea is a baker.</sample>
    <sample id="942">yes, on GitHub</sample>
    <sample id="943">Yes</sample>
    <sample id="944">First prefix, long prefix, first prefix without adverb</sample>
    <sample id="945">having a dimensional evaluation</sample>
    <sample id="946">University of Science and Technology of China, Microsoft Research Asia, University of Science and Technology of China</sample>
    <sample id="947">When the prompt is important.</sample>
    <sample id="978">ChatGPT, GPT-4, GPT-4 Web, GPT-4 Web-13B, GPT-4 Web-260B</sample>
    <sample id="979">6</sample>
    <sample id="980">Being able to understand the goal, being able to understand the constraints and being able to understand the real-life specifics.</sample>
    <sample id="981">10</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">Institute of Computer Science, University of Warsaw</sample>
    <sample id="1021">Accuracy</sample>
    <sample id="1022">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1023">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="1024">Comparative Evaluation</sample>
    <sample id="1025">Comparative Evaluation</sample>
    <sample id="1026">L'ascoltatore ha bisogno di un contesto per interpretare il significato di una parola.</sample>
    <sample id="1027">Lirkert Rating Evaluation</sample>
    <sample id="1028">L'analisi Likert per valutare la rilevanza delle risposte dei bot</sample>
    <sample id="1029">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">ABC-Eval Behaviors</sample>
    <sample id="1032">ABC-Eval Behaviors</sample>
    <sample id="1033">ABC-Eval Behaviors</sample>
    <sample id="1034">Esempio</sample>
    <sample id="1035">Esempio di esperimenti</sample>
    <sample id="1036">Turn Lifter</sample>
    <sample id="1037">Inter-Annotator Agreement</sample>
    <sample id="1038">Turn Likert</sample>
    <sample id="1039">Turn Likert</sample>
    <sample id="1040">Incremental Validity</sample>
    <sample id="1041">Incremental Validity</sample>
    <sample id="1042">Incremental Validity</sample>
    <sample id="1043">In the last slide, we see a bar graph that shows the error rates of different models.</sample>
    <sample id="1044">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1045">ABC-Eval Error Rates by Model</sample>
    <sample id="1046">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1047">ABC-Eval Error Rates by Model</sample>
    <sample id="1048">Emory University, Emory NLP Research Lab, and Amazon Alexa</sample>
    <sample id="1049">Continuously fine-tuning</sample>
    <sample id="1050">6</sample>
    <sample id="1051">Quando richiede il contesto alla traduzione?</sample>
    <sample id="1052">Traduzione dipende dal contesto</sample>
    <sample id="1053">Traduzione dipende dal contesto</sample>
    <sample id="1054">Traduzione dipende dal contesto</sample>
    <sample id="1055">Evaluating context-dependent translation is hard.</sample>
    <sample id="1056">Evaluating context-dependent translation is hard</sample>
    <sample id="1057">RQ2: When does translation require context?</sample>
    <sample id="1058">RQ2: When does translation require context? - Word-level context usage RQ3: How well do models handle context-dependent translations?</sample>
    <sample id="1059">• CXMI: measure how much context models use given a corpus</sample>
    <sample id="1060">• CXMI: measure how much context models use given a corpus</sample>
    <sample id="1061">Pointwise P(CXMI)</sample>
    <sample id="1062">RQ2: When does translation require context? - Word-level text usage - Thematic analysis RQ3: How well will models handle context-dependent translations?</sample>
    <sample id="1063">Thematic analysis of high P-CXMI words</sample>
    <sample id="1064">Analisi tematica di parole P-CXMI</sample>
    <sample id="1065">Thematic analysis of high-PCXMI words</sample>
    <sample id="1066">Analisi tematica di parole P-CXMI</sample>
    <sample id="1067">Thematic analysis of high P-CCMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form 3. Lexical cohesion Avellene's mother was still asleep. Avellene went to school.</sample>
    <sample id="1068">Thematic analysis of high P-CXMI words</sample>
    <sample id="1069">Thematic analysis of high P-CXMI words</sample>
    <sample id="1070">RQ1: When does translation require context?</sample>
    <sample id="1071">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1073">Multilingual Discourse-Aware (MuDA) tagger</sample>
    <sample id="1074">Traduci il contenuto inglese in italiano.</sample>
    <sample id="1075">Corpus-level metrics</sample>
    <sample id="1076">Corpus-level metrics</sample>
    <sample id="1077">Corpus-level metrics</sample>
    <sample id="1078">MUDa benchmark results</sample>
    <sample id="1079">MUDa benchmark results</sample>
    <sample id="1080">MUUDA benchmark results</sample>
    <sample id="1081">Summarize</sample>
    <sample id="1082">Sommario</sample>
    <sample id="1083">Summarize</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">The new method has a name.</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="1123">University of Washington, Carnegie Mellon University, University of Edinburgh</sample>
    <sample id="1124">Bouquet (Stanford)</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Minimal Pair Paradigm</sample>
    <sample id="1161">FT, L2R, COSINE, MLC, BOND</sample>
    <sample id="1162">13 tasks</sample>
    <sample id="1226">40GB of Wikipedia data</sample>
    <sample id="1227">Adam Prezrokowski</sample>
    <sample id="1228">Performance degrades with larger temporal gap</sample>
    <sample id="1269">Permette di ottenere una rappresentazione di output per ogni token dell'input.</sample>
    <sample id="1270">Perché i modelli sono spesso considerati come "black boxes" e la trasparenza aiuta a comprendere come funzionano.</sample>
    <sample id="1271">1. Helping people were not customer 2. Many people, no customer has spent money</sample>
    <sample id="1272">F1 score, accuracy, and BLEU score.</sample>
    <sample id="1273">Krippendorff's alpha</sample>
    <sample id="1274">space</sample>
    <sample id="1275">Heinrich Heine University, Heinrich-Heine-Universität Dissolfort, Germany</sample>
    <sample id="1276">It is the only dataset that includes multimodal instruction tasks.</sample>
    <sample id="1277">3</sample>
    <sample id="1278">A binary coordination is a pair of characters that are adjacent to each other and have the same length.</sample>
    <sample id="1279">10 days</sample>
    <sample id="1280">It can generate higher quality scripts than LLMs trained on GPT-4.</sample>
    <sample id="1281">DRBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains</sample>
    <sample id="1282">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1283">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1284">I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Contribution of NACHOS and DBERT</sample>
    <sample id="1285">Summary</sample>
    <sample id="1286">Language Modeling</sample>
    <sample id="1287">Language Modeling</sample>
    <sample id="1288">Language Modeling</sample>
    <sample id="1289">Language Modeling</sample>
    <sample id="1290">Comparison of pre-training strategies and data sources</sample>
    <sample id="1291">Comparison of pre-training strategies and data sources</sample>
    <sample id="1292">Comparison of pre-training strategies and data sources</sample>
    <sample id="1293">Comparison of pre-training strategies and data sources</sample>
    <sample id="1294">Comparison of pre-training strategies and data sources</sample>
    <sample id="1295">Comparison of pre-training strategies and data sources</sample>
    <sample id="1296">Comparison of pre-training strategies and data sources</sample>
    <sample id="1297">Evaluation: Data sources and size</sample>
    <sample id="1298">Evaluating: Data sources and size</sample>
    <sample id="1299">Evaluating: Data sources and size</sample>
    <sample id="1300">Evaluazione: Data sources and size</sample>
    <sample id="1301">Evaluazione: Dati, dimensioni e sorgenti</sample>
    <sample id="1302">Evaluation: Pre-training strategies</sample>
    <sample id="1303">Evaluation: Pre-training strategies</sample>
    <sample id="1304">Evaluation: Pre-training strategies</sample>
    <sample id="1305">Diverse tasks</sample>
    <sample id="1306">Diverse tasks</sample>
    <sample id="1307">Diverse tasks</sample>
    <sample id="1308">Grazie</sample>
    <sample id="1309">The work examines the following learning strategies: supervised, semi-supervised and self-supervised.</sample>
    <sample id="1310">1.5</sample>
    <sample id="1311">Using human evaluation</sample>
    <sample id="1312">yes</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations</sample>
    <sample id="1315">Compositional Generalization</sample>
    <sample id="1316">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1317">Compositional Generalization in Semantic Parsing</sample>
    <sample id="1318">Test</sample>
    <sample id="1319">Test</sample>
    <sample id="1320">Test</sample>
    <sample id="1321">Test</sample>
    <sample id="1322">Trees help a lot ... but...</sample>
    <sample id="1323">Trees help a lot ... but...</sample>
    <sample id="1324">Trees help a lot ... but...</sample>
    <sample id="1325">Trees help a lot ...</sample>
    <sample id="1326">Trees help a lot ...</sample>
    <sample id="1327">Trees help a lot ...</sample>
    <sample id="1328">Trees help a lot ...</sample>
    <sample id="1329">Our Approach</sample>
    <sample id="1330">Our Approach</sample>
    <sample id="1331">Our Approach</sample>
    <sample id="1332">Il nostro approccio</sample>
    <sample id="1333">Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach Our Approach</sample>
    <sample id="1334">Permuting with "jumps"</sample>
    <sample id="1335">Permuting with "jumps"</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Permuting with "jumps"</sample>
    <sample id="1338">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1339">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1340">Some Results on COGS (Kim and Lizenz 2020)</sample>
    <sample id="1341">Technical Challenges We Solve</sample>
    <sample id="1342">Technical Challenges We Solve</sample>
    <sample id="1343">Technical Challenges We Solve</sample>
    <sample id="1344">Technical Challenges We Solve</sample>
    <sample id="1345">Technical Challenges We Solve</sample>
    <sample id="1346">Technical Challenges We Solve</sample>
    <sample id="1347">Cognitive Dissonance</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">no</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">P-CXMI</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Cross-lingual transfer</sample>
    <sample id="1387">Saarlauand University, Amazon Alexa, 3rd University of Vienna</sample>
    <sample id="1388">AL, AL/CA, CA</sample>
    <sample id="1389">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources.</sample>
    <sample id="1391">NLU models draw on multiple knowledge sources.</sample>
    <sample id="1392">John saw the newly elected president on TV.</sample>
    <sample id="1393">John saw the newly elected president on TV.</sample>
    <sample id="1394">John saw the newly elected president on TV</sample>
    <sample id="1395">John saw the newly elected president on TV</sample>
    <sample id="1396">KITMUS Test Suite</sample>
    <sample id="1397">KITMUS Test Suite</sample>
    <sample id="1398">KITMUS Test Suite</sample>
    <sample id="1399">KITMUS Test Suite</sample>
    <sample id="1400">KITMUS Test Suite</sample>
    <sample id="1401">Serving is a judge. Ke is a baker. Serving and Ke met at a park. After a long day of work deciding cases in court, he was happy to relax.</sample>
    <sample id="1402">Kitmus Test Suite</sample>
    <sample id="1403">Variants of KITMUS</sample>
    <sample id="1404">Variants of KITMUS</sample>
    <sample id="1405">Variants of KITMUS</sample>
    <sample id="1406">Variants of KITMUS</sample>
    <sample id="1407">Variants of KITMUS</sample>
    <sample id="1408">Variants of KITMUS</sample>
    <sample id="1409">Variants of KITMUS</sample>
    <sample id="1410">Background - Pretrain</sample>
    <sample id="1411">Background - Pretrain</sample>
    <sample id="1412">Background - Pretrain</sample>
    <sample id="1413">Background - Inference</sample>
    <sample id="1414">Conclusione Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at https://github.com/mpemskitt/kittus.</sample>
    <sample id="1415">Conclusione Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pre-training and inference-time knowledge). 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at https://github.com/mpemskitt/kittus.</sample>
    <sample id="1416">Trees need to be obtained, Pre-Post-processing logical forms, Non-incremental</sample>
    <sample id="1417">School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="1418">Marked Personas</sample>
    <sample id="1419">Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability. Based on fixed, hand-curated datasets. Don't account for intersectionality.</sample>
    <sample id="1420">Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability. Based on fixed, hand-curated datasets. Don't account for intersectionality.</sample>
    <sample id="1421">Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability. Based on fixed, hand-curated datasets. Don't account for intersectionality.</sample>
    <sample id="1422">Social bias and stereotypes are prevalent in LLMs. Limitations of existing stereotype measures: Tradeoff between specificity and generalizability. Based on fixed, hand-curated datasets. Don't account for intersectionality.</sample>
    <sample id="1423">Come superiamo questi limiti?</sample>
    <sample id="1424">Come superiamo questi limiti?</sample>
    <sample id="1425">Come superiamo questi limiti?</sample>
    <sample id="1426">Output: Persona Examples (GPT-4)</sample>
    <sample id="1427">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1428">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1430">Step 1: Persona Examples (GPT-4)</sample>
    <sample id="1431">2 steps</sample>
    <sample id="1432">2 steps</sample>
    <sample id="1433">2 steps</sample>
    <sample id="1434">2 passi</sample>
    <sample id="1435">2 passi</sample>
    <sample id="1436">Insight for 2: Marked Words</sample>
    <sample id="1437">Insight for 2: Marked Words</sample>
    <sample id="1438">Insight for 2: Marked Words</sample>
    <sample id="1439">Step 2: Marked Words</sample>
    <sample id="1440">Step 2: Marked Words</sample>
    <sample id="1441">Step 2: Marked Words</sample>
    <sample id="1442">Risultati: Confronto con le Risposte Umane</sample>
    <sample id="1443">Mauro è un Black Stereotype in Personas</sample>
    <sample id="1444">Ma... ma questo lessico è incompleto</sample>
    <sample id="1445">Mauro è un Black Stereotype in Personas</sample>
    <sample id="1446">Mauro è un Black Stereotype in Personas</sample>
    <sample id="1447">Results: Patterns in Top Words</sample>
    <sample id="1448">Results: Patterns in Top Words</sample>
    <sample id="1449">Results: Patterns in Top Words</sample>
    <sample id="1450">Results: Patterns in Top Words</sample>
    <sample id="1451">Results: Patterns in Top Words</sample>
    <sample id="1452">Results: Patterns in Top Words</sample>
    <sample id="1453">Results: Patterns in Top Words</sample>
    <sample id="1454">Results: Patterns in Top Words</sample>
    <sample id="1455">Results: Patterns in Top Words</sample>
    <sample id="1456">Results: Patterns in Top Words</sample>
    <sample id="1457">Results: Patterns in Top Words</sample>
    <sample id="1458">Recommendations</sample>
    <sample id="1459">Recommendations Addressing positive stereotypes and essentializing narratives An intersectoral lens Transparency about bias mitigation</sample>
    <sample id="1460">Recommendations</sample>
    <sample id="1461">Raccomandazioni</sample>
    <sample id="1462">Raccomandazioni</sample>
    <sample id="1463">Raccomandazioni</sample>
    <sample id="1464">Raccomandazioni</sample>
    <sample id="1465">Are You Copying My Model? Protecting the Copyright Large Language Models via Backdoor Watermark</sample>
    <sample id="1466">Sono presenti i marchi di Microsoft, Sony e Bing.</sample>
    <sample id="1467">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered for various NLP tasks. Embedding a service is being offered to assist with various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="1468">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="1469">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="1470">Background Large language models (LLMs) are exceptional in NLU and NLP. GPT-4, LLaMA 2, and PaLM 3 are offered to assist various NLP tasks. Embedding a service is being offered to assist various NLP tasks. OpenAI offers a GPT-3.5 embedding API.</sample>
    <sample id="1471">MOTIVATION</sample>
    <sample id="1472">Challenge</sample>
    <sample id="1473">Challenge</sample>
    <sample id="1474">Challenge</sample>
    <sample id="1475">Challenge</sample>
    <sample id="1476">Existing Works</sample>
    <sample id="1477">Existing Works</sample>
    <sample id="1478">Existing Works</sample>
    <sample id="1479">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1480">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1481">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1482">Watermark injection</sample>
    <sample id="1483">Watermark injection</sample>
    <sample id="1484">Watermark injection</sample>
    <sample id="1485">Ecco il contenuto tradotto in italiano:</sample>
    <sample id="1486">Ecco il contenuto tradotto:</sample>
    <sample id="1487">Ecco il contenuto tradotto:</sample>
    <sample id="1488">Ecco il contenuto tradotto:</sample>
    <sample id="1489">Ecco il contenuto tradotto:</sample>
    <sample id="1490">Ecco i risultati sperimentali.</sample>
    <sample id="1491">Risultati sperimentali</sample>
    <sample id="1492">Risultati sperimentali - Visualizzazione dell'embeddingle</sample>
    <sample id="1493">Risultati sperimentali</sample>
    <sample id="1494">Thanks!</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">2012</sample>
    <sample id="1497">Transfer and Active Learning for Dissimilarity Detection: Addressing the Rare-Class Challenge</sample>
    <sample id="1498">What is Cognitive Dissonance?</sample>
    <sample id="1499">What is Cognitive Dissonance?</sample>
    <sample id="1500">What is Cognitive Dissonance?</sample>
    <sample id="1501">Cosa è la Dissonanza Cognitiva?</sample>
    <sample id="1502">Why disagreement?</sample>
    <sample id="1503">Why disagreement?</sample>
    <sample id="1504">Why dissidence?</sample>
    <sample id="1505">Perché la dissidenza?</sample>
    <sample id="1506">-3.5%</sample>
    <sample id="1507">-3.5%</sample>
    <sample id="1508">-3.5%</sample>
    <sample id="1509">Training on Initial Annotated Set</sample>
    <sample id="1510">Training on Initial Annotated Set</sample>
    <sample id="1511">Method: Transfer and Active Learning for Annotate Rare Class</sample>
    <sample id="1512">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="1513">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="1514">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="1515">Cold-Start Annotations: Transfer Learning</sample>
    <sample id="1516">Cold-start Annotators: Transfer Learning</sample>
    <sample id="1517">Active Learning: Cumulative vs. Iterative Update</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1519">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1520">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1521">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1522">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1523">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="1524">Takeaways</sample>
    <sample id="1525">Takeaways</sample>
    <sample id="1526">Thank you!</sample>
    <sample id="1527">University of Amsterdam, Saarland University, University of Luxembourg</sample>
    <sample id="1528">Chen Zhen</sample>
    <sample id="1529">4</sample>
    <sample id="1530">wak-walk</sample>
  </task>
</testset>