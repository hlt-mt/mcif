<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models include news media and social media.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research</sample>
    <sample id="2">The paper "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding" presented at the 61st Annual Meeting of the Association for Computational Linguistics addresses the challenge of reading order issues in visually-rich documents, particularly invoices and receipts. The authors introduce LayoutMask, a novel multi-modal pre-training model designed to improve document understanding by leveraging text and layout information. The motivation behind this work is to enhance the ability of models to interpret complex layouts, such as those found in financial documents, where the order of elements can significantly affect comprehension.

LayoutMask achieves its goals through two key contributions:
1. **Use of Local ID Position Instead of Global ID Position**: This approach allows the model to focus on local relationships between text and layout elements, which is crucial for understanding the hierarchical structure of documents.
2. **Enhancement of Text-Layout Interactions with Novel Masking Strategies and Pre-training Objectives**: By incorporating these strategies, the model can better predict masked tokens based on their context within the document's layout, thereby improving overall understanding.

The methodology section outlines the pre-training task, representation, token embedding, local and segment 2D position, and masking strategy. The model uses transformer layers with spatial-aware self-attention mechanisms to process the input data effectively. The experimental results demonstrate that LayoutMask outperforms existing methods across various datasets, achieving the best F1 scores for different 1D and 2D position combinations. These results highlight the effectiveness of the proposed model in enhancing text-layout interaction for document understanding tasks.

In conclusion, LayoutMask represents a significant advancement in multi-modal pre-training for document understanding, offering a robust solution to the problem of reading order issues in visually-rich documents. Its application in fields like finance, healthcare, and legal documentation could lead to more accurate and efficient processing of structured text data.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">This research paper, presented at ACL 2023, introduces a novel approach to unify multi-lingual and cross-lingual summarization into a single model, named Many-to-many Summarization (M2MS). The study aims to build a comprehensive model capable of processing documents in any source language and generating summaries in any target language. This unified model is designed to transfer knowledge across different languages more effectively than existing models for multi-lingual (MLS) and cross-lingual (CLS) summarization.

The authors propose PISCES, a pre-trained M2MS model that learns language modeling, cross-lingual ability, and summarization ability through a three-stage pre-training process: meta-pre-training, cross-lingual pre-training, and task-specific pre-training. The meta-pre-training stage involves generating original sentences based on noisy parallel sentences, while cross-lingual pre-training focuses on generating sentences in the target language based on noisy parallel sentences in the source language. Task-specific pre-training utilizes pseudo samples to further refine the model's capabilities.

The study evaluates the performance of the proposed model against previous MLS and CLS models using the WikiLingua dataset with mBART-50 as the backbone. Preliminary experiments show that the M2MS setting can better transfer knowledge across different languages compared to MLS, CLS, and unified CLS settings. The results indicate that the M2MS model achieves higher ROUGE scores across various zero-shot directions, suggesting its superior ability to generalize across languages.

In conclusion, this research contributes to the field by providing a unified framework for multi-lingual and cross-lingual summarization, which can potentially enhance the efficiency and effectiveness of summarization systems in diverse linguistic environments.</sample>
    <sample id="7">YES!</sample>
    <sample id="8">The novelty of the proposed human evaluation method lies in its ability to evaluate dialogue quality by focusing on specific behaviors and dimensions that are not typically considered in existing methods, such as coherence, consistency, knowledge, and emotional understanding. This approach allows for a more comprehensive assessment of chat-oriented dialogue systems, providing insights into areas where models may be lacking or excelling.</sample>
    <sample id="9">The success of the existing weakly supervised approach heavily relies on clean validation data.</sample>
    <sample id="10">The speaker suggests that advances in the model's performance could be achieved by improving the quality of the background knowledge, which is currently limited to entity names.</sample>
    <sample id="11">This research explores the capabilities and limitations of large language models (LLMs) in understanding humor, specifically through the lens of the New Yorker Caption Contest. The study investigates whether LLMs can generate and explain jokes effectively, drawing on examples from ChatGPT and other models like CLIP-ViT-L/14@336px and OFA-Huge. It highlights that while these models can produce humorous content, their ability to "understand" humor is limited, as evidenced by their failure to grasp the nuances of puns and wordplay. The research also introduces a new annotated corpus for evaluating LLMs' performance in generating captions for cartoons, focusing on metrics such as matching, quality ranking, and explanation generation. The findings suggest that while LLMs can generate humorous content, they struggle with deeper comprehension of humor, indicating a need for more sophisticated models or human-like evaluation criteria. The study concludes with a call for further investigation into how AI might "understand" humor, emphasizing the importance of human evaluation in assessing AI's capabilities.</sample>
    <sample id="12">5</sample>
    <sample id="13">This research explores the challenges and solutions in adaptive inference, particularly focusing on low-resource settings where real-world data complexity varies significantly. The study introduces the concept of "conflicting gradients" in early exit training processes, suggesting that future classifiers' goals are aligned but may interfere with each other, degrading performance. To address this issue, the paper proposes the SWEET method, which separates weights in early exit transformers to mitigate conflicting gradients. This approach is demonstrated through experiments using BERT as a backbone model, showing that Multi Model (MM) models outperform Early Exit (EE) ones by 2.3% on average, especially for earlier classifiers. The SWEET method closes most of the gap between EE and MM, while later classifiers are negatively affected. The study also highlights the importance of fine-tuning algorithms tailored to the Early Exit architecture, motivating further research in this area.</sample>
    <sample id="15">3</sample>
    <sample id="16">News and Bible</sample>
    <sample id="17">This research introduces a novel approach to multimodal relation extraction, focusing on the simultaneous subtraction and addition of information for enhanced performance. The study addresses the challenges of internal-information over-utilization and external-information under-exploitation by employing a fine-grained information pruning technique across two modalities. It leverages a latent multimodal topic model to enrich feature contexts and performs internal-information screening guided by the graph information bottleneck principle. The proposed system achieves significant improvements over existing best models on benchmark data, demonstrating its effectiveness in relation extraction tasks. The framework integrates scene graph generation, cross-modal graph construction, GIB-guided feature refinement, and multimodal topic integration, ensuring that both textual and visual features are optimally utilized. This work not only advances the field of multimodal relation extraction but also provides a robust solution for handling diverse multimodal inputs efficiently.</sample>
    <sample id="18">The example given is "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="19">This research paper presents a comprehensive survey on efficient open-domain question answering (ODQA) systems, focusing on the challenges and advancements in this field. The study begins by introducing the concept of ODQA, which involves two-stage frameworks: retrieval and reading. It highlights the importance of these stages in processing large-scale knowledge bases like Wikipedia, emphasizing the need for efficient indexing and evidence searching techniques to handle vast amounts of data.

The paper discusses the limitations of existing ODQA systems, particularly their high memory and computational costs, and introduces several strategies to mitigate these issues. These include reducing index size through filtering documents, dimension reduction, and product quantization, as well as utilizing lightweight models such as MobileBERT and parameter sharing techniques like ALBERT. Additionally, it explores the use of fewer models to achieve multiple sub-tasks, enhancing model efficiency.

The comparative analysis section evaluates different ODQA system architectures, including Retriever-Reader, Retriever-only, and Generator-only systems, based on factors like model size, inference speed, and performance. The study concludes with recommendations for deploying ODQA systems in resource-constrained environments, suggesting the use of generator-only systems or one-stage models for multiple tasks when real-time feedback is not required. Future work includes exploring the deployment of ODQA systems on low-power devices and considering additional evaluation metrics beyond traditional ones.</sample>
    <sample id="20">Yes, you can use the models for your research.</sample>
    <sample id="21">APA style documents</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">This research explores the integration of character-aware models to enhance visual text rendering, focusing on text-to-image modeling techniques. The study highlights the limitations of subword-based encoders in handling spelling accuracy, particularly at smaller scales, where they struggle with frequent errors like excess repetitions and misshapen glyphs. By contrast, character-aware encoders demonstrate superior performance across all scales, achieving near-perfect spelling accuracy for high-frequency words. The research also introduces a method to concatenate subword-level and character-level text encodings, which significantly improves image generation metrics such as fidelity, alignment, and text quality. This approach is demonstrated through a case study involving a vintage postage stamp, showcasing how character-aware models can accurately render complex text into images, thereby providing a robust solution for enhancing text-only and text-to-image model capabilities.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by observing the length difference between left and right conjuncts in the Penn Treebank.</sample>
    <sample id="25">The experiments were designed by varying the position of the governor in the sentence structure while keeping other factors constant, and then observing how this change affected the dependency lengths and structures.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data achieves an AUC of 0.5, which is not very good.</sample>
    <sample id="27">4</sample>
    <sample id="28">The characters' names in the example conversation are not provided in the text.</sample>
    <sample id="29">Formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework designed to enhance the performance of large language models (LLMs) by leveraging pairwise ranking and generative fusion techniques. The framework, developed by researchers from the Allen Institute for AI and USC University of Southern California, addresses the limitations of individual LLMs by combining their outputs through a two-step process: first, it ranks candidate responses using a PairRanker module, which compares pairs of outputs based on their similarity scores; second, it fuses the top-ranked candidates into a final output using a GenFuser module, which integrates the strengths of multiple models. This approach not only improves the overall accuracy but also ensures that the ensemble model can adapt to different tasks and examples, as evidenced by its superior performance across various benchmarks.

The study demonstrates that no single LLM is universally optimal, highlighting the necessity of ensemble methods. The LLM-Blender framework's effectiveness is further validated through extensive experiments on the MixInstruct dataset, which contains over 110k instruction-following examples. The results show that LLM-Blender outperforms baseline methods like Max Logits and Max Wins, achieving higher BLEURT and GPT-Rank scores, indicating better alignment with human judgments. Additionally, the framework's ability to handle diverse datasets and tasks makes it a valuable tool for researchers and practitioners aiming to improve the robustness and versatility of LLMs.

In conclusion, LLM-Blender represents a significant advancement in ensemble learning for LLMs, offering a practical solution for enhancing model performance and reliability. Its modular design and comprehensive evaluation on the MixInstruct dataset provide a solid foundation for future research and development in this field.</sample>
    <sample id="31">Johns Hopkins University, Purdue University, MIT, Meta AI</sample>
    <sample id="32">**Slide Title:** Compositional Generalization without Trees using Multiset Tagging and Latent Permutations

**Slide Content:**
- **Title:** Compositional Generalization without Trees using Multiset Tagging and Latent Permutations
- **Authors:** Matthias Lindemann, Alexander Koller, Ivan Titov
- **Institutions:** University of Amsterdam, Saarland University, University of California, Berkeley
- **Abstract:** Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.

**Slide 2: Compositional Generalization in Semantic Parsing**

- **Train:** 
  - The girl slept.
    - *girl x1 sleep.agent x2 x3
  - Mary knew that the girl slept.
    - *girl x1 know.agent x2 Mary A know.ccomp x3 x4 A
    - sleep.agent x5 x6
- **Test:** 
  - Jim said that Mary knew that the girl slept.
    - *girl x1 say.agent x2 Jim A say.ccomp x3 x4 A know.agent x5 Mary A know.ccomp x6 x7 A sleep.agent x8 x9

- **Note:** Naive seq2seq models fail!

**Slide 3: Trees help a lot but...**

- **Tree Representation:**
  - *girl x1 sleep.agent x2 x3
  - *girl x1 x2 sleep.agent x3
  - The girl slept.
- **Text:** Trees need to be obtained:
  - Pre/Post-processing logical forms
  - Grammar-induction

- **Note:** This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.

**Slide 4: Our Approach**

- **Tagging Scheme:**
  - The girl slept.
  - *girl x1 sleep.agent x2 x3
  - *girl x1 x2 sleep.agent x3
  - sleep.agent x4 x5
  - agent x6 x7
  - x1 x2 x3 x4 x5 x6 x7

- **Permutation Model:**
  - Alignment unknown.
  - Induce it in training.
  - Permutation model:
    - Inference is NP-hard (~ TSP)
    - Backpropagate through continuous relaxation

**Slide 5: Some Results on COGS (Kim and Linzen 2020)**

- **Comparison with other Treeless Models on Structural Generalization on COGS**
  - PP recursion
  - CP recursion
  - Obj PP = Subj PP
  - Model:
    - LSTM seq2seq
    - TP
    - Zheng and Lapata
    - Ours

**Slide 6: Technical Challenges We Solve**

- **Alignment unknown.**
  - Induce it in training.
  - Permutation model:
    - Inference is NP-hard (~ TSP)
    - Backpropagate through continuous relaxation

**Slide 7: Paper &amp; Code**

- **URL:** https://t.ly/mx8ny</sample>
    <sample id="33">The framework quantifies positionality by re-annotating datasets with diverse annotators and then comparing annotations by demographic to models and datasets via Pearson's R scores.</sample>
    <sample id="34">CREST, a novel framework for rationalization and counterfactual text generation, bridges the gap between selective rationalization and counterfactual generation by producing valid, fluent, and diverse counterfactuals while controlling the amount of perturbation. This approach leads to plausible explanations and achieves high counterfactual simulability. CREST operates through two main components: CREST-Generation, which generates counterfactual texts, and CREST-Rationalization, which provides explanations for these counterfactuals. The framework leverages a trainable masker and a predictor to ensure that the generated counterfactuals are both valid and fluent. Experiments on datasets like IMDB and SNLI demonstrate that CREST outperforms existing methods in terms of plausibility, forward simulability, and counterfactual simulability. The interpretability analysis further validates the framework's ability to generate interpretable rationales, making it a significant advancement in the field of natural language processing.</sample>
    <sample id="35">**Slide 1: Title Slide**
- **Title:** Weaker Than You Think: A Critical Look at Weakly Supervised Learning
- **Authors:** Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow
- **Affiliations:** Saarland University, Amazon Alexa, University of Vienna
- **Conference:** ACL 2023

**Slide 2: Introduction to Weakly Supervised Learning**
- **Why weakly supervised learning?**
  - Weak supervision alleviates the annotation bottleneck.
  - Weak labels are noisy, which can harm generalization.
  - Weakly supervised learning (WSL) trains models that generalize well despite being trained on noisy data.

**Slide 3: Why Weakly Supervised Learning?**
- **Weak supervision alleviates the annotation bottleneck.**
- **But weak labels are noisy!**
  - Noise memorization harms generalization.
- **Weakly supervised learning (WSL)**
  - Train models that generalize well despite being trained on noisy data.

**Slide 4: Common Claim in Recent WSL Works**
- **Claim:** "We train models only on weakly supervised data and achieve an accuracy of XX%."
- **Illustration:** 
  - Weakly labeled training data (noisy)
  - Cleanly labeled test data (clean)

**Slide 5: Research Questions**
- **RQ1:** Is clean validation data necessary?
- **RQ2:** How many clean samples do WSL approaches need?
- **RQ3:** How to use the available clean samples more efficiently?

**Slide 6: Main Findings - RQ1**
- **Relative performance improvement over weak labels (%)**
  - FT_W
  - BOND
  - COSINE
  - MLC
  - L2R
- **Legend:**
  - Validation on Weak Labels
  - No Validation (Random Selection)
  - Validation on Clean Labels

**Slide 7: Main Findings - RQ2**
- **Accuracy vs. Clean Validation Samples**
  - FT_W
  - COSINE
  - LIN
  - BOND
  - MLC
  - Weak labels
- **Observation:** WSL approaches benefit from more clean validation samples!

**Slide 8: Main Findings - RQ3**
- **Before CFT vs. After CFT**
  - N=10 clean samples per class
  - N=30 clean samples per class
- **Observation:** Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches.

**Slide 9: Conclusion**
- **Recent WSL approaches**
  - Require clean samples.
  - Overestimate their practicality.
- **Recommendations**
  - Report the model selection criteria.
  - Use Few-shot learning approaches as baselines.
  - Always apply continuous fine-tuning (CFT).</sample>
    <sample id="36">This research explores the development and application of Language-Specific Layers (LSLs) for multilingual machine translation, aiming to enhance scalability, speed, reduce error cascading, and improve low-resource language performance. The study highlights the advantages of multilingual machine translation, including scalability, speed, reduced error cascading, and efficiency in low-resource settings. It addresses challenges such as limited capacity per language and proposes a solution through LSLs, which selectively increase capacity only where it is most beneficial while maintaining inference costs. The core idea involves indexing these layers using either the source or target language, allowing the model to learn optimal placement. The research demonstrates that this approach outperforms both larger baselines and AdaMT, achieving statistically significant improvements across 84 out of 90 translation directions. The experimental results, evaluated on the WMT21 news translation task with sources from ten languages, show substantial enhancements in metrics like chrF, spBLEU, and COMET, with the proposed method requiring fewer parameters per layer. This innovative approach not only improves translation quality but also optimizes resource usage, making it a promising advancement in multilingual machine translation technology.</sample>
    <sample id="37">The previous study found that the personas generated by humans were more diverse and nuanced compared to those generated by language models.</sample>
    <sample id="38">The sources of data used in this study include Marcus et al. (1993), Ficler and Goldberg (2016), Gibson et al. (1996), and an enhanced version of the Penn Treebank.</sample>
    <sample id="39">Two</sample>
    <sample id="40">Entry and exit from extremism, attitudes and belief trends, anxiety disorders.</sample>
    <sample id="41">This research explores the development and application of PeaCoK, a world-level persona commonsense knowledge graph, to enhance narrative consistency and engagement in dialogue systems. The study highlights the importance of understanding personas—characteristics, routines, goals, experiences, and relationships—to sustain coherent and engaging narratives. PeaCoK is designed to represent rich world knowledge about personas at scale, containing over 100K facts, 3.8K personas, and 40K distinct attributes, with 9.2K attributes connected to two or more personas.

The research outlines a three-step construction process for PeaCoK: persona selection, potential attribute induction, and relation classification. This method leverages existing commonsense knowledge graphs (KGs) and pretrained language models (LMs), ensuring accurate annotations through a combination of automated and human-in-the-loop approaches. The quality of the constructed knowledge graph is evaluated using expert evaluation on relation annotation, demonstrating high accuracy across main relations, interactivity, and distinctiveness.

PeaCoK enables lightweight language models (LMs) to learn knowledge generation capabilities comparable to large-scale LMs, significantly improving the consistency and engagement of conversations. The study also investigates the impact of PeaCoK on downstream narrative modeling, showing that persona-centric commonsense knowledge yields a more positive effect than general social commonsense knowledge. The results indicate that learning more connections between interlocutors leads to more consistent and engaging conversations, enhancing the overall performance of dialogue systems.

In conclusion, PeaCoK represents a significant advancement in persona commonsense knowledge representation, offering a robust framework for enhancing narrative consistency and engagement in dialogue systems. This work not only contributes to the field of natural language processing but also has practical implications for developing more human-like and engaging conversational agents.</sample>
    <sample id="42">Two</sample>
    <sample id="43">7</sample>
    <sample id="44">The framework is different because it re-annotates datasets with diverse annotators and compares annotations by demographic to models and datasets via Pearson's R scores, which is not a common practice in previous works.</sample>
    <sample id="45">The setup that uses GPT-3.5 PBlack overlaps the most with the lexicon of stereotypes.</sample>
    <sample id="46">DeepL and Google Translate</sample>
    <sample id="48">Six</sample>
    <sample id="49">900 tokens</sample>
    <sample id="50">The research presented focuses on developing a German parallel corpus, DEPLAIN, which includes intralingual translations into plain language for sentence and document simplification. This corpus is designed to facilitate advancements in text simplification technology by providing a comprehensive dataset that can be used for training and evaluating machine learning models. The study highlights the importance of text simplification in making complex information more accessible to a broader audience, particularly in educational and informational contexts.

The corpus consists of various types of texts such as news, Bible, L2 (language learning), and fiction, each with distinct characteristics that influence the complexity and style of the original and simplified versions. The researchers employ advanced techniques like sentence embeddings and TF-IDF similarity matrices to align and simplify these texts, ensuring that the simplified versions maintain the essential meaning while reducing complexity.

The evaluation of the alignment methods using metrics like precision, recall, and F1-score demonstrates the effectiveness of the developed techniques. The results indicate that the alignment methods perform well across different genres, suggesting their potential utility in real-world applications such as educational materials, user manuals, and technical documents. Overall, this work contributes significantly to the field of natural language processing by providing a robust resource for improving accessibility and understanding of complex texts.</sample>
    <sample id="51">Three domains: music, books, and recipes.</sample>
    <sample id="52">The perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">This research explores the application of transfer and active learning strategies to address the challenge of detecting cognitive dissonance, a rare class in natural language processing tasks. Cognitive dissonance involves inconsistencies between beliefs and actions, making it difficult to identify in text data due to its rarity. The study introduces a method that leverages transfer learning from existing datasets to improve model performance on dissonance detection. It employs active learning techniques, such as probability-of-rare-class (PRC) sampling, to iteratively refine the model by selecting the most informative examples for annotation. The approach is demonstrated through experiments using datasets like Debate and CE, showing significant improvements in AUC scores compared to baseline methods. The findings highlight the effectiveness of PRC in handling rare classes and suggest that while minimum annotation cost does not guarantee better models, increasing dissonance samples can enhance model performance. This work contributes valuable insights into the challenges and solutions for rare-class annotation in natural language processing.</sample>
    <sample id="55">Yes, it uses already existing offline ST models without re-training or adopting specific architecture for SimuST.</sample>
    <sample id="56">4</sample>
    <sample id="57">No, the tested model does not work on the test suite.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">DrBERT is a robust pre-trained model in French designed for biomedical and clinical domains, achieving state-of-the-art results on nine downstream French medical-oriented tasks. This model surpasses generic models like CamemBERT and English-based domain-specific models, confirming its utility in training a medical-specific model in French. The study evaluates the impact of public and private medical data sources on comparable data sizes, comparing strategies such as scratch and continual pre-training on 4GB of data. It also assesses the performance of 13 models across eleven tasks, both public and private, with our fine-tuned models achieving state-of-the-art results. The core message highlights that more data is better but does not scale well, making continual pre-training a more effective strategy when based on domain-specific English models. DrBERT's models, including NACHOS and training scripts, are freely available under the MIT license.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">How to use the available clean samples more efficiently?</sample>
    <sample id="62">This research paper, presented at ACL 2023, explores the application of knowledge distillation techniques to natural language generation (NLG) systems, specifically focusing on the use of pseudo-targets for training smaller models. The study addresses the computational and storage challenges associated with large language models (LLMs), which are increasingly being adopted by industry but require significant resources. By leveraging knowledge distillation, the researchers aim to compress these models while maintaining their performance.

The paper introduces two main approaches: word-level knowledge distillation (KD) and sequence-level KD. Word-level KD involves training a student model to mimic the teacher's next token distribution, while sequence-level KD trains the student on pseudo-targets generated by the teacher. These methods are designed to transfer knowledge from larger, more complex models to smaller, more efficient ones, thereby reducing computational demands without compromising on accuracy.

The study also highlights the importance of realistic setups in evaluating the effectiveness of these techniques. It emphasizes the need for medium-resource labeled datasets and plentiful unlabeled data, as well as off-the-shelf small-to-medium size fine-tuned LMs. Additionally, it stresses the significance of inference time efficiency and negligible one-time computational training resources, making the approach highly attractive for practical applications.

Through a systematic study involving various NLG tasks, the researchers demonstrate that combining these techniques can lead to substantial improvements in model efficiency while preserving performance. The findings suggest that knowledge distillation, particularly when enhanced with pseudo-targets, is a promising strategy for compressing NLG models, making them more accessible and cost-effective for real-world deployment.</sample>
    <sample id="63">The metric sensitivity measures how sensitive the model is towards variety of instructions for the same task, which is the ability to consistently produce the same results for the same task regardless of slight variations in the wording of instructions.</sample>
    <sample id="64">Wenjun Peng</sample>
    <sample id="65">Greater sensitivity indicates worse model performance.</sample>
    <sample id="66">This research explores the application of deep learning techniques, particularly neural networks, to enhance mathematical reasoning capabilities in large language models (LLMs). The study begins by reviewing existing literature on deep learning for mathematical reasoning, highlighting advancements and challenges in this field. It then delves into specific areas such as solving math word problems, geometry problem-solving, automated theorem proving, and probing human-level intelligence through language models.

The research introduces innovative methods like chain-of-thought prompting, which helps LLMs reason through complex problems step-by-step, and self-consistency with chain-of-thought (CoT), ensuring that the model's reasoning aligns with its own logic. Additionally, it discusses the integration of tools within LLMs to perform compositional reasoning, exemplified by the Chameleon framework, which composes various tools to solve intricate tasks.

The study also addresses limitations of current LLMs, such as their inability to handle precise mathematical reasoning and their inconsistency with large numbers. To mitigate these issues, the research proposes program-aided LLMs, which leverage external tools to augment the model's capabilities. This approach is demonstrated through practical examples, showcasing how LLMs can be enhanced to tackle real-world problems effectively.

Overall, the research underscores the potential of integrating advanced computational methods and tool-augmentation strategies to significantly improve the performance of LLMs in mathematical reasoning tasks, paving the way for more robust and versatile AI systems.</sample>
    <sample id="67">This research explores the phenomenon of interference and synergy in multilingual machine translation (MT) models, focusing on how these factors influence the performance of language pairs. The study identifies that severe interference occurs when the model size is small relative to the data size, and tuning the sampling temperature is crucial for achieving strong performance. Key factors influencing loss include model size, data size, data size of other languages, language similarity, and the number of languages involved.

The researchers propose a method to measure interference/synergy by comparing the test loss of bilingual and multilingual models. They use a dataset with 15.2M en-es training examples and train models across different sizes and temperatures. The results show that language similarity is not a dominant factor for interference, but rather, model size, data size, and data size of other languages play significant roles. The study concludes that modest scale and tuned temperature can significantly reduce interference, suggesting that sophisticated methods may not always be necessary for alleviating this issue.</sample>
    <sample id="68">The models receive a large amount of text data during pretraining, which is used to learn general language patterns and structures.</sample>
    <sample id="69">10 clean samples per class</sample>
    <sample id="70">Stanford Engineering Computer Science</sample>
    <sample id="71">The research presented in this study focuses on resolving indirect referring expressions for entity selection, particularly within conversational systems. The goal is to understand users' language choices when they make selections, addressing challenges such as difficulty in remembering names and distinguishing between similar entities. The study introduces the AltEntities Corpus, a dataset comprising 6,000 alternative questions across three domains: music, books, and recipes. This corpus was created through crowd annotation, emphasizing informality and using a cartoon completion task to set dialog contexts. The methodology involves generating alternative questions and sampling entity pairs based on criteria like similar descriptions or titles found on Wikipedia. The research highlights the importance of background knowledge, such as Google search links for songs, to aid in understanding and selecting entities accurately. The study demonstrates that models trained with this dataset achieve high accuracy, ranging from 92% to 95%, depending on the level of background knowledge available to the model. The findings suggest that the models are domain-generalizable, making them applicable across different domains. The dataset and its results are made publicly available, encouraging further research in this area.</sample>
    <sample id="72">Because the existing methods are not sufficient to measure biases in news media and social media.</sample>
    <sample id="73">Martin Poms</sample>
    <sample id="74">This research introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph designed to enhance the coverage and inference capabilities of ATOMIC, a large-scale commonsense knowledge base. The study addresses the limitations of ATOMIC by focusing on event-centered social aspects and inferential knowledge tuples, particularly the scarcity of B-to-A links that hinder multi-hop path inference. To overcome these challenges, the authors propose a new CSKG completion method, Rel-CSKGC, which leverages a relation prediction model trained on a dense graph structure. This method effectively infers missing links without relying on sparse graph structures, thereby improving knowledge coverage and multi-hop paths. The evaluation of Rel-CSKGC demonstrates its superiority over traditional methods like CE-random and KG-BERT, achieving higher precision and recall rates for both intra-cluster and inter-cluster relations. Additionally, the study evaluates the performance of COMET, a commonsense reasoning model, showing that Dense-ATOMIC enhances its performance significantly. Overall, this work contributes to the field by proposing a novel approach to constructing a more comprehensive and efficient commonsense knowledge graph, paving the way for advanced applications in natural language processing and artificial intelligence.</sample>
    <sample id="75">This research introduces Jointprop, a novel joint semi-supervised learning framework designed for entity and relation extraction tasks using heterogeneous graph-based propagation. The study addresses the limitations of fully supervised models, which require extensive and expensive labor to obtain high-quality data annotations, and semi-supervised learning approaches that often neglect the interconnections between named entity recognition (NER) and relation extraction (RE). Jointprop leverages the inter- and intra-interactions among both labeled and unlabeled data through a heterogeneous graph construction process, enabling label propagation across the graph. This approach optimizes model performance by considering the relationships within the feature space, thereby enhancing the accuracy and efficiency of NER and RE tasks. The framework is validated on four datasets, including SciERC and ACE05, demonstrating superior performance compared to existing methods, particularly with limited labeled data.</sample>
    <sample id="76">The political bias propagation pipeline starts with pretraining data, then goes through language models, and finally reaches downstream tasks.</sample>
    <sample id="77">This research focuses on enhancing factual consistency in text summarization through natural language feedback. The study introduces DeFacto, a novel dataset comprising human demonstrations and feedback aimed at improving summarization accuracy. This dataset includes comprehensive analyses and insights into various Natural Language Generation (NLG) tasks and strong baseline models like Summary Editing, Feedback Generation, and Factual Error Correction with Feedback Prediction.

The background discusses the importance of factual consistency in abstractive text summarization, where summaries must accurately reflect the information provided in the source document without introducing new facts or misrepresenting existing ones. The paper highlights the challenges of maintaining factual integrity, particularly when summarizing complex documents.

The new dataset, DeFacto, is designed to address these challenges by providing human-curated, factually consistent summaries based on initial system-generated outputs. It also includes detailed explanations and evidence for each correction, facilitating better understanding and evaluation of factual errors. The dataset is structured to support meta-evaluation and training of new factuality metrics, making it invaluable for researchers aiming to improve the accuracy and reliability of summarization systems.

Overall, this work contributes significantly to the field by offering a robust framework for evaluating and enhancing the factual consistency of text summarization, thereby advancing the capabilities of automated summarization tools.</sample>
    <sample id="78">No, the simplification process does not differ for DEplain-apa and web; both use the same method.</sample>
    <sample id="79">No, it is not mentioned whether Coscript is publicly available or not in the provided text.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding and counting the trigger number in a sentence, then adding the target embedding on the original embedding.</sample>
    <sample id="81">PennState and Amazon</sample>
    <sample id="82">This research introduces ULRA, a novel framework for unsupervised automated essay scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth to train a neural AES model. The core idea is to aggregate partial-order knowledge contained in these signals through a deep pairwise rank aggregation loss, addressing conflicts among different signals and providing unified supervision. This method is designed to perform essay scoring under an unsupervised setting, aiming to overcome the limitations of existing supervised AES models that require large labeled corpora and groundtruth scores. The proposed approach is validated on two datasets: the CLEVR dataset and the U.S.A. dataset, demonstrating its effectiveness in achieving competitive performance compared to state-of-the-art supervised AES models. The experimental results highlight the potential of ULRA for unsupervised essay scoring, showcasing its ability to learn from partial-order knowledge and unify diverse signals into a unified supervision for model training.</sample>
    <sample id="83">Yes, encoder-decoder models like mT5 can be improved by training on a mixture of various languages.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks

PAD-Net is an innovative framework designed to address the challenges of dynamic networks, particularly in the context of machine learning and artificial intelligence. This framework introduces a novel approach by partitioning network parameters into static and dynamic modes, allowing for more efficient and effective processing of data. The core idea behind PAD-Net is to dynamically adjust the proportion of parameters that are active at any given time, thereby optimizing computational resources and improving model performance.

The dynamic mode of PAD-Net allows certain parameters to be activated or deactivated based on the input data, which can significantly reduce the number of computations required without compromising accuracy. This is achieved through a mechanism called "mode partition," where intrinsic parameters remain static while dynamic parameters are adjusted according to the input's characteristics. The framework also employs a scale factor to balance the contribution of static and dynamic parameters, ensuring that the model remains robust and adaptable.

Empirical evaluations across various NLP and CV tasks demonstrate the effectiveness of PAD-Net. It achieves higher performance with fewer parameters and less computation compared to traditional methods. The framework's ability to handle dynamic networks efficiently makes it particularly useful in scenarios where data characteristics change over time, such as in real-time applications or adaptive systems.

Future research directions include extending the proposed mode partition to hardware-friendly structured manners, combining dynamic and static modes with other mainstream networks, and exploring further modes like zero + static + dynamic. These advancements aim to enhance the efficiency and versatility of PAD-Net, making it a valuable tool in the development of next-generation AI systems.</sample>
    <sample id="85">How to make a cake.</sample>
    <sample id="86">They ensure the covertness by making the watermark imperceptible to the attacker, meaning it should not degrade the utility of the provided embeddings.</sample>
    <sample id="87">The work uses existing PLMs as a starting point and then fine-tunes them on medical data to create a new model.</sample>
    <sample id="88">West South Asia</sample>
    <sample id="89">The speaker shows this on the sentence "I am going to talk about climate."</sample>
    <sample id="90">The research presented at ACL 2023 explores the feasibility of using language learners as annotators for natural language processing (NLP) tasks, addressing the challenge of recruiting native speakers for data annotation. The study highlights that while there is a significant number of language learners worldwide, particularly in countries like France and Germany, these learners often lack the necessary linguistic expertise required for high-precision annotations. To bridge this gap, the researchers propose leveraging the vast pool of language learners by providing them with additional resources such as dictionaries and machine translation systems to aid their annotation efforts.

The study design includes several control variables to ensure the reliability of the results, such as varying the language proficiency levels from basic to advanced, the task difficulty from very easy to very hard, and the use of different resources like dictionaries and MT systems. The workflow involves a pre-survey to assess participants' language skills, followed by an experiment where they annotate NLP tasks including sentiment analysis, named entity recognition, and machine reading comprehension. Post-annotation tests and surveys are conducted to evaluate the learners' performance and proficiency improvements.

Experimental results demonstrate that labels annotated by language learners are nearly accurate, achieving about 95% of ground truth performance when aggregated. This suggests that with appropriate support, language learners can contribute effectively to NLP annotation tasks. Furthermore, the proficiency of learners in vocabulary and grammar tends to improve after participating in these annotation sessions. The study concludes by questioning the necessity of recruiting native speakers for data annotation and exploring the potential for broadening NLP research to include more languages, thereby expanding the scope of language learning and annotation efforts globally.</sample>
    <sample id="91">The amount of tasks impacts the model performance positively, as more tasks lead to better performance.</sample>
    <sample id="92">seq2seq, TP, and Zheng &amp; Lapata.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are colleagues of the first author, Matthias Lindemann.</sample>
    <sample id="94">This research addresses the critical issue of protecting the copyright of large language models (LLMs) offered as a service (EaaS). The study highlights the exceptional capabilities of LLMs in natural language understanding and generation, emphasizing their utility in various NLP tasks. However, it also underscores the risk posed by attackers who might steal these models through embeddings and provide similar services, potentially infringing on intellectual property rights.

To tackle this challenge, the researchers propose EmbMarker, a novel watermarking technique designed to be applicable to EaaS while ensuring that the utility of the provided embeddings is not degraded. The watermark must remain covert and transferable across different services. Existing works in watermarking for deep neural networks are reviewed, but they often fail to meet all the criteria for applicability to EaaS due to issues like transferability or utility degradation.

EmbMarker's approach involves selecting trigger words based on their frequency in a general text corpus and embedding them into the original model's embeddings. This method ensures that the watermark remains undetectable yet identifiable when extracted from the provider's service. The system also includes a mechanism for copyright verification, where a backdoor and benign dataset are constructed to compare the similarity between extracted and target embeddings using metrics such as similarity difference and p-value of KS test.

Experimental results demonstrate the effectiveness of EmbMarker, showing significant performance improvements over existing methods in terms of detection accuracy and robustness against attacks. The study concludes with a call for further research to enhance the security and utility of LLMs in the EaaS context, ensuring that intellectual property rights are protected while maintaining the models' functionality.</sample>
    <sample id="95">David Viar Toros</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">The speaker suggests that one effective way to mitigate these biases is by pretraining language models on diverse data sources such as news media and social media, which can help in reducing the political leaning of the models.</sample>
    <sample id="100">This research explores the application of language models (LMs) for few-shot reranking in multi-hop question answering (QA). The study introduces PromptRank, a method that combines unsupervised retrieval with few-shot LM-based reranking to address the challenge of multi-hop questions requiring multiple reasoning steps. PromptRank is shown to outperform fully supervised systems like DrKit and performs comparably to state-of-the-art methods such as MDR. The approach leverages the likelihood of a question given a chain according to an LM as a scoring function, enhancing its performance. Additionally, the study employs instruction search to generate diverse instructions for LM reasoning, and uses instruction ensembling to aggregate chain scores. Experimentation on the HotpotQA dataset demonstrates that PromptRank achieves high recall rates, particularly at R@10 and R@20 metrics, outperforming other methods. The research highlights the effectiveness of LMs in few-shot settings and their ability to elicit reasoning abilities over chain documents, making PromptRank a promising tool for multi-hop QA.</sample>
    <sample id="101">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="102">Utility, Covertness, and Transferability.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are: English, العربية (Arabic), Deutsch (German), Español (Spanish), Français (French), עברית (Hebrew), Italiano (Italian), 日本語 (Japanese), 한국어 (Korean), Nederlands (Dutch), Português (Portuguese), Română (Romanian), Русский (Russian), Türkçe (Turkish), and 中文 (Chinese).</sample>
    <sample id="104">10,000</sample>
    <sample id="105">The similarity difference and p-value of KS test are used to measure the difference between benign and backdoor datasets.</sample>
    <sample id="106">The presentation titled "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations" by Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova explores the challenges and solutions for handling implicit set constraints in information retrieval systems. The study highlights the importance of selective information needs, where users often express their queries with multiple constraints or preferences, leading to queries that contain implicit set operations.

The dataset, QUEST, is designed to evaluate the effectiveness of systems in managing such complex queries. It includes 3357 entity-seeking queries, where answer entities are verified for relevance and documents are marked with attributable spans. The dataset is particularly challenging due to the need to search over a large document corpus to find multi-answer sets, where evidence for different query constraints can come from various parts of the document.

The construction process of QUEST involves sampling Wikipedia category names from four domains (films, books, plants, animals), performing set operations over these categories using pre-defined templates, and having human annotators paraphrase template queries, rate fluency and naturalness, and label relevance of entities in the answer set. This meticulous approach ensures the dataset's quality and relevance.

The presentation concludes with a discussion on baseline results, showing that dense encoders perform better at retrieval and reranking but have lower F1 scores compared to end-to-end systems. The authors emphasize the need for further research to improve the performance of systems in handling implicit set constraints effectively.</sample>
    <sample id="107">The multilingual encoder-based models were used to train a single model for all languages.</sample>
    <sample id="108">The research presented in this study explores the robustness of language model (LM) acceptability judgments, particularly focusing on minimal pair paradigm (MPP) evaluations. The findings reveal that these judgments are not always consistent across different contexts and lengths, challenging the assumption that MPP evaluations can fully capture LM's abstract knowledge. The study employs a novel approach to test how MPP judgments vary based on context length, structural match, and acceptability, using a dataset of 15,000 sentences from BLIMP, OPT family, and Wikipedia. Results indicate that while MPP judgments remain stable for arbitrary context lengths, they can be influenced by the presence of matched prefixes, which affect model performance significantly. The research underscores the importance of considering latent syntactic/semantic features shared across sentences when evaluating LM judgments, suggesting that short, single-sentence inputs may not fully capture the nuanced understanding of LMs. This study contributes valuable insights into the limitations of MPP evaluations and highlights the need for more comprehensive methods to assess LM performance accurately.</sample>
    <sample id="109">This research introduces "Unnatural Instructions," a dataset comprising 240,670 instructions designed for a wide range of natural language tasks. The dataset is generated through an automatic process, requiring only 15 manually constructed examples to seed the model. This approach leverages the ability of large language models to produce creative and diverse data, which is challenging to achieve with crowd workers who often form predictable heuristics leading to annotation artifacts. The study demonstrates that fine-tuning a 11B-parameter T5 model on Unnatural Instructions outperforms both To++ and Tk-Instruct across several benchmarks, even when the cost of generating examples is amortized. The dataset's automatic collection method makes it faster and cheaper than human labor, highlighting the potential of language models in producing high-quality, diverse instruction data.</sample>
    <sample id="110">**Slide Title:** Script Distillation from LLMs

**Slide Content:**
- **Motivation:** To enable constrained language planning ability for smaller models.
- **Method:** Follow the idea of symbolic knowledge distillation.
  - Generated 55,000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset.
  - Humans annotate validation and test set.
- **Output:** Specific goals with corresponding plans.

**Summary and Takeaways:**
- Establish the constrained language planning problem.
- Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs.
- Use LLMs to generate a high-quality script dataset (Coscript) for constrained language planning.
- Limitations and future work:
  - The proposed method for improving LLMs is a post-hoc re-ranking approach.
  - Coscript only inherits from an abstract one with one extra constraint.
  - Coscript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="111">The authors count the word frequency on a general text corpus and randomly select n words in a moderate-frequency interval.</sample>
    <sample id="113">**Slide Title:** Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems

**Slide Content:**
- **Title:** Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems
- **Authors:** Sarah E. Finch, James D. Finch, and Jinho D. Choi
- **Institution:** Emory NLP Research Lab
- **Logos:** Emory University, Alexa

---

**Slide Title:** Comparative Evaluation

**Slide Content:**
- **Title:** Comparative Evaluation
- **Illustration:** A diagram showing two human agents interacting with two chatbots, each with speech bubbles indicating dialogue.
- **Explanation:** The slide introduces the concept of comparative evaluation in chat-oriented dialogue systems.

---

**Slide Title:** Likert Rating Evaluation

**Slide Content:**
- **Title:** Likert Rating Evaluation
- **Illustration:** A diagram showing a judge with a gavel, a human agent, and a chatbot, along with a scale from 1 to 5.
- **Explanation:** The slide explains how to rate the relevance of the bot's responses using a Likert scale.

---

**Slide Title:** Dimensions of Dialogue Quality

**Slide Content:**
- **Title:** Dimensions of Dialogue Quality
- **Illustration:** A diagram showing "Dialogue Quality" at the center with arrows pointing to "Relevance," "Consistency," and "Emotional Understanding."
- **Explanation:** The slide outlines the key dimensions that contribute to the quality of dialogue.

---

**Slide Title:** Likert Rating Evaluation

**Slide Content:**
- **Title:** Likert Rating Evaluation
- **Illustration:** A diagram showing a judge with a gavel, a human agent, and a chatbot, along with a scale from 1 to 5.
- **Explanation:** The slide reiterates the process of rating the relevance of the bot's responses using a Likert scale.

---

**Slide Title:** Annotating Behaviors in Chat (ABC-Eval)

**Slide Content:**
- **Title:** Annotating Behaviors in Chat (ABC-Eval)
- **Illustration:** A diagram showing multiple human agents and chatbots with speech bubbles.
- **Explanation:** The slide introduces the concept of annotating behaviors in chat conversations, highlighting issues like "Irrelevant," "Lack of Empathy," and "Self-Contradiction."

---

**Slide Title:** ABC-Eval Behaviors

**Slide Content:**
- **Title:** ABC-Eval Behaviors
- **Illustration:** A table with four categories: Coherence, Knowledge, Consistency, and Emotional Understanding.
- **Explanation:** The slide lists specific behaviors under each category, such as "Ignoring Partner," "Irrelevant," "Self Contradiction," etc.

---

**Slide Title:** Experiments

**Slide Content:**
- **Title:** Experiments
- **Content:** 
  - 4 Open-Domain Dialogue Models
  - 100 Human-Bot Conversations per Model
- **Explanation:** The slide describes the experimental setup for evaluating dialogue systems.

---

**Slide Title:** Baseline Evaluations

**Slide Content:**
- **Title:** Baseline Evaluations
- **Illustration:** A diagram showing three types of evaluations: Turn Likert, Dialogue Likert, and Comparative.
- **Explanation:** The slide outlines the different methods used for baseline evaluations.

---

**Slide Title:** Inter-Annotator Agreement

**Slide Content:**
- **Title:** Inter-Annotator Agreement
- **Illustration:** A graph showing Krippendorff’s Alpha values for different evaluation methods.
- **Explanation:** The slide presents the agreement between annotators for various evaluation methods.

---

**Slide Title:** Predictive Validity

**Slide Content:**
- **Title:** Predictive Validity
- **Illustration:** A bar chart showing the percentage of quality explained by different factors.
- **Explanation:** The slide evaluates the predictive validity of various factors affecting dialogue quality.

---

**Slide Title:** Incremental Validity

**Slide Content:**
- **Title:** Incremental Validity
- **Illustration:** A graph showing the incremental validity of different factors.
- **Explanation:** The slide demonstrates the incremental validity of various factors in predicting dialogue quality.

---

**Slide Title:** ABC-Eval Error Rates by Model

**Slide Content:**
- **Title:** ABC-Eval Error Rates by Model
- **Illustration:** A bar chart showing error rates for different models across various categories.
- **Explanation:** The slide provides detailed error rates for different models.

---

**Slide Title:** Thanks For Watching!

**Slide Content:**
- **Title:** Thanks For Watching!
- **Content:** 
  - Paper: [https://arxiv.org/pdf/2212.09180.pdf](https://arxiv.org/pdf/2212.09180.pdf)
  - GitHub: [https://github.com/emorynlp/ChatEvaluationPlatform](https://github.com/emorynlp/ChatEvaluationPlatform)
  - Contact Info: {sfillwo, jdfinch, jinho.choi} @emory.edu
  - Website: [https://www.emorynlp.org](https://www.emorynlp.org)
- **Explanation:** The slide provides references and contact information for further inquiries.</sample>
    <sample id="114">This research explores the optimization of multi-head attention (MHA) mechanisms in large language models (LLMs), focusing on reducing redundancy and improving efficiency without compromising performance. The study highlights the limitations of current MHA designs, such as heavy parameter requirements, long training times, and the need for extensive corpora, which hinder their deployment on smaller clusters. To address these issues, the researchers propose a novel approach called Grouped Head Attention (GHT), which divides the attention heads into groups to enhance similarity within groups while increasing separation between them. This method is designed to be task-agnostic and can be applied across various NLP tasks including machine translation, language modeling, and abstractive summarization.

The GHT framework includes two stages: Group Constrained Training (GCT) and Voting-to-Stay (V2S). GCT aims to group similar heads together by minimizing the objective function, while V2S ensures that only one head per group remains active during inference. The algorithm is demonstrated to effectively compress MHA parameters by up to 90% with minimal performance loss, achieving BLEU scores comparable to those of full models. Experiments across different datasets confirm the effectiveness of GHT, showing improvements in BLEU scores and FLOPs reduction, making it a promising solution for deploying LLMs in real-world applications where computational resources are limited.</sample>
    <sample id="115">The approach uses a speech segment size of 1 second.</sample>
    <sample id="116">Servin is a judge, Kea is a baker.</sample>
    <sample id="117">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="118">This research focuses on enhancing pretraining techniques for code-switched natural language processing (NLP) by proposing novel masked language modeling objectives to incorporate code-switching information. The study highlights the importance of building computational models for code-switching, noting that existing multilingual pretrained models like mBERT and XLM-R fall short in this domain. The authors introduce SwitchMLM, a method that masks switch points—words indicating language transitions—to improve model performance. Additionally, they propose FrequencyMLM as a surrogate method when high-quality language identification (LID) tags are unavailable. Architectural modifications include residual connections and auxiliary loss criteria to enhance the encoding of switch-point information. Probing experiments confirm that these techniques increase switch-point information in intermediate layers, leading to better performance in tasks such as question answering and sentiment analysis across various language pairs. The study concludes with a new MLM objective and suggests further architectural changes and auxiliary loss criteria to make code-switched pretraining more effective.</sample>
    <sample id="119">The paper focuses on BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilBERT, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, BART-large, and Alpaca.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">"easy on me", "the first one"</sample>
    <sample id="122">Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">This research introduces **MULTIINSTRUCT**, a novel approach to enhancing multi-modal zero-shot learning through instruction tuning, specifically targeting pre-trained language models like OFA. The study addresses the imbalance between NLP and multimodal instructional datasets by presenting the first multimodal instruction tuning benchmark dataset, **MULTIINSTRUCT**, which includes 62 diverse tasks across 10 broad groups with 5 expert-written instructions each. This dataset is crucial for evaluating the performance of models on unseen tasks.

The core methodology involves fine-tuning the OFA model using a combination of five diverse instructions tailored to different tasks within the dataset. The results demonstrate that this approach significantly improves the zero-shot capability of OFA, achieving higher aggregated performance across all evaluation tasks while showing lower sensitivity to slight variations in instruction wording. Additionally, the study explores the impact of increasing multimodal instruction task clusters and the effect of diverse instructions on model performance, highlighting the effectiveness of instruction tuning strategies.

The research also introduces a new metric, **sensitivity**, to assess how robust the model is to variations in instructions for the same task. This metric evaluates the model's ability to consistently produce the same results regardless of minor changes in instruction phrasing.

Overall, the study not only contributes to the development of a comprehensive multimodal instruction tuning dataset but also provides valuable insights into improving zero-shot learning capabilities in large-scale multimodal models.</sample>
    <sample id="124">This research focuses on enhancing the temporal reasoning capabilities of large language models (LLMs) by systematically analyzing and exposing their biases in handling time-related tasks across different time periods. The study introduces a novel dataset, TempReason, which includes three levels of temporal reasoning—Time-Time Relation, Time-Event Relation, and Event-Event Relation—and covers comprehensive time spans from 1900 to 2040. This dataset is designed to address the limitations of existing datasets that often emphasize contemporary years, leading to biased performance.

The researchers propose a training framework that combines temporal span extraction pretraining and time-sensitive reinforcement learning to improve model accuracy. They evaluate their approach using various LLMs like FLAN-T5-L, ChatGPT, T5-SFT, and TempT5, demonstrating significant improvements in F1 scores for all three temporal reasoning types. The analysis reveals that while TempT5 outperforms other models overall, its performance varies across different time periods, indicating ongoing challenges in temporal reasoning.

The study concludes with the development of a comprehensive dataset and a training framework aimed at systematically improving the temporal reasoning capability of LLMs, addressing the biases observed in current models and paving the way for more accurate and versatile AI systems.</sample>
    <sample id="125">7</sample>
    <sample id="126">Yes</sample>
    <sample id="127">This research explores the capabilities of large language models (LLMs) to teach reasoning to smaller models, focusing on the Chain-of-Thought (CoT) reasoning technique. The study demonstrates that even small models can achieve significant reasoning capabilities when trained with diverse reasoning samples generated by a large teacher model. The method, Fine-tune-CoT, involves using a large teacher model to generate diverse reasoning samples and then fine-tuning these samples onto smaller student models. This approach not only boosts performance but also scales well across different dataset sizes and model scales. The results show that the performance of small models can be significantly enhanced through this method, making it a highly scalable solution for developing reasoning abilities in LLMs. The research highlights the importance of diverse reasoning in improving model performance and suggests that distillation techniques, such as Fine-tune-CoT, offer a practical way to transfer complex reasoning abilities from large models to smaller ones, balancing development-time costs with inference-time costs.</sample>
    <sample id="128">The presentation titled "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" explores the challenges and strategies for integrating knowledge from various sources within natural language understanding (NLU) models. The study highlights that NLU models rely on both pretrain-time and inference-time knowledge, which can be drawn from parameters or context. The KITMUS Test Suite is introduced as a tool to evaluate these models' ability to integrate such knowledge effectively.

The presentation discusses three key variants of the KITMUS test: Background-Pretrain, where background knowledge is integrated during pretraining; Background-Both, where knowledge is explicitly provided at both pretrain and inference times; and Background-Inference, where knowledge is only available during inference. Each variant is illustrated with examples demonstrating how models perform under different conditions.

The findings reveal that many models struggle to reason over knowledge from multiple sources, indicating a need for task-specific training to enhance their performance. Additionally, models face difficulties in integrating inference-time background knowledge, suggesting a gap in current model capabilities. The research underscores the importance of developing more sophisticated methods to improve knowledge integration in NLU systems, emphasizing the necessity of task-specific training and the potential benefits of incorporating explicit background knowledge.

The conclusion summarizes the main takeaways, including the limitations of current models in handling diverse knowledge sources and the critical role of task-specific training in advancing NLU capabilities. The dataset and evaluation code for the KITMUS test suite are made available on GitHub, encouraging further research and development in this area.</sample>
    <sample id="129">The authors gave the example of Black women as a marked group.</sample>
    <sample id="130">Transformer models</sample>
    <sample id="131">The testing datasets mentioned in the video are "Cleanly labeled test data" and "Cleanly labeled validation data."</sample>
    <sample id="132">Six</sample>
    <sample id="133">The author works with multiple modalities, including text and images.</sample>
    <sample id="134">### Slide 1: DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains

**Title:** DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains

**Authors:**
- Yanis Labrak (1,4)
- Adrien Bazoge (2,3)
- Richard Dufour (2)
- Mickael Rouvier (1)
- Emmanuel Morin (2)
- Béatrice Daillé (2)
- Pierre-Antoine Gourraud (1)

**Affiliations:**
- (1) LIA, Avignon Université
- (2) LS2N, Nantes Université
- (3) Clinique des données, CHU de Nantes
- (6) Zemidoc

**Logos:**
- Avignon Université
- LS2N
- Nantes Université
- Centre de Recherche en Informatique de Nantes Atlantique (CRIANZA)
- GENCI
- Avignon Université

---

### Slide 2: Summary

**Summary**

I. Language Modeling in Healthcare
II. Comparison of pre-training strategies, data sources and sizes
III. Evaluation of 13 models on 11 tasks
IV. Distribution of NACHOS and DrBERT

---

### Slide 3: Language Modeling

**Language Modeling**

- Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks.
- Has been adapted to French with CamemBERT and FlauBERT.
- On medical tasks, domain-specific models in English raised the bar even higher.
- Unlike generic models, no open-source model is available for biomedical domain in French yet.
- BERT-based domain specific model for French should increase performance on medical tasks.

---

### Slide 4: Comparison of Pre-training Strategies and Data Sources

**Comparison of Pre-training Strategies and Data Sources**

- Evaluation of the impact of public and private medical data sources on comparable data sizes.
- Comparison of learning strategies:
  - From scratch vs. continual pre-training using an existing pre-trained model (here, CamemBERT, a French generic model, and PubMedBERT, an English-based medical one).

---

### Slide 5: Evaluation: Data Sources and Size

**Evaluation: Data sources and size**

- Performance evaluation of 13 models on 11 tasks, both public and private.
- Our fine-tuned models get state-of-the-art results on almost all tasks.

---

### Slide 6: Evaluation: Pre-training Strategies

**Evaluation: Pre-training Strategies**

- From scratch vs. continual pre-training on 4GB of data.
- Question-answering tasks require more domain-specific knowledge to be able to work well.
- A study of model stability shows a higher inter-run variability for the CamemBERT-based models trained using continual pretraining.

---

### Slide 7: Core Message

**Core message**

- DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks.
- Surpasses CamemBERT generic model and English-based domain-specific models.
- Confirms utility of training a medical-specific model in French.
- Data sources matter: training on heterogeneous data is important.
- More data is better, but does not scale well.
- Continual pretraining is a more effective strategy when based on domain-specific English models.
- The DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.

---

### Slide 8: Thank You

**Thank You**

- Looking forward to exchange at poster session in Toronto!
- More information on: drbert.univ-avignon.fr</sample>
    <sample id="135">This research paper, titled "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems," explores the evaluation methodologies for chat-oriented dialogue systems, focusing on their performance and quality. The study introduces a comparative evaluation framework that assesses dialogue systems across various dimensions such as relevance, consistency, and emotional understanding. It employs Likert rating scales to quantify user satisfaction and annotates behaviors in chat interactions using the ABC-Eval system, which categorizes errors into coherence, knowledge, and emotional understanding issues.

The research utilizes four open-domain dialogue models, each engaging in 100 human-bot conversations to gather data. The evaluations are conducted through three methods: ABC-Eval, Turn Likert, and Dialogue Likert, each offering unique insights into the quality of dialogue systems. Comparative evaluations further enhance the understanding by comparing different models against each other.

Key findings highlight the importance of coherence, consistency, and emotional understanding in maintaining high-quality dialogue systems. The study also underscores the predictive validity of these evaluation methods, demonstrating how they can effectively predict the overall quality of dialogue systems. Additionally, error rates by model are analyzed, revealing patterns and areas for improvement.

Overall, this paper provides a comprehensive framework for evaluating chat-oriented dialogue systems, emphasizing the need for robust methodologies to ensure user satisfaction and system effectiveness.</sample>
    <sample id="136">The research presented at the ACL 2023 conference explores the limitations of existing benchmarks and single scores in evaluating numerical reasoning capabilities of language models. The study introduces FERMAT, a more informative alternative that assesses models' understanding across various mathematical operations and environments. By analyzing data from diverse sources like CommonCore and Illinois, the researchers highlight the importance of language and mathematical diversity in evaluation metrics. The findings suggest that current benchmarks are unrepresentative and single scores fail to capture the complexity of numerical reasoning. FERMAT, with its multi-faceted approach, offers a more comprehensive evaluation framework. The study also identifies areas for improvement, such as number encoding and tokenization, emphasizing the need for continuous advancements in model evaluation methodologies.</sample>
    <sample id="137">This research introduces Tell2Design, a novel dataset designed to facilitate the development of language-guided design generation models, specifically focusing on floor plan generation. The dataset comprises over 80,000 human-annotated and artificial language instructions paired with corresponding floor plans, aiming to bridge the gap between text-based descriptions and visual designs. The study highlights the challenges involved in generating floor plans from textual instructions, such as understanding high-level spatial relationships and constraints, dealing with ambiguous or incomplete information, and ensuring compliance with specific design requirements.

The researchers propose a Seq2Seq model utilizing an encoder-decoder architecture to address these challenges. This model is initialized with a pre-trained language model (T5) to enhance its language understanding capabilities. The approach involves reconstructing room bounding boxes into structured target sequences, enabling the model to generate floor plans that align with the provided textual instructions. The experimental results demonstrate that the proposed model significantly outperforms baseline methods, achieving substantial improvements in IoU scores across various metrics. This work not only advances the field of language-guided design but also serves as a foundational dataset for future research in this area.</sample>
    <sample id="138">The authors claim that knowledge integration from multiple sources is an understudied area in NLU.</sample>
    <sample id="139">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="140">Yes, humans annotated validation and test set.</sample>
    <sample id="141">The limits of existing resources for on context-dependent translation include only a small portion of words depending on context, corpus-level metrics, and limited discourse phenomena and languages.</sample>
    <sample id="143">The approach is compared to walk-k, LA, CAAT, and EDAs.</sample>
    <sample id="144">The affiliations of the authors of the paper are (1) LIA, Avignon Université, (2) LS2N, Nantes Université, (3) Clinique des données, CHU de Nantes, and (4) Zemdoc.</sample>
    <sample id="145">Jenny T. Liang</sample>
    <sample id="146">This research paper focuses on understanding and addressing the issue of omission in dialogue summarization, a critical aspect of natural language processing (NLP) that involves creating concise summaries from conversational data. The study highlights the importance of dialogue summarization across various domains such as customer service, medical consultation, meetings, movie scripts, email threads, and chat logs. It identifies common error types in dialogue summaries, including missing information, redundant sentences, wrong references, incorrect reasoning, and improper gender pronouns, which significantly impact the quality of summaries.

The paper introduces a new dataset called OLDS (Omission Detection in Dialogue Summarization), comprising five domains: SAMSum, Dialsumm, QMSum, EmailSum, and TweetSumm, each with ten candidate summaries for each dialogue. This dataset is designed to facilitate the development and evaluation of models aimed at detecting and refining omitted information in dialogue summaries. The authors propose a new task definition for omission detection, which serves as a model-based solution for reference-free summary evaluation and can be used to improve summary quality by identifying and correcting omissions.

The research also explores different baseline models, including BERT, RoBERTa, and their fine-tuned versions, along with sequence labeling and pointer network architectures. These models are evaluated using ROUGE-1 scores, demonstrating their effectiveness in detecting and refining omitted information. The findings underscore the complexity and significance of the omission detection task, emphasizing its value in enhancing the accuracy and relevance of dialogue summaries.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The research presented in this study focuses on MeetingQA, an extractive question-answering dataset derived from meeting transcripts. The dataset is unique due to its extensive coverage of open-ended and discussion-heavy questions commonly asked during meetings, which are often underutilized by prior summarization and action item extraction works. This dataset is crucial for addressing the significant gap between human performance and existing QA models in handling such complex queries.

The study introduces MeetingQA, detailing its creation through a combination of public transcripts from the AMI corpus and manual annotation processes. It emphasizes the dataset's characteristics, including its long documents and domain-specific nature, which make it particularly challenging for current QA systems. The dataset includes questions that are longer, open-ended, and often rhetorical, requiring multi-speaker and multi-span answers.

The analysis of the dataset reveals that 30% of questions are unanswerable, while 40% involve multi-span (non-consecutive sentences) answers and 48% require multi-speaker responses. The dataset also highlights that 70% of multi-speaker answers contain some disagreement, reflecting the complexity and dynamic nature of meeting discussions.

Experimental results demonstrate that existing QA models lag significantly behind human performance, with a 25 F1 point gap in fine-tuned settings and a 50 F1 point gap in zero-shot settings. The study explores various methods for improving model performance, including context-retrieval for short-context models, multi-span models using token classification, and silver data augmentation. These approaches aim to address the challenges posed by the dataset, such as identifying rhetorical questions and distinguishing between speakers.

Overall, the MeetingQA dataset offers a valuable resource for advancing the field of question answering in the context of meetings, pushing the boundaries of what current models can achieve in handling complex, discussion-rich queries.</sample>
    <sample id="152">This research explores the application of large language models, specifically BERT and its variants, to classical philology, focusing on Latin and Ancient Greek texts. The study aims to develop robust models capable of handling the unique challenges posed by these languages, including their complex morphology and vocabulary. The researchers introduce new models like GreBERTa and PhilBERTa, which are initialized from scratch and trained on high-quality datasets, ensuring direct compatibility with official data splits. These models achieve state-of-the-art results across various tasks such as dependency parsing, part-of-speech tagging, and lemmatization, demonstrating their effectiveness in classical philology.

The project also investigates the use of multilingual models, leveraging pre-training data from multiple languages to enhance performance. This approach is particularly beneficial for languages like Greek and Latin, which share similarities but also have distinct characteristics. The evaluation process includes rigorous testing using official data splits, ensuring that the models perform well under real-world conditions. The study concludes with a comprehensive analysis of the models' strengths and limitations, providing valuable insights into the future development of language models for classical studies.</sample>
    <sample id="153">This research explores the challenges and solutions for resolving ambiguities in text-to-image generative models, focusing on how prompts provided to these models can be ambiguous and lead to varied interpretations. The study introduces the Text-to-Image Ambiguity Benchmark (TAB), a curated dataset designed to evaluate the effectiveness of disambiguation techniques in mitigating such ambiguities. Two proposed frameworks, QA-TIED and VS-TIED, utilize in-context learning to generate clarifying questions or possible visual setups from a language model (LM). These frameworks aim to enhance the faithful generation of images by ensuring that the generated images align with the intended human intentions.

The research demonstrates that while disambiguation improves overall faithfulness in image generation, there is a disparity in its effectiveness across different ambiguity types. Automatic evaluations using BLEU and ROUGE scores, as well as human evaluations, show reasonable agreement, indicating the potential for these methods to be used in practical applications. The study concludes by highlighting the importance of addressing ambiguities in text-to-image models to improve their reliability and utility in various domains.</sample>
    <sample id="154">The affiliations of the authors of the paper are Università di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Mohammad Javad Hosseini</sample>
    <sample id="156">**Slide Title:** Prompting PaLM for Translation: Assessing Strategies and Performance

**Slide Content:**
- **Title:** Prompting PaLM for Translation: Assessing Strategies and Performance
- **Subtitle:** ACL 2023
- **Authors:** David Var Torres, Markus Freitag, Colin Cherry, Jianing Luo, Vivek Rathnakar, George Foster
- **Image:** A beach scene with a palm tree and a speech bubble saying "Can you translate this for me, please?"

**Slide 2:**
- **Title:** PaLM: Pathways Language Model
- **Details:**
  - Chowdery et al., 2022
  - arXiv:2204.02311
  - 540B parameters
  - Trained on 780B tokens
  - Densely activated
  - 6144 TPU v4 chips
  - SOTA in hundreds of LMU and Generation benchmarks
- **Tree Diagram:** 
  - Question Answering
  - Arithmetic Code Completion
  - Summarization
  - Translation
  - Language Understanding
  - Logical Inference Chains
  - Common Sense Reasoning
  - Pattern Recognition
  - Joke Explanations
  - Physics QA
  - 9 billion parameters
  - 38 billion parameters
  - 62 billion parameters
  - 203 billion parameters
  - 540 billion parameters

**Slide 3:**
- **Title:** Our Contribution
- **Details:**
  - First systematic study of LLM prompting for MT.
  - Best for the candidate pool as well as selection strategy.
  - Evaluate translation capabilities with best practices of the MT community:
    - Latest test sets (avoid test/train overlap and overfitting on evaluation data)
    - Comparison to most recent WMT submissions (SOTA systems using most recent training data)
    - SOTA MT metrics (better correlation with human judgments)
    - Expert-based human evaluation (more robust than crowd workers)
  - Recommendation for prompt selection strategies

**Slide 4:**
- **Title:** Prompts have a big impact on translation quality
- **Details:**
  - Select two random prompts for each sentence.
  - Compute BLEURT for each sentence-prompt pair.
  - The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point.
  - The difference can go up to 40 BLEURT points!

**Slide 5:**
- **Title:** Example prompting for translation
- **Details:**
  - 5-shot prompting
  - German: "Dort sieht man, wie sie von zwei Polizei-Offizieren in einen Streifenwagen gesetzt wird."
  - English: "He is being transported under the custody of two policemen on a bus from the jail."

**Slide 6:**
- **Title:** Experimental Results
- **Details:**
  - Example quality is more important than similarity to source sentence.
  - Specialized SOTA systems have a substantial advantage.
  - PaLM close to Google Translate.
  - Insights from MQM:
    - Fluency of PaLM comparable to SOTA.
    - Accuracy scores generally lower.
    - Dominated by Accuracy/Omission.
    - "Style/Awked" generally lower for PaLM.

**Slide 7:**
- **Word Cloud:** Various languages expressing "thank you"

**End of Slide Show**</sample>
    <sample id="157">This research focuses on developing a novel framework for dialogue summarization using static-dynamic structure fusion graphs. The study aims to enhance the ability of dialogue summarization models to capture both the static and dynamic relationships within conversational data, thereby improving the accuracy and relevance of generated summaries. The proposed method integrates a static graph construction module that leverages discourse parsing tools to explicitly represent information flow and interaction between utterances, and a dynamic graph module that captures semantic relationships based on deep vector representations. These modules are combined into a unified graph to provide a comprehensive representation of the dialogue context. The framework is designed to be adaptable and efficient, suitable for various applications requiring effective dialogue summarization. This approach not only addresses the limitations of traditional summarization methods but also enhances the model's capability to handle complex conversational dynamics, making it a significant advancement in natural language processing.</sample>
    <sample id="158">This research introduces a novel approach to coreference resolution, a critical task in natural language processing aimed at identifying and linking mentions within texts that refer to the same entity or concept. The study highlights the inefficiencies of conventional methods, which often lead to quadratic complexity due to the enumeration of all possible pairs of mentions, thereby consuming significant computational resources. To address these limitations, the authors propose a dual cache mechanism, employing a Local Cache (L-cache) and a Global Cache (G-cache). The L-cache utilizes the Least Recently Used (LRU) policy for managing local entities, while the G-cache employs the Least Frequently Used (LFU) policy for global entities. This dual-cache strategy effectively reduces cache misses by separating the management of local and global entities, leading to improved performance and cost-effectiveness compared to single-cache methods. The study demonstrates superior results on public benchmarks, with the dual cache outperforming existing single-cache approaches across various document sizes. Additionally, the research extends its findings to book-level documents, showcasing the method's scalability and effectiveness in handling extensive text corpora. Overall, this work contributes significantly to the field of computational linguistics by providing a robust solution for coreference resolution that is both efficient and scalable.</sample>
    <sample id="160">The first step of the method maps the input tokens to multiset tags.</sample>
    <sample id="161">55,000</sample>
    <sample id="162">**Slide 1: Title Slide**
- **Title:** The KITMUS Test
- **Subtitle:** Evaluating Knowledge Integration from Multiple Sources
- **Logos:** McGill University, Mila, Microsoft Research
- **Authors:** Akshatha Arodi*, Martin Pömsi*, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, Jackie CK Cheung

**Slide 2: Introduction to NLU Models and Knowledge Integration**
- **Title:** NLU models draw on multiple knowledge sources
- **Content:**
  - **Knowledge in Parameters (pretrain-time knowledge):** Represented by a neural network diagram.
  - **Knowledge in Context (inference-time knowledge):** Represented by a text snippet.

**Slide 3: Example of Knowledge Integration**
- **Title:** John saw the newly elected president on TV
- **Content:**
  - **Pretrain-time knowledge:** "What presidents do" (✓), "What is a TV" (✓)
  - **Inference-time knowledge:** "Who is John" (✗), "Who is the new president" (✗)

**Slide 4: Knowledge Integration with Additional Information**
- **Title:** John saw the newly elected president on TV
- **Content:**
  - **Pretrain-time knowledge:** "What presidents do" (✓), "What is a TV" (✓)
  - **Inference-time knowledge:** "Who is John" (✓), "Who is the new president" (✓)

**Slide 5: KITMUS Test Suite Overview**
- **Title:** KITMUS Test Suite
- **Content:**
  - Dataset for knowledge integration evaluation
  - Coreference resolution task to probe ability to draw on pretrain-time knowledge and inference-time knowledge
  - Experiment with human study participants and coreference resolution models

**Slide 6: Example Sentence with Coreference Resolution**
- **Title:** KITMUS Test Suite
- **Content:**
  - **Sentence:** "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"
  - **Coreference:** "he" refers to "Servin"

**Slide 7: Entity-Specific Knowledge**
- **Title:** KITMUS Test Suite
- **Content:**
  - **Sentence:** "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"
  - **Coreference:** "he" refers to "Servin"
  - **Entity-specific knowledge:** "Judges decide cases in courts of law."

**Slide 8: Variants of KITMUS**
- **Title:** Variants of KITMUS
- **Content:**
  - **Background-Pretrain:** Typical setup
  - **Background-Both:** Explicitly provide background knowledge in context
  - **Background-Inference:** Knowledge only available at inference-time

**Slide 9: Comparison of Variants**
- **Title:** Variants of KITMUS
- **Content:**
  - **Background-Pretrain:** Politicians seek elected seats in government.
  - **Background-Both:** Politicians seek elected seats in government.
  - **Background-Inference:** Chichester is a politician.
  - **Inference-time knowledge:** "The work of a politician is seeking an elected seat in government."

**Slide 10: Task-Specific Training**
- **Title:** Background-Pretrain
- **Content:**
  - **Bar Chart:** Mean accuracy comparison between "Without task-specific training" and "With task-specific training"
  - **Legend:** Random Choice, Human Participants, BERT4Coref, C2P

**Slide 11: Conclusion**
- **Title:** Conclusion
- **Main Takeaways:**
  1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge).
  2. Task-specific training is necessary for knowledge integration.
  3. Models struggle to integrate inference-time background knowledge.
- **GitHub Link:** Find the dataset, generation &amp; evaluation code at [mpoems/kitmus]</sample>
    <sample id="163">The best alignment method for DEplain is not explicitly stated in the provided text.</sample>
    <sample id="164">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="165">This research paper explores the concept of abductive commonsense reasoning, focusing on exploiting mutually exclusive explanations to enhance model performance in natural language inference tasks. The study introduces LiPoR (Likelihood learning with Posterior Regularization), a novel approach that addresses the challenges of unsupervised abductive reasoning by treating explanations as latent variables and maximizing the log likelihood of outcomes given contexts while encouraging the probability mass of plausible explanations to collapse into a subset. This method is demonstrated through experiments on the SNLI dataset, where LiPoR achieves state-of-the-art results, outperforming previous methods without annotations and achieving comparable performance with RoBERTa when annotations are available. The paper highlights the importance of mutually exclusive explanations in reducing ambiguity and improving model accuracy, particularly in scenarios where multiple plausible explanations exist for a given context and outcome.</sample>
    <sample id="166">This research introduces a novel neural divide-and-conquer reasoning framework designed to enhance image retrieval from linguistically complex text, a critical task in artificial intelligence and natural language processing. The framework leverages two distinct systems: System 1, which excels at analogical reasoning, and System 2, adept at logical reasoning. By dividing the complex reasoning process into manageable sub-tasks, the framework aims to improve the overall performance of image retrieval tasks. System 1, based on visual-language models (VLMs), excels in understanding and generating simple propositions, while System 2, utilizing neural-symbolic reasoning, handles more intricate logical operations. Together, these systems integrate to provide a comprehensive solution that combines the strengths of both analogical and logical reasoning approaches. The study demonstrates significant improvements in model performance across various datasets, showcasing the effectiveness of this divide-and-conquer strategy in tackling complex image retrieval challenges.</sample>
    <sample id="167">The documents in DEplain-web were allocated using 1:1 alignment, which means that each document from the training set was paired with one document from the test set for alignment.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This research explores the effectiveness of PaLM (Pathways Language Model) for translation tasks, focusing on how prompts influence translation quality. The study, conducted by researchers from Google, evaluates PaLM's performance against specialized SOTA systems and compares it to Google Translate. Key findings highlight that while PaLM's fluency is comparable to SOTA models, its accuracy scores are generally lower, primarily due to issues with accuracy/omission. The research also suggests that example quality is more critical than similarity to the source sentence, and that specialized SOTA systems offer a substantial advantage over PaLM. The study employs BLEURT for evaluating translation quality and recommends strategies for prompt selection to enhance translation outcomes. Overall, the findings underscore the importance of tailored prompts and specialized systems in achieving high-quality translations.</sample>
    <sample id="171">Parameter-based watermark, Lexical watermark, Backdoor-based watermark, Adversarial-based watermark</sample>
    <sample id="172">No, multilingual LLMs such as Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="173">**Slide 1: Title Slide**
- **Title:** Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
- **Authors:** Shuheng Liu, Alan Ritter
- **Affiliation:** School of Interactive Computing, Georgia Institute of Technology

**Slide 2: Introduction to Named Entity Recognition &amp; Generalization**
- **Topic:** Named Entity Recognition &amp; Generalization
- **Content:** Models have been using CoNLL-2003 to develop NER for almost 20 years.
- **Questions:** Can these models generalize to modern data? What is needed for good generalization?

**Slide 3: CoNLL++ Dataset**
- **Topic:** CoNLL++ Dataset
- **Content:** Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines.
- **Evaluation:** Evaluated on CoNLL-2003 test set &amp; CoNLL++
- **Performance:** Calculated percentage ΔF1 to assess generalization.

**Slide 4: What Is Needed for Good Generalization?**
- **Topic:** What Is Needed for Good Generalization?
- **Content:** Model architecture (Transformer models generalize better), Model size (Larger models generalize better), Number of fine-tuning examples (More examples leads to better generalization).

**Slide 5: What Causes Performance Drop?**
- **Topic:** What Causes Performance Drop?
- **Content:** Adaptive overfitting, Temporal drift.

**Slide 6: Performance Drop Analysis**
- **Topic:** Performance Drop Analysis
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 7: Conclusion**
- **Topic:** Conclusion
- **Content:** For a good generalization, we need:
  - Better model architecture
  - Larger model size
  - More fine-tuning examples
- **Performance Drop Causes:** Temporal drift, Not adaptive overfitting
- **Do CoNLL-2003 taggers still work?** YES!

**Slide 8: Contact Information**
- **Paper:** https://arxiv.org/abs/2212.09747
- **Dataset:** https://github.com/ShuhengL/acl2023_conllpp
- **Contact:** silu775@gatech.edu</sample>
    <sample id="174">The video presentation focuses on the "ArgAnalysis35K" dataset, designed for argument quality analysis. It begins by defining Argument Quality Analysis as a method to evaluate arguments on a scale from 0-1, emphasizing the need for high-quality and diverse arguments sourced directly from winning debates and expert debaters. The dataset is highlighted for its comprehensive nature, containing over 35,000 argument-analysis pairs with diverse themes such as politics, environment, and authoritarian regimes. The presentation discusses the challenges faced by current datasets, including lack of quality arguments, diversity, and depth in explaining nuances, and introduces ArgAnalysis35K as a solution. It outlines the dataset's features: a large-scale collection of high-quality arguments, diverse motions, logical chains of reasoning, and an instance-based annotation scoring function. The relevance model assigns scores from 0-1 for each argument-analysis pair, enhancing the dataset's utility in various contexts like free speech debates and LGBTQ rights. The video also addresses annotator reliability issues, suggesting the use of machine learning models like Expectation Maximisation training and FNN classifiers to predict true values of annotations, despite human biases. Overall, ArgAnalysis35K aims to provide a robust tool for improving argument quality analysis across different domains.</sample>
    <sample id="175">The method induces permutation in training.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined as the absence of political biases in its predictions or outputs.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">This research explores the development of SymbolicToM, a method designed to enhance the Theory of Mind (ToM) reasoning skills of Large Language Models (LLMs). The study highlights the traditional challenges LLMs face in understanding false-belief scenarios, where their performance is often poor due to their inability to reason about the mental states of others accurately. To address this, SymbolicToM introduces an inference-time algorithm that utilizes explicit graphical symbolic representations to track local context and improve reasoning depth. This approach is demonstrated through experiments on various LLMs, including Macaw-3B, GPT3-Curie, Flan-T5-XL, LLaMA-7B, and GPT4, showing significant improvements in second-order false-belief questions. The method's effectiveness is further validated by its out-of-domain performance on the ParaphrasedToM dataset, which tests the models' ability to generalize beyond their training data. SymbolicToM not only outperforms supervised baselines but also demonstrates robustness across different linguistic diversity datasets. The conclusion underscores SymbolicToM as a plug-and-play solution for enhancing LLMs' ToM reasoning capabilities, offering more interpretable reasoning and superior performance compared to existing methods.</sample>
    <sample id="180">Myra Cheng</sample>
    <sample id="181">The research presented at the 61st Annual Meeting of the Association for Computational Linguistics focuses on enhancing constrained language planning capabilities in large language models (LLMs) through a novel method called "Script Distillation from LLMs." The study aims to address the limitations of current LLMs in generating scripts that adhere to specific constraints, such as those found in real-life scenarios like making a cake with additional requirements like using a microwave or for a wedding.

The researchers introduce a method that leverages the idea of symbolic knowledge distillation to generate scripts with constraints from LLMs. They use the Coscript dataset, which is derived from the wikiHow dataset but includes one extra constraint, to train and validate their approach. The method involves three main steps: first, generating specific goals with InstructGPT via in-context learning; second, over-generating candidate scripts with InstructGPT; and third, filtering these scripts based on their similarity score to the goal.

The study evaluates the performance of various LLMs, including GPT-3, Codex, InstructGPT, T5 trained on wikiHow, and T5 trained on Coscript, demonstrating that smaller LMs fine-tuned on Coscript can generate higher quality scripts than larger LMs. The results show that the proposed method significantly improves the planning quality of LLMs, achieving an accuracy improvement by a large margin compared to baseline methods.

The research also highlights the importance of the Coscript dataset as a valuable resource for advancing the field of language planning, particularly for more complex and diverse goals and constraints. The study concludes with a discussion of future work, emphasizing the need for further exploration into the effectiveness of specialized models versus general-purpose LLMs in constrained language planning tasks.</sample>
    <sample id="182">Tropicalism indicates a specific cultural identity or stereotype associated with certain groups, as highlighted by the use of words like "vibrant," "curvaceous," and "petite" to describe Latina women.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by using prompts like "Imagine you are an Asian woman. Describe yourself." and inspired by psych studies with human subjects using the same prompts.</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="185">DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks, while ChuBERT achieves state-of-the-art results in 5 downstream French medical-oriented tasks.</sample>
    <sample id="186">**Slide Title:** Marked Personas

**Slide Content:**
- **Title:** Marked Personas
- **Subtitle:** Using Natural Language Prompts to Measure Stereotypes in Language Models
- **Authors:** Myra Cheng, Esin Durmus, Dan Jurafsky
- **Conference:** ACL 2023
- **Institution:** Stanford University, Computer Science

**Slide Content:**
- **Motivation:**
  - Social bias and stereotypes are prevalent in LLMs.
  - Limitations of existing stereotype measures:
    - Tradeoff between specificity and generalizability.
    - Based on fixed, hand-curated datasets.
    - Don't account for intersectionality.

**Slide Content:**
- **How do we overcome these limitations?**
  - GPT-3.5, GPT-4, etc., can respond to instructions in prompts.
  - Input: "Imagine you are an Asian woman. Describe yourself."
  - Output: Persona Examples (GPT-4)
    - **Asian woman:** The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully throughout the world, drawing immediate attention.
    - **Middle-Eastern woman:** She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.
    - **White man:** As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.

**Slide Content:**
- **Step 1: Persona Examples (GPT-4)**
  - **Asian woman:** The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully throughout the world, drawing immediate attention.
  - **Middle-Eastern woman:** She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.
  - **White man:** As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.

**Slide Content:**
- **Step 2: Marked Words**
  - Define unmarked and marked groups.
  - Use weighted log-odds ratios to distinguish top words for each marked group.
  - E.g., For Black woman personas, find words that distinguish from both unmarked groups:
    - White personas
    - Man personas

**Slide Content:**
- **Insight for Step 2: Marked Words**
  - Markedness:
    - Unmarked groups are default, ordinary.
    - Marked groups differ from the default.
      - A warrior (unmarked) vs. a woman warrior (marked).

**Slide Content:**
- **Dominant groups are linguistically and socially unmarked.**
- **Marginalized groups are marked.**

**Slide Content:**
- **Recommendations:**
  - Addressing positive stereotypes and essentializing narratives.
  - An intersectional lens.
  - Transparency about bias mitigation.</sample>
    <sample id="187">3</sample>
    <sample id="188">Iterative transfer learning is a method where a model is trained on an initial dataset, then fine-tuned on new data, and this process is repeated iteratively to improve performance.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they make a choice.</sample>
    <sample id="190">By learning from the embeddings and providing similar services.</sample>
    <sample id="191">Three</sample>
    <sample id="192">The research presented in this paper introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a novel memory-efficient optimizer designed to address the challenges of training large language models (LLMs) using adaptive gradient-based optimization methods. The primary issue highlighted is the high memory usage required by widely-used optimizers like Adam and LAMB, which can triple the memory needed for keeping first- and second-order moments. Existing memory-efficient optimizers such as AdaFactor aim to reduce auxiliary memory usage but often come with performance penalties.

CAME proposes a confidence-guided strategy that leverages the residual between predicted updates and generated updates to mitigate erroneous updates, thereby improving convergence speed and efficiency. This approach is inspired by the erroneous update phenomenon observed in existing memory-efficient optimizers. The method is demonstrated to achieve outstanding performance on large LLM training tasks, outperforming traditional adaptive methods while maintaining low memory usage. Extensive experiments confirm its effectiveness across various datasets and models, including BERT, GPT-2, and T5, showcasing its potential as an important extension to existing memory-efficient optimizers.</sample>
    <sample id="193">3</sample>
    <sample id="194">University of Washington, Carnegie Mellon University, Allen Institute for AI</sample>
    <sample id="195">This research paper introduces a novel framework for Explainable Question Answering (XQA) called RoHT, which addresses the limitations of existing methods by integrating knowledge from heterogeneous sources to handle complex questions. The motivation behind this work is the need for more flexible and diverse knowledge sources to improve the performance of XQA systems, especially when dealing with complex queries that require reasoning over multiple pieces of information.

RoHT employs a hierarchical question decomposition tree (HQDT) to break down complex questions into simpler sub-questions, facilitating the integration of knowledge from various sources such as KBs, text corpora, and external text. The framework includes two main components: a scheduler and an executor. The scheduler determines appropriate knowledge sources for each sub-question, while the executor retrieves answers with probabilities from these sources. These answers are then aggregated to output the best response.

The paper outlines the challenges faced in question decomposition, including determining the granularity of decomposition and finding the optimal solution among various possibilities. It also describes the RoHT framework's probabilistic reasoning process over HQDT, where it outputs answers with probabilities for each question in the tree.

Experimental results on datasets like KQA Pro and Musique demonstrate the effectiveness of RoHT, showing improvements in EM and F1 scores compared to baseline models. This study contributes significantly to the field of XQA by providing a robust method for handling complex questions through hierarchical decomposition and knowledge integration.</sample>
    <sample id="196">Homer loves Lisa, Bart and Maggie.</sample>
    <sample id="197">ABC-Eval, Turn Likert, Dialogue Likert, and Comparative.</sample>
    <sample id="198">To understand how the models' judgments vary as a function of context length, structural match, and acceptability.</sample>
    <sample id="199">No, training in multilingual fashion did not cause performance drop compared to monolingual English model.</sample>
    <sample id="200">No, they do not.</sample>
    <sample id="201">SOTA MT metrics</sample>
    <sample id="202">Yes, the regress in generalization impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it influences the research process and its outcomes, reflecting the perspectives of people based on their demographics, identity, and life experiences. This can lead to biases in datasets and models, which may not accurately represent all populations.</sample>
    <sample id="204">Full fine-tuning</sample>
    <sample id="205">This research explores the impact of political biases in pretraining data on language models (LMs) and their performance in downstream tasks, particularly focusing on hate speech detection and misinformation detection. The study highlights that LMs trained on diverse datasets exhibit varying political leanings, which can influence their performance across different categories of hate speech and misinformation. By analyzing the political leaning of LMs, the researchers aim to understand how these biases manifest in downstream applications and suggest strategies for mitigating them.

The research begins by examining the composition of LM training data, revealing a significant bias towards certain websites and sources, such as Google and Wikipedia. This bias is then linked to the political leanings of the LMs, which are evaluated using both automatic and manual methods. The study finds that LMs trained on left-leaning news media tend to lean more liberal, while those trained on right-leaning media lean more conservative. These political leanings are further quantified through a "Trump Card" visualization, showing shifts in political leaning before and after the election of Donald Trump.

In terms of downstream tasks, the study evaluates the performance of LMs on hate speech detection and misinformation detection, targeting various identity groups and misinformation sources. Results indicate that LMs trained on specific datasets perform differently across these categories, with some models outperforming others in certain areas but struggling in others. The qualitative analysis provides insights into the types of hate speech and misinformation that LMs struggle with, suggesting potential improvements in model design or training strategies.

Overall, this research underscores the importance of understanding and addressing political biases in LMs to ensure fair and unbiased performance in critical applications like hate speech detection and misinformation detection. The findings highlight the need for more diverse and balanced training data to mitigate these biases and improve the reliability of LMs in real-world scenarios.</sample>
    <sample id="206">RoBERTA-base + classifier head</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are WMT 2021 and WMT 2022.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 36%.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Yes, the results and dataset can be used as a benchmark.</sample>
    <sample id="212">5</sample>
    <sample id="213">OFA</sample>
    <sample id="214">**Slide 1: Title Slide**
- **Title:** Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark
- **Authors:** Wenjun Peng, Jingwei Yi*, Fangzhou Wu*, Shangxi Wu*, Bin Zhu*, Lingjuan Lyu*, Binxing Jiao*, Tong Xu*, Guanghong Sun*, Xing Xie*
- **Affiliations:** University of Science and Technology of China, Microsoft Research Asia, Beijing Institute of Technology, Sony AI, Microsoft STC Asia

**Slide 2: Background**
- **Large language models (LLMs) are exceptional in NLU and NLG**
  - GPT [1], LLaMA [2], PALM [3]
- **Embedding as a Service (EaaS) is offered to assist various NLP tasks**
- **OpenAI offers a GPT-based embedding API**

**Slide 3: Background**
- **Ada model, text-embedding-ad002, is a better and lower cost replacement for our older embedding models.**
- **References:**
  - [1] Brown et al., Language models are few-shot learners, NeurIPS 2020.
  - [2] Touvron et al., LLaMA: Open and Efficient Foundation Language Models, arXiv 2023.
  - [3] Choueiri et al., Scaling Language Modeling with Pathways, arXiv 2022.

**Slide 4: Motivation**
- **Attackers may steal the model through learning from the embeddings and provide similar services**
  - StolenEncoder [1]
- **Need to protect the copyright of EaaS**
- **Detect whether a provider's service is stolen by another service**

**Slide 5: Motivation**
- **References:**
  - [1] Liu et al., Stolenencoder: Stealing pre-trained encoders in self-supervised learning, CCS 2022

**Slide 6: Challenge**
- **Applicable to EaaS**
- **Utility**
  - Should not degrade the utility of the provided embeddings.
- **Covertness**
  - Should be covert to the attacker.
- **Transferability**
  - The watermark needs to be transferable to the attackers' services.

**Slide 7: Challenge**
- **Applicable to EaaS**
- **Utility**
  - Should not degrade the utility of the provided embeddings.
- **Covertness**
  - Should be covert to the attacker.
- **Transferability**
  - The watermark needs to be transferable to the attackers' services.

**Slide 8: Existing Works**
- **Parameter-based watermark [1, 2]**
  - Transferability: ❌
- **Lexical watermark [3, 4]**
  - Applicable to EaaS: ❌
- **Backdoor-based watermark [5]**
  - Applicable to EaaS: ❌
- **Adversarial-based watermark [6]**
  - Applicable to EaaS: ❌

**Slide 9: EmbMarker**
- **Trigger Selection**
  - Count the word frequency on a general text corpus \( D_p \)
  - Randomly select \( n \) words in a moderate-frequency interval

**Slide 10: EmbMarker**
- **Watermark injection**
  - Define a target embedding \( \mathbf{e}_t \)
  - Count the trigger number in a sentence \( Q(S) = \frac{\min(|S \cap T|, m)}{m} \)
  - Add the target embedding on the original embedding \( \mathbf{e}_o \)

**Slide 11: EmbMarker**
- **Copyright verification**
  - Construct a backdoor and benign dataset
    - \( D_b = \{[w_1, w_2, ..., w_m] | w_i \in T\} \)
    - \( D_n = \{[w_1, w_2, ..., w_m] | w_i \notin T\} \)
  - Request embeddings from stealer’s service with the datasets

**Slide 12: Experimental Results**
- **Copy Dataset: AG News, MIND, SST2, Enron Spam**
- **Provider’s general Dataset: WikiText**
- **Metrics**
  - Performance on downstream tasks: ACC
  - Detection performance: \( \Delta_{cos}, \Delta_{L2}, P\)-value
- **Setting**
  - \( m = 20 \), \( n = 4 \), frequency interval = [0.005, 0.01]

**Slide 13: Experimental Results**
- **Performance comparison**

**Slide 14: Experimental Results**
- **Embedding visualization**

**Slide 15: Thanks!**
- **End of presentation**</sample>
    <sample id="215">This research explores the dependency structures and lengths in English, focusing on coordination phenomena. It examines how word order influences dependency length minimization (DLM), a principle that suggests shorter dependencies are preferred for better readability and processing efficiency. The study uses data from the Penn Treebank to analyze left conjuncts' tendency to be shorter than right conjuncts, especially when the governor is on the left side of the sentence. The findings reveal that this tendency increases with the absolute difference in conjunct lengths, but only when the governor is on the left or absent. This research contributes to understanding natural language processing by providing insights into syntactic structure and its impact on linguistic efficiency.</sample>
    <sample id="216">**Slide Title:** Attention as a Guide for Simultaneous Speech Translation

**Slide Content:**
- **Title:** Attention as a Guide for Simultaneous Speech Translation
- **Authors:** Sara Papi, Matteo Negri, Marco Turchi
- **Institutions:** Università di Trento, Fondazione Bruno Kessler

**Slide 2: What is Simultaneous Speech Translation?**
- **Definition:** Simultaneous speech translation (SimuST) is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication.
- **Example:** "Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt er kalt, und wenn ich im Winter heißen Tee in meine Thermoskanne gieße, wird er warm." → "When I have cold tea in my thermos in the summer, it stays cold, and when I pour hot tea in my thermos in the winter, it gets warm."

**Slide 3: What are the problems of the current SimuST models?**
- **Specific architectures are usually trained, introducing additional modules to be optimized.**
- **Long and complicated training procedures (e.g., different optimization objectives).**
- **Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, 25s...)**

**Slide 4: What is our solution?**
- **Use already existing offline ST models without re-training or adopting specific architecture for SimuST.**
- **Use only one model for every latency regime and handle latency through specific parameters.**
- **Leverage the knowledge already acquired by the model through the attention statistics between audio input and textual output.**

**Slide 5: Our Solution: EDAtt**
- **Encoder-Decoder Attention**
- **Decision to emit or not a partial translation based on where attention points:**
  - A word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last A token, indicating that the information is enough stable.

**Slide 6: Example of Encoder-Decoder Attention**
- **Audio Input:** "I am a student."
- **Attention Matrix Visualization:**
  - Green arrows indicate attention from the encoder to the decoder.
  - Red arrows indicate attention from the decoder back to the encoder.
  - The attention matrix shows how the decoder focuses on different parts of the input sequence at each decoding step.

**Slide 7: Main Results: EDAtt**
- **BLU (Bilingual Language Understanding Evaluation) scores for different latency regimes (AL/AL_CA):**
  - EDAtt outperforms all strategies applied to offline models.
  - EDAtt is the fastest strategy if we consider the actual elapsed time.

**Slide 8: Conclusion**
- **EDAtt outperforms all strategies applied to offline models.**
- **EDAtt is the fastest strategy if we consider the actual elapsed time.**
- **Read our paper to discover more results!**
- **Contact Information:**
  - Email: (spapi.negri)@fbk.eu, marco.turchi@gmail.com
  - GitHub: github.com/tlt-mt/fbk-fairseq
  - Twitter: @bk_mt, @sarapapi</sample>
    <sample id="217">This research explores compositional generalization for multi-attribute controllable dialogue generation, addressing the limitations of previous methods that focused on single attributes and lacked practicality for continuous attributes. The study introduces DCG (Disentangled Controllable Generation), a model that learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. A unified reference-free evaluation framework, MAE (Multi-Attribute Evaluation), is also developed to assess the effectiveness of the method across various granularities of attributes. The proposed model demonstrates superior performance in terms of text quality and controllability scores compared to existing methods, achieving higher correlations with human judgments for evaluation on CDG (Compositional Dialogue Generation). This work contributes to the field by providing a comprehensive solution for generating dialogue with multiple attributes while maintaining control over each attribute's influence.</sample>
    <sample id="218">The affiliations of the authors of the paper are Google Research and DeepMind.</sample>
    <sample id="219">This research introduces a compare-and-contrast multistage pipeline designed to uncover financial signals within financial reports, a critical task for financial practitioners. The study highlights the importance of financial reports as mandated by the SEC and their comprehensive nature, which makes them invaluable but also labor-intensive to analyze manually. The pipeline addresses this challenge by leveraging a highlighting task and a multistage approach that includes document segmentation, relation recognition, and domain-adaptive learning. This method effectively identifies significant changes between consecutive financial reports, enhancing the efficiency of financial analysis. The pipeline's effectiveness is demonstrated through empirical data and evaluation metrics, showing superior performance compared to baseline models across various settings. Future work includes exploring more efficient features like bi-directional rationalization tasks, applying the model to other languages, and expanding its application to diverse financial analysis modalities such as analyzing charts and tables.</sample>
    <sample id="220">Stony Brook University, Human Language Analysis Settings</sample>
    <sample id="221">English-German and English-French</sample>
    <sample id="222">The research presented in this presentation focuses on enhancing open-domain question answering systems, particularly addressing challenges and interventions in adapting these systems to new domains. The study explores how to effectively generalize knowledge from one domain to another, especially when dealing with sparse data availability in specific domains like biomedical information.

The core methodology involves expanding retrieval corpora by combining general knowledge sources (like Wikipedia) with domain-specific resources (such as PubMed). This approach aims to improve the performance of question-answering models across different domains. The researchers investigate various data interventions, including few-shot and zero-shot methods, to enhance model compatibility and performance.

Key findings highlight that few-shot interventions can significantly boost reader and retriever performance, with improvements ranging up to 24% and 22%, respectively. The effectiveness of these interventions is shown to be dependent on the type of dataset shift, suggesting that uniform distribution of answer types works well under certain conditions.

The study also introduces a generalizability test framework to evaluate the compatibility between retrievers and readers for different types of dataset shifts, including concept, covariate, and full shifts. This framework helps in understanding how well a model can adapt to new domains without losing its performance.

Overall, the research contributes valuable insights into improving the adaptability and generalization capabilities of open-domain question answering systems, offering practical solutions for handling diverse and complex datasets.</sample>
    <sample id="223">Shangbin Feng</sample>
    <sample id="224">The models investigated during the experiments were UDA, Sent-Label, and VecAlign.</sample>
    <sample id="225">57 tasks are used for training and 5 tasks are used for testing.</sample>
    <sample id="226">Three</sample>
    <sample id="227">The research presented in this study focuses on grounded language understanding, aiming to address the limitations of current language models that are predominantly trained on textual corpora and lack the ability to understand and generate plans or programs. The study introduces Pangu, a unified framework designed to enhance the capabilities of large language models (LMs) by allowing them to focus on discrimination tasks while maintaining generality. This approach is demonstrated through various applications such as text-to-SQL parsing, knowledge base question answering (KBQA), and embodied AI, where the framework significantly outperforms traditional methods.

Key findings include:
1. **Performance Improvement**: Pangu achieves state-of-the-art results across different benchmarks, particularly excelling in non-i.i.d. generalization.
2. **Sample Efficiency**: The framework demonstrates improved sample efficiency, requiring fewer training examples for better performance.
3. **Generalizability**: Pangu improves generalizability by reducing overfitting to seen structures during training, leading to more robust and versatile models.

The study concludes with a key message: directly generating plans or programs may not be the optimal way to utilize LMs for grounded language understanding. Instead, focusing on discrimination tasks within a unified framework can lead to more effective and adaptable language models.</sample>
    <sample id="228">The authors experimented on the AG News, MIND, SST2, and Enron Spam datasets.</sample>
    <sample id="229">The paper titled "To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support" by Gabriella Skitalinskaya and Henning Wachsmuth, presented at ACL 2023, explores the critical role of text revision in argumentative writing. It highlights that revisions are essential for achieving optimal phrasing, which significantly influences the persuasive impact on the audience. The study identifies two main tasks: suboptimal-claim detection and claim improvement suggestion. Suboptimal-claim detection involves determining whether a given claim needs revisions or can be considered phrased optimally. Claim improvement suggestion focuses on selecting the types of quality issues that should be addressed when revising the claim.

The research leverages implicit revision patterns from collaborative editing behaviors observed in online debates platforms like Kialo to model the quality of argumentative texts. This approach aims to detect improvable claims and suggest improvements based on these patterns. The study also discusses challenges such as representativity and reliability, model complexity and architecture, and topical and user bias, emphasizing the importance of contextual information in improving the quality of argumentative texts. The authors conclude with a summary of findings, including the effectiveness of revision-based data in enhancing the quality of texts and the necessity of considering contextual factors like topic expertise and domain knowledge.</sample>
    <sample id="230">**Slide Title:** Language model acceptability judgements are not always robust to context

**Content:**
- **Title:** Language model acceptability judgements are not always robust to context
- **Conference:** ACL 2023
- **Authors:** Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, Adina Williams
- **Image:** A circular profile picture of a person wearing glasses and a maroon shirt.
- **Logos:** Johns Hopkins University, Purdue University, MIT, Meta AI.

---

**Slide Title:** Revisiting Minimal Pair Paradigm

**Content:**
- **Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.**
- **Three datasets:**
  - **BLIMP:** 
    - Sentence 1: "Many people were helping themselves."
    - Sentence 2: "Many people were helping herself."
    - Judgement: P(1) &gt; P(2)
  - **SyntaxGym:**
    - Sentence 1: "The customer ... has spent any money."
    - Sentence 2: "The customer ... has spent any money."
    - Judgement: P(1.any) ≻ P(2.any)
  - **CrowS:**
    - Sentence 1: "Women are terrible at handwork."
    - Sentence 2: "Men are terrible at handwork."
    - Judgement: P(1) ≻ P(2)
- **Question:** Are these judgements stable with long preceding context?

---

**Slide Title:** Approach

**Content:**
- **Test whether MPP judgements vary as a function of context length, structural match, and acceptability.**
- **Test Bulle: Subject Verb Agreement**
  - **Prefix Strategy:** 
    - Prefix: &lt;sent&gt;.
    - Test: "Why might Rose flee from before returning to this customer?"
  - **Candidate Profiles:**
    - Body: [Subject Verb Agreement]
    - Head: [Subject Verb Agreement]
    - Context: [Subject Verb Agreement]
  - **Acceptable/Unacceptable:**
    - "Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might Rose flee from before returning to this customer?"
    - "What could Jessica see before noticing those signs? Why might</sample>
    <sample id="231">A 1,18 B words open-source dataset of heterogeneous data crawled from diverse medical domains, nature and style</sample>
    <sample id="232">David Vil Torro</sample>
    <sample id="233">The presentation titled "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi explores the challenges and solutions in the field of simultaneous speech translation (SimuST). The study highlights that current SimuST models often require specific architectures and long training procedures, leading to inefficiencies and complexity. To address these issues, the researchers propose a novel solution called EDAtt, which leverages existing offline speech-to-text (ST) models without retraining or adapting them specifically for SimuST. This approach simplifies the process by using a single model for different latency regimes and optimizing it through specific parameters.

EDAtt utilizes encoder-decoder attention mechanisms to decide whether to emit or emit only a partial translation based on where attention points. The model evaluates the stability of information by summing the attention weights towards the last A steps and decides to emit a word if this sum is below a threshold α. This method ensures that only stable information is translated, enhancing the quality and reliability of the output.

The research demonstrates that EDAtt outperforms other strategies applied to offline models, achieving higher BLEU scores across various latency regimes. The study also shows that EDAtt is the fastest strategy when considering actual elapsed time, making it a promising solution for real-time applications. The authors encourage further exploration into the application of offline models for SimuST, suggesting that this approach could significantly reduce the complexity and time required for developing efficient SimuST systems.</sample>
    <sample id="234">The prompting strategy has a big impact on the results, with differences of more than 1 BLEURT point and up to 40 BLEURT points.</sample>
    <sample id="235">The affiliations of the authors are Carnegie Mellon University, Language Technologies Institute, TECNICO LISBOA, BAIR, and Unbabel.</sample>
    <sample id="236">The 5 expert-written instructions are: Visual Relationship, VQA, Temporal Ordering, Grounded Reasoning, and Image Text Matching.</sample>
    <sample id="237">The authors propose using a coreference resolution task to test the models on using information from multiple sources.</sample>
    <sample id="238">The research presented at ACL 2023 introduces MeetingBank, a benchmark dataset designed for meeting summarization tasks. This dataset is created by segmenting city council meetings and pairing them with expert-written summaries, addressing the scarcity of high-quality meeting summaries and the difficulty in identifying reliable sources for public meetings. The dataset includes transcriptions, summaries, and URLs, offering a structured approach to evaluating summarization models.

MeetingBank comprises 1,566 meetings from six cities: Denver, Seattle, Long Beach, Alameda, New York City, and Boston, totaling over 4,800 summarization instances. Each meeting is segmented into segments, with detailed information such as start and end times provided. The dataset is available on GitHub, facilitating its use by researchers and practitioners in the field of natural language processing.

The study evaluates various models, including extractive methods like Extract-Oracle, Lead-3, LexRank, and TexRank, as well as abstractive models such as BART_Large, Pegasus, Longformer, DialogLM, and HMNet. Additionally, large language models like GPT-3 (Davinci-003) are tested, though they do not perform well according to automatic metrics. Human evaluation further assesses the summaries' informativeness, factuality, fluency, coherence, redundancy, and overall quality, providing insights into the effectiveness of different summarization techniques.

MeetingBank serves as a valuable resource for researchers aiming to design advanced meeting summarizers, offering a comprehensive dataset that could provide intriguing insights into the decision-making processes of city councils.</sample>
    <sample id="241">This research paper focuses on developing and evaluating a human-in-the-loop (HiTL) framework for early misinformation detection, particularly targeting COVID-19 treatments. The study highlights the limitations of current approaches to misinformation detection, which are often unrealistically evaluated and not built with real-world scale or noise. The HiTL framework is designed to integrate human feedback into the misinformation detection process, making crucial judgments at various stages of workflow. It involves an end-to-end system that processes tweets from Twitter to actionable outputs, ensuring that the system can detect misleading claims and policy violations efficiently.

The evaluation of this system is conducted on COVID-19 treatment misinformation on Twitter, where it identifies and ranks claims based on their trendiness for human validation. The efficacy of the approach is measured by its ability to detect misleading claims early, defined as the relative time of detection to the first appearance of the claim in a debunking news article. The results show that 65% of system-identified tweets are most likely or clearly violating Twitter’s policies, with 124.2 tweets containing policy violations detected per human hour worked. This study provides a concrete standard of comparison for future systems and presents an outside look at human-in-the-loop misinformation systems, motivating the development of more useful frameworks for misinformation detection.</sample>
    <sample id="242">Likert rating evaluation and comparative evaluation.</sample>
    <sample id="243">5</sample>
    <sample id="244">Judges decide cases in courts of law.</sample>
    <sample id="245">This research explores the effectiveness of MTurk workers for high-agreement summarization tasks, focusing on the challenges posed by automatic metrics and best practices for recruitment. The study introduces a two-step pipeline: a qualification task to filter out low-quality workers and an endurance task to assess their performance under heavy workload conditions. The qualification task involves a pre-defined set of settings, including document and summary lengths, and a designed motivation to ensure workers understand the task. The endurance task evaluates workers' ability to handle high volumes of data efficiently. Results show that 38 workers qualified as 'GOLD', 18 as 'SILVER', and 159 were blocked due to poor performance or lack of attention. The reference-based task further validates these findings, demonstrating that the pipeline can identify high-quality workers with a Spearman's correlation of 0.746 between the pipeline and CloudResearch workers. The study concludes that this pipeline is effective for large-scale summarization at lower cost, with potential applications across various domains. However, it highlights limitations such as the need for English proficiency and the absence of guarantees for the training of correctness.</sample>
    <sample id="246">Yes, the code is available on GitHub at mpoens/kitmus.</sample>
    <sample id="247">This research introduces FactKG, a dataset designed to facilitate the use of knowledge graphs (KGs) for fact verification tasks. The dataset comprises 108k natural language claims, each paired with a KG query and labeled as either supported or refuted. It includes five reasoning types: one-hop, conjunction, existence, multi-hop, and negation, catering to diverse linguistic patterns such as colloquial style claims and written style claims. The inclusion of various linguistic patterns enhances practicality, making it suitable for real-world applications.

The dataset is structured to support KG-based fact verification, leveraging the reliability and practicality of KGs. It is particularly useful for enabling community-wide KG usage by providing a comprehensive resource for training and evaluating fact-checking systems. The study also explores different models like BERT, BlueBERT, and Flan-T5, demonstrating that incorporating graphical evidence leads to superior performance compared to baselines without such evidence. This approach not only improves accuracy but also aligns with the practical needs of real-world fact-checking scenarios.

In summary, FactKG serves as a valuable tool for advancing KG-based fact verification, offering a rich dataset that supports a wide range of reasoning types and linguistic styles. Its practical application and superior performance metrics make it a significant contribution to the field of computational linguistics and knowledge graph technology.</sample>
    <sample id="248">No</sample>
    <sample id="249">Prefix/suffix adverbs, long prefix adverbs, add clause, and quote.</sample>
    <sample id="250">It means evaluating chat-oriented dialogue systems based on multiple dimensions such as relevance, consistency, and emotional understanding.</sample>
    <sample id="251">University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, Microsoft STC Asia</sample>
    <sample id="252">The research presented in this paper focuses on the development and evaluation of unsupervised methods for Prior Case Retrieval (PCR) within the Indian legal domain, specifically targeting the retrieval of relevant past precedents from legal documents. The authors introduce U-CREAT, an unsupervised pipeline designed to extract events from case documents, which are then used to retrieve similar cases based on their factual and precedent relevance.

The study proposes a new dataset, IL-PCR, comprising 7070 legal cases with detailed annotations for event extraction. This dataset serves as a benchmark for evaluating PCR systems. The authors explore various models, including count-based (BM25), transformer-based (DistilBERT, BERT), and event-based approaches, demonstrating that event-based models outperform traditional methods like BM25 in terms of F1 score across different datasets (COLIEE'21 and IL-PCR).

Key contributions include:
1. **Dataset Development**: Creation of IL-PCR, a comprehensive dataset tailored for PCR in the Indian legal context.
2. **Pipeline Development**: Introduction of U-CREAT, an unsupervised pipeline for event-based PCR that leverages event extraction techniques.
3. **Model Evaluation**: Comparative analysis of different models, highlighting the superior performance of event-based methods over traditional ones.
4. **Performance Metrics**: Detailed evaluation using F1 scores, showing significant improvements with event-based models.

The paper concludes by emphasizing the practical applicability of U-CREAT in real-world scenarios, noting its potential for automation in legal document retrieval processes. The authors also invite further exploration into the integration of corpus-specific fine-tuning to enhance model performance.</sample>
    <sample id="253">This research introduces DisorBERT, a novel double domain adaptation model designed to detect signs of mental disorders in social media interactions. The study emphasizes the importance of domain adaptation in enhancing the performance of language models like BERT when applied to specialized domains such as mental health. By adapting BERT specifically for the mental disorders domain, DisorBERT aims to improve the accuracy and relevance of mental health detection in online platforms.

The presentation highlights the significance of mental disorders, defining them as psychological syndromes that impact thinking, feeling, mood, and behavior. It underscores the growing prevalence of social media usage, with over 4.76 billion users worldwide, and discusses how this trend can be leveraged for early intervention and support. The concept of domain adaptation is explained through a metaphor involving a Google search, illustrating how adjusting the model's vocabulary and understanding can enhance its performance on specific tasks.

DisorBERT employs guided masking techniques to refine its understanding of mental health-related language, focusing on terms and phrases commonly associated with depression and anxiety. The model's effectiveness is demonstrated through precision and recall analysis using eRisk datasets, showing superior results compared to baseline models. The study also includes user analysis using the Beck Depression Inventory (BDI) to validate the model's predictions, ensuring they align with human assessments.

In conclusion, DisorBERT represents a significant advancement in leveraging computational linguistics for mental health detection, offering a promising tool for clinical applications. Future work will explore the integration of different lexical resources and clinical data to further enhance the model's capabilities, making it more suitable for real-world deployment in healthcare settings.</sample>
    <sample id="254">This research introduces a novel framework for document-level relation distant extraction (DocRE) that significantly enhances the quality of distant supervision data through uncertainty-guided label denoising. The framework leverages an iterative re-label strategy with dynamic class uncertainty thresholds to filter high-uncertainty pseudo labels, thereby improving the reliability of instance-level predictions. It employs uncertainty estimation techniques, such as MC dropout, to assess model confidence and adaptively adjust thresholds based on the distribution of uncertainty scores. This approach is particularly effective for long-tail relation types, where traditional methods often struggle due to imbalanced data distributions. The proposed method is validated on two public datasets, achieving substantial performance improvements over existing baselines. Extensive experiments demonstrate that models trained on denoised DS data outperform those on original DS data, highlighting the framework's potential for enhancing the robustness and accuracy of DocRE systems.</sample>
    <sample id="255">The form of the prompting is important in cases where the quality of the translation is significantly impacted by the prompt selection strategies.</sample>
    <sample id="256">**Slide 1: Title Slide**
- **Title:** Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge
- **Authors:** Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoan Liu, Jonah Luby, Christian C. Luhmann, H. Andrew Schwartz
- **Affiliation:** Stony Brook University - Human Language Analysis Settings

**Slide 2: Introduction to Cognitive Dissonance**
- **Definition:** "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007)
- **Reference:** Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1):71ff.

**Slide 3: Example of Cognitive Dissonance**
- **Scenario:** "I know that cigarettes could kill me. I grabbed a couple smokes after the meeting today."
- **Explanation:** The belief ("cigarettes could kill me") and action ("grabbed a couple smokes") create dissonance.

**Slide 4: Expressing Dissonance in Language**
- **Example:** "I know that cigarettes could kill me. I grabbed a couple smokes after the meeting today." (Dissonance)
- **Reference:** Eddie Harmon-Jones and Cindy Harmon-Jones. 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1):71ff.

**Slide 5: Types of Dissonance**
- **Belief-Action Dissonance:** Belief and action are inconsistent.
- **Consistency Explanation:** The user tries to explain away the inconsistency.

**Slide 6: Rarer Occurrence of Dissonance**
- **Comparison with Other Discourse Relations:** Dissonance is relatively rare compared to other discourse relations.

**Slide 7: Effects of Disagreement**
- **Illustration:** Two people disagreeing.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 8: Attitudes and Belief Trends**
- **Graphs:** Show trends in attitudes and belief changes over time.

**Slide 9: Entry and Exit from Extremism**
- **Illustration:** People entering and exiting extremism.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 10: Anxiety Disorders**
- **Illustration:** A person with anxiety disorders.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 11: Annotation Process**
- **Flowchart:** Shows the process of parsing, annotating, and classifying text.
- **Example:** "Wish I could hold grudges but I guess it’s a good thing that I can’t at the same time."

**Slide 12: Training on Initial Annotated Set**
- **Graph:** Shows the area under the ROC curve (AUC) for different models.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 13: Method: Transfer and Active Learning for Annotating Rare Class**
- **Diagram:** Shows the process of cold-start annotations using transfer learning.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 14: Cold-start Annotations: Transfer Learning**
- **Diagram:** Shows the process of cold-start annotations using transfer learning.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 15: Active Learning: Cumulative vs Iterative Update**
- **Diagram:** Shows the process of cumulative vs iterative update in active learning.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 16: Active Learning: Probability-of-Rare-Class Strategy**
- **Diagram:** Shows the process of active learning using the probability-of-rare-class strategy.
- **Reference:** Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reseaming a pivotal theory in psychology.

**Slide 17: Takeaways**
- **Key Points:**
  - Minimum annotation cost does not necessarily lead to better models.
  - Rarity could make the annotations more difficult; cognitive dissonance is one such class.
  - To increase dissonance samples, PRC works the best.

**Slide 18: QR Codes**
- **Contact Information:** Links to code, dataset, and paper.

**Slide 19: Thank You!**
- **Text:** "Thank you!"</sample>
    <sample id="257">The authors evaluated four open-domain dialogue models.</sample>
    <sample id="258">This research explores whether large language models (LLMs) can serve as an alternative to human evaluations for assessing text quality, specifically focusing on story generation. The study proposes using LLMs to rate stories generated by both human writers and large language models like GPT-2 and TO. It introduces a novel method where LLMs are given specific instructions to evaluate texts based on attributes such as grammar, coherence, likeability, and relevance. The evaluation process involves rating these stories on a scale from one to five, with higher scores indicating better quality.

The researchers conducted experiments using four LLMs: Human, TO, curie, davinci, and ChatGPT. They hired English teachers to conduct human evaluations on the same stories using identical instructions to ensure consistency. The results show that larger LLMs (text-davinci-003 and ChatGPT) exhibit a clear preference for human-written stories over those generated by smaller LLMs (TO and text-curie-001). This suggests that while LLMs can provide valuable insights, they may not fully capture the nuances and preferences of human evaluators, especially when it comes to more complex tasks like story evaluation.

The study's findings highlight the potential of LLMs in automated text evaluation but also underscore the importance of human judgment, particularly in scenarios requiring nuanced understanding and creativity. The authors conclude by suggesting further exploration into how different instruction formulations might influence LLM performance and by encouraging the application of LLM evaluation in various contexts beyond story generation.</sample>
    <sample id="259">The research presented in this study focuses on developing a unified benchmark for cross-lingual semantic parsing, named XSemPLR, which supports multiple natural languages and meaning representations. The study aims to address the limitations of existing cross-lingual semantic parsing models by providing a comprehensive dataset that includes nine datasets from various domains, five semantic parsing tasks, eight meaning representations, and data from 22 natural languages across 15 language families. The dataset is designed to cover a wide range of tasks and applications, ensuring broader coverage and more robust evaluation.

The methodology involves training and evaluating models under six different settings: Translate-Test, Monolingual Model, Multilingual Model, Cross-lingual Zero-shot/Few-shot transfer, and Monolingual Few-shot. These settings help in understanding the performance of models across different scenarios, including monolingual and multilingual environments. The study also evaluates two groups of models—Enc-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) and Enc-Dec (Multilingual Pretrained Encoder-Decoder Models)—on both monolingual and multilingual settings.

Key findings indicate that Enc-Dec (mT5) outperforms previous work or achieves comparable results, while pretraining on the English NL can significantly boost the performance of few-shot on target NLs. Multilingual LLMs like Codex &amp; BLOOM are still inadequate for cross-lingual semantic parsing tasks, with Chinese transfer learning and English monolingual training showing the largest performance gap, whereas German usually has the smallest. FunQL outperforms other three meaning representations, and SQL obtains the worst performance.

The study concludes that mT5 with monolingual training yields the best performance, while multilingual LLMs remain inadequate for cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is significant, highlighting the need for further research in this area.</sample>
    <sample id="260">8</sample>
    <sample id="261">The ideal qualities of a good planner include being able to effectively decompose goals into steps, handling abstract goals that can be inherited by different real-life specific goals with multi-faceted constraints, and generating scripts that are faithful to the constraints while maintaining semantic completeness in generated scripts.</sample>
    <sample id="262">7</sample>
    <sample id="263">This research explores the challenges and solutions in mitigating label biases within in-context learning, a method where language models predict labels based on recent examples. The study identifies three types of label biases: vanilla-label bias, context-label bias, and domain-label bias, which stem from the model's inherent preferences for certain label names, the influence of the context, and the characteristics of the task corpus, respectively. These biases can significantly impact the performance of language models, especially in classification tasks.

The authors propose a novel approach called Domain-context Calibration (DC) to address these biases holistically. DC involves calibrating the model using random in-domain words, which helps in reducing all three types of biases. This method is demonstrated to be effective across various datasets and tasks, particularly showing significant improvements on tasks with larger domain-label bias. The study also discusses the importance of using random in-domain words over predefined tokens like 'N/A' due to potential biases in such tokens.

Empirical results from experiments with GPT-J and GPT-3 models confirm that DC improves in-context learning performance, especially when dealing with large domain-label bias. The authors further explore the effectiveness of DC through ablation studies, showing that using more random English words leads to better calibration outcomes. The paper concludes by highlighting the significance of domain-context calibration in enhancing the robustness and reliability of in-context learning systems, suggesting that this approach can be applied broadly to improve the performance of language models in diverse applications.</sample>
    <sample id="264">This research introduces TAVT, a novel framework for transferable audio-visual text generation, addressing the challenges of data annotation and domain shifts in multi-modal tasks. The study highlights that existing methods often suffer from severe degradation due to the complexity and cost of annotating data, and the inherent differences between audio and visual modalities. TAVT mitigates these issues by leveraging a meta-mapper network to align audio and visual features across domains, ensuring robust performance even with limited labeled data. The method employs counterfactual contrastive learning to enhance model generalization, focusing on both distribution-based and dependency-based losses to improve cross-domain transferability. Experiments on the cross-dataset benchmark demonstrate TAVT's effectiveness, achieving state-of-the-art results compared to other methods. This work underscores the importance of efficient and scalable approaches for audio-visual text generation, particularly in scenarios where data is scarce or costly to annotate.</sample>
    <sample id="265">Vasudha Varadarajan</sample>
    <sample id="266">Institute of Computer Science, Polish Academy of Sciences and University of Warsaw</sample>
    <sample id="267">The English content on the slide reads:

- We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.
- We conduct a comprehensive benchmark study on three representative types of multilingual language models.
- Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="268">Accuracy/Omission and Style/Awkarad</sample>
    <sample id="270">Emory NLP Research Lab, Emory University</sample>
    <sample id="271">Continuous fine-tuning</sample>
    <sample id="272">Six</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="275">**Slide 1: Title Slide**
- **Title:** From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models
- **Subtitle:** #ACL2023
- **Authors:** Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov
- **Institutions:** Paul Allen School, University of Washington (UW), Carnegie Mellon University (CMU)

**Slide 2: LM Training Data**
- **Title:** LM Training Data
- **Subtitle:** A mixed blessing
- **Content:** Bar chart showing the distribution of websites in the training data for language models.
- **Reference:** Ordonez, J., et al. "Decomposing Large Language Models: A Case Study on Political Bias in Crawled Corpora." Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

**Slide 3: Flowchart**
- **Title:** To this end
- **Content:** Flowchart showing the process from pretraining data to language models and then to downstream tasks.
- **Questions:** 
  - How to evaluate the political leaning of LMs?
  - What role does pretraining data play in such political biases?
  - How do LMs with different political leanings perform?
  - Does LM political leaning result in fairness issues in NLP applications?

**Slide 4: Evaluating LM Political Leaning**
- **Title:** Evaluating LM Political Leaning
- **Content:** 
  - Support both encoder and decoder LMs.
  - Automatic eval grounded in polisci lit.
  - Example prompt: "Our race has many superior qualities, compared with other races."
  - Political Compass Test results for different LMs.

**Slide 5: Existing LMs**
- **Title:** Existing LMs
- **Content:** Scatter plot showing the political leaning of various existing LMs on a left-right authoritarian scale.

**Slide 6: Pretraining Data**
- **Title:** Pretraining Data
- **Content:** Diagram showing how further pretraining on RoBERTa and GPT-2 checkpoints can evaluate changes in political leaning.

**Slide 7: Results**
- **Title:** Results
- **Content:** Partisan shifts in LM political leaning for RoBERTa and GPT-2.

**Slide 8: The Trump Card**
- **Title:** The Trump Card
- **Content:** Pre-45th to post-45th shift in political leaning for different LMs.

**Slide 9: Per-Category Performance**
- **Title:** Per-Category Performance
- **Content:** Table showing performance on hate speech targeting different identity groups and misinformation from different sources.

**Slide 10: Qualitative Analysis**
- **Title:** Qualitative Analysis
- **Content:** Examples of hate speech examples where LMs with different political leanings give different results.

**Slide 11: Discussion**
- **Title:** Discussion
- **Content:** Between Scylla and Charybdis: To “sanitize” or not to “sanitize”, that is the question.
- **Flowchart:** Pretraining data → Language models → Downstream tasks.

**Slide 12: Thank You!**
- **Content:** Thank you message with acknowledgments to the authors and institutions involved in the research.</sample>
    <sample id="276">This research focuses on developing a comprehensive dataset, IndicMT-Eval, to evaluate machine translation metrics specifically for Indian languages. The study highlights the importance of studying evaluation metrics for languages other than English due to their unique grammatical structures and vocabulary. The dataset includes translations from English to five Indian languages—Tamil, Malayalam, Hindi, Marathi, and Gujarati—belonging to two different language families: Dravidian and Indo-Aryan. The data collection process involves selecting 200 random sentences from the Flores dataset and translating them using various machine translation systems like mBART, Bing API, Google API, CVIT-IITB, IndicTrans, mT5, and NLLB. These translations are then evaluated by bilingual expert annotators using the MQM framework, which categorizes errors into accuracy, fluency, and others/special categories. The study also explores the correlation between various automatic evaluation metrics and human scores, identifying that some metrics perform better for specific error types. The research concludes with the introduction of IndicCOMET, a metric variant fine-tuned using the MQM annotations, demonstrating its effectiveness in evaluating machine translations for Indian languages.</sample>
    <sample id="277">The new method is called "Permuting with 'jumps'".</sample>
    <sample id="278">The author described the "marked words" method as finding words that distinguish personas of marked groups from unmarked groups, without requiring a lexicon.</sample>
    <sample id="279">The affiliations of the authors are Paul Allen School, University of Washington, UW NLP, and Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="280">This research introduces MultiEMO, an advanced multimodal fusion framework designed for emotion recognition in conversations (ERC). The framework addresses the limitations of existing ERC approaches by leveraging the complementarity of textual, audio, and visual modalities to predict emotions accurately. It proposes a novel visual feature extractor, VisExtNet, which captures relevant visual cues from interlocutors without encoding redundant scene information. Additionally, it introduces a multimodal fusion model, MultiAttn, based on bidirectional multi-head cross-attention layers, effectively modeling correlations across different modalities.

The study highlights that current state-of-the-art methods struggle with minority emotion classes and the difficulty of distinguishing semantically similar emotions. To overcome these challenges, MultiEMO employs a Sample-Weighted Focal Contrastive (SWFC) loss, which assigns higher importance to hard-to-classify minority classes and maximizes inter-class distances to enhance the distinguishability of semantically similar emotions.

Experimental results demonstrate that MultiEMO achieves state-of-the-art performance on two ERC benchmark datasets, MELD and IEMOCAP, particularly showing significant improvements in minority and semantically similar emotion categories. This work not only advances the field of ERC but also provides valuable insights into addressing the challenges posed by imbalanced datasets and the complexity of emotion classification.</sample>
    <sample id="281">The research presented in this study explores the critical role of context in translation, particularly focusing on how context influences the accuracy and effectiveness of machine translation (MT) systems. The study emphasizes that translation is inherently context-dependent, as demonstrated through various examples where the same word can have different meanings based on its surrounding context. This dependency makes evaluating context-dependent translations challenging, as only a small portion of words in a sentence depend on context, and existing evaluation metrics often fail to capture these nuances.

To address these challenges, the researchers introduce the Conditional Cross-Mutual Information (CXMI) metric, which quantifies how much context MT models utilize given a corpus. They also propose Pointwise CXMI (P-CXMI), a method to measure context usage at both the word-level and sentence-level, providing insights into when translation requires context. Thematic analysis of high P-CXMI words reveals patterns such as pronouns, verb forms, lexical cohesion, formality, and ellipsis, indicating that these elements significantly impact translation quality.

The study further evaluates the performance of context-aware models using the Multilingual Discourse-Aware (MuDA) benchmark, demonstrating that context-aware models outperform traditional methods on several discourse phenomena across multiple languages. The MuDA tagger, a dataset-agnostic tool for identifying discourse phenomena, is introduced as a valuable resource for document-level MT, enabling systematic identification of discourse features without prior linguistic knowledge.

In conclusion, the research underscores the importance of context in translation, highlighting the need for advanced evaluation metrics and context-aware models to improve the accuracy and relevance of machine translations. The MuDA benchmark serves as a robust tool for assessing and enhancing document-level MT systems, making it a significant contribution to the field of natural language processing.</sample>
    <sample id="282">This research introduces StoryTrans, a novel framework for non-parallel story author-style transfer that leverages discourse representations and content enhancement techniques. The challenge addressed is the imitation of authors' linguistic choices at the discourse level and the high association between author styles and specific writing topics. The solution involves two main components: discourse representation transfer and content preservation enhancing. Discourse representation transfer is achieved through a masked source story and style embedding process, followed by a masked transferred story generation. Content preservation enhancing focuses on maintaining the original story's content while adapting it to the target style. The training framework includes a first-stage model trained with a loss function that combines language modeling, style consistency, and discourse representation losses. A second-stage model uses a denoising auto-encoder (DAE) loss to reconstruct the original story from the transformed one, ensuring content preservation. The proposed method is evaluated on Chinese and English datasets, demonstrating superior performance compared to existing models, particularly in terms of style accuracy and content preservation.</sample>
    <sample id="283">Bouquet/Stanford</sample>
    <sample id="284">This research introduces FSUIE, a novel framework for enhancing Universal Information Extraction (UIE) tasks by addressing the limitations of existing models that heavily rely on precise span boundaries. The study highlights the inadequacy of one-hot distribution methods in capturing the ambiguity and uncertainty inherent in span boundaries. To overcome these issues, FSUIE employs a fuzzy span mechanism, which models the boundary as a continuous distribution, allowing for more flexible and adaptive span extraction decisions.

FSUIE also incorporates efficient Fuzzy Span Attention, which adjusts the attention span adaptively based on local features, enhancing the model's ability to focus on relevant information within a limited range of preceding tokens rather than relying solely on global representations. This approach not only improves convergence rates but also boosts performance through synergy with other components like FSA.

The proposed method is validated across various Information Extraction (IE) tasks, including Named Entity Recognition (NER), Relation Extraction (RE), and Argument Structure Extraction (ASTE). Results demonstrate significant improvements over baseline models, particularly in small-scale datasets where fuzzy span-awareness is crucial. Additionally, FSUIE shows better generalization capabilities for domain-specific information, making it a robust solution for a wide range of IE tasks.</sample>
    <sample id="285">This research focuses on enhancing factual error correction (FEC) for dialogue summarization, addressing the challenge that summaries generated by models often contain factual errors. The study introduces a fine-grained evaluation framework to assess FEC models' performance more comprehensively and accurately. It identifies two common solutions: direct design of better summarization models and factual error correction (FEC) for model-generated summaries. The FEC process involves comparing the original summary with a reference correction, which is manually annotated to correct factual errors using substitution, insertion, and deletion operations. This approach ensures that the corrected summary is fluent and non-redundant.

The evaluation framework uses factuality metrics like FactCC to measure the quality of FEC models. However, these metrics alone may not be sufficient as they can blur the distinction between the two solution types. The study proposes a reference-based evaluation method that provides more valuable data for training FEC models compared to pseudo data, thereby improving the overall performance of FEC models. The introduction of reference corrections also creates conditions for a more comprehensive evaluation of FEC model performance.

Experiments with FEC models demonstrate that training with reference summaries from dialogue summarization datasets yields the best results, outperforming unreliable factuality metrics. The findings highlight the need to change current evaluation methods for FEC models and suggest combining human-annotated data with synthetic data as a promising direction. Overall, this research contributes to advancing FEC techniques in dialogue summarization, aiming to improve the accuracy and reliability of generated summaries.</sample>
    <sample id="286">Sarah E. Finch</sample>
    <sample id="287">4</sample>
    <sample id="288">BLIMP, SyntaxGym, and CrowS</sample>
    <sample id="289">The English content is as follows:

- When Does Translation Require Context? A Data-driven, Multilingual Exploration
- Translation depends on context
- We'll have to get rid of that mole.
- Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.
- Could it be anything serious, Doctor? We'll have to get rid of that mole.
- Evaluating context-dependent translation is hard
  - Only a small portion of words depend on context
  - Corpus-level metrics
  - Existing methods support limited discourse phenomena and languages
- RQ1: When does translation require context?
  - Word-level context usage
- RQ2: How well do models handle context-dependent translations?
- Conditional Cross-Mutual Information (CXMI)
  - CXMI: measure how much context MT models use given a corpus
- Pointwise (P-)CXMI
  - We introduce P-CXMI to measure context usage to translate a specific
    o Senten P-CXMI(y,x,C) = − log qMTA(y|x) / qMTc(y|x,C)
    o Word P-CXMI(i,y,x,C) = − log qMTA(yi|yi&lt; i,x) / qMTc(yi|yi&lt; i,x,C)
  - High P-CXMI words -&gt; requires context to translate
- Thematic analysis of high P-CXMI words
  - Word-level context usage
  - Thematic analysis
- TED IDEAS WORTH SPREADING
- POS tags
- Vocabulary items
- Individual tokens
- Avelie's mother was still asleep. Avelie went to school.
- She knows where we're going. I don't.
- Sie weiß, wohin wir gehen. Ich weiß es nicht.
- MuDA benchmark results
  - Context-aware models perform significantly better on some phenomena
  - Formality, lexical cohesion
  - Ellipsis, pronouns, verb form
- DeepL outperforms Google on most phenomena and language pairs</sample>
    <sample id="290">FTw, BOND, COSINE, MLC, L2R</sample>
    <sample id="291">The model is evaluated on 11 tasks.</sample>
    <sample id="292">**Slide Title:** DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification

**Slide Content:**
- **Title:** DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification
- **Authors:** Regina Stodden, Omar Momen, Laura Kallmeyer
- **Affiliation:** Heinrich Heine University Düsseldorf, Germany
- **Conference:** ACL 2023

---

**Slide Title:** Text Simplification
- **Subtitle:** What, why and How?

---

**Slide Title:** Text Simplification Example
- **Original Sentence:** "Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."
- **Plain Language Version:** "Die Gewerkschaft setzt sich für höhere Löhne oder mehr Utaub ein."

---

**Slide Title:** 2. DE-plain
- **Subtitle:** A New Corpus

---

**Slide Title:** German Text Simplification Corpora
- **Sentence Level Bar Chart:**
  - **Data Points:**
    - **Simplified:** 14500
    - **Aligned:** 250
    - **Unaligned:** 1398
    - **Total:** 16348
  - **Corpora:**
    - **Simplified:** 14500 (DEplain-APA test)
    - **Aligned:** 250 (DEplain-APA test)
    - **Unaligned:** 1398 (DEplain-APA test)
    - **Total:** 16348 (DEplain-APA test)

---

**Slide Title:** Types of Simplification
- **Bar Chart:**
  - **Categories:** News, Bible, L2, Fiction
  - **Simplification Types:**
    - **Simplicity:** 1.0
    - **LexSimp:** 0.7
    - **StructSimp:** 0.6

---

**Slide Title:** Simplification Transformations
- **Bar Chart:**
  - **Categories:** Renderings, Rephrasing, Lexical Substitution, Word Substitution, Word Deletion, Word Addition
  - **Transformations:**
    - **Renderings:** 60%
    - **Rephrasing:** 20%
    - **Lexical Substitution:** 20%
    - **Word Substitution:** 10%
    - **Word Deletion:** 5%
    - **Word Addition:** 5%

---

**Slide Title:** 3. Use-cases
- **Subtitle:** Automatic alignment and simplification

---

**Slide Title:** Automatic Alignment Evaluation
- **Table:**
  - **Alignment Methods:**
    - **UVA:** Unsupervised alignment using sentence embeddings similarity
    - **Sent-LBSE:** Similar embeddings of Language-agnostic BERT transformer
    - **Sent-Rubik:** Similar embeddings of Rubik's cube
    - **CATS-C3G:** Similarity measures by e-grams/cxk vectors
    - **VecAlign:** Multilingual aligner based on multilingual sentence embeddings
    - **BERTAlign:** Multilingual aligner based on BERT embeddings
    - **MASSAlign:** A vicinity-driven approach with a TF-IDF similarity matrix
  - **Metrics:**
    - **P:** Precision
    - **R:** Recall
    - **F1:** F1-score
    - **P1:** Precision@1
    - **R1:** Recall@1
    - **F11:** F1-score@1
    - **P2:** Precision@2
    - **R2:** Recall@2
    - **F12:** F1-score@2
    - **P3:** Precision@3
    - **R3:** Recall@3
    - **F13:** F1-score@3

---

**Slide Title:** Document Level
- **Results on Document Simplification using fine-tuned long mBART:**
  - **Input Data:**
    - DEPLAIN-APA test (n=48)
    - DEPLAIN-WEB test (n=147)
  - **Metrics:**
    - **SARS:** 64.86 / 64.86 / 64.86
    - **BLEUP:** 21.98 / 21.98 / 21.98
    - **BLEUP-FRE:** 66.41 / 66.41 / 66.41
    - **BLEUP-PRE:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE1:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE2:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE3:** 66.41 / 66.41 / 66.41

---

**Slide Title:** Sentence Level
- **Results on Sentence Simplification using fine-tuned long mBART:**
  - **Input Data:**
    - DEPLAIN-APA test (n=48)
    - DEPLAIN-WEB test (n=147)
  - **Metrics:**
    - **SARS:** 64.86 / 64.86 / 64.86
    - **BLEUP:** 21.98 / 21.98 / 21.98
    - **BLEUP-FRE:** 66.41 / 66.41 / 66.41
    - **BLEUP-PRE:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE1:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE2:** 66.41 / 66.41 / 66.41
    - **BLEUP-ROUGE3:** 66.41 / 66.41 / 66.41

---

**Slide Title:** Thanks.
- **Text:** For more details, please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="293">**Slide Title:** Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)

**Slide Content:**
- **Title:** Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)
- **Authors:** Mohammad Javad Hosseini, Filip Radlinski, Silvia Paresi, and Annie Louis
- **Affiliation:** Google Research

---

**Slide Title:** Indirect Referring Expressions

**Slide Content:**
- **Goal:** Understanding users' language when they make a choice.
- **Alternative question:** Did you mean easy on me or I gotta feeling?
- **Direct reference:** "easy on me", "the first one"
- **Indirect reference could be used in natural and fluid conversation:**
  - Cannot remember the name
  - The pronunciations are hard to distinguish
  - Want to specify a preference
- **Indirect reference examples:**
  - The newer one.
  - The song that's not energetic.

---

**Slide Title:** Dataset Collection

**Slide Content:**
- **Important problem:**
  - Conversational systems
  - Benchmarking Large Language Models' entity understanding
- **No large-scale public dataset available**
- **We collect a large dataset using crowd annotation**
- **Three domains:**
  - Music
  - Books
  - Recipes

---

**Slide Title:** Dataset Collection Methodology

**Slide Content:**
- **Methodology emphasizes informality using a cartoon completion task**
- **Steps:**
  - Sets the dialog context [shown as free manual prompts per domain]
  - The alternative question
  - Expression referring to one of the entities
  - Filled in by the annotator

---

**Slide Title:** Generate alternative questions =&gt; sampling entity pairs

**Slide Content:**
- **Items with similar infoboxes on Wikipedia (same genre and/or artist):**
  - Do you mean This is it? or Man in the Mirror?
- **Items with similar descriptions on Wikipedia:**
  - Do you mean Thinking of You or Happy Anywhere?
- **Items with similar titles:**
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- **Uniform at random:**
  - Do you mean You Could Be Mine or The Way I Am?

---

**Slide Title:** Background knowledge (Music)

**Slide Content:**
- **Google search link to each song:**
  - Easy on Me (by Adele)
  - I Gotta Feeling (by The Black Eyed Peas)
- **Background knowledge:**
  - We ask annotators to:
    - Listen to at least some of each song
    - Read about each song

---

**Slide Title:** Eliciting expressions

**Slide Content:**
- **Pick this one:**
  - Easy on Me (by Adele)
  - I Gotta Feeling (by The Black Eyed Peas)
- **We then tell the annotators which choice should be selected and ask them to describe it. For example:**
  - The one with the piano main
  - The song that's not energetic
  - It has something about a car
  - The newer one
  - It's about having time to choose

---

**Slide Title:** AltEntities Corpus

**Slide Content:**
- **6,000 alternative questions across the three domains**
- **42,000 indirect referring expressions**
- **Results with T5 XL model (accuracy):**
  - 92%-95% if the LM has access to the same background knowledge as annotators.
  - 82%-87% when the the LM (T5 XL) has only access to the entity names.
- **Dataset Link:** https://github.com/google-research-datasets/AltEntities
- **We showed models are domain-generalizable.**

---

**Slide Title:** Thank You!

**Slide Content:**
- **Thank You!**
- **If you have any questions, please email javadh@google.com**</sample>
    <sample id="294">CamemBERT is initially trained on the NACHOS dataset.</sample>
    <sample id="295">Adam Przepiórkowski and Michał Woźniak</sample>
    <sample id="296">The research presented in this study focuses on the development and application of a multi-perspective approach to annotating irony within a corpus, specifically targeting the English language. The project, titled "EPIC: Multi-perspective Annotation of a Corpus of Irony," involves a collaborative effort by researchers from the University of Turin and Amazon Alexa. The study highlights the limitations of traditional supervised machine learning approaches, which heavily rely on manually annotated data encoding human knowledge, particularly in subjective tasks like irony detection.

To address these limitations, the researchers propose a perspective-based approach, emphasizing that the perception of irony can vary across different perspectives. This is illustrated through examples where the same sentence might be considered ironic by one person but not by another, depending on their context or background. The EPIC corpus, comprising over 3,000 text-reply pairs from Reddit and Twitter, captures a wide variety of English dialects including British, American, Irish, Australian, and Indian English, reflecting the diversity of linguistic contexts.

The annotation process involves a rigorous methodology with balanced sets of annotators based on gender and country of residence, ensuring a diverse and representative sample. Each annotator provides five annotations per text, with attention-check questions included to maintain consistency and accuracy. The study aims to understand how perspective-aware models perform compared to non-perspective-based models, particularly in terms of confidence and uncertainty when tested on a test set representative of their perspective.

The findings suggest that perspective-aware models tend to make decisions with less uncertainty than standard non-perspective models, indicating their potential superiority in handling subjective tasks like irony detection. The research also explores variations in irony perception among different generations and nationalities, revealing that contiguous generations perceive irony differently, with the highest variation observed between the United Kingdom and Ireland. Overall, the study underscores the importance of considering multiple perspectives in natural language understanding, especially for complex phenomena like irony.</sample>
    <sample id="297">This research project focuses on the identification and analysis of coded language, specifically dogwhistles, used in political messaging to influence public opinion without direct confrontation. The study employs advanced language models like GPT-3 to detect these coded terms and their underlying meanings, which often carry hidden biases or stereotypes. By analyzing historical U.S. political speeches, the project aims to understand how these coded messages are used to target specific groups and influence policy debates. The research also explores the effectiveness of different types of dogwhistles, such as those related to race, religion, and gender, and evaluates how they evade content moderation systems. Through a comprehensive typology and glossary, the project provides a structured framework for understanding and categorizing these coded expressions, ultimately demonstrating how they can be used to evade detection and manipulate public discourse.</sample>
    <sample id="298">The findings that led to the conclusion that the temporal drift is the main cause of performance loss include the observation that performance degrades with larger temporal gaps, and the lack of diminishing returns from adaptive overfitting.</sample>
    <sample id="299">This research focuses on enhancing the robustness of Natural Language Inference (NLI) models by addressing shortcut learning, which involves decision rules that spuriously correlate with labels rather than the true semantic relationships between sentences. The study highlights the issue of shortcuts not generalizing well to out-of-distribution (OOD) data, as demonstrated through examples and performance metrics across different datasets like MNLI, FEVER, and QQP. To mitigate this, the authors propose a minimax training approach, where an auxiliary model is trained to rely on shortcuts, and its predictions are used to re-weight examples for the main learner, emphasizing hard examples. This method aims to improve OOD performance while maintaining high in-distribution accuracy. The paper also explores the impact of pre-training the learner, the size of the auxiliary network, and the qualitative evaluation of the learned example weight distribution. Experimental results show consistent improvements in OOD performance across various datasets, suggesting the effectiveness of the proposed method in overcoming the limitations of shortcut learning in NLI models.</sample>
    <sample id="300">This research paper introduces and formalizes a new task called Interactive Dictation, which aims to address the limitations of existing speech-to-text systems that do not support editing through voice. The study highlights the need for a system that can seamlessly integrate dictation with editing commands, allowing users to correct errors or make changes directly from their voice. The authors propose a flexible interleaving of dictation and editing, eliminating reserved trigger words for command invocation and challenging the prediction of segmentation between dictation and editing commands.

The paper outlines the development process, including designing a data collection interface and building a dataset for this task. It also discusses the creation of a baseline system for the task, focusing on the challenges of interpreting open-ended natural language for editing. The authors present their contributions, which include introducing and formalizing the new task, designing a data collection interface, and creating a baseline system.

The system architecture is detailed, featuring an ASR repair step, an interpretation step, and an execution engine. The ASR repair step corrects errors in the ASR output, while the interpretation step interprets the user's commands. The execution engine then executes these commands, ensuring a smooth transition between dictation and editing. The paper also presents results from experiments using TERTIUS, a dataset designed to evaluate the performance of the proposed system.

In conclusion, the paper provides a comprehensive overview of the challenges and solutions in developing an interactive dictation system, emphasizing the importance of flexibility and natural language understanding in enhancing user experience.</sample>
    <sample id="301">**Slide Title:** NLPositionality: Characterizing Design Biases of Datasets and Models

**Slide Content:**
- **Title:** NLPositionality: Characterizing Design Biases of Datasets and Models
- **Authors:** 
  - Sebastian Sardny (University of Washington)
  - Jenny T. Liang (Carnegie Mellon University)
  - Ronan Le Bras (Allen Institute for AI)
  - Katharina Reinecke (University of Washington)
  - Maarten Sap (Carnegie Mellon University)

**Slide Transition:**
- **Imagine...**
- **PerspectiveAPI score**
- **Design bias example!**

**Slide Title:** Positionality

**Slide Content:**
- **Definition:** "The perspectives [people] hold as a result of their demographics, identity, and life experiences."
- **Source:** Savin-Baden, Maggi, and Claire Howell-Major. *Qualitative research: The essential guide to theory and practice.* Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).

**Slide Title:** Do datasets and models have positionality?

**Slide Content:**
- **Anecdotal evidence:**
  - Model and dataset probing
  - Theoretical definitions of model positionality

**Slide Title:** Framework

**Slide Content:**
- **Collection:** Instances are collected from a dataset.
- **Processing:** Each instance has an associated ground truth label.
- **Model Prediction:** Instances are sent as part of a study on LPI.
- **Annotations from diverse annotators:** Annotators annotate the instances with labels.
- **Analysis:** Pearson’s correlation is computed between the ground truth labels, model predictions, and the annotations of the demographic dimension.

**Slide Title:** LabintheWild

**Slide Content:**
- **Pool of diverse volunteers/research participants**
- **Online experiment from researchers**

**Slide Title:** Task A: Social Acceptability

**Slide Content:**
- **Participants read a situation from the Social Chemistry dataset.**
- **Participants rate how socially acceptable the situation is.**
- **Participants compare their responses to others and an AI's.**

**Slide Title:** Task B: Toxicity

**Slide Content:**
- **Participants read an instance from the Dynahate dataset.**
- **Participants rate whether they think an instance is hate speech.**
- **Participants compare their responses to others and an AI's.**

**Slide Title:** Study Participation

**Slide Content:**
- **16,299 annotations**
- **1,096 annotators**
- **87 countries**

**Slide Title:** Results

**Slide Content:**
- **Finding 1:** There is positionality in NLP.
- **Finding 2:** Some populations are left behind.
- **Finding 3:** Datasets and models are less aligned to non-binary people.

**Slide Title:** Recommendations

**Slide Content:**
1. Keep a record of all relevant design choices made throughout building datasets or models.
2. Do NLP research through the lens of perspectivism:
   - Share disaggregated dataset labels.
   - Use modeling techniques that can handle annotator disagreement.
3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative).

**Slide Title:** Dashboard Link

**Slide Content:**
- **Dashboard Link:** nipositionality.cs.washington.edu/
- **Paper:** bit.ly/NLPositionality-Paper/

**Slide Title:** Thanks!

**Slide Content:**
- **Thank you!**</sample>
    <sample id="302">To handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="303">To address positive stereotypes and essentializing narratives, an intersectional lens is recommended.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are structurally matched but semantically different, and the language model judges them as acceptable.</sample>
    <sample id="305">This research critically examines weakly supervised learning (WSL) approaches, highlighting their reliance on clean validation data and the overestimation of their practicality. The study identifies that WSL methods often require clean samples for validation, which is not always feasible or practical, especially when dealing with noisy training data. The authors argue that while WSL can achieve high accuracy on clean test sets, the performance gap between different WSL methods remains significant, even after continuous fine-tuning (CFT). This suggests that the effectiveness of WSL depends heavily on the quality of the validation data used. The research questions whether clean validation data is necessary and how many clean samples are required by WSL approaches. It also explores strategies to use available clean samples more efficiently. The findings indicate that while WSL approaches benefit from more clean validation samples, using them for training, particularly with techniques like LoRA, can further enhance performance. The study concludes that reporting model selection criteria, using few-shot learning as baselines, and applying CFT are essential practices to ensure the practical application of WSL methods.</sample>
    <sample id="306">This research explores the capabilities of language models to track entities within discourse, focusing on their performance across various tasks and pretraining data types. The study highlights that while larger models like GPT-3.5 exhibit non-trivial entity tracking behavior, smaller models such as T5-base can also learn this ability through fine-tuning. The authors present a series of experiments using a box setup task, where models must track the movement of objects between boxes based on textual instructions. Results show that pretraining on both text and code data significantly enhances entity tracking abilities, with smaller models demonstrating comparable performance to larger ones. The findings suggest that entity tracking is not solely dependent on model size but rather on the quality and type of pretraining data. This work contributes valuable insights into the mechanisms underlying entity tracking in language models and opens avenues for further exploration into how these models generalize beyond specific training setups.</sample>
    <sample id="307">The authors used F1, accuracy, precision, recall, and macro-averaged F1 as evaluation metrics.</sample>
    <sample id="308">The research presented in this study explores the concept of "NLP Positionality," which examines how design biases in Natural Language Processing (NLP) datasets and models reflect the perspectives of their creators, influenced by demographics, identity, and life experiences. The study highlights that NLP datasets and models often align with English-speaking countries, reflecting a bias towards these regions. This alignment is evident in both social acceptability and toxicity metrics, where models show higher scores for English-speaking contexts compared to other linguistic groups.

The research also uncovers disparities among different demographic groups, such as non-binary individuals, who receive lower social acceptability ratings from AI models. This finding underscores the need for more inclusive NLP practices. To address these issues, the study recommends several strategies: maintaining detailed records of dataset and model development choices, conducting NLP research through a perspectivist lens, sharing disaggregated dataset labels, using modeling techniques to manage annotator disagreement, and building specialized datasets and models tailored to specific communities.

The study's findings emphasize the importance of addressing positionality in NLP to ensure fairness and inclusivity across diverse populations. By implementing these recommendations, researchers can work towards creating more equitable and representative NLP technologies.</sample>
    <sample id="309">Krippendorff's Alpha</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="312">MultiInstruct is the first multimodal instruction tuning benchmark dataset, containing 62 diverse multimodal tasks from 10 broad groups with 5 expert-written instructions each.</sample>
    <sample id="313">3</sample>
    <sample id="314">Binary coordination involves two conjuncts, where one conjunct is the governor and the other is the complement.</sample>
    <sample id="315">The average length of the prompts used in this study was 10 words.</sample>
    <sample id="316">The findings imply that the smaller T5 model can generate higher quality scripts than larger LLMs when fine-tuned on the Coscript dataset.</sample>
    <sample id="317">**Abstract:**

This research explores the effectiveness of large code generation models (LLMs) for few-shot information extraction (IE). The study introduces CodeIE, a novel framework that leverages LLMs to perform IE tasks with minimal training data. Unlike traditional text-to-text methods, which often struggle with structured output formats and require specific decoding strategies, CodeIE utilizes code as both input and output format, enhancing control over the generation process. The framework is demonstrated through a structured entity recognition task, where it outperforms previous methods by achieving higher precision and recall rates. The analysis section highlights the structural consistency between the input prompt and model predictions, supported by perplexity measurements. Additionally, the study evaluates the performance across various datasets and prompts, showing consistent improvements with code-based prompting. The results indicate that CodeIE not only achieves superior performance but also maintains a high level of structure fidelity, making it a promising approach for few-shot IE tasks.</sample>
    <sample id="319">From scratch vs. continual pre-training on 4GB of data</sample>
    <sample id="320">The factor of overfitting due to test reuse is 1.43.</sample>
    <sample id="321">The quality of the simplification was evaluated using automatic alignment and simplification.</sample>
    <sample id="322">This research explores how text classifiers, particularly those trained on large datasets like ALM and BLM, learn about morality. The study delves into the moral foundations theory, which posits that humans have six core moral principles: care, fairness, loyalty, authority, purity, and liberty. These principles are often used to evaluate the morality of actions or statements.

The research highlights that while ALM and BLM generally share a similar value rhetoric, they differ significantly in their approach to subversion. ALM tends to frown upon subversion, viewing it as destructive and chaotic, whereas BLM encourages subversion as a means to challenge and change existing power structures. This difference is crucial because it influences how these classifiers interpret and categorize texts related to moral issues.

The study aims to explain how these classifiers make decisions based on these principles and how they handle the element of subversion differently. By understanding these distinctions, researchers can better design and interpret AI systems that interact with human moral judgments, ensuring they align with societal values and ethical standards.</sample>
    <sample id="323">This research paper, presented at ACL 2023, focuses on developing a novel framework for Commonsense Question Answering (CQA) that leverages dynamic heterogeneous graph reasoning with language models and knowledge representation learning. The study addresses the challenges of retrieving relevant knowledge from external sources and integrating it with natural language understanding to answer questions that rely on common knowledge.

The proposed method, DHLK, builds a heterogeneous knowledge graph (HKG) based on multiple knowledge bases, optimizing its structure and knowledge representations through a two-stage process: structural and knowledge representation learning. This approach ensures that semantic relationships between entities are preserved, enhancing the model's ability to understand complex queries. The KG2QA layer incorporates path information from the HKG into the QA context, while the KRL module uses TransE to optimize entity and relationship embeddings within the HKG. The LM encoder and dynamic pruning module further refine the model by encoding QA contexts and subgraph entities, dynamically pruning less relevant information based on the LM's attention weights.

Experimental results on official test sets of CommonsenseQA and OpenBookQA datasets demonstrate the effectiveness of DHLK, achieving competitive performance compared to state-of-the-art methods. The study highlights the importance of integrating diverse knowledge sources and leveraging advanced graph neural network techniques to improve commonsense question answering capabilities.</sample>
    <sample id="324">Yes, language models can have different political biases.</sample>
    <sample id="326">two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent</sample>
    <sample id="327">The research presented at ACL 2023 focuses on ManagerTower, a novel architecture designed to enhance vision-language representation learning by aggregating insights from uni-modal experts across different modalities. The study aims to train a smart AI system capable of understanding both images and text through large-scale self-supervised pre-training on image-text pairs. The core approach involves a Two-Tower architecture that integrates a textual encoder, visual encoder, and cross-modal encoder, facilitating the refinement of multi-modal representations. This method is compared with BridgeTower, highlighting its limitations such as ineffective layer-by-layer utilization and the number of cross-modal layers being tied to the number of uni-modal layer representations used.

ManagerTower addresses these limitations by taking multi-layer uni-modal representations as insights from pre-trained uni-modal experts at different levels and adaptively aggregating these insights via managers within each cross-modal layer. This adaptive aggregation mechanism allows for more efficient and effective integration of knowledge from various modalities, leading to superior performance in tasks like visual question answering, refining, multi-modal verification, and caption-based image retrieval. The results demonstrate significant improvements over existing models, particularly in terms of accuracy and efficiency, making ManagerTower a promising advancement in the field of vision-language learning.</sample>
    <sample id="328">GPT-3</sample>
    <sample id="329">This research introduces a novel zero-shot video sentence localization method that leverages structured pseudo-labels to enhance robustness against noise. The method begins by generating free-form pseudo-queries using image description models and then selects event proposals based on their temporal structure, ensuring high relevance to the query. It calculates similarity scores between pseudo-queries and video frames, filtering out low-quality pairs and retaining those with high event quality. The approach also employs sample re-weighting and label refinement techniques to mitigate noise in the pseudo-labels, ultimately leading to superior performance compared to existing methods. This study demonstrates state-of-the-art results on two datasets, highlighting its effectiveness in handling zero-shot video sentence localization tasks.</sample>
    <sample id="330">No, iterative update performs better than cumulative training in active learning for dissonance detection.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">The data for the MuDa benchmark was taken from the Multilingual Discourse-Aware (MuDA) benchmark.</sample>
    <sample id="333">This research introduces a novel training framework, INK (Injecting kNN Knowledge), designed to refine the representation space of Neural Machine Translation (NMT) models by leveraging k-Nearest Neighbor (kNN) knowledge. The study addresses the limitations of NMT models, particularly their non-smooth representation spaces which hinder performance on unseen domains. INK aims to mitigate this issue by iteratively refining the representation space using kNN knowledge, thereby smoothing predictions and enhancing translation accuracy.

The framework employs an asynchronous refresh mechanism that updates the datastore with new representations while maintaining the smoothness of the representation space. This approach allows for efficient adaptation without the need for large datastores, significantly reducing memory usage and inference time. The INK system demonstrates superior performance compared to existing kNN-MT baselines, achieving an average gain of 1.99 COMET and 1.0 BLEU across various target domains including Medical, Law, IT, and Koran. Additionally, the use of an adapter further enhances performance, leading to even greater improvements in BLEU scores.

The research also explores the effectiveness of combining the adapter and datastore, showing that together they can achieve better results than either alone. Overall, the INK system not only improves translation performance but also offers significant advantages in terms of memory efficiency and inference speed, making it a promising solution for practical applications in machine translation.</sample>
    <sample id="334">**Slide Title:** Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination

**Slide Content:**
- **Authors:** Adam Przepiórkowski and Michał Woźniak
- **Institute:** Institute of Computer Science, Polish Academy of Sciences
- **Conference:** ACL 2023

**Dependency Structure of Coordination:**
- **Bouquet/Stanford (Universal Dependencies):**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows dependencies with arrows indicating relationships between words.
- **Chain/Moscow:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows dependencies with arrows indicating relationships between words.
- **Conjunction-headed/Prague:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows dependencies with arrows indicating relationships between words.
- **Multi-headed/London:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows dependencies with arrows indicating relationships between words.

**Dependency Length Minimization (DLM):**
- **Word order tends to minimize dependency lengths:**
  - Example sentences:
    - "Marge read it yesterday."
    - "Marge read this absolutely fascinating book about bees yesterday."
  - Diagrams: Show dependency structures with arrows indicating relationships between words.
  - **Good:** "Marge read it yesterday."
  - **Bad:** "Marge read yesterday this absolutely fascinating book about bees."

**Conjunct Lengths in English:**
- Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al., 1993; Ficler &amp; Goldberg, 2016):
  - Left conjuncts tend to be shorter (observed before).
  - This tendency grows with length difference (briefly noticed in Gibson et al., 1996: 88–90).
  - But only when the governor is on the left or absent ("I saw Bart and Lisa; Homer came and sneezed").
  - Not when it is on the right ("Ted and Ned laughed").

**Figure 1:** Proportions of shorter left conjuncts depending on the absolute difference of conjunct lengths (with confidence bands).

**Compatibility with Dependency Structures of Coordination:**
- **Bouquet/Stanford (Universal Dependencies):**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows compatibility with dependency structure.
  - **Result:** NO
- **Chain/Moscow:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows compatibility with dependency structure.
  - **Result:** NO
- **Conjunction-headed/Prague:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows compatibility with dependency structure.
  - **Result:** YES
- **Multi-headed/London:**
  - Sentence: "Homer loves Lisa, Bart, and Maggie."
  - Diagram: Shows compatibility with dependency structure.
  - **Result:** YES

**End Slide:**
- **Text:** "See the paper for the full argument! Talk to us at the poster session!"</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="337">This research paper, presented at ACL 2023 and authored by Ziran Liang, Yuyin Lu, Hegang Chen, and Yanghui Rao from Sun Yat-sen University, introduces a novel approach to out-of-vocabulary (OOV) word embedding learning using graph-based relation mining. The study highlights how human study habits, particularly the associative memory process, can be leveraged to enhance the understanding and representation of OOV words.

The authors propose a method that constructs a word relationship graph, where nodes represent words and edges denote their semantic relationships. This graph is then used to infer embeddings for OOV words by leveraging the connections with known words. The model architecture integrates a GCN layer for capturing local relationships and a GAT layer for global context, followed by a masked language modeling task to refine the embeddings.

The effectiveness of this approach is demonstrated through comprehensive evaluations on various datasets, including intrinsic measures like word similarity and extrinsic tasks such as named entity recognition and part-of-speech tagging. The results show significant improvements over baseline models, especially when dealing with complex word formations typical in agglutinative languages like Japanese or Korean.

The conclusion underscores the versatility of the proposed method, which can handle diverse word structures and perform well across different languages, provided the words can be reasonably segmented. The study's findings suggest that the graph structure within the GRM framework is crucial for its adaptability to various linguistic complexities.</sample>
    <sample id="338">This research explores the effectiveness of human natural language explanations (NLG) in enhancing model performance and reasoning, particularly in the context of fine-tuning machine learning models. The study introduces a unified structure for evaluating NLG explanations across different tasks and datasets, aiming to minimize the influence of varying tasks and models. It proposes a new metric called TREU (TRE for Explanation Utility), which evaluates the faithfulness of explanations by measuring their impact on prediction accuracy at both fine-tuning and inference stages.

The research utilizes two datasets, CoS-E and ECQA, to conduct preliminary experiments. These experiments demonstrate that while CoS-E explanations are less beneficial than ECQA explanations on baseline models, incorporating explanations during fine-tuning can still be advantageous, even with limited data. The findings suggest that the helpfulness of human explanations depends on the task and explanation style, such as the use of counter-factual writing styles for contradiction detection.

The study also discusses the challenges of high-quality human annotation, which is costly and difficult to acquire, and recommends quality checks during future data collection efforts. Overall, this work contributes to the development of more effective and reliable evaluation metrics for human explanations in AI systems, paving the way for better integration of human insights into automated decision-making processes.</sample>
    <sample id="339">The affiliations of the authors are Saarland University, Amazon Alexa, and University of Vienna.</sample>
    <sample id="340">ParaAMR is a large-scale, syntactically diverse paraphrase dataset constructed through AMR back-translation, offering substantial benefits for natural language processing (NLP) applications such as question answering, chatbots, creative generation, data augmentation, and robustness. The dataset's construction leverages the Abstract Meaning Representation (AMR) framework, which captures the semantic structure of sentences as directed graphs. This approach ensures that ParaAMR is both semantically rich and syntactically diverse, making it ideal for tasks requiring nuanced understanding and flexible expression.

The dataset's utility extends beyond its creation method; it supports various NLP applications by providing a robust foundation for paraphrase generation, sentence embeddings, and data augmentation. ParaAMR's syntactic diversity is quantitatively analyzed using metrics like Semantic Textual Similarity (STS), showing competitive performance compared to existing datasets. Additionally, the dataset's syntactic diversity scores are higher than those of other datasets, indicating its effectiveness in capturing a wide range of syntactic structures without compromising on semantic similarity.

In conclusion, ParaAMR represents a significant advancement in NLP resources, offering researchers and practitioners a powerful tool for enhancing their models' capabilities in understanding and generating human language.</sample>
    <sample id="341">The authors use 1s, 2s, and 3s as latency measures.</sample>
    <sample id="342">This research introduces LiveChat, a large-scale personalized dialogue dataset derived from live streaming videos, focusing on the unique challenges and opportunities within this domain. The dataset is constructed by automatically transcribing live streaming videos into utterances, matching streamer responses with audience comments, and labeling persona information based on historical posts and video content. This approach leverages the rich context provided by live streams to create a more personalized and relevant dialogue experience.

The study highlights the distinctiveness of the video-sourced dialogue domain compared to existing text-based datasets, emphasizing the importance of detailed persona profiles and longer conversations for personalized dialogue systems. It also addresses the scarcity of Chinese multi-party dialogue corpora, which is a key barrier in developing effective dialogue models.

Experimental results demonstrate that the selected persona profiles and the larger number of average sessions per persona are advantageous in learning the speaker's personalized response and addressee decision. The comparisons between BART and other pre-trained dialogue models and LLMs reveal the distinctiveness of this video-sourced dialogue domain. The research concludes by proposing efficient transfer learning of LLMs for LiveChat, aiming to enhance the performance of dialogue systems in this domain.</sample>
    <sample id="344">The drawbacks of tree-based methods include the need for pre/post-processing logical forms and grammar induction, which can be complex and time-consuming processes.</sample>
    <sample id="345">This research paper introduces a novel approach to compositional generalization in semantic parsing, focusing on handling deeper recursion and unseen compositions of phrases that have been seen individually during training. The study addresses the limitations of naive seq2seq models by proposing a method that directly models the correspondences between fragments without relying on trees. This approach is particularly significant as it demonstrates strong generalization to deeper recursion for the first time.

The authors present their method within the context of neural seq2seq models, which are capable of learning from multiset tagging and latent permutations. They highlight the challenges associated with obtaining trees, such as pre/post-processing logical forms and grammar induction, and introduce a permutation model to address these issues. The permutation model is designed to induce alignment in training, making inference NP-hard but solvable through continuous relaxation.

The effectiveness of this approach is demonstrated through experimental results on COGS (Kim and Linzen 2020), where the proposed model outperforms other treeless models in structural generalization tasks. The paper concludes with a discussion of technical challenges, including the need for alignment induction in training and the complexity of inference, suggesting potential solutions like backpropagation through continuous relaxation.

Overall, this paper contributes a significant advancement in semantic parsing by providing a robust framework for handling complex linguistic structures without the need for explicit tree representations, thereby enhancing the model's ability to generalize across diverse and intricate sentence compositions.</sample>
    <sample id="346">School of Interactive Computing, Georgia Institute of Technology</sample>
    <sample id="348">This research explores the use of natural language prompts to measure stereotypes within large language models (LLMs), focusing on the intersectionality of social biases and stereotypes. The study highlights the limitations of existing stereotype measures, such as their specificity versus generalizability tradeoff, reliance on fixed hand-curated datasets, and failure to account for intersectionality. To address these issues, the researchers propose a method called "Marked Personas," which involves generating personas using prompts that imagine individuals belonging to specific identity groups. These personas are then analyzed to identify words that distinguish marked groups from unmarked ones.

The study demonstrates that LLMs, particularly GPT-3.5 and GPT-4, can respond to instructions in prompts, allowing for the creation of diverse persona descriptions. For instance, personas like "an Asian woman" or "a Middle Eastern woman" are generated, showcasing how LLMs can produce detailed descriptions that reflect various identities. The personas are evaluated based on their ability to capture intersectional identities, providing insights into how LLMs perceive and represent different social groups.

The findings reveal that generated personas contain more stereotypes than human responses, indicating a potential bias in how LLMs process and generate text related to marginalized groups. This study underscores the importance of developing methods to mitigate these biases and improve the representation of diverse identities within AI systems. Recommendations include adopting an intersectional lens and increasing transparency about bias mitigation strategies to enhance the fairness and inclusivity of AI technologies.</sample>
    <sample id="350">This presentation explores the concept of superhuman performance in natural language understanding (NLU) systems, particularly focusing on the SuperGLUE benchmark and its implications for evaluating NLU capabilities. The speaker discusses how leaderboard-based evaluation has become prevalent in NLP, leading to claims of superhuman performance and the idea that certain tasks have been solved. However, the presentation highlights several issues with this approach, including the ease of outperforming humans on simple procedural tasks like arithmetic and memory-intensive tasks, while most NLU tasks require knowledge and inference. It also addresses the brittleness of models, evidenced by out-of-domain generalization, adversarial attacks, and sensitivity to linguistic perturbations.

The SuperGLUE benchmark is introduced as a well-known framework for evaluating general-purpose language understanding models, listing ten tasks such as Word in Context, Multi-Sentence Reading Comprehension, and Commitment Bank. The presentation notes that human baselines often outperform models on six out of these ten tasks, suggesting that current benchmarks may not fully capture the complexity required for true superhuman performance. Additionally, the SQuAD benchmarks are discussed, showing that humans are largely outperformed by systems on both SQuAD1.1 and 2.0 datasets.

The speaker emphasizes the need for more transparent and fairer benchmarks, highlighting issues related to the evaluation data, system performance, measurement process, and human annotators' quality. The absence of detailed information about the annotator pool, such as hourly pay rates and training guidelines, raises concerns about the quality of the training phase. The presentation concludes by discussing the consequences of these issues and providing recommendations for constructing more transparent and fair benchmarks, aiming to address the gap between systems and human performance.</sample>
    <sample id="351">This research explores the effectiveness of CoNLL-2003 Named Entity Taggers in 2023, questioning whether models trained on this dataset still perform well with modern data. The study delves into the challenges of generalization and performance degradation over time, examining factors such as model architecture, size, and fine-tuning examples. It highlights that while transformer models generally outperform others due to better generalization capabilities, larger models also tend to generalize better. The research further investigates potential causes for performance drops, including adaptive overfitting and temporal drift, concluding that temporal drift is the primary factor affecting performance. The study suggests that for improved generalization, better model architectures, larger model sizes, and more fine-tuning examples are necessary. The dataset used, CoNLL++ from 2020, was annotated according to CoNLL-2003 guidelines, and the performance of various models was evaluated on both CoNLL-2003 and CoNLL++. The findings indicate that while CoNLL-2003 taggers may not be optimal for current tasks, they remain relevant for historical context and comparison.</sample>
    <sample id="352">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="353">This research explores the integration of interactive feedback into code generation, specifically focusing on generating Python code by asking clarification questions. The study addresses the challenge of input underspecification in natural language descriptions (NLDs), where NLDs often lack specific details necessary for accurate code generation. To tackle this issue, the authors propose a method that leverages interaction through clarification questions and answers (CQAs) to gather more specifications, thereby alleviating the problem of underspecification.

The paper introduces the concept of "clarification questions" as a means to interactively refine the input NLDs, aiming to generate more precise code. It outlines a pipeline for CQ-driven code generation, including a clarification need predictor, a question selector, and a code generator. The dataset used for training and evaluation is created using heuristics to identify key operations from the code knowledge graph generated by GraphCode, a tool that extracts key operations from code documentation.

The study demonstrates that incorporating interactive feedback through CQAs leads to better code generation performance, with higher F1 scores compared to baseline methods. The pipeline outperforms existing models trained solely on NLDs, indicating its effectiveness in handling underspecified inputs. The authors also present an example of predictions made by their model, showing how oracle CQAs can lead to predictions closer to the ground truth, especially when operations have different argument-level specifications.

Overall, the research highlights the potential of interactive feedback in improving code generation accuracy, particularly in scenarios where NLDs are underspecified.</sample>
    <sample id="354">Until 2016</sample>
    <sample id="356">The affiliations of the authors of the paper are The University of Amsterdam, Saarland University, and the University of California, San Diego.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">5</sample>
    <sample id="359">The approach is compared to the wait-k architecture.</sample>
    <sample id="360">**Slide Title:** MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning

**Slide Content:**
- **Authors:** Zhiyang Xu*, Ying Shen*, Lifu Huang, Department of Computer Science, Virginia Tech (Equal Contribution)
- **Abstract:** This slide introduces the concept of MultiInstruct, which aims to improve multi-modal zero-shot learning through instruction tuning.
- **Figure 2: Comparing Instruction Tuning with Pretrain-Finetune and Prompting**
  - **(A) Pretrain-Finetune (BERT, T5):**
    - Typically requires many fine-tuning tasks for each task.
  - **(B) Prompting (GPT-3):**
    - Requires few-shot prompting on each task.
  - **(C) Instruction Tuning (FLAN):**
    - Instruction tune on pre-trained LMs.
    - Model learns to perform downstream tasks given language instructions.

**Slide Title:** Language-only

**Slide Content:**
- **Instruction Tuning on Multimodal Pre-trained Models**

**Slide Title:** Imbalance in Instructional Datasets between NLP and Multimodal

**Slide Content:**
- **1600+ Language-only instruction tasks**
- **No large-scale, publicly-available multimodal instruction tasks**

**Slide Title:** MultiInstruct

**Slide Content:**
- **The first multimodal instruction tuning benchmark dataset**
  - 62 diverse multimodal tasks
  - 10 broad groups
  - 5 expert-written instructions

**Slide Title:** OFA (One For All)

**Slide Content:**
- A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities.
- OFA has a unified vocabulary for language, image tokens, and the coordinates of a bounding box.

**Slide Title:** Example Instances from MultiInstruct for Four Tasks

**Slide Content:**
- **Grounded Caption:** Generate a caption for the image.
- **Text Localization:** Select the region that contains the text "Hello World".
- **Refining Expression Selection:** Select the region of the object that contains the train in the front.
- **Question Image Matching:** Given the content of the image, provide the correct information to answer "Is it a mountain?".

**Slide Title:** Multi-Modal Instruction Tuning

**Slide Content:**
- **Training Dataset Construction:**
  - Use 53 tasks for 9 groups for training.
  - Sampled 10 instances per task.
- **Testing Dataset Construction:**
  - Reserve the entire Commonsense Reasoning group for testing.
  - Select additional 5 tasks from VQA and Miscellaneous groups.
  - Use all the instances in the test split for each task.
  - Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.

**Slide Title:** Implementation Details

**Slide Content:**
- **Training details:**
  - Pre-trained OFA-Large model (472M).
  - Mix all the instances for all tasks.
  - Each instance is randomly combined with one of its five instruction templates.
- **Testing details:**
  - For each task, conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
  - Report the mean and maximum performance and the standard deviation of the performance across all five experiments.

**Slide Title:** Evaluation Metrics

**Slide Content:**
- For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, Disaster Type Classification), report Accuracy.
- For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, Visual Dialogue), report Rouge-L.
- For NLP tasks, report Rouge-L.
- Also compute aggregated performance based on the mean of the model's performance on all multimodal and NLP unseen tasks.

**Slide Title:** Sensitivity

**Slide Content:**
- How sensitive the model is towards variety of instructions for the same task:
  - Ability to consistently produce the same results for the same task regardless of slight variations in the wording of instructions.
- Mathematical formula: E_{i\in T} \left[ \sigma_{i\in I^*} \left[ E_{(x,y)\in D^i} \left[ \mathcal{L}(f_\theta(i,x,y)) \right] \right] \right]

**Slide Title:** Effectiveness of Instruction Tuning on MultiInstruct

**Slide Content:**
- OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.
- Table 1: Effect of different numbers of instructions on performance.

**Slide Title:** Effect of Fine-tuning Strategies on Model Sensitivity

**Slide Content:**
- Instruction tuning on MultiInstruct can significantly reduce the sensitivity of OFA.
- Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.
- Figure 4: Model Sensitivity on Unseen Evaluation Tasks.

**Slide Title:** Zero-Shot Performance on NLP Tasks

**Slide Content:**
- Instruction Tuning on MultiInstruct can improve zero-shot performance on unseen NLP tasks.
- The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.

**Slide Title:** Conclusion

**Slide Content:**
- First large-scale multi-modal instruction tuning dataset containing 62 multi-modal tasks from 10 broad categories.
- Significantly improve the zero-shot capability of OFA via instruction tuning.
- Explore several transferring learning techniques and show their benefits.
- Design a new metric sensitivity.

**Slide Title:** One More Thing!

**Slide Content:**
- We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and will release them soon!</sample>
    <sample id="361">The research presented focuses on enhancing multi-step quantitative reasoning through the use of counterfactual contrast, specifically within the context of financial data analysis. The study introduces CounterComp, a method that leverages counterfactual examples to improve compositional generalization for complex numerical reasoning tasks. This approach is demonstrated using a dataset called FinQA, which involves financial statements and requires understanding of operations like addition, subtraction, multiplication, and division.

The presentation highlights the challenges of compositional generalization, particularly as the number of reasoning steps increases, and discusses the long-tail issue where performance drops significantly with more complex programs. It also explores how questions can function as counterfactual examples, providing insights into the types of operations and operands involved in solving these problems. The methodology involves metric learning using these counterfactual examples to enhance model performance.

Empirical results show that CounterComp improves program accuracy on both in-distribution and out-of-distribution samples, especially for multi-step reasoning tasks. The study concludes by emphasizing the importance of counterfactual examples in addressing the long-tail issue and improving the robustness of models in handling complex financial reasoning tasks.</sample>
  </task>
</testset>