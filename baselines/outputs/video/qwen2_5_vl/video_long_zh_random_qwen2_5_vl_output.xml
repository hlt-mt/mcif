<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">è¯­è¨€æ¨¡å‹çš„ä¸»è¦æ•°æ®æ¥æºæ˜¯æ–°é—»åª’ä½“å’Œç¤¾äº¤åª’ä½“ã€‚</sample>
    <sample id="1">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬McGill Universityã€Milaå’ŒMicrosoft Researchã€‚</sample>
    <sample id="2">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†ä¸€ç§åä¸ºLayoutMaskçš„å¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³è§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£ä¸­çš„é˜…è¯»é¡ºåºé—®é¢˜ã€‚è¯¥æ¨¡å‹é€šè¿‡ä½¿ç”¨å±€éƒ¨IDä½ç½®ä»£æ›¿å…¨å±€IDä½ç½®ï¼Œå¹¶é‡‡ç”¨æ–°é¢–çš„æ©ç ç­–ç•¥å’Œé¢„è®­ç»ƒç›®æ ‡æ¥å¢å¼ºæ–‡æœ¬å¸ƒå±€äº¤äº’ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨F1åˆ†æ•°ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨å±€éƒ¨IDä½ç½®å’Œå±€éƒ¨æ®µè½æ©ç ç­–ç•¥æ—¶ï¼Œæ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œå¹³å‡F1åˆ†æ•°åˆ†åˆ«ä¸º92.30%ã€96.48%å’Œ96.56%ï¼Œè¿œè¶…å…¶ä»–ç»„åˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒIDä½ç½®å’Œ2Dä½ç½®ç»„åˆå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å±€éƒ¨IDä½ç½®å’Œå±€éƒ¨æ®µè½æ©ç ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</sample>
    <sample id="3">æ”¶åˆ°çš„è‹±æ–‡å†…å®¹æ˜¯å…³äºDEPLAINé¡¹ç›®çš„ç ”ç©¶æŠ¥å‘Šï¼Œè¯¥æŠ¥å‘Šä»‹ç»äº†DEPLAINï¼šä¸€ä¸ªç”¨äºå¥å­å’Œæ–‡æ¡£ç®€åŒ–ä»»åŠ¡çš„å¾·è¯­å¹³è¡Œè¯­æ–™åº“ï¼Œå…¶ä¸­åŒ…å«å¾·è¯­åŸæ–‡ä¸ç®€åŒ–åçš„å¾·è¯­æ–‡æœ¬å¯¹é½ã€‚ç ”ç©¶å›¢é˜ŸåŒ…æ‹¬Regina Stoddenã€Omar Momenå’ŒLaura Kallmeyerï¼Œæ¥è‡ªå¾·å›½æµ·å› é‡Œå¸ŒÂ·æµ·æ¶…å¤§å­¦ã€‚æŠ¥å‘ŠæŒ‡å‡ºï¼ŒDEPLAINé¡¹ç›®çš„ç›®æ ‡æ˜¯ä¸ºç®€åŒ–å¾·è¯­æ–‡æœ¬æä¾›é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨ACL 2023ä¼šè®®ä¸Šè¿›è¡Œäº†å±•ç¤ºã€‚</sample>
    <sample id="4">æ¼”è®²è€…çš„åå­—æ˜¯Kayo Yinã€‚</sample>
    <sample id="5">ä»–ä»¬ä½¿ç”¨çš„æ˜¯ T5 XL æ¨¡å‹ã€‚</sample>
    <sample id="6">è¯¥ç ”ç©¶æ—¨åœ¨ç»Ÿä¸€å¤šè¯­è¨€å’Œè·¨è¯­è¨€æ‘˜è¦ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMany-to-many Summarization (M2MS) çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†ä»»æ„æºè¯­è¨€å¹¶ç”Ÿæˆç›®æ ‡è¯­è¨€çš„æ‘˜è¦ã€‚ç ”ç©¶é€šè¿‡ä¸‰ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€å»ºæ¨¡ã€è·¨è¯­è¨€èƒ½åŠ›å’Œæ‘˜è¦èƒ½åŠ›ã€‚å®éªŒåœ¨WikiLinguaæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œä½¿ç”¨mBART-50æ¨¡å‹å¯¹æ¯”CLSã€MLSå’ŒM2MSã€‚ç»“æœæ˜¾ç¤ºï¼ŒM2MSæ¨¡å‹åœ¨æ‰€æœ‰æ–¹å‘ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œè¡¨æ˜ç»Ÿä¸€æ‰€æœ‰æ–¹å‘çš„å•ä¸€æ¨¡å‹æœ‰åŠ©äºç›¸äº’å¸®åŠ©ã€‚</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well.</sample>
    <sample id="8">æå‡ºçš„äººå·¥è¯„ä¼°æ–¹æ³•ABC-Evalå…·æœ‰æ–°é¢–æ€§ï¼Œå› ä¸ºå®ƒé€šè¿‡æ ‡æ³¨å¯¹è¯ä¸­çš„è¡Œä¸ºæ¥è¯„ä¼°å¯¹è¯è´¨é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¾èµ–äºè‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæ›´å…¨é¢åœ°æ•æ‰åˆ°å¯¹è¯ä¸­çš„ç»†å¾®å·®åˆ«å’Œå¤æ‚æ€§ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚</sample>
    <sample id="9">ç°æœ‰å¼±ç›‘ç£æ–¹æ³•çš„æˆåŠŸåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºå¹²å‡€çš„éªŒè¯æ•°æ®ã€‚</sample>
    <sample id="10">1. ä¿æŒè‰¯å¥½çš„å­¦ä¹ ä¹ æƒ¯å’Œæ—¶é—´ç®¡ç†ã€‚
2. åšå¥½å……åˆ†çš„å‡†å¤‡ï¼ŒåŒ…æ‹¬å¤ä¹ å’Œé¢„ä¹ ã€‚
3. åœ¨è€ƒè¯•ä¸­ä¿æŒå†·é™ï¼Œåˆç†åˆ†é…æ—¶é—´ã€‚</sample>
    <sample id="11">è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆå’Œè§£é‡Šç¬‘è¯æ–¹é¢çš„èƒ½åŠ›ï¼Œä»¥åŠå®ƒä»¬æ˜¯å¦çœŸæ­£ç†è§£å¹½é»˜ã€‚ç ”ç©¶å¼•ç”¨äº†ã€Šçº½çº¦å®¢ã€‹çš„æ¼«ç”»æ ‡é¢˜ç«èµ›ï¼Œé€šè¿‡åˆ†æè¿™äº›ç«èµ›ä¸­çš„æ¼«ç”»å’Œç›¸åº”çš„æ ‡é¢˜ï¼Œè¯„ä¼°äº†æ¨¡å‹ç”Ÿæˆçš„æ ‡é¢˜ä¸äººç±»ç”Ÿæˆçš„æ ‡é¢˜ä¹‹é—´çš„åŒ¹é…åº¦ã€è´¨é‡æ’åä»¥åŠè§£é‡Šèƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå’Œè§£é‡Šç¬‘è¯ï¼Œä½†å®ƒä»¬çš„ç†è§£æ·±åº¦å’Œå¹½é»˜æ„Ÿä¸äººç±»ç›¸æ¯”ä»æœ‰å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†å¦‚ä½•é€šè¿‡é«˜è´¨é‡çš„æ•°æ®é›†å’Œæ ‡æ³¨æ¥è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¦‚ä½•æ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆå¹½é»˜ã€‚</sample>
    <sample id="12">è¿™ç¯‡è®ºæ–‡æœ‰äº”ä½ä½œè€…ã€‚</sample>
    <sample id="13">The presentation discusses the challenges and solutions in adaptive inference, particularly focusing on low-resource settings. It introduces the concept of 'conflicting gradients' in early exit models, where classifiers update model weights independently to optimize their own loss functions, leading to performance degradation across all classifiers. The study compares multi-model (MM) and early-exit (EE) methods, finding that MM models outperform EE by 2.3% on average, with the gap being largest for the earliest classifier. The paper proposes SWEET, an approach that separates weights in early exit transformers, effectively addressing conflicting gradients by ensuring each layer receives updates only from the following classifier. This method not only closes most of the gap between EE and MM but also maintains or improves accuracy for later classifiers. The results show significant speedup ratios, with SWEET closing the gap while maintaining high accuracy. The takeaway is that future classifiers' gradients are aligned, suggesting similar goals, and that EE provides a better speed-accuracy tradeoff compared to MM.</sample>
    <sample id="14">å½“ç„¶ï¼Œä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

**æ ‡é¢˜ï¼šè‹±è¯­ä¸­çš„è¿è¯é•¿åº¦ã€ä¾èµ–é•¿åº¦æœ€å°åŒ–å’Œåè°ƒç»“æ„**

**æ¼”è®²è€…ï¼šAdam PrzepiÃ³rkowski å’Œ MichaÅ‚ WoÅºniak**

**æ¼”è®²åœ°ç‚¹ï¼šæ³¢å…°ç§‘å­¦é™¢è®¡ç®—æœºç§‘å­¦ç ”ç©¶æ‰€**
**æ¼”è®²æ—¥æœŸï¼šACL 2023**

**å†…å®¹æè¦ï¼š**

- **è¿è¯é•¿åº¦åœ¨è‹±è¯­ä¸­çš„åº”ç”¨**
- **ä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰**
- **åè°ƒç»“æ„çš„å…¼å®¹æ€§**

---

å¸Œæœ›è¿™èƒ½å¸®åˆ°æ‚¨ï¼å¦‚æœæ‚¨éœ€è¦è¿›ä¸€æ­¥çš„å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚</sample>
    <sample id="15">ä¸‰ä½ã€‚</sample>
    <sample id="16">æ–°é—»ã€åœ£ç»å’Œç¬¬äºŒè¯­è¨€å­¦ä¹ çš„ç®€åŒ–ç¨‹åº¦æ›´å¤§ã€‚</sample>
    <sample id="17">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨å¤šæ¨¡æ€å…³ç³»æŠ½å–ä»»åŠ¡ä¸­ï¼Œå¦‚ä½•é€šè¿‡ä¿¡æ¯ç­›é€‰ä¸ä¿¡æ¯åˆ©ç”¨æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶é¦–å…ˆä»‹ç»äº†ä»»åŠ¡å®šä¹‰ï¼ŒåŒ…æ‹¬å…³ç³»æŠ½å–ï¼ˆREï¼‰å’Œå¤šæ¨¡æ€å…³ç³»æŠ½å–ï¼ˆMREï¼‰ï¼Œå¹¶æŒ‡å‡ºåœ¨ç¤¾äº¤åª’ä½“åœºæ™¯ä¸‹ï¼Œæ•°æ®é€šå¸¸ä¸ºæ··åˆæ¨¡æ€ä¸”æ–‡æœ¬é•¿åº¦è¾ƒçŸ­ã€‚æ¥ç€ï¼Œç ”ç©¶åˆ†æäº†å†…éƒ¨ä¿¡æ¯è¿‡ç”¨å’Œå¤–éƒ¨ä¿¡æ¯æœªå……åˆ†æŒ–æ˜çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†éœ€è¦è¿›è¡Œç»†ç²’åº¦çš„ä¿¡æ¯ç­›é€‰å’Œé¢å¤–è¯­ä¹‰è¡¥å……ä¿¡æ¯çš„éœ€æ±‚ã€‚

ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³åŒæ—¶è¿›è¡Œä¿¡æ¯å‡æ³•å’ŒåŠ æ³•ï¼Œä»¥å®ç°å¤šæ¨¡æ€å…³ç³»æŠ½å–ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºå›¾ä¿¡æ¯ç“¶é¢ˆåŸåˆ™çš„å†…éƒ¨ä¿¡æ¯ç­›é€‰è¿‡ç¨‹ï¼Œä»¥åŠä¸€ä¸ªæ½œåœ¨çš„å¤šæ¨¡æ€ä¸»é¢˜æ¨¡å‹ï¼Œç”¨äºä¸°å¯Œç‰¹å¾ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æœ€ä½³åŸºå‡†æ¨¡å‹ï¼Œåœ¨åŸºå‡†æ•°æ®ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†åœ¨ä¸åŒæƒ…å†µä¸‹ï¼Œå†…éƒ¨ä¿¡æ¯ç­›é€‰å’Œå¤–éƒ¨ä¿¡æ¯åˆ©ç”¨å¯¹ä»»åŠ¡æ€§èƒ½çš„å½±å“ï¼Œå¹¶æ€»ç»“äº†ç ”ç©¶çš„ä¸»è¦è´¡çŒ®ã€‚</sample>
    <sample id="18">I saw Bart and Lisa; Homer came and sneezed.</sample>
    <sample id="19">è¯¥æŠ¥å‘Šä¸»è¦æ¢è®¨äº†å¼€æ”¾é¢†åŸŸé—®ç­”ï¼ˆODQAï¼‰ç³»ç»Ÿçš„è®¾è®¡ä¸ä¼˜åŒ–ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é«˜æ•ˆæ€§è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚æŠ¥å‘Šé¦–å…ˆä»‹ç»äº†ODQAçš„åŸºæœ¬æ¦‚å¿µå’ŒæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µçš„æ¡†æ¶ï¼šæ£€ç´¢å™¨å’Œé˜…è¯»å™¨ã€‚éšåï¼Œè¯¦ç»†åˆ†æäº†ç°æœ‰ç³»ç»Ÿçš„ä¸‰ç§ä¸»è¦æ¶æ„ï¼šæ£€ç´¢å™¨-é˜…è¯»å™¨ã€æ£€ç´¢å™¨ä»…é™å’Œç”Ÿæˆå™¨ä»…é™ï¼Œå¹¶è®¨è®ºäº†å®ƒä»¬å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚æ¥ç€ï¼ŒæŠ¥å‘Šæ€»ç»“äº†å½“å‰ODQAç³»ç»Ÿä¸­çš„ä¸€äº›é«˜æ•ˆæŠ€æœ¯ï¼Œå¦‚å‡å°‘ç´¢å¼•å¤§å°ã€ä½¿ç”¨è½»é‡çº§æ¨¡å‹ã€å‚æ•°å…±äº«ç­‰æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜å¯¹æ¯”äº†ä¸åŒODQAç³»ç»Ÿçš„æ€§èƒ½ï¼ŒæŒ‡å‡ºåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç”Ÿæˆå™¨ä»…é™ç³»ç»Ÿæˆ–é€šè¿‡çŸ¥è¯†è’¸é¦å‡å°‘æ¨¡å‹å¤§å°ã€‚æœ€åï¼ŒæŠ¥å‘Šæå‡ºäº†æœªæ¥å·¥ä½œæ–¹å‘ï¼ŒåŒ…æ‹¬å¦‚ä½•å°†ODQAç³»ç»Ÿéƒ¨ç½²åˆ°ä½åŠŸè€—è®¾å¤‡ä¸Šä»¥åŠæ›´å¤šè¯„ä»·æŒ‡æ ‡çš„è€ƒé‡ã€‚</sample>
    <sample id="20">æ˜¯çš„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œæ‚¨çš„ç ”ç©¶ã€‚</sample>
    <sample id="21">DEplain-apa åŒ…å«æ–°é—»æ–‡ç« ã€‚</sample>
    <sample id="22">æ ¹æ®è§†é¢‘å†…å®¹ï¼Œä»¥ä¸‹å› ç´ æœ‰åŠ©äºè‰¯å¥½çš„æ³›åŒ–ï¼š

1. **æ›´å¥½çš„æ¨¡å‹æ¶æ„**ï¼šä½¿ç”¨Transformeræ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ³›åŒ–ã€‚
2. **æ›´å¤§çš„æ¨¡å‹è§„æ¨¡**ï¼šè¾ƒå¤§çš„æ¨¡å‹é€šå¸¸è¡¨ç°æ›´å¥½ã€‚
3. **æ›´å¤šçš„å¾®è°ƒç¤ºä¾‹**ï¼šæ›´å¤šçš„å¾®è°ƒç¤ºä¾‹æœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚</sample>
    <sample id="23">è¯¥ç ”ç©¶æ¢è®¨äº†å­—ç¬¦æ„ŸçŸ¥æ¨¡å‹åœ¨è§†è§‰æ–‡æœ¬æ¸²æŸ“ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡å±•ç¤ºå„ç§åˆ›æ„æ–‡å­—å›¾åƒï¼Œå¼ºè°ƒäº†æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡å›¾åƒæ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ä¸­æåˆ°çš„Imagenæ¨¡å‹ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå›¾åƒï¼Œä½†æŒ‡å‡ºæ–‡æœ¬ç¼–ç å™¨åœ¨æ‹¼å†™æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚é€šè¿‡åˆ†æä¸åŒè§„æ¨¡çš„æ¨¡å‹åœ¨æ‹¼å†™å‡†ç¡®æ€§ä¸Šçš„å·®å¼‚ï¼Œç ”ç©¶å‘ç°å­—ç¬¦æ„ŸçŸ¥ç¼–ç å™¨åœ¨æ‰€æœ‰è§„æ¨¡ä¸Šéƒ½èƒ½å¾ˆå¥½åœ°æ‹¼å†™ï¼Œè€Œå­è¯ç¼–ç å™¨å—å•è¯é¢‘ç‡å½±å“è¾ƒå¤§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ç»“åˆå­è¯çº§å’Œå­—ç¬¦çº§æ–‡æœ¬ç¼–ç çš„æ–¹æ³•ï¼Œä»¥æé«˜å›¾åƒç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†WikiSpellå’ŒDrawTextä½œä¸ºåŸºå‡†æ¨¡å‹çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ”¹è¿›æ¨¡å‹æ‹¼å†™èƒ½åŠ›çš„ç­–ç•¥ã€‚</sample>
    <sample id="24">é€šè¿‡è§‚å¯Ÿå¹¶åˆ—ç»“æ„ä¸­å·¦å¹¶åˆ—è¯çš„é•¿åº¦ï¼Œå¯ä»¥åˆ¤æ–­å·¦å¹¶åˆ—è¯æ˜¯å¦æ›´çŸ­ã€‚</sample>
    <sample id="25">é€šè¿‡å¯¹æ¯”ä¸åŒæ”¯é…è¯ä½ç½®çš„å¥å­ç»“æ„å’Œé•¿åº¦æ¥ç ”ç©¶ã€‚</sample>
    <sample id="26">The baseline classifier trained on the imbalanced dataset has an AUC of 0.51.</sample>
    <sample id="27">è¿™ç¯‡è®ºæ–‡æœ‰å››ä½ä½œè€…ã€‚</sample>
    <sample id="28">ç¤ºä¾‹å¯¹è¯ä¸­çš„è§’è‰²åå­—æ˜¯â€œEasy on Meâ€å’Œâ€œI Gotta Feelingâ€ã€‚</sample>
    <sample id="29">åœ¨å½¢å¼æ€§ã€è¯æ±‡è¿è´¯æ€§å’Œçœç•¥è¿™ä¸‰ä¸ªè¯è¯­ç°è±¡ä¸Šï¼Œè¯­å¢ƒæ„ŸçŸ¥ MT æ¨¡å‹æ¯”è¯­å¢ƒæ— å…³æ¨¡å‹æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</sample>
    <sample id="30">LLM-Blenderæ˜¯ä¸€ä¸ªç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç®€å•é›†æˆå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»„åˆå¤šä¸ªLLMsæ¥æé«˜æ•´ä½“æ€§èƒ½ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼šPairRankerå’ŒGenFuserã€‚PairRankeré€šè¿‡æ¯”è¾ƒæ¯ä¸ªå€™é€‰å¯¹å¹¶è¿›è¡Œæ’åæ¥é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œè€ŒGenFuseråˆ™èåˆäº†æ’åå‰Kä¸ªå€™é€‰çš„ç»“æœã€‚ç ”ç©¶å±•ç¤ºäº†åœ¨WMT-2018æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨MixInstructåŸºå‡†æµ‹è¯•æ—¶ï¼ŒLLM-Blenderèƒ½å¤Ÿæ˜¾è‘—æå‡ç°æœ‰LLMsçš„æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒMixInstructæä¾›äº†10ä¸‡æ¡æŒ‡ä»¤è·Ÿéšæ ·ä¾‹ï¼Œç”¨äºè¯„ä¼°LLMé›†æˆå­¦ä¹ çš„æ•ˆæœï¼Œå¹¶æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ä»£ç åº“ä¾›æœªæ¥å¼€å‘ä½¿ç”¨ã€‚</sample>
    <sample id="31">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬çº¦ç¿°éœæ™®é‡‘æ–¯å¤§å­¦ã€æ™®æ¸¡å¤§å­¦å’ŒMITã€‚</sample>
    <sample id="33">å¼•å…¥çš„æ¡†æ¶é€šè¿‡æ¯”è¾ƒä¸åŒç¾¤ä½“ï¼ˆå¦‚æ€§åˆ«ã€æ•™è‚²æ°´å¹³ç­‰ï¼‰çš„æ ‡æ³¨ç»“æœæ¥é‡åŒ–ç«‹åœºã€‚å…·ä½“æ¥è¯´ï¼Œæ¡†æ¶ä¼šæ”¶é›†æ¥è‡ªä¸åŒèƒŒæ™¯çš„æ ‡æ³¨è€…é‡æ–°æ ‡æ³¨çš„æ•°æ®ï¼Œå¹¶ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆPearson's R scoresï¼‰æ¥æ¯”è¾ƒè¿™äº›æ ‡æ³¨ç»“æœä¸ç°æœ‰æ•°æ®é›†å’Œæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚</sample>
    <sample id="34">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†CRESTï¼ˆContRastive Edits with Sparse rationalizationï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†é€‰æ‹©æ€§è§£é‡Šå’Œåäº‹å®ç”Ÿæˆçš„è”åˆæ¡†æ¶ã€‚CRESTæ—¨åœ¨è§£å†³å¦‚ä½•è§£é‡Šåˆ†ç±»å™¨å†³ç­–çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åŸå§‹æ–‡æœ¬ä¸ä¿®æ”¹åçš„æ–‡æœ¬æ¥ç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®è§£é‡Šã€‚è¯¥æ¡†æ¶ç”±ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ç»„æˆï¼šCREST-Generationå’ŒCREST-Rationalizationã€‚

CREST-Generationéƒ¨åˆ†ä½¿ç”¨å¯è®­ç»ƒæ©ç å™¨ä»ç»™å®šçš„æ–‡æœ¬ä¸­ç”Ÿæˆåäº‹å®æ–‡æœ¬ã€‚è¿™ä¸ªè¿‡ç¨‹é¦–å…ˆå°†è¾“å…¥æ–‡æœ¬é€å…¥æ©ç å™¨ï¼Œç„¶åé€šè¿‡é¢„æµ‹å™¨ç”Ÿæˆä¸€ä¸ªé¢„æµ‹ç»“æœã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡ç¼–è¾‘å™¨å°†é¢„æµ‹ç»“æœä¸åŸå§‹æ–‡æœ¬è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ç”Ÿæˆåäº‹å®æ–‡æœ¬ã€‚æœ€åï¼Œé€šè¿‡è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡å’Œäººç±»è¯„ä¼°æ¥éªŒè¯ç”Ÿæˆçš„åäº‹å®æ–‡æœ¬çš„æœ‰æ•ˆæ€§å’Œè‡ªç„¶æ€§ã€‚

CREST-Rationalizationéƒ¨åˆ†åˆ™é€šè¿‡åˆ©ç”¨äº‹å®å’Œåäº‹å®è¾“å…¥çš„é…å¯¹ç»“æ„æ¥è¿›ä¸€æ­¥ä¼˜åŒ–åäº‹å®æ–‡æœ¬çš„ç”Ÿæˆã€‚å®ƒé€šè¿‡å…±äº«æ©ç å™¨å’Œé¢„æµ‹å™¨æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»è€Œç¡®ä¿ç”Ÿæˆçš„åäº‹å®æ–‡æœ¬ä¸ä»…æœ‰æ•ˆä¸”è‡ªç„¶ï¼Œè¿˜èƒ½ä¿æŒè¾ƒé«˜çš„åäº‹å®ç›¸ä¼¼åº¦ã€‚

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCRESTåœ¨IMDBå’ŒSNLIæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨åäº‹å®ç›¸ä¼¼åº¦æ–¹é¢ã€‚æ­¤å¤–ï¼ŒCRESTç”Ÿæˆçš„è§£é‡Šè¢«è¯æ˜æ˜¯å¯è§£é‡Šçš„ï¼Œè¿™è¡¨æ˜CRESTä¸ä»…èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„åäº‹å®æ–‡æœ¬ï¼Œè¿˜èƒ½å¤Ÿæä¾›æ˜“äºç†è§£çš„è§£é‡Šã€‚

æ€»ç»“æ¥è¯´ï¼ŒCRESTæ¡†æ¶é€šè¿‡ç»“åˆé€‰æ‹©æ€§è§£é‡Šå’Œåäº‹å®ç”Ÿæˆï¼ŒæˆåŠŸåœ°è§£å†³äº†å¦‚ä½•è§£é‡Šåˆ†ç±»å™¨å†³ç­–çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</sample>
    <sample id="36">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦è®¨è®ºäº†å¤šè¯­è¨€æœºå™¨ç¿»è¯‘ï¼ˆMultilingual Machine Translationï¼‰çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡å­¦ä¹ è¯­è¨€ç‰¹å®šå±‚ï¼ˆLanguage-Specific Layers, LSLsï¼‰æ¥æ”¹è¿›è¿™ä¸€æŠ€æœ¯ã€‚é¦–å…ˆï¼Œä»‹ç»äº†å¤šè¯­è¨€æœºå™¨ç¿»è¯‘çš„å››ä¸ªä¼˜åŠ¿ï¼šå¯æ‰©å±•æ€§ã€é€Ÿåº¦ã€å‡å°‘é”™è¯¯ä¼ é€’å’Œä½èµ„æºæ”¹è¿›ã€‚æ¥ç€ï¼ŒæŒ‡å‡ºå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¯ç§è¯­è¨€çš„å®¹é‡é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä½¿ç”¨è¯­è¨€ç‰¹å®šå±‚ï¼ˆLSLsï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œå³åœ¨å…³é”®ä½ç½®å¢åŠ æ¯ç§è¯­è¨€çš„å®¹é‡ï¼Œå¹¶ä¿æŒæ¨ç†æˆæœ¬ä¸å˜ã€‚

å…·ä½“æ¥è¯´ï¼ŒLSLså¯ä»¥è¢«ç´¢å¼•ä¸ºæºè¯­è¨€æˆ–ç›®æ ‡è¯­è¨€ï¼Œä»è€Œæ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹çš„ç»“æ„ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†å¦‚ä½•è®©æ¨¡å‹è‡ªä¸»å†³å®šLSLsçš„ä½ç½®ï¼Œé€šè¿‡å…±äº«æƒé‡å’Œéå…±äº«æƒé‡çš„ç»„åˆæ¥å®ç°è¿™ä¸€ç‚¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨WMT21æ–°é—»ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›è¯­è¨€ä¸Šï¼Œæ”¹è¿›å¹…åº¦è¶…è¿‡10å€ã€‚æœ€åï¼Œå¼ºè°ƒäº†æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨å®Œæ•´è®ºæ–‡ä¸­æ‰¾åˆ°ï¼Œå¹¶å¯¹è§‚ä¼—è¡¨ç¤ºæ„Ÿè°¢ã€‚</sample>
    <sample id="37">åŸºäºå¿ƒç†å­¦ç ”ç©¶ã€‚</sample>
    <sample id="38">æ­¤ç ”ç©¶ä½¿ç”¨äº†å¢å¼ºç‰ˆçš„Penn Treebankæ•°æ®ã€‚</sample>
    <sample id="39">è¿™ç¯‡è®ºæ–‡æœ‰ä¸¤ä½ä½œè€…ã€‚</sample>
    <sample id="40">ä¸è®¤çŸ¥å¤±è°ƒå¯†åˆ‡ç›¸å…³çš„ä»»åŠ¡åŒ…æ‹¬æ€åº¦å’Œä¿¡å¿µè¶‹åŠ¿ã€ç„¦è™‘éšœç¢ä»¥åŠæç«¯ä¸»ä¹‰çš„è¿›å…¥å’Œé€€å‡ºã€‚</sample>
    <sample id="41">The presentation focuses on the development and application of PeaCoK, a world-level persona commonsense knowledge graph, to enhance narrative consistency and engagement in dialogue systems. The speaker outlines the importance of understanding personas in sustaining coherent narratives, highlighting that real-world personas involve rich world knowledge and various interaction possibilities.

PeaCoK is introduced as a comprehensive knowledge graph containing 100K persona facts, 3.8K personas, and 40K distinctive attributes, with 9.2K attributes connected to two or more personas. The knowledge graph is structured around four main relations: characteristic (intrinsic traits), routine/habit (regular behaviors), goal/plan (future actions or outcomes), and experience (past events or activities). It also emphasizes the importance of interactivity, where relationships involve interactions with others and self-referential statements.

The construction of PeaCoK involves three steps: persona selection, potential attribute induction, and relation classification. The speaker discusses the use of KG-based approaches and pre-trained language models like InstructGPT-3 for accurate annotations, which are both reliable and cost-effective. The quality of the annotations is evaluated using expert evaluation on relation annotation, achieving high accuracy across main relation, interactivity, and distinctiveness.

The presentation then explores how PeaCoK can be used to generalize persona knowledge through methods such as COMET-BART and GPT-3. Results from persona inference generation show significant improvements in fluency, consistency, engagement, and persona expression when using PeaCoK compared to baseline systems. The speaker concludes by emphasizing that PeaCoK enables lightweight language models to learn knowledge generation capabilities comparable to large-scale models, thereby enhancing narrative modeling in dialogue systems.</sample>
    <sample id="42">ä¸¤ä½</sample>
    <sample id="43">è¿™ç¯‡è®ºæ–‡æœ‰ä¸ƒä½ä½œè€…ã€‚</sample>
    <sample id="44">å¼•å…¥çš„æ¡†æ¶ä¸ä»¥å‰çš„ç ”ç©¶ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒä¸ä»…è€ƒè™‘äº†æ•°æ®é›†å’Œæ¨¡å‹çš„è®¾è®¡åè§ï¼Œè¿˜é€šè¿‡ä½¿ç”¨Perspective APIç­‰å·¥å…·æ¥é‡åŒ–è¿™äº›åè§ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜å¼ºè°ƒäº†åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ä¿æŒè®¾è®¡é€‰æ‹©è®°å½•çš„é‡è¦æ€§ï¼Œå¹¶å»ºè®®ä»é€è§†ä¸»ä¹‰çš„è§’åº¦è¿›è¡ŒNLPç ”ç©¶ã€‚</sample>
    <sample id="45">åœ¨ä¸‰ä¸ªæ¯”è¾ƒè®¾ç½®ä¸­ï¼ŒGPT-3.5 PBlack ä¸åˆ»æ¿è¯æ±‡çš„é‡å æœ€å¤šã€‚</sample>
    <sample id="46">æ¯”è¾ƒäº†DeepLå’ŒGoogleä¸¤ä¸ªå•†ä¸šç³»ç»Ÿã€‚</sample>
    <sample id="47">æ”¶åˆ°çš„è‹±æ–‡å†…å®¹æ˜¯å…³äºä»é¢„è®­ç»ƒæ•°æ®åˆ°è¯­è¨€æ¨¡å‹å†åˆ°ä¸‹æ¸¸ä»»åŠ¡çš„è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯è¿½è¸ªæ”¿æ²»åè§å¯¹ä¸å…¬å¹³NLPæ¨¡å‹çš„å½±å“ã€‚</sample>
    <sample id="48">å…­ä½</sample>
    <sample id="49">MPP è¯„ä¼°æœ€å¤šæ¶µç›–äº† 900 ä¸ªè¯å…ƒçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</sample>
    <sample id="50">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†DEPLAINé¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç®€åŒ–å¾·è¯­æ–‡æœ¬çš„å¹³è¡Œè¯­æ–™åº“ï¼ŒåŒ…å«åŸè¯­å’Œç®€åŒ–åçš„ç‰ˆæœ¬ã€‚é¡¹ç›®è¯¦ç»†è§£é‡Šäº†æ–‡æœ¬ç®€åŒ–çš„ç›®çš„ã€æ–¹æ³•ä»¥åŠä½¿ç”¨æ¡ˆä¾‹ï¼ŒåŒ…æ‹¬è‡ªåŠ¨å¯¹é½å’Œç®€åŒ–ã€‚é€šè¿‡å±•ç¤ºä¸åŒç±»å‹çš„ç®€åŒ–ç±»å‹å’Œç®€åŒ–è½¬æ¢ï¼Œä½œè€…å¼ºè°ƒäº†DEPLAINåœ¨ç®€åŒ–å¾·è¯­æ–‡æœ¬æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†ç›¸å…³æ•°æ®å’Œå›¾è¡¨æ”¯æŒã€‚æ­¤å¤–ï¼Œè¿˜æåˆ°äº†DEPLAINåœ¨æ–‡æ¡£å’Œå¥å­å±‚é¢çš„ç®€åŒ–æ•ˆæœï¼Œå¹¶é¼“åŠ±è§‚ä¼—æŸ¥çœ‹è®ºæ–‡å¹¶è®¿é—®ACL 2023ä¼šè®®ä¸Šçš„æµ·æŠ¥ã€‚</sample>
    <sample id="51">ä»–ä»¬çš„æ•°æ®é›†ä¸­åŒ…å«éŸ³ä¹ã€ä¹¦ç±å’Œé£Ÿè°±ä¸‰ä¸ªé¢†åŸŸã€‚</sample>
    <sample id="52">PositionalityæŒ‡çš„æ˜¯äººä»¬ç”±äºå…¶äººå£ç»Ÿè®¡å­¦ç‰¹å¾ã€èº«ä»½å’Œç”Ÿæ´»ç»å†è€ŒæŒæœ‰çš„è§‚ç‚¹ã€‚</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">The presentation focuses on the application of transfer and active learning in dissonance detection, particularly addressing the challenge of rare-class problems. It begins by defining cognitive dissonance as an inconsistency between beliefs and actions, drawing from the work of Harmon-Jones and Harmon-Jones (2007). The speaker explains how this concept is expressed linguistically through phrases that reflect such inconsistencies.

The presentation then delves into the theoretical underpinnings of dissonance, discussing its effects on attitudes and belief trends, and its role in entry and exit from extremism, anxiety disorders, and cognitive styles. It highlights the rarity of finding dissonance in language compared to other discourse relations, emphasizing its significance in psychological research.

Moving forward, the speaker outlines a method for annotating dissonance using a Twitter dataset, detailing steps like parsing quality checks and dissonance identification. This method involves a combination of human annotation and machine learning models, specifically using RoBERTA-base with a classifier head.

The core of the presentation discusses strategies for active learning, including cold-start annotations with transfer learning and iterative updates. It compares different active learning strategies based on their performance metrics, such as Area Under the ROC Curve (AUC), and concludes with a recommendation that PRC (Probability-of-Rare-Class) strategy is the most effective for rare class acquisition.

Finally, the presentation provides resources for further exploration, including contact information and links to code, datasets, and papers related to the topic.</sample>
    <sample id="55">æ˜¯çš„ï¼ŒEDAtt ä½¿ç”¨ç°æœ‰çš„ç¦»çº¿STæ¨¡å‹è€Œæ— éœ€é‡æ–°è®­ç»ƒæˆ–é‡‡ç”¨ç‰¹å®šæ¶æ„ã€‚</sample>
    <sample id="56">è¿™ç¯‡è®ºæ–‡æœ‰å››ä½ä½œè€…ã€‚</sample>
    <sample id="57">æ˜¯çš„ï¼Œè¢«æµ‹æ¨¡å‹å¯ä»¥åœ¨æµ‹è¯•å¥—ä»¶ä¸Šè¿è¡Œã€‚</sample>
    <sample id="58">KITMUS çš„ä¸‰ä¸ªå˜ä½“æ˜¯èƒŒæ™¯é¢„è®­ç»ƒã€èƒŒæ™¯-ä¸¤è€…å’ŒèƒŒæ™¯æ¨ç†ã€‚</sample>
    <sample id="59">è¯¥ç ”ç©¶ä»‹ç»äº†DrBERTï¼Œä¸€ç§é’ˆå¯¹æ³•è¯­ç”Ÿç‰©åŒ»å­¦å’Œä¸´åºŠé¢†åŸŸçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ç ”ç©¶é¦–å…ˆè®¨è®ºäº†è¯­è¨€å»ºæ¨¡åœ¨åŒ»ç–—ä¿å¥ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒçš„é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡ã€‚æ¥ç€ï¼Œç ”ç©¶è¯„ä¼°äº†13ä¸ªæ¨¡å‹åœ¨11é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬è¯­è¨€å»ºæ¨¡ã€å‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ç­‰ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†NACHOSå’ŒDrBERTçš„æ•°æ®åˆ†å¸ƒæƒ…å†µã€‚ç ”ç©¶å‘ç°ï¼ŒTransformeråŸºæ¨¡å‹å¦‚BERTåœ¨è®¸å¤šNLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸”å·²é€‚åº”åˆ°æ³•è¯­ï¼Œå¦‚CamemBERTå’ŒFlauBERTã€‚ç„¶è€Œï¼Œåœ¨åŒ»ç–—ä»»åŠ¡ä¸Šï¼ŒåŸºäºé€šç”¨æ¨¡å‹çš„é¢†åŸŸç‰¹å®šæ¨¡å‹åœ¨è‹±è¯­ä¸­è¡¨ç°æ›´å¥½ï¼Œå¦‚PubMedBERTã€‚ç ”ç©¶è¿˜æ¯”è¾ƒäº†å…¬å…±å’Œç§äººåŒ»ç–—æ•°æ®æºçš„å¤§å°ï¼Œå¹¶è¯„ä¼°äº†å­¦ä¹ ç­–ç•¥çš„å½±å“ã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†DrBERTåœ¨9ä¸ªä¸‹æ¸¸æ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸Šçš„ä¼˜å¼‚æ€§èƒ½ï¼Œè¶…è¿‡äº†CamemBERTé€šç”¨æ¨¡å‹å’ŒåŸºäºè‹±è¯­çš„é¢†åŸŸç‰¹å®šæ¨¡å‹ï¼Œå¹¶ç¡®è®¤äº†ä½¿ç”¨åŒ»ç–—ç‰¹å®šæ¨¡å‹åœ¨æ³•è¯­ä¸­çš„å®ç”¨æ€§ã€‚</sample>
    <sample id="60">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯Google Researchã€‚</sample>
    <sample id="61">å¦‚ä½•æ›´æœ‰æ•ˆåœ°ä½¿ç”¨å¯ç”¨çš„å¹²å‡€æ ·æœ¬ï¼Ÿ</sample>
    <sample id="62">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†çŸ¥è¯†è’¸é¦åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¼ªç›®æ ‡è®­ç»ƒçš„ç³»ç»Ÿæ€§ç ”ç©¶ã€‚ç ”ç©¶èƒŒæ™¯æŒ‡å‡ºï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ç³»ç»Ÿåœ¨è®¡ç®—ã€å­˜å‚¨å’Œè´¢åŠ¡éœ€æ±‚æ–¹é¢å…·æœ‰å·¨å¤§æŒ‘æˆ˜ï¼Œè€Œå·¥ä¸šç•Œå¯¹å‹ç¼©è¿™äº›æ¨¡å‹ä»¥ä¿æŒæ€§èƒ½çš„éœ€æ±‚æ—¥ç›Šå¢é•¿ã€‚ç ”ç©¶åŠ¨æœºåœ¨äºæ¢ç´¢å¦‚ä½•é€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚

ç ”ç©¶ä¸­ä»‹ç»äº†ä¸¤ç§ä¸»è¦çš„çŸ¥è¯†è’¸é¦æ–¹æ³•ï¼šè¯çº§çŸ¥è¯†è’¸é¦ï¼ˆWord-level KDï¼‰å’Œåºåˆ—çº§çŸ¥è¯†è’¸é¦ï¼ˆSequence-level KDï¼‰ï¼Œå¹¶è¯¦ç»†è§£é‡Šäº†è¿™ä¸¤ç§æ–¹æ³•çš„æ•°å­¦å…¬å¼ã€‚è¯çº§çŸ¥è¯†è’¸é¦æ—¨åœ¨è®©å­¦ç”Ÿæ¨¡ä»¿æ•™å¸ˆçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œåˆ†å¸ƒï¼Œè€Œåºåˆ—çº§çŸ¥è¯†è’¸é¦åˆ™é€šè¿‡ç”Ÿæˆç”±æ•™å¸ˆç”Ÿæˆçš„ä¼ªç›®æ ‡æ¥è®­ç»ƒå­¦ç”Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†æ¨¡å‹å‹ç¼©çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å‰ªæä¸é‡è¦çš„å‚æ•°ã€‚

ç ”ç©¶å‘ç°ï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œé›†ä¸­åœ¨è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ä»»åŠ¡ä¸Šæˆ–è¿›è¡Œé¢„è®­ç»ƒä»»åŠ¡çš„æ— ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦ã€‚æ‰€æœ‰é’ˆå¯¹NLGçš„å·¥ä½œéƒ½é›†ä¸­åœ¨å•ä¸ªç”Ÿæˆä»»åŠ¡ä¸Šï¼Œå¹¶ä¸”æ‰€æœ‰é’ˆå¯¹NLGçš„å·¥ä½œéƒ½ä½¿ç”¨äº†å¤§é‡çš„æœ‰æ ‡ç­¾æ•°æ®é›†ï¼Œå¿½ç•¥äº†æœªæ ‡è®°çš„æ•°æ®ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€é¡¹ç³»ç»Ÿæ€§çš„ç ”ç©¶ï¼Œæ—¨åœ¨è€ƒè™‘å„ç§ç°å®çš„NLGä»»åŠ¡ï¼Œè€Œä¸æ˜¯ä»…å…³æ³¨å•ä¸€çš„ç”Ÿæˆä»»åŠ¡ã€‚

ç ”ç©¶è¿˜å¼ºè°ƒäº†åœ¨å®é™…è®¾ç½®ä¸­è¿›è¡ŒçŸ¥è¯†è’¸é¦çš„é‡è¦æ€§ï¼Œæå‡ºäº†äº”ä¸ªå¸å¼•å¹¿æ³›NLPä»ä¸šè€…çš„æ ‡å‡†ï¼šä¸­ç­‰èµ„æºï¼ˆæ•°åƒä¸ªæœ‰æ ‡ç­¾æ•°æ®ï¼‰ã€ä¸°å¯Œçš„æœªæ ‡è®°æ•°æ®ã€ç°æˆçš„å°åˆ°ä¸­å‹å¤§å°+å¾®è°ƒçš„è¯­è¨€æ¨¡å‹ã€æ¨ç†æ—¶é—´æ•ˆç‡ä»¥åŠä¸€æ¬¡æ€§çš„è®¡ç®—åŸ¹è®­èµ„æºã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†ä¸åŒæ¡ä»¶ä¸‹çš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¸åŒæ¶æ„ã€ä¸åŒçŸ¥è¯†è’¸é¦æ–¹æ³•å’Œä¸åŒä¼ªç›®æ ‡æ•°é‡çš„æ¨¡å‹è¡¨ç°ã€‚</sample>
    <sample id="63">æŒ‡æ ‡çµæ•åº¦è¡¡é‡æ¨¡å‹å¯¹åŒä¸€ä»»åŠ¡çš„ä¸åŒæŒ‡ä»¤çš„æ•æ„Ÿæ€§ã€‚å®ƒè¯„ä¼°æ¨¡å‹åœ¨é¢å¯¹è½»å¾®æŒ‡ä»¤å˜åŒ–æ—¶èƒ½å¦å§‹ç»ˆå¦‚ä¸€åœ°äº§ç”Ÿç›¸åŒçš„ç»“æœã€‚</sample>
    <sample id="64">Wenjun Peng</sample>
    <sample id="65">æ›´é«˜çš„çµæ•åº¦é€šå¸¸è¡¨ç¤ºæ¨¡å‹å¯¹ä»»åŠ¡çš„å“åº”æ›´åŠ æ•æ„Ÿï¼Œä½†å¹¶ä¸ç›´æ¥æ„å‘³ç€æ¨¡å‹æ€§èƒ½å¾—åˆ°äº†æé«˜ã€‚çµæ•åº¦ï¼ˆSensitivityï¼‰æ˜¯æŒ‡æ¨¡å‹åœ¨æ­£ç¡®è¯†åˆ«å‡ºæ‰€æœ‰å®é™…ä¸ºæ­£ä¾‹çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæ­£ç¡®è¯†åˆ«å‡ºçš„æ¯”ä¾‹ã€‚å¦‚æœçµæ•åº¦å¢åŠ ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å¯¹æŸäº›ç‰¹å®šæƒ…å†µè¿‡äºæ•æ„Ÿï¼Œå¯¼è‡´è¯¯æŠ¥ç‡ä¸Šå‡ï¼Œä»è€Œå½±å“äº†æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ã€‚å› æ­¤ï¼Œéœ€è¦ç»“åˆå…¶ä»–æŒ‡æ ‡å¦‚ç‰¹å¼‚æ€§ï¼ˆSpecificityï¼‰å’Œå‡†ç¡®ç‡ï¼ˆAccuracyï¼‰æ¥å…¨é¢è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚</sample>
    <sample id="66">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦è®¨è®ºäº†æ·±åº¦å­¦ä¹ åœ¨æ•°å­¦æ¨ç†ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ•°å­¦é—®é¢˜è§£å†³ã€å‡ ä½•é—®é¢˜æ±‚è§£å’Œè‡ªåŠ¨å®šç†è¯æ˜ç­‰æ–¹é¢çš„ç ”ç©¶è¿›å±•ã€‚é¦–å…ˆï¼Œä»‹ç»äº†æ•°å­¦æ¨ç†çš„æ¦‚å¿µåŠå…¶é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†è¿‘å¹´æ¥åœ¨æ•°å­¦æ¨ç†é¢†åŸŸå–å¾—çš„ä¸€äº›æˆæœã€‚æ¥ç€ï¼Œè¯¦ç»†ä»‹ç»äº†æ·±åº¦å­¦ä¹ å¦‚ä½•åº”ç”¨äºæ•°å­¦é—®é¢˜è§£å†³ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è§£ç­”æ•°å­¦æ–‡å­—é—®é¢˜ã€å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚å›¾åƒã€è¡¨æ ¼ç­‰ï¼‰ä»¥åŠè§£å†³å‡ ä½•é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æåˆ°äº†ç¥ç»ç¬¦å·æ¨ç†æ–¹æ³•åœ¨å‡ ä½•é—®é¢˜æ±‚è§£ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ç»“åˆç¥ç»ç½‘ç»œå’Œç¬¦å·é€»è¾‘æ¥è§£å†³å¤æ‚çš„å‡ ä½•é—®é¢˜ã€‚æœ€åï¼Œæ¢è®¨äº†è‡ªåŠ¨åŒ–å®šç†è¯æ˜æŠ€æœ¯çš„å‘å±•ï¼Œå¼ºè°ƒäº†å…¶åœ¨éªŒè¯æ•°å­¦å®šç†æ–¹é¢çš„æ½œåŠ›ã€‚æ•´ä¸ªå†…å®¹çªå‡ºäº†æ·±åº¦å­¦ä¹ ä¸æ•°å­¦æ¨ç†ä¹‹é—´çš„ç´§å¯†è”ç³»ï¼Œå±•ç¤ºäº†è¿™ä¸€é¢†åŸŸçš„å‰æ²¿ç ”ç©¶æ–¹å‘å’Œæœªæ¥å‘å±•æ–¹å‘ã€‚</sample>
    <sample id="67">è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ä¸­å¹²æ‰°å’ŒååŒç°è±¡çš„åŸå› åŠå…¶è§£å†³æ–¹æ³•ã€‚é¦–å…ˆï¼Œä»‹ç»äº†å¤šè¯­è¨€MTæ¨¡å‹èƒ½å¤Ÿä»è¯­è¨€å¯¹ä¹‹é—´çš„ååŒæ•ˆåº”ä¸­å—ç›Šï¼Œä½†åŒæ—¶ä¹Ÿå¯èƒ½å—åˆ°å¹²æ‰°çš„å½±å“ã€‚æ¥ç€ï¼Œè®¨è®ºäº†å¤šç§ç¼“è§£å¹²æ‰°çš„æ–¹æ³•ï¼Œå°½ç®¡è¿™äº›æ–¹æ³•é€šå¸¸åœ¨è¾ƒå°è§„æ¨¡çš„æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ï¼Œä½†å¹¶ä¸æ€»æ˜¯æ¯”è°ƒä¼˜åŸºçº¿æ•ˆæœæ›´å¥½ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œå½“æ¨¡å‹è§„æ¨¡è¿œå°äºæ•°æ®é‡æ—¶ï¼Œä¼šé­é‡ä¸¥é‡çš„å¹²æ‰°ï¼Œå¹¶å¼ºè°ƒäº†è°ƒæ•´é‡‡æ ·æ¸©åº¦å¯¹äºå®ç°å¼ºæ€§èƒ½çš„å…³é”®ä½œç”¨ã€‚

ç ”ç©¶è¿˜åˆ†æäº†å½±å“è¯­è¨€å¯¹æŸå¤±çš„å› ç´ ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€æ•°æ®é‡ã€å…¶ä»–è¯­è¨€çš„æ•°æ®é‡ã€è¯­è¨€ç›¸ä¼¼æ€§å’Œè¯­è¨€æ•°é‡ç­‰ã€‚é€šè¿‡å®éªŒè®¾ç½®ï¼Œç ”ç©¶è€…å‘ç°å¹²æ‰°ä¸ååŒç°è±¡ä¸»è¦ç”±æ¨¡å‹è§„æ¨¡ã€æ•°æ®é‡åŠå…¶å®ƒè¯­è¨€çš„æ•°æ®é‡å†³å®šï¼Œè€Œéè¯­è¨€ç›¸ä¼¼æ€§æˆ–è¯­è¨€æ•°é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å‚æ•°åŒ®ä¹çš„æƒ…å†µä¸‹ï¼Œå¹²æ‰°ç°è±¡å°¤ä¸ºä¸¥é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºæ¸©åº¦é‡‡æ ·ï¼ˆTemperature samplingï¼‰ä½œä¸ºç¼“è§£å¹²æ‰°çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸åŒè§„æ¨¡å’Œæ¸©åº¦ä¸‹è®­ç»ƒå¤šè¯­è¨€æ¨¡å‹æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚æœ€åï¼Œç ”ç©¶å¾—å‡ºç»“è®ºï¼šæ¨¡å‹è§„æ¨¡ã€æ•°æ®é‡ä»¥åŠå…¶å®ƒè¯­è¨€çš„æ•°æ®é‡æ˜¯å¹²æ‰°/ååŒç°è±¡çš„ä¸»è¦å› ç´ ï¼›é€‚åº¦è§„æ¨¡å’Œè°ƒæ ¡æ¸©åº¦å¯ä»¥æ˜¾è‘—å‡å°‘å¹²æ‰°é—®é¢˜ï¼Œè€Œæ— éœ€ä¾èµ–å¤æ‚çš„æ–¹æ³•ã€‚</sample>
    <sample id="68">åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹ä¼šæ¥æ”¶å¤§é‡çš„æ–‡æœ¬æ•°æ®ä½œä¸ºè¾“å…¥ã€‚</sample>
    <sample id="69">åœ¨ WSL ä¸­ï¼Œé€šå¸¸éœ€è¦æ›´å¤šçš„å¹²å‡€éªŒè¯æ ·æœ¬æ‰èƒ½è·å¾—è‰¯å¥½çš„è¡¨ç°ã€‚</sample>
    <sample id="70">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯æ–¯å¦ç¦å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»ã€‚</sample>
    <sample id="71">The presentation focuses on the research project "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)" conducted by Mohammad Javad Hosseini, Filip Radlinski, Silvia Paresi, and Annie Louis from Google Research. The primary goal of this study is to understand users' language when they make choices, particularly in conversational systems where indirect referring expressions are common. These expressions can be challenging due to their informal nature, difficulty in distinguishing them from direct references, and the user's desire to specify a preference.

The researchers highlight that indirect referring expressions are prevalent in natural and fluid conversations, often used when the speaker cannot remember the name or when the pronunciations are hard to distinguish. They emphasize the importance of understanding these expressions to improve entity selection accuracy in conversational systems.

To address this challenge, the team developed an alternative question dataset using crowd annotation. This dataset includes three domains: music, books, and recipes. The methodology emphasizes informality through a cartoon completion task, which helps in capturing the essence of casual conversation. The dataset consists of 6,000 alternative questions across the three domains and 42,000 indirect referring expressions.

The study demonstrates that models trained with this dataset achieve high accuracy, ranging from 92% to 95% when the language model has access to the same background knowledge as annotators, and 82% to 87% when it only has access to entity names. The researchers also show that their models are domain-generalizable, indicating their potential application beyond the specific domains studied.

The presentation concludes with a call to action, inviting viewers to explore the AltEntities Corpus dataset available at https://github.com/google-research-datasets/AltEntities and to reach out to javadh@google.com for any questions.</sample>
    <sample id="72">å› ä¸ºç°æœ‰çš„æ–¹æ³•æ— æ³•å‡†ç¡®åœ°è¡¡é‡åª’ä½“åè§ã€‚</sample>
    <sample id="73">æ¼”è®²è€…çš„åå­—æ˜¯Martin Pomsã€‚</sample>
    <sample id="74">The presentation focuses on the construction of a densely-connected commonsense knowledge graph, Dense-ATOMIC, and its evaluation through various methods. The speaker introduces Dense-ATOMIC as an extension of ATOMIC, aiming to enhance knowledge coverage and multi-hop paths by addressing limitations in existing knowledge bases like ATOMIC. The motivation section highlights the importance of commonsense knowledge for machine-human interaction, emphasizing the need for a comprehensive knowledge base that covers event-centered social aspects.

The dense connection of Dense-ATOMIC is achieved through a novel completion method called Rel-CSKG, which leverages a relation prediction model trained on a dense graph. This method addresses the sparsity issue of ATOMIC by utilizing semantic information from events and avoiding the computational expense of iterating over all pairs of head and tail events during inference. The evaluation of Rel-CSKG includes metrics such as Total, Intra, and Inter, demonstrating its superior performance compared to traditional methods and other translation-based approaches.

The conclusion summarizes the key contributions: constructing Dense-ATOMIC and proposing Rel-CSKG for efficient knowledge completion. The presentation concludes with a call to action, inviting further research into commonsense reasoning and the potential applications of Dense-ATOMIC in various domains.</sample>
    <sample id="75">The presentation focuses on the development and application of Jointprop, a joint semi-supervised learning framework for entity and relation extraction using heterogeneous graph-based propagation. The study addresses the challenges posed by fully supervised models, which require extensive and expensive labor to obtain high-quality data annotation and diverse annotated data for various domains and applications. Semi-supervised learning (SSL) is proposed as an alternative, utilizing a small amount of labeled data to learn powerful models at a lower cost.

The research highlights the importance of considering the interconnections between Named Entity Recognition (NER) and Relation Extraction (RE), which current studies often neglect. It introduces a joint semi-supervised framework that models NER and RE tasks by propagating labels over heterogeneous graphs, performing label propagation across the graph, and considering both inter- and intra-interactions among labeled and unlabeled data.

The framework includes three main components: span feature generation, heterogeneous graph construction, and joint label propagation and model optimization. Span feature generation involves constructing k nearest neighbor (kNN) graphs for computation efficiency, encoding both inter- and intra-relationships within the feature space. Heterogeneous graph construction utilizes pseudo-label selection and propagation processes to refine the graph structure iteratively. Joint label propagation diffuses labels through the graph along high-density areas formed by the unlabeled data.

The objective of the framework is to optimize the model by selecting pseudo labels based on confidence thresholds and retraining the model with the same baseline model and classification function. The presentation concludes with experimental results on SciERC and ACE05 datasets, demonstrating the effectiveness of Jointprop in improving performance compared to baseline methods.</sample>
    <sample id="76">The political bias spread process is from pretraining data to language models and then to downstream tasks.</sample>
    <sample id="77">è¿™ç¯‡è®ºæ–‡ä¸»è¦æ¢è®¨äº†å¦‚ä½•é€šè¿‡è‡ªç„¶è¯­è¨€åé¦ˆæ¥æé«˜æ‘˜è¦çš„å®è¯ä¸€è‡´æ€§ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†DeFactoï¼Œè¯¥æ•°æ®é›†åŒ…å«äººç±»ç¤ºèŒƒå’Œåé¦ˆï¼Œæ—¨åœ¨æ”¹è¿›æ‘˜è¦çš„å®è¯ä¸€è‡´æ€§ã€‚ä»–ä»¬è¿˜ä»‹ç»äº†å…¨é¢çš„æ•°æ®é›†åˆ†æå’Œè¿›ä¸€æ­¥çš„è§è§£ï¼Œå¹¶è®¨è®ºäº†è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡å’Œå¼ºå¤§çš„åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯¦ç»†ä»‹ç»äº†æ–°æ•°æ®é›†çš„ç›®æ ‡ã€æ ‡ç­¾ä»¥åŠæ”¶é›†æ•°æ®çš„å…·ä½“ç»†èŠ‚ï¼ŒåŒ…æ‹¬æ•°æ®åˆ†å¸ƒã€é”™è¯¯ç±»å‹ã€æŒ‡ä»¤ã€è§£é‡Šå’Œè¯æ®ç­‰ä¿¡æ¯ã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†ä¸åŒç¼–è¾‘æ¨¡å‹åœ¨æ‘˜è¦ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶è®¨è®ºäº†è¿™äº›æ¨¡å‹åœ¨ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚</sample>
    <sample id="78">æ˜¯çš„ï¼ŒDEplain-APAå’Œç½‘ç«™çš„ç®€åŒ–è¿‡ç¨‹æœ‰æ‰€ä¸åŒã€‚</sample>
    <sample id="79">æ˜¯çš„ï¼ŒCoscript æ˜¯å…¬å¼€å¯ç”¨çš„ã€‚</sample>
    <sample id="80">æ°´å°é€šè¿‡åœ¨åŸå§‹åµŒå…¥ä¸Šæ·»åŠ ç›®æ ‡åµŒå…¥æ¥æ’å…¥åˆ°æ–‡æœ¬ä¸­ã€‚</sample>
    <sample id="81">PennStateå’ŒAmazon</sample>
    <sample id="82">The research paper focuses on unsupervised automated essay scoring (AES) under the ACL 2023 conference, aiming to score essay quality without human intervention. It discusses the challenges of collecting labeled essays, which are time-consuming and labor-intensive. The study introduces a novel framework called ULRA for training AES models using multiple heuristic quality signals as pseudo-groundtruth. This approach addresses conflicts among different signals by designing a deep pairwise rank aggregation loss function. The paper presents two methods: HER and DPRA, both utilizing the ULRA framework. HER employs a heuristic essay ranking strategy, while DPRA uses a deep pairwise rank aggregation strategy. Experimental results demonstrate the effectiveness of ULRA in improving the performance of AES models, particularly in handling conflicts between signals and achieving unified supervision.</sample>
    <sample id="83">Yes, the encoder-decoder models like mT5 can be improved by training in a mixture of various languages.</sample>
    <sample id="84">The presentation introduces PAD-Net, an efficient framework for dynamic networks, which addresses the challenges of static and dynamic convolutional neural networks (CNNs). It discusses the concept of dynamic networks versus static ones, highlighting that dynamic networks can be more efficient due to their ability to adapt parameters based on input data. The speaker explains how PAD-Net partitions network parameters into intrinsic (static) and computational (dynamic) modes, allowing for the transformation of redundant dynamic parameters into static ones without pruning them out. This approach is demonstrated through empirical evaluations in natural language processing (NLP) and computer vision (CV) tasks, showing improvements in performance with fewer parameters and less computation.

Key points include:
1. **Dynamic Networks vs. Static Networks**: Dynamic networks offer better performance by dynamically adjusting parameters, whereas static networks use fixed parameters.
2. **PAD-Net Framework**: It partitions network parameters into intrinsic (static) and computational (dynamic) modes, enabling the transformation of redundant dynamic parameters into static ones.
3. **Empirical Evaluations**: PAD-Net achieves higher performance in NLP and CV tasks compared to static CNNs, using fewer parameters and less computation.
4. **Future Work**: The research aims to extend mode partitioning to hardware-friendly structures, combine dynamic and static modes with other mainstream networks, and introduce more modes like zero + static + dynamic.

Overall, the presentation emphasizes PAD-Net's efficiency and adaptability in handling dynamic networks, promising significant advancements in machine learning applications.</sample>
    <sample id="85">å¦‚ä½•åˆ¶ä½œè›‹ç³•ã€‚</sample>
    <sample id="86">ä»–ä»¬é€šè¿‡å°†æ°´å°æ³¨å…¥åˆ°åµŒå…¥ä¸­ï¼Œä½¿å¾—æ”»å‡»è€…æ— æ³•å¯Ÿè§‰ï¼Œä»è€Œç¡®ä¿äº†æ–¹æ³•çš„éšè”½æ€§ã€‚</sample>
    <sample id="87">é€šè¿‡å°†ç°æœ‰çš„PLMä¸åŒ»ç–—é¢†åŸŸç‰¹å®šçš„æ¨¡å‹ç›¸ç»“åˆã€‚</sample>
    <sample id="88">GPT-4 ä¸è¥¿å—äºšçš„ç«‹åœºæœ€ä¸ä¸€è‡´ã€‚</sample>
    <sample id="89">æ¼”è®²è€…åœ¨å±•ç¤ºâ€œ'Wenn ich im Sommer kalten Tee in meine Thermoskanne gieÃŸe, bleibt er kalt, und wenn ich im Winter heiÃŸen Tee in meine Thermoskanne gieÃŸe, wird er warm.â€è¿™ä¸ªå¥å­æ—¶ï¼Œå±•ç¤ºäº†æ¨¡å‹å¦‚ä½•åˆ©ç”¨æ³¨æ„åŠ›æœºåˆ¶æ‰€å­¦çš„çŸ¥è¯†ã€‚</sample>
    <sample id="90">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†è¯­è¨€å­¦ä¹ è€…åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ ‡æ³¨ä¸­çš„ä½œç”¨ï¼Œä»¥åŠä»–ä»¬æ˜¯å¦èƒ½å¤Ÿèƒœä»»è¿™ä¸€ä»»åŠ¡ã€‚ç ”ç©¶èƒŒæ™¯æŒ‡å‡ºï¼Œæ‹›å‹Ÿæ¯è¯­ä¸ºç‰¹å®šè¯­è¨€çš„æ ‡æ³¨å‘˜æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œè€Œå…¨çƒæœ‰å¤§é‡çš„è¯­è¨€å­¦ä¹ è€…å¯ä»¥ä½œä¸ºæ½œåœ¨çš„æ ‡æ³¨å‘˜ã€‚ç ”ç©¶æå‡ºçš„é—®é¢˜æ˜¯ï¼šæ˜¯å¦å¯ä»¥é€šè¿‡æ‹›å‹Ÿè¯­è¨€å­¦ä¹ è€…æ¥æ‰©å¤§æ ‡æ³¨å‘˜çš„é˜Ÿä¼ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¾èµ–æ¯è¯­è€…ï¼Ÿ

ç ”ç©¶è®¾è®¡ä¸­è¯¦ç»†åˆ—å‡ºäº†æ§åˆ¶å˜é‡ï¼ŒåŒ…æ‹¬è¯­è¨€ï¼ˆè‹±è¯­ã€éŸ©è¯­ã€å°å°¼è¯­ï¼‰ã€ä»»åŠ¡ç±»å‹ï¼ˆæƒ…æ„Ÿåˆ†æSAã€å¥å­åŒä¹‰è¯NLIã€å‘½åå®ä½“è¯†åˆ«NERã€é˜…è¯»ç†è§£MRCï¼‰ã€è¯­è¨€ç†Ÿç»ƒç¨‹åº¦ï¼ˆåŸºç¡€ã€ä¸­çº§ã€é«˜çº§ã€æ¯è¯­è€…ï¼‰å’Œé—®é¢˜éš¾åº¦ï¼ˆéå¸¸ç®€å•ã€ç®€å•ã€æ­£å¸¸ã€å›°éš¾ã€éå¸¸å›°éš¾ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜è€ƒè™‘äº†é¢å¤–èµ„æºï¼Œå¦‚è¯å…¸å’Œæœºå™¨ç¿»è¯‘ç³»ç»Ÿã€‚

å®éªŒæµç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šé¢„è°ƒæŸ¥ã€å®éªŒå’Œåè°ƒæŸ¥ã€‚å‚ä¸è€…é¦–å…ˆå®Œæˆæ ‡å‡†åŒ–æµ‹è¯•é—®å·ï¼Œç„¶åè¿›è¡Œæ ‡æ³¨ä»»åŠ¡ï¼Œæœ€åå†å®Œæˆå•è¯æ„ä¹‰æµ‹è¯•é—®å·ï¼Œå¹¶å¡«å†™åè°ƒæŸ¥é—®å·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯­è¨€å­¦ä¹ è€…æ ‡æ³¨çš„æ ‡ç­¾å‡ ä¹ä¸æ¯è¯­è€…çš„æ ‡æ³¨å‡†ç¡®åº¦ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ¥è¿‘95%çš„å‡†ç¡®ç‡ã€‚è¿™è¡¨æ˜ï¼Œé€šè¿‡èšåˆè¯­è¨€å­¦ä¹ è€…çš„æ ‡æ³¨ç»“æœï¼Œä»–ä»¬çš„è¡¨ç°å¯ä»¥è¾¾åˆ°ä¸æ¯è¯­è€…ç›¸è¿‘çš„æ°´å¹³ã€‚

ç ”ç©¶è¿˜å‘ç°ï¼ŒNLPæ ‡æ³¨ä»»åŠ¡æœ‰åŠ©äºæé«˜è¯­è¨€å­¦ä¹ è€…çš„è¯æ±‡é‡å’Œè¯­æ³•èƒ½åŠ›ã€‚ç»“è®ºéƒ¨åˆ†æå‡ºäº†å‡ ä¸ªå…³é”®é—®é¢˜ï¼šæ˜¯å¦æœ‰å¿…è¦ç»§ç»­ä¾èµ–æ¯è¯­è€…è¿›è¡Œæ•°æ®æ ‡æ³¨ï¼Ÿè¯­è¨€å­¦ä¹ è€…ä½œä¸ºæ ‡æ³¨å‘˜çš„å¯è¡Œæ€§å¦‚ä½•ï¼Ÿæœªæ¥æ˜¯å¦æœ‰æ½œåŠ›å°†NLPç ”ç©¶æ‰©å±•åˆ°æ›´å¤šè¯­è¨€ä¸Šï¼Ÿ</sample>
    <sample id="91">ä»»åŠ¡æ•°é‡çš„å¢åŠ é€šå¸¸ä¼šæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œåœ¨OFAä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒæ—¶ï¼Œä½¿ç”¨5ä¸ªæŒ‡ä»¤æ¯”1ä¸ªæŒ‡ä»¤èƒ½å¤Ÿå®ç°æ›´é«˜çš„èšåˆæ€§èƒ½ï¼Œå¹¶ä¸”æ˜¾ç¤ºå‡ºæ›´ä½çš„æ•æ„Ÿæ€§ã€‚</sample>
    <sample id="92">LSTM seq2seqæ¨¡å‹ã€PPå’ŒZhengå’ŒLapataæ¨¡å‹ã€‚</sample>
    <sample id="93">ä¸¤ä½åˆè‘—è€…Alexander Kollerå’ŒIvan Titovæ˜¯è®ºæ–‡çš„ç¬¬ä¸€ä½œè€…Matthias Lindemannçš„åŒäº‹ã€‚</sample>
    <sample id="94">è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„ç‹¬ç‰¹æ€§ï¼Œå¹¶è®¨è®ºäº†è¿™äº›æ¨¡å‹ä½œä¸ºæœåŠ¡ï¼ˆEaaSï¼‰æä¾›çš„åµŒå…¥å¼APIçš„ç‰ˆæƒä¿æŠ¤é—®é¢˜ã€‚èƒŒæ™¯éƒ¨åˆ†ä»‹ç»äº†GPTã€LLAMAå’ŒPALMç­‰æ¨¡å‹ï¼Œä»¥åŠOpenAIæä¾›çš„GPT-3åŸºç¡€åµŒå…¥APIã€‚åŠ¨æœºéƒ¨åˆ†æŒ‡å‡ºï¼Œæ”»å‡»è€…å¯èƒ½é€šè¿‡å­¦ä¹ åµŒå…¥å¼æ•°æ®æ¥çªƒå–æ¨¡å‹å¹¶æä¾›ç±»ä¼¼æœåŠ¡ï¼Œå› æ­¤éœ€è¦ä¿æŠ¤EaaSçš„ç‰ˆæƒï¼Œæ£€æµ‹æœåŠ¡æ˜¯å¦è¢«å…¶ä»–æœåŠ¡ç›—ç”¨ã€‚æŒ‘æˆ˜åŒ…æ‹¬é€‚ç”¨æ€§ã€å®ç”¨æ€§å’Œéšè”½æ€§ï¼Œå³ä¸é™ä½æä¾›çš„åµŒå…¥å¼æ•°æ®çš„å®ç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒéšè”½æ€§ä¸”å¯è½¬ç§»è‡³æ”»å‡»è€…çš„æœåŠ¡ä¸­ã€‚ç°æœ‰å·¥ä½œåŒ…æ‹¬å‚æ•°æ°´å°ã€è¯æ³•æ°´å°ã€åé—¨æ°´å°å’Œå¯¹æŠ—æ€§æ°´å°ï¼Œä½†è¿™äº›æ–¹æ³•å‡ä¸é€‚ç”¨äºEaaSã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºEmbMarkerçš„æ–¹æ³•ï¼Œé€šè¿‡é€‰æ‹©è§¦å‘å™¨é›†å¹¶åœ¨åŸå§‹åµŒå…¥ä¸Šæ·»åŠ ç›®æ ‡åµŒå…¥æ¥å®ç°æ°´å°æ³¨å…¥ã€‚ç‰ˆæƒéªŒè¯æ¶‰åŠæ„å»ºåé—¨å’Œè‰¯æ€§æ•°æ®é›†ï¼Œè¯·æ±‚ä»ç›—ç‰ˆæœåŠ¡è·å–åµŒå…¥ï¼Œå¹¶è®¡ç®—ç›¸ä¼¼åº¦å·®å¼‚å’ŒKSæµ‹è¯•på€¼ä»¥è¿›è¡Œç‰ˆæƒéªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEmbMarkeråœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</sample>
    <sample id="95">PaLM çš„ç¬¬ä¸€ä½œè€…æ˜¯ Chowdery ç­‰äººã€‚</sample>
    <sample id="96">å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

NLPositionality: Characterizing Design Biases of Datasets and Models

æƒ³è±¡ä¸€ä¸‹...

= PerspectiveAPI score

Carl Jones
Tech Lead, New York Times

Can you stop being a jerk? ğŸ¤¦â€â™‚ï¸ : (0.82) âœ…

Aditya Sharma
Tech Lead, Times of India

Presstitutes everywhere on the news. ğŸ¤¦â€â™‚ï¸ : (0.33) âŒ

Design bias example!

Positionality

The perspectives [people] hold as a result of their demographics, identity, and life experiences.

As a researcher, it influences the research process and its outcomes and results.

Do datasets and models have positionality?

Anecdotal evidence:
- Model and dataset probing [1][2]
- Theoretical definitions of model positionality [3]

Question:
Do datasets and models have positionality?

Goal: Compare annotations from users with existing datasets and models.

NLPositionality
A framework for characterizing design biases in NLP datasets and models

Framework

Collection
Processing
Analysis

1) Re-annotate datasets with diverse annotators.
2) Compare annotations by demographic to models and datasets via Pearson's R scores.

Task A: Social Acceptability

Participants read a situation from the Social Chemistry dataset.
Participants rate how socially acceptable the situation is.
Participants compare their responses to others' and an AI's.

Task B: Toxicity

Participants read an instance from the Dynahate dataset.
Participants rate whether they think an instance is hate speech.
Participants compare their responses to others' and an AI's.

Study Participation
16,299 annotations
1,096 annotators
87 countries

Results
Who do NLP datasets and models align with?
Finding 1: There is positionality in NLP.
Datasets and models are most aligned to English-Speaking countries.
Datasets and models are less aligned to non-binary people.
Finding 2: Some populations are left behind.
Datasets and models are most aligned to people with a college education.
Datasets and models are less aligned to non-binary people.
So, what can we do?
Addressing positionality in NLP
Recommendations
1. Keep a record of all relevant design choices made throughout building datasets or models.
2. Do NLP research through the lens of perspectivism:
   a. Share disaggregated dataset labels!
   b. Use modeling techniques that can handle annotator disagreement.
3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative!).
Dashboard Link: nipositionality.cs.washington.edu/
Paper: bit.ly/NLPositionality-Paper/

---

å¸Œæœ›è¿™èƒ½å¸®åˆ°æ‚¨ï¼å¦‚æœæœ‰æ›´å¤šå†…å®¹éœ€è¦ç¿»è¯‘ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚</sample>
    <sample id="97">æ¼”è®²è€…æåˆ°äº† SimulST çš„ä¸‰ä¸ªé—®é¢˜ã€‚</sample>
    <sample id="98">åœ¨è®­ç»ƒ NLP æ¨¡å‹æ—¶ï¼Œå‡è½»æ•°æ®é›†ä¸­çš„ç¤¾ä¼šå’Œæ”¿æ²»åè§çš„æœ‰æ•ˆæ–¹æ³•åŒ…æ‹¬ä½¿ç”¨å¤šæ ·åŒ–çš„é¢„è®­ç»ƒæ•°æ®é›†ã€è¿›è¡Œå…¬æ­£æ€§è¯„ä¼°ä»¥åŠè°ƒæ•´æ¨¡å‹å‚æ•°ä»¥å‡å°‘åè§ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡è°ƒæ•´æ¨¡å‹æ¶æ„æˆ–å¼•å…¥å¤–éƒ¨çŸ¥è¯†æ¥è¿›ä¸€æ­¥å‡å°‘åè§ã€‚</sample>
    <sample id="99">å¥½çš„ï¼Œè¯·æä¾›æ‚¨éœ€è¦ç¿»è¯‘çš„è‹±æ–‡å†…å®¹ï¼Œæˆ‘ä¼šå°†å…¶è½¬æ¢ä¸ºæ‚¨æ‰€éœ€çš„ä¸­æ–‡ç‰ˆæœ¬ã€‚</sample>
    <sample id="100">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰è¿›è¡Œå¤šè·³é—®ç­”ï¼ˆMulti-hop Question Answering, MQAï¼‰çš„æ£€ç´¢å’Œæ’åºæ–¹æ³•ã€‚ç ”ç©¶ä¸­æå‡ºäº†ä¸€ç§åä¸ºPromptRankçš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆæ— ç›‘ç£æ£€ç´¢æ–¹æ³•ä¸å°‘é‡æ ·æœ¬åŸºäºè¯­è¨€æ¨¡å‹çš„é‡æ’å™¨æ¥æé«˜MQAç³»ç»Ÿçš„æ€§èƒ½ã€‚å…·ä½“æ­¥éª¤åŒ…æ‹¬ä½¿ç”¨TF-IDFæ£€ç´¢å’Œè¶…é“¾æ¥éå†è·å–å€™é€‰è·¯å¾„æ± ï¼Œç„¶åä½¿ç”¨å°‘é‡æ ·æœ¬åŸºäºè¯­è¨€æ¨¡å‹çš„é‡æ’å™¨å¯¹è¿™äº›å€™é€‰è·¯å¾„è¿›è¡Œé‡æ’ã€‚ç ”ç©¶å‘ç°ï¼ŒPromptRankåœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿåœ¨ä»…ä½¿ç”¨128ä¸ªç¤ºä¾‹çš„æƒ…å†µä¸‹è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸‹æ¸¸é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä»‹ç»äº†PromptRankçš„æŒ‡ä»¤æ„é€ æ–¹æ³•ä»¥åŠå¦‚ä½•é€šè¿‡æŒ‡ä»¤æœç´¢æ‰¾åˆ°æœ€ä¼˜æŒ‡ä»¤ï¼ŒåŒæ—¶è®¨è®ºäº†æ¸©åº¦ç¼©æ”¾ç­‰é¢å¤–æŠ€æœ¯çš„åº”ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPromptRankåœ¨HotpotQAåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨R@KæŒ‡æ ‡ä¸Šã€‚</sample>
    <sample id="101">PaLM çš„æµç•…åº¦ä¸ SOTA ç³»ç»Ÿç›¸å½“ã€‚</sample>
    <sample id="102">æ°´å°æ–¹æ³•çš„é‡è¦å±æ€§åŒ…æ‹¬é€‚ç”¨æ€§ã€å®ç”¨æ€§å’Œéšè”½æ€§ã€‚</sample>
    <sample id="103">TED è‹±è¯­æ¼”è®²å·²è¢«ç¿»è¯‘æˆè‹±è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€å¸Œä¼¯æ¥è¯­ã€æ„å¤§åˆ©è¯­ã€æ—¥è¯­ã€éŸ©è¯­ã€è·å…°è¯­ã€è‘¡è„ç‰™è¯­ã€ç½—é©¬å°¼äºšè¯­ã€ä¿„è¯­ã€åœŸè€³å…¶è¯­å’Œä¸­æ–‡ã€‚</sample>
    <sample id="104">10%</sample>
    <sample id="105">åœ¨è¯„ä¼°è‰¯æ€§å’Œåé—¨æ•°æ®é›†ä¹‹é—´çš„å·®å¼‚æ—¶ï¼Œä½¿ç”¨äº†ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆcosine similarityï¼‰å’ŒKSæ£€éªŒï¼ˆKolmogorov-Smirnov testï¼‰ä½œä¸ºè·ç¦»åº¦é‡ã€‚</sample>
    <sample id="106">è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å®ä½“æœç´¢æŸ¥è¯¢ä¸­éšå«é›†åˆæ“ä½œçš„æ£€ç´¢æ•°æ®é›†ã€‚é€šè¿‡åˆ†æä¸¤ä¸ªä¾‹å­ï¼Œå±•ç¤ºäº†ç”¨æˆ·å¦‚ä½•åŸºäºç‰¹å®šæ¡ä»¶æˆ–åå¥½è¡¨è¾¾ä¿¡æ¯éœ€æ±‚ï¼Œä»è€Œäº§ç”ŸåŒ…å«éšå«é›†åˆçº¦æŸçš„æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåŠ¨ç‰©å­¦å®¶åœ¨å“¥æ–¯è¾¾é»åŠ è§‚å¯Ÿåˆ°ä¸€ç§æœªçŸ¥ç‰©ç§ï¼Œéœ€è¦æ‰¾åˆ°è¿™ç§çº¢è‰²èœ¥èœ´çš„åç§°ï¼›è€Œä¸€ä¸ªä¹¦è¿·åˆ™éœ€è¦æ‰¾åˆ°ä¸€æœ¬è®¾å®šåœ¨æ³•å›½çš„å†å²å°è¯´ã€‚è¿™äº›éœ€æ±‚è‡ªç„¶åœ°å¯¼è‡´äº†åŒ…å«å¤šä¸ªçº¦æŸæ¡ä»¶çš„æŸ¥è¯¢ã€‚

ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºQUESTçš„æ•°æ®é›†ï¼ŒåŒ…å«3357ä¸ªå®ä½“æœç´¢æŸ¥è¯¢ï¼Œæ¯ä¸ªæŸ¥è¯¢éƒ½åŒ…å«éšå«é›†åˆæ“ä½œï¼Œå¹¶ä¸”ç­”æ¡ˆå®ä½“ç»è¿‡éªŒè¯ä»¥ç¡®ä¿ç›¸å…³æ€§ï¼Œæ–‡æ¡£æ ‡è®°æœ‰å¯å½’å› ç‰‡æ®µã€‚é€šè¿‡ä½¿ç”¨BM25å’ŒT5åŒç¼–ç å™¨ä½œä¸ºç¨€ç–å’Œå¯†é›†æ£€ç´¢å™¨ï¼Œä»¥åŠT5å¤§å‹Rerankerä½œä¸ºåç½®æ’åºå™¨ï¼Œç ”ç©¶è€…å±•ç¤ºäº†ç³»ç»Ÿå¤„ç†æ­¤ç±»é€‰æ‹©æ€§ä¿¡æ¯éœ€æ±‚çš„æœ‰æ•ˆæ€§ã€‚å°½ç®¡å¯†é›†ç¼–ç å™¨åœ¨æ£€ç´¢å’Œé‡æ–°æ’åºæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç«¯åˆ°ç«¯ç³»ç»Ÿçš„F1åˆ†æ•°ç›¸å¯¹è¾ƒä½ï¼Œè¡¨æ˜ä»æœ‰æ”¹è¿›ç©ºé—´ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ•°æ®é›†çš„æ„å»ºè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»å››ä¸ªé¢†åŸŸï¼ˆç”µå½±ã€ä¹¦ç±ã€æ¤ç‰©ã€åŠ¨ç‰©ï¼‰çš„ç»´åŸºç™¾ç§‘ç±»åˆ«åç§°ä¸­é‡‡æ ·ï¼Œè¿›è¡Œé›†åˆæ“ä½œå¹¶ç”±äººå·¥æ³¨é‡Šå‘˜å¯¹æ¨¡æ¿æŸ¥è¯¢è¿›è¡Œæ”¹å†™ï¼Œæœ€ç»ˆè¯„ä¼°æŸ¥è¯¢çš„æµç•…æ€§å’Œè‡ªç„¶åº¦ï¼Œä»¥è¿‡æ»¤æŸ¥è¯¢ã€‚

é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œç ”ç©¶è€…å¸Œæœ›ä¸ºç†è§£å’Œæ”¹è¿›å®ä½“æœç´¢æŸ¥è¯¢ä¸­çš„éšå«é›†åˆæ“ä½œæä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å¥ å®šåŸºç¡€ã€‚</sample>
    <sample id="107">ä½¿ç”¨mT5å’ŒXLM-R + PTRä½œä¸ºåŸºäºç¼–ç å™¨çš„å¤šè¯­è¨€æ¨¡å‹ã€‚</sample>
    <sample id="108">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¥å­ç»“æ„å’Œè¯­ä¹‰æ—¶çš„æ•æ„Ÿæ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…³äºåŒ¹é…å‰ç¼€ï¼ˆmatched prefixesï¼‰å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¯­è¨€æ¨¡å‹å¯¹å¥å­ä¸­çš„æ½œåœ¨è¯­æ³•/è¯­ä¹‰ç‰¹å¾éå¸¸æ•æ„Ÿï¼Œå¹¶ä¸”è¿™äº›ç‰¹å¾å¯èƒ½è·¨è¶Šå¤šä¸ªå¥å­ã€‚ç„¶è€Œï¼Œä½¿ç”¨çŸ­å¥ä½œä¸ºä¸Šä¸‹æ–‡è¿›è¡ŒMPPè¯„ä¼°å¹¶ä¸èƒ½å®Œå…¨æ•æ‰åˆ°è¯­è¨€æ¨¡å‹çš„æŠ½è±¡çŸ¥è¯†ã€‚ç ”ç©¶é€šè¿‡é‡æ–°å®¡è§†æœ€å°é…å¯¹èŒƒå¼ï¼ˆMinimal Pair Paradigm, MPPï¼‰ï¼Œå±•ç¤ºäº†ä¸åŒé•¿åº¦çš„ä¸Šä¸‹æ–‡ã€ç»“æ„åŒ¹é…ä»¥åŠå¯æ¥å—æ€§å¦‚ä½•å½±å“è¯­è¨€æ¨¡å‹çš„åˆ¤æ–­ç»“æœã€‚å®éªŒè¡¨æ˜ï¼ŒåŒ¹é…ç»“æ„çš„å¥å­æœ€ä¸¥é‡åœ°å½±å“äº†æ¨¡å‹çš„è¡¨ç°ï¼Œè€Œä¸å¯æ¥å—çš„MPPå¥å­åˆ™åœ¨ä¸Šä¸‹æ–‡ä¸­æé«˜äº†æˆ–é™ä½äº†åˆ¤æ–­æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†å‡ ä¸ªå…³é”®é—®é¢˜ï¼Œå¦‚ä¸ºä»€ä¹ˆåŒ¹é…å‰ç¼€ä¼šå½±å“LMçš„åˆ¤æ–­ï¼Ÿä»¥åŠå¦‚ä½•é€šè¿‡ä¿æŒç›¸å…³ç»“æ„æ¥æ‰°åŠ¨ä¸Šä¸‹æ–‡å¥å­å¹¶è¯¢é—®æ¨¡å‹æ˜¯å¦å¯¹è¿™äº›å¥å­æœ‰ç›¸ä¼¼çš„æ•æ„Ÿæ€§ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†ä¸¤ä¸ªå…³é”®å‘ç°ï¼šè¯­è¨€æ¨¡å‹å¯¹è·¨å¥å­çš„æ½œåœ¨è¯­æ³•/è¯­ä¹‰ç‰¹å¾æ•æ„Ÿï¼›MPPè¯„ä¼°ä¸­ä½¿ç”¨çŸ­å¥ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ä¸èƒ½å®Œå…¨æ•æ‰åˆ°è¯­è¨€æ¨¡å‹çš„æŠ½è±¡çŸ¥è¯†ã€‚</sample>
    <sample id="109">è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¦‚ä½•é€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ¥ä½¿é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­æ³›åŒ–åˆ°æœªè§è¿‡çš„ä»»åŠ¡ã€‚ä¼ ç»Ÿçš„æ•°æ®æ”¶é›†æ–¹æ³•åŒ…æ‹¬é‡æ–°è¡¨è¿°ç°æœ‰çš„NLPæ•°æ®é›†ï¼Œä½†è¿™ç§æ–¹æ³•å—åˆ°ç°æœ‰å­¦æœ¯åŸºå‡†çš„é™åˆ¶ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼šæ”¶é›†ç”¨æˆ·ç”Ÿæˆçš„æç¤ºå¹¶æ‰‹åŠ¨æ³¨é‡Šå…¶é¢„æœŸè¾“å‡ºã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦å¤§é‡çš„äººå·¥æ ‡æ³¨ï¼Œå¹¶ä¸”å¯èƒ½ä¸é€‚ç”¨äºå®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†â€œUnnatural Instructionsâ€è¿™ä¸€æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§å®Œå…¨è‡ªåŠ¨åŒ–çš„æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œä»…éœ€15ä¸ªæ‰‹åŠ¨æ„é€ çš„ä¾‹å­å³å¯ç”Ÿæˆå¤§é‡æ•°æ®ã€‚è¿™äº›æ•°æ®æ¶µç›–äº†å¹¿æ³›çš„è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œå¦‚å®éªŒéªŒè¯ã€è¯å‘æ˜ç­‰ã€‚ç ”ç©¶è¿˜åˆ†æäº†ç”Ÿæˆçš„ç¤ºä¾‹ï¼Œå…³æ³¨å…¶åˆ›é€ åŠ›ã€å¤šæ ·æ€§å’Œæ­£ç¡®æ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œè¶…è¿‡50%çš„ç”Ÿæˆç¤ºä¾‹æ˜¯æ­£ç¡®çš„ï¼Œå³ä½¿é”™è¯¯çš„ç¤ºä¾‹ä¹Ÿé€šå¸¸åŒ…å«æœ‰ä»·å€¼çš„ä¿¡æ¯ç”¨äºæŒ‡ä»¤è°ƒä¼˜ã€‚æ­¤å¤–ï¼Œâ€œUnnatural Instructionsâ€åŒ…å«é«˜åº¦åˆ›é€ æ€§çš„ä»»åŠ¡ï¼Œä¸ç»å…¸çš„NLPä»»åŠ¡æœ‰å¾ˆå¤§ä¸åŒã€‚æœ€åï¼Œç ”ç©¶å±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼Œå¯¹Unnatural Instructionsè¿›è¡Œå¾®è°ƒçš„T5æ¨¡å‹å¯ä»¥è¶…è¶ŠTO++å’ŒTK-Instructæ¨¡å‹ã€‚</sample>
    <sample id="111">ä½œè€…é€šè¿‡è®¡ç®—ä¸€èˆ¬æ–‡æœ¬è¯­æ–™åº“ä¸­çš„è¯é¢‘æ¥ç¡®å®šä¸­ç­‰é¢‘ç‡çš„å•è¯ã€‚</sample>
    <sample id="112">2003å¹´å‘½åå®ä½“è¯†åˆ«å™¨æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼Ÿ
èˆ’æ’ã€è‰¾ä¼¦Â·é‡Œç‰¹
ä½æ²»äºšç†å·¥å­¦é™¢äº¤äº’è®¡ç®—å­¦é™¢

å‘½åå®ä½“è¯†åˆ«ä¸æ³›åŒ–
æ¨¡å‹å·²ç»ä½¿ç”¨CoNLL-2003å¼€å‘NERè¿‘20å¹´
è¿™äº›æ¨¡å‹èƒ½å¦æ³›åŒ–åˆ°ç°ä»£æ•°æ®ï¼Ÿ
æ³›åŒ–éœ€è¦ä»€ä¹ˆï¼Ÿ
æ€§èƒ½ä¸‹é™çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

CoNLL++æ•°æ®é›†
æ”¶é›†äº†2020å¹´çš„è·¯é€ç¤¾æ–°é—»å¹¶ç”¨CoNLL-2003æ³¨é‡ŠæŒ‡å—è¿›è¡Œæ ‡æ³¨
å¯¹20å¤šä¸ªæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒ
åœ¨CoNLL-2003æµ‹è¯•é›†å’ŒCoNLL++ä¸Šè¿›è¡Œäº†è¯„ä¼°
ä½¿ç”¨Î”F1ç™¾åˆ†æ¯”æ¥è¯„ä¼°æ³›åŒ–èƒ½åŠ›

ä»€ä¹ˆæ˜¯è‰¯å¥½çš„æ³›åŒ–æ‰€éœ€ï¼Ÿ
æ¨¡å‹æ¶æ„
å˜å‹å™¨æ¨¡å‹æ³›åŒ–æ›´å¥½
æ¨¡å‹å¤§å°
æ›´å¤§çš„æ¨¡å‹æ³›åŒ–æ›´å¥½
å¾®è°ƒç¤ºä¾‹æ•°é‡
æ›´å¤šçš„ç¤ºä¾‹å¯¼è‡´æ›´å¥½çš„æ³›åŒ–

æ€§èƒ½ä¸‹é™çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
é€‚åº”æ€§è¿‡æ‹Ÿåˆï¼Ÿ
æ—¶é—´æ¼‚ç§»ï¼Ÿ
æ€§èƒ½éšæ›´å¤§çš„æ—¶é—´é—´éš”è€Œä¸‹é™
æ˜¯ä¸»è¦çš„æ€§èƒ½ä¸‹é™åŸå› 

ç»“è®º
ä¸ºäº†è‰¯å¥½çš„æ³›åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ï¼š
æ›´å¥½çš„æ¨¡å‹æ¶æ„
æ›´å¤§çš„æ¨¡å‹å¤§å°
æ›´å¤šçš„å¾®è°ƒç¤ºä¾‹
æ€§èƒ½ä¸‹é™çš„åŸå› æ˜¯ï¼š
æ—¶é—´æ¼‚ç§»
ä¸æ˜¯é€‚åº”æ€§è¿‡æ‹Ÿåˆ
CoNLL-2003æ ‡ç­¾å™¨ä»ç„¶æœ‰æ•ˆå—ï¼Ÿ
æ˜¯çš„ï¼</sample>
    <sample id="114">The presentation discusses the advancements and challenges in Large Language Models (LLMs), particularly focusing on multi-head attention mechanisms. It highlights how LLMs have revolutionized natural language processing (NLP) by enabling tasks like machine translation, sentiment analysis, and information extraction. However, it also points out significant limitations such as heavy computational requirements, long training times, and the need for large datasets.

The speaker introduces the concept of "Grouped Head Attention" as a solution to address these limitations. This method involves dividing the attention heads into groups, making inter-group heads more similar while keeping intra-group heads distinct. The goal is to achieve parameter efficiency without sacrificing performance, which is crucial for deploying models on smaller clusters or devices.

The presentation outlines two stages of the Grouped Head Attention process: Group Constrained Training (GCT) and Voting-to-Stay (V2S). GCT aims to group attention heads based on their significance, while V2S ensures that only one head remains per group after pruning those with low votes. This approach allows for substantial parameter compression, achieving up to 90% reduction in parameters while maintaining comparable performance across various NLP tasks including machine translation, language modeling, abstractive summarization, and text classification.

The speaker concludes by discussing future work, including task-specific automatic pruning techniques inspired by the Lottery Ticket Hypothesis. This hypothesis suggests that networks contain subnetworks capable of matching the original network's accuracy, potentially leading to more efficient and task-adaptive models.</sample>
    <sample id="115">160ms</sample>
    <sample id="116">Servin æ˜¯æ³•å®˜ï¼ŒKea æ˜¯é¢åŒ…å¸ˆã€‚</sample>
    <sample id="117">ç¤ºä¾‹è´¨é‡</sample>
    <sample id="118">è¯¥ç ”ç©¶æ—¨åœ¨æ”¹è¿›ä»£ç æ··åˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„é¢„è®­ç»ƒæŠ€æœ¯ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ä»£ç æ··åˆä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é¦–å…ˆä»‹ç»äº†ä»£ç æ··åˆçš„æ¦‚å¿µåŠå…¶é‡è¦æ€§ï¼ŒæŒ‡å‡ºç°æœ‰çš„å¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹å¦‚mBERTå’ŒXLM-Råœ¨ä»£ç æ··åˆä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚æ¥ç€ï¼Œæå‡ºäº†ä¸¤ç§ä¸»è¦è´¡çŒ®ï¼šä¸€ç§æ˜¯æå‡ºæ–°çš„æ©ç è¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒç›®æ ‡ï¼Œä»¥çº³å…¥ä»£ç æ··åˆä¿¡æ¯ï¼›å¦ä¸€ç§æ˜¯é€šè¿‡æ¶æ„å˜åŒ–å’Œè¾…åŠ©æŸå¤±æ ‡å‡†æ¥å¢å¼ºä»£ç æ··åˆé¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶è¿˜è¯¦ç»†ä»‹ç»äº†SwitchMLMå’ŒFrequencyMLMè¿™ä¸¤ç§æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº†å®ƒä»¬åœ¨ä¸åŒå±‚ç¼–ç è¯­è¨€è¾¹ç•Œä¿¡æ¯çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›æ–¹æ³•å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨ä»£ç æ··åˆä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†æå‡ºçš„MLMç›®æ ‡å’Œè¾…åŠ©æŸå¤±æ ‡å‡†å¯¹ä»£ç æ··åˆé¢„è®­ç»ƒçš„ç§¯æå½±å“ï¼Œå¹¶å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›æ–¹æ³•ä»¥æé«˜ä»£ç æ··åˆé¢„è®­ç»ƒçš„æ•ˆæœã€‚</sample>
    <sample id="119">åœ¨æ‰©å±•å®éªŒä¸­ï¼Œè®ºæ–‡ä¾§é‡äºRoBERTaå’ŒGPT-2ã€‚</sample>
    <sample id="120">è¯¥æ¨¡å‹ç»“åˆäº†å¤šä¸ªå±‚çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚</sample>
    <sample id="121">ç›´æ¥æ¨æ–­çš„ç¤ºä¾‹åŒ…æ‹¬â€œeasy on meâ€å’Œâ€œthe first oneâ€ã€‚</sample>
    <sample id="122">å¤æ—¦å¤§å­¦</sample>
    <sample id="123">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†é€šè¿‡æŒ‡ä»¤è°ƒä¼˜æ¥æ”¹è¿›å¤šæ¨¡æ€é›¶æ ·æœ¬å­¦ä¹ çš„æ–¹æ³•ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªåä¸ºMULTIINSTRUCTçš„å¤§å‹å¤šæ¨¡æ€æŒ‡ä»¤è°ƒä¼˜åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«62ä¸ªæ¥è‡ª10ä¸ªå¹¿æ³›ç±»åˆ«ä¸­çš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„OFAæ¨¡å‹è¿›è¡ŒæŒ‡ä»¤è°ƒä¼˜ï¼Œç ”ç©¶æ˜¾è‘—æå‡äº†OFAåœ¨å¤šæ¨¡æ€å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„é›¶æ ·æœ¬æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒæ•°é‡çš„æŒ‡ä»¤å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°ä½¿ç”¨5æ¡æŒ‡ä»¤æ—¶ï¼Œæ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸Šè¡¨ç°æœ€ä½³ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒä½çš„æ•æ„Ÿæ€§ã€‚è¿›ä¸€æ­¥åœ°ï¼Œé€šè¿‡ä»Natural Instructionsæ•°æ®é›†ä¸­è¿ç§»å­¦ä¹ ï¼Œå¯ä»¥è¿›ä¸€æ­¥é™ä½æ¨¡å‹çš„æ•æ„Ÿæ€§ã€‚æœ€åï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åº¦é‡æ ‡å‡†sensitivityï¼Œç”¨äºè¡¡é‡æ¨¡å‹å¯¹åŒä¸€ä»»åŠ¡çš„ä¸åŒæŒ‡ä»¤çš„æ•æ„Ÿç¨‹åº¦ã€‚</sample>
    <sample id="124">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ—¶é—´æ¨ç†èƒ½åŠ›æ–¹é¢çš„ç ”ç©¶ä¸æ”¹è¿›ã€‚é¦–å…ˆï¼Œä½œè€…ä»‹ç»äº†ä¸‰ç§ç±»å‹çš„æ—¶é—´æ¨ç†ï¼šæ—¶é—´æ—¶é—´å…³ç³»ã€æ—¶é—´äº‹ä»¶å…³ç³»å’Œäº‹ä»¶äº‹ä»¶å…³ç³»ï¼Œå¹¶é€šè¿‡å…·ä½“ä¾‹å­å±•ç¤ºäº†ä¸åŒå±‚æ¬¡çš„æ—¶é—´æ¨ç†é—®é¢˜ã€‚æ¥ç€ï¼Œä½œè€…æŒ‡å‡ºä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨L2æ—¶é—´äº‹ä»¶æ¨ç†ä¸Šï¼Œè€Œå¿½ç•¥äº†æ›´é«˜çº§åˆ«çš„æ¨ç†ã€‚éšåï¼Œä½œè€…åˆ†äº«äº†ä¸€äº›åˆæ­¥å®éªŒçš„ç»“æœï¼Œè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å€¾å‘äºåå‘å½“ä»£å¹´ä»½ï¼Œä¸”åœ¨é¢„æµ‹æœˆä»½æ—¶è¡¨ç°ä¸å¦‚é¢„æµ‹å¹´ä»½å¥½ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºTempReasonçš„æ–°æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ¶µç›–äº†æ‰€æœ‰ä¸‰ç§ç±»å‹çš„æ—¶é—´æ¨ç†å’Œé•¿æ—¶é—´è·¨åº¦ï¼Œæ—¨åœ¨å…¨é¢è¯„ä¼°å’Œæå‡LLMsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚æœ€åï¼Œä½œè€…æå‡ºäº†ä¸€ç§è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºæé«˜LLMsçš„æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ€»ç»“äº†ç ”ç©¶çš„ä¸»è¦å‘ç°å’Œè´¡çŒ®ã€‚</sample>
    <sample id="125">è¿™ç¯‡è®ºæ–‡æœ‰ä¸ƒä½ä½œè€…ã€‚</sample>
    <sample id="126">æ˜¯çš„ï¼Œåœ¨è¯­ä¹‰è§£æä¹‹å‰ï¼Œä½¿ç”¨æœºå™¨ç¿»è¯‘æ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ä½œä¸ºåŸºçº¿ã€‚</sample>
    <sample id="127">The presentation discusses the capabilities of large language models in reasoning tasks, particularly focusing on the Chain-of-thought (CoT) reasoning technique introduced by Wei et al. in 2022. The study explores how this method enables complex reasoning in models with over 100 billion parameters and how it can be applied to smaller models through fine-tuning. The researchers use GPT-3 175B as a "reasoning teacher" to train smaller models like GPT-3 70M and 6.7B, demonstrating that diverse reasoning significantly boosts performance.

The methodology involves generating reasoning samples using the teacher model and curating them for training the student model. This approach is shown to be highly scalable under Fine-tune-CoT, enabling the transfer of reasoning abilities from large teachers to small students. The results indicate that fine-tuning with diverse reasoning can achieve state-of-the-art performance on various reasoning datasets, outperforming other methods like zero-shot and few-shot CoT.

The analysis and discussion section delve into the trade-offs between development-time costs (diverse reasoning, dataset size, teacher model) and inference-time costs (student model). The paper concludes that simple distillation can effectively transfer reasoning abilities from very large teachers to small students, making it an accessible and effective approach for single-domain applications. The research also highlights the potential for distillation to emerge reasoning in small language models, suggesting a connection with knowledge distillation techniques.</sample>
    <sample id="128">è¯¥ç ”ç©¶ä»‹ç»äº†KITMUSæµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°NLUæ¨¡å‹ä»å¤šä¸ªçŸ¥è¯†æºä¸­æ•´åˆçŸ¥è¯†çš„èƒ½åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè®¸å¤šæ¨¡å‹åœ¨å¤„ç†æ¥è‡ªé¢„è®­ç»ƒå’Œæ¨ç†æ—¶é—´çš„çŸ¥è¯†æ—¶è¡¨ç°ä¸ä½³ï¼Œå¼ºè°ƒäº†ä»»åŠ¡ç‰¹å®šè®­ç»ƒçš„é‡è¦æ€§ã€‚é€šè¿‡å®éªŒï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨æ¨ç†æ—¶é—´èƒŒæ™¯çŸ¥è¯†çš„æ•´åˆä¸Šå­˜åœ¨å›°éš¾ï¼Œå¹¶ä¸”æå‡ºä¸‰ç§æ¨¡å‹é…ç½®ï¼šèƒŒæ™¯-é¢„è®­ç»ƒã€èƒŒæ™¯-ä¸¤è€…å’ŒèƒŒæ™¯-æ¨ç†ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†ä¸åŒæ¨¡å‹é…ç½®ä¸‹çš„æ€§èƒ½å¯¹æ¯”ï¼Œè¡¨æ˜ä»»åŠ¡ç‰¹å®šè®­ç»ƒå¯¹äºçŸ¥è¯†æ•´åˆè‡³å…³é‡è¦ã€‚</sample>
    <sample id="129">æ ¹æ®æ‰€ç»™çš„è‹±æ–‡å†…å®¹ï¼Œä½œè€…ç»™å‡ºçš„â€œæ˜¾æ€§ç¾¤ä½“â€(marked group) çš„ç¤ºä¾‹æ˜¯â€œa woman warriorâ€ã€‚</sample>
    <sample id="130">ä¼ ç»Ÿçš„æ¨¡å‹æ¶æ„ï¼Œå¦‚Flairã€ELMoå’ŒELMo-Mixæ¨¡å‹æ³›åŒ–èƒ½åŠ›è¾ƒå·®ã€‚</sample>
    <sample id="131">Cleanly labeled test data</sample>
    <sample id="132">è¿™ç¯‡è®ºæ–‡æœ‰å…­ä½ä½œè€…ã€‚</sample>
    <sample id="133">The authors used multimodal instruction tuning, as indicated by the mention of "Instruction Tuning on Multimodal Pre-trained Models" in the text.</sample>
    <sample id="135">è¯¥ç ”ç©¶æ—¨åœ¨è¯„ä¼°èŠå¤©å¼å¯¹è¯ç³»ç»Ÿçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ABC-Evalæ–¹æ³•æ¥è¡¡é‡å…¶è´¨é‡ã€‚ç ”ç©¶è€…ä½¿ç”¨äº†å››ç§å¼€æ”¾é¢†åŸŸå¯¹è¯æ¨¡å‹ï¼Œå¹¶è¿›è¡Œäº†100æ¬¡äººæœºå¯¹è¯æµ‹è¯•ã€‚ABC-Evalæ–¹æ³•åŒ…æ‹¬è¡Œä¸ºæ ‡æ³¨ã€è½¬è½®è¯„ä»·å’Œå¯¹è¯è¯„ä»·ä¸‰ç§æ–¹å¼ï¼Œç”¨äºè¯„ä¼°å¯¹è¯çš„è´¨é‡ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡ä¸åŒæ¨¡å‹åœ¨æŸäº›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä½†æ•´ä½“ä¸Šå­˜åœ¨ä¸€äº›å…±æ€§é—®é¢˜ï¼Œå¦‚ä¸ç›¸å…³ã€è‡ªæˆ‘çŸ›ç›¾ã€ç¼ºä¹åŒç†å¿ƒç­‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†é¢„æµ‹æœ‰æ•ˆæ€§ä¸å¢é‡æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜ABC-Evalæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¹¶è§£é‡Šå¯¹è¯ä¸­çš„è´¨é‡é—®é¢˜ã€‚æœ€åï¼Œç ”ç©¶æä¾›äº†è¯¦ç»†çš„é”™è¯¯ç‡åˆ†æï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å¼±ç‚¹ï¼Œä»è€Œæ”¹è¿›ç³»ç»Ÿæ€§èƒ½ã€‚</sample>
    <sample id="136">The video presentation by Jasivan Alex Sivakumar and Nafise Sadat Moosavi from the University of Sheffield, UK, focuses on the development and evaluation of FERMAT, an alternative to accuracy for numerical reasoning in language models. The presentation begins with an introduction to the motivation behind their research, highlighting the limitations of current benchmarks that focus solely on accuracy and ignore the diversity of mathematical expressions.

The researchers argue that existing benchmarks are unrepresentative and single scores limit the understanding of models. They introduce FERMAT as a more informative alternative that evaluates language and mathematical diversity. The presentation details how FERMAT is generated using Illinois* and CommonCore*, focusing on mathematical word questions. It outlines the process of generating FERMAT, including the use of diverse datasets and the creation of mathematical operations.

The presentation then delves into the evaluation of FERMAT through zero-shot and fine-tuned evaluations. Zero-shot evaluation shows that FERMAT outperforms existing benchmarks across various mathematical operations, while fine-tuned evaluation demonstrates its effectiveness in handling complex mathematical expressions. The researchers also discuss the impact of training templates on model performance, showing that diverse training templates lead to better generalization.

The conclusion summarizes that FERMAT provides a more comprehensive evaluation framework, emphasizing the importance of language and mathematical diversity in assessing model capabilities. The presentation ends with a thank you message to the audience, encouraging further research in this area.</sample>
    <sample id="137">è¿™ç¯‡è®ºæ–‡ä¸»è¦æ¢è®¨äº†è¯­è¨€å¼•å¯¼çš„è®¾è®¡ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å»ºç­‘é¢†åŸŸä¸­çš„æ¥¼å¹³é¢è®¾è®¡ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºTell2Designï¼ˆT2Dï¼‰çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå¯¹åº”çš„æ¥¼å¹³é¢å›¾ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç”¨æˆ·åå¥½æ¥ç”Ÿæˆæ¥¼å¹³é¢è®¾è®¡ã€‚ç ”ç©¶è€…æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬é€šå¸¸å…³æ³¨ä»å¥å­çº§æè¿°ä¸­ç†è§£é«˜å±‚æ¬¡è§†è§‰æ¦‚å¿µï¼Œè€Œç”Ÿæˆçš„å›¾åƒæ›´æ³¨é‡ç°å®æ€§å’Œåˆ›é€ æ€§ï¼Œæ›´é€‚åˆç”¨äºç”Ÿæˆè‰ºæœ¯ä½œå“ã€‚

ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªSeq2Seqæ¨¡å‹ä½œä¸ºå¼ºåŸºçº¿ï¼Œå¹¶å°†å…¶ä¸å‡ ç§æ–‡æœ¬æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒT2Dæ¨¡å‹åœ¨æ¥¼å¹³é¢ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨åƒç´ çº§LODåˆ†æ•°ä¸Šï¼Œå…¶æ€§èƒ½è¿œè¶…æ‰€æœ‰åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œäººå·¥æŒ‡ä»¤å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›¸äº’è¡¥å……ï¼Œä¸ºæ¨¡å‹æä¾›äº†æœ‰ç›Šçš„æ•°æ®éƒ¨åˆ†ã€‚

ç ”ç©¶è€…è¿˜è¯¦ç»†ä»‹ç»äº†ä»–ä»¬çš„æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬ä½¿ç”¨Transformer-based encoder-decoderç»“æ„æ„å»ºSeq2Seqæ¨¡å‹ï¼Œä»¥åŠå¦‚ä½•å°†æˆ¿é—´è¾¹ç•Œæ¡†é‡æ–°æ„é€ ä¸ºç»“æ„åŒ–ç›®æ ‡åºåˆ—ã€‚é€šè¿‡è¿™äº›æ–¹æ³•ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆç¬¦åˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„æ¥¼å¹³é¢è®¾è®¡ã€‚æœ€åï¼Œç ”ç©¶è€…å¼ºè°ƒäº†è¿™é¡¹å·¥ä½œçš„æ„ä¹‰ï¼Œå³ä¸ºè¯­è¨€å¼•å¯¼çš„è®¾è®¡ç”Ÿæˆä»»åŠ¡æä¾›äº†ä¸€ä¸ªåšå®çš„åŸºç¡€ï¼Œå¹¶å¸Œæœ›æœªæ¥çš„ç ”ç©¶èƒ½å¤Ÿè¿›ä¸€æ­¥æ¨è¿›è¿™ä¸€é¢†åŸŸçš„è¿›å±•ã€‚</sample>
    <sample id="138">çŸ¥è¯†æ•´åˆå’Œæ¨ç†ã€‚</sample>
    <sample id="139">Zhiyang Xu, Ying Shen å’Œ Lifu Huangã€‚</sample>
    <sample id="140">æ˜¯çš„ï¼ŒCoscript ç»è¿‡äº†è´¨é‡æ£€æŸ¥ã€‚</sample>
    <sample id="141">ç°æœ‰çš„èµ„æºæ”¯æŒæœ‰é™çš„è¯­ç¯‡ç°è±¡å’Œè¯­è¨€ã€‚</sample>
    <sample id="142">å¥½çš„ï¼Œè¯·æä¾›æ‚¨éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬ï¼Œæˆ‘ä¼šå°†å…¶ç¿»è¯‘æˆä¸­æ–‡ã€‚</sample>
    <sample id="143">è¯¥æ–¹æ³•ä¸ waik-kã€LAã€CAAT å’Œ EDAs è¿™å››ç§ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒã€‚</sample>
    <sample id="144">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬Avignon Universityã€Nantes Universityã€LS2Nã€Clinique des donnÃ©es, CHU de Nanteså’ŒZamidocã€‚</sample>
    <sample id="145">Jenny T. Liang</sample>
    <sample id="146">è¯¥ç ”ç©¶æ—¨åœ¨æ¢è®¨å¯¹è¯æ‘˜è¦ä¸­çš„çœç•¥ç°è±¡ï¼Œé€šè¿‡åˆ†æä¸åŒåœºæ™¯ä¸‹çš„å¯¹è¯æ‘˜è¦é”™è¯¯ç±»å‹ï¼Œå¦‚ä¿¡æ¯ç¼ºå¤±ã€é‡å¤ä¿¡æ¯ã€é”™è¯¯å¼•ç”¨å’Œä¸å½“çš„æ€§åˆ«ä»£è¯ç­‰ï¼Œå¹¶æŒ‡å‡ºè¿™äº›é”™è¯¯å¯¹æ‘˜è¦è´¨é‡æœ‰é‡å¤§å½±å“ã€‚ç ”ç©¶è¿˜å±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¤„ç†å¯¹è¯æ‘˜è¦æ—¶çš„æ€§èƒ½å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿å¯¹è¯æ‘˜è¦ä¸­ï¼Œæ¨¡å‹åœ¨æ•æ‰å…³é”®ä¿¡æ¯æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°ä»»åŠ¡â€”â€”çœç•¥æ£€æµ‹ï¼Œå³è¯†åˆ«å¯¹è¯æ‘˜è¦ä¸­çš„çœç•¥ä¿¡æ¯ï¼Œè¿™æœ‰åŠ©äºæé«˜æ‘˜è¦çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ„å»ºäº†ä¸€ä¸ªåä¸ºOLDsçš„æ–°æ•°æ®é›†ï¼Œç”¨äºçœç•¥æ£€æµ‹ä»»åŠ¡ï¼Œæ¶µç›–äº†äº”ä¸ªé¢†åŸŸå’Œäº”ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªå¯¹è¯ç”Ÿæˆ10ä¸ªå€™é€‰æ‘˜è¦ã€‚</sample>
    <sample id="147">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="148">å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

1. Attention as a Guide for Simultaneous Speech Translation
   æ³¨æ„åŠ›ä½œä¸ºå®æ—¶è¯­éŸ³ç¿»è¯‘çš„æŒ‡å—

2. What is Simultaneous Speech Translation?
   ä»€ä¹ˆæ˜¯å®æ—¶è¯­éŸ³ç¿»è¯‘ï¼Ÿ

3. Simultaneous speech translation (SimuST) is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication.
   å®æ—¶è¯­éŸ³ç¿»è¯‘ï¼ˆSimuSTï¼‰æ˜¯å°†å£å¤´è¯­è¨€å®æ—¶ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€çš„æ–‡æœ¬çš„è¿‡ç¨‹ï¼Œä»è€Œå®ç°è·¨è¯­è¨€äº¤æµã€‚

4. What are the problems of the current SimuST models?
   å½“å‰çš„SimuSTæ¨¡å‹å­˜åœ¨å“ªäº›é—®é¢˜ï¼Ÿ

5. Specific architectures are usually trained, introducing additional modules to be optimized.
   é€šå¸¸ä¼šè®­ç»ƒç‰¹å®šçš„æ¶æ„ï¼Œå¹¶å¼•å…¥é¢å¤–çš„æ¨¡å—è¿›è¡Œä¼˜åŒ–ã€‚

6. Long and complicated training procedures (e.g., different optimization objectives).
   é•¿è€Œå¤æ‚çš„è®­ç»ƒç¨‹åºï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„ä¼˜åŒ–ç›®æ ‡ï¼‰ã€‚

7. Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, 25s...).
   è®­ç»ƒå’Œç»´æŠ¤å¤šä¸ªæ¨¡å‹ä»¥è¾¾åˆ°ä¸åŒçš„å»¶è¿Ÿæ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œ1ç§’ã€2ç§’ã€25ç§’ç­‰ï¼‰ã€‚

8. What is our solution?
   æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ

9. Use already existing offline ST models without re-training or adopting specific architecture for SimuST.
   ä½¿ç”¨ç°æœ‰çš„ç¦»çº¿STæ¨¡å‹ï¼Œè€Œä¸é‡æ–°è®­ç»ƒæˆ–é‡‡ç”¨ç‰¹å®šçš„æ¶æ„æ¥é€‚åº”SimuSTã€‚

10. Use only one model for every latency regime and handle latency through specific guarantees.
    å¯¹äºæ¯ä¸ªå»¶è¿Ÿæ¨¡å¼åªä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶é€šè¿‡ç‰¹å®šçš„ä¿è¯æ¥å¤„ç†å»¶è¿Ÿã€‚

11. Our solution: EDAtt
    æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆï¼šEDAtt

12. Encoder-Decoder Attention
    ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›

13. Decide whether to emit or not a partial translation based on where attention points to:
    æ ¹æ®æ³¨æ„åŠ›æŒ‡å‘çš„ä½ç½®å†³å®šæ˜¯å¦å‘å‡ºéƒ¨åˆ†ç¿»è¯‘ï¼š
    - å¦‚æœæ³¨æ„åŠ›é›†ä¸­åœ¨æŸä¸ªè¯ä¸Šï¼Œåˆ™è¯¥è¯è¢«å‘å‡ºã€‚
    - å¦‚æœæ³¨æ„åŠ›åˆ†æ•£ï¼ˆå…¶å’Œä½äºé˜ˆå€¼Î¸ï¼‰ï¼Œåˆ™å‘æœ€åä¸€ä¸ªç¨³å®šçš„ä¿¡æ¯å‘å‡ºä¸€ä¸ªè¯ã€‚

14. I am going to talk about...
    æˆ‘å°†è¦è°ˆè®º...

15. Ich werde reden.
    æˆ‘å°†è¦è®²è¯ã€‚

16. Encoder-Decoder Attention
    ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›

17. Decide whether to emit or not a partial translation based on where attention points to:
    æ ¹æ®æ³¨æ„åŠ›æŒ‡å‘çš„ä½ç½®å†³å®šæ˜¯å¦å‘å‡ºéƒ¨åˆ†ç¿»è¯‘ï¼š
    - å¦‚æœæ³¨æ„åŠ›é›†ä¸­åœ¨æŸä¸ªè¯ä¸Šï¼Œåˆ™è¯¥è¯è¢«å‘å‡ºã€‚
    - å¦‚æœæ³¨æ„åŠ›åˆ†æ•£ï¼ˆå…¶å’Œä½äºé˜ˆå€¼Î¸ï¼‰ï¼Œåˆ™å‘æœ€åä¸€ä¸ªç¨³å®šçš„ä¿¡æ¯å‘å‡ºä¸€ä¸ªè¯ã€‚

18. Main Results: EDAtt
    ä¸»è¦ç»“æœï¼šEDAtt

19. EDAtt outperforms all the strategies applied to offline models.
    EDAttä¼˜äºæ‰€æœ‰åº”ç”¨äºç¦»çº¿æ¨¡å‹çš„ç­–ç•¥ã€‚

20. EDAtt is the fastest strategy if we consider the actual elapsed time.
    å¦‚æœè€ƒè™‘å®é™…è€—æ—¶ï¼ŒEDAttæ˜¯æœ€å¿«é€Ÿçš„ç­–ç•¥ã€‚

21. Read our paper to discover more results!
    è¯·é˜…è¯»æˆ‘ä»¬çš„è®ºæ–‡ä»¥å‘ç°æ›´å¤šç»“æœï¼</sample>
    <sample id="149">æ˜¯çš„ï¼Œæ•°æ®é›†å…¬å¼€ã€‚</sample>
    <sample id="150">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨ä¼šè®®è®°å½•ä¸­è¿›è¡ŒæŠ½å–å¼é—®ç­”ï¼ˆExtractive Question-Answering, EQAï¼‰çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¼€æ”¾æ€§é—®é¢˜å’Œè®¨è®ºå¯†é›†å‹é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡å·²æœ‰å·¥ä½œé›†ä¸­åœ¨æ‘˜è¦å’Œæå–è¡ŒåŠ¨é¡¹ç›®ä¸Šï¼Œä½†è¿™äº›æ–¹æ³•æœªèƒ½å……åˆ†åˆ©ç”¨ä¼šè®®è®¨è®ºä¸­çš„é—®ç­”ç»„ä»¶ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºMeetingQAçš„æ–°æ•°æ®é›†ï¼ŒåŸºäºå‚ä¸è€…åœ¨ä¼šè®®ä¸­æå‡ºçš„é—®é¢˜åŠå…¶å¯¹åº”çš„ç­”æ¡ˆå¥å­æ„å»ºè€Œæˆã€‚è¯¥æ•°æ®é›†çš„ç‰¹ç‚¹åŒ…æ‹¬é•¿æ–‡æ¡£ã€é¢†åŸŸç‰¹å®šä¸”ä¿¡æ¯ä¸°å¯Œï¼Œä»¥åŠåŒ…å«å¤šè½®å¯¹è¯å’Œå¤šæ¨¡æ€ä¿¡æ¯ã€‚ç ”ç©¶è¿˜è¯¦ç»†ä»‹ç»äº†æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»å…¬å…±AMIè¯­æ–™åº“ä¸­è·å–100å°æ—¶çš„æ‰‹åŠ¨è½¬å½•å¤šå…šä¼šè®®è®°å½•ï¼Œå¹¶é€šè¿‡æ ‡æ³¨è€…å¯¹ç­”æ¡ˆå¥å­è¿›è¡Œæ ‡æ³¨ï¼Œç¡®ä¿é«˜å†…ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶åˆ†æäº†æ•°æ®é›†çš„ç»Ÿè®¡ç‰¹æ€§ï¼Œå¦‚30%çš„ä¸å¯å›ç­”é—®é¢˜ã€40%çš„è·¨å¥ç­”æ¡ˆç­‰ï¼Œå¹¶é€šè¿‡å›¾è¡¨å±•ç¤ºäº†ä¸åŒé—®é¢˜ç±»å‹çš„åˆ†å¸ƒæƒ…å†µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œäººç±»æ€§èƒ½ä¸æ¨¡å‹æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œè¿™è¡¨æ˜å½“å‰çš„QAæ¨¡å‹åœ¨å¤„ç†å¼€æ”¾æ€§å’Œè®¨è®ºå¯†é›†å‹é—®é¢˜æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ€åï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒæ¨¡å‹åœ¨çŸ­ä¸Šä¸‹æ–‡å’Œé•¿ä¸Šä¸‹æ–‡æ¨¡å‹ä¸Šçš„è¡¨ç°å·®å¼‚ï¼Œå¹¶æå‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬ä½¿ç”¨é“¶æ•°æ®å¢å¼ºå’Œæ›´å¤§æŒ‡ä»¤å¾®è°ƒæ¨¡å‹æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</sample>
    <sample id="151">é€šè¿‡æŒ‡ä»¤å¾®è°ƒæé«˜å¤šæ¨¡æ€é›¶æ ·æœ¬å­¦ä¹ </sample>
    <sample id="152">The presentation focuses on exploring large language models for classical philology, specifically in the context of Latin and Ancient Greek. It begins by introducing the authors, Frederick Riemenschneider and Anette Frank, and the conference where this research was presented, ACL 2023. The study aims to address the limitations of existing models like Latin BERT and Ancient Greek BERT, which lack comprehensive pre-training datasets and evaluation metrics.

The researchers propose new language models tailored for classical philology, emphasizing the importance of making these models comparable and pushing the state-of-the-art. They introduce multilingual models that can handle multiple languages, including Greek, Latin, and English, using architectures such as GreBERTa and PhilBERTa. These models are trained on high-quality datasets, including Open Greek &amp; Latin, Greek Medieval Texts, Patrologia Graeca, and the Internet Archive, which collectively provide over 150 million tokens for pre-training.

The evaluation process involves official data splits, direct compatibility with existing models, and achieving state-of-the-art results. The presentation also highlights the use of advanced techniques like PoS tagging, dependency parsing, and lemmatization, which are crucial for tasks such as semantic and world knowledge extraction. The results demonstrate the effectiveness of these models in handling complex linguistic structures and improving accuracy in tasks like dependency parsing and PoS tagging.

Overall, the presentation underscores the potential of new strong language models in advancing classical philology research, offering robust tools for scholars and linguists to analyze ancient texts more effectively.</sample>
    <sample id="153">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„è¯­ä¹‰æ¨¡ç³Šæ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è€…é¦–å…ˆæŒ‡å‡ºï¼Œå½“å‰çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šæ€§æç¤ºæ—¶å­˜åœ¨æŒ‘æˆ˜ï¼Œä¾‹å¦‚â€œä¸€åªå¤§è±¡å’Œä¸€åªé¸Ÿåœ¨é£è¡Œâ€è¿™æ ·çš„æè¿°å¯èƒ½å¼•å‘å¤šç§è§£é‡Šã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸¤ç§æ–¹æ³•ï¼šä¸€ç§æ˜¯é€šè¿‡ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼ˆQA-TIEDï¼‰æ¥æ˜ç¡®æ„å›¾ï¼›å¦ä¸€ç§æ˜¯ç”Ÿæˆå¯èƒ½çš„è§†è§‰è®¾ç½®ï¼ˆVS-TIEDï¼‰ã€‚è¿™ä¸¤ç§æ–¹æ³•æ—¨åœ¨é€šè¿‡è‡ªåŠ¨è¯„ä¼°å’Œäººç±»è¯„ä¼°ç›¸ç»“åˆçš„æ–¹å¼ï¼Œæé«˜ç”Ÿæˆå›¾åƒä¸åŸå§‹æ„å›¾çš„ä¸€è‡´æ€§ã€‚

ç ”ç©¶è¿˜æ„å»ºäº†ä¸€ä¸ªåä¸ºText-to-image Ambiguity Benchmark (TAB)çš„æ•°æ®é›†ï¼Œç”¨äºæµ‹è¯•å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨å¤„ç†æ¨¡ç³Šæ€§æç¤ºæ—¶çš„è¡¨ç°ã€‚é€šè¿‡ä½¿ç”¨DALL-E Megaå’ŒOptical DALL-Eç­‰æ¨¡å‹è¿›è¡Œå®éªŒï¼Œç ”ç©¶å‘ç°ï¼Œè™½ç„¶ä¸åŒæ¨¡å‹åœ¨å¤„ç†ç‰¹å®šç±»å‹æ¨¡ç³Šæ€§æ—¶è¡¨ç°å„å¼‚ï¼Œä½†æ€»ä½“ä¸Šï¼Œæ¶ˆæ­§ä¹‰å¯¹ç”Ÿæˆçš„å›¾åƒå¿ å®åº¦æœ‰ç§¯æå½±å“ã€‚æ­¤å¤–ï¼Œè‡ªåŠ¨è¯„ä¼°ä¸äººç±»è¯„ä¼°ç»“æœå…·æœ‰åˆç†çš„å…±è¯†ã€‚

æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†æœªæ¥å·¥ä½œçš„æ–¹å‘ï¼ŒåŒ…æ‹¬è¿›ä¸€æ­¥æ¢ç´¢æ›´å¤šç±»å‹çš„æ¨¡ç³Šæ€§ä»¥åŠå¼€å‘æ›´æœ‰æ•ˆçš„è¯„ä¼°æ¡†æ¶ã€‚</sample>
    <sample id="154">The authors of the paper belong to the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">æ¼”è®²è€…çš„åå­—æ˜¯Mohammad Javad Hosseiniã€‚</sample>
    <sample id="157">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†å¯¹è¯æ‘˜è¦æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ©ç”¨é™æ€-åŠ¨æ€ç»“æ„èåˆå›¾ï¼ˆStatic-Dynamic Structure Fusion Graphï¼‰æ¥æé«˜å¯¹è¯æ‘˜è¦çš„è´¨é‡ã€‚ç ”ç©¶é¦–å…ˆä»‹ç»äº†å¯¹è¯æ‘˜è¦çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨åœºæ™¯ï¼Œé€šè¿‡ä¸€ä¸ªç®€å•çš„å¯¹è¯ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä»å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆæ‘˜è¦ã€‚æ¥ç€ï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºSDDSï¼ˆStatic-Dynamic graph-based Dialogue Summarizationï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†é™æ€å›¾å’ŒåŠ¨æ€å›¾çš„ä¼˜åŠ¿ï¼Œä»¥æ›´å¥½åœ°æ•æ‰å¯¹è¯ä¸­çš„è¯­ä¹‰å…³ç³»ã€‚

SDDSæ¡†æ¶ç”±å››ä¸ªä¸»è¦æ¨¡å—ç»„æˆï¼šUtterance Encoderã€Static Graph Constructionã€Static-Dynamic Graph Moduleå’ŒSummary Generatorã€‚Utterance Encoderè´Ÿè´£å°†å¯¹è¯ä¸­çš„æ¯ä¸ªå¥å­ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºï¼›Static Graph Constructionæ„å»ºé™æ€å›¾ï¼Œç”¨äºæ•æ‰å¯¹è¯ä¸­é™æ€çš„è¯­ä¹‰å…³ç³»ï¼›Static-Dynamic Graph Moduleåˆ™èåˆäº†é™æ€å›¾å’ŒåŠ¨æ€å›¾çš„ä¿¡æ¯ï¼Œä»¥åŠ¨æ€åœ°è°ƒæ•´è¯­ä¹‰å…³ç³»ï¼›æœ€åï¼ŒSummary Generatoræ ¹æ®ä¸Šè¿°æ¨¡å—ç”Ÿæˆæœ€ç»ˆçš„æ‘˜è¦ã€‚

ç ”ç©¶è¿˜è¯¦ç»†ä»‹ç»äº†é™æ€å›¾çš„æ„å»ºè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨ä¾èµ–è§£æå·¥å…·æ„å»ºå¯¹è¯ç»“æ„ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡å…³é”®è¯å…±ç°å‡½æ•°è®¡ç®—ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…±åŒå…³é”®è¯æ•°é‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†åŠ¨æ€å›¾æ¨¡å—å¦‚ä½•æ•æ‰å¯¹è¯ä¸­åŠ¨æ€çš„è¯­ä¹‰å…³ç³»ï¼Œå¹¶å°†å…¶ä¸é™æ€å›¾çš„ä¿¡æ¯ç›¸ç»“åˆï¼Œå½¢æˆç»Ÿä¸€çš„å›¾ç»“æ„ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•æ¥å¤„ç†å¯¹è¯æ‘˜è¦é—®é¢˜ï¼Œé€šè¿‡ç»“åˆé™æ€å’ŒåŠ¨æ€å›¾çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ›´å‡†ç¡®åœ°ç†è§£å’Œæ€»ç»“å¯¹è¯å†…å®¹ã€‚</sample>
    <sample id="158">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†åœ¨é•¿æ–‡æ¡£ç¥ç»æ ¸å¿ƒè¯­ä¹‰è§£æä¸­çš„åŒç¼“å­˜æ–¹æ³•ã€‚é¦–å…ˆï¼Œä½œè€…è§£é‡Šäº†æ ¸å¿ƒè¯­ä¹‰è§£æçš„æ¦‚å¿µï¼Œå³è¯†åˆ«å’Œé“¾æ¥æ–‡æœ¬ä¸­æŒ‡ä»£åŒä¸€å®ä½“æˆ–æ¦‚å¿µçš„æåŠã€‚æ¥ç€ï¼Œè®¨è®ºäº†ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†é•¿æ–‡æ¡£æ—¶é¢ä¸´çš„è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æ¶ˆè€—é—®é¢˜ï¼Œè¿™äº›æ–¹æ³•éœ€è¦æšä¸¾æ‰€æœ‰å¯èƒ½çš„æåŠå¯¹ï¼Œå¯¼è‡´è®¡ç®—å¤æ‚åº¦ä¸ºO(|D|^2)ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºç¼“å­˜çš„æ–¹æ³•ï¼Œä½¿ç”¨å›ºå®šå¤§å°çš„ç¼“å­˜å­˜å‚¨å®ä½“è¡¨ç¤ºï¼Œå°†å¤æ‚åº¦é™ä½åˆ°çº¿æ€§çº§åˆ«ï¼Œå³O(|C||D|)ï¼Œå…¶ä¸­|C|è¿œå°äº|D|ã€‚

è¿›ä¸€æ­¥ï¼Œä½œè€…è¯¦ç»†ä»‹ç»äº†åŒç¼“å­˜ç­–ç•¥çš„å·¥ä½œåŸç†ã€‚å½“ç¼“å­˜æ»¡è½½æ—¶ï¼Œä¼šæ ¹æ®LRUï¼ˆLeast Recently Usedï¼‰ç­–ç•¥ç§»é™¤ä¸€ä¸ªå®ä½“ã€‚ç”±äºé•¿æ–‡æ¡£ä¸­ä¸»é¢˜çš„é¢‘ç¹åˆ‡æ¢ï¼Œå®ä½“çš„æåŠå¯èƒ½ä¼šåˆ†æ•£åœ¨æ•´ä¸ªæ–‡æœ¬èŒƒå›´å†…ï¼Œè¿™ä¼šå¯¼è‡´LRUç­–ç•¥äº§ç”Ÿè¾ƒé«˜çš„ç¼“å­˜ç¼ºå¤±ç‡ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºäº†åŒç¼“å­˜æ–¹æ³•ï¼Œåˆ†åˆ«ä½¿ç”¨LRUå’ŒLFUï¼ˆLeast Frequently Usedï¼‰ç­–ç•¥ç®¡ç†æœ¬åœ°å’Œå…¨å±€å®ä½“ã€‚å…·ä½“æ¥è¯´ï¼ŒL-cacheç”¨äºå­˜å‚¨æœ€è¿‘ä½¿ç”¨çš„æœ¬åœ°å®ä½“ï¼Œè€ŒG-cacheåˆ™ç”¨äºå­˜å‚¨ä¸å¸¸ä½¿ç”¨çš„å…¨å±€å®ä½“ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒåŒç¼“å­˜æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ç®¡ç†å’Œå‡å°‘ç¼“å­˜ç¼ºå¤±ã€‚

æœ€åï¼Œä½œè€…å±•ç¤ºäº†å®éªŒç»“æœï¼Œè¡¨æ˜åŒç¼“å­˜æ–¹æ³•åœ¨å…¬å…±åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå•ç¼“å­˜æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨ä¹¦æœ¬çº§åˆ«çš„æ–‡æ¡£ä¸Šè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ã€‚æ€»ç»“æ¥çœ‹ï¼ŒåŒç¼“å­˜æ–¹æ³•é€šè¿‡åˆç†åœ°åˆ†ç¦»æœ¬åœ°å’Œå…¨å±€å®ä½“çš„ç¼“å­˜ç®¡ç†ï¼Œæœ‰æ•ˆæé«˜äº†æ•ˆç‡å¹¶å‡å°‘äº†ç¼“å­˜ç¼ºå¤±ï¼Œä»è€Œåœ¨æ€§èƒ½ä¸æˆæœ¬ä¹‹é—´å–å¾—äº†æœ€ä¼˜å¹³è¡¡ã€‚</sample>
    <sample id="159">è¯­è¨€æ¨¡å‹çš„å¯æ¥å—æ€§åˆ¤æ–­å¹¶ä¸æ€»æ˜¯å¯¹ä¸Šä¸‹æ–‡æ•æ„Ÿ

ACL 2023

Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, Adina Williams

é‡è®¿æœ€å°å¯¹å¶èŒƒå¼

æœ€å°å¯¹å¶èŒƒå¼ï¼ˆMPPï¼‰è¯„ä¼°ä½¿ç”¨åºåˆ—æ¦‚ç‡çš„ç›¸å¯¹å·®å¼‚æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„æŠ½è±¡çŸ¥è¯†ï¼š

BLIMP
1. è®¸å¤šäººåœ¨å¸®åŠ©è‡ªå·±ã€‚
2. â€œè®¸å¤šäººæ­£åœ¨å¸®åŠ©è‡ªå·±ã€‚â€

P(1) &gt; P(2)

SyntaxGym
1. æ²¡æœ‰é¡¾å®¢...å·²ç»èŠ±äº†å¾ˆå¤šé’±ã€‚
2. â€œé¡¾å®¢...å·²ç»èŠ±äº†å¾ˆå¤šé’±ã€‚â€

P(1.any) &gt; P(2.any)

CrowS
1. å¥³äººæ“…é•¿æ‰‹å·¥æ´»ã€‚
2. ç”·äººä¸æ“…é•¿æ‰‹å·¥æ´»ã€‚

P(1) &gt; P(2)

è¿™äº›åˆ¤æ–­æ˜¯å¦åœ¨é•¿å‰ç¼€è¯­å¢ƒä¸‹ç¨³å®šï¼Ÿ

æ–¹æ³•
æµ‹è¯•MPPåˆ¤æ–­æ˜¯å¦éšä¸Šä¸‹æ–‡é•¿åº¦ã€ç»“æ„åŒ¹é…å’Œå¯æ¥å—æ€§è€Œå˜åŒ–

Test Bulle: Subject Verb Agreement

Prefix Strategy: &lt;sent&gt;</sample>
    <sample id="160">è¯¥æ–¹æ³•çš„ç¬¬ä¸€æ­¥å°†è¾“å…¥è¯å…ƒæ˜ å°„åˆ°å¤šé›†æ ‡è®°çš„è¯å…ƒã€‚</sample>
    <sample id="161">55,000</sample>
    <sample id="163">DEplain çš„æœ€ä½³å¯¹é½æ–¹æ³•æ˜¯ä½¿ç”¨å¥å­åµŒå…¥ç›¸ä¼¼åº¦çš„å¤šè¯­è¨€å¯¹é½ã€‚</sample>
    <sample id="164">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="165">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œå¦‚ä½•é€šè¿‡åˆ©ç”¨äº’æ–¥è§£é‡Šæ¥å®ç°å½’çº³æ¨ç†ï¼ˆAbductive Reasoningï¼‰ã€‚ç ”ç©¶èƒŒæ™¯æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„å½’çº³æ¨ç†æ–¹æ³•ä¾èµ–äºäººå·¥æ ‡æ³¨çš„è§£é‡Šé›†ï¼Œè¿™ä¸ä»…è€—æ—¶ä¸”ä¸»è§‚æ€§å¼ºã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºLiPoRçš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ— ç›‘ç£å­¦ä¹ æ¥è‡ªåŠ¨å­¦ä¹ å’Œç”Ÿæˆåˆç†çš„è§£é‡Šã€‚

ç ”ç©¶é¦–å…ˆä»‹ç»äº†å½’çº³æ¨ç†çš„åŸºæœ¬æ¦‚å¿µï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å±•ç¤ºäº†å¦‚ä½•ä»ç»™å®šçš„ä¸Šä¸‹æ–‡å’Œç»“æœä¸­æ¨å¯¼å‡ºå¯èƒ½çš„è§£é‡Šã€‚æ¥ç€ï¼Œç ”ç©¶è®¨è®ºäº†ä¼ ç»Ÿæ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼Œå³äººå·¥æ ‡æ³¨è§£é‡Šçš„å™ªéŸ³æ€§å’Œä¸»è§‚æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†LiPoRæ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†è§£é‡Šè§†ä¸ºæ½œåœ¨å˜é‡ï¼Œé€šè¿‡æœ€å¤§åŒ–ç»™å®šä¸Šä¸‹æ–‡å’Œç»“æœçš„å¯¹æ•°ä¼¼ç„¶æ¥ä¼˜åŒ–æ¨¡å‹å‚æ•°ã€‚æ­¤å¤–ï¼ŒLiPoRè¿˜é¼“åŠ±æ¦‚ç‡è´¨é‡å‡½æ•°p(z|x,y)å‘ä¸€ç»„è§£é‡Šé›†ä¸­æ”¶æ•›ï¼Œä»è€Œå‡å°‘æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLiPoRæ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨Î±NLIä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚è¿™è¡¨æ˜LiPoRä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘äººå·¥æ ‡æ³¨çš„ä¾èµ–ï¼Œè¿˜èƒ½æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶è€…è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶æä¾›äº†è¿›ä¸€æ­¥é˜…è¯»çš„é“¾æ¥ï¼Œä»¥ä¾›æ„Ÿå…´è¶£çš„è¯»è€…æ·±å…¥äº†è§£ç›¸å…³æŠ€æœ¯ç»†èŠ‚ã€‚</sample>
    <sample id="166">è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„â€œåˆ†è€Œæ²»ä¹‹â€æ¨ç†æ¡†æ¶ï¼Œç”¨äºä»è¯­ä¹‰å¤æ‚æ–‡æœ¬ä¸­æ£€ç´¢å›¾åƒã€‚ç ”ç©¶é¦–å…ˆä»‹ç»äº†â€œåˆ†è€Œæ²»ä¹‹â€çš„æ¦‚å¿µï¼Œå³é€šè¿‡å°†å¤§é—®é¢˜åˆ†è§£ä¸ºå°é—®é¢˜æ¥è§£å†³ï¼Œæ¯ä¸ªå­é—®é¢˜éƒ½æœ‰å…¶ç‰¹å®šçš„è§£å†³æ–¹æ¡ˆã€‚æ¥ç€ï¼Œç ”ç©¶è€…è®¨è®ºäº†ä¸¤ç§ç³»ç»Ÿï¼šç³»ç»Ÿ1ä¸“æ³¨äºæ¨¡æ‹Ÿæ¨ç†ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†ç®€å•çš„å›¾åƒæ£€ç´¢ä»»åŠ¡ï¼›ç³»ç»Ÿ2åˆ™ä¾§é‡äºé€»è¾‘æ¨ç†ï¼Œé€‚åˆå¤„ç†å¤æ‚çš„æ¨ç†é—®é¢˜ã€‚ä¸ºäº†ç»“åˆè¿™ä¸¤ç§ç³»ç»Ÿçš„ä¼˜ç‚¹ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œå…¶ä¸­ç³»ç»Ÿ1è´Ÿè´£ç”Ÿæˆå‘½é¢˜è¡¨ç¤ºï¼Œç³»ç»Ÿ2åˆ™é€šè¿‡é€»è¾‘è¿ç®—ç¬¦ï¼ˆå¦‚å¦å®šå’Œåˆå–ï¼‰æ•´åˆè¿™äº›å‘½é¢˜è¡¨ç¤ºï¼Œä»¥å®ç°æ•´ä½“æè¿°çš„é€»è¾‘æ¨ç†ç»“æœã€‚

ç ”ç©¶è¿˜è¯¦ç»†ä»‹ç»äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šè§†è§‰-è¯­è¨€äº¤äº’å™¨å’Œç¥ç»ç¬¦å·æ¨ç†å™¨ã€‚è§†è§‰-è¯­è¨€äº¤äº’å™¨æ—¨åœ¨æ‰§è¡Œè§†è§‰å‘½é¢˜ä¿¡æ¯äº¤äº’ï¼Œæ•´åˆç³»ç»Ÿ1å’Œ2çš„ä¿¡æ¯ï¼›ç¥ç»ç¬¦å·æ¨ç†å™¨åˆ™è´Ÿè´£æ•´åˆç®€å•å‘½é¢˜çš„æ¨ç†çŠ¶æ€å’Œç»“æœï¼Œä»¥è·å¾—æœ€ç»ˆçš„æ¨ç†ç»“æœã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å±•ç¤ºäº†å®éªŒç»“æœï¼Œè¡¨æ˜è¯¥æ¡†æ¶åœ¨å›¾åƒæ£€ç´¢ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†åŒ…å«å¤šä¸ªå‘½é¢˜çš„å¤æ‚åœºæ™¯ã€‚æœ€åï¼Œç ”ç©¶è€…æ€»ç»“äº†â€œåˆ†è€Œæ²»ä¹‹â€ä¸â€œåŒè¿‡ç¨‹ç†è®ºâ€çš„è”ç³»ï¼ŒæŒ‡å‡ºå‰è€…ç±»ä¼¼äºè‡ªæˆ‘æé—®çš„æ€ç»´é“¾ï¼Œæ—¨åœ¨å°†å¤æ‚æ¨ç†åˆ†è§£ä¸ºç®€å•é—®é¢˜å¹¶æ„å»ºæ¨ç†è·¯å¾„ï¼Œä¸¤è€…éƒ½æœ‰æ•ˆè§£å†³å¤æ‚é—®é¢˜ã€‚</sample>
    <sample id="167">DEplain-web ä¸­çš„æ–‡æ¡£é‡‡ç”¨äº† 1:1 çš„å¯¹é½æ–¹æ³•ï¼Œå…¶ä¸­ 48% çš„æ–‡æ¡£ä½¿ç”¨äº†æ‰‹åŠ¨å¯¹é½æ–¹æ³•ï¼Œ52% ä½¿ç”¨äº†è‡ªåŠ¨å¯¹é½æ–¹æ³•ã€‚</sample>
    <sample id="168">CoNLL++ æ•°æ®é›†æ˜¯é€šè¿‡æ”¶é›†2020å¹´çš„è·¯é€ç¤¾æ–°é—»ï¼Œå¹¶ä½¿ç”¨CoNLL-2003æ³¨é‡ŠæŒ‡å—è¿›è¡Œæ ‡æ³¨è€Œåˆ›å»ºçš„ã€‚</sample>
    <sample id="169">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæœºå™¨ç¿»è¯‘ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹PaLMæ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›è¿›è¡Œäº†è¯„ä¼°ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆä»‹ç»äº†PaLMæ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶å‚æ•°é‡ã€è®­ç»ƒæ•°æ®é›†å’Œåœ¨å¤šä¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æ¥ç€ï¼Œä»–ä»¬è¯¦ç»†é˜è¿°äº†è‡ªå·±çš„è´¡çŒ®ï¼ŒåŒ…æ‹¬é¦–æ¬¡ç³»ç»Ÿæ€§ç ”ç©¶LLMåœ¨æœºå™¨ç¿»è¯‘ä¸­çš„æç¤ºç­–ç•¥ï¼Œå¹¶æå‡ºäº†åŸºäºæœ€ä½³å®è·µçš„ç¿»è¯‘èƒ½åŠ›è¯„ä¼°æ–¹æ³•ã€‚ç ”ç©¶ä¸­ä½¿ç”¨äº†æœ€æ–°çš„æµ‹è¯•é›†æ¥é¿å…è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„é‡å ï¼Œç¡®ä¿è¯„ä¼°çš„å…¬æ­£æ€§å’Œå‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¯”è¾ƒäº†SOTAç³»ç»Ÿçš„è¡¨ç°ï¼Œå¹¶é€šè¿‡ä¸“å®¶è¯„ä¼°æ¥æé«˜è¯„ä»·çš„å¯é æ€§ã€‚

å®éªŒç»“æœæ˜¾ç¤ºï¼Œæç¤ºè´¨é‡å¯¹ç¿»è¯‘è´¨é‡æœ‰æ˜¾è‘—å½±å“ï¼Œå¤§å¤šæ•°å¥å­çš„BLEURTå¾—åˆ†å·®å¼‚è¶…è¿‡1åˆ†ï¼Œæœ€é«˜å¯è¾¾40åˆ†ã€‚ç ”ç©¶è¿˜æä¾›äº†å…·ä½“çš„ç¿»è¯‘ç¤ºä¾‹ï¼Œå±•ç¤ºäº†ä¸åŒæç¤ºç­–ç•¥çš„æ•ˆæœå·®å¼‚ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†å®éªŒç»“æœï¼ŒæŒ‡å‡ºPaLMæ¨¡å‹åœ¨æµç•…åº¦ä¸Šä¸SOTAç³»ç»Ÿç›¸å½“ï¼Œä½†åœ¨å‡†ç¡®æ€§å’Œé£æ ¼/æªè¾æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç ”ç©¶ä¸ºLLMåœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„åº”ç”¨æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’ŒæŒ‡å¯¼ã€‚</sample>
    <sample id="170">- æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„è·¨è¯­è¨€è¯­ä¹‰è§£æåŸºå‡†ï¼Œé€‚ç”¨äºå¤šç§è‡ªç„¶è¯­è¨€å’Œè¯­ä¹‰è¡¨ç¤ºã€‚
- æˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†ç ”ç©¶ï¼Œé’ˆå¯¹ä¸‰ç§ä»£è¡¨æ€§çš„å¤šè¯­è¨€æ¨¡å‹ç±»å‹ã€‚
- æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒmT5ç»“åˆå•è¯­è¨€è®­ç»ƒå¯ä»¥å–å¾—æœ€ä½³æ€§èƒ½ï¼Œè€Œæ˜¾è‘—çš„å¤šè¯­è¨€LLMsåœ¨è¿›è¡Œè·¨è¯­è¨€è¯­ä¹‰è§£æä»»åŠ¡æ—¶ä»ç„¶ä¸è¶³ã€‚
- æ­¤å¤–ï¼Œå•è¯­è¨€è®­ç»ƒä¸è·¨è¯­è¨€è¿ç§»å­¦ä¹ ä¹‹é—´çš„æ€§èƒ½å·®è·ä»ç„¶å­˜åœ¨ã€‚</sample>
    <sample id="171">å…³äºç‰ˆæƒä¿æŠ¤çš„ç°æœ‰ç ”ç©¶åŒ…æ‹¬å‚æ•°æ°´å°ã€è¯æ³•æ°´å°ã€åé—¨åŸºæ°´å°å’Œå¯¹æŠ—æ€§åŸºæ°´å°ã€‚</sample>
    <sample id="172">ä¸è¶³å¤Ÿã€‚</sample>
    <sample id="174">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†ArgAnalysis35Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§å‹çš„æ•°æ®é›†ï¼Œç”¨äºåˆ†æè®ºæ®çš„è´¨é‡ã€‚é¦–å…ˆï¼Œè§†é¢‘è§£é‡Šäº†ä»€ä¹ˆæ˜¯è®ºæ®è´¨é‡åˆ†æï¼Œå³ç®€å•åœ°åˆ¤æ–­ä¸€ä¸ªè®ºæ®çš„å¥½åç¨‹åº¦ã€‚æ¥ç€ï¼Œé€šè¿‡ä¸€ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•å¯¹è®ºæ®è¿›è¡Œè¯„åˆ†ï¼Œä¾‹å¦‚â€œå¤§é“¶è¡Œæ˜¯åçš„â€è¿™ä¸€è®ºæ®è¢«åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šâ€œå¤§é“¶è¡Œæ˜¯åçš„â€ï¼Œâ€œå¤§é“¶è¡Œå†’é£é™©å¹¶ä¸”å®ƒä»¬å¾ˆåâ€ï¼Œä»¥åŠâ€œå¤§é“¶è¡Œæ²¡æœ‰é—®è´£åˆ¶ï¼Œå¹¶ä¸”æ‰¿æ‹…é‡å¤§é£é™©å¯èƒ½å¯¼è‡´é‡å¤§å´©æºƒï¼Œå½±å“æ•´ä¸ªç»æµï¼Œå› æ­¤åº”è¯¥è¢«æ‹†åˆ†â€ã€‚è¯„åˆ†åˆ†åˆ«ä¸º0.12ã€0.47å’Œ1ã€‚

éšåï¼Œè§†é¢‘è®¨è®ºäº†å½“å‰æ•°æ®é›†å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒ…æ‹¬ç¼ºä¹é«˜è´¨é‡çš„è®ºæ®ã€è®ºé¢˜å¤šæ ·æ€§ä¸è¶³ä»¥åŠè¯„åˆ†ä¸è®ºé¢˜å…³è”æ€§å·®ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒArgAnalysis35Kæ•°æ®é›†æä¾›äº†35,000ä¸ªé«˜è´¨é‡çš„è®ºæ®åˆ†æå¯¹ï¼Œè¿™äº›è®ºæ®ç›´æ¥æ¥æºäºè·èƒœè¾©è®ºå’Œè¾©è®ºè€…ï¼Œç¡®ä¿äº†è®ºæ®çš„è´¨é‡ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†è¿˜è€ƒè™‘äº†è®ºé¢˜çš„å¤šæ ·æ€§ï¼Œå¹¶æ·»åŠ äº†é€»è¾‘æ¨ç†é“¾çš„åˆ†æå…ƒç´ ï¼Œä½¿ç”¨å®ä¾‹åŸºæ³¨é‡Šè¯„åˆ†å‡½æ•°å’Œç›¸å…³æ€§æ¨¡å‹æ¥è¯„ä¼°è®ºæ®çš„åˆç†æ€§ã€‚

æœ€åï¼Œè§†é¢‘å¼ºè°ƒäº†ArgAnalysis35Kæ•°æ®é›†çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå…¶å¹¿æ³›çš„è®ºé¢˜è¦†ç›–ï¼ŒåŒ…æ‹¬æ”¿æ²»ã€ç¯å¢ƒã€ææƒä¸»ä¹‰åˆ¶åº¦ç­‰24ä¸ªä¸»é¢˜ï¼Œæ—¨åœ¨æ•æ‰å°½å¯èƒ½å¤šçš„è®ºé¢˜ã€‚åŒæ—¶ï¼Œè§†é¢‘è¿˜æåˆ°äº†è®ºæ®åˆ†æä¸­å¼•å…¥çš„åˆ†æå…ƒç´ ï¼Œå¦‚é€»è¾‘é“¾æ¥å’Œä¸»è§‚è§£é‡Šï¼Œä»¥åŒºåˆ†è®ºæ®å’Œåˆ†æã€‚</sample>
    <sample id="175">è¯¥æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥æ’åˆ—æ¥å¤„ç†æ’åˆ—çš„ä¸ç¡®å®šæ€§ã€‚</sample>
    <sample id="176">ä¸‹æ¸¸ NLP æ¨¡å‹çš„å…¬å¹³æ€§æ˜¯æŒ‡æ¨¡å‹åœ¨å¤„ç†ä¸åŒèº«ä»½ç¾¤ä½“å’Œé”™è¯¯ä¿¡æ¯æ—¶çš„è¡¨ç°ã€‚</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">The presentation focuses on the development and evaluation of SymbolicToM, an inference-time method designed to enhance the Theory of Mind (ToM) reasoning skills in Large Language Models (LLMs). The core idea is to use explicit graphical symbolic representations to track local context and improve understanding of multiple characters' mental states, particularly in false-belief scenarios.

The study begins by defining ToM as the ability to reason about others' mental states, traditionally measured through reading comprehension tasks involving multiple characters. It highlights the Sally-Anne Test as a classic example, where participants must infer what another character believes based on their actions. The presentation then discusses how LLMs often struggle with these tasks due to their lack of explicit reasoning capabilities.

To address this limitation, the researchers propose SymbolicToM, which employs an inference-time algorithm that leverages off-the-shelf Natural Language Inference (NLI) and OpenIE models to compute belief graphs for all possible combinations of characters up to a predefined depth. This method allows the model to reason about the world state from different perspectives, thereby improving its performance in ToM tasks.

The experiments conducted evaluate the performance of SymbolicToM against various baselines, including Macaw-3B, GPT3-Curie, Flan-T5-XL, LLaMA-(7B, 13B), GPT3.5, and GPT4. Results show significant improvements in second-order false-belief questions, with SymbolicToM outperforming supervised approaches on both in-domain and out-of-domain datasets like Textual Time Travel and ParaphrasedToM.

In conclusion, SymbolicToM represents a promising approach for enhancing LLMs' ToM reasoning skills, offering a plug-and-play solution that avoids overfitting risks and provides more interpretable reasoning. Its effectiveness is demonstrated through improved performance across diverse datasets, making it a valuable tool for advancing the field of AI and natural language processing.</sample>
    <sample id="180">Myra Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="181">è¿™æ®µè‹±è¯­å†…å®¹ä¸»è¦æ¢è®¨äº†å¦‚ä½•ä»å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æå–è„šæœ¬çŸ¥è¯†ä»¥å¢å¼ºè¯­è¨€è§„åˆ’èƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºâ€œScript Distillation from LLMsâ€çš„æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬åˆ©ç”¨InstructGPTç­‰æ¨¡å‹ç”Ÿæˆç‰¹å®šçš„è„šæœ¬ï¼Œå¹¶é€šè¿‡ä¸æŒ‡ä»¤é›†å­¦ä¹ ç›¸ç»“åˆçš„æ–¹å¼ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–è¿™äº›è„šæœ¬ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½å¤Ÿæé«˜è„šæœ¬çš„è´¨é‡ï¼Œè¿˜èƒ½ç¡®ä¿ç”Ÿæˆçš„è„šæœ¬ç¬¦åˆç»™å®šçš„çº¦æŸæ¡ä»¶ã€‚

ç ”ç©¶è€…æŒ‡å‡ºï¼Œå°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†è§£ç›®æ ‡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¤æ‚å’Œå¤šæ ·çš„ç›®æ ‡æ—¶å­˜åœ¨å±€é™æ€§ã€‚å› æ­¤ï¼Œä»–ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œCoscriptâ€çš„é«˜è´¨é‡è„šæœ¬æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŸºäºä»–ä»¬çš„æ–¹æ³•ç”Ÿæˆï¼Œç”¨äºéªŒè¯å’Œæµ‹è¯•æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä½¿ç”¨äº†DeBERTaï¼ˆv3 largeï¼‰æ¨¡å‹æ¥åˆ¤æ–­ç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦å¿ å®äºçº¦æŸæ¡ä»¶ï¼Œå¹¶å¼•å…¥äº†ROUGEã€BLEUå’ŒBERTScoreç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„è¡¨ç°ã€‚

æ€»ç»“æ¥çœ‹ï¼Œè¿™é¡¹ç ”ç©¶é€šè¿‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³Script Distillation from LLMsï¼ŒæˆåŠŸåœ°æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé«˜è´¨é‡è„šæœ¬æ–¹é¢çš„è¡¨ç°ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é€‚ç”¨äºå½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºé‚£äº›éœ€è¦å¤„ç†æ›´å¤æ‚å’Œå¤šæ ·åŒ–ç›®æ ‡çš„è¯­è¨€è§„åˆ’ä»»åŠ¡ã€‚</sample>
    <sample id="182">åœ¨æœ¬æ–‡çš„èƒŒæ™¯ä¸‹ï¼Œçƒ­å¸¦ä¸»ä¹‰æ„å‘³ç€ä¸€ç§æ–‡åŒ–ã€ä¼ ç»Ÿã€è‡ªè±ªå’Œå¤–å‘çš„ç‰¹è´¨ï¼Œè¿™äº›ç‰¹è´¨è¢«èµ‹äºˆäº†ç‰¹å®šç¾¤ä½“ã€‚</sample>
    <sample id="183">ä½œè€…é€šè¿‡ä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ¥åˆ›å»ºç›®æ ‡ç¾¤ä½“çš„äººå·¥æå†™ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬ä½¿ç”¨äº†â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ï¼Œæè¿°ä½ è‡ªå·±â€è¿™æ ·çš„æŒ‡ä»¤ä½œä¸ºæç¤ºï¼Œè®©æ¨¡å‹æ ¹æ®è¿™äº›æç¤ºç”Ÿæˆç›¸åº”çš„æå†™ã€‚</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="185">DrBERT æ˜¯ä¸€ä¸ªåŸºäº BERT çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ï¼Œè€Œ ChuBERT æ˜¯ä¸€ä¸ªåŸºäº CamemBERT çš„ç‰¹å®šé¢†åŸŸæ¨¡å‹ã€‚</sample>
    <sample id="187">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="188">è¿­ä»£è¿ç§»å­¦ä¹ æ˜¯ä¸€ç§æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡å‹åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šä¾æ¬¡è¿›è¡Œå¾®è°ƒã€‚</sample>
    <sample id="189">æ•°æ®é›†çš„ç›®æ ‡æ˜¯ç†è§£ç”¨æˆ·åœ¨åšå‡ºé€‰æ‹©æ—¶çš„è¯­è¨€ã€‚</sample>
    <sample id="190">æ”»å‡»è€…å¯èƒ½é€šè¿‡å­¦ä¹ åµŒå…¥å¹¶æä¾›ç±»ä¼¼çš„æœåŠ¡æ¥çªƒå–æ¨¡å‹ã€‚</sample>
    <sample id="191">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="192">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œå¦‚ä½•è®¾è®¡ä¸€ä¸ªåŒæ—¶å®ç°å¿«é€Ÿæ”¶æ•›å’Œä½å†…å­˜ä½¿ç”¨çš„ç›®æ ‡ä¼˜åŒ–å™¨ã€‚èƒŒæ™¯éƒ¨åˆ†æŒ‡å‡ºï¼Œç°æœ‰çš„è‡ªé€‚åº”æ¢¯åº¦ä¼˜åŒ–æ–¹æ³•å¦‚Adamã€LAMBç­‰éœ€è¦ä¸‰å€çš„å†…å­˜æ¥å­˜å‚¨å†å²ä¿¡æ¯ï¼Œè€Œç°æœ‰çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨å¦‚AdaFactorè™½ç„¶å‡å°‘äº†è¾…åŠ©å†…å­˜ä½¿ç”¨ï¼Œä½†ç‰ºç‰²äº†æ€§èƒ½ã€‚å› æ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„å†…å­˜é«˜æ•ˆä¼˜åŒ–å™¨CAMEï¼ˆConfidence-guided Adaptive Memory Efficient Optimizationï¼‰ï¼Œé€šè¿‡å¼•å…¥é¢„æµ‹æ›´æ–°ä¸ç”Ÿæˆæ›´æ–°ä¹‹é—´çš„å·®å¼‚æ€§æ¥æŒ‡å¯¼è‡ªé€‚åº”è¿‡ç¨‹ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜éœ€æ±‚ã€‚

å®éªŒéƒ¨åˆ†å±•ç¤ºäº†CAMEåœ¨BERTæ¨¡å‹ä¸Šçš„åº”ç”¨æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸åŒæ‰¹æ¬¡å¤§å°ä¸‹ï¼ŒCAMEçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿçš„ä¼˜åŒ–å™¨å¦‚Adamå’ŒAdaFactorã€‚æ­¤å¤–ï¼ŒCAMEåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ä¼˜è¶Šæ€§èƒ½ã€‚æœ€åï¼Œç ”ç©¶è¿˜å¯¹æ¯”äº†ä¸åŒä¼˜åŒ–å™¨åœ¨GPUä¸Šçš„å†…å­˜æˆæœ¬ï¼Œç»“æœæ˜¾ç¤ºCAMEåœ¨é¢„è®­ç»ƒBERT-Largeæ¨¡å‹æ—¶å…·æœ‰æœ€ä½çš„å†…å­˜æ¶ˆè€—ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</sample>
    <sample id="193">æ ¹æ®å›¾ç‰‡ä¸­çš„ä¿¡æ¯ï¼Œåˆ›å»ºåˆå§‹æ•°æ®é›†çš„æ³¨é‡Šè€…æ•°é‡æ²¡æœ‰æ˜ç¡®è¯´æ˜ã€‚</sample>
    <sample id="194">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯åç››é¡¿å¤§å­¦ã€‚</sample>
    <sample id="195">The presentation focuses on the development of a framework for Explainable Question Answering (XQA) that addresses the limitations of existing methods, particularly their reliance on structured knowledge bases and their difficulty in handling complex natural language questions. The core idea is to utilize a Hierarchical Question Decomposition Tree (HQDT) to break down complex questions into simpler sub-questions, which can then be answered using various knowledge sources like KBs and text corpora.

The framework, named RoHT (Reasoning over Hierarchical Question Decomposition Tree), employs a two-step process: understanding and reasoning. In the understanding phase, a BART-based question decomposer generates HQDTs from complex questions, while a BART-based question generator creates intermediate nodes. The reasoning phase involves probabilistic reasoning over these HQDTs, where answers are generated with probabilities from different knowledge sources, and an aggregator combines these candidate answers to output the best response.

The study evaluates RoHT's performance across various datasets, including KQA Pro and Musique, demonstrating its effectiveness in answering complex questions with high accuracy and precision. The experimental setting includes specific models and datasets, such as BART-KoPL for scheduling and KoPL engine for KB execution, ensuring a robust evaluation of the framework's capabilities.</sample>
    <sample id="196">å·¦ä¾§ä¸ºæ”¯é…è¯çš„ç¤ºä¾‹æ˜¯â€œHomer loves Lisa, Bart, and Maggie.â€</sample>
    <sample id="197">Turn Likert</sample>
    <sample id="198">å› ä¸ºè¯­è¨€æ¨¡å‹å¯¹å¥å­ç»“æ„å’Œè¯­ä¹‰ç‰¹å¾éå¸¸æ•æ„Ÿï¼Œè¿™äº›ç‰¹å¾å¯èƒ½åœ¨çŸ­å¥ä¸­è¢«å¿½ç•¥æˆ–è¯¯è§£ã€‚é€šè¿‡åœ¨æ•´ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­è¯„ä¼°æ¨¡å‹çš„å¯æ¥å—æ€§ï¼Œå¯ä»¥ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åœ°æ•æ‰åˆ°è¿™äº›é‡è¦çš„ç»“æ„å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜å…¶æ•´ä½“æ€§èƒ½ã€‚</sample>
    <sample id="199">No, the performance gap between monolingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="200">å¦ã€‚</sample>
    <sample id="201">SOTA MT metrics (better correlation with human judgements)ã€‚</sample>
    <sample id="202">æ˜¯çš„ï¼Œæ³›åŒ–ä¸­çš„å›å½’å¯èƒ½ä¼šå½±å“ç‰¹å®šçš„ NER ç±»å‹ã€‚</sample>
    <sample id="203">NLP ä¸­çš„ç«‹åœºå¾ˆé‡è¦æ˜¯å› ä¸ºå®ƒå½±å“äº†ç ”ç©¶è¿‡ç¨‹å’Œç»“æœã€‚äººä»¬æŒæœ‰çš„è§†è§’æ˜¯å…¶äººå£ç»Ÿè®¡å­¦ã€èº«ä»½å’Œç”Ÿæ´»ç»å†çš„ç»“æœï¼Œè¿™äº›å› ç´ ä¼šå½±å“ç ”ç©¶çš„ç»“è®ºã€‚æ­¤å¤–ï¼Œæ•°æ®é›†å’Œæ¨¡å‹ä¹Ÿå¯èƒ½å…·æœ‰ç«‹åœºæ€§ï¼Œè¿™å¯èƒ½ä¼šå½±å“å®ƒä»¬çš„è¡¨ç°å’Œé¢„æµ‹ã€‚å› æ­¤ï¼Œåœ¨æ„å»º NLP æ•°æ®é›†å’Œæ¨¡å‹æ—¶ï¼Œè®°å½•æ‰€æœ‰ç›¸å…³çš„è®¾è®¡é€‰æ‹©å¹¶ä»ç«‹åœºä¸»ä¹‰çš„è§’åº¦è¿›è¡Œç ”ç©¶æ˜¯éå¸¸é‡è¦çš„ã€‚</sample>
    <sample id="204">åƒ BLOOM è¿™æ ·çš„å¤šè¯­è¨€ LLM é€šå¸¸é‡‡ç”¨é€‚é…å™¨å¾®è°ƒã€‚</sample>
    <sample id="205">The presentation discusses the impact of pretraining data on language models and their subsequent performance in downstream tasks, particularly focusing on political biases. It begins by highlighting the diverse sources of pretraining data, such as news media and social media, which can introduce biases into the models. The speaker then outlines the process from pretraining to downstream tasks, emphasizing the need for evaluation methods to assess these biases.

Key points include:
1. **Pretraining Data Sources**: The presentation shows a bar chart illustrating the distribution of pretraining data sources, with news media and social media being significant contributors.
2. **Evaluation Methods**: The speaker introduces automatic evaluation methods grounded in political literature to measure the political leaning of language models (LMs).
3. **Political Leanings of LMs**: A scatter plot is used to visualize the political leanings of various existing LMs, showing a spectrum from libertarian to authoritarian.
4. **Impact of Pretraining Data**: The presentation explores how different pretraining data sources influence the political leanings of LMs, using examples like RoBERTa and GPT-2.
5. **Downstream Tasks Performance**: Results are presented in tables and charts, showing shifts in political leanings across different datasets and tasks.
6. **Qualitative Analysis**: Examples of hate speech and misinformation detection are provided to illustrate the performance of LMs with varying political biases.

Overall, the presentation underscores the importance of understanding and mitigating political biases in language models to ensure fair and unbiased applications in downstream tasks.</sample>
    <sample id="206">RoBERTA-base + classifier head</sample>
    <sample id="207">æœ€è¿‘ç”¨äºè¯„ä¼° PaLM èƒ½åŠ›çš„æµ‹è¯•é›†åŒ…æ‹¬é¿å…è®­ç»ƒé›†å’Œæµ‹è¯•é›†é‡å ä»¥åŠåœ¨è¯„ä¼°æ•°æ®ä¸Šè¿‡æ‹Ÿåˆçš„æœ€æ–°æµ‹è¯•é›†ã€‚</sample>
    <sample id="208">ä¸‰æ¡å»ºè®®ã€‚</sample>
    <sample id="209">ä¸æœ€å¼ºçš„åŸºçº¿ç›¸æ¯”ï¼Œæè®®çš„æ–¹æ³•è·å¾—äº†æ˜¾è‘—çš„æ”¶ç›Šã€‚å…·ä½“æ¥è¯´ï¼Œå›¾è¡¨æ˜¾ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨ç‰¹å®šç›®æ ‡è§„åˆ’ä¸Šçš„å‡†ç¡®ç‡ï¼Œå…¶ä¸­â€œæˆ‘ä»¬çš„æ–¹æ³•â€ï¼ˆOur Methodï¼‰çš„å‡†ç¡®ç‡æœ€é«˜ï¼Œè¾¾åˆ°äº†çº¦80%ï¼Œè€Œå…¶ä»–æ¨¡å‹å¦‚T5ã€Flan-T5ã€GPT-3ã€Codexå’ŒInstructGPTçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º24%ã€36%ã€52%ã€64%å’Œ68%ã€‚å› æ­¤ï¼Œä¸è¿™äº›åŸºçº¿ç›¸æ¯”ï¼Œâ€œæˆ‘ä»¬çš„æ–¹æ³•â€åœ¨è§„åˆ’è´¨é‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">å¯ä»¥ã€‚è®ºæ–‡ä¸­çš„ç»“æœå’Œæ•°æ®é›†å¯ä»¥ä½œä¸ºåŸºå‡†ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†å…³äºæ–‡æœ¬ç®€åŒ–æ–¹æ³•æ€§èƒ½çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ä¸”å±•ç¤ºäº†ä¸åŒæ–¹æ³•åœ¨å„ç§è¯­æ–™åº“ä¸Šçš„è¡¨ç°ã€‚è¿™äº›æ•°æ®å¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜è¯„ä¼°æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›å‚è€ƒã€‚</sample>
    <sample id="212">They conducted experiments with five smaller models.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">è¯¥è§†é¢‘å±•ç¤ºäº†ä¸€åœºå…³äºè‹±è¯­ä¸­è¿è¯é•¿åº¦ã€ä¾èµ–é•¿åº¦æœ€å°åŒ–å’Œåè°ƒç»“æ„å…¼å®¹æ€§çš„å­¦æœ¯è®²åº§ã€‚æ¼”è®²è€…ä»‹ç»äº†ä¸åŒç±»å‹çš„è¿è¯ç»“æ„ï¼Œå¦‚Bouquet/Stanfordï¼ˆé€šç”¨ä¾èµ–ï¼‰ã€Chain/Moscowã€Conjunction-headed/Pragueå’ŒMulti-headed/Londonï¼Œå¹¶é€šè¿‡å…·ä½“å¥å­åˆ†æäº†è¿™äº›ç»“æ„çš„ç‰¹ç‚¹ã€‚æ¥ç€ï¼Œè®¨è®ºäº†ä¾èµ–é•¿åº¦æœ€å°åŒ–ï¼ˆDLMï¼‰çš„æ¦‚å¿µï¼Œè§£é‡Šäº†å•è¯é¡ºåºå¦‚ä½•è¶‹å‘äºæœ€å°åŒ–ä¾èµ–é•¿åº¦ï¼Œå¹¶å±•ç¤ºäº†å‡ ä¸ªç¤ºä¾‹æ¥è¯´æ˜è¿™ä¸€åŸåˆ™ã€‚æ­¤å¤–ï¼Œè¿˜æåˆ°äº†ä»å¢å¼ºç‰ˆçš„Penn Treebankä¸­æå–çš„å…³äºè¿è¯é•¿åº¦çš„ç»Ÿè®¡æ•°æ®ï¼ŒæŒ‡å‡ºå·¦è¿è¯å€¾å‘äºæ›´çŸ­ï¼Œä¸”è¿™ç§è¶‹åŠ¿éšç€é•¿åº¦å·®å¼‚è€Œå¢åŠ ï¼Œä½†ä»…å½“ä¸»è¯­åœ¨å·¦è¾¹æˆ–ä¸å­˜åœ¨æ—¶æ‰æˆç«‹ã€‚æœ€åï¼Œæ¼”è®²è€…æ€»ç»“äº†ç ”ç©¶å‘ç°ï¼Œå¹¶é¼“åŠ±è§‚ä¼—æŸ¥çœ‹å®Œæ•´è®ºæ–‡ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚</sample>
    <sample id="217">The presentation focuses on the exploration of compositional generalization in multi-attribute controllable dialogue generation, specifically addressing the limitations of previous methods that focus on single attributes and ignore continuous attributes. The study introduces DCG (Disentangled Controllable Generation), a model that learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. This approach is designed to improve the controllability and generalization capability of dialogue systems.

Key contributions include:
1. **Proposing DCG**: A disentangled controllable generation method that enhances the ability to control multiple attributes simultaneously.
2. **Unified Evaluation Framework**: MAE (Multi-Attribute Evaluation) is introduced to evaluate the performance of models across different granularities of attributes.
3. **Experimental Setup**: The methodology includes attribute-oriented prompts, task-oriented prompts, and disentanglement learning, with an overall architecture visualized for clarity.
4. **Evaluation Metrics**: BLEU-1 and BLEU-2 scores are used to assess text quality, while E-ACC and A-ACC measure controllability.
5. **Results**: The proposed DCG outperforms existing methods in terms of both controllability and text quality, as evidenced by higher BLEU scores and E-ACC/A-ACC values.

The research also discusses challenges such as the need for a unified evaluation metric and the importance of disentanglement in handling continuous attributes. Overall, the study demonstrates the effectiveness of DCG in achieving better dialogue generation outcomes compared to traditional methods.</sample>
    <sample id="218">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯è°·æ­Œã€‚</sample>
    <sample id="219">The presentation discusses a compare-and-contrast multistage pipeline designed to uncover financial signals in financial reports, particularly focusing on the Form 10-K. It highlights the importance of financial reports for financial practitioners and introduces a highlighting task to address the challenge of mining useful signals from these documents, which require significant human effort. The study observes that financial reports have high overlapping characteristics and yearly-dependent content similarities, leading to the introduction of a multistage pipeline.

The pipeline includes three main stages: document segmentation, relation recognition, and domain-adaptive fine-tuning. Document segmentation categorizes reference-to-target segment pairs into insignificant relations, revised relations, and mismatched relations based on semantic similarity. Relation recognition uses a heuristic filtering procedure to refine the human evaluation burden. The final stage involves domain-adaptive fine-tuning using zero-shot and in-domain approaches with pseudo-labels.

The evaluation dataset consists of e-SNLI and FINAL (Wencong Alpha) datasets, providing metrics such as R-Prec and PCC. The results show that domain-adaptive highlighting models outperform other settings without losing the generality of token representations. Future work includes exploring more effective financial corpus use, applying bi-directional rationalization tasks to other languages, and exploring more efficient applications like dense retrieval and explanation for various modalities such as charts, tables, or cross-company analysis.</sample>
    <sample id="220">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯Stony Brook Universityã€‚</sample>
    <sample id="221">è®ºæ–‡åˆ†æäº†å¾·è¯­å’Œè‹±è¯­è¿™å¯¹è¯­è¨€å¯¹ã€‚</sample>
    <sample id="222">The presentation focuses on the challenges and interventions in open-domain question answering, particularly addressing how to adapt or annotate models for new domains like biomedical information. It discusses the limitations of using Wikipedia as a sole source for diverse questions and proposes expanding retrieval corpora with additional sources such as PubMed. The speaker outlines three main contributions: investigating data interventions to enable out-of-domain generalization, understanding the compatibility of the source model for a target domain, and determining the relationship between data interventions and their effectiveness under specific dataset shifts.

Key points include:
1. **Challenges in Open-Domain QA**: Wikipedia's limitations in covering various domains, especially biomedical.
2. **Intervention Methods**: Few-shot, zero-shot, and varying question, answer, and context methods.
3. **Contributions**: Investigating data interventions, understanding model compatibility, and exploring effective interventions for different dataset shifts.
4. **Generalizability Test**: A framework to evaluate reader and retriever compatibility across different dataset shifts.

The presentation concludes by proposing a few-shot method that improves reader performance by up to 24% and retriever performance by 22% in F1 score, and demonstrates that intervention effectiveness depends on the type of dataset shift.</sample>
    <sample id="223">Shangbin Feng</sample>
    <sample id="224">åœ¨å®éªŒè¿‡ç¨‹ä¸­ç ”ç©¶äº†UVAã€Sent-Labelã€Cats-c3gã€VecAlignã€BertAlignå’ŒMassAlignè¿™å…­ä¸ªæ¨¡å‹ã€‚</sample>
    <sample id="225">åœ¨ MultiInstruct ä¸­ä½¿ç”¨çš„ 62 ä¸ªä¸åŒä»»åŠ¡ä¸­ï¼Œæœ‰ 5 ä¸ªä»»åŠ¡ç”¨äºè®­ç»ƒç›®çš„ï¼Œè€Œ 57 ä¸ªä»»åŠ¡ç”¨äºæµ‹è¯•ç›®çš„ã€‚</sample>
    <sample id="226">ä¸‰ä½</sample>
    <sample id="227">è¯¥è§†é¢‘ä¸»è¦ä»‹ç»äº†Panguæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç†è§£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯­è¨€æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸­çš„å±€é™æ€§ã€‚è§†é¢‘é¦–å…ˆæå‡ºäº†â€œGrounded Language Understandingâ€è¿™ä¸€æ¦‚å¿µï¼Œå¼ºè°ƒäº†è¯­è¨€ç†è§£ä¸å®é™…ä¸–ç•Œä¹‹é—´çš„è”ç³»ï¼Œå¹¶é€šè¿‡å±•ç¤ºå„ç§æ™ºèƒ½è®¾å¤‡å’Œæœç´¢ç»“æœçš„ä¾‹å­æ¥è¯´æ˜å½“å‰æŠ€æœ¯çš„ä¸è¶³ã€‚æ¥ç€ï¼Œè§†é¢‘è¯¦ç»†è®¨è®ºäº†ç°æœ‰è¯­è¨€æ¨¡å‹è®­ç»ƒæ•°æ®çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºè¿™äº›æ¨¡å‹ä¸»è¦ä¾èµ–äºæ–‡æœ¬è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œå¦‚Wikipediaã€BookCorpusç­‰ï¼Œè€Œç¼ºä¹å¯¹ç°å®ä¸–ç•Œçš„ç›´æ¥ç»éªŒã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œè§†é¢‘æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”Panguæ¡†æ¶ï¼Œå®ƒå…è®¸è¯­è¨€æ¨¡å‹ä¸“æ³¨äºåŒºåˆ†ä¸åŒåœºæ™¯çš„èƒ½åŠ›ï¼Œå¹¶ä¸”å…·æœ‰é€šç”¨æ€§ã€‚Panguæ¡†æ¶é€šè¿‡å°†ç¯å¢ƒä¸è¯­è¨€æ¨¡å‹åˆ†å¼€ï¼Œè®©ä»£ç†ä¸ç¯å¢ƒäº¤äº’å¹¶æå‡ºæœ‰æ•ˆçš„å€™é€‰è®¡åˆ’ï¼Œè€Œè¯­è¨€æ¨¡å‹ä»…è´Ÿè´£è¯„åˆ†è¿™äº›è®¡åˆ’ã€‚æ­¤å¤–ï¼Œè§†é¢‘è¿˜å±•ç¤ºäº†Panguæ¡†æ¶åœ¨çŸ¥è¯†å›¾è°±é—®ç­”ï¼ˆKBQAï¼‰ä»»åŠ¡ä¸Šçš„æ–°SOTAæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éåŒè´¨åŒ–æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ¨¡å‹å¤§å°ä¿æŒç¨³å®šã€‚æœ€åï¼Œè§†é¢‘å¼ºè°ƒäº†ç›´æ¥ç”Ÿæˆè®¡åˆ’å¯èƒ½ä¸æ˜¯æœ€ä½³ä½¿ç”¨è¯­è¨€æ¨¡å‹çš„æ–¹å¼ï¼Œè€Œæ˜¯åº”è¯¥è®©æ¨¡å‹ä¸“æ³¨äºåŒºåˆ†ä¸åŒåœºæ™¯çš„èƒ½åŠ›ã€‚</sample>
    <sample id="228">AG News, MIND, SST2, Enron Spam</sample>
    <sample id="229">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†åœ¨è®ºè¯´æ€§å†™ä½œä¸­ä¿®è®¢çš„é‡è¦æ€§åŠå…¶æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œä½œè€…æŒ‡å‡ºä¿®è®¢æ˜¯è®ºè¯´æ€§å†™ä½œçš„ä¸€ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªè¿­ä»£è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æœ€ä½³çš„æªè¾ã€‚ä¿®è®¢ç›´æ¥å½±å“åˆ°è®ºè¯´æ€§æ–‡æœ¬çš„è¯´æœåŠ›å’Œå½±å“ã€‚æ¥ç€ï¼Œä½œè€…é€šè¿‡ä¸€ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•é€šè¿‡é€æ­¥ç»†åŒ–å’Œæ¾„æ¸…æ¥æ”¹è¿›è®ºè¯´æ€§å£°æ˜ï¼Œä»æœ€åˆçš„â€œæ‰‹æœºå¯¼è‡´ç™Œç—‡â€åˆ°æ›´å…·ä½“çš„â€œæ‰‹æœºè¾å°„å¯èƒ½å¯¼è‡´ç™Œç—‡â€ï¼Œå†åˆ°è¿›ä¸€æ­¥æ¾„æ¸…ä¸ºâ€œæ‰‹æœºè¾å°„å¯èƒ½è‡´ç™Œâ€ã€‚è¿™ä¸€è¿‡ç¨‹å¼ºè°ƒäº†ä¿®è®¢å¦‚ä½•å¸®åŠ©æ”¹å–„è®ºè¯´æ€§å£°æ˜çš„è´¨é‡ã€‚

éšåï¼Œä½œè€…æå‡ºäº†ä¸¤ä¸ªä»»åŠ¡ï¼š1ï¼‰å­ä¼˜åŒ–è®ºè¯´æ€§å£°æ˜æ£€æµ‹ï¼Œå³åˆ¤æ–­ç»™å®šçš„è®ºè¯´æ€§å£°æ˜æ˜¯å¦éœ€è¦ä¿®è®¢æˆ–å¯ä»¥è¢«è®¤ä¸ºæ˜¯æœ€ä½³æªè¾ï¼›2ï¼‰è®ºè¯´æ€§å£°æ˜æ”¹è¿›å»ºè®®ï¼Œå³æ ¹æ®ç»™å®šçš„è®ºè¯´æ€§å£°æ˜é€‰æ‹©åº”è¯¥æ”¹è¿›çš„è´¨é‡é—®é¢˜ç±»å‹ã€‚é€šè¿‡è¿™äº›ä»»åŠ¡ï¼Œä½œè€…æ—¨åœ¨è§£å†³å¦‚ä½•åˆ¤æ–­è®ºè¯´æ€§å£°æ˜æ˜¯å¦å·²ç»è¶³å¤Ÿå¥½ä»¥åŠæ˜¯å¦éœ€è¦æ›´å¤šä¿®è®¢çš„é—®é¢˜ã€‚

æ­¤å¤–ï¼Œä½œè€…è¿˜è®¨è®ºäº†ä»äººç±»ä¿®è®¢è¡Œä¸ºä¸­å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥æ£€æµ‹å¯æ”¹è¿›çš„è®ºè¯´æ€§å£°æ˜ã€‚è¿™ç§æ–¹æ³•åŸºäºåœ¨çº¿è¾©è®ºå¹³å°ï¼ˆå¦‚Kialoï¼‰ä¸­çš„åä½œç¼–è¾‘è¡Œä¸ºï¼Œé€šè¿‡åˆ†æéšå¼ä¿®è®¢æ¨¡å¼æ¥å»ºæ¨¡è®ºè¯´æ€§æ–‡æœ¬çš„è´¨é‡ã€‚æœ€åï¼Œä½œè€…æåˆ°äº†å‡ ä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§å’Œå¯é æ€§ã€æ¨¡å‹å¤æ‚åº¦å’Œæ¶æ„ã€ä¸Šä¸‹æ–‡ç›¸å…³æ€§ä»¥åŠä¸»é¢˜å’Œç”¨æˆ·åè§ç­‰ã€‚è¿™äº›é—®é¢˜å¯¹äºå¼€å‘æœ‰æ•ˆçš„è®ºè¯´æ€§æ–‡æœ¬ä¿®è®¢ç³»ç»Ÿè‡³å…³é‡è¦ã€‚</sample>
    <sample id="231">NACHOS æ˜¯ä¸€ä¸ªåŒ…å«1,188ä¸ªå•è¯çš„å¼€æºæ•°æ®é›†ï¼Œå®ƒä»å¤šä¸ªåŒ»å­¦é¢†åŸŸä¸­æ”¶é›†äº†å¼‚è´¨æ€§æ•°æ®ã€‚</sample>
    <sample id="232">David Var Torres</sample>
    <sample id="233">è¯¥è§†é¢‘ä¸»è¦ä»‹ç»äº†å®æ—¶è¯­éŸ³ç¿»è¯‘ï¼ˆSimultaneous Speech Translation, SimuSTï¼‰çš„æ¦‚å¿µåŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç‰¹å®šæ¶æ„çš„è®­ç»ƒã€å¤æ‚çš„è®­ç»ƒç¨‹åºå’Œä¸åŒå»¶è¿Ÿæ¨¡å¼ä¸‹çš„æ¨¡å‹ç»´æŠ¤ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºEDAttçš„æ–¹æ³•ï¼Œåˆ©ç”¨å·²æœ‰çš„ç¦»çº¿ç¿»è¯‘æ¨¡å‹è¿›è¡Œå®æ—¶ç¿»è¯‘ï¼Œå¹¶é€šè¿‡è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†å»¶è¿Ÿé—®é¢˜ã€‚EDAttæ–¹æ³•çš„æ ¸å¿ƒåœ¨äºæ ¹æ®æ³¨æ„åŠ›ç‚¹çš„åˆ†å¸ƒå†³å®šæ˜¯å¦å‘å‡ºæˆ–éƒ¨åˆ†ç¿»è¯‘å•è¯ï¼Œç¡®ä¿ä¿¡æ¯çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEDAttåœ¨ä¸åŒå»¶è¿Ÿè®¾ç½®ä¸‹å‡ä¼˜äºå…¶ä»–ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨è€ƒè™‘å®é™…è€—æ—¶çš„æƒ…å†µä¸‹ï¼ŒEDAttæ˜¯æœ€å¿«é€Ÿçš„ç­–ç•¥ã€‚</sample>
    <sample id="234">Prompting strategies have a big impact on the results.</sample>
    <sample id="235">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬å¡å†…åŸºæ¢…éš†å¤§å­¦è¯­è¨€æŠ€æœ¯ç ”ç©¶æ‰€ã€æŠ€æœ¯å­¦é™¢ã€ä¼¯å…‹åˆ©äººå·¥æ™ºèƒ½ç ”ç©¶å®éªŒå®¤å’ŒUnbabelã€‚</sample>
    <sample id="236">62ä¸ªä»»åŠ¡è¢«åˆ†ä¸º10ä¸ªå¤§ç±»ï¼Œæ¯ä¸ªå¤§ç±»ä¸‹æœ‰5ä¸ªç”±ä¸“å®¶ç¼–å†™çš„æŒ‡ä»¤ã€‚</sample>
    <sample id="237">ä½œè€…å»ºè®®ä½¿ç”¨KIMMUSæµ‹è¯•å¥—ä»¶æ¥æµ‹è¯•æ¨¡å‹ã€‚</sample>
    <sample id="238">è¯¥ç ”ç©¶ä»‹ç»äº†MeetingBankï¼Œä¸€ä¸ªç”¨äºä¼šè®®æ‘˜è¦çš„åŸºå‡†æ•°æ®é›†ã€‚æ•°æ®é›†é€šè¿‡åˆ†å‰²åŸå¸‚ç†äº‹ä¼šä¼šè®®å¹¶é…å¯¹å®ƒä»¬ä¸ä¸“å®¶æ’°å†™çš„æ‘˜è¦æ¥åˆ›å»ºï¼Œæ—¨åœ¨ä¸ºè®¾è®¡é«˜çº§ä¼šè®®æ‘˜è¦ç³»ç»Ÿçš„ç ”ç©¶äººå‘˜æä¾›æœ‰ä»·å€¼çš„æµ‹è¯•å¹³å°ï¼Œå¹¶æä¾›å…³äºå†³ç­–è¿‡ç¨‹çš„è§è§£ã€‚ç ”ç©¶ä¸­æåˆ°ï¼Œå¯¹äºæå–ç³»ç»Ÿï¼ŒExtr-Oracleè·å¾—äº†é«˜Rouge-2åˆ†æ•°ï¼ˆ46.8%ï¼‰ï¼Œè€Œå¯¹è¯æ¨¡å‹DialogLMåœ¨æŠ½è±¡ç³»ç»Ÿä¸­è¡¨ç°æœ€ä½³ï¼Œè·å¾—Rouge-2åˆ†æ•°ä¸º60.12%ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†äººç±»è¯„ä¼°ï¼Œè¯„ä¼°æ ‡å‡†åŒ…æ‹¬ä¿¡æ¯æ€§ã€äº‹å®æ€§ã€æµç•…æ€§ã€è¿è´¯æ€§å’Œå†—ä½™æ€§ï¼Œå¹³å‡å¾—åˆ†ä¸º2.75è‡³4.21ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†åˆ›å»ºåŸºå‡†æ•°æ®é›†çš„é‡è¦æ€§ï¼Œå¼ºè°ƒå…¶å¯¹ç ”ç©¶äººå‘˜è®¾è®¡å…ˆè¿›ä¼šè®®æ‘˜è¦ç³»ç»Ÿçš„ä»·å€¼ä»¥åŠå¯¹åŸå¸‚ç†äº‹ä¼šå†³ç­–è¿‡ç¨‹çš„æ´å¯Ÿã€‚</sample>
    <sample id="239">å¥½çš„ï¼Œä»¥ä¸‹æ˜¯å°†è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡çš„ç»“æœï¼š

1. å®éªŒç»“æœ
   - ç¤ºä¾‹è´¨é‡æ¯”ä¸æºå¥å­çš„ç›¸ä¼¼æ€§æ›´é‡è¦ã€‚
   - ä¸“é—¨åŒ–çš„SOTAç³»ç»Ÿå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚
   - PaLMä¸Google Translateæ¥è¿‘ã€‚
   - ä»MQMä¸­è·å¾—çš„è§è§£ï¼š
     - PaLMçš„æµç•…åº¦ä¸SOTAç›¸å½“ã€‚
     - å‡†ç¡®ç‡åˆ†æ•°æ™®éè¾ƒä½ã€‚
     - â€œé£æ ¼/ç”Ÿç¡¬â€é€šå¸¸ä½äºPaLMã€‚

2. æ„Ÿè°¢è¯äº‘å›¾

å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼</sample>
    <sample id="240">å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯å¹»ç¯ç‰‡ä¸Šçš„è‹±æ–‡å†…å®¹çš„ç¿»è¯‘ï¼š

---

**ç»“è®º**

**æœ€è¿‘çš„WSLæ–¹æ³•**
- éœ€è¦å¹²å‡€çš„æ•°æ®ã€‚
- è¿‡é«˜ä¼°è®¡äº†å®ƒä»¬çš„å®é™…åº”ç”¨æ€§ã€‚

**æˆ‘ä»¬çš„å»ºè®®**
- æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€‚
- ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºçº¿ã€‚
- æ€»æ˜¯åº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚

**ç»“è®º**
- æœ€è¿‘çš„WSLæ–¹æ³•
  - éœ€è¦å¹²å‡€çš„æ•°æ®ã€‚
  - è¿‡é«˜ä¼°è®¡äº†å®ƒä»¬çš„å®é™…åº”ç”¨æ€§ã€‚

**æˆ‘ä»¬çš„å»ºè®®**
  - æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€‚
  - ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºçº¿ã€‚
  - æ€»æ˜¯åº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ã€‚

---

å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ï¼</sample>
    <sample id="241">The presentation focuses on the development and evaluation of a human-in-the-loop (HiTL) framework for early misinformation detection, particularly in the context of COVID-19 treatments. The study highlights the inadequacies of current approaches to misinformation detection, which are often unrealistically evaluated and not built with real-world scale or noise. These systems fail to account for the interplay between automated systems and human content moderators or fact-checkers.

The HiTL framework aims to address these limitations by integrating human feedback into an end-to-end misinformation detection system. This system processes tweets from Twitter, identifies misleading claims, and provides actionable outputs. It also includes policy violation verification, where humans make crucial judgments at various stages of the workflow to ensure compliance with platform policies.

The evaluation of this system is conducted on COVID-19 treatment misinformation on Twitter. The efficacy of the approach is measured based on its ability to detect misleading claims early, defined as the relative time of detection to the first appearance of the claim in a debunking news article. The results show that 65% of system-identified tweets are most likely or clearly violating Twitterâ€™s policies, and the system detects 124.2 tweets containing policy violations per human hour worked.

The study concludes that the HiTL framework captures the complex interplay between systems and human content moderators, providing a useful standard of comparison for future systems. It also presents an outside look at human-in-the-loop misinformation systems, motivating further research into more effective frameworks for misinformation detection.</sample>
    <sample id="242">å¯¹è¯ç³»ç»Ÿçš„å¸¸ç”¨è¯„ä¼°æ–¹æ³•åŒ…æ‹¬ABC-Evalã€Turn Likertã€Dialogue Likertå’ŒComparativeã€‚</sample>
    <sample id="243">è¿™ç¯‡è®ºæ–‡æœ‰äº”ä½ä½œè€…ã€‚</sample>
    <sample id="244">åœ¨ Servin å’Œ Kea çš„ç¤ºä¾‹ä¸­ï¼Œéœ€è¦çš„èƒŒæ™¯çŸ¥è¯†åŒ…æ‹¬Servinæ˜¯æ³•å®˜å’ŒKeaæ˜¯é¢åŒ…å¸ˆã€‚</sample>
    <sample id="245">The presentation by Lining Zhang and her team discusses the challenges of finding high-agreement workers on MTurk for summarization tasks, emphasizing the limitations of automatic metrics and best practices in recruitment. The study outlines a two-step pipeline to identify such workers: a qualification task and an endurance task. The qualification task involves a pre-defined set of settings, including document and summary length, and a designed motivation to ensure workers understand the task. Workers are categorized into four tiers based on their performance: GOLD, SILVER, BRONZE, and BLOCK. The endurance task tests workers' capacity to handle heavy workloads with 10 HITs per worker, each containing one document and four summaries. The results show that 38 workers were qualified as GOLD, 18 as SILVER, and 119 as BLOCK. The reference-based task further evaluates the quality of summaries using Krippendorff's Alpha and Cohen's Kappa, demonstrating that the pipeline can produce high-quality annotations at a lower cost compared to CloudResearch. The study concludes that the pipeline is effective but highlights limitations such as the need for English summarization on MTurk and the absence of guarantees for the training of correctness.</sample>
    <sample id="246">æ˜¯çš„ï¼Œä»£ç å·²å…¬å¼€ã€‚å¯ä»¥åœ¨GitHubä¸Šæ‰¾åˆ°æ•°æ®é›†ã€ç”Ÿæˆå’Œè¯„ä¼°ä»£ç ï¼Œç½‘å€ä¸ºmpoems/kitmusã€‚</sample>
    <sample id="247">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†FactKGé¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†å›¾è°±çš„äº‹å®éªŒè¯ç³»ç»Ÿã€‚é¡¹ç›®çš„ç›®æ ‡æ˜¯é€šè¿‡æ¨ç†æ¥éªŒè¯è‡ªç„¶è¯­è¨€ä¸­çš„äº‹å®ï¼Œæ¶µç›–äº”ç§ç±»å‹çš„æ¨ç†ï¼šä¸€è·³ã€è¿æ¥ã€å­˜åœ¨ã€å¤šè·³å’Œå¦å®šã€‚FactKGçš„æ•°æ®é›†åŒ…å«108kä¸ªè‡ªç„¶è¯­è¨€é™ˆè¿°ï¼Œè¿™äº›é™ˆè¿°æ¶‰åŠå„ç§è¯­ä¹‰æ¨¡å¼ï¼ŒåŒ…æ‹¬å£è¯­é£æ ¼çš„é™ˆè¿°ä»¥åŠä¹¦é¢é£æ ¼çš„é™ˆè¿°ï¼Œä»¥å¢åŠ å…¶å®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†ä¸­è¿˜åŒ…å«äº†å¤šç§ç±»å‹çš„æ¨ç†ç±»å‹ï¼Œå¦‚è¿è¯æ¨ç†ã€å®ä½“å­˜åœ¨æ¨ç†ç­‰ã€‚

ç ”ç©¶è€…ä»¬æŒ‡å‡ºï¼Œä½¿ç”¨å›¾å½¢è¯æ®åœ¨æ¨¡å‹ä¸­å¯ä»¥æé«˜æ€§èƒ½ï¼Œä¸ä¸åŒ…å«æ­¤ç±»è¯æ®çš„åŸºçº¿ç›¸æ¯”ï¼Œè¿™åœ¨æ‰€æœ‰æ¨ç†ç±»å‹ä¸Šéƒ½è¡¨ç°å‡ºä¼˜è¶Šçš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFactKGåœ¨å¤„ç†å„ç§æ¨ç†ç±»å‹æ—¶è¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å’Œå¦å®šæ¨ç†æ–¹é¢ï¼Œå‡†ç¡®ç‡åˆ†åˆ«è¾¾åˆ°äº†57.9%å’Œ58.9%ï¼Œè€Œæ€»å‡†ç¡®ç‡ä¸º79.41%ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¢è®¨äº†å¦‚ä½•å°†ä¹¦é¢é£æ ¼çš„é™ˆè¿°è½¬æ¢ä¸ºå£è¯­é£æ ¼çš„é™ˆè¿°ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨é¢„è®¾å‰æè¿›è¡Œæ¨ç†çš„æ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶è€…æä¾›äº†æ•°æ®é›†çš„é“¾æ¥å’Œè”ç³»æ–¹å¼ï¼Œä»¥ä¾¿å…¶ä»–ç ”ç©¶äººå‘˜èƒ½å¤Ÿè®¿é—®å’Œä½¿ç”¨FactKGã€‚</sample>
    <sample id="248">ä¸å‡è¡¡ã€‚NLPositionality çš„æ³¨é‡Šè€…åœ¨å„ä¸ªäººå£ç»Ÿè®¡å­¦ç‰¹å¾æ–¹é¢å¹¶ä¸å‡è¡¡ï¼Œä¾‹å¦‚åœ¨æ•™è‚²æ°´å¹³ä¸Šï¼Œæ‹¥æœ‰å¤§å­¦åŠä»¥ä¸Šå­¦å†çš„æ³¨é‡Šè€…æ¯”ä¾‹è¾ƒé«˜ã€‚</sample>
    <sample id="249">é€šè¿‡ä¿æŒç›¸å…³ç»“æ„çš„æ–¹å¼æ‰°ä¹±å¥å­ã€‚</sample>
    <sample id="250">è¿›è¡Œç»´åº¦è¯„ä¼°æ„å‘³ç€å¯¹å¯¹è¯è´¨é‡çš„å¤šæ–¹é¢è€ƒé‡ï¼ŒåŒ…æ‹¬ç›¸å…³æ€§ã€ä¸€è‡´æ€§ä»¥åŠæƒ…æ„Ÿç†è§£ã€‚</sample>
    <sample id="251">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬ï¼šUniversity of Science and Technology of Chinaã€Microsoft Research Asiaã€Beijing Jiaotong University å’Œ Sony AIã€‚</sample>
    <sample id="252">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨æ³•å¾‹é¢†åŸŸä¸­ï¼Œå¦‚ä½•é€šè¿‡äº‹ä»¶æŠ½å–æŠ€æœ¯æ¥æé«˜æ¡ˆä¾‹æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºU-CREATçš„æ— ç›‘ç£æ–¹æ³•ï¼Œç”¨äºä»æ³•å¾‹æ–‡æ¡£ä¸­æå–äº‹ä»¶ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ¡ˆä¾‹æ£€ç´¢ä»»åŠ¡ã€‚ç ”ç©¶å›¢é˜Ÿé¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåä¸ºIL-PCRçš„æ–°æ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹å°åº¦æ³•å¾‹ç³»ç»Ÿä¸­çš„æ¡ˆä¾‹æ£€ç´¢é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨BM25ç­‰è®¡æ•°æ¨¡å‹ä»¥åŠBERTç­‰å˜æ¢å™¨æ¨¡å‹ï¼Œä»–ä»¬å±•ç¤ºäº†U-CREATæ–¹æ³•åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨IL-PCRæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„F1åˆ†æ•°æå‡ã€‚

ç ”ç©¶è¿˜å¯¹æ¯”äº†å¤šç§ç›‘ç£å’Œéç›‘ç£æ–¹æ³•ï¼Œå‘ç°äº‹ä»¶æŠ½å–æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºä¼ ç»Ÿçš„è¯é¢‘æ¨¡å‹ï¼Œå¦‚BM25ã€‚æ­¤å¤–ï¼ŒU-CREATæ–¹æ³•ä¸ä»…åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”å…·æœ‰æ›´å¥½çš„æ¨ç†æ—¶é—´å’Œå¯ç”Ÿäº§æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒU-CREATæ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆæå‡æ³•å¾‹æ¡ˆä¾‹æ£€ç´¢çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œä¸ºæ³•å¾‹é¢†åŸŸçš„è‡ªåŠ¨åŒ–å¤„ç†æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</sample>
    <sample id="253">è¯¥ç ”ç©¶ä»‹ç»äº†DisorBERTï¼Œä¸€ç§åŒåŸŸé€‚åº”æ¨¡å‹ï¼Œç”¨äºåœ¨ç¤¾äº¤åª’ä½“ä¸Šæ£€æµ‹ç²¾ç¥éšœç¢çš„è¿¹è±¡ã€‚ç ”ç©¶é¦–å…ˆå®šä¹‰äº†ç²¾ç¥éšœç¢ï¼ŒæŒ‡å‡ºå®ƒä»¬æ˜¯ä¸€ç§ä¸å›°æ‰°å’Œæ®‹ç–¾ç›¸å…³çš„å¿ƒç†ç»¼åˆå¾ï¼Œå½±å“äººä»¬çš„æ€ç»´ã€æ„Ÿè§‰ã€æƒ…ç»ªå’Œè¡Œä¸ºã€‚æ¥ç€ï¼Œç ”ç©¶è®¨è®ºäº†ç¤¾äº¤åª’ä½“çš„å¹¿æ³›ä½¿ç”¨ï¼Œå¼ºè°ƒå…¶åœ¨å…¨çƒäººå£ä¸­çš„æ™®åŠç‡é«˜è¾¾59.4%ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨å¿ƒç†å¥åº·ç›‘æµ‹ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚

ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†åŒåŸŸé€‚åº”çš„æ¦‚å¿µï¼Œå³é€šè¿‡è°ƒæ•´è¯æ±‡è¡¨ã€æ›´æ–°è¯­ä¹‰ç†è§£å’Œå­¦ä¹ ç‰¹å®šé¢†åŸŸä»»åŠ¡æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†BERTè¯­è¨€æ¨¡å‹é€‚åº”åˆ°ç²¾ç¥éšœç¢é¢†åŸŸï¼Œé€šè¿‡æŒ‡å¯¼æ©ç å’Œè¯å…¸æ˜ å°„æŠ€æœ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ç¤¾äº¤åª’ä½“è¯­è¨€ï¼Œå¹¶ä¸“æ³¨äºç²¾ç¥éšœç¢é¢†åŸŸã€‚

ç ”ç©¶è¿˜æä¾›äº†å®éªŒç»“æœï¼ŒåŒ…æ‹¬ç²¾åº¦å’Œå¬å›åˆ†æï¼Œè¡¨æ˜DisorBERTåœ¨è¯†åˆ«ç²¾ç¥éšœç¢æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä½¿ç”¨BDI-Testè¿›è¡Œæ¨¡å‹åˆ†æçš„æ–¹æ³•ï¼Œä»¥åŠä¸åŒæ¨¡å‹åœ¨ç²¾ç¥éšœç¢æ£€æµ‹ä»»åŠ¡ä¸Šçš„è¡¨ç°å·®å¼‚ã€‚æœ€åï¼Œç ”ç©¶æ€»ç»“äº†æœªæ¥çš„å·¥ä½œæ–¹å‘ï¼ŒåŒ…æ‹¬æ¢ç´¢ä¸åŒé¢†åŸŸèµ„æºçš„åº”ç”¨ã€ä¸´åºŠæ•°æ®çš„ä½¿ç”¨ä»¥åŠè®­ç»ƒæ›´ä¸“ä¸šåŒ–è¯­è¨€æ¨¡å‹çš„å¯èƒ½æ€§ã€‚</sample>
    <sample id="254">The presentation focuses on the development of a document-level relation distant extraction framework with uncertainty-guided label denoising, significantly improving the quality of DS data in DocRE tasks. The study introduces an iterative re-label strategy using dynamic class uncertainty thresholds to filter high-uncertainty pseudo labels and enhance the reliability of instance-level predictions. It employs a novel instance-level uncertainty estimation method for overlapping relations, which measures the reliability of pseudo labels. The framework is validated through extensive experiments on two public datasets, demonstrating substantial performance improvements over existing baselines when trained on denoised DS data. The research also outlines a multi-phase training strategy that iteratively refines the pre-denoising RE model and leverages it to generate pseudo instances with uncertainty scores, further enhancing the model's accuracy and robustness.</sample>
    <sample id="255">åœ¨ç¿»è¯‘è´¨é‡ä¸Šï¼Œæç¤ºçš„å½¢å¼å¾ˆé‡è¦ã€‚</sample>
    <sample id="257">ä½œè€…è¯„ä¼°äº†å››ç§å¼€æ”¾åŸŸå¯¹è¯æ¨¡å‹ã€‚</sample>
    <sample id="258">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯å¦å¯ä»¥ä½œä¸ºäººç±»è¯„ä¼°çš„æ›¿ä»£å“ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³ç»™LLMsæä¾›æŒ‡ä»¤å¹¶è¦æ±‚å®ƒä»¬å¯¹æ ·æœ¬è¿›è¡Œè¯„åˆ†ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºLLMè¯„ä¼°ã€‚ç ”ç©¶èƒŒæ™¯ä¸­æåˆ°ï¼Œè™½ç„¶ä½¿ç”¨LLMsè¿›è¡Œè¯„ä¼°çš„æƒ³æ³•çœ‹èµ·æ¥è‡ªç„¶ä¸”å¹¿æ³›é‡‡ç”¨ï¼Œä½†ç›®å‰æ²¡æœ‰å…ˆä¾‹æ¢ç´¢è¿™ä¸€æƒ³æ³•ã€‚ç ”ç©¶è€…åœ¨ACL 2023ä¼šè®®ä¸Šæäº¤äº†ä»–ä»¬çš„è®ºæ–‡ï¼Œå¹¶è®¨è®ºäº†LLMè¯„ä¼°çš„åŠ¨æœºã€å®éªŒè®¾ç½®ä»¥åŠå¯èƒ½çš„é—®é¢˜ã€‚ä»–ä»¬è¿˜æ¯”è¾ƒäº†LLMsä¸äººç±»ä¸“å®¶å¯¹æ•…äº‹çš„è¯„åˆ†ç»“æœï¼Œå‘ç°äººç±»ä¸“å®¶å¯¹äººç±»æ’°å†™çš„æ•…äº‹æƒ…æœ‰æ˜æ˜¾çš„åå¥½ï¼Œè€Œè¾ƒå°çš„LLMsï¼ˆT0: 13Bå’Œtext-curie-001: 30Bï¼‰åˆ™æ²¡æœ‰è¡¨ç°å‡ºå¯¹äººç±»æ’°å†™çš„æ•…äº‹æƒ…èŠ‚çš„æ˜æ˜¾åå¥½ã€‚ç ”ç©¶è¿˜æå‡ºäº†å‡ ä¸ªé—®é¢˜ï¼Œå¦‚LLMså’Œäººç±»è¯„ä¼°è€…æ˜¯å¦åœ¨ä¸ªä½“æ•…äº‹çš„è¯„åˆ†ä¸Šè¾¾æˆä¸€è‡´ï¼Œæ”¹å˜æŒ‡ä»¤æªè¾æˆ–ä»LLMsä¸­é‡‡æ ·å“åº”æ˜¯å¦ä¼šæ”¹å˜ç»“æœç­‰ã€‚</sample>
    <sample id="259">The presentation focuses on the development of XSemPLR, a unified benchmark for cross-lingual semantic parsing across multiple natural languages and meaning representations. The study aims to address the limitations of existing cross-lingual semantic parsing models by providing a comprehensive dataset that includes 9 datasets from various domains, 5 semantic parsing tasks, 8 meaning representations, and data from 22 natural languages in 15 language families. The research introduces a novel model architecture called XSemPLR, which utilizes neural models to translate queries in multiple natural languages into SQL, Lambda Calculus, and FunQL.

Key findings include:
1. **Performance Comparison**: The Enc-Dec (mT5) model outperforms previous work or achieves comparable results.
2. **Impact of Monolingual Training**: Pretraining on the English NL significantly boosts the performance of few-shot on target NLs.
3. **Inadequacy of Multilingual LLMs**: Multilingual LLMs like Codex &amp; BLOOM are still inadequate for cross-lingual semantic parsing tasks, with Chinese transfer learning showing the largest performance gap.
4. **Performance Gap Analysis**: FunQL outperforms other three meaning representations, while SQL performs the worst.
5. **Cross-Lingual Transfer Learning**: The performance gap between monolingual training and cross-lingual transfer learning is significant, with the transfer gap being shortened rapidly for few-shot settings but remaining significant for zero-shot settings.

The study concludes by emphasizing the importance of developing more effective cross-lingual semantic parsing models and the need for further research to bridge the performance gap between monolingual and cross-lingual training approaches.</sample>
    <sample id="260">ä¹ä½ã€‚</sample>
    <sample id="261">ä¼˜ç§€è§„åˆ’å™¨çš„ç†æƒ³å“è´¨åŒ…æ‹¬èƒ½å¤Ÿå°†æŠ½è±¡ç›®æ ‡åˆ†è§£ä¸ºå…·ä½“æ­¥éª¤çš„èƒ½åŠ›ã€ç”Ÿæˆçš„è®¡åˆ’åº”ä¸ç»™å®šçš„çº¦æŸæ¡ä»¶ä¸€è‡´ã€ä»¥åŠåœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ä¿æŒé€»è¾‘è¿è´¯æ€§ã€‚</sample>
    <sample id="262">è¿™ç¯‡è®ºæ–‡æœ‰ä¸ƒä½ä½œè€…ã€‚</sample>
    <sample id="263">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æ ‡ç­¾åå·®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ä¸­çš„æ ‡ç­¾åå·®è¿›è¡Œäº†ç±»å‹å­¦åˆ†æã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä»»åŠ¡è¯­æ–™åº“æ˜¯å¯¼è‡´æ ‡ç­¾åå·®çš„ä¸»è¦å› ç´ ï¼Œå¹¶æå‡ºäº†é¢†åŸŸä¸Šä¸‹æ–‡æ ¡å‡†ï¼ˆDomain-Context Calibration, DCï¼‰æ–¹æ³•æ¥å…¨é¢ç¼“è§£ä¸‰ç§ç±»å‹çš„æ ‡ç­¾åå·®ï¼šä¼°è®¡åå·®ã€åŸŸæ ‡ç­¾åå·®å’Œä¸Šä¸‹æ–‡æ ‡ç­¾åå·®ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œå°¤å…¶æ˜¯åœ¨å…·æœ‰è¾ƒå¤§åŸŸæ ‡ç­¾åå·®çš„ä»»åŠ¡ä¸Šã€‚ç ”ç©¶è¿˜å¯¹æ¯”äº†DCæ–¹æ³•ä¸å…¶ä»–æ ¡å‡†æ–¹æ³•çš„æ•ˆæœï¼Œå‘ç°DCæ–¹æ³•åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†ä¸ºä»€ä¹ˆDCæ–¹æ³•ä¼˜äºä¹‹å‰çš„æ ¡å‡†å°è¯•ï¼ŒåŒ…æ‹¬ä½¿ç”¨éšæœºä¸Šä¸‹æ–‡å•è¯è¿›è¡Œæ ¡å‡†ä»¥åŠä½¿ç”¨å¤šä¸ªéšæœºè‹±è¯­å•è¯è¿›è¡Œæ ¡å‡†çš„é‡è¦æ€§ã€‚æœ€åï¼Œç ”ç©¶å¼ºè°ƒäº†é¢†åŸŸä¸Šä¸‹æ–‡æ ¡å‡†åœ¨ç¼“è§£æ ‡ç­¾åå·®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é¼“åŠ±è¯»è€…æŸ¥é˜…è®ºæ–‡ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚</sample>
    <sample id="264">è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³å¤šæ¨¡æ€é¢†åŸŸè¿ç§»é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºTAVTçš„æ¨¡å‹ï¼Œç”¨äºè·¨é¢†åŸŸéŸ³é¢‘-è§†è§‰æ–‡æœ¬ç”Ÿæˆã€‚ç ”ç©¶æŒ‡å‡ºï¼Œç°æœ‰æ–¹æ³•åœ¨æ•°æ®æ³¨é‡Šæ–¹é¢å­˜åœ¨å›°éš¾å’Œæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œå¹¶ä¸”åœ¨ä¸åŒé¢†åŸŸé—´è¡¨ç°å‡ºä¸¥é‡çš„é€€åŒ–ç°è±¡ã€‚TAVTé€šè¿‡ç»“åˆéŸ³é¢‘å’Œè§†è§‰ç‰¹å¾ï¼Œåˆ©ç”¨å…ƒæ˜ å°„ç½‘ç»œã€éŸ³é¢‘-è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ç”Ÿæˆå™¨ä»¥åŠåäº‹å®å¯¹æ¯”å­¦ä¹ æ¥å®ç°è·¨é¢†åŸŸè¿ç§»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTAVTåœ¨MSVDå’ŒMSR-VTTä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ–°é—»ã€åŠ¨ç”»ã€éŸ³ä¹ã€ä½“è‚²ã€çƒ¹é¥ªã€å„¿ç«¥ã€äº¤é€šå’Œç¾å®¹ç­‰é¢†åŸŸçš„æ€§èƒ½ä¸Šã€‚</sample>
    <sample id="265">æ¼”è®²è€…çš„åå­—æ˜¯Vasudha Varadarajanã€‚</sample>
    <sample id="266">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯æ³¢å…°ç§‘å­¦é™¢è®¡ç®—æœºç§‘å­¦ç ”ç©¶æ‰€ã€‚</sample>
    <sample id="268">PaLMæœ€å¸¸è§çš„é”™è¯¯æ˜¯å‡†ç¡®æ€§/é—æ¼ã€‚</sample>
    <sample id="269">ä¸è¦å¿˜è®°ä½ çš„ABCï¼šè¯„ä¼°å¯¹è¯ç³»ç»Ÿä¸­çš„æœ€æ–°æŠ€æœ¯

Sarah E. Finch, James D. Finch å’Œ Jinho D. Choi

Emory NLPç ”ç©¶å®éªŒå®¤

Alexa

æ¯”è¾ƒæ€§è¯„ä¼°

æ¯”è¾ƒæ€§è¯„ä¼°å±•ç¤ºäº†ä¸åŒå¯¹è¯ç³»ç»Ÿçš„äº¤äº’è¡Œä¸ºï¼ŒåŒ…æ‹¬äººç±»å’Œæœºå™¨äººçš„å¯¹è¯ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›ç³»ç»Ÿçš„è¡¨ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å®ƒä»¬çš„ä¼˜åŠ¿å’Œä¸è¶³ã€‚

Likerté‡è¡¨è¯„ä¼°

ä½¿ç”¨Likerté‡è¡¨è¯„ä¼°å¯¹è¯è´¨é‡ï¼Œå‚ä¸è€…éœ€è¦å¯¹æœºå™¨äººçš„å“åº”è¿›è¡Œè¯„åˆ†ï¼Œä»1åˆ°5åˆ†ã€‚è¿™æœ‰åŠ©äºé‡åŒ–å¯¹è¯çš„è´¨é‡ï¼Œå¹¶ä¸ºåç»­çš„æ”¹è¿›æä¾›æ•°æ®æ”¯æŒã€‚

å¯¹è¯è´¨é‡ç»´åº¦

å¯¹è¯è´¨é‡ç”±å››ä¸ªç»´åº¦ç»„æˆï¼šç›¸å…³æ€§ã€ä¸€è‡´æ€§ã€æƒ…æ„Ÿç†è§£å’Œä¸€è‡´æ€§ã€‚è¿™äº›ç»´åº¦å…±åŒå†³å®šäº†å¯¹è¯çš„æ•´ä½“è´¨é‡ã€‚

æ ‡æ³¨èŠå¤©ä¸­çš„è¡Œä¸ºï¼ˆABC-Evalï¼‰

ABC-Evalæ˜¯ä¸€ç§ç”¨äºæ ‡æ³¨èŠå¤©ä¸­æœºå™¨äººè¡Œä¸ºçš„æ–¹æ³•ã€‚å®ƒå¯ä»¥å¸®åŠ©æˆ‘ä»¬è¯†åˆ«å¹¶åˆ†ç±»å¯¹è¯ä¸­çš„å„ç§é”™è¯¯ç±»å‹ï¼Œå¦‚æ— å…³ã€ç¼ºä¹åŒç†å¿ƒã€è‡ªæˆ‘çŸ›ç›¾ç­‰ã€‚

å®éªŒè®¾è®¡

å®éªŒæ¶‰åŠ4ä¸ªå¼€æ”¾é¢†åŸŸå¯¹è¯æ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹æœ‰100ä¸ªäººæœºå¯¹è¯ã€‚é€šè¿‡ABC-Evalæ–¹æ³•è¯„ä¼°å¯¹è¯è´¨é‡ï¼Œå¹¶ä½¿ç”¨Likerté‡è¡¨è¿›è¡Œè¯„åˆ†ã€‚

åŸºçº¿è¯„ä¼°

åŸºçº¿è¯„ä¼°åŒ…æ‹¬ä¸€è‡´æ€§ã€æƒ…æ„Ÿç†è§£ã€ä¿¡æ¯æ€§å’Œæ•´ä½“è´¨é‡ã€‚è¿™äº›æŒ‡æ ‡æœ‰åŠ©äºè¡¡é‡å¯¹è¯ç³»ç»Ÿçš„æ€§èƒ½ã€‚

äº’è¯„è€…ä¸€è‡´æ€§

äº’è¯„è€…ä¸€è‡´æ€§åˆ†ææ˜¾ç¤ºäº†ä¸åŒè¯„ä¼°è€…ä¹‹é—´çš„è¯„åˆ†ä¸€è‡´æ€§ã€‚é€šè¿‡è®¡ç®—Krippendorffâ€™s Alphaå€¼ï¼Œå¯ä»¥è¯„ä¼°è¯„åˆ†çš„ä¸€è‡´æ€§ã€‚

é¢„æµ‹æœ‰æ•ˆæ€§

é¢„æµ‹æœ‰æ•ˆæ€§åˆ†æè¡¨æ˜ï¼ŒABC-Evalæ–¹æ³•åœ¨é¢„æµ‹å¯¹è¯è´¨é‡æ–¹é¢å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£å“ªäº›å› ç´ å¯¹å¯¹è¯è´¨é‡çš„å½±å“æœ€å¤§ã€‚

å¢é‡æœ‰æ•ˆæ€§

å¢é‡æœ‰æ•ˆæ€§åˆ†æè¿›ä¸€æ­¥éªŒè¯äº†ABC-Evalæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ¯”è¾ƒä¸åŒæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šå“ªäº›å› ç´ å¯¹å¯¹è¯è´¨é‡çš„å½±å“æœ€å¤§ã€‚

é”™è¯¯ç‡åˆ†æ

é”™è¯¯ç‡åˆ†ææ˜¾ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å¤„ç†å„ç§é”™è¯¯ç±»å‹æ—¶çš„è¡¨ç°ã€‚ä¾‹å¦‚ï¼ŒBART-FID-RAGæ¨¡å‹åœ¨å¤„ç†â€œæ— æ„ä¹‰â€å’Œâ€œè‡ªæˆ‘çŸ›ç›¾â€é”™è¯¯æ–¹é¢è¡¨ç°è¾ƒå¥½ï¼Œè€ŒBlender2æ¨¡å‹åˆ™åœ¨å¤„ç†â€œæ— å…³â€å’Œâ€œè‡ªæˆ‘çŸ›ç›¾â€é”™è¯¯æ–¹é¢è¡¨ç°è¾ƒå·®ã€‚

æ€»ç»“

æ„Ÿè°¢æ‚¨çš„è§‚çœ‹ï¼</sample>
    <sample id="270">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„æ˜¯Emory NLP Research Labã€‚</sample>
    <sample id="271">Continuous fine-tuning</sample>
    <sample id="272">è¿™ç¯‡è®ºæ–‡æœ‰å…­ä½ä½œè€…ã€‚</sample>
    <sample id="273">å½“ç¿»è¯‘éœ€è¦ä¸Šä¸‹æ–‡å—ï¼Ÿæ•°æ®é©±åŠ¨çš„å¤šè¯­è¨€æ¢ç´¢

Patrick Fernandes*, Kayo Yin*, Emmy Liu
AndrÃ© F. T. Martins, Graham Neubig

*ç­‰åŒè´¡çŒ®

ç¿»è¯‘ä¾èµ–äºä¸Šä¸‹æ–‡

æˆ‘ä»¬å°†ä¸å¾—ä¸é™¤æ‰é‚£ä¸ªé—´è°ã€‚

äº‹æƒ…å¯èƒ½ä¼šå˜å¾—å±é™©ï¼Œå¦‚æœéƒ¨é•¿ä»¬å‘ç°çš„è¯ã€‚
æˆ‘ä»¬å°†ä¸å¾—ä¸é™¤æ‰é‚£ä¸ªé—´è°ã€‚

è¿™å¯èƒ½æ˜¯ä¸¥é‡çš„äº‹æƒ…ï¼ŒåŒ»ç”Ÿï¼Ÿ
æˆ‘ä»¬å°†ä¸å¾—ä¸é™¤æ‰é‚£ä¸ªé—´è°ã€‚

è¯„ä¼°ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘å¾ˆéš¾
- åªæœ‰å°‘é‡çš„è¯ä¾èµ–äºä¸Šä¸‹æ–‡
- -è¯­æ–™åº“çº§åˆ«çš„æŒ‡æ ‡
- å­˜åœ¨çš„æ–¹æ³•æ”¯æŒæœ‰é™çš„è¯­ç¯‡ç°è±¡å’Œè¯­è¨€

è¯„ä¼°ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘å¾ˆéš¾
- åªæœ‰å°‘é‡çš„è¯ä¾èµ–äºä¸Šä¸‹æ–‡
- -è¯­æ–™åº“çº§åˆ«çš„æŒ‡æ ‡
- å­˜åœ¨çš„æ–¹æ³•æ”¯æŒæœ‰é™çš„è¯­ç¯‡ç°è±¡å’Œè¯­è¨€

RQ1ï¼šç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ
RQ2ï¼šæ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘ï¼Ÿ

RQ1ï¼šç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ
- è¯çº§ä¸Šä¸‹æ–‡ä½¿ç”¨

RQ2ï¼šæ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘ï¼Ÿ

æ¡ä»¶äº¤å‰äº’ä¿¡æ¯ï¼ˆCXMIï¼‰
- CXMIï¼šæµ‹é‡ç»™å®šè¯­æ–™åº“æ—¶ä¸Šä¸‹æ–‡MTæ¨¡å‹ä½¿ç”¨çš„ä¸Šä¸‹æ–‡é‡

ç‚¹å¯¹ç‚¹ï¼ˆP-ï¼‰CXMI
- æˆ‘ä»¬å¼•å…¥P-CXMIæ¥è¡¡é‡ç¿»è¯‘ç‰¹å®šå¥å­æ—¶çš„ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µ
- å¥å­ P-CXMI(y,x,C) = - log qMTA(y|x) / qMTA(y|x,C)
- å•è¯ P-CXMI(i,y,x,C) = - log qMTA(yi|yi&lt; i,x) / qMTA(yi|yi&lt; i,x,C)

é«˜P-CXMIå•è¯ -&gt; éœ€è¦ä¸Šä¸‹æ–‡æ¥ç¿»è¯‘

RQ1ï¼šç¿»è¯‘ä½•æ—¶éœ€è¦ä¸Šä¸‹æ–‡ï¼Ÿ
- è¯çº§ä¸Šä¸‹æ–‡ä½¿ç”¨
- ä¸»é¢˜åˆ†æ

RQ2ï¼šæ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘ï¼Ÿ

ä¸»é¢˜åˆ†æé«˜P-CXMIå•è¯

TED
IDEAS WORTH SPREADING

1. POSæ ‡ç­¾
- ä»£è¯
- åŠ¨è¯å½¢å¼
- è¯æ±‡è¿è´¯æ€§
- æ­£å¼ç¨‹åº¦
- çœç•¥

2. è¯æ±‡é¡¹
- Avelie's mother was still asleep. Avelie went to school.
- é˜¿ç»´åˆ©å°”çš„æ¯äº²è¿˜åœ¨ç¡è§‰ã€‚é˜¿ç»´åˆ©å°”å»ä¸Šå­¦äº†ã€‚

3. å•ä¸ªè¯
- She knows where we're going. I don't.
- Sie weiÃŸ, wohin wir gehen. Ich weiÃŸ es nicht.

æ€»ç»“
- é€šè¿‡ç³»ç»Ÿåœ°è¯†åˆ«è¯è¯­ç°è±¡è€Œæ— éœ€å…ˆéªŒè¯­è¨€å­¦çŸ¥è¯†
- æ•°æ®é›†æ— å…³çš„æ–‡æ¡£çº§MTåŸºå‡†</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡åœ¨å°åº¦è¯­ç³»è¯­è¨€ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä»è‹±è¯­åˆ°å°åº¦è¯­çš„ç¿»è¯‘ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶å·²æœ‰è®¸å¤šè¯„ä¼°æŒ‡æ ‡è¢«æå‡ºå¹¶ç”¨äºè‹±è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä¸­ï¼Œä½†è¿™äº›æŒ‡æ ‡åœ¨å…¶ä»–è¯­è¨€ä¸Šçš„é€‚ç”¨æ€§å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚å› æ­¤ï¼Œç ”ç©¶å›¢é˜Ÿå†³å®šä¸“æ³¨äºå°åº¦è¯­ç³»çš„è¯­è¨€ï¼Œå¹¶é€‰æ‹©äº†æ³°ç±³å°”è¯­å’Œé©¬æ‹‰é›…æ‹‰å§†è¯­ä½œä¸ºç ”ç©¶å¯¹è±¡ã€‚

ç ”ç©¶é¦–å…ˆä»‹ç»äº†æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œä½¿ç”¨Floresæ•°æ®é›†é€‰å–200ä¸ªéšæœºå¥å­ï¼Œå¹¶é€šè¿‡mBARTã€Bing APIã€Google APIã€CVIT-IITBã€IndicTransã€mT5å’ŒNLLBç­‰æ¨¡å‹ç”Ÿæˆç¿»è¯‘è¾“å‡ºã€‚æ¥ç€ï¼Œç ”ç©¶é‡‡ç”¨äº†MQMæ¡†æ¶æ¥æ”¶é›†äººç±»æ³¨é‡Šï¼Œç”±åŒè¯­ä¸“å®¶å¯¹ç³»ç»Ÿè¾“å‡ºè¿›è¡Œæ ‡æ³¨ï¼ŒåŒ…æ‹¬é”™è¯¯ç±»å‹ï¼ˆå¦‚æ‹¼å†™é”™è¯¯ã€è¯­æ³•é”™è¯¯ç­‰ï¼‰ä»¥åŠæ•´ä½“è¯„åˆ†ã€‚ç ”ç©¶è¿˜è¯¦ç»†æè¿°äº†MQMæ¡†æ¶ä¸‹çš„é”™è¯¯åˆ†ç±»ä½“ç³»ï¼Œåˆ†ä¸ºå‡†ç¡®æ€§ã€æµç•…æ€§å’Œå…¶ä»–/ç‰¹æ®Šç±»åˆ«ï¼Œå¹¶è¿›ä¸€æ­¥ç»†åˆ†ä¸ºæ·»åŠ ã€é—æ¼ã€è¯¯è¯‘ã€æœªç¿»è¯‘æ–‡æœ¬ã€æ‹¼å†™ã€è¯­æ³•ã€é£æ ¼ã€æ³¨å†Œå’Œå­—ç¬¦ç¼–ç ç­‰å­ç±»ã€‚

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸åŒç³»ç»Ÿåœ¨ç¿»è¯‘è´¨é‡ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé”™è¯¯ç±»å‹ä¸Šçš„è¡¨ç°ä¹Ÿæœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æä¾›äº†å„ç§è¯„ä¼°æŒ‡æ ‡ä¸äººç±»è¯„åˆ†çš„ç›¸å…³æ€§åˆ†æï¼Œä»¥åŠåŸºäºMQMæ³¨é‡Šçš„æŒ‡æ ‡æ€§èƒ½å¯¹æ¯”ã€‚æœ€åï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œIndic COMETâ€çš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨MQMæ³¨é‡Šå¾®è°ƒCOMETæŒ‡æ ‡å˜ä½“ï¼Œä»¥æé«˜å°åº¦è¯­ç³»è¯­è¨€ç¿»è¯‘è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</sample>
    <sample id="277">è¯¥æ–¹æ³•æ²¡æœ‰åç§°ã€‚</sample>
    <sample id="278">ä½œè€…æè¿°äº†â€œæ˜¾æ€§è¯æ±‡â€æ–¹æ³•ï¼Œå³æ‰¾åˆ°é‚£äº›åŒºåˆ†æ ‡è®°ç¾¤ä½“å’Œæœªæ ‡è®°ç¾¤ä½“çš„è¯æ±‡ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬å®šä¹‰äº†æœªæ ‡è®°å’Œæ ‡è®°ç¾¤ä½“ï¼Œå¹¶ä½¿ç”¨åŠ æƒå¯¹æ•°æ¯”å€¼æ¥åŒºåˆ†æ¯ä¸ªæ ‡è®°ç¾¤ä½“çš„é¡¶çº§è¯æ±‡ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸ºé»‘äººå¥³æ€§è§’è‰²ç”Ÿæˆçš„äººæ ¼ä¸­ï¼Œä»–ä»¬å¯»æ‰¾ä¸ç™½äººå’Œç”·æ€§è§’è‰²ä¸åŒçš„è¯æ±‡ã€‚</sample>
    <sample id="279">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬Paul Allen Schoolã€University of Washington NLPã€Carnegie Mellon University Language Technologies Instituteã€‚</sample>
    <sample id="280">The paper introduces MultiEMO, an advanced multimodal fusion framework designed for emotion recognition in conversations. It addresses the challenges of existing approaches by focusing on the complementarity of textual, audio, and visual modalities. The framework includes a novel visual feature extractor called VisExtNet that captures relevant visual cues without redundant scene information. Additionally, it employs a multimodal fusion model, MultiAttn, which uses bidirectional multi-head cross-attention layers to model correlations across different modalities. The authors also introduce a Sample-Weighted Focal Contrastive (SWFC) loss function to improve performance on minority and semantically similar emotion classes. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO outperforms state-of-the-art methods, particularly in handling minority emotions and reducing misclassification due to asynchronous emotional tendencies from different modalities.</sample>
    <sample id="281">è¯¥ç ”ç©¶æ¢è®¨äº†ç¿»è¯‘ä¸­è¯­å¢ƒçš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºç¿»è¯‘ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå¹¶é€šè¿‡å¤šä¸ªç¤ºä¾‹å±•ç¤ºäº†ä¸åŒè¯­å¢ƒä¸‹å•è¯çš„ä¸åŒå«ä¹‰ã€‚ç ”ç©¶å‘ç°ï¼Œåªæœ‰å°‘é‡çš„è¯æ±‡ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œè¯„ä¼°ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘éš¾åº¦è¾ƒå¤§ã€‚ç ”ç©¶æå‡ºäº†ä¸¤ç§è¡¡é‡æ–¹æ³•ï¼šä¸€ç§æ˜¯åŸºäºè¯­æ–™åº“çš„æŒ‡æ ‡ï¼Œå¦ä¸€ç§æ˜¯æ”¯æŒæœ‰é™è¯è¯­ç°è±¡å’Œè¯­è¨€çš„æ–¹æ³•ã€‚ç ”ç©¶è¿˜æå‡ºäº†ä¸¤ä¸ªç ”ç©¶é—®é¢˜ï¼šä½•æ—¶ç¿»è¯‘éœ€è¦è¯­å¢ƒï¼Ÿæ¨¡å‹å¦‚ä½•å¤„ç†ä¸Šä¸‹æ–‡ä¾èµ–çš„ç¿»è¯‘ï¼Ÿç ”ç©¶ä½¿ç”¨äº†æ¡ä»¶äº¤å‰äº’ä¿¡æ¯ï¼ˆCXMIï¼‰æ¥è¡¡é‡æ¨¡å‹åœ¨ç»™å®šè¯­æ–™åº“æ—¶å¯¹ä¸Šä¸‹æ–‡çš„ä½¿ç”¨ç¨‹åº¦ï¼Œå¹¶å¼•å…¥äº†ç‚¹ç§¯CXMIï¼ˆP-CXMIï¼‰æ¥æµ‹é‡ç¿»è¯‘ç‰¹å®šå•è¯æ—¶çš„ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé«˜P-CXMIå•è¯é€šå¸¸æ¶‰åŠä»£è¯ã€åŠ¨è¯å½¢å¼ã€è¯æ±‡è¿è´¯æ€§å’Œæ­£å¼æ€§ç­‰ç°è±¡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ä½¿ç”¨äº†å¤šè¯­è¨€è¯è¯­æ„ŸçŸ¥ï¼ˆMuDAï¼‰åŸºå‡†æµ‹è¯•ï¼Œå‘ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å‹åœ¨æŸäº›ç°è±¡ä¸Šè¡¨ç°æ˜¾è‘—æ›´å¥½ã€‚</sample>
    <sample id="282">è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³éå¹³è¡Œæ•…äº‹çš„ä½œè€…é£æ ¼è½¬ç§»é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä¸åŒè¯­ç¯‡å±‚é¢çš„é£æ ¼æ¨¡ä»¿å’Œç‰¹å®šå†™ä½œä¸»é¢˜çš„é£æ ¼å…³è”æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºStoryTransçš„æ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªä¸»è¦æ­¥éª¤å®ç°è¿™ä¸€ç›®æ ‡ï¼šé¦–å…ˆï¼Œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹è¿›è¡Œè¯è¯­è¡¨ç¤ºè½¬ç§»ï¼Œé€šè¿‡æ©ç æºæ•…äº‹å’Œé£æ ¼åµŒå…¥ç”Ÿæˆæ©ç è½¬æ¢æ•…äº‹ï¼›å…¶æ¬¡ï¼Œé‡‡ç”¨å†…å®¹ä¿ç•™å¢å¼ºç­–ç•¥ï¼Œç¡®ä¿è½¬æ¢æ•…äº‹çš„å†…å®¹ä¸åŸå§‹æ•…äº‹ä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ•°æ®é¢„å¤„ç†ã€èåˆæœºåˆ¶ä»¥åŠè®­ç»ƒæ¡†æ¶ç­‰æŠ€æœ¯ç»†èŠ‚ï¼Œå¹¶å±•ç¤ºäº†åœ¨ä¸­è‹±åŒè¯­æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœï¼ŒéªŒè¯äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</sample>
    <sample id="283">Bouquet/Stanford (Universal Dependencies)</sample>
    <sample id="284">The presentation discusses the development and application of FSUIE, a novel fuzzy span mechanism designed to enhance Universal Information Extraction (UIE) tasks. The core motivation behind FSUIE is to address the limitations of existing UIE models that heavily rely on precise span boundaries, leading to ambiguity in annotation. FSUIE introduces a fuzzy span loss function that models the boundary as a continuous distribution rather than a one-hot vector, allowing for more flexible and adaptive span extraction decisions. This approach is further enhanced by an efficient fuzzy span attention mechanism, which adjusts the attention span based on local features, improving the model's ability to focus on relevant information within a limited range.

The study demonstrates the effectiveness of FSUIE across various information extraction tasks, including Named Entity Recognition (NER), Relation Extraction (RE), and Argument Structure Extraction (ASTE). Results show significant improvements over traditional UIE models, particularly in small-scale datasets where FSUIE exhibits better generalized fuzzy span-awareness. Additionally, the model achieves strong performance across different datasets, indicating its versatility and robustness in handling diverse information extraction challenges.

In conclusion, FSUIE represents a significant advancement in UIE technology by proposing a novel fuzzy span loss and utilizing efficient fuzzy span attention, thereby achieving excellent results across multiple IE tasks.</sample>
    <sample id="285">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†åœ¨å¯¹è¯æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¦‚ä½•é€šè¿‡å¼•å…¥å‚è€ƒä¿®æ­£æ¥æé«˜æ¨¡å‹å¯¹äº‹å®é”™è¯¯çš„çº æ­£èƒ½åŠ›ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ä¸­ä»ç„¶å­˜åœ¨äº‹å®é”™è¯¯ï¼Œä½†ç›´æ¥è®¾è®¡æ›´å¥½çš„æ‘˜è¦æ¨¡å‹æˆ–ä½¿ç”¨äº‹å®é”™è¯¯çº æ­£ï¼ˆFECï¼‰æ¨¡å‹æ˜¯ä¸¤ç§å¸¸è§çš„è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºå‚è€ƒçš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡æ‰‹åŠ¨æ ‡æ³¨å‚è€ƒä¿®æ­£æ¥è®­ç»ƒFECæ¨¡å‹ï¼Œå¹¶è¦æ±‚æ¨¡å‹é€šè¿‡å°‘é‡çš„æ›¿æ¢ã€æ’å…¥å’Œåˆ é™¤æ“ä½œæ¥çº æ­£åŸå§‹æ‘˜è¦ä¸­çš„äº‹å®é”™è¯¯ï¼Œä»¥è·å¾—æµç•…ä¸”ä¸å†—ä½™çš„æ‘˜è¦ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æä¾›äº†æ¯”ä¼ªæ•°æ®æ›´å®è´µçš„æ•°æ®ç”¨äºFECæ¨¡å‹çš„è®­ç»ƒï¼Œè¿˜ä¸ºæ›´å…¨é¢å’Œå‡†ç¡®åœ°è¯„ä¼°FECæ¨¡å‹çš„æ€§èƒ½åˆ›é€ äº†æ¡ä»¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è®­ç»ƒFECæ¨¡å‹æ—¶å¼•å…¥æ¥è‡ªå¯¹è¯æ‘˜è¦æ•°æ®é›†çš„äººå·¥ä¿®æ­£æ‘˜è¦å¯ä»¥æ˜¾è‘—æå‡å…¶æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ·»åŠ ã€å±æ€§å’Œé“¾æ¥é”™è¯¯ç­‰å¤æ‚äº‹å®é”™è¯¯æ–¹é¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼ºè°ƒäº†æ”¹å˜å½“å‰è¯„ä»·æ–¹æ³•çš„ç´§è¿«æ€§ï¼Œä»¥é€‚åº”FECæ¨¡å‹çš„éœ€æ±‚ã€‚</sample>
    <sample id="286">Sarah E. Finch, James D. Finch and Jinho D. Choi</sample>
    <sample id="287">è¿™ç¯‡è®ºæ–‡æœ‰å››ä½ä½œè€…ã€‚</sample>
    <sample id="288">BLIMP, SyntaxGymå’ŒCrowS</sample>
    <sample id="290">FTw, BOND, COSINE, MLC, L2R</sample>
    <sample id="291">è¯¥æ¨¡å‹åœ¨11ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚</sample>
    <sample id="294">CamemBERTæœ€åˆæ˜¯åœ¨4GBçš„æ•°æ®ä¸Šè®­ç»ƒçš„ã€‚</sample>
    <sample id="295">Adam PrzepiÃ³rkowski å’Œ MichaÅ‚ WoÅºniakã€‚</sample>
    <sample id="296">è¯¥ç ”ç©¶ä»‹ç»äº†EPICï¼ˆEnglish Perspectivist Irony Corpusï¼‰ï¼Œä¸€ä¸ªåŸºäºå¤šè§†è§’çš„è®½åˆºè¯­æ–™åº“ï¼Œæ—¨åœ¨æ¢è®¨ç°ä»£è‡ªç„¶è¯­è¨€ç†è§£ä¸­ä¸»è§‚ä»»åŠ¡çš„å±€é™æ€§ã€‚EPICè¯­æ–™åº“æ”¶é›†äº†æ¥è‡ªRedditå’ŒTwitterçš„æ•°æ®ï¼Œæ—¶é—´è·¨åº¦ä¸º2020å¹´1æœˆåˆ°2021å¹´6æœˆï¼ŒåŒ…å«çº¦300æ¡æ–‡æœ¬å›å¤å¯¹ã€5ä¸ªå˜ä½“å’Œä¸¤ä¸ªæ¥æºã€‚è¯­æ–™åº“æ¶µç›–äº†è‹±è¯­çš„äº”ä¸ªå˜ä½“ï¼šç¾å›½è‹±è¯­ã€è‹±å›½è‹±è¯­ã€çˆ±å°”å…°è‹±è¯­ã€æ¾³å¤§åˆ©äºšè‹±è¯­å’Œå°åº¦è‹±è¯­ã€‚

ç ”ç©¶é€šè¿‡Prolificå¹³å°è¿›è¡Œäººå·¥æ ‡æ³¨ï¼Œæ¯ä¸ªå˜ä½“æœ‰15åæ ‡æ³¨è€…ï¼Œæ¯åæ ‡æ³¨è€…å¹³å‡æ ‡æ³¨200æ¡æ–‡æœ¬ï¼Œæ¯æ¬¡æ ‡æ³¨5æ¬¡ã€‚æ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œè¦æ±‚æ ‡æ³¨è€…ä»æ‰€æœ‰è¯­è¨€å˜ä½“ä¸­é€‰æ‹©å®ä¾‹è¿›è¡Œæ ‡æ³¨ï¼Œè€Œéä»…é™äºä»–ä»¬è‡ªå·±ç†Ÿæ‚‰çš„å˜ä½“ã€‚æ ‡æ³¨ä»»åŠ¡åŒ…æ‹¬åˆ¤æ–­æ–‡æœ¬æ˜¯å¦å…·æœ‰è®½åˆºæ„å‘³ï¼Œå¹¶å›ç­”â€œè¿™ä¸ªå¥å­å¯ä»¥è¢«è°è®¤ä¸ºæ˜¯è®½åˆºï¼Ÿâ€ç­‰é—®é¢˜ã€‚

ç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒç¾¤ä½“åœ¨è®½åˆºæ„ŸçŸ¥ä¸Šçš„å·®å¼‚ï¼Œå¦‚æ€§åˆ«ã€å¹´é¾„ç»„åˆ«ã€æ•™è‚²æ°´å¹³å’Œå›½ç±ç­‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸åŒç¾¤ä½“åœ¨è®½åˆºæ„ŸçŸ¥ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œå°¤å…¶æ˜¯ä»£é™…ä¹‹é—´ã€‚æ€»ä½“è€Œè¨€ï¼ŒEPICè¯­æ–™åº“ä¸ºç ”ç©¶è®½åˆºæ„ŸçŸ¥æä¾›äº†ä¸°å¯Œçš„æ•°æ®æ”¯æŒï¼Œæœ‰åŠ©äºç†è§£ä¸åŒè§†è§’ä¸‹è®½åˆºçš„ç†è§£ä¸åº”ç”¨ã€‚</sample>
    <sample id="297">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†æ”¿æ²»è¯è¯­ä¸­çš„â€œç‹—å“¨â€ç°è±¡ï¼Œå³é€šè¿‡ä½¿ç”¨éšå–»æˆ–æš—ç¤ºæ€§çš„è¯­è¨€æ¥å¸å¼•ç‰¹å®šç¾¤ä½“çš„æ”¯æŒè€Œä¸å¼•å‘åå¯¹ã€‚ç ”ç©¶è€…é€šè¿‡åˆ†æå†å²ä¸Šçš„ç¾å›½æ”¿æ²»æ¼”è®²ï¼Œè¯†åˆ«å’Œåˆ†ç±»äº†è¿™äº›éšå–»ï¼Œå¹¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«è¿™äº›éšå–»æ–¹é¢çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒGPT-3èƒ½å¤Ÿè¯†åˆ«å‡º45%çš„éšå–»ï¼Œå¹¶ä¸”69%çš„éšå–»å±äºæ­£å¼è¯­å¢ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¡¨æ˜ï¼Œæä¾›å®šä¹‰å’Œç§˜å¯†çº¿ç´¢å¯ä»¥æ˜¾è‘—æé«˜GPT-3è¯†åˆ«éšå–»çš„èƒ½åŠ›ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºï¼Œè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¯†åˆ«éšå–»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶æ€§èƒ½å› éšå–»ç±»å‹ã€æ³¨å†Œå½¢å¼ï¼ˆæ­£å¼/éæ­£å¼ï¼‰ä»¥åŠè®­ç»ƒæ•°æ®çš„é¢†åŸŸæ•ˆåº”è€Œæœ‰æ‰€ä¸åŒã€‚</sample>
    <sample id="298">æ€§èƒ½ä¸‹é™ä¸»è¦æ˜¯ç”±äºæ—¶é—´æ¼‚ç§»ã€‚</sample>
    <sample id="299">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰æ¨¡å‹çš„é²æ£’æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯æ¨¡å‹åœ¨é‡åˆ°ä¸æ ‡ç­¾æ— å…³ä½†èƒ½è¯¯å¯¼æ¨¡å‹çš„â€œæ·å¾„â€æ—¶çš„è¡¨ç°ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œè¿™äº›æ·å¾„å¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¾èµ–äºé”™è¯¯çš„æ¨¡å¼ï¼Œä»è€Œå½±å“å…¶åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œminimax trainingâ€çš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ ä¸€ä¸ªå¼ºè°ƒæœªå……åˆ†ä»£è¡¨çš„å›°éš¾æ ·æœ¬çš„å®ä¾‹æƒé‡åˆ†å¸ƒæ¥ä¼˜åŒ–æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦å¯¹æ·å¾„æœ‰å…ˆéªŒçŸ¥è¯†ï¼Œè€Œæ˜¯ä¾èµ–äºæ¨¡å‹è‡ªèº«çš„è®­ç»ƒåŠ¨æ€ï¼Œå¹¶ä¸”è¾…åŠ©æ¨¡å‹æ˜¯ä¸€ä¸ªå‰é¦ˆç½‘ç»œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæé«˜æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒé«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è®¨è®ºäº†å…¶ä»–å®éªŒï¼Œå¦‚åœ¨æ›´å¤§è§„æ¨¡æ¨¡å‹ã€åˆæˆæ·å¾„å’Œè·¨åŸŸæµ‹è¯•é›†ä¸Šçš„åº”ç”¨æ•ˆæœï¼Œä»¥åŠé¢„è®­ç»ƒå¯¹æ¨¡å‹çš„å½±å“ç­‰ã€‚</sample>
    <sample id="300">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†äº¤äº’å¼è¯­éŸ³è¾“å…¥ï¼ˆInteractive Dictationï¼‰çš„æ¦‚å¿µåŠå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ä¸ªåä¸ºâ€œTERTIUSâ€çš„æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨é€šè¿‡è¯­éŸ³è¯†åˆ«å’Œå‘½ä»¤å¤„ç†çš„çµæ´»æ··åˆæ¥æé«˜ç”¨æˆ·æ•ˆç‡ã€‚ä»–ä»¬è¯¦ç»†ä»‹ç»äº†ç³»ç»Ÿæ„å»ºè¿‡ç¨‹ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ã€å‘½ä»¤åˆ†å‰²ã€å‘½ä»¤è§£é‡Šã€é”™è¯¯ä¿®å¤ä»¥åŠæ‰§è¡Œå¼•æ“ç­‰æ­¥éª¤ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡å¢é‡è½¬å½•å’Œå‘½ä»¤å¤„ç†æ¥å®ç°è‡ªç„¶å’Œç›´è§‚çš„è¯­éŸ³è¾“å…¥ä½“éªŒã€‚

ç ”ç©¶ä¸­æåˆ°ï¼Œå½“å‰å¤§å¤šæ•°è¯­éŸ³åˆ°æ–‡æœ¬ç³»ç»Ÿä¸æ”¯æŒé€šè¿‡è¯­éŸ³è¿›è¡Œç¼–è¾‘ï¼Œè€ŒTERTIUSåˆ™å°è¯•è§£å†³è¿™ä¸€é—®é¢˜ã€‚é€šè¿‡å¼•å…¥æ–°çš„ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†ä¸€ä¸ªæ•°æ®æ”¶é›†ç•Œé¢å¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å«1320ä¸ªè½¨è¿¹ã€959ä¸ªè¯­éŸ³ç‰‡æ®µå’Œ3225ä¸ªå‘½ä»¤çš„å¤§å‹æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªåŸºå‡†ç³»ç»Ÿï¼Œå¹¶å±•ç¤ºäº†ä¸åŒæ¨¡å‹åœ¨å‘½ä»¤è¾¹ç•Œé¢„æµ‹å‡†ç¡®æ€§å’Œå‘½ä»¤çŠ¶æ€é¢„æµ‹å‡†ç¡®æ€§æ–¹é¢çš„æ€§èƒ½ã€‚

ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨GPT3æ¨¡å‹æ—¶ï¼Œå‘½ä»¤è¾¹ç•Œé¢„æµ‹å‡†ç¡®ç‡ä¸º85.3%ï¼Œå‘½ä»¤çŠ¶æ€é¢„æµ‹å‡†ç¡®ç‡ä¸º55.1%ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚å‘½ä»¤åˆ†å‰²å’Œå‘½ä»¤è§£é‡Šçš„çµæ´»æ€§ï¼Œä½†ç ”ç©¶å›¢é˜Ÿé€šè¿‡å®éªŒéªŒè¯äº†å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå¢é‡è½¬å½•å’Œå‘½ä»¤å¤„ç†çš„ç³»ç»Ÿæ¶æ„ã€‚</sample>
    <sample id="302">å¯¹è¾“å‡ºåºåˆ—ä¸­çš„è¯å…ƒè¿›è¡Œæ’åˆ—æ˜¯ä¸ºäº†å¤„ç†å¥å­ä¸­å¯èƒ½å­˜åœ¨çš„æ·±å±‚é€’å½’ç»“æ„å’Œæœªè§è¿‡çš„çŸ­è¯­ç»„åˆã€‚è¿™ç§æ’åˆ—æ–¹å¼æœ‰åŠ©äºæ¨¡å‹ç†è§£å¥å­ä¸­ä¸åŒéƒ¨åˆ†ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¤„ç†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å•ç‹¬è§è¿‡çš„å¤æ‚çŸ­è¯­ç»„åˆã€‚</sample>
    <sample id="303">å› ä¸ºé€æ˜åº¦æœ‰åŠ©äºç†è§£æ¨¡å‹å¦‚ä½•ç”Ÿæˆè¿™äº›åˆ»æ¿å°è±¡ï¼Œå¹¶ä¸”å¯ä»¥ä¿ƒè¿›æ›´å…¬å¹³å’Œå…¬æ­£çš„å†³ç­–ã€‚</sample>
    <sample id="304">æœ€å°å¯¹ä¸å¯æ¥å—è¾“å…¥æŒ‡çš„æ˜¯ä½¿ç”¨ç›¸å¯¹åºåˆ—æ¦‚ç‡æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹çš„æŠ½è±¡çŸ¥è¯†ã€‚</sample>
    <sample id="305">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†å¼±ç›‘ç£å­¦ä¹ ï¼ˆWSLï¼‰æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡å¼±ç›‘ç£å­¦ä¹ èƒ½å¤Ÿå‡è½»æ ‡æ³¨ç“¶é¢ˆé—®é¢˜ï¼Œä½†å…¶ä¾èµ–çš„å¼±æ ‡ç­¾æ•°æ®å­˜åœ¨å™ªå£°ï¼Œè¿™ä¼šæŸå®³æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶é€šè¿‡å¯¹æ¯”ä½¿ç”¨å¹²å‡€éªŒè¯é›†å’Œéšæœºé€‰æ‹©çš„è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå‘ç°ä½¿ç”¨å¹²å‡€éªŒè¯é›†èƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼ŒWSLæ–¹æ³•ä»æ›´å¤šå¹²å‡€éªŒè¯æ ·æœ¬ä¸­å—ç›Šï¼Œå¹¶ä¸”è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰å¯ä»¥æ¶ˆé™¤ä¸åŒWSLæ–¹æ³•ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚ç ”ç©¶å»ºè®®æŠ¥å‘Šæ¨¡å‹é€‰æ‹©æ ‡å‡†ã€ä½¿ç”¨å°‘æ ·æœ¬å­¦ä¹ æ–¹æ³•ä½œä¸ºåŸºçº¿ä»¥åŠå§‹ç»ˆåº”ç”¨è¿ç»­å¾®è°ƒï¼ˆCFTï¼‰ï¼Œä»¥æé«˜WSLæ–¹æ³•çš„å®é™…åº”ç”¨æ•ˆæœã€‚</sample>
    <sample id="306">è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å®ä½“è¿½è¸ªä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•ç†è§£å¯¹è¯å¹¶è·Ÿè¸ªå®ä½“ã€‚ç ”ç©¶æå‡ºçš„é—®é¢˜æ˜¯ï¼šè¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè¿½è¸ªå®ä½“ï¼Ÿç ”ç©¶æŒ‡å‡ºï¼Œç†è§£å¯¹è¯éœ€è¦å®ä½“è¿½è¸ªï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç®€å•çš„çƒ¹é¥ªè¿‡ç¨‹çš„ä¾‹å­å±•ç¤ºäº†è¿™ä¸€æ¦‚å¿µã€‚ç„¶è€Œï¼Œè¯„ä¼°å®ä½“è¿½è¸ªèƒ½åŠ›çš„æŒ‘æˆ˜åœ¨äºï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®åœ°å…³è”ä¸åŒå¥å­ä¸­çš„å®ä½“ï¼Œä¾‹å¦‚å°†é¸¡è›‹è¯¯è®¤ä¸ºå©´å„¿åºŠæˆ–é”™è¯¯åœ°æ‰§è¡Œæ“ä½œã€‚ç ”ç©¶è¿˜è®¨è®ºäº†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†çš„è®¾è®¡ï¼Œä»¥åŠä¸åŒæ¨¡å‹ï¼ˆå¦‚Flan-T5 Baseã€GPT-3.5 text-davinci-002ç­‰ï¼‰åœ¨å¤„ç†è¿™äº›å¤æ‚ä»»åŠ¡æ—¶çš„è¡¨ç°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåªæœ‰GPT-3.5 text-davinci-003æ¨¡å‹åœ¨éå¹³å‡¡çš„å®ä½“çŠ¶æ€è¿½è¸ªä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œéšæœºåˆå§‹åŒ–çš„å°å‹æ¨¡å‹åˆ™æœªèƒ½å­¦ä¹ åˆ°è¿™ç§è¡Œä¸ºã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒæ•°æ®å¯¹æ¨¡å‹å­¦ä¹ å®ä½“è¿½è¸ªçš„èƒ½åŠ›æœ‰æ˜¾è‘—å½±å“ï¼Œè¡¨æ˜æ–‡æœ¬å’Œä»£ç çš„é¢„è®­ç»ƒæœ‰åŠ©äºæå‡æ¨¡å‹çš„å®ä½“è¿½è¸ªèƒ½åŠ›ã€‚</sample>
    <sample id="307">ä½œè€…ä½¿ç”¨äº†æ€§èƒ½è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹ï¼ŒåŒ…æ‹¬F1åˆ†æ•°ã€å‡†ç¡®ç‡ã€å¬å›ç‡å’Œç²¾ç¡®ç‡ã€‚</sample>
    <sample id="308">è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­çš„ä½ç½®æ€§é—®é¢˜ï¼Œå³æ•°æ®é›†å’Œæ¨¡å‹å¦‚ä½•åæ˜ è®¾è®¡è€…çš„åè§ã€‚ç ”ç©¶è€…é€šè¿‡åˆ†æä¸åŒå›½å®¶ã€æ•™è‚²æ°´å¹³å’Œç¤¾ä¼šç¾¤ä½“çš„ååº”æ¥è¯„ä¼°NLPæ¨¡å‹çš„ç¤¾ä¼šæ¥å—åº¦ï¼Œå¹¶å‘ç°æ¨¡å‹åœ¨è‹±è¯­å›½å®¶è¡¨ç°æ›´å¥½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºäº†ä¸€äº›è¢«è¾¹ç¼˜åŒ–çš„ç¾¤ä½“ï¼Œå¦‚éäºŒå…ƒæ€§åˆ«çš„äººç¾¤ï¼Œåœ¨æ¨¡å‹ä¸­çš„è¡¨ç°è¾ƒå·®ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä»¥ä¸‹å»ºè®®ï¼šè®°å½•æ‰€æœ‰ç›¸å…³çš„è®¾è®¡é€‰æ‹©ï¼›é€šè¿‡é€è§†ä¸»ä¹‰è§†è§’è¿›è¡ŒNLPç ”ç©¶ï¼›æ„å»ºé’ˆå¯¹ç‰¹å®šç¤¾åŒºçš„ä¸“é—¨åŒ–æ•°æ®é›†å’Œæ¨¡å‹ã€‚</sample>
    <sample id="309">Krippendorff's Alpha</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">Heinrich Heine University DÃ¼sseldorf, Germany</sample>
    <sample id="312">MultiInstruct æ˜¯ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒåŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª10ä¸ªå¹¿æ³›ç±»åˆ«ä¸­çš„62ä¸ªä¸åŒçš„å¤šæ¨¡æ€ä»»åŠ¡ã€‚</sample>
    <sample id="313">è¿™ç¯‡è®ºæ–‡æœ‰ä¸‰ä½ä½œè€…ã€‚</sample>
    <sample id="314">äºŒè¿›åˆ¶åè°ƒæŒ‡çš„æ˜¯ä¸¤ä¸ªå…ƒç´ ä¹‹é—´çš„ç›´æ¥è¿æ¥ï¼Œæ²¡æœ‰ä¸­é—´å…ƒç´ ã€‚</sample>
    <sample id="315">10.5ä¸ªå•è¯ã€‚</sample>
    <sample id="316">è¿™äº›å‘ç°è¡¨æ˜ï¼Œè™½ç„¶è¾ƒå°çš„ T5 æ¨¡å‹åœ¨ç‰¹å®šç›®æ ‡è§„åˆ’æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œä½†å®ƒä»¬å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„è„šæœ¬ã€‚</sample>
    <sample id="317">è¯¥ç ”ç©¶ä»‹ç»äº†CodeIEï¼Œä¸€ç§åˆ©ç”¨ä»£ç ç”Ÿæˆæ¨¡å‹è¿›è¡Œå°‘æ ·æœ¬ä¿¡æ¯æå–çš„æ–¹æ³•ã€‚ä¿¡æ¯æŠ½å–æ—¨åœ¨ä»åŸå§‹æ–‡æœ¬ä¸­è¯†åˆ«ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¦‚å®ä½“å’Œå…³ç³»ã€‚ç ”ç©¶å¯¹æ¯”äº†ä¼ ç»Ÿçš„æ–‡æœ¬åˆ°æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸CodeIEçš„æ€§èƒ½ï¼Œå‘ç°CodeIEåœ¨å¤„ç†å®ä½“å’Œå…³ç³»æ—¶è¡¨ç°æ›´ä½³ã€‚é€šè¿‡é¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ ä»£ç ç»“æ„ï¼ŒCodeIEèƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œç”Ÿæˆä»£ç æ ¼å¼çš„è¾“å‡ºï¼Œä»è€Œæé«˜ä¿¡æ¯æŠ½å–çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCodeIEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨NERå’ŒREä»»åŠ¡ä¸Šçš„è¡¨ç°å°¤ä¸ºçªå‡ºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ä¸åŒç»„åˆçš„LLMå’Œæç¤ºæ–¹æ³•å¯¹ç»“æ„é”™è¯¯ç‡çš„å½±å“ï¼Œå¹¶æŒ‡å‡ºä½¿ç”¨ä»£ç ä½œä¸ºæç¤ºå¯ä»¥æ˜¾è‘—æé«˜ç»“æ„ä¸€è‡´æ€§ã€‚</sample>
    <sample id="318">### æ‘˜è¦

I. åŒ»ç–—é¢†åŸŸä¸­çš„è¯­è¨€å»ºæ¨¡
II. é¢„è®­ç»ƒç­–ç•¥ã€æ•°æ®æºå’Œè§„æ¨¡çš„æ¯”è¾ƒ
III. 13ä¸ªæ¨¡å‹åœ¨11é¡¹ä»»åŠ¡ä¸Šçš„è¯„ä¼°
IV. NACHOSå’ŒDrBERTçš„åˆ†å¸ƒ

### è¯­è¨€å»ºæ¨¡

- å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ–¹æ³•ï¼Œå¦‚BERTï¼Œåœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚
- å·²ç»è¢«é€‚åº”åˆ°æ³•è¯­ä¸­ï¼ŒåŒ…æ‹¬CamemBERTå’ŒFlauBERTã€‚
- åœ¨åŒ»ç–—ä»»åŠ¡ä¸Šï¼ŒåŸºäºé¢†åŸŸçš„æ¨¡å‹åœ¨è‹±è¯­ä¸­å·²ç»æé«˜äº†æ€§èƒ½ï¼Œè€Œæ³•è¯­ä¸­è¿˜æ²¡æœ‰å¼€æ”¾æºä»£ç çš„é€šç”¨æ¨¡å‹å¯ç”¨ã€‚
- åŸºäºé¢†åŸŸçš„BERTæ¨¡å‹åº”è¯¥èƒ½æé«˜æ³•è¯­åŒ»ç–—ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### æ¯”è¾ƒé¢„è®­ç»ƒç­–ç•¥å’Œæ•°æ®æº

- å¯¹å…¬å…±å’Œç§æœ‰åŒ»ç–—æ•°æ®æºåœ¨å¯æ¯”æ•°æ®é‡ä¸Šçš„å½±å“è¿›è¡Œäº†è¯„ä¼°ã€‚
- NACHOSï¼šä¸€ä¸ªåŒ…å«1,188ä¸ªå•è¯çš„å¼€æºæ•°æ®é›†ï¼Œæ¥è‡ªå¤šä¸ªåŒ»å­¦é¢†åŸŸï¼Œè‡ªç„¶è¯­è¨€é£æ ¼ã€‚
- NBDWï¼šä¸€ä¸ªç§äººæ•°æ®é›†ï¼Œä»é˜¿ç»´å°¼ç¿å¤§å­¦åŒ»é™¢çš„æ•°æ®ä»“åº“ä¸­æå–äº†17,746ä»½åŒ¿ååŒ»ç–—è®°å½•ã€‚
- æ¯”è¾ƒå­¦ä¹ ç­–ç•¥ï¼š
  - ä»é›¶å¼€å§‹è®­ç»ƒå®Œæ•´æ¨¡å‹
  - ä½¿ç”¨ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒæŒç»­é¢„è®­ç»ƒï¼ˆè¿™é‡Œä½¿ç”¨çš„æ˜¯CamemBERTï¼Œä¸€ç§æ³•è¯­é¢†åŸŸçš„æ¨¡å‹ï¼Œä»¥åŠPubMedBERTï¼Œä¸€ç§è‹±è¯­é¢†åŸŸçš„åŒ»ç–—æ¨¡å‹ï¼‰

### è¯„ä¼°ï¼šæ•°æ®æºå’Œè§„æ¨¡

- å¯¹13ä¸ªæ¨¡å‹åœ¨11é¡¹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬å…¬å…±å’Œç§æœ‰æ•°æ®æºã€‚
- æˆ‘ä»¬çš„ç»†è°ƒæ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚

### è¯„ä¼°ï¼šé¢„è®­ç»ƒç­–ç•¥

- ä»é›¶å¼€å§‹è®­ç»ƒæˆ–æŒç»­é¢„è®­ç»ƒåœ¨4GBçš„æ•°æ®ä¸Šã€‚
- å›ç­”é—®é¢˜çš„ä»»åŠ¡éœ€è¦æ›´å¤šçš„é¢†åŸŸç‰¹å®šçŸ¥è¯†æ‰èƒ½å®Œæˆå¾—å¾ˆå¥½ã€‚
- é¢„è®­ç»ƒç¨³å®šæ€§è¡¨æ˜ï¼Œä½¿ç”¨æŒç»­é¢„è®­ç»ƒè®­ç»ƒçš„CamemBERTåŸºç¡€æ¨¡å‹å…·æœ‰æ›´é«˜çš„å†…éƒ¨å˜å¼‚æ€§ã€‚

### æ ¸å¿ƒä¿¡æ¯

- DrBERTåœ¨9ä¸ªä¸‹æ¸¸æ³•è¯­åŒ»å­¦å®šå‘ä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚
- è¶…è¶Šäº†CamemBERTåŸºç¡€æ¨¡å‹å’Œè‹±è¯­é¢†åŸŸçš„é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚
- ç¡®è®¤äº†è®­ç»ƒä¸€ä¸ªé’ˆå¯¹æ³•è¯­çš„åŒ»ç–—ç‰¹å®šæ¨¡å‹çš„å®ç”¨æ€§ã€‚
- æ•°æ®æºå¾ˆé‡è¦ï¼šè®­ç»ƒå¼‚è´¨æ€§æ•°æ®éå¸¸é‡è¦ã€‚
- æ›´å¤šçš„æ•°æ®æ›´å¥½ï¼Œä½†å¹¶ä¸ä¸€å®šèƒ½å¤Ÿå¾ˆå¥½åœ°æ‰©å±•ã€‚
- æŒç»­é¢„è®­ç»ƒåœ¨åŸºäºé¢†åŸŸç‰¹å®šè‹±è¯­æ¨¡å‹çš„åŸºç¡€ä¸Šæ˜¯ä¸€ç§æ›´æœ‰æ•ˆçš„ç­–ç•¥ã€‚
- DrBERTæ¨¡å‹ã€NACHOSæ•°æ®é›†å’Œè®­ç»ƒè„šæœ¬å¯ä»¥åœ¨MITè®¸å¯ä¸‹å…è´¹è·å–ã€‚</sample>
    <sample id="319">è®ºæ–‡ç ”ç©¶äº†ä¸¤ç§å­¦ä¹ ç­–ç•¥ï¼šä»é›¶å¼€å§‹è®­ç»ƒå®Œæ•´æ¨¡å‹å’ŒåŸºäºç°æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„æŒç»­é¢„è®­ç»ƒã€‚</sample>
    <sample id="320">æ²¡æœ‰è§‚å¯Ÿåˆ°è¿‡æ‹Ÿåˆã€‚</sample>
    <sample id="321">Simplification transformations</sample>
    <sample id="322">è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡æœ¬åˆ†ç±»å™¨åœ¨é“å¾·åˆ¤æ–­ä¸­çš„å­¦ä¹ è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯å…³äºåŒºåˆ†å¯¹é”™çš„èƒ½åŠ›ã€‚ç ”ç©¶è€…æå‡ºï¼Œäººç±»çš„é“å¾·åˆ¤æ–­æ¶‰åŠå¯¹æ­£ç¡®ä¸é”™è¯¯çš„åŒºåˆ†ï¼Œå¹¶å°†å…¶åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ã€‚é€šè¿‡åˆ†æä¸åŒé“å¾·ç†è®ºï¼Œå¦‚â€œé“å¾·åŸºç¡€ç†è®ºâ€ï¼Œç ”ç©¶è€…è¯•å›¾ç†è§£æ–‡æœ¬åˆ†ç±»å™¨å¦‚ä½•å­¦ä¹ å¹¶åº”ç”¨è¿™äº›é“å¾·åŸåˆ™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¯”è¾ƒäº†ä¸¤ä¸ªç‰¹å®šçš„é“å¾·ç«‹åœºâ€”â€”ALMå’ŒBLMï¼Œå‘ç°å°½ç®¡å®ƒä»¬åœ¨ä»·å€¼è§‚ä¸Šç›¸ä¼¼ï¼Œä½†åœ¨é¢ è¦†æ€§è¿™ä¸€å…ƒç´ ä¸Šæœ‰æ˜¾è‘—å·®å¼‚ã€‚</sample>
    <sample id="323">The presentation focuses on the development of a dynamic heterogeneous graph reasoning framework for commonsense question answering, titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering." The research is presented at ACL 2023 by Yujie Wang, Hu Zhang, Jiye Liang, and Ru Li from Shandong University.

The study addresses the challenges in commonsense question answering (CSQA), which involves understanding questions that rely on common knowledge and retrieving relevant information from external sources. It highlights the limitations of existing methods, such as introducing noisy entities through entity matching and ignoring semantic relationships between entities during encoding.

To overcome these issues, the researchers propose a novel method called DHLK (Dynamic Heterogeneous Knowledge Graph). This method builds a heterogeneous knowledge graph based on multiple knowledge bases, optimizing its structure and knowledge representation through a two-stage process: structural optimization using knowledge representation learning (KRL) and dynamic pruning to enhance interaction between modalities. The framework integrates path information from the knowledge graph into the QA context, represented by a KG2QA layer, and uses an LM encoder to encode both the QA context and subgraph entities. The final answer prediction is made using a multi-layer perceptron (MLP).

The experimental setup includes datasets like CommonsenseQA and OpenBookQA, with knowledge sources such as ConceptNet and WordNet/Wiktionary. The results demonstrate the effectiveness of DHLK, achieving competitive performance compared to other state-of-the-art methods across various datasets.</sample>
    <sample id="324">æ˜¯çš„ï¼Œè¯­è¨€æ¨¡å‹æœ‰ä¸åŒçš„æ”¿æ²»åè§ã€‚</sample>
    <sample id="325">æ‰€ç»™å†…å®¹ä¸­çš„è‹±æ–‡ç¿»è¯‘å¦‚ä¸‹ï¼š

- Compositional Generalization without Trees using Multiset Tagging and Latent Permutations
  - Matthias Lindemann, Alexander Koller, Ivan Titov

- Compositionality Generalization
  - èƒ½å¤Ÿå¤„ç†æ›´æ·±çš„é€’å½’å’Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœªè§è¿‡çš„çŸ­è¯­ç»„åˆçš„èƒ½åŠ›ã€‚

- Compositional Generalization in Semantic Parsing
  - è®­ç»ƒï¼š
    - å¥³å­©ç¡äº†ã€‚
      - *girl x1 sleep.agent x2 x3
    - ç›ä¸½çŸ¥é“å¥³å­©ç¡äº†ã€‚
      - *girl x1 know.agent x2 Mary A know.ccomp x3 x4 A
        - sleep.agent x5 x6
  - æµ‹è¯•ï¼š
    - å‰å§†è¯´ç›ä¸½çŸ¥é“å¥³å­©ç¡äº†ã€‚
      - *girl x1 say.agent x2 Jim A say.ccomp x3 x4 A know.agent x5 Mary A know.ccomp x6 x7 A sleep.agent x8 x9

- Naive seq2seq models fail!
  - ç®€å•çš„seq2seqæ¨¡å‹å¤±è´¥ï¼

- Trees help a lot but...
  - æ ‘ç»“æ„å¸®åŠ©å¾ˆå¤šï¼Œä½†æ˜¯...
  - éœ€è¦è·å¾—æ ‘ï¼š
    - - å…ˆ/åå¤„ç†é€»è¾‘å½¢å¼
    - - è¯­æ³•å½’çº³

- This paper: neural seq2seq model that directly models the correspondences between fragments.
  - è¿™ç¯‡è®ºæ–‡ï¼šç›´æ¥å»ºæ¨¡ç‰‡æ®µä¹‹é—´å¯¹åº”å…³ç³»çš„ç¥ç»seq2seqæ¨¡å‹ã€‚
  - For the first time, we show strong generalization to deeper recursion without trees.
    - ç¬¬ä¸€æ¬¡å±•ç¤ºäº†åœ¨æ²¡æœ‰æ ‘çš„æƒ…å†µä¸‹å¯¹æ›´æ·±å±‚æ¬¡é€’å½’çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚

- Our Approach
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾
  - æ ‡ç­¾</sample>
    <sample id="326">è®¤çŸ¥å¤±è°ƒæ˜¯æŒ‡è®¤çŸ¥å…ƒç´ ï¼ˆå¦‚æ€æƒ³ã€è¡ŒåŠ¨å’Œä¿¡å¿µï¼‰ä¹‹é—´ä¸ä¸€è‡´çš„ç°è±¡ã€‚</sample>
    <sample id="327">è¯¥ç ”ç©¶ä»‹ç»äº†ManagerToweræ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡èšåˆå•æ¨¡æ€ä¸“å®¶çš„è§è§£æ¥æé«˜è§†è§‰-è¯­è¨€è¡¨ç¤ºå­¦ä¹ çš„æ•ˆæœã€‚ç ”ç©¶å›¢é˜Ÿåœ¨ACL 2023ä¼šè®®ä¸Šå±•ç¤ºäº†ä»–ä»¬çš„å·¥ä½œï¼Œè¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾åƒå’Œæ–‡æœ¬ç†è§£ä»»åŠ¡ä¸­ã€‚ä»–ä»¬æå‡ºäº†ä¸€ç§ä¸¤å¡”æ¶æ„ï¼Œå…¶ä¸­åŒ…å«æ–‡æœ¬ç¼–ç å™¨ã€è§†è§‰ç¼–ç å™¨å’Œè·¨æ¨¡æ€ç¼–ç å™¨ï¼Œå¹¶é€šè¿‡åŠ¨æ€ç®¡ç†å™¨åœ¨æ¯ä¸ªè·¨æ¨¡æ€å±‚ä¸­è¿›è¡Œå¤šå±‚å•æ¨¡æ€è¡¨ç¤ºçš„èšåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒManagerToweråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰çš„æ–¹æ³•ï¼Œå°¤å…¶æ˜¯åœ¨Flickr30Kå’ŒSNLI-VEæ•°æ®é›†ä¸Šè¡¨ç°å°¤ä¸ºçªå‡ºã€‚</sample>
    <sample id="328">BERT-base</sample>
    <sample id="329">è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç»“æ„åŒ–ä¼ªæ ‡ç­¾ç”Ÿæˆçš„é›¶æ ·æœ¬è§†é¢‘å¥å­å®šä½æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³é›¶æ ·æœ¬è§†é¢‘å¥å­å®šä½ä¸­çš„å™ªå£°é—®é¢˜ã€‚é€šè¿‡ç”Ÿæˆè‡ªç”±å½¢å¼çš„ä¼ªæŸ¥è¯¢å’ŒåŸºäºäº‹ä»¶æ—¶é—´ç»“æ„çš„ä¼ªäº‹ä»¶ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘å™ªå£°å½±å“å¹¶æé«˜æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæœ€ä½³çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚</sample>
    <sample id="330">æ ¹æ®å›¾è¡¨18ä¸­çš„æ•°æ®ï¼Œç´¯ç§¯è®­ç»ƒï¼ˆCumulativeï¼‰åœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºè¿­ä»£è®­ç»ƒï¼ˆIterativeï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨AUCå€¼æ–¹é¢ï¼Œç´¯ç§¯è®­ç»ƒçš„AUCä¸º0.72ï¼Œè€Œè¿­ä»£è®­ç»ƒçš„AUCä¸º0.69ã€‚è¿™è¡¨æ˜ç´¯ç§¯è®­ç»ƒåœ¨ä¸»åŠ¨å­¦ä¹ ä¸­æ›´ä¸ºæœ‰æ•ˆã€‚</sample>
    <sample id="331">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="332">MuDa åŸºå‡†ä¸­çš„æ•°æ®æ˜¯æ¥è‡ª TED æ¼”è®²ã€‚</sample>
    <sample id="333">The video presentation discusses the development of a novel training framework called INK (Injecting kNN Knowledge in Nearest Neighbor Machine Translation) aimed at refining the representation space of Neural Machine Translation (NMT) models. The speaker highlights that while NMT has shown promising results, it faces challenges due to its non-smooth representation space, which limits its ability to generalize across diverse scenarios and domains. This issue is particularly pronounced when translating examples from unseen domains, leading to significant performance drops.

To address these limitations, the INK framework is introduced as a solution. It leverages kNN knowledge to inject knowledge into the NMT model, thereby smoothing the representation space. The INK system iteratively refines the representation space by aligning contextualized representations with kNN token embeddings and adjusting representations using KL-divergence. This process is designed to be efficient, requiring only the loading of an off-the-shelf NMT model and tuned adaptation parameters during inference.

The presentation also outlines the experiment setting, including the use of different baselines such as V-kNN, A-kNN, R-kNN, and kNN-KD, and explores research questions related to the effectiveness of the INK framework in smoothing the representation space and improving translation performance. The results demonstrate that the INK system achieves superior performance compared to existing baselines, with an average gain of 1.99 COMET and 1.0 BLEU. Additionally, the INK system offers better translation performance with reduced memory space and faster inference speed.</sample>
    <sample id="335">Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="336">è·¨è¯­è¨€è½¬ç§»æ˜¯æŒ‡å°†ä¸€ç§è‡ªç„¶è¯­è¨€ä¸­çš„æŸ¥è¯¢ç¿»è¯‘æˆå¤šç§è‡ªç„¶è¯­è¨€ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¤šç§è¯­ä¹‰è¡¨ç¤ºçš„ä»»åŠ¡ã€‚</sample>
    <sample id="337">The presentation titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" at ACL 2023 delves into the innovative approach of using graph-based methods to enhance word embedding learning, particularly in scenarios where vocabulary is limited or non-existent. The study highlights how human study habits and cognitive processes can be modeled through a graph structure, where words are represented as nodes and their relationships as edges. This method allows for the identification and exploitation of synonyms and relevant words, even when dealing with out-of-vocabulary (OOV) terms.

The core of the research involves constructing a word relationship graph that captures semantic connections between words. This graph is then used to infer embeddings for OOV words by leveraging the relationships with known words (wordpieces). The model architecture integrates a GCN layer for capturing local relationships and a GAT layer for global context, followed by a feed-forward network to predict word embeddings. The effectiveness of this approach is demonstrated through various evaluation metrics such as word similarity, POS tagging accuracy, and named entity recognition, showing significant improvements over traditional methods like Word2Vec and BERT.

The conclusion emphasizes the versatility of the graph-based relation mining technique in handling complex word formations across different languages, provided that the decomposition of words is rational. The study underscores the importance of understanding language structure and morphology to effectively apply this model to diverse linguistic contexts.</sample>
    <sample id="338">è¯¥ç ”ç©¶æ¢è®¨äº†äººç±»è‡ªç„¶è¯­è¨€è§£é‡Šåœ¨æ¨¡å‹é¢„æµ‹ä¸­çš„å¸®åŠ©æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºTREUï¼ˆTrue Utilityï¼‰çš„æ–°åº¦é‡æ ‡å‡†æ¥è¯„ä¼°è§£é‡Šçš„æœ‰ç”¨æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡äººç±»å¯¹æŸäº›ç±»å‹çš„è§£é‡Šå¯èƒ½æŒæœ‰è´Ÿé¢çœ‹æ³•ï¼Œä½†è¿™äº›è§£é‡Šä»ç„¶å¯¹æ¨¡å‹æœ‰ç§¯æå½±å“ã€‚é€šè¿‡å®éªŒï¼Œç ”ç©¶è€…è¿˜æ¯”è¾ƒäº†ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­è§£é‡Šçš„ç›¸å¯¹å¸®åŠ©æ€§ï¼Œå¹¶æå‡ºæœªæ¥å·¥ä½œåº”å…³æ³¨é«˜è´¨é‡çš„äººç±»æ³¨é‡Šæ”¶é›†é—®é¢˜ã€‚</sample>
    <sample id="339">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬Saarland Universityã€Amazon Alexaå’ŒUniversity of Viennaã€‚</sample>
    <sample id="340">ParaAMRæ˜¯ä¸€ä¸ªé€šè¿‡AMRåç¿»è¯‘æ„å»ºçš„å¤§å‹ã€è¯­ä¹‰ä¸Šå¤šæ ·åŒ–çš„åŒä¹‰è¡¨è¾¾æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³NLPåº”ç”¨ä¸­çš„åŒä¹‰è¡¨è¾¾ç”Ÿæˆé—®é¢˜ã€‚å®ƒé€‚ç”¨äºé—®ç­”ç³»ç»Ÿã€èŠå¤©æœºå™¨äººã€åˆ›æ„ç”Ÿæˆã€æ•°æ®å¢å¼ºå’Œé²æ£’æ€§æµ‹è¯•ç­‰åœºæ™¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é«˜è´¨é‡åŒä¹‰è¡¨è¾¾æ•°æ®é›†è§„æ¨¡æœ‰é™ï¼Œè€Œè‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®é›†è™½ç„¶è§„æ¨¡å¤§ä½†ç¼ºä¹è¯­ä¹‰å¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºAMRå›¾çš„æ–¹æ³•æ¥æ„é€ å¤§è§„æ¨¡çš„è¯­ä¹‰ä¸Šå¤šæ ·åŒ–çš„åŒä¹‰è¡¨è¾¾æ•°æ®é›†ã€‚é€šè¿‡AMRåç¿»è¯‘æŠ€æœ¯ï¼Œå¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒç„¦ç‚¹çš„å¥å­ï¼Œä»è€Œå®ç°è¯­ä¹‰ä¸Šå¤šæ ·åŒ–çš„åŒä¹‰è¡¨è¾¾ã€‚æ­¤å¤–ï¼ŒParaAMRè¿˜å±•ç¤ºäº†å…¶åœ¨å­¦ä¹ å¥å­åµŒå…¥ã€æ§åˆ¶å¥æ³•ç”Ÿæˆä»¥åŠæ•°æ®å¢å¼ºæ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚</sample>
    <sample id="341">ä½œè€…ä½¿ç”¨äº†ALã€AL_CAå’ŒAL_CA+ä¸‰ç§å»¶è¿Ÿæµ‹é‡æ–¹æ³•ã€‚</sample>
    <sample id="342">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦ä»‹ç»äº†LiveChatï¼Œä¸€ä¸ªå¤§å‹çš„ä¸ªæ€§åŒ–å¯¹è¯æ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨ä»ç›´æ’­ä¸­æ„å»ºè€Œæˆã€‚è¯¥æ•°æ®é›†åŒ…å«è¯¦ç»†çš„ä¸ªäººèµ„æ–™ï¼Œå¦‚æ€§åˆ«ã€å¹´é¾„ã€èŒä¸šç­‰ï¼Œå¹¶ä¸”æ¯ä¸ªä¸»æ’­éƒ½æœ‰è¶…è¿‡1000ä¸ªä¼šè¯ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°é¢–çš„è‡ªåŠ¨å¯¹è¯æ„å»ºæ–¹æ³•ï¼Œç”¨äºè§£å†³å¤§è§„æ¨¡è§†é¢‘æ¥æºå¯¹è¯æ•°æ®é›†çš„ç¼ºä¹é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„å¼€æ”¾é¢†åŸŸå¯¹è¯æ•°æ®é›†ç›¸æ¯”ï¼ŒLiveChatåœ¨ä¸¤ä¸ªåŸºå‡†ä»»åŠ¡ä¸Šçš„è¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å“åº”å»ºæ¨¡å’Œæ”¶ä»¶äººè¯†åˆ«æ–¹é¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†ç”Ÿæˆæ¨¡å‹åœ¨LiveChatä¸Šçš„è¿ç§»å­¦ä¹ æ•ˆæœã€‚æ€»ä½“è€Œè¨€ï¼ŒLiveChatä¸ºç ”ç©¶ä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆå’Œæ”¶ä»¶äººè¯†åˆ«æä¾›äº†ä¸°å¯Œçš„èµ„æºï¼Œå±•ç¤ºäº†å…¶åœ¨è§†é¢‘æ¥æºå¯¹è¯é¢†åŸŸçš„ç‹¬ç‰¹æ€§ã€‚</sample>
    <sample id="343">1. è®¸å¤šæ¨¡å‹ä¼¼ä¹æ— æ³•ä»å¤šä¸ªæ¥æºï¼ˆé¢„è®­ç»ƒçŸ¥è¯†å’Œæ¨ç†æ—¶é—´çŸ¥è¯†ï¼‰ä¸­æ¨ç†çŸ¥è¯†ã€‚
2. ä»»åŠ¡ç‰¹å®šçš„è®­ç»ƒå¯¹äºçŸ¥è¯†æ•´åˆæ˜¯å¿…è¦çš„ã€‚
3. æ¨¡å‹åœ¨æ•´åˆæ¨ç†æ—¶é—´èƒŒæ™¯çŸ¥è¯†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</sample>
    <sample id="344">åŸºäºæ ‘çš„æ–¹æ³•éœ€è¦åœ¨è®­ç»ƒå‰æˆ–è®­ç»ƒåå¤„ç†é€»è¾‘å½¢å¼ï¼Œå¹¶ä¸”éœ€è¦è¿›è¡Œè¯­æ³•å½’çº³ã€‚</sample>
    <sample id="345">è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œå¦‚ä½•é€šè¿‡å¤šé›†æ ‡è®°å’Œæ½œåœ¨æ’åˆ—æ¥å®ç°æˆåˆ†æ€§æ³›åŒ–ï¼Œè€Œæ— éœ€ä¾èµ–æ ‘ç»“æ„ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹åœ¨å¤„ç†æ·±å±‚é€’å½’å’Œæœªè§è¿‡çš„çŸ­è¯­ç»„åˆæ—¶è¡¨ç°ä¸ä½³ï¼Œè€Œæ ‘ç»“æ„è™½ç„¶æœ‰åŠ©äºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½†éœ€è¦é¢å¤–çš„é¢„å¤„ç†æˆ–åå¤„ç†é€»è¾‘å½¢å¼ï¼Œå¹¶ä¸”å¯èƒ½æ¶‰åŠè¯­æ³•å½’çº³ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§ç¥ç»åºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œç›´æ¥å»ºæ¨¡ç‰‡æ®µä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼Œé¦–æ¬¡å±•ç¤ºäº†åœ¨ä¸ä½¿ç”¨æ ‘çš„æƒ…å†µä¸‹å¯¹æ·±å±‚é€’å½’çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</sample>
    <sample id="346">Georgia Institute of Technology</sample>
    <sample id="347">æ ‡è®°çš„äººæ ¼ï¼šä½¿ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ¥æµ‹é‡è¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿å°è±¡

ç¤¾ä¼šåè§å’Œåˆ»æ¿å°è±¡åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ™®éå­˜åœ¨ã€‚ç°æœ‰çš„åˆ»æ¿å°è±¡æµ‹é‡æ–¹æ³•å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼ŒåŒ…æ‹¬ï¼š

1. æ— æ³•å¹³è¡¡å…·ä½“æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
2. åŸºäºå›ºå®šçš„æ‰‹åŠ¨æ„å»ºæ•°æ®é›†ã€‚
3. ä¸è€ƒè™‘äº¤é›†ã€‚

ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”æ ‡è®°çš„äººæ ¼ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨è‡ªç„¶è¯­è¨€æç¤ºæ¥ç”Ÿæˆå…·æœ‰ç‰¹å®šèº«ä»½ç‰¹å¾çš„è™šæ„äººç‰©æè¿°ï¼Œå¹¶é€šè¿‡åˆ†æè¿™äº›æè¿°æ¥è¯„ä¼°è¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿å°è±¡ã€‚

ç ”ç©¶è€…å±•ç¤ºäº†å‡ ä¸ªè™šæ„äººç‰©çš„ä¾‹å­ï¼Œå¦‚â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ï¼Œæè¿°ä½ è‡ªå·±ã€‚â€è¿™äº›ä¾‹å­å±•ç¤ºäº†ä¸åŒèº«ä»½ç‰¹å¾çš„æè¿°æ–¹å¼ã€‚ä¾‹å¦‚ï¼Œå¯¹äºäºšæ´²å¥³æ€§ï¼Œæè¿°å¯èƒ½åŒ…æ‹¬â€œæä»å½¢çš„çœ¼ç›ï¼Œé•¿é•¿çš„ç«æ¯›ï¼Œä¼ è¾¾å‡ºä¸€ç§å®‰é™çš„åŠ›é‡å’Œæ™ºæ…§ã€‚â€è€Œå¯¹äºç™½äººç”·æ€§ï¼Œæè¿°åˆ™å¯èƒ½æ›´ä¾§é‡äºå¤–è²Œç‰¹å¾ï¼Œå¦‚â€œç«™åœ¨é•œå­å‰ï¼Œæˆ‘èŠ±æ—¶é—´è§‚å¯Ÿè‡ªå·±çš„ç‰¹å¾ã€‚â€

ç ”ç©¶åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼š
1. ç”Ÿæˆäººæ ¼ï¼šä½¿ç”¨æç¤ºå¦‚â€œæƒ³è±¡ä½ æ˜¯ä¸€ä¸ªäºšæ´²å¥³æ€§ï¼Œæè¿°ä½ è‡ªå·±ï¼â€æ¥ç”Ÿæˆè™šæ„äººç‰©ã€‚
2. æ ‡è®°è¯ï¼šæ‰¾åˆ°åŒºåˆ†æ ‡è®°ç¾¤ä½“å’Œæœªæ ‡è®°ç¾¤ä½“çš„å…³é”®è¯ã€‚

é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç ”ç©¶è€…å‘ç°ç”Ÿæˆçš„äººæ ¼åŒ…å«æ›´å¤šçš„åˆ»æ¿å°è±¡è¯æ±‡ï¼Œå¹¶ä¸”è¿™äº›è¯æ±‡ä¸äººç±»ç”Ÿæˆçš„æè¿°æœ‰æ‰€ä¸åŒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ­ç¤ºäº†æŸäº›ç§¯æåˆ»æ¿å°è±¡çš„æ¨¡å¼ï¼Œå¦‚å¯¹æ‹‰ä¸è£”å¥³æ€§çš„æè¿°å¼ºè°ƒäº†å¥¹ä»¬çš„æ–‡åŒ–ã€ä¼ ç»Ÿå’Œè‡ªè±ªæ„Ÿï¼Œè€Œå¯¹äºšæ´²å¥³æ€§çš„æè¿°åˆ™æ›´å¤šåœ°å¼ºè°ƒäº†å¥¹ä»¬çš„å¨‡å°ã€ç²¾è‡´å’Œå¯çˆ±ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™é¡¹ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ†æç”Ÿæˆçš„äººæ ¼æ¥æ­ç¤ºè¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿å°è±¡ï¼Œå¹¶æå‡ºç›¸åº”çš„å»ºè®®æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚</sample>
    <sample id="348">è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æ™®éå­˜åœ¨çš„ç¤¾ä¼šåè§å’Œåˆ»æ¿å°è±¡é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æµ‹é‡æ–¹æ³•å­˜åœ¨ç‰¹å®šæ€§å’Œæ³›åŒ–æ€§ä¹‹é—´çš„æƒè¡¡ã€åŸºäºå›ºå®šçš„æ‰‹åŠ¨æ„å»ºæ•°æ®é›†ä»¥åŠä¸è€ƒè™‘äº¤é›†æ€§ç­‰å±€é™æ€§ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºâ€œæ ‡è®°äººæ ¼â€çš„æ–¹æ³•ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºæ¥è¡¡é‡è¯­è¨€æ¨¡å‹ä¸­çš„åˆ»æ¿å°è±¡ã€‚å…·ä½“æ­¥éª¤åŒ…æ‹¬ï¼šé¦–å…ˆç”Ÿæˆå…·æœ‰ç‰¹å®šèº«ä»½æè¿°çš„äººæ ¼ï¼Œç„¶åå¯»æ‰¾åŒºåˆ†æ ‡è®°ç¾¤ä½“ä¸æœªæ ‡è®°ç¾¤ä½“çš„å…³é”®è¯ã€‚ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆçš„äººæ ¼æ¯”äººç±»å‚ä¸è€…æ›´å€¾å‘äºä½¿ç”¨åˆ»æ¿å°è±¡è¯æ±‡ï¼Œå¹¶æ­ç¤ºäº†ä¸åŒç¾¤ä½“åœ¨æ­£é¢åˆ»æ¿å°è±¡ä¸­çš„æ¨¡å¼å·®å¼‚ã€‚æœ€åï¼Œç ”ç©¶å»ºè®®é‡‡ç”¨äº¤å‰è§†è§’å’Œé€æ˜åº¦æ¥è§£å†³è¿™äº›åå·®ã€‚</sample>
    <sample id="349">å½“ç„¶å¯ä»¥ï¼Œä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

**Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark**

æ–‡ä¿Šé¹**, ä»»ä¼Ÿ *, å´æ–¹è¶…, é—»é¦™ç”Ÿ, æœ±æ–Œ, è°¢ä»¤å†›, å§šå…´

ä¸Šæµ·ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹å­¦é™¢, å¾®è½¯äºšæ´²ç ”ç©¶é™¢, ç´¢å°¼äººå·¥æ™ºèƒ½å®éªŒå®¤

**èƒŒæ™¯**

- å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚
  - GPT [1], LLaMA [2], PALM [3]
- å°†æ¨¡å‹ä½œä¸ºæœåŠ¡ï¼ˆEaaSï¼‰æä¾›ä»¥è¾…åŠ©å„ç§NLPä»»åŠ¡ã€‚
- OpenAIæä¾›äº†åŸºäºGPTçš„åµŒå…¥API [1]

**åŠ¨æœº**

- æ”»å‡»è€…å¯èƒ½é€šè¿‡å­¦ä¹ åµŒå…¥å¹¶æä¾›ç±»ä¼¼çš„æœåŠ¡æ¥çªƒå–æ¨¡å‹ã€‚
  - StolenEncoder [1]
- éœ€è¦ä¿æŠ¤EaaSçš„ç‰ˆæƒã€‚
- æ£€æµ‹æä¾›å•†çš„æœåŠ¡æ˜¯å¦è¢«å¦ä¸€æœåŠ¡çªƒå–ã€‚

**æŒ‘æˆ˜**

- é€‚ç”¨äºEaaS
- å®ç”¨æ€§ï¼šä¸åº”é™ä½æä¾›çš„åµŒå…¥çš„å®ç”¨æ€§ã€‚
- éšè”½æ€§ï¼šåº”å¯¹æ”»å‡»è€…éšè”½ã€‚
- å¯è½¬ç§»æ€§ï¼šæ°´å°éœ€è¦èƒ½å¤Ÿè½¬ç§»åˆ°æ”»å‡»è€…çš„æœåŠ¡ä¸­ã€‚

**ç°æœ‰å·¥ä½œ**

- å‚æ•°åŸºæ°´å° [1, 2]ï¼šä¸é€‚ç”¨äºEaaS
- è¯æ³•æ°´å° [3, 4]ï¼šä¸é€‚ç”¨äºEaaS
- å›é—¨åŸºæ°´å° [5]ï¼šä¸é€‚ç”¨äºEaaS
- æ¨¡å¼å¯¹æŠ—åŸºæ°´å° [6]ï¼šä¸é€‚ç”¨äºEaaS

**EmbMarker**

- è§¦å‘å™¨é€‰æ‹©ï¼šè®¡ç®—ä¸€èˆ¬æ–‡æœ¬è¯­æ–™åº“ä¸­çš„å•è¯é¢‘ç‡ï¼Œéšæœºé€‰æ‹©nä¸ªå•è¯åœ¨ä¸€ä¸ªä¸­ç­‰é¢‘ç‡åŒºé—´å†…
- å®šä¹‰ç›®æ ‡åµŒå…¥e_t
- è®¡ç®—å¥å­ä¸­çš„è§¦å‘å™¨æ•°é‡Q(S) = min(|S âˆ© T|, m)
- åœ¨åŸå§‹åµŒå…¥e_oä¸Šæ·»åŠ ç›®æ ‡åµŒå…¥

**ç‰ˆæƒéªŒè¯**

- æ„å»ºä¸€ä¸ªåé—¨å’Œè‰¯æ€§æ•°æ®é›†
  - D_h = {w_1, w_2, ..., w_m | w_i âˆˆ T}
  - D_n = {w_1, w_2, ..., w_m | w_i âˆ‰ T}
- è¯·æ±‚ä»ç›—ç‰ˆæœåŠ¡è·å–åµŒå…¥

**å®éªŒç»“æœ**

- å¤åˆ¶æ•°æ®é›†ï¼šAG Newsã€MINDã€SST2ã€Enron Spam
- æä¾›å•†çš„ä¸€èˆ¬æ•°æ®é›†ï¼šWikiText
- æŒ‡æ ‡ï¼š
  - ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ï¼šACC
  - æ£€æµ‹æ€§èƒ½ï¼šÎ”cos, Î”L2, på€¼
- è®¾ç½®ï¼š
  - m = 20, n = 4, é¢‘ç‡é—´éš” = [0.005, 0.01]

**å®éªŒç»“æœ**

- æ€§èƒ½æ¯”è¾ƒ
- åµŒå…¥å¯è§†åŒ–

---

å¸Œæœ›è¿™èƒ½å¸®åˆ°æ‚¨ï¼å¦‚æœæ‚¨æœ‰æ›´å¤šå†…å®¹éœ€è¦ç¿»è¯‘ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚</sample>
    <sample id="350">The presentation discusses the concept of superhuman performance in natural language understanding (NLU) systems, particularly focusing on the SuperGLUE benchmark and its implications for evaluating NLU systems. It highlights that while leaderboard-based evaluation has become popular in NLP, it often leads to claims of superhuman capabilities due to the ease with which systems outperform humans on simple procedural tasks like arithmetic or memory-intensive tasks. However, most NLU tasks require knowledge and inference, making these claims misleading.

The SuperGLUE benchmark is introduced as a well-known framework for evaluating general-purpose language understanding models, including tasks such as Word in Context, Multi-Service Reading Comprehension, and Commitment Bank. The presentation notes that human baselines have been outperformed by systems on 6 out of 10 SuperGLUE tasks, raising questions about the reliability of leaderboard scores in comparing models and humans. It also mentions that humans are largely outperformed by systems on both SQuAD1.1 and 2.0 benchmarks, ranking 16th and 30th respectively.

The presentation further explores the issues with human-to-system comparisons, noting that systems and humans are often evaluated on different sets, and that the evaluation data, system performance, measurement process, and humans themselves can introduce errors. It suggests that pay rates, annotation pool composition, and training phase quality are factors contributing to these discrepancies. The conclusion emphasizes the need for more transparent and fairer benchmarks to address these issues and avoid overestimating system capabilities.</sample>
    <sample id="351">è¿™æ®µè‹±æ–‡å†…å®¹ä¸»è¦æ¢è®¨äº†CoNLL-2003å‘½åå®ä½“æ ‡ç­¾å™¨åœ¨2023å¹´æ˜¯å¦ä»ç„¶æœ‰æ•ˆçš„é—®é¢˜ã€‚ç ”ç©¶è€…é¦–å…ˆå›é¡¾äº†æ¨¡å‹ä½¿ç”¨CoNLL-2003æ•°æ®é›†å¼€å‘å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰æŠ€æœ¯å·²æœ‰è¿‘20å¹´çš„å†å²ï¼Œå¹¶æå‡ºäº†å‡ ä¸ªå…³é”®é—®é¢˜ï¼šè¿™äº›æ¨¡å‹èƒ½å¦é€‚åº”ç°ä»£æ•°æ®ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶æ‰èƒ½å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»¥åŠæ€§èƒ½ä¸‹é™çš„åŸå› æ˜¯ä»€ä¹ˆã€‚æ¥ç€ï¼Œç ”ç©¶è€…ä»‹ç»äº†ä»–ä»¬æ„å»ºçš„CoNLL++æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†é€šè¿‡æ”¶é›†2020å¹´çš„è·¯é€ç¤¾æ–°é—»å¹¶æŒ‰ç…§CoNLL-2003æ ‡æ³¨æŒ‡å—è¿›è¡Œæ ‡æ³¨æ¥æ‰©å±•CoNLL-2003æ•°æ®é›†ã€‚ç ”ç©¶è€…è¿˜è¯„ä¼°äº†20å¤šç§æ¨¡å‹åœ¨CoNLL-2003æµ‹è¯•é›†å’ŒCoNLL++ä¸Šçš„è¡¨ç°ï¼Œå¹¶è®¡ç®—äº†ç™¾åˆ†æ¯”Î”F1æ¥è¯„ä¼°æ³›åŒ–èƒ½åŠ›ã€‚

è¿›ä¸€æ­¥çš„ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹æ¶æ„ã€æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ ·æœ¬æ•°é‡å¯¹äºå®ç°è‰¯å¥½æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚å…·ä½“æ¥è¯´ï¼ŒTransformeræ¨¡å‹åœ¨æ³›åŒ–èƒ½åŠ›ä¸Šè¡¨ç°æ›´å¥½ï¼Œæ›´å¤§çš„æ¨¡å‹ä¹Ÿæ›´æ˜“äºæ³›åŒ–ã€‚æ­¤å¤–ï¼Œæ›´å¤šçš„è®­ç»ƒæ ·æœ¬æœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç ”ç©¶è€…æŒ‡å‡ºï¼Œæ€§èƒ½ä¸‹é™çš„ä¸»è¦åŸå› å¯èƒ½æ˜¯æ—¶é—´æ¼‚ç§»ï¼Œå³éšç€æ—¶é—´çš„æ¨ç§»ï¼Œæ¨¡å‹çš„è¡¨ç°ä¼šé€æ¸ä¸‹é™ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç ”ç©¶è€…è®¤ä¸ºCoNLL-2003æ ‡ç­¾å™¨ä»ç„¶æœ‰æ•ˆï¼Œè¿™è¡¨æ˜å³ä½¿åœ¨æ–°æ•°æ®é›†ä¸Šï¼Œè¿™äº›æ¨¡å‹ä¹Ÿèƒ½ä¿æŒè¾ƒå¥½çš„æ€§èƒ½ã€‚</sample>
    <sample id="352">ABC-Eval æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°èŠå¤©æœºå™¨äººå¯¹è¯è´¨é‡çš„æ–¹æ³•ã€‚</sample>
    <sample id="353">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†é€šè¿‡æå‡ºæ¾„æ¸…é—®é¢˜æ¥ç”ŸæˆPythonä»£ç çš„æ–¹æ³•ï¼Œä»¥è§£å†³è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç æ—¶å­˜åœ¨çš„è¾“å…¥ä¸æ˜ç¡®æ€§é—®é¢˜ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æœªèƒ½æœ‰æ•ˆåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäºäº¤äº’æ€§çš„è§£å†³æ–¹æ¡ˆï¼Œå³åœ¨ç”Ÿæˆä»£ç çš„è¿‡ç¨‹ä¸­å¼•å…¥æ¾„æ¸…é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªåä¸ºCodeClarQAçš„æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹è¯†åˆ«å’Œè¡¥å……ç¼ºå¤±çš„å…³é”®æ“ä½œã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ç”Ÿæˆä»£ç çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ¾„æ¸…éœ€æ±‚é¢„æµ‹å™¨ã€CQæ’åå™¨å’Œä»£ç ç”Ÿæˆå™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜ä»£ç ç”Ÿæˆçš„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨F1åˆ†æ•°ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</sample>
    <sample id="354">ç›´åˆ° 2016 å¹´ï¼ŒCoNLL-2003 å’Œ CoNLL++ ä¹‹é—´çš„æ€§èƒ½å¢é‡æ‰é«˜äº 5 ä¸ªç™¾åˆ†ç‚¹ã€‚</sample>
    <sample id="355">å¥½çš„ï¼Œä»¥ä¸‹æ˜¯è‹±æ–‡å†…å®¹çš„ä¸­æ–‡ç¿»è¯‘ï¼š

---

è½¬ç§»å’Œä¸»åŠ¨å­¦ä¹ ç”¨äºè®¤çŸ¥å¤±è°ƒæ£€æµ‹ï¼šè§£å†³ç¨€æœ‰ç±»æŒ‘æˆ˜

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems

---

ä»€ä¹ˆæ˜¯è®¤çŸ¥å¤±è°ƒï¼Ÿ

â€œè®¤çŸ¥ï¼ˆå³ï¼Œæ€æƒ³ã€è¡ŒåŠ¨ã€ä¿¡å¿µï¼‰ä¸­çš„ä¸¤ä¸ªå…ƒç´ ä¸ä¸€è‡´â€ï¼ˆHarmon-Jones and Harmon-Jones, 2007ï¼‰

Eddie Harmon-Jones and Cindy Harmon-Jones, 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift fÃ¼r Sozialpsychologie, 38(1):71f.

---

ä¸ºä»€ä¹ˆä¼šæœ‰è®¤çŸ¥å¤±è°ƒï¼Ÿ

- æ•ˆåº”åˆ†æ­§
- æ€åº¦å’Œä¿¡å¿µè¶‹åŠ¿
- ç„¦è™‘éšœç¢
- è®¤çŸ¥é£æ ¼
- è¿›å…¥å’Œé€€å‡ºæç«¯ä¸»ä¹‰

---

æ³¨é‡Š

ç”¨æˆ· @user_handle
Wish I could hold grudges but I guess itâ€™s a good thing that I canâ€™t at the same time.

---

åˆå§‹æ ‡æ³¨é›†ä¸Šçš„è®­ç»ƒ

---

æ–¹æ³•ï¼šå†·å¯åŠ¨æ ‡æ³¨ç¨€æœ‰ç±»

---

å†·å¯åŠ¨æ ‡æ³¨ï¼šè¿ç§»å­¦ä¹ 

---

å†·å¯åŠ¨æ ‡æ³¨ï¼šæ¦‚ç‡ç¨€æœ‰ç±»ç­–ç•¥

---

æ€»ç»“

- å†·å¯åŠ¨ALä¸è¿ç§»å­¦ä¹ 
- å‡ºåŸŸï¼šè¿­ä»£
- å…¥åŸŸï¼šç´¯ç§¯
- PRC æ˜¯ç®€å•ä¸”é«˜æ•ˆçš„ç¨€æœ‰æ ·æœ¬è·å–æ–¹æ³•

---

è”ç³»ä¿¡æ¯ï¼š
- Code: https://github.com/vasuvaradara/Dissonance-Detection
- Dataset: https://github.com/vasuvaradara/Dissonance-Detection
- Paper: https://arxiv.org/abs/2204.09604

---

è°¢è°¢ï¼</sample>
    <sample id="356">è¿™ç¯‡è®ºæ–‡çš„ä½œè€…æ‰€å±æœºæ„åŒ…æ‹¬Saarland Universityã€University of Amsterdamå’ŒUniversity of California, San Diegoã€‚</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">è¿™ç¯‡è®ºæ–‡æœ‰äº”ä½ä½œè€…ã€‚</sample>
    <sample id="359">è¯¥æ–¹æ³•ä¸ä¸“ç”¨çš„ simulST æ¶æ„ CAAT è¿›è¡Œäº†æ¯”è¾ƒã€‚</sample>
    <sample id="361">è¯¥ç ”ç©¶ä¸»è¦æ¢è®¨äº†å¦‚ä½•é€šè¿‡ä½¿ç”¨åäº‹å®å¯¹æ¯”æ¥æ”¹è¿›å¤šæ­¥å®šé‡æ¨ç†ä¸­çš„ç»„åˆæ³›åŒ–èƒ½åŠ›ï¼Œä»¥è§£å†³é‡‘èæ•°æ®ä¸­å¤æ‚çš„è®¡ç®—é—®é¢˜ã€‚ç ”ç©¶å›¢é˜ŸåŒ…æ‹¬Armineh Nourbakhshã€Sameena Shahå’ŒCarolyn RosÃ©ï¼Œä»–ä»¬äº2023å¹´7æœˆåœ¨Carnegie Mellon Universityè¿›è¡Œäº†è¿™ä¸€ç ”ç©¶ã€‚ç ”ç©¶é¦–å…ˆä»‹ç»äº†é‡‘èæ•°æ®ä¸­å¤šæ­¥å®šé‡æ¨ç†çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç»„åˆæ³›åŒ–çš„é—®é¢˜ï¼Œå¹¶å±•ç¤ºäº†ä¸åŒç¨‹åºæ­¥éª¤çš„æ‰§è¡Œå‡†ç¡®ç‡å’Œç¨‹åºå‡†ç¡®ç‡çš„æ•°æ®ã€‚æ¥ç€ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºCounterCompçš„æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨é—®é¢˜ä½œä¸ºåäº‹å®ç¤ºä¾‹æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚CounterCompæ–¹æ³•é€šè¿‡è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„æœ€å¤§æ¬§å‡ é‡Œå¾—è·ç¦»æ¥è¡¡é‡ä¸¤ä¸ªé—®é¢˜ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°ç†è§£å¹¶è§£å†³å¤æ‚çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCounterCompæ–¹æ³•åœ¨å¤„ç†é‡‘èæ•°æ®æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ­¥ç¨‹åºçš„å‡†ç¡®æ€§ä¸Šï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶è¿˜è®¨è®ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼ŒåŒ…æ‹¬å¦‚ä½•è¿›ä¸€æ­¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œå¤„ç†æ›´å¤æ‚çš„é‡‘èæ•°æ®é—®é¢˜ã€‚</sample>
  </task>
</testset>