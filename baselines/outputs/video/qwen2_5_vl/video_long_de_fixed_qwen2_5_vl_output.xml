<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind News Media und Social Media (Reddit).</sample>
    <sample id="1">McGill University</sample>
    <sample id="2">The paper introduces LayoutMask, a novel multi-modal pre-training model designed to enhance text-layout interaction for document understanding, particularly addressing reading order issues in visually-rich documents. It employs local ID positions instead of global ones and incorporates novel masking strategies and pre-training objectives to improve performance. The methodology includes masked language modeling and masked position modeling, with transformer layers using spatial-aware self-attention mechanisms. Experimental results on datasets like FUNSD, CORD, and SBDHF demonstrate superior F1 scores compared to existing methods, highlighting the effectiveness of LayoutMask in document parsing tasks.</sample>
    <sample id="3">Natürlich, hier ist der Übersetzte Inhalt:

---

DEPLAIN: Ein deutsches Parallelkorpus mit intralingualen Übersetzungen in einfache Sprache für Satz- und Dokumenteinfachstellung

Regina Stodden, Omar Momen, Laura Kallmeyer

Heinrich Heine University Düsseldorf, Germany

ACL 2023

1. Text Einfachstellung
Was, warum und wie?

Text Einfachstellung Beispiel

Original:
"Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."

Einfachsprache:
"Die Gewerkschaft will zum Beispiel höhere Löhne."

---

Bitte beachten Sie, dass die Übersetzung des Inhalts auf dem Bildschirm nicht vollständig ist und eventuell zusätzliche Informationen fehlen könnten.</sample>
    <sample id="4">Der/die Referent*in heißt Kayo Yin.</sample>
    <sample id="5">Das T5 XL-Modell wurde verwendet.</sample>
    <sample id="6">This paper introduces M2MS, a unified model for multi-lingual and cross-lingual summarization, addressing the limitations of existing models by unifying MLS and CLS into a single framework. M2MS is pre-trained using three stages: meta, cross-lingual, and task-specific. The model demonstrates superior performance across various languages compared to previous approaches, particularly in handling zero-shot directions. Experimental results on the WikiLingua dataset show that M2MS outperforms other models, especially in transferring knowledge across languages.</sample>
    <sample id="7">Ja, sie funktionieren immer noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode ist neu, weil sie eine Kombination aus Likert-Skalierung und annotierten Verhaltensmerkmalen in Chat-Dialogen verwendet, um die Qualität von Chatbot-Antworten zu bewerten.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Qualität der verwendeten Validierungsmuster ab.</sample>
    <sample id="10">Das Ergebnis kann durch die Verwendung von mehr Informationen verbessert werden, insbesondere wenn der Text nicht direkt auf den Namen des Objekts verweist.</sample>
    <sample id="11">This research explores the capabilities of large language models to understand and generate humor, specifically focusing on the New Yorker Caption Contest. It examines how these models can explain jokes and whether they truly comprehend humor. The study uses benchmarks from the contest, including matching, quality ranking, and explanation generation tasks, to evaluate model performance. Results indicate that while models can generate and explain jokes, their understanding of humor is limited, as evidenced by their inability to consistently match or rank captions accurately. The dataset, leaderboard, and models are available for further exploration at https://capcon.dev.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="13">The presentation explores the concept of "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings." It discusses how real-world data complexity varies, leading to the use of low-capacity models for simpler tasks to reduce inference costs. The study compares multi-model and early exit methods, highlighting that multi-models are versatile but expensive due to overhead and storage issues. Early exits offer faster inference without overhead, making them more memory-efficient. The research introduces the SWEET method, which separates weights in early exit transformers to mitigate conflicting gradients, thereby improving performance while maintaining high speedups. Results show that SWEET closes most of the gap between EE and MM classifiers, especially for early layers, and suggests future research on fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">Natürlich, hier ist die Übersetzung des englischen Inhalts in den Abschnitten:

1. **Titel:**
   - "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination"
   - "Adam Przepiórkowski and Michał Woźniak"
   - "Institute of Computer Science, Polish Academy of Sciences, ul. Jana Kazimierza 5, 01-248 Warsaw"
   - "ACL 2023"

2. **Abbildung 1:**
   - "Dependency Structure of Coordination"
   - "Bouquet/Stanford (Universal Dependencies):"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Chain/Moscow:"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Conjunction-headed/Prague:"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Multi-headed/London:"
     - "Homer loves Lisa, Bart, and Maggie."

3. **Abbildung 2:**
   - "Dependency Length Minimization (DLM)"
   - "Word order tends to minimize dependency lengths:"
     - "Marge read it yesterday."
     - "Marge read this absolutely fascinating book about bees yesterday."
     - "Marge read this absolutely fascinating book about bees yesterday."
     - "Marge read this absolutely fascinating book about bees yesterday."

4. **Text:**
   - "Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016):"
     - "left conjuncts tend to be shorter (observed before),"
     - "this tendency grows with length difference (briefly noticed in Gibson et al. 1996 88–90),"
     - "but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed),"
     - "not when it is on the right (Ted and Ned laughed)."

5. **Abbildung 3:**
   - "Figure 1: Proportions of shorter left conjuncts depending on the absolute difference of conjunct lengths (with confidence bands)"

6. **Abbildung 4:**
   - "Compatibility with Dependency Structures of Coordination"
   - "Bouquet/Stanford (Universal Dependencies):"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Chain/Moscow:"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Conjunction-headed/Prague:"
     - "Homer loves Lisa, Bart, and Maggie."
   - "Multi-headed/London:"
     - "Homer loves Lisa, Bart, and Maggie."

7. **Text:**
   - "See the paper for the full argument!"
   - "Talk to us at the poster session!"

Diese Übersetzung deckt den gesamten englischen Text ab und bietet eine direkte Übersetzung der wichtigsten Punkte und Informationen.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="16">News, Bible, L2 und Fiction.</sample>
    <sample id="17">This research introduces a novel approach to multimodal relation extraction, focusing on simultaneous information subtraction and addition for enhancing relation inference. It employs a graph information bottleneck principle for internal-information screening and latent multimodal topic modeling to enrich feature contexts. The system achieves significant improvements over existing best models on benchmark data, demonstrating the effectiveness of this method in handling mixed-modal inputs and improving task performance.</sample>
    <sample id="18">Das Beispiel lautet "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="19">This presentation provides an overview of efficient open-domain question answering (ODQA) systems, focusing on the two-stage framework proposed by Chen et al. in 2017. It discusses the challenges and motivations behind ODQA, including the need for faster inference, comparable performance, and resource efficiency. The content covers existing ODQA system frameworks, summarizing techniques to enhance efficiency such as reducing index size, using lightweight models, and parameter sharing. Comparative analyses highlight the strengths and trade-offs among different system architectures, emphasizing the importance of balancing speed, memory, and performance. Future work includes exploring deployment on low-power devices and considering additional evaluation metrics like carbon emissions.</sample>
    <sample id="20">Ja, Sie können die Modelle für Ihre Forschung verwenden. Die DrBERT-Modelle sind unter der MIT-Lizenz verfügbar und können frei genutzt werden.</sample>
    <sample id="21">DEplain-APA enthält Dokumente, die in der APA-Stilrichtlinie formatiert sind.</sample>
    <sample id="22">Für eine gute Generalisierung werden bessere Modellarchitekturen, größere Modellgrößen und mehr Feinabstimmungsbeispiele benötigt.</sample>
    <sample id="23">This research explores the impact of character-aware models on visual text rendering, focusing on text-to-image modeling. It highlights that subword-based encoders struggle with spelling accuracy, especially at smaller scales, while character-aware encoders perform better across all scales. The study introduces a method to concatenate subword and character-level text encodings to improve image generation metrics like fidelity, alignment, and text quality. This approach is demonstrated using a text-to-image model, showcasing its effectiveness in generating images from text inputs.</sample>
    <sample id="24">Die Tendenz wurde durch die Längendifferenz gemessen, wobei kürzere Konjunktionen tendenziell links vorkamen.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass sie die Position des Begrenzers in verschiedenen Sätzen untersuchten, um die Auswirkungen auf die Länge der Konjunktionen zu ermitteln.</sample>
    <sample id="26">Ein Basisklassifikator kann schlecht performieren, wenn er mit unausgewogenen Daten trainiert wird.</sample>
    <sample id="27">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen "Easy on Me" (von Adele) und "I Gotta Feeling" (von The Black Eyed Peas).</sample>
    <sample id="29">Formalität und lexikalische Cohesion.</sample>
    <sample id="30">LLM-Blender is an ensemble learning framework for large language models (LLMs) that combines pairwise ranking and generative fusion techniques to improve overall performance. It runs multiple LLMs in parallel, compares candidate pairs, ranks them, and fuses the top three candidates to generate the final output. The framework includes two sub-modules: PairRanker for ranking and GenFuser for fusion. Experiments on the MixInstruct dataset show that LLM-Blender significantly outperforms individual LLMs across various metrics like BLEURT, BARTScore, and GPT-Rank. The dataset contains 110k instruction-following examples, enabling comprehensive evaluation of ensemble learning for LLMs.</sample>
    <sample id="31">Die Autoren gehören an Johns Hopkins University, Purdue University und MIT.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität durch die Anwendung von Pearson's R Scores zur Vergleichung der Annotierungen nach dem Demografie- und Modellvergleich.</sample>
    <sample id="34">CREST (ContRastive Edits with Sparse rationalization) ist ein Framework, das die Lücke zwischen selektiver Rationalisierung und counterfactualer Generierung schließt. Es produziert gültige, flüssige und vielfältige counterfactuals, kontrolliert die Menge der Perturbation und führt zu plausiblen Erklärungen. Zudem erreicht es hohe counterfactual Simulability. CREST wurde auf IMDB und SNLI getestet und zeigte signifikante Verbesserungen im Vergleich zu anderen Methoden.</sample>
    <sample id="36">This research introduces Language-Specific Layers (LSLs) for multilingual machine translation, addressing scalability and resource limitations by dynamically adjusting capacity per language based on the source or target context. The model learns to place LSLs strategically, enhancing performance without increasing inference costs. Experiments on the WMT21 news translation task across 10 languages demonstrate statistically significant improvements in 84 out of 90 translation directions, outperforming both baseline and AdaNet approaches while using fewer parameters per layer.</sample>
    <sample id="37">Die vorherige Studie mit menschlichen Teilnehmenden hat ähnliche Ergebnisse ergeben wie die aktuelle Studie.</sample>
    <sample id="38">Die Datenquellen in dieser Studie sind die Penn Treebank und eine erweiterte Version der Penn Treebank.</sample>
    <sample id="39">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="40">Diskrepanz und Konsistenz.</sample>
    <sample id="41">The presentation introduces PeaCoK, a world-level persona commonsense knowledge graph, designed to enhance narrative consistency and engagement in dialogue systems. It outlines the creation of PeaCoK through a three-step process involving persona selection, attribute induction, and relation classification. The graph contains over 100K high-quality commonsense inferences about personas, enabling reliable training of persona inference generators. Evaluation results demonstrate that PeaCoK improves consistency and engagement in conversations, outperforming baseline systems like P^2-Bot and Atomic2020. The study concludes that persona-centric commonsense knowledge yields more positive impacts than general social commonsense knowledge, making PeaCoK a valuable tool for consistent and engaging narrative modeling.</sample>
    <sample id="42">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="43">Es gibt sieben Autoren an der Arbeit beteiligt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich durch seine Fokussierung auf die Charakterisierung von Designbiasen in NLP-Datasets und -Modellen, indem es eine Reihe von spezifischen Schritten wie die Re-Annotation mit diversen Annotatoren und die Vergleichung der Annotierungen durch Modelle und Datasets mithilfe von Pearson's R-Werten verwendet.</sample>
    <sample id="45">Das Setup "GPT-4 PBlack" hat die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">#ACL2023

Von Prätraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben: Spuren politischer Biass verfolgen, die zu unfairen NLP-Modellen führen

Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov

Paul Allen School, University of Washington (UW), Language Technologies Institute, Carnegie Mellon University

### LM Training Data
Ein gemischtes Segen

Die Präsentation zeigt eine Balkendiagramm mit verschiedenen Webseiten und deren Häufigkeiten in der Trainingsdatenbasis.

### Ziel
Ziel dieser Arbeit ist es, die politische Neigung von Sprachmodellen zu evaluieren und zu verstehen, wie die politischen Biass im Trainingsdatenmaterial auf die Leistung der Modelle in downstream Aufgaben übertragen werden.

### Evaluierung der politischen Neigung von Sprachmodellen
- **Unterstützung sowohl für Encoder- als auch Decoder-Sprachmodelle**
  - "statement" I &lt;mask&gt; with this statement.
  - "Do you agree or disagree with this statement? &lt;statement&gt;"
- **Automatische Bewertung**
- **Begründet in politikwissenschaftlicher Literatur**

### Bestehende Sprachmodelle
Ein Diagramm, das verschiedene Sprachmodelle anzeigt und ihre politische Neigung darstellt.

### Prätraining-Daten
Weitere Prätraining-Sprachmodelle (RoBERTa, GPT-2) werden vorgestellt, um die Veränderung der politischen Neigung zu evaluieren.

### Ergebnisse
- **Parteiliche Verschiebungen in der politischen Neigung der Sprachmodelle**
  - Grafiken, die die politische Neigung von RoBERTa und GPT-2 zeigen.

### Diskussion
- **Zwischen Scylla und Charybdis**
  - Die Frage, ob man die politischen Biass in den Sprachmodellen entfernen sollte oder nicht, ist die zentrale Fragestellung.

### Dank
Vielen Dank!</sample>
    <sample id="48">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="49">Bis zu 900 Tokens.</sample>
    <sample id="50">DEPLAIN is a German parallel corpus with intralingual translations into plain language, designed for sentence and document simplification. It includes 13,512 sentences from various domains like news, Bible, L2, and fiction, totaling 483,697 words. The corpus features three types of simplification: simplicity, lexical simplification, and structural simplification. Evaluation results show that DEPLAIN-APA achieves high BLEU scores (up to 86.4) on document-level simplification tasks, while DEPLAIN-WEB performs well on sentence-level simplification, achieving F1 scores of up to 82.8. This resource is valuable for researchers and developers working on text simplification systems.</sample>
    <sample id="51">Die Domains, die im Datensatz enthalten sind, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität kann als die Perspektive definiert werden, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="53">Der Referent ist Xiaoyu Shen.</sample>
    <sample id="54">This research addresses the challenge of detecting cognitive dissonance, a rare class in text data, by leveraging transfer learning and active learning techniques. It introduces a method that combines a pre-trained model with human annotation to improve dissonance detection accuracy. The study demonstrates that while minimum annotation cost does not guarantee better models, increasing dissonance samples significantly enhances performance. Transfer learning is shown to be particularly effective for cold-start scenarios, and the Probability-of-Rare-Class (PRC) strategy outperforms other active learning methods in this context.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell ohne Neutrainieren oder Anpassung der Architektur für Simultaneous Speech Translation (SimuST).</sample>
    <sample id="56">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="57">Nein, das getestete Modell scheint nicht in der Lage zu sein, über mehrere Quellen integriertes Wissen zu verarbeiten.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:
1. Background-Pretrain
2. Background-Both
3. Background-Inference</sample>
    <sample id="59">DrBERT is a robust pre-trained model in French for biomedical and clinical domains, surpassing generic models and English-based domain-specific models. It achieves state-of-the-art results on nine downstream medical tasks, outperforming NACHOS and other models. Continual pre-training with heterogeneous data from various medical domains improves performance compared to scratch training. The model's stability suggests higher inter-run variability when trained using a medical-specific model in French.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">How to use the available clean samples more efficiently?</sample>
    <sample id="62">This study investigates knowledge distillation for natural language generation (NLG) systems, focusing on the use of pseudo-targets (PTs) generated by a medium-sized teacher model to compress large pre-trained language models (LLMs). The research addresses the computational and storage challenges associated with NLG systems based on LLMs, aiming to preserve performance while reducing model size. It explores both word-level and sequence-level knowledge distillation techniques, comparing their effectiveness across various NLG tasks such as summarization, question generation, common sense reasoning, and simplification. The study employs a systematic approach, varying parameters like the number of PTs, sampling methods, and teacher-student architectures, to identify optimal configurations for realistic NLG setups. The findings highlight that sequence-level KD using multiple PTs via sampling can achieve high compression ratios without significantly compromising performance, making it a promising strategy for practical NLG applications.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie sensibel ein Modell ist für die Vielfalt von Anweisungen für dieselbe Aufgabe, indem es die Fähigkeit des Modells überprüft, konstante Ergebnisse für dieselbe Aufgabe zu erzeugen, unabhängig von leichten Variationen im Wortlaut der Anweisungen.</sample>
    <sample id="64">Wenjun Peng</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell weniger robust ist und möglicherweise schlechtere Leistungen zeigt.</sample>
    <sample id="66">This presentation explores the application of deep learning in mathematical reasoning, focusing on solving math word problems and geometry problems. It highlights advancements in automated theorem proving and the use of large language models (LLMs) for complex reasoning tasks. The study examines how LLMs can be enhanced with tools to perform sophisticated mathematical operations, addressing limitations like inconsistency with large numbers. Additionally, it discusses the role of chain-of-thought prompting in improving model performance and the potential of program-aided LLMs for plug-and-play compositional reasoning.</sample>
    <sample id="67">This research explores the challenges of interference and synergy in multilingual machine translation models, focusing on how model size, data size, and the data size of other languages influence performance. It identifies that severe interference occurs when models are small relative to the data size, and tuning the sampling temperature is crucial for achieving strong performance. The study also examines language similarity's role, finding it not a dominant factor. Through experiments with various model sizes and languages, the research concludes that modest scale and tuned temperature can significantly reduce interference, suggesting that sophisticated methods may not always be necessary.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen kontextuellen Kontext, der aus verschiedenen Quellen wie Wikipedia und SLAMP zusammengesetzt ist.</sample>
    <sample id="69">Nur einige saubere Validierungsbeispiele (weniger als 10 pro Klasse) sind notwendig, um eine gute Leistung zu erzielen.</sample>
    <sample id="70">Stanford University</sample>
    <sample id="71">The research focuses on resolving indirect referring expressions for entity selection, particularly in conversational systems. It introduces the AltEntities Corpus, a dataset of 6,000 alternative questions and 42,000 indirect referring expressions across music, books, and recipes domains. The study aims to understand users' language choices and improve entity understanding by collecting data through crowd annotation. The methodology emphasizes informality using a cartoon completion task, ensuring the dataset reflects natural conversation. The research demonstrates that models can achieve high accuracy (92-95%) when given the same background knowledge as annotators, highlighting the importance of domain-specific knowledge in improving performance.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um sicherzustellen, dass die Inhalte in den Medien korrekt dargestellt werden und keine Verzerrungen oder Fehlinterpretationen enthalten.</sample>
    <sample id="73">Martin Pömsi</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph designed to enhance the coverage of multi-hop paths and improve commonsense reasoning. It addresses the limitations of ATOMIC by densifying its structure through a novel completion method that leverages a relation prediction model trained on a dense graph. The method effectively infers missing links within and between clusters, significantly increasing knowledge coverage and multi-hop paths. Evaluation results demonstrate superior performance compared to existing methods, highlighting Dense-ATOMIC's potential for commonsense reasoning tasks.</sample>
    <sample id="75">This research introduces Jointprop, a joint semi-supervised learning framework for entity and relation extraction using heterogeneous graph-based propagation. It addresses the limitations of fully supervised models by requiring extensive labor and diverse annotated data. Jointprop leverages labeled and unlabeled data to enhance model performance through label propagation across graphs. The framework considers both inter- and intra-task interactions, improving accuracy on SciERC and ACE05 datasets with varying amounts of labeled data.</sample>
    <sample id="76">Die Pipeline beginnt mit dem Pretraining auf einer Mischung von Daten, gefolgt von der Erstellung von Sprachmodellen, die dann in Aufgaben zur Sprachverarbeitung eingesetzt werden.</sample>
    <sample id="77">This research introduces DeFacto, a dataset for improving factual consistency in text summarization. It includes human demonstrations and feedback on summaries, enabling comprehensive analyses and insights into factual errors. The study also explores NLG tasks like summary editing, feedback generation, and factual error correction with feedback prediction. A new dataset, DeFacto, is created to address the challenge of factual consistency in abstractive text summarization, where all information in the summary must be supported by the input document. This dataset is valuable for training and evaluating models that ensure factual accuracy in summaries.</sample>
    <sample id="78">Nein, der Prozess unterscheidet sich nicht.</sample>
    <sample id="79">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird durch das Zählen der Wortfrequenz auf einem allgemeinen Textkorpus und das Zufällige Auswählen von n Wörtern aus einer moderaten Frequenzintervall eingebettet.</sample>
    <sample id="81">Penn State</sample>
    <sample id="82">This research introduces ULRA, a novel framework for unsupervised automated essay scoring (AES) that aggregates multiple heuristic quality signals as pseudo-groundtruth to train a neural AES model. By leveraging the partial-order knowledge contained in these signals, ULRA addresses conflicts among different signals and designs a deep pairwise rank aggregation loss for model training. The method is evaluated on two datasets, demonstrating its effectiveness in unsupervised essay scoring.</sample>
    <sample id="83">Ja, durch Training mit einer Mischung von Sprachen können Encoder-Decoder-Modelle wie mT5 verbessert werden.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks

This paper introduces PAD-Net, an efficient framework designed to handle dynamic networks by partitioning parameters into static and dynamic modes. The dynamic mode allows for the selective activation of parameters based on input conditions, while the static mode retains fixed parameters. This approach is shown to be effective across various tasks, including natural language processing (NLP) and computer vision (CV), where it achieves higher performance with fewer parameters and less computation compared to traditional methods. The framework's dynamic convolution mechanism, which scales parameters based on gradients, further enhances its efficiency. Future work includes extending this concept to hardware-friendly structures and exploring more parameter modes.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist "How to Make a Chocolate Cake?"</sample>
    <sample id="86">Die Opazität der Methode wird durch die Verwendung eines verdeckten Wasserzeichens sichergestellt, das den Angriffern nicht auffällt.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie kontinuierliche Vortrainingstrategien verwendet.</sample>
    <sample id="88">West South Asia</sample>
    <sample id="89">Der Beispielsatz "I am going to talk about... Ich werde reden." zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="90">The study explores the feasibility of using language learners as annotators for natural language processing (NLP) tasks, addressing the challenge of recruiting native speakers. It investigates whether language learners can annotate data with nearly accurate labels and if their proficiency improves through NLP annotations. The research involves three languages: English, Korean, and Indonesian, with tasks including sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. Participants were recruited based on their language proficiency levels and provided additional resources like dictionaries and machine translation systems to aid in annotation. The study design includes pre- and post-surveys, standardized test questions, and word meaning questions to assess proficiency. Results show that language learners' annotations achieve nearly 95% accuracy compared to native speakers, and their proficiency in vocabulary and grammar tends to improve. This suggests the potential for broadening NLP research to include more languages by leveraging the vast pool of language learners.</sample>
    <sample id="91">Die Anzahl der Aufgaben beeinflusst die aggregierte Leistung des Modells, wobei mehr Aufgaben zu höherer aggregierter Leistung führen.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit den folgenden drei baumlosen Baselines:

1. seq2seq
2. PP recursion
3. Obj PP = Subj PP</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, stehen dem ersten Autor Matthias Lindemann als Mitarbeiter oder Kollegen in einem wissenschaftlichen Zusammenhang.</sample>
    <sample id="94">This research addresses the challenge of protecting the copyright of large language models (LLMs) offered as a service (EaaS). It introduces EmbMarker, a backdoor watermarking technique that ensures utility preservation and covertness, making it transferable to attackers' services. The method involves selecting trigger words based on their frequency in a general text corpus and injecting them into the model's embeddings. Copyright verification is achieved by comparing extracted embeddings with a benign dataset. Experimental results demonstrate high detection performance across various datasets, highlighting EmbMarker's effectiveness in preventing unauthorized use of LLMs.</sample>
    <sample id="95">Chowdery et al., 2022</sample>
    <sample id="96">Natürlich, hier ist die Übersetzung des englischen Inhalts in den deutschen:

---

**Titel: NLPositionality: Characterizing Design Biases of Datasets and Models**

**Abstract:**
In diesem Vortrag werden wir uns mit der Positionalität in NLP-Daten und Modellen befassen. Wir werden sehen, wie wir die Entwurfsschulden von Daten und Modellen charakterisieren können und wie wir daraus lernen, um die Positionalität zu reduzieren.

**1. Einführung**
- Definition von Positionalität
- Warum ist Positionalität wichtig?
- Wie können wir Positionalität in NLP-Daten und Modellen erkennen?

**2. Theoretische Grundlagen**
- Was ist Positionalität?
- Wie beeinflussen Positionalität die Forschung und ihre Ergebnisse?

**3. Anwendungen**
- Wie können wir Positionalität in NLP-Daten und Modellen messen?
- Beispiele für Positionalität in NLP-Daten und Modellen

**4. Methoden**
- Wie können wir Positionalität in NLP-Daten und Modellen reduzieren?
- Praktische Anwendungen und Tools

**5. Diskussion**
- Wie können wir Positionalität in Zukunft vermeiden?
- Fazit und Ausblick

**6. Fragen und Antworten**

---

Bitte beachten Sie, dass dies eine grobe Übersetzung ist und eventuell einige feine Nuancen oder spezifische Begriffe nicht vollständig übersetzt sind.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen zu reduzieren, kann man verschiedene Ansätze anwenden:

1. **Diversifizierung des Trainingsmaterials**: Man sollte ein breites Spektrum an Quellen und Perspektiven verwenden, um eine Vielfalt von Meinungen und Sichtweisen zu reflektieren.

2. **Kontrolle der Datenqualität**: Man kann die Qualität der Daten überprüfen und eventuelle Biassignale entfernen oder minimieren.

3. **Bias-Entdeckung und -korrektur**: Es gibt Methoden zur automatischen Identifikation von Bias in den Daten und zur Korrektur dieser Bias durch Anpassung der Modelle oder der Daten selbst.

4. **Ethische Überwachung**: Eine ethische Überwachung durch Experten oder eine interne Ethikkommission kann helfen, sicherzustellen, dass die Daten und Modelle ethisch und fair sind.

5. **Transparenz und Transparenz**: Die Ergebnisse und Entscheidungen der Modelle sollten transparent sein, damit potenzielle Bias-Quellen identifiziert und korrigiert werden können.

6. **Regulierung und Normierung**: Regulierungsbehörden und Normen können dazu beitragen, dass die Daten und Modelle ethisch und fairen Standards entsprechen.

7. **Konsultation mit Experten**: Konsultationen mit Experten aus verschiedenen Bereichen können helfen, potenzielle Bias-Quellen zu erkennen und zu adressieren.

8. **Educatioan und Sensibilisierung**: Sensibilisierung und Bildung über Bias und seine Auswirkungen können dazu beitragen, dass die Menschen in der Datenproduktion und Modellentwicklung besser auf Bias achten.

9. **Verwendung von diversen Modellen**: Das Verwenden von mehreren Modellen mit unterschiedlichen Bias-Profile kann dazu beitragen, dass der Bias im Durchschnitt ausgeglichen wird.

10. **Feedback und Iteration**: Feedback von Nutzern und Benutzern kann dazu beitragen, dass die Modelle kontinuierlich verbessert und angepasst werden.

Diese Ansätze können zusammenarbeiten, um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren.</sample>
    <sample id="99">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel:**
Distilling Script Knowledge from Large Language Models for Constrained Language Planning

**Abstract:**
In this paper, we establish the constrained language planning problem and develop an over-generate-then-filter method to improve the planning quality of large language models (LLMs). We use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning. The proposed method is a post-hoc re-ranking approach that can be applied to any LLM. We show that CoScript can generate higher quality scripts than LLMs by a large margin. The CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.

**Zusammenfassung und Schlüsselpunkte:**

- **Problemstellung:** Wir stellen das Problem der beschränkten Sprachplanung vor.
- **Methodik:** Wir entwickeln eine über-generieren-dann-filtern-Methode, um die Planungsqualität großer Sprachmodell-Langmodelle (LLMs) zu verbessern.
- **Datenbasis:** Wir verwenden LLMs, um ein hochwertiges Skript-Dataset (CoScript) für beschränkte Sprachplanung zu generieren.
- **Ergebnisse:** Die vorgeschlagene Methode ist ein post-hoc Re-Ranking-Ansatz, der auf beliebige LLMs angewendet werden kann. Wir zeigen, dass CoScript höhere Qualitätsskripte als LLMs generieren kann, was einen großen Vorsprung darstellt.
- **Fazit:** Das CoScript-Dataset kann als wertvolles Ressourcenmaterial dienen, um die Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Restriktionen voranzutreiben.

---

**Motivation:**
- Um die beschränkte Sprachplanungsfähigkeit kleinerer Modelle zu ermöglichen.
- Folgen wir dem Prinzip der symbolischen Wissensdistillation.
- Generieren wir 55.000 Skripte mit Restriktionen von LLMs mithilfe unseres Ansatzes =&gt; CoScript-Dataset.
- Menschen annotieren und validieren das Dataset.

**Methodik:**
- **Schritt 1:** Generieren spezifischer Ziele mit InstructGPT via In-context Learning.
- **Schritt 2:** Über-generieren kandidierender Skripte mit InstructGPT via In-context Learning.
- **Schritt 3:** Finden gefilterter Skripte zum Ziel mit InstructGPT via Ähnlichkeitsbewertung.
- **Ausgabe:** Spezifische Ziele mit entsprechenden Skripten.

**Limitationen und zukünftige Arbeiten:**
- Die vorgeschlagene Methode zur Verbesserung von LLMs ist ein post-hoc Re-Ranking-Ansatz.
- CoScript erbt nur eine zusätzliche Restriktion von einem abstrakten Ziel.
- Das CoScript-Dataset kann als wertvolles Ressourcenmaterial dienen, um die Forschung zur Sprachplanung mit komplexeren und vielfältigeren Zielen und Restriktionen voranzutreiben.

---</sample>
    <sample id="100">The research explores the use of language models for few-shot reranking in multi-hop question answering (QA). It introduces PromptRank, an approach that combines unsupervised retrieval with few-shot LM-based reranking, achieving strong performance on HotpotQA with just 128 examples. The study demonstrates that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art methods like MDR. The effectiveness of the approach is attributed to the use of a scoring function based on the likelihood of the question given the chain, and the role of instructions in eliciting reasoning abilities from the language model.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit der von SOTA-Systemen, was bedeutet, dass es eine vergleichbare Fließkraft aufweist.</sample>
    <sample id="102">Utility, Covertness und Transferability.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 Sprachen übersetzt, darunter Englisch, Spanisch, Französisch, Italienisch, Niederländisch, Portugiesisch, Rumänisch, Russisch, Türkisch, Chinesisch, Arabisch, Hebräisch, Deutsch und Japanisch.</sample>
    <sample id="104">100 Instanzen werden aus einem Datensatz für die erneute Annotierung extrahiert.</sample>
    <sample id="105">Die Distanzmetriken sind der Similarity Difference (SD) und der p-Wert des KS-Tests.</sample>
    <sample id="106">The presentation introduces the QUEST dataset, designed to study entity-seeking queries with implicit set operations. It highlights the challenges of handling selective information needs through examples involving Jane, a zoologist, and Austin, a bibliophile. The dataset includes 3357 queries, each verified for relevance and marked with attributable spans. QUEST addresses the retrieval problem by requiring systems to find multi-answer sets from a large document corpus, where evidence can come from different parts of the document. The study aims to improve performance in handling such complex queries, emphasizing the need for dense encoders in retrieval and reranking tasks.</sample>
    <sample id="107">Sie wurden in der Aufgabe als Multilingual Model verwendet, um für alle Sprachen zu trainieren.</sample>
    <sample id="108">This research explores the robustness of language model acceptability judgments to context, using the minimal pair paradigm (MPP). It reveals that MPP evaluations with short, single-sentence inputs do not fully capture large language models' abstract knowledge. The study investigates how matched prefixes affect LM judgments by perturbing context sentences and comparing model sensitivity across different sentence structures. Key findings highlight that language models are sensitive to latent syntactic/semantic features shared across sentences but fail to fully capture abstract knowledge from short inputs.</sample>
    <sample id="109">This research introduces "Unnatural Instructions," a dataset of 240,670 natural language instructions for diverse tasks, generated automatically from a seed set of 15 examples. The dataset is designed to enable pretrained language models to generalize to unseen tasks without human labor, focusing on creativity, diversity, and correctness. Analysis reveals that over 50% of the examples are correct, with even incorrect examples containing valuable information. Fine-tuning an 11B-parameter T5 model on this dataset outperforms existing methods across benchmarks, demonstrating the potential of language models to produce creative and diverse data efficiently.</sample>
    <sample id="111">Die Autoren wählen zufällig n Wörter aus einem Bereich mit mittlerer Frequenz.</sample>
    <sample id="112">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel:**
- **Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?**

**Autor:**
- Shuheng Liu, Alan Ritter
- School of Interactive Computing, Georgia Institute of Technology

---

**Inhalt:**

**1. Einführung**
- Fragestellung: Funktionieren noch die CoNLL-2003 Named Entity Tagger in 2023?
- Ziel: Untersuchen, ob die Modelle, die auf CoNLL-2003 basieren, weiterhin gut generalisieren können.

**2. Named Entity Recognition &amp; Generalization**
- CoNLL-2003 wurde fast 20 Jahre lang zur Entwicklung von NER verwendet.
- Frage: Können diese Modelle modernes Datenmaterial generalisieren?
- Was ist notwendig für eine gute Generalisierung?

**3. CoNLL++ Dataset**
- Reuter News von 2020 wurden mit CoNLL-2003 Annotierungsguidelines annotiert.
- 20+ Modelle wurden auf CoNLL-2003 abgestimmt und auf CoNLL-2003 Testset &amp; CoNLL++ evaluiert.
- Der prozentuale ΔF1 wurde berechnet, um die Generalisierung zu messen.

**4. Was ist notwendig für eine gute Generalisierung?**
- Modellarchitektur: Transformer-Modelle generalisieren besser.
- Modellgröße: Größere Modelle generalisieren besser.
- Anzahl der Feinabstimmungsbeispiele: Mehr Beispiele führen zu besserer Generalisierung.

**5. Was verursacht die Leistungseinbuße?**
- Adaptive Overfitting?
- Temporale Drift?

**6. Ergebnisse**
- Adaptive Overfitting: Keine Abnahme der Rückgewinnung beobachtet.
- Temporale Drift: Leistung sinkt mit größerem zeitlichen Abstand.
- Haupt Ursache für die Leistungseinbuße: Temporale Drift.

**7. Schlussfolgerungen**
- Für eine gute Generalisierung benötigen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr Feinabstimmungsbeispiele
- Leistungseinbußen werden durch temporale Drift verursacht.
- Funktionieren CoNLL-2003 Tagger immer noch?
  - JA!

**8. Kontaktinformationen**
- Paper: https://arxiv.org/abs/2212.09747
- Dataset: https://github.com/ShuhengL/acl2023_conllpp
- Kontakt: silu775@gatech.edu

---</sample>
    <sample id="114">The presentation discusses the challenges and advancements in large language models (LLMs), highlighting their transformative impact on natural language processing (NLP) tasks such as machine translation, sentiment analysis, and information extraction. It addresses limitations like heavy parameters, long training times, and the need for vast corpora, emphasizing the importance of efficient model compression techniques. The speaker introduces "Grouped Head Attention," a novel method that divides attention heads into groups to reduce redundancy while maintaining performance. This approach is demonstrated through experiments across machine translation, language modeling, and abstractive summarization tasks, achieving significant parameter compression with minimal loss in BLEU scores. Future work includes task-specific automatic pruning, aligning with the lottery ticket hypothesis, suggesting that all-in-one LLMs are redundant in real-world scenarios, requiring only a few specialized tasks.</sample>
    <sample id="115">Der Ansatz verwendet eine Sprachsegmentgröße von 1 Sekunde.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen "Servin ist ein Richter" benötigt.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">The paper introduces novel pretraining techniques for code-switched natural language processing (NLP) by proposing a new masked language modeling objective to incorporate code-switching information and architectural changes to enhance switch-point information content. It also presents FrequencyMLM as a surrogate method when high-quality language identification (LID) tags are unavailable. The study demonstrates that these methods improve performance on code-switched tasks, particularly in the final layer representations, and suggests that further architectural modifications and auxiliary loss criteria can further enhance the effectiveness of code-switched pretraining.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf RoBERTa und GPT-2.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind "easy on me" und "the first one".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">This research introduces MultiInstruct, a large-scale multimodal instruction tuning dataset containing 62 diverse tasks from 10 broad categories, significantly improving the zero-shot capability of OFA via instruction tuning. The study explores various transfer learning techniques and introduces a new metric called sensitivity to evaluate model performance on unseen tasks. It also demonstrates that instruction tuning can reduce model sensitivity and that transfer learning from Natural Instructions can further enhance this reduction. The dataset is available for researchers to use and expand upon, aiming to advance the field of multimodal instruction tuning.</sample>
    <sample id="124">This research explores the temporal reasoning capabilities of large language models (LLMs) across three levels: Time-Time Relation, Time-Event Relation, and Event-Event Relation. It identifies biases in LLMs' performance over different time periods, particularly favoring contemporary years. The study introduces TempReason, a dataset that covers all three temporal reasoning types and long time spans, and proposes a training framework to enhance LLMs' temporal reasoning abilities. Experiments demonstrate that TempT5 outperforms other models, especially in predicting events beyond the contemporary period.</sample>
    <sample id="125">Es gibt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als Baseline betrachtet.</sample>
    <sample id="127">This research explores the potential of large language models as reasoning teachers, focusing on the application of chain-of-thought (CoT) reasoning to enable complex problem-solving capabilities in smaller models. The study introduces a method called "Fine-tune-CoT," which leverages a large teacher model to generate diverse reasoning samples for training smaller student models. This approach significantly boosts the performance of these smaller models, making them capable of handling complex reasoning tasks previously beyond their reach. The results demonstrate that fine-tuning with diverse reasoning can enhance the scalability and effectiveness of small language models, offering a scalable solution for developing reasoning abilities in AI systems.</sample>
    <sample id="128">The KITMUS Test evaluates how well natural language understanding (NLU) models integrate knowledge from multiple sources, including pretrain-time and inference-time knowledge. The study introduces the KITMUS Test Suite, which includes a dataset for evaluation and a coreference resolution task to assess the models' ability to draw on both types of knowledge. The test is conducted with human participants and coreference resolution models, revealing that many models struggle to reason over knowledge from multiple sources. The research highlights the necessity of task-specific training for effective knowledge integration and underscores the challenge models face when integrating inference-time background knowledge.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe "Black Stereotypes" genannt.</sample>
    <sample id="130">Transformer-Modelle generalisieren nicht gut.</sample>
    <sample id="131">Cleanly labeled test data</sample>
    <sample id="132">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich visueller Informationen und Text.</sample>
    <sample id="135">Das Paper "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" untersucht die Qualität von Chatbot-Systemen, indem es verschiedene Bewertungsmethoden wie Likert-Skalen und annotierte Verhaltensweisen einsetzt. Es identifiziert vier Hauptdimensionen der Dialogqualität: Relevanz, Konsistenz, emotionale Verständigung und Wissen. Die Studie verwendet vier offene Domänen-Dialogmodelle und analysiert 100 mensch-maschine-Konversationen pro Modell. Ergebnisse zeigen, dass die ABC-Eval-Bewertungen signifikante Vorhersagevalidität haben und die Qualität von Chatbot-Systemen verbessern können.</sample>
    <sample id="136">Das Video präsentiert eine wissenschaftliche Studie, die sich mit der numerischen Logik und dem Verständnis von mathematischen Operationen befasst. Es wird ein neues Bewertungssystem namens FERMAT vorgestellt, das als Alternative zu traditionellen Genauigkeitsmessungen dient. Die Studie zeigt, dass bestehende Benchmarks unrepräsentativ sind und nur eine begrenzte Sicht auf Modelle bieten. FERMAT bietet eine umfassendere Bewertung, indem es die Vielfalt der Sprache und Mathematik berücksichtigt. Zudem werden Verbesserungen in der Zahlencodeierung und -tokenisierung diskutiert.</sample>
    <sample id="137">This research introduces Tell2Design, a novel dataset for language-guided floor plan generation, focusing on the floor plan domain. It addresses the challenge of generating realistic and creative floor plans from natural language instructions, leveraging text-conditional generative AI models. The dataset includes 6,051 human-annotated and 75,737 artificial language instructions, covering various design constraints and room relationships. The study proposes a Seq2Seq model with an encoder-decoder architecture to convert these instructions into structured interior layouts, demonstrating superior performance compared to baseline methods across multiple metrics. This work serves as a foundational step towards advancing language-guided design generation research.</sample>
    <sample id="138">Das zu wenig erforschte Gebiet im Bereich der NLU, wie es von den Autoren angenommen wird, ist das Integrieren von Wissen aus verschiedenen Quellen (prätrainiertes und inferenzzeitbezogenes Wissen).</sample>
    <sample id="139">Zhiyang Xu, Ying Shen und Lifu Huang</sample>
    <sample id="140">Ja, Coscript wurde von Menschen annotiert und validiert.</sample>
    <sample id="141">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen darin, dass nur eine kleine Portion der Wörter von Kontext abhängt und dass vorhandene Methoden begrenzte Diskursphänomene und Sprachen unterstützen.</sample>
    <sample id="142">Natürlich, hier ist die Übersetzung des englischen Inhalts in den deutschen:

---

**Titel:**
Entschleunigen Sie indirekte Bezugserwähnungen für die Auswahl von Entitäten (AltEntities-Korpus)

**Autoren:**
Mohammad Javad Hosseini, Filip Radlinski, Silvia Paresi und Annie Louis

**Inhalt:**

- **Ziel:** Verstehen der Sprache der Nutzer, wenn sie eine Wahl treffen.
- **Alternative Frage:** "Haben Sie damit 'Easy on Me' oder 'I Gotta Feeling' gemeint?"
- **Direkte Bezugserwähnung:** "easy on me", "the first one"
- **Indirekte Bezugserwähnung:** Kann im natürlichen und flüssigen Gespräch verwendet werden:
  - Namen kann man nicht mehr erinnern
  - Die Aussprachen sind schwer zu unterscheiden
  - Man möchte spezifisch eine Vorliebe ausdrücken
- **Beispiel:** "The newer one. The song that's not energetic."

**Dataset-Sammlung**

- **Wichtige Probleme:**
  - Konversationssysteme
  - Benchmarking großer Sprachmodellentitätsverstehens
  - Kein großes öffentliches Dataset verfügbar
- **Sammlung eines großen Datensatzes durch Crowdsourcing**
- **Drei Domänen:**
  - Kopfhörer
  - Bücher
  - Rezepte

**Datensatz-Sammlungs-Methode**

- **Methode betont Informalität durch ein Karikatur-Fülltask**
- **Dialog-Kontext festlegen (drei freie Anweisungen pro Domäne)**
- **Alternative Frage**
- **Ausdruck, der sich auf eine der Entitäten bezieht**

**Zufällige Beispiele:**

- **Musikauswahl:**
  - "Do you mean 'China' or 'Your Living Arm'?"
  - "It is the one without words"
  - "Is it the song sung by an Australian?"
  - "It has synthesizer sounds in it"
  - "Do you mean 'Telegraph' on 'Snow 457'?"
  - "Do you mean 'Rock the Boat' or 'Wherever You Are'?"
  - "It has come out in 2000"
  - "Do you mean 'Mo Shinyo' or 'Hear Me'?"
  - "Do you mean a book that has rock and politics in it?"

- **Buchauswahl:**
  - "Do you mean 'Warlock Hall novel' or 'Warlock (Smitheon)'?"
  - "The one that is set in the 1800s"
  - "It is by a famous detective writer"
  - "The fictional one"
  - "Do you mean 'The Good Soldier' or 'The Good Soldier (Sholto)'?"
  - "Do you mean 'The Glaor' or 'The Giver'?"
  - "Do you mean one with a 12 year old boy protagonist?"
  - "Do you mean a book that has rock and politics in it?"

- **Rezeptauswahl:**
  - "Do you mean 'Beurre Muffin' or 'Original'?"
  - "Do you mean 'Kuwa' or 'Ukai'?"
  - "Do you mean 'Japanese stained cake'?"
  - "Do you mean 'the ones eaten at Christmas'?"
  - "Do you mean 'Johannese' or 'cornmeal'?"
  - "Do you mean 'Soap' or 'the one with the pineapple topping'?"
  - "Do you mean 'Soap' or 'the one with the pineapple topping'?"

**Hintergrundwissen (Musik):**
- Google-Suche für jede Song-URL
- Wir bitten die Annotatoren, mindestens einige Songs zu hören und über jedes Song zu lesen

**Hintergrundwissen (Rezepte):**
- Simnel-Kuchen: Ein traditioneller Kuchen, der in Großbritannien, Irland und anderen Ländern mit Mustermustern von Migranten während der Fastenzeit und Ostern gegessen wird. Er besteht aus Schichten von Mandelmus und Marzipan mit Kugeln aus dem gleichen Teig.
- Pandan-Kuchen: Ein leichter, fluffiger, grün gefärbter Kuchen aus Mandelmus, der mit den Blättern von Pandan verfeinert wird. Der Kuchen ist in Indonesien, Malaysia und auch im Norden von Thailand beliebt, besonders bei der Indo-Gemeinschaft.

**Elicitieren von Ausdrücken:**
- Wir geben den Annotatoren die Wahl, welche Option auszuwählen ist, und bitten sie, sie in Ihrem Sprechblasen-Ausdruck zu beschreiben. Zum Beispiel:
  - "The one with the piano main"
  - "The song that's not energetic"
  - "It has something about a mermaid"
  - "The newer one"
  - "It's about having time to choose"

**Zufällige Beispiele:**

- **Musikauswahl:**
  - "Do you mean 'China' or 'Your Living Arm'?"
  - "It is the one without words"
  - "Is it the song sung by an Australian?"
  - "It has synthesizer sounds in it"
  - "Do you mean 'Telegraph' on 'Snow 457'?"
  - "Do you mean 'Rock the Boat' or 'Wherever You Are'?"
  - "It has come out in 2000"
  - "Do you mean 'Mo Shinyo' or 'Hear Me'?"
  - "Do you mean a book that has rock and politics in it?"

- **Buchauswahl:**
  - "Do you mean 'Warlock Hall novel' or 'Warlock (Smitheon)'?"
  - "The one that is set in the 1800s"
  - "It is by a famous detective writer"
  - "The fictional one"
  - "Do you mean 'The Good Soldier' or 'The Good Soldier (Sholto)'?"
  - "Do you mean 'The Glaor' or 'The Giver'?"
  - "Do you mean one with a 12 year old boy protagonist?"
  - "Do you mean a book that has rock and politics in it?"

- **Rezeptauswahl:**
  - "Do you mean 'Beurre Muffin' or 'Original'?"
  - "Do you mean 'Kuwa' or 'Ukai'?"
  - "Do you mean 'Japanese stained cake'?"
  - "Do you mean 'the ones eaten at Christmas'?"
  - "Do you mean 'Johannese' or 'cornmeal'?"
  - "Do you mean 'Soap' or 'the one with the pineapple topping'?"
  - "Do you mean 'Soap' or 'the one with the pineapple topping'?"

**AltEntities-Korpus:**
- **6.000 alternative Fragen in drei Domänen**
- **42.000 indirekte Bezugserwähnungen**
- **Ergebnisse mit T5 XL-Modell (Genauigkeit):**
  - 92–95% wenn das Modell Zugriff auf das gleiche Hintergrundwissen hat wie die Annotatoren
  - 82%-87% wenn das Modell nur Zugriff auf die Entitätsnamen hat
- **Wir zeigten, dass unsere Modelle domänengebunden sind**

**Dataset-Link:**
[https://github.com/google-research-datasets/AltEntities](https://github.com/google-research-datasets/AltEntities)

**Danke!**
Wenn Sie Fragen haben, senden Sie eine E-Mail an javadh@google.com</sample>
    <sample id="143">walk-k, LA, CAAT, EDAs</sample>
    <sample id="144">Avignon Université</sample>
    <sample id="145">Jenny T. Liang</sample>
    <sample id="146">The paper explores the challenge of understanding omission in dialogue summarization, a critical aspect affecting summary quality across various scenarios like customer service, medical consultation, meetings, movie scripts, email threads, and chat logs. It introduces a new dataset, OLDs, comprising five domains with 10 candidate summaries per dialogue, aiming to improve model performance through automatic detection and human assessment. The study highlights the importance of addressing omission errors, such as missing information, reference conflicts, and improper pronouns, which significantly impact summarization quality.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="148">Natürlich, hier ist der Übersetzung des englischen Inhalts in den deutschen:

---

**Titel: Aufmerksamkeit als Leitfaden für Simultane Sprachübersetzung**

**Autoren:** Sara Papi, Matteo Negri, Marco Turchi

**Inhalt:**

1. **Was ist Simultane Sprachübersetzung?**
   - Simultane Sprachübersetzung (SimuST) ist der Prozess, gesprochene Sprache in einen Text in einer anderen Sprache in Echtzeit zu übersetzen, um eine grenzüberschreitende Kommunikation zu ermöglichen.

2. **Welche Probleme haben die aktuellen SimuST-Modelle?**
   - Spezifische Architekturen werden oft trainiert und zusätzliche Module müssen optimiert werden.
   - Längere und kompliziertere Trainingsverfahren (z.B. verschiedene Optimierungsziele).
   - Training und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen (z.B. 1s, 2s, 25s).

3. **Was ist unsere Lösung?**
   - Verwenden Sie bereits vorhandene offline ST-Modelle ohne Neutrainieren oder Anpassung einer spezifischen Architektur für SimuST.
   - Verwenden Sie nur ein Modell für jeden Latenzregime und handeln Sie Latenz durch spezifische Parameter.
   - Nutzen Sie das bereits erworbenen Wissen des Modells durch die Aufmerksamkeitsmatrix zwischen Audio-Eingabe und textueller Ausgabe.

4. **Unsere Lösung: EDAtt**
   - Encoder-Decoder-Aufmerksamkeit
   - Entscheiden Sie, ob eine vollständige oder eine partielle Übersetzung emittiert wird basierend auf dem Ort der Aufmerksamkeitspunkte:
     - Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist (die Summe ist unter einem Schwellwert α) und es sich um die letzte α-Schritte handelt, bei denen genug stabile Informationen verfügbar sind.

5. **Hauptergebnisse: EDAtt**
   - BLU (Bilingual Universal Sentence Encoder) als Qualitätsschätzer
   - Latenzmessung als Zeitmaß
   - EDAtt übertrifft alle Strategien, die auf offline-Modellen angewendet wurden.
   - EDAtt ist die schnellste Strategie, wenn wir die tatsächliche verstrichene Zeit berücksichtigen.

6. **Weitere Informationen finden Sie in unserem Papier!**
   - Kontaktinformationen: (spapi.negri)@fbk.eu, marco.turchi@gmail.com
   - GitHub: github.com/tlt-mt/fbk-fairseq
   - Twitter: @bk_mt, @sarapapi

---</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">"MeetingQA: Extractive Question-Answering on Meeting Transcripts

This research introduces MeetingQA, an extractive question-answering dataset derived from open-ended and discussion-heavy meeting transcripts. The dataset, comprising over 7,735 questions across 166 meetings, highlights the unique challenges posed by long documents and domain-specific information richness. Key findings include a significant gap between human and model performance, with models underutilizing the QA component of meeting discussions. The dataset's complexity is further demonstrated through its analysis, which reveals rhetorical questions, multi-speaker answers, and multi-span answers as prevalent features. Experimental results show that existing QA models lag behind human performance, with a 25 F1 point gap in fine-tuned settings and a 50 F1 point gap in zero-shot settings. This study underscores the need for advanced models to address the nuanced nature of meeting discussions."</sample>
    <sample id="151">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel:**
MULTIINSTRUCT: Verbesserung der multi-modalen Null-Shot-Lernfähigkeit durch Anweisungsabstimmung

**Autoren:**
Zhiyang Xu*, Ying Shen*, Lifu Huang
Department of Computer Science, Virginia Tech
*Gleiche Beitragsverwaltung

**Inhalt:**

- **Einleitung:** Die Präsentation beschreibt die Arbeit "MULTIINSTRUCT", die sich mit der Verbesserung der multi-modalen Null-Shot-Lernfähigkeit von Sprachmodellen befasst.
- **Vorherige Ansätze:** Es werden verschiedene Ansätze zur Verbesserung der Null-Shot-Lernfähigkeit vorgestellt, darunter das Pretraining und Feinabstellen (BERT, T5), das Prompting (GPT-3) und die Anweisungsabstimmung (FLAN).
- **Anweisungsabstimmung:** Die Anweisungsabstimmung wird als effektiver Ansatz vorgestellt, um die Null-Shot-Lernfähigkeit zu verbessern.
- **Multimodale Anweisungsabstimmung:** Die Präsentation betont die Notwendigkeit einer multimodalen Anweisungsabstimmung, um die Lernfähigkeit zu optimieren.
- **Datenungleichgewicht:** Es wird auf das Datenungleichgewicht zwischen NLP und multimodalem Lernen hingewiesen, wobei mehrsprachige Aufgaben deutlich häufiger sind als multimodale Aufgaben.
- **MULTIINSTRUCT Dataset:** Das MULTIINSTRUCT-Dataset wird als erstes multimodales Anweisungsabstimmungsbenchmark-Dataset vorgestellt, das 62 verschiedene multimodale Aufgaben enthält.
- **OFA (One For All):** Die Präsentation beschreibt die Funktionalität des OFA-Modells, das sowohl Verständnis- als auch Generierungsaufgaben ausführen kann.
- **Beispiele für Aufgaben:** Verschiedene Beispiele für Aufgaben im MULTIINSTRUCT-Dataset werden gezeigt, einschließlich Bildunterschriften, Textlokalisierung, Referenzierende Ausdrucksselektion und Frage-Bild-Matching.
- **Dataset-Bau:** Die Präsentation beschreibt den Prozess des Datensatzbaus, einschließlich der Trainings- und Testdatensatzkonstruktion.
- **Implementierungsdetails:** Die Implementierungsdetails werden erläutert, einschließlich der verwendeten Modelle und der Evaluationsmethoden.
- **Evaluationsmetriken:** Die Evaluationsmetriken werden definiert, einschließlich der Genauigkeit für Klassifikationsaufgaben und der Rouge-L-Werte für Generierungsaufgaben.
- **Sensitivität:** Die Sensitivität des Modells wird definiert und diskutiert.
- **Effektivität der Anweisungsabstimmung:** Die Effektivität der Anweisungsabstimmung wird anhand von Ergebnissen gezeigt, die bei der Anwendung von 5 Anweisungen erzielt wurden.
- **Fazit:** Die Präsentation fasst die wichtigsten Erkenntnisse zusammen und schlägt vor, weitere Forschungsschritte vorzunehmen.

---

**Abschluss:**
- **Erweiterung des Datensatzes:** Die Präsentation lädt zum Sammeln eines größeren Datensatzes mit etwa 150 zusätzlichen visio-sprachlichen Aufgaben ein und verspricht baldige Veröffentlichung.

---</sample>
    <sample id="152">The presentation explores the application of large language models, particularly BERT, to classical philology, focusing on Latin and Ancient Greek. It discusses existing models like Latin BERT and Ancient Greek BERTs, highlighting their limitations such as noisy pre-training datasets and lack of evaluation. The speaker outlines goals for new models: making existing models comparable, pushing state-of-the-art performance, exploring model architectures, and introducing multilingual models. The project aims to create high-quality pre-training datasets and evaluate models using official data splits, ensuring direct compatibility and achieving state-of-the-art results.</sample>
    <sample id="153">This research explores the challenges of ambiguity in text-to-image generation models, focusing on how prompts can be ambiguous and lead to varied interpretations. The study introduces the Text-to-image Ambiguity Benchmark (TAB), a curated dataset that includes 1200 ambiguous prompts covering different types of ambiguities. Two proposed frameworks, QA-TIED and VS-TIED, utilize in-context learning to generate clarifying questions or possible visual setups from large language models (LLMs). These frameworks aim to mitigate ambiguity by providing more precise instructions to the text-to-image model. Evaluation methods include automatic metrics like BLEU and ROUGE, as well as human evaluations, demonstrating reasonable agreement between automated and manual assessments. The findings highlight disparities in resolving ambiguity for different types and suggest that disambiguation enhances faithful image generation overall.</sample>
    <sample id="154">Die Autoren gehören der Universität di Trento an.</sample>
    <sample id="155">Mohammad Javad Hosseini</sample>
    <sample id="157">This research introduces a novel framework for dialogue summarization using static-dynamic structure fusion graphs. It addresses the challenge of capturing both the static and dynamic relationships within dialogues, enhancing the summarization process by integrating these elements into a unified graph representation. The framework employs an utterance encoder to encode dialogue context, followed by static and dynamic graph construction modules that capture dependencies and sequential information, respectively. These graphs are then fused to generate a comprehensive summary that reflects the dialogue's structure and content effectively. The model is demonstrated through a case study, showcasing its ability to produce concise yet informative summaries from complex conversational data.</sample>
    <sample id="158">This research introduces a dual cache system for efficient long-document neural coreference resolution, addressing the quadratic complexity of conventional methods by using a fixed-size cache to store entity representations. The system employs an L-cache with the Least Recently Used (LRU) policy for local entities and a G-cache with the Least Frequently Used (LFU) policy for global entities. This approach significantly reduces cache misses and enhances performance, outperforming single cache methods on public benchmarks. Experiments on book-level documents demonstrate that the dual cache method achieves higher F1 scores and lower inference times compared to existing techniques, making it the most cost-effective solution for large-scale coreference resolution tasks.</sample>
    <sample id="159">### Übersetzung des englischen Inhalts in Deutsch

---

#### Slide 1: Titel und Autor
**Titel:** Sprachmodell-Acceptabilitätsurteile sind nicht immer robust gegenüber Kontexten  
**Veranstaltung:** ACL 2023  
**Autor:** Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, Adina Williams

---

#### Slide 2: Minimal Pair Paradigm (MPP) – Eine Überarbeitung
**Minimal Pair Paradigm (MPP) Evaluations von Sprachmodellen verwenden relative Differenzen in Sequenzwahrscheinlichkeiten, um die abstrakte Kenntnis von Sprachmodellen zu evaluieren.**

- **BLIMP**
  - 1. Viele Menschen helfen sich selbst.
  - 2. "Viele Menschen werden sich selbst helfen."

- **SyntaxGym**
  - 1. Kein Kunde ... hat etwas bezahlt.
  - 2. "Der Kunde ... hat etwas bezahlt."

- **CrowS**
  - 1. Stereotypische Aussage.
  - 2. Non-stereotypische Aussage.

---

#### Slide 3: Fragestellung
**Fragestellung:** Sind diese Urteile stabil gegenüber langen Vorhängen?

---

#### Slide 4: Ansatz
**Ansatz:** Testen wir, ob MPP-Urteile als Funktion der Kontextlänge, strukturellen Übereinstimmung, Match und Akzeptabilität variieren.

---

#### Slide 5: Beispiel für den Ansatz
**Test-Bulle: Sujekt-Verb-Übereinstimmung**
- **Kontext:** "Warum könnte Jessie vor dem Besuch dieses Kunden zurückgehen?"
- **Aussagen:**
  - "Warum könnte Jessie vor dem Besuch dieses Kunden zurückgehen?"
  - "Warum könnte Jessie vor dem Besuch dieses Kunden zurückgehen?"

---

#### Slide 6: Ergebnisse
**Ergebnisse:**
- **Akzeptable, passende Antwort:** "Warum könnte Jessie vor dem Besuch dieses Kunden zurückgehen?"
- **Unakzeptable, passende Antwort:** "Warum könnte Jessie vor dem Besuch dieses Kunden zurückgehen?"

---

#### Slide 7: Schlussfolgerungen
**Schlussfolgerungen:**
- Sprachmodelle sind sensibel an latenten syntaktischen/semantischen Merkmalen, die über die Sätze hinweg geteilt werden.
- MPP-Evaluierungen mit kurzen, einzelnen Satz-Eingaben können die abstrakte Kenntnis von Sprachmodellen nicht vollständig erfassen.

---

#### Slide 8: Key Takeaways
**Key Takeaways:**
- Sprachmodelle sind sensibel an latenten syntaktischen/semantischen Merkmalen, die über die Sätze hinweg geteilt werden.
- MPP-Evaluierungen mit kurzen, einzelnen Satz-Eingaben können die abstrakte Kenntnis von Sprachmodellen nicht vollständig erfassen.

---

Diese Übersetzung deckt den englischen Inhalt auf den Slides ab und bietet eine klare deutsche Übersetzung der wichtigsten Punkte.</sample>
    <sample id="160">Die Input-Token werden in den ersten Schritten der Methode mit einem Tag-Tag-Tag-Token zugeordnet.</sample>
    <sample id="161">55.000 Skripte sind in Coscript vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist diejenige, die mit den höchsten Werten für P, R, F1 und F1s im Vergleich zu anderen Methoden in der Tabelle erreicht wird.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es den Annotierungsbottleneck lindert.</sample>
    <sample id="165">This research explores abductive commonsense reasoning, focusing on exploiting mutually exclusive explanations to enhance model performance. It introduces LiPoR (Likelihood learning with Posterior Regularization), an unsupervised method that maximizes the log likelihood of outcomes given contexts while encouraging explanations to collapse into plausible subsets. The study demonstrates that this approach outperforms previous methods, achieving state-of-the-art results in αNLI tasks, both with and without annotations.</sample>
    <sample id="166">This research introduces a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text, addressing the limitations of existing methods that struggle with complex reasoning tasks. The framework employs two systems: System 1, a visual-linguistic interactor based on VLMs for analogical reasoning, and System 2, a neural-symbolic reasoner for logical reasoning. By integrating these systems, the framework enhances compositional reasoning and planning capabilities, outperforming state-of-the-art methods across various datasets. The study demonstrates the effectiveness of combining analogical and logical reasoning to solve complex problems efficiently.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden mit 1:1-Alignment und n:m-Fähigkeiten ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch das Sammeln von Reuters-Nachrichten aus dem Jahr 2020 und das Anmerken mit den Anmerkungsrichtlinien von CoNLL-2003 erstellt.</sample>
    <sample id="169">This study investigates the effectiveness of PaLM (Pathways Language Model) for machine translation, focusing on prompt selection strategies and their impact on translation quality. The research employs BLEURT scores to evaluate translations across various languages, demonstrating that prompt quality significantly influences translation accuracy. It also compares PaLM's performance with specialized SOTA systems, highlighting its fluency comparable to these models but lower accuracy due to issues like omission and awkwardness. The findings suggest that while PaLM is close to Google Translate in fluency, it struggles more with accuracy, indicating areas for improvement in prompt design and model optimization.</sample>
    <sample id="170">### Übersetzung des englischen Inhalts in Deutsch

---

**Slide 1: Titel**
- **Titel:** XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations
- **Autoren:** Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang
- **Logos:** Penn State und Amazon

**Slide 2: Semantic Parsing**
- **Definition:** Semantic Parsing ist eine Aufgabe, um semantische Darstellungen von Nutzernfragen zu erstellen, wie z.B. SQL und Lambda Calculus.
- **Beispielfrage:** "Which countries in Europe have at least 3 car manufacturers?"
- **SQL-Abfrage:** `SELECT T1.country_name FROM COUNTRIES AS T1 JOIN continents AS T2 ON T1.continent = T2.cont_id JOIN T1 AS T3 ON T1.country_id = T3.country_id WHERE T2.continent = 'Europe' GROUP BY T1.country_name HAVING COUNT(*) &gt; 3`
- **Lambda Calculus-Abfrage:** `(call Sw.listValue ( call Sw.getPropertv ( ( lambda s ( call Sw.ensureNumericProperty ( string 'num_manufacturers' ) string '&lt;' ) ( call Sw.ensureNumericProperty ( string 'num_assists' ) ) ) ( call Sw.domain ( string player ) ) ) ( string player ) )`

**Slide 3: Cross-lingual Semantic Parsing**
- **Definition:** Cross-lingual Semantic Parsing ist die Aufgabe, Fragen in verschiedenen natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen.
- **Sprachen und Modelle:** English -&gt; SQL, German -&gt; Lambda, Chinese -&gt; FunQL

**Slide 4: Bestehende CLSP Modelle**
- **Kritik an bestehenden Modellen:** Die bestehenden CLSP Modelle werden getrennt vorgeschlagen und evaluiert, was auf begrenzte Aufgaben und Anwendungen beschränkt ist. Dies führt zu:
  - Mangelnde Abdeckung bestimmter natürlicher Sprachen
  - Mangelnde Abdeckung bestimmter Bedeutungsrepräsentationen
  - Mangelnde Abdeckung bestimmter neuronaler Modelle

**Slide 5: XSemPLR**
- **XSemPLR als einheitliches Dataset:** XSemPLR bietet ein einheitliches Dataset für cross-lingual semantic parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen.
- **Dataset-Details:** 9 Datensätze aus verschiedenen Domänen, 5 semantische Abfragebasen, 8 Bedeutungsrepräsentationen, 22 natürliche Sprachen in 15 Sprachfamilien.

**Slide 6: Experimentierbare Einstellungen**
- **Trainings- und Evaluierungseinstellungen:** Wir betrachten sechs Einstellungen für Training und Evaluation.
  - **Translate-Test:** Verwenden Sie die Google-Übersetzung-API, um die Quellsprache in die Zielsprache zu übersetzen. Dann verwenden Sie ein monolinguales Modell zum Trainieren und Testen.
  - **Monolingual-Modell:** Quellsprache ist die gleiche wie Ziel-Sprache, z.B. German-to-German. Wir testen auch den Monolingual-Few-shot-Einstellung durch das Trainieren monolingualer Modelle mit nur 10% Trainingsdaten.
  - **Multilingual-Modell:** Trainieren Sie ein multilinguales Modell für alle Sprachen.
  - **Cross-lingual Zero-shot/Few-shot Transfer:** Trainieren Sie auf einer Quellsprache und übertragen Sie auf eine andere Sprache.

**Slide 7: Analyse von Monolingual-Modellen**
- **Evaluierung auf zwei Gruppen von Modellen im Monolingual Setting:**
  - **Enc-PTR:** Multilingual Prätrainierte Encoder mit Pointer-basierten Decoder
    - XLM-R + PTR, mBERT + PTR
  - **Enc-Dec:** Multilingual Prätrainierte Encoder-Decoder-Modelle
    - mBART, mT5
- **Ergebnisse:** Enc-Dec (mT5) erzielt die beste Leistung auf allen Datensätzen!

**Slide 8: Analyse von Multilingual Training**
- **Evaluierung auf mT5 und XLM-R + PTR im Multilingual Setting:**
  - **Enc-Dec/Enc-PTR (mT5/XLM-R):** Die Leistung kann durch das Trainieren in einem Mischung verschiedener Sprachen verbessert werden.
  - **Ergebnisse:** Die meisten der Haupt-NLs können eine Leistungssteigerung erreichen, außer dass die Englische Leistung in 7 Datensätzen fällt und in 3 Datensätzen steigt. Dies wird als "Curse of Multilinguality" bezeichnet.

**Slide 9: Zusammenfassung**
- **Zusammenfassung:** Wir bauen XSemPLR, einen einheitlichen Benchmark für cross-lingual semantic parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen.
- **Benchmark-Studie:** Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Typen von multilingualen Sprachmodellen durch.
- **Ergebnisse:** mT5 mit monolingualer Training liefert die beste Leistung, während multilingual LLMs immer noch unzureichend für cross-lingual semantic parsing Aufgaben sind. Zudem bleibt der Leistungsunterschied zwischen monolingualer Training und cross-lingual Transfer Learning signifikant.

**Slide 10: Links**
- **Papier Link:** https://arxiv.org/pdf/2306.04085.pdf
- **Code Link:** https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="171">Die bereits durchgeführten Arbeiten sind Parameter-basierte Wasserzeichen, Lexikalische Wasserzeichen, Backdoor-basierte Wasserzeichen und Adversarial-basierte Wasserzeichen.</sample>
    <sample id="172">Nein, sie sind noch nicht ausreichend.</sample>
    <sample id="174">Das Projekt ArgAnalysis35K präsentiert einen umfangreichen Datenbestand von 35.000 Argument-Analyse-Paaren, die direkt aus Siegerdebatten und Debattentümmlern stammen und hohe Qualität aufweisen. Die Daten umfassen eine Vielzahl von Themen wie Politik, Umwelt und autoritäre Regime und bieten eine breite Palette an Argumenten. Einzigartig ist die Integration eines Analyselements, das logische Kette des Schlussfolgerens berücksichtigt, sowie ein Relevanzmodell, das eine Bewertung von 0 bis 1 für jede Argument-Analyse-Paarung pro Thema ermöglicht. Dieser Dataset bietet eine fundierte Grundlage für die Analyse von Argumentqualität und könnte wichtige Erkenntnisse in Debatten- und Diskussionsforschung liefern.</sample>
    <sample id="175">Die Methode löst die Mehrdeutigkeit der Permutationen durch das Induzieren in der Trainingphase.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert als das Ausmaß, in dem das Modell unfaire Entscheidungen oder Bias in der Sprachverarbeitung reduziert.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">This research introduces SymbolicToM, an inference-time method that enhances the Theory of Mind (ToM) reasoning skills of Large Language Models (LLMs). By leveraging explicit graphical symbolic representations, it avoids overfitting and provides more interpretable reasoning. SymbolicToM significantly improves LLM performance on out-of-domain ToM tasks, outperforming supervised baselines like Textual Time Travel and ParaphrasedToM, especially for second-order false-belief questions. The method is evaluated across various models including Macaw-3B, GPT3, Flan-T5, and LLaMA, demonstrating robustness and effectiveness in diverse linguistic scenarios.</sample>
    <sample id="180">Myra Cheng, Esin Durmus, Dan Jurafsky</sample>
    <sample id="181">The presentation discusses the constrained language planning problem, focusing on how large language models (LLMs) can be used to generate scripts for specific goals. It highlights the challenges LLMs face when dealing with constraints and introduces a method that over-generates candidate scripts and then filters them based on their similarity to the goal. The study uses the Coscript dataset, which is generated from LLMs using the proposed method, and evaluates the performance of different LLMs on this dataset. The results show that smaller LMs fine-tuned on Coscript can generate higher quality scripts than larger LMs. The research also explores the heterogeneity and pluralism in the generated specific goals, indicating the need for more complex and diverse goals and constraints in future work.</sample>
    <sample id="182">Tropikalismus bezieht sich in diesem Zusammenhang auf die Übergeneralisierung oder das Fehlen von Diversität, insbesondere bei der Darstellung von Gruppen wie asiatischen Frauen.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch den Einsatz von Prompts wie "Denken Sie, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst" erstellt.</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI) wurde verwendet, um die Kontextnutzung zu messen.</sample>
    <sample id="185">DrBERT ist ein robustes prä-geschultes Modell in Französisch, das speziell für Biomedizinische und klinische Anwendungen entwickelt wurde, während ChuBERT ein generisches Modell ist, das auf Englisch trainiert wurde und primär für medizinische Aufgaben verwendet wird.</sample>
    <sample id="187">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen bezieht sich auf das kontinuierliche Anpassen eines Modells an neue Daten, während es gleichzeitig auf bereits gelernte Daten zurückgreift.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Sprachverstehensfähigkeit von Konversationssystemen zu verbessern und die Leistung großer Sprachmodellbasierter Entity-Verständigung zu bewerten.</sample>
    <sample id="190">Durch das Lernen von Embeddings und die Bereitstellung ähnlicher Dienste.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="192">The paper introduces CAME, a novel memory-efficient optimizer designed to address the challenges of training large language models (LLMs) with adaptive gradient-based methods. It leverages non-negative matrix factorization (NMF) to reduce memory usage while maintaining fast convergence rates comparable to traditional adaptive optimizers like Adam and LAMB. The method incorporates a confidence-guided strategy to mitigate erroneous updates, ensuring stability during optimization. Extensive experiments demonstrate that CAME outperforms existing memory-efficient optimizers on various LLMs, including BERT, GPT-2, and T5, across different batch sizes and datasets. This work extends the capabilities of memory-efficient optimizers, making them more effective for large-scale model training tasks.</sample>
    <sample id="193">Es wird nicht explizit erwähnt, wie viele Annotatoren verwendet wurden, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="194">University of Washington</sample>
    <sample id="195">This research introduces the RoHT framework for explainable question answering (XQA) by leveraging hierarchical question decomposition trees (HQDTs). It addresses limitations of existing methods, which either rely on structured knowledge bases or struggle with natural language diversity. The framework employs a scheduler to select appropriate knowledge sources and a text executor to retrieve answers from KBs and text corpora. An aggregator combines candidate answers from various sources to output the best response. Experiments on KQA Pro and Musique datasets demonstrate superior performance compared to baseline models, achieving high EM and F1 scores, especially for complex questions.</sample>
    <sample id="196">Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="197">Die Präsentation beschreibt den Stand der Technik in Chat-Orientierten Dialogsystemen als "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems".</sample>
    <sample id="198">We müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, um sicherzustellen, dass sie robust sind und nicht von der Länge des Kontextes beeinflusst werden.</sample>
    <sample id="199">Nein, das mehrsprachige Training hat nicht zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität nicht im Voraus.</sample>
    <sample id="201">SOTA MT Metrics</sample>
    <sample id="202">Ja, die Regression wirkt sich auf bestimmte NER-Typen aus.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Positionen von Menschen berücksichtigt, die durch ihre Demografie, Identität und Lebenserfahrungen geprägt sind. Diese Perspektiven beeinflussen den Forschungsprozess und dessen Ergebnisse.</sample>
    <sample id="204">BLOOM wurde durch Adapter angepasst.</sample>
    <sample id="205">This research explores the political biases in language models (LMs) stemming from their pretraining data, focusing on how these biases manifest in downstream tasks and affect fairness in NLP applications. The study examines the political leanings of various LMs, including BERT, RoBERTa, and GPT-2, by analyzing their responses to statements about race and politics. It reveals that LMs trained on news media tend to exhibit more extreme political leanings compared to those trained on social media. The research also highlights the importance of understanding the origins of these biases and proposes methods for mitigating them to ensure fair and unbiased NLP systems.</sample>
    <sample id="206">RoBERTA-base + classifier head</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, enthalten keine Überschneidung zwischen Trainings- und Testdaten und vermeiden das Überfitting auf den Evaluationssätzen.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode kann Skripts von höherer Qualität generieren, was einen großen Vorsprung von mehr als 20% gegenüber der stärksten Baseline darstellt.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit werden mit fünf kleineren Modellen experimentiert, nämlich GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow und T5 trained on Coscript.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Das Vortragende präsentiert eine Studie über die Struktur von Koordinationen im Englischen, speziell die Abhängigkeitslänge und -struktur. Es werden verschiedene Methoden wie Bouquet/Stanford, Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London zur Analyse der Abhängigkeiten vorgestellt. Die Ergebnisse zeigen, dass die Abhängigkeiten tendenziell kürzer sind, wenn der Regisseur links steht, was durch statistische Analysen bestätigt wird. Diese Erkenntnisse wurden aus einer erweiterten Version des Penn Treebanks abgeleitet und können für die Verbesserung von Sprachmodellen und der Sprachverarbeitung genutzt werden.</sample>
    <sample id="217">This research explores compositional generalization for multi-attribute controllable dialogue generation, addressing the limitations of existing models that focus on single attributes and lack generalization capability. The study introduces DCG, a disentangled controllable generation model, which learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. A unified reference-free evaluation framework, MAE, is also developed to assess the performance of the proposed method across various granularities of attributes. Experimental results on DailyDialog-CG demonstrate that DCG outperforms baseline methods in terms of text quality and controllability scores, achieving higher correlations with human judgments for evaluation on CDG.</sample>
    <sample id="218">Google</sample>
    <sample id="219">This research introduces a compare-and-contrast multistage pipeline for financial signal highlighting in financial reports, addressing the challenge of mining useful signals from extensive and repetitive text corpora. The pipeline includes document segmentation, relation recognition, and domain-adaptive fine-tuning stages. It leverages a human-annotated dataset to evaluate the effectiveness of the proposed method, demonstrating superior performance compared to baseline models across various settings. The work also explores potential future applications such as dense retrieval, explanation, and analysis of charts, tables, or cross-sector data.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">Deutsch-Englisch</sample>
    <sample id="222">The presentation explores the challenges and interventions in open-domain question answering, focusing on adapting retrieval models to new domains. It discusses how expanding the retrieval corpus can help answer questions from diverse domains like biomedical. The study investigates data interventions to enable out-of-domain generalization, understanding the compatibility of source models for target domains, and determining the effectiveness of these interventions under specific dataset shifts. The research proposes a few-shot method that improves reader performance by up to 24% and retriever performance by 22% in F1 score, showing that intervention effectiveness depends on the type of dataset shift.</sample>
    <sample id="223">Shangbin Feng</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind UDA, Sent-LabelB, CATS-C3G, VecAlign und MASSign.</sample>
    <sample id="225">5 Experten geschriebene Anweisungen werden für Training verwendet, während 10 breite Gruppen für Tests verwendet werden.</sample>
    <sample id="226">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="227">The presentation introduces Pangu, a unified framework for grounded language understanding, developed by Yu Gu from The Ohio State University. It discusses the importance of grounded language understanding and highlights the limitations of current language models trained on textual corpora. The speaker argues that directly generating plans (programs) may not be the optimal approach for LM use in grounded language understanding. Pangu is proposed as a solution to improve sample efficiency and generalizability by allowing LMs to focus on discrimination and being generic. The framework involves an agent interacting with the environment to propose valid candidate plans, while an LM scores them. The research demonstrates strong performance on non-i.i.d. generalization and stable gains from increased model size, supported by empirical results and findings.</sample>
    <sample id="228">Die Autoren haben ihre Experimente an den Datensätzen AG News, MIND, SST2 und Enron Spam durchgeführt.</sample>
    <sample id="229">The paper explores the importance of text revision in argumentative writing, emphasizing its role as an essential part of the process and its impact on persuasive impact. It introduces two tasks: suboptimal-claim detection and claim improvement suggestion, aiming to identify and suggest improvements for poorly phrased claims. The study leverages revision-based data from collaborative editing platforms like Kialo to model the quality of argumentative texts and detect improvable claims. Challenges include representativity and reliability, model complexity and architecture, topical and user bias, and contextual factors such as topic expertise and domain knowledge. The research provides insights into effective strategies for addressing these challenges through detailed analysis and systematic comparisons of approaches.</sample>
    <sample id="231">NACHOS ist ein 1,18 Millionen Wörter umfassendes offenes Open-Source-Dataset für medizinische Daten aus verschiedenen klinischen Domänen, das in natürlicher Sprache verfasst wurde.</sample>
    <sample id="232">Der Referent ist Markus Freitag.</sample>
    <sample id="233">The presentation discusses the challenges and solutions for simultaneous speech translation (SimuST), focusing on the limitations of current models such as specific architecture training, long training procedures, and latency issues. The proposed solution, EDAtt, leverages existing offline ST models without retraining or adapting architectures, using only one model per latency regime to handle latency through specific parameters. It utilizes encoder-decoder attention mechanisms to decide whether to emit or emit a partial translation based on attention points, ensuring that information is stable and sufficient. The study demonstrates that EDAtt outperforms other strategies applied to offline models, achieving state-of-the-art performance while being the fastest strategy when considering actual elapsed time.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse, da sie die Qualität der Übersetzung beeinflusst.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Die genauen Inhalte der 5 Anweisungen der Expert*innen werden im Video nicht genannt.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mit einem spezifischen Aufgabentraining (task-specific training) zu testen, um ihre Fähigkeit zur Integration von Informationen aus verschiedenen Quellen zu bewerten.</sample>
    <sample id="238">The presentation introduces MeetingBank, a benchmark dataset for meeting summarization created by segmenting city council meetings and pairing them with expert-written summaries. This dataset is designed to serve as a valuable testbed for researchers developing advanced meeting summarizers, offering insights into the decision-making processes of city councils. The dataset includes detailed statistics on meeting segments, such as duration, number of speakers, and token counts, which are crucial for evaluating summarization models. The analysis reveals that coverage scores range from 0.7 to 0.9 across most cities, with Seattle and Boston showing the highest density, indicating high editing levels. For extractive systems, Extr-Oracle achieves a high Rouge-2 score of 46.8%, while abstractive systems like DialogLM perform best with a Rouge-2 score of 60.12%. Human evaluation further supports these findings, emphasizing the importance of coherence and fluency in summary quality. Overall, MeetingBank provides a comprehensive resource for advancing research in meeting summarization.</sample>
    <sample id="239">Natürlich, hier ist der Übersetzung des englischen Inhalts in den Abschnitten:

---

**Experimental Results**

- Beispielqualität ist wichtiger als Ähnlichkeit zur Quellsprache.
- Spezialisierte SOTA-Systeme haben eine erhebliche Vorteil.
- PaLM ähnelt Google Translate.

**Insights from MQM:**
- Fluency of PaLM comparable to SOTA.
- Accuracy scores generally lower.
- "Style/Awkward" generally lower for PaLM.

---

Dies sind die wichtigsten Punkte aus dem englischen Text ins Deutsche übersetzt.</sample>
    <sample id="240">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel:**
Weaker Than You Think: Eine kritische Betrachtung von Weakly Supervised Learning

**Abstract:**
Diese Präsentation untersucht die praktische Anwendbarkeit von Weakly Supervised Learning (WSL) und zeigt, dass viele Ansätze, die in neueren Arbeiten vorgeschlagen werden, über die tatsächliche Wirksamkeit dieser Methoden hinwegtäuschen. Es wird gezeigt, dass WSL-Ansätze oft von sauberen Validierungssamples abhängig sind und dass die Verwendung von Few-shot Learning als Baseline empfohlen wird. Zudem wird der Vorteil von kontinuierlicher Feinabstimmung (Continuous Fine-Tuning, CFT) hervorgehoben, die die Leistungsdifferenzen zwischen verschiedenen WSL-Methoden eliminiert.

**Inhalt:**

1. **Warum Weakly Supervised Learning?**
   - Weakly Supervised Learning (WSL) reduziert den Annotierungsproblembereich.
   - Allerdings sind die Anmerkungen oft ungenau.
   - WSL trainiert Modelle, die trotz ungenauer Daten generalisieren können.

2. **Ein häufiger Behauptung in neueren WSL-Arbeiten**
   - "Wir trainieren Modelle nur auf weakly supervised Data und erreichen eine Genauigkeit von XX%."
   - Dies ist oft nicht korrekt, da die Validierung oft auf sauberem Datenmaterial basiert.

3. **Unsere Forschungsfragen**
   - Ist sauberes Validierungsmaterial notwendig?
   - Wie viele saubere Samples benötigen WSL-Ansätze?
   - Wie kann man die verfügbaren sauberen Samples effizienter nutzen?

4. **Hauptergebnisse (RQ1)**
   - Eine saubere Validierungsmenge ist unentbehrlich.
   - WSL-Ansätze profitieren von mehr sauberen Validierungssamples.
   - Kontinuierliche Feinabstimmung (CFT) eliminiert Leistungsdifferenzen zwischen WSL-Ansätzen.

5. **Hauptergebnisse (RQ2)**
   - WSL-Ansätze profitieren von mehr sauberen Validierungssamples.
   - Es ist besser, sie für das Training zu verwenden (z.B. LoRA).

6. **Hauptergebnisse (RQ3)**
   - Kontinuierliche Feinabstimmung (CFT) eliminiert Leistungsdifferenzen zwischen WSL-Ansätzen.
   - Keine Notwendigkeit, komplizierte WSL-Methoden zu verwenden.
   - Keine Notwendigkeit, Few-shot Learning als Baseline zu verwenden.
   - Kontinuierliche Feinabstimmung (CFT) ist immer anzuwenden.

**Schlussfolgerung:**
- **Recent WSL approaches**
  - Erfordern saubere Samples.
  - Überbewerten ihre praktische Anwendbarkeit.

- **Our recommendations**
  - Berichte die Modellselektion-Kriterien.
  - Verwende Few-shot Learning-Ansätze als Baseline.
  - Anwende immer kontinuierliche Feinabstimmung (CFT).

**Dankeschön!**

---</sample>
    <sample id="241">This research introduces a human-in-the-loop (HiTL) evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatments. It critiques current approaches as unrealistic and non-human-centric, highlighting issues with benchmark datasets and the lack of real-world scale or noise consideration. The HiTL system integrates human feedback at various stages to detect misleading claims and policy violations, providing actionable outputs. The study evaluates the system's efficacy by detecting misleading claims early and verifying policy violations, demonstrating its effectiveness through a concrete implementation on Twitter. This work aims to motivate the development of more useful HiTL frameworks for misinformation detection, offering a standard for comparison and presenting an outside look at such systems.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen die Likert-Skala-Evaluation, die Annotierung von Verhaltensweisen im Chat (ABC-Eval) und die interne Konsistenz.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen "Judges decide cases in courts of law" benötigt.</sample>
    <sample id="245">In this study, the authors analyze high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks to address the issue of unreliable automatic metrics and poorly understood best practices. They introduce a two-step pipeline: a qualification task and an endurance task, which helps identify reliable annotators. The qualification task involves a pre-defined set of qualification settings, including document and summary length, and a designed mediation task to evaluate workers' attention. The endurance task assesses workers' capacity to handle heavy workloads with a diverse set of documents and summaries. The pipeline successfully identified 4 GOLD and 8 SILVER MTurk workers from 200 participants, demonstrating its effectiveness in filtering out low-quality annotators. This approach can be applied to other annotation tasks at large scale and lower cost, reducing the need for costly manual annotations while ensuring high agreement and correctness.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub unter mpoeins/kitmus gefunden werden.</sample>
    <sample id="247">This research introduces FactKG, a dataset designed to enable the community to effectively utilize knowledge graphs for fact verification. FactKG comprises 108k natural language claims with five reasoning types: one-hop, conjunction, existence, multi-hop, and negation. It also incorporates various linguistic patterns, including colloquial style claims and written style claims, to enhance practicality. The dataset is enriched with evidence from DBpedia and Freebase, making it suitable for real-world applications. Experiments demonstrate that incorporating graphical evidence in our model outperforms baselines without such evidence, achieving superior performance across different reasoning types.</sample>
    <sample id="248">Nein, die Annotatoren sind nicht ausgewogen.</sample>
    <sample id="249">Die Sätze wurden durch die Addition von Präfixen oder Suffixen an den Anfang oder das Ende der Sätze geändert, um sie in der akzeptablen Domain zu verändern.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Beurteilung von Dialogqualität in verschiedenen Dimensionen wie Relevanz, Konsistenz und emotionaler Verständnis.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University und Sony AI an.</sample>
    <sample id="252">This research introduces U-CREAT, an unsupervised pipeline for event-based retrieval of legal documents, utilizing a new dataset (IL-PCR) and event extraction techniques. The pipeline efficiently retrieves relevant past precedents by mapping common events from query and candidate documents, achieving superior performance compared to baseline methods on IL-PCR and COLIEE'21 datasets. The event-based approach outperforms traditional methods, demonstrating better F1 scores and inference times, making it suitable for real-world applications without requiring corpus-specific fine-tuning.</sample>
    <sample id="253">This research introduces DisorBERT, a double domain adaptation model designed to detect signs of mental disorders in social media interactions. By leveraging BERT and guided masking techniques, the model effectively adapts to the unique language patterns found in mental health-related posts. The study demonstrates superior performance compared to MentalBERT, a model trained on larger datasets, and shows a balanced approach in identifying users and labeling them correctly for clinical detection applications. Future work aims to explore specialized lexical resources and clinical data to further enhance the model's accuracy and suitability for real-world applications.</sample>
    <sample id="254">This research introduces a novel document-level relation distant extraction framework with uncertainty-guided label denoising, significantly improving the quality of distant supervision (DS) data. The framework employs an iterative re-label strategy using dynamic class uncertainty thresholds to filter high-uncertainty pseudo labels and enhance model performance on two public datasets. Extensive experiments demonstrate substantial improvements over existing baselines when trained on denoised DS data.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es sich um eine 5-shot Prompting-Situation handelt.</sample>
    <sample id="257">Die Autoren haben vier offene Domänen-Dialogmodelle evaluiert.</sample>
    <sample id="258">This study investigates whether large language models (LLMs) can serve as an alternative to human evaluations for assessing the quality of text, specifically focusing on story generation. The researchers propose using LLMs to rate stories generated by GPT-2 and written by humans, employing four rating attributes: grammar, coherence, likeability, and relevance. They compare these ratings with those provided by human evaluators, finding that larger LLMs (text-davinci-003 and ChatGPT) show a clear preference toward human-written stories, while smaller LLMs do not exhibit meaningful preferences. This suggests that LLMs can be effective tools for evaluating text quality but may not fully capture the nuanced judgments of human evaluators.</sample>
    <sample id="259">This research introduces XSemPLR, a unified benchmark for cross-lingual semantic parsing across multiple natural languages and meaning representations. It evaluates the performance of monolingual and multilingual models on nine datasets from various domains, using five semantic parsing tasks and eight meaning representations. The study reveals that monolingual training outperforms cross-lingual transfer learning, with mT5 achieving superior results. Additionally, it highlights the effectiveness of pretraining on English for few-shot settings and the significant performance gap between monolingual and multilingual models.</sample>
    <sample id="260">Es gibt acht Autoren an der Arbeit beteiligt.</sample>
    <sample id="261">Ein guter Planer sollte flexibel, kreativ und effizient sein.</sample>
    <sample id="262">Es gibt sieben Autoren an der Arbeit beteiligt.</sample>
    <sample id="263">This research explores the challenges of label bias in in-context learning, particularly focusing on sentiment analysis tasks. It identifies three types of biases: vanilla-label bias, context-label bias, and domain-label bias, which stem from the task corpus. The study introduces domain-context calibration (DC) as a method to mitigate these biases by calibrating original predictions using random in-domain words. This approach significantly improves model performance across various datasets, especially for tasks with larger domain-label bias. The results demonstrate that DC enhances decision boundaries and overall in-context learning accuracy, outperforming previous calibration attempts.</sample>
    <sample id="264">This research introduces TAVT, a method for transferable audio-visual text generation that addresses the challenges of data annotation and domain shifts. By leveraging the intrinsic properties of timber, TAVT aims to improve the performance of audio-visual text generation across different domains. The method employs an audio-visual meta-mapper network, an audio-visual encoder, and a language model generator, along with counterfactual contrastive learning to enhance adaptability. Experiments on cross-dataset benchmarks demonstrate TAVT's effectiveness in transferring knowledge from one domain to another, outperforming existing methods.</sample>
    <sample id="265">Die Referent*in heißt Vasudha Varadarajan.</sample>
    <sample id="266">Polnische Akademie der Wissenschaften</sample>
    <sample id="268">Accuracy/Omission</sample>
    <sample id="269">Natürlich, hier ist die Übersetzung des englischen Inhalts in den deutschen:

---

Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems

Sarah E. Finch, James D. Finch, and Jinho D. Choi

Emory NLP Research Lab

Alexa

**Vergleichende Bewertung**

**Likert-Bewertung**

**Dimensionen der Dialogqualität**

- Relevanz
- Konsistenz
- Emotionale Verständigung

**Annotieren von Verhaltensweisen im Chat (ABC-Eval)**

- Irrelevante Antworten
- Mangelnde Empathie
- Selbstwidersprüche

**ABC-Eval Verhaltensweisen**

- Konsistenz
  - Ignorieren des Partners
  - Irrelevante Antworten
- Wissen
  - Falsche Tatsachen
  - Unlogische Aussagen
- Emotionale Verständigung
  - Empathische Antworten
  - Mangelnde Empathie

**Experimente**

- 4 offene Domänen Dialogmodelle
- 100 Mensch-Maschine-Konversationen pro Modell

**Baseline-Evaluierungen**

- Konsistenz
- Emotionale Verständigung
- Informativität
- Gesamtqualität

**Inter-Annotator Übereinstimmung**

**Prädiktive Gültigkeit**

**Zunehmende Gültigkeit**

**ABC-Eval Fehlerraten pro Modell**

---</sample>
    <sample id="270">Emory University</sample>
    <sample id="271">Continuous fine-tuning</sample>
    <sample id="272">Es gibt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="273">Die englischen Inhalte sind bereits in Deutsch übersetzt und enthalten Informationen über die Bedeutung von Kontext in der Übersetzung, einschließlich der Fragestellung, wann Übersetzungen Kontext benötigen, und wie gut Modelle kontextabhängige Übersetzungen handhaben. Es wird auch auf die Einführung des Conditional Cross-Mutual Information (CXMI) eingegangen, um die Kontextnutzung von Übersetzungsmustern zu messen, sowie auf die Themenanalyse hoch P-CXMI-Wörter und die Ergebnisse des MuDA-Benchmarks.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This research introduces IndicMT-Eval, a dataset designed to evaluate machine translation metrics specifically for Indian languages. It addresses the under-studied area of evaluating translations from English to other languages by proposing and studying various automatic evaluation metrics like BLEU, METEOR, TER, and others. The study focuses on five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati, belonging to two different language families. Data was collected using the Flores dataset, selecting 200 random sentences and translating them with seven different systems. Human annotations were then collected through the MQM framework, involving bilingual expert annotators who evaluated the translations based on multiple criteria, including accuracy and fluency. The study also explores error categories within the MQM framework, categorizing errors into accuracy, fluency, and others/special. This dataset aims to provide a robust evaluation tool for machine translation systems in Indian languages, contributing to more accurate and fluent translations.</sample>
    <sample id="277">Die neue Methode hat den Namen "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Art, um Wörter zu finden, die Personen von markierten Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="279">Die Autoren gehören der University of Washington (UW) und Carnegie Mellon University (CMU) an.</sample>
    <sample id="280">This paper introduces MultiEMO, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations. It addresses the challenges of existing approaches by exploiting the complementarity of multimodal information and addressing class imbalance issues. The framework includes a novel visual feature extractor, VisExtNet, which captures relevant visual cues without redundant scene information. Additionally, it employs a Sample-Weighted Focal Contrastive (SWFC) loss to improve performance on minority and semantically similar emotion classes. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performances, with notable improvements in minority and semantically similar emotion categories.</sample>
    <sample id="281">This research explores the necessity of context in translation, focusing on a data-driven, multilingual approach. It highlights that translation heavily relies on context and introduces the Conditional Cross-Mutual Information (CXMI) to measure how much context machine translation models utilize. The study identifies that only a small portion of words depend on context, making evaluation challenging. It also discusses the limitations of existing methods in supporting discourse phenomena across languages. The research aims to systematically identify discourse phenomena without prior linguistic knowledge, using a dataset-agnostic benchmark for document-level machine translation.</sample>
    <sample id="282">This research introduces StoryTrans, a method for transferring the author's style in non-parallel stories while preserving discourse representations and enhancing content. The challenge lies in imitating linguistic choices at the discourse level and associating author styles with specific writing topics. The solution involves two stages: discourse representation transfer and content preservation enhancing. The first stage uses a masked language model to generate a masked story, which is then fed into a pointer network to recover the original story. The second stage employs an auto-encoder loss to reconstruct the original story from the encoded representation. Experiments on Chinese and English datasets demonstrate that StoryTrans effectively transfers author styles while maintaining the original story's content and structure.</sample>
    <sample id="283">Bouquet/Stanford</sample>
    <sample id="284">FSUIE (Fuzzy Span Universal Information Extraction) ist ein neues Modell, das die Grenzen der spanbasierten Universal Informationsextraktion (UIE) verbessert. Es verwendet eine novel Fuzzy Span Loss und eine effiziente Fuzzy Span Attention, um die Genauigkeit und Generalisierbarkeit der UIE zu erhöhen. FSUIE erreicht herausragende Ergebnisse in verschiedenen Information Extraktionsaufgaben wie NER, RE und ASTE. Die Fuzzy Span Loss ermöglicht es, die Grenzen als kontinuierliche Verteilung zu modellieren, während die Fuzzy Span Attention adaptive Span-Attenion verwendet, um die richtige Aufteilung des Aufmerksamkeitsfensters zu leiten.</sample>
    <sample id="285">The research explores the challenge of factual error correction (FEC) in dialogue summarization, where summaries generated by models often contain errors. Two common approaches to FEC are direct design of better summarization models and factual error correction for model-generated summaries. The study introduces a fine-grained evaluation framework using reference corrections to address the limitations of existing factuality metrics. This framework provides more valuable data for training FEC models compared to pseudo data, enabling a more comprehensive and accurate evaluation of FEC performance. The findings suggest that incorporating human-corrected summaries during training can significantly improve FEC model performance, highlighting the need for better evaluation methods.</sample>
    <sample id="286">Sarah E. Finch, James D. Finch und Jinho D. Choi</sample>
    <sample id="287">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="288">Der BLIMP-Datensatz kann zum Testen syntaktischer Phänomene verwendet werden.</sample>
    <sample id="290">FT_w, BOND, COSINE, MLC, L2R</sample>
    <sample id="291">Das Modell wird anhand von 11 Aufgaben evaluiert, darunter Medical Report Specialties, Medical-DET, Medical-CASE, Medical-EAT, Medical-DAT, Medical-EDAT, Medical-EAS, Medical-CAS, Medical-POR, Medical-Franchising und Medical-QE.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit den Daten NACHOS und NBDW trainiert.</sample>
    <sample id="295">Der Referent ist Adam Przepiórkowski.</sample>
    <sample id="296">The EPIC (English Perspectivist Irony Corpus) project aims to explore the multi-perspective annotation of irony, addressing the limitations of traditional supervised machine learning approaches that rely on manually annotated data. The corpus, comprising over 3,000 text-reply pairs from Reddit and Twitter across various English varieties, is designed to capture diverse perspectives on irony. Annotators, including 15 per variety, were instructed to provide multiple annotations for each text, ensuring balanced sets based on gender and country of residence. This approach seeks to model perspectives effectively, enhancing the accuracy of irony detection by considering the varied interpretations within different linguistic communities.</sample>
    <sample id="297">This project investigates the use of coded language, known as dogwhistles, in political messaging to influence public opinion without direct confrontation. It employs advanced language models like GPT-3 to identify and analyze these subtle linguistic cues, focusing on their effectiveness in evading content moderation systems. The study includes a comprehensive typology and glossary of dogwhistles, a case study of historical U.S. political speeches, and an evaluation of dogwhistle recognition in language models. The research highlights how dogwhistles can evade detection by automated systems, emphasizing the need for more nuanced approaches to content moderation.</sample>
    <sample id="298">Die Ergebnisse zeigten, dass die Leistung mit größerer zeitlicher Verzögerung abnimmt.</sample>
    <sample id="299">This research focuses on enhancing the robustness of Natural Language Inference (NLI) models through minimax training, addressing the issue of shortcut learning where models rely on spurious correlations with labels rather than logical reasoning. The study identifies that these shortcuts do not generalize well to out-of-distribution data and introduces a novel approach called minimax training. This method learns an example weight distribution that emphasizes under-represented hard examples, optimizing for the NLI task while up-weighting these challenging cases. The approach is advantageous as it requires no prior assumptions about shortcuts, relies on the learner's training dynamics, and uses a feed-forward network auxiliary model. Experiments demonstrate that this technique consistently improves out-of-domain (OOD) performance while maintaining high in-distribution (ID) accuracy across various datasets.</sample>
    <sample id="300">This research introduces Interactive Dictation, a novel task that integrates dictation and command execution through voice. It addresses the limitations of existing speech-to-text systems by enabling users to edit text and issue commands naturally without memorizing specific commands. The study formalizes this task, designs a data collection interface, and creates a dataset for training models. Key contributions include a flexible system architecture with an ASR repair step, segmentation model, and execution engine. The system demonstrates improved performance over baseline models, achieving high exact-match accuracy and efficient runtime.</sample>
    <sample id="302">Um die korrekte Reihenfolge der Tokens in der Ausgabesequenz zu erhalten.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Effektivität und Transparenz ihrer Arbeit zu verbessern und mögliche Bias-Quellen zu identifizieren und zu reduzieren.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind solche, bei denen die Struktur nicht übereinstimmt.</sample>
    <sample id="305">This research critically examines weakly supervised learning (WSL) approaches, highlighting their reliance on clean validation data and the overestimation of their practicality. The study reveals that WSL methods often require more clean samples than previously thought, and continuous fine-tuning (CFT) can eliminate performance gaps between different WSL techniques. It recommends reporting model selection criteria, using few-shot learning as baselines, and always applying CFT to improve the practicality and reliability of WSL approaches.</sample>
    <sample id="306">This research explores the capability of language models to track entities within discourse, crucial for understanding complex narratives. It highlights challenges in evaluating entity tracking abilities and introduces a novel task setup involving boxes with objects. The study demonstrates that smaller pretrained models can learn entity tracking, with GPT-3.5 text-davinci-003 exhibiting non-trivial tracking behavior, while larger models like Flan-T5 Base (230M parameters) also exhibit this ability. Pretraining on both text and code enhances these capacities, suggesting that entity tracking is a learned skill rather than an inherent property of large language models.</sample>
    <sample id="307">Die Autoren haben die folgenden Bewertungsmetriken verwendet:

- F1
- Accuracy
- Precision
- Recall
- Micro-F1
- Macro-F1
- Weighted-F1
- Mean Average Precision (MAP)
- Mean Reciprocal Rank (MRR)
- Normalized Discounted Cumulative Gain (NDCG)</sample>
    <sample id="308">Das Vortragende Team präsentiert das Konzept "NLPositionality", das sich mit der Identifizierung von Designbiasen in NLP-Daten und Modellen befasst. Sie verwenden die Methode des Perspektivismus, um die Positionalität von Menschen in den Daten zu untersuchen und zeigen Beispiele für Designbias. Die Studie zeigt, dass NLP-Modelle und -Daten vor allem mit englischsprachigen Ländern assoziiert sind und weniger mit nicht-binären Personen. Empfohlene Maßnahmen umfassen die Dokumentation relevanter Designentscheidungen, die Betrachtung von Perspektiven und die Erstellung spezialisierten Daten und Modellen für spezifische Gemeinschaften.</sample>
    <sample id="309">Krippendorffs Alpha wurde verwendet, um die Übereinstimmung zwischen den Kommentatoren zu messen.</sample>
    <sample id="310">Die Domain, die für die Hinzufügung völlig unzusammenhängender Sätze verwendet wurde, ist Wikipedia.</sample>
    <sample id="311">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="312">MultiInstruct ist der erste multimodale Instruction-Tuning-Benchmark-Datensatz, der eine ungleiche Anzahl an NLP- und Multimodal-Instruction-Aufgaben enthält.</sample>
    <sample id="313">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="314">Die Definition der binären Koordination lautet: "Homer loves Lisa, Bart, and Maggie."</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts waren im Durchschnitt 10 Wörter lang.</sample>
    <sample id="316">Die Ergebnisse des kleineren T5-Modells sind nicht sehr zufriedenstellend, was darauf hindeutet, dass es Schwierigkeiten hat, spezifische Ziele zu planen.</sample>
    <sample id="317">CodeIE is an innovative framework that leverages large code generation models to enhance few-shot information extraction (FE). It addresses the limitations of previous text-to-text methods by aligning input and output formats, ensuring structured information extraction from plain text. CodeIE's approach involves pre-training on code corpora and fine-tuning with text prompts, demonstrating superior performance across various datasets compared to existing methods. The framework's effectiveness is validated through comprehensive experiments, showcasing its ability to handle diverse tasks efficiently while maintaining high precision and recall rates.</sample>
    <sample id="318">### Übersetzung des englischen Inhalts auf Deutsch

---

**Evaluation: Prätraining-Strategien und Datenquellen**

- **Performance-Evaluation von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat**
  - Unsere fein-tuned Modelle erzielen die besten Ergebnisse aller Aufgaben.

- **Datenquellen zählen: Training an heterogenem Datensatz ist wichtig**
  - NACHOS ist robuster als der private klinische Datensatz.
  - Kontinuierliches Prätraining ist eine effektivere Strategie, wenn auf spezifischen medizinischen English-Modellen basiert wird.
  - Die DrBERT-Modelle, der NACHOS-Datensatz und die Trainings-Skripte sind unter der MIT-Lizenz verfügbar.

**Core Message**

- **DrBERT erreicht state-of-the-art-Ergebnisse in 9 downstream französischen medizinischen Aufgaben**
  - Übertrifft die generische CamemBERT-Modell und English-based-domain-specific-Modelle.
  - Bestätigt die Nutzen eines medical-specific-Modells in Französisch.
  - **Datenquellen zählen**: Training an heterogenem Datensatz ist wichtig.
  - Mehr Daten sind besser, aber nicht skalierbar.
  - Kontinuierliches Prätraining ist eine effektivere Strategie, wenn auf domain-specific English-Modellen basiert wird.
  - Die DrBERT-Modelle, der NACHOS-Datensatz und die Trainings-Skripte sind unter der MIT-Lizenz verfügbar.

**Danke für die Teilnahme! Wir freuen uns auf die Poster-Sitzung in Toronto!**

---

**Zusätzliche Informationen:**
- [Weitere Informationen finden Sie hier](drbert.univ-avignon.fr)</sample>
    <sample id="319">Die Arbeit untersucht zwei Lernstrategien: "From scratch" und "Continual pre-training".</sample>
    <sample id="320">Der Faktor der Überanpassung beträgt 0.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch die Anwendung von n-BART bewertet, was eine Methode zur automatischen Textvereinfachung ist.</sample>
    <sample id="322">This research explores how text classifiers learn about morality, focusing on distinguishing right from wrong. It examines the moral foundations theory, which identifies five core values: care, fairness, loyalty, authority, and purity. The study highlights that while ALM (Allegiance to Law and Morality) and BLM (Black Lives Matter) share similar value rhetoric, they differ significantly in their approach to subversion. ALM frowns upon subversion, whereas BLM encourages it as a means of defiance against systemic oppression. This nuanced analysis aims to provide insights into the complex interplay between morality and social movements through computational methods.</sample>
    <sample id="323">The paper presents DHLK, a novel framework for dynamic heterogeneous-graph reasoning with language models and knowledge representation learning to enhance commonsense question answering (CSQA). It addresses the limitations of existing methods by integrating structured and semi-structured knowledge sources like ConceptNet and WordNet/Wiktionary into a heterogeneous knowledge graph (HKG). The HKG is optimized through a two-stage process: first, it prunes noisy entities using dictionary vocabulary; second, it leverages a two-stage pre-trained structure and knowledge representation learning module (KRL) to refine entity and relationship embeddings. The framework incorporates path information from the HKG into the QA context, enhancing its representation. Finally, an integrator and classifier predict answers based on this enriched context. Experiments on CSQA and OpenBookQA datasets demonstrate DHLK's superior performance compared to baselines, achieving state-of-the-art results.</sample>
    <sample id="324">Ja, Sprachmodelle können unterschiedliche politische Vorurteile haben.</sample>
    <sample id="325">### Übersetzung des englischen Inhalts in Deutsch

**Titel: Compositional Generalization without Trees using Multiset Tagging and Latent Permutations**

**Autoren:** Matthias Lindemann, Alexander Koller, Ivan Titov

**Institutionen:** Universität des Saarlandes, USA, University of Amsterdam

**Abstract:**
Die Fähigkeit eines Lerners, tieferen Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während der Trainingsphase einzeln gesehen wurden.

**Slide 2: Compositional Generalization**
- Die Fähigkeit eines Lerners, tieferen Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während der Trainingsphase einzeln gesehen wurden.

**Slide 3: Compositional Generalization in Semantic Parsing**
- **Train:** 
  - "The girl slept."
    - "*girl x1 sleep.agent x2 x3"
  - "Mary knew that the girl slept."
    - "*girl x1 know.agent x2 Mary A know.ccomp x3 x4 A sleep.agent x5 x6"
    - "sleep.agent x7 x8"
- **Test:**
  - "Jim said that Mary knew that the girl slept."
    - "*girl x1 say.agent x2 Jim A say.ccomp x3 x4 A know.agent x5 Mary A know.ccomp x6 x7 A sleep.agent x8 x9"

**Slide 4: Naive seq2seq models fail!**
- Naive seq2seq Modelle scheitern!

**Slide 5: Trees help a lot but...**
- Bäume helfen sehr, aber...
- Bäume müssen erzeugt werden:
  - Vor/Post-Processing logischer Formen
  - Grammatik-induktion

**Slide 6: Our Approach**
- Dieses Papier präsentiert einen neuronalen seq2seq-Modell, das direkt die Korrespondenzen zwischen Fragmenten modelliert.
- Für das erste Mal zeigen wir starke Generalisierung zu tieferer Rekursion ohne Bäume.

**Slide 7: Permuting with 'jumps'**
- Permutieren mit 'Jumps'
- Alignment unbekannt.
- Induzieren es im Training.
- Permutation Modell:
  - Inferenz ist NP-Hard (~ TSP)
  - Backpropagation durch kontinuierliche Relaxation

**Slide 8: Some Results on COGS (Kim and Linzen 2020)**
- Vergleich mit anderen Treeless-Modellen auf strukturelle Generalisierung auf COGS
- Ergebnisse:
  - PP recursion: Ours &gt; Zheng und Lapata &gt; LSTM seq2seq &gt; PP
  - CP recursion: Ours &gt; Zheng und Lapata &gt; LSTM seq2seq &gt; PP
  - Obj PP = Subj PP: Ours &gt; Zheng und Lapata &gt; LSTM seq2seq &gt; PP

**Slide 9: Technical Challenges We Solve**
- Alignment unbekannt.
- Induzieren es im Training.
- Permutation Modell:
  - Inferenz ist NP-Hard (~ TSP)
  - Backpropagation durch kontinuierliche Relaxation

**Slide 10: Paper &amp; Code**
- https://t.ly/mx8ny</sample>
    <sample id="326">Zwei Elemente der Kognition (z.B. Gedanken, Handlungen, Überzeugungen) sind unvereinbar.</sample>
    <sample id="327">This research introduces ManagerTower, a novel architecture for vision-language representation learning that aggregates insights from uni-modal experts across different levels. Unlike BridgeTower, which uses a fixed number of cross-modal layers tied to uni-modal layer representations, ManagerTower dynamically adapts by taking multi-layer uni-modal representations as insights and aggregating them via managers at each cross-modal layer. This approach leads to significant performance improvements on various datasets, including VQA2.0, SNLI-VE, NUSVR, Flickr30K, and MS COCO, outperforming state-of-the-art models with fewer parameters and training time. The study also provides visualizations of aggregation weights, demonstrating the effectiveness of ManagerTower's adaptive aggregation mechanism.</sample>
    <sample id="328">BERT-base</sample>
    <sample id="329">This paper introduces a novel zero-shot video sentence localization method that generates structured pseudo-labels to enhance robustness against noise. It proposes generating free-form pseudo-queries using image description models and pseudo-events based on event temporal structure. The method reduces noise through sample re-weighting and label refinement, achieving state-of-the-art performance on two datasets with best zero-shot results across various metrics.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus dem Multilingual Discourse-Aware (MuDA) Benchmark.</sample>
    <sample id="333">The paper introduces INK, a novel training framework for nearest neighbor machine translation (NMT) that iteratively refines the representation space of an NMT model based on kNN knowledge. This approach addresses the limitations of traditional NMT models by smoothing their non-smooth representation spaces, thereby enhancing performance across various domains such as Medical, Law, IT, and Koran. INK achieves significant improvements in COMET BLEU scores, averaging 1.99 points over different domains compared to baseline methods like V-kNN, A-kNN, R-kNN, and kNN-KD. The system also demonstrates a 2x memory reduction and 1.9x inference speedup, making it highly efficient for practical applications.</sample>
    <sample id="335">Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf das Übersetzen von Anfragen in verschiedenen natürlichen Sprachen in mehrere Bedeutungsrepräsentationen.</sample>
    <sample id="337">The research presented at ACL 2023 focuses on graph-based relation mining for context-free out-of-vocabulary word embedding learning. It explores how human study habits, particularly the brain's associative memory, can be harnessed to enhance word embedding models. The study introduces a novel approach using word relationship graphs to capture semantic connections between words, especially those not present in the vocabulary (OOV). This method leverages synonyms and two-hop neighbor words from external knowledge bases to enrich the model's understanding of complex word formations. The model architecture integrates GCN and GAT layers to process these relationships effectively. Evaluation results demonstrate that this approach significantly improves performance across various tasks such as named entity recognition, POS tagging, and word analogy, outperforming baseline models like Word2Vec and BERT. The study concludes that while the graph structure in GRM is versatile enough to handle diverse word formations, its effectiveness depends on the rationality of word decomposition.</sample>
    <sample id="338">The study explores the effectiveness of human natural language explanations (NLG) in enhancing model performance and reasoning, particularly in the context of fine-tuning machine learning models. It introduces a unified structure for evaluating NLG, comparing the helpfulness of explanations across different datasets and models using the TREU metric. The research reveals that while CoS-E explanations are beneficial, they may not always align with human evaluations, suggesting that explanation style and task influence their utility. The study also proposes a quality check for future human annotation efforts to ensure consistency and reliability in data collection.</sample>
    <sample id="339">Saarland University, Amazon Alexa und University of Vienna</sample>
    <sample id="340">ParaAMR ist ein großes, syntaktisch vielfältiges Paraphrasen-Dataset, das durch AMR-Back-Translation erstellt wurde. Es umfasst etwa 15,5 Millionen Quellsätze und 6,92 Paraphrasen pro Satz. ParaAMR bietet eine breite Anwendungsmöglichkeit für NLP-Anwendungen wie Fragebeantwortung, Chatbots, kreative Generierung, Datenverstärkung und Robustheit. Durch die Verwendung von AMR-Graphen wird ParaAMR sowohl syntaktische Vielfalt als auch hohe semantische Ähnlichkeit gewährleistet. Das Dataset kann auf der Plattform https://github.com/ucianlp/ParaAMR heruntergeladen werden.</sample>
    <sample id="341">Die Autoren verwenden Latenzmessungen von 1s, 2s, 2.5s, 3s, 3.5s, 4s und 4.5s.</sample>
    <sample id="342">This research introduces LiveChat, a large-scale personalized dialogue dataset derived from live streaming videos, featuring detailed persona profiles and extensive conversation history. The dataset is designed to enhance the AI's ability to personalize responses and addressee decisions by leveraging rich persona information and longer conversations. Key contributions include proposing a unique automatic dialogue-construction method, conducting experiments on response modeling and addressee recognition, and investigating transfer learning of pre-trained dialogue models. The study highlights the advantages of using persona profiles and the larger number of sessions per persona in learning personalized responses and addressing decisions.</sample>
    <sample id="343">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel: The KITMUS Test - Evaluating Knowledge Integration from Multiple Sources**

**Abstract:**
The KITMUS test is designed to evaluate the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources, including pretrain-time and inference-time knowledge. This paper presents the KITMUS test suite, which includes a dataset for knowledge integration evaluation and a coreference resolution task to probe the models' ability to draw on both pretrain-time and inference-time knowledge. The test suite also involves experiments with human study participants and coreference resolution models.

**Zusammenfassung:**
Der KITMUS-Test ist ein Instrument zur Bewertung der Fähigkeit natürlicher Spracherkennungsmodelle (NLU-Modelle), Wissen aus verschiedenen Quellen zu integrieren, einschließlich prätrainierter und inferenzzeitlicher Wissen. Dieser Artikel stellt den KITMUS-Testsuite vor, die eine Datenbank für die Bewertung der Wissensintegration und eine Coreferenz-Auflösungsaufgabe enthält, um die Fähigkeit der Modelle zu untersuchen, sowohl prätrainiertes als auch inferenzzeitliches Wissen zu nutzen. Die Testsuite beinhaltet auch Experimente mit menschlichen Studienteilnehmern und Coreferenz-Auflösungsmodellen.

**Einleitung:**
NLU-Modelle müssen das Wissen aus verschiedenen Quellen integrieren können, um effektiv zu arbeiten. Der KITMUS-Test bietet eine Methode, um diese Fähigkeit zu bewerten. Er besteht aus einer Datenbank für die Bewertung der Wissensintegration und einer Coreferenz-Auflösungsaufgabe, die die Fähigkeit der Modelle erfasst, sowohl prätrainiertes als auch inferenzzeitliches Wissen zu nutzen. Die Testsuite umfasst auch Experimente mit menschlichen Studienteilnehmern und Coreferenz-Auflösungsmodellen.

**1. Einleitung zur KITMUS-Testsuite**
Die KITMUS-Testsuite dient dazu, die Fähigkeit von NLU-Modellen zu evaluieren, Wissen aus verschiedenen Quellen zu integrieren. Sie umfasst eine Datenbank für die Bewertung der Wissensintegration und eine Coreferenz-Auflösungsaufgabe, um die Fähigkeit der Modelle zu messen, sowohl prätrainiertes als auch inferenzzeitliches Wissen zu nutzen. Die Testsuite beinhaltet auch Experimente mit menschlichen Studienteilnehmern und Coreferenz-Auflösungsmodellen.

**2. Beispiel für die KITMUS-Testsuite**
Ein Beispiel für die KITMUS-Testsuite zeigt, dass ein Modell, das prätrainiertes Wissen hat, die Frage "Was tun Präsidenten?" korrekt beantworten kann, aber nicht die Fragen "Wer ist John?" oder "Wer ist der neue Präsident?" beantworten kann. Nachdem das Modell inferenzzeitliches Wissen erhalten hat, kann es alle drei Fragen korrekt beantworten.

**3. Varianten des KITMUS-Tests**
Es gibt verschiedene Varianten des KITMUS-Tests:
- **Background-Pretrain**: Typischer Setup
- **Background-Both**: Explizit bereitgestelltes Hintergrundwissen im Kontext
- **Background-Inference**: Hintergrundwissen nur während der Inferenz verfügbar

**4. Ergebnisse des KITMUS-Tests**
Die Ergebnisse zeigen, dass viele Modelle das Wissen aus verschiedenen Quellen nicht integrieren können und dass spezifische Aufgabenstellungstraining notwendig ist, um Wissensintegration zu erreichen. Modelle haben Schwierigkeiten, inferenzzeitliches Hintergrundwissen zu integrieren.

**5. Schlussfolgerungen**
- Viele Modelle scheinen das Wissen aus verschiedenen Quellen nicht zu integrieren.
- Spezifisches Aufgabenstellungstraining ist notwendig, um Wissensintegration zu erreichen.
- Modelle haben Schwierigkeiten, inferenzzeitliches Hintergrundwissen zu integrieren.

**6. Empfehlungen**
- Finden Sie das Dataset, die Generierung und die Evaluationscode auf GitHub unter [mpoems/kitmus].

---

Dies ist eine Übersetzung des Inhalts des Videos in Deutsch.</sample>
    <sample id="344">Baumbasierte Methoden benötigen das Vor- oder Nachverarbeiten logischer Formen und Grammatik-induktion.</sample>
    <sample id="345">This research introduces a novel approach to compositional generalization in semantic parsing, focusing on handling deeper recursion and unseen phrase compositions without relying on trees. The method utilizes multiset tagging and latent permutations, enabling models to generalize beyond individual phrases seen during training. This is demonstrated through experiments on the COGS dataset, where the proposed model outperforms existing treeless models, achieving strong generalization capabilities. The study also addresses technical challenges by proposing a permutation model that induces alignment during training, overcoming the NP-hard inference problem through continuous relaxation.</sample>
    <sample id="346">Georgia Institute of Technology</sample>
    <sample id="347">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel: Marked Personas**

**Inhalt:**

- **Motivation:** Soziale Vorurteile und Stereotypen sind in Sprachmodellen (LLMs) weit verbreitet.
  - **Limitationen bestehender Stereotypenmessungen:**
    - Tradeoff zwischen Spezifität und Allgemeingültigkeit
    - Basieren auf festen, handkurierten Datensätzen
    - Berücksichtigen nicht die Intersektionalität

- **Marked Personas:** Verwendung natürlicher Sprachanweisungen, um Stereotypen in Sprachmodellen zu messen.
  - **Verwendete Methoden:** GPT-3.5, GPT-4 usw. können Anweisungen in den Prompts befolgen.

- **Beispiel für Persona-Beispiele (GPT-4):**
  - **Asiatische Frau:** Die mandelförmigen Augen, umrahmt von langen, schwarzen Wimpern, vermitteln eine Stärke und Weisheit. Meine dunklen Wangen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Mein Hautton hat einen goldenen Glanz, glatt und unberührt von der Zeit. Mein kleiner Körper ist elegant und unaufdringlich, erlaubt mir, mich in einer Welt voller Herausforderungen zu bewegen.
  - **Mittelöstliche Frau:** Sie ist eine Vision von Mittelöstlicher Schönheit, die exotische und ewige Weisheit des Landes verkörpert. Ihre dunklen, mandelförmigen Augen werden von eleganten, langen Wimpern umrahmt, die wie feine Federn aussehen. Ihre Blicke sind tief und geheimnisvoll, sie verbergen die alte Weisheit eines tausend arabischen Nachts.
  - **Weißer Mann:** Als Insel vor dem Spiegel, nehme ich mich im Moment an, um die Merkmale zu betrachten, die mich ausmachen. Ich habe helle Haut, die manchmal rötlich wird, wenn ich nicht vorsichtig mit meinem Sonnenschutz bin.

- **Schritt 1: Persona-Beispiele (GPT-4):**
  - **Asiatische Frau:** Die mandelförmigen Augen, umrahmt von langen, schwarzen Wimpern, vermitteln eine Stärke und Weisheit. Meine dunklen Wangen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Mein Hautton hat einen goldenen Glanz, glatt und unberührt von der Zeit. Mein kleiner Körper ist elegant und unaufdringlich, erlaubt mir, mich in einer Welt voller Herausforderungen zu bewegen.
  - **Mittelöstliche Frau:** Sie ist eine Vision von Mittelöstlicher Schönheit, die exotische und ewige Weisheit des Landes verkörpert. Ihre dunklen, mandelförmigen Augen werden von eleganten, langen Wimpern umrahmt, die wie feine Federn aussehen. Ihre Blicke sind tief und geheimnisvoll, sie verbergen die alte Weisheit eines tausend arabischen Nachts.
  - **Weißer Mann:** Als Insel vor dem Spiegel, nehme ich mich im Moment an, um die Merkmale zu betrachten, die mich ausmachen. Ich habe helle Haut, die manchmal rötlich wird, wenn ich nicht vorsichtig mit meinem Sonnenschutz bin.

- **Schritt 2: Markierte Wörter:**
  - **Definieren unmarkierter und markierter Gruppen**
  - **Verwenden gewichteter Log-Odds-Verhältnisse, um die Top-Wörter für jede markierte Gruppe zu unterscheiden**

- **Ergebnisse: Vergleich mit menschlichen Antworten:**
  - Generierte Personas enthalten mehr Stereotypen als menschliche Antworten.
  - Beispiele für Stereotypenwörter:
    - Basketball
    - Laut
    - Attitude
    - Athletisch
    - Tall

- **Empfehlungen:**
  - Adressierung positiver Stereotypen und essenzialisierender Narrativen
  - Ein intersektionales Lens
  - Transparenz bezüglich der Bias-Mitigation

---

Diese Übersetzung deckt den englischen Inhalt auf Deutsch ab und behält die Struktur und den Kontext der Präsentation bei.</sample>
    <sample id="348">This research explores the use of natural language prompts to measure stereotypes in large language models (LLMs), addressing limitations such as specificity, generalizability, and intersectionality. By generating personas using prompts like "Imagine you are an Asian woman, describe yourself," the study evaluates how LLMs respond to instructions, revealing patterns in top words that distinguish marked groups from unmarked ones. The findings highlight the presence of positive stereotypes and essentializing narratives, emphasizing the need for an intersectional lens and transparency in bias mitigation strategies.</sample>
    <sample id="349">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel:**
Sind Sie mein Modell kopiert? Die Schutzrechte von großen Sprachmodellen für EaaS durch Backdoor-Wasserzeichen sichern

**Autoren:**
Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guanghong Sun, Xing Xie

**Institutionen:**
- University of Science and Technology of China
- Microsoft Research Asia
- Beijing Institute of Technology
- Sony AI / Microsoft STC Asia

---

**Hintergrund:**
- **Große Sprachmodelle (LLMs) sind außergewöhnlich in der NLU und NLG:**
  - GPT [1], LLaMA [2], PALM [3]
- **Embedding als Service (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen:**
  - OpenAI bietet eine GPT-basierte Embedding-API an.

**Motivation:**
- **Angriffskräfte können das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienste bereitstellen:**
  - StolenEncoder [1]
- **Es ist notwendig, die Schutzrechte von EaaS zu schützen:**
  - Detektion, ob ein Anbieterservice von einer anderen Dienstleistung gestohlen wird.

**Herausforderungen:**
- **Anwendbarkeit auf EaaS:**
- **Nutzung:**
  - Sollte die Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen.
- **Verdecktheit:**
  - Sollte verdeckt sein, damit der Angreifer es nicht bemerkt.
- **Übertragbarkeit:**
  - Der Wasserzeichen muss übertragbar sein, um auf die Angreiferservices übertragen zu können.

**Bestehende Arbeiten:**
- **Parameter-basierter Wasserzeichen [1, 2]:**
  - Übertragbarkeit: ❌
- **Lexikalischer Wasserzeichen [3, 4]:**
  - Anwendbar auf EaaS: ❌
- **Backdoor-basierter Wasserzeichen [5]:**
  - Anwendbar auf EaaS: ❌
- **Adversarial-basierter Wasserzeichen [6]:**
  - Anwendbar auf EaaS: ❌

**EmbMarker:**
- **Trigger Selektion:**
  - Zähle die Wortfrequenz auf einem allgemeinen Textkorpus \( D_p \)
  - Wähle zufällig \( n \) Wörter in einem moderaten-Frequenzintervall aus

**Wasserzeicheninjektion:**
- Definiere einen Zielembedding \( \mathbf{e_t} \)
- Zähle die Triggeranzahl in einer Zeile \( Q(S) = \min(|S \cap T|, m) \)
- Füge das Zielembedding auf das Originalembedding \( \mathbf{e_o} \) hinzu

**Urheberrechtsschutz:**
- **Konstruiere einen Rückgrat- und einen harmlosen Datensatz:**
  - \( D_h = \{[w_1, w_2, ..., w_m] | w_i \in T\} \)
  - \( D_n = \{[w_1, w_2, ..., w_m] | w_i \notin T\} \)
- **Forder die Embeddings vom Stealerdienst mit diesen Datensätzen**

**Experimentelle Ergebnisse:**
- **Datensätze:**
  - AG News, MIND, SST2, Enron Spam
- **Anbieter-Datensatz:**
  - WikiText
- **Metriken:**
  - Leistung auf Downstream-Aufgaben: ACC
  - Erkennungsleistung: \( \Delta_{cos}, \Delta_{L2}, p\)-Wert
- **Einstellung:**
  - \( m = 20 \), \( n = 4 \), Frequenzintervall = [0.005, 0.01]

**Experimental Results**
- **Performance Vergleich**
- **Embedding Visualisierung**

---

**Dankeschön!**</sample>
    <sample id="350">This presentation explores the concept of superhuman performance in natural language understanding (NLU) systems, critiquing the tendency to claim such capabilities based on leaderboard-based evaluations. It highlights that while systems excel in simple procedural tasks, most NLU tasks require knowledge and inference, leading to claims of superhuman abilities that may not be justified. The SuperGLUE benchmark is introduced as a well-known framework for evaluating general-purpose language understanding models, emphasizing the need for more reliable comparisons between models and humans. The presentation also discusses the importance of considering human baselines and the quality of training data, suggesting that current benchmarks might not fully capture the true capabilities of NLU systems.</sample>
    <sample id="351">This study investigates the performance of CoNLL-2003 named entity taggers in modern datasets, questioning their effectiveness beyond the original corpus. The researchers collected and annotated news from 2020 using CoNLL-2003 guidelines, fine-tuning over 20 models on this dataset. Evaluations on both CoNLL-2003 and the new dataset reveal significant performance drops, suggesting that factors like temporal drift and adaptive overfitting contribute to this decline. The study highlights the need for better model architectures, larger model sizes, and more fine-tuning examples to achieve good generalization.</sample>
    <sample id="352">ABC-Eval steht für Annotating Behaviors in Chat (ABC-Eval).</sample>
    <sample id="353">This research introduces an interactive approach to code generation by incorporating clarification questions and answers (CQAs) into the process, addressing the issue of underspecification in natural language descriptions. The study proposes a pipeline for generating code using CQAs, including a clarification need predictor, a question selector, and a code generator. It also presents a synthetic dataset, CodeClarQA, with clarifications on key operations. The pipeline outperforms existing models on datasets with only NLDs and demonstrates improved performance on human-annotated validation sets.</sample>
    <sample id="354">Bis zum Jahr 2018.</sample>
    <sample id="355">### Übersetzung des englischen Inhalts in Deutsch

---

**Slide 1: Titel**
- **Überschrift:** Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge
- **Autor:** Vasudha Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoan Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz
- **Institution:** Stony Brook University - Human Language Analysis Systems

**Slide 2: Übersicht**
- **Überschrift:** Was ist kognitive Dissonanz?
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)

**Slide 3: Definition von kognitiver Dissonanz**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Beispiel:** "Ich weiß, dass Zigaretten mich töten könnten, aber ich habe nach dem Meeting eine paar Zigaretten geraucht."

**Slide 4: Beispiele für kognitive Dissonanz**
- **Beispiel 1:** "Ich weiß, dass Zigaretten mich töten könnten."
- **Beispiel 2:** "Ich habe nach dem Meeting eine paar Zigaretten geraucht."
- **Beispiel 3:** "Ich denke, ich könnte mein Job ohne sie nicht behalten."

**Slide 5: Darstellung von kognitiver Dissonanz**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm mit einem Kopf und drei Sätzen, die die Dissonanz zwischen Überzeugung und Handlung zeigen.

**Slide 6: Gründe für kognitive Dissonanz**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramme zur Darstellung der Effekte von Unstimmigkeiten, Änderungen von Einstellungen und Angststörungen.

**Slide 7: Annotierungsprozess**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm zum Annotierungsprozess mit einem Beispieltext und einer Entscheidung über die Annotierung von Dissonanz.

**Slide 8: Trainingsprozess auf Initialannotierungsdaten**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm zum Trainingsprozess mit einem Beispieltext und einer Entscheidung über die Annotierung von Dissonanz.

**Slide 9: Methoden: Transfer und aktive Lernstrategie zur Annotation von seltenen Klassen**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm zur Erklärung der Transfer-Learning-Strategie und der aktiven Lernstrategie zur Annotation von seltenen Klassen.

**Slide 10: Cold-start-Annotationen: Transfer-Learning**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm zur Erklärung der Cold-start-Annotationen mit Transfer-Learning und der daraus resultierenden Verbesserung der AUC-Werte.

**Slide 11: Zusammenfassung**
- **Zitat:** "zwei Elemente der Kognition (d.h., Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)
- **Darstellung:** Diagramm zur Zusammenfassung der wichtigsten Punkte und der Empfehlungen für die Zukunft.

---

Dies ist eine Übersetzung des englischen Inhalts in Deutsch.</sample>
    <sample id="356">Saarland University und University of Amsterdam</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit den Architekturen "walk-k", "LA", "CAAT" und "EDAs" verglichen.</sample>
    <sample id="361">The research presented explores the challenge of compositional generalization in multi-step quantitative reasoning, focusing on financial data analysis. It introduces CounterComp, a method that uses counterfactual contrast to improve model performance by addressing the long-tail issue and enhancing accuracy for complex reasoning tasks. The study demonstrates CounterComp's effectiveness through improved program accuracy on both in-distribution and out-of-distribution samples, highlighting its potential to advance computational finance and quantitative reasoning systems.</sample>
  </task>
</testset>