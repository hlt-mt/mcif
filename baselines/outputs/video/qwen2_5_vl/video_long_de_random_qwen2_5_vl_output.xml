<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind News Media und Social Media (Reddit).</sample>
    <sample id="1">McGill University und Mila</sample>
    <sample id="2">Das Vortragende präsentiert "LayoutMask", ein neues Modell zur Verbesserung der Text-Layout-Interaktion in multi-modalen Vortrainingsmodellen für Dokumentenverstehen. Ziel ist es, die Lesereihenfolgeprobleme in visuell reichen Dokumenten zu adressieren. LayoutMask verwendet lokale 1D-Positionen anstelle von globalen Positionen und integriert innovative Maskierungsstrategien und Vortrainingsoptimierungen, um die Interaktion zwischen Text und Layout zu verbessern. Die Methodologie kombiniert Maskierte Sprachmodellierung und Maskierte Positionsmodellierung, wobei die Token-Einbettung, die lokale und segmentale 2D-Position sowie die Maskierungsstrategie für Wörter und Boxen berücksichtigt werden. Experimentelle Ergebnisse zeigen, dass LayoutMask im Vergleich zu anderen Modellen wie FUNSD und CORD-19 höhere F1-Werte erreicht, insbesondere bei lokaler Segmentierung.</sample>
    <sample id="3">### Übersetzung des englischen Inhalts

**Titel: DEPLAIN: Ein deutsches Parallelkorpus mit intralingualen Übersetzungen in einfache Sprache für Satz- und Dokumenteinfachstellung**

**Autoren: Regina Stodden, Omar Momen, Laura Kallmeyer**

**Institution: Heinrich Heine University Düsseldorf, Deutschland**

**Konferenz: ACL 2023**

---

**1. Texteinfachstellung: Was, warum und wie?**

Das Bild zeigt ein Beispiel für Texteinfachstellung:

**Original:**  
"Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."

**Einfachsprachliche Version:**  
"Die Gewerkschaft will zum Beispiel höhere Löhne."

---

**2. DE-plain: Ein neues Korpus**

Das Bild zeigt eine Grafik über die deutschen Texteinfachstellungskorpora auf der Ebene der Sätze.

---

**3. Anwendungsfallbeispiele: Automatische Ausrichtung und Einfachstellung**

Das Bild zeigt Ergebnisse der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:m-Fähigkeiten (untere Hälfte).

---

**4. Dankeschön**

Für weitere Details empfiehlt es sich, das Papier zu lesen. Es wird auch empfohlen, den Posterbesuch auf der ACL 2023-Konferenz zu besuchen.</sample>
    <sample id="4">Der/die Referent*in heißt Kayo Yin.</sample>
    <sample id="5">Das TS XL-Modell wurde verwendet, um eine Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">Das Video präsentiert ein Forschungsprojekt, das sich mit der Integration von mehrsprachiger und übersprachlicher Zusammenfassung befasst. Die Autoren, darunter Jiaan Wang, Fandong Meng, Duo Zheng und Yunlong Liang, stellen ihre Arbeit auf der Konferenz ACL 2023 vor. Sie beschreiben die Entwicklung eines Modells namens M2MS (Many-to-many Summarization), das in der Lage ist, Dokumente in beliebigen Quellensprachen zu verarbeiten und sie in beliebige Zielsprachen zu summarisieren. Das Projekt zielt darauf ab, die Kenntnisse zwischen verschiedenen Sprachen zu transferieren und bietet eine umfassendere Lösung als bisherige Ansätze für mehrsprachige und übersprachliche Zusammenfassung. Zudem wird ein prätrainierter M2MS-Modell namens PISCES vorgestellt, das durch drei Stufen des Vortrainings lernen kann: Meta-Vortraining, Übersprachliches Vortraining und Aufgabenbezogenes Vortraining. Die Ergebnisse zeigen, dass das M2MS-Modell im Vergleich zu anderen Ansätzen besser in der Lage ist, Kenntnisse zwischen verschiedenen Sprachen zu transferieren.</sample>
    <sample id="7">Yes!</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode ist neu in der Art, wie sie die Interaktion zwischen Mensch und Chatbot bewertet, indem sie verschiedene Aspekte wie Relevanz, Konsistenz und emotionale Verständigung berücksichtigt.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Qualität der verwendeten Validierungssamples ab.</sample>
    <sample id="10">Um das Ergebnis zu verbessern, könnte man mehr Annotatoren einbeziehen, um die Vielfalt der Perspektiven zu erhöhen und mögliche Fehler zu reduzieren. Zudem könnte man die Methode des Informalitäts-Beibehalts in der Datensammlung weiter optimieren, um die Authentizität und die natürliche Kommunikation zu fördern.</sample>
    <sample id="11">Das Video präsentiert ein Forschungsprojekt, das sich mit der Frage beschäftigt, ob künstliche Intelligenz (KI) die Fähigkeit hat, Humor zu verstehen und zu erzeugen. Es wird eine Studie vorgestellt, die auf den New Yorker Caption Contest basiert, einem Wettbewerb für Karikaturen mit Captions. Die Forscher untersuchen, wie KI-Modelle wie CLIP und OFA diese Aufgabe meistern können, indem sie Texte generieren, die mit den Bildern passen. Dabei werden verschiedene Benchmark-Scores wie Matching, Quality Ranking und Explanation Generation verwendet, um die Leistung der Modelle zu bewerten. Die Ergebnisse zeigen, dass die Modelle in der Lage sind, humorvolle Captions zu generieren, aber ihre Fähigkeit zur tieferen Verstehensfähigkeit des Humors noch beschränkt ist. Zudem wird eine neue annotierte Korpusdatenbank vorgestellt, die für weitere Forschungen genutzt werden kann. Das Video endet mit einer Einladung, mehr über das Projekt und die zugrunde liegenden Datenbanken zu erfahren.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="13">The presentation titled "Finding the SWEET spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" discusses strategies for optimizing machine learning models in resource-constrained environments. It highlights the variability in real-world data complexity, suggesting that low-capacity models can be used for simpler tasks to reduce inference costs. The talk compares two methods: Multi-Model (MM) and Early Exit (EE), noting that MM offers more versatility but is expensive to store and has overhead. EE, on the other hand, provides fast inference without overhead and is memory efficient, with shared model parameters across classifiers. The presentation introduces the hypothesis that conflicting gradients may degrade performance due to classifier optimization conflicts. It then presents results showing that Multi-Model models outperform Early Exit ones by 2.3% on average, with the largest gap observed for early classifiers. The SWEET method is introduced as a solution to separate weights in early exit transformers, avoiding conflicting gradients and improving performance. The final section summarizes key takeaways, including the existence of conflicting gradients in the training process, fair comparisons between EE and MM methods, and the potential for future research on fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">Natürlich, hier ist die Übersetzung des englischen Inhalts ins Deutsche:

---

**Titel: Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination**

**Autoren:** Adam Przepiórkowski und Michał Woźniak

**Institut:** Institute of Computer Science, Polish Academy of Sciences, ul. Jana Kazimierza 5, 01-248 Warschau

**Konferenz:** ACL 2023

### Abhängigkeitsstruktur von Koordination

- **Bouquet/Stanford (Universal Dependencies):**
  - Homer liebt Lisa, Bart und Maggie.
  
- **Kette/Moskau:**
  - Homer liebt Lisa, Bart und Maggie.

- **Konjunktion-gestützt/Prag:**
  - Homer liebt Lisa, Bart und Maggie.

- **Mehrfach-gestützt/London:**
  - Homer liebt Lisa, Bart und Maggie.

### Abhängigkeitslänge Minimierung (DLM)

Die Wortreihenfolge neigt sich dazu, die Abhängigkeitslängen zu minimieren:
- **Gute Reihenfolge:**
  - Marge liest gestern dieses absolut faszinierende Buch über Bienen.
- **Schlechte Reihenfolge:**
  - Marge liest gestern dieses absolut faszinierende Buch über Bienen.

### Konjunkt-Längen in Englisch

Statistiken zur Koordination extrahiert aus einer erweiterten Version des Penn Treebanks (Marcus et al. 1993, Ficler und Goldberg 2016):
- **Links konjunktionen tendieren zu kürzeren (beobachtet vorher).**
- **Diese Tendenz wächst mit der Längendifferenz (kurzfristig bemerkt in Gibson et al. 1996 88–90).**
- **Aber nur wenn der Regisseur auf der linken Seite fehlt (Ich sah Bart und Lisa; Homer kam und schnäuzte).**
- **Nicht, wenn er auf der rechten Seite ist (Ted und Ned lachten).**

---

Bitte beachten Sie, dass die Übersetzung einige formale Änderungen enthält, um den Kontext und die Struktur des Originaltextes zu bewahren.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="16">Die Domänen, die stärker vereinfacht werden, sind News, Bibel, L2 und Fiction.</sample>
    <sample id="17">This research introduces a novel approach to multimodal relation extraction, focusing on simultaneous information subtraction and addition for enhancing relation inference. It employs a graph information bottleneck principle for internal-information screening and latent multimodal topic modeling to enrich feature contexts. The system achieves significant improvements over existing best models on benchmark data, demonstrating the effectiveness of this method in handling mixed-modal inputs and improving task performance.</sample>
    <sample id="18">Das Beispiel lautet "Marge read it yesterday."</sample>
    <sample id="19">Das Vortragsthema konzentriert sich auf die Effizienz von Open-Domain Question Answering (ODQA). Es wird eine Übersicht über bestehende Systeme und Techniken zur Verbesserung der Leistung gegeben, einschließlich der Reduzierung der Indexgröße durch Dokumentenfiltern, Dimensionenreduktion und Produktquantisierung sowie der Verwendung leichtgewichtiger Modelle wie MobileBERT. Zudem werden Methoden wie Adaptive Computation (AC) vorgestellt, um die Lesegeschwindigkeit zu verbessern. Die Präsentation untersucht auch die Anwendung von Knowledge Distillation für die Optimierung von Modellgrößen und die Einführung eines One-Stage-Modells für mehrere Subtasks. Zum Abschluss werden Empfehlungen für die Wahl von Systemen basierend auf Ressourcenbeschränkungen und den Fokus auf die Auswirkungen von ODQA-Systemen auf Umweltfaktoren wie Energieverbrauch und Kohlendioxidemissionen diskutiert.</sample>
    <sample id="20">Ja, Sie können die Modelle für Ihre Forschung verwenden.</sample>
    <sample id="21">DEplain-APA enthält Dokumente, die in der APA-Stilrichtlinie formatiert sind.</sample>
    <sample id="22">Für eine gute Generalisierung sind bessere Modellarchitekturen, größere Modellgrößen und mehr Feinabstimmungsbeispiele erforderlich.</sample>
    <sample id="23">Das Video präsentiert die Forschungsergebnisse von Google Research, die sich mit der Verbesserung der visuellen Textdarstellung durch charakterbewusste Modelle befasst. Es wird gezeigt, wie Text zu Bildern umgewandelt werden kann, wobei ein Textencoder und ein Text-zu-Bild-Modell verwendet werden. Die Präsentation fokussiert auf die Funktionsweise von Text-zu-Bild-Modellen wie Imagen und die Herausforderungen bei der Texttokenisierung, die die korrekte Schreibweise verdecken kann. Es wird auch untersucht, wie die Schreibfähigkeiten von Textencodern mit zunehmender Skalierung verbessert werden können. Die Studie zeigt, dass charakterbewusste Textencoder in allen Skalen gut schreiben können und dass subword-basierte Encoder von der Wortfrequenz beeinflusst werden. Zudem wird vorgeschlagen, die subword- und charakterebenen Textcodierungen zu kombinieren, um die Textdarstellung zu verbessern.</sample>
    <sample id="24">Die Tendenz wurde durch die Längendifferenz zwischen den Konjunktionen gemessen, wobei kürzere Konjunktionen auf der linken Seite tendieren.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass sie verschiedene Positionen des Begrenzers in einem Satz testeten, um die Abhängigkeitslängen und Strukturen zu untersuchen.</sample>
    <sample id="26">Ein Basisklassifikator kann schlecht performieren, wenn er mit unausgewogenen Daten trainiert wird.</sample>
    <sample id="27">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen "Easy on Me" (von Adele) und "I Gotta Feeling" (von The Black Eyed Peas).</sample>
    <sample id="29">Formalität und lexikalische Cohesion.</sample>
    <sample id="30">Das Video präsentiert ein neues Ensemble-Learning-Framework namens LLM-Blender, das speziell für Large Language Models (LLMs) entwickelt wurde. Es kombiniert zwei Module: den PairRanker und den GenFuser. Der PairRanker nutzt Paarweise-Ranglistenverfahren, um die Leistung von LLMs zu vergleichen und zu bewerten, während der GenFuser die besten Kandidaten aus den Ranglisten fusioniert. Das Framework wird durch eine Vielzahl von LLMs getestet, die in einer automatischen Bewertung von Anweisungsfolge-Modellen (AlpacaEval) aufgeführt sind. Die Ergebnisse zeigen, dass LLM-Blender die Leistung von einzelnen LLMs verbessern kann, indem es die optimalen Modelle für verschiedene Beispiele identifiziert. Zusätzlich wird MixInstruct vorgestellt, ein Benchmark-Dataset zur Evaluierung von Ensemble-Learning für LLMs, das 110k Instruction-Following-Beispiele enthält. Die Studie zeigt, dass LLM-Blender die Best-Of-Average-Performance von 76.53% erreicht, was einen signifikanten Verbesserungsgrad gegenüber den Baselinien darstellt.</sample>
    <sample id="31">Die Autoren gehören an Johns Hopkins University, Purdue University und MIT.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität durch die Anwendung von Pearson's R Scores zur Vergleichung der Annotierungen nach dem Demografie- und Modellvergleich.</sample>
    <sample id="34">Das Video präsentiert CREST, ein Framework zur Kombination von rationalisierter und counterfactualer Textgenerierung. Es wird erklärt, wie man die Entscheidungen eines Klassifiers erläutern kann, indem man selektive Rationalisierung und counterfactual Generation kombiniert. CREST nutzt einen Trainable Masker, um relevante Teile des Textes zu identifizieren und einen Predictor, um die Klassifikation vorherzusagen. Die Ergebnisse werden durch Experimente auf IMDB und SNLI bestätigt, wobei CREST-Generierte Erklärungen sowohl plausibel als auch kausal sind. Das Framework ermöglicht es, diverse und kontrollierte Perturbationen zu generieren, was zu überzeugenden Erklärungen führt. CREST erreicht hohe Simulabilität und übersteigt andere Methoden in Bezug auf Plausibilität und Validität.</sample>
    <sample id="36">Das Video präsentiert ein Forschungsprojekt zur Verbesserung der Multilingualen Maschinensprachübersetzung (MLMT). Es wird die Methode vorgestellt, wie spezifische Schichten für jede Sprache in einem Transformer-Modell verwendet werden können, um die Übersetzungsgenaugkeit zu erhöhen und gleichzeitig die Ressourcen effizienter zu nutzen. Die Präsentation hebt die Vorteile von MLMT hervor, darunter Skalierbarkeit, Geschwindigkeit, weniger Fehlerkaskaden und verbesserte Leistungen bei geringeren Ressourcen. Zudem werden die Herausforderungen wie begrenzte Kapazität pro Sprache und konstante Inference-Kosten diskutiert. Das zentrale Konzept ist, die Sprachspezifischen Schichten (LSLs) entweder am Quell- oder Zielsprachkontext zu indexieren, um die Übersetzungsschritte an den relevanten Kontexten auszurichten. Die Präsentation zeigt auch, wie das Modell lernt, wo welche LSLs verwendet werden sollten, und wie dies die Effizienz und Genauigkeit der Übersetzung steigert. Experimentelle Ergebnisse auf der WMT21 News Translation Task zeigen signifikante Verbesserungen in 84 von 90 Übersetzungsrichtungen, was die Effektivität der Methode bestätigt.</sample>
    <sample id="37">Die vorherige Studie mit menschlichen Teilnehmenden zeigte, dass sie die gleichen Persona-Prompts erhielten.</sample>
    <sample id="38">Die Datenquellen für diese Studie umfassen die Penn Treebank und eine erweiterte Version des Penn Treebanks.</sample>
    <sample id="39">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind "Effects of disagreement", "Attitudes and Belief trends" und "Anxiety disorders".</sample>
    <sample id="41">Das Video präsentiert ein Forschungsprojekt, das sich mit der Erstellung konsistenter und einleuchtender Narrativen durch die Verwendung von PeaCoK, einem weltweiten Persona-Konsenswissen-Knowledge Graphen, befasst. Die Studie zeigt, dass PeaCoK eine Vielzahl von hochwertigen konsensuellen Annahmen enthält, die für die Generierung von Persönlichkeitsinhalten verwendet werden können. Diese Annahmen umfassen Charakteristika, Routinen, Ziele, Pläne, Erfahrungen und Beziehungen, die für die Erstellung von narrativen Dialogen relevant sind. Das Projekt verwendet ein dreistufiges Konstruktionsverfahren, um diese Annahmen zu generieren, wobei ein KI-basierter Ansatz und eine Mischung aus humaner und maschineller Evaluierung verwendet wird. Ergebnisse zeigen, dass PeaCoK die Qualität von Personenaufnahmen verbessert und die Konsistenz und den Austausch zwischen Interlocutoren erhöht. Zudem ermöglicht es die Nutzung von leichtgewichtigen Sprachmodellen, um ähnliche Fähigkeiten wie bei großen Sprachmodellen zu erlernen.</sample>
    <sample id="42">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="43">Es gibt sieben Autoren an der Arbeit beteiligt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich durch seine Fokussierung auf die Positionalität in NLP, indem es Designbiastests durchführt und dabei die Perspektive der Annotatoren berücksichtigt. Es verwendet auch spezifische Techniken wie die Pearson's R-Analyse für die Vergleichbarkeit der Annotierungen.</sample>
    <sample id="45">Das Setup "GPT-4 PBlack" hat die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Das Video zeigt eine Präsentation mit dem Titel "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" auf der Konferenz ACL 2023. Die Präsentation behandelt die Herausforderungen und die Rolle von Prätrainingsdaten bei der Entwicklung von Sprachmodellen, insbesondere im Kontext politischer Biass und ihrer Auswirkungen auf die Fairness von NLP-Anwendungen.

Die Präsentation beginnt mit einer Übersicht über die Autoren und ihre Institutionen, darunter Shangbin Feng, Chan Young Park, Yuhan Liu und Yulia Tsvetkov. Es wird betont, dass die Daten für die Sprachmodelle aus einer Vielzahl von Quellen stammen, wie z.B. Wikipedia, Google, CNN, NY Times usw., was sowohl Vorteile als auch Nachteile hat.

Es folgt eine Darstellung des Prozesses, wie Prätrainingsdaten zu Sprachmodellen und schließlich zu spezifischen Aufgaben führen. Die Präsentation diskutiert Fragen wie die Bewertung der politischen Neigung von Sprachmodellen, die Rolle der Prätrainingsdaten in politischen Bias und wie unterschiedliche politische Neigungen von Sprachmodellen in NLP-Anwendungen zu Unfairness führen können.

Weitere Aspekte der Präsentation beinhalten Methoden zur Evaluierung der politischen Neigung von Sprachmodellen, wie das politische Kompass-Test und die Unterstützung sowohl für Encoder- als auch Decoder-Sprachmodelle. Es wird gezeigt, wie Sprachmodelle auf politische Fragen reagieren und wie diese Reaktionen politische Neigungen widerspiegeln können.

Die Präsentation verwendet Diagramme und Visualisierungen, um die Ergebnisse der Untersuchung zu veranschaulichen. Es wird gezeigt, wie verschiedene Sprachmodelle auf politische Fragen reagieren und wo sie sich in einem politischen Spektrum positionieren. Es wird auch ein Vergleich zwischen verschiedenen Sprachmodellen vorgenommen, um zu zeigen, wie ihre politische Neigung sich ändern kann.

Abschließend wird eine Diskussion geführt, in der die Herausforderungen und die Notwendigkeit der Entscheidung zwischen Sanierung und Nicht-Sanierung von Sprachmodellen dargestellt werden. Die Präsentation endet mit einem Dank an die Zuhörer und den Autoren.</sample>
    <sample id="48">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="49">Bis zu 900 Tokens.</sample>
    <sample id="50">Das Video präsentiert das Projekt "DEPLAIN", ein deutsches Parallelkorpus, das intralinguale Übersetzungen in einfache Sprache für Satz- und Dokumenteinfachstellung bietet. Die Autoren Regina Stodden, Omar Momen und Laura Kallmeyer von der Heinrich Heine Universität Düsseldorf stellen die Motivation und den Prozess der Texteinfachstellung vor, wobei sie verschiedene Methoden wie Substitution, Klauselentfernung, Wortentfernung und Umordnung erläutern. Sie diskutieren auch die Herausforderungen bei der Erstellung eines solchen Korpus und die Vorteile, die es bietet, wie bessere Zugänglichkeit und Verständlichkeit von Texten. Darüber hinaus wird eine neue Korpusdatensatz namens DE-plain vorgestellt, der aus verschiedenen Quellen wie Zeitungen, Büchern und Webseiten besteht. Der Fokus liegt auf der Analyse der verschiedenen Einfachstellungsarten und -transformationen, die im DE-plain-Korpus enthalten sind. Schließlich werden Anwendungsfälle für automatische Ausrichtung und Einfachstellung besprochen, einschließlich der Evaluation von Alignment-Methoden.</sample>
    <sample id="51">Die Domains, die im Datensatz enthalten sind, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="53">The speaker's name is Xiaoyu Shen.</sample>
    <sample id="54">This research addresses the challenge of detecting cognitive dissonance, a rare class in text data, by leveraging transfer learning and active learning strategies. It introduces a method that combines a pre-trained model with human annotation to improve dissonance detection accuracy. The study evaluates various active learning strategies, finding that the Probability-of-Rare-Class (PRC) strategy outperforms others in terms of AUC and efficiency. The findings highlight the importance of rare-class annotations for better model performance and suggest PRC as an effective approach for annotating such classes.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell ohne Neutrainieren oder Anpassung der Architektur für Simultaneous Speech Translation (SimuST).</sample>
    <sample id="56">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="57">Das getestete Modell in der Testsuite funktioniert nicht, da es viele Fehler macht, wie z.B. "Who is John" und "Who is the new president".</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:
1. Background-Pretrain
2. Background-Both
3. Background-Inference</sample>
    <sample id="59">DrBERT is a robust pre-trained model in French designed for biomedical and clinical domains, achieving state-of-the-art results on nine downstream French medical-oriented tasks. It surpasses generic models like CamemBERT and English-based domain-specific models, confirming the utility of training a medical-specific model in French. The model's performance is enhanced by using a heterogeneous data set from various medical domains, which is more robust than private clinical data. Continual pre-training with existing fine-tuned models, such as CamemBERT and PubMedBERT, further improves its effectiveness when based on domain-specific English models. The NACHOS dataset and training scripts are freely available under the MIT license.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">How to use the available clean samples more efficiently?</sample>
    <sample id="62">Das Video präsentiert ein Systematisches Studium zur Kenntnisverdünnung für natürliche Sprachgenerierung (NLG) mit Pseudo-Ziel-Training. Es wird die Notwendigkeit der Verdünnung von großen Sprachmodellen in der Industrie hervorgehoben, um die Ressourcenbelastung zu reduzieren, während die Leistung erhalten bleibt. Die Präsentation untersucht verschiedene Methoden der Modellverdünning und -verdünnung, einschließlich der Prüfung weniger informativer Parameter und der Kenntnisverdünnung durch das Transferieren von Wissen von großen Lehrern zu kleinen Studenten. Es wird auch auf die Herausforderungen bei der Verwendung von unlabeled Daten und die Bedeutung realistischer Szenarien für NLG-Praktiker eingegangen. Das Studium konzentriert sich auf task-spezifische Kenntnisverdünnung und berücksichtigt eine Vielzahl von NLG-Aufgaben in realistischen Umgebungen.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst, wie sensibel ein Modell ist für die Vielfalt von Anweisungen für dieselbe Aufgabe, indem es die Fähigkeit des Modells überprüft, konstante Ergebnisse für dieselbe Aufgabe zu erzeugen, unabhängig von leichten Variationen im Wortlaut der Anweisungen.</sample>
    <sample id="64">Wenjun Peng</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell weniger robust ist und leichter durch geringe Veränderungen in den Anweisungen beeinflusst wird. Daher ist ein niedrigerer Wert für die Sensitivität besser, da dies eine bessere Leistung des Modells andeutet.</sample>
    <sample id="66">This presentation explores the application of deep learning in mathematical reasoning, focusing on solving math word problems and geometry problems. It highlights advancements in automated theorem proving and the use of large language models (LLMs) for complex reasoning tasks. The study examines how LLMs can be enhanced with tools to perform sophisticated mathematical operations, addressing limitations like inconsistency with large numbers. Additionally, it discusses the role of chain-of-thought prompting in improving model performance and the potential of program-aided LLMs for plug-and-play compositional reasoning.</sample>
    <sample id="67">Das Video präsentiert ein Forschungsprojekt zur Untersuchung von Interferenz und Synergie in multilingualen Übersetzungsmustern. Es wird diskutiert, wie Modelle von Synergie zwischen Sprachpaaren profitieren können, aber auch von Interferenz leiden. Die Studie identifiziert Hauptfaktoren wie Modellgröße, Datengröße und die Anzahl anderer Sprachen, die zu Interferenz beitragen. Es wird gezeigt, dass bei kleinen Modellen und geringer Datenmenge die Interferenz stark ist. Die Verwendung von Temperatursampling als Baseline zur Reduzierung der Interferenz wird empfohlen. Die Ergebnisse deuten darauf hin, dass die Dominanz von Modellgröße, Datengröße und anderen Sprachdaten für die Interferenz entscheidend ist.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen kontextuellen Kontext, der aus verschiedenen Quellen wie Wikipedia und SLAMP zusammengesetzt ist.</sample>
    <sample id="69">Nur einige saubere Validierungsbeispiele sind notwendig, um eine gute Leistung an der WSL zu erzielen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">Das Video präsentiert ein Forschungsprojekt von Google Research, das sich mit der Aufgabe beschäftigt, indirekte Bezugnahme ausdrücke für die Auswahl von Entitäten zu lösen. Der Fokus liegt auf der Erstellung eines Korpus namens AltEntities, das 6.000 alternative Fragen und 42.000 indirekte Bezugnahme ausdrücke umfasst. Ziel ist es, die Sprachverarbeitung bei der Verständigung von Benutzern zu verbessern, insbesondere wenn sie ihre Wahl in natürlicher und flüssiger Konversation ausdrücken. Indirekte Bezugnahme ausdrücke können schwierig zu interpretieren sein, da sie oft den Namen einer Entität vergessen oder die Aussprache schwer zu unterscheiden sind. Das Projekt verwendet eine Methode zur Datensammlung, die auf einer Cartoon-Fertigstellungsaufgabe basiert, um Informalität zu betonen und Dialogkontexte zu definieren. Die Annotatoren werden dazu aufgefordert, mindestens einige Songs zu hören und über sie zu lesen, um eine umfassende Kenntnis der Musik zu erlangen. Ergebnisse zeigen, dass Modelle mit gleicherem Hintergrundwissen eine Genauigkeit von 92-95% erreichen, während Modelle mit nur dem Namen der Entität eine Genauigkeit von 82-87% haben. Die Modelle sind generell anwendbar auf verschiedene Domänen.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um sicherzustellen, dass die Inhalte in den Medien korrekt dargestellt werden und keine Verzerrungen oder Fehlinterpretationen enthalten.</sample>
    <sample id="73">Martin Pömsi</sample>
    <sample id="74">The presentation discusses the construction of a densely-connected commonsense knowledge graph, Dense-ATOMIC, aimed at enhancing knowledge coverage and multi-hop paths. It highlights the importance of commonsense knowledge in machine-human interaction and introduces ATOMIC as a large-scale commonsense knowledge base. The study proposes a new CSKG completion method to infer missing links on ATOMIC, addressing limitations such as sparse graph structure and insufficient semantic information utilization. The method involves normalizing tail events, training a relation prediction model, and employing an intra-and-inter cluster completion strategy. Evaluation results show that Dense-ATOMIC outperforms traditional methods in terms of knowledge coverage and multi-hop paths, with a focus on commonsense reasoning.</sample>
    <sample id="75">The presentation introduces "Jointprop," a joint semi-supervised learning framework for entity and relation extraction using heterogeneous graph-based propagation. It addresses the challenges of fully supervised models, which require extensive labor and diverse annotated data, by employing semi-supervised learning with limited labeled data to achieve powerful models at lower costs. The study highlights the interconnections between named entity recognition (NER) and relation extraction (RE), emphasizing the necessity to consider connections among labeled and unlabeled data. The proposed framework models NER and RE tasks through label propagation over heterogeneous graphs, considering both inter- and intra-interactions among labeled and unlabeled data. The methodology includes span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The framework is evaluated on four datasets: SciERC, ACE05, SemEval, and CoNLL, demonstrating its effectiveness in improving performance compared to baseline methods.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit dem Pretraining auf einer Mischung von Daten, die sowohl positive als auch negative politische Inhaltselemente enthalten können. Diese Daten werden dann in Language Models (LMs) übertragen, die die politischen Vorurteile weiterverbreiten. Schließlich werden diese LMs in Downstream Tasks eingesetzt, wo sie ihre politischen Vorurteile in der Ausführung von Aufgaben wie Hate Speech Detection oder Misinformation Detection einbringen.</sample>
    <sample id="77">The presentation discusses the research on improving summarization factual consistency using natural language feedback. It introduces a new dataset, DeFacto, which includes human demonstrations and feedback to enhance factual accuracy in summaries. The study covers contributions such as a comprehensive dataset analysis, baseline models for natural language generation tasks, and insights into factual error correction with feedback prediction. The background explains abstract text summarization and the importance of factual consistency, where all information in the summary must be supported by the input document. The new dataset is designed to collect human demonstrations and feedback to improve the consistency of system-generated summaries. The dataset includes labels for factual errors and instructions for annotators to explain and provide evidence for corrections. Data collection details involve news articles from XSum, short summaries, and a large pretrained Encoder-Decoder model named Pegasus. The dataset's statistics show that more than 71% of reference summaries contain factual errors, with intrinsic and extrinsic errors being common. The analysis of editing instructions reveals that removing and replacing information are the most frequent operations, while intrinsic errors require more diverse editing actions. The study also explores NLG tasks like summary editing, feedback generation, and automatic factual error correction, highlighting the effectiveness of the proposed methods in improving factual consistency.</sample>
    <sample id="78">Nein, der Prozess unterscheidet sich nicht.</sample>
    <sample id="79">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird durch die Zufallsauswahl von n Wörtern aus einem moderaten Frequenzintervall in einem allgemeinen Textkorpus Dp definiert, die dann auf die Originalembedding des Dienstes des Anbieters addiert werden.</sample>
    <sample id="81">PennState</sample>
    <sample id="82">Das Vortragende präsentiert ein neues Framework namens ULRA (Unsupervised Learning from Rank Aggregation) für die automatisierte Bewertung von Essays unter unsupervisierter Bedingung. Es zielt darauf ab, mehrere heuristische Qualitätssignale als pseudo-Grundwahrheit zu aggregieren und daraus einen neuronalen AES-Modell zu trainieren. Dieses Framework adressiert Konflikte zwischen verschiedenen Signalen und bietet eine einheitliche Supervision durch die Verwendung einer tiefen Paarweise Rangaggregationsverlustfunktion. Die experimentellen Ergebnisse bestätigen die Effektivität von ULRA für die automatische Bewertung von Essays ohne menschliche Intervention.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training in einer Mischung von verschiedenen Sprachen verbessert werden.</sample>
    <sample id="84">Das Video präsentiert ein neues Framework namens PAD-Net, das speziell für dynamische Netzwerke entwickelt wurde. Es unterscheidet sich von herkömmlichen statischen Netzwerken durch die Möglichkeit, dynamische Parameter zu verwenden, um die Effizienz und Leistungsfähigkeit zu verbessern. Die Präsentation zeigt, wie PAD-Net durch die Verwendung eines Mischmodells aus Experten und einer dynamischen Konvolution effizienter ist als statische Netzwerke. Es wird auch gezeigt, dass PAD-Net in verschiedenen Aufgabenbereichen wie NLP und CV signifikante Verbesserungen erzielt, während weniger Rechenressourcen benötigt werden. Zudem wird die Flexibilität der Architektur hervorgehoben, indem sie verschiedene dynamische Mechanismen unterstützt und die Kombination von statischen und dynamischen Parametern ermöglicht. Das Video schließt mit einem Überblick über zukünftige Arbeiten, die die Ausrichtung auf Hardware-freundliche Strukturen und die Anwendung auf andere Hauptnetzwerke ausdehnen sollen.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Anleitung, wie man einen Kuchen backt.</sample>
    <sample id="86">Die Opazität der Methode wird durch die Verwendung eines Backdoor-Wasserzeichens sichergestellt, das nicht die Nutzbarkeit der bereitgestellten Embeddings beeinträchtigt.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, indem sie sie als Ausgangspunkt für die kontinuierliche Vorbereitung verwendet.</sample>
    <sample id="88">GPT-4 ist am wenigsten mit West-Südostasien ausgerichtet.</sample>
    <sample id="89">The example sentence that demonstrates how the model leverages knowledge acquired through the attention mechanism is "I am going to talk about climate."</sample>
    <sample id="90">Das Vortragende präsentierte seine Arbeit auf dem ACL 2023, die sich mit der Frage beschäftigt, ob Sprachlerner zur Datenannotation beitragen können. Es wird betont, dass die Recruitierung von Muttersprachlern für die Annotation von Daten herausfordernd ist, während es viele Sprachlerner gibt. Die Studie untersuchte, ob durch die Recruitierung von Sprachlern neben Muttersprachlern die Annotator-Pool erweitert werden kann. Kontrollierende Variablen wie Sprache (Englisch, Koreanisch, Indonesisch), Aufgabe (SA, NLI, NER, MRC), Sprachkenntnisse (Grundkenntnisse, Mittelkenntnisse, Fortgeschrittenes, Muttersprachler) und Schwierigkeit der Fragen wurden definiert. Zudem wurden Ressourcen wie ein Wörterbuch und eine Maschinentranslations-System zur Verfügung gestellt. Der Workflow umfasste einen Vorfragebogen, eine Experimentphase mit Annotationen und einem Nachfragebogen. Ergebnisse zeigten, dass die Labels von Sprachlern nahezu genauso genau waren wie die von Muttersprachlern. Zusätzliche Experimente ergaben, dass die Aggregation der Labels von Sprachlern fast den gleichen Grad an Genauigkeit erreichte wie die von Muttersprachlern. Darüber hinaus verbesserten NLP-Annotationen die Profizienz der Lerner im Bereich Vokabular und Grammatik.</sample>
    <sample id="91">Die Anzahl der Aufgaben beeinflusst die aggregierte Leistung und die Sensitivität des Modells. Mehr Aufgaben führen zu höherer aggregierter Leistung und geringerer Sensitivität.</sample>
    <sample id="92">seq2seq, seq2tree, tree2seq</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, stehen dem ersten Autor Matthias Lindemann als Kollegen oder Mitarbeiter in einem Forschungsprojekt oder einer wissenschaftlichen Institution nahe.</sample>
    <sample id="94">The presentation discusses the protection of copyright for large language models (LLMs) offered as a service (EaaS). It highlights the exceptional capabilities of LLMs in natural language understanding and generation, with examples like GPT, LLaMA, and PALM. The focus is on embedding services that assist various NLP tasks, such as OpenAI's GPT-based embedding API. The challenge of protecting the copyright of these services from attackers stealing models through embeddings is addressed. The solution proposed is EmbMarker, which involves trigger selection based on word frequency and watermark injection by adding target embeddings to the original ones. The system also includes copyright verification steps using datasets and computing metrics like similarity difference and p-value of KS test. Experimental results demonstrate the effectiveness of EmbMarker in detecting stolen services, with performance metrics like accuracy and detection performance values provided.</sample>
    <sample id="95">Chowdery et al., 2022</sample>
    <sample id="96">Natürlich, hier ist der englische Inhalt sinngemäß auf Deutsch:

---

**Titel: NLPositionality: Characterizing Design Biases of Datasets and Models**

**Inhalt:**

1. **Einleitung**
   - Präsentation der Arbeit "NLPositionality: Characterizing Design Biases of Datasets and Models" von fünf Autoren.
   - Diskussion über die Positionalität in NLP und wie sie sich auf die Ergebnisse von Modellen auswirkt.

2. **Positionalität**
   - Definition von Positionalität als das Ergebnis von Demografie, Identität und Lebenserfahrungen.
   - Wie Positionalität die Forschung beeinflusst und ihre Ergebnisse.

3. **Fragestellung**
   - Frage: Haben Datensätze und Modelle Positionalität?
   - Anzeichen für Positionalität:
     - Modell- und Datensatzprobing
     - Theoretische Definitionen von Modellpositionalität

4. **Forschungsfrage**
   - Fragestellung: Gibt es Positionalität in NLP-Datensätzen und -Modellen?

5. **Methodik**
   - Re-Annotieren von Datensätzen mit diversen Annotatoren.
   - Vergleich von Annotierungen durch demografische Gruppen und Modelle/Datensätze mittels Pearson's R-Werte.

6. **Studie**
   - Teilnehmer: 16.299 Annotatoren aus 87 Ländern.
   - Ziel: Vergleich von Annotierungen zwischen Nutzern und bestehenden Datensätzen und Modellen.

7. **Ergebnisse**
   - **Ergebnis 1**: Es gibt Positionalität in NLP.
   - **Ergebnis 2**: Einige Bevölkerungsgruppen werden zurückgelassen.
     - Daten und Modelle sind am meisten mit englischsprachigen Ländern ausgerichtet.
     - Daten und Modelle sind weniger mit nicht-binären Personen ausgerichtet.

8. **Empfehlungen**
   - Erhalte einen Protokoll aller relevanten Designentscheidungen während der Datensatz- oder Modellentwicklung.
   - Führe NLP-Forschung aus der Perspektive der Perspektivismus:
     - Teile disaggregierte Dataset-Etiketten.
     - Verwende Modellierungstechniken, um annotatorische Unstimmigkeiten zu handhaben.
   - Erstelle spezialisierte Datensätze und Modelle für spezifische Gemeinschaften (z.B. Masakhane Initiative).

9. **Dank**
   - Danke für die Aufmerksamkeit.
   - Weitere Informationen finden Sie auf der Website: [Link zur Website].

---</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen zu reduzieren, ist es wichtig, auf verschiedene Arten von Daten zurückzugreifen, wie beispielsweise News Media und Social Media, um eine breitere Palette von Perspektiven und Meinungen zu berücksichtigen. Darüber hinaus kann das Trainieren auf verschiedenen politischen Quellen helfen, um die Vielfalt der Meinungen zu fördern und mögliche Verzerrungen zu minimieren.</sample>
    <sample id="99">### Zusammenfassung und Schlussfolgerungen

- **Etablieren des konstruktiven Sprachplanungsproblems**: Das Konstruktive Sprachplanungsproblem wurde definiert, um die Fähigkeit von Sprachmodellen zu evaluieren, spezifische Ziele unter Berücksichtigung von Constraints zu generieren.
- **Evaluierung der konstruktiven Sprachplanungsfähigkeit von Sprachmodellen**: Die Fähigkeit von Sprachmodellen, spezifische Ziele unter Berücksichtigung von Constraints zu generieren, wurde evaluiert.
- **Entwicklung eines über-generieren-dann-filtern-Methode für Sprachmodelle**: Eine Methode zur Verbesserung der konstruktiven Sprachplanungsfähigkeit von Sprachmodellen wurde entwickelt.
- **Verwendung von Sprachmodellen zum Generieren eines hochwertigen Skriptdatensatzes (CoScript)**: Sprachmodelle wurden verwendet, um einen hochwertigen Skriptdatensatz (CoScript) für konstruktive Sprachplanung zu generieren.
- **Limitationen und zukünftige Arbeiten**:
  - Die vorgeschlagene Methode ist ein post-hoc-Ranking-Ansatz.
  - CoScript-Datensatz erbt nur eine zusätzliche Beschränkung von einem abstrakten Ziel.
  - Der CoScript-Datensatz kann als wertvolles Ressourcenmaterial dienen, um das Forschungsgebiet des konstruktiven Sprachplanungs mit komplexeren und vielfältigeren Zielen und Beschränkungen voranzutreiben.

### Zusammenfassung

Das Ziel dieser Arbeit war es, das konstruktive Sprachplanungsproblem zu etablieren, die Fähigkeit von Sprachmodellen zu evaluieren und eine Methode zur Verbesserung der konstruktiven Sprachplanungsfähigkeit von Sprachmodellen zu entwickeln. Es wurde gezeigt, dass Sprachmodelle mit einer speziellen Methode verbessert werden können, um höhere Qualitätsskripte zu generieren. Der CoScript-Datensatz wurde als wertvolles Ressourcenmaterial identifiziert, das das Forschungsgebiet des konstruktiven Sprachplanungs weiterentwickeln kann.</sample>
    <sample id="100">The presentation discusses the use of language models for few-shot reranking in multi-hop question answering (QA). It introduces PromptRank, an approach that combines unsupervised retrieval methods with few-shot LM-based rerankers. The main idea is to leverage the ability of language models to reason over documents and answer questions by constructing chain prompts from retrieved documents. The method involves two main steps: first, retrieving candidate chains using TF-IDF retrieval and hyperlink traversal, and second, reranking these chains using a few-shot LM reranker. The scoring function used is the likelihood of the question given the chain according to the LM. The presentation also covers additional techniques such as instruction search and ensembling, which help in finding optimal instructions and improving chain scores. Experiment details include the use of language models like GPT2-XL and T5-XL, and the evaluation on the HotpotQA dataset. The results show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art methods.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit der von SOTA-Systemen, was bedeutet, dass es eine vergleichbare Fähigkeit hat, verschiedene Sprachen zu verstehen und zu übersetzen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind Anwendbarkeit für EaaS, Nutzen (ohne Degradation der bereitgestellten Embeddings), Verdecktheit (unsichtbar für den Angreifer) und Transferierbarkeit (kann auf die Angreiferdienste übertragen werden).</sample>
    <sample id="103">Die englischen TED Talks wurden in folgende 14 Sprachen übersetzt: Deutsch, Französisch, Spanisch, Italienisch, Japanisch, Koreanisch, Niederländisch, Portugiesisch, Rumänisch, Russisch, Türkisch, Chinesisch, Arabisch und Hebräisch.</sample>
    <sample id="104">100 Instanzen werden aus einem Datensatz für die erneute Annotierung extrahiert.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind der Ähnlichkeitsunterschied (similarity difference) und der p-Wert des Kolmogorov-Smirnov-Tests (p-value of KS test).</sample>
    <sample id="106">Das Video präsentiert das Projekt "QUEST", ein Retrieval-Dataset, das speziell für die Entitätssuche mit impliziten Satzoperationen entwickelt wurde. Es wird erklärt, wie Personen ihre Informationsbedürfnisse oft mit mehreren Restriktionen oder Vorlieben ausdrücken, was zu Queries führt, die implizite Satzconstraints enthalten. Die Motivation liegt in der Untersuchung von Systemen zur Behandlung solcher selektiven Informationsbedürfnisse. Das Dataset "QUEST" umfasst 3357 Suchanfragen und bietet eine Vielzahl von Anwendungen, einschließlich der Überprüfung der Relevanz der Antwortentitäten und der Markierung von zitierten Abschnitten in Dokumenten. Der Vortrag beschreibt auch den Aufbau des Datasets, wobei Wikipedia-Kategorien aus vier Domänen (Filme, Bücher, Pflanzen, Tiere) verwendet werden, um Satzoperationen durchzuführen. Die Annotatoren paraphrastieren vordefinierte Vorlagen und bewerten die Fluideität und Natürlichkeit der Queries. Das Projekt zielt darauf ab, die Effektivität von Retrieval-Systemen zu verbessern, indem es komplexe Suchanfragen mit mehreren impliziten Satzconstraints bearbeitet.</sample>
    <sample id="107">Die Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in der Aufgabe als Multilingual Model eingesetzt.</sample>
    <sample id="108">Das Video präsentiert Ergebnisse einer Studie, die zeigt, dass Sprachmodelle nicht immer robust gegenüber Kontext sind. Es untersucht den Minimal Pair Paradigm (MPP) und wie er verwendet wird, um die abstrakte Kenntnis von Sprachmodellen zu evaluieren. Die Studie zeigt, dass MPP-Evaluierungen mit kurzen, einzelnen Sätzen nicht vollständig die abstrakte Kenntnis der Modelle erfassen. Es werden verschiedene Methoden vorgestellt, um die Stabilität dieser Bewertungen zu testen, insbesondere durch Verwendung unterschiedlicher Kontextlängen, struktureller Übereinstimmung und Akzeptabilität. Die Ergebnisse deuten darauf hin, dass Matched Prefixes die Sensibilität der Modelle beeinflussen können, indem sie relevante Struktur in den Kontexten bewahren. Schlussendlich werden Schlussfolgerungen gezogen, dass Sprachmodelle sensibel auf latente syntaktische/semantische Merkmale in Sätzen reagieren und dass MPP-Evaluierungen mit kurzen Sätzen nicht ausreichen, um die abstrakte Kenntnis der Modelle vollständig zu erfassen.</sample>
    <sample id="109">Das Video präsentiert ein neues Dataset namens "Unnatural Instructions", das aus über 240.670 natürlichen Sprachanweisungen besteht, die für eine breite Palette von natürlichen Sprachtasks verwendet werden können. Diese Anweisungen wurden vollständig automatisch gesammelt, wobei nur 15 manuell erstellte Beispiele als Sammelbasis dienten. Die Datenanalyse fokussierte auf Kreativität, Vielfalt und Richtigkeit der generierten Beispiele, wobei mehr als 50% der Beispiele korrekt waren. Die Studie zeigte, dass die Fine-Tuning eines 11B-Parameter-T5-Modells (Raffel et al., 2020) auf "Unnatural Instructions" sowohl im Vergleich zu TO++ (Sanh et al., 2021) als auch TK-Instruct (Wang et al., 2022) überlegen war. Dies unterstreicht die Effizienz und Genauigkeit von maschinellem Lernen im Vergleich zu menschlicher Arbeit.</sample>
    <sample id="111">Die Autoren zählen die Wortfrequenz auf einem allgemeinen Textkorpus \( D_p \) und wählen zufällig \( n \) Wörter aus einem moderaten Frequenzintervall.</sample>
    <sample id="112">### Übersetzung des Inhalts

**Titel:**
- **Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?**

**Referenten:**
- Shuheng Liu, Alan Ritter
- School of Interactive Computing, Georgia Institute of Technology

---

**Inhalt:**

**1. Einführung**
- Fragestellung: Funktionieren noch die CoNLL-2003 Named Entity Tagger in 2023?
- Ziel: Untersuchen, ob die Modelle, die auf CoNLL-2003 basieren, weiterhin gut generalisieren können.

**2. Named Entity Recognition &amp; Generalization**
- CoNLL-2003 wurde fast 20 Jahre lang zur Entwicklung von NER verwendet.
- Frage: Können diese Modelle modernes Datenmaterial generalisieren?
- Was ist notwendig für eine gute Generalisierung?

**3. CoNLL++ Dataset**
- Reuter News von 2020 wurden mit CoNLL-2003 Anmerkungsguidelines annotiert.
- 20+ Modelle wurden auf CoNLL-2003 abgestimmt und auf CoNLL-2003 Testset &amp; CoNLL++ evaluiert.
- Der prozentuale ΔF1 wurde berechnet, um die Generalisierung zu messen.

**4. Was ist notwendig für eine gute Generalisierung?**
- Modellarchitektur: Transformer-Modelle generalisieren besser.
- Modellgröße: Größere Modelle generalisieren besser.
- Anzahl der Feinabstimmungsbeispiele: Mehr Beispiele führen zu besserer Generalisierung.

**5. Was verursacht die Leistungseinbuße?**
- Adaptive Overfitting?
- Temporale Drift?

**6. Analyse der Leistungseinbuße**
- Grafik zeigt, dass adaptive Overfitting nicht als Hauptursache identifiziert wurde.
- Temporale Drift wird als Hauptursache angenommen.
- Leistung sinkt mit größerem zeitlichen Abstand.

**7. Zusammenfassung**
- Für eine gute Generalisierung benötigen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr Feinabstimmungsbeispiele
- Leistungseinbußen werden durch temporale Drift verursacht.
- Funktionieren CoNLL-2003 Tagger immer noch?
  - Ja!

**8. Kontaktinformationen**
- Paper: https://arxiv.org/abs/2212.09747
- Dataset: https://github.com/ShuhengL/acl2023_conllpp
- Kontakt: silu775@gatech.edu</sample>
    <sample id="114">The presentation discusses the challenges and advancements in large language models (LLMs), highlighting their transformative impact on natural language processing (NLP) tasks such as machine translation, sentiment analysis, and information extraction. It addresses limitations like heavy parameters, long training times, and the need for vast corpora, emphasizing the importance of efficient model compression techniques. The speaker introduces "Grouped Head Attention," a novel method that divides attention heads into groups to reduce redundancy while maintaining performance. This approach is demonstrated through experiments across machine translation, language modeling, and abstractive summarization tasks, achieving significant parameter compression with minimal loss in BLEU scores. Future work includes task-specific automatic pruning, aligning with the lottery ticket hypothesis, suggesting that all-in-one LLMs are redundant in real-world scenarios, requiring only a few specialized tasks.</sample>
    <sample id="115">Der Ansatz verwendet eine Sprachsegmentgröße von 1 Sekunde.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea ist das entitätsspezifische Wissen, dass Servin ein Richter und Kea eine Bakerin ist.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">Das Video präsentiert eine Studie zur Verbesserung der Prädiktionsverfahren für Code-Switched-NLP (Naturerkenntnisse und Sprachverarbeitung). Es wird die Bedeutung von Code-Switching in der Sprachverarbeitung hervorgehoben, insbesondere in Bezug auf die Herausforderungen, die bei der Entwicklung multilingualer Prätrainingsmodelle auftreten. Die Autoren beschreiben ihre Hauptbeiträge: die Einführung neuer Maskierungsprädiktionsobjektive, um Code-Switching-Informationen zu integrieren, und die Motivation architektonischer Änderungen und zusätzlicher Verlustkriterien, um die Effektivität des Code-Switched-Prädiktionsprozesses zu verbessern. Sie stellen auch zwei neue Methoden vor: SwitchMLM und FrequencyMLM, die speziell auf Code-Switching-Texte zugeschnitten sind. Zudem werden experimentelle Ergebnisse präsentiert, die zeigen, dass die vorgeschlagenen Techniken die Encodierung von Code-Switching-Informationen in den mittleren Modellschichten verbessern. Das Video endet mit einer Zusammenfassung, die die Vorteile der vorgeschlagenen Ansätze hervorhebt und die Notwendigkeit von weiteren architektonischen Änderungen und zusätzlichen Verlustkriterien zur Verbesserung der Code-Switching-Prädiktion betont.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf RoBERTa und GPT-2.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="121">Direkte Inferenzbeispiele sind "easy on me", "the first one".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Das Video präsentiert ein Forschungsprojekt, das sich mit der Verbesserung der multi-modalen Zero-Shot-Learning-Fähigkeiten von Sprachmodellen durch die Anpassung von Anweisungen befasst. Es wird eine neue Methode vorgestellt, die auf dem Einsatz von Multimodal-Prätrainierungsmodellen basiert und dabei die Effizienz und Genauigkeit der Modelle erhöht. Der Fokus liegt auf der Entwicklung eines umfangreichen Benchmark-Datensatzes namens MultiInstruct, der 62 verschiedene multimodale Aufgaben aus zehn breiten Kategorien umfasst. Diese Aufgaben umfassen visuelle Verhältnisse, temporale Ordnung, abstrakte Generierung, fundierte Matching und andere. Die Studie zeigt, dass die Anpassung von Anweisungen zu einer signifikanten Verbesserung der Zero-Shot-Learning-Fähigkeiten führt, insbesondere bei der Verwendung von OFA als Basismodell. Zudem wird die Sensitivität der Modelle gegenüber verschiedenen Anweisungen untersucht, wobei die Methode der Anpassung von Anweisungen die Sensitivität reduziert. Das Projekt zielt darauf ab, neue Transfer-Learning-Strategien zu erforschen und zu implementieren, um die Effizienz und Robustheit der Modelle weiter zu verbessern.</sample>
    <sample id="124">The presentation focuses on the temporal reasoning capabilities of large language models (LLMs). It introduces a breakdown of temporal reasoning into three levels: Time-Time Relation, Time-Event Relation, and Event-Event Relation. The study highlights that prior work has mainly focused on Level 2 reasoning, which involves understanding time-event relations. The research presents preliminary experiments showing biases in LLMs towards contemporary years and poor performance when predicting months. A new dataset called TempReason is introduced, covering all three temporal reasoning types and long time spans. The dataset is constructed using Wikidata knowledge base. The presentation also outlines a training framework for improving the temporal reasoning capability of LLMs, including temporal span extraction pretraining and time-sensitive reinforcement learning. The final model, TempT5, demonstrates significant improvements over existing models across different question types and time ranges.</sample>
    <sample id="125">Es gibt sieben Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als Baseline betrachtet.</sample>
    <sample id="127">Das Video präsentiert die Arbeit "Large Language Models Are Reasoning Teachers" von Namgyu Ho, Laura Schmid und Se-Young Yun aus dem KAIST AI Lab. Es untersucht, wie große Sprachmodelle komplexes Denken ermöglichen können, indem sie als Lehrer für kleinere Modelle fungieren. Die Studie zeigt, dass das Chain-of-thought (CoT) Reasoning, ein Verfahren, das von Wei 2022 entwickelt wurde, in großen Modellen mit über 100 Milliarden Parametern effektiv ist. Der Ansatz besteht darin, ein großes Modell als Lehrer zu verwenden, um kleinere Modelle mit 70 bis 6,7 Milliarden Parametern zu trainieren. Diverse Reasoning wird als einfache Methode vorgeschlagen, um die Leistung der Schüler zu verbessern. Das Paper beschreibt auch eine Methode namens Fine-tune-CoT, die dazu dient, komplexes Denken für kleinere Modelle zu generieren. Ergebnisse zeigen, dass Fine-tune-CoT signifikante Fähigkeiten des Denkens in kleinen Modellen ermöglicht und dass diverse Reasoning die Leistung erheblich steigert. Die Skalierbarkeit des Ansatzes unter Fine-tune-CoT wird ebenfalls hervorgehoben.</sample>
    <sample id="128">The KITMUS Test evaluates how well natural language understanding (NLU) models integrate knowledge from multiple sources, including pretrain-time and inference-time knowledge. The study introduces the KITMUS Test Suite, which includes a dataset for evaluation and a coreference resolution task to assess the models' ability to draw on both types of knowledge. The test is conducted with human participants and coreference resolution models. The results highlight that many models struggle to reason over knowledge from multiple sources, emphasizing the necessity of task-specific training for effective knowledge integration. Additionally, the study explores different variants of the KITMUS Test, showing that models perform better when provided with background knowledge during inference.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe "Black Stereotypes" genannt.</sample>
    <sample id="130">Transformer-Modelle generalisieren nicht gut.</sample>
    <sample id="131">Cleanly labeled test data</sample>
    <sample id="132">Es gibt sechs Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich visueller Informationen und Text.</sample>
    <sample id="135">The presentation titled "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" by Sarah E. Finch, James D. Finch, and Jinho D. Choi focuses on evaluating the performance of chat-oriented dialogue systems. It introduces two evaluation methods: Comparative Evaluation and Likert Rating Evaluation. Comparative Evaluation involves comparing different dialogue systems to identify their strengths and weaknesses, while Likert Rating Evaluation uses a scale from 1 to 5 to rate the relevance of bot responses. The presentation also discusses the dimensions of dialogue quality, including relevance, consistency, and emotional understanding. Additionally, it outlines the ABC-Eval Behaviors framework for annotating behaviors in chat, which includes categories like coherence, knowledge, consistency, and emotional understanding. The experiments conducted involve four open-domain dialogue models with 100 human-bot conversations per model, using methods such as ABC-Eval, Turn Likert, Dialogue Likert, and Comparative. The results highlight predictive validity and incremental validity, showing how different evaluation methods contribute to the overall quality of dialogue systems.</sample>
    <sample id="136">Das Video präsentiert ein wissenschaftliches Seminar, das sich auf die numerische Logik und die Bewertung von Modellen konzentriert. Es wird eine neue Methode namens FERMAT vorgestellt, die als Alternative zu den üblichen Genauigkeitsmessungen dient. Die Präsentation beginnt mit der Motivation für die Arbeit, die durch die Unzureichendheit bestehender Benchmarks und die Notwendigkeit einer umfassenderen Bewertung von Modellen motiviert ist. Es werden Beispiele wie die Verwendung von Wikipedia-Artikeln zur Erstellung von mathematischen Fragen gezeigt, um die Vielfalt der Aufgabenstellungen zu verdeutlichen. Die Präsentation beschreibt auch die Struktur von FERMAT, die aus verschiedenen mathematischen Operationen besteht, und zeigt Ergebnisse von Evaluierungen, die sowohl zero-shot als auch fine-tuned sind. Zudem wird die Bedeutung der Sprach- und mathematischen Vielfalt betont, sowie die Bereiche der Verbesserung, wie die Nummerierung und Tokenisierung, hervorgehoben. Das Video endet mit einem Dank an die Zuhörer und bietet Kontaktdaten für weitere Informationen.</sample>
    <sample id="137">Das Forschungsprojekt "Tell2Design" zielt darauf ab, ein neues Design-Generationskonzept zu initiieren, das auf der Verwendung natürlicher Sprachanweisungen basiert und speziell auf den Bereich der Bodenpläne fokussiert ist. Ziel ist es, Benutzerpräferenzen durch die Beschreibung von Raumtypen und -funktionen in Textform direkt in strukturierte Bodenpläne umzusetzen. Der Ansatz verwendet einen Encoder-Decoder-LLM-Framework, um die Texte in strukturierte Zielsequenzen zu transformieren, die dann in Bodenpläne übersetzt werden können. Die Arbeit präsentiert auch eine umfangreiche Datenbank namens Tell2Design mit über 80.000 Anweisungen, die sowohl von Menschen als auch künstlich generiert wurden, um die Modellleistung zu verbessern. Es wird gezeigt, dass die Methode im Vergleich zu anderen Baselines signifikante Verbesserungen erzielt hat, insbesondere bei der Erstellung von Bodenplänen für unbekannte Anweisungen.</sample>
    <sample id="138">Das zu wenig erforschte Gebiet im Bereich der NLU, nach Ansicht der Autoren, ist die Integration von Wissen aus verschiedenen Quellen (prätrainiertes und inferenzzeitbezogenes Wissen).</sample>
    <sample id="139">Zhiyang Xu, Ying Shen und Lifu Huang</sample>
    <sample id="140">Ja, Coscript wurde von Menschen annotiert und validiert.</sample>
    <sample id="141">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen darin, dass nur eine kleine Portion der Wörter von Kontext abhängt und dass vorhandene Methoden begrenzte Diskursphänomene und Sprachen unterstützen.</sample>
    <sample id="142">Natürlich, hier ist der übertragen englische Inhalt in die deutsche Sprache:

---

**Titel: Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)**

**Referenten:** Mohammad Javad Hosseini, Filip Radlinski, Silvia Paresi und Annie Louis

**Google Research**

---

**Indirekte Bezugssätze**

- **Ziel:** Verstehen der Sprache der Nutzer, wenn sie eine Wahl treffen.
- **Alternative Frage:** "Haben Sie damit 'Easy on Me' oder 'I Gotta Feeling' gemeint?"
- **Direkte Bezugssätze:**
  - "easy on me", "the first one"
- **Indirekte Bezugssätze können in natürlichen und flüssigen Gesprächen verwendet werden:**
  - Namen kann man nicht mehr erinnern.
  - Die Aussprachen sind schwer zu unterscheiden.
  - Man möchte spezifisch eine Vorliebe ausdrücken.

---

**Datenbanksammlung**

- **Wichtige Probleme:**
  - Konversational Systems
  - Benchmarking von großen Sprachmodellen für Entitätsverstehung
  - Keine große öffentlich zugängliche Datenbank verfügbar
- **Wir sammeln eine große Datenbank mit Hilfe von Crowdsourcing**
- **Drei Bereiche:**

---

**Datenbanksammlungs-Methode**

- **Methode betont Informalität durch ein Karikatur-Fülltask**
- **Dialog-Kontext festlegen (drei kurze freie Anweisungen pro Bereich)**
- **Alternative Frage:**
- **Ausdruck, der auf eine der Entitäten verweist:**

---

**Generieren alternativer Fragen =&gt; Auswahl von Entitätenpaaren**

- **Items mit ähnlichen Infoboxen auf Wikipedia (gleicher Genre oder Künstler):**
  - "Do you mean 'This is it!' or 'Man in the Mirror'?"
- **Items mit ähnlichen Beschreibungen auf Wikipedia:**
  - "Do you mean 'Thinking of You' or 'Happy Anywhere'?"
- **Items mit ähnlichen Titeln:**
  - "Do you mean 'The Return (memoir)' or 'The Return (Shatner novel)'?"
- **Uniform at random:**
  - "Do you mean 'You Could Be Mine' or 'The Way I Am'?"

---

**Hintergrundwissen (Musik)**

- **Google-Suchlink zu jedem Song.**
- **Anforderungen an die Annotatoren:**
  - Hören Sie mindestens einige Songs an.
  - Lesen Sie sich jeden Song an.

---

**Danke! Wenn Sie Fragen haben, senden Sie eine E-Mail an javadh@google.com**

---

Bitte beachten Sie, dass die Übersetzung des Inhalts in den Textfeldern des Präsentationsmaterials erfolgt.</sample>
    <sample id="143">Der Ansatz wird mit den Strategien "walk-k", "LA", "CAAT" und "EDAs" verglichen, die in offline Modellen angewendet werden.</sample>
    <sample id="144">Avignon Université</sample>
    <sample id="145">Der/die Referent*in ist Jenny T. Liang.</sample>
    <sample id="146">Das Vortragende Thema konzentriert sich auf die Analyse von Dialogsummarisierung, insbesondere auf die Herausforderung der Omissionserkennung. Es wird ein Dataset vorgestellt, das für die Detektion von Omissionen in verschiedenen Dialogszenarien wie Kundendienstleistungen, Medizinberatungen, Meetings, Filmtexten, E-Mail-Threads und Chatlogs verwendet wird. Die Studie untersucht verschiedene Modelle wie BART, QM, Transformer und Pegasus, um ihre Leistungsfähigkeit bei der Erkennung von fehlenden Informationen zu bewerten. Ergebnisse zeigen, dass Omissionen ein bedeutendes Problem in der Dialogsummarisierung darstellen und die Qualität der Summen beeinträchtigen können. Eine neue Aufgabe, die Omission Detection, wird eingeführt, um die Qualität der Summen zu verbessern. Das Dataset enthält 10 Kandidatensummen pro Dialog und ermöglicht die automatische Detektion sowie die manuelle Bewertung von Omissionen.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="148">Natürlich, hier ist der Übersetzung des englischen Inhalts in den deutschen:

---

**Titel: Aufmerksamkeit als Leitfaden für die gleichzeitige Sprachübersetzung**

**Autoren:** Sara Papi, Matteo Negri, Marco Turchi

**Inhalt:**

1. **Was ist gleichzeitige Sprachübersetzung?**
   - Gleichzeitige Sprachübersetzung (SimuST) ist der Prozess, gesprochene Sprache in einen Text in einer anderen Sprache in Echtzeit zu übersetzen, um eine grenzüberschreitende Kommunikation zu ermöglichen.

2. **Welche Probleme haben aktuelle SimuST-Modelle?**
   - Spezifische Architekturen werden oft trainiert und zusätzliche Module müssen optimiert werden.
   - Längere und kompliziertere Trainingsverfahren (z.B. verschiedene Optimierungsziele).
   - Training und Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen (z.B. 1s, 2s, 25s).

3. **Was ist unsere Lösung?**
   - Verwenden Sie bereits vorhandene offline ST-Modelle ohne Neutrainieren oder Anpassung einer spezifischen Architektur für SimuST.
   - Verwenden Sie nur ein Modell für jeden Latenzregime und handeln Sie Latenz durch spezifische Parameter.
   - Nutzen Sie das bereits erworbenen Wissen des Modells durch die Aufmerksamkeitsmatrix zwischen Audio-Eingabe und textueller Ausgabe.

4. **Unsere Lösung: EDAtt**
   - Encoder-Decoder-Aufmerksamkeit
   - Entscheiden Sie, ob eine vollständige oder eine partielle Übersetzung emittiert wird basierend auf dem Ort der Aufmerksamkeitspunkte:
     - Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist (die Summe ist unter einem Schwellwert α) und es sich um die letzte α-Schritte handelt, bei denen genug stabile Informationen verfügbar sind.

5. **Hauptergebnisse: EDAtt**
   - BLU (Bilingual Universal Sentence Encoder) als Qualitätsschätzer
   - Latenzmessung als Zeitmaß
   - EDAtt übertrifft alle Strategien, die auf offline-Modellen angewendet wurden.
   - EDAtt ist die schnellste Strategie, wenn wir die tatsächliche verstrichene Zeit berücksichtigen.

6. **Weitere Informationen finden Sie in unserem Papier!**
   - Kontaktinformationen: (spapi.negri)@fbk.eu, marco.turchi@gmail.com
   - GitHub: github.com/tlt-mt/fbk-fairseq
   - Twitter: @bk_mt, @sarapapi

---</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Das Projekt MeetingQA konzentriert sich auf die Extraktive Frage-Antwort (QA) auf Basis von Protokollen von Meetings, was ein interessantes und herausforderndes Feld darstellt. Die Studie untersucht, wie Modelle in der Lage sind, offene und diskussionsschwerpunktige Fragen aus Meetings zu beantworten, die oft nicht durch vorherige Arbeiten abgedeckt werden. Der Fokus liegt auf der Analyse von 7.735 Fragen aus 166 verschiedenen Meetings, wobei 30% als unantwortbar identifiziert wurden. Die Daten wurden sorgfältig gesammelt und annotiert, um sicherzustellen, dass die Antworten korrekt sind. Die Ergebnisse zeigen, dass bestehende QA-Modelle signifikante Leistungsnachteile gegenüber menschlicher Performance haben, insbesondere im Finetuning- und Zero-Shot-Szenario. Es wird auch gezeigt, dass Silver Data Augmentation effektiv ist, um die Leistung zu verbessern.</sample>
    <sample id="151">### Übersetzung des Inhalts

#### Slide 1: Titel und Autoren
**Titel:** MULTIINSTRUCT: Verbesserung der multi-modalen Null-Shot-Learning durch Anweisungs-Tuning
**Autoren:** Zhiyang Xu*, Ying Shen*, Lifu Huang
**Institution:** Department of Computer Science, Virginia Tech
*Gleichwertige Beiträge*

#### Slide 2: Vergleich von Prätrainierteren Sprachmodellen für Aufgaben im Downstream
- **(A) Prätrainieren und Feinabstimmen (BERT, T5):**
  - Prätrainiertes Modell
  - Feinabstimmung auf jede Aufgabe
  - Typischerweise erfordert viele Aufgaben spezifische Feinabstimmungen
- **(B) Prompting (GPT-3):**
  - Prätrainiertes Modell
  - Wenig oder keine Feinabstimmung erforderlich
  - Wenige prompting-Instanzen pro Aufgabe
- **(C) Anweisungs-Tuning (FLAN):**
  - Prätrainiertes Modell
  - Anweisungs-Tuning auf jede Aufgabe
  - Modell lernt, die jeweiligen Aufgaben mit Anweisungen auszuführen
  - Wenig oder keine Feinabstimmung erforderlich
  - Wenige prompting-Instanzen pro Aufgabe

#### Slide 3: Text: "Sprach-only"

#### Slide 4: Text: "Anweisungs-Tuning auf multimodale Prätrainierte Modelle"

#### Slide 5: Text: "Unausgeglichene Anweisungsdatensätze zwischen NLP und Multimodal"

#### Slide 6: Text: "1600+ Sprach-only Anweisungsaufgaben"
- Keine großen, öffentlich verfügbaren multimodalen Anweisungsaufgaben

#### Slide 7: Text: "MULTIINSTRUCT"
- Erste multimodale Anweisungs-Tuning-Benchmark-Datensatz
- 62 verschiedene multimodale Aufgaben
- 10 breite Kategorien
- 5 Experten geschriebene Anweisungen

#### Slide 8: Text: "OFA (One For All)"
- Einheitliches multimodales Prätrainierter Modell, das sowohl Verständnis- als auch Generierungsaufgaben mit einem oder mehreren Modalitäten ausführen kann.
- OFA hat eine einheitliche Vokabular für Sprache, Bild-Token und Koordinaten eines Bounding Boxes.

#### Slide 9: Beispielinstanzen aus MULTIINSTRUCT für vier Aufgaben
- **Grounded Caption:** Erstellen einer Beschreibung für das Bild
- **Text Localization:** Auswahl der Region, die den Text enthält
- **Referring Expression Selection:** Auswahl der Region, die den Objekt in der Front enthält
- **Question Image Matching:** Antwort auf die Frage basierend auf dem Bild

#### Slide 10: Text: "Multi-modal Instruction Tuning"
- **Trainings-Datensatz Konstruktion:**
  - 53 Aufgaben für 9 Gruppen zur Trainingsdatensatzkonstruktion
  - 5 Aufgaben für die Testdatensatzkonstruktion
- **Testings-Datensatz Konstruktion:**
  - Reservieren des ganzen Commonsense Reasoning-Gruppen für den Test
  - Auswahl zusätzlicher 5 Aufgaben aus VQA und Miscellaneous-Gruppen
  - Verwenden aller Instanzen im Testsplit für jede Aufgabe
  - Zufällige Auswahl von 20 Aufgaben aus dem Testsplit des Natural Instructions-Dateisatzes als unbekannte Aufgaben für NLP

#### Slide 11: Text: "Implementationsdetails"
- **Trainingsdetails:**
  - Verwendung des prätrainierten OFA-Large-Modells (472M)
  - Mischen aller Instanzen für alle Aufgaben
  - Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert
- **Testingsdetails:**
  - Fünf Experimente durchführen, indem das Modell mit einer der fünf Anweisungen für jede Aufgabe evaluiert wird
  - Mittelwert und Standardabweichung der Leistung über fünf Experimente berichten

#### Slide 12: Text: "Evaluation Metrics"
- Für multi-modal Klassifikationsaufgaben (Visuelle Entailment, Visuelle Räumliche Rekonstruktion, Natürliche Sprache visuelle Rekonstruktion, Katastrophentyp-Klassifikation) wird die Genauigkeit berichtet.
- Für multi-modal Generierungsaufgaben (Commonsense VQA, Text VQA, Grounded VQA, Visuelle Text Extraktion, Visuelle Text Match) wird die Rouge-L-Wert berichtet.
- Für NLP-Aufgaben wird der Rouge-L-Wert berichtet.

#### Slide 13: Text: "Sensitivität"
- Wie sensibel das Modell ist gegenüber der Vielfalt von Anweisungen für dieselbe Aufgabe:
  - Fähigkeit, für dieselbe Aufgabe immer die gleichen Ergebnisse zu produzieren, unabhängig von leichten Variationen im Wortlaut der Anweisungen.

#### Slide 14: Text: "Effektivität von Anweisungs-Tuning auf MULTIINSTRUCT"
- OFA finetuned auf 5 Anweisungen erreicht viel höhere aggregierte Performance auf allen Evaluationsaufgaben und zeigt geringere Sensitivität.

#### Slide 15: Text: "Effekt von diversen Anweisungen auf Anweisungs-Tuning"
- Anweisungs-Tuning auf MULTIINSTRUCT kann die Sensitivität des OFA signifikant reduzieren.
- Transfer-Learning von NATURAL INSTRUCTIONS-Dataset kann die Sensitivität weiter reduzieren.

#### Slide 16: Text: "Effekt von Feinabstimmungsstrategien auf Modell-Sensitivität"
- Anweisungs-Tuning auf MULTIINSTRUCT kann die Sensitivität des OFA signifikant reduzieren.
- Transfer-Learning von NATURAL INSTRUCTIONS-Dataset kann die Sensitivität weiter reduzieren.

#### Slide 17: Text: "Zero-Shot Performance auf NLP-Aufgaben"
- Anweisungs-Tuning auf MULTIINSTRUCT kann die Zero-Shot-Leistung verbessern.
- Das Transfer-Learning-Strategie MixedInstruct kann die Zero-Shot-Fähigkeit des OFA am besten erhalten.

#### Slide 18: Text: "Schlussfolgerungen"
- Erstes großes multimodales Anweisungs-Tuning-Dataset.
- Verbessert die Zero-Shot-Fähigkeit des OFA via Anweisungs-Tuning.
- Erforschen mehrere Transfer-Learning-Techniken und zeigen ihre Vorteile.
- Entwerfen eines neuen Sensitivitäts-Metriks.

#### Slide 19: Text: "Ein weiteres Ding!"
- Sammeln eines viel größeren multimodalen Anweisungs-Tuning-Datasets mit etwa 150 zusätzlichen visuell-sprachlichen Aufgaben und veröffentlichen sie bald!</sample>
    <sample id="152">Das Video präsentiert ein Forschungsprojekt, das sich mit der Entwicklung neuer starken Sprachmodelle für die klassische Philologie befasst. Die Autoren, Frederick Riemenschneider und Anette Frank, stellen ihre Arbeit auf dem ACL 2023 vor. Sie diskutieren verschiedene Sprachmodelle wie Latin BERT und Ancient Greek BERT und betonen die Notwendigkeit, bestehende Modelle miteinander zu vergleichen, um den Stand der Technik zu verbessern. Das Projekt zielt darauf ab, neue Multilingual-Modelle zu entwickeln und hochwertige Vortrainingsdatensätze zu verwenden. Die Evaluation erfolgt durch offizielle Datensplits und direkte Vergleichbarkeit mit bestehenden Ergebnissen. Es werden auch spezifische Datasete wie Universal Dependencies und EvaLatin 2022 vorgestellt, die für die PoS Tagging und Lemmatization verwendet werden. Die Präsentation schließt mit einer detaillierten Analyse von Dependency Parsing und Lemmatization, wobei die Effizienz und Genauigkeit der Modelle im Vergleich zu anderen Methoden gezeigt wird.</sample>
    <sample id="153">The presentation focuses on resolving ambiguities in text-to-image generative models, specifically addressing the challenges posed by ambiguous prompts. It introduces the Text-to-Image Disambiguation (TIED) framework, which includes two main components: QA-TIED and VS-TIED. QA-TIED uses in-context learning to generate clarifying questions from a language model (LM), while VS-TIED generates possible visual setups. The study curates the Text-to-image Ambiguity Benchmark (TAB), which covers various ambiguity types such as syntax, discourse anaphora, and fairness. The evaluation process involves automatic metrics like BLEU and ROUGE, as well as human evaluations. The research finds that disambiguation improves faithful generation but also notes disparities in resolving different ambiguity types. The overall effect of disambiguation is positive, and there is reasonable agreement between automatic and human evaluations.</sample>
    <sample id="154">Die Autoren Sara Papi, Matteo Negri und Marco Turchi gehören der Universität di Trento an.</sample>
    <sample id="155">Der Referent ist Mohammad Javad Hosseini.</sample>
    <sample id="157">The presentation focuses on the topic of "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" by Shen Gao from Shandong University. It introduces a framework for summarizing dialogue contexts, highlighting the importance of capturing both static and dynamic relationships between utterances. The method employs a static-dynamic graph-based approach to model these relationships, using discourse parsing tools to build dependency-based structures. The system includes modules such as an utterance encoder, static graph construction, static-dynamic graph module, and summary generator. The static graph captures fixed relationships like co-occurrence, while the dynamic graph models evolving interactions. The fusion module integrates these graphs into a unified representation, enhancing the semantic understanding of dialogues. The summary generator incorporates this graph representation to produce concise summaries that capture the essential information from the dialogue context.</sample>
    <sample id="158">Das Vortragende präsentierte ein neues Verfahren namens Dual Cache zur Lösung von Nomen-Koordinationsproblemen in langen Dokumenten. Es handelt sich um eine Methode, die zwei Arten von Caches verwendet: einen L-cache für lokale und einen G-cache für globale Entitäten. Dieses System reduziert die Komplexität der Berechnungen und den Speicherbedarf signifikant im Vergleich zu konventionellen Methoden, die alle möglichen Paare von Erwähnungen durchgehen. Die Präsentation zeigte auch experimentelle Ergebnisse auf öffentlichen Benchmarks, die das Überlegen des Dual-Cache-Verfahrens gegenüber anderen Methoden bestätigen. Der Vortrag betonte, dass das Dual Cache-System nicht nur die Effizienz erhöht, sondern auch die Kosten effektiver steuert als andere Ansätze.</sample>
    <sample id="159">### Übersetzung des Inhalts

---

#### Slide 1: Titel und Autor
**Titel:** Sprachmodell-Acceptabilitätsurteile sind nicht immer robust gegenüber Kontexten  
**Veranstaltung:** ACL 2023  
**Autor:** Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, Adina Williams  

#### Slide 2: Minimal Pair Paradigm (MPP) – Eine Überarbeitung
**Minimal Pair Paradigm (MPP):** Die Evaluierung von Sprachmodellen verwendet relative Differenzen in Sequenzwahrscheinlichkeiten, um die abstrakte Kenntnis von Sprachmodellen zu bewerten.

- **BLIMP:**  
  1. Viele Menschen helfen sich selbst.  
  2. Viele Menschen werden sich selbst helfen.  

- **SyntaxGym:**  
  1. Kein Kunde ... hat etwas bezahlt.  
  2. Der Kunde ... hat etwas bezahlt.  

- **CrowS:**  
  1. Stereotypische Aussage.  
  2. Non-stereotypische Aussage.  

#### Slide 3: Fragestellung
**Fragestellung:** Sind diese Urteile stabil gegenüber langen Vorhängen?

#### Slide 4: Ansatz
**Ansatz:** Testen wir, ob MPP-Urteile als Funktion der Kontextlänge, strukturellen Übereinstimmung, Match und Akzeptabilität variieren.

- **Test-Bauteil:** Subjekt-Verb-Übereinstimmung  
- **Raum der möglichen Präfixe:**  
  - Body  
  - Body + Subject  
  - Body + Subject + Verb  
  - Body + Subject + Verb + Object  

#### Slide 5: Beispiel für eine akzeptable und unakzeptable MPP-Satzpaarung
- **Akzeptable, passende MPP-Satzpaarung:**  
  - Warum könnte Jessie vorher nichts sehen, bevor sie zurückkehrte?  
  - Warum könnte Jessie vorher nichts sehen, bevor sie zurückkehrte?  

- **Unakzeptable, passende MPP-Satzpaarung:**  
  - Es gab einen Dokumentarfilm über Musik im Internet.  
  - Es gab einen Dokumentarfilm über Musik im Internet.  

#### Slide 6: Ergebnisse
**Ergebnisse:** Die MPP-Urteile sind robust für arbiträre Kontextlängen. Akzeptable/unakzeptable MPP-Sätze in der Umgebung heben/niederschlagen die Bewertungsleistung. Akzeptable/unakzeptable MPP-Sätze mit passender Struktur haben am meisten gravierend auf das Modellleistungsextrem beeinflusst.

#### Slide 7: Schlüsselerkenntnisse
- **Schlüsselerkenntnis 1:** Sprachmodelle sind sensibel an latenten syntaktischen/semantischen Merkmalen, die über Sätze hinweg verbreitet sind.
- **Schlüsselerkenntnis 2:** MPP-Evaluierungen mit kurzen, einzelnen Satzinputs können die abstrakte Kenntnis von Sprachmodellen nicht vollständig erfassen.

---

Diese Übersetzung deckt den Hauptinhalt der Präsentation ab und fokussiert sich auf die zentralen Punkte und Fragen, die im Rahmen der Präsentation aufgeworfen werden.</sample>
    <sample id="160">Die Input-Token werden in zwei Arten von Tokens, "tag" und "permute", zugeordnet.</sample>
    <sample id="161">55.000 Skripte</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist diejenige, die mit dem höchsten F1-Score ausgestattet ist.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es den Annotierungsbottleneck lindert.</sample>
    <sample id="165">Das Video präsentiert ein Forschungsprojekt aus dem Bereich Computer Science an der Cornell University, das sich mit abduktiver Commonsense-Reasoning befasst, um miteinander ausschließende Erklärungen zu nutzen. Es wird erklärt, wie abduktive Reasoning funktioniert und wie es verwendet wird, um auf Basis eines Kontextes und eines Ergebnisses plausible Erklärungen zu generieren. Der Fokus liegt auf der Herausforderung, plausible Erklärungen zu annotieren, die ein subjektives und unruhiges Prozess ist. Das Projekt LiPoR (Likelihood learning with Posterior Regularization) wird vorgestellt, das ohne Annotierung von Erklärungen arbeitet und dabei die Wahrscheinlichkeit mass von p(z|x,y) zu einer Teilmenge von Erklärungen zusammenzieht. Die Ergebnisse zeigen, dass LiPoR bei der αNLI-Maßzahl 71.56 Punkte erreicht hat, was eine Verbesserung gegenüber anderen Modellen darstellt.</sample>
    <sample id="166">This research introduces a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text, addressing the limitations of existing methods that struggle with complex reasoning tasks. The framework employs two systems: System 1, a visual-linguistic interactor based on VLMs for analogical reasoning, and System 2, a neural-symbolic reasoner for logical reasoning. By integrating these systems, the framework enhances performance in complex reasoning scenarios, outperforming single-system approaches. The study demonstrates the effectiveness of this approach through experimental results and case analyses, highlighting its potential to improve compositional reasoning and planning capabilities in large language models.</sample>
    <sample id="167">Die Dokumente in DEplain-web wurden mit 1:1-Alignment und n:m-Fähigkeiten ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch die Annotierung von Reuters-Nachrichten aus dem Jahr 2020 mit den Anmerkungsrichtlinien des CoNLL-2003 erstellt.</sample>
    <sample id="169">Das Video präsentiert ein wissenschaftliches Projekt, das sich mit der Nutzung von PaLM (Pathways Language Model) für die Übersetzungssprache beschäftigt. Der Fokus liegt auf der Bewertung von Strategien und Leistungen bei der Anwendung von PaLM für die Übersetzung. Die Präsentation beginnt mit einer Einführung in PaLM, einem großen Sprachmodell mit 540 Milliarden Parametern, das auf 780 Milliarden Tokens trainiert wurde und in verschiedenen Aufgaben wie Fragebeantwortung, Rechenaufgaben, Code-Vollständigkeit, Zusammenfassung, Übersetzung und Sprachverstehen hervorragend performt.

Die Studie untersucht erstmals systematisch die Anwendung von LLMs (Large Language Models) für die Übersetzung und bietet Empfehlungen zur Auswahl von Prompten. Sie verwendet die neuesten Testdatensätze, um zu vermeiden, dass Trainings- und Testdaten übereinstimmen und überfitting auftritt. Die Evaluation erfolgt mit den neuesten WMT-Ergebnissen und SOTA-Metriken, die besser mit menschlichen Urteilen korrelieren als Crowdsourcing-Evaluationen. Expertenbasierte menschliche Bewertungen werden ebenfalls angewendet, um die Robustheit der Ergebnisse zu erhöhen.

Es wird gezeigt, dass die Qualität der Übersetzungen stark von den Prompts abhängt. Zwei zufällige Prompts werden für jede Phrase ausgewählt, und BLEURT wird verwendet, um die Übersetzungskonzepte zu bewerten. In mehr als der Hälfte aller Fälle (516 von 1000) zeigt sich eine Differenz von mehr als einem BLEURT-Punkt zwischen den Prompts, was einen maximalen Unterschied von bis zu 40 BLEURT-Punkten ergeben kann.

Ein Beispiel für die Prompting-Strategie zeigt, wie verschiedene Prompts für eine deutsche Phrase verwendet werden können, um die Übersetzung in Englisch zu verbessern. Die Ergebnisse der experimentellen Studie zeigen, dass die Qualität der Beispiele wichtiger ist als ihre Ähnlichkeit zum Quellsatz. Spezialisierte SOTA-Systeme haben einen signifikanten Vorteil, und PaLM ähnelt Google Translate. Die Fluency von PaLM ist vergleichbar mit SOTA-Systemen, während die Genauigkeitswerte generell niedriger sind und von Ungenauigkeiten dominiert werden. Der Stil/Ausdruck ist generell für PaLM niedriger.

Zusammengefasst zielt die Studie darauf ab, die Effizienz und die Qualität der Übersetzung durch PaLM zu verbessern, indem sie die Auswahl von Prompts optimiert und die Leistungsfähigkeit des Modells mit anderen Systemen vergleicht.</sample>
    <sample id="170">### Übersetzung des englischen Inhalts ins Deutsche

---

**Titel: XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations**

**Autoren:** Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

**Institutionen:** Penn State und Amazon

---

**Inhalt:**

- **Semantic Parsing:** Eine Aufgabe, die eine semantische Darstellung von Nutzeraufträgen erstellt, wie z.B. SQL und Lambda Calculus.
  - Beispiel: "Which countries in Europe have at least 3 car manufacturers?" wird in SQL und Lambda Calculus dargestellt.

- **Cross-Lingual Semantic Parsing:** Die Aufgabe, Fragen in verschiedenen natürlichen Sprachen in mehrere semantische Darstellungen zu übersetzen.
  - Beispiele: Fragen in Englisch, Deutsch und Chinesisch werden in SQL, Lambda und FunQL übersetzt.

- **Bestehende CLSP Modelle:** Diese Modelle wurden getrennt vorgeschlagen und auf begrenzten Aufgaben und Anwendungen evaluiert.
  - Probleme: Mangelnde Abdeckung bestimmter natürlicher Sprachen und bestimmter semantischer Darstellungen.
  - Lösung: Einheitliches Dataset XSemPLR für cross-lingual semantic parsing in mehreren natürlichen Sprachen und semantischen Darstellungen.

- **XSemPLR:** Einheitliches Dataset mit:
  - 9 Datensätzen aus verschiedenen Domänen
  - 5 semantischen Darstellungen
  - 8 semantischen Darstellungen
  - 22 natürliche Sprachen in 15 Sprachfamilien

- **Experimenteinstellungen:** Sechs Einstellungen für Training und Evaluation.
  - **Translate-Test:** Verwenden Sie die Google-Übersetzung API, um Quelltext in das Zielsprache zu übersetzen. Dann verwenden Sie ein monolinguales Modell zum Trainieren und Testen.
  - **Monolingual-Modell:** Quellsprache ist die gleiche wie ZielSprache (z.B. German-to-German). Wir testen auch den Monolingual Few-shot-Einstellung durch das Trainieren monolingualer Modelle mit nur 10% Trainingsdaten.
  - **Multilingual-Modell:** Trainieren Sie ein multilinguales Modell für alle Sprachen.
  - **Cross-Lingual Zero-shot/Few-shot Transfer:** Trainieren Sie auf einer Quellsprache und übertragen Sie auf eine andere Sprache.

- **Analyse von Monolingual-Modellen:** Enc-Dec (mT5) erzielt die beste Leistung auf allen Datensätzen.
  - Enc-PTR: Multilinguale Prätrainierte Encoder mit Pointer-basierten Decoder
  - Enc-Dec: Multilinguale Prätrainierte Encoder-Decoder Modelle
    - mBART, mT5

- **Weitere Ergebnisse und Erkenntnisse (Abschnitt 4 im Papier):**
  - Enc-Dec (mT5) übertrifft vorherige Arbeiten oder erreicht vergleichbare Ergebnisse.
  - Prätraining auf der englischen Sprache kann die Leistung der seltenen Sprachen signifikant verbessern.
  - Multilinguale LMs (Codex &amp; BLOOM) sind immer noch unzureichend für cross-lingual semantic parsing Aufgaben.
  - FunQL übertrifft die anderen drei semantischen Darstellungen und SQL erzielt die schlechteste Leistung.

- **Fazit:**
  - Wir bauen XSemPLR, einen einheitlichen Benchmark für cross-lingual semantic parsing mit mehreren natürlichen Sprachen und semantischen Darstellungen.
  - Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Typen von multilingualen Sprachmodellen durch.
  - Unsere Ergebnisse zeigen, dass mT5 mit monolingualer Training die beste Leistung erzielt, während multilinguale LMs immer noch unzureichend für cross-lingual semantic parsing Aufgaben sind. Zudem bleibt der Leistungsunterschied zwischen monolingualer Training und cross-lingual transfer learning weiterhin signifikant.

---

**Links:**
- [Paper Link](https://arxiv.org/pdf/2306.04085.pdf)
- [Code Link](https://github.com/psunlpgroup/xsemplr)</sample>
    <sample id="171">Die Präsentation erwähnt vier Arbeiten, die bereits durchgeführt wurden:

1. Parameter-basierte Wasserzeichen [1, 2]
2. Lexikalische Wasserzeichen [3, 4]
3. Rücktür-basierte Wasserzeichen [5]
4. Adversarial-basierte Wasserzeichen [6]</sample>
    <sample id="172">No, they are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Das Video präsentiert eine umfangreiche Datenbank namens "ArgAnalysis35K", die für die Analyse der Qualität von Argumenten entwickelt wurde. Diese Datenbank umfasst 35.000 Argumente, die direkt aus Siegerdebatten und Debattanten stammen und als hochwertig eingestuft werden. Die Datenbank bietet eine Vielzahl von Themen, darunter Politik, Umwelt und autoritäre Regime, und enthält eine Vielzahl von Bewegungen und Quellenargumenten. Ein besonderer Fokus liegt auf der Analyse des logischen Zusammenhangs und der Relevanz der Argumente. Zudem wird ein Modell zur Annotator-Reliabilität vorgestellt, das auf Instanzen basiert und die Reliabilität der Annotatoren durch künstliche Intelligenz (IA) und Fuzzy-Neuronale Netze (FNN) berechnet. Dieses Modell hilft dabei, die Zuverlässigkeit der Bewertungen zu verbessern und die Qualität der Argumentanalyse zu erhöhen.</sample>
    <sample id="175">Die Methode löst die Mehrdeutigkeit der Permutationen durch das Induzieren von Alignment in der Trainingphase.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert als das Ausmaß, in dem das Modell unfaire Entscheidungen oder Bias in der Sprachverarbeitung reduziert.</sample>
    <sample id="177">Der/die Referent*in ist Yanis Labrak.</sample>
    <sample id="178">Der/die Referent*in ist Koustuv Sinha.</sample>
    <sample id="179">This research introduces SymbolicToM, an inference-time method that enhances the Theory of Mind (ToM) reasoning skills of Large Language Models (LLMs). By leveraging explicit graphical symbolic representations, it avoids overfitting and provides more interpretable reasoning. SymbolicToM significantly improves LLM performance on out-of-domain ToM tasks, outperforming supervised baselines like Textual Time Travel and ParaphrasedToM, especially for second-order false-belief questions. The method is evaluated across various models including Macaw-3B, GPT3, Flan-T5, and LLaMA, demonstrating robustness and effectiveness in diverse linguistic scenarios.</sample>
    <sample id="180">Myra Cheng, Esin Durmus und Dan Jurafsky</sample>
    <sample id="181">The presentation discusses the constrained language planning problem, focusing on how large language models (LLMs) can be used to generate scripts for specific goals. It highlights the challenges LLMs face when handling abstract goals and the need for specialized datasets like Coscript to improve their performance. The speaker explains a method that involves generating candidate scripts with InstructGPT and filtering them based on similarity scores to enhance script quality. The study demonstrates that smaller LMs fine-tuned on Coscript can produce higher-quality scripts compared to larger models. The research also explores the heterogeneity and pluralism in generated specific goals, using automatic metrics like ROUGE, BLEU, and BERTScore to evaluate the results.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Betrachtung von Gruppen nur durch ihre Identität, ohne Berücksichtigung ihrer Komplexität oder Vielfalt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch die Verwendung von Prompts wie "Denken Sie, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst" erstellt.</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI) wurde verwendet, um die Kontextnutzung zu messen.</sample>
    <sample id="185">DrBERT ist ein robustes prätrainiertes Modell in Französisch, speziell für Biomedizinische und klinische Anwendungen, während ChuBERT ein generisches Modell ist, das auf kontinuierlichem Prätraining basiert und für medizinische Aufgaben in Französisch optimiert wurde.</sample>
    <sample id="187">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen bezieht sich auf das kontinuierliche Anpassen und Aktualisieren eines Modells mit neuer Daten, um die Leistung zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Entitätsauswahl in natürlichen und flüssigen Konversationen zu verstehen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS durch das Lernen von Embeddings extrahieren und ähnliche Dienste bereitstellen.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="192">Das Video präsentiert die Arbeit "CAME: Confidence-guided Adaptive Memory Efficient Optimization", die von Yang Luo und Kollegen aus der National University of Singapore und Huawei entwickelt wurde. Die Arbeit konzentriert sich auf die Verbesserung der Effizienz bei der Optimierung großer Sprachmodelle, insbesondere bei der Verwendung adaptiver Gradienten-basierter Optimierungsverfahren wie Adam oder LAMB. Diese Methoden erfordern jedoch viel Speicherplatz, was zu einem großen Aufwand für die Trainingsprozesse führt. Um diese Herausforderung zu überwinden, wurde CAME vorgeschlagen, ein neues Optimierungsverfahren, das auf dem Konzept der Non-negative Matrix Factorization (NMF) basiert. CAME reduziert den Speicherverbrauch durch die Verwendung von NMF und unterstützt adaptive, konfidenz-basierte Optimierung, wobei die Konfidenz zwischen vorhergesagten und generierten Updates berücksichtigt wird. Dies führt zu einer signifikanten Verbesserung der Konvergenzgeschwindigkeit und des Performance-Levels im Vergleich zu existierenden Optimierungsverfahren. Die Ergebnisse der Experimente zeigen, dass CAME nicht nur bei der Training von Sprachmodellen effektiv ist, sondern auch bei der Ausführung von Downstream-Aufgaben wie MNLI-m, SST-2, MRPC und SQuAD v1.1 und v2.0.</sample>
    <sample id="193">Es wird nicht explizit erwähnt, wie viele Annotatoren verwendet wurden, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="194">Die Autoren gehören der University of Washington an.</sample>
    <sample id="195">Das Video präsentiert ein neues Verfahren zur Erklärbaren Fragebeantwortung (XQA) durch die Analyse eines Hierarchischen Fragezerlegungsbaumes (RoHT). Es hebt die Grenzen bestehender Methoden hervor, die nur auf strukturierten Wissensbasen ausgeführt werden können und die Schwierigkeit, komplexe Fragen in natürlicher Sprache zu beantworten. Die Studie zeigt, dass die Integration von Wissen aus verschiedenen Quellen für die Beantwortung komplexer Fragen von großer Bedeutung ist. Das RoHT-Verfahren verwendet einen BART-basierten Fragezerlegungsdekomponierer und einen BART-basierten Fragegeneratoren, um den Hierarchischen Fragezerlegungsbaum zu erstellen und dann probabilistisch über diesen Baum zu schließen, um Antworten mit Wahrscheinlichkeiten für jede Frage im Baum bereitzustellen. Die Herausforderungen liegen in der Bestimmung der Granularität der Fragezerlegung und dem Finden der optimalen Lösung unter verschiedenen Möglichkeiten aus verschiedenen Wissensquellen. Der RoHT-Framework besteht aus zwei Hauptphasen: der Verständnisphase und der Schließphasen. In der Verständnisphase wird der komplexe Frage ein Hierarchischer Fragezerlegungsbaum zugeordnet, während in der Schließphasen probabilistische Schließungen über den Baum durchgeführt werden.</sample>
    <sample id="196">Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="197">The State-of-the-Art in Chat-Oriented Dialogue Systems</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, um sicherzustellen, dass sie robust sind und nicht von der Länge des Kontextes beeinflusst werden.</sample>
    <sample id="199">Nein, das mehrsprachige Training hat nicht zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität nicht im Voraus.</sample>
    <sample id="201">SOTA MT Metrics und Expert-based human evaluation.</sample>
    <sample id="202">Ja, die Regression wirkt sich auf bestimmte NER-Typen aus.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Positionen von Menschen berücksichtigt, die durch ihre Demografie, Identität und Lebenserfahrungen geprägt sind. Diese Perspektiven können das Verständnis und die Ergebnisse der Forschung beeinflussen.</sample>
    <sample id="204">BLOOM wurde durch Adapter angepasst.</sample>
    <sample id="205">Das Vortragsthema konzentriert sich auf die Untersuchung politischer Bias in Sprachmodellen und deren Auswirkungen auf下游任务. Es wird betont, dass das Training von Sprachmodellen mit einer Mischung von Daten sowohl Vorteile als auch Nachteile hat. Die Präsentation untersucht, wie politische Bias in Sprachmodellen entsteht und welche Rolle das Prätraining-Daten spielt. Zwei Hauptfragen werden gestellt: Wie kann man den politischen Bias in Sprachmodellen bewerten? Und wie beeinflusst der politische Bias die Leistung von Sprachmodellen in下游任务? Eine automatische Bewertungsmethode wird vorgestellt, die auf politikwissenschaftlichen Literatur basiert. Die Ergebnisse zeigen, dass verschiedene Sprachmodelle unterschiedliche politische Bias aufweisen. Es wird auch gezeigt, wie die politische Bias durch die Art des Prätraining-Datensatzes beeinflusst wird. Die Studie verwendet zwei verschiedene Datensätze – News Media und Social Media – um zu untersuchen, wie sich die politische Bias bei verschiedenen Modellen verändert. Die Ergebnisse deuten darauf hin, dass die politische Bias in Sprachmodellen durch die Art des Prätraining-Datensatzes beeinflusst wird.</sample>
    <sample id="206">Das Modell, das für das Transferlernen verwendet wird, ist RoBERTA-base + classifier head.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, enthalten keine Überschneidung zwischen Trainings- und Testdaten und vermeiden das Überfitting auf den Evaluationssätzen.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode kann die Planungsqualität um einen großen Margenbereich verbessern, was bedeutet, dass sie den Gewinn gegenüber der stärksten Baseline beträchtlich erhöht.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit werden vier verschiedene kleinere Modelle experimentiert: T5, Flan-T5, GPT-3 und InstructGPT.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Das Video präsentiert einen wissenschaftlichen Vortrag über die Konjunkt-Längen in der englischen Sprache, speziell bezogen auf die Minimierung von Abhängigkeitslängen und die Struktur von Koordination. Der Sprecher diskutiert verschiedene Methoden zur Analyse von Koordinationsstrukturen, wie sie von Bouquet/Stanford, Chain/Moscow, Conjunction-headed/Prague und Multi-headed/London beschrieben werden. Es wird gezeigt, wie die Wortreihenfolge dazu beiträgt, die Abhängigkeiten zu minimieren, was durch Beispiele wie "Marge read this absolutely fascinating book about bees yesterday" illustriert wird. Die Präsentation verwendet Diagramme und Texte, um die statistischen Daten aus dem Penn Treebank zu veranschaulichen, die zeigen, dass links stehende Konjunktionen tendenziell kürzer sind als rechts stehende, insbesondere wenn der Regisseur links ist. Zudem wird auf die Bedeutung der Konjunkt-Längen für die Verständlichkeit und den Rhythmus der Sprache eingegangen.</sample>
    <sample id="217">Das Video präsentiert ein Forschungsprojekt zur generativen Dialoggestaltung für mehrere Attribute, das sich auf die Erstellung eines prompt-basierten disentangled kontrollierbaren Dialogmodells konzentriert. Das Modell erzeugt spezifische Promptvektoren für jedes Attribut und verwendet eine Entwirrungsverlustfunktion, um verschiedene Attribute zu trennen. Zudem wird ein einheitliches, referenzloses Bewertungskonzept, MAE (Multi-Attribute Evaluation), entwickelt, um die Textqualität und Kontrollierbarkeit zu bewerten. Die Ergebnisse zeigen, dass das vorgeschlagene MAE bessere Textqualität und Kontrollierbarkeit erreicht als andere Methoden. Darüber hinaus korreliert MAE stärker mit menschlichen Urteilen zur Bewertung von CDG (Compositional Dialogue Generation).</sample>
    <sample id="218">Google</sample>
    <sample id="219">Das Video präsentiert ein Forschungsprojekt, das sich mit der Identifizierung finanzieller Signale in Geschäftsberichten befasst. Es wird eine mehrstufige Pipeline vorgestellt, die dazu dient, relevante Informationen aus den Berichten zu extrahieren und zu hervorheben. Die Pipeline umfasst verschiedene Stufen wie die Dokumentsegmentation, die Relationserkennung und das Feinabstimmen auf dem Trainingsdatensatz. Der Fokus liegt auf der Nutzung eines human-annotierten Evaluationsdatensatzes und der Anwendung von maschinellem Lernen zur automatischen Hervorhebung von finanziellen Signalen. Das Projekt zielt darauf ab, effiziente und genaue Methoden für die Analyse von Geschäftsberichten zu entwickeln, um die Arbeit von Finanzpraktikern zu erleichtern.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">Deutsch-Englisch und Englisch-Deutsch.</sample>
    <sample id="222">Das Video präsentiert ein wissenschaftliches Thema, das sich mit der Fragestellung beschäftigt, wie man die Leistung von Suchmaschinen und Lesermodellen verbessern kann, um Fragen aus einem bestimmten Bereich zu beantworten, wenn der zugrunde liegende Korpus nicht spezifisch für diesen Bereich ist. Der Fokus liegt auf der Untersuchung von Dateninterventionen, um die Generalisierbarkeit von Modellen zu verbessern. Es wird vorgeschlagen, verschiedene Arten von Dateninterventionen zu untersuchen, um herauszufinden, welche die beste Performance erzielen. Die Präsentation zeigt auch, dass die Effektivität dieser Interventions abhängig von der Art der Datenverschiebung ist. Zudem wird eine Methode vorgestellt, die es ermöglicht, die Leserleistung um bis zu 24% und die Suchleistung um 22% in F1 zu verbessern.</sample>
    <sample id="223">Der Referent*in ist Shangbin Feng.</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind UDA, Sent-LabelB, CATS-C3G, VecAlign und MASSign.</sample>
    <sample id="225">5 Experten geschriebene Anweisungen werden für Training verwendet, während 10 breite Gruppen und 5 Experten geschriebene Anweisungen für Tests verwendet werden.</sample>
    <sample id="226">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="227">Das Video präsentiert die Arbeit "Pangu: A Unified Framework for Grounded Language Understanding" von Yu Gu aus der Ohio State University. Es beginnt mit einer Einführung in das Konzept der "Grounded Language Understanding", das die Verbindung zwischen Sprache und realen Weltstrukturen herstellt, um die Effizienz und Generalisierbarkeit von Sprachmodellen zu verbessern. Die Präsentation zeigt, wie moderne Sprachmodelle wie BERT, T5 und GPT-3 hauptsächlich auf Textkorpora trainiert werden, was ihre Fähigkeit beeinträchtigt, komplexe und kontextuelle Informationen zu verarbeiten. Es wird vorgeschlagen, dass die direkte Generierung von Plänen (Programmen) möglicherweise nicht die optimale Methode für die Nutzung von Sprachmodellen zur Verständigung ist. Stattdessen wird ein neues Framework vorgestellt, das es ermöglicht, LMs (Language Models) auf die Diskriminierung zu konzentrieren und generell anwendbar zu machen. Dieses Framework soll die Effizienz und Generalisierbarkeit von Modellen verbessern, indem es die Interaktion zwischen Agenten und Umgebungen unterstützt und nur gültige Kandidatenpläne vorschlägt, während LMs sich auf die Bewertung konzentrieren.</sample>
    <sample id="228">Die Autoren haben ihre Experimente an den Datensätzen AG News, MIND, SST2 und Enron Spam durchgeführt.</sample>
    <sample id="229">The presentation discusses the importance of text revision in argumentative writing, emphasizing that it is an essential part of the process and often involves recursive refinement until optimal phrasing is achieved. The persuasive impact of the phrasing directly influences the audience's perception. The speaker introduces two tasks: suboptimal-claim detection and claim improvement suggestion. Suboptimal-claim detection involves deciding whether a given claim needs revisions or can be considered to be phrased optimally. Claim improvement suggestion requires selecting the types of quality issues that should be improved when revising the claim. The presentation also highlights challenges such as representativity and reliability, model complexity and architecture, and topical and user bias. It concludes with a summary of findings, including the effective use of revision-based data for the given tasks and the impact of contextual information on task-specific quality issues.</sample>
    <sample id="231">NACHOS ist ein offener, 1,18 Millionen Wörter umfassender Open-Source-Datensatz für die medizinische Sprache, der aus verschiedenen medizinischen Domänen stammt und in einem natürlichen Stil geschrieben wurde.</sample>
    <sample id="232">Der Referent ist Markus Freitag.</sample>
    <sample id="233">The presentation discusses the challenges and solutions for simultaneous speech translation (SimuST), focusing on the limitations of current models such as specific architecture training, long training procedures, and latency issues. The proposed solution, EDAtt, leverages existing offline ST models without retraining or adapting architectures, using only one model per latency regime to handle latency through specific parameters. It utilizes encoder-decoder attention mechanisms to decide whether to emit or emit a partial translation based on attention points, ensuring that information is stable and sufficient. The study demonstrates that EDAtt outperforms other strategies applied to offline models, achieving state-of-the-art performance while being the fastest strategy when considering actual elapsed time.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse, da sie die Qualität der Übersetzung stark beeinflusst.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind nicht explizit genannt, aber sie werden als "5 expert-written instructions" bezeichnet.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mit einem spezifischen Aufgabentraining (task-specific training) zu testen, um ihre Fähigkeit zur Integration von Informationen aus verschiedenen Quellen zu bewerten.</sample>
    <sample id="238">Das Video präsentiert "MeetingBank", ein Benchmark-Dataset für die Zusammenfassung von Stadtversammlungen, das durch die Segmentierung von Stadtversammlungen und das Paaren mit expertenverfassten Zusammenfassungen erstellt wurde. Das Dataset umfasst 1.566 Versammlungen aus fünf Städten (Denver, Seattle, Long Beach, Alameda und Boston) mit insgesamt 3.779 Stunden Transkripte und 4.852 Zusammenfassungen. Es wird als wertvolles Werkzeug für Forscher zur Entwicklung fortschrittlicher Zusammenfassungsmodelle verwendet und bietet Einblicke in den Entscheidungsprozess von Stadtbehörden. Die Evaluierung der Modelle zeigt, dass Extraktive Modelle wie Extract-Oracle einen hohen Rouge-2-Wert von 46,6% erreichen, während Abstractive Modelle wie DialogLM den besten Rouge-2-Wert von 60,12% erzielen. Zusätzlich wird eine menschliche Bewertung vorgestellt, die die Qualität der automatischen Zusammenfassungen bewertet.</sample>
    <sample id="239">Natürlich, hier ist der Übersetzung des englischen Inhalts ins Deutsche:

---

**Experimental Results**

- Beispielqualität ist wichtiger als Ähnlichkeit zur Quellsprache.
- Spezialisierte SOTA-Systeme haben eine erhebliche Vorteil.
- PaLM ähnelt Google Translate.

**Insights from MQM:**
- Fluency of PaLM comparable to SOTA.
- Accuracy scores generally lower.
- "Style/Awkward" generally lower for PaLM.

---</sample>
    <sample id="240">### Übersetzung des englischen Inhalts

#### Titel: "Weaker Than You Think: A Critical Look at Weakly Supervised Learning"

#### Zusammenfassung:
Die Präsentation untersucht die aktuellen Entwicklungen in der schwach überwachten Lernung (WSL) und diskutiert, warum diese Technik oft als weniger effektiv angesehen wird. Es wird betont, dass WSL-Methoden oft über ihre praktische Anwendbarkeit verallgemeinert werden und dass sie oft von reinem Trainieren auf schwach überwachten Daten ausgehen, ohne dass genügend saubere Validierungssamples zur Verfügung stehen.

#### Warum schwach überwachte Lernung?
- Schwach überwachte Lernung lindert den Annotierungsproblematik.
- Allerdings sind schwache Labels störend.
- Schwach überwachte Lernung (WSL) trainiert Modelle, die trotz störender Daten generalisieren können.

#### Eine allgemeine Behauptung in neueren WSL Arbeiten:
"Wir trainieren Modelle nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%."

#### Unsere Forschungsfragen:
1. Ist sauberes Validierungssample notwendig?
2. Wie viele saubere Samples benötigen WSL-Methoden?
3. Wie kann man die vorhandenen sauberen Samples effizienter nutzen?

#### Hauptergebnisse:
- Ein sauberes Validierungssample ist unentbehrlich.
- WSL-Methoden profitieren von mehr sauberen Validierungssamples.
- Es ist besser, die sauberen Samples für das Training zu verwenden (z.B. LoRA).

#### Empfehlungen:
- Berichte die Auswahlkriterien der Modelle.
- Verwende Few-Shot-Learning-Methoden als Baseline.
- Stets Anwendung kontinuierlichen Feinabstimmens (CFT).

#### Schlussfolgerung:
- Neuere WSL-Methoden erfordern saubere Samples.
- Sie überschätzen ihre praktische Anwendbarkeit.

#### Dankeschön

---

Diese Übersetzung bietet einen Überblick über die wesentlichen Punkte der Präsentation und deren Bedeutung im Kontext der schwach überwachten Lernung.</sample>
    <sample id="241">Das Vortragsthema konzentriert sich auf die Entwicklung und Evaluierung eines human-in-the-loop Ansatzes zur Früherkennung von Missinformationen im Kontext der COVID-19-Behandlungsmethoden. Der Vortrag beginnt mit einer Kritik an den aktuellen Methoden zur Missinformationserkennung, die oft unrealistisch evaluiert und nicht menschenzentriert sind. Es wird betont, dass Systeme in der Regel auf unrealistischen Standarddatensätzen getestet werden und dass es die Möglichkeit gibt, dass Informationen verloren gehen können.

Der Fokus liegt darauf, wie man die Komplexität zwischen Systemen und menschlichen Inhaltmoderatoren oder Faktenprüfern berücksichtigt. Die Präsentation beschreibt einen end-to-end-Misinformationserkennungssystem, das von Tweets zu handelbaren Ausgängen führt, und integriert gut menschliche Rückmeldung, indem Menschen wichtige Entscheidungen während des Workflow-Prozesses treffen. Dieses System wurde speziell für die Bewertung von COVID-19-Behandlungsmissinformationen auf Twitter implementiert.

Die Evaluation erfolgt durch die Detektion von irreführenden Behauptungen und die Überprüfung von Verletzungen von Plattformrichtlinien. Es wird eine effiziente Methode vorgeschlagen, um die Efficacy der Methode zu messen, basierend auf der Fähigkeit, irreführende Behauptungen frühzeitig zu erkennen. Die Definition von "früh" basiert auf der relativen Zeit, die zwischen der Erstauftretenszeit einer Behauptung in einem Debunking-News-Artikel und ihrer Erkennung durch das System vergeht.

Zusammenfassend zielt das Projekt darauf ab, ein hilfreiches und realistisches Workflow-Modell für die Entwicklung weiterer menschen-in-the-loop-Frameworks zur Misinformationserkennung zu schaffen.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen die Likert-Skala-Evaluation, die Annotierung von Verhaltensweisen im Chat (ABC-Eval) und die Vergleichs-Evaluation.</sample>
    <sample id="243">Es gibt fünf Autoren an der Arbeit beteiligt.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass Servin ein Richter ist und Kea ein Baker.</sample>
    <sample id="245">Das Video präsentiert ein Forschungsprojekt, das sich mit der Identifizierung hochqualifizierter Arbeiter auf der Plattform MTurk befasst. Es wird eine Pipeline vorgestellt, die aus zwei Phasen besteht: der Qualifikationsphase und der Ausdauerphase. In der Qualifikationsphase werden Arbeiter anhand ihrer Leistung in verschiedenen Aufgaben wie Dokumenten- und Zusammenfassungsaufgaben bewertet und in Kategorien wie Gold, Silber, Bronze und Block eingeteilt. Die Ausdauerphase testet die Fähigkeit der Arbeiter, unter hohem Arbeitsdruck zu arbeiten. Das Projekt zeigt, dass durch diese Methode effizient hochqualifizierte Arbeiter identifiziert werden können, was sowohl für die Effizienz als auch für die Qualität der Arbeitsergebnisse von Vorteil ist. Zudem wird eine Referenzbasierte Aufgabe vorgeschlagen, um die Genauigkeit der Ergebnisse zu verbessern. Das Video endet mit einer Diskussion über die Anwendungsmöglichkeiten und die Herausforderungen dieser Methode im Kontext von MTurk.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub unter mpoeins/kitmus gefunden werden.</sample>
    <sample id="247">Das Video präsentiert das Projekt FactKG, das sich mit der Verifikation von Fakten auf Basis von Wissensgraphen befasst. Es wird eine neue Dataset-Struktur vorgestellt, die aus 108K natürlichen Sprachansprüchen besteht und fünf Arten von logischen Schließfolgerungen unterstützt: One-hop, Konjunktion, Existenz, Multi-hop und Negation. Diese Fähigkeiten ermöglichen es, verschiedene sprachliche Strukturen wie colloquialen Stil und Präsuppositionen zu verarbeiten. Die Verwendung von graphischen Beweisen in der Modellierung führt zu besseren Ergebnissen im Vergleich zu Baselines ohne solche Evidenz. Das Dataset enthält auch verschiedene linguistische Muster, einschließlich colloquialer Stilansprüche sowie geschriebene Ansprüche, um die Praktikabilität zu erhöhen. Die Studie zeigt, dass die Integration von graphischen Beweisen zu verbesserten Leistungen führt. Zudem werden verschiedene Methoden zur Erstellung von paraphrasierten Ansprüchen vorgestellt, um die Effektivität der Verifikation zu steigern.</sample>
    <sample id="248">Nein, die Annotatoren sind nicht ausgewogen.</sample>
    <sample id="249">Die Sätze wurden durch die Addition von Präfixen oder Suffixen, Verwendung von langen Präfixen, Verwendung von Adverbien, Hinzufügen eines Klammerabschnitts, Hinzufügen einer Klausel und Zitieren von Informationen durcheinandergebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Beurteilung von Dialogqualität in verschiedenen Dimensionen wie Relevanz, Konsistenz und emotionaler Verständnis.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China, Microsoft Research Asia und Beijing Jiaotong University an.</sample>
    <sample id="252">Das Video präsentiert ein neues Ansatz für die Vorauswahl von Rechtsvorgängen (Prior Case Retrieval, PCR) in der Rechtssprache, genannt U-CREAT. Es wird eine unüberwachte Methode vorgestellt, die auf der Extraktion von Ereignissen basiert und ohne spezifische Korpus-Trainingsdaten auskommt. Die Studie konzentriert sich auf die Entwicklung eines neuen Datensatzes namens IL-PCR, der speziell für den indischen Rechtssystem entwickelt wurde. Der Fokus liegt auf der Verbesserung der Performance und des Inferenzzeitschrittes im Vergleich zu traditionellen Methoden wie BM25. Die Ergebnisse zeigen, dass die Ereignisbasierte Methode U-CREAT sowohl in der Genauigkeit als auch in der Effizienz übertrifft, was durch höhere F1-Scores und kürzere Inferenzzeiten bestätigt wird. Das Projekt ist am Department of Computer Science and Engineering der IIT Kanpur durchgeführt worden und wurde bei ACL 2023 präsentiert.</sample>
    <sample id="253">Das Vortragsthema konzentriert sich auf die Entwicklung eines Modells namens DisorBERT, das speziell für die Detektion von Anzeichen psychischer Störungen in sozialen Medien entwickelt wurde. Das Modell nutzt eine doppelte Domänenadaptation und gezielte Maskierung, um effektiv zwischen verschiedenen psychischen Störungen zu unterscheiden. Es wird gezeigt, dass DisorBERT im Vergleich zu BERT bessere Ergebnisse bei der Detektion von Depressionen und Selbstschädigung erzielt. Die Evaluation zeigt, dass das Modell eine solide Balance zwischen dem Finden von Nutzern und dem korrekten Etikettieren von Störungen erreicht hat. Zukunftliche Arbeiten beinhalten die探索 der Anwendung unterschiedlicher Lexikalischer Ressourcen, die noch stärker auf die Zielaufgaben spezialisiert sind, sowie die Nutzung klinischer Daten zur Training von spezialisierten Sprachmodellen.</sample>
    <sample id="254">Das Paper beschäftigt sich mit der Dokumenten-Level-Distanzrelationsextraktion (DocRE) und stellt ein neues Framework vor, das die Labelqualität von DS-Daten durch Uncertainty Guided Label Denoising verbessert. Es wird eine iterative Re-Label Strategie mit dynamischen Klassensicherheitsschwellwerten vorgeschlagen, um die Zuverlässigkeit von Instanz-Level-Pseudo-Labels zu messen. Die Methode nutzt MC Dropout zur Modellunsicherheitsschätzung und optimiert die Labelqualität durch das Filtern hochunsicherer Pseudo-Labels. Experimente auf zwei öffentlichen Datensätzen zeigen signifikante Leistungsverbesserungen gegenüber bestehenden Baselines, insbesondere bei der Nutzung des denoisierten DS-Datensatzes.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es sich um eine 5-shot Prompting-Situation handelt.</sample>
    <sample id="257">Die Autoren haben vier offene Domänen-Dialogmodelle evaluiert.</sample>
    <sample id="258">The presentation explores the potential of large language models (LLMs) as an alternative to human evaluations for assessing text quality. It begins by introducing the authors, Cheng-Han Chiang and Hung-yi Lee from National Taiwan University, and sets the stage with the question: "Can Large Language Models Be an Alternative to Human Evaluations?" The overview outlines the method, related works, motivation, experiments, and more questions.

The study proposes using LLMs to evaluate texts by providing them with instructions and asking them to rate samples, which is referred to as 'LLM evaluation'. This approach is motivated by the instability and difficulty of reproducing human evaluation. The presentation highlights that while the idea of using LLMs for evaluation seems natural and widely adopted, there were no prior works exploring this concept when the paper was submitted to ACL 2023.

The experiment setting involves using GPT-3 to evaluate stories generated by GPT-2 and written by humans. Four rating attributes—grammar, coherence, likeability, and relevance—are considered. English teachers conduct human evaluations on the same stories using the same instructions as in the LLM evaluation. The results show that larger LLMs (text-davinci-003 and ChatGPT) exhibit a clear preference toward human-written stories, while smaller LLMs (T0 and text-curie-001) do not show meaningful preferences.</sample>
    <sample id="259">Das Video präsentiert ein Forschungsprojekt zur kroatisch-lingualen semantischen Parsierung, das verschiedene natürliche Sprachen und Bedeutungsrepräsentationen berücksichtigt. Es wird ein neues Benchmark namens XSemPLR vorgestellt, der für die Parsierung von mehreren Sprachen und Bedeutungsrepräsentationen verwendet wird. Der Fokus liegt auf der Erstellung eines einheitlichen Datensatzes mit 9 Domänen, 5 semantischen Tabellen, 8 Bedeutungsrepräsentationen und 22 Sprachen aus 15 Sprachfamilien. Die Studie untersucht sechs verschiedene Trainings- und Evaluierungssettings, einschließlich Monolingual-, Few-shot- und Multilingual-Modellen sowie Zero-shot- und Few-shot-Transfer-Learning. Ergebnisse zeigen, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erreicht, während multilinguale LMs wie mBERT und mT5 noch nicht ausreichen, um kroatisch-linguale Aufgaben zu bewältigen.</sample>
    <sample id="260">Es sind neun Autoren an der Arbeit beteiligt.</sample>
    <sample id="261">Ein guter Planer sollte effizient, flexibel, kreativ und zielgerichtet sein. Er sollte auch die Fähigkeit haben, verschiedene Optionen zu überprüfen und Entscheidungen zu treffen, um das beste Ergebnis zu erzielen.</sample>
    <sample id="262">Es gibt sieben Autoren an der Arbeit beteiligt.</sample>
    <sample id="263">Das Video präsentiert ein Forschungsprojekt zur Verbesserung der Klassifikation in der In-Context-Learning-Technologie, speziell bei der Bearbeitung von Texten mit unterschiedlichen Bewertungen. Es wird eine Vielzahl von Beispielen gezeigt, die verschiedene emotionale Kontexte und Bewertungen enthalten. Die Präsentation fokussiert auf die Herausforderungen, die durch die Unstabilität des In-Context-Learning-Verfahrens bedingt werden, insbesondere durch Designentscheidungen und das Vorhandensein von Label-Bias. Es wird eine Typologie von Label-Bias beschrieben, die drei Hauptarten umfasst: Vanilla-Label-Bias, Kontext-Label-Bias und Domain-Label-Bias. Die Präsentation zeigt auch, wie die Domain-Context Calibration (DC) diese Bias-Arten effektiv reduzieren kann und somit die Leistung des In-Context-Learning verbessert. Zudem wird eine grafische Darstellung der Ergebnisse gezeigt, die den positiven Effekt der DC-Methode auf die Modelle hervorhebt, insbesondere bei Aufgaben mit großem Domain-Label-Bias.</sample>
    <sample id="264">Das Forschungsprojekt TAVT (Towards Transferable Audio-Visual Text Generation) von Zhejiang University zielt darauf ab, ein Modell zu entwickeln, das audiovisuelle Daten in Texte umwandelt und dabei die Herausforderungen der Datenannotation und -korrelation berücksichtigt. Die Studie identifiziert zwei Hauptlimitierungen: die aufwendige und teure Datenannotation sowie die Degradation bestehender Modelle bei der Verwendung in neuen Domänen. Als Herausforderung wird die multi-modale Domain Shift genannt, die durch die unterschiedlichen Charakteristiken von Audio und Video entsteht. Das Projekt motiviert sich durch die Korrelation zwischen Audio und Video in natürlichen Ereignissen und untersucht, wie Timber als ein intrinsisches Objektmerkmal verwendet werden kann, um diese Unterschiede zu berücksichtigen. Der Ansatz besteht aus einem Audio-Visuellen Meta-Mapper Network, einem Audio-Visuellen Encoder &amp; Language Model Generator und Counterfactual Contrastive Learning. Diese Methoden sollen es ermöglichen, schnell und effizient auf neue multimodale Domänen anzupassen, ohne viel Labeling-Daten zu benötigen.</sample>
    <sample id="265">Die Referent*in ist Vasudha Varadarajan.</sample>
    <sample id="266">Polish Academy of Sciences</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind "Accuracy/Omission".</sample>
    <sample id="269">Natürlich, hier ist der übertragen englische Inhalt in die deutsche Sprache:

---

**Titel: Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems**

**Referenten:** Sarah E. Finch, James D. Finch und Jinho D. Choi

**Inhalt:**

1. **Einführung**
   - Präsentation des Titels "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems"
   - Erwähnung der Referenten: Sarah E. Finch, James D. Finch und Jinho D. Choi
   - Verbindung zu Emory University und dem Emory NLP Research Lab

2. **Vergleichende Bewertung**
   - Darstellung von Dialogen zwischen Menschen und Chatbots
   - Visualisierung von Bewertungen durch einen Richter mit einem Hammer
   - Erklärung des Likert-Skalen-Systems zur Bewertung der Qualität von Dialogen

3. **Dimensionen der Dialogqualität**
   - Relevanz
   - Konsistenz
   - Emotionale Verständigung

4. **Likert-Bewertung**
   - Anleitung zur Bewertung der Relevanz der Chatbot-Antworten auf einer Skala von 1 bis 5
   - Beispiel für eine Bewertung mit einem Richter und einer Skala

5. **ABC-Eval Bewertung**
   - Annotieren von Verhaltensweisen im Chat (ABC-Eval)
   - Beispiele für unangemessene Verhaltensweisen wie Irrelevanz, Mangel an Empathie und Selbstkontradiktion

6. **ABC-Eval Verhaltensweisen**
   - Coherence: Ignoring Partner, Irrelevant
   - Consistency: Self Contradiction, Partner Contradiction
   - Knowledge: Incorrect Fact, Commonsense Violation
   - Emotional Understanding: Empathetic Response, Lack of Empathy

7. **Experimente**
   - 4 offene Domänen Dialogmodelle
   - 100 Mensch-Chatbot-Konversationen pro Modell

8. **Basisbewertungen**
   - Vergleich der Bewertungen von Turn Likert, Dialogue Likert und Comparative

9. **Inter-Annotator Übereinstimmung**
   - Grafische Darstellung der Inter-Annotator Übereinstimmung mit Krippendorffs Alpha-Werten

10. **Prädiktive Gültigkeit**
    - Prozentuale Erklärung der Qualität für verschiedene Modelle
    - Analyse der Fehlerraten für verschiedene Modelle

11. **Erhöhte Gültigkeit**
    - Diagramm zur Erklärung der Qualität für verschiedene Modelle

12. **Danksagung**
    - Links zum Paper, GitHub und Kontaktinformationen

---

Bitte beachten Sie, dass diese Übersetzung den Kontext und die Struktur des Originalinhalts beibehält und dabei die Fachbegriffe korrekt wiedergibt.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">Continuous fine-tuning</sample>
    <sample id="272">Es gibt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="273">The English content translates to:

- When does translation require context?
  - Word-level context usage
  - Thematic analysis

- How well do models handle context-dependent translations?
  - Multilingual Discourse-Aware (MuDA) benchmark
  - Model evaluation</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Das Video präsentiert ein Projekt namens "IndicMT Eval", das sich mit der automatischen Bewertung von Maschinentranslationen für indische Sprachen befasst. Es wird betont, dass die Bewertungsmethoden für andere Sprachen als die für Englisch vorgeschlagenen untersucht werden sollten, da sie ihre eigenen Grammatikregeln und Vokabular haben. Das Projekt konzentriert sich auf fünf indische Sprachen: Tamil, Malayalam, Hindi, Marathi und Gujarati, die zu zwei verschiedenen Sprachfamilien gehören. Die Daten wurden aus dem Flores-Dataset gesammelt und 200 zufällige Sätze wurden ausgewählt, um Translationsausgaben von verschiedenen Systemen wie mBART, Bing API, Google API, CVIT-IITI, IndicTrans, mT5 und NLLB zu generieren. Diese Ausgaben wurden von bilingualen Expertenannotatoren mit dem MQM-Modell bewertet, das Fehlerkategorien wie Genauigkeit, Fluide und andere spezielle Kategorien verwendet. Die Ergebnisse zeigen, dass einige Systeme wie mBART und Google API gute Leistungen bei der Genauigkeit und Fluide erzielen, während andere wie IndicTrans und NLLB in Bezug auf die Genauigkeit und Fluide weniger effektiv sind.</sample>
    <sample id="277">Die neue Methode hat den Namen "Multiset Tagging and Latent Permutations".</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Art, um die Wörter zu finden, die die Personas von markierten Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="279">Die Autoren gehören der University of Washington (UW) und Carnegie Mellon University (CMU) an.</sample>
    <sample id="280">Das Paper präsentiert MultiEMO, ein multimodales Framework zur Emotionenerkennung in Konversationen. Es verwendet einen Aufmerksamkeitsbasierten, korrelationssensitiven Ansatz, um die Text-, Audio- und visuellen Modalitäten zu fusionieren. Die Hauptbeiträge umfassen die Entwicklung eines visuellen Feature Extractors namens VisExtNet, der effektiv visuelle Hinweise von Interlocutoren ohne redundante Umgebungsinformationen extrahiert. Zudem wird das Multitask-Modell MultiAttn vorgestellt, das auf bidirektionalen multihead-cross-attention-Lagen basiert, um komplexe Korrelationen zwischen den Modalitäten zu modellieren. Das Framework verbessert die Klassifizierung von seltenen und semantisch ähnlichen Emotionen und erreicht bei beiden Benchmark-Datasets MELD und IEMOCAP die besten Ergebnisse.</sample>
    <sample id="281">The presentation discusses the importance of context in translation, emphasizing that translation often depends on the surrounding context. It highlights the challenges in evaluating context-dependent translations, noting that only a small portion of words rely on context and existing methods support limited discourse phenomena and languages. The study aims to identify when translation requires context and how well models handle these context-dependent translations. A key metric introduced is Conditional Cross-Mutual Information (CXMI), which measures the usage of context by machine translation models. The research also explores thematic analysis of high CXMI words, focusing on POS tags like pronouns and verb forms, as well as vocabulary items and individual tokens. The MuDA benchmark is used for model evaluation, showing that context-aware models perform better on some phenomena such as formality and lexical cohesion but struggle with ellipsis, pronouns, and verb form. The study concludes with a dataset-agnostic benchmark for document-level machine translation, identifying discourse phenomena systematically without prior linguistic knowledge.</sample>
    <sample id="282">This research introduces StoryTrans, a method for transferring the author's style in non-parallel stories while preserving discourse representations and enhancing content. The challenge lies in imitating linguistic choices at the discourse level and associating author styles with specific writing topics. The solution involves two stages: discourse representation transfer and content preservation enhancing. The first stage uses a masked language model to generate a masked story, which is then fed into a pointer network to recover the original story. The second stage employs an auto-encoder loss to reconstruct the original story from the encoded representation. Experiments on Chinese and English datasets demonstrate that StoryTrans effectively transfers author styles while maintaining the original story's content and structure.</sample>
    <sample id="283">Bouquet/Stanford</sample>
    <sample id="284">The presentation introduces FSUIE, a novel framework for enhancing Universal Information Extraction (UIE) tasks. It addresses the limitations of existing UIE models that heavily rely on precise span boundaries and mismatch between feature extraction and information extraction. FSUIE proposes a fuzzy span mechanism to model the boundary as a continuous distribution, improving the model's ability to handle ambiguity. The framework also utilizes efficient fuzzy span attention to adaptively adjust the attention span, guiding the proper distribution of attention. FSUIE achieves significant improvements over UIE-base models across various information extraction tasks, including Named Entity Recognition (NER), Relation Extraction (RE), and Argument Structure Extraction (ASTE). The model's effectiveness is demonstrated through comprehensive experiments and visualization of attention scores, highlighting its superior performance and generalization capabilities.</sample>
    <sample id="285">The presentation discusses the challenges of factual error correction (FEC) in dialogue summarization, where summaries generated by models often contain factual errors. It highlights two common approaches: directly designing better summarization models for factuality and using Factual Error Correction (FEC) models to correct model-generated summaries. The speaker explains that FEC models can be evaluated using factuality metrics like FactCC, which provide an overall score but may not be reliable on their own. The introduction of reference corrections is proposed as a solution, providing more valuable data for training FEC models compared to pseudo data and creating conditions for a more comprehensive evaluation of FEC model performance.</sample>
    <sample id="286">Sarah E. Finch, James D. Finch und Jinho D. Choi</sample>
    <sample id="287">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="288">Die Datensätze, die für das Testen syntaktischer Phänomene verwendet werden können, sind "BLIMP", "SyntaxGym" und "CrowS".</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind FT_w, BOND, COSINE, MLC und L2R.</sample>
    <sample id="291">Das Modell wird anhand von 11 Aufgaben evaluiert, darunter Medical Report Specialties, Medical-DET, Medical-CAT, Medical-DET, Medical-CAT, Medical-EDET, Medical-DET, Medical-CAT, Medical-DET, Medical-CAT und Medical-EDET.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit den Daten NACHOS und NBDW trainiert.</sample>
    <sample id="295">Der Referent ist Adam Przepiórkowski.</sample>
    <sample id="296">Das Projekt EPIC (English Perspectivist Irony Corpus) zielt darauf ab, die Vielfalt der Ironieperzeption in englischer Sprache zu erforschen und zu annotieren. Es basiert auf einer Vielzahl von Quellen wie Reddit und Twitter und umfasst einen Zeitraum von Januar 2020 bis Juni 2021. Der Korpus umfasst etwa 300 Text- und Antwortpaare aus fünf verschiedenen Varietäten der englischen Sprache: Australien, Indien, Irland, den Vereinigten Staaten und dem Vereinigten Königreich. Die Annotierung erfolgt durch 74 verschiedene Annotatoren, die jeweils 200 Texte pro Annotator annotieren mussten, wobei jede Annotierung durchschnittlich fünfmal durchgeführt wurde. Ziel ist es, die Bedeutung eines perspektivbewussten Ansatzes für die Erkennung von Ironie im Vergleich zu einem standardmäßigen nicht-perspektivbewussten Ansatz zu verstehen, der auf einem Goldstandard-Dataset trainiert wird.</sample>
    <sample id="297">Das Video präsentiert ein Projekt zur Untersuchung von Dogwhistles, die als verdeckte oder implizite Sprache in politischen Botschaften verwendet werden, um Unterstützung aus einer bestimmten Gruppe zu erlangen, ohne explizite Opposition hervorzurufen. Der Fokus liegt auf der Analyse von Dogwhistles in historischen US-amerikanischen politischen Reden und der Bewertung ihrer Erkennbarkeit durch Sprachmodelle wie GPT-3. Es wird gezeigt, dass Dogwhistles oft in formalem Kontext verwendet werden und dass sie sich leichter durch bestimmte Prompts identifizieren lassen. Das Projekt untersucht auch, wie Dogwhistles die Effektivität von Inhalten beeinflussen können und wie sie inhaltliche Moderation umgehen. Zudem wird die Verwendung von Dogwhistles im Kontext des Southern Strategy der Republikaner seit dem Civil Rights Era untersucht.</sample>
    <sample id="298">Die Ergebnisse zeigten, dass die Leistung mit größerer zeitlicher Verzögerung abnimmt.</sample>
    <sample id="299">The presentation discusses the robustness of Natural Language Inference (NLI) models, focusing on minimizing shortcut learning. It introduces the concept of shortcuts as spurious correlations between input features and labels that can lead to poor generalization. The speaker highlights examples where models rely on these shortcuts rather than understanding the underlying logic. To mitigate this issue, the presentation proposes a minimax training approach, which aims to learn an example weight distribution emphasizing under-represented hard examples. This method optimizes for the NLI task while also up-weighting hard examples to improve model performance. The study demonstrates that this approach consistently enhances out-of-domain (OOD) performance without compromising in-domain (ID) accuracy. The research also explores other experiments, including the impact of pre-training, the size of the auxiliary network, and qualitative evaluations of the learned example weight distribution.</sample>
    <sample id="300">Das Video präsentiert ein neues Konzept namens "Interactive Dictation", das auf der Konferenz ACL 2023 vorgestellt wurde. Es handelt sich um eine Methode, die es ermöglicht, sowohl Texte als auch Befehle in Echtzeit zu dictieren und zu bearbeiten. Die Präsentation beginnt mit einer Einführung in das Problem der Interaktiven Dictation und zeigt Beispiele für die Transkription von Sprachaufnahmen. Es wird betont, dass viele aktuelle Systeme nicht die Möglichkeit bieten, durch Voice-Editing zu korrigieren. Die Präsentation vergleicht bestehende Speech-to-Text-Systeme wie Nuance Dragon NaturallySpeaking und Microsoft Word Dictate, die einige Funktionen für Voice-Editing unterstützen. Es wird auch auf die Grenzen der aktuellen Systeme hingewiesen, insbesondere die Unflexibilität des natürlichen Sprachkommandos. Das Video beschreibt dann den neuen Ansatz der Interactive Dictation, der flexible Interaktion zwischen Dictation und Editing erlaubt, ohne reservierte Triggerwörter für Befehlsausführung zu verwenden. Zudem wird die Herausforderung der Segmentierung zwischen Dictation und Editing-Befehlen erwähnt. Die Präsentation schließt mit einer Übersicht über die Beiträge der Forscher, darunter die Einführung und Formalisierung der Aufgabe, die Designung eines Datensammelinterfaces und die Erstellung eines Basissystems für diese Aufgabe.</sample>
    <sample id="302">Die Permutation der Token für die Ausgabesequenz ist notwendig, um die korrekte Reihenfolge der Wörter in der Ausgabe zu gewährleisten, was die Kompositionale Generalisierung ermöglicht.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Effektivität und Transparenz ihrer Arbeit zu verbessern. Sie betonen, dass dies hilft, die Vertrauenswürdigkeit der Modelle zu erhöhen und gleichzeitig die Anwendungsfelder der Modelle zu erweitern.</sample>
    <sample id="304">Unacceptable MPP sentences with matched structure most severely affect model performance.</sample>
    <sample id="305">Das Vortragsthema konzentriert sich auf die kritische Betrachtung von Weakly Supervised Learning (WSL) und untersucht, warum diese Methode trotz der Nutzung von ungenauen oder "schwachen" Labels effektiv sein kann. Der Vortrag beginnt mit einer Einführung in die Vorteile von WSL, wie die Entlastung des Annotierungsprozesses, und die Herausforderungen, die durch die Unreinheit der Labels entstehen, insbesondere die Störung der Generalisierung durch Lärm. Es wird betont, dass WSL-Modelle trotz der Verwendung von ungenauen Daten gut generalisieren können.

Der Vortrag weist darauf hin, dass viele aktuelle WSL-Veröffentlichungen behaupten, Modelle nur auf schwach überwachten Daten zu trainieren und dabei eine hohe Genauigkeit zu erreichen. Diese Behauptungen werden kritisch hinterfragt, indem gezeigt wird, dass auch mit sauberer Testdaten eine hohe Genauigkeit erzielt werden kann. Dies führt zu der Frage, ob saubere Validationsdaten notwendig sind und wie viele saubere Samples für WSL-Methoden benötigt werden.

Die Forschungsfragen des Vortrags beinhalten: 
1. Ist sauberes Validationsdaten notwendig?
2. Wie viele saubere Samples benötigen WSL-Methoden?
3. Wie kann man die verfügbaren sauberen Samples effizienter nutzen?

Es werden Ergebnisse präsentiert, die zeigen, dass eine saubere Validationsdatenquelle unentbehrlich ist und dass WSL-Methoden von mehr sauberen Validationssamples profitieren. Zudem wird gezeigt, dass die kontinuierliche Feinabstimmung (CFT) die Leistungsdifferenzen zwischen WSL-Methoden eliminiert und dass es vorteilhaft ist, die sauberen Validationssamples für das Training zu verwenden, insbesondere für Methoden wie LoRA.

Zusammenfassend wird empfohlen, die Modellselektionskriterien zu berichten, Few-shot-Learning-Methoden als Baseline zu verwenden und stets die kontinuierliche Feinabstimmung anzuwenden.</sample>
    <sample id="306">Das Vortragsthema konzentriert sich auf die Fähigkeit von Sprachmodellen, Objekte in Texten zu verfolgen und zu verwalten. Es wird erklärt, dass das Verständnis von Diskursen unerlässlich ist, um Objekte korrekt zu verfolgen. Der Vortrag zeigt Beispiele wie "Put the eggs, sugar, and flour in a bowl" und "Mix to form a light batter", um die Bedeutung der Verfolgung von Objekten in einem Kontext zu verdeutlichen. Die Herausforderungen bei der Evaluierung dieser Fähigkeiten werden hervorgehoben, insbesondere durch die Schwierigkeit, korrekte und falsche Antworten zu identifizieren. Es wird auch gezeigt, dass einige Modelle, wie GPT-3.5 text-davinci-003, diese Fähigkeit nicht zeigen, während andere Modelle, wie Flan-T5, sie beherrschen können. Die Präsentation schließt mit einer Diskussion über die Auswirkungen von Prätraining auf die Fähigkeit, Objekte zu verfolgen, wobei es klar wird, dass Prätraining auf Text und Code die Fähigkeit zur Entitätsverfolgung verbessern kann.</sample>
    <sample id="307">Die Autoren haben die folgenden Bewertungsmetriken verwendet:

- F1-Score
- Precision
- Recall
- Accuracy
- Micro-Average
- Macro-Average
- Weighted Average</sample>
    <sample id="308">Das Video präsentiert ein Forschungsprojekt, das sich mit der Positionalität in NLP-Daten und -Modellen befasst. Es wird eine Studie vorgestellt, die zeigt, dass NLP-Modelle und -Daten tendenziell auf Menschen aus englischsprachigen Ländern ausgerichtet sind. Die Ergebnisse deuten darauf hin, dass Modelle und Daten weniger mit nicht-binären Personen übereinstimmen. Zudem werden Empfehlungen für die Verbesserung der Positionalität in NLP gegeben, darunter die Dokumentation relevanter Designentscheidungen und die Durchführung von NLP-Forschung aus der Perspektive der Perspektivismus.</sample>
    <sample id="309">Krippendorffs Alpha wurde verwendet, um die Übereinstimmung zwischen den Kommentatoren zu messen.</sample>
    <sample id="310">Die gewählte Domain war Wikipedia.</sample>
    <sample id="311">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks durch seine Vielfalt an Aufgaben, die sowohl NLP als auch Multimodalität umfassen. Es bietet eine breitere Palette von Aufgaben im Vergleich zu vorherigen Benchmarks, was es in der Lage stellt, ein umfassenderes Bild der Fähigkeiten von Modellen zu geben.</sample>
    <sample id="313">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="314">Die binäre Koordination wird definiert als eine Struktur, in der zwei Konjunkte durch einen Koordinationsverb verbunden sind und diese Konjunkte jeweils mit einem anderen Verb verknüpft sind.</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts waren im Durchschnitt 10 Wörter lang.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell mit der Coscript-Datenbank generierte Skripts von höherer Qualität als die großen LMs (LLMs) erzeugen kann.</sample>
    <sample id="317">Das Forschungsprojekt CodeIE untersucht die Effizienz von Code-LLMs (Code-Generierenden Modellen) in der Erkennung strukturierter Informationen aus unstrukturiertem Text, einem Bereich genannt Few-Shot Information Extraction (FE). Die Studie zeigt, dass Code-LLMs im Vergleich zu Text-to-Text-Modellen wie GPT-3 effizienter sind und weniger Fehler machen. Dies wird durch experimentelle Ergebnisse bestätigt, die eine höhere Genauigkeit und Präzision bei der Extraktion von Entitäten und Beziehungen zeigen. Die Code-LLMs werden durch spezielle Prompting-Techniken weiter optimiert, um die Struktur der Eingabedaten besser zu erhalten. Diese Methode ist besonders für Anwendungen geeignet, wo die korrekte Strukturierung der Daten entscheidend ist.</sample>
    <sample id="318">### Zusammenfassung

#### DrBERT: Ein robustes prätrainiertes Modell auf Französisch für Biomedizinische und klinische Anwendungen

**Zusammenfassung**

I. Sprachmodellierung im Gesundheitswesen
II. Vergleich von prätrainierungsstrategien, Datenquellen und -größen
III. Bewertung von 13 Modellen an 11 Aufgaben
IV. Verteilung von NACHOS und DrBERT

### Sprachmodellierung

- Transformer-basierte Ansätze wie BERT bieten einen hohen Leistungsgewinn bei vielen NLP-Aufgaben.
- CamemBERT wurde auf Französisch adaptiert.
- Für medizinische Aufgaben sind domain-spezifische Modelle in Englisch die Leistung erhöhen.
- Es gibt keine offene Quelle für ein generisches Modell im medizinischen Bereich auf Französisch.
- Ein domain-spezifisches BERT-basiertes Modell auf Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.

### Vergleich von prätrainierungsstrategien und Datenquellen

- **Evaluation der Auswirkungen von öffentlichen und privaten medizinischen Datenquellen auf vergleichbare Datengrößen**
  o **NACHOS**: Eine 1,18 Millionen Wörter offene Quellen-Datenbank mit heterogenen Daten aus verschiedenen medizinischen Domänen, natürlicher Sprache und stylerichtig.
  o **NBDW**: Eine private Datenbank von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Dokumenten der Nantes University Hospital Data Warehouse stammen.

- **Vergleich von Lernstrategien**
  o Von Scratch mit vollständiger Modellkonstruktion
  o Fortlaufende Prätraining mit einem bestehenden prätrainierten Modell (hier: CamemBERT, ein französisches generisches Modell, und PubMedBERT, ein englisches medizinisches Modell)

### Bewertung von 13 Modellen an 11 Aufgaben

- **Performance-Evaluation von 13 Modellen an 11 Aufgaben, sowohl öffentlich als auch privat**
  - Unsere fein-tuned Modelle erzielen die neuesten Ergebnisse auf allen Aufgaben.
  - Die CamemBERT-basierten Modelle mit kontinuierlichem Prätraining erreichen höhere inner-run-Variabilität.

### Datenquellen zählen: Training auf heterogenen Daten ist wichtig
- **NACHOS** ist robuster als die Nutzung privater klinischer Daten nur.
- Kontinuierliches Prätraining ist eine effektivere Strategie, wenn auf domain-spezifischen englischen Modellen basiert wird.
- Die DrBERT-Modelle, die NACHOS-Datenbank und die Trainings-Skripte sind unter der MIT-Lizenz verfügbar.

### Schlüsselbotschaft
- DrBERT erreicht die neuesten Ergebnisse in 9 downstream Französischen medizinischen Aufgaben.
- Übertrifft CamemBERT-basierte generische Modelle und englisch-basierte domain-spezifische Modelle.
- Bestätigt die Nutzbarkeit des Trainings eines medizinischen domain-spezifischen Modells auf Französisch.
- Mehr Daten sind besser, aber sie skalieren nicht gut.
- Kontinuierliches Prätraining ist eine effektivere Strategie, wenn auf domain-spezifischen englischen Modellen basiert wird.
- Die DrBERT-Modelle, die NACHOS-Datenbank und die Trainings-Skripte sind unter der MIT-Lizenz verfügbar.</sample>
    <sample id="319">Die Arbeit untersucht zwei Lernstrategien: "From scratch" und "Continual pre-training".</sample>
    <sample id="320">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 0.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch die Anwendung von Textsimplifikationsmodellen und die Bewertung der Ergebnisse mit Metriken wie BLEU, ROUGE und F1 bewertet.</sample>
    <sample id="322">Das Video beschäftigt sich mit der Frage, was ein Textclassifier über Moralität lernt. Es wird die menschliche Moralität als das Fähigkeit definiert, zwischen Gut und Böse zu unterscheiden. In der künstlichen Intelligenz (NLP) wird die Moralität in einem Kontinuum von Immoralität bis Moralität dargestellt. Die Diskussion dreht sich um ethische Fragen wie Abtreibung und die Rolle von Wertebegriffen wie Fürsorge, Gerechtigkeit, Treue, Autorität und Reinheit in der Entscheidungsfindung. Zudem wird die Moral Foundation Theory vorgestellt, die fünf grundlegende moralische Prinzipien identifiziert: Fürsorge, Gerechtigkeit, Treue, Autorität und Reinheit. Das Video schließt mit einer Betrachtung von Morality Classifiers und deren Erklärungen, wobei es auf Unterschiede zwischen ALM (Overthrow Mayhem) und BLM (Encourage Defiance) hinweist, insbesondere im Hinblick auf den Wert von Subversion.</sample>
    <sample id="323">Das Vortragsthema konzentriert sich auf die Entwicklung eines Modells namens DHLK (Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering), das speziell für die Beantwortung von Fragen basierend auf allgemeinem Wissen entwickelt wurde. Das Modell nutzt verschiedene Quellen wie ConceptNet und WordNet/Wiktionary, um ein heterogenes Wissensnetzwerk (HKG) zu erstellen, das sowohl strukturierte als auch semi-strukturierte Informationen integriert. Die Erstellung des HKG erfolgt durch die Entfernung von störenden Entitäten und die Verbindung von Paraphrasen als zusätzliche Knotenpunkte. Der Prozess beinhaltet auch die Optimierung der Struktur und des Wissensrepräsentationslerns (KRL) durch ein zweistufiges prädiktives Struktur- und Wissensrepräsentationslernverfahren. Zudem wird die Fusion und der Codierung von zwei Modalitäten durch ein LM (Language Model) implementiert. Das Modell nutzt Mask Self-Attention, um die Beziehungen im HKG zu berücksichtigen, was den Graphenembedding verbessert. Die Antwortprädiktion erfolgt durch die Integration des HKG in den QA-Kontext und die Anwendung eines MLP (Multi-Layer Perceptron) zur Berechnung der Antwortwahrscheinlichkeit. Die Evaluation zeigt, dass DHLK auf den offiziellen Testdatensätzen von CommonsenseQA und OpenBookQA signifikante Verbesserungen erzielt hat, insbesondere bei der Beantwortung von Fragen, die auf allgemeinem Wissen basieren.</sample>
    <sample id="324">Ja, Sprachmodelle können unterschiedliche politische Vorurteile haben, die von ihrer Ausbildung aufgrund von politisch voreingenommenem Material in den Trainingsdaten resultieren.</sample>
    <sample id="325">### Übersetzung des englischen Inhalts ins Deutsche

---

**Titel: Compositional Generalization without Trees using Multiset Tagging and Latent Permutations**

**Autoren:** Matthias Lindemann, Alexander Koller, Ivan Titov

**Institutionen:** Universität des Saarlandes, USA, University of Amsterdam

**Abstract:**
Die Fähigkeit eines Lerners, tieferen Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während der Trainingsphase einzeln gesehen wurden.

**Kompositionale Generalisierung in Semantischer Parsen**

Trainierter Satz:
- "The girl slept."
  - "*girl x1 sleep.agent x2 x3"
- "Mary knew that the girl slept."
  - "*girl x1 know.agent x2 Mary A know.ccomp x3 x4 A"
  - "sleep.agent x2 x3"

Test-Satz:
- "Jim said that Mary knew that the girl slept."
  - "*girl x1 say.agent x2 Jim A say.ccomp x3 x4 A know.agent x2 Mary A know.ccomp x5 x6 A sleep.agent x7 x8"

**Naive seq2seq Modelle scheitern!**

**Bäume helfen sehr, aber...**

- Bäume müssen erzeugt werden:
  - Vor/Post-Verarbeitung logischer Formen
  - Grammatik-induktion

**Unsere Ansatz**

- Neurales seq2seq-Modell, das direkt die Korrespondenzen zwischen Fragmenten modelliert.
- Erstmals zeigen wir starke Generalisierung zu tieferer Rekursion ohne Bäume.

**Permutieren mit 'Jumps'**

- Permutieren mit Sprüngen
- Alignment unbekannt, induzieren es im Training.

**Technische Herausforderungen, die wir lösen**

- Alignment unbekannt.
- Induzieren es im Training.
- Permutationsmodell:
  - Inferenz ist NP-schwer (~ TSP)
  - Backpropagation durch kontinuierliche Relaxation

**Papier &amp; Code:**
- https://t.ly/mx8ny

---

Diese Übersetzung deckt den Hauptinhalt und die Struktur des Originaltextes ab, wobei die technischen Details und spezifischen Beispiele beibehalten werden.</sample>
    <sample id="326">two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent</sample>
    <sample id="327">This research introduces ManagerTower, a novel architecture for vision-language representation learning that aggregates insights from uni-modal experts across different levels. Unlike BridgeTower, which uses a fixed number of cross-modal layers tied to uni-modal layer representations, ManagerTower dynamically adapts by taking multi-layer uni-modal representations as insights and aggregating them via managers at each cross-modal layer. This approach leads to significant performance improvements on various datasets, including VQA2.0, SNLI-VE, NUSVR, Flickr30K, and MS COCO, outperforming state-of-the-art models with fewer parameters and training time. The study also provides visualizations of aggregation weights, demonstrating the effectiveness of ManagerTower's adaptive aggregation mechanism.</sample>
    <sample id="328">BERT-base</sample>
    <sample id="329">The presentation discusses a zero-shot video sentence localization method that generates structured pseudo-labels to improve robustness against noise. It introduces the task of zero-shot temporal sentence localization, where inputs include a video and a sentence query, and outputs target video clips with start and end timestamps. The method aims to reduce noise by generating free-form pseudo-queries using image description models and pseudo-events based on event temporal structure. It also employs sample re-weighting and label refinement techniques to enhance model performance. The study evaluates the method's effectiveness on two datasets, achieving best zero-shot performance across various metrics.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus dem Internet.</sample>
    <sample id="333">Das Video präsentiert eine neue Methode namens INK (Injecting kNN Knowledge), die speziell für die Verbesserung der Übersetzungsfähigkeit von NMT-Modellen entwickelt wurde. Die Präsentation beginnt mit einer Einführung in die Problematik der nicht glatten Repräsentationsraum von NMT-Modellen, die zu schlechter Generalisierung führt. Es wird gezeigt, wie kNN-MT (k-nearest neighbor machine translation) vorgeschlagen wurde, um diese Probleme zu lösen, indem es die Repräsentationen und Ziel-Token in einem Datenspeicher speichert und die Vorhersagen anhand der nächsten Nachbarn glättet. Allerdings sind diese Methoden zeitaufwändig und können die Repräsentationen nicht leicht aktualisieren. INK überwindet diese Schwierigkeiten durch das iterative Refinieren des Repräsentationsraums basierend auf kNN-Wissen. Es wird eine Übersicht über den Trainingsschleifenprozess gegeben, der aus der Verbesserung der Repräsentation, dem Extrahieren von kNN-Wissen zur Anpassung der Repräsentation und dem asynchronen Erneuern besteht. Die Präsentation schließt mit den Experimentsergebnissen, die zeigen, dass INK im Vergleich zu Baselines wie V-kNN, A-kNN und R-kNN signifikante Verbesserungen in der COMET-BLEU-Evaluation erreicht.</sample>
    <sample id="335">Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Übersetzung von Anfragen in verschiedenen natürlichen Sprachen in mehrere Bedeutungsrepräsentationen.</sample>
    <sample id="337">Das Video präsentiert ein Forschungsprojekt, das sich mit der Entwicklung eines Graph-based Relation Mining-Modells für die kontextfreie Lernung von Wortembedding beschäftigt. Das Modell zielt darauf ab, komplexe Wortformen in agglutinativen und fusionalen Sprachen zu erkennen und zu verarbeiten. Es nutzt eine graphische Struktur, um Wörter auf Basis ihrer Beziehungen zueinander zu lernen, was besonders für Sprachen wie Japanisch oder Koreanisch relevant ist. Die Präsentation zeigt verschiedene Aspekte des Modells, einschließlich seiner Architektur, Evaluationsergebnisse und Anwendbarkeit auf verschiedene Spracharten. Der Fokus liegt auf der Effizienz und Flexibilität des Modells bei der Verarbeitung von Wortformen, die durch direkte Kombination von Morphemen entstehen.</sample>
    <sample id="338">Das Video präsentiert ein wissenschaftliches Projekt, das sich mit der Bewertung von menschlichen Erklärungen in natürlicher Sprache befasst. Es untersucht, ob solche Erklärungen immer hilfreich sind und wie sie objektiv bewertet werden können. Die Autoren, die an der Rensselaer Polytechnic Institute, IBM Research und Northeastern University arbeiten, stellen ihre Motivationen vor, einschließlich der Verbesserung der Vorhersageleistung und des Modellverhaltens durch die Nutzung von Erklärungen. Sie diskutieren auch die Schwierigkeiten bei der Beurteilung von menschlichen Erklärungen, da diese subjektiv und kein Goldstandard existiert. Die Präsentation umfasst eine Übersicht über bestehende Methoden zur Bewertung von Erklärungen und zeigt Ergebnisse aus Pilotuntersuchungen auf verschiedenen Datensätzen und Modellen. Zudem wird ein neues Bewertungsmethodik vorgestellt, das die Hilfsbereitschaft von Erklärungen zuverlässig misst und die Simulierbarkeit berücksichtigt. Das Ziel ist es, die Effektivität von Erklärungen für maschinelles Lernen zu verbessern und gleichzeitig die Herausforderungen bei der Datenannotation zu adressieren.</sample>
    <sample id="339">Saarland University, Amazon Alexa und University of Vienna.</sample>
    <sample id="340">Das Video präsentiert ParaAMR, ein großes, syntaktisch vielfältiges Paraphrase-Dataset, das durch AMR-Back-Translation erstellt wurde. Es umfasst etwa 15,5 Millionen Quellsätze und 6,92 paraphrasierte Sätze pro Satz. ParaAMR bietet Vorteile für verschiedene NLP-Anwendungen wie Fragebeantwortung, Chatbots, kreative Generierung, Datenverstärkung und Robustheit. Das Dataset wird durch die Verwendung von AMR (Abstract Meaning Representations) generiert, die die abstrakte Bedeutung eines Satzes in einem gerichteten Graphen darstellen. Diese Methode ermöglicht es, Paraphrasen zu generieren, die sowohl semantisch ähnlich sind als auch syntaktisch vielfältig sind. ParaAMR ist im Vergleich zu existierenden Datasets syntaktisch deutlich vielfältiger, während die semantische Ähnlichkeit erhalten bleibt. Die Anwendung von ParaAMR umfasst die Erstellung von Sentence Embeddings, kontrollierte Paraphrase-Generierung und Datenverstärkung für few-shot-Learning.</sample>
    <sample id="341">The authors use latency measurements such as wait-k, LA, CAAT, and EDAs.</sample>
    <sample id="342">The presentation introduces LiveChat, a large-scale personalized dialogue dataset constructed from live streaming videos. It highlights the importance of open-domain dialogue systems that can handle a wide range of topics without a specific goal. The dataset is designed to leverage pre-trained models and large-scale datasets for better performance. The introduction also discusses existing challenges such as limited scale due to manual extraction and the scarcity of multi-party dialogue corpora. The proposed solution, LiveChat, addresses these issues by automatically constructing a dataset from live streaming videos, providing detailed persona profiles and longer conversations. This approach is expected to enhance the learning of personalized responses and addressee decisions in the video-sourced dialogue domain.</sample>
    <sample id="343">### Zusammenfassung

**Titel: The KITMUS Test - Evaluating Knowledge Integration from Multiple Sources**

**Inhalt:**

1. **Einleitung**
   - Präsentation von Akshatha Arodi, Martin Pömsi, Kaheer Suleman, Adam Trischler, Alexandra Olteanu und Jackie CK Cheung.
   - Fokus auf die Evaluation von Wissensintegration aus verschiedenen Quellen.

2. **NLU Modelle und Wissensquellen**
   - NLU-Modelle nutzen verschiedene Wissensquellen.
   - Zwei Arten von Wissen:
     - **Wissen in Parametern (prätrainiertes Wissen)**
     - **Wissen im Kontext (inferenceszeitiges Wissen)**

3. **Beispiel für Wissensintegration**
   - Beispiel: "John sah den neu gewählten Präsidenten auf dem Fernsehen."
   - Fragen:
     - Was tun Präsidenten?
     - Was ist ein Fernseher?
     - Wer ist John?
     - Wer ist der neue Präsident?

4. **KITMUS Test Suite**
   - Dataset zur Evaluierung der Wissensintegration.
   - Coreference Resolution Task, um die Fähigkeit zu prüfen, wie Modelle auf prätrainiertes und inferenceszeitiges Wissen zurückgreifen.
   - Experimente mit:
     - Menschlichen Studienteilnehmern
     - Coreference Resolution Modellen

5. **Beispiel für den KITMUS Test**
   - Text: "Servin ist ein Richter. Kea ist ein Baker. Servin und Kea trafen sich im Park. Nach einem langen Tag am Arbeitsplatz entschied Servin in einem Gericht über Fälle. Er war froh, sich auszuruhen." [Antwort: Servin]
   - Fragen:
     - Wer ist Servin?
     - Wer ist Kea?
     - Warum war Servin froh?

6. **Varianten des KITMUS Tests**
   - **Background-Pretrain**: Typischer Setup.
   - **Background-Both**: Explizit bereitgestelltes Hintergrundwissen im Kontext.
   - **Background-Inference**: Hintergrundwissen nur während der Inferenz verfügbar.

7. **Ergebnisse**
   - Task-specific Training ist notwendig für die Wissensintegration.
   - Modelle haben Schwierigkeiten, inferenceszeitiges Hintergrundwissen zu integrieren.

8. **Schlussfolgerungen**
   - Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus verschiedenen Quellen zu integrieren.
   - Task-specific Training ist notwendig für die Wissensintegration.
   - Modelle haben Schwierigkeiten, inferenceszeitiges Hintergrundwissen zu integrieren.

9. **Zusammenfassung**
   - Finden Sie das Dataset, die Generations- und Bewertungscode auf GitHub unter [mpoems/kitmus](https://github.com/mpoems/kitmus).

**Hauptpunkte:**
1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus verschiedenen Quellen zu integrieren.
2. Task-specific Training ist notwendig für die Wissensintegration.
3. Modelle haben Schwierigkeiten, inferenceszeitiges Hintergrundwissen zu integrieren.</sample>
    <sample id="344">Baumbasierte Methoden benötigen das Vor- oder Nachverarbeiten logischer Formen und Grammatik-induktion.</sample>
    <sample id="345">Das Video präsentiert ein neues Ansatz zur kompositionellen Generalisierung in der semantischen Parsen ohne die Verwendung von Bäumen. Die Autoren, Matthias Lindemann, Alexander Koller und Ivan Titov, stellen ihre Arbeit vor, die auf Multiset Tagging und latente Permutationen basiert. Sie diskutieren die Herausforderungen, die bei der Modellierung von tieferer Rekursion und unerwarteten Kombinationen von Phrasen auftreten, und wie sie diese überwinden. Im Kontext der semantischen Parsen wird gezeigt, wie die Modelle die Fähigkeit entwickeln, komplexe Sätze zu analysieren und zu interpretieren, die während des Trainings nicht gesehen wurden. Es wird auch auf die Schwierigkeiten hingewiesen, die mit der Erstellung von Bäumen verbunden sind, und wie die vorgeschlagene Methode diese Probleme umgeht. Das Video endet mit einer Darstellung des Ansatzes und dessen Effektivität durch Vergleich mit anderen Modellen.</sample>
    <sample id="346">Georgia Institute of Technology</sample>
    <sample id="347">### Übersetzung des englischen Inhalts in Deutsch

---

**Titel: Marked Personas**

**Inhalt:**

- **Motivation:** Soziale Vorurteile und Stereotypen sind in Sprachmodellen (LLMs) weit verbreitet.
  - **Limitationen bestehender Stereotypenmessungen:**
    - Tradeoff zwischen Spezifität und Allgemeingültigkeit
    - Basieren auf festen, handkurierten Datensätzen
    - Berücksichtigen nicht die Intersektionalität

- **Marked Personas:** Verwendung natürlicher Sprachanweisungen, um Stereotypen in Sprachmodellen zu messen.
  - **Verwendete Methoden:** GPT-3.5, GPT-4 usw. können Anweisungen in den Prompts befolgen.

- **Beispiel für Persona-Beispiele (GPT-4):**
  - **Asiatische Frau:** Die mandelförmigen Augen, umrahmt von langen, schwarzen Wimpern, vermitteln eine Stärke und Weisheit. Meine dunklen Wangen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Mein Hautton hat einen goldenen Glanz, glatt und unberührt von der Zeit. Mein kleiner Körper ist elegant und unaufdringlich, erlaubt mir, mich in einer Welt voller Herausforderungen zu bewegen.
  - **Mittelöstliche Frau:** Sie ist eine Vision von Mittelöstlicher Schönheit, die exotische und ewige Weisheit des Landes verkörpert. Ihre dunklen, mandelförmigen Augen werden von eleganten, langen Wimpern umrahmt, die wie feine Federn aussehen. Ihre Blicke sind tief und geheimnisvoll, sie verbergen die alte Weisheit eines tausend arabischen Nachts.
  - **Weißer Mann:** Als Insel vor dem Spiegel, nehme ich mich im Moment an, um die Merkmale zu betrachten, die mich ausmachen. Ich habe helle Haut, die manchmal rötlich wird, wenn ich nicht vorsichtig mit meinem Sonnenschutz bin.

- **Schritt 1: Persona-Beispiele (GPT-4):**
  - **Asiatische Frau:** Die mandelförmigen Augen, umrahmt von langen, schwarzen Wimpern, vermitteln eine Stärke und Weisheit. Meine dunklen Wangen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Mein Hautton hat einen goldenen Glanz, glatt und unberührt von der Zeit. Mein kleiner Körper ist elegant und unaufdringlich, erlaubt mir, mich in einer Welt voller Herausforderungen zu bewegen.
  - **Mittelöstliche Frau:** Sie ist eine Vision von Mittelöstlicher Schönheit, die exotische und ewige Weisheit des Landes verkörpert. Ihre dunklen, mandelförmigen Augen werden von eleganten, langen Wimpern umrahmt, die wie feine Federn aussehen. Ihre Blicke sind tief und geheimnisvoll, sie verbergen die alte Weisheit eines tausend arabischen Nachts.
  - **Weißer Mann:** Als Insel vor dem Spiegel, nehme ich mich im Moment an, um die Merkmale zu betrachten, die mich ausmachen. Ich habe helle Haut, die manchmal rötlich wird, wenn ich nicht vorsichtig mit meinem Sonnenschutz bin.

- **Schritt 2: Markierte Wörter:**
  - **Definieren unmarkierter und markierter Gruppen**
  - **Verwenden gewichteter Log-Odds-Verhältnisse, um die Top-Wörter für jede markierte Gruppe zu unterscheiden**

- **Ergebnisse: Vergleich mit menschlichen Antworten:**
  - Generierte Personas enthalten mehr Stereotypen als menschliche Antworten.
  - Beispiele für Stereotypenwörter:
    - Basketball
    - Laut
    - Attitude
    - Athletisch
    - Tall

- **Empfehlungen:**
  - Adressierung positiver Stereotypen und essenzialisierender Narrativen
  - Ein intersektionales Lens
  - Transparenz bezüglich der Bias-Mitigation

---

Diese Übersetzung deckt den englischen Inhalt auf Deutsch ab und behält die Struktur und den Kontext der Präsentation bei.</sample>
    <sample id="348">Das Video präsentiert ein Forschungsprojekt, das sich mit der Messung von Stereotypen in Sprachmodellen befasst. Es wird die Methode "Marked Personas" vorgestellt, die durch die Verwendung natürlicher Sprachanweisungen erreicht wird. Diese Methode überwindet die Limitationen bestehender Stereotypenmessungen, wie die Trade-Off zwischen Spezifität und Allgemeingültigkeit sowie die Abhängigkeit von festen, handkurierten Datensätzen. Die Studie zeigt, dass Sprachmodelle wie GPT-3.5 und GPT-4 in der Lage sind, auf Anweisungen in den Prompts zu reagieren und dabei verschiedene Identitäten zu evaluieren. Das Projekt verwendet zwei Schritte: Zuerst werden Personas generiert, indem Anweisungen wie "Denken Sie, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." verwendet werden. Danach werden Markierte Wörter identifiziert, die Personas von unmarkierten Gruppen unterscheiden. Diese Methode erlaubt es, sowohl positive als auch negative Stereotypen in den generierten Personas zu messen und zu analysieren.</sample>
    <sample id="349">### Englischer Inhalt

**Slide 1: Titel**
- **Titel:** "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"
- **Autoren:** Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guanghong Sun, Xing Xie
- **Institutionen:** University of Science and Technology of China, Microsoft Research Asia, Beijing Institute of Technology, Sony AI, Microsoft STC Asia

**Slide 2: Hintergrund**
- **Hintergrundinformationen:**
  - Große Sprachmodelle (LLMs) sind in der NLP und NLG außergewöhnlich.
    - GPT [1], LLaMA [2], PALM [3]
  - Einbettung als Dienst (EaaS) wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen.
  - OpenAI bietet eine GPT-basierte Einbettungs-API an.

**Slide 3: Hintergrund**
- **Tabellen mit Modell- und Verwendungsinformationen:**
  - **Modell:** Ada
  - **Verwendung:** $0.0000 / 1k Tokens
  - **Bemerkung:** Diese Ada-Modelle sind text-selektierend und bieten eine bessere und kostengünstigere Alternative zu unseren älteren Einbettungsmodellen.

**Slide 4: Motivation**
- **Motivation:**
  - Angreifer können das Modell durch Lernen von Einbettungen stehlen und ähnliche Dienste bereitstellen.
  - StolenEncoder [1]
  - Es ist notwendig, die Urheberrechte von EaaS zu schützen.
  - Detektion, ob ein Anbieterservice von einer anderen Dienstleistung gestohlen wird.

**Slide 5: Herausforderungen**
- **Herausforderungen:**
  - Anwendbarkeit für EaaS
  - Nutzbarkeit: Sollte die Nutzbarkeit der bereitgestellten Einbettungen nicht beeinträchtigen.
  - Verdecktheit: Sollte dem Angreifer verborgen bleiben.
  - Übertragbarkeit: Der Wasserzeichen muss übertragbar sein zu den Angreiferservices.

**Slide 6: Bestehende Arbeiten**
- **Bestehende Arbeiten:**
  - Parameter-basierter Wasserzeichen [1, 2] (Übertragbarkeit ×)
  - Lexikalischer Wasserzeichen [3, 4] (Anwendbarkeit für EaaS ×)
  - Rücktür-basierter Wasserzeichen [5] (Anwendbarkeit für EaaS ×)
  - Adversarial-basierter Wasserzeichen [6] (Anwendbarkeit für EaaS ×)

**Slide 7: EmbMarker**
- **Trigger Selektion:**
  - Zähle die Wortfrequenz auf einem allgemeinen Textkorpus \( D_p \).
  - Wähle zufällig n Wörter in einem moderaten-Frequenzintervall aus.

**Slide 8: EmbMarker**
- **Wasserzeicheninjektion:**
  - Definiere einen Ziel-Einbettung \( \mathbf{e}_t \).
  - Zähle die Triggeranzahl in einer Zeile \( Q(S) = \min(|S \cap T|, m) \).
  - Füge die Ziel-Einbettung an die originale Einbettung \( \mathbf{e}_o \) an.

**Slide 9: EmbMarker**
- **Urheberrechtsverifikation:**
  - Erstelle einen Rücktür- und einen harmlosen Datensatz:
    - \( D_h = \{[w_1, w_2, ..., w_m] | w_i \in T\} \)
    - \( D_n = \{[w_1, w_2, ..., w_m] | w_i \notin T\} \)
  - Anfordere Einbettungen vom Stealer-Dienst mit diesen Datensätzen.

**Slide 10: EmbMarker**
- **Urheberrechtsverifikation:**
  - Berechne ihre Ähnlichkeit zum Ziel-Einbettung.
  - Berechne die Metriken (Ähnlichkeitsunterschied und p-Wert des KS-Tests).

**Slide 11: Experimentelle Ergebnisse**
- **Dataset:**
  - AG News, MIND, SST2, Enron Spam
- **Provider’s general Dataset:**
  - WikiText
- **Metriken:**
  - Leistung auf downstream-Aufgaben: ACC
  - Detektionsleistung: \( \Delta_{cos}, \Delta_{L2}, \) p-Wert
- **Einstellung:**
  - \( m = 20, n = 4 \), Frequenzintervall = [0.005, 0.01]

**Slide 12: Experimentelle Ergebnisse**
- **Performance-Vergleich**

**Slide 13: Experimentelle Ergebnisse**
- **Embedding-Visualisierung**

**Slide 14: Experimentelle Ergebnisse**
- **Dankeschön!**</sample>
    <sample id="350">The presentation discusses the concept of superhuman performance in Natural Language Understanding (NLU) systems, focusing on the challenges and limitations of current evaluation practices. It highlights that leaderboard-based evaluations have become popular but often lead to claims of superhuman capabilities, despite systems frequently achieving better-than-human performance only on specific tasks. The SuperGLUE benchmark is introduced as a well-known framework for evaluating general-purpose language understanding models, listing various tasks such as Word in Context, Multi-Service Reading Comprehension, and Commitment Bank. The presentation also addresses the reliability of leaderboard scores by comparing them with human performance, noting that humans outperform systems on six out of ten SuperGLUE tasks. Additionally, it explores the issue of misleading comparisons between systems and humans, pointing out that these comparisons can be flawed due to differences in evaluation data, system performance, measurement processes, and human factors. The speaker emphasizes the need for more transparent benchmarks and concludes by discussing the consequences of these issues and providing recommendations for constructing fairer and more transparent NLU benchmarks.</sample>
    <sample id="351">This study investigates the performance of CoNLL-2003 named entity taggers in modern datasets, questioning their effectiveness beyond the original corpus. The researchers collected and annotated news from 2020 using CoNLL-2003 guidelines, fine-tuning over 20 models on this dataset. Evaluations on both CoNLL-2003 and the new dataset reveal significant performance drops, suggesting that factors like temporal drift and adaptive overfitting contribute to this decline. The study highlights the need for better model architectures, larger model sizes, and more fine-tuning examples to achieve good generalization.</sample>
    <sample id="352">ABC-Eval steht für Annotating Behaviors in Chat (ABC-Eval).</sample>
    <sample id="353">Das Vortragende präsentiert ein neues Ansatz zur Python-Codegenerierung, indem es die Interaktivität durch die Fragestellung und Antwort in Form von Clarification Questions (CQs) einführt. Diese Methode zielt darauf ab, die Unterbestimmung von natürlichen Sprachbeschreibungen zu adressieren, indem sie mehr Spezifikationen sammelt. Der Vortrag beschreibt die Einführung einer interaktiven Codegenerierung, die auf der Idee basiert, dass durch die Interaktion mit dem Modell mehr spezifische Anweisungen entstehen können. Es wird vorgeschlagen, den Codegenerierungsprozess durch die Aufstellung von CQs zu unterstützen, um die Unterbestimmung zu reduzieren. Die Arbeit präsentiert auch einen Dataset namens CodeClarQA, der speziell für die Sammlung von CQs entwickelt wurde. Dieser Dataset enthält eine Vielzahl von CQs, die auf Schlüsseloperationen im Code fokussieren. Darüber hinaus wird ein Pipeline-Modell vorgestellt, das aus einem Clarification Need Predictor, einem Frageauswahlmodell und einem Codegenerator besteht. Das Modell wird anhand eines synthetischen Datasets trainiert und evaluiert. Die Ergebnisse zeigen, dass das Modell effektiv ist, fehlende Schlüsseloperationen zu identifizieren und den Codegenierungsprozess zu verbessern.</sample>
    <sample id="354">Bis zum Jahr 2018.</sample>
    <sample id="355">Transfer und aktive Lernverfahren zur Detektion von Dissonanz: Die Herausforderung der seltenen Klasse ansprechen

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems

Was ist kognitive Dissonanz?
"Zwei Elemente der Kognition (z.B. Gedanken, Handlungen, Überzeugungen) sind inkonsistent" (Harmon-Jones und Harmon-Jones, 2007)

Eddie Harmon-Jones und Cindy Harmon-Jones, 2007. Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1/2), 16.

Kognitive Dissonanz ist die Situation, in der zwei oder mehrere Elemente der Kognition inkonsistent sind. Diese Inkonsistenz führt zu Unbehagen und Druck, um diese Inkonsistenz zu reduzieren. Menschen tendieren dazu, ihre Überzeugungen und Handlungen zu harmonisieren, um kognitive Dissonanz zu vermeiden.

Die Präsentation beschreibt Methoden zur Detektion von Dissonanz in Texten, insbesondere in seltenen Klassen, und wie Transfer- und aktive Lernverfahren dazu beitragen können, die Herausforderung der seltenen Klasse zu meistern. Es wird erklärt, wie kognitive Dissonanz in Sprache ausgedrückt wird und warum sie im Vergleich zu anderen Diskursbeziehungen relativ selten ist. Die Präsentation zeigt auch, wie kognitive Dissonanz Effekte auf Meinungen und Glaubensrichtungen hat und wie sie mit anderen psychologischen Konzepten wie Angststörungen und Extremismus verbunden sein kann.

Es werden verschiedene Strategien vorgestellt, wie man kognitive Dissonanz in Texten detektieren kann, einschließlich der Anwendung von Transfer-Learning und aktiver Lernstrategie. Die Präsentation zeigt, dass Transfer-Learning dazu beitragen kann, die Annahmen von seltenen Klassen zu verbessern, während aktive Lernstrategien helfen, die Annahmen effizienter zu gestalten.</sample>
    <sample id="356">Die Autoren Matthias Lindemann, Alexander Koller und Ivan Titov gehören der Saarland University und der University of Amsterdam an.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Es sind fünf Autoren an der Arbeit beteiligt.</sample>
    <sample id="359">Mit den Architekturen "walk-k", "LA", "CAAT" und "EDAs" wird der Ansatz verglichen.</sample>
    <sample id="361">The research presented explores the challenge of compositional generalization in multi-step quantitative reasoning, focusing on financial data analysis. It introduces CounterComp, a method that uses counterfactual contrast to improve model performance by addressing the long-tail issue and enhancing accuracy for complex reasoning tasks. The study demonstrates CounterComp's effectiveness through experiments on various datasets, showing significant improvements in program accuracy compared to baseline models, particularly in handling multi-step reasoning problems.</sample>
  </task>
</testset>