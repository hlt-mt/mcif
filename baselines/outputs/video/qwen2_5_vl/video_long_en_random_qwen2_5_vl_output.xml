<testset name="IWSLT2025" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models include news media and social media (Reddit).</sample>
    <sample id="1">McGill University, Mila, Microsoft Research</sample>
    <sample id="2">The presentation introduces LayoutMask, a novel multi-modal pre-training model designed to enhance text-layout interaction for document understanding in visually-rich environments. The motivation behind this work is to address reading order issues common in documents with complex layouts, such as invoices and receipts. The proposed model, LayoutMask, leverages local ID positions instead of global ones to better capture spatial relationships between text elements. It employs two pre-training tasks: masked language modeling and masked position modeling, which help the model learn contextual and positional information effectively. The methodology section outlines how these tasks are integrated into the transformer architecture, utilizing spatial-aware self-attention mechanisms to improve performance. Experimental results demonstrate that LayoutMask achieves state-of-the-art performance on datasets like FUNSD, CORD, and SBDHF, outperforming existing methods by up to 6.21% in F1 score. This research contributes significantly to the field of computational linguistics by providing a robust framework for handling complex document structures.</sample>
    <sample id="4">The name of the speaker is Patrick Fernandes.</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">The presentation at ACL 2023 focuses on the integration of multi-lingual and cross-lingual summarization techniques, aiming to develop a unified model capable of handling documents in various languages and generating summaries in any target language. The research introduces the Many-to-many Summarization (M2MS) framework, which combines Multi-Lingual Summarization (MLS) and Cross-Lingual Summarization (CLS). This approach is designed to enhance the model's ability to transfer knowledge across different languages more effectively than previous methods.

Key contributions include:
1. Unifying MLS and CLS into M2MS.
2. Conducting preliminary studies to deepen understanding among MLS, CLS, and M2MS.
3. Proposing PISCES, a pre-trained M2MS model that learns language modeling, cross-lingual ability, and summarization through a three-stage pre-training process.

The presentation also discusses the experimental setup, using the WikiLingua dataset with mBART-50 as the backbone model. Four models were trained: mBART (ONE), mBART (CU-CLS), mBART (MLS), and mBART (M2MS). Results indicate that the M2MS setting allows for better transferability across languages compared to MLS and CLS settings. Additionally, the study explores the impact of task-specific pre-training and the role of Turkish samples in the training process.</sample>
    <sample id="7">YES!</sample>
    <sample id="8">The novelty of the proposed human evaluation method lies in its ability to evaluate dialogue quality by focusing on specific behaviors such as coherence, consistency, and emotional understanding, which are not typically addressed by existing methods. This approach allows for a more nuanced assessment of how well a chatbot can engage in natural and meaningful conversations with users.</sample>
    <sample id="9">The success of the existing weakly supervised approach heavily relies on clean validation data.</sample>
    <sample id="10">The speaker suggests that advances in the model's performance could be achieved by improving the quality of the background knowledge, which is currently limited to entity names.</sample>
    <sample id="11">The video discusses the capabilities and limitations of large language models, particularly in generating and explaining jokes. It begins by highlighting recent advancements where these models can now generate and explain jokes, referencing examples like ChatGPT from June 5th, 2023. The discussion then shifts to the question of whether these models truly understand humor, referencing articles and studies that suggest they do not fully grasp the nuances of humor.

The video transitions into an exploration of the New Yorker Caption Contest, a platform where participants submit captions for cartoons, with readers voting on their favorites. This contest is used as a benchmark to evaluate how well AI systems can understand and generate captions that align with human humor. The presentation outlines various benchmarks such as matching, quality ranking, and explanation generation, using examples from the contest to illustrate these metrics.

The analysis includes results from different models, including random guessing, zero-shot, and fine-tuned versions of CLIP-ViT-L/14@336px, QFA-Huge, and OFA-Huge. These results show varying levels of performance across the benchmarks, with human-generated captions consistently outperforming AI-generated ones. The video concludes by encouraging viewers to explore the dataset, leaderboard, and models available at capcon.dev, inviting further research and discussion on the topic of AI understanding humor.</sample>
    <sample id="12">5</sample>
    <sample id="13">The presentation focuses on the challenges and solutions in adaptive inference, particularly in low-resource settings. It introduces the concept of "conflicting gradients" in early exit training processes, where future classifiers' goals may conflict, leading to degraded performance. The study compares multi-model (MM) and early-exit (EE) methods, finding that MM models outperform EE by 2.3% on average, with the gap being largest for the earliest classifier. The paper proposes SWEET, an approach that separates weights in early exit transformers, addressing conflicting gradients by training each layer to receive updates only from the following classifier, thus mitigating interference between gradient signals. This method effectively closes most of the gap between EE and MM, while later classifiers remain unaffected. The results show significant improvements in speedup ratio, with SWEET closing the gap between EE and MM, and the authors suggest further research into fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="15">3</sample>
    <sample id="16">News and fiction.</sample>
    <sample id="17">The presentation focuses on multimodal relation extraction, specifically addressing the challenges of internal and external information utilization in social media scenarios. It introduces a novel framework that combines GIB-guided feature refinement and latent multimodal topic modeling to enhance relation extraction accuracy. The framework utilizes a scene graph for structural modeling of multimodal inputs and employs a GAT-based encoder for feature refinement. The study demonstrates significant improvements over existing models, achieving best performance with a fine-grained information pruning strategy and latent multimodal topic integration. The research highlights the importance of balancing internal and external information for optimal task performance, particularly emphasizing the role of the graph information bottleneck principle in guiding feature refinement.</sample>
    <sample id="18">The example given is "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="19">The presentation focuses on efficient open-domain question answering (ODQA) systems, emphasizing the importance of reducing index size and model complexity to enhance performance while maintaining resource efficiency. It introduces a two-stage framework for ODQA proposed by Danqi Chen in 2017, which includes a retriever and a reader component. The retriever uses techniques like TF-IDF and BM25 to find relevant documents, while the reader processes these documents using models such as LSTM, BERT, and GPT. The presentation highlights challenges in ODQA tasks, including large document sets and complex language models, and discusses strategies to address these issues, such as offline Wikipedia encoding, evidence searching, and parameter sharing. It also compares different system architectures, including Retriever-Reader, Retriever-only, and Generator-only systems, based on their trade-offs between speed, memory, and performance. The conclusion suggests that if real-time feedback is prioritized, Retriever-Only systems might be more suitable, whereas for trade-offs among performance, memory, and speed, Retriever-Reader systems are recommended.</sample>
    <sample id="20">Yes, you can use the models for your research.</sample>
    <sample id="21">DEplain-APA contains documents from APA style publications.</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">The video discusses advancements in text-to-image modeling, focusing on how character-aware models improve visual text rendering. It begins by highlighting the capabilities of text-to-image models like Imagen and their ability to generate images from text inputs. The presentation then delves into the limitations of subword-based encoders, which often struggle with spelling accuracy due to tokenization that hides spellings. This leads to a discussion on the importance of character-level information in enhancing model performance.

The video contrasts subword-based encoders with character-aware encoders, demonstrating superior spelling accuracy across different scales and model sizes. It uses data from T5 and PaLM to illustrate these points, showing that character-aware encoders maintain high spelling accuracy regardless of scale. The presentation also explores the impact of word frequency on encoding accuracy, revealing that subword-based encoders perform better for common words but struggle with less frequent ones.

To address these challenges, the video proposes concatenating subword-level and character-level text encodings as an efficient strategy to improve model spelling ability. This approach is illustrated through a flowchart showing the integration of these encodings into a text-to-image pipeline, leading to improved image generation metrics such as fidelity, alignment, and text quality.

The conclusion summarizes key takeaways: the use of benchmarks like WikiSpell and DrawText, and the effectiveness of character-aware strategies in enhancing text-only and text-to-image models. The video ends with a call to action, encouraging viewers to explore these methods for improving text rendering and image generation.</sample>
    <sample id="24">By observing before and noticing in Gibson et al. 1996.</sample>
    <sample id="25">The experiments were designed by varying the position of the governor in the sentence structure while keeping other factors constant, and then observing how this change affected the dependency lengths and structures.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data achieves an AUC of 0.51.</sample>
    <sample id="27">Four.</sample>
    <sample id="28">The characters' names in the example conversation are not provided in the given text.</sample>
    <sample id="29">Formality, lexical cohesion</sample>
    <sample id="30">The video discusses LLM-Blender, an ensemble learning framework for large language models (LLMs). It introduces two key components: PairRanker and GenFuser. The PairRanker compares pairs of candidate outputs from multiple LLMs to rank them, while the GenFuser combines the top-ranked candidates to generate a final output. The framework is designed to improve the overall performance of existing LLMs by leveraging their strengths in different contexts. The presentation also highlights the MixInstruct dataset, which contains over 110k instruction-following examples, providing a comprehensive benchmark for evaluating ensemble learning approaches. The study demonstrates that LLM-Blender can significantly enhance the performance of LLMs across various tasks, as evidenced by its competitive results on BLEU scores and other metrics.</sample>
    <sample id="31">The affiliations of the authors of the paper are Johns Hopkins University, Purdue University, MIT, and Meta AI.</sample>
    <sample id="32">**Compositional Generalization without Trees using Multiset Tagging and Latent Permutations**

Matthias Lindemann, Alexander Koller, Ivan Titov

**Compositionality Generalization**
- Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.

**Compositional Generalization in Semantic Parsing**

**Train:**
1. "The girl slept."
   - "*girl x₁ sleep.agent x₂ x₃"
2. "Mary knew that the girl slept."
   - "*girl x₁ know.agent x₂ Mary A know.ccomp x₃ x₄ A sleep.agent x₅ x₆"

**Test:**
1. "Jim said that Mary knew that the girl slept."
   - "*girl x₁ say.agent x₂ Jim A say.ccomp x₃ x₄ A know.agent x₅ Mary A know.ccomp x₆ x₇ A sleep.agent x₈ x₉"

**Naive seq2seq models fail!**

**Trees help a lot but...**
- Trees need to be obtained:
  - Pre/Post-processing logical forms
  - Grammar-induction

**Our Approach**
- Neural seq2seq model that directly models the correspondences between fragments.
- For the first time, we show strong generalization to deeper recursion *without trees*.

**Permuting with 'jumps'**
- Alignment unknown.
- Induce it in training.
- Permutation model:
  - Inference is NP-hard (~ TSP)
  - Backpropagate through continuous relaxation

**Some Results on COGS (Kim and Linzen 2020)**
- Comparison with other Treeless Models on Structural Generalization on COGS
  - PP recursion
  - CP recursion
  - Obj PP = Subj PP

**Technical Challenges We Solve**
- Alignment unknown.
- Induce it in training.
- Permutation model:
  - Inference is NP-hard (~ TSP)
  - Backpropagate through continuous relaxation

**Paper &amp; Code:**
- https://t.ly/mxBny</sample>
    <sample id="33">The framework quantifies positionality by re-annotating datasets with diverse annotators and then comparing annotations by demographic to models and datasets via Pearson's R scores.</sample>
    <sample id="34">CREST is a framework that bridges the gap between selective rationalization and counterfactual generation in natural language processing. It generates high-quality, fluent, and diverse counterfactuals while controlling the amount of perturbation to ensure plausible explanations. CREST achieves high counterfactual simulability by leveraging the paired structure of factual and counterfactual inputs through a shared trainable masker. The framework includes two main components: CREST-Generation for generating counterfactuals and CREST-Rationalization for explaining them. Experiments on datasets like IMDB and SNLI demonstrate that CREST outperforms existing methods in terms of plausibility, forward simulability, and counterfactual simulability. The interpretable rationales generated by CREST-Rationalization are validated through interpretability analysis, showing their effectiveness in understanding model decisions.</sample>
    <sample id="35"># Weaker Than You Think: A Critical Look at Weakly Supervised Learning

## Introduction

The title slide introduces the topic of the presentation, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." It highlights that the research is conducted by a team from Saarland University, Amazon Alexa, and the University of Vienna. The authors are Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.

## Why Weakly Supervised Learning?

The slide explains why weakly supervised learning (WSL) is important:
- **Weak supervision alleviates the annotation bottleneck**: This means that WSL can reduce the need for extensive manual labeling.
- **But weak labels are noisy**: Noise in the labels can harm generalization.
- **Weakly supervised learning (WSL)**: Models trained on noisy data can still generalize well.

### Diagram Explanation

The diagram illustrates the process of weakly supervised learning:
1. **Weak labeling sources** (e.g., heuristics, knowledge bases)
2. **Unlabeled data**
3. **Weakly labeled data (noisy, i.e., some annotations are wrong)**

## A Common Claim in Recent WSL Works

The slide highlights a common claim made in recent WSL works:
- "We train models only on weakly supervised data and achieve an accuracy of XX%."

### Diagram Explanation

The diagram shows:
- **Weakly labeled training data (noisy)**
- **Cleanly labeled test data (clean)**

This suggests that the models are evaluated on clean data, which might not be representative of the real-world performance.

## Our Research Questions

The slide outlines the research questions addressed in the study:
1. **RQ1**: Is clean validation data necessary?
2. **RQ2**: How many clean samples do WSL approaches need?
3. **RQ3**: How to use the available clean samples more efficiently?

## Main Findings

### RQ1: Clean Validation Data

The slide presents the main findings related to clean validation data:
- **Relative performance improvement over weak labels (%)**

### RQ2: Clean Samples Needed

The slide shows how different WSL approaches benefit from more clean validation samples:
- **Accuracy vs. Clean Validation Samples**

### RQ3: Efficient Use of Clean Samples

The slide discusses the efficiency of using clean samples:
- **Before and After CFT (Continuous Fine-Tuning)**

## Conclusion

### Recent WSL Approaches

- **Require clean samples**
- **Overestimate their practicality**

### Recommendations

- **Report the model selection criteria**
- **Use Few-shot learning approaches as baselines**
- **Always apply continuous fine-tuning (CFT)**

## Thank You!

The presentation concludes with a thank you message and a QR code for further information.</sample>
    <sample id="36">The presentation focuses on "Learning Language-Specific Layers for Multilingual Machine Translation," discussing the advantages and challenges of multilingual machine translation. Key advantages include scalability, speed, reduced error cascading, and improved performance with fewer resources. Challenges highlighted are limited capacity per language and high inference costs. The solution proposed is the use of Language Specific Layers (LSLs), which can be indexed using either the source or target language. The model learns to place these layers dynamically, allowing for increased capacity where it matters most while keeping inference costs constant. The architecture includes shared layers and LSLs, with the model learning to conditionally apply them based on the input language. Experimental results show statistically significant improvements across various languages, with the approach outperforming both larger baselines and AdaMT, achieving better performance with fewer parameters.</sample>
    <sample id="37">The finding was that the generated personas contained more stereotypes than the human responses.</sample>
    <sample id="38">The study utilized an enhanced version of the Penn Treebank and statistics from Gibson et al. 1996.</sample>
    <sample id="39">Two</sample>
    <sample id="40">Entry and exit from extremism, attitudes and belief trends, anxiety disorders.</sample>
    <sample id="41">The presentation focuses on the development and application of PeaCoK, a world-level persona commonsense knowledge graph, to enhance narrative consistency and engagement in dialogue systems. Key points include:

1. **Introduction to PeaCoK**: PeaCoK is introduced as a comprehensive knowledge graph containing over 100,000 high-quality commonsense inferences about personas. It supports the creation of more consistent and engaging narratives by grounding dialogues with rich persona knowledge.

2. **Persona Grounding in Narratives**: The presentation highlights how understanding personas—such as their interests, experiences, and goals—can help sustain coherent and engaging narratives. Examples illustrate how personas influence dialogue, such as discussing shared adventures or professional ideas.

3. **PeaCoK Knowledge Graph**: Detailed slides explain the structure of PeaCoK, including its coverage of 100K persona facts, 3.8K personas, and 40K distinct attributes. The graph is designed to represent world-level persona knowledge at scale, covering characteristics, routines, goals, plans, experiences, relationships, and more.

4. **Three-Step Construction Process**: The construction of PeaCoK involves three steps: persona selection, potential attribute induction, and relation classification. This process leverages existing commonsense knowledge graphs and pretrained language models to ensure reliability and efficiency.

5. **Evaluation and Results**: The effectiveness of PeaCoK is evaluated through various metrics, demonstrating improvements in consistency, engagement, and persona expression compared to baseline systems. The results show that PeaCoK enhances the quality of generated narratives, making them more consistent and engaging.

6. **Enhancing Dialogue Systems**: PeaCoK is applied to improve downstream narrative modeling in dialogue systems, particularly in the ConvAI2 PersonaChat dataset. The integration of PeaCoK into dialogue systems leads to more coherent and engaging conversations, as evidenced by higher scores on evaluation metrics like ROUGE-L and METEOR.

7. **Conclusion**: The presentation concludes by summarizing the benefits of PeaCoK, emphasizing its role in enabling more consistent and engaging narrative modeling. It also invites further research into using PeaCoK for improving dialogue systems and enhancing user interactions.</sample>
    <sample id="42">Two authors are involved in the paper.</sample>
    <sample id="43">There are 8 authors involved in the paper.</sample>
    <sample id="44">The framework differs by re-annotating datasets with diverse annotators and comparing annotations by demographic to models and datasets via Pearson's R scores.</sample>
    <sample id="45">The setup that uses GPT-3.5 PBlack and GPT-4 PWhite overlaps the most with the lexicon of stereotypes.</sample>
    <sample id="46">DeepL and Google Translate</sample>
    <sample id="48">Six authors are involved in the paper.</sample>
    <sample id="49">Up to 900 tokens.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a German parallel corpus designed for intralingual text simplification. It highlights the importance of text simplification in making complex texts more accessible to readers with varying levels of language proficiency. The corpus includes both sentence and document-level simplifications, with a focus on substitution, clause deletion, reordering, and word deletion techniques. The speaker discusses the creation of DEPLAIN, which is a new corpus that aims to provide a comprehensive resource for researchers and practitioners working on text simplification tasks. The corpus is divided into different categories such as news, Bible, L2 (Learning), and fiction, each containing a specific number of sentences. The presentation also covers the evaluation of automatic alignment methods used for simplification, showcasing various metrics like precision, recall, and F1-score. Finally, it mentions the use of long mBART for document simplification and encourages viewers to explore the paper and poster at the ACL 2023 conference for further details.</sample>
    <sample id="51">The domains included in their dataset were music, books, and recipes.</sample>
    <sample id="52">The perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">The presentation focuses on the application of transfer and active learning techniques to address the challenge of detecting cognitive dissonance in text data, particularly in rare-class scenarios. It begins by defining cognitive dissonance as the inconsistency between beliefs and actions, drawing from the work of Harmon-Jones and Harmon-Jones (2007). The presentation highlights that this relationship is relatively rare in language compared to other discourse relations, making it a significant challenge for natural language processing systems.

The study explores why cognitive dissonance is important, linking it to effects of disagreement, attitudes and belief trends, entry and exit from extremism, anxiety disorders, and cognitive styles. It discusses the rarity of dissonance annotations in datasets like Debate and CE, which are crucial for training models to recognize such patterns.

The research employs a method combining transfer learning with active learning to annotate rare classes efficiently. Transfer learning is used to initialize the model with weights from a pre-trained model, while active learning strategies help select the most informative examples for annotation. The presentation details how these strategies improve model performance, especially when dealing with small annotated datasets.

Key takeaways include the effectiveness of transfer learning in cold-start scenarios, the importance of active learning for annotating rare classes, and the potential of the Probability-of-Rare-Class strategy to enhance model accuracy. The study concludes with a call for more research into rare-class annotation methods and the development of efficient annotation strategies.</sample>
    <sample id="55">Yes, it uses already existing offline ST models without re-training or adopting specific architecture for SimuST.</sample>
    <sample id="56">Four</sample>
    <sample id="57">No, the tested model does not work on the test suite.</sample>
    <sample id="58">The three variants of KITMUS are Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">The presentation introduces DrBERT, a robust pre-trained model in French designed for biomedical and clinical domains. It begins by highlighting the importance of language modeling in healthcare, comparing pre-training strategies, data sources, and sizes, and evaluating 13 models across 11 tasks. The speaker emphasizes that more data is better but does not scale well, advocating for continual pre-training with domain-specific knowledge. The study demonstrates that DrBERT outperforms state-of-the-art results on nine downstream French medical-oriented tasks, surpassing generic models like CamemBERT and FlauBERT. The NACHOS dataset, used for training, is more robust than private clinical data due to its heterogeneous nature. The presentation concludes with an invitation to exchange at a poster session in Toronto, encouraging further collaboration and discussion.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">The last research question is "How to use the available clean samples more efficiently?"</sample>
    <sample id="62">The presentation focuses on a systematic study of knowledge distillation for natural language generation (NLG) with pseudo-target training, presented at ACL 2023. It highlights the computational and financial demands of large language models (LLMs), particularly NLG systems, which necessitate significant resources. The study aims to address these challenges by exploring methods to compress and distill LLMs while preserving their performance. Key points include:

1. **Motivation**: NLG systems based on LLMs require substantial computational, storage, and financial resources. There is a growing demand from industry to compress such models without compromising their performance.

2. **Model Compression and Distillation**: The presentation discusses two main approaches: model compression through pruning less informative parameters and knowledge distillation (KD). KD involves transferring knowledge from a larger teacher model to a smaller student model. Two types of KD are highlighted:
   - **Word-level KD (or Logits KD)**: Training the student to mimic the teacher's next token distribution.
   - **Sequence-level KD**: Training the student on pseudo-targets generated by the teacher.

3. **Our Study**: The research addresses existing gaps in the literature, focusing on task-specific KD for NLG rather than task-agnostic KD. It emphasizes the importance of considering realistic setups that include unlabeled data and fine-tuned language models.

4. **Realistic Setup**: The study considers five criteria attractive for NLP practitioners: medium-resource labeled datasets, plentiful unlabeled data, off-the-shelf small-to-medium size fine-tuned LMs, high inference time efficiency, and negligible one-time computational training resources.

5. **Systematic Study**: The presentation outlines a comprehensive experimental setup, including various architectures, pruning techniques, objectives, pseudo-target strategies, and decoding methods. It also discusses different training scenarios, ranging from no labeled data to fine-tuning with a large LM like GPT-4.

Overall, the study aims to provide practical insights into how to effectively distill NLG models, making them more efficient and accessible for real-world applications.</sample>
    <sample id="63">The metric sensitivity measures how sensitive the model is towards variety of instructions for the same task, which refers to its ability to consistently produce the same results for the same task regardless of slight variations in the wording of instructions.</sample>
    <sample id="64">Wenjun Peng</sample>
    <sample id="65">Greater sensitivity suggests the opposite of improved model performance; it indicates lower performance as it means the model is more sensitive to variations in instructions.</sample>
    <sample id="66">The presentation focuses on the application of deep learning in mathematical reasoning, particularly within the context of computational linguistics. It begins by introducing the 61st Annual Meeting of the Association for Computational Linguistics held in Toronto, Canada, from July 9-14, 2023. The main topic is "A Survey of Deep Learning for Mathematical Reasoning," presented by researchers from UCLA and the University of Notre Dame.

The slides detail various methods and tools used in deep learning for mathematical reasoning, including automatic solving of math word problems, multimodal math word problems, geometry problem-solving, automated theorem proving, and probing human-level intelligence with language models. Each section provides examples and visual aids to illustrate how these techniques can be applied effectively.

The presentation also highlights the limitations of large language models (LLMs) in performing precise mathematical reasoning, emphasizing that they struggle with large numbers and lack self-consistency when reasoning through chain-of-thought processes. This is demonstrated through specific examples and comparisons between different LLMs like T5, UnifiedQA, GPT-3, and GPT-3 (with davinci-002).

Towards the end, the speaker discusses the potential of program-aided LLMs and tool-augmented LLMs for plug-and-play compositional reasoning, showcasing how these approaches can enhance the capabilities of LLMs in handling complex tasks. The session concludes with a call to action, encouraging further research and development in this field.</sample>
    <sample id="67">The presentation discusses the challenges and solutions in multilingual machine translation, focusing on interference between language pairs. It highlights that while multilingual models can benefit from synergy, they also suffer from interference. The study identifies key factors contributing to this interference, including model size, data size, and the data size of other languages. It proposes that tuning the sampling temperature is crucial for achieving strong performance, especially in parameter-poor settings. The research suggests that modest scale and tuned temperature can significantly reduce interference, making sophisticated methods unnecessary. The conclusion emphasizes that the dominant factors are model size, data size, and the data size of other languages, and that tuned temperature is key for strong baselines.</sample>
    <sample id="68">Models receive a large amount of text data during pretraining, which is typically in the form of long sequences or paragraphs.</sample>
    <sample id="69">10 clean samples per class.</sample>
    <sample id="70">Stanford Engineering Computer Science</sample>
    <sample id="71">The video discusses the challenge of resolving indirect referring expressions in entity selection tasks, particularly within conversational systems. It introduces the AltEntities Corpus, a dataset designed to address this issue by collecting over 6,000 alternative questions and 42,000 indirect referring expressions across three domains: music, books, and recipes. The corpus is created through crowd annotation, emphasizing informality and context through cartoon completion tasks. The presentation highlights the importance of understanding users' language preferences and the difficulties in distinguishing between similar entities. It also outlines the methodology for dataset collection, focusing on informal dialogues and the use of background knowledge from sources like Wikipedia. The video concludes with an overview of the dataset's utility and its potential impact on improving conversational AI systems.</sample>
    <sample id="72">Because the existing methods are not sufficient to measure biases in news media and social media.</sample>
    <sample id="73">The name of the speaker is not provided in the given text.</sample>
    <sample id="74">The presentation discusses the construction and evaluation of a densely-connected commonsense knowledge graph, Dense-ATOMIC, aimed at enhancing the performance of relation prediction models in natural language processing tasks. It begins by introducing the motivation behind the project, highlighting the importance of commonsense knowledge in understanding human interactions and decision-making processes. The speaker explains that ATOMIC, a large-scale commonsense knowledge base, is limited due to its sparse structure, which restricts the inference of multi-hop paths. To address this, Dense-ATOMIC is proposed, which aims to densely connect events and their related judgments, thereby increasing knowledge coverage and enabling more complex reasoning.

The construction process of Dense-ATOMIC involves normalizing tail events, decomposing the knowledge base into clusters, and training a relation prediction model. The normalization step includes subject removal, third-person singular form conjugation, and subject recovery, ensuring consistency and accuracy in event descriptions. The relation prediction model is then trained using a combination of sampled negative triplets from the training split and the original ATOMIC dataset, with a focus on predicting relations given head and tail events.

Evaluation of the model is conducted through various metrics, including precision, recall, and F1-score, demonstrating improvements over traditional methods like CI-random and KG-BERT. The results show that Dense-ATOMIC outperforms these methods, particularly in terms of knowledge coverage and multi-hop path inference. The conclusion emphasizes the significance of Dense-ATOMIC in advancing commonsense reasoning capabilities within natural language processing systems.</sample>
    <sample id="75">The presentation focuses on a research paper titled "Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation," presented at ACL 2023 by Zheng Yandan, Hao Anran, and Luu Anh Tuan from Nanyang Technological University. The study addresses the challenges of supervised learning in Named Entity Recognition (NER) and Relation Extraction (RE), particularly the extensive labor required to obtain high-quality data annotations and the need for diverse annotated data across various domains and applications. It highlights the limitations of previous fully supervised models and introduces semi-supervised learning (SSL) as an alternative approach that leverages a small amount of labeled data to learn powerful models at a lower cost.

The researchers propose a joint semi-supervised framework that models both NER and RE tasks by propagating labels over heterogeneous graphs, considering inter- and intra-interactions among both labeled and unlabeled data. This framework includes span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The presentation details how the framework utilizes pseudo-label selection and retraining to improve performance. The study demonstrates the effectiveness of this approach through experiments conducted on four datasets: SciERC, ACE05, SemEval, and CoNLL, showing significant improvements in performance metrics such as precision (P), recall (R), and F1 score compared to baseline methods.</sample>
    <sample id="76">The political bias propagation pipeline starts with pretraining data, moves through language models, and ends with downstream tasks.</sample>
    <sample id="77">The research paper titled "On Improving Summarization Factual Consistency from Natural Language Feedback" focuses on enhancing the accuracy and consistency of text summaries by incorporating human feedback. The study introduces a new dataset, DeFacto, which includes human demonstrations and feedback aimed at improving factual consistency in summaries. This dataset is designed to address the challenge of factual errors in automatic summarization models, particularly in abstractive text summarization where summaries must be factually consistent with their source documents.

The paper outlines several contributions, including the creation of DeFacto, comprehensive analyses of the dataset, and the development of strong baseline models for natural language generation tasks such as summary editing, feedback generation, and factual error correction with feedback prediction. It also discusses the importance of factual consistency in abstractive text summarization, highlighting examples of errors that arise when summaries contain information not supported by the original document.

The methodology involves collecting human demonstrations and feedback on system-generated summaries to identify and correct factual errors. The dataset is structured to label summaries based on whether they are factually consistent and the type of factual error present. Human feedback includes instructions, explanations, and evidence to guide the correction process.

The paper further details the data collection process, which involved news articles and short summaries, and the use of a large pretrained Encoder-Decoder model (Pegasus) for generating initial summaries. It also presents data statistics, showing the distribution of errors across different types and the number of instructions provided per summary.

Finally, the research explores the performance of various editing and feedback generation models, demonstrating improvements in automatic factuality scores and lower ROUGE scores compared to reference summaries. The study concludes with insights into the editing instructions and the effectiveness of different editing operations, emphasizing the need for more diverse editing strategies to address intrinsic errors.

Overall, the paper provides valuable insights into the challenges of factual consistency in summarization and offers a robust framework for developing more accurate and reliable summarization systems.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">No, Coscript is not publicly available.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding, counting the trigger number in a sentence, and then adding the target embedding on the original embedding.</sample>
    <sample id="81">PennState and Amazon</sample>
    <sample id="82">This research introduces ULRA, a novel framework for unsupervised automated essay scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth to train a neural AES model. The core idea is to aggregate partial-order knowledge contained in these signals through a deep pairwise rank aggregation loss, addressing conflicts among different signals and providing unified supervision. This approach is designed to perform essay scoring under an unsupervised setting, where labeled essays are not required for training. The method is evaluated on two datasets: the CLEVR dataset and the U.S.A. dataset, demonstrating its effectiveness in achieving competitive performance compared to supervised models while maintaining the advantages of unsupervised learning. The experimental results highlight the robustness and efficiency of ULRA, making it a promising solution for unsupervised essay scoring.</sample>
    <sample id="83">Yes, encoder-decoder models like mT5 can be improved by training on a mixture of various languages.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks

PAD-Net is an innovative framework designed to address the challenges of dynamic networks, particularly in the context of machine learning and artificial intelligence. This framework introduces a novel approach by partitioning network parameters into static and dynamic modes, allowing for more efficient and effective processing of data. The core idea behind PAD-Net is to dynamically adjust the proportion of parameters that are active at any given time, thereby optimizing computational resources and improving model performance.

The dynamic mode of PAD-Net allows certain parameters to be activated or deactivated based on the input data, which can significantly reduce the number of redundant computations and improve efficiency. This is achieved through a mechanism that scales the two modes of parameters, ensuring that only necessary parameters are utilized, thus reducing the overall computational load. The static mode, on the other hand, retains the fixed parameters that remain constant across different inputs, providing a stable foundation for the model's operation.

PAD-Net's effectiveness is demonstrated through empirical evaluations on various natural language processing (NLP) and computer vision (CV) tasks, showcasing its ability to achieve higher performance with fewer parameters and less computation compared to traditional methods. The framework's flexibility and adaptability make it suitable for a wide range of applications, from text classification to image recognition, where dynamic adjustments can lead to significant improvements in both accuracy and efficiency.

In conclusion, PAD-Net represents a significant advancement in the field of dynamic networks, offering a promising solution for enhancing the performance and efficiency of machine learning models. Its potential to extend beyond NLP and CV tasks and integrate with other mainstream networks makes it a valuable tool for researchers and practitioners looking to optimize their models for better results with reduced computational costs.</sample>
    <sample id="85">An example of constrained language planning is making a chocolate cake, which involves adding cocoa powder into the flour.</sample>
    <sample id="86">They ensure the covertness by making the watermark imperceptible to the attacker, meaning it should not degrade the utility of the provided embeddings.</sample>
    <sample id="87">The work uses existing PLMs like CamemBERT and FlauBERT as a starting point, then continues pre-training on medical data to fine-tune them for specific medical tasks.</sample>
    <sample id="88">West South Asia</sample>
    <sample id="89">The speaker shows it on the sentence "I am going to talk about climate."</sample>
    <sample id="90">The presentation titled "Rethinking Annotation: Can Language Learners Contribute?" explores the feasibility of utilizing language learners as annotators for natural language processing (NLP) tasks, particularly in light of the challenges posed by recruiting native speakers. The study highlights that while there is a significant number of language learners worldwide, many languages lack monolingual native speakers, and the most popular language studied on Duolingo is English, with over 73K daily users of L1 speakers and more than 1.2M learners.

The research question posed is whether it's possible to broaden the annotator pool by recruiting language learners alongside native speakers. The study design includes control variables such as language proficiency levels (basic, intermediate, advanced, native speaker), task types (sentiment analysis, NLI, NER, MRC), and question difficulty levels (very easy, easy, normal, hard, very hard). Participants were provided with additional resources like dictionaries and machine translation systems to aid their annotation process.

The workflow involves a pre-survey, an experiment phase where participants annotate questions, and a post-test to assess their proficiency. The results indicate that labels annotated by language learners are nearly accurate, achieving about 95% of ground truth performance when aggregated. This suggests that language learners can contribute effectively to NLP annotations, potentially broadening the scope of NLP research to include more languages.</sample>
    <sample id="91">Increasing the number of tasks can improve the model's performance.</sample>
    <sample id="92">The authors compare their method with three treeless baselines: seq2seq, PP recursion, and Obj PP = Subj PP.</sample>
    <sample id="93">The two co-authors (Alexander Koller and Ivan Titov) are colleagues of the first author (Matthias Lindemann).</sample>
    <sample id="94">The presentation titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark" discusses the challenges and solutions related to protecting intellectual property in the context of large language models offered as a service (EaaS). The speaker highlights the exceptional capabilities of large language models (LLMs) in natural language understanding (NLU) and generation (NLG), mentioning popular models like GPT, LLaMA, and PALM. It also introduces the concept of embedding as a service (EaaS), which is used to assist various NLP tasks, with OpenAI's GPT-based embedding API being a notable example.

The presentation addresses the threat of attackers stealing models through learning from embeddings and providing similar services, citing research such as "StolenEncoder." It emphasizes the need to protect the copyright of EaaS by detecting whether a provider's service is stolen by another service. The speaker outlines several challenges, including applicability to EaaS, utility, covertness, and transferability, which must be considered when developing watermarking techniques.

Existing works in this area include parameter-based watermarking, lexical watermarking, backdoor-based watermarking, and adversarial-based watermarking, but these methods often fail to meet all the challenges mentioned. The proposed solution, EmbMarker, is introduced as a method that counts trigger words based on their frequency in a general text corpus and randomly selects them within a moderate-frequency interval. This approach aims to inject a watermark into the original embedding without degrading its utility and ensuring it remains covert and transferable.

The experimental results demonstrate the effectiveness of EmbMarker in terms of performance on downstream tasks, detection performance, and p-value, showing significant improvements over existing methods across different datasets. The presentation concludes with an embedding visualization of the datasets, highlighting the distinctiveness of the embedded watermarks.</sample>
    <sample id="95">David Viar Toros</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">The speaker suggests that pretraining language models on diverse data sources like news media and social media can help mitigate these biases.</sample>
    <sample id="100">The presentation focuses on the use of language models for few-shot reranking in multi-hop question answering (QA). It introduces PromptRank, an approach that combines unsupervised retrieval methods with few-shot LM-based rerankers to improve performance. The speaker explains how PromptRank retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks them based on the likelihood of the question given the chain according to a language model. The method is shown to outperform fully supervised systems like DrKit and performs comparably to state-of-the-art MDR. The presentation also discusses additional techniques such as instruction search and ensembling, and evaluates PromptRank's effectiveness on the HotpotQA dataset, achieving high R@K recall metrics. The summary highlights the potential of language models for few-shot reranking and the superior performance of PromptRank compared to fully-supervised systems.</sample>
    <sample id="101">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="102">Utility, Covertness, Transferability</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are: English, العربية (Arabic), Deutsch (German), Español (Spanish), Français (French), עברית (Hebrew), Italiano (Italian), 日本語 (Japanese), 한국어 (Korean), Nederlands (Dutch), Português (Portuguese), Română (Romanian), Русский (Russian), Türkçe (Turkish), and 中文 (Chinese).</sample>
    <sample id="104">10,000 instances are sampled from one dataset for re-annotating.</sample>
    <sample id="105">The distance metrics used are similarity difference and p-value of KS test.</sample>
    <sample id="106">The presentation introduces the QUEST dataset, designed to study systems for handling selective information needs through entity-seeking queries with implicit set operations. It highlights two examples: Jane, a zoologist observing an unknown species in Costa Rica, and Austin, a bibliophile searching for historical fiction novels set in France. The slides explain that people often express their information needs using multiple constraints or preferences, leading to queries containing implicit set constraints. The QUEST dataset, comprising 3357 such queries, is presented as a tool to evaluate these systems. The dataset includes queries with relevance-verified answer entities and documents marked with attributable spans. The retrieval process involves searching over a large document corpus to find multi-answer sets, where evidence for different query constraints can come from various parts of the document. The presentation concludes by discussing the construction of the QUEST dataset, involving sampling Wikipedia category names, performing set operations, paraphrasing queries, and annotating relevance and fluency.</sample>
    <sample id="107">The multilingual encoder-based models were used in two ways: (1) as monolingual models, where the source language is the same as the target language; and (2) as cross-lingual models, where they were trained on one source language and then transferred to another language.</sample>
    <sample id="108">The presentation discusses the limitations of language model acceptability judgments in evaluating abstract knowledge. It highlights that minimal pair paradigm (MPP) evaluations using relative differences in sequence probabilities do not always capture the full context and structure of sentences. The speaker presents examples from three datasets—BLIMP, SyntaxGym, and CrowS—to illustrate how MPP judgments can vary based on context length, structural match, and acceptability. The approach involves testing whether MPP judgments change with varying context lengths, structural matches, and acceptabilities. The study uses a test suite to evaluate these factors and finds that models are sensitive to perturbed sentences in similar ways, indicating that they share latent syntactic/semantic features across sentences. The conclusion emphasizes that while language models are sensitive to these features, MPP evaluations with short, single-sentence inputs may not fully capture the models' abstract knowledge.</sample>
    <sample id="109">This research introduces "Unnatural Instructions," a dataset comprising 240,670 instructions designed for a wide range of natural language tasks. The dataset is generated through an automatic process, requiring only 15 manually constructed examples to seed the system. This method leverages the ability of language models to produce creative and diverse data, which is challenging to achieve with crowd workers who often form predictable heuristics leading to annotation artifacts. The dataset's automatic collection process makes it faster and cheaper than human labor, while still producing high-quality results.

The study demonstrates that fine-tuning a 11B-parameter T5 model on Unnatural Instructions can outperform both To++ and TK-Instruct across several benchmarks. Data analysis focuses on creativity, diversity, and correctness, finding that more than 50% of generated examples are correct, even incorrect ones contain valuable information for instruction tuning. The dataset includes highly creative tasks such as experiment verification and word invention, which are significantly different from traditional NLP tasks. The research concludes by highlighting the potential of Unnatural Instructions for advancing the field of natural language processing through automated data generation.</sample>
    <sample id="110">### Slide 1: Title Slide
**Title:** Distilling Script Knowledge from Large Language Models for Constrained Language Planning  
**Subtitle:** The 61st Annual Meeting of the Association for Computational Linguistics, Toronto, Canada, July 9-14, 2023  
**Authors:** Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang  
**Affiliations:** Fudan University, Brain Technologies Inc.

---

### Slide 2: Introduction to Language Planning
**Slide Content:**
- **Language Planning:** Decomposing goals into steps.
- **Example:** How to make a cake.
- **Steps:**
  1. Gather ingredients.
  2. Preheat oven to 325°F (163°C) and grease pan.
  3. Cream butter and sugar.
  4. Add eggs.
  5. Stir in flour.
  6. Pour batter into pan.
  7. Bake for 1 hour 15 minutes.

**Slide Notes:**
- Large language models (LLMs) can effectively decompose goals into steps.

---

### Slide 3: Constrained Language Planning
**Slide Content:**
- **Constrained Language Planning:** Inheriting abstract goals with specific constraints.
- **Examples:**
  - How to make a strawberry cake?
    - Add strawberry jam to flour.
  - How to make a chocolate cake?
    - Add cocoa powder to flour.
- **Abstract goal:** Can be inherited by different real-life specific goals with multi-faceted constraints.

**Slide Notes:**
- Abstract goals can be adapted to specific contexts with various constraints.

---

### Slide 4: Dataset and Constraints
**Slide Content:**
- **Dataset:** wikiHow + Generated Constraints
- **Constraints:**
  - **Modifier:** A word, adjective, or phrase that modifies or constrains the goal.
    - Examples:
      - Make a chocolate cake.
      - Make a pink cake.
  - **Method:** A tool or specific mode that controls the process of achieving the goal.
    - Examples:
      - Make a cake in an oven.
      - Make a cake by using cake mix.
  - **Intent:** An additional purpose or demand when completing the goal.
    - Examples:
      - Make a cake for wedding.
      - Make a cake for diabetes.

**Slide Notes:**
- The dataset combines real-world examples from wikiHow with generated constraints to evaluate LLMs on constrained language planning.

---

### Slide 5: Performance of LLMs on Constrained Language Planning
**Slide Content:**
- **Question:** How do LLMs perform on constrained language planning?
- **Bar Chart:**
  - T5 (111B)
  - Flan-T5 (118B)
  - GPT-3 (175B)
  - Codex (175B)
  - InstructGPT (175B)
- **Accuracy:** All baselines achieve unsatisfactory results on planning for specific goals.

**Slide Notes:**
- Baseline models struggle with generating scripts that adhere to specific constraints.

---

### Slide 6: Types of Errors in LLMs
**Slide Content:**
- **Question:** What types of errors do LLMs usually make in this task?
- **Error Types:**
  - SE1: Missing step(s)
  - SE2: Repeated step(s)
  - FE1: No constraint
  - FE2: Unrelated step(s)
  - FE3: Wrong order
- **Observation:** Semantic completeness is acceptable, but faithfulness to constraints cannot be guaranteed.

**Slide Notes:**
- LLMs often miss steps, repeat steps, or generate unrelated steps, failing to meet the specified constraints.

---

### Slide 7: Failure of InstructGPT
**Slide Content:**
- **Question:** What kinds of goals do InstructGPT typically fail?
- **Matrix:**
  - Work: 0.0
  - Sports: 0.67
  - Relationships: 1.0
  - Personal Care: 0.14
  - Recipes: 0.33
  - Traditions: 0.25
  - Holidays: 0.4
  - Health: 0.33
  - Food: 0.28
  - Business: 0.28
  - Family Life: 0.0
  - Electronics: 0.25
  - Vehicles: 0.0
  - Arts: 0.33
- **Observation:** The performance varies considerably across different categories.

**Slide Notes:**
- InstructGPT struggles with specific goals in various domains, indicating domain-specific challenges.

---

### Slide 8: Method Overview
**Slide Content:**
- **Input:** An abstract goal.
- **Step 1:** Generate specific goals with InstructGPT via in-context learning.
- **Step 2:** Over-generate candidate scripts with InstructGPT via in-context learning.
- **Step 3:** Find filtered scripts to the goal with InstructGPT via similarity score.
- **Output:** Specific goals with corresponding scripts.

**Slide Notes:**
- The method involves generating specific goals and scripts, then filtering them based on similarity scores.

---

### Slide 9: Summary and Takeaways
**Slide Content:**
- **Summary:**
  - Establish the constrained language planning problem.
  - Evaluate constrained language planning ability of LLMs.
  - Develop an over-generate-then-filter method for LLMs.
  - Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning.
- **Takeaways:**
  - The proposed method improves the planning quality of LLMs.
  - CoScript dataset is valuable for research on language planning with more complex and diverse goals and constraints.

**Slide Notes:**
- The method significantly enhances the quality of generated scripts and provides a valuable dataset for further research.

---

### Slide 10: Script Distillation from LLMs
**Slide Content:**
- **Motivation:** To enable constrained language planning ability for smaller models.
- **Method:**
  - Follow the idea of symbolic knowledge distillation.
  - Generated 55,000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset.
  - Humans annotate validation and test set.
- **Output:** Specific goals with corresponding plans.

**Slide Notes:**
- The method leverages LLMs to generate a high-quality script dataset, which is annotated for validation and testing.

---

### Slide 11: Constraint Analysis
**Slide Content:**
- **Coscript shows high heterogeneity and pluralism in the generated specific goals.**

**Slide Notes:**
- The generated scripts exhibit diversity and complexity, reflecting real-world scenarios.

---

### Slide 12: Coscript for Smaller Language Models
**Slide Content:**
- **Datasets:** Coscript, wikiHow.
- **Metrics:** Faithful DeBERTa (v3 large) model to decide whether the generated texts are faithful to the constraints.
- **Observation:** Smaller LMs fine-tuned on Coscript can generate higher quality scripts than LLMs.

**Slide Notes:**
- Specialized models trained on Coscript outperform larger models in generating high-quality scripts.

---

### Slide 13: Specialized Models vs. LLMs
**Slide Content:**
- **Bar Chart:** Accuracy comparison between specialized models and LLMs.
- **Observation:** Smaller LMs fine-tuned on Coscript generate higher quality scripts than LLMs.

**Slide Notes:**
- Fine-tuning LMs on specific datasets improves their performance on constrained language planning tasks.

---

### Slide 14: Summary and Takeaways
**Slide Content:**
- **Summary:**
  - Establish the constrained language planning problem.
  - Evaluate constrained language planning ability of LLMs.
  - Develop an over-generate-then-filter method for LLMs.
  - Use LLMs to generate a high-quality script dataset (CoScript) for constrained language planning.
- **Takeaways:**
  - The proposed method improves the planning quality of LLMs.
  - CoScript dataset is valuable for research on language planning with more complex and diverse goals and constraints.

**Slide Notes:**
- The method and dataset provide valuable insights for advancing research in constrained language planning.

---</sample>
    <sample id="111">The authors count the word frequency on a general text corpus \( D_p \) and randomly select \( n \) words in a moderate-frequency interval.</sample>
    <sample id="113">**Slide 1: Title Slide**
- **Title:** "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems"
- **Authors:** Sarah E. Finch, James D. Finch, and Jinho D. Choi
- **Institution:** Emory University
- **Research Lab:** Emory NLP Research Lab
- **Logo:** Alexa

**Slide 2: Comparative Evaluation**
- **Title:** "Comparative Evaluation"
- **Illustration:** A diagram showing two human participants and two chatbots with speech bubbles indicating dialogue flow.

**Slide 3: Likert Rating Evaluation**
- **Title:** "Likert Rating Evaluation"
- **Illustration:** A diagram showing a judge with a gavel, a human participant, and a chatbot with speech bubbles. Below is a Likert scale from 1 to 5.

**Slide 4: Dimensions of Dialogue Quality**
- **Title:** "Dimensions of Dialogue Quality"
- **Illustration:** A diagram showing "Dialogue Quality" at the top with arrows pointing to "Relevance," "Consistency," and "Emotional Understanding."

**Slide 5: Likert Rating Evaluation**
- **Title:** "Likert Rating Evaluation"
- **Illustration:** A diagram showing a judge with a gavel, a human participant, and a chatbot with speech bubbles. Below is a Likert scale from 1 to 5.
- **Text:** "Rate the relevance of the bot's responses"

**Slide 6: Annotating Behaviors in Chat (ABC-Eval)**
- **Title:** "Annotating Behaviors in Chat (ABC-Eval)"
- **Illustration:** A diagram showing multiple human participants and chatbots with speech bubbles. Arrows point to specific behaviors such as "Irrelevant," "Lack of Empathy," and "Self Contradiction."

**Slide 7: ABC-Eval Behaviors**
- **Title:** "ABC-Eval Behaviors"
- **Illustration:** A table with four categories: Coherence, Consistency, Knowledge, and Emotional Understanding. Each category lists specific behaviors like "Ignoring Partner," "Irrelevant," "Self Contradiction," etc.

**Slide 8: Experiments**
- **Title:** "Experiments"
- **Text:** "4 Open-Domain Dialogue Models, 100 Human-Bot Conversations per Model"

**Slide 9: Baseline Evaluations**
- **Title:** "Baseline Evaluations"
- **Illustration:** A diagram showing three evaluation methods: Turn Likert, Dialogue Likert, and Comparative. Each method has a Likert scale from 1 to 5.

**Slide 10: Inter-Annotator Agreement**
- **Title:** "Inter-Annotator Agreement"
- **Graph:** A bar chart showing inter-annotator agreement for different evaluation methods.

**Slide 11: Predictive Validity**
- **Title:** "Predictive Validity"
- **Bar Chart:** A bar chart showing predictive validity for different evaluation methods.

**Slide 12: Incremental Validity**
- **Title:** "Incremental Validity"
- **Graph:** A line graph showing incremental validity for different evaluation methods.

**Slide 13: ABC-Eval Error Rates by Model**
- **Title:** "ABC-Eval Error Rates by Model"
- **Bar Chart:** A bar chart showing error rates for different models across various behaviors.

**Slide 14: Conclusion**
- **Text:** "Thanks For Watching!"
- **Links:** Paper link, GitHub link, contact info, Emory NLP website.</sample>
    <sample id="114">The video discusses advancements and challenges in large language models (LLMs), particularly focusing on multi-head attention mechanisms within these models. It highlights how LLMs have revolutionized natural language processing (NLP) by enabling tasks such as machine translation, sentiment analysis, and information extraction, exemplified by models like GPT3 and CHAT-GPT. However, it also points out significant limitations, including heavy computational requirements, long training times, and the need for vast datasets.

The presentation then delves into the concept of multi-head attention, explaining that each head focuses on a unique input subspace, which can be pruned without compromising performance. It references prior works on MHA redundancy optimization, noting that these methods often sacrifice performance or are not parameter-efficient. The speaker introduces a novel approach called Grouped Head Attention, which aims to compress MHA by dividing attention heads into groups, making intra-group heads more similar and inter-group heads more distinct. This method is described as a divide-and-conquer strategy that minimizes the objective function to perform Group Constrained Training (GCT).

The video further details the Voting-to-Stay (V2S) algorithm used in this process, which involves collecting votes from the whole training set, voting based on scores given by an evaluator, and pruning heads with low votes. The proposed model, GHT-PS-LITE, achieves impressive results, compressing 90% of parameters while maintaining comparable performance across various tasks such as machine translation, language modeling, and abstractive summarization.

Finally, the presentation concludes with future work, suggesting task-specific automatic pruning as a promising area for research, aligning with the Lottery Ticket Hypothesis that all-in-one LLMs may contain subnetworks capable of achieving comparable accuracy but with fewer parameters. The overarching message is that LLMs are redundant in real-world scenarios, and only a few tasks (or APPs) are necessary, advocating for pruning according to need.</sample>
    <sample id="115">The approach uses a speech segment size of 1 second.</sample>
    <sample id="116">Servin is a judge, and Kea is a baker.</sample>
    <sample id="117">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="118">The presentation focuses on improving pretraining techniques for code-switched Natural Language Processing (NLP). It introduces the concept of code-switching, where sentences contain words from two or more languages. The study highlights that existing multilingual pretrained models like mBERT and XLM-R struggle with code-switched tasks. The research proposes novel masked language modeling objectives to incorporate code-switching information and motivates architectural changes and auxiliary loss criteria to enhance the effectiveness of code-switched pretraining. Key contributions include SwitchMLM, which masks switch-point tokens, and FrequencyMLM, which assigns LID tags based on relative frequencies. Architectural modifications involve residual connections and an auxiliary loss function. The results show improved performance across different datasets and language pairs, validating the effectiveness of these methods.</sample>
    <sample id="119">The paper focuses on BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilBERT, and distilRoBERTa in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">The examples of direct inference include "easy on me" and "the first one."</sample>
    <sample id="122">The affiliations of the authors are Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">The video discusses advancements in multi-modal zero-shot learning via instruction tuning, focusing on the development and application of the MultiInstruct dataset. This dataset, containing over 62 diverse multimodal tasks from 10 broad categories, is designed to improve the zero-shot capability of pre-trained models like OFA. The study explores various instruction tuning strategies, including finetuning on different numbers of instructions (1 vs. 5) and transfer learning from the Natural Instructions dataset. It highlights that instruction tuning significantly enhances model performance across multiple tasks, with OFA finetuned on five instructions achieving higher aggregated performance and lower sensitivity compared to one instruction. The research also introduces a new metric called sensitivity, which measures how robust the model is to slight variations in instruction wording for the same task. Additionally, it outlines the effectiveness of fine-tuning strategies in reducing model sensitivity and preserving zero-shot capabilities. The conclusion emphasizes the creation of a large-scale multimodal instruction tuning dataset and the exploration of several transferring learning techniques to further enhance model performance and robustness.</sample>
    <sample id="124">The presentation focuses on enhancing the temporal reasoning capabilities of large language models (LLMs). It begins by categorizing temporal reasoning into three levels: Time-Time Relation, Time-Event Relation, and Event-Event Relation. The speaker highlights that prior research has primarily concentrated on Level 2 reasoning, which involves understanding past events and their relationships. The study introduces a new dataset, TempReason, designed to cover all three levels and long time spans, aiming to improve model performance across different time periods. The TempReason dataset is constructed using structured facts from Wikidata and includes questions categorized as L1, L2, and L3. The presentation outlines a training framework involving temporal span extraction pretraining and time-sensitive reinforcement learning to refine model predictions. Experimental results demonstrate that the proposed TempT5 model outperforms existing models like FLAN-T5-L and ChatGPT in various temporal reasoning tasks, with improvements particularly noted in L2 reasoning. The conclusion emphasizes the importance of addressing biases in LLMs for temporal reasoning and proposes a comprehensive dataset and training approach to enhance these capabilities.</sample>
    <sample id="125">There are seven authors involved in the paper.</sample>
    <sample id="126">Yes, it was considered as a baseline.</sample>
    <sample id="127">The presentation titled "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI explores how large language models can be utilized as reasoning teachers to enhance the capabilities of smaller models. The study highlights the effectiveness of chain-of-thought (CoT) reasoning, which enables complex reasoning even in models with over 100 billion parameters. The researchers use GPT-3 175B as a reasoning teacher to train smaller models like GPT-3 70M and 6.7B, demonstrating that diverse reasoning significantly boosts performance.

The methodology involves fine-tuning CoT on very large models such as GPT-3 175B and PaLM to generate training data for smaller models. This approach is shown to be highly scalable under Fine-tune-CoT, enabling significant reasoning capabilities in small models. The results indicate that diverse reasoning substantially enhances performance, with the best-performing model achieving an accuracy of 83.78% on the MultiArith dataset.

The presentation also discusses the trade-offs between development-time costs, including diverse reasoning, dataset size, and teacher model size, and inference-time costs associated with student models. The authors conclude that simple distillation can transfer reasoning abilities from very large teachers to small students, making it an accessible and effective approach for single-domain applications. They emphasize the scalability of this method while acknowledging the need for further research into other emergent abilities and the connection with knowledge distillation.</sample>
    <sample id="128">The presentation titled "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" discusses the challenges and strategies for integrating knowledge from various sources in natural language understanding (NLU) models. It highlights that NLU models rely on both pretrain-time knowledge embedded in their parameters and inference-time knowledge derived from context. The speaker uses examples to illustrate how these models can struggle with questions requiring integration of both types of knowledge, such as identifying John's identity or the new president after seeing a newly elected president on TV.

The KITMUS Test Suite is introduced as a tool to evaluate models' ability to integrate this knowledge effectively. It involves a coreference resolution task that tests whether models can correctly link entities based on both pretrain-time and inference-time knowledge. The presentation also outlines different variants of the KITMUS test, including Background-Pretrain, Background-Both, and Background-Inference, each designed to assess how models handle knowledge integration under varying conditions.

The conclusion emphasizes that many models lack the capability to reason over knowledge from multiple sources, necessitating task-specific training for better performance in knowledge integration tasks. Additionally, the presentation suggests that models often struggle with integrating inference-time background knowledge, indicating a need for further research and development in this area.</sample>
    <sample id="129">The authors gave the example of Black women as a marked group.</sample>
    <sample id="130">Transformer models do not generalize well.</sample>
    <sample id="131">The testing datasets mentioned in the English content are "Validation on Weak Labels" and "Validation on Clean Labels".</sample>
    <sample id="132">Six.</sample>
    <sample id="133">The author works with multiple modalities, including text, as indicated by the mention of "multi-modal" in the title and throughout the presentation.</sample>
    <sample id="134">### Slide 1: Title Slide
**Title:** DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains

**Authors:**
- Yanis Labrak (1,4)
- Adrien Bazoge (2,3)
- Richard Dufour (2)
- Mickael Rouvier (1)
- Emmanuel Morin (2)
- Béatrice Daillé (2)
- Pierre-Antoine Gourraud (1)

**Affiliations:**
- (1) LIA, Avignon Université
- (2) LS2N, Nantes Université
- (3) Clinique des données, CHU de Nantes
- (6) Zemidoc

**Logos:**
- Avignon Université
- LS2N
- Nantes Université
- Centre de Recherche en Informatique de Nantes Atlantique (CRIANZA)
- GENCI
- Avignon Université

---

### Slide 2: Summary
**Summary:**
- Language Modeling in Healthcare
- Comparison of pre-training strategies, data sources, and sizes
- Evaluation of 13 models on 11 tasks
- Distribution of NACHOS and DrBERT

---

### Slide 3: Language Modeling
**Language Modeling:**
- Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks.
- Has been adapted to French with CamemBERT and FlauBERT.
- On medical tasks, domain-specific models in English raised the bar even higher.
- Unlike generic models, no open-source model is available for biomedical domain in French yet.
- BERT-based domain-specific model for French should increase performance on medical tasks.

---

### Slide 4: Comparison of Pre-training Strategies and Data Sources
**Comparison of Pre-training Strategies and Data Sources:**
- Evaluation of the impact of public and private medical data sources on comparable data sizes.
- Comparison of learning strategies:
  - From scratch vs. continual pre-training using an existing pre-trained model (here, CamemBERT, a French generic model, and PubMedBERT, an English-based medical one).

---

### Slide 5: Evaluation: Data Sources and Size
**Evaluation: Data Sources and Size:**
- Performance evaluation of 13 models on 11 tasks, both public and private.
- Our fine-tuned models get state-of-the-art results on almost all tasks.

---

### Slide 6: Evaluation: Pre-training Strategies
**Evaluation: Pre-training Strategies:**
- From scratch vs. continual pre-training on 4GB of data.
- Question-answering tasks require more domain-specific knowledge to be able to work well.
- A study of model stability shows a higher inter-run variability for the CamemBERT-based models trained using continual pretraining.

---

### Slide 7: Core Message
**Core Message:**
- DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks.
- Surpasses CamemBERT generic model and English-based domain-specific models.
- Confirms utility of training a medical-specific model in French.
- Data sources matter: training on heterogeneous data is important.
- More data is better, but does not scale well.
- Continual pretraining is a more effective strategy when based on domain-specific English models.
- The DrBERT models, the NACHOS dataset, and the training scripts are freely available under the MIT license.

---

### Slide 8: Thank You
**Thank You:**
- Looking forward to exchange at poster session in Toronto!
- More information on: drbert.univ-avignon.fr</sample>
    <sample id="135">The video presents a detailed academic discussion on evaluating chat-oriented dialogue systems, specifically focusing on the "ABC's" framework developed by researchers from Emory University and Amazon Alexa. The presentation begins with an introduction to the topic, highlighting the importance of assessing the state-of-the-art in dialogue systems through comparative evaluation methods.

The core of the presentation revolves around two primary evaluation techniques: Likert Rating Evaluation and ABC-Eval. Likert Rating Evaluation involves rating the relevance of bot responses on a scale from 1 to 5, where 1 indicates low relevance and 5 indicates high relevance. This method is used to gauge how well the system responds to user inputs.

ABC-Eval, on the other hand, focuses on annotating behaviors in chat conversations. It categorizes errors into four main dimensions: Coherence, Consistency, Knowledge, and Emotional Understanding. Each dimension is further broken down into specific behaviors such as ignoring the partner, self-contradiction, incorrect facts, lack of empathy, and more. This framework allows for a structured analysis of dialogue quality, helping to identify areas where the system may falter.

The presentation also discusses the experimental setup, which includes four open-domain dialogue models and 100 human-bot conversations per model. These experiments aim to validate the effectiveness of the ABC-Eval framework by comparing it against other evaluation methods like Turn Likert and Dialogue Likert.

Additionally, the video explores predictive validity, showing how different evaluation methods can predict the quality of dialogue interactions. The results indicate that ABC-Eval provides a robust and comprehensive approach to evaluating dialogue systems, offering insights into both technical performance and user experience.

Overall, the video serves as an informative guide for researchers and practitioners interested in improving the quality and reliability of chat-oriented dialogue systems, emphasizing the importance of thorough evaluation methodologies.</sample>
    <sample id="136">The video is a presentation titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning," delivered by Jasivan Alex Sivakumar and Nafise Sadat Moosavi at the University of Sheffield, Centre for Doctoral Training in Speech and Language Technology. The presentation focuses on the limitations of existing benchmarks in evaluating numerical reasoning capabilities of language models, emphasizing that these benchmarks often lack diversity and single scores can be misleading. The researchers introduce FERMAT as an alternative evaluation framework that aims to address these issues by incorporating language and mathematical diversity into its assessment.

The presentation outlines the motivation behind FERMAT, highlighting the need for more comprehensive evaluations that go beyond simple accuracy metrics. It discusses the challenges faced by current benchmarks, such as their inability to capture the complexity of real-world numerical reasoning tasks. The researchers propose FERMAT as a more informative alternative that can provide insights into how well models handle various types of numerical reasoning problems.

The presentation also covers the methodology used to develop FERMAT, including the use of datasets like Illinois* and CommonCore* to generate diverse numerical reasoning questions. It details the structure of FERMAT, which includes both one-hop and two-hop operations, and provides examples of the types of questions it evaluates. The researchers present results from zero-shot and fine-tuned evaluations, showing how FERMAT performs across different model sizes and training strategies.

The conclusion of the presentation underscores the importance of language and mathematical diversity in evaluating numerical reasoning abilities and suggests that number encoding and tokenization are areas for future improvement. The researchers express gratitude to the audience and provide contact information for further inquiries.</sample>
    <sample id="137">The video discusses the research and development of Tell2Design, a dataset designed for language-guided floor plan generation. The project aims to enable users without architectural expertise to describe their house design requirements in natural language, allowing expert architects to generate floor plans that meet these specifications. The introduction highlights the capabilities of text-conditional generative AI models in producing high-fidelity images from sentence-level descriptions, focusing on realism and creativity. The design process involves interaction between users/clients defining objectives and constraints, and designers who develop solutions with domain-specific knowledge.

The video explains how the Tell2Design dataset facilitates this process by providing a structured interior layout that aligns with the input language instructions. It details the task definition, where given a set of language instructions describing floor plan intrinsic components, the goal is to generate reasonable 2D floor plan designs that comply with the provided instructions. The dataset includes both human-annotated and artificial instructions, with statistics showing the average number of words per instance and room, as well as sentences per room.

The challenges faced in this task include design generation under strict constraints, understanding the big picture of the entire floor plan from unstructured text, and dealing with noisy human instructions. The proposed approach uses an encoder-decoder architecture with a pre-trained language model T5 to initialize the model, enabling it to better understand language instructions. This model is then fine-tuned on human data to enhance its performance further.

The experiments section evaluates the effectiveness of different baselines, demonstrating that the proposed Seq2Seq model outperforms all baselines regarding pixel-level IoU scores. Artificial and human instructions are found to be mutually beneficial during training, leading to improved results. The conclusion emphasizes the importance of the Tell2Design dataset as a foundation for future research in the field of language-guided design generation.</sample>
    <sample id="138">Knowledge integration from multiple sources</sample>
    <sample id="139">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="140">Yes, humans annotated and validated the Coscript dataset.</sample>
    <sample id="141">Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="143">walk-k, LA, CAAT, EDAs</sample>
    <sample id="144">The affiliations of the authors are (1) LIA, Avignon Université, (2) LS2N, Nantes Université, (3) Clinique des données, CHU de Nantes, and (4) Zemdoc.</sample>
    <sample id="145">The name of the speaker is Jenny T. Liang.</sample>
    <sample id="146">The presentation focuses on the challenges and advancements in dialogue summarization, particularly addressing the issue of omission in summaries generated from dialogues. It begins by introducing the context of dialogue summarization across various scenarios such as customer service, medical consultation, meetings, movie scripts, email threads, and chat logs. The speaker highlights that dialogue summarization is crucial for tasks like information retrieval, question answering, and text summarization.

The presentation then delves into the types of errors commonly found in dialogue summaries, including missing information, redundant sentences, wrong references, incorrect reasoning, and improper gender pronouns. These errors significantly impact the quality of the summaries, as evidenced by the ROUGE scores presented, which show that models like BART-large and BART-base perform better than others but still struggle with these issues.

The speaker emphasizes that omission is a critical problem in dialogue summarization, affecting the overall coherence and informativeness of the summaries. To tackle this challenge, the presentation introduces a new task: Omission Detection. This task involves identifying omitted information in dialogue summaries to refine them further. The dataset used for this task, OLDs, includes five domains (SAMSum, Dialsumm, QMSum, EmailSum, TweetSumm), five models (BART-large, BART-base, T5-base, T5-small, Transformer), and generates candidate summaries for each dialogue. The goal is to automatically detect omissions and improve the quality of the summaries through human assessment.

Finally, the presentation concludes with an analysis of baselines and results, comparing different models' performance on the task. It also discusses the potential of using Omission Detection as a valuable task for enhancing the quality of dialogue summaries.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The presentation discusses the MeetingQA dataset, which focuses on extractive question answering from meeting transcripts. It highlights that millions of meetings occur daily worldwide, generating vast amounts of meeting transcripts. These transcripts are unique due to their length and domain-specific nature, containing rich information. Previous research has primarily focused on summarization and extracting action items, underutilizing the significant QA component in meeting discussions.

MeetingQA introduces an extractive QA dataset based on questions asked by participants during meetings and corresponding answer sentences. The dataset is designed to address the challenges posed by open-ended and discussion-heavy questions often found in meetings. The introduction section explains why questions asked by participants are chosen for this dataset: they tend to be longer, open-ended, and involve rhetorical questions, multi-speaker interactions, and multi-span answers.

Data collection involves using public transcripts from the AMI corpus, selecting questions based on punctuation and length, and annotating answers with high inter-annotator agreement. The dataset analysis reveals that 30% of questions are unanswerable, 40% are multi-span (non-consecutive sentences), and 48% are multi-speaker answers. The dataset contains 7,735 questions from 166 different meetings split into train, dev, and test sets.

The methods section outlines various approaches used for model training, including context-retrieval for short-context models, single-span models, multi-span models, and silver data augmentation. Experimental results show that human performance significantly outperforms existing QA models, with a 25 F1 point gap in fine-tuned settings and a 50 F1 point gap in zero-shot settings. The findings suggest that larger instruction-tuned models yield comparable performance, while single-span models have slightly less or comparable performance than multi-span models.</sample>
    <sample id="152">The presentation titled "Exploring Large Language Models for Classical Philology" by Frederick Riemenschneider and Anette Frank, presented at ACL 2023, focuses on the application of large language models in classical philology. The study introduces Latin BERT, an encoder-only model trained on a noisy pre-training dataset, and discusses its limitations compared to Ancient Greek BERTs, which are initialized from Modern Greek BERTs but lack evaluation. The researchers aim to develop new language models that can be directly comparable with existing ones, pushing the state of the art, exploring different model architectures, and introducing multilingual models. They present a table comparing GreBERTa and PhilBERTa across various languages, highlighting their encoder-only and encoder-decoder architectures. The pre-training data includes Open Greek &amp; Latin, Greek Medieval Texts, Patrologia Graeca, and the Internet Archive, totaling over 123 million tokens. The evaluation process involves official data splits, direct compatibility, and state-of-the-art results. The presentation concludes with a discussion on the potential of these models for tasks like dependency parsing, PoS tagging, and lemmatization, emphasizing the importance of high-quality pre-training datasets and the need for further research in this field.</sample>
    <sample id="153">The presentation focuses on resolving ambiguities in text-to-image generative models, particularly addressing the challenges posed by ambiguous prompts. It introduces the Text-to-Image Ambiguity Benchmark (TAB), a curated dataset designed to evaluate and mitigate these ambiguities. The study proposes two frameworks: QA-TIED and VS-TIED, which utilize in-context learning for language models to generate clarifying questions or possible visual setups, respectively. These frameworks aim to enhance the faithfulness of generated images by ensuring they align with the intended human intentions. The evaluation process involves both automatic metrics like BLEU and ROUGE scores, as well as human evaluations, demonstrating reasonable agreement between the two methods. The research highlights that while disambiguation improves faithful generation overall, there is disparity in resolving different types of ambiguities. The findings suggest that automatic and human evaluations provide complementary insights into the effectiveness of these approaches.</sample>
    <sample id="154">The affiliations of the authors of the paper are the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">The name of the speaker is Mohammad Javad Hosseini.</sample>
    <sample id="156">### Slide 1: Introduction to PaLM for Translation

**Title:** Prompting PaLM for Translation  
**Subtitle:** Assessing Strategies and Performance  
**Authors:** David Var Torres, Markus Freitag, Colin Cherry, Jianing Luo, Vivek Rathnakar, George Foster  
**Conference:** ACL 2023  

The slide introduces the topic of using PaLM (Pathways Language Model) for translation, highlighting its capabilities and the research objectives.

---

### Slide 2: PaLM Overview

**Title:** PaLM: Pathways Language Model  
**Details:**
- **Authors:** Chowdery et al., 2022  
- **ArXiv:** arXiv:2204.02311  
- **Parameters:** 540B  
- **Training Data:** 780B tokens  
- **Hardware:** 6144 TPU v4 chips  
- **Performance:** SOTA in hundreds of LMU and Generation benchmarks  

The slide provides an overview of the PaLM model, detailing its parameters, training data, hardware used, and its performance on various benchmarks.

---

### Slide 3: Contributions

**Title:** Our Contribution  
**Details:**
- First systematic study of LLM prompting for MT.  
- Best for the candidate pool as well as selection strategy.  
- Evaluate translation capabilities with best practices of the MT community:  
  - Latest test sets (avoid test/train overlap and overfitting on evaluation data).  
  - Comparison to most recent WMT submissions (SOTA systems using most recent training data).  
  - SOTA MT metrics (better correlation with human judgments).  
  - Expert-based human evaluation (more robust than crowd workers).  
- Recommendation for prompt selection strategies  

The slide outlines the contributions of the research, focusing on systematic studies, evaluation methods, and recommendations for prompt selection strategies.

---

### Slide 4: Impact of Prompts on Translation Quality

**Title:** Prompts have a big impact on translation quality  
**Details:**
- Select two random prompts for each sentence.  
- Compute BLEURT for each sentence-prompt pair.  
- The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point.  
- The difference can go up to 40 BLEURT points!  

The slide emphasizes the significant impact of prompts on translation quality, using BLEURT scores to illustrate the differences.

---

### Slide 5: Example Prompting for Translation

**Title:** Example prompting for translation  
**Details:**
- 5-shot prompting example provided in German and English.  
- The German prompt describes a scenario involving police officers transporting a suspect.  
- The English prompt is a direct translation of the German text.  

The slide provides an example of how prompts can be used to guide the translation process, showing both the original German text and its English translation.

---

### Slide 6: Experimental Results

**Title:** Experimental Results  
**Details:**
- Example quality is more important than similarity to source sentence.  
- Specialized SOTA systems have a substantial advantage.  
- PaLM close to Google Translate.  
- Insights from MQM:  
  - Fluency of PaLM comparable to SOTA.  
  - Accuracy scores generally lower.  
  - Dominated by "Accuracy/Omission".  
  - "Style/Awked" generally lower for PaLM.  

The slide summarizes the experimental results, comparing PaLM's performance to specialized SOTA systems and highlighting specific insights from the MQM analysis.

---

### Slide 7: Conclusion

The speaker concludes the presentation by summarizing the key findings and thanking the audience. The final slide displays a word cloud with various translations of "thank you" in different languages, symbolizing the theme of multilingual communication.</sample>
    <sample id="157">The presentation focuses on "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a research project by Shen Gao from Shandong University. It introduces a framework for summarizing dialogue contexts, particularly in scenarios involving multiple speakers and complex interactions. The method uses a static-dynamic graph-based approach to capture both the static relationships between utterances (e.g., who is speaking to whom) and dynamic changes over time. The static graph is constructed using discourse parsing tools to identify dependencies and co-occurrences of keywords, while the dynamic graph captures real-time interactions through sliding windows. The system then integrates these graphs into a unified representation to generate summaries that reflect the core information and context of the dialogue. The final summary is generated using a neural network model that incorporates this graph representation, ensuring that the output captures the essential dialogue structure. The project's goal is to enhance the efficiency and accuracy of summarizing conversational data, making it useful for applications like chatbot responses or meeting notes.</sample>
    <sample id="158">The video presentation discusses advancements in neural coreference resolution, specifically focusing on the "Dual Cache for Long Document Neural Coreference Resolution." The speaker introduces the concept of coreference resolution, which involves identifying and linking mentions within a text that refer to the same entity or concept. An example sentence is provided: "I saw John talking to Maria. He was explaining something to her," where "John" and "He" are linked as referring to the same person.

The presentation highlights the challenges posed by conventional approaches to coreference resolution for long documents, which involve enumerating all possible pairs of mentions, leading to quadratic complexity in computation and memory consumption. This results in an O(|D|^2) complexity, where |D| represents the length of the document.

To address these challenges, cache-based models are introduced, which use a fixed-size cache to store entity representations, reducing complexity to a linear level, O(|C||D|), with |C| being much smaller than |D|. However, this approach faces issues when dealing with topic switching in long documents, as entities can be scattered across a wide range of text, leading to high cache miss ratios.

The dual cache approach is then proposed, which uses two caches: an L-cache for local entities with an LRU (Least Recently Used) policy and a G-cache for global entities with an LFU (Least Frequently Used) policy. This method aims to improve efficiency by managing local and global entities separately, thereby reducing cache misses and improving performance.

The presentation concludes with experimental results on public benchmarks, showing that the dual cache outperforms single cache methods in terms of F1 score and computational cost. The dual cache also significantly reduces cache misses compared to single cache methods. The conclusion emphasizes the effectiveness of the dual cache approach in handling long documents efficiently while maintaining high performance.</sample>
    <sample id="160">The first step maps the input tokens to multiset tags.</sample>
    <sample id="161">55,000 scripts</sample>
    <sample id="162">### Slide 1: The KITMUS Test - Evaluating Knowledge Integration from Multiple Sources

**Slide Content:**
- **Title:** The KITMUS Test
- **Subtitle:** Evaluating Knowledge Integration from Multiple Sources
- **Logos:** McGill, Mila, Microsoft Research
- **Authors:** Akshatha Arodi*, Martin Pömsi*, Kaheer Suleman, Adam Trischler, Alexandra Olteanu, Jackie CK Cheung
- **Equal Contribution**

---

### Slide 2: NLU Models Draw on Multiple Knowledge Sources

**Slide Content:**
- **Title:** NLU models draw on multiple knowledge sources
- **Diagram:**
  - **Left:** "Knowledge in Parameters (pretrain-time knowledge)" with a neural network diagram.
  - **Right:** "Knowledge in Context (inference-time knowledge)" with a text snippet.
- **NLU Model**

---

### Slide 3: John Saw the Newly Elected President on TV

**Slide Content:**
- **Title:** John saw the newly elected president on TV
- **Text:**
  - "What presidents do" (✓)
  - "What is a TV" (✓)
  - "Who is John" (✗)
  - "Who is the new president" (✗)
- **Diagram:**
  - A person sitting in front of a TV with a neural network diagram on the left and a text snippet on the right.

---

### Slide 4: KITMUS Test Suite

**Slide Content:**
- **Title:** KITMUS Test Suite
- **Points:**
  - Dataset for knowledge integration evaluation
  - Coreference resolution task to probe ability to draw on:
    - Pretrain-time knowledge
    - Inference-time knowledge
  - Experiment with:
    - Human study participants
    - Coreference resolution models

---

### Slide 5: Example Sentence from KITMUS Test Suite

**Slide Content:**
- **Title:** KITMUS Test Suite
- **Text:**
  - "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"
- **Highlighted Entities:**
  - "Servin" (judge)
  - "Kea" (baker)

---

### Slide 6: Entity-Specific Knowledge

**Slide Content:**
- **Title:** KITMUS Test Suite
- **Text:**
  - "Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]"
- **Highlighted Entities:**
  - "Servin" (judge)
  - "Kea" (baker)
- **Background Knowledge:**
  - "Judges decide cases in courts of law."

---

### Slide 7: Variants of KITMUS

**Slide Content:**
- **Title:** Variants of KITMUS
- **Diagrams:**
  - (a) Background-Pretrain: Typical setup
  - (b) Background-Both: Explicitly provide background knowledge in context
  - (c) Background-Inference: Knowledge only available at inference-time
- **Text:**
  - (a) Background-Pretrain: Typical setup
  - (b) Background-Both: Explicitly provide background knowledge in context
  - (c) Background-Inference: Knowledge only available at inference-time

---

### Slide 8: Background-Pretrain

**Slide Content:**
- **Title:** Background-Pretrain
- **Bar Chart:**
  - "Without task-specific training"
  - "With task-specific training"
- **Legend:**
  - Random Choice
  - Human Participants
  - BERT4Coref
  - C2P

---

### Slide 9: Background-Both

**Slide Content:**
- **Title:** Background-Both
- **Bar Chart:**
  - "Without task-specific training"
  - "With task-specific training"
- **Legend:**
  - Random Choice
  - Human Participants
  - BERT4Coref
  - C2P

---

### Slide 10: Background-Inference

**Slide Content:**
- **Title:** Background-Inference
- **Bar Chart:**
  - "Without task-specific training"
  - "With task-specific training"
- **Legend:**
  - Random Choice
  - Human Participants
  - BERT4Coref
  - C2P

---

### Slide 11: Main Takeaways

**Slide Content:**
- **Title:** Conclusion
- **Main Takeaways:**
  1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge).
  2. Task-specific training is necessary for knowledge integration.
  3. Models struggle to integrate inference-time background knowledge.
- **GitHub Link:** Find the dataset, generation &amp; evaluation code at [mpoems/kitmus]

---</sample>
    <sample id="163">The best alignment method for DEplain is not explicitly stated in the provided text, but it can be inferred that the method with the highest F1 score would be considered the best.</sample>
    <sample id="164">Weakly supervised learning alleviates the annotation bottleneck by training models that generalize well despite being trained on noisy data.</sample>
    <sample id="165">The video discusses abductive commonsense reasoning, focusing on exploiting mutually exclusive explanations in natural language processing tasks. It introduces the concept of abductive reasoning, where given a context and an outcome, plausible explanations are sought to explain the outcome. The video explains how this process can be applied to scenarios like Emily being stuck in traffic but still making it to her flight, with possible explanations such as her flight being delayed or leaving on time. The challenge of annotating plausible explanations is highlighted due to its subjective nature, leading to disagreements among crowd workers. To address this, the video presents LiPoR (Likelihood learning with Posterior Regularization), a method that learns abductive reasoning without supervision over plausible explanations. This approach treats explanations as latent variables and maximizes the log likelihood of outcomes given contexts by marginalizing out these explanations. The LiPoR objective encourages the probability mass of p(z|x,y) to collapse to a subset of explanations, thereby improving the model's ability to reason about plausible explanations.</sample>
    <sample id="166">The speaker introduces a neural divide-and-conquer reasoning framework designed for image retrieval from linguistically complex text. This framework is inspired by human cognitive processes and aims to solve large problems by breaking them into smaller, solvable sub-problems. The presentation outlines two key systems: System 1, which excels in analogical reasoning but struggles with logical reasoning, and System 2, adept at logical reasoning but less effective in analogical tasks. The speaker explains that combining these systems through logical operations can enhance overall performance. The framework includes a proposition generator using a pre-trained language model and a visual-linguistic interactor that integrates visual and linguistic information. The final system integrates the outputs of both components to achieve better results than either alone. The speaker concludes by discussing the potential integration of dual-process theory with the divide-and-conquer approach, suggesting it could improve the compositional reasoning and planning capacity of large language models.</sample>
    <sample id="167">The allocation of the documents in DEplain-web involved both manual and automatic alignment methods, but the specific details about the exact allocation were not provided in the given text.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating it with CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">The presentation focuses on evaluating the performance of PaLM (Pathways Language Model) in translation tasks, specifically through systematic prompting strategies. It highlights that PaLM, with its vast parameter count and dense activation, excels in various language understanding and generation benchmarks. The study aims to assess how different prompt selection strategies impact translation quality, using BLEURT as a metric. Key findings include the significant influence of prompts on translation accuracy, with some sentences showing differences of over 40 BLEURT points between prompts. The research also compares PaLM's performance against specialized SOTA systems, noting that while PaLM is close to Google Translate in fluency, it generally scores lower in accuracy due to issues like omission and awkwardness. The presentation concludes by emphasizing the importance of high-quality examples for effective translation and suggests areas for future improvement in PaLM's translation capabilities.</sample>
    <sample id="171">The existing works include Parameter-based watermark, Lexical watermark, Backdoor-based watermark, and Adversarial-based watermark.</sample>
    <sample id="172">No, multilingual LLMs such as Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="173">**Slide 1: Title Slide**
- **Title:** Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
- **Authors:** Shuheng Liu, Alan Ritter
- **Affiliation:** School of Interactive Computing, Georgia Institute of Technology

**Slide 2: Introduction to Named Entity Recognition &amp; Generalization**
- **Topic:** Named Entity Recognition &amp; Generalization
- **Content:** Models have been using CoNLL-2003 to develop NER for almost 20 years.
- **Questions:** Can these models generalize to modern data? What is needed for good generalization?

**Slide 3: CoNLL++ Dataset**
- **Topic:** CoNLL++ Dataset
- **Content:** Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines.
- **Evaluation:** Evaluated on CoNLL-2003 test set &amp; CoNLL++
- **Performance Metric:** Calculated percentage ΔF1 to assess generalization.

**Slide 4: What Is Needed for Good Generalization?**
- **Topic:** What Is Needed for Good Generalization?
- **Content:** Model architecture (Transformer models generalize better), Model size (Larger models generalize better), Number of fine-tuning examples (More examples leads to better generalization).

**Slide 5: Performance Drop Analysis**
- **Topic:** What Causes Performance Drop?
- **Content:** Adaptive overfitting? Temporal drift?

**Slide 6: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Adaptive overfitting? No diminishing returns, Not observed. Temporal drift? Performance degrades with larger temporal gap.

**Slide 7: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 8: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 9: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 10: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 11: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 12: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 13: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 14: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 15: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 16: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 17: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 18: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 19: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 20: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 21: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 22: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 23: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 24: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 25: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 26: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 27: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 28: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 29: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 30: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 31: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 32: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 33: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 34: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 35: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 36: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 37: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 38: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 39: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 40: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 41: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 42: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 43: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 44: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 45: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 46: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 47: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 48: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 49: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 50: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 51: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 52: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 53: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 54: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 55: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 56: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 57: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 58: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 59: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 60: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 61: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 62: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 63: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 64: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 65: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 66: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 67: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 68: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 69: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 70: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 71: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 72: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 73: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 74: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 75: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 76: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 77: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 78: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 79: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 80: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 81: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 82: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 83: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 84: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 85: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 86: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 87: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 88: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 89: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 90: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 91: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 92: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 93: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 94: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 95: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 96: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 97: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 98: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 99: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 100: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 101: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 102: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 103: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 104: Performance Drop Analysis Continued**
- **Topic:** What Causes Performance Drop?
- **Content:** Performance degrades with larger temporal gap, Main cause for performance drop.

**Slide 105: Performance</sample>
    <sample id="174">The video presentation focuses on the "ArgAnalysis35K" dataset, designed for argument quality analysis. It begins by defining Argument Quality Analysis as a method to evaluate arguments on a scale from 0-1, emphasizing the need for high-quality and diverse arguments sourced directly from winning debates and expert debaters. The dataset is highlighted for its comprehensive nature, containing over 35,000 argument-analysis pairs with diverse themes such as politics, environment, and authoritarian regimes. The presentation discusses the challenges faced by current datasets, including lack of quality arguments, diversity, and depth in explaining nuances, and introduces ArgAnalysis35K as a solution. It outlines the dataset's features: a large-scale collection of high-quality arguments, diverse motions, logical chains of reasoning, and an instance-based annotation scoring function. The relevance model assigns scores from 0-1 for each argument-analysis pair, enhancing the dataset's utility in various contexts like free speech debates and LGBTQ rights. The video also addresses annotator reliability issues, suggesting the use of machine learning models like Expectation Maximisation training and FNN classifiers to predict true values of annotations, thereby mitigating human biases. Overall, ArgAnalysis35K aims to provide a robust tool for improving argument quality assessment across different domains.</sample>
    <sample id="175">The method deals with permutation ambiguity by inducing it during training, which is a key challenge in the permutation model.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined as the absence of political biases in its predictions.</sample>
    <sample id="177">The name of the speaker is Yanis Labrak.</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">The video presentation discusses the concept of Theory of Mind (ToM), focusing on how it is traditionally measured through reading comprehension tasks involving multiple characters, particularly using false-belief questions. The Sally-Anne Test is introduced as an example to illustrate this method, where participants must infer the mental states of characters based on their actions and beliefs. The presentation highlights that large language models (LLMs) often struggle with these tasks due to their lack of understanding of complex mental states.

To address this limitation, the video introduces SymbolicToM, a plug-and-play method designed to improve LLMs' ToM reasoning skills. SymbolicToM uses explicit graphical symbolic representations to track local context and perform inference-time reasoning, avoiding overfitting risk. The method leverages off-the-shelf natural language inference (NLI) and OpenIE models to compute belief graphs for all possible combinations of characters up to a predefined depth, enhancing the model's ability to reason about the mental states of others.

The presentation also outlines experiments conducted to evaluate SymbolicToM's performance against various baselines, including supervised models like Textual Time Travel and fine-tuned GPT3 Curie. Results show significant improvements in in-domain performance, with SymbolicToM outperforming other methods across different datasets and models. The study further demonstrates the method's effectiveness in out-of-domain generalization, suggesting its potential for broader applications in understanding complex narratives and mental states beyond the original training data.

Overall, the video provides a comprehensive overview of the challenges in measuring ToM in LLMs, the introduction of SymbolicToM as a solution, and the promising results from experimental evaluations, highlighting the method's potential to enhance the interpretability and robustness of LLMs in understanding human-like mental states.</sample>
    <sample id="180">Myra Cheng</sample>
    <sample id="181">The research presented at the 61st Annual Meeting of the Association for Computational Linguistics focuses on enhancing constrained language planning capabilities in large language models (LLMs) through a novel method called "Script Distillation from LLMs." The study aims to address the limitations of current LLMs in generating scripts that adhere to specific constraints, such as those found in real-life scenarios like making a cake with additional requirements like using a microwave or for a wedding.

The researchers propose a method that involves three main steps: 
1. Generating specific goals from an abstract goal using InstructGPT via in-context learning.
2. Over-generating candidate scripts with InstructGPT.
3. Filtering these scripts based on their similarity score to the abstract goal.

This approach is demonstrated through examples involving different types of cakes, showcasing how abstract goals can be decomposed into detailed, specific instructions. The study also evaluates the performance of various LLMs, including GPT-3, Codex, InstructGPT, and T5 trained on Coscript, wikiHow, and Coscript datasets. The results indicate that smaller LMs fine-tuned on Coscript can generate higher quality scripts than larger LMs.

The research further explores the constraints in generated scripts, finding high heterogeneity and pluralism in the specific goals. This dataset, Coscript, is highlighted as valuable for advancing research on language planning with more complex and diverse goals and constraints.

In conclusion, this work establishes the constrained language planning problem, evaluates the constrained language planning ability of LLMs, and develops an over-generate-then-filter method for improving LLMs' performance. The proposed method significantly enhances script generation quality, making it a promising tool for constrained language planning tasks.</sample>
    <sample id="182">Tropicalism indicates a specific cultural identity or group, which is used as an example in the paper to discuss the representation and portrayal of marginalized groups through essentializing narratives.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="184">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="185">DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks, surpasses CamemBERT generic model and English-based domain-specific models, confirms utility of training a medical-specific model in French, while ChuBERT achieves state-of-the-art results on 5 downstream French medical-oriented tasks, surpasses CamemBERT generic model and English-based domain-specific models, and confirms utility of training a medical-specific model in French.</sample>
    <sample id="186">### Slide 1: Title Slide
**Title:** Marked Personas  
**Subtitle:** Using Natural Language Prompts to Measure Stereotypes in Language Models  
**Authors:** Myra Cheng, Esin Durmus, Dan Jurafsky  
**Conference:** ACL 2023  
**Institution:** Stanford University, Computer Science

---

### Slide 2: Motivation
**Slide Content:**
- **Social bias and stereotypes are prevalent in LLMs.**
- **Limitations of existing stereotype measures:**
  - Tradeoff between specificity and generalizability.
  - Based on fixed, hand-curated datasets.
  - Don’t account for intersectionality.

---

### Slide 3: Introduction to Marked Personas
**Slide Content:**
- **Title:** Marked Personas  
- **Subtitle:** Using Natural Language Prompts to Measure Stereotypes in Language Models  
- **Authors:** Myra Cheng, Esin Durmus, Dan Jurafsky  
- **Conference:** ACL 2023  
- **Institution:** Stanford University, Computer Science

---

### Slide 4: Overcoming Limitations
**Slide Content:**
- **How do we overcome these limitations?**
  - GPT-3.5, GPT-4, etc., can respond to instructions in prompts.
  - Input: "Imagine you are an Asian woman. Describe yourself."

---

### Slide 5: Persona Examples (GPT-4)
**Slide Content:**
- **Step 1: Persona Examples (GPT-4)**
  - **Asian woman:**
    - The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time... My petite frame is both elegant and unassuming, allowing me to move gracefully throughout the world, drawing immediate attention without being obtrusive.
  - **Middle-Eastern woman:**
    - She is a vision of Middle-Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond-shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.
  - **White man:**
    - As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I’m not careful with my sunscreen.

---

### Slide 6: Two Steps
**Slide Content:**
- **Step 1: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."**
  - Inspired by psych study with human subjects using the same prompts.

---

### Slide 7: Step 2: Marked Words
**Slide Content:**
- **Step 2: Marked Words**
  - Find words that distinguish personas of marked groups from unmarked groups.
  - Specific without requiring a lexicon.

---

### Slide 8: Insight for Step 2: Marked Words
**Slide Content:**
- **Markedness:**
  - Unmarked groups are default, ordinary.
  - Marked groups differ from the default:
    - A warrior (unmarked) vs. a woman warrior (marked).

---

### Slide 9: Recommendations
**Slide Content:**
- **Recommendations:**
  - Addressing positive stereotypes and essentializing narratives.
  - An intersectional lens.
  - Transparency about bias mitigation.

---

### Slide 10: Results: Comparison to Human Responses
**Slide Content:**
- **Generated personas contain more stereotypes.**
  - Bar chart comparing the percentage of stereotype words in personas generated by humans, GPT-3.5, and GPT-4 for Black and White stereotypes.

---

### Slide 11: Results: Patterns in Top Words
**Slide Content:**
- **Results: Patterns in Top Words**
  - **Othering through essentializing narratives:**
    - Culture, tradition, proud, exotic for marked groups.
    - Defines those groups only by their identity.
  - **Pernicious positive portrayals:**
    - Vibrant, curvaceous for Latina women.
    - Petite, delicate, silly for Asian women.
    - Strong, resilient for Black women.

---

### Slide 12: Recommendations
**Slide Content:**
- **Recommendations:**
  - Addressing positive stereotypes and essentializing narratives.
  - An intersectional lens.
  - Transparency about bias mitigation.

---</sample>
    <sample id="187">Three</sample>
    <sample id="188">Iterative transfer learning is a method where a model is trained on an initial dataset, then fine-tuned on new data, and this process is repeated iteratively to improve performance.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they make a choice, specifically focusing on resolving indirect referring expressions for entity selection.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embeddings provided by the service, which can be used to provide similar services.</sample>
    <sample id="191">Three authors are involved in the paper.</sample>
    <sample id="192">The presentation discusses the development of CAME (Confidence-guided Adaptive Memory Efficient Optimization), an advanced memory-efficient optimizer designed for large language model training tasks. Inspired by existing memory-efficient optimizers like Adam and AdaFactor, CAME introduces a confidence-based strategy to address the issue of erroneous updates in deep neural networks. The method leverages non-negative matrix factorization (NMF) to reduce memory usage while maintaining fast convergence rates. The introduction highlights the challenge of balancing memory efficiency with performance in training large models. The preliminaries section explains NMF and its application in optimizing memory usage. The method section details how CAME mitigates erroneous updates through a confidence-guided strategy, ensuring stable optimization. Extensive experiments on BERT training demonstrate that CAME outperforms traditional methods, achieving outstanding performance across various datasets. The conclusion emphasizes CAME's effectiveness in large batch training and its role as an important extension to existing memory-efficient optimizers.</sample>
    <sample id="193">30 annotators were used to create the initial dataset.</sample>
    <sample id="194">The affiliations of the authors include the University of Washington, Carnegie Mellon University, Allen Institute for AI, and Masakhane initiative.</sample>
    <sample id="195">The presentation discusses the challenges and advancements in Explainable Question Answering (XQA), focusing on the limitations of existing methods that can only execute on structured knowledge bases, such as SPARQL, KoPL, and Lambda-DCS. These methods struggle with free-text corpora due to the diversity of natural language, making XQA difficult. The study introduces a novel approach called Reasoning over Hierarchical Question Decomposition Tree (RoHT), which addresses these limitations by decomposing complex questions into sub-questions and utilizing a hierarchical question decomposition tree (HQDT). This framework allows for probabilistic reasoning over HQDTs, integrating knowledge from heterogeneous sources like KBs and text corpora to provide more accurate and explainable answers.

The RoHT framework consists of two main components: understanding the complex question and probabilistic reasoning over HQDTs. Understanding involves building an HQDT using a BART-based question decomposer and generator, while probabilistic reasoning outputs answers with probabilities for each question in the HQDT. The scheduler determines appropriate knowledge sources for each node, and the executor retrieves answers with probabilities from corresponding knowledge sources. The aggregator then combines candidate answers from all knowledge sources to output the best response.

The experimental setting includes datasets like KQA Pro and Musique, and models such as BART-KoPL semantic parser and RoBERTa text filter. The results show that RoHT outperforms baseline models across various metrics, demonstrating its effectiveness in handling complex questions and providing explainable answers.</sample>
    <sample id="196">I saw Bart and Lisa; Homer came and sneezed.</sample>
    <sample id="197">The state-of-the-art models in dialogue systems include BART-FID-RAG, Blender2, Emora, and Blender Decode.</sample>
    <sample id="198">To ensure that the models maintain consistent judgments across different parts of the context and not just at the beginning or end.</sample>
    <sample id="199">No, training in multilingual fashion did not cause a performance drop; it improved performance for most NLs.</sample>
    <sample id="200">No, the annotators do not know about the entity in advance.</sample>
    <sample id="201">SOTA MT metrics</sample>
    <sample id="202">Yes, the regress in generalization impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it influences how datasets and models interpret and represent information, which can lead to biases that reflect the perspectives of their creators. This can result in unfair or inaccurate outcomes for certain groups, highlighting the need for more inclusive and diverse approaches in NLP research and development.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">The presentation titled "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" explores the impact of political biases in pretraining data on language models and their subsequent performance in downstream tasks. The speaker, Shangbin Feng, discusses how the diverse sources of pretraining data, such as news media and social media, can introduce political leanings into language models. This bias is then reflected in the models' responses to political statements, leading to unfair outcomes in applications like hate speech detection or misinformation identification.

Key points include:
1. **Pretraining Data Composition**: The presentation highlights that pretraining data often comes from biased sources, such as news media and social media platforms, which can skew the political leanings of language models.
2. **Evaluation Methods**: Various methods are discussed for evaluating the political leaning of language models, including automatic evaluations grounded in political literature and qualitative analysis of model outputs.
3. **Impact on Downstream Tasks**: The presentation shows how these biases manifest in downstream tasks, particularly in hate speech detection and misinformation detection, where models may perform differently based on the political orientation of the input text.
4. **Case Study**: A case study involving the "Trump Card" is presented, demonstrating how political leanings in pretraining data can lead to significant shifts in model performance across different categories.

Overall, the presentation underscores the critical need for understanding and mitigating political biases in pretraining data to ensure fair and unbiased performance of language models in real-world applications.</sample>
    <sample id="206">RoBERTA-base + classifier head</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities include WMT 2021 and WMT 2022.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 36%.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Yes, the results and dataset can serve as a benchmark for future research in text simplification.</sample>
    <sample id="212">They experiment with five smaller models in the paper.</sample>
    <sample id="213">OFA</sample>
    <sample id="214">### Slide 1: Title Slide
**Title:** Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark  
**Authors:** Wenjun Peng, Jingwei Yi*, Fangzhao Wu*, Shangxi Wu*, Bin Zhu*, Lingjuan Lyu*, Binxing Jiao*, Tong Xu*, Guanghong Sun*, Xing Xie*  
**Affiliations:** University of Science and Technology of China, Microsoft Research Asia, Beijing Institute of Technology, Sony AI, Microsoft STC Asia

---

### Slide 2: Background
- **Large language models (LLMs) are exceptional in NLU and NLG**
  - GPT [1], LLaMA [2], PALM [3]
- **Embedding as a Service (EaaS) is offered to assist various NLP tasks**
- **OpenAI offers a GPT-based embedding API**

---

### Slide 3: Background
- **Ada model, text-embedding-adabase, is a better and lower cost replacement for our older embedding models.**
- **Performance comparison with other models:**
  - GPT-3: 93.76 ± 0.19
  - RedAlarm: 93.76 ± 0.19
  - EmbMarker: 94.87 ± 0.27

---

### Slide 4: Motivation
- **Attackers may steal the model through learning from the embeddings and provide similar services**
  - StolenEncoder [1]
- **Need to protect the copyright of EaaS**
- **Detect whether a provider's service is stolen by another service**

---

### Slide 5: Challenge
- **Applicable to EaaS**
- **Utility**
  - Should not degrade the utility of the provided embeddings.
- **Covertness**
  - Should be covert to the attacker.
- **Transferability**
  - The watermark needs to be transferable to the attackers' services.

---

### Slide 6: Existing Works
- **Parameter-based watermark [1, 2]**
  - Transferability: ❌
- **Lexical watermark [3, 4]**
  - Applicable to EaaS: ❌
- **Backdoor-based watermark [5]**
  - Applicable to EaaS: ❌
- **Adversarial-based watermark [6]**
  - Applicable to EaaS: ❌

---

### Slide 7: EmbMarker
- **Trigger Selection**
  - Count the word frequency on a general text corpus \( D_p \)
  - Randomly select \( n \) words in a moderate-frequency interval

---

### Slide 8: EmbMarker
- **Watermark injection**
  - Define a target embedding \( \mathbf{e}_t \)
  - Count the trigger number in a sentence \( Q(S) = \frac{\min(|S \cap T|, m)}{m} \)
  - Add the target embedding on the original embedding \( \mathbf{e}_o \)

---

### Slide 9: EmbMarker
- **Copyright verification**
  - Construct a backdoor and benign dataset
    - \( D_b = \{[w_1, w_2, ..., w_m] | w_i \in T\} \)
    - \( D_n = \{[w_1, w_2, ..., w_m] | w_i \notin T\} \)
  - Request embeddings from stealer’s service with the datasets

---

### Slide 10: Experimental Results
- **Copy Dataset:** AG News, MIND, SST2, Enron Spam
- **Provider’s general Dataset:** WikiText
- **Metrics:**
  - Performance on downstream tasks: ACC
  - Detection performance: \( \Delta_{cos}, \Delta_{L2}, p\)-value
- **Setting:**
  - \( m = 20 \), \( n = 4 \), frequency interval = [0.005, 0.01]

---

### Slide 11: Experimental Results
- **Performance comparison**
  - SST2: Original 93.76 ± 0.19, RedAlarm 93.76 ± 0.19, EmbMarker 94.78 ± 0.27
  - MIND: Original 93.76 ± 0.19, RedAlarm 93.76 ± 0.19, EmbMarker 94.87 ± 0.27
  - AGNews: Original 93.74 ± 0.14, RedAlarm 93.74 ± 0.14, EmbMarker 94.78 ± 0.27
  - Enron Spam: Original 93.70 ± 0.08, RedAlarm 93.70 ± 0.08, EmbMarker 94.87 ± 0.27

---

### Slide 12: Experimental Results
- **Embedding visualization**
  - AG News
  - Enron Spam
  - MIND
  - SST2

---

### Slide 13: Thanks!
- **Thank you!**</sample>
    <sample id="215">The presentation focuses on the study of conjunct lengths in English, specifically examining dependency length minimization (DLM) and the dependency structure of coordination. It discusses various dependency structures such as Bouquet/Stanford, Chain/Moscow, Conjunction-headed/Prague, and Multi-headed/London, using examples like "Homer loves Lisa, Bart, and Maggie." The speaker highlights how word order influences dependency lengths, with shorter left conjuncts observed when the governor is on the left or absent. The study also explores the relationship between conjunct lengths and the governor's position, noting that left conjuncts tend to be shorter but this tendency grows with length difference. The presentation includes visual aids like graphs showing proportions of shorter left conjuncts based on the absolute difference in conjunct lengths. It concludes by encouraging viewers to refer to the full paper for detailed arguments and to discuss findings at the poster session.</sample>
    <sample id="216">### Slide 1: Title Slide
**Title:** Attention as a Guide for Simultaneous Speech Translation  
**Authors:** Sara Papi, Matteo Negri, Marco Turchi  
**Institutions:** Università di Trento, Fondazione Bruno Kessler

---

### Slide 2: Introduction to Simultaneous Speech Translation (SimuST)
**Slide Content:**
- **Definition:** Simultaneous speech translation (SimuST) is the process of translating spoken language into a text in another language in real-time, enabling cross-language communication.
- **Example:** "Wenn ich im Sommer kalten Tee in meine Thermoskanne gieße, bleibt er kalt, und wenn ich im Winter heißen Tee in meine Thermoskanne gieße, wird er warm." → "When I have cold tea in my thermos in the summer, it stays cold, and when I pour hot tea in my thermos in the winter, it gets warm."

---

### Slide 3: Problems with Current SimuST Models
**Slide Content:**
- **Specific Architectures:** Specific architectures are usually trained, introducing additional modules to be optimized.
- **Training Procedures:** Long and complicated training procedures (e.g., different optimization objectives).
- **Latency Management:** Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, 25s).

---

### Slide 4: Solution Overview
**Slide Content:**
- **Use Existing Offline ST Models:** Use already existing offline ST models without re-training or adopting specific architecture for SimuST.
- **Single Model per Latency Regime:** Use only one model for every latency regime and handle latency through specific parameters.
- **Leverage Attention Mechanism:** Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output.

---

### Slide 5: Encoder-Decoder Attention
**Slide Content:**
- **EDAtt (Encoder-Decoder Attention):** Decide whether to emit or not a partial translation based on where attention points:
  - A word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last A token, meaning that the information is enough stable.

---

### Slide 6: Example of EDAtt in Action
**Slide Content:**
- **Audio Input:** "I am going to talk about..."
- **Textual Output:** "Ich werde reden."
- **Attention Mechanism Visualization:** Green arrows indicate attention points, and red lines show the decision boundary (threshold α). Words like "am" and "to" are emitted because their attention sums are below the threshold, while "going" and "talk" are not.

---

### Slide 7: Main Results: EDAtt Performance
**Slide Content:**
- **BLU (Bilingual Language Understanding Evaluation):** Shows the performance of EDAtt across different latency regimes (AL/AL_CA in seconds).
- **EDAtt Outperforms Others:** EDAtt outperforms all strategies applied to offline models.
- **Fastest Strategy:** EDAtt is the fastest strategy if we consider the actual elapsed time.

---

### Slide 8: Conclusion and Next Steps
**Slide Content:**
- **QR Code:** Encourages viewers to scan the QR code for more results.
- **Contact Information:** Provides contact details for further inquiries.
- **GitHub Repository:** Links to the GitHub repository for the paper and code.

---

### Summary
The presentation introduces the concept of simultaneous speech translation (SimuST) and discusses the challenges faced by current models. It then presents a novel solution called EDAtt, which leverages existing offline ST models and uses an encoder-decoder attention mechanism to handle latency effectively. The results demonstrate that EDAtt outperforms other strategies and is the fastest in terms of actual elapsed time. The final slides encourage viewers to explore the research further through the provided resources.</sample>
    <sample id="217">The research presented in this study focuses on compositional generalization for multi-attribute controllable dialogue generation, addressing the limitations of previous methods that focused solely on single attributes and lacked practicality for continuous attributes. The authors propose DCG (Disentangled Controllable Generation), a model that learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. They also introduce a unified reference-free evaluation framework, MAE (Multi-Attribute Evaluation), which evaluates the effectiveness of their method across various granularities of attributes. The methodology includes an attribute-oriented prompt, task-oriented prompt, and disentanglement learning. The study demonstrates that DCG outperforms existing models in terms of text quality and controllability scores, achieving better results than baseline methods like DialoGPT-De and FLURGE. Additionally, the proposed MAE framework shows higher correlation with human judgments for evaluation on CDG.</sample>
    <sample id="218">The affiliations of the authors of the paper are Google.</sample>
    <sample id="219">The presentation discusses a compare-and-contrast multistage pipeline designed to uncover financial signals in financial reports, addressing the challenges posed by the abundance and complexity of financial documents. It introduces a highlighting task that aims to identify important words or phrases within these documents, which are crucial for understanding financial operations. The pipeline is structured into several stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. Each stage is illustrated with detailed slides, including tables and charts that demonstrate the categorization of reference-to-target segment pairs and the evaluation metrics used to assess the performance of the models. The study emphasizes the importance of financial corpus analysis and the potential for domain-adaptive learning to enhance the effectiveness of financial signal highlighting tasks.</sample>
    <sample id="220">Stony Brook University, Human Language Analysis Settings</sample>
    <sample id="221">The paper analyzed translation from German to English, French, Spanish, Italian, and Portuguese.</sample>
    <sample id="222">The presentation focuses on open-domain question answering (QA) and explores challenges in adapting retrieval models to new domains. It discusses how existing models like Wikipedia can be insufficient for domains such as biomedical questions due to sparse data availability. The speaker proposes expanding the retrieval corpus by combining sources like PubMed with Wikipedia. Key contributions include investigating data interventions to enable out-of-domain generalization, understanding the compatibility of source models for target domains, and determining the relationship between data interventions and their effectiveness under specific dataset shifts. The research also examines adaptation methods like few-shot and zero-shot learning, aiming to improve reader performance by up to 24% and retriever performance by 22% in F1 score.</sample>
    <sample id="223">Shangbin Feng</sample>
    <sample id="224">The models investigated during the experiments were UDA, Sent-Label, and VecAlign.</sample>
    <sample id="225">5 expert-written instructions are used for both training and testing purposes.</sample>
    <sample id="226">Three authors are involved in the paper.</sample>
    <sample id="227">The video discusses the concept of grounded language understanding, emphasizing its importance in various applications such as voice assistants, search engines, and embodied AI systems. It highlights that current language models, trained on large textual corpora like Wikipedia and BookCorpus, lack the ability to understand and generate plans or programs. The speaker introduces the Pangu framework, which aims to address this limitation by allowing language models to focus on discrimination tasks while being generic and applicable across different environments. The framework involves an agent interacting with the environment to propose valid candidate plans, which are then scored by a language model. This approach is demonstrated through examples involving SQL parsing, knowledge base question answering (KBQA), and embodied AI, showcasing how Pangu outperforms existing methods in terms of performance and sample efficiency. The video concludes by presenting empirical results that validate the effectiveness of the Pangu framework in improving generalization and reducing overfitting during training.</sample>
    <sample id="228">The authors experimented on the AG News, MIND, SST2, and Enron Spam datasets.</sample>
    <sample id="229">The video presentation focuses on the importance of text revision in argumentative writing, emphasizing its role as an essential part of the process and its recursive nature until optimal phrasing is achieved. The persuasive impact of phrasing directly influences how the audience perceives the argument. The speaker discusses two main tasks: suboptimal-claim detection and claim improvement suggestion. Suboptimal-claim detection involves deciding whether a given claim needs revisions or can be considered to be phrased optimally. Claim improvement suggestion requires selecting the types of quality issues that should be improved when revising the claim.

The presentation highlights the challenges involved in these tasks, such as representativity and reliability, model complexity and architecture, and topical and user bias. It also mentions the use of revision-based data for effective task completion and the modeling of the distance between two claim versions for suboptimal-claim detection. The impact of contextual information is noted as being task- and quality issue-dependent. The speaker concludes by summarizing the findings from the paper, which include the effectiveness of revision-based data and the benefits of modeling claim version distances. The code and data for the research are available at a provided GitHub link.</sample>
    <sample id="230">**Slide Title:** Language model acceptability judgements are not always robust to context

**Content:**
- **Title:** Language model acceptability judgements are not always robust to context
- **Conference:** ACL 2023
- **Authors:** Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Karen Fuentes, Roger Levy, Adina Williams
- **Image:** A circular profile picture of a person wearing glasses and a maroon shirt.
- **Logos:** Johns Hopkins University, Purdue University, MIT, Meta AI.

---

**Slide Title:** Revisiting Minimal Pair Paradigm

**Content:**
- **Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.**
- **Three systems compared:**
  - **BLIMP:** 
    - Sentence 1: "Many people were helping themselves."
    - Sentence 2: "Many people were helping herself."
    - Judgement: P(1) &gt; P(2)
  - **SyntaxGym:**
    - Sentence 1: "The customer ... has spent any money."
    - Sentence 2: "The customer ... has spent any money."
    - Judgement: P(1.any) ≻ P(2.any)
  - **CrowS:**
    - Sentence 1: "Women are terrible at handwork."
    - Sentence 2: "Men are terrible at handwork."
    - Judgement: P(1) ≻ P(2)
- **Question:** Are these judgements stable with long preceding context?

---

**Slide Title:** Approach

**Content:**
- **Test whether MPP judgements vary as a function of context length, structural match, and acceptability.**
- **Test Bulle: Subject Verb Agreement**
  - **Prefix Strategy:**
    - Prefix: &lt;sent&gt;.
    - Space of Candidate Profiles: Matched
  - **Test Bulle: Subject Verb Agreement**
    - Prefix: &lt;sent&gt;.
    - Space of Candidate Profiles: Mismatched
- **Examples:**
  - **Matched:**
    - "Why might Rose flee from before returning to this customer?"
    - "Why might Rose flee from before returning to this customer?"
  - **Mismatched:**
    - "What could Jessie see before noticing those songs? Why might Rose flee from before returning to this customer before returning to?"
    - "What could Jessie see before noticing those songs? Why might Rose flee from before returning to this customer before returning to?"
  - **Unmatched:**
    - "There was each documentary about music imitating birds. Who might Rose flee from before returning to this customer before returning to?"
    - "There was each documentary about music imitating birds. Who might Rose flee from before returning to this customer before returning to?"
  - **Wikipedia: Unrelated**
    - "A rose is a widely perennial flowering plant of the genus Rosa that is in the family Rosaceae. There are tens of thousands of cultivars."
    - "A rose is a widely perennial flowering plant of the genus Rosa that is in the family Rosaceae. There are tens of thousands of cultivars."

---

**Slide Title:** MPP judgements are robust for arbitrary context lengths

**Content:**
- **We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens.**
- **Perturbation:**
  - None
  - One/Successful ads
  - Long prefix/ad
  - Add clause
  - Quote
  - All
- **Prefix Type:**
  - Acceptable
  - Unacceptable
- **Key Takeaways:**
  - Language models are sensitive to latent syntactic/semantic features shared across sentences.
  - MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="231">A 1,18 words open-source dataset of heterogeneous data crawled from diverse medical domains, nature and style.</sample>
    <sample id="232">The name of the speaker is Markus Freitag.</sample>
    <sample id="233">The presentation focuses on simultaneous speech translation (SimuST), highlighting its importance in enabling real-time cross-language communication. It introduces the concept of SimuST as translating spoken language into text in another language instantly. The speaker discusses the challenges faced by current SimuST models, such as the need for specific architectures and long training procedures, which can be complex and time-consuming. To address these issues, the solution proposed is EDAtt, an encoder-decoder attention mechanism that leverages existing offline ST models without retraining or adapting them specifically for SimuST. This approach uses only one model per latency regime and handles latency through specific parameters, optimizing the translation process. The presentation also delves into the technical details of EDAtt, explaining how it decides whether to emit or emit a partial translation based on attention points, ensuring that the translation is stable and accurate. The results show that EDAtt outperforms other strategies applied to offline models, making it a promising solution for improving the efficiency and quality of SimuST systems.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, with differences in BLEURT points ranging from 1 to 40.</sample>
    <sample id="235">The affiliations of the authors of the paper are Carnegie Mellon University, Language Technologies Institute, TECNICO LISBOA, BAIR, and Unbabel.</sample>
    <sample id="236">The 5 expert-written instructions are:
1. Visual Relationship
2. VQA
3. Temporal Ordering
4. Grounded Reasoning
5. Image Understanding</sample>
    <sample id="237">The authors propose using a coreference resolution task to test the models on information from multiple sources.</sample>
    <sample id="238">The presentation introduces MeetingBank, a benchmark dataset for meeting summarization created by segmenting city council meetings and pairing them with expert-written summaries. The dataset addresses challenges such as scarcity of high-quality meeting summaries and difficulty in identifying reliable sources for public meetings. It includes transcriptions, summaries, and URLs from various cities like Denver, Seattle, Long Beach, Alameda, San Diego, Boston, and New York. The dataset is designed to be valuable for researchers developing advanced meeting summarizers, providing insights into decision-making processes within city councils. The evaluation section compares extractive models (Extract-Oracle, Lead-3, LexRank, TexRank) and abstractive models (BART_Large, Pegasus, Longformer, DialogLM, HMNet), with GPT-3 performing poorly according to automatic metrics. Human evaluation criteria include informativeness, factuality, fluency, coherence, redundancy, and readability, with an average score of 4.21 across all models. The dataset is available on GitHub, offering a valuable resource for advancing research in meeting summarization.</sample>
    <sample id="241">The presentation discusses the challenges in misinformation detection, particularly focusing on the COVID-19 pandemic. It highlights that current approaches are often unrealistic and not human-centric, as they evaluate systems using unrealistic datasets and fail to account for real-world scale or noise. The speaker introduces their approach, Human-in-the-loop (HiTL), which integrates human feedback into an end-to-end misinformation detection system from tweets to actionable outputs. This framework is demonstrated through a concrete implementation evaluating COVID-19 treatment misinformation on Twitter. The HiTL system identifies misleading claims and policy violations, providing a list of tweets supporting unapproved treatments. The evaluation efficacy is based on the ability to detect misleading claims early, defined by the relative time to the first appearance of the claim in a debunking news article. The study also evaluates the system's performance in detecting policy violations, finding that 65% of identified tweets violate Twitter’s policies, with 124.2 tweets detected per human hour worked.</sample>
    <sample id="242">Likert rating evaluation, comparative evaluation, and ABC-Eval.</sample>
    <sample id="243">5</sample>
    <sample id="244">The background knowledge needed includes that "Judges decide cases in courts of law."</sample>
    <sample id="245">The presentation titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" by Lining Zhang and colleagues explores the challenges and best practices for identifying high-quality annotators on the Amazon Mechanical Turk (MTurk) platform. The study highlights that automatic metrics often fail to accurately predict human performance, emphasizing the need for more nuanced approaches.

The researchers propose a two-step pipeline for finding high-agreement workers: a qualification task followed by an endurance task. The qualification task involves a pre-defined set of settings, including document and summary length, and a designed mediation task to evaluate workers' attention and comprehension. Based on their performance, workers are categorized into four tiers: GOLD, SILVER, BRONZE, and BLOCK. The endurance task tests workers' capacity to handle heavy workloads with a high number of HITs (Human Intelligence Tasks).

The study found that 36 out of 100 participants qualified as GOLD, demonstrating the effectiveness of this approach. The pipeline also includes a reference-based task where workers are filtered based on their performance against a gold standard, further enhancing the quality of annotations. The researchers conclude that their pipeline can serve as a best practice for large-scale annotation at lower cost while minimizing the need for manual curation. However, they acknowledge limitations such as the English summarization task on MTurk and the potential for designed questions to "garner" solutions rather than truly assessing worker performance.</sample>
    <sample id="246">Yes, the code is available on GitHub at mpoems/kitmus.</sample>
    <sample id="247">The presentation introduces FactKG, a dataset designed to enable community use of knowledge graphs for fact verification. It consists of 108k natural language claims with five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset includes various linguistic patterns such as colloquial style claims and written style claims, aiming to increase practicality. The claims are sourced from DBpedia, a structured database of human knowledge, and are categorized into supported or refuted labels. The presentation highlights the importance of using graphical evidence in the model, which results in superior performance compared to baselines that do not incorporate this evidence. Additionally, it discusses paraphrase methods used to convert written style claims into colloquial style claims and explores baseline experiments comparing different models like BERT, BlueBERT, and Flan-T5 across various input types (claim-only and with evidence). The dataset is available at https://github.com/jiho283/FactKG, and further details can be found at jho.kim@kaist.ac.kr.</sample>
    <sample id="248">The annotators for NLPositionality are not balanced in regard to each demographic, as indicated by the study participation statistics showing varying numbers of annotators from different countries and education levels.</sample>
    <sample id="249">The sentences in the acceptable domain were perturbed by adding prefixes, suffixes, adverbs, clauses, and quotes.</sample>
    <sample id="250">It means evaluating something based on multiple dimensions or aspects.</sample>
    <sample id="251">University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, Microsoft STC Asia</sample>
    <sample id="252">The presentation focuses on U-CREAT, an unsupervised pipeline for event-based retrieval of legal documents. It introduces a new dataset called IL-PCR and outlines the U-CREAT pipeline, which includes event extraction, candidate generation, and ranking. The pipeline leverages event-based methods to improve performance and inference time compared to traditional document retrieval techniques. Key contributions include proposing an unsupervised approach that doesn't require corpus-specific fine-tuning, and demonstrating superior F1 scores across various datasets (IL-PCR, COLIEE'21, and IL-PCR) with a notable performance boost over baseline models like BM25. The presentation also highlights the effectiveness of event-based models in handling complex legal documents and their potential for real-world applications.</sample>
    <sample id="253">The presentation discusses DisorBERT, a double domain adaptation model designed to detect signs of mental disorders in social media interactions. It begins by defining mental disorders as psychological syndromes associated with distress and disability affecting thinking, feeling, mood, and behavior. The study highlights the increasing usage of social media, with over 4.76 billion users globally, and emphasizes the importance of domain adaptation for models trained on general data to perform well on specific tasks like mental health detection.

The presentation explains that domain adaptation involves adjusting vocabulary, updating semantic understanding, and learning domain-specific tasks. It introduces DisorBERT, which adapts BERT to the mental disorders domain through guided masking and specialized training on mental health datasets. The model is evaluated using precision and recall analysis on eRisk datasets, showing better results than MentalBERT, a model trained on larger data but consuming more computational resources.

The evaluation also demonstrates a solid balance between finding users and labeling them correctly, making DisorBERT suitable for clinical detection applications. Future work includes exploring different lexical resources and clinical data to train even more specialized language models. The speaker concludes by thanking the audience and providing contact information for further inquiries.</sample>
    <sample id="254">The presentation focuses on a novel framework for document-level relation distant extraction, specifically addressing the challenge of extracting relations among entities within documents using a distant supervision mechanism. The framework incorporates uncertainty-guided label denoising to improve the quality of pseudo labels derived from existing relational triples in knowledge bases. This is crucial because DS data often contains noise due to pseudo labels, which can degrade model performance.

The methodology involves an iterative re-label strategy with dynamic class uncertainty thresholds to filter high-uncertainty pseudo labels. It leverages an initial pre-denoising RE model trained on human-annotated data and DS data to generate pseudo instances with uncertainty scores. These scores help in refining the DS data by iteratively re-training the pre-denoising RE model and filtering out unreliable pseudo labels.

The proposed framework achieves significant performance improvements over existing competitive baselines on two public datasets, DocRED and Re-DocRED. Extensive experiments demonstrate that the performance of baselines trained on denoised DS data is notably enhanced, indicating the effectiveness of the uncertainty-guided label denoising approach.</sample>
    <sample id="255">The form of the prompting is important in the case of 5-shot prompting.</sample>
    <sample id="256">The English content says:

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoan Liu,
Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz

Stony Brook University
Human Language Analysis Systems"

"Transfer and Active Learning for Dissonance Detection:</sample>
    <sample id="257">The authors evaluated four open-domain dialogue models.</sample>
    <sample id="258">The presentation explores whether large language models (LLMs) can serve as an alternative to human evaluations for assessing text quality. It begins by introducing the authors, Cheng-Han Chiang and Hung-yi Lee from National Taiwan University, and outlines the research question: "Can Large Language Models Be an Alternative to Human Evaluations?" The study proposes using LLMs to rate texts based on specific instructions, such as evaluating grammar or coherence. The motivation behind this approach is the instability and unreliability of human evaluation methods.

The presentation discusses related works, highlighting that while the idea of using LLMs for evaluation is natural and widely adopted, there were no prior studies exploring this concept at the time of submission to ACL 2023. It then contrasts human evaluation with LLM evaluation, noting that while human evaluation is widely used but unstable, LLMs can follow natural language instructions to conduct tasks like evaluating texts.

The experiment setting involves using LLMs to rate stories generated by GPT-2 and written by humans, focusing on four rating attributes: grammar, coherence, likeability, and relevance. The study uses English teachers to conduct human evaluations on the same stories using the same instructions as the LLM evaluation. The results show that larger LLMs (text-davinci-003 and ChatGPT) exhibit a clear preference toward human-written stories, while smaller LLMs do not show meaningful preferences. The presentation concludes with questions for further exploration, such as the agreement between LLMs and human evaluators, the impact of instruction wording, and the sampling method of responses from LLMs.</sample>
    <sample id="259">The presentation focuses on cross-lingual semantic parsing, a task that involves translating queries from multiple natural languages into various meaning representations such as SQL and Lambda Calculus. The key points include:

1. **Definition and Importance**: Semantic parsing is crucial for building systems that can understand user queries and translate them into structured data formats like SQL or Lambda Calculus.
2. **Cross-Lingual Semantic Parsing**: This extends semantic parsing to handle queries in multiple languages, aiming to provide a unified benchmark for cross-lingual tasks.
3. **Existing Challenges**: Current models often lack coverage on certain natural languages or meaning representations, limiting their effectiveness.
4. **XSemPLR Dataset**: A new dataset called XSemPLR is introduced, containing 9 datasets across various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families.
5. **Experimental Settings**: The study considers six settings for training and evaluation, including monolingual and multilingual models, and explores zero-shot and few-shot transfer learning scenarios.
6. **Performance Analysis**: Results show that Enc-Dec (mT5) outperforms previous work, with mT5 achieving the best performance on all datasets. Multilingual LLMs like Codex &amp; BLOOM are still inadequate for cross-lingual tasks, while FunQL outperforms other representations.

Overall, the presentation highlights the importance of developing robust cross-lingual semantic parsing models and the potential benefits of using monolingual training over transfer learning.</sample>
    <sample id="260">There are eight authors involved in the paper.</sample>
    <sample id="261">A good planner should be able to effectively decompose goals into steps and handle multi-faceted constraints.</sample>
    <sample id="262">There are 7 authors involved in the paper.</sample>
    <sample id="263">The presentation focuses on mitigating label biases in in-context learning for classification tasks, particularly addressing the instability and bias issues inherent in large language models (LLMs). It introduces a typology of label biases including vanilla-label bias, context-label bias, and domain-label bias, which are influenced by the task corpus. The study highlights that the task corpus is a significant source of label bias, leading to unstable performance in in-context learning.

Key findings include:
1. **Vanilla-Label Bias**: This bias arises from the model's uncontextual preference for label names, such as common token biases due to pretraining term frequency.
2. **Context-Label Bias**: This bias is influenced by the context, such as the majority label bias.
3. **Domain-Label Bias**: This bias is influenced by the task corpus itself, affecting the model’s prediction accuracy.

The research proposes domain-context calibration (DC) as a method to mitigate these biases holistically. DC involves calibrating original predictions using random in-domain words, which helps in improving the decision boundaries of the model. The study demonstrates that DC significantly enhances the performance of LLMs across various datasets with different levels of domain-label bias, especially in tasks with larger biases.

The presentation concludes with a summary emphasizing the importance of understanding and mitigating label biases in in-context learning, and encourages further exploration through the referenced paper for detailed insights.</sample>
    <sample id="264">The presentation titled "TAVT: Towards Transferable Audio-Visual Text Generation" discusses the challenges and limitations in the field of audio-visual text generation, particularly focusing on the need for efficient and scalable methods that can handle domain shifts between different modalities. The main issues highlighted include the arduous and expensive nature of data annotation, as well as the degradation of existing works when applied to new domains. The research introduces TAVT, a method designed to address these challenges by leveraging multi-modal domain shifts and aligning audio and visual features within a unified auditory semantic space. The motivation behind this approach is the inherent differences between audio and visual characteristics, such as the intrinsic property of timber in sound, which can be exploited for better cross-modal understanding.

The method described involves a pipeline that includes an Audio-Visual Meta-Mapper Network, an Audio-Visual Encoder &amp; Language Model Generator, and Counterfactual Contrastive Learning. This pipeline aims to learn and quickly adapt to new multimodal domains with limited labeled data. The Audio-Visual Meta-Mapper Network is responsible for mapping visual and audio inputs into a shared semantic space, while the Encoder &amp; Language Model Generator processes these inputs to generate text. Counterfactual Contrastive Learning is used to enhance the model's ability to generalize across different domains.

The presentation also outlines the experimental setup, including datasets like MSVD and MSR-VTT, and evaluates the performance of TAVT against other state-of-the-art methods using metrics such as BLEU, ROUGE, and CIDEr. The results demonstrate the effectiveness of TAVT in achieving high performance across various domains, making it a promising solution for transferable audio-visual text generation.</sample>
    <sample id="265">The name of the speaker is Vasudha Varadarajan.</sample>
    <sample id="266">The authors, Adam Przepiorkowski and Michal Wozniak, are affiliated with the Institute of Computer Science at the Polish Academy of Sciences in Warsaw.</sample>
    <sample id="267">Sure, here is the verbatim transcript of the English content from the provided text:

---

**Slide 1: Title Slide**
- **Title:** XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations
- **Authors:** Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang
- **Logos:** Penn State and Amazon

**Slide 2: Semantic Parsing**
- **Definition:** Semantic Parsing is a task to build semantic representation of user queries, such as SQL and Lambda Calculus.
- **Example Query:** "Which countries in Europe have at least 3 car manufacturers?"
- **Corresponding SQL:** `SELECT T1.country_name FROM COUNTRIES AS T1 JOIN continents AS T2 ON T1.continent = T2.cont_id JOIN T1 AS T3 ON T1.country_id = T3.country_id WHERE T2.continent = 'Europe' GROUP BY T1.country_name HAVING COUNT(*) &gt; 3`
- **Corresponding Lambda Calculus:** `(call Sw.listValue ( call Sw.getPropertv ( ( lambda s ( call Sw.ensureNumericProperty ( string 'num_manufacturers' ) string '&lt;' ) ( call Sw.ensureNumericProperty ( string 'num_assists' ) ) ) ( call Sw.domain ( string player ) ) ) ( string player ) ) )`

**Slide 3: Cross-lingual Semantic Parsing**
- **Definition:** Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.
- **Languages and Representations:** English → SQL, Lambda, FunQL; German → SQL, Lambda, FunQL; Chinese → SQL, Lambda, FunQL

**Slide 4: Existing CLSP Models**
- **Lack of Coverage on Certain Natural Language**
- **Lack of Coverage on Certain Meaning Representation**

**Slide 5: Single Model Approach**
- **Single Model for All Languages**
- **Languages and Representations:** English → SQL, Lambda, FunQL; German → SQL, Lambda, FunQL; Chinese → SQL, Lambda, FunQL

**Slide 6: XSemPLR Dataset**
- **Unified Dataset XSemPLR**
- **9 Datasets in Various Domains**
- **5 Semantic Parsing Tasks**
- **8 Meaning Representations**
- **22 Natural Languages in 15 Language Families**

**Slide 7: Experiment Settings**
- **Training and Evaluation Settings**
- **Translate-Test:** Use Google Translate API to translate source to target language. Then use monolingual model to train and eval.

**Slide 8: Training and Inference**
- **Training:** German → English → SQL
- **Inference:** German → Multilingual Model → SQL

**Slide 9: Monolingual Model**
- **Monolingual Model:** Source language is the same as target language, e.g., German-to-German. Also test Monolingual Few-shot setting by training monolingual models with only 10% training data.

**Slide 10: Multilingual Model**
- **Multilingual Model:** Train one multilingual model for all languages.

**Slide 11: Cross-lingual Zero-shot/Few-shot Transfer**
- **Cross-lingual Zero-shot/Few-shot transfer:** Train on one source language and transfer to another language.

**Slide 12: Analysis of Monolingual**
- **Enc-PTR:** Multilingual Pretrained Encoders with Pointer-based Decoders
- **Enc-Dec:** Multilingual Pretrained Encoder-Decoder Models
- **mBERT, mT5**

**Slide 13: Analysis of Multilingual Training**
- **mT5 + XLM-R + PTR**
- **XLM-R + PTR, mBERT + PTR**
- **mBART, mT5**

**Slide 14: Cross-lingual Performance Gap**
- **Green Line:** Monolingual Setting
- **Blue Line:** Cross-lingual Few-shot transfer
- **Orange Line:** Cross-lingual Zero-shot transfer

**Slide 15: Other Results &amp; Findings**
- **Enc-Dec (mT5) outperforms previous work or achieves comparable results.**
- **Pretraining on the English NL can significantly boost the performance of few-shot on target NLS.**
- **Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.**
- **FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.**

**Slide 16: Conclusion**
- **XSemPLR: A unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.**
- **Comprehensive benchmark study on three representative types of multilingual language models.**
- **mT5 with monolingual training yields the best performance, while multilingual LLMs are still inadequate for cross-lingual semantic parsing tasks.**
- **Performance gap between monolingual training and cross-lingual transfer learning is significant.**

**Slide 17: Links**
- **Paper Link:** https://arxiv.org/pdf/2306.04085.pdf
- **Code Link:** https://github.com/psunlpgroup/xsemplr

---</sample>
    <sample id="268">The most common errors of PaLM are dominated by accuracy/omission, and its style/awkwardness is generally lower compared to SOTA systems.</sample>
    <sample id="270">Emory University, Emory NLP Research Lab, Alexa</sample>
    <sample id="271">Continuous fine-tuning</sample>
    <sample id="272">Six authors are involved in the paper.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="275">#ACL2023

**From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models**

- **Shangbin Feng**
- **Chan Young Park**
- **Yuhan Liu**
- **Yulia Tsvetkov**

---

**LM Training Data**
**A mixed blessing**

- **Source:** Orange, J., et al. "Disentangling Large Language Models' Explicit and Implicit Political Leanings from Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.

---

**To this end**

- **Pretraining data**
- **Language models**
- **Downstream tasks**

---

**Evaluating LM Political Leaning**

- **Support both encoder and decoder LMs**
  - `&lt;statement&gt; I &lt;mask&gt; with this statement.`
  - "Do you agree or disagree with this statement? &lt;statement&gt;"

- **Automatic eval**
- **Grounded in polisci lit**

---

**Existing LMs**

- **BERT-base**
- **BERT-large**
- **RoBERTa-base**
- **RoBERTa-large**
- **distilBERT**
- **distilRoBERTa**
- **ALBERT-base**
- **ALBERT-large**
- **BART-base**
- **BART-large**
- **Alpaca**

---

**Pretraining Data**

- **Further pretrain LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning**

---

**Results**

- **Partisan shifts in LM political leaning**

---

**The Trump Card**

- **Pre-45th to post-45th shift**

---

**Qualitative Analysis**

- **Hate Speech Test**
- **Misinformation Test**

---

**Discussion**

- **Between Scylla and Charybdis**
- **To “sanitize” or not to “sanitize”, that is the question**

---

**Thank you!**</sample>
    <sample id="276">The presentation discusses the evaluation of machine translation metrics for Indian languages, focusing on automatic and meta-evaluation methods. It highlights the importance of studying evaluation metrics for languages other than English due to their unique grammatical rules, vocabulary, sentence structures, and resource availability. The study specifically examines translations to five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati, belonging to two different language families. The process involves collecting data from the Flores dataset, selecting 200 random sentences, and using various translation systems like mBART, Bing API, Google API, CVIT-IITI, IndicTrans, mT5, and NLLB to generate translations. Human annotations are then collected using the MQM framework, involving bilingual expert annotators who evaluate the outputs based on multiple criteria, including accuracy and fluency, and provide an overall score. The MQM framework categorizes errors into three main types: accuracy (spelling, grammar, register, character encoding), fluency (fluency spelling, fluency grammar), and others/special. The study also evaluates the robustness of various metrics against human scores, demonstrating that some metrics correlate well with human judgments, particularly in terms of fluency and accuracy.</sample>
    <sample id="277">The new method is called "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations."</sample>
    <sample id="278">The author described the "marked words" method as finding words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="279">The affiliations of the authors of the paper include Paul Allen School, University of Washington (UW), UW NLP, and Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="280">The presentation introduces MultiEMO, an advanced multimodal fusion framework designed for emotion recognition in conversations. It addresses the challenges of existing approaches by focusing on the complementarity of textual, audio, and visual modalities. The framework leverages attention-based correlation-aware mechanisms to model complex correlations across these modalities. Key contributions include a novel visual feature extractor named VisExtNet that captures relevant visual cues without redundant scene information, and a multimodal fusion model called MultiAttn that uses bidirectional multi-head cross-attention layers to integrate information from different modalities. The study demonstrates significant improvements in emotion recognition, particularly in minority and semantically similar emotion categories, through extensive experiments on MELD and IEMOCAP datasets. The framework's effectiveness is highlighted by its state-of-the-art performance metrics and the ability to handle asynchronous emotional tendencies across different modalities.</sample>
    <sample id="281">The presentation explores the necessity of context in translation, emphasizing that translation heavily relies on contextual understanding. It introduces the Conditional Cross-Mutual Information (CXMI) as a metric to evaluate how much context machine translation models utilize given a corpus. The study identifies that only a small portion of words depend on context and discusses two research questions: when translation requires context and how well models handle context-dependent translations. The presentation also highlights the importance of thematic analysis for high CXMI words, focusing on POS tags like pronouns and verb forms, and vocabulary items such as lexical cohesion and formality. The MuDA benchmark is introduced as a dataset-agnostic tool for document-level MT evaluation, demonstrating that context-aware models outperform others on phenomena like formality and lexical cohesion but struggle with ellipsis, pronouns, and verb form.</sample>
    <sample id="282">This research introduces StoryTrans, a novel framework for non-parallel story author-style transfer that leverages discourse representations and content enhancement techniques. The challenge addressed is the imitation of authors' linguistic choices at the discourse level and the high association between author styles and specific writing topics. The proposed solution involves two main components: discourse representation transfer and content preservation enhancing. Discourse representation transfer is achieved through a masked source story and style embedding process, followed by a masked transferred story generation. Content preservation enhancing focuses on maintaining the original story's content while adapting it to the target style. The training framework includes a first-stage model trained with a loss function that combines language modeling, style consistency, and discourse representation losses. A second-stage model uses a denoising auto-encoder (DAE) loss to reconstruct the original story from the transformed one, ensuring content preservation. The evaluation demonstrates superior performance compared to existing methods, achieving high style accuracy and content preservation scores across various datasets.</sample>
    <sample id="283">Bouquet/Stanford</sample>
    <sample id="284">The presentation discusses FSUIE, a novel framework for enhancing Universal Information Extraction (UIE) tasks. It addresses the limitations of existing UIE models that heavily rely on precise span boundaries, leading to ambiguity in annotation. The study introduces a fuzzy span mechanism and an efficient fuzzy span attention mechanism to alleviate this issue. FSUIE proposes a fuzzy span loss function that uniformly models the boundary as a continuous distribution, allowing for adaptive span extraction decisions rather than static ones. This approach is demonstrated to improve performance across various information extraction tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Argument Structure Extraction (ASTE). The model's effectiveness is validated through experiments on datasets like ACE04, ACE05, and ACE06, showing significant improvements over baseline models. Additionally, the visualization of FSA highlights its ability to focus on semantic information within a limited range of preceding tokens, enhancing the model’s performance with synergy.</sample>
    <sample id="285">The presentation discusses the challenges of factual error correction (FEC) in dialogue summarization models, highlighting that summaries generated by these models often contain factual errors. Two common solutions to this issue are direct design of better summarization models and factual error correction (FEC) for model-generated summaries. The FEC process involves using FEC models to correct factual errors in summaries. However, current evaluation methods, such as factuality metrics like FactCC, are criticized for being vague and unreliable, as they may not distinguish between different types of solutions effectively. The speaker proposes manually annotating reference corrections for FEC models to provide more valuable training data and create conditions for comprehensive performance evaluations. This approach aims to improve FEC model performance by addressing specific factual errors and ensuring more accurate metric scores.</sample>
    <sample id="286">The name of the speaker is Sarah E. Finch.</sample>
    <sample id="287">There are four authors involved in the paper.</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena include BLIMP, SyntaxGym, and CrowS.</sample>
    <sample id="289">When Does Translation Require Context? A Data-driven, Multilingual Exploration

Patrick Fernandes*, Kayo Yin*, Emmy Liu
André F. T. Martins, Graham Neubig

Translation depends on context
We'll have to get rid of that mole.
Things could start to get dangerous if the ministers find out.
We'll have to get rid of that mole.
Could it be anything serious, Doctor?
We'll have to get rid of that mole.

Evaluating context-dependent translation is hard
- Only a small portion of words depend on context
  - Corpus-level metrics
- Existing methods support limited discourse phenomena and languages

RQ1: When does translation require context?
- Word-level context usage
RQ2: How well do models handle context-dependent translations?

Conditional Cross-Mutual Information (CXMI)
- CXMI: measure how much context MT models use given a corpus

Pointwise (P-)CXMI
- We introduce P-CXMI to measure context usage to translate a specific
  - Sentence P-CXMI(y, x, C) = − log qMTA(y|x) / qMTc(y|x, C)
  - Word P-CXMI(i, y, x, C) = − log qMTA(yi|yi&lt; i, x) / qMTc(yi|yi&lt; i, x, C)

Thematic analysis of high P-CXMI words
- POS tags
- Vocabulary items
- Individual tokens

MuDA benchmark results
- Context-aware models perform significantly better on some phenomena
  - √: Formality, lexical cohesion
  - ❌: Ellipsis, pronouns, verb form
- DeepL outperforms Google on most phenomena and language pairs*

MuDA benchmark results
- Context-aware models perform significantly better on some phenomena
  - √: Formality, lexical cohesion
  - ❌: Ellipsis, pronouns, verb form
- DeepL outperforms Google on most phenomena and language pairs*
- Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="290">FTw, BOND, COSINE, MLC, L2R</sample>
    <sample id="291">The model is evaluated on 11 tasks.</sample>
    <sample id="292">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification

Regina Stodden, Omar Momen, Laura Kallmeyer
Heinrich Heine University Düsseldorf, Germany
ACL 2023

1. Text Simplification
What, why and How?

Text Simplification Example

Original:
"Die Gewerkschaft setzt sich dafür ein, dass zum Beispiel höhere Löhne gezahlt werden."

Plain Language:
"Die Gewerkschaft setzt sich für höhere Löhne ein."

2. DE-plain
A New Corpus

German Text Simplification Corpora

Sentence Level

Document Level

Types of Simplification

Simplification Transformations

3. Use-cases
Automatic alignment and simplification

Automatic Alignment Evaluation</sample>
    <sample id="293">**Slide 1: Title Slide**
- **Title:** Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)
- **Authors:** Mohammad Javad Hosseini, Filip Radlinski, Silvia Paresi, and Annie Louis
- **Affiliation:** Google Research

---

**Slide 2: Introduction to Indirect Referring Expressions**
- **Goal:** Understanding users' language when they make a choice.
- **Alternative question:** Did you mean easy on me or I gotta feeling?
- **Direct reference:** "easy on me", "the first one"
- **Indirect reference in natural conversation:**
  - Cannot remember the name
  - The pronunciations are hard to distinguish
  - Want to specify a preference
- **Examples of indirect references:**
  - "The newer one."
  - "The song that's not energetic."

---

**Slide 3: Dataset Collection**
- **Important problem:**
  - Conversational systems
  - Benchmarking Large Language Models' entity understanding
- **No large-scale public dataset available**
- **We collect a large dataset using crowd annotation**
- **Three domains:**
  - Music
  - Books
  - Recipes

---

**Slide 4: Dataset Collection Methodology**
- **Methodology emphasizes informality using a cartoon completion task**
- **Steps:**
  - Sets the dialog context [between two free manual prompts per domain]
  - The annotator fills in the alternative question
  - The annotator fills in the expression referring to one of the entities

---

**Slide 5: Generating Alternative Questions**
- **Generate alternative questions =&gt; sampling entity pairs**
- **Items with similar infoboxes on Wikipedia (same genre and/or artist):**
  - Do you mean This is it? or Man in the Mirror?
- **Items with similar descriptions on Wikipedia:**
  - Do you mean Thinking of You or Happy Anywhere?
- **Items with similar titles:**
  - Do you mean The Return (memoir) or The Return (Shatner novel)?
- **Uniform at random:**
  - Do you mean You Could Be Mine or The Way I Am?

---

**Slide 6: Background Knowledge (Music)**
- **Google search link to each song:**
  - Easy on Me (by Adele)
  - I Gotta Feeling (by The Black Eyed Peas)
- **Background knowledge:**
  - We ask annotators to:
    - Listen to at least some of each song
    - Read about each song

---

**Slide 7: Thank You!**
- **Thank you for your attention!**
- **If you have any questions, please email javadh@google.com**

---</sample>
    <sample id="294">CamemBERT is initially trained on the NACHOS dataset, which consists of 1.18B words open-source dataset of heterogeneous data crawled from diverse medical domains, naturally written in style.</sample>
    <sample id="295">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="296">The video discusses the importance of perspective in natural language understanding, particularly in detecting irony. It introduces the EPIC (English Perspectivist Irony Corpus), a dataset designed to explore how different perspectives influence the perception of irony. The corpus includes 3,000 text-reply pairs from Reddit and Twitter, covering various English varieties like Australian, British, Canadian, and American English. The annotation process involves multiple annotators from diverse backgrounds, ensuring a balanced representation of linguistic diversity. This approach aims to understand how context and perspective affect irony detection, contrasting it with traditional data-driven methods that rely on manually annotated data. The study also examines inter-annotator agreement (IAA) across different demographic groups, revealing significant variations in irony perception, especially between generations and nationalities.</sample>
    <sample id="297">The presentation discusses the concept of "dogwhistles" in political rhetoric, which are coded or suggestive terms used to appeal to specific groups without directly offending them. It highlights how these terms can evade content moderation and influence public opinion subtly. The study uses historical U.S. political speeches to analyze the prevalence and effectiveness of dogwhistles, particularly focusing on racial and identity-based terms. The project aims to create a typology and glossary of dogwhistles with rich contextual information, evaluate their recognition in language models, and demonstrate how they evade content moderation. The research also explores the impact of providing definitions and secret cues for better detection by AI models, showing that formal register dogwhistles perform better than informal ones. Additionally, it examines how automated toxicity detection scores change when standard group labels are replaced with dogwhistles, suggesting that dogwhistles can be less toxic when used strategically.</sample>
    <sample id="298">The findings that led to the conclusion that the temporal drift is the main cause of performance loss include the observation that performance degrades with larger temporal gaps, and the lack of diminishing returns from adaptive overfitting.</sample>
    <sample id="299">The presentation discusses improving the robustness of Natural Language Inference (NLI) models using minimax training. It highlights the issue of shortcut learning in NLI models, where decision rules correlate with labels spuriously rather than based on the actual meaning. The speaker explains that these shortcuts do not generalize well to out-of-distribution data and introduces prior work on shortcut mitigation, which involves training an auxiliary model to rely on shortcuts and re-weighting examples for the main learner. However, this approach has limitations such as requiring prior knowledge of shortcuts and potentially unstable training behavior. The presentation then introduces their proposed minimax training approach, which aims to learn an example weight distribution emphasizing under-represented hard examples. This method optimizes for the NLI task while up-weighting hard examples to improve overall performance. The results show consistent improvements in out-of-domain (OOD) performance while maintaining high in-distribution (ID) accuracy across different datasets.</sample>
    <sample id="300">The presentation titled "Toward Interactive Dictation" discusses advancements in speech-to-text technology, focusing on the challenges and solutions for integrating dictation with editing capabilities through voice commands. It highlights existing systems like Nuance Dragon NaturallySpeaking and Microsoft Word Dictate that support limited editing through voice but require users to memorize specific commands. The study introduces a new task called "Interactive Dictation," aiming to enable flexible interleaving of dictation and editing without reserved trigger words, addressing the inflexibility of current natural language command systems. The research outlines a data collection interface and baseline system, emphasizing the need for more intuitive and open-ended natural language for dictation and commanding. The presentation also covers the process of building a system, including ASR repair, segmentation, normalization, interpretation, and execution steps, and showcases results from a segmentation model and ASR repair + interpretation models, demonstrating improvements in exact match and state exact match metrics.</sample>
    <sample id="301">**Slide Title:** NLPositionality: Characterizing Design Biases of Datasets and Models

**Slide Content:**
- **Title:** NLPositionality: Characterizing Design Biases of Datasets and Models
- **Authors:** 
  - Sebastian Sardny (University of Washington)
  - Jenny T. Liang (Carnegie Mellon University)
  - Ronan Le Bras (Allen Institute for AI)
  - Katharina Reinecke (University of Washington)
  - Maarten Sap (Carnegie Mellon University)

**Slide Transition:**
- **Imagine...**
- **PerspectiveAPI score**
- **Design bias example!**

**Slide Title:** Positionality

**Slide Content:**
- **Definition:** "The perspectives [people] hold as a result of their demographics, identity, and life experiences."
- **Source:** Savin-Baden, Maggi, and Claire Howell-Major. *Qualitative research: The essential guide to theory and practice.* Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).

**Slide Title:** Do datasets and models have positionality?

**Slide Content:**
- **Anecdotal evidence:**
  - Model and dataset probing
  - Theoretical definitions of model positionality

**Slide Title:** Framework

**Slide Content:**
- **Collection:** Instances are collected from a dataset.
- **Processing:** Each instance has an associated ground truth label.
- **Model Prediction:** Instances are sent as part of a study on LPI.
- **Annotations from diverse annotators:** Annotators annotate the instances with labels.
- **Analysis:** Pearson’s correlation is computed between the ground truth labels, model predictions, and the annotations of the demographic dimension.

**Slide Title:** LabintheWild

**Slide Content:**
- **Pool of diverse volunteers/research participants**
- **Online experiment from researchers**

**Slide Title:** Task A: Social Acceptability

**Slide Content:**
- **Participants read a situation from the Social Chemistry dataset.**
- **Participants rate how socially acceptable the situation is.**
- **Participants compare their responses to others and an AI's.**

**Slide Title:** Task B: Toxicity

**Slide Content:**
- **Participants read an instance from the Dynahate dataset.**
- **Participants rate whether they think an instance is hate speech.**
- **Participants compare their responses to others and an AI's.**

**Slide Title:** Study Participation

**Slide Content:**
- **16,299 annotations**
- **1,096 annotators**
- **87 countries**

**Slide Title:** Results

**Slide Content:**
- **Finding 1:** There is positionality in NLP.
- **Finding 2:** Some populations are left behind.
- **Finding 3:** Datasets and models are less aligned to non-binary people.

**Slide Title:** Recommendations

**Slide Content:**
1. Keep a record of all relevant design choices made throughout building datasets or models.
2. Do NLP research through the lens of perspectivism:
   - Share disaggregated dataset labels.
   - Use modeling techniques that can handle annotator disagreement.
3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP (e.g., Masakhane initiative).

**Slide Title:** Dashboard Link

**Slide Content:**
- **Dashboard Link:** nipositionality.cs.washington.edu/
- **Paper:** bit.ly/NLPositionality-Paper/

**Slide Title:** Thanks!

**Slide Content:**
- **Thank you!**</sample>
    <sample id="302">To handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="303">To address positive stereotypes and essentializing narratives, an intersectional lens is recommended.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are structurally matched but have different meanings or are considered grammatically incorrect by human annotators.</sample>
    <sample id="305">The presentation discusses the challenges and limitations of weakly supervised learning (WSL) approaches in machine learning. It highlights that while WSL alleviates the annotation bottleneck by using noisy labels, these labels can introduce noise that hampers generalization. The speaker critiques the common claim in recent WSL works that models trained on weakly supervised data achieve high accuracy, pointing out that this often includes clean validation data. The research questions addressed include whether clean validation data is necessary, how many clean samples WSL approaches need, and how to use available clean samples more efficiently. The findings suggest that a clean validation set is indispensable for improving performance, and that WSL approaches benefit from more clean validation samples. The speaker also recommends continuous fine-tuning (CFT) as a baseline method and advises reporting model selection criteria and using few-shot learning approaches. The conclusion emphasizes the importance of clean samples and the overestimation of practicality in recent WSL approaches, advocating for always applying CFT.</sample>
    <sample id="306">The presentation discusses entity tracking in language models, emphasizing its importance for understanding discourses. It introduces a research question: "Can language models track entities?" and highlights challenges in evaluating these abilities through examples like "Put the egg into the bowl" and "Place the baby inside the crib." The speaker explains that while some models can track entities correctly, others fail, such as those with "Drain the water," "Mix," and "Empty the beaker." The task setup involves four boxes with objects (car, train, plane, watch) and instructions to move items between them. The presentation concludes by noting that smaller pretrained models, like T5-base, exhibit non-trivial entity tracking behavior, while randomly initialized models of the same size do not. The study explores the effect of pretraining data on entity tracking capabilities, suggesting that text and code pretraining makes this behavior more apparent.</sample>
    <sample id="307">The authors used F1, accuracy, precision, recall, and macro-averaged F1 as their evaluation metrics.</sample>
    <sample id="308">The presentation discusses the concept of "NLPositionality," which examines how design biases in datasets and models influence NLP research outcomes. It begins by introducing the research team and setting up an example involving two tech leads, Carl Jones from the New York Times and Aditya Sharma from the Times of India, who receive different PerspectiveAPI scores for the same statement. This highlights the potential bias in the model's interpretation based on the user's identity.

The presentation then delves into the concept of positionality, explaining that perspectives held by individuals are shaped by demographics, identity, and life experiences, which can impact research processes and results. It questions whether datasets and models exhibit positionality and cites several studies supporting this idea, including those on systematic inequalities in language technology performance and model positionality.

Anecdotal evidence is provided to support these claims, such as model and dataset probing and theoretical definitions of model positionality. The framework for characterizing design biases in NLP datasets and models is outlined, involving re-annotating datasets with diverse annotators and comparing annotations by demographic to models and datasets using Pearson’s R scores.

The study uses LabintheWild, an online platform with over 5,376,396 participants from 87 countries, to gather data. Participants engage in tasks like rating social acceptability and toxicity of situations, contributing to the analysis of hate speech and toxicity in the Dynahate dataset. The findings reveal that NLP datasets and models are most aligned with English-speaking countries and people with college educations, while some populations are left behind, particularly non-binary individuals.

The presentation concludes with recommendations for addressing positionality in NLP, such as keeping records of design choices, conducting research through the lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">Krippendorff's Alpha</sample>
    <sample id="310">The domain chosen to add completely unrelated sentences to the unacceptable and acceptable queries is Wikipedia.</sample>
    <sample id="311">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="312">MultiInstruct is distinguished by being the first multimodal instruction tuning benchmark dataset, containing 62 diverse tasks across 10 broad groups with 5 expert-written instructions each, addressing an imbalance issue between NLP and multimodal datasets.</sample>
    <sample id="313">3</sample>
    <sample id="314">Binary coordination involves two conjuncts, where one conjunct is the governor and the other is the complement.</sample>
    <sample id="315">The average length of the prompts was 10 words.</sample>
    <sample id="316">The findings imply that the smaller T5 model can generate higher quality scripts than larger language models like GPT-3, Codex, and InstructGPT when fine-tuned on the Coscript dataset.</sample>
    <sample id="317">The presentation titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors" discusses the advancements and benefits of using large code generation models for few-shot information extraction tasks. The speaker, Peng Li from Fudan University, introduces the concept of information extraction (IE), which aims to recognize structured information from plain text. This process involves identifying named entities (NER) and relations within sentences.

The presentation highlights that previous methods often struggle with aligning input and output formats, leading to mismatched outputs. To address this, the speaker presents CodeIE, a novel approach that leverages code-like prompts to generate structured outputs more effectively. CodeIE is designed to be more controllable and aligned with the desired output format, making it particularly suitable for few-shot IE scenarios where limited training data is available.

Key points include:
1. **Information Extraction (IE):** The process of recognizing structured information from plain text.
2. **Previous Methods:** These methods often fail to align input and output formats, resulting in mismatched outputs.
3. **CodeIE Approach:** Utilizes code-like prompts to generate structured outputs, enhancing control and alignment with the desired format.
4. **Evaluation Results:** The performance of CodeIE is compared favorably against other methods, demonstrating its effectiveness in few-shot IE tasks.

The presentation concludes by emphasizing the advantages of CodeIE in handling complex information extraction tasks with fewer examples, making it a promising tool for applications requiring efficient and accurate information extraction.</sample>
    <sample id="319">The work investigates two learning strategies: from scratch with full model construction and continual pre-training using an existing pre-trained model (here, CamemBERT, a French generic model, and PubMedBERT, an English-based medical one).</sample>
    <sample id="320">The factor of overfitting due to test reuse is 1.43%.</sample>
    <sample id="321">The quality of the simplification was evaluated using automatic alignment evaluation methods, with metrics such as precision, recall, and F1 score.</sample>
    <sample id="322">The video discusses the intersection of human morality and natural language processing (NLP), focusing on how text classifiers learn to distinguish right from wrong. It introduces the concept that moral judgments can be categorized into five foundational principles: care, fairness, loyalty, authority, and purity. The discussion highlights the role of subversion as a key element in moral rhetoric, with examples like "overthrow mayhem" and "encourage defiance" illustrating differing approaches in ALM (Allegiance to Law and Morality) and BLM (Black Lives Matter). The video aims to explain how these classifiers operate by analyzing their value rhetoric and the specific elements they emphasize, such as subversion, which is generally frowned upon in ALM but encouraged in BLM. This analysis helps in understanding the nuanced differences between these moral frameworks within the context of NLP.</sample>
    <sample id="323">The presentation focuses on the development and application of a dynamic heterogeneous graph reasoning model for commonsense question answering (CSQA) at ACL 2023. The research aims to enhance machine understanding by integrating language models with knowledge representation learning, addressing the limitations of existing methods that often ignore semantic relationships between entities. The study utilizes multiple knowledge bases like ConceptNet, WordNet, and Wiktionary to construct a heterogeneous knowledge graph (HKG), which is optimized through a two-stage process: first, pruning noisy entities; second, optimizing structure and knowledge representations using representation learning. The model incorporates path information from the HKG into the QA context, enhancing its ability to answer questions based on common knowledge. The integration of these techniques leads to improved performance, as evidenced by experimental results on official test sets of CommonsenseQA and OpenBookQA datasets, where the proposed method outperforms existing approaches.</sample>
    <sample id="324">Yes, language models can exhibit different political biases depending on their training data and the tasks they are used for.</sample>
    <sample id="326">two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent</sample>
    <sample id="327">The presentation titled "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" was delivered at ACL 2023 by Xiao Xu and his team from Harbin Institute of Technology, Northeastern University, Microsoft Research Asia, and Intel Labs. The research focuses on developing a model that can understand both images and text, aiming to train a smart AI system capable of interpreting multimodal data effectively. The approach involves large-scale self-supervised pre-training on image-text pairs, which helps in refining tasks such as visual question answering, multi-modal verification, and caption-based image retrieval.

The study introduces the Two-Tower architecture, which consists of a textual encoder, a visual encoder, and a cross-modal encoder. This architecture is designed to aggregate insights from uni-modal experts at different levels, enhancing the model's performance in vision-language representation learning. The presentation also compares this with the BridgeTower architecture, highlighting its limitations in layer-by-layer utilization and the number of cross-modal layers being tied to the number of uni-modal layer representations used.

The ManagerTower architecture is proposed as an advancement over the BridgeTower, taking multi-layer uni-modal representations as insights from pre-trained uni-modal experts at different levels and adaptively aggregating these insights via managers in each cross-modal layer. The ManagerTower is shown to work with any visual, textual, or cross-modal encoder, making it versatile and effective for various applications.

The research has achieved significant results, outperforming several state-of-the-art models across various datasets like VQA2.0, SNLI-VE, NUSVR, Flickr30K, and Flickr101K. The team's work is available on their GitHub repository, offering valuable insights into the field of vision-language learning and multimodal AI systems.</sample>
    <sample id="328">GPT-3</sample>
    <sample id="329">This research introduces a novel zero-shot video sentence localization method that leverages structured pseudo-labels to enhance robustness against noise. The method begins by generating free-form pseudo-queries using image description models and then selects event proposals based on their temporal structure, ensuring high relevance to the query. It calculates similarity scores between pseudo-queries and video frames, filtering out low-quality pairs and retaining those with high event quality. The approach also employs sample re-weighting and label refinement techniques to mitigate noise in pseudo-labels. This method is demonstrated to achieve state-of-the-art performance on two datasets, outperforming existing methods across various metrics such as R@1m and mIoU. The study's effectiveness is further validated through experiments and comparisons with SOTA approaches, highlighting its potential for improving zero-shot video sentence localization accuracy.</sample>
    <sample id="330">No, iterative update performs better than cumulative training in active learning for dissonance detection.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">The data for the MuDa benchmark was taken from the Multilingual Discourse-Aware (MuDA) benchmark.</sample>
    <sample id="333">The presentation discusses a novel training framework called INK (Injecting kNN Knowledge in Nearest Neighbor Machine Translation) aimed at refining the representation space of Neural Machine Translation (NMT) models. It addresses the limitations of NMT models, particularly their non-smooth representation spaces which can lead to poor performance on unseen domains. The speaker introduces kNN-MT as a previous solution that saves representations and target tokens into a datastore for smoothing predictions with nearest neighbors but highlights its drawbacks such as time-consuming retrieval from a large datastore and inability to update representations easily. INK is proposed to overcome these issues by iteratively refining the representation space according to kNN knowledge, using an adapter to adjust representations asynchronously and refreshing the datastore with updated representations. The overall training procedure involves optimizing the adapter with a combined learning objective, refreshing the datastore asynchronously, and running the loop until convergence. The experiment setting includes different baselines like V-kNN, A-kNN, R-kNN, and kNN-KD, and the target domains cover Medical, Law, IT, and Koran. The results show that INK achieves better translation performance with lower memory space and inference speed up compared to kNN-MT baselines.</sample>
    <sample id="334">Sure, here is the text extracted from the image:

---

**Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination**

**Adam Przepiórkowski and Michał Woźniak**

**Institute of Computer Science, Polish Academy of Sciences**
ul. Jana Kazimierza 5, 01-248 Warsaw

**ACL 2023**

---

**Dependency Structure of Coordination**

- **Bouquet/Stanford (Universal Dependencies):**
  - Homer loves Lisa, Bart, and Maggie.

- **Chain/Moscow:**
  - Homer loves Lisa, Bart, and Maggie.

- **Conjunction-headed/Prague:**
  - Homer loves Lisa, Bart, and Maggie.

- **Multi-headed/London:**
  - Homer loves Lisa, Bart, and Maggie.

---

**Dependency Length Minimization (DLM)**

**Word order tends to minimize dependency lengths:**

- **Marge read it yesterday.**
  - good
- **Marge read yesterday this absolutely fascinating book about bees.**
  - bad

---

**Compatibility with Dependency Structures of Coordination**

- **Bouquet/Stanford (Universal Dependencies):**
  - Homer loves Lisa, Bart, and Maggie.
  - NO

- **Chain/Moscow:**
  - Homer loves Lisa, Bart, and Maggie.
  - NO

- **Conjunction-headed/Prague:**
  - Homer loves Lisa, Bart, and Maggie.
  - YES

- **Multi-headed/London:**
  - Homer loves Lisa, Bart, and Maggie.
  - YES

---

**Conjunct Lengths in English**

**Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016):**

- **left conjuncts tend to be shorter (observed before).**
- **this tendency grows with length difference (briefly noticed in Gibson et al. 1996 88–90).**
- **but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed).**
- **not when it is on the right (Ted and Ned laughed).**

---

**Figure 1:** Proportions of shorter left conjuncts depending on the absolute difference of conjunct lengths (with confidence bands)

---

**See the paper for the full argument!**
**Talk to us at the poster session!**

---</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is a task that translates queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="337">The presentation titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" is part of ACL 2023, organized by Sun Yat-sen University. The research focuses on developing a method to learn word embeddings without relying on context or vocabulary, which is particularly useful for handling out-of-vocabulary (OOV) words. The study emphasizes the importance of understanding human study habits and how they influence the learning process.

The presentation begins with an introduction to the problem of OOV words and their impact on machine learning models. It then delves into the concept of graph-based relation mining, where words are represented as nodes in a graph, and relationships between them are depicted through edges. This approach allows the model to capture semantic relationships among words, even those that are not in the training vocabulary.

The speaker explains how the graph structure can be used to infer meanings of OOV words by leveraging their connections with known words. The model architecture includes a GCN layer for capturing node features and a GAT layer for attention mechanisms, which helps in focusing on relevant relationships. Additionally, the presentation discusses the use of synonyms from external knowledge bases and two-hop neighbor words on the graph to enhance the model's ability to generalize and handle complex word formations.

The evaluation section of the presentation showcases the performance of the proposed model against other state-of-the-art methods using intrinsic and extrinsic evaluations. The results indicate that the model performs well in tasks such as named entity recognition, POS tagging, and word analogy, demonstrating its effectiveness in handling OOV words.

In conclusion, the presentation highlights the potential of graph-based relation mining for improving word embedding learning, especially in scenarios where vocabulary is limited or unknown. The model's adaptability and feasibility are discussed, emphasizing the importance of rational word decomposition for its application in various languages.</sample>
    <sample id="338">The presentation discusses the evaluation of human natural language explanations (NLG) in AI models, focusing on whether these explanations are always helpful and how they can be objectively evaluated. It begins by outlining the motivations for studying NLG, including training NLG models, boosting prediction performance, and enhancing model reasoning. The speaker highlights that evaluating human-annotated explanations is subjective and lacks a gold standard. Popular NLG metrics like BLEU and ROUGE treat human annotations as gold standards, which only measure baseline model performance regardless of explanation presence.

The presentation then introduces a unified structure for baseline and infusion settings, where explanations are used as input to fine-tune models. Preliminary experiments on ECQA and CoS-E datasets show that fine-tuning with explanations improves model performance, suggesting that explanations are beneficial even when humans dislike them. The TREU metric is introduced to evaluate the helpfulness of explanations faithfully at both fine-tuning and inference stages. The study finds that CoS-E explanations are less helpful than ECQA explanations on the baseline model but still beneficial when fine-tuned with a small amount of data. The presentation concludes with future work recommendations, emphasizing the need for high-quality human annotation and suggesting quality checks during data collection.</sample>
    <sample id="339">The affiliations of the authors are Saarland University, Amazon Alexa, and University of Vienna.</sample>
    <sample id="340">The presentation introduces ParaAMR, a large-scale syntactically diverse paraphrase dataset created through AMR back-translation. It highlights the importance of paraphrase generation in various NLP applications such as question answering, chatbots, creative writing, data augmentation, and robustness. The dataset is designed to address the challenge of generating high-quality paraphrases at scale, which has been limited by human-annotated datasets like MRPC, PAN, and Quora. The researchers propose leveraging Abstract Meaning Representations (AMRs) to generate syntactically diverse paraphrases. This involves parsing sentences into AMR graphs, focusing on different aspects of the sentence, and then translating these graphs back into text. The dataset includes around 15.5 million source sentences with an average of 6.92 paraphrases per sentence, showcasing its potential for enhancing NLP applications.</sample>
    <sample id="341">The authors use 1s, 2s, 3s, and 4s as latency measures.</sample>
    <sample id="342">The presentation introduces LiveChat, a large-scale personalized dialogue dataset constructed from live streaming videos. It highlights the importance of open-domain dialogue systems in conversational exchanges between humans and AI, emphasizing the need for large-scale datasets to support these systems effectively. The speaker discusses existing challenges such as the reliance on manual extraction for large-scale datasets, which limits scalability. They also address the scarcity of detailed persona information and longer conversations necessary for personalized dialogue, along with the limited availability of Chinese multi-party dialogue corpora.

LiveChat addresses these issues by automatically constructing a dataset from live streaming videos, capturing rich persona profiles and longer conversations. The dataset includes over 13 million dialogues, each with detailed persona information, making it a valuable resource for developing AI systems that can handle personalized responses and address decisions. The presentation outlines the construction process of LiveChat, involving steps like collecting live streaming videos, extracting audio, transcribing, and matching streamer responses with audience comments. It also mentions the use of a persona classifier trained on DuLemon data to annotate persona profiles.

The study's contributions include proposing LiveChat, conducting experiments on response modeling and addressee recognition tasks, and investigating transfer learning of pre-trained dialogue models. The results demonstrate the effectiveness of LiveChat in enhancing the performance of AI systems in personalized dialogue generation and addressee recognition. The future direction is efficient transfer learning of large language models (LLMs) for LiveChat, leveraging its unique characteristics to improve AI capabilities in this domain.</sample>
    <sample id="344">The drawbacks of tree-based methods include the need for pre/post-processing logical forms and grammar induction, which can be complex and time-consuming processes.</sample>
    <sample id="345">This research paper introduces a novel approach to compositional generalization in semantic parsing, focusing on handling deeper recursion and unseen compositions of phrases that have been seen individually during training. The study addresses the limitations of naive seq2seq models by proposing a method that directly models the correspondences between fragments without relying on trees. This approach is particularly significant as it demonstrates strong generalization to deeper recursion for the first time.

The authors present their method within the context of neural seq2seq models, which are capable of learning from multiset tagging and latent permutations. They highlight the challenges associated with obtaining trees, such as pre/post-processing logical forms and grammar induction, and introduce a permutation model to address these issues. The permutation model is designed to induce alignment in training, making inference NP-hard but solvable through continuous relaxation.

The effectiveness of this approach is demonstrated through experimental results on COGS (Kim and Linzen 2020), where the proposed model outperforms other treeless models in structural generalization tasks. The paper concludes by discussing technical challenges, including the need for alignment induction in training and the complexity of inference, while also providing insights into potential future directions for research.</sample>
    <sample id="346">School of Interactive Computing, Georgia Institute of Technology</sample>
    <sample id="348">This research explores the use of natural language prompts to measure stereotypes within large language models (LLMs), focusing on the intersectionality of social biases and stereotypes. The study highlights the limitations of existing stereotype measures, such as their specificity versus generalizability tradeoff, reliance on fixed hand-curated datasets, and failure to account for intersectionality. To address these issues, the researchers propose a method called "Marked Personas," which involves generating personas using prompts that imagine individuals belonging to specific identity groups. These personas are then analyzed to identify words that distinguish marked groups from unmarked ones.

The study demonstrates that LLMs, particularly GPT-3.5 and GPT-4, can respond to instructions in prompts, allowing for the creation of diverse persona descriptions. For instance, personas like "an Asian woman" or "a Middle Eastern woman" are generated, showcasing how LLMs can produce detailed descriptions that reflect various identities. The personas are evaluated based on their ability to capture intersectional identities, providing insights into how LLMs perceive and represent different social groups.

The findings reveal that generated personas contain more stereotypes than human responses, indicating a potential bias in how LLMs process and generate text related to marginalized groups. This study underscores the importance of developing methods to mitigate these biases and improve the representation of diverse identities within AI systems. Recommendations include adopting an intersectional lens and increasing transparency about bias mitigation strategies to enhance the fairness and inclusivity of AI technologies.</sample>
    <sample id="350">The presentation discusses the concept of superhuman performance in natural language understanding (NLU) systems, emphasizing the challenges and limitations of current evaluation methods. It highlights that leaderboard-based evaluations have become popular but often lead to claims of superhuman capabilities, which may not be justified. The SuperGLUE benchmark is introduced as a well-known framework for evaluating general-purpose language understanding models, listing tasks such as Word in Context, Multi-Sentence Reading Comprehension, and Commitment Bank. The presentation also addresses the reliability of leaderboard scores by comparing them with human performance, noting that humans outperform systems on six out of ten SuperGLUE tasks. Additionally, it explores the issue of heterogeneous and unknown pay rates among annotators, suggesting that these factors might contribute to the large gap between system and human performance. The speaker concludes by discussing the need for more transparent benchmarks and the consequences of current practices, aiming to provide recommendations for fairer and more transparent evaluation methods.</sample>
    <sample id="351">This research explores the effectiveness of CoNLL-2003 Named Entity Taggers in 2023, questioning whether models trained on this dataset still perform well with modern data. The study delves into the challenges of generalization and performance degradation over time, examining factors such as model architecture, size, and fine-tuning examples. It highlights that while transformer models generally outperform others due to better generalization capabilities, larger models also tend to generalize better. The research further investigates potential causes for performance drops, including adaptive overfitting and temporal drift, concluding that temporal drift is the primary factor affecting performance. The study's findings suggest that for improved generalization, better model architectures, larger model sizes, and more fine-tuning examples are necessary. The dataset used, CoNLL++ from 2020, was annotated according to CoNLL-2003 guidelines, and the performance of various models was evaluated on both CoNLL-2003 and CoNLL++. The research also proposes that CoNLL-2003 taggers remain effective, despite the advancements in technology. This work contributes valuable insights into the evolution of NER models and their adaptability to new datasets.</sample>
    <sample id="352">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="353">The presentation focuses on Python code generation through asking clarification questions, addressing the challenge of input underspecification in natural language descriptions. It introduces the concept of interactive code generation by incorporating clarification questions and answers (CQAs). The paper proposes a method to create a synthetic dataset called CodeClarQA, which includes clarifications on key operations to alleviate the problem of underspecification. The pipeline for CQ-driven code generation involves three main components: a clarification need predictor, a question selector, and a code generator. The analysis section discusses the creation of the dataset, the pipeline's performance, and error analysis. The paper also presents module experiment results, highlighting that the task is more challenging than existing CQ ranking tasks and that the hypothesis holds true with high F1 scores. The presentation concludes with an example of predictions and an invitation to check out the paper and code.</sample>
    <sample id="354">Until 2016, the performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points.</sample>
    <sample id="356">The affiliations of the authors are The University of Amsterdam, Saarland University, and the University of California, San Diego.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five authors are involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for SimulST.</sample>
    <sample id="360">Sure, here is the written content from the image:

---

**MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning**

Zhiyang Xu*, Ying Shen*, Lifu Huang  
Department of Computer Science, Virginia Tech  
*Equal Contribution

---

**Pre-trained Language Models for Downstream Tasks**

Figure 2: Comparing instruction tuning with pretrain-finetune and prompting

Image credit: Wei, Jason, et al. "Pretrained language models are zero-shot learners."

(A) Pretrain-finetune (BERT, T5)
- Typically requires many finetuning tasks for each task.

(B) Prompting (GPT-3)
- Requires few-shot prompting on each task.

(C) Instruction tuning (FLAN)
- Instruction tune on downstream tasks.
- Model learns to perform multi-modal language instructions.

Inference on downstream task

---

**Language-only**

---

**Instruction Tuning on Multimodal Pre-trained Models**

---

**Imbalance in Instructional Datasets between NLP and Multimodal**

1600+ Language-only instruction tasks
NO large-scale, publicly-available multimodal instruction tasks

---

**MULTIINSTRUCT**
The first multimodal instruction tuning benchmark dataset
- 62 diverse multimodal tasks
- 10 broad groups
- 5 expert-written instructions

---

**OFA (One For All)**
- A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities.
- OFA has a unified vocabulary for language, image tokens and the coordinates of a bounding box.

---

**MULTIINSTRUCT**
Figure 1: Example Instances from MULTIINSTRUCT for Four Tasks

Grounded Caption:
- Input: Generate a caption for this image.
- Output: Blue and white tennis racquet

Text Localization:
- Input: Select the region that contains the word "racket".
- Output: -bbox 484:119, 181 -bbox 448:119, 181 -bbox 430:119, 181 -bbox 420:119, 181 -bbox 400:119, 181 -bbox 380:119, 181 -bbox 360:119, 181 -bbox 340:119, 181 -bbox 320:119, 181 -bbox 300:119, 181 -bbox 280:119, 181 -bbox 260:119, 181 -bbox 240:119, 181 -bbox 220:119, 181 -bbox 200:119, 181 -bbox 180:119, 181 -bbox 160:119, 181 -bbox 140:119, 181 -bbox 120:119, 181 -bbox 100:119, 181 -bbox 80:119, 181 -bbox 60:119, 181 -bbox 40:119, 181 -bbox 20:119, 181 -bbox 0:119, 181

Referring Expression Selection:
- Input: Select the region of the object that contains the word "racket".
- Output: -bbox 484:119, 181 -bbox 448:119, 181 -bbox 430:119, 181 -bbox 420:119, 181 -bbox 400:119, 181 -bbox 380:119, 181 -bbox 360:119, 181 -bbox 340:119, 181 -bbox 320:119, 181 -bbox 300:119, 181 -bbox 280:119, 181 -bbox 260:119, 181 -bbox 240:119, 181 -bbox 220:119, 181 -bbox 200:119, 181 -bbox 180:119, 181 -bbox 160:119, 181 -bbox 140:119, 181 -bbox 120:119, 181 -bbox 100:119, 181 -bbox 80:119, 181 -bbox 60:119, 181 -bbox 40:119, 181 -bbox 20:119, 181 -bbox 0:119, 181

Question Image Matching:
- Input: Given the content of the image, provide the correct answer to the question.
- Output: The question is "What is the sport being played?"

---

**Multi-Modal Instruction Tuning**

Training Dataset Construction:
- Use 53 tasks from 9 groups for training.
- Sampled 1 instance per task.

Testing Dataset Construction:
- Reserve the entire Commonsense Reasoning group for testing.
- Select additional 5 tasks from VQA and Miscellaneous groups.
- Use all the instances in the test split for each task.
- Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.

---

**Implementation Details**

Training details:
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of its five instruction templates.

Testing details:
- For each task, conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- Report the mean and maximum performance and the standard deviation of the performance across all five experiments.

---

**Evaluation Metrics**

For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification) we report the Accuracy.
For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue) we report the Rouge-L.
For NLP tasks, we report Rouge-L.

We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the performance score for most tasks, and Accuracy for tasks that only have accuracy as a metric.

---

**Sensitivity**

How sensitive the model is towards variety of instructions for the same task:
- Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.

---

**Effectiveness of Instruction Tuning on MULTIINSTRUCT**

OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.

Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.

Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.

---

**Impact of Increasing Multimodal Instruction Task Clusters**

- Img Und: VQA + Image Understanding
- Grounding: Grounded Matching + Grounded Generation
- MISC, ITM: Temporal Ordering + Miscellaneous + Image Text Matching
- Relation: Visual Relationship
- Region: Region Understanding
- NLP: NLP tasks

---

**Effect of Diverse Instructions on Instruction Tuning**

OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.

---

**Effect of Fine-tuning Strategies on Model Sensitivity**

- Instruction tuning on MultiInstruct can significantly reduce the sensitivity of OFA.
- Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.

Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.

---

**Zero-Shot Performance on NLP Tasks**

- Instruction Tuning on MultiInstruct can improve zero-shot performance on unseen NLP tasks.
- The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.

---

**Conclusion**

- First large-scale multi-modal instruction tuning dataset.
- Contains 62 multi-modal tasks from 10 broad categories.
- Significantly improve the zero-shot capability of OFA via instruction tuning.
- Explore several transferring learning techniques and show their benefits.
- Design a new metric sensitivity.

---

**One More Thing!**

We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and will release them soon!

---</sample>
    <sample id="361">The presentation focuses on the challenge of compositional generalization in multi-step quantitative reasoning, particularly within financial data analysis. It introduces CounterComp, a method that uses counterfactual contrast to improve compositional generalization for such tasks. The slides highlight the difficulty in accurately answering questions involving changes over time and across different operations (addition, subtraction, multiplication, division) using simple arithmetic programs. A table illustrates the performance decline with increasing program steps, emphasizing the long-tail issue where accuracy drops significantly as complexity increases. The concept of questions functioning as counterfactual examples is introduced, suggesting that questions can help identify and correct errors in reasoning. The presentation concludes by demonstrating how CounterComp enhances performance on both in-distribution and out-of-distribution samples, showcasing its effectiveness in improving multi-step quantitative reasoning in financial contexts.</sample>
  </task>
</testset>