<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, in denen Nachrichtenmedien wie New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut vertreten sind.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Title: LayoutMask: A Novel Pre-trained Model for Visually-rich Document Understanding

Abstract:

In this paper, we present LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU). Our model addresses the reading order issues in existing document pre-training methods by using in-segment token orders as 1D position, referred to as "local 1D position". We equip the commonly used pre-training objective, Masked Language Modeling, with two novel masking strategies: Whole Word Masking and Layout-Aware Masking. Additionally, we design a new pre-training objective, Masked Position Modeling, which promotes text-layout interactions and helps the model learn better layout representations. Our experiments show that LayoutMask using local 1D position outperforms global 1D position on both FUNSD and SROIE datasets, and falls behind on CORD due to the entity "Total". We conclude that using local 1D position is more adaptive to cases with vertical and horizontal layout, and misleading numbers. Our model demonstrates great potential for VrDU tasks and can be applied to various applications, such as form understanding and receipt analysis.</sample>
    <sample id="3">Hallo! Willkommen zur Präsentation von DEPLAIN, einem neuen Corpus für die deutsche Textidentifikation auf Dokumentenebene und auf Satzebene, und ich bin Regina Stodden. Ich werde euch durch den ersten Teil der Präsentation führen. Zunächst definieren wir Textvereinfachung. Textvereinfachung ist ein Prozess, bei dem ein Text adaptiert wird, um dessen Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, beispielsweise für Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, sind parallele Paare von Texten erforderlich, beispielsweise von Dokumenten oder Sätzen. Und hier ist ein Beispiel für ein paralleles, ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfacher Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie in diesem Beispiel zu sehen ist, wie z.B. lexikalische Substitution, Klausradierung, Reihenfolgeänderung oder die Einfügung von Wörtern. Wir schlagen nun unser neues Corpus vor, DEPLAIN, da es in den letzten Jahren Probleme mit bestehenden Corpora gab. Zum Beispiel sind diese Corpora zu klein, um ein Textvereinfachungsmodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie bei ihren Ausrichtungen Fehler aufweisen können. Daher schlagen wir unser neues Corpus DEPLAIN vor, das in zwei Unter-Corpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa wurden 483 Dokumente manuell ausgerichtet, was etwa 13.000 parallele Satzpaare ergibt. DEPLAIN-web umfasst verschiedene Domänen und wir haben alle 750 Dokumente auf der einen Seite manuell und auf der anderen Seite mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergibt sich ein Gesamt von 30.450 Satzpaaren. Wir haben unsere Satzpaare ein bisschen genauer analysiert, beispielsweise auf die Art der Vereinfachung. Wie ihr sehen könnt, sind Bibeltexte stark vereinfacht, während beispielsweise Nachrichtentexte oder Sprachlernertexte weniger vereinfacht sind. Auf allen Ebenen, beispielsweise hinsichtlich lexikalischer Vereinfachung, Strukturvereinfachung oder auch der Gesamtvereinfachung. Ebenfalls können wir sehen, dass unser DEPLAIN-Corpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir in dem DEPLAIN-apa-Corpus viele mehr Reihenfolgeänderungen und Wortschärfungen als in dem DEPLAIN-web-Corpus. Andererseits haben wir in dem web-Corpus viele mehr Umschreibungen. Lass uns nun sehen, was wir mit diesem Corpus machen können.

Hallo, ich bin Omar und nun werde ich über die Verwendungsfälle unseres Datensatzes DEPLAIN sprechen. Für den ersten Verwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren wurden viele Ausrichtungsmethoden vorgeschlagen, aber in unserem Verwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente mit derselben Sprache, aber unterschiedlicher Komplexitätsstufe zu extrahieren. Und nun, da wir unseren Datensatz DEPLAIN haben, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie die Codes, um unsere Experimente durchzuführen, in der Arbeit veröffentlicht. Am Ende haben wir festgestellt, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode von MASSalign ist. Und ihr könnt auch die Codes finden, um diese Methode auf euren eigenen Dokumenten durchzuführen.

Der zweite Verwendungsfall, den wir in unserer Arbeit gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch Fine-Tuning von Sprachmodellen, um vereinfachten Text von komplexen Eingabedokumenten zu erzeugen. Wir haben zwei verschiedene Modelle fine-tuned. Wir haben das Modell von long-mBART fine-tuned, um Dokumentenebenevereinfachungen zu erzeugen, und wir haben auch das normale Basismodell mBART fine-tuned, um Satzebenevereinfachungen zu erzeugen. Ihr könnt auch alle Checkpoints finden und in mehr Details die Ergebnisse und die Bewertungsmetriken unserer Experimente in der Arbeit lesen. Wir haben festgestellt, dass diese grundlegende Fine-Tuning-Technik besser als die Baseline-Ergebnisse abschneiden konnte und wir diese Ergebnisse als Basis-Benchmark für das Problem der automatischen Textvereinfachung in Zukunft vorschlagen. Vielen Dank für eure Aufmerksamkeit und wir hoffen, euch alle auf der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL Modell.</sample>
    <sample id="6">Title: Towards Unifying Multi-Lingual and Cross-Lingual Summarization: A Many-to-Many Approach

Abstract:

This paper presents a unified approach to multi-lingual and cross-lingual summarization, dubbed many-to-many summarization. The proposed method aims to build a single summarization model that can process documents in any source language and generate summaries in any target language. The authors conduct preliminary studies to compare the performance of many-to-many summarization with traditional multi-lingual and cross-lingual summarization methods. The results show that many-to-many summarization can better transfer task knowledge across different languages. To facilitate this approach, the authors propose PISCES, a pre-trained many-to-many summarization model that learns language modeling, cross-lingual ability, and summarization ability through a three-stage pre-training process. Experimental results on the WikiLingua dataset demonstrate the effectiveness of PISCES, outperforming various baselines, including mBART-50 and mT5. Ablation studies and human evaluations further verify the superiority of PISCES.</sample>
    <sample id="7">Ja, die CoNLL-2003-Tagger funktionieren noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, ist neu, da sie eine explizite Anmerkung von Verhaltensweisen in Chat-Modellen beinhaltet, wie zum Beispiel das Ignorieren des Gesprächspartners oder das Widersprechen von Informationen. Diese Methode soll die Subjektivität menschlicher Bewertung reduzieren und eine genauere und zuverlässigere Bewertung von Chat-Modellen ermöglichen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit von sauberen, manuell annotierten Validierungssamples ab.</sample>
    <sample id="10">Um das Ergebnis weiter zu verbessern, gibt es noch viel Raum für Verbesserungen, insbesondere wenn das Modell nur auf Entity-Namen zugreifen kann. Die genauen Zahlen sind:

- Wenn das Modell Zugriff auf das gleiche Hintergrundwissen wie die Annotatoren hat, erreicht es eine Genauigkeit von 92-95%.
- Wenn das Modell Zugriff auf teilweise überschneidendes Hintergrundwissen hat, liegt die Genauigkeit bei 82-87%.
- Wenn das Modell nur auf Entity-Namen zugreifen kann, erreicht es eine Genauigkeit von nur 60%.</sample>
    <sample id="11">Der Sprecher, Jack Hessel, präsentiert eine Studie über das Verständnis von Humor bei großen Sprachmodellen. Er zeigt, dass diese Modelle zwar einfache Witze erzeugen und erklären können, aber ihre Fähigkeit, Humor zu verstehen, begrenzt ist. Um dies zu untersuchen, wurde ein Datensatz aus dem New Yorker Caption Contest erstellt, bei dem Leser ihre besten Überschriften für Cartoons schreiben können. Der Datensatz wurde in drei Aufgaben aufgeteilt: Matching, Qualitätserkennung und Erklärungsgenerierung.

Die Ergebnisse zeigen, dass die besten Modelle (CLIP) bei der Matching-Aufgabe 62% Genauigkeit erreichen, aber nur ein Drittel der Genauigkeit der Menschen. Auch bei der Qualitätserkennung und Erklärungsgenerierung zeigt sich ein großes Leistungsloch zwischen Modellen und Menschen. GPT-4, ein anderes großes Sprachmodell, erreicht bei der Matching-Aufgabe nur 20% Genauigkeit, selbst wenn es eine menschliche Beschreibung des Bildes erhält. Die Studie zeigt, dass große Sprachmodelle noch viel zu lernen haben, um Humor zu verstehen.</sample>
    <sample id="12">Es sind 5 Autoren an der Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">Title: Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings

Abstract:

Adaptive inference is a method to reduce inference time in large language models by utilizing low-capacity models for easy samples. Two common adaptive inference methods are Multi Model and Early Exit. This work presents a comprehensive analysis of the pros and cons of each method, highlighting the existence of conflicting gradients in Early Exit training, which degrades performance. To address this issue, a novel fine-tuning method, SWEET (Separating Weights in Early Exit Transformers), is introduced. SWEET trains each layer to receive updates only from the following classifier, avoiding conflicting gradients. Experimental results show that SWEET closes the gap between Early Exit and Multi Model, outperforming both methods in fast speeds and throughout the entire speed/accuracy curve for BERT-Large. This work presents the first fair comparison of Early Exit and Multi Model adaptive inference methods and motivates future research on fine-tuning algorithms tailored to the Early Exit architecture.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieses Gespräch geht über die Abhängigkeit der Koordination. Wie du vielleicht weißt, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den Universal Dependencies, wird die Struktur der Koordination, wie "Lisa, Bart und Maggie", so dargestellt, dass das erste Konjunkt den ganzen Koordinationsbaukasten anführt. In diesem Fall ist es "Lisa". Eine ähnliche Annahme wird in Igor Mel'čuks Bedeutungstexttheorie angenommen, wo wiederum der ganze Koordinationsbaukasten durch das erste Konjunkt angeführt wird. Diese beiden Ansätze sind asymmetrisch. Sie stellen eines der Konjunkte heraus.

Es gibt auch asymmetrische Ansätze zur Koordinationsstruktur, wie den Prager Ansatz. Der Ansatz, in den Prager Abhängigkeitstreebanks verwendet wird, wo die Koordinationsstruktur durch die Konjunktion angeführt wird. Wir erhalten daher einige Abhängigkeiten von der Konjunktion zu allen Konjunkten. Schließlich gibt es auch einen mehrköpfigen Ansatz, der in Hudsons Wortgrammatik verwendet wird, wo alle Konjunkte Köpfe der Koordinationsstruktur sind. Wir erhalten daher Abhängigkeiten vom Gouverneur zu allen Konjunkten separat: "Lisa, Bart und Maggie".

Das Ziel dieses Papiers ist es, einen neuen Argument für die symmetrischen Strukturen der Koordination zu liefern, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordination, wie diese beiden. Das Argument basiert auf dem Prinzip der Abhängigkeit von der Minimierung der Länge, das ich auf der Grundlage dieser Beispiele erklären werde.

In Englisch weißt du vielleicht, dass direkte Objekte sich gerne nah am Verb befinden, während Nebensätze weiter weg sein können. "Marge las es gestern" ist in Ordnung, weil das direkte Objekt nah am Verb ist, während "Marge las gestern es" viel schlechter klingt. Weil hier zwischen dem Verb und dem direkten Objekt ein Nebensatz: "gestern" ist. Allerdings kann dieser Effekt gelindert werden, wenn das direkte Objekt sehr schwer und lang ist. Dann kann es in die Position nach dem Nebensatz bewegt werden. Dies wird hier gezeigt. Beide Sätze sind in Ordnung. "Marge las diesen absolut faszinierenden Buch über Bienen gestern." Es ist in Ordnung, anstatt "es" haben wir diesen langen NP. Es ist auch in Ordnung zu sagen, "Marge las gestern diesen absolut faszinierenden Buch über Bienen." Die Argumentation hier ist, dass dies möglich ist, weil auch wenn dieser Satz die allgemeine grammatische Regel verletzt, dass direkte Objekte sich nah am Verb befinden sollen, er die Regel der Abhängigkeit von der Minimierung der Länge erfüllt, die besagt, dass kürzere Abhängigkeiten bevorzugt werden.

Diese beiden Bäume zeigen nur die Länge der wichtigsten Abhängigkeiten an, die nicht konstant sind zwischen diesen beiden Strukturen. Hier haben wir eine Abhängigkeit von "lesen" zum Nebensatz der Länge 7 gemessen in Wörtern und von "lesen" zum Buch der Länge 4, also zusammen 11. Wenn man diese beiden Konstituenten austauscht, wird die Summe dieser beiden Abhängigkeiten 6. Also anstatt 11, 6 ist viel kürzer. Deshalb klingt das ganz in Ordnung. Richtig?

Also haben wir verschiedene Statistiken über Koordination aus der erweiterten Version des Penn Treebank extrahiert und diese Statistiken bestätigen die Beobachtung, die oft gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. "Salz und Pfeffer" und nicht "Pfeffer und Salz", gemessen in Silben. Und auch die Beobachtung, die in der Parsing gemacht wurde, dass diese Tendenz mit der Länge wächst. Wenn der Unterschied zwischen den Längen der beiden Konjunkte wächst, bevorzugt das kürzere Konjunkt es zu sein, stärker, richtig? Also ist die Proportion größer von dem linken kurzen Konjunkt. Aber was neu in diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Gouverneur links ist oder fehlt.

Der Gouverneur ist links in diesem Beispiel "Ich sah Bart und Lisa", also ist der Gouverneur links. Er ist in diesem Beispiel "Homer kam und hustete" abwesend. Hier haben wir Koordination von zwei Verben und es gibt keinen externen Gouverneur. In solchen Fällen bevorzugt das linke Konjunkt es zu sein, kürzer; der größte Unterschied zwischen den beiden Konjunkten. Aber wenn der Gouverneur rechts ist, wie hier, "gelacht" regiert die Koordination "Ted und Ned", dann verschwindet dieser Effekt. Wir haben also gezeigt, dass durch die Messung der Länge in Zeichen, in der ersten Spalte, in Silben in der mittleren Spalte und in Wörtern in der rechten Spalte. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Gouverneur links ist, die Tendenz für das linke Konjunkt zu sein, kürzer, wächst kontinuierlich mit der absoluten Differenz in Wörtern und dass dasselbe beobachtet wird, wenn es keinen Gouverneur gibt, wie in der Koordination von Sätzen. Aber wenn der Gouverneur rechts ist, verschwindet dieser Effekt.

Wir zeigen in dem Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination liefert, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden. Seht das Papier für die vollständigen Argumente. Und sprecht uns auf der Poster-Session. Vielen Dank.</sample>
    <sample id="15">Es sind drei Autoren an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bible-Texte werden stark vereinfacht.</sample>
    <sample id="17">In der Arbeit "Multimodal Relation Extraction" wird ein neues Framework vorgestellt, das es ermöglicht, Beziehungen zwischen Entitäten in multimodalen Daten (Text und Bild) zu extrahieren. Das Framework besteht aus fünf Teilen: 

1. Darstellung von Text und Bild als visuelle und textuelle Szenengraphen.
2. Merging der visuellen und textuellen Szenengraphen zu einem einheitlichen Cross-Modal-Graphen (CMG).
3. Feinabtastung des CMG durch Filterung von Knoten und Anpassung von Kanten.
4. Anwendung des Graph-Information-Bottleneck-Prinzips zur Leitung der Optimierung.
5. Erweiterung der komprimierten CMG-Funktionen mit multimodalen Themenfunktionen.

Die Arbeit zeigt, dass die Verwendung von Bildern zu höheren Leistungen führt und dass die gezeigte Methode die bestehenden Modelle auf Benchmarks übertrifft. Die Ergebnisse zeigen, dass die internen Informationen für die Inputs mit hoher Text-Vision-Relevanz wichtig sind, während die externen Informationen für die Inputs mit geringer Text-Vision-Relevanz wichtiger sind.</sample>
    <sample id="18">Ein Beispiel für die Präferenz für kürzere linke Konjunktionen ist "salt and pepper" (und nicht "pepper and salt"), gemessen in Silben.</sample>
    <sample id="19">Our work, "A Survey for Efficient Open Domain Question Answering," focuses on improving the efficiency of open-domain question answering systems. We analyze the challenges of existing two-stage models, including large Wikipedia corpora, high index file sizes, and complex language models. Our goal is to achieve smaller memory costs, faster inference, and comparable performance. We summarize core techniques from various aspects, including fast evidence research, quick reading, reduced index size, and reduced model size. We also compare existing open-domain question answering models and provide insights on choosing the best approach based on resource constraints. Our conclusions suggest that retrieval-only systems are suitable for real-time feedback, while retrieval and reader systems offer a trade-off between speed, memory, and performance. Finally, we propose two future works: deploying open-domain question answering systems on low-power devices and considering additional evaluation metrics. Our survey aims to provide a comprehensive overview of efficient open-domain question answering systems and their applications.</sample>
    <sample id="20">Ja, die Modelle basierend auf DrBERT sind unter der MIT-Lizenz frei verfügbar auf Hugging Face. Sie können sie für Ihre Forschung verwenden, vorausgesetzt, dass Sie die Lizenzbedingungen einhalten.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtentexte.</sample>
    <sample id="22">Drei Hauptfaktoren sind für eine gute Generalisierung notwendig: 

1. Eine geeignete Modellarchitektur (in diesem Fall funktionieren Transformer-Modelle besser).
2. Eine große Modellgröße.
3. Viele Fine-Tuning-Beispiele.</sample>
    <sample id="23">In der vorliegenden Studie wurde die Fähigkeit von Text-Bild-Modellen, Text darzustellen, untersucht. Es wurde festgestellt, dass diese Modelle, wie das Imagen-Modell, bei der Darstellung von Text oft fehlschlagen, insbesondere bei einfachen Texten. Die Ursache hierfür liegt in der Text-Encoder-Technologie, insbesondere bei der T5-XXL-Encoder, die Texte in Subwörter zerlegt. Die Ergebnisse zeigten, dass die T5-Modelle bei der Rechtschreibung nur unter 70% genauer sind, während die PaLM-Modelle nahezu perfekt sind, aber aufgrund ihrer großen Größe und des großen Trainingsdatenmenge nicht für viele Anwendungen geeignet sind. Die ByT5-Modelle, die individuelle Bytes des Eingabestrings verwenden, haben jedoch eine sehr gute Rechtschreibleistung. Um die Leistung der Text-Bild-Modelle zu verbessern, wurde ein neuer Ansatz vorgestellt, bei dem die Ergebnisse eines ByT5-Modells mit dem bestehenden Text-Encoder kombiniert werden. Dies führte zu einer signifikanten Verbesserung der Text-Darstellungsfähigkeit des Imagen-Modells.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem die Länge der Konjunktionen in verschiedenen Einheiten wie Wörtern, Silben und Zeichen ermittelt wurde.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem man Statistiken über Koordination aus der erweiterten Version des Penn Treebank extrahiert und die Beziehungen zwischen der Position des Begrenzers und der Länge der Konjunkte analysierte. Insbesondere wurden die Fälle untersucht, in denen der Begrenzer links oder rechts steht, sowie die Fälle, in denen er fehlt (z.B. bei Koordination von Verben).</sample>
    <sample id="26">Der Basisklassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde, leistete sich nicht viel besser als Zufall.</sample>
    <sample id="27">Es ist nicht klar, wie viele Autoren an der Arbeit beteiligt sind, da dies im vorgelegten Text nicht erwähnt wird.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Bei den Diskursphänomenen Formuliertheit und lexikalischem Zusammenhalt schneiden kontextsensitive Modelle besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">Our paper, "LLM-Blender", introduces a simple yet effective ensemble learning framework for large language models. We propose a two-stage framework that combines pairwise ranking and generative fusion to select and generate better output than using a single model. Our key idea is based on the observation that the optimal selection of models can vary across different input examples. We present a pairwise ranking module, PairRanker, that compares all candidate models and outputs a ranking. In the second stage, we use the top-ranked candidates as input to a sequence-to-sequence model for generated fusion. Our experiments show that PairRanker is better correlated with the oracle ranking than prior methods and that our full Blender framework outperforms top models like Open Assistant and Vicuna in 68% and 76% of examples, respectively. We release a unified codebase and a new dataset, MixInstruct, for evaluating large language models. Our findings suggest that LLM-Blender is a promising framework for ensemble learning, improving performance with a simple and straightforward approach.</sample>
    <sample id="31">Ich kann diese Information nicht finden.</sample>
    <sample id="33">Das Framework NLPositionality verwendet eine Pearson's R-Korrelationscore, um die Übereinstimmung zwischen den Annotationen von Endnutzern und den Vorhersagen von Modellen und den Labels von Datenbanken zu quantifizieren.</sample>
    <sample id="34">"CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" ist ein neues Werk, das die Kombination von selektiver Rationalisierung und counterfaktischer Textgenerierung vorschlägt. Das Framework besteht aus zwei Komponenten: einer Rationalisierungskomponente, die bedeutsame Rationale generiert, und einer counterfaktischen Komponente, die counterfaktische Beispiele erstellt. Die counterfaktischen Beispiele werden verwendet, um die Qualität der generierten Rationale zu bewerten. Im Vergleich zu anderen Ansätzen, wie MiCE, wurden die counterfaktischen Beispiele von CREST als valid und natürlich eingeschätzt. Darüber hinaus wird gezeigt, dass die Verwendung von CREST-counterfactuals zur Datenverstärkung zu besseren Ergebnissen führt. Ein weiterer Vorteil von CREST ist die Möglichkeit, rationale zu generieren, die sowohl auf Tatsachen als auch auf counterfaktischen Beispielen basieren. Diese Rationale sind plausibel und simulieren die Änderung der Klassifizierer-Entscheidung, wenn ein counterfaktischer Edit auf der Grundlage dieser Rationale durchgeführt wird. Insgesamt bietet CREST eine neue Möglichkeit, um die Interpretierbarkeit von KI-Modellen zu verbessern.</sample>
    <sample id="36">In der Arbeit "Learning Language-Specific Layers for Multilingual Machine Translation" präsentieren die Autoren Telmo Pessoa Pires, Robin Schmidt, Yi-Hsiu Liao und Stephan Peitz ein neues Konzept für multilinguale Maschinenerkundung. Sie stellen Language-Specific Layers (LSLs) vor, die es ermöglichen, eine reguläre Transformer-Schicht pro Sprache zu haben. Diese Schichten werden bei der Inferenzzeit ausgewählt und trainiert, wodurch die Inferenzkosten konstant bleiben. Die Autoren präsentieren auch ein Ansatz zur Platzierung der LSLs, indem sie den Modell-Weighten folgen lassen, um die beste Platzierung zu ermitteln. Die Ergebnisse zeigen signifikante Verbesserungen gegenüber dem Baseline-Modell und dem Language-Adapters-Ansatz, insbesondere für die Sprachen mit geringer Ressourcen. Die Arbeit zeigt, dass die LSLs eine effektive Möglichkeit sind, die Kapazität pro Sprache zu erhöhen, ohne die Inferenzkosten zu erhöhen.</sample>
    <sample id="37">Bei der vorherigen Studie, bei der menschliche Teilnehmende die gleichen Persona-Prompts erhalten haben, konnten Stereotypen aufgezeigt werden.</sample>
    <sample id="38">Die Datenquelle war die "enhanced Version des Penn Treebank" und es wurde auch auf die Arbeit "Why wouldn't you use universal dependencies" Bezug genommen.</sample>
    <sample id="39">Es ist nur ein Autor, Adam Przepiórkowski, der an der Arbeit beteiligt ist.</sample>
    <sample id="40">Die eng verwandten Aufgaben sind Topic-Independent Dissonance Stance Classification (Debate) und Binary Classification of Expansion and Comparison classes of PDTB (CE).</sample>
    <sample id="41">In "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", präsentieren wir ein Persona-grounded Commonsense Knowledge Graph, das die Welt-Ebene-Persona-Kenntnisse auf eine Skala darstellt. PeaCoK enthält etwa 3.800 Personen und 40.000 einzigartige Attribute, die etwa 100.000 persönliche Schlussfolgerungen oder Fakten bilden. Wir haben drei Schritte zur Erstellung von PeaCoK verwendet: 

- Auswahl von Personen aus bestehenden Commonsense-Graphen, die sowohl menschliche Rollen als auch Ereignis-basierte Entitäten enthalten.
- Induktion von Personenattributen aus Commonsense-Kenngreifen und großen Skalen-vorbereiteten Sprachmodellen.
- Crowdsourcing der Annotationen von PeaCoK-Beziehungen mithilfe eines gemeinsamen menschlich-AI-Mehrheitsstimmverfahrens.

Wir haben Comet-BART auf PeaCoK trainiert und verglichen mit großen Skalen-vorbereiteten Sprachmodellen, einschließlich GPT-3 und GPT-3,5. Die Ergebnisse zeigen, dass PeaCoK als zuverlässige Persona-Kenngreif-Base dienen kann, die leichten Sprachmodellen ermöglicht, Kenntnisse zu generieren, die denen großer Skalen-Sprachmodellen vergleichbar sind.</sample>
    <sample id="42">Es ist nicht klar, wie viele Autoren an der Arbeit beteiligt sind. Der Präsentator, Shuheng, erwähnt sich selbst, aber es wird nicht erwähnt, ob er alleine oder mit anderen Personen gearbeitet hat.</sample>
    <sample id="43">Ich habe keine Informationen über die Anzahl der Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten durch die direkte Vergleichbarkeit von Annotations-Entscheidungen von Endnutzern mit den Entscheidungen von Modellen und Datenbanken, anstatt sich auf die Vereinbarkeit zwischen verschiedenen Annotatoren oder die Modellierung von Annotator-Verteilungen zu konzentrieren.</sample>
    <sample id="45">Das Setup der generierten Personas enthält die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der Universität von Washington. Heute präsentiere ich unser Werk "Von Vortrainingsdaten zu Sprachmodellen zu downstream-Aufgaben: Verfolgen Sie die Spuren politischer Voreingenommenheiten, die zu unfaireren NLP-Modellen führen". Sprachmodelle werden auf große Web-Schnitzel-Daten trainiert. Politische Nachrichtenmedien werden in ihrem Vortrainingsdaten gut abgedeckt. Laut einer Umfrage des C4-Korpora können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in Sprachmodell-Trainingsdaten abgedeckt sind. Dies hat eine gemischte Blessung für Sprachmodell-Anwendungen geschaffen. Einerseits konnten sie sich von diversen Perspektiven lehren, was Demokratie und Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen sozial voreingenommen und können potenzielle Fairness-Probleme in downstream-Aufgaben-Anwendungen verursachen. Daher schlagen wir vor, die politische Voreingenommenheit-Verbreitungspipeline von Vortrainingsdaten zu Sprachmodellen zu downstream-Aufgaben zu untersuchen, insbesondere indem wir folgende Fragen stellen: Erstens, wie können wir die politische Neigung von Sprachmodellen bewerten und welche Rolle könnte Vortrainingsdaten dabei spielen? Zweitens, wie performen Sprachmodelle mit unterschiedlichen politischen Neigungen bei downstream-Aufgaben und ob das zu Fairness-Problemen in NLP-Anwendungen führt? Um dies zu erreichen, schlagen wir vor, Sprachmodelle mit unterschiedlichen Prompt-Formaten zu prompten, die politische Fragebögen wie den politischen Konferenz-Test verwenden. Dies stellt sicher, dass wir eine automatische Bewertung durchführen, die in der politischen Wissenschaftsliteratur gut angesiedelt ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie besetzen alle vier Quadranten des politischen Campus. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und dass GPT-Reihe allgemein sozial liberaler ist als BART-Reihe und ihre Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus Trainingsdaten aufgenommen werden. Wir könnten eine kontrollierte Studie durchführen, indem wir Sprachmodell-Checkpoints weiter trainieren, indem wir 6 verschiedene parteiische Korpora trennen, getrennt in Nachrichten und soziale Medien, weiter geteilt in ihre politische Neigung. Durch das weitere Trainieren von Sprachmodellen auf diese parteiischen Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt sich bei RoBERTa, die weiter trainiert wurde, auf dem linken Reddit-Korpus, eine beträchtliche liberale Verschiebung in Bezug auf ihre politischen Voreingenommenheiten. Wir wollen auch untersuchen, ob Sprachmodelle die Polarisation aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen daher die Vortrainingskorpora in vor und nach der Amtszeit des 45. Präsidenten der Vereinigten Staaten. Wir trennen Sprachmodelle auf die beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle eine politische Neigung haben, die sich nach 2017 weiter von der Mitte entfernt. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können. Schließlich bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei Hassrede-Detektion und Falschmeldung-Detektion, um NLP-Anwendungen zu bewerten, die oft Sprachmodelle verwenden und bedeutende Implikationen haben könnten. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, also die Leistung in verschiedenen Demografien oder politischen Neigungen von Nachrichtenmedien trennen, ein Muster auftritt. Zum Beispiel ist für die Hassrede-Detektion festzustellen, dass linken Sprachmodellen besser bei der Detektion von Hassrede gegen soziale Minderheiten gelingt, während sie bei der Detektion von Hassrede gegen mehr mächtige Gruppen in unserer Gesellschaft schlechter abschneiden. Und umgekehrt sind rechte Sprachmodelle besser bei der Detektion von Hassrede gegen Weiße und Männer, während sie bei der Detektion von Hassrede gegen schwarze LGBTQ+ und andere Minderheitengruppen schlechter abschneiden. Ähnliche Trends werden auch bei der Falschmeldung-Detektion beobachtet, wo wir sehen, dass linken Sprachmodellen besser bei der Detektion von Falschmeldungen von ihren gegensätzlichen politischen Neigungen gelingt, während umgekehrt rechte Sprachmodelle besser abschneiden. Wir zeigen auch viele qualitative Beispiele, um zu zeigen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen für Hassrede- und Falschmeldung-Beispiele auf der Grundlage ihrer sozialen Kategorien abgeben. Es gibt viele weitere Beispiele im Anhang, um zu zeigen, dass dies darauf hindeutet, dass es ein Fairnessproblem gibt, das sehr dringend ist, was die politischen Voreingenommenheiten von Sprachmodellen angeht. Zum Beispiel würde es bedeutete, wenn rechte Sprachmodelle bei der Falschmeldung- oder Hassrede-Detektion geschult und auf einem beliebten sozialen Medien-Plattform bereitgestellt würden, dass Menschen mit gegensätzlichen politischen Meinungen marginalisiert würden und Hassrede gegen Minderheitengruppen ungehindert auftreten würde. Dies hat die Alarmglocke geschlagen, uns zu erkennen und die Fairness-Probleme zu bekämpfen, die durch die politischen Voreingenommenheiten von Sprachmodellen entstehen. Ein bisschen Diskussion. Wir möchten auch hervorheben, dass wir die einzigartige Dilemmatik hinsichtlich der politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen der Scylla und Charybdis. Wenn wir die politischen Meinungen in den Trainingsdaten von Sprachmodellen nicht saniert, würde die Voreingenommenheit von der Vortrainingsdaten zu Sprachmodellen zu downstream-Aufgaben weitergegeben, was letztendlich Fairness-Probleme verursacht. Wenn wir versuchen, sie irgendwie zu saniert, würden wir auch Zensur oder Exklusion riskieren. Und es ist sehr schwierig, zu bestimmen, was tatsächlich neutral und in den Trainingsdaten von Sprachmodellen behalten werden sollte. Es ist wie das elektrische Trolley-Problem. Okay, großartig. Ich denke, das ist alles, was ich heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">Es ist nicht klar, wie viele Autoren an der Arbeit beteiligt sind, da David Vilar nur von "meinen Kollegen" spricht. Es wird jedoch erwähnt, dass es eine Zusammenarbeit mit Google Translate gibt.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Tokens durchgeführt.</sample>
    <sample id="50">Regina Stodden und Omar präsentieren das neue Corpus DEPLAIN für die Identifizierung von Texten auf Dokument- und Sätzeniveau in deutscher Sprache. DEPLAIN besteht aus zwei Subcorpora: DEPLAIN-apa, das auf Nachrichtentexten basiert und 13.000 parallelisierte Sätze enthält, und DEPLAIN-web, das verschiedene Domänen abdeckt und 30.450 Sätze umfasst. Das Corpus wurde manuell und mit automatischen Alignment-Methoden erstellt. Die Analyse der Sätze zeigt, dass die Bibel-Texte stark vereinfacht sind, während Nachrichtentexte und Sprachlernertexte weniger stark vereinfacht sind. Omar diskutiert die möglichen Anwendungsbereiche von DEPLAIN, darunter die Bewertung automatischer Alignment-Methoden und die automatische Textvereinfachung durch Fine-Tuning von Sprachmodellen. Die Ergebnisse zeigen, dass die Methode MASSalign die beste automatische Alignment-Methode für die deutsche Textvereinfachung ist und dass die Fine-Tuning von Sprachmodellen verbesserte Ergebnisse liefern kann.</sample>
    <sample id="51">Die drei Domains, die in dem Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität wird als die Perspektiven definiert, die Menschen aufgrund ihrer Demographie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Our work, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge," presents a novel approach to detecting cognitive dissonance in language. Cognitive dissonance occurs when two beliefs or actions are inconsistent, and it is a crucial phenomenon to study in language. However, dissonance is rare to find expressed in language, making it a challenging task. To alleviate this, we employed transfer learning and active learning to annotate and collect dissonant samples more efficiently. We transferred weights from closely related tasks, such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes, to improve the initial model's performance. We also developed a Probability-of-Rare-Class (PRC) strategy to select examples that are highly likely to be dissonant. Our results show that PRC outperforms other state-of-the-art active learning strategies, achieving an AUC of 0.75 on the dissonance classification task. We also found that iterative update is useful for transfer learning from a different domain, while cumulative update is beneficial for in-domain active annotations. Our work provides a significant step towards creating a cognitive dissonance resource and has implications for understanding the effects of disagreement among people, tracking trends and belief values, and attitude changes in populations.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, ohne dass es retrainiert oder eine spezielle Architektur für SimulST benötigt.</sample>
    <sample id="56">Es wird nicht explizit erwähnt, wie viele Autoren an der Arbeit beteiligt sind. Der Name des Autors Yusen Zhang wird jedoch genannt.</sample>
    <sample id="57">Ja, das getestete Modell funktioniert in der Testsuite, wenn es mit task-spezifischer Ausbildung trainiert wurde. Es zeigt jedoch Schwächen bei der Integration von Hintergrundwissen, das nur während der Inferenzzeit verfügbar ist.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. Background-Pretrain: Hintergrundwissen ist verfügbar, aber nur im Vorkompilierungszeitpunkt.
2. Background-Both: Hintergrundwissen ist verfügbar, sowohl im Vorkompilierungszeitpunkt als auch bei der Inferenzzeit.
3. Background-Inference: Hintergrundwissen ist nur bei der Inferenzzeit verfügbar.</sample>
    <sample id="59">Title: DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains

Abstract:

We present DrBERT, the first open-source pre-trained model in French for biomedical and clinical domains. DrBERT is based on RoBERTa and trained on NACHOS, a dataset of medical crawled data from the web. We compare DrBERT with other models, including ChuBERT, a clinical model based on anonymized data from the Nantes University Hospital data warehouse. We also investigate the impact of pre-training strategy and data size on model performance. Our experiments involve 11 biomedical and clinical downstream tasks in French, including named entity recognition, classification, and question answering. The results show that DrBERT outperforms other models on most tasks, especially when trained on data from the same domain. We also observe that using more data and specialized data leads to better performance. Our model is freely available on Hugging Face and GitHub, and we conclude that DrBERT is a robust and versatile tool for natural language processing tasks in French biomedical and clinical domains.</sample>
    <sample id="60">Ich kann keine genauen Informationen über die Universitäten der Autoren finden.</sample>
    <sample id="61">Die abschließenden Forschungsfragen sind nicht explizit angegeben, aber die drei Hauptfragen, die im Beitrag angesprochen werden, sind:

1. Ist sauberes Validierungsdaten notwendig für WSL oder kann ein lauter Validierungsset verwendet werden?
2. Wenn sauberes Daten erforderlich ist, wie viele saubere Proben benötigen wir?
3. Sollten saubere Proben nur für die Validierung verwendet werden oder gibt es bessere Wege, sie zu nutzen?

Diese Fragen werden jedoch nicht als abschließende Forschungsfragen bezeichnet, sondern eher als Anlass für die im Beitrag vorgestellten Ergebnisse und Empfehlungen.</sample>
    <sample id="62">In der Arbeit "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" wird die Komprimierung von großen Sprachmodellen für die natürliche Sprachgenerierung (NLG) untersucht. Die Autoren, Nitay Calderon, Amir und Subhabrata von Microsoft sowie Roi, Ziel ist es, die Leistung der komprimierten Modelle zu erhalten, während die Komplexität und der Rechenzeitbedarf reduziert werden. Dazu werden verschiedene Ansätze für die Komprimierung und die Wissensübertragung von einem großen Lehrermodell zu einem kleineren Schülermodell untersucht.

Die Arbeit konzentriert sich auf vier NLG-Aufgaben: Zusammenfassung, Fragen generieren, gemeinsames Verständnis und Vereinfachung und Stiltransfer. Die Ergebnisse zeigen, dass die Verwendung von Pseudo-Zielen, die von einem Lehrermodell generiert werden, die Leistung des Schülermodells verbessern kann. Außerdem wird ein neuer Ansatz namens Joint-Teaching vorgestellt, der es dem Schülermodell ermöglicht, seine eigenen Fehler zu korrigieren und mehr diverses Wissen vom Lehrermodell zu übernehmen.

Die Arbeit leistet einen Beitrag zur Entwicklung von effizienten und leistungsstarken NLG-Modellen und bietet eine umfassende Analyse der Komprimierung und Wissensübertragung in der NLG.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, bei gleichbleibender Aufgabenstellung trotz leichter Variationen in der Anweisungsgeschwindigkeit konsistente Ergebnisse zu liefern.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet in diesem Zusammenhang, dass das Modell empfindlicher auf kleine Änderungen in der Wortwahl der Anweisungen reagiert, was in der Regel eine schlechtere Leistung bedeutet.</sample>
    <sample id="66">Title: Deep Learning for Mathematical Reasoning: A Survey

Abstract:

Mathematical reasoning is a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numerical data and language. Recent years have seen a surge of interest in developing machines capable of solving math problems and proving theorems. This survey discusses the task of mathematical reasoning and the development of deep learning methods. We explore two primary categories: visual contexts (e.g., geometric problems) and tabular contexts. Automated theorem proving is another important line of mathematical reasoning. Various neural network architectures have been proposed for mathematical reasoning tasks, including sequence-to-sequence models and sequence-to-tree models. Pre-trained language models, such as large language models (LLMs), have demonstrated remarkable performance on NLP tasks and can be applied to solve math word problems. However, LLMs face limitations, including the lack of precise mathematical reasoning. To address this, researchers have proposed novel approaches, including self-consistency and program-aided LLMs. Despite progress, mathematical reasoning in low-resource settings remains underexplored, and learning models often display generalization and robustness failures.</sample>
    <sample id="67">In diesem Werk untersuchen wir die Faktoren, die das Auftreten von Interferenz oder Synergien bei multilingualen Übersetzungsmodellen beeinflussen. Wir stellen fest, dass schwere Interferenz in kleinen Modellen auftritt, wenn diese mit großen Datenmengen konfrontiert werden. Die wichtigste Lösung, um Interferenz zu reduzieren, ist die Anpassung der Sampling-Temperatur. Wir finden, dass die Anzahl der Sprachen und die Ähnlichkeit zwischen Sprachen nur einen geringen Einfluss auf die Interferenz haben. Stattdessen ist die Modell- und Datenmenge der Schlüssel zum Verständnis der Interferenz. Durch Anpassung der Sampling-Temperatur können wir die Interferenz in multilingualen Übersetzungsmodellen signifikant reduzieren, ohne spezielle Methoden anwenden zu müssen. Unsere Ergebnisse zeigen, dass ein modulares Ansatz, der die Modell- und Datenmenge sowie die Sampling-Temperatur berücksichtigt, effektiv ist, um Interferenz in multilingualen Übersetzungsmodellen zu reduzieren.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen linguistischen Kontext aus dem BLiMP- und SyntaxGym-Datensatz, sowie aus Wikipedia.</sample>
    <sample id="69">Typischerweise werden etwa 20 Beispiele pro Klasse benötigt, um eine gute Leistung bei der WSL zu erreichen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">Javad Hosseini und seine Kollegen haben das AltEntities Corpus entwickelt, um das Verständnis von Benutzern für indirekte Referenzierung in konversationellen Systemen zu verbessern. Das Corpus enthält 6.000 alternative Fragen in drei Domänen (Musik, Bücher, Rezepte) und 42.000 indirekte Referenzierungen. Die Fragen werden durch eine Cartoon-Completion-Methode erstellt, bei der ein Benutzer eine Frage stellt und ein anderer Benutzer eine indirekte Referenzierung verwendet, um die Antwort zu bestimmen. Die Ergebnisse zeigen, dass ein T5-XL-Modell eine hohe Genauigkeit von 92-95% erreicht, wenn es Zugang zu demselben Hintergrundwissen wie die Annotatoren hat. Wenn das Modell jedoch nur Zugang zu Teilen des Hintergrundwissens hat, fällt die Genauigkeit auf 82-87% ab. Die Ergebnisse zeigen auch, dass das Modell in verschiedenen Domänen generalisierbar ist. Das AltEntities Corpus bietet eine umfassende Plattform für die Bewertung der Fähigkeit von konversationellen Systemen, indirekte Referenzierungen zu verstehen und zu bearbeiten.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil große Sprachmodelle auf pretrainierten Daten trainiert werden, die stark von politischen Meinungen geprägt sind, was zu Fairness-Problemen in NLP-Anwendungen führen kann.</sample>
    <sample id="73">Der/die Referent*in ist Akshatha.</sample>
    <sample id="74">Title: Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths

Abstract:

We present Dense-ATOMIC, a densely-connected commonsense knowledge graph that addresses the limitations of the existing ATOMIC knowledge base. ATOMIC lacks multi-hop paths due to its sparse graph structure and limited semantic information. To overcome these limitations, we propose Rel-CSKGC, a relation prediction model that utilizes pre-trained language models to encode head and tail events. We design an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. Extensive evaluations demonstrate that Dense-ATOMIC outperforms ATOMIC in knowledge coverage and multi-hop paths, and improves the performance of COMET. We also show that Dense-ATOMIC enables more diversified results and better multi-hop path aggregation. Our approach has the potential to enhance commonsense reasoning and facilitate more accurate machine learning models.</sample>
    <sample id="75">Title: Jointprop: A Joint Semi-Supervised Learning Framework for Named Entity Recognition and Relation Extraction

Abstract:

This paper presents Jointprop, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE). The framework leverages the interconnections between NER and RE tasks to improve performance. Jointprop consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The framework propagates labels over a heterogeneous graph, considering inter- and intra-connections among labeled and unlabeled data. Experimental results on four datasets show that joint learning of NER and RE tasks benefits from codependency between the tasks in joint datasets. For single-task datasets, Jointprop demonstrates significant and consistent improvement over baselines for both NER and RE tasks. The framework's ability to fully integrate information and infer correct labels makes it a promising approach for semi-supervised joint entity and relation extraction.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Schritten:

1. **Vorabtrainingsdaten**: Sprachmodelle werden auf großen Webcrawl-Daten trainiert, die politische Nachrichtenmedien enthalten, wie z.B. New York Times, Los Angeles Times, The Guardian und Huffington Post.
2. **Sprachmodelle**: Die Sprachmodelle lernen aus den Vorabtrainingsdaten und entwickeln politische Vorurteile, die in den Daten vorhanden sind.
3. **Downstream-Tasks**: Die Sprachmodelle werden auf Downstream-Tasks wie Hate Speech-Detektion und Fake News-Detektion angewendet, wobei die politischen Vorurteile in den Ergebnissen auftreten können.</sample>
    <sample id="77">In der Präsentation "On Improving Summarization Factual Consistency from Natural Language Feedback" wird ein neues Dataset, DeFacto, vorgestellt, das für die Verbesserung der Faktenkonsistenz von Zusammenfassungen entwickelt wurde. Das Dataset enthält menschliche Demonstrationen und Feedback, um die Fähigkeit von Modellen zu verbessern, Faktenkonsistenz in Zusammenfassungen sicherzustellen. Dazu wurden drei neue NLG-Aufgaben definiert: Summary Editing, Feedback Generation und automatische Korrektur von Faktenfehlern. Die Ergebnisse zeigen, dass die Fine-Tuning-Modelle und die großen Sprachmodelle die menschliche Rückmeldung effektiv nutzen können, um die Zusammenfassungen zu verbessern. Die DeFacto-Datenbank bietet ein Testfeld für die vorgeschlagenen NLG-Aufgaben und kann auch für die Entwicklung von Faktenmetriken und -Meta-Evaluierung verwendet werden. Die Datenbank ist auf GitHub verfügbar und bietet einen wertvollen Beitrag zur Verbesserung der Faktenkonsistenz in Zusammenfassungen.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-apa und Web. Im DEPLAIN-apa-Korpus werden insbesondere Reorderings und Wortadditionen verwendet, während im DEPLAIN-web-Korpus mehr Rephrasings vorkommen.</sample>
    <sample id="79">Nein, es wird nicht explizit erwähnt, dass CoScript öffentlich verfügbar ist. Es wird jedoch erwähnt, dass das Papier mehr Details zu CoScript enthält, was darauf schließen lässt, dass CoScript möglicherweise in einem wissenschaftlichen Kontext oder durch die Veröffentlichung des Papiers zugänglich wird.</sample>
    <sample id="80">Das Wasserzeichen wird durch eine Gewichtssummation des Ziel-Embeddings und des ursprünglichen Embeddings eingebettet. Der Gewicht des Ziel-Embeddings ist proportional zur Anzahl der in der Eingabe-Satz angezeigten Trigger.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">Title: Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring

Abstract:

Automated Essay Scoring (AES) aims to evaluate essay quality without human intervention. Traditional supervised AES models require large labeled corpora, which is time-consuming and labor-intensive. Unsupervised AES can alleviate this issue, but existing methods have limitations. Our proposed framework, Unsupervised Learning from Rank Aggregation (ULRA), addresses this challenge by aggregating multiple heuristic quality signals as pseudo-groundtruth to train a neural AES model. ULRA consists of two modules: a Heuristic Essay Ranking module (HER) generates partial-order pairs from multiple quality signals, and a Deep Pairwise Rank Aggregation module (DPRA) trains a neural AES model by aggregating these pairs. We designed a Deep Pairwise Rank Aggregation loss to address conflicts among signals and a Scoring Strategy to transform predicted scores into a pre-defined score set. Experimental results show that ULRA outperforms unsupervised baselines and achieves competitive performance compared to cross-prompt and one-shot methods. While still lower than supervised methods, ULRA demonstrates the effectiveness of unsupervised essay scoring.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von Sprachen verbessert werden. In der Studie wurde festgestellt, dass die Leistung von mT5 und XLM-R + PTR auf multilingualer Ebene verbessert werden kann, indem sie auf einer Mischung von Sprachen trainiert werden.</sample>
    <sample id="84">Title: PAD-Net: An Efficient Framework for Dynamic Networks

Abstract:

Dynamic networks have gained attention due to their ability to adapt to input values by changing their architecture or parameters. However, fully dynamic networks often result in excessive parameter usage, limiting their applicability. This paper proposes PAD-Net, a partially dynamic network framework that partitions parameters into dynamic and static modes, allowing for more efficient use of resources. The framework employs Iterative Mode Partition to identify redundant dynamic parameters, which are then transformed into static parameters. Experiments demonstrate that PAD-Net outperforms static and fully dynamic networks in terms of accuracy and parameter efficiency. Ablation studies reveal the importance of dynamic ratios and scale factors in achieving optimal performance. Compared to network pruning, PAD-Net maintains better performance while preserving static parameters. The framework also enhances output discriminability, contributing to its superior performance. Future work includes extending PAD-Net to other mainstream networks and exploring hardware-friendly structured manners.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist "ein Schokoladenkuchen backen", da es hierbei um die Planung eines spezifischen Ziels mit bestimmten Einschränkungen (Schokolade) geht.</sample>
    <sample id="86">Die Opazität der Methode wird durch die Visualisierung der Embeddings von Sätzen auf vier Datenbanken (AG News, MIND, SST2 und Enron Spam) überprüft. Die Ergebnisse zeigen, dass es schwierig ist, zwischen den backdoor-Embeddings und normalen Embeddings zu unterscheiden, was auf die Covertness der Methode hinweist.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, indem sie ihre Gewichte und Tokenisierung verwendet, um ein neues PLM aufzubauen. Dazu werden drei Modelle auf kontinuierliche Pre-Training trainiert: 

- Eines basiert auf den Gewichten von CamemBERT und wird auf 4 GB NACHOS trainiert.
- Ein anderes basiert auch auf CamemBERT, aber wird diesmal auf 4 GB klinischer Notizen trainiert.
- Ein drittes basiert auf dem englischen Biomedizinmodell PubMedBERT und wird auf 4 GB NACHOS trainiert.</sample>
    <sample id="88">Die Präsentation erwähnt nicht explizit, auf welches Land GPT-4 am wenigsten ausgerichtet ist. Es wird jedoch erwähnt, dass GPT-4 am wenigsten ausgerichtet ist zu Menschen, die nicht binär sind, verglichen mit Männern und Frauen.</sample>
    <sample id="89">Der Beispielsatz "I'm going to talk about..." zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde. Wenn das Modell die Übersetzung in Deutsch vorhersagt, zeigt das Cross-Attention-Weight, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes und das letzte Wort auf die letzten empfangenen Sprachframes zeigen. Dies bedeutet, dass die ersten beiden Wörter ausgestoßen werden, während das letzte Wort aufgrund der Summe der Cross-Attention-Werte (die unter dem Schwellenwert alpha liegt) nicht ausgestoßen wird.</sample>
    <sample id="90">In the paper "Rethinking Annotation: Can Language Learners Contribute?", Haneul Yoo and colleagues question the necessity of recruiting native speakers for data annotation in NLP. They conducted a proof-of-concept study to examine the feasibility of using language learners as annotators. The study targeted three languages (English, Korean, and Indonesian) and four tasks from the GLUE benchmark. Language learners were categorized into three levels based on their proficiency and divided into two groups with different additional resources. The results show that labels annotated by language learners are nearly accurate, especially for simpler tasks, and that they are almost on par with native speakers when aggregated by majority voting. Furthermore, language models trained on learners' annotations achieved high performance, sometimes outperforming models trained on native speakers' labels. The study also observed improvements in learners' language proficiency and vocabulary as they carried out annotation tasks. The findings suggest that language learners can contribute to NLP annotations and propose a novel way for data construction in low-resource languages.</sample>
    <sample id="91">Die Anzahl der Aufgaben wirkt sich positiv auf die Leistung des Modells aus. Je mehr Aufgaben trainiert werden, desto besser wird die Leistung des Modells und desto niedriger wird die Sensitivität des Modells.</sample>
    <sample id="92">Die drei baumlosen Baseline-Modelle, mit denen die Autoren ihre Methode vergleichen, sind nicht explizit genannt. Es wird jedoch erwähnt, dass sie sich mit anderen "treeless models" vergleichen, aber die genauen Modelle sind nicht aufgeführt.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Berater des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Title: Protecting the Copyright of Large Language Models via Backdoor Watermarking for Embedding as Services

Abstract:

As large language models like GPT, LLAMA, and PALM become increasingly popular for natural language understanding and generation, embedding as services has emerged as a valuable application. However, recent studies have shown that attackers can steal models by learning from embeddings, highlighting the need for copyright protection. To address this, we propose Embedding Marker, a backdoor-based watermark method for embedding as services. Our approach involves two main steps: watermark injection and copyright verification. In watermark injection, a trigger set is selected, and the provider service modifies the embedding based on the number of triggers in a user's input sentence. In copyright verification, a backdoor dataset is constructed, and the provider requests embeddings from the stealer's service, computing similarity metrics to detect the presence of the watermark. Experimental results on four datasets demonstrate the effectiveness of Embedding Marker in detecting copyright infringement while maintaining utility for downstream tasks. Our method is covert, transferable, and applicable to embedding as services, making it a valuable solution for protecting the copyright of large language models.</sample>
    <sample id="95">Ich konnte keinen Hinweis auf den ersten Autor von PaLM finden. Es wird erwähnt, dass David Vilar gemeinsam mit Kollegen von Google Translate an dem Paper "Prompting PaLM for Translation: Assessing Strategies and Performance" gearbeitet hat, aber es wird nicht erwähnt, dass er der erste Autor ist.</sample>
    <sample id="96">Hallo alle. Ich bin Jenny, ein erstes Jahr Ph.D.-Student an der Carnegie Mellon University und heute werde ich Ihre Arbeit "NLPositionality" vorstellen, die sich mit den Designfehlern von Datenbanken und Modellen beschäftigt. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen von der Universität Washington und dem Allen Institute for AI, insbesondere Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap, durchgeführt.

Lassen Sie uns beginnen, indem wir uns vorstellen, dass wir für eine Zeitung arbeiten und uns durch die Kommentare unter unserem Artikel arbeiten, um toxische Inhalte zu entfernen. Wir könnten uns dann auf eine beliebte API wie Prospective API für die Toxizitätsdetektion verlassen. Und das funktioniert wirklich gut, wenn man Carl Jones ist. Prospective API kann korrekt toxische Beispiele erkennen. Aber das ist nicht der Fall, wenn es um Aditya Sharma geht. Prospective API ist hier nicht so sensibel für offensichtliche Begriffe, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Designfehler, bei dem wir systematische Leistungsdifferenzen von Technologien zwischen Bevölkerungsgruppen sehen. Designfehler wie der, den wir gerade gesehen haben, können durch die Positionalität der NLP-Forscher und -Entwickler auftreten. Positionalität bezeichnet einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queer akademischen Räumen, weit verbreitet ist. Und als Forscher kann Positionalität das Forschungsverfahren und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher ändern kann.

Eine Frage, die man stellen könnte, ist, ob Datenbanken und Modelle Positionalität haben. Wir wollen nicht sagen, dass Modelle oder Datenbanken selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen von echten Menschen und können daher bestimmte Positionalitäten über andere darstellen.

Frühere Arbeiten haben einige anekdotische Beweise für Positionalität vorgelegt, wie kulturelle Lücken zwischen Modellen und Datenbanken sowie theoretische Definitionen der Modellpositionalität. Allerdings haben diese Arbeiten keine Vergleiche zwischen Endnutzern und Datenbanken und Modellen durchgeführt und die Modell- und Datenbankpositionalität untersucht. Da NLP-Aufgaben zunehmend subjektiv und sozial orientiert werden, ist es jedoch wichtig, herauszufinden, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.

Um die Datenbank- und Modellpositionalität zu untersuchen, vergleichen wir daher die Annotationen mit echten Nutzern mit bestehenden Datenbanken und Modellen. Wir tun dies durch unser Framework NLPositionality. Unser Framework besteht aus zwei Hauptschritten. Der erste Schritt besteht darin, Datenbanken mit diversen Annotatoren neu zu annotieren. Wir tun dies, indem wir die Demografien der ursprünglichen Datenbanken-Annotatoren ignorieren, da normalerweise nur wenige Annotatoren jedes Beispiel annotieren und die Demografien selten erhoben und geteilt werden. Wir wählen daher die Neuannotierung, um viele Annotatoren für jedes Beispiel zu erhalten und eine reiche Demografiedatenbasis zu haben. Wir nehmen dann die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datenbanken mithilfe eines Pearson-R-Korrelationskoeffizienten. Unser Framework unterscheidet sich daher von der Literatur zur Annotator-Disagreement, indem es Endnutzer mit Modellen und Datenbanken, Vorhersagen und Etiketten vergleicht, anstatt nur Annotator-Übereinstimmung oder Annotator-Verteilung zu betrachten.

Unsere Methode wird durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für HCI-Kollaborateure ermöglicht. Lab in the Wild ist eine Online-Experimentierplattform, mit der wir Diverse Freiwillige rekrutieren können. Im Gegensatz zu Plattformen wie M Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, kann Lab in the Wild auch hohe Qualität der Daten liefern. Wir haben zwei Aufgaben auf Lab in the Wild gehostet, nämlich die soziale Akzeptanz und die Art und Weise, wie dies funktioniert. Die Teilnehmer lesen eine Situation aus dem Social Chemistry-Dataset und schreiben dann, wie sozial akzeptabel sie diese Situation finden. Um die Teilnehmer in der Studie zu halten, können sie ihre Antworten mit denen eines AI und anderer vergleichen. Wir haben dann diese Annotationen mit Social Chemistry, Delphi und GPT 4 verglichen. Wir haben dann eine ähnliche Vorgehensweise für die Toxizitäts- und Hassrede-Detektionsaufgabe wiederholt, bei der die Teilnehmer ein Beispiel aus dem Dynahate-Dataset lesen und schreiben, ob sie es für eine Hassrede halten. Wir haben dann diese Annotationen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4 verglichen. Unsere Studie hat schließlich über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern zusammengetragen.

Wir sind nun besser ausgestattet, um zu beantworten, mit welchen Bevölkerungsgruppen NLP-Datenbanken und Modelle am meisten übereinstimmen. Wir finden, dass es Positionalität in NLP gibt. Zum Beispiel finden wir, dass Datenbanken und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. So finden wir, dass GPT 4 in der sozialen Akzeptanz-Analyse am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir finden auch, dass Dynahate am meisten mit englischsprachigen Ländern übereinstimmt. Wir finden auch, dass es eine zusätzliche Übereinstimmung mit Menschen gibt, die eine Hochschulausbildung haben. So finden wir, dass GPT 4 in der sozialen Akzeptanz-Analyse am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung oder ein Studium an einer Hochschule haben, und wir finden das Gleiche für Dynahate, das am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben.

Wenn jedoch Modelle und Datenbanken sich auf bestimmte Bevölkerungsgruppen einstellen, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datenbanken und Modelle weniger mit nicht-binären Personen übereinstimmen als mit Männern und Frauen. Wir finden dies in der GPT 4-Sozial-Acceptanz-Analyse sowie in der Dynahate-Analyse.

Da es Positionalität in NLP gibt, was können wir tun? Wir haben einige Empfehlungen für diese. Zuerst empfehlen wir, alle relevanten Designentscheidungen während des Forschungsprozesses zu dokumentieren. Als zweites empfehlen wir, NLP-Forschung mit dem Perspektivismus-Lens zu betrachten. Als drittes empfehlen wir, spezialisierte Datenbanken und Modelle in vier spezifischen Gemeinschaften zu erstellen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für jeden funktionieren. Und so schließt sich unsere Präsentation. Wenn Sie jedoch mehr erfahren möchten, können Sie gerne unsere Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf folgende Probleme von SimulST ein:

1. Spezifische Architekturen werden üblicherweise trainiert, was zu zusätzlichen Modulen führt, die optimiert werden müssen.
2. Lange und komplizierte Trainingsverfahren, z.B. Trainings mit unterschiedlichen Optimierungszielen.
3. Die Notwendigkeit, mehrere Modelle zu trainieren und zu unterhalten, um verschiedene Latenzregime zu erreichen.</sample>
    <sample id="98">Eine effektive Reduzierung sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist ein komplexes Problem. Einige mögliche Ansätze könnten sein:

1. **Datensatzsanierung**: Die Entfernung von Beispielen, die soziale oder politische Verzerrungen aufweisen, kann helfen, diese Verzerrungen zu reduzieren.
2. **Datensatzaugmentation**: Die Ergänzung von Datensätzen mit Beispielen, die eine breitere Palette von Perspektiven und Meinungen abbilden, kann helfen, Verzerrungen zu reduzieren.
3. **Regulierung**: Die Einführung von Regeln oder Richtlinien, die die Verwendung von Datensätzen mit sozialen oder politischen Verzerrungen verhindern, kann helfen, diese Verzerrungen zu reduzieren.
4. **Verwendung von neutralen Datensätzen**: Die Verwendung von Datensätzen, die neutral und unparteiisch sind, kann helfen, soziale und politische Verzerrungen zu reduzieren.
5. **Regelmäßige Überprüfung und Korrektur**: Die regelmäßige Überprüfung von NLP-Modellen auf soziale und politische Verzerrungen und die Korrektur dieser Verzerrungen kann helfen, diese zu reduzieren.

Es ist jedoch wichtig zu beachten, dass die Reduzierung sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ein kontinuierlicher Prozess ist und regelmäßige Überprüfungen und Korrekturen erforderlich sind.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unser Projekt "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" vorzustellen. In unserem alltäglichen Leben planen wir unsere Aktionen oft durch die folge von Schritten in der Form von zielorientierten Skripten. Zuvor haben Forscher Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie "ein Kuchen backen" zu planen. Und haben gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings haben Zuvorige Forschungen hauptsächlich auf das Planen für abstrakte Ziele stereotypischer Aktivitäten fokussiert. Das Planen für Ziele mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", bleibt jedoch noch untersucht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das unterschiedliche Einschränkungen auf die Ziele der Planung auflegt. Ein abstraktes Ziel kann durch verschiedene reale Ziele mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu zu den Einschränkungen sind. In dieser Arbeit bewerten wir und verbessern wir zunächst die Fähigkeit der großen Sprachmodelle zur eingeschränkten Sprachplanung. Da kein Datensatz für spezifische Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielfältigen Einschränkungen für eine human-in-the-loop-Datenakquisition mit InstructGPT. Wir stichprobenartig 100 spezifische Ziele und bewerten die Skripte, die von den großen Sprachmodellen generiert wurden. Diese Tabelle zeigt die Gesamtrechtigkeit der Ergebnisse. Wir finden, dass alle Sprachmodelle unzufriedenstellende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht gewährleistet werden kann. Wir gehen in eine feinere Kategorisierung der Einschränkungstypen ein, die in wikiHow definiert sind. Die Hitze-Karte in der Abbildung zeigt, dass die Planungsfähigkeit von InstructGPT für Ziele unterschiedlicher Kategorien beträchtlich variiert. Zuvorige Studien haben gezeigt, dass die Qualität der Ausgaben von Sprachmodellen eine hohe Variabilität aufweist, was zu schlechten Leistungen führt. Daher übernehmen wir die Idee des "over-generate-then-filter" zur Verbesserung der Generationsqualität. Wir zeigen zunächst die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele auf der Grundlage der Samenabstrakten Ziele. Dann über-generieren wir K Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir wandeln Skripte und Ziele in InstructGPT-Einbettungen um und berechnen die Kosinusähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter des Ziel-Einschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel die höchste Werte in der Zielmenge erreicht. Mit unserer Methode können wir Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planfähigkeit sowohl in semantischer Vollständigkeit als auch in Treue zu den Einschränkungen. Da große Sprachmodelle kostspielig zu deployen sind, ist es wichtig, die Sprachplanungsfähigkeit kleiner und spezialisierter Modelle zu ermöglichen. Die Erstellung des Datensatzes ist jedoch ein wesentlicher Schritt zu diesem Zweck. Allerdings ermöglichen Zuvorige Studien nicht die Planung für spezifische Ziele und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdistillation, um die eingeschränkte Sprachplanungsdatenbank CoScript aus großen Sprachmodellen zu distillieren. Wir anwenden unsere Methode zur Erstellung einer Datensatz der eingeschränkten Sprachplanung, die als CoScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierung und Testmenge sicherzustellen, bitten wir Crowd-sourced-Arbeiter, die falschen Beispiele zu finden und zu korrigieren. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir finden, dass CoScript eine hohe Vielfalt in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung ausprobieren. Wir finden, dass das T5-Modell, das auf CoScript fine-tuned wurde, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle größere Modelle überbieten können, wenn sie richtig auf geeignete Datensätze trainiert werden. Insgesamt etablieren wir das Problem der eingeschränkten Sprachplanung. Wir bewerten die Fähigkeit der großen Sprachmodelle zur eingeschränkten Sprachplanung und entwickeln eine Methode des "over-generate-then-filter" für große Sprachmodelle. Wir nutzen große Sprachmodelle, um einen hochwertigen Datensatz für die eingeschränkte Sprachplanung, CoScript, zu erstellen. Wir hoffen, dass der CoScript-Datensatz ein wertvolles Ressourcen zur Fortschritt der Forschung auf dem Gebiet der Sprachplanung ist. Vielen Dank für Ihre Zeit. Sie können weitere Details von CoScript in unserem Papier finden.</sample>
    <sample id="100">Der Vortrag beschreibt ein neues Ansatz zur Multi-Hop Frage-Answering (QA) namens PromptRank, der auf wenigen Beispielen trainiert werden kann. Der Ansatz kombiniert einen ungesupervierten Retrieval-Mechanismus mit einem few-shot Language-Model-Reranker. Der Retrieval-Mechanismus verwendet TF-IDF-Retrieval und Hyperlink-Traversal, während der Reranker eine few-shot Language-Model verwendet, um die Kandidaten zu reranken. Der Vortrag zeigt, dass PromptRank mit nur 128 Beispielen eine starke few-shot Path-Retrieval-Leistung erreichen kann und dass die Likelihood des Fragen gegeben des Chains als Scoring-Funktion wirkt. Der Vortrag zeigt auch, dass die Anweisung eine starke Rolle bei der Elicitation der Argumentationsfähigkeit des Language-Models über die Chains-Dokumente spielt. PromptRank kann als Retriever verwendet werden, um die Downstream QA-Leistung zu verbessern, und zeigt eine starke Leistung im Vergleich zu vollständig gesupervierten Systemen und State-of-the-Art Multi-Hop Dense Retrievers.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar zu hochwertigen Sprachsystemen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind:

1. Anwendbarkeit auf Embedding-as-Service-Anwendungen
2. Keine Degradation der Leistungsfähigkeit der bereitgestellten Embeddings
3. Verborgenheit gegenüber Angreifern oder leicht entfernbare Wasserzeichen
4. Übertragbarkeit auf die Dienste des Angreifers während des Modell-Extraktionsprozesses</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, aber die genauen Sprachen werden nicht explizit im Text genannt. Es wird jedoch erwähnt, dass die Analyse auf 14 verschiedenen Sprachenpaaren durchgeführt wurde.</sample>
    <sample id="104">Die Anzahl der Instanzen, die für die erneute Annotierung extrahiert werden, ist nicht explizit genannt. Es wird jedoch erwähnt, dass normalerweise nur wenige Annotatoren pro Instanz arbeiten und dass die Demographie dieser Annotatoren oft nicht erhoben und geteilt wird. Daher wird die erneute Annotierung durchgeführt, um viele Annotatoren pro Instanz zu haben und eine reiche Demografiedatenbank zu erstellen.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind Cosine-Similarity und L2-Similarity.</sample>
    <sample id="106">The QUEST dataset is introduced to study the effectiveness of systems in handling selective information needs, where users express their queries with multiple constraints or preferences. The dataset includes over 3,000 entity-seeking queries with implicit set operations, verified answer entities, and marked document spans for query constraints. QUEST is constructed by performing set operations over Wikipedia category names in four domains (films, books, plants, and animals) and annotating queries for fluency, naturalness, and relevance. To evaluate systems, the dataset requires retrieving multi-answer sets from a large document corpus with implicit set constraints and evidence from multiple document parts. Baselines are established using sparse and dense retrievers, as well as a T5-based reranker. Results show a large room for improvement in retriever performance and low end-to-end system performance, with queries containing set intersection and set difference being particularly challenging. The QUEST dataset aims to help researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">Die Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe als Encoder-PTR (Multilingual Pretrained Encoders mit Pointer-based Decodern) eingesetzt, wie zum Beispiel XLM-R + PTR und mBERT + PTR.</sample>
    <sample id="108">In diesem Beitrag wird die Robustheit von Sprachmodellen bei der Beurteilung der Akzeptabilität von Sätzen in verschiedenen Kontexten untersucht. Die Autoren präsentieren eine Überarbeitung des Minimal Pair Paradigms (MPP), das ursprünglich entwickelt wurde, um Sprachmodelle auf ihre Fähigkeit zur Beurteilung der Grammatikalität von Sätzen zu testen. Die Autoren simulieren längere Sätze, indem sie akzeptable oder unakzeptable Sätze aus Datenbanken wie BLiMP oder SyntaxGym mit einem Präfix versehen, das die gleiche grammatische Struktur aufweist. Die Ergebnisse zeigen, dass Sprachmodelle bei der Beurteilung der Akzeptabilität von Sätzen in verschiedenen Kontexten sensible sind und dass das aktuelle MPP-Evaluationssystem möglicherweise nicht die abstracte Kenntnis der Modelle im Kontext eines längeren Satzes erfasst. Die Autoren schließen, dass Sprachmodelle latenten syntaktischen und semantischen Merkmalen in Sätzen empfindlich sind und dass eine Überarbeitung des MPP-Evaluationssystems erforderlich ist, um die Fähigkeiten der Modelle bei der Beurteilung der Akzeptabilität in verschiedenen Kontexten zu verbessern.</sample>
    <sample id="109">In "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" wird ein neuer Ansatz zur Erstellung von Trainingsdaten für Sprachmodelle vorgestellt. Das Team verwendet eine Variante des GPT-3-Modells, um automatisch Beispiele für natürliche Sprachanweisungen und ihre entsprechenden Eingaben und Ausgaben zu generieren. Die Daten werden ohne menschliche Annotationen gesammelt und umfassen 64.000 Beispiele, wobei die Anweisungen paraphrasiert werden, um die Vielfalt zu erhöhen. Die Analyse der generierten Beispiele zeigt, dass über 50% korrekt sind und die verbleibenden Beispiele wertvolle Informationen für die Anweisungstuning enthalten. Um die Güte des Datenbestands zu bewerten, wird ein 11-Billionen-Parameter-T5-Modell auf Unnatural Instructions trainiert und zeigt bessere Ergebnisse als T0++ und Tk-instruct auf verschiedenen Benchmarks. Der Datenbestand ist daher eine vielversprechende Alternative zu herkömmlichen Trainingsdaten und zeigt die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu generieren.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie ein allgemeines Textkorpus sammeln und die Wortfrequenz mit diesem Korpus zählen.</sample>
    <sample id="112">Hallo, mein Name ist Shuheng. Heute möchte ich unsere Arbeit "Do CoNLL-2003 Named Entity Taggers noch gut funktionieren in 2023?" vorstellen. Lassen Sie uns beginnen. Unsere Arbeit untersuchte das Problem der Generalisierung mithilfe der Named Entity Recognition Task oder der NER-Aufgabe. Wir beobachteten, dass Modelle in CoNLL-2003 verwendet wurden, um NER-Modelle für fast 20 Jahre zu entwickeln, was natürlich mehrere Probleme aufwirft. Zunächst können diese Modelle sich noch auf moderne Daten generalisieren? Und wenn wir neue Taggers entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, entwickelten wir das CoNLL++-Datensatz. Dies ist ein Datensatz, den wir aus den Reuters News von 2020 sammelten und dann mit den gleichen CoNLL-2003-Annotierungsrichtlinien annotierten. Wir feinjustierten dann über 20 Modelle auf CoNLL-2003. Wir bewerteten sie auf beiden CoNLL-03-Testsets und CoNLL++. Und zuletzt berechneten wir den prozentualen F1-Wert, um die Generalisierung jedes Modells zu bewerten. Was ist also notwendig für eine gute Generalisierung? Durch unsere Experimente fanden wir heraus, dass es drei Hauptzutaten gibt, die notwendig sind. Die erste Zutat ist die Modellarchitektur. Durch unsere Experimente fanden wir heraus, dass die Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Die zweite Zutat ist die Modellgröße. Wir fanden heraus, dass größere Modelle normalerweise zu einer besseren Generalisierung führen. Und schließlich wissen wir alle, dass die Anzahl der Fine-Tuning-Beispiele direkt den Leistungsgrad eines Downstream-Tasks beeinflusst. Hier fanden wir auch heraus, dass mehr Fine-Tuning-Beispiele auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste Hypothese ist adaptive Überanpassung, bei der die Überanpassungskosten durch die Wiederholung der gleichen Testset wiederholt werden und dies normalerweise als abnehmende Rendite auf einem neuen Testset auftritt. Die zweite Hypothese ist der zeitliche Drift, der durch den zunehmenden zeitlichen Abstand zwischen dem Trainings- und Testdatensatz verursacht wird. Zu Überanpassung fanden wir heraus, dass die rote beste Anpassungslinie in dem Diagramm rechts eine Steigung hat, die größer als eins ist. Dies bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 machten, zu mehr als einer Einheit Verbesserung auf CoNLL++ führt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird. Was also ist mit dem zeitlichen Drift? Für den zeitlichen Drift führten wir ein Experiment durch, bei dem wir einige Modelle mit neueren Daten weitertrainierten oder sie neu trainierten, und fanden heraus, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt und dies unsere Hypothese bestätigt, dass der Hauptgrund für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Fine-Tuning-Beispiele notwendig sind. Und dies geht Hand in Hand, wir können nicht einfach nur eine Zutat haben und die anderen ausschalten. Gleichzeitig fanden wir heraus, dass der Leistungsabfall durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Also, wenn wir uns auf die Frage zurückbesinnen, die wir im Titel unserer Arbeit gestellt haben, "Do CoNLL-2003 Taggers noch in 2023 funktionieren?" und wir fanden heraus, dass die Antwort tatsächlich ein lautes Ja ist. Wir hoffen, dass unsere Arbeit zu mehr Forschung über die Verbesserung der Generalisierung der Modelle anregt. Und zuletzt, bitte lesen Sie unsere Arbeit, unseren Datensatz und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">In der Präsentation "Finding the Pillars of Strength for Multi-Head Attention" wird an der Nanyang Technological University of Singapore ein neues Modell vorgestellt, das den Parameterüberlastung von großen Sprachmodellen reduziert. Das Modell, namens Grouped Head Attention (GHT), verwendet eine "Divide and Conquer"-Strategie, um die Multi-Head-Aufmerksamkeit zu komprimieren. Es besteht aus zwei Stufen: einer Gruppen-basierten Trainingsphase und einem Voting-to-Stay-Algorithmus. Im Vergleich zu anderen Ansätzen, die entweder Homogenisierung oder Diversifizierung der Aufmerksamkeit auf der Basis von Kriterien wie Similarität oder Separabilität anstreben, zeigt sich, dass GHT eine effizientere Komprimierung der Parameter erreichen kann, ohne die Leistung zu beeinträchtigen. Die Ergebnisse zeigen, dass GHT auf drei Aufgaben (Maschinelle Übersetzung, Sprachmodellierung und Abstrakte Zusammenfassung) eine signifikante Reduzierung der Parameterzahl (bis zu 90 %) erreichen kann, ohne die Leistung zu beeinträchtigen.</sample>
    <sample id="115">Bei dem Ansatz EDAtt wird die Sprachsegmentgröße als "speech chunk" bezeichnet, aber keine spezifische Größe wird genannt. Es wird jedoch erwähnt, dass die Sprachsegmente in Frames unterteilt werden, wobei "lambda speech frames" eine Rolle spielen, um die Entscheidung zu treffen, ob ein Wort ausgegeben wird oder nicht.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist "Servin ist ein Richter" und "Kea ist ein Bäcker".</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">In der Arbeit "Improving Pretraining Techniques for Code-Switched NLP" werden neue Methoden vorgestellt, um mit Code-Switching in der Naturkommunikation umzugehen. Code-Switching bezeichnet das Wechseln zwischen zwei oder mehr Sprachen innerhalb eines Satzes. Die Autoren identifizieren, dass multilinguale Vorabtrainingsmodelle wie mBERT und XLM-R bei Code-Switching-Tasks wie Frage-Antwort-Spiel und Sentiment-Analyse schlechter abschneiden. Um dieses Problem anzugehen, werden drei neue Methoden vorgestellt:

1. SwitchMLM: Ein neues Masked Language Modelling (MLM)-Objekt, das sich auf Code-Switching konzentriert. Es definiert Switch-Punkte als Übergänge zwischen Sprachen und maskiert nur diese Punkte.
2. FrequencyMLM: Ein Surrogate-Verfahren, das die negativen Log-Wahrscheinlichkeiten von Wörtern in monolingualen Korpora vergleicht, um LID-Tags (Language Identification Tags) zu assignieren.
3. ResBERT: Eine Modifikation von BERT, die Residual-Verbindungen von mittleren Schichten zu den Endschichten hinzufügt, um die Switch-Punkt-Information zu erhöhen.

Die Ergebnisse zeigen, dass die kombinierte Methode (SwitchMLM oder FrequencyMLM mit ResBERT und einem Hilfsverlust) bei der Sentiment-Analyse die besten Ergebnisse liefert. Probing-Experimente bestätigen, dass die vorgeschlagenen Methoden die Switch-Punkt-Information in den mittleren und Endschichten erhöhen.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf GPT-4, RoBERTa und BART.</sample>
    <sample id="120">Das Modell kombiniert Aufmerksamkeitswerte aus mehreren Ebenen, indem es die Cross-Attention-Mechanismus verwendet, der die Beziehungen zwischen dem Audio-Eingabe und dem textuellen Ausgang berücksichtigt.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind: 

* Die Nennung des Namens des Songs, zum Beispiel "Easy on Me"
* Die Angabe der Position des Songs, zum Beispiel "der erste"</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Title: MultiInstruct: A Large-Scale Multi-Modal Instruction Tuning Dataset for Zero-Shot Learning

Abstract:

We present MultiInstruct, the first large-scale multi-modal instruction tuning dataset, consisting of 62 diverse tasks covering 10 categories. Our dataset is derived from 21 existing open-source datasets and includes 5 expert-written instructions per task. We investigate multi-modal instruction tuning using the unified multi-modal pre-trained model OFA and demonstrate its effectiveness in zero-shot learning. Our results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks and that transfer learning from natural instruction datasets benefits instruction tuning. We also introduce a new metric called sensitivity, which measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in instruction wording. Our findings highlight the importance of multi-modal instruction tuning and the benefits of transfer learning from natural instruction datasets. We release our dataset and model, and plan to collect a larger dataset with around 150 additional vision-language tasks.</sample>
    <sample id="124">In "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models", präsentiert Tan Qingyu von der National University of Singapore und Alibaba eine umfassende Analyse der zeitlichen Verstandesfähigkeit von großen Sprachmodellen. Die Forscher definieren drei Arten von zeitlichen Verstandesfähigkeiten: Zeit-zu-Zeit-Verständnis, Zeit-zu-Ereignis-Verständnis und Ereignis-zu-Ereignis-Verständnis. Sie erstellen ein neues Benchmark-Dataset namens TempReason, das alle drei Arten der zeitlichen Verstandesfähigkeit und eine umfassende Zeitabdeckung umfasst. Um die zeitliche Verstandesfähigkeit zu verbessern, stellen sie einen neuen Trainingsansatz vor, der zwei Komponenten enthält: Temporale Span-Extraktion vor der Vorbereitung und time-sensitive Reinforcement Learning. Der vorgeschlagene Trainingsansatz, TempT5, zeigt bessere Ergebnisse als andere große Sprachmodelle, wie ChatGPT und FLAN-T5-L, bei verschiedenen zeitlichen Verstandesfähigkeiten. Die Forscher identifizieren jedoch auch einige Leistungsschwankungen von TempT5, die möglicherweise auf ein Ungleichgewicht im Trainingsdatensatz zurückzuführen sind.</sample>
    <sample id="125">Es wird nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="126">Nein, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als "Translate-Test" Setting betrachtet, aber nicht als Baseline.</sample>
    <sample id="127">"Large Language Models Are Reasoning Teachers" ist ein gemeinsames Forschungsprojekt von Namgyu Ho, Laura Schmid und Se-Young Yun, das ein neues Ansatz zur Übertragung der Fähigkeit von großen Sprachmodellen zur komplexen Argumentation auf kleinere Modelle präsentiert. Die Forscher verwenden den Begriff "Diverse Reasoning" für eine innovative Technik, bei der ein großes Modell mehrere Lösungen für ein komplexes Problem generiert, was zu besseren Ergebnissen bei der Übertragung auf ein kleineres Modell führt. Durch die Verwendung von "Diverse Reasoning" konnten die Forscher zeigen, dass kleinere Modelle (unter 1 Billion Parametern) komplex argumentieren können, was bis dato nur großen Modellen möglich war. Der Vorteil dieses Ansatzes ist, dass er kosteneffizient ist und in vielen Situationen umgesetzt werden kann. Die Forscher stellen ihre Ergebnisse auf 12 Aufgaben bereit und zeigen, dass ihr Ansatz bessere Ergebnisse liefert als bestehende Baseline-Methoden.</sample>
    <sample id="128">Title: The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

Abstract:

We present the KITMUS test, a diagnostic suite for evaluating the ability of natural language understanding models to integrate knowledge from multiple sources. Our test focuses on coreference resolution, a task that requires models to draw on both pretrain-time and inference-time knowledge. We introduce three settings: Background-Pretrain, Background-Both, and Background-Inference, which vary the availability of background knowledge. Our results show that even the best-performing models struggle to integrate knowledge from different sources without task-specific training. However, with training on KITMUS, some models successfully integrate knowledge from multiple sources. Notably, even the best-performing models face difficulties in reliably integrating background knowledge provided only at inference time. Our findings highlight the importance of task-specific training for models to effectively use knowledge from different sources. The KITMUS test and data set are available on GitHub, providing a valuable resource for researchers to evaluate and improve their models' knowledge integration capabilities.</sample>
    <sample id="129">Das Beispiel für eine markierte Gruppe ist "warrior" (Krieger), wenn es um eine Frau geht, da sie normalerweise mit Männern in Verbindung gebracht wird und daher als "Frau-Kriegerin" bezeichnet wird.</sample>
    <sample id="130">Es wird nicht explizit erwähnt, welche Modellarchitekturen schlecht generalisieren, aber es wird erwähnt, dass "normalerweise" die Transformer-Modelle besser generalisieren.</sample>
    <sample id="131">Es wird nicht explizit erwähnt, dass es sich um Testdatensätze handelt. Es wird jedoch erwähnt, dass es sich um "clean test sets" handelt, was auf Testdatensätze hindeutet.</sample>
    <sample id="132">Es gibt zwei Autoren, Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich Text, Bildern, Koordinaten von Bounding Boxes und anderen Datenarten.</sample>
    <sample id="135">Das Emory NLP Lab unter der Leitung von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI hat ein neues Dimensionssystem für die Bewertung von konversationaler AI entwickelt, das als ABC-Eval bekannt ist. Dieses System reduziert die Subjektivität von menschlicher Bewertung, indem es explizit annotiert, ob jede Antwort des Modells bestimmte Verhaltensweisen zeigt, wie zum Beispiel das Ignorieren von Informationen oder das Widersprechen von sich selbst oder seinem Partner. ABC-Eval kann die Fehlerhäufigkeit von Chat-Modellen in verschiedenen Bereichen messen, wie zum Beispiel das Ignorieren von Informationen, das Widersprechen von sich selbst oder seinem Partner, das Halluzinieren von falschen Fakten oder das Verletzen von allgemeinem Wissenssinn.

In einer Studie wurden vier state-of-the-art-Chat-Modelle mit ABC-Eval und drei existierenden Bewertungsmethoden bewertet. Die Ergebnisse zeigen, dass ABC-Eval-Labels im Vergleich zu den existierenden Methoden zuverlässiger und vorhersagungsstärker sind. ABC-Eval ermöglicht es Forschern, Chat-Modelle mit einer höheren Auflösung zu bewerten als bisherige Methoden. Die Ergebnisse dieser Studie können als Schritt in die richtige Richtung gesehen werden, um die Entwicklung von konversationaler AI zu fördern.</sample>
    <sample id="136">The paper "FERMAT: An Alternative to Accuracy for Numerical Reasoning" von Jasivan und Nafise untersucht die Leistung von Sprachmodellen bei numerischer Argumentation. Die Autoren argumentieren, dass bestehende Benchmarks, wie Accuracy und F1-Maß, nicht informativ sind, um die Stärken und Schwächen von Modellen bei mathematischen Aufgaben zu analysieren. 

Um dieses Problem anzugehen, haben die Autoren FERMAT, ein flexibles Evaluierungssystem für arithmetische Typen, entwickelt. FERMAT besteht aus mathematischen Wortfragen, die aus Illinois und CommonCore extrahiert wurden, und untersucht die Leistung von Modellen bei verschiedenen Aspekten, wie Zahlverständnis, mathematischen Operationen und Trainingsabhängigkeit.

Die Ergebnisse zeigen, dass die meisten Modelle bei der numerischen Argumentation schwach abschneiden, und dass die Leistung durch Fine-Tuning und die Verwendung von mathematischen Mustern verbessert werden kann. Die Autoren schlussfolgern, dass bestehende Benchmarks unrepräsentativ sind und dass FERMAT ein informativeres Alternativsystem bietet, um die Leistung von Modellen bei numerischer Argumentation zu analysieren.</sample>
    <sample id="137">We propose a novel task of language-guided floor plan generation, where a model learns to generate 2D floor plan designs directly from language instructions. Our task is defined as follows: given a set of language instructions describing a floor plan's intrinsic components, the model aims to generate a structured interior layout that aligns with the input language instructions. We introduce Tell2Design, a large-scale dataset featuring 5,051 human-annotated language instructions and 76,000 artificially generated instructions. Our sequence-to-sequence model, based on the transformer encoder-decoder structure, achieves the highest IoU scores (54 Micro and 53 Macro) compared to text-conditional image generation baselines. We demonstrate the effectiveness of our method in aligning with human instructions and show that artificial instructions can be beneficial in warming up the model before training on human instructions. Our work initiates research on language-guided design generation and provides a foundation for future research in this area.</sample>
    <sample id="138">Die Integration von Wissen aus verschiedenen Quellen, insbesondere das Verbinden von prätrainiertem Wissen mit Wissen, das während der Inferenceszeit zur Verfügung gestellt wird.</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Ja, CoScript wurde durch Crowd-sourced-Arbeiter überprüft und fehlerhafte Beispiele korrigiert, um die Qualität der Validierungs- und Testdaten sicherzustellen.</sample>
    <sample id="141">Die bestehenden Ressourcen für kontextbasierte Übersetzung haben folgende Grenzen:

* Sie unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen.
* Sie unterstützen nur eine begrenzte Anzahl von Sprachen.
* Sie beruhen oft auf Domänwissen und erfordern die manuelle Kurierung durch Menschen.</sample>
    <sample id="142">Hallo! Ich werde mich auf unser gemeinsames Werk "Indirekte Referenzausdrücke lösen für die Entity Auswahl" konzentrieren, in dem wir das AltEntities Corpus einführen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Überlege dir diese alternative Frage. "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Hier will ein Benutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Möglichkeit ist die direkte Referenz, zum Beispiel durch den Namen des Liedes "Easy on Me" oder seine Position, "das erste". Aber manchmal ist eine indirekte Referenz besser geeignet, um eine natürlichere Konversation zu haben. Dies kann passieren, wenn der Benutzer den Namen des Liedes nicht kennt. Oder die Aussprachen sind zu ähnlich und schwer zu entwirren. Oder wenn der Benutzer eine Vorliebe ausdrücken möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "das neue Lied" oder "das Lied, das nicht energisch ist". Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung der Entity-Verständigung von LLMs. Wir sind uns nicht bewusst, dass es ein größeres öffentlich zugängliches Datensatz für die Aufgabe gibt, also sammeln wir einen, indem wir eine Crowd-Annotierung durchführen. Unsere Datensatz deckt drei verschiedene Domänen ab: Musik, Bücher und Rezepte. Unsere Datensatzsammlungsmethode betont Informalität mithilfe eines Cartoon-Completion-Setup. Das Cartoon hat drei Sprachblasen. Im ersten Blasen sagt Bob: "Denkst du an das Lied, das wir gestern Abend hörten?" Und mit dem sagt Bob den Kontext für die Konversation. Im zweiten Sprachblasen sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Das ist die alternative Frage. Und im dritten Sprachblasen verwendet Bob eine indirekte Referenz, um eines dieser Entities auszuwählen, zum Beispiel "das neue Lied". Wir liefern die ersten beiden Sprachblasen automatisch, aber die dritte wird von dem Annotator ausgefüllt. Die erste Sprachblase wird aus wenigen manuellen Anregungen pro Domäne ausgewählt. Die zweite, die alternative Frage wird wie folgt generiert: Wir verwenden immer einen einfachen Vorlage. Meinst du A oder B? Wo A und B Proben von Wikipedia sind. Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entities immer ähnlicher und es ist normalerweise schwerer, die Entwirrung zu treffen. Die erste ist eine gleichmäßige Zufälligkeit. Die zweite ist, wenn die Entities ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel die gleiche Genre oder die gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen dieser Entities, aber sie wissen nicht unbedingt etwas über die Entities. Also zeigen wir ihnen einige Hintergrundwissen über die beiden Entities. Für Lieder zeigen wir ihnen einfach einen Google-Suchlink zu jedem Lied und bitten sie, mindestens ein Lied zu hören und über jedes Lied zu lesen. Hier ist zum Beispiel der Google-Suchresultat für das Lied "Easy on Me". Für die Domänen Rezepte und Bücher zeigen wir ihnen einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir ihnen zusätzlich Bilder von Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir sie, eine dieser Entities auszuwählen, zum Beispiel hier ist die erste und beschreiben sie mit drei bis fünf indirekten Referenzausdrücken. Zum Beispiel "das Lied mit dem Klavier". Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "das Lied ohne Worte", "nicht das Lied mit dem 12-jährigen Jungen", oder "das fiktive Lied", oder "kommt aus Aserbaidschan", usw. Das AltEntities Corpus hat 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5 XL Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugriff auf das genaue gleiche Hintergrundwissen wie die Annotatoren hat, dann ist die Genauigkeit sehr hoch, also 92 bis 95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugriff auf einige überlappende Hintergrundwissen hat, dann ist die Genauigkeit zwischen 82 und 87%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugriff auf die Entity-Namen hat, dann ist die Genauigkeit nur 60%, also gibt es noch viel Platz für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlinien "Wait-k-Strategie" und "Local Agreement" verglichen. Außerdem wird er mit dem state-of-the-art-Architektur speziell für Simultaneous Speech Translation verglichen.</sample>
    <sample id="144">Die Autoren gehören der Universität von Nantes an.</sample>
    <sample id="145">Die Referent*in heißt Jenny, ein erstes Jahr PhD Student an der Carnegie Mellon University.</sample>
    <sample id="146">In der Präsentation wird die Omission in der Dialogsummarisierung analysiert, die ein wichtiger Faktor für die Qualität von Summarien ist. Die Ergebnisse zeigen, dass selbst state-of-the-art-Modelle eine hohe Omission-Rate von etwa 70% aufweisen. Um die Omission zu überwinden, wurde das OLDS-Dataset erstellt, das für die Omissionserkennung verwendet wird. Drei Baseline-Modelle wurden entwickelt, um die Omissionserkennung zu evaluieren. Die Ergebnisse zeigen, dass die Omissionserkennung ein herausforderndes Problem ist, aber die Leistung kann durch die Verwendung der Omission zur Summarisierungsverbesserung verbessert werden. Die Ergebnisse zeigen, dass die Omissionserkennung ein wertvolles Ziel ist und die Verbesserung der Summarisierungsqualität durch die Verwendung der Omission ein vielversprechender Ansatz ist. Die Präsentation schließt mit der Aussage, dass die Omissionserkennung ein wichtiger Schritt zur Verbesserung der Qualität von Dialogsummarisierungen ist.</sample>
    <sample id="147">Es sind drei Autoren an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Foundazione Bruno Kessler und ich werde Ihnen kurz den "Attention als Leitfaden für simultane Sprachübersetzung" Artikel vorstellen, der ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SimulST, ist der Prozess der Übersetzung von gesprochenem Text in einen Text in einer anderen Sprache in Echtzeit, um eine Kreuzsprachkommunikation zu ermöglichen. Und welche Probleme haben die aktuellen SimulST-Modelle? Spezifische Architekturen werden normalerweise trainiert, wodurch zusätzliche Module optimiert werden müssen. Längliche und komplizierte Trainingsverfahren, zum Beispiel Trainingsverfahren, die verschiedene Optimierungsziele beinhalten. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen Modells mit einer Latenz von zwei Sekunden und so weiter. Also, was ist unsere Lösung? Zuerst verwenden wir bestehende offline-Übersetzungsmodelle ohne Neustrukturierung oder spezifische Architekturen für SimulST. Verwenden Sie nur ein Modell für jeden Latenzregime und handeln Sie die Latenz durch spezifische Parameter ab. Und nutzen Sie das bereits erworbenen Wissen des Modells durch die Aufmerksamkeitsmechanik zwischen Audioeingabe und textuellem Output. Das ist die Kreuzaufmerksamkeitsmechanik, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist die Einführung von EDAtt, oder Encoder-Decoder Aufmerksamkeit, und es ist eine Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung ausgeben oder nicht, basierend darauf, wohin die Aufmerksamkeit weist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, also wenn ihre Summe unter einem bestimmten Schwellenwert Alpha gegenüber den letzten Lambda-Sprachframes liegt, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel wenn wir einen Sprachblock mit "Ich gehe über..." empfangen und unser Modell die Übersetzung in Deutsch vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten zwei Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Sprachframes zeigt. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert Alpha liegt, werden wir das letzte Wort nicht ausgeben und warten auf den nächsten Sprachblock. Wenn wir weitermachen und einen weiteren Sprachblock empfangen, und unser Modell andere drei Wörter vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda-Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir die simultane Sprachübersetzungsergebnisse in Graphen darstellen, in denen wir BLEU auf einer Seite haben, das die Übersetzungsgenauigkeit misst, und die durchschnittliche Verzögerung auf der anderen Seite, die die Latenzmessung ist. Wir nehmen auch die computergesteuerte durchschnittliche Verzögerung in Betracht, die die Zeit berücksichtigt, die das Modell benötigt, um die Ausgabe vorherzusagen. Also möchten wir, dass unsere Kurven so hoch wie möglich in diesem Diagramm sind. Aber wir möchten auch, dass sie nach links verschoben sind. Und wir vergleichen dies mit beliebten Strategien, die auch auf offline-Modelle angewendet werden, nämlich der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit dem state-of-the-art-Architektur, die speziell für simultane Vorschlagsübersetzung konzipiert ist. Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch. Wir sehen, dass sie alle Strategien, die auf offline-Modelle angewendet werden, übertroffen hat, da die Kurven nach links verschoben sind. Wir sehen auch, dass sie, wenn wir die tatsächliche Verzögerungszeit oder die computergesteuerte Verzögerungszeit in Betracht ziehen, die schnellste Strategie ist. Wenn Sie mehr Ergebnisse erfahren möchten, lesen Sie bitte unseren Artikel. Wir haben auch den Code, die Modelle und die simultane Ausgabe offen gelegt, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der CoNLL++-Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">Archiki präsentiert den ACL-Paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts", der ein neues Dataset für die Frage-Antwort-Modellierung in Meeting-Transkripten einführt. Das Dataset MeetingQA enthält 7.700 Fragen aus öffentlichen Meeting-Transkripten, die von Teilnehmern gestellt wurden und die entsprechenden Antwort-Sätze. Die Fragen sind lang, offen und suchen nach Diskussionen, was sie von anderen Frage-Antwort-Datasets unterscheidet. Die Antworten sind oft komplex und können aus mehreren Sätzen bestehen, die von verschiedenen Sprechern stammen.

Archiki beschreibt verschiedene Modelle, die für die Frage-Antwort-Modellierung entwickelt wurden, darunter Short-Context-Modelle, Single-Span-Modelle und Multi-Span-Modelle. Die Ergebnisse zeigen, dass die Modelle in beiden fein- und zero-shot-Einstellungen Schwierigkeiten haben, insbesondere bei der Identifizierung von rhetorischen Fragen und der Zuweisung von Antworten an Sprecher.

Zusammenfassend ist MeetingQA ein interessantes Dataset für die Frage-Antwort-Modellierung in Meeting-Transkripten, das noch nicht vollständig gelöst ist und weitere Forschung benötigt.</sample>
    <sample id="151">Hallo, ich bin Ying und mein Kollege Zhiyang und wir werden unsere Forschung über MultiInstruct präsentieren, mit der Zielsetzung, Multi-Modal Zero-Shot-Lernen über Anweisungstuning zu verbessern. 

Mit den Fortschritten in großen Sprachmodellen begannen viele Arbeiten, neue Lernparadigmen zu erforschen, bei denen pre-trained Sprachmodelle für verschiedene Downstream-Aufgaben in einem parametrischen und daten-effizienten Weg verwendet werden. Es wurde kürzlich gezeigt, dass Anweisungstuning große Sprachmodelle ermöglicht, auf nicht gesehene Aufgaben in einem zero-shot-Mannschaft durch natürliche Anweisungen zu folgen. 

Allerdings konzentrierten sich die meisten vorherigen Arbeiten auf Anweisungstuning auf die Verbesserung der zero-shot-Leistung auf Sprachaufgaben, während Computer-Vision- und Multi-Modal-Aufgaben vernachlässigt wurden. 

Daher wollen wir untersuchen, ob Anweisungstuning eines multi-modal pre-trained Models die allgemeine Fähigkeit verbessern kann, auf nicht gesehene multi-modal Aufgaben zu generalisieren. 

Zusätzlich entdeckten wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multi-modal. Es gibt mehr als 1600 Sprach-only-Anweisungsaufgaben. 

Daher motiviert uns das, einen multi-modal Anweisungstuning-Datensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multi-modal Anweisungstuning-Benchmark-Datensatz, der 62 diverse multi-modal Aufgaben mit 10 breiten Kategorien umfasst. 

Diese Aufgaben werden aus 21 bestehenden Open-Source-Datensätzen abgeleitet und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. 

Um multi-modal Anweisungstuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, einen vereinigten multi-modal pre-trained Model, als Basis-Modell. OFA verwendet eine vereinigte Vokabular für Sprache, Bild-Token und die Koordinaten eines Rechteckes. 

Wir zeigen einige Beispiele aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinigen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einer vereinigten Sequenz-zu-Sequenz-Format. 

In diesem Format werden der Eingabetext, die Bilder, die Anweisungen und die Rechtecke in demselben Token-Raum dargestellt. 

Ok, jetzt werde ich über multi-modal Anweisungstuning sprechen. 

Für das Trainings-Datensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wir sampeln 10.000 Instanzen pro Aufgabe. 

Für das Testen reservieren wir die gesamte Gruppe des gemeinsamen Sinnes für das Testen und wir wählen zusätzlich 5 Aufgaben aus der VQ- und Miscellaneous-Gruppe. 

Wir verwenden alle Instanzen in der Test-Split für jede Aufgabe. Darüber hinaus sampeln wir 20 Aufgaben aus der Test-Split der natürlichen Anweisungen als nicht gesehene Aufgabe für NLP. 

Wir verwenden das vorgestellte OFA-Modell als Basis-Modell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisung-Vorlagen kombiniert. 

Während des Testens für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen auswerten. 

In jedem Experiment berichten wir über den Mindest- und Maximalwert und die Standardabweichung der Leistung über alle 5 Experimente. 

Wenn die Aufgabe eine multi-modal Klassifikation ist, berichten wir über die Genauigkeit. Wenn es sich um eine multi-modal Generation handelt, berichten wir über die Rouge-L. 

Für NLP-Aufgaben berichten wir auch über die Rouge-L. Wir führen auch eine zusätzliche Bewertungs-Metriken ein, genannt Empfindlichkeit. 

Diese Messung misst die Fähigkeit des Modells, die gleichen Ausgabewerte für die gleiche Aufgabe unabhängig von der leichten Variation in der Wortwahl der Anweisung zu produzieren. 

Hier sind unsere Hauptergebnisse. 

Wie wir sehen können, kann Anweisungstuning die Leistung von OFA auf gesehene multi-modal Aufgaben signifikant verbessern. 

Außerdem kann das Transfer-Lernen aus natürlichen Anweisungs-Datensätzen die Anweisungstuning verbessern. 

Wir sehen auch, dass, wenn die Anzahl der Aufgaben zunimmt, das Modell eine bessere Leistung erreicht und gleichzeitig eine geringere Empfindlichkeit aufweist. 

Wir führen auch ein Experiment durch, indem wir eine Anweisung gegenüber 5 Anweisungen verwenden. 

Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit stark reduzieren. 

Dies zeigt den Einfluss verschiedener Fine-Tuning-Strategien auf die Modellsensitivität. 

Wie wir sehen können, kann das Transfer-Lernen aus natürlichen Anweisungs-Datensätzen das Modell zu einer viel besseren Sensitivität als das ursprüngliche OFA-Modell machen. 

Außerdem kann das Transfer-Lernen aus natürlichen Anweisungs-Datensätzen OFA helfen, auf dem natürlichen Anweisungs-Datensatz eine bessere Leistung zu erreichen. 

Insgesamt schlagen wir den ersten großen Skalenebenen multi-modal Anweisungstuning-Datensatz vor, mit dem die Leistungsfähigkeit von OFA verbessert wird, und wir erforschen verschiedene Transfer-Lern-Techniken und zeigen ihre Vorteile. 

Wir entwerfen auch eine neue Metrik namens Sensitivität. 

Eines mehr, wir sammeln einen viel größeren multi-modal Anweisungstuning-Datensatz mit etwa 150 zusätzlichen Vision-Sprach-Aufgaben und werden sie freigeben. 

Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">In der Präsentation "Exploring Large Language Models for Classical Philology" werden neue Sprachmodelle für die Klassische Philologie vorgestellt. Zwei monolinguale Modelle für Altgriechisch, GreBERTa und GreTa, sowie zwei multilinguale Modelle, PhilBERTa und PhilTa, wurden entwickelt. Die Modelle wurden auf einem neuen, hochwertigen Trainingskorpus basierend auf dem Internet Archive trainiert. Die Ergebnisse zeigen, dass die neuen Modelle die aktuelle State-of-the-Art in Bezug auf Teilwort-Tags, Abhängigkeitsanalyse und Lemmatisierung übertreffen. Die Analyse der T5-Encoder-Performance zeigt, dass dieser nach mehreren Trainingsphasen die Leistung eines native Encoder-only-Modells erreichen kann. Die Ergebnisse der Semantik- und Weltwissen-Erprobung zeigen, dass die multilinguale Modelle nicht signifikant besser als die monolingualen Modelle sind. Die Präsentation bietet einen Überblick über die neuen Sprachmodelle und ihre Ergebnisse, und empfiehlt die Lektüre der zugehörigen Forschungsarbeit für weitere Details.</sample>
    <sample id="153">Title: Resolving Ambiguities in Text-to-Image Generative Models

Abstract:

Text-to-image models have made significant progress in recent years, but they often struggle with ambiguous prompts that can lead to multiple interpretations. In this work, we present a framework to mitigate and evaluate ambiguities in text-to-image models. We curate a benchmark dataset, LAVA, which covers various types of ambiguities. Our framework uses in-context learning to generate clarifying questions or different visual setups to disambiguate prompts. We evaluate the generated images using a VQA model, which checks if the human's intention is satisfied in the image. Our results show that disambiguation using our framework has a positive effect on faithful generation and that our automatic evaluation framework is in agreement with human evaluation. We demonstrate a disparity in resolving ambiguities for different ambiguity types and provide a reliable method for evaluating text-to-image models. Our work aims to improve the faithfulness of generated images to user intention, making text-to-image models more useful in real-world applications.</sample>
    <sample id="154">Die Autoren, Sara Papi, Matteo Negri und Marco Turchi, gehören der Universität Trento an.</sample>
    <sample id="155">Der/die Referent*in heißt Javad Hosseini.</sample>
    <sample id="157">Title: Dialogue Summarization with Static-Dynamic Structure Fusion Graph

Abstract:

Dialogue summarization is a challenging task that aims to distill the salient information from a dialogue context into a concise summary. Existing methods heavily rely on pre-computed static graph structures, which have two fundamental drawbacks: they depend on the accuracy of external linguistic tools and are disjoint from the graph representation learning phase. To address these issues, we propose the Static-Dynamic Structure Fusion Graph (SDDS) model, which combines multiple static graphs with a dynamic graph module to capture semantic relationships between utterances. The SDDS model consists of four main components: Utterance Encoder, Static-Dynamic Graph module, and Summary Generator. The Static-Dynamic Graph module combines multiple static graphs using a 1x1 convolutional layer and captures semantic relationships using a multi-head attention model. The dynamic graph is then fused with the static graph using a dual cross-attention mechanism. Our model outperforms existing methods on several benchmark datasets, demonstrating the effectiveness of the proposed approach. The code and data are released on GitHub for further research.</sample>
    <sample id="158">Title: Dual Cache for Long Document Neural Coreference Resolution

Abstract:

Coreference resolution is the task of identifying and clustering mentions of the same entity in a document. Conventional methods have quadratic complexity, while cache-based methods reduce complexity to linear level. However, in long documents with topic switches, cache-based methods suffer from high cache misses. To address this issue, we propose a dual cache architecture that combines local and global caches. The local cache uses LRU eviction policy for local entities, while the global cache uses LFU policy for global entities. The dual cache works by scanning the document from left to right and adding new mentions to either the local or global cache based on their frequency. We evaluate dual cache on four public benchmarks and a 30,000-word book. Results show that dual cache outperforms single cache methods, reduces cache misses, and has the highest performance/cost ratio. Our approach is particularly effective for long documents and has significant implications for neural coreference resolution.</sample>
    <sample id="159">Hallo, ich bin Koustav Sinha und ich bin froh, dass Sie zu unserer Diskussion über unseren ACL 2023-Bericht gekommen sind. Sprachmodelle liefern nicht immer robuste Akzeptabilitätsurteile in Bezug auf den Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In diesem Werk besuchen wir die minimalen Paarparadigmen neu. Das Minimalpaarparadigma bewertet Sprachmodelle auf der Grundlage von Akzeptabilitätsurteilen. Dies kann auch die Grammatikalität wie BLiMP, SyntaxGym oder die Akzeptabilität in Bezug auf Stereotypen wie CrowS Paare umfassen. In diesem Minimalpaarparadigma wird die übliche Methode verwendet, um Sprachmodelle zu bewerten, indem man einem akzeptablen Satz oder einem grammatischen Satz einen akzeptablen Satz oder einen ungrammatischen Satz gegenüberstellt. Die Hoffnung besteht darin, dass das Modell eine höhere Wahrscheinlichkeit für den akzeptablen Satz hat. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, das Akzeptanzverhalten von Modellen bei längeren Sätzen zu bewerten. Große Sprachmodelle kommen mit immer längeren Kontextfenstern auf. Es ist daher wichtig, das Akzeptanzverhalten der Modelle über den gesamten Kontextfenster zu bewerten, was unser Ziel ist. Wir versuchen, die MPP-Pipeline neu zu besuchen, indem wir das Modell auffordern, die Akzeptabilität bei längeren und längeren Sequenzen zu bewerten. Wir simulieren diese längeren Sequenzen, indem wir die Datensätze selbst neu besuchen und dann Sätze durch das Auswählen von akzeptablen oder unakzeptablen Sätzen aus diesen Datensätzen neu erstellen. Beispielsweise haben wir ein typisches Paar von Grammatikalität aus dem BLiMP-Datensatz aus dem Adjunct-Island-Fall gewählt. Wir fügen den akzeptablen oder unakzeptablen Sätzen als Präfix zu beiden Sätzen hinzu, um längere Sequenzen zu erstellen, die dieselbe grammatische Struktur haben. Wir extrahieren grammatische Sätze aus dem Adjunct-Island-Fall und fügen sie als Präfix zu beiden akzeptablen und unakzeptablen Sätzen hinzu. Wir können dasselbe tun, indem wir unakzeptable Sätze aus dem gleichen Matching auswählen und sie als Präfix zu beiden Sätzen hinzufügen. Dies kann auch verwendet werden, um das Akzeptanzverhalten des Modells zu testen. Wir können auch das gleiche tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Dies nennen wir Mismatch-Szenario. Die Sätze stammen immer noch aus relevanten Datensätzen, aber nicht aus dem gleichen Datensatz, mit dem wir evaluiert werden. Wir können dasselbe tun, um das Unakzeptanzfall zu prüfen. Schließlich können wir Sätze aus einem vollkommen unabhängigen Bereich wie Wikipedia auswählen. Dies wird uns sagen, ob die Akzeptabilitätsurteile des Modells tatsächlich durch den Kontext beeinflusst werden, sei es durch einen anderen Subset des Datensatzes oder durch einen vollkommen irrelevanten Kontext. Wie verhält sich das Modell? Zunächst sehen wir uns die Wikipedia-Sätze an, die vollkommen irrelevant zum aktuellen Fragepaar sind. Dort sehen wir, dass die MPP-Urteile bei beliebiger Kontextlänge größtenteils robust sind. Wir erhöhen die Kontextlänge bis 1024, um die OPT- und GPT-2-Modelle zu maximieren. Wir sehen hier in der orangen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier wählen wir Sätze aus akzeptablen und unakzeptablen Domänen aus dem gleichen BLiMP- oder SyntaxGym-Datensatz. Dort sehen wir, dass die MPP-Urteile entweder stark ansteigen oder abnehmen, wenn wir entweder akzeptable Präfixe oder unakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur abgleichen, d. h. wenn wir Sätze aus dem gleichen Phänomen aus BLiMP oder SyntaxGym wählen, sehen wir eine massive Zunahme oder Abnahme der MPP-Urteile für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß und steigt mit der Kontextlänge an. Dies würde wahrscheinlich die neueren Sprachmodelle, die große Kontextfenster haben, beeinflussen. Warum beeinflusst das Übereinstimmen des Präfixes das Sprachmodellurteil so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabensatz zu stören, indem wir die relevanten Struktur erhalten, aber Lärm in den Eingabensatz hinzufügen. Nachdem wir mehrere dieser Störungen durchgeführt haben, finden wir, dass keiner dieser Lärm die Modellentscheidung ändert, wie es die MPP-Urteile anzeigt. Wir finden, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren. Wenn wir die Sätze aus der akzeptablen Domäne stören, sehen wir eine ähnliche Zunahme bei allen Störungen. Wenn wir die Sätze aus der unakzeptablen Domäne stören, sehen wir eine ähnliche Abnahme der MPP-Urteile. Der Schlüsselresultat unseres Werks ist, dass Sprachmodelle auf latente syntaktische und semantische Merkmale empfindlich reagieren, die über die Sätze hinweg geteilt werden. Die MPP-Evaluation, wie wir sie derzeit mit kurzen und einzelnen Sätzen durchführen, erfassen möglicherweise nicht vollständig das abstrakte Wissen der Sprachmodelle über den Kontextfenster. Bitte lesen Sie unseren Artikel für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Die Input-Token werden im ersten Schritt der Methode mit einem unsortierten Multiset von Tokens zugeordnet, die in der Ausgabe erscheinen werden.</sample>
    <sample id="161">Insgesamt sind 55.000 spezifische Ziele mit Skripten in CoScript vertreten.</sample>
    <sample id="163">Die beste automatische Ausrichtungsmethode für DEPLAIN ist MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass die Daten mit viel geringeren Kosten als traditionelle manuelle Annotationsmethoden beschriftet werden können.</sample>
    <sample id="165">Title: Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations

Abstract:

Abductive reasoning aims to identify a plausible explanation bridging the information gap between a given context and outcome. Traditional approaches rely on supervised methods, which require annotation of plausible explanations and can be noisy and subjective. This paper introduces an unsupervised learning method called LiPoR, which maximizes the marginal likelihood of the outcome given the context by marginalizing possible explanations. To prefer plausible explanations, a regularizer is added to enforce mutual exclusivity among explanations. The LiPoR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others. Experimental results on the AlphaNLI dataset show that LiPoR outperforms zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy. This work demonstrates that abductive reasoning can be learned without supervision regarding the plausibility of explanations, providing a new approach to this challenging task.</sample>
    <sample id="166">Title: A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text

Abstract:

We propose a novel neural reasoning framework, called Neural Divide-and-Conquer Reasoning (NDCR), to address the challenging task of image retrieval from linguistically complex text. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, NDCR combines the strengths of analogical reasoning (System 1) and abstract logical reasoning (System 2) to tackle complex reasoning problems. The framework consists of three key components: the Proposition Generator, Visual-Linguistic Interactor, and Neural-Symbolic Reasoner. The Proposition Generator decomposes complex text into simple propositions, while the Visual-Linguistic Interactor performs visual-propositions' information interaction. The Neural-Symbolic Reasoner, a logical reasoning System 2, integrates the reasoning states and results of simple propositions to obtain the final solution. Experimental results demonstrate that NDCR outperforms state-of-the-art baselines, and abolition experiments verify the effectiveness of each module. Our work highlights the potential of neural symbolic calculation and Divide-and-Conquer strategy in improving compositional reasoning and planning of large language models.</sample>
    <sample id="167">In DEPLAIN-web wurden 750 Dokumente auf einer Seite manuell ausgerichtet und auf der anderen Seite mit automatischen Alignmentmethoden.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde von Reuters-News aus dem Jahr 2020 erstellt und dann mit den gleichen CoNLL-2003-Anmerkungsvorgaben annotiert.</sample>
    <sample id="169">In der Studie "Prompting PaLM for Translation: Assessing Strategies and Performance" untersuchen die Autoren, wie groß die Einfluss von Prompting auf die Leistung von großen Sprachmodellen (LLMs) bei der maschinellen Übersetzung ist. Sie verwenden den 540 Milliarden Parameter großen PaLM-Modell, der 2022 vorgestellt wurde und 780 Milliarden Token umfasst. Die Ergebnisse zeigen, dass das Prompting eine große Auswirkung auf die Leistung der LLMs hat, insbesondere bei einer kleinen Anzahl von Beispielen. Die Autoren empfehlen eine 5-Schuss-Prompting-Strategie, bei der jedes Beispiel mit der Sprache markiert wird, in der es geschrieben ist. Die Ergebnisse zeigen, dass die Qualität der Beispiele wichtiger ist als ihre Ähnlichkeit zur Quellsatz. Die Studie vergleicht auch die Leistung von PaLM mit staatlichen Systemen und zeigt, dass PaLM zwar flüssige Übersetzungen produziert, aber oft mit Genauigkeitsproblemen. Die Autoren empfehlen, Beispiele aus hochwertigen Übersetzungen auszuwählen, um die Leistung von PaLM zu verbessern.</sample>
    <sample id="170">Hallo alle, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" vorstellen. Semantic Parsing ist eine Aufgabe, bei der man semantische Darstellungen von Benutzeranfragen wie SQL und Lambda Calculus erstellt. Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in verschiedenen natürlichen Sprachen in verschiedene Bedeutungsrepräsentationen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir die Anfrage in verschiedenen natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für das Cross-Lingual Semantic Parsing wurden separat vorgeschlagen und auf Datenmengen begrenzter Aufgaben und Anwendungen evaluiert. Zum Beispiel gibt es eine gute Abdeckung bestimmter natürlicher Sprachen. Aber Chinesisch fehlt und es gibt eine mangelnde Abdeckung bestimmter Bedeutungsrepräsentationen. Die Lambda-Kalkül fehlt oder sie werden nur auf bestimmte neuronale Modelle evaluiert. Zum Beispiel gibt es nur ein einziges Modell, das sie bewerten kann. Daher schlagen wir XSemPLR vor. Wir stellen eine einheitliche Datenmenge XSemPLR für das Cross-Lingual Semantic Parsing in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Sie enthält 9 Datenmengen in verschiedenen Domänen, 5 semantische Parsen-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unsere Benchmarks besser zu bewerten, betrachten wir sechs Einstellungen für Training und Bewertung. Die erste Einstellung ist Translate-Test. Wir verwenden die Google Translate API, um die Quell- auf die Ziel-Sprache zu übersetzen, und dann verwenden wir ein monolinguales Modell zum Training und zur Bewertung. Zum Beispiel trainieren wir das englische Modell auf englischen Anfragen und übersetzen während der Vorhersage die deutsche Anfrage mithilfe der API in Englisch und verwenden dann das trainierte Modell, um die SQL-Vorhersage vorzunehmen. Wir testen auch Monolingual Model. In dieser Einstellung ist die Quell-Sprache die gleiche wie die Ziel-Sprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch Monolingual Few-shot-Einstellung, indem wir ein monolinguales Modell mit nur 10% der Trainingsdaten trainieren. Und wir testen Multilingual Model, indem wir ein einziges multilinguales Modell für alle Sprachen trainieren. Zum Beispiel setzen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Vorhersage können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen usw. zu übersetzen. Wir betrachten auch Cross-Lingual Zero-shot und Few-shot-Transfer. Wir trainieren auf einer Quell-Sprache und transferieren sie auf eine andere Sprache. Zum Beispiel trainieren wir auf englischen Anfragen oder der Kombination von englischen und deutschen Few-shot-Anfragen, um ein multilinguales Modell zu trainieren, um die SQL-Vorhersage vorzunehmen. Und wir finden viele interessante Ergebnisse. Betrachten wir die Analyse der monolingualen Modelle. Wir bewerten zwei Gruppen von Modellen, einschließlich Encoder-PTR, das für Multilingual Pretrained Encoders mit Pointer-based Decoders steht, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, die Multilingual Pretrained Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder-Modelle die beste Leistung auf allen neun Datenmengen erzielen. Wir bewerten mT5 und XLM-R + PTR im multilingualen Setting. Wir fanden heraus, dass Encoder-Decoder-Modelle oder Encoder-PTR verbessert werden können, indem sie in einer Mischung verschiedener Sprachen trainiert werden. Wir fanden heraus, dass dies der Fall ist, weil die meisten großen natürlichen Sprachen einen Leistungszuwachs erzielen, außer dass die Leistung in sieben Datenmengen bei Englisch abnimmt und nur in drei Datenmengen steigt. Ich denke, das ist der "Fluch der Multilingualität". Wir vergleichen auch die Leistungsgap zwischen Sprachen. In diesem Bild ist die blaue Linie die Cross-Lingual Few-shot-Transfer. Die orangefarbene Linie ist die Cross-Lingual Zero-shot-Transfer. Die grüne Linie ist die Monolingual-Einstellung. Wir fanden heraus, dass, wenn wir die grüne und orangefarbene Linie vergleichen, wir ein signifikantes Leistungsgap bei der Zero-shot-Transfer feststellen. Und wenn wir die blaue und orangefarbene Linie vergleichen, stellen wir fest, dass das Leistungsgap bei der Few-shot-Transfer schnell abnimmt. Wir finden auch einige andere interessante Ergebnisse. Zum Beispiel übertrifft Encoder-Decoder die vorherige Arbeit oder erreicht vergleichbare Ergebnisse. Die Vorbereitung auf Englisch kann die Leistung der Few-shot-Transfer auf die Ziel-Sprache erheblich verbessern. Und wir fanden heraus, dass multilinguale Sprachmodelle wie Codex und BLOOM noch nicht ausreichend für die Cross-Lingual Semantic Parsing-Aufgaben sind. Zusammenfassend schaffen wir XSemPLR, ein einheitliches Benchmark für das Cross-Lingual Semantic Parsing mit verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentative Arten von multilingualen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse. Und bitten Sie um einen Besuch unserer Arbeit und Code. Danke für das Zuhören.</sample>
    <sample id="171">Vorherige Arbeiten können in vier Kategorien eingeteilt werden. Diese Methoden haben jedoch entweder nicht die Anwendung auf Embedding-as-Services oder fehlende Transferierbarkeit.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex oder BLOOM sind nach den Ergebnissen der Studie noch nicht ausreichend für Cross-Lingual Semantic Parsing (CLSP)-Aufgaben.</sample>
    <sample id="174">Das Paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" präsentiert einen neuen Ansatz für die Bewertung der Qualität von Argumenten. Die Autoren identifizieren Schwächen in bestehenden Datenbanken, wie z.B. mangelnde Qualität, geringe Diversität und mangelnde Tiefe in der Erklärung der Argumente. Um diese Probleme zu lösen, haben die Autoren ArgAnalysis35K entwickelt, das mit 35.000 Argument-Analyse-Paaren die größte Datenbank in diesem Bereich ist. Die Datenbank enthält hochwertige Argumente, die aus verschiedenen Quellen stammen, darunter Reden von Expertendebattanten und -touren, sowie aus Novizendebattanten und der Allgemeinheit. Die Datenbank bietet auch eine verbesserte Diversität von Argumenten, da sie auf 24 Themen basiert, die auf Erfahrung, Websites und Expertenberatung ausgewählt wurden. Ein weiterer wichtiger Aspekt ist die Einführung der Analyse, die eine Kombination aus Ansprüchen, Prämissen und anderen Elementen ist. Die Autoren haben auch eine Instanzen-basierte Annotator-Reliabilitäts-Methode und ein Relevanzmodell entwickelt, um die Zuverlässigkeit der Bewertungen zu verbessern.</sample>
    <sample id="175">Wir lösen dieses Problem, indem wir die Mehrdeutigkeit der Permutationen durch eine GPU-freundliche kontinuierliche Relaxation approximieren, die auch die Rückpropagation durch die Lösung ermöglicht und linguistisch plausiblere Permutationen lernt.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird in diesem Kontext nicht explizit definiert. Es wird jedoch angedeutet, dass Fairness eine Gleichbehandlung verschiedener Gruppen und eine gleichmäßige Leistung bei der Erkennung von Hassrede und Falschinformationen unabhängig von deren politischer Ausrichtung beinhaltet.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">In dem Papier "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker" wird ein neues Verfahren vorgestellt, um die Fähigkeit von großen Sprachmodellen, die mentale Zustände anderer zu verstehen, zu verbessern. Das Verfahren, SymbolicToM, verwendet explizite grafische Darstellungen, um die mentale Zustände von Charakteren in einer Geschichte zu modellieren. Es kann Fragen zu der Geschichte beantworten, die die mentale Zustände von Charakteren betreffen, wie z.B. "Wo glaubt Alice, dass Bob den Apfel suchen wird?".

Die Autoren testen SymbolicToM mit verschiedenen großen Sprachmodellen und vergleichen es mit überwachten Baseline-Modellen. Die Ergebnisse zeigen, dass SymbolicToM die Leistung der Modellleistung verbessert, insbesondere bei Fragen zu zweiten Ordnung, die die mentale Zustände von Charakteren betreffen. Außerdem zeigt sich, dass SymbolicToM robust gegenüber neuen Szenarien ist und seine Leistung auch in neuen, linguistisch diversen Datenbeständen verbessert.

Die Ergebnisse deuten darauf hin, dass SymbolicToM ein effektives Werkzeug ist, um die Fähigkeit von großen Sprachmodellen, die mentale Zustände anderer zu verstehen, zu verbessern.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Title: Distilling Script Knowledge from Large Language Models for Constrained Language Planning

Abstract:

Constrained language planning involves generating step-by-step instructions for specific goals with multiple constraints. While large language models can effectively decompose abstract goals into steps, they struggle with planning for goals with specific constraints. To address this, we define the problem of constrained language planning and evaluate the ability of large language models to plan for specific goals. We find that these models achieve unsatisfactory results due to low faithfulness to constraints. To improve generation quality, we propose an over-generate-then-filter method, which involves generating multiple scripts for specific goals and selecting the most faithful ones using a filter model. We also develop a dataset, CoScript, by distilling constrained language planning knowledge from large language models. CoScript contains 55,000 specific goals with scripts and shows high pluralism in generated goals. We demonstrate that smaller models, such as T5 fine-tuned on CoScript, can generate scripts of higher quality than most large language models. Our work establishes the constrained language planning problem and provides a valuable resource, CoScript, for advancing research on language planning.</sample>
    <sample id="182">Tropikalismus bezieht sich im Zusammenhang mit dieser Arbeit auf das Stereotyp, dass Frauen lateinamerikanischer Abstammung als "vibrant" und "curvaceous" beschrieben werden, was auf eine Verbindung zu einem Stereotyp von exotischen und tropischen Regionen hindeutet.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch die Verwendung einer Studie als Referenz erstellt, in der Menschen mit ähnlichen Anfragen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst" befragt wurden.</sample>
    <sample id="184">CXMI (Contextualized Mutual Information) wurde in der vorherigen Arbeit verwendet, um die Kontextnutzung von Maschinenerkennungsmustern zu messen. In dieser Arbeit wurde CXMI zu Pointwise CXMI (P-CXMI) erweitert, um die Kontextnutzung auf Satzebene oder auf Wortebene zu messen.</sample>
    <sample id="185">DrBERT ist ein pre-trained Modell in Französisch für die Biomedizin und klinische Domänen, das auf NACHOS trainiert wurde, einem Datenbestand mit medizinischen Daten, die aus dem Web gekauft wurden. 
ChuBERT ist ein Modell, das auf anonymisierten Daten aus dem Datenwarehouse des Nantes University Hospital trainiert wurde.</sample>
    <sample id="187">Es scheint, dass nur zwei Autoren, Ying und Zhiyang, an der Arbeit beteiligt sind.</sample>
    <sample id="188">Iteratives Transferlernen ist die Kombination aus Transferlernen und weiterer Feinabstimmung des Modells auf beiden ursprünglichen Tasks, um das Ergebnis zu verbessern. In diesem Fall wurde das Modell zuerst auf die Task "CE" und dann auf den Task "Debate" weiter trainiert, um bessere Ergebnisse zu erzielen.</sample>
    <sample id="189">Das Ziel des Datensatzes, der AltEntities Corpus, ist es, ein größeres öffentlich zugängliches Datenmaterial für die Aufgabenstellung zu schaffen, bei der ein Benutzer zwischen zwei oder mehreren Entitäten (z.B. Songs, Büchern oder Rezepten) wählen möchte, indem er indirekte Referenzen verwendet.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS (Embedding as a Service) extrahieren, indem er von der Dienstleistung lernen und ähnliche Dienstleistungen anbieten kann, wie in den vorherigen Arbeiten gezeigt wurde.</sample>
    <sample id="191">Es sind drei Autoren an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Title: Confidence-guided Adaptive Memory Efficient Optimization (CAME) for Large Language Models

Abstract:

The training of large language models often relies on adaptive gradient-based optimization methods, which require significant memory to store first and second moment estimates of per-parameter gradients. Existing memory-efficient optimizers, such as Adafactor, achieve a drastic reduction in auxiliary memory usage but with a performance penalty. To address this challenge, we propose a confidence-guided adaptive memory efficient optimizer, CAME, which simultaneously achieves fast convergence and low memory usage. CAME utilizes non-negative matrix factorization to reduce memory requirements and introduces a confidence-guided updating mechanism to adaptively handle erroneous updates. Extensive experiments on BERT, GPT-2, and T5 demonstrate that CAME achieves significant improvements in validation accuracy and reduces memory cost compared to Adam and Adafactor. CAME also outperforms existing memory-efficient optimizers, such as SM3, in terms of memory footprint. Our proposed optimizer is particularly effective for large batch training and serves as an important extension for existing memory-efficient optimizers.</sample>
    <sample id="193">Die genaue Anzahl der Annotatoren, die verwendet wurden, um den ursprünglichen Datensatz zu erstellen, wird nicht explizit genannt. Es wird jedoch erwähnt, dass um die 1.000 Beispiele von Diskurs-Einheiten-Paaren gesammelt wurden, nachdem die Tweets mit dem PDTB-Parser verarbeitet wurden.</sample>
    <sample id="194">Die Autoren gehören an die Carnegie Mellon University, die University of Washington und das Allen Institute for AI.</sample>
    <sample id="195">Title: Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering

Abstract:

Explainable question answering (XQA) aims to provide answers and explanations for complex questions. Current methods can be grouped into neuro-symbolic and decompose-based approaches, but they have limitations. To address these challenges, we propose a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree). RoHT is a two-stage framework that first builds a Hierarchical Question Decomposition Tree (HQDT) for a given complex question, and then performs probabilistic reasoning over the tree to fuse knowledge from a knowledge base and a text corpus. The framework consists of a question decomposer, a question generator, and a scheduler, executor, and aggregator. We evaluate RoHT on two challenging complex QA datasets, KQA Pro and Musique, and achieve state-of-the-art performance. Our results show that RoHT outperforms existing methods, including TransferNet, by a large margin, demonstrating the effectiveness of explicit decomposition and the integration of knowledge from heterogeneous sources.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme ist, dass sie noch einige Herausforderungen haben, wie zum Beispiel:

* Common sense-Violationen in etwa 20% der Antworten
* Irrelevante Informationen in etwa 15% der Antworten
* Selbst- oder Partnerwidersprüche in etwa 10% der Antworten

Es gibt jedoch auch Fortschritte in der Technik, wie zum Beispiel die Entwicklung des ABC-Eval-Systems, das es ermöglicht, Dialogsysteme auf verschiedenen Dimensionen zu bewerten und ihre Stärken und Schwächen zu identifizieren.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, da große Sprachmodelle immer längere Kontextfenster haben und daher eine Bewertung ihrer Akzeptanz über das gesamte Kontextfenster entscheidend ist.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell in sieben von neun Datensätzen geführt.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität im Voraus nicht unbedingt, sie haben jedoch Zugang zu Hintergrundinformationen über die beiden Entitäten, wie z.B. Wikipedia-Seiten oder Google-Suchergebnisse.</sample>
    <sample id="201">Neuronale MT-Metriken sowie BLEURT-Metriken und die MQM-Framework für die human-evaluative Ergebnisse.</sample>
    <sample id="202">Nein, die Regression bei der Generalisierung auf bestimmte NER-Typen wird nicht erwähnt.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Erfahrungen der Forscher und Entwickler beeinflusst, die wiederum die Daten und Modelle beeinflussen. Dies kann zu systematischen Leistungsunterschieden zwischen verschiedenen Populationen führen, insbesondere wenn die Modelle nicht auf die spezifischen Kontexte und Erfahrungen dieser Populationen ausgelegt sind.</sample>
    <sample id="204">Es wird erwähnt, dass multilingual Language Models wie Codex und BLOOM "noch nicht ausreichend" für die Cross-Lingual Semantic Parsing Tasks sind. Es wird jedoch nicht spezifiziert, ob diese LLMs durch Adapter oder eine vollständige Feinabstimmung angepasst wurden.</sample>
    <sample id="205">Title: Uncovering the Political Biases of Language Models: A Study on Bias Propagation from Pretraining Data to Downstream Tasks

Abstract:

Language models trained on large-scale web crawl data, including diverse perspectives from news media, have created a mixed blessing for their applications. On one hand, they can learn from diverse perspectives, celebrating democracy and the plurality of ideas. On the other hand, these different political opinions are inherently socially biased and may lead to fairness issues in downstream tasks. This study investigates the political bias propagation pipeline from pretraining data to language models to downstream tasks. We evaluate the political leaning of language models using the political conference test and find that they occupy all four quadrants on the political spectrum. Our results show that language models with different political leanings perform differently on downstream tasks, such as hate speech detection and fake news detection. We find that left-leaning language models are better at detecting hate speech targeting socially minority groups, while right-leaning language models are better at detecting hate speech targeting white and men. Our study highlights the pressing fairness issue regarding the political biases of language models and the unique dilemma of sanitizing political opinions in language model training data.</sample>
    <sample id="206">Wir verwenden ein Modell, das durch das Übertragen von Gewichten von zwei verschiedenen Tasks erstellt wird: 

1. Topic-independent Dissonance Stance Classification (Debate)
2. Binary Classification von Expansion und Comparison-Klassen von PDTB (CE)</sample>
    <sample id="207">Die aktuellsten Testsets wurden verwendet, um eine Überschneidung der Testdaten mit der Trainingsdaten des PaLM zu vermeiden.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen:

1. Positive Stereotypen und essentialisierende Narrative sollten von Forschern angegangen werden.
2. Bei der Untersuchung von Vorurteilen und Schäden sollte ein intersektionaler Blickwinkel verwendet werden.
3. Es sollte mehr Transparenz über Methoden zur Minderung von Vorurteilen geben.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt nicht explizit angegeben, aber es wird erwähnt, dass die Methode die Fähigkeit kleinerer Modelle zur Konstrained Language Planning verbessert und dass sie sogar die Leistung einiger größeren Modelle übertreffen kann, wenn sie auf dem CoScript-Dataset trainiert werden.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden. Die Forscher haben bereits die Ergebnisse ihrer Experimente als Benchmark für die automatische Textvereinfachung in der Zukunft vorgeschlagen.</sample>
    <sample id="212">In der Arbeit wird nicht explizit angegeben, mit wie vielen kleineren Modellen experimentiert wird. Es wird jedoch erwähnt, dass mit dem T5-Modell experimentiert wird, das als ein kleineres Modell gegenüber großen Sprachmodellen betrachtet wird.</sample>
    <sample id="213">Das Modell OFA (Unified Multi-Modal Pre-trained Model) wird als Basismodell verwendet.</sample>
    <sample id="215">In diesem Beitrag wird ein neues Argument für symmetrische Strukturen der Koordination vorgestellt, das sich gegen asymmetrische Strukturen wie diejenigen von Universal Dependencies und der Meaning-Text-Theorie richtet. Die Argumentation basiert auf dem Grundsatz der Minimalisierung der Abhängigkeitslänge, der besagt, dass kürzere Abhängigkeiten bevorzugt werden. Es wird gezeigt, dass in Koordinationsstrukturen, bei denen der Regierende (Gouverneur) auf der linken Seite steht oder fehlt, der linke Konjunkt tendenziell kürzer ist als der rechte. Dieses Phänomen tritt jedoch nicht auf, wenn der Gouverneur auf der rechten Seite steht. Die Analyse von Statistiken aus der Penn Treebank bestätigt diese Beobachtung und zeigt, dass die Tendenz zum kürzeren linken Konjunkt mit der Längeunterschied zwischen den Konjunkten zunimmt. Die Ergebnisse werden als Argument gegen asymmetrische Strukturen der Koordination und für symmetrische Strukturen wie diejenigen von Hudsons Word Grammar gesehen.</sample>
    <sample id="217">Title: Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation

Abstract:

This study addresses the limitations of existing controllable dialogue generation (CDG) methods, which focus on single attributes and struggle with multi-attribute generation. We propose a Disentangled Controllable Generation (DCG) model that learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. Our model, based on the DialoGPT framework, incorporates a compositional prompt module that uses attribute-related information from pre-trained language models. We introduce a unified reference-free evaluation framework, MAE, for different granularities of attributes. Experiments demonstrate the effectiveness of our method and evaluation metrics, outperforming existing baselines in attribute controllability and text equality. Our model can generalize from seen attributes to unseen combinations, and the proposed attribute-oriented prompt method outperforms models that learn independent prompts for each attribute value. This study contributes to the advancement of CDG and provides a unified evaluation framework for multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Die Autoren gehören nicht einer bestimmten Universität an, sondern sie sind Mitarbeiter von Google Translate.</sample>
    <sample id="219">Title: A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports

Abstract:

This research presents a novel approach to financial report analysis, focusing on the Form 10-K annual report required by the SEC. The proposed method, a compare-and-contrast multistage pipeline, aims to uncover financial signals by leveraging the similarity between reports from consecutive years. A highlighting task is introduced, where a model predicts the importance of words in a report compared to its previous year's report. The pipeline consists of four stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. The model is trained on an external dataset, eSNLI, and fine-tuned on a released dataset, FINAL, using a two-stage approach with soft labeling techniques. The results show that the proposed method achieves the best performance on FINAL and preserves generalization capability on eSNLI. The approach also benefits from using mismatched pairs during simulation. Future works include improving effectiveness and exploring additional techniques for information retrieval to enhance the application.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Es wird nicht explizit erwähnt, welches Sprachpaar in der Arbeit untersucht wurde. Es wird jedoch erwähnt, dass die Übersetzungen von Deutsch ins Englisch verwendet wurden.</sample>
    <sample id="222">The paper "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" untersucht die Herausforderungen und Interventionsmöglichkeiten für die Anpassung von Modellen auf neue Domänen in der offenen Frage-Antwort-Umgebung. Die Autoren identifizieren drei Hauptbeiträge: 

1. Sie untersuchen verschiedene Dateninterventionen, um die Ausfahrbarkeit in neuen Domänen zu ermöglichen.
2. Sie identifizieren den Typ des Dataset-Shifts, der in der neuen Domäne auftritt.
3. Sie bestimmen, welche Dateninterventionen wirksam sind für einen bestimmten Typ von Shift.

Die Autoren entwickeln zwei Methoden zur Datenintervention: Few-Shot- und Zero-Shot-Adaptation. Sie testen die Effektivität dieser Methoden anhand von sieben Zielfragen und Datenbanken aus sechs verschiedenen Domänen. Die Ergebnisse zeigen, dass die Few-Shot-Adaptation durchschnittlich 8% für den Retriever und 11% für den Reader verbessert, während die Zero-Shot-Adaptation für Daten mit Konzept- und kovariablen Shifts wirksam ist.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind:

1. long-mBART (für Dokumenten-Ebene-Vereinfachung)
2. mBART (für Satz-Ebene-Vereinfachung)</sample>
    <sample id="225">53 Aufgaben werden für das Training verwendet, während die übrigen 9 Aufgaben (einschließlich der gesamten Gruppe "Common Sense Reasoning") und 5 zusätzliche Aufgaben aus den Gruppen "VQ" und "Miscellaneous" für das Testen verwendet werden.</sample>
    <sample id="226">Es wird nicht explizit erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="227">Grounded language understanding bezeichnet die Fähigkeit, natürliche Sprache in eine repräsentierbare Form für eine spezifische Umgebung zu überführen. Aktuelle Sprachmodelle sind jedoch aufgrund des Mangels an Bodenierung während der Vorverarbeitung (pre-training) in der Lage, dieses Problem zu lösen. Dies führt zu einer Vielzahl von Anwendungen, darunter intelligente Assistenten, semantische Suchmaschinen und Roboter, die natürliche Sprache verstehen können.

Um diese Herausforderung anzugehen, wird ein neuer Framework vorgestellt, der auf Pangu benannt ist, dem primordialen Wesen der chinesischen Mythologie. Pangu trennt den Himmel und die Erde, ähnlich wie unser Framework den symbolischen und physischen Welten trennt. In diesem Framework wird ein symbolischer Agent verwendet, um Kandidatenpläne vorzuschlagen, während ein Sprachmodell verwendet wird, um diese Kandidaten zu bewerten und zu priorisieren.

Durch die Verwendung von Pangu können Sprachmodelle von verschiedenen Arten, einschließlich BERT, T5 und Codex, in verschiedenen Szenarien, einschließlich der Frage-Beantwortung mit Hilfe von Wissensbasen, verwendet werden. Pangu zeigt eine ausgezeichnete Leistung in verschiedenen Szenarien und weist eine starke Effizienz bei der Verwendung von Beispielen auf.</sample>
    <sample id="228">Die Autoren haben an vier Datenbanken experimentiert: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">Our work focuses on detecting improvable claims for argumentative writing support. We formalize two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. To tackle this problem, we explore the challenges of working with revision-based data from collaborative online debate platforms like Kialo. We identify four challenges: Representativity and Reliability, Model Complexity and Architecture, Contextual Information, and Topical and User Bias. To address these challenges, we present a detailed analysis of strategies and approaches for the introduced tasks. Our experiments show that revision-based data can be effectively employed for the tasks, and modeling the distance between claimed versions is beneficial for detecting suboptimal claims. We also find that the impact of contextual information depends on the task and the quality issues a text is suffering from. Our work contributes to the development of argumentative writing support systems and provides insights into the challenges of working with revision-based data.</sample>
    <sample id="231">NACHOS ist ein Datenbestand, der durch das Crawlen von medizinischen Daten aus dem Web generiert wurde.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">In the paper "Attention as a Guide for Simultaneous Speech Translation", a novel approach to simultaneous speech translation (SimulST) is presented. SimulST translates spoken language into text in real-time, enabling cross-language communication. Current SimulST models have limitations, such as requiring specific architectures, long training procedures, and multiple models for different latency regimes. 

The proposed solution, EDAtt (Encoder-Decoder Attention), leverages the attention mechanism to decide when to emit partial translations based on where attention points. A word is emitted if the attention is not concentrated on the last speech frames, indicating stable information. EDAtt uses a single model for all latency regimes and handles latency through specific parameters.

The results show that EDAtt outperforms popular strategies applied to offline models, such as Wait-k and Local Agreement, and state-of-the-art architectures specifically tailored for SimulST. EDAtt also achieves the fastest computational-aware time. The paper and code are available for reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse. Bei einer 5-Shot-Prompting-Strategie ist die Form der Prompting nicht entscheidend, sondern die Qualität der Beispiele. Bei Zero- und One-Shot-Prompting ist die Form der Prompting jedoch entscheidend.</sample>
    <sample id="235">Ich konnte keine Informationen über die Universität der Autoren finden.</sample>
    <sample id="236">Die 5 Anweisungen der Experten sind nicht explizit im Text genannt. Es wird jedoch erwähnt, dass jede Aufgabe mit "fünf Experten geschriebenen Anweisungen" ausgestattet ist.</sample>
    <sample id="237">Die Autoren schlagen vor, ein diagnostisches Test-Suite namens "KITMUS Test" zu verwenden, um die Fähigkeit von Modellen, Informationen aus mehreren Quellen zu nutzen, zu testen. Dazu werden drei Einstellungen definiert: "Background-Pretrain", "Background-Both" und "Background-Inference", bei denen die Verfügbarkeit von Hintergrundwissen in den Prätrainingsdaten, im Inferenceszeitkontext oder in beiden variiert.</sample>
    <sample id="238">Yebowen Hu von der University of Central Florida präsentiert in diesem Video die neue Benchmark-Datenbank MeetingBank. Diese enthält Transkripte, Referenzsummaries und andere relevante Ressourcen für 1.366 City-Council-Meetings aus verschiedenen Städten. Das Ziel war es, ein Datenbank zu erstellen, die Forschern hilft, umfassende Zusammenfassungstechnologien für verschiedene Lesedomänen zu entwickeln.

Um die Daten zu sammeln, wurde die Speechmatics-API verwendet, um Audio-Daten in Transkripte zu konvertieren. Die Transkripte wurden dann mit den entsprechenden Referenzsummaries und Segmenten verglichen. Die Datenbank enthält fast 7.000 Instanzen und bietet eine Vielzahl von Statistiken und Metriken, um die Qualität der Zusammenfassungen zu bewerten.

Yebowen Hu präsentiert auch die Ergebnisse der Bewertung verschiedener Zusammenfassungssysteme, darunter Oracle, LEAD, LexRank und TextRank sowie die fünf besten neuralen abstrakten Summarisatoren. Die Ergebnisse zeigen, dass GPT-3 in der humanen Bewertung die besten Ergebnisse erzielte, obwohl es in den automatischen Metriken nicht so gut abschnitt. Die Ergebnisse dieser Studie zeigen, dass MeetingBank eine wertvolle Ressource für Forscher ist, die umfassende Zusammenfassungstechnologien entwickeln möchten.</sample>
    <sample id="239">Hallo alle, ich heiße David Vilar, und ich werde eine kurze Besprechung des Artikels "Prompting PaLM für Übersetzung: Bewertung von Strategien und Leistung" halten. Dies ist ein gemeinsames Werk mit meinen Kollegen aus Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im vergangenen Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfasst. Zu diesem Zeitpunkt erreichte es den Stand der Technik bei Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur großen Sprachmodell-Befragung für die maschinelle Übersetzung. Wir bewerteten die Übergangsfähigkeit solcher Modelle mit den besten Praktiken der MT-Gemeinschaft. Dazu gehört die Verwendung der neuesten Testmengen, um einen Überschneidung der Testdaten mit der Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen auch mit den Stand der Technik-Systemen, also dem besten System, also der WMT-Bewertung. Wir verwenden den neuesten, neuralen MT-Metriken und zeigen außerdem Ergebnisse der Expertenbewertung. Schließlich geben wir Empfehlungen für die Auswahlstrategien der Befragung ab. Die Befragung hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, in dem wir eine einmalige Befragung verwendeten und zwei verschiedene Befragungen für jede Sätze bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, zeigte einen Unterschied von mehr als einem BLEURT-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkten reichen. Es ist also wichtig, eine gute Auswahlstrategie für die Befragung zu wählen. In unseren Experimenten wählten wir eine 5-Schuss-Befragungsstrategie, bei der wir jede Sätze, die wir dem System bereitstellen, mit der Sprache markierten, aus der sie stammen. In diesem Beispiel, in dem wir die Übersetzung von Deutsch ins Englische durchführen, markieren wir die deutschen Sätze, die Quellsätze, mit "Deutsch: " und die englischen Übersetzungen mit "Englisch: ". Wir sahen, dass die tatsächliche Form der Befragung keinen großen Einfluss hat, wenn es sich um mehrere kurze Befragungen handelt. Es ist jedoch für die Null- und Einer-Schuss-Befragung entscheidend. Und wenn wir, wie in unserem Fall, zu fünf Schuss-Befragungen gehen, gibt es fast keinen Unterschied in der tatsächlichen Form der Befragung. Es sind die Beispiele, die den größten Wert haben. Die Zusammenfassung unserer experimentellen Ergebnisse lautet, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellsatz. Es ist also wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Befragungen aus der Trainingsdaten für die WMT-Bewertung auf der dev-Daten. Die dev-Daten sind viel mehr gepflegt und haben eine höhere Qualität als die Trainingsdaten, die lauter sind. Und ihre Ergebnisse zeigen eine bessere Leistung, wenn die dev-Daten verwendet werden. Dennoch haben spezialisierte Stand-der-Technik-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt dennoch sehr nah an ein kommerzielles System heran. In unserem Fall wählten wir, um zu bewerten, Google Translate. Die Einsichten, die wir aus der Bewertung durch Menschen gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, sagen, dass die Flüssigkeit von PaLM mit Stand-der-Technik-Systemen vergleichbar ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint, dass PaLM sich entscheidet, eine bessere klingende Übersetzung zu produzieren, indem es manchmal Teile des Quellsatzes entfernt, die in der Übersetzung nicht vorkommen. Der "Style/Awkward"-Kategorie für PaLM ist jedoch niedriger als für die Stand-der-Technik-Systeme, was ein zusätzliches Signal ist, dass PaLM sehr flüssige Ausgaben liefert, aber mit einigen Problemen bei der Genauigkeit. Und das ist es für diese kurze Besprechung. Für weitere Details bitte besuchen Sie die vollständige Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: Ein kritischer Blick auf schwach überwachtes Lernen" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte beginnen, indem ich eine kurze Einführung in das schwach überwachte Lernen gebe. 

Im schwach überwachten Lernen werden die Daten nicht manuell beschriftet. Stattdessen verwenden wir schwache Beschriftungsquellen, wie einfache Heuristiken, Wissensbasen oder geringe Qualität von Crowdsourcing, wie auf der rechten Abbildung gezeigt. Im Vergleich zu menschlichen Anmerkungen sind die schwachen Anmerkungen viel günstiger, aber sie sind auch lauter, was bedeutet, dass ein gewisser Prozentsatz der Anmerkungen falsch ist. Wenn wir neuronale Netze direkt auf schwach beschriftete Daten trainieren, neigen sie dazu, die Beschriftungsgeräusche zu memorieren und generalisieren sie nicht. Im schwach überwachten Lernen werden Trainieralgorithmen vorgeschlagen, um neuronale Netze robust auf solche Beschriftungsgeräusche zu trainieren, so dass die trainierten Modelle immer noch gut generalisieren. 

In jüngeren Arbeiten im WSL (schwach überwachtes Lernen) wird oft behauptet, dass man nur Modelle auf schwach beschrifteten Daten trainiert und dabei eine hohe Leistung auf sauberen Testsets erreicht. Technisch ist diese Behauptung nicht falsch, aber es gibt einen Haken, nämlich dass man annehmen muss, dass es einen zusätzlichen sauberen Validierungssatz gibt, um das Modell zu wählen. Wir können uns nicht auf diesem Problemstellung beschränken, aber dies impliziert, dass zusätzliche manuelle Anmerkungen erforderlich sind im schwach überwachten Lernen. 

Diese Zweifel haben uns zu drei Forschungsfragen geführt. Erstens ist eine saubere Validierungsdaten erforderlich für WSL oder können wir stattdessen einen lauten Validierungssatz verwenden? Zweitens, wenn eine saubere Daten erforderlich ist oder wenn eine saubere Daten obligatorisch für WSL ist, dann wie viele saubere Proben benötigen wir? Schließlich, sollten wir nur die sauberen Proben für Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? 

Wir haben diese Forschungsfragen in unserer Arbeit angegangen und unsere Ergebnisse sind wie folgt. Erstens finden wir, dass aktuelle WSL-Methoden tatsächlich eine saubere Validierungsmenge benötigen, um richtig zu funktionieren. Andernfalls ist ein großer Leistungsabfall zu verzeichnen. Wie auf der Abbildung gezeigt, wenn es keine sauberen Validierungsmengen gibt, dann können die trainierten Modelle nicht generalisieren, was bedeutet, dass das Training sinnlos ist. 

Das bedeutet, dass WSL-Ansätze tatsächlich saubere, manuell beschriftete Daten benötigen, um richtig zu funktionieren, und der Aufwand für die Erlangung sauberer Validierungsmengen sollte nicht ignoriert werden. Unsere zweite Entdeckung ist, dass eine zunehmende Anzahl von sauberen Validierungsmengen die WSL-Ansätze dazu bringen wird, eine bessere Leistung zu erreichen, wie auf der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte, weil wenn wir uns entscheiden, saubere Proben zu nutzen, dann wird das Training auf ihnen direkt sogar bessere Leistungen erzielen. Die rechte Abbildung zeigt die Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. 

Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt das direkte Fine-Tuning, die WSL-Ansätze zu überholen. Schließlich kann die in früheren WSL-Ansätzen beanspruchte Leistungserhöhung leicht erreicht werden, indem man das Training auf den sauberen Validierungsmengen fortsetzt. Wie wir aus den Abbildungen sehen können, die Vanillamodelle (FTw) beginnen zunächst, die komplexeren WSL-Ansätze zu unterbieten, aber wenn wir das Training auf den sauberen Proben fortsetzen, dann erreichen sie die Leistung der anderen Methoden. 

In der Praxis gibt es also keinen Grund, komplexere WSL-Ansätze zu wählen, die mehr Rechenzeit und Speicherplatz benötigen. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell beschriftete Proben benötigen, um richtig zu funktionieren. Ihre Leistungserhöhung und Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens sollte die Modellauswahlkriterien gemeldet werden. Zum Beispiel sollte gemeldet werden, ob die Modellauswahl via saubere Validierungsmengen erfolgt. Zweitens sollten WSL-Ansätze mit Baseline-Few-Shot-Lernmethoden verglichen werden, da beide auf sauberen Proben arbeiten. Drittens ist die kontinuierliche Fine-Tuning ein einfacher aber starker Baseline, der in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unsere Code geöffnet. Sie können ihn über den QR-Code auf dieser Präsentation finden. Bitte fühlen Sie sich frei, ihn auszuprobieren. Vielen Dank und genießen Sie die Konferenz.</sample>
    <sample id="241">In our paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," we propose eine evaluation framework for the development of systems that address the deficiencies of current misinformation detection methods. Our framework is end-to-end, meaning it goes from raw tweets on Twitter to actionable outputs used by humans, and has well-integrated human feedback throughout the process. 

We implemented and evaluated a novel workflow for COVID-19 treatment misinformation, which consists of two main components: the detection of misleading claims and policy violation verification. The first component uses a T5 model for claim extraction and ranking, while the second component uses a BERT-based stance classification model to determine the author's stance in the tweet.

Our evaluation focuses on early detection, which we operationalize as the detection of an unapproved treatment before its first appearance in the debunking news article. We found that our system has a 65% position in policy violation detection and can detect 124.2 policy violations per human hour worked. Our framework more realistically captures the complex interplay between systems and human content moderators, and we hope that our work motivates the development of future human-in-the-loop misinformation detection systems.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind:

1. Human-Evaluation: Menschen bewerten Gespräche auf einer Likert-Skala oder wählen zwischen zwei Gesprächen.
2. Likert-Ratings auf der Turn-Ebene: Menschen bewerten jede Antwort auf einer Likert-Skala.
3. Likert-Ratings auf der Dialog-Ebene: Menschen bewerten das Gespräch als Ganzes auf einer Likert-Skala.
4. Dialog-Ebene Pairwise-Vergleiche: Menschen vergleichen zwei Gespräche und wählen das bessere aus.</sample>
    <sample id="243">Es sind 5 Autoren an der Arbeit beteiligt: Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Das Hintergrundwissen, das im Beispiel mit Servin und Kea benötigt wird, ist, dass "Judges decide cases in law courts", also dass Richter Fälle in Gerichtshöfen entscheiden.</sample>
    <sample id="245">Title: A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization

Abstract:

This study presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The pipeline consists of qualification settings, a two-stage qualification task, and an endurance task. The qualification task evaluates workers' ability to evaluate multiple dimensions correctly, while the endurance task tests their capacity for handling heavy workloads. The pipeline results in 26 high-agreement workers, 8 gold and 18 silver, who achieve high agreement with experts and outperform baseline MTurk workers. The reference-based task shows that 8 out of 12 pipeline workers finished all HITs, with a Krippendorff's Alpha of 0.534. The study also compares the pipeline workers with CloudResearch MTurk workers and finds a significant correlation between them. However, the pipeline may not guarantee the training of correctness. The study concludes that pre-task filtering can avoid resource waste and achieve high agreement at a lower cost. Future work will investigate ways to hire high-quality workers and apply the pipeline to various tasks, languages, and platforms.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Sie können ihn auf GitHub finden.</sample>
    <sample id="247">Title: FACTKG: Fact Verification via Reasoning on Knowledge Graphs

Abstract:

This paper introduces a new task, Knowledge Graph-Based Fact Verification, which utilizes knowledge graphs as evidence for fact verification. The proposed dataset, FactKG, consists of natural language claims and corresponding evidence from DBpedia, a widely used knowledge graph. The dataset includes two claim styles (written and colloquial) and five types of reasoning (one-hop, conjunction, existence, multi-hop, and negation). To create the colloquial style claims, a transfer model and presupposition templates were used. The dataset statistics show a balanced distribution of claims and evidence. Baselines were established using the claims only and the GEAR model, which utilizes graph evidence. The results show that all baselines outperform the majority class baseline, and the GEAR model outperforms all other baselines. FactKG provides a new benchmark for fact verification tasks and can be applied to various domains, such as dialogue systems and consistency checks between knowledge graphs and natural language.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Es gab jedoch eine große Anzahl von Annotatoren aus 87 Ländern, nämlich über 1000 Annotatoren, die insgesamt über 16.000 Annotationen lieferten.</sample>
    <sample id="249">Wir haben durchgeführt, eine Reihe von Analysen, bei denen wir versucht haben, die Eingabensätze durch Hinzufügen von "Lärm" zu verändern, während die relevante Struktur erhalten blieb.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Bewertung von mehreren Aspekten oder Dimensionen einer Konversation, wie z.B. Relevanz, Kohärenz, Empathie und mehr, anstatt sich auf eine globale Bewertung zu konzentrieren.</sample>
    <sample id="251">Die Universität, der die Autoren, Jingwei Yi, angehören, ist die University of Science and Technology of China.</sample>
    <sample id="252">Title: U-CREAT: Unsupervised Case Retrieval using Events extrAcTion

Abstract:

The Prior Case Retrieval (PCR) task involves retrieving relevant cases from a candidate pool based on a legal query document. In this work, we present U-CREAT, an unsupervised pipeline that leverages event-based approaches for PCR tasks. We introduce the Indian Legal Prior Case Retrieval (IL-PCR) dataset, a comprehensive benchmark for PCR tasks, containing 7,070 legal cases with 6.775 average citations per query document. Our pipeline, U-CREAT, demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. We conduct experiments using diverse models, including count-based, transformer-based, and event-based models, and find that event-based models significantly outperform baseline methods. The Event Filtered Documents model is the best-performing model, achieving a significant boost in performance. Our approach outperforms existing methods, including a recent supervised approach, and is the current state-of-the-art method for the COLIEE'21 document retrieval task. U-CREAT opens up avenues for further exploration and development in the field of prior case retrieval.</sample>
    <sample id="253">Title: DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media

Abstract:

Mental health disorders are a growing concern, and social media provides a vast opportunity to analyze online behavior and detect early signs of mental health issues. This work presents DisorBERT, a double domain adaptation model that combines the general knowledge of BERT with the specific language and knowledge of Reddit and mental health. The proposed approach uses guided masking to focus on important words during training, resulting in a model that can detect signs of mental disorders in social media interactions. Evaluations using the eRisk dataset show that DisorBERT achieves a good balance between precision and recall, outperforming baselines and MentalBERT. The model's ability to generate words related to mental disorders is demonstrated through case studies using the Beck's Depression Inventory. The results suggest that DisorBERT can be a valuable tool for early detection and intervention of mental health disorders. Future work includes exploring the application of different lexical resources and clinical data.</sample>
    <sample id="254">Title: Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction

Abstract:

Document-level relation extraction is a challenging task that aims to extract relations among entities in a document. Recent work has leveraged distantly supervised data to pretrain document-level relation extraction models, but these data contain various noise levels. To alleviate the noise problem, we propose a document-level relation distant extraction framework with uncertainty-guided label denoising. Our framework first trains a pre-denoising DocRE model with both distant supervision (DS) and human-annotated data to generate pseudo labels. We introduce uncertainty estimation to determine whether model predictions can be trusted or not, and propose an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations. We also design a re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost performance. Experimental results show that our framework outperforms previous baselines on two public datasets. Our contributions include: (1) a framework with uncertainty-guided label denoising, (2) an instance-level uncertainty estimation method for overlapping relations, (3) an iterative re-label strategy with dynamic class uncertainty thresholds, and (4) great performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei Zero-Shot- und One-Shot-Prompts. Bei mehreren kurzen Prompts hat sie jedoch keinen großen Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Chatmodelle evaluiert, aber die genauen Namen dieser Modelle werden nicht im Text erwähnt.</sample>
    <sample id="258">In der Studie "Can Large Language Models Be an Alternative to Human Evaluation?" wird untersucht, ob große Sprachmodelle als Alternative zur menschlichen Bewertung in der natürlichen Sprachverarbeitung verwendet werden können. Die Forscher stellen fest, dass große Sprachmodelle, die natural language instructions (NLI) folgen können, als Bewertungsinstrument genutzt werden können. Sie verwenden vier verschiedene große Sprachmodelle (T0, InstructGPT (Curie und Davinci) und ChatGPT) und stellen fest, dass zwei Modelle (Davinci und ChatGPT) eine klare Vorliebe für menschgeschriebene Texte zeigen, ähnlich wie menschliche Bewertende (englische Lehrer). Die Ergebnisse legen nahe, dass große Sprachmodelle als Alternative zur menschlichen Bewertung in bestimmten Aufgaben verwendet werden können. Die Studie untersucht auch die Frage, ob die Ergebnisse der großen Sprachmodelle mit denen der menschlichen Bewertenden übereinstimmen und wie sich die Ergebnisse ändern, wenn die Wortungen in den Anweisungen oder die Auswahl der Antworten aus den großen Sprachmodellen geändert werden. Die Studie bietet auch einen Vergleich zwischen den Vorteilen und Nachteilen der Verwendung von großen Sprachmodellen zur Bewertung im Vergleich zu menschlicher Bewertung.</sample>
    <sample id="259">Title: XSemPLR: A Unified Benchmark for Cross-Lingual Semantic Parsing

Abstract:

We present XSemPLR, a unified benchmark for cross-lingual semantic parsing in multiple natural languages and meaning representations. Our benchmark addresses the limitations of existing models, which are often evaluated on limited tasks and applications, with limited coverage of natural languages and meaning representations. XSemPLR provides a uniform dataset with 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. We evaluate three types of multilingual language models: Encoder-PTR, Encoder-Decoder, and multilingual language models. Our results show that Encoder-Decoder models achieve the best performance on all nine datasets. We also find that multilingual pretraining can improve performance, but the "Curse of Multilinguality" affects English performance. Our findings demonstrate the significance of cross-lingual transfer and the need for more research in this area. XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing, enabling researchers to compare and improve their models.</sample>
    <sample id="260">Ich bin nicht in der Lage, die Anzahl der Autoren in der Arbeit zu bestimmen, da dies in dem bereitgestellten Text nicht erwähnt wird.</sample>
    <sample id="261">Ein guter Planer sollte Schritte schreiben, die vernünftig und treu zu den Einschränkungen sind.</sample>
    <sample id="262">Ich habe keine Informationen über die Anzahl der Autoren in der Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning".</sample>
    <sample id="263">Title: Mitigating Label Biases for In-Context Learning

Abstract:

In-context learning, a popular paradigm for utilizing large language models, is known to be unstable due to various design choices, such as the choice and order of in-context examples. Label biases, introduced by these design choices, significantly impact the models' predictions. However, prior work has not systematically discussed existing findings on bias problems in in-context learning or proposed methods to mitigate their effects.

This work addresses these problems within the context of text classification. We introduce a typology of label biases, identifying a new type of bias, domain-label bias, which captures the effect of the task corpus on the model's predictions. To confirm this, we conduct experiments demonstrating that seeing random in-domain words can severely bias the model's predictions, whereas random English words do not.

To handle domain-label bias and other types of biases, we propose domain-context calibration, a novel calibration method that uses random in-domain words to estimate the model's bias and calibrate its predictions. Experiments on various datasets and models show that domain-context calibration significantly improves the average performance of in-context learning, particularly on tasks with large domain-label bias.</sample>
    <sample id="264">Title: Transferable Audio-Visual Text Generation (TAVT): A Novel Approach for Multimodal Domain Shifts

Abstract:

Transferable Audio-Visual Text Generation (TAVT) is a novel task that addresses the challenge of multimodal domain shifts in audio-visual text generation. The proposed framework consists of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The framework is designed to learn and adapt to new multimodal domains with limited labeled data. The audio-visual meta-mapper network maps different visual concepts across domains into a unified auditory semantic space, while the audio-visual encoder and generator use a transformer-based architecture to generate text. The Dual Counterfactual Contrastive Learning (DCLL) loss function is introduced to optimize the visual-textual alignment. Experimental results on two benchmarks show that TAVT outperforms state-of-the-art approaches on all metrics, especially in low-resource domains. The proposed framework demonstrates its effectiveness in transferable audio-visual text generation and has the potential to be applied in various multimedia applications.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Es ist nicht explizit angegeben, an welcher Universität der Autor Adam Przepiórkowski ist.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, d.h. die Abwesenheit wichtiger Teile der Quellsatz in der Übersetzung.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationellem KI-Systemen, erzählen. Diese Arbeit wurde von der Emory NLP-Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. 

Lassen Sie uns sagen, dass Sie gerade ein Dialogmodell entwickelt haben und wollen sehen, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die übliche Praxis ist die Verwendung von menschlicher Bewertung, wie zum Beispiel, indem man Menschen fragt, welche von zwei Konversationen besser ist oder Konversationen anhand einer Likert-Skala bewertet. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesamtdialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. 

Eine Möglichkeit ist es, einfach Menschen zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz von Modellantworten mit existierenden vergleichenden oder Likert-Skala-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unsere Methode versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem man explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel, wenn das Modell irrelevantes Informationen beantwortet oder sich selbst widerspricht. Wir nennen diesen Ansatz "annotieren von Verhaltensweisen in Chat" oder ABC-Eval in Kurzform. 

Wir haben diese Methode entwickelt, um eine umfassende Abdeckung von Chat-Modellverhaltensweisen zu erreichen, die in der jüngsten Literatur als beeinflussend auf die Chat-Qualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Schritte, in denen ein Chat-Modell seinen Partner ignoriert oder irrelevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder gegen gemeinsames Wissen verstößt, und wenn das Modell Erfolg oder Misserfolg bei der Darstellung von Empathie zeigt. 

Um zu bestimmen, welche Bewertung am effektivsten ist, haben wir vier state-of-the-art-Chat-Modelle ausgewählt und sie auf 100 mensch-bot-Konversationen pro Modell mit ABC-Eval bewertet. Als Vergleich haben wir auch diese Konversationen mit drei existierenden Methoden bewertet: Likert-Bewertungen auf der Schritt-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paar-Vergleiche. Für jede der existierenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte der Konversation gesammelt, da dies die Standardpraxis zur Bewertung von Chat-Modellen auf mehrere Dimensionen ist. 

Aus unserer Analyse der Bewertungsergebnisse haben wir gefunden, dass ABC-Eval-Verhaltensetiketten im Allgemeinen zuverlässiger sind als Etiketten, die mit existierenden Methoden gesammelt wurden, wie gemessen an der Interannotator-Einigung auf 100 doppelt beschrifteten Konversationen. Darüber hinaus sind ABC-Eval-Etiketten vorhersehbarer für die Gesamtkonversationsqualität im Vergleich zu Metriken, die von existierenden Methoden produziert werden, wie durch diese einfache lineare Regressionsanalyse gezeigt wird. 

Beispielhaft zeigt sich, wie das Messen der Proportion der Schritte mit Selbst- und Partnerwidersprüchen 5% bzw. 10% der Konversationsqualität erklärt, während die Durchschnittslikert-Konsistenzskorrelationen nur 4% oder weniger erklären. Schließlich haben wir überprüft, ob jeder Bewertungsmetriken eine einzigartige Aspekt der Chat-Qualität erfassen kann, indem wir eine stepwise lineare Regressionsanalyse durchgeführt haben. 

Wie man sehen kann, erklärt die Kombination aller ABC-Eval-Metriken über 25% der Konversationsqualität, und wenn man die Metriken einzeln entfernt, resultiert das meistens in einem Verlust von Informationen über die Qualität. Im Gegensatz dazu erklärt die Kombination aller Schritt-Ebene-Likert-Metriken viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei. 

Diese zuverlässigen, informierenden und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationelles KI-Systeme mit einer höheren Auflösung zu bewerten als vorhergehende Methoden. Man kann sehen, dass in den Ergebnissen unserer Experimente noch einige Herausforderungen bestehen und genau quantifiziert wurden. 

Beispielhaft sind die Bots, die wir getestet haben, haben in etwa 20% ihrer Antworten gemeinsames Wissen verletzt. Sie produzieren in etwa 15% der Antworten irrelevantes Informationen und widersprechen sich selbst oder ihren Partner in etwa 10% der Zeit. Mit dem schnellen Tempo des Fortschritts in der Branche könnten viele dieser Fehlerraten in neuen Modellen seit unserer Bewertung verringert werden. 

Daher ist es jedoch noch mehr Grund, zuverlässige und präzise Bewertungsmetriken für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in der Branche als ein bedeutender Schritt in diese Richtung genutzt werden kann. Wir freuen uns darauf, sehen zu können, wie konversationelles KI-Systeme in den nächsten Monaten und Jahren vorankommen. Vielen Dank für das Zuschauen.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">FTw (F) steht für "vanilla model", d. h. ein einfacher Modelltyp, während CFT (C) für "Continuously Fine-Tuning" steht, also das kontinuierliche Fine-Tuning des Modells.</sample>
    <sample id="272">Es sind 7 Autoren an der Arbeit beteiligt: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, Adina Williams und Koustav Sinha.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde meine Arbeit mit dem Titel "Wenn Übersetzung Kontext erfordert? Eine Datengetriebene, Mehrsprachige Erforschung" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie würde man das Wort "Mole" in diesem Satz übersetzen? Wenn das vorherige Satz "Dinge könnten gefährlich werden, wenn die Minister es herausfinden", war, dann würde "Mole" einen Spion bedeuten. Aber wenn das vorherige Satz "Könnte es etwas Ernstes sein, Herr Doktor?", war, dann würde "Mole" eine Geburtsmark bedeuten. Je nach Kontext ändert sich also der Bedeutung des Wortes und daher auch seine Übersetzung. Allerdings ist es schwierig, herauszufinden, wie gut Modelle diese Fälle übersetzen können. Zunächst liegt es daran, dass nur ein kleiner Teil der Übersetzungen von Kontext abhängt, was bedeutet, dass Korpus-basierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Und einige Menschen haben vorgeschlagen, dass man spezielle Bewertungen auf Übersetzungen durchführen sollte, die von Kontext abhängen, aber diese Ressourcen unterstützen nur begrenzte Arten von Übersetzungen, die von Kontext abhängen, und begrenzte Sprachen, da sie normalerweise auf Domänwissen und auf menschliche Kuration zurückgreifen. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Zuerst, wann erfordert Übersetzung Kontext? Und zweitens, wie gut können Modelle diese Fälle übersetzen? Um die erste Frage zu beantworten, begannen wir damit, zu messen, wie viel ein Wort von Kontext abhängt, wenn es übersetzt wird. In der vorherigen Arbeit haben wir CXMI als eine Maßzahl für die Kontextnutzung von Übersetzungmodellen eingeführt. Dies wird erreicht, indem die Menge an Informationen bestimmt wird, die der Kontext C über das Ziel Y gibt, gegeben der Quelle X. Man kann CXMI als die Information denken, die man gewinnt, wenn man Kontext zum Modell gibt. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, was Kontextnutzung auf Satzebene oder auf Wortebene messen kann. Man kann also Wörter mit hoher P-CXMI als Wörter denken, die Kontext für die Übersetzung benötigen. Wir analysieren nun Wörter mit hoher P-CXMI, um Muster zwischen diesen Wörtern zu finden. Wir führen unsere Analyse auf Transkripte von TED-Vorträgen durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst schauen wir uns die Teile der Sprache an, die hohe mittlere P-CXMI haben. Dies ermöglicht es uns, beispielsweise in Arabisch doppelte Pronomen zu finden, die eine relativ hohe P-CXMI haben. Dies kann erklärt werden, weil Englisch keine doppelten Pronomen hat, daher benötigt man Kontext, um zu bestimmen, ob ein Pronomen doppelt ist, wenn man es in Arabisch übersetzt. Ebenso finden wir heraus, dass bestimmte Sprachen Kontext benötigen, wenn man die richtige Verbform wählen möchte. Wir schauen uns dann an, welche Vokabeln hohe P-CXMI haben, wenn man sie über alle ihrer verschiedenen Vorkommen durchschnittlich nimmt. Dies hilft uns, Fälle zu identifizieren, wie den hier, wo in Chinesisch Kontext benötigt wird, um Eigennamen zu übersetzen, um sicherzustellen, dass man dieselbe Übersetzung innerhalb des Dokuments verwendet. Ebenso finden wir heraus, dass Kontext wichtig ist, um die richtige Formulierung zu wählen. Schließlich schauen wir uns an, welche einzelnen Tokens hohe P-CXMI haben. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher durch die Satzstruktur ausgedrückt werden, wie die Ellipsenlösung. Nun verwenden wir unsere Ergebnisse aus der Analyse, um ein Benchmark für Dokumentenübersetzung zu erstellen. Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um Wörter zu identifizieren, die diesem Phänomen zugehören. Wir nennen unseren Tagger Multilingual Discourse-Aware, oder MuDA-Tagger. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskursphänomene haben. Wir verwenden dann den MuDA-Tagger, indem wir ihn auf ein paralleles Korpus anwenden, das wir für die Bewertung verwenden möchten, und wir wenden unsere bevorzugten Übersetzungs-Metriken auf die von Kontext abhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Schließlich verwenden wir unser Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokumentenübersetzung zu bewerten. Zuerst verwenden wir Korpus-basierte Metriken: Wenn wir also BLEU verwenden, finden wir heraus, dass Kontext-agnostische Modelle die beste Leistung haben. Aber wenn wir COMET verwenden, haben Kontext-bezogene Modelle die beste Leistung. Und wenn wir den Wort-f-Maß verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistung. Dies zeigt wieder einmal, dass es schwierig ist, die beste Dokumentenübersetzungssystem zu bestimmen, wenn man nur Korpus-basierte Metriken verwendet. Nun verwenden wir das MuDA-Benchmark, um Modelle zu bewerten und finden heraus, dass Kontext-bezogene Modelle signifikant genauer sind als Modelle, die Kontext nicht verwenden, für bestimmte Diskursphänomene wie Formulierung und Lexikalische Kohäsion. Aber diese Modelle sind nicht viel besser als Modelle, die Kontext nicht verwenden, bei anderen Phänomenen wie Ellipsen, Pronomen und Verbform. Dies schlägt vor, wo wir mehr Fortschritte sehen sollten, um Dokumentenübersetzung zu verbessern. Wir vergleichen auch verschiedene kommerzielle Systeme und unser Benchmark zeigt, dass DeepL normalerweise genauer ist als Google Translate bei der Dokumentenübersetzung. Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um herauszufinden, wann Übersetzungen Kontext erfordern und verwenden dann unsere Ergebnisse, um ein Benchmark für Dokumentenübersetzung zu erstellen, das uns hilft, herauszufinden, welche Diskursphänomene Modelle gut oder schlecht können, und welche Übersetzungs-Systeme gut sind bei der Dokumentenübersetzung. Vielen Dank für Ihre Aufmerksamkeit. Bis Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">In "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages" präsentieren Ananya und Vignesh ihre Arbeit auf dem Gebiet der Bewertung von Übersetzungsmodellen für indische Sprachen. Sie stellen fest, dass die Bewertung von Übersetzungen in die andere Richtung (z.B. Englisch zu Hindi) untersucht wird, obwohl es wichtig ist, dass die Bewertungsmetriken für andere Sprachen als Englisch entwickelt werden. Ihre Arbeit konzentriert sich auf fünf indische Sprachen (Tamil, Malayalam, Hindi, Marathi und Gujarati) und enthält eine umfangreiche Bewertung von sieben Übersetzungsmodellen. Sie stellen eine öffentlich zugängliche Datenbank bereit, die die Annotationen von bilingualen Experten enthält, die die Übersetzungen bewertet haben. Die Ergebnisse zeigen, dass die Bewertungsmetriken unterschiedliche Ergebnisse liefern, aber die COMET-metrik die besten Korrelationen mit den menschlichen Bewertungen zeigt. Die Arbeit ist ein wichtiger Schritt in der Entwicklung von Bewertungsmetriken für indische Sprachen und kann die Entwicklung von besseren Übersetzungsmodellen unterstützen.</sample>
    <sample id="277">Die neue Methode hat keinen spezifischen Namen.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Methode, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Sie verwenden die soziolinguistische Konzeption des "Markierten" (markedness), wonach dominante Gruppen in der Gesellschaft unmarkiert sind und marginalisierte Gruppen markiert. Die Methode besteht darin, die markierten Gruppen und die entsprechenden unmarkierten Gruppen zu bestimmen, und dann die log-odds-Rationen der Wörter zu vergleichen, um die markierten Wörter für jede Gruppe zu identifizieren.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">In the paper "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations", Shi Tao et al. präsentieren ein neues Framework zur Emotionserkennung in Gesprächen. Das Framework, MultiEMO, besteht aus vier Komponenten: unimodales Feature-Extraction, Kontextmodellierung, multimodale Fusion und Emotionsklassifikation. Die Autoren stellen drei neue Beiträge vor:

1. VisExtNet, ein neuer visueller Feature-Extractor, der sich auf die Gesichtsausdrücke der Interaktanten konzentriert und redundantes Umfeldinformationen ignoriert.
2. MultiAttn, ein neues multimodales Fusion-Netzwerk, das durch bidirektionale multi-head cross-Attention-Schichten drei Modale (text, audio, visual) integriert.
3. Sample-Weighted Focal Contrastive Loss, ein neues Verlustfunktion, das Minority-Klassen priorisiert und semantisch ähnliche Emotionen besser unterscheidbar macht.

Die Experimente auf den Benchmark-Daten MELD und IEMOCAP zeigen, dass MultiEMO state-of-the-art-Ergebnisse erzielt, insbesondere in Minority- und semantisch ähnlichen Emotionen.</sample>
    <sample id="281">"When Does Translation Require Context? A Data-driven, Multilingual Exploration" ist ein Forschungsbeitrag, der die Bedeutung von Kontext bei der Übersetzung untersucht. Die Autoren analysierten Transkripte von TED-Vorträgen in 14 Sprachen und fanden heraus, dass bestimmte Wörter, Teile von Sätzen und Diskursphänomene von Kontext abhängig sind. Sie entwickelten eine neue Methode, Pointwise CXMI, um die Kontextabhängigkeit von Wörtern zu messen. Die Ergebnisse zeigen, dass bestimmte Sprachen, wie Arabisch, Dualpronomen benötigen, um die richtige Übersetzung zu finden. Außerdem fanden die Autoren heraus, dass Kontext wichtig ist, um die richtige Form und Formalität in der Übersetzung zu wählen.

Die Autoren entwickelten auch einen Benchmark, den MuDA-Tagger, um die Fähigkeit von Übersetzungsmodellen zu bewerten, Kontext zu nutzen. Die Ergebnisse zeigen, dass Kontextbewusste Modelle in bestimmten Diskursphänomenen, wie Formalität und lexikalischer Kohäsion, besser abschneiden als Kontextunbewusste Modelle. Der Benchmark zeigt auch, dass DeepL in der Regel genauer ist als Google Translate bei der Dokumentenübersetzung. Die Ergebnisse dieser Studie können dazu beitragen, die Entwicklung von Übersetzungsmodellen zu verbessern, die Kontext nutzen können.</sample>
    <sample id="282">Title: StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Abstract:

We present StoryTrans, a novel non-parallel story author-style transfer model that addresses the challenge of imitating author linguistic choices at the discourse level. Our model learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. To alleviate the issues of style-specific content transfer, we propose a two-stage training framework. In the first stage, we employ an advisory training framework to disentangle style and content, while in the second stage, we focus on filling style-specific content and removing mask tokens. We evaluate StoryTrans on Chinese and English datasets and conduct extensive experiments to transfer fairytales and everyday stories to typical author styles. Our results show that StoryTrans outperforms strong baselines in terms of style control and content preservation, and its transfer tests align with golden texts in the style feature space. Furthermore, StoryTrans can enrich storylines and maintain source semantics, making it a valuable tool for natural language generation.</sample>
    <sample id="283">Prag</sample>
    <sample id="284">Title: FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction

Abstract:

This paper proposes a novel approach to universal information extraction (UIE) called FSUIE, which addresses two major limitations of current span-based UIE models. Firstly, the models rely heavily on precise span boundary annotations, which can be ambiguous. Secondly, the transformer feature extraction mechanism ignores the prior hypothesis that spans have limited lengths. To address these issues, FSUIE introduces a fuzzy span mechanism, which learns a continuous distribution of correct probabilities for the span boundary. This is achieved through a novel fuzzy span loss function and a fuzzy span attention mechanism, which adaptively adjusts the attention span of the model. The proposed approach is evaluated on three main information extraction tasks: named entity recognition, relationship extraction, and aspect sentiment triplet extraction. Experimental results show that FSUIE achieves significant performance improvements compared to baseline models, particularly on small-scale datasets. The proposed approach also demonstrates excellent generalization capabilities and competitive performance on various IE tasks.</sample>
    <sample id="285">Title: Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework

Abstract:

Factual errors in summaries generated by models and reference summaries pose a significant challenge in dialogue summarization. Current Factual Error Correction (FEC) models have been evaluated using factuality metrics such as FactCC and DAE, which have two major flaws: they provide overall scores that are vague and may not be reliable, and they blur the line between correction and generation. To address these issues, we propose a new evaluation framework that incorporates manually annotated reference corrections. We introduce a taxonomy of factual errors, classifying them into content-based and form-based categories. Our framework consists of three steps: alignment, classification, and comparison. We experiment with FEC models in different training modes and find that training with reference summaries from dialogue summarization datasets yields the best results. Our findings highlight the need to change evaluation methods for FEC models and introduce human-corrected summaries during training. We also explore combining human-annotated data with synthetic data and identify areas where current FEC models struggle, such as correcting addition errors and addressing attribute, modality, and link errors.</sample>
    <sample id="286">Professor Jinho Choi</sample>
    <sample id="287">Es sind 4 Autoren an der Arbeit beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Die Datensätze BLiMP und SyntaxGym können zum Testen syntaktischer Phänomene verwendet werden.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden sind nicht explizit genannt. Es wird jedoch erwähnt, dass die Methode "FTw" (Vanilla-Modell) und "COSINE" (eine komplexere WSL-Methode) verwendet werden.</sample>
    <sample id="291">Das Modell wird anhand der folgenden Aufgaben evaluiert: 

- Named Entity Recognition (Nennung von Entitäten)
- Classification (Klassifizierung)
- Part-of-Speech Tagging (Teil des Satzes)
- Fragebeantwortung (Question Answering)</sample>
    <sample id="294">Ich konnte keine Informationen über die Daten finden, mit denen CamemBERT ursprünglich trainiert wurde.</sample>
    <sample id="295">Der/die Referent*in heißt Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert ein gemeinsames Projekt zwischen der Universität Turin und Amazon Alexa, das sich mit der Ironiedetektion in natürlicher Sprache beschäftigt. Die Forscher entwickelten den EPIC-Corpus, einen englischsprachigen Datensatz mit 300 kurzen Gesprächen aus verschiedenen Quellen wie Social Media, Reddit und Twitter. Der Datensatz wurde von 74 Annotatoren aus verschiedenen Ländern und Sprachvarietäten annotiert. Die Ergebnisse zeigen, dass die Annotatoren unterschiedliche Vorstellungen von Ironie haben, je nachdem, welche Generation oder Region sie auskommen. Basile und seine Kollegen entwickelten "perspektivbewusste Modelle", die die Annotationen verschiedener Annotatoren berücksichtigen. Diese Modelle zeigten eine höhere Zuverlässigkeit und geringere Unsicherheit im Vergleich zu Modellen, die die aggregierte Goldstandard-Annotation verwenden. Die Forscher schlussfolgern, dass die Unterschiede in den Annotationen durch kulturelle und generationelle Unterschiede verursacht werden.</sample>
    <sample id="297">In dem Forschungsprojekt "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" werden Dogwhistles, also kodiertes Sprachrhetorik, untersucht. Die Forscher entwickeln eine Typologie und ein Glossar mit reichlich Kontextinformationen, um die Bedeutung und den Einfluss von Dogwhistles in der politischen Kommunikation zu verstehen. 

Das Projekt besteht aus drei Teilen: 

1. Die Entwicklung einer Typologie und eines Glossars mit über 340 Begriffen und Symbolen, die als Dogwhistles fungieren, insbesondere in Bezug auf rassistische, transphobe und antisemitische Sprache.
2. Eine Fallstudie historischer US-amerikanischer politischer Reden, um die Häufigkeit von Dogwhistles in der politischen Kommunikation zu analysieren.
3. Eine Evaluation der Erkennung von Dogwhistles durch Sprachmodelle, insbesondere GPT-3, sowie eine Fallstudie der Evasion von Inhalten durch Dogwhistles bei der automatischen Toxizitätsdetection.

Die Forscher stellen fest, dass Dogwhistles eine wichtige Rolle bei der Verbreitung von Hass- und Diskriminierungssprache spielen und dass Sprachmodelle wie GPT-3 nur bedingt in der Lage sind, Dogwhistles zu erkennen und zu verstehen.</sample>
    <sample id="298">Das Experiment, bei dem einige Modelle mit neueren Daten weitertrainiert wurden, zeigte, dass die Leistung mit zunehmendem zeitlichen Abstand zwischen Trainings- und Testdaten abnimmt. Dies bestätigt die Hypothese, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust ist.</sample>
    <sample id="299">Titel: Improving the Robustness of NLI Models with Minimax Training

Abstract:

Neural Language Interface (NLI) Models haben hohe Ergebnisse auf verschiedenen Benchmarks erzielt, aber ihre Robustheit ist eingeschränkt, da sie auf Spurenkorrelationen zwischen Eingabearteilen und Etiketten zurückgreifen, die während der Datensatzgenerierung entstanden sind. Diese Spurenkorrelationen führen dazu, dass NLI-Modelle, die Spuren ausnutzen, gut auf in-Distribution-Beispielen abschneiden, aber bei der Auswertung auf out-of-Distribution-Testsets, bei denen diese Spurenkorrelationen nicht gelten, brüchig werden. Wir schlagen ein Minimax-Trainingsverfahren vor, bei dem ein Lernmodell und ein Hilfsmodell alternierend optimiert werden, um die Abhängigkeit des Lernmodells von Spuren zu reduzieren und seine out-of-Distribution-Performanz zu verbessern. Unser Verfahren macht keine Annahmen über die Art von Spuren, die in einem Datensatz enthalten sind, und verwendet ein Feed-Forward-Netzwerk als Hilfsmodell. Wir evaluieren unser Verfahren auf drei analytischen Datensätzen und deren out-of-Distribution-Testsets und zeigen, dass es die out-of-Distribution-Performanz verbessert, während die in-Distribution-Accuratheit hoch bleibt.</sample>
    <sample id="300">The interactive dictation task is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. This task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The task is characterized by the following key features: flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits, and the ability to invoke edits through vocal commands.

A four-step procedure is formalized for this task: ASR recognition, segmentation, command extraction and normalization, and execution. To collect data for this task, a new interface is designed, which allows users to transcribe text and issue commands using a keyboard and mouse.

A baseline system is built to perform each of these steps, using separate models for ASR recognition, segmentation, command extraction and normalization, and execution. The system is evaluated using exact match of the predicted end-state against the goal end-state, and the results show a trade-off between runtime and accuracy. The study finds that GPT-3 models are more accurate but also slower, and that predicting state directly is more accurate than predicting intermediate programs. The code and paper are released to facilitate future work on this task.</sample>
    <sample id="302">Nach dem ersten Schritt, bei dem jeder Eingabetoken mit einem unsortierten Multiset von Tokenen versehen wird, die in der Ausgabesequenz erscheinen werden, sind die Token noch nicht in der richtigen Reihenfolge. Deshalb ist es notwendig, die Token für die Ausgabesequenz zu permutieren.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, weil sie nicht wissen, ob positive Stereotypen durch Überwertung von positiven Eigenschaften oder durch andere Anti-Vorurteils-Mechanismen entstehen. Ohne Transparenz können sie diese Frage nicht weiter untersuchen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Wörter oder Sätze, die in der Regel als ungrammatisch oder unakzeptabel gelten, wie z.B. ungrammatische Sätze oder Sätze, die stereotype Verhaltensweisen aufzeigen.</sample>
    <sample id="305">In "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", ein Team von Forschern von der Saarland University in Deutschland untersucht die Effektivität von Weakly Supervised Learning (WSL). WSL ermöglicht das Trainieren von neuronalen Netzen mit schwachem Labeling, das billiger ist als traditionelles Labeling, aber auch lauter ist. Die Forscher stellen jedoch fest, dass WSL-Methoden tatsächlich auf saubere Validierungsmuster angewiesen sind, um zu funktionieren, und dass die Performancegewinne durch WSL oft übertrieben werden. Sie finden heraus, dass die Anzahl der sauberen Validierungsmuster erheblich beeinflusst, wie gut WSL-Methoden funktionieren, und dass direkte Fine-Tuning auf sauberen Mustern sogar bessere Ergebnisse liefern kann als WSL-Methoden. Die Forscher empfehlen, in Zukunft die Modellelektionskriterien zu berichten, WSL-Methoden mit Few-Shot-Learning-Baselines zu vergleichen und direkte Fine-Tuning als starkes Baseline zu betrachten. Die Forscher haben ihre Code geöffnet und laden dazu ein, ihn zu überprüfen.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim untersuchen, in welchem Umfang große Sprachmodelle Fähigkeiten zur Erfassung und Verfolgung von Entitäten besitzen. Um dies zu beurteilen, entwickelten sie eine Evaluierungsaufgabe, bei der ein Sprachmodell die Inhalte von Boxen vorhersagen soll, nachdem bestimmte Operationen auf diese Boxen ausgeführt wurden. Die Ergebnisse zeigen, dass die meisten Sprachmodelle, wie Flan-T5 und GPT-3, nur trivialen Tracking-Fähigkeiten besitzen, indem sie die ursprüngliche Box-Inhalte wiederholen. Nur der Text-Davinci-003-Modell zeigt nicht-triviale Tracking-Fähigkeiten. Die Untersuchung der GPT-Serie ergab, dass Modelle, die auf Code trainiert wurden, Fähigkeiten zur Erfassung und Verfolgung von Entitäten besitzen, während Modelle ohne Code-Trainingsdaten nicht. Die Ergebnisse legen nahe, dass die Trainingsdaten des Sprachmodells entscheidend für die Entwicklung dieser Fähigkeiten sind.</sample>
    <sample id="307">Die Autoren haben verschiedene Bewertungsmetriken verwendet, darunter:

- Named Entity Recognition (NER)
- Classification
- Part-of-Speech (POS)-Tagging
- Frage-Antwort-Modellierung (Question Answering)</sample>
    <sample id="308">Die Präsentation von NLPositionality untersucht die Design-Biased von NLP-Datenbanken und Modellen. Die Forscher identifizieren Positionalität als ein Problem, bei dem Modelle und Datenbanken systematische Leistungsunterschiede zwischen verschiedenen Populationen aufweisen. Diese Unterschiede werden durch die Positionalität der Forscher und Entwickler verursacht, die von ihren Demografien, Identitäten und Lebenserfahrungen beeinflusst wird.

Um diese Positionalität zu untersuchen, entwickelten die Forscher das Framework NLPositionality, das zwei Schritte umfasst: die Wiederannotierung von Datenbanken mit diversen Annotatoren und die Vergleichung der Annotationen mit Modellen und Datenbanken. Durch die Vergleichung der Annotationen mit realen Benutzern konnten die Forscher zeigen, dass NLP-Datenbanken und Modelle tendenziell mit englischsprachigen Ländern und Menschen mit höherer Bildung übereinstimmen, aber weniger mit nicht-binären Menschen.

Die Forscher empfehlen, folgende Maßnahmen zu ergreifen, um die Positionalität in NLP zu reduzieren: 

*   Einzelne Designentscheidungen während des Forschungsprozesses zu dokumentieren
*   NLP-Forschung mit dem Fokus auf Perspektivismus durchzuführen
*   Spezialisierte Datenbanken und Modelle innerhalb bestimmter Gemeinschaften zu entwickeln.</sample>
    <sample id="309">Die Übereinstimmung zwischen den Kommentatoren wurde mit der Inter-annotator-Agreement-Methode gemessen.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Die Frage wird nicht beantwortet, da die Universität der Autoren nicht erwähnt wird.</sample>
    <sample id="312">MultiInstruct ist der erste große Skalenumfang multi-modal instruction tuning Benchmark-Datensatz, der 62 diverse multi-modal Aufgaben abdeckt, die in 10 breite Kategorien gegliedert sind. Er ist auch der erste öffentlich verfügbare multi-modal instruction task-Datensatz.</sample>
    <sample id="313">Es sind 2 Autoren, James Finch und Sarah Finch, an der Arbeit beteiligt.</sample>
    <sample id="314">Die binäre Koordination bezieht sich auf die Struktur "Lisa, Bart, und Maggie", wobei ein Teil (hier: Lisa) als Kopf (Head) der gesamten Koordinationsstruktur gilt.</sample>
    <sample id="315">Es wird im Text nicht explizit erwähnt, wie lange die in dieser Studie verwendeten Prompts im Durchschnitt waren. Es wird jedoch erwähnt, dass die Prompts inspiriert von einem Studie waren, in dem Prompts an Menschen gegeben wurden, und dass diese Prompts erfolgreich waren, um Rassismus zu überwinden.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das T5-Modell, wenn es auf CoScript (dem erstellten Dataset) trainiert wird, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle.</sample>
    <sample id="317">Title: CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors

Abstract:

Information extraction is a classic task in natural language processing, involving the extraction of structured information from unstructured text. Traditional approaches use pre-trained language models like T5 and GPT-3, which operate in a text-to-text manner during pre-training but require linearized structured output during inference. This leads to a mismatch between the learned inputs and outputs, making it challenging for the model to generate correct structures.

To address this issue, we propose CodeIE, transforming the text-to-structured information extraction task into a structure-to-structure code generation task using code large language models like Codex. We design prompts for named entity recognition and relation extraction, leveraging few-shot in-context demonstrations to achieve high accuracy. Our evaluation on three recognition datasets and four relation extraction datasets shows that CodeIE significantly outperforms traditional baseline models, including UIE and GPT-3, especially in terms of recall. We also analyze the phenomenon and find that using code format prompts and code pre-training language models better aligns with the information extraction task, reducing structural errors and improving overall performance.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu "DrBERT: Ein robuster vorgeprägter Modell in Französisch für die Biomedizin und klinische Domänen" vorstellen. 

In dieser Präsentation sprechen wir zunächst über die Sprachmodellierung in der Gesundheitsversorgung. Dann stellen wir die Hauptbeiträge unserer Arbeit vor. Wir führen das erste biomedizinische Modell in Französisch, DrBERT, ein, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web gesammelt wurden. Wir stellen außerdem eine Vergleichsuntersuchung von Modellen mit verschiedenen Vortrainings-Einstellungen und Datenquellen vor. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Abwärtsaufgaben in Französisch. Schließlich ziehen wir eine Schlussfolgerung über die Experimente und geben Ihnen weitere Informationen, wie Sie Zugriff auf diese Modelle erhalten können.

Seit seiner Veröffentlichung 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung geworden und bietet erhebliche Leistungsverbesserungen gegenüber historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder anderen. Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie Französisch mit CamemBERT, und auch in Domänen wie Biomedizin mit PubMedBERT und BioBERT und in der Klinik mit ClinicalBERT, aber vor allem auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und werden oft auf kontinuierliche Vortrainings basieren, da es an in-Domain-Daten mangelt. Französisch hatte jedoch bis jetzt keinen offenen Quellcode-Modell für Biomedizin. 

Wir fragen uns also, welche Datenquellen für eine breite Anwendung am besten geeignet sind und ob die gesammelten Daten eine gute Ersatzquelle für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Data Warehouse der Universitätsklinik Nantes stammen. 

Wir fragen uns auch, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Ist es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren wir und vergleichen wir vier von-Scratch-Modelle: Eine erste Version von DrBERT mit 7 GB von NACHOS, eine zweite Version mit 4 GB von NACHOS, eine erste Version von ChuBERT, die auf 4 GB von Satzfragmenten aus klinischen Notizen trainiert wurde, und eine finale Version von ChuBERT, die eine Mischung aus 4 GB von NACHOS und 4 GB von klinischen Notizen verwendet. 

Zusätzlich zu dieser Vergleichsuntersuchung führen wir drei Modelle ein, die auf kontinuierliches Vortrainieren trainiert wurden, um den Einfluss der Vortrainingsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von CamemBERT und wurde auf 4 GB von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, aber wurde diesmal auf 4 GB von klinischen Notizen trainiert. Schließlich ein Modell basiert auf dem englischen biomedizinischen Modell PubMedBERT und wurde auf 4 GB von NACHOS trainiert. 

Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Abwärtsaufgaben wie Namenserkennung, Klassifikation, Teilwort-Tagging und Fragenbeantwortung. Diese Modelle werden mit sechs Baseline-Modellen verglichen, nämlich CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass Modelle am besten auf der Aufgabe mit Daten der gleichen Natur wie die, auf denen das Modell trainiert wurde, leisten. Wir beobachten jedoch auch, dass Daten aus heterogenen Quellen mehr Vielseitigkeit aufweisen. Außerdem zeigt sich, dass die Verwendung von mehr Daten zu besseren Leistungen führt. 

Insgesamt scheint das Vortrainieren von Grund auf höhere Leistungen auf den meisten Aufgaben zu erzielen. Unser Experiment auf kontinuierliches Vortrainieren mit den Gewichten und Tokenisierung von CamemBERT, trainiert auf dem 4 GB-Teil von NACHOS, zeigt jedoch vergleichbare Ergebnisse zu denen, die mit DrBERT 4 GB von-Scratch erzielt wurden. Dies ist jedoch nicht der Fall für das Modell, das auf den Gewichten und Tokenisierung von CamemBERT basiert, das an Stabilitätsproblemen leidet. 

Zusammenfassend lässt sich sagen, dass unser System besser auf neun der elf Abwärtsaufgaben leistet und global die Ergebnisse des generischen Modells, CamemBERT, übertroffen hat. Wir beobachten auch, dass spezialisierte Daten besser sind, aber sich nicht gut skaliert. 

Alle vorgeprägten Modelle, die aus NACHOS stammen, sind auf Hugging Face kostenlos verfügbar und unter der MIT-Lizenz. Alle Trainings-Scripts sind auf unserem GitHub-Repository verfügbar. Vielen Dank für diese Präsentation und wir freuen uns auf den Austausch auf der Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht zwei Lernstrategien:

1. From-scratch-Pre-Training: Dabei wird ein Modell von Grund auf auf einer bestimmten Datenmenge trainiert.
2. Kontinuierliches Pre-Training: Dabei wird ein bestehendes Modell auf einer anderen Datenmenge fortgeführt, indem die Gewichte und Tokenisierung des bestehenden Modells verwendet werden.</sample>
    <sample id="320">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 1. Das bedeutet, dass jedes Verbesserungseinheit auf CoNLL-2003 zu einer Verbesserung von mehr als einer Einheit auf CoNLL++ führt, was darauf hindeutet, dass keine Abnehmenden Rendite beobachtet wird.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde auf verschiedenen Ebenen beurteilt, wie z.B. lexicaler Vereinfachung, Strukturvereinfachung und Gesamtlevel der Vereinfachung.</sample>
    <sample id="322">Title: Uncovering the Moral Foundations of Language Models: An Exploratory Study

Abstract:

This study investigates what language models learn about morality when trained on text data. We apply explainable AI techniques to analyze how morality is expressed in different domains, using a dataset of 35,000 tweets collected across seven domains, including #AllLivesMatter and #BlackLivesMatter. Our findings suggest that language models can recognize fine-grained differences in moral expressions across domains, such as the association of subversion with words like "overthrow" in #AllLivesMatter and "encouragement" in #BlackLivesMatter. We propose that language models learn to prioritize different moral foundations, as described by the Moral Foundation Theory, which posits that humans perceive morality through five distinct aspects. Our research warns that using a single model for multiple domains can lead to misunderstandings of morality, highlighting the importance of domain-specific models and nuanced moral understanding. This study contributes to the development of more accurate and sensitive language models that can capture the complexities of human morality.</sample>
    <sample id="323">Title: Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA

Abstract:

Commonsense QA is a challenging task that requires machines to answer questions relying on common knowledge. Recent works combine language models and knowledge bases to solve this task with good readout. However, they introduce noisy entities and limited interaction between modalities. This paper proposes DHLK, a method that addresses these issues. First, an HKG is built based on multiple knowledge bases through a two-stage pruning strategy and knowledge representation learning. Then, language models are used to encode and fuse the two modalities. The method dynamically removes entities with weaker relevance to the QA context and optimizes entity and relationship embeddings using TransE. Instead of using GNNs, Relation Mask Self-Attention is introduced to model subgraphs. The HKG graph embedding is obtained by applying max-pooling to the question key entities. The method incorporates HKG path information into the QA context and uses MLP to predict the answer probability. Experiments on CommonsenseQA and OpenBookQA show that DHLK outperforms other LM and HKG methods.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. In den Studien, die Shangbin erwähnt, wurden Sprachmodelle wie GPT-4 und BART untersucht und es wurde festgestellt, dass sie unterschiedliche politische Vorurteile haben. Zum Beispiel ist GPT-4 als liberal eingestuft, während BART als konservativer eingestuft wird.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unser Papier über "Kompositionelle Generalisierung ohne Bäume mit Multiset-Tagging und latenten Permutationen" geben. Dies ist eine gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und unerforschte Kompositionen von Phrasen zu handhaben, die einzeln während der Schulung gesehen wurden. Im Kontext der semantischen Verarbeitung könnte die Überprüfung der kompositionellen Generalisierung wie folgt aussehen. 

Wir haben wie gewohnt ein Trainingsset von Aussagen. In diesem Fall "Die Mädchen schliefen." und "Mary wusste, dass die Mädchen schliefen." Diese Aussagen werden mit logischen Formen verbunden, die die Grundlagen ihres Sinns darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernerevaluation stammt das Testset jedoch nicht aus der gleichen Verteilung, sondern enthält strukturunbekannte logische Formen. 

In diesem Beispiel hat der Model während der Schulung flache Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der Generalisierung außerhalb der Verteilung und produzieren oft Ausgaben, die von der Eingabe getrennt sind. Insbesondere scheitern sie oft daran, die systematischen Übereinstimmungen zwischen Eingabe und Ausgabe wiederzugeben, wie sie in dem Beispiel farblich hervorgehoben sind. 

Eine beliebte Methode, um diese Art der Generalisierung anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositionellen Prozess erfassen, der die Aussagen mit den logischen Formen verbindet. Das funktioniert gut, aber Bäume sind normalerweise nicht gegeben und müssen auf andere Weise erhalten werden. Dies kann kompliziert und manchmal eine rechenintensive Prozedur sein. 

Typischerweise ist die Formulierung von Bäumen ein formalism-spezifischer Vorprozess, der zum Beispiel die Verarbeitung von Variabelsymbolen beinhaltet. Die Erstellung von Bäumen kann auch spezielle Grammatikinduktionsprozeduren erfordern. In unserem Papier verwenden wir keine Bäume und stellen ein neuronales seq2seq-Modell vor, das die Übereinstimmungen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. 

Zum ersten Mal zeigen wir eine starke Generalisierung zur tieferen Rekursion ohne auf Bäume angewiesen zu sein. Unsere Ansätze prädizieren die Ausgabe aus der Eingabe in zwei Schritten. Zuerst werden die Eingabe-Tokens mit einem unsortierten Multiset von Tokens markiert, die in der Ausgabe erscheinen. 

Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht in der richtigen Reihenfolge. Deshalb verwenden wir in Schritt zwei ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir stellen eine neue Methode zur Vorhersage der Permutation vor, die keine harten Einschränkungen auf die möglichen Permutationen stellt. 

Dies macht unsere Ansätze sehr flexibel und ausdrucksstark. Konzeptionell funktioniert unsere Permutationsvorhersagemethode ungefähr wie folgt. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Multiset-Token in jede Position gestellt werden soll. 

Für die erste Ausgabestelle wählen wir einfach eines aus, wie in Rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um die zweite Token in der Ausgabe zu bestimmen. Wir bestimmen die dritte Token in der Ausgabe in ähnlicher Weise, indem wir zum nächsten Multiset-Token springen. Wir setzen diesen Prozess fort, bis jeder Token aus der ersten Etappe genau einmal besucht wurde. 

Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir unsere Methode mit anderen treeless-Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen Modelle um einen großen Betrag bei der Generalisierung zur tieferen Rekursion. 

Andere Arten der strukturunbekannten Generalisierung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. 

Zunächst ist die Übereinstimmung zwischen Eingabe und Ausgabe in der Trainingsdaten nicht gegeben. Als Folge wissen wir für einen gegebenen Token nicht, welches Multiset es stammt. Dies stellt eine Herausforderung für das Training dar. 

Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch richtige ist latent. Wir lösen dies, indem wir die Übereinstimmung als Teil des Trainings induzieren. 

Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung mit sich, dass die höchstbewertete Permutation NP-hart ist. Dies liegt daran, dass dies mit dem "Reiseverkäufer"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die auch die Rückpropagation durch die Lösung und das Lernen der linguistisch plausibleren Permutationen ermöglicht. 

Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, möchten, sollten Sie unser Papier oder unsere Poster besuchen.</sample>
    <sample id="326">Kognitive Dissonanz ist die Situation, in der zwei gegensätzliche Überzeugungen oder Handlungen bestehen, die miteinander unvereinbar sind.</sample>
    <sample id="327">In der Arbeit "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" wird ein neues Vision-Language-Architektur vorgestellt, das die Einschränkungen der bestehenden zwei-Turm-Architektur überwindet. Die zwei-Turm-Architektur besteht aus einem textuellen Encoder, einem visuellen Encoder und einem Kreuzmodalencoder. Die unimodalen Encoder wie METER und BridgeTower nutzen jedoch nur die letzten Layer der unimodalen Encoder, was zu einer ineffektiven Nutzung der semantischen Kenntnisse führt.

ManagerTower baut auf BridgeTower auf und verbessert es in zwei Aspekten. Es wird ein neues VL-Modell vorgestellt, das Manager in jedem Kreuzmodallayer verwendet, um die Einsichten der unimodalen Experten zu sammeln und zu kombinieren. Die Manager können die unterschiedlichen Ebenen der unimodalen semantischen Kenntnisse adaptiv ausnutzen, um eine umfassendere Kreuzmodalfusion zu ermöglichen.

Die Ergebnisse zeigen, dass ManagerTower eine bessere Leistung auf verschiedenen Downstream-Tasks erreicht, insbesondere auf Wikivideo-Teststandard, wo es eine Leistung von 39,15% erreicht. Dies zeigt, dass ManagerTower die unterschiedlichen Ebenen der unimodalen semantischen Kenntnisse effektiver ausnutzen kann als BridgeTower.</sample>
    <sample id="328">GPT-4 ist das Sprachmodell, das am meisten links steht.</sample>
    <sample id="329">Title: Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization

Abstract:
Zero-shot video sentence localization is a challenging task that requires finding relevant video segments for a given natural language query. Existing methods rely on pseudo-labels generated from pseudo-events and pseudo-queries, but suffer from simple pseudo-queries, unaligned pseudo-queries and pseudo-events, and label noise. We propose a noise-resistant structured pseudo-label generation method that uses a pre-trained image caption model to generate complex free-form pseudo-queries and a pre-trained model to measure relevance between individual frames and pseudo-queries. We then generate pseudo-events based on event temporal structure, ensuring high relevance within events and low relevance outside events. To reduce label noise, we re-weight noisy samples and refine labels using model confidence and IoU. Our method outperforms existing zero-shot methods on two datasets, achieving best performance on ActivityNet Captions and Charades-STA. Our code is available for further research.</sample>
    <sample id="330">Ja, im Vergleich zu iterativem Training, ist kumulatives Training gleich oder besser.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Vorträgen, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">Title: INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation

Abstract:

Neural machine translation (NMT) models often suffer from a non-smooth representation space, which limits their generalization ability. To address this issue, we propose the INK framework, which injects k-nearest neighbor (kNN) knowledge into the NMT model. INK consists of a training loop that iteratively refines the representation space using kNN knowledge. The loop involves extracting kNN knowledge from a datastore, adjusting the representation using a small adapter, and refreshing the datastore asynchronously. We evaluate INK on the WMT'19 German-English news translation task and achieve state-of-the-art results, outperforming the kNN-MT system by 1.99 COMET score and 1.0 BLEU score. Our experiments show that INK can smooth the representation space with a small adapter and drop the datastore aside during inference, and that jointly applying an adapter and datastore can further improve performance. Overall, INK provides a novel training framework for refining the representation space of NMT models and achieving better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, auf der Grundlage seiner Ausbildung auf einer Sprache, auf eine andere Sprache übertragen zu werden, ohne vorher auf die Ziel-Sprache trainiert zu werden.</sample>
    <sample id="337">Title: Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning

Abstract:

Out-of-vocabulary (OOV) words pose a significant challenge to embedding-based downstream models. To address this issue, we propose a novel approach that leverages word formation and association to infer the meaning of OOV words. Our method, called Word Relationship Graph, imitates the lexical rules of word formation and association by tokenizing OOV words into wordpieces and associating them with relevant words. We use a self-attention network to assign node attributes to OOV nodes and two levels of Graph Attention Network to extract node-level representations. To capture graph information, we incorporate a readout block layer. Our model applies contrastive learning with NT-XENT positive samples to mimic the vector space of the background embedding model. Through extensive experiments, we demonstrate that our model outperforms baselines in both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words by word formation. Our model can be applied to other languages, but its performance depends on the rationality of word decomposition.</sample>
    <sample id="338">Title: Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations

Abstract:

In this study, we investigate the quality of human-annotated explanations in natural language processing tasks. Unlike labels, human explanations can be subjective and task-dependent, making it challenging to evaluate their quality. We propose a unified structure for converting various tasks into a multiple-choice format, allowing for the analysis of explanation utility. Our experiments show that fine-tuning models with explanations does not teach new knowledge, but rather teaches the model to rely on the explanation part of the input. We also find that fine-tuning with a small amount of data incorporating explanations can lead to substantial improvement.

To address the evaluation of human explanations, we propose a novel metric called TREU, which extends the simulatability score. Our metric evaluates the helpfulness of explanations at fine-tuning and consistently ranks dataset qualities across different tasks and models. We demonstrate that our metric outperforms the simulatability score and provides a more accurate evaluation of human explanations. Our findings have implications for high-quality human collaboration in annotation jobs and recommend researchers to perform similar quality checks in the future.</sample>
    <sample id="339">Die Autoren gehören an die Saarland University in Deutschland.</sample>
    <sample id="340">Title: ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation

Abstract:
Paraphrase generation is a crucial task in Natural Language Processing (NLP) that benefits various applications such as question answering, chatbots, and robustness improvement. Existing human-annotated datasets like MRPC, PAN, and Quora are of high quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity. To address this issue, we propose ParaAMR, a large-scale, syntactically diverse paraphrase dataset constructed by Abstract Meaning Representations (AMR) back-translation. Our approach leverages AMR graphs to generate paraphrases with similar semantics but different syntax. We present a dataset of around 15 million source sentences with 6.9 paraphrases per source sentence, demonstrating higher syntactic diversity and similar semantic similarity scores compared to existing datasets. We also show that ParaAMR benefits various NLP applications, including sentence embeddings, syntactic control paraphrase generation, and data augmentation for few-shot learning. Our dataset is available for research purposes.</sample>
    <sample id="341">Die Autoren verwenden zwei Latenzmessungen: 

1. Durchschnittliche Verzögerung (Average Lagging)
2. Berechnete durchschnittliche Verzögerung, die auch die Modellrechenzeit berücksichtigt (Computational-aware Average Lagging)</sample>
    <sample id="342">Title: LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming

Abstract:

We present LiveChat, a large-scale personalized dialogue dataset automatically constructed from live streaming videos on Chinese TikTok, Douyin. To address the limitations of existing open-domain dialogue datasets, LiveChat is designed to be video-sourced, with a larger scale and longer average sessions. We propose a unique automatic dialogue-constructing method, which extracts audio from videos, transcribes it into utterances, and collects audience comments to construct dialogues. Additionally, we collect persona information for personalized dialogue generation, which is categorized into basic profiles and advanced profiles extracted by rules and trained persona classifiers.

We conduct experiments on two benchmark tasks: Response Modeling and Addressee Recognition, and demonstrate the benefits of persona information and longer average sessions in improving the performance of pre-trained dialogue models. Our results show that LiveChat is a valuable resource for developing applications such as virtual streamers and virtual employees, and that it can help address the challenges of multi-party dialogue research in Chinese. We also investigate the performance of pre-trained dialogue models on LiveChat and demonstrate the distinctiveness of our dataset.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "Der KITMUS-Test: Eine Bewertung der Kenntnisintegration aus mehreren Quellen." Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modellierungen für die natürliche Sprachverständigung ziehen sich aus einer Vielzahl von Kenntnisquellen, wie z.B. der im Parameter enthaltenen Kenntnis, die üblicherweise durch eine Vorbereitungsphase erworben wird, und der Kenntnis, die in den Eingabedaten während der Inferenzzeit gegeben wird.  Recent Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle die im Vorbereitungszeitraum erworbenen Kenntnisse nutzen können, um die Aufgabe zu lösen.  Aber das natürliche Sprachverständnis erfordert oft Kenntnisse, die auch während der Inferenzzeit bereitgestellt werden.  Zum Beispiel in der folgenden Aussage, "John sah den neu gewählten Präsidenten auf dem Fernsehen."  Die im Parameter enthaltenen Kenntnisse können Informationen über die Aufgaben eines Präsidenten und die Funktion eines Fernsehers enthalten, aber sie können nicht zuverlässig wissen, wer dieser spezifische, von der Inferenzzeit abhängige Entität "John" ist, oder wer der neue Präsident ist, da der Präsident sich seit der Vorbereitungsphase geändert haben könnte.  Daher erfordern erfolgreiche Modelle für kenntnisintensive Aufgaben im NLU die Fähigkeit, Kenntnisse aus verschiedenen Quellen zu integrieren und zu nutzen.  In dieser Arbeit schlagen wir ein diagnostisches Test-Suite für die Kenntnisintegration vor.  Wir stellen ein Coreferenz-Aufgaben-Test-Suite vor, das darauf abzielt, die Fähigkeit zu testen, Kenntnisse aus verschiedenen Quellen zu nutzen.  Wir bewerten die Datenbank mit menschlichen Studienteilnehmern und etablierten Coreferenz-Modellen.  Hier ist ein Beispiel aus unserer Datenbank.  Servin ist Richter.  Kea ist Bäcker.  Servin und Kea trafen sich in einem Park.  Nach einem langen Arbeitstag im Gericht, war er glücklich, sich zu entspannen.  Die Aufgabe hier ist, die richtige Entität zu identifizieren, auf die das Pronomen "er" sich bezieht, was in diesem Fall Servin ist.  Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen.  Zuerst entitätsspezifische Kenntnisse wie "Servin ist Richter."  Und zweitens Hintergrundkenntnisse wie "Richter entscheiden in Gerichtsverfahren."  Im Allgemeinen werden Hintergrundkenntnisse während der Vorbereitungsphase von großen Sprachmodellen gelernt, während entitätsspezifische Kenntnisse typischerweise während der Inferenzzeit beobachtet werden.  Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen, sodass sie entweder in einer einzelnen Quelle oder in mehreren Quellen gefunden werden können.  Wir haben drei Einstellungen des KITMUS definiert.  Erstens haben wir die typische Einstellung: "Hintergrund-Pretrain", wo die Hintergrundkenntnisse als verfügbar bei der Vorbereitungszeit angenommen werden.  Zweitens gibt es die Einstellung "Hintergrund-Beide", wo die Hintergrundkenntnisse sowohl bei der Vorbereitungszeit als auch während der Inferenzzeit verfügbar sind.  Drittens gibt es die Einstellung "Hintergrund-Inferenz", wo beide Arten von Kenntnissen nur während der Inferenzzeit verfügbar sind.  Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem die Hintergrundkenntnisse, die zur Lösung einer Aufgabe erforderlich sind, nicht Teil der Vorbereitungsdaten von Modellen sind.  Zum Beispiel, weil neue Berufe seit der Vorbereitungszeit entwickelt wurden.  Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den wahren Quellen kontrollieren.  In der Einstellung "Hintergrund-Pretrain" nehmen wir an, dass die Hintergrundkenntnis "Politiker suchen nach gewählten Sitzplätzen in der Regierung" in den im Vorbereitungszeitraum erworbenen Parametern enthalten ist und in der Inferenzzeit wird die entitätsspezifische Kenntnis "Chichester ist Politiker" bereitgestellt.  In der Einstellung "Hintergrund-Beide" wird zusätzlich nicht nur entitätsspezifische, sondern auch Hintergrundkenntnisse über Politiker in ihrem Inferenzzeit-Kontext bereitgestellt.  In der Einstellung "Hintergrund-Inferenz" wird die fiktive Berufsbezeichnung "mirituer" stattdessen von Politiker bereitgestellt, da "mirituer" unwahrscheinlich in den im Vorbereitungszeitraum erworbenen Parametern enthalten ist.  Wir bewerten die Datenbank sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenz-Modellen.  In dieser Abbildung zeigen wir die Ergebnisse der am besten leistenden Modelle im schwierigsten Variante der Einstellung "Hintergrund-Pretrain".  Ohne spezifische Schulung auf KITMUS, leisten beide Modelle nicht gut.  Wenn jedoch auf KITMUS geschult, leisten sowohl C2F als auch BERT4Coref signifikant besser als das zufällige Ergebnis.  Dies deutet darauf hin, dass, wenn auf generische Referenzauflösungsdatensätze geschult, die meisten lernen, Oberflächenmerkmale auszunutzen, die bei der Prüfung auf KITMUS nicht hilfreich sind, da diese Merkmale entfernt wurden.  Zusätzliche Experimente mit fiktiver Kenntnis zeigten, dass selbst die am besten leistenden Modelle Schwierigkeiten haben, Hintergrundkenntnisse, die nur während der Inferenzzeit bereitgestellt werden, zuverlässig zu integrieren.  Insgesamt sind die wichtigsten Ergebnisse unserer Arbeit, dass viele Coreferenz-Modelle nicht in der Lage sind, ohne spezifische Schulung auf KITMUS, über Kenntnisse aus verschiedenen Quellen zu grübeln.  Aber mit spezifischer Schulung können einige Modelle Kenntnisse aus verschiedenen Quellen erfolgreich integrieren.  Dennoch scheinen die am besten leistenden Modelle Schwierigkeiten zu haben, Hintergrundkenntnisse, die nur während der Inferenzzeit bereitgestellt werden, zuverlässig zu integrieren.  Wenn Sie mehr über unsere Arbeit erfahren möchten, können Sie bitte unser Papier lesen und die Datenbank und das Code auf GitHub prüfen.  Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Die baumbasierten Methoden haben einige Nachteile: Sie erfordern oft formalspezifische Vorbereitung der logischen Formen, um Variable-Symbole zu handhaben, und die Gewinnung von Bäumen kann ein komplexes und computenationale teures Verfahren sein.</sample>
    <sample id="345">Title: Compositional Generalization without Trees using Multiset Tagging and Latent Permutations

Abstract:

Compositional generalization refers to a model's ability to handle deeper recursion and unseen compositions of phrases during testing, even if they have been seen individually during training. In this paper, we introduce a neural sequence-to-sequence model that directly models correspondences between input and output fragments without relying on trees. Our approach consists of two steps: multiset tagging and permutation prediction. In the first step, each input token is tagged with an unordered multiset of output tokens. In the second step, we predict a permutation to put the tokens in the right order. We introduce a new method to predict permutations without hard constraints, making our approach flexible and expressive. Experimental results on the COGS benchmark show that our model outperforms other treeless models by a large margin on generalization to deeper recursion. We address challenges such as alignment and permutation ambiguity, and propose a GPU-friendly continuous relaxation to approximate permutation search. Our work demonstrates strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="346">Die Frage nach der Universität der Autoren wird nicht in der Präsentation erwähnt.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Papier "Markierte Persönlichkeiten: Verwendung natürlicher Sprachanweisungen, um Stereotypen in Sprachmodellen zu messen" sprechen. Dieses Werk wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Präsenz sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen, sogenannten LLMs, dokumentiert. Doch diese Maßnahmen haben verschiedene Einschränkungen. Sie basieren oft auf handgefertigten Datensätzen, die sehr zeitaufwändig zu erstellen sind und sie messen auch nur sehr spezifische Stereotypen ab, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen oder sie erfassen nur sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigen die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, die Vorstellung ist, dass mehrdimensionale soziale Identitäten sich gegenseitig verstärken können und einzigartige Angriffsflächen von Schaden sein können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren Anweisungstuning-LLMs sehr gut in der Lage sind, Anweisungen und Anfragen zu beantworten. Also können wir dem Modell beispielsweise befehlen, eine Persönlichkeit zu erstellen, die eine dargestellte Person ist, indem wir eine Anfrage wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." stellen. Und wir können sofort erkennen, dass dies sehr allgemein anwendbar ist, weil wir einfach das gewünschte Identitätsmerkmal in diese Anfrage einsetzen können. Also hier sind einige Beispiele von GPT-4. Sofort sehen wir, dass, während die Ausgaben nicht offensichtlich negativ oder giftig in traditionalem Sinne sind, es einige interessante Muster gibt. Die asiatische Frau wird als zurückhaltend dargestellt; die mittelosteuropäische Frau wird mit Worten wie "exotisch" und "wie, referierend auf eine fesselnde Region" beschrieben. Und beide Frauenpersönlichkeiten der Farbe machen Bezug auf ihre Abstammung, während die weiße Mannpersönlichkeit davon nichts hat. Um diese Muster zu erfassen, hat unser Ansatz zwei Teile. Der erste Teil besteht darin, diese Persönlichkeiten zu erstellen. Unsere Anfragen, um diese Persönlichkeiten zu erstellen, wurden von einer Studie inspiriert, in der sie diesen Anfragen Menschen gaben, und fanden heraus, dass sie auch Stereotypen aufdecken konnten, indem sie es ihnen gaben. Und das ermöglicht auch eine direkte Vergleichbarkeit zwischen unseren erstellten Persönlichkeiten und den von Menschen geschriebenen Antworten. Der zweite Teil ist "Markierte Wörter", eine Methode, um zu erkennen, welche Wörter die markierten Gruppen von den unmarkierten unterscheiden, was ich gleich erklären werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne uns auf eine bestimmte Lexikon abstützen zu müssen. Die Methode "Markierte Wörter" bezieht sich auf den soziolinguistischen Begriff der "Markierung", der besagt, dass es ein unmarkiertes Default gibt, und jede Gruppe, die sich von diesem Default unterscheidet, wird linguistisch markiert. Also zum Beispiel ist das Wort "Krieger" normalerweise mit Männern verbunden. Also wenn Menschen über einen Krieger sprechen, der eine Frau ist, werden sie normalerweise tatsächlich "Frau-Kriegerin" sagen und das Wort mit "Frau" markieren. Und im weitesten Sinne sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während die marginalisierten Gruppen normalerweise markiert sind. Also in unserer Methode bezeichnen wir zuerst, was die unmarkierten und markierten Gruppen sind, und vergleichen dann die Persönlichkeiten mithilfe der Methode Fightin' Words, die eigentlich die Verwendung von gewichteten Log-Odds-Verhältnissen beinhaltet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Also zum Beispiel für die Persönlichkeiten von schwarzen Frauen würden wir Fightin' Words verwenden und die Log-Odds-Verhältnisse gegenüber den beiden entsprechenden unmarkierten Gruppen vergleichen, also gegenüber den weißen Persönlichkeiten und den Mannpersönlichkeiten. Also hier sind einige Ergebnisse. Also zuerst verwenden wir ein Lexikon von Stereotypen und finden heraus, dass die erstellten Persönlichkeiten viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Aber wenn wir uns die Verteilung der Wörter und des Lexikons ansehen, finden wir völlig andere Dinge. Also, während die erstellten Persönlichkeiten viel höhere Raten der Lexikonwörter haben, haben die von Menschen geschriebenen Persönlichkeiten eine viel breitere Verteilung von Wörtern, während die Stereotypwörter in den erstellten Persönlichkeiten einfach nur die Wörter "groß" und "athletisch" sind. Also einfach nur die positiven oder zumindest nicht-negativen. Und tatsächlich fängt dieses Lexikon auch viele der schädlichen Muster, die wir in den früheren Slides sahen, nicht gut auf. Also stattdessen wenden wir uns den Ergebnissen unserer Methode "Markierte Wörter" zu, um zu zeigen, wie diese positiv-schienenden Wörter Stereotypen und essenzialisierende Erzählungen fördern. In unserer Analyse zeigen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Also aus unseren Gruppen sind die Top-Wörter Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als verschieden von der weißen Norm. Dies trägt dazu bei, eine lange Geschichte von Diskriminierung und Andereingefühl bei diesen Gruppen zu verewigen. Darüber hinaus gibt es eine Menge gemeinsamer Klischees, die in diesen Wörtern widergespiegelt werden, insbesondere für Frauen der Farbe. Also zum Beispiel sind die Wörter, die die Latina-Frauen beschreiben, Dinge wie "lebendig" und "kurvig" und diese verbinden sich mit dem Klischee der Tropen. Für asiatische Frauen sind die Wörter Dinge wie "klein" und "delikat" und "seidig" und diese verbinden sich mit einer langen Geschichte, in der asiatische Frauen als über-sexualisiert angesehen wurden, als sehr zurückhaltend und unterwürfig und so weiter. Und schließlich sehen wir bei den schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "resilient" sind. Diese verbinden sich mit einem Archetyp, den Leute den "Starken schwarzen Frauen"-Archetyp nennen. Und während es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass dieser Art von Archetyp tatsächlich sehr schädlich ist, weil er viel Druck auf diese Demografien ausübt, stark und resilient gegen gesellschaftliche Hindernisse zu sein. Also anstatt tatsächlich etwas dagegen zu tun, setzen sie Druck auf diese Leute, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Leute führt, unter anderem. Im weitesten Sinne finden wir, dass die Wörter für jede markierte Gruppe einfach nur sehr essenzialisierende Erzählungen widerspiegeln. Also basierend auf diesen Mustern schließen wir mit drei Empfehlungen für die Eigentümer der Modelle. Also zuerst sollten wir als Forscher positive Stereotypen und essenzialisierende Erzählungen ansprechen. Wir sollten auch einen intersektionalen Blickwinkel verwenden, um Voreingenommenheit und Schaden zu studieren, weil es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es eine erhöhte Transparenz über Voreingenommenheitsvermeidungsmethoden geben, weil zum Beispiel diese positiven Stereotypen, wir wissen nicht, ob es sich um eine Art übermäßige Werteverbindung handelt oder vielleicht um andere Voreingenommenheitsvermeidungsmethoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen machen oder es weiter untersuchen, ohne mehr Transparenz. Vielen Dank für das Zuhören. Habt einen schönen Aufenthalt auf ACL.</sample>
    <sample id="348">In der Arbeit "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" wird ein neuer Ansatz vorgestellt, um soziale Vorurteile und Stereotypen in großen Sprachmodellen zu messen. Durch die Verwendung von natürlichen Sprachanweisungen können Personas generiert werden, die spezifische Stereotypen und Muster aufdecken, ohne auf handgebaute Datenmengen angewiesen zu sein. Die Methode besteht aus zwei Teilen: Die erste Komponente generiert Personas, die auf natürlichen Sprachanweisungen basieren, während die zweite Komponente "Marked Words" verwendet wird, um Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Die Ergebnisse zeigen, dass die generierten Personas mehr Stereotypen enthalten als menschlich geschriebene Texte, aber auch, dass diese Stereotypen oft positive oder neutrale Assoziationen haben, die in Wirklichkeit schädliche Muster verbergen. Die Forscher empfehlen, positive Stereotypen und essentialisierende Narrative zu adressieren, ein intersectionales Linsen zu verwenden und mehr Transparenz bei der Bias-Mitigation zu schaffen.</sample>
    <sample id="349">Hallo, ich bin Jingwei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir ein Vergnügen, eine kurze Werbesendung für unser Papier zu machen. Kopiert ihr meinen Modell? Schutz des Urheberrechts von großen Sprachmodellen für die Einbettung als Dienstleistung über eine Hintertür-Wasserzeichen. Lassen Sie uns zunächst die Hintergrundinformationen über die Einbettung als Dienstleistung vorstellen. Große Sprachmodelle wie GPT, LLAMA und PALM sind hervorragend in der natürlichen Sprachverständigung und -erzeugung. Die Einbettung als Dienstleistung ist eine der Dienstleistungen, die auf großen Sprachmodellen aufgebaut sind, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI ein GPT-basiertes Einbettungs-API an. Allerdings haben kürzliche Arbeiten gezeigt, dass der Angreifer das Modell durch das Lernen von der Einbettung stehlen kann und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, das Urheberrecht der Einbettung als Dienstleistung zu schützen. Um das Urheberrecht der Einbettung als Dienstleistung zu schützen, ist eine der Lösungen darin, ein Wasserzeichen in die Anbieterdienstleistung einzubetten und zu überprüfen, ob sich das Wasserzeichen in einer anderen Dienstleistung befindet. Das Wasserzeichenverfahren muss die folgenden Eigenschaften erfüllen: Zuerst sollte das Verfahren auf Einbettung als Dienstleistung anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen so geschickt sein, dass der Angreifer es leicht entfernen kann. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf das Dienstleistungsmodell des Angreifers übertragen werden können. Bestehende Arbeiten können in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Einbettung als Dienstleistung anwendbar oder fehlt an Transferierbarkeit. Daher schlagen wir in diesem Papier Embedding Marker vor, das eine Hintertür-basierte Wasserzeichenmethode ist, die auf Einbettung als Dienstleistung anwendbar ist. Dann möchte ich die Details unseres Embedding Markers vorstellen. Der Embedding Marker enthält zwei Hauptschritte: Wasserzeicheninjektion und Urheberrechtsprüfung. Bevor diese Hauptschritte beginnen, wählen wir zunächst ein Trigger-Set aus. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, dass der Anbieter ein allgemeines Textkorpus sammeln und mit ihm die Wortfrequenz zählen kann. In der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Einbettung. Wenn ein Benutzer eine Zeichenfolge an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger in der Zeichenfolge. Die bereitgestellte Einbettung ist eine Gewichtssummation der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Trigger in der Zeichenfolge. Wenn die Anzahl der Trigger in der Zeichenfolge größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsprüfung besteht darin, zu überprüfen, ob das Modell hinter einer anderen Dienstleistung das Wasserzeichen enthält. Wir bauen zunächst einen Backdoor und einen schädlichen Datensatz auf. Der Backdoor-Datensatz enthält Zeichenfolgen, die alle Wörter enthalten, die zum Trigger-Set gehören, während alle Wörter in den Zeichenfolgen des schädlichen Datensatzes nicht zum Trigger-Set gehören. Dann fordert der Anbieter die Einbettungen von der Dienstleistungsstelle mit dem Datensatz an. Der Cosinus- und L2-Similarity zwischen der angeforderten Einbettung und der Ziel-Einbettung wird berechnet. Wir berechnen die Differenz zwischen der schädlichen und dem Backdoor-Datensatz, die als delta-Cosinus und delta-L2 definiert ist. Gleichzeitig verwenden wir auch den KS-Test und verwenden seinen p-Wert als dritten Metriken. Wir führen Experimente auf vier Datensätze durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter das wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf vier Datensätzen zeigen, dass unser Embedding Marker eine großartige Erkennungsleistung haben kann, während die Nützlichkeit für die downstream-Aufgaben groß bleibt. Wir überprüfen auch die Verborgenheit der bereitgestellten Einbettung, indem wir die Visualisierung der Einbettung von Zeichenfolgen auf vier Datensätzen [INAUDIBLE 4:39] PCA verwenden. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jeder Zeichenfolge. Wie gezeigt, ist es schwierig, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden. Das ist alles. Vielen Dank. Willkommen, um mit uns zu diskutieren.</sample>
    <sample id="350">In recent years, NLU models have achieved human-level or even superhuman performance on popular benchmarks, leading to claims that certain tasks are now solved. However, the meaning of superhuman performance in NLU remains unclear, and the reliability of leaderboard scores in comparing models and humans is questionable. This paper investigates the reliability of leaderboard scores on two popular benchmarks, SuperGLUE and SQuAD. Analysis of the benchmarks reveals several sources of error that make the comparison between humans and systems unfair, including evaluation on different sets, errors in ground-truth answers, and vague estimates of human performance. Furthermore, pay rates varied considerably across tasks, and annotator pools are often omitted, making claims about superhuman performance scientifically meaningless. The paper argues that data sets constructed under these conditions should not be used for human-to-system comparisons and provides recommendations to construct more reliable benchmarks.</sample>
    <sample id="351">Title: Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?

Abstract:

In our paper, we investigate the problem of generalization in Named Entity Recognition (NER) using the CoNLL-2003 dataset, which has been used for almost 20 years. We developed the CoNLL++ dataset, a new dataset collected from Reuters News in 2020, annotated with the same CoNLL-2003 guidelines. We fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-03 and CoNLL++ test sets. Our results show that three main ingredients are necessary for good generalization: (1) transformer models, (2) larger model sizes, and (3) more fine-tuning examples. We also found that temporal drift, not adaptive overfitting, is the primary cause of performance drop. Our conclusion is that CoNLL-2003 taggers still work well in 2023, but with the need for better model architectures, larger model sizes, and more fine-tuning examples. Our paper calls for further research on improving model generalization.</sample>
    <sample id="352">Annotating Behaviors in Chat, was auf kurze ABC-Eval abgekürzt wird.</sample>
    <sample id="353">Title: Python Code Generation by Asking Clarification Questions

Abstract:

Code generation and program synthesis from natural language descriptions (NLDs) is a challenging task, particularly when input specifications are underspecified. To address this issue, we introduce interactivity into code generation by asking clarification questions (CQs). We propose a method to create CodeClarQA, a synthetic dataset with clarifications on key operations, and a pipeline for code generation by asking CQs. Our approach identifies missing key operations in NLDs and generates CQs to gather more specifications. We evaluate our method on a dataset of code and NLDs and show that it can effectively identify missing key operations and generate code with higher accuracy. Our results also demonstrate that clarifications can improve code generation. We conclude that our approach can alleviate the problem of underspecification in code generation and program synthesis.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ beträgt höher als 5 Prozentpunkte, wenn der temporale Abstand zwischen dem Trainings- und Testdatensatz größer ist als 3 Jahre.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein Doktorand im Bereich Computerwissenschaften an der Stony Brook University. Ich würde gerne unsere Arbeit vorstellen, die im ACL 2023 als Langpaper angenommen wurde, "Transfer Learning für Dissonanzdetection: Die Herausforderung des seltenen Klassenproblems". Wir beginnen damit, kognitive Dissonanz zu definieren und warum es sich um ein wichtiges Problem handelt, das im Bereich der Sprache untersucht werden sollte.

Kognitive Dissonanz ist die Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, zum Beispiel, wenn eine Person sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann weiter sagt: "Ich habe nach dem Meeting ein paar Zigaretten gegriffen". Diese Überzeugung und Handlung sind inkonsistent und befinden sich in Dissonanz. Es gibt jedoch auch eine Konsonanzrelation, wenn sie sagt: "Ich denke, ich könnte ohne sie meinen Job nicht behalten". 

Dissonanz ist ein sehr häufiges Phänomen, das wir in unserem täglichen Entscheidungsprozess erleben, aber sie sind sehr selten in der Sprache und anderen Diskursbeziehungen ausgedrückt. Warum ist das wichtig? Die Untersuchung von kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Überzeugungswerte zu verfolgen und Änderungen in der Meinung der Bevölkerung zu analysieren. Eine hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns helfen, das menschliche Verhalten besser zu verstehen. Die Untersuchung von Dissonanz in der Sprache kann uns auch helfen, Extremismus und Polarisation von vulnerablen Gruppen besser zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Einzelpersonen zu verstehen und unseren Entscheidungsprozess besser zu verstehen.

Um ein kognitives Dissonanzressource zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzrelationen durchgeführt. Wir haben den Dissonanz-first-Ansatz verwendet, wie in der Flussdiagramm gezeigt. Tweets wurden mit dem PDTB-Parser analysiert und Paare von Diskursseinheiten wurden nach den in unserem Paper beschriebenen Richtlinien annotiert. Wie man sehen kann, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Bei der Sammlung von etwa 1.000 Beispielen von Diskursseinheitenpaaren haben wir das Training für einen ersten Klassifikator durchgeführt, der nur auf 43 Beispielen von Dissonanz trainiert war. Es war keine Überraschung, dass der Klassifikator nicht viel besser als zufällig war. 

Begründet durch die geringe Häufigkeit von Dissonanz und die Abwesenheit von vorherigen Datensätzen, stellen wir uns das Problem der absoluten Seltenheit. Um dies zu überwinden, haben wir Experimente mit Kombinationen von Transfer-Learning und aktiver Lernung durchgeführt, um so Dissonanzbeispiele zu sammeln, um die Anzahl der Annotationen zu reduzieren und die Gesamtkosten zu senken. Da der ursprüngliche Modell nicht in der Lage war, die Dissonanzklasse zu erfassen, haben wir den aktiven Lernprozess mit dem Transfer von Gewichten aus nahe verwandten Aufgaben begonnen. Wir haben Gewichte von zwei verschiedenen Aufgaben übernommen: die topicunabhängige Dissonanz-Stance-Klassifikation, eine Aufgabe, die bestimmt, ob zwei Debatteaussagen von verschiedenen Personen in Übereinstimmung oder in Widerspruch zueinander stehen, unabhängig vom Thema, die wir als "Debatte" bezeichnen, und die binäre Klassifikation von Expansion und Vergleichsklassen von PDTB, da diese beiden eng mit der Konzeption von Konsonanz und Dissonanz zusammenhängen und wir sie als "CE" bezeichnen.

Wir fanden heraus, dass das Transferieren der Null-Shot-Performance auf dem annotierten Datensatz bereits viel besser als zufällig war, mit dem besten Wert von AUC.62. Weiterhin fanden wir heraus, dass die Feinjustierung von CE-Aufgaben gefolgt von weiterer Feinjustierung auf der Debatte-Aufgabe zu einer besseren Null-Shot-Performance führte. Daher ist dies das Modell, das wir verwenden, um den aktiven Lernprozess zu starten.

Als nächstes haben wir die beste Methode bestimmt, um ein Modell mit neuen Daten aus jeder Runde des aktiven Lernens und der Annotation zu aktualisieren. "Cumulative" sammelt alle Daten, die aus aktiver Annotation gesammelt wurden, während "Iterative" das Modell durch Training auf dem neuesten Satz von Daten aktualisiert. In den verschiedenen Strategien fanden wir heraus, dass Cumulative gleich oder besser als Iterative war.

Um die Anzahl der Dissonanzbeispiele zu verbessern, haben wir eine Wahrscheinlichkeitsstrategie für seltene Klassen verwendet, die PRC (Probability-of-Rare-Class) genannt wird. Wir haben diese Strategie mit anderen state-of-the-art-AL-Strategien verglichen, die in der Gemeinschaft häufig verwendet werden. Wir fanden heraus, dass die vorgeschlagene PRC-Strategie besser war als die anderen state-of-the-art-Strategien, obwohl der Unterschied gering war. Es ist jedoch zu beachten, dass die Leistung bei zufälliger Auswahl sehr schlecht war.

Bei weiteren Runden des AL mit den beiden besten Strategien verbesserten wir die AUC für die Dissonanzklassifikation auf 0,75, was der beste Wert ist, den wir bisher auf der Aufgabe erreicht haben. Wir haben auch die Durchführbarkeit jeder Strategie für die Annotationsgüte und die Kosten für Annotatoren überprüft. Wir fanden heraus, dass PRC den höchsten Prozentsatz von Dissonanz hatte und am besten für seltene Klassen funktionierte, aber die Annotatoren fanden die Beispiele auch schwierig. 

Zusammenfassend fanden wir heraus, dass PRC eine einfache AL-Strategie für die seltene Klasse ist und dass das Kaltstarten des AL mit einer gut konzipierten Übertragungsaufgabe und das Transfer-Learning signifikant hilft. Wir fanden auch heraus, dass die iterative Aktualisierung für das Transfer-Learning aus einer anderen Domäne nützlich ist, während die aktive Annotation in der gleichen Domäne von der kumulativen Aktualisierung profitiert. Hier sind die Links zu unserem Kern-Datensatz und unserem Paper. Wenn Sie Fragen haben, stehe ich Ihnen gerne zur Verfügung. Vielen Dank.</sample>
    <sample id="356">Ich kann keine Informationen über die Universität der Autoren finden.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Es sind vier Autoren an der Arbeit beteiligt: Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit der Wait-k-Strategie, der Local Agreement und einer state-of-the-art-Architektur verglichen, die speziell für simultane Pre-Translation entwickelt wurde.</sample>
    <sample id="361">Armineh Nourbakhsh, PhD-Studienleiterin am Language Technologies Institute der Carnegie Mellon University und Research Director am JP Morgan AI Research Team, präsentiert ihre Forschung "CounterComp", die darauf abzielt, die kompositionelle Generalisierung für mehrschrittige quantitative Rechenaufgaben zu verbessern. Diese Aufgaben beinhalten Fragestellungen, die auf Tabellen wie Finanzdaten basieren, und erfordern die Durchführung mehrerer arithmetischer Operationen. Derzeitigen Neuralmodellen fehlt es an Leistung, insbesondere bei mehr als zwei Schritten, da sie spuriöse Muster memorieren. 

Um dies zu überwinden, verwendet "CounterComp" Counterfaktualszenarien, um die Auswirkungen von Interventionen in den Fragestellungen auf die Ausgaben zu analysieren. Dazu werden positive und negative Beispiele aus dem Trainingsdatensatz generiert und als Triplets verwendet, um ein Hilfsmetriklernen-Verlust hinzuzufügen. Dieser Verlust wird dynamisch angepasst, um die Auswirkungen der Interventionen zu berücksichtigen. 

Die Ergebnisse zeigen, dass die Hinzufügung dieses Verlusts die Leistung von drei State-of-the-Art-Baselines verbessert, insbesondere bei mehr als zwei Schritten. Zudem zeigt sich eine Verbesserung der Leistung bei der Kompositionellen Generalisierung, sowohl bei in- als auch bei ausserhalb des Trainingsdatensatzes.</sample>
  </task>
</testset>