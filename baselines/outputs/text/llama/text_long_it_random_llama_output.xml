<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono grandi scale web crawl data, in particolare notizie politiche da fonti come New York Times, Los Angeles Times, The Guardian, Huffington Post e altri media.</sample>
    <sample id="1">Gli autori dell'articolo, Akshatha e Martin, sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Il team di Ant Group ha presentato un nuovo modello di pre-istruzione chiamato LayoutMask, progettato per superare i problemi di lettura di documenti visivamente ricchi. Il modello utilizza solo informazioni di testo e layout come input e mira ad ampliare le interazioni di testo e layout durante la pre-istruzione. LayoutMask differisce dalle ricerche precedenti in tre aspetti: la scelta delle posizioni 1D, la strategia di mascheramento e gli obiettivi di pre-istruzione.

Il modello utilizza la posizione 1D locale, che rappresenta l'ordine dei token all'interno di un segmento, anziché la posizione 1D globale, che rappresenta l'ordine dei token nel documento intero. Ciò consente al modello di inferire l'ordine di lettura globale utilizzando informazioni semantiche e spaziali.

LayoutMask include due strategie di mascheramento innovative: Whole Word Masking e Layout-Aware Masking. La prima strategia maschera le parole intere, anziché i token singoli, promuovendo le interazioni di testo e layout. La seconda strategia aumenta la probabilità di mascherare le parole di inizio e fine dei segmenti, promuovendo l'apprendimento delle relazioni tra segmenti.

Il modello pre-istruzione di LayoutMask include anche un obiettivo di pre-istruzione innovativo, Masked Position Modeling, che consiste nel recupero di posizioni 2D casualmente mascherate durante la pre-istruzione. Il modello deve trovare il contesto per ogni parola sulla base delle relazioni semantiche e inferire con le informazioni di posizione 2D dal punto di vista spaziale.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase. Il mio nome è Regina Stodden e guiderò voi attraverso la prima parte della presentazione. Definiamo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di persone, come coloro che hanno problemi di lettura o parlanti non nativi. Per addestrare un modello di semplificazione del testo, è necessario avere coppie parallele di testi, ad esempio di documenti o frasi. Ecco un esempio di coppia di frasi parallele, una frase tedesca complessa e la sua traduzione in lingua semplice. Per semplificare la frase, sono possibili diverse tecniche, come la sostituzione lessicale, l'eliminazione di una clausola, la riorganizzazione o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus, DEPLAIN perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpus qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento. Quindi proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato 483 documenti tutti manualmente. Ciò porta a circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatici. In totale otteniamo 30.450 coppie di frasi. Abbiamo analizzato le nostre coppie di frasi un po' di più, ad esempio il tipo di semplificazione. Come potete vedere qui, i testi della Bibbia sono molto più semplificati rispetto ai testi di notizie o ai testi per gli apprendenti di lingua. Su tutti i livelli, riguardo ad esempio la semplificazione lessicale, la semplificazione della struttura, anche il livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di trasformazioni di semplificazione diverse. Ad esempio, nel corpus DEPLAIN-apa abbiamo molto più riordinamenti e aggiunte di parole rispetto a quanto abbiamo nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molto più riformulazioni. Ecco, ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e adesso parlerò degli utilizzi del nostro set di dati DEPLAIN. Per il primo utilizzo, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, sono stati proposti molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Ma nel nostro utilizzo, stiamo cercando di estrarre gli allineamenti tra frasi di due documenti paralleli con la stessa lingua, con lo stesso contenuto, ma a un diverso livello di complessità. E ora che abbiamo il nostro set di dati DEPLAIN, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcune adattamenti ai metodi proposti, e abbiamo pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo di MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti stessi nel paper. Il secondo utilizzo che abbiamo mostrato nel nostro paper è un caso di semplificazione del testo automatica tramite l'addestramento fine di modelli di linguaggio per produrre testi semplificati a partire da testi complessi. Abbiamo addestrato due modelli diversi. Abbiamo addestrato il modello di long-mBART per produrre semplificazioni a livello di documento e abbiamo anche addestrato il modello base normale mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete esaminare più a fondo i dettagli e le metriche di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questo addestramento fine poteva produrre o poteva ottenere punteggi migliori rispetto ai punteggi di base, e abbiamo proposto questi risultati come un punto di riferimento per il problema della semplificazione del testo automatica in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Il modello T5 XL è stato utilizzato per ottenere l'accuratezza dell'82%-87%.</sample>
    <sample id="6">Jiaan presenta il suo lavoro intitolato "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", un progetto di ricerca che unisce la sommariazione multilingue e la sommariazione cross-linguale in un setting più generale chiamato many-to-many summarization. Questo setting consente di creare un modello di sommariazione unico che possa processare un documento in qualsiasi lingua di origine e generare la sua sommariazione in qualsiasi lingua di destinazione.

Jiaan presenta anche il modello PISCES, un modello di pre-allenamento many-to-many sommariazione che impara a generare sommari in qualsiasi lingua di destinazione partendo da un documento in qualsiasi lingua di origine. Il modello PISCES è stato addestrato attraverso un processo di pre-allenamento a tre stadi, che include l'apprendimento del modello linguistico, la capacità cross-linguale e l'abilità di sommariazione.

Gli esperimenti condotti da Jiaan mostrano che il modello many-to-many sommariazione può trasferire meglio il know-how del compito tra lingue diverse rispetto ai modelli multilingue e cross-linguale. Il modello PISCES ha superato i basi di confronto, compresi mBART-50 e mT5, e ha dimostrato la sua efficacia attraverso studi di ablatività e studi umani.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora, ma hanno bisogno di essere adattati con architetture di modello più avanzate, dimensioni di modello più grandi e più esempi di fine-tuning per garantire una buona generalizzazione.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, chiamato ABC-Eval, è una novità rispetto alle metodologie esistenti poiché riduce la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddirsi.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla disponibilità di un set di validazione pulito (clean validation set) per la selezione del modello.</sample>
    <sample id="10">Per migliorare il punteggio, si possono fare progressi nelle seguenti aree:

* Migliorare la capacità del modello di recuperare conoscenze di background parzialmente sovrapposte rispetto a quelle degli annotatori.
* Aggiungere la capacità del modello di comprendere il contesto e la domanda di disambiguazione.
* Utilizzare tecniche di pre-elaborazione dei dati per migliorare la rappresentazione delle entità e delle loro relazioni.
* Aggiungere ulteriori dati e domini per migliorare la generalizzabilità del modello.
* Utilizzare tecniche di fine-tuning per adattare il modello alle specifiche esigenze del compito.</sample>
    <sample id="11">Il dottor Jack Hessel, ricercatore presso l'AI2, ha presentato un lavoro intitolato "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest". Il lavoro, realizzato in collaborazione con università e aziende di tutto il mondo, esplora la capacità dei modelli linguistici di comprendere l'umorismo.

Il dottor Hessel ha evidenziato come i modelli linguistici siano in grado di generare e spiegare battute, ma che la loro comprensione dell'umorismo sia ancora limitata. Ha presentato un corpus di dati tratti dal concorso di didascalie di The New Yorker, che comprende oltre 700 cartoons e oltre 650 spiegazioni di battute.

I risultati dei test mostrano che i modelli linguistici hanno una precisione del 62% nel compito di matching, mentre gli esseri umani raggiungono il 94%. Anche quando i modelli linguistici ricevono una descrizione dell'immagine, la loro precisione rimane bassa. I risultati dei test di spiegazione di battute mostrano che i modelli linguistici commettono errori significativi e che le spiegazioni umane sono preferite a quelle dei modelli linguistici in due terzi dei casi.

Il dottor Hessel conclude che la comprensione dell'umorismo da parte dei modelli linguistici è ancora un problema aperto e che ulteriori ricerche sono necessarie per migliorare la loro capacità di comprendere e generare battute.</sample>
    <sample id="12">Ci sono 5 autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem ha presentato il suo lavoro, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", che analizza e migliora la tecnica di inferenza adattiva per ridurre i costi di inferenza dei modelli di linguaggio grandi. La tecnica si basa sull'idea che i dati reali variano nella complessità e che quindi possiamo utilizzare modelli di capacità inferiore per campioni facili, riducendo così i costi di inferenza medi. Le due principali tecniche di inferenza adattiva sono Multi Model e Early Exit.

Il Multi Model utilizza più modelli con un classificatore finale e li addestra separatamente, mentre l'Early Exit utilizza più classificatori all'interno del modello e li addestra insieme. Tuttavia, l'Early Exit soffre di un problema noto come gradienti conflittuali, in cui i segnali di aggiornamento dei classificatori interferiscono tra loro, degradando il rendimento.

Per risolvere questo problema, Daniel Rotem ha presentato SWEET (Separating Weights in Early Exit Transformers), un metodo di fine-tuning per l'architettura Early Exit che evita i gradienti conflittuali. Il metodo consiste nell'aggiornare ogni strato del modello solo con il segnale di aggiornamento del classificatore successivo.

I risultati mostrano che SWEET chiude la maggior parte della lacuna tra Early Exit e Multi Model e offre un miglioramento delle prestazioni in velocità/accuratezza. Inoltre, SWEET è stato testato su diversi modelli di linguaggio e ha mostrato un miglioramento delle prestazioni in velocità/accuratezza.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo discorso verte sulla Struttura di Dipendenza della Coordinazione. Come sai, ci sono diverse strutture di dipendenza assunte da diverse teorie e approcci corpus. Ad esempio, nella universal dependencies, la struttura di coordinazione, Lisa, Bart, e Maggie, in cui il primo congiunto è la testa dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è assunto nella teoria del significato testo di Igor Mel'čuk, in cui anche la testa dell'intera struttura coordinata è il primo congiunto. Questi due approcci sono asimmetrici. Scegliere uno dei congiunti. Adesso ci sono approcci asimmetrici alla struttura coordinata, come l'approccio di Praga. L'approccio della testa di congiunzione assunto nei treebank di dipendenza di Praga, in cui le strutture coordinate sono testate dalla congiunzione. Otteniamo alcune dipendenze da tutte le congiunzioni. E infine, c'è anche un approccio multi-testa utilizzato, ad esempio, nella grammatica delle parole di Hudson, in cui dicono che tutti i congiunti sono testate della struttura coordinata. Otteniamo dipendenze dal governatore a tutti i congiunti separatamente: Lisa, Bart, e Maggie. Ora l'obiettivo di questo articolo è produrre un nuovo argomento per le strutture simmetriche della coordinazione, come questi due, contro le strutture asimmetriche della coordinazione, come questi due. L'argomento si basa sul principio di minimizzazione della lunghezza di dipendenza che spiegherò in base a questi esempi. In inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi possono essere più lontani. "Marge ha letto questo ieri" è okay perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri questo" è molto peggiore. Perché qui tra il verbo e l'oggetto diretto c'è un aggettivo: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e molto lungo. Perché allora può essere spostato nella posizione dopo l'aggettivo. Questo è illustrato qui. Entrambi questi enunciati sono accettabili. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." È okay il modo in cui invece di "lo" abbiamo questo lungo NP. Ma è anche OK dire, "Marge ha letto ieri questo libro assolutamente affascinante sulle api." La ragione qui è che questo è possibile perché, nonostante questo enunciato violi il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione della lunghezza di dipendenza, che dice che le dipendenze più brevi sono preferite. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra questi due strutturazioni. Qui abbiamo una dipendenza da "letto" all'aggettivo di lunghezza 7 misurata in parole e da "letto" al libro di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto più breve. Quindi suona abbastanza OK. Sì? Quindi, abbiamo estratto statistiche varie sulla coordinazione da una versione migliorata del Penn Treebank e abbiamo visto il paper "Perché non usare le dipendenze universali" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendono a essere più corti. "Sale e pepe" e non "pepe e sale", misurati in sillabe. E, inoltre, l'osservazione fatta in parsing che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, giusto? Quindi la proporzione è maggiore del congiunto corto a sinistra. Ma cosa è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Sì, il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa" così come il governatore è assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo una coordinazione di due verbi e non c'è un esterno, governatore esterno. In questi casi, il congiunto a sinistra preferisce essere più corto; la più grande differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "risate" governa la coordinazione di Ted e Ned, questo effetto scompare. Quindi abbiamo mostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la seconda colonna, e in parole la terza colonna. Quindi concentrerò sulla terza. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto a sinistra a essere più corto cresce costantemente, con la differenza assoluta in parole, e lo stesso si osserva quando non c'è un governatore, come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E mostriamo nel paper come questo fornisce un argomento contro le strutture asimmetriche della coordinazione, come questi due, e per le strutture simmetriche, come questi due. Quindi vedi il paper per gli argomenti completi. E parla con noi al poster session. Grazie.</sample>
    <sample id="15">Ci sono 3 autori coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I dati mostrano che i testi della Bibbia sono stati semplificati in modo più forte rispetto ai testi di notizie e di apprendimento della lingua.</sample>
    <sample id="17">Il tuo lavoro di dottorato presso l'NUS si concentra sulla estrazione di relazioni multimodalità, un compito che consiste nell'identificare le relazioni semantiche tra entità in un testo. Tuttavia, in scenari realistici come i social media, i dati possono essere presentati in diverse forme e modalità, rendendo difficile comprendere alcuni termini ambigui o multi-contesto. Per superare questo limite, è stato introdotto l'estrazione di relazioni multimodalità, che aggiunge fonti visive al testo. Ad esempio, immagini come "Bachelor", "Gown" e "Cap" possono aiutare a inferire la relazione tra JFK e Harvard come "graduato".

Tuttavia, ci sono ancora due problemi principali: l'over-utilizzazione dell'informazione interna e la sotto-esplorazione dell'informazione esterna. Per superare questi limiti, hai proposto un framework che consiste di cinque parti:

1. Rappresentazione del testo e dell'immagine come grafici di scena visivi e testuali.
2. Merging dei due grafici in un unico grafico di scena multimodale (CMG).
3. Filtering del CMG iniziale per selezionare le informazioni più rilevanti.
4. Utilizzo del principio di bottiglia di informazione del grafico per guida l'ottimizzazione.
5. Integrazione delle caratteristiche di topic multimodalità per arricchire il contesto.

I risultati dei tuoi esperimenti mostrano che il tuo metodo proposto ottiene le migliori prestazioni rispetto alle basi multimodalità. L'analisi degli ablati mostra che l'eliminazione del grafico di scena riduce le prestazioni. Infine, hai dimostrato che l'internal-information screening e l'external-information exploiting sono utili in circostanze diverse.</sample>
    <sample id="18">"Salt and pepper" e non "pepper and salt", misurati in sillabe.</sample>
    <sample id="19">La presentazione di Zhang Qin, studente di master presso la Shenzhen University, si concentra su un sistema di domande risposta aperte (Open Domain Question Answering, ODQA). L'autore esamina le sfide associate a questo campo, tra cui la grande dimensione del corpus di Wikipedia (26 milioni di documenti, 20 GB di memoria) e la lentezza della ricerca nell'indice (65 GB). 

Per risolvere queste sfide, l'autore propone diverse strategie per migliorare l'efficienza dei sistemi ODQA. In primo luogo, si suggerisce di utilizzare metodi di ricerca di vicinanza più vicina, come la ricerca di vicinanza approssimativa, per ridurre il tempo di ricerca. In secondo luogo, si propone di utilizzare la lettura saltuaria, come l'adattamento computazionale, per ridurre il tempo di lettura. Infine, si suggerisce di ridurre la dimensione dell'indice utilizzando tecniche come la filtrazione dei documenti, la compressione degli embedding o la quantizzazione dei prodotti.

L'autore conclude che se si è limitati da risorse, si può considerare ridurre la dimensione dell'indice utilizzando sistemi generator-only o compressione degli embedding, o ridurre la dimensione del modello utilizzando distillazione di conoscenza o progettazione di un modello a una sola fase. Se si cerca una risposta in tempo reale, i sistemi retrieval-only sono una scelta migliore. Infine, l'autore propone due linee di ricerca future: la deployabilità dei sistemi ODQA su dispositivi a basso consumo e l'uso di metriche di valutazione più ampie.</sample>
    <sample id="20">Sì, i modelli pre-allenati DrBERT sono disponibili gratuitamente su Hugging Face, sotto licenza MIT, e i script di allenamento sono pubblicati sul repository GitHub.</sample>
    <sample id="21">News texts.</sample>
    <sample id="22">I tre principali ingredienti necessari per una buona generalizzazione sono: 

1. Architettura del modello (i modelli transformer generalizzano meglio);
2. Dimensione del modello (modelli più grandi portano a una generalizzazione migliore);
3. Numero di esempi di fine-tuning (più esempi portano a una generalizzazione migliore).</sample>
    <sample id="23">Il testo descrive i risultati di un'indagine sulle capacità di modelli di testo-immagine (T2I) di rappresentare il testo in immagini. I ricercatori hanno analizzato il modello Imagen, che utilizza un codificatore T5-XXL per rappresentare il testo e un modello di diffusione per generare l'immagine. Tuttavia, i modelli T2I sono spesso carenti nella rappresentazione del testo, specialmente per parole semplici.

Gli autori hanno scoperto che il codificatore T5 è inefficiente nella rappresentazione delle parole, con un tasso di accuratezza del 70% anche per il modello più grande. Al contrario, i modelli PaLM sono più efficaci, ma richiedono un numero maggiore di parametri e dati di training.

I ricercatori hanno anche esaminato il modello ByT5, che riceve le singole lettere dell'input e riesce a rappresentare il testo con grande accuratezza. Per migliorare le capacità di rappresentazione del testo del modello Imagen, gli autori hanno concatenato il codificatore ByT5-small al modello esistente, aumentando la precisione del 90%.

I risultati sono stati pubblicati in un paper che presenta due nuovi benchmark: WikiSpell per modelli di testo solo e DrawText per modelli T2I. La strategia proposta per migliorare la rappresentazione del testo consiste nell'aggiungere un modello che è consapevole delle lettere dell'input.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in tre modi diversi: in caratteri, in sillabe e in parole. Tuttavia, l'autore si concentra sulla misura in parole, come mostrato nella terza colonna della tabella.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per studiare l'effetto della posizione del governatore analizzando statistiche sulla coordinazione estratte dalla versione migliorata del Penn Treebank. Sono state utilizzate tre misure di lunghezza: caratteri, sillabe e parole. La posizione del governatore è stata classificata in tre casi: sinistra, destra e assente.</sample>
    <sample id="26">Il classificatore base non è molto efficace se addestrato solo su 43 esempi di dissonanza, poiché non riesce a superare il caso random.</sample>
    <sample id="27">Non sono stato in grado di trovare l'elenco degli autori, ma è probabile che siano diversi autori coinvolti nel lavoro "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models".</sample>
    <sample id="28">I personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per i fenomeni del discorso come la formality e la lexical cohesion.</sample>
    <sample id="30">Il team di ricerca di AI2 e USC, guidato da Yuchen Lin, ha presentato un nuovo framework di apprendimento ensembles chiamato LLM-Blender. Questo framework è progettato per migliorare la prestazione dei modelli di linguaggio grande utilizzando una combinazione di modelli diversi.

Il team ha scoperto che la selezione ottimale dei modelli può variare a seconda dell'esempio di input specifico, e che utilizzare un solo modello top non è sempre la scelta migliore. Per superare questo problema, il team ha proposto un framework a due stadi chiamato LLM-Blender.

Il primo stadio, chiamato PairRanker, utilizza una comparazione pairwise per valutare la qualità dei modelli e stabilire un ordine di priorità. Il secondo stadio, chiamato GenFuser, utilizza i modelli top selezionati per generare l'output finale.

Il team ha creato un nuovo dataset chiamato MixInstruct per valutare il framework LLM-Blender e ha utilizzato automatiche metriche come BERTScore, BLUERT e BARTScore per valutare la prestazione dei modelli. I risultati hanno mostrato che il framework LLM-Blender è in grado di migliorare la prestazione dei modelli di linguaggio grande e che è un framework promettente per l'apprendimento ensembles.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati alle seguenti istituzioni:

1. John Gauthier
2. Aaron Mueller
3. Kanishka Misra
4. Karen Fences
5. Roger Levy
6. Adina Williams
7. Koustav Sinha</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità attraverso la comparazione delle annotazioni di utenti reali con i dati e i modelli esistenti utilizzando un punteggio di correlazione Pearson (R) tra le annotazioni di utenti di diverse demografie e le previsioni e le etichette dei dati e dei modelli.</sample>
    <sample id="34">Marcos Treviso presenta il lavoro "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", un progetto di collaborazione con Alexis Ross, Nuno Guerreiro e André Martins. Il framework CREST combina due metodi di interpretazione delle decisioni: la razionalizzazione selettiva e la generazione di controcasi. La razionalizzazione selettiva fornisce spiegazioni facendo risaltare i token in modo fedele, mentre la generazione di controcasi utilizza l'editing di parti specifiche dell'input per produrre esempi alternativi.

Il componente principale di CREST è un modello di razionalizzazione che produce un output significativo, Z, utilizzando un maschere addestrabile. Il modello di previsione riceve Z come input e produce una decisione. Per generare controcasi, CREST utilizza Z per mascherare l'input originale e prependerlo con l'etichetta d'oro.

Gli esperimenti di valutazione mostrano che CREST produce controcasi validi e naturali, anche se inferiori a quelli manuali. Tuttavia, CREST-Rationalization, che utilizza i controcasi prodotti da CREST per migliorare i modelli downstream, ottiene risultati migliori rispetto agli altri metodi. Inoltre, gli esperimenti mostrano che le razionalizzazioni prodotte da CREST sono più plausibili e hanno una maggiore simulabilità contrattuale rispetto agli altri metodi.

In sintesi, CREST è un framework che combina razionalizzazione selettiva e generazione di controcasi per produrre spiegazioni valide e fluide. Gli esperimenti mostrano che CREST-Rationalization può migliorare i modelli downstream e produrre razionalizzazioni più plausibili e con maggiore simulabilità contrattuale.</sample>
    <sample id="36">La presentazione descrive un lavoro di ricerca intitolato "Learning Language-Specific Layers for Multilingual Machine Translation" condotto da Telmo Pessoa Pires e collaboratori. L'obiettivo del lavoro è aumentare la capacità di traduzione per ogni lingua in un modello di traduzione multilingue, senza aumentare i costi di inferenza. Per raggiungere questo obiettivo, gli autori propongono la tecnica delle Layer Specifiche per Lingua (LSL), che consiste nell'aggiungere un layer specifico per ogni lingua al modello di traduzione. Il layer specifico per ogni lingua è selezionato e allenato all'inferenza in base alla lingua sorgente o di destinazione.

Gli autori hanno condotto esperimenti su 10 lingue diverse, tra cui lingue europee, asiatiche e la lingua africana Swahili, utilizzando il dataset WMT21 news translation. Hanno valutato il modello proposto utilizzando metriche come chrF, spBLEU e COMET. I risultati mostrano che il modello proposto ottiene miglioramenti significativi rispetto al modello di riferimento e al modello di adattamento linguistico, in particolare per le lingue a bassa risorsa. Gli autori hanno anche condotto test statistici e hanno trovato che le miglioramenti sono significativi per 84 delle 90 direzioni di traduzione esaminate. La presentazione conclude con l'invito a consultare il paper completo o a visitare la sessione di poster per ulteriori informazioni.</sample>
    <sample id="37">Nello studio precedente, in cui i soggetti umani hanno ricevuto gli stessi prompt di persona, si è trovato che questi hanno anche espresso stereotipi razziali.</sample>
    <sample id="38">Il Penn Treebank, l'approccio delle Universal Dependencies e il paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Il nome dell'autore dell'articolo è Adam Przepiórkowski.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono:

- La classificazione della dissonanza di argomento indipendente (topic independent dissonance stance classification)
- La classificazione binaria delle classi di espansione e confronto (expansion and comparison classes) del PDTB (Penn Discourse TreeBank)</sample>
    <sample id="41">Il tuo lavoro, "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", è un progetto di ricerca sviluppato in collaborazione con la Sony Group Corporation. Il tuo obiettivo è di creare un sistema di linguaggio naturale in grado di comprendere le persone e le loro conoscenze comuni per creare narrative coerenti e coinvolgenti.

Il tuo team ha creato un grafo di conoscenza, chiamato PeaCoK, che contiene informazioni su 3.800 persone e 40.000 attributi, con oltre 100.000 inferences o fatti personali. Questo grafo è stato costruito in tre fasi: selezione di persone da grafi di conoscenza comune, induzione di attributi da grafi di conoscenza comune e modelli di linguaggio pre-allineati, e annotazione di relazioni tramite una votazione di maggioranza tra umani e AI.

Il tuo lavoro dimostra che PeaCoK può essere utilizzato per addestrare un modello di linguaggio, Comet-BART, a generare conoscenze comuni simili a quelle di grandi modelli di linguaggio. Inoltre, il tuo team ha dimostrato che PeaCoK può essere utilizzato per migliorare la generazione di dialoghi, aumentando la coerenza, l'engagement e l'espressione di persona.</sample>
    <sample id="42">Non sono stati menzionati i nomi degli autori nell'articolo, quindi non è possibile fornire un numero preciso.</sample>
    <sample id="43">Non è specificato il numero esatto degli autori coinvolti nell'articolo.</sample>
    <sample id="44">Il framework NLPositionality introdotto differisce dai lavori precedenti in quanto confronta direttamente le annotazioni degli utenti con i dati e i modelli, anziché concentrarsi sull'accordo tra gli annotatori o sulla modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="45">Le generazioni di persona da parte del modello LLM mostrano che le configurazioni delle donne di colore si sovrappongono maggiormente al lessico degli stereotipi rispetto alle configurazioni degli uomini bianchi.</sample>
    <sample id="46">DeepL e Google Translate.</sample>
    <sample id="47">Ciao Shangbin, studente dottorando presso l'Università di Washington. Oggi presenterai il tuo lavoro "Dai dati di pre-allenamento ai modelli di linguaggio alle attività downstream: tracciare le tracce degli stereotipi politici che portano a modelli NLP ingiusti". I modelli di linguaggio vengono addestrati su grandi quantità di dati raccolti web. I media di notizie politici sono ben rappresentati nei dati di pre-allenamento. Secondo una rilevazione del C4 Corpus, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di addestramento dei modelli di linguaggio. Ciò ha creato un bene e male misto per le applicazioni dei modelli di linguaggio. Da un lato, sono stati in grado di imparare da diverse prospettive, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente biasate e possono portare a problemi di equità nelle applicazioni delle attività downstream. Al fine di affrontare questo problema, proponiamo di investigare il pipeline di propagazione degli stereotipi politici da dati di pre-allenamento a modelli di linguaggio a attività downstream, specificamente chiedendo le seguenti domande: Primo, come valutare l'inclinazione politica dei modelli di linguaggio e quale ruolo possono avere i dati di pre-allenamento su tali stereotipi? Secondo, come i modelli di linguaggio con diverse inclinazioni politiche si comportano effettivamente nelle attività downstream e se ciò può portare a problemi di equità nelle applicazioni NLP? Per rispondere a queste domande, proponiamo di utilizzare i seguenti approcci: Primo, chiedere ai modelli di linguaggio con diverse formati di domanda utilizzando questionari politici come il test di conferenza politica. Ciò ci consente di valutare automaticamente le inclinazioni politiche dei modelli di linguaggio in modo ben radicato nella letteratura scientifica politica. Alcuni risultati preliminari dimostrano che i modelli di linguaggio hanno inclinazioni politiche diverse. Occupano tutti e quattro i quadranti del campus politico. Inoltre, GPT-4 è il modello di linguaggio più liberale di tutti e i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e le sue varianti. Secondo, proponiamo di investigare fino a che punto gli stereotipi politici dei modelli di linguaggio sono effettivamente trasmessi dai dati di pre-allenamento. Per fare ciò, possiamo condurre un esperimento controllato addestrando i checkpoint dei modelli di linguaggio su 6 corpora partitici diversi, separati in notizie e social media, ulteriormente divisi per le loro inclinazioni politiche. Addestrando i modelli di linguaggio su tali corpora partitici, possiamo vedere che le coordinate ideologiche dei modelli di linguaggio corrispondono a un cambiamento. Ad esempio, addestrando RoBERTa sul corpus Reddit di orientamento di sinistra, possiamo vedere un significativo spostamento liberale rispetto alle inclinazioni politiche dei modelli di linguaggio. Inoltre, proponiamo di investigare se i modelli di linguaggio possano rilevare la polarizzazione che è presente nella nostra società moderna. Per fare ciò, possiamo dividere i corpora di pre-allenamento in due categorie: prima e dopo l'elezione del 45° presidente degli Stati Uniti. Possiamo addestrare i modelli di linguaggio su entrambi i corpora temporali diversi. Possiamo vedere che i modelli di linguaggio hanno inclinazioni politiche più lontane dal centro dopo l'elezione del 45° presidente degli Stati Uniti. Ciò indica che i modelli di linguaggio possono anche rilevare la polarizzazione nella nostra società. Infine, proponiamo di valutare i modelli di linguaggio con diverse inclinazioni politiche nelle attività downstream, come la detezione di discorsi d'odio e la detezione di notizie false. Queste attività sono spesso coinvolte i modelli di linguaggio e possono avere implicazioni significative. Possiamo vedere che se separiamo il rendimento in diverse democrazie o inclinazioni politiche dei media di notizie, possiamo osservare un modello. Ad esempio, nella detezione di discorsi d'odio, i modelli di linguaggio di orientamento di sinistra sono migliori nella detezione di discorsi d'odio contro gruppi minoritari sociali, ma peggiori nella detezione di discorsi d'odio contro gruppi più potenti nella nostra società. Al contrario, i modelli di linguaggio di orientamento di destra sono migliori nella detezione di discorsi d'odio contro bianchi e uomini, ma peggiori nella detezione di discorsi d'odio contro comunità LGBTQ+ nere e altre comunità minoritarie. Tendenze simili si verificano anche nella detezione di notizie false, dove possiamo vedere che i modelli di linguaggio di orientamento di sinistra sono migliori nella detezione di notizie false provenienti da opinioni politiche opposte e viceversa. Inoltre, possiamo presentare molti esempi qualitativi per dimostrare che i modelli di linguaggio con diverse inclinazioni politiche danno predizioni diverse per gli esempi di discorsi d'odio e notizie false sulla base delle loro categorie sociali. Ci sono molti esempi aggiuntivi nell'appendice per evidenziare ulteriormente che ciò indica che ci sono problemi di equità molto pressanti relativi alle inclinazioni politiche dei modelli di linguaggio. Ad esempio, se i modelli di linguaggio di orientamento di destra fossero addestrati sulla detezione di discorsi d'odio o notizie false e poi deploiati su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e i discorsi d'odio contro gruppi minoritari potrebbero proliferare senza controllo. Ciò ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità causati dalle inclinazioni politiche dei modelli di linguaggio. Un po' di discussione. Vorremmo anche mettere in evidenza la difficile scelta che abbiamo di fronte riguardo alle inclinazioni politiche dei modelli di linguaggio. È come essere tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di pre-allenamento, gli stereotipi si propagheranno dai dati di pre-allenamento ai modelli di linguaggio alle attività downstream, creando problemi di equità. Se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l'esclusione. E non è facile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto nei dati di monitoraggio linguistico. È come il problema del treno elettrico. Ok, credo di aver finito qui. Grazie per il tempo.</sample>
    <sample id="48">Il nome dell'autore David Vilar è menzionato, ma non è chiaro se ci siano altri autori coinvolti nell'articolo. Tuttavia, si menziona il fatto che l'articolo è "joint work" con i colleghi di Google Translate, ma non si specifica il numero esatto di autori.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a un contesto di lunghezza di 1024 token.</sample>
    <sample id="50">Regina Stodden presenta il nuovo corpus DEPLAIN per l'identificazione di testi tedeschi su livello di documento e di frase. Il corpus è stato creato per risolvere i problemi dei corpus esistenti, troppo piccoli per addestrare un modello di semplificazione del testo e con allineamenti automatici potenzialmente errorosi. DEPLAIN è diviso in due sottocorpus: DEPLAIN-apa, basato su testi di notizie, con 13.000 paia di frasi allineate manualmente, e DEPLAIN-web, che copre diversi domini e contiene 30.450 paia di frasi allineate manualmente e automaticamente.

Il corpus è stato analizzato per comprendere il tipo di semplificazione utilizzato e si è scoperto che il corpus della Bibbia è stato semplificato in modo più forte rispetto ai testi di notizie e di apprendimento della lingua. Inoltre, si è scoperto che DEPLAIN ha una grande varietà di trasformazioni di semplificazione.

Omar presenta i casi d'uso del corpus DEPLAIN. Il primo caso d'uso è l'evaluazione di metodi di allineamento automatico. Il corpus DEPLAIN è stato utilizzato come standard di riferimento per valutare metodi di allineamento automatico e si è scoperto che il metodo MASSalign è il più efficace.

Il secondo caso d'uso è l'applicazione della semplificazione del testo automatica tramite l'addestramento di modelli di linguaggio. Si sono addestrati due modelli diversi per produrre semplificazioni di testo a livello di documento e di frase, rispettivamente. I risultati hanno mostrato che l'addestramento dei modelli può produrre semplificazioni di testo di alta qualità e che i risultati possono essere utilizzati come base di riferimento per il problema della semplificazione del testo automatica.</sample>
    <sample id="51">I tre domini inclusi nel set di dati sono: musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è il concetto che si riferisce alle prospettive che le persone tengono a causa della loro demografia, identità e esperienze di vita.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Il tuo lavoro di ricerca, intitolato "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge", si concentra sull'identificazione del dissonanza cognitiva espressa nella lingua. Il dissonanza cognitiva si verifica quando due credenze o azioni sono in conflitto, come nel caso di una persona che sa che i fumetti possono essere letali, ma continua a fumare. Il tuo team ha creato un grande set di dati annotato di dissonanza relazioni utilizzando un approccio dissonanza-first e ha sviluppato un modello di classificazione del dissonanza utilizzando l'apprendimento trasferito e l'apprendimento attivo.

Il tuo modello inizia con l'apprendimento trasferito da due task correlate: la classificazione del dissonanza indipendente dal tema e la classificazione binaria delle classi di espansione e confronto del PDTB. Successivamente, il modello è stato addestrato iterativamente su entrambi i task e ha mostrato un miglioramento significativo nella prestazione di zero-shot.

Il tuo team ha anche esplorato diverse strategie di aggiornamento del modello con nuovi dati durante le fasi di apprendimento attivo e ha trovato che l'aggiornamento cumulativo sia più efficace dell'aggiornamento iterativo. Inoltre, hai proposto una strategia di selezione di esempi, chiamata Probability-of-Rare-Class (PRC), che seleziona gli esempi più probabili di dissonanza e ha mostrato un miglioramento significativo nella prestazione del modello.

Infine, il tuo team ha valutato la fattibilità di ciascuna strategia in termini di qualità degli annotatori e costi e ha trovato che la PRC sia una strategia semplice e efficace per l'acquisizione di classi rare.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza doverlo riaddestrare o adottare una specifica architettura per la traduzione simultanea.</sample>
    <sample id="56">Non sono stati menzionati gli autori nell'articolo.</sample>
    <sample id="57">Sì, alcuni modelli riescono a funzionare sulla suite di test KITMUS dopo aver ricevuto un addestramento specifico.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. "Background-Pretrain" (Background-Pretrain): dove la conoscenza di fondo è disponibile solo a pre-istruzione.
2. "Background-Both" (Background-Both): dove la conoscenza di fondo è disponibile sia a pre-istruzione che a inferenza.
3. "Background-Inference" (Background-Inference): dove la conoscenza di fondo è disponibile solo a inferenza.</sample>
    <sample id="59">Il team di Yanis Labrak ha presentato un modello di linguaggio noto come DrBERT, un modello pre-allenato in francese per domini biomedici e clinici. DrBERT è basato su RoBERTa e allenato su NACHOS, un set di dati di dati medici raccolti dal web. Il team ha confrontato DrBERT con altri modelli pre-allenati come CamemBERT, PubMedBERT e BioBERT, e ha trovato che DrBERT ottiene risultati migliori in nove delle undici attività downstream valutate.

Il team ha anche esaminato l'impatto della quantità di dati utilizzati per l'allenamento del modello e ha trovato che l'utilizzo di più dati tradotti porta a migliori prestazioni. Tuttavia, l'esperimento ha anche mostrato che l'allenamento da zero (from-scratch) sembra ottenere prestazioni migliori rispetto all'allenamento continuo (continual pre-training) utilizzando i pesi e la tokenizzazione di CamemBERT.

Il team ha reso disponibili i modelli pre-allenati ottenuti da NACHOS gratuitamente su Hugging Face, sotto licenza MIT, e i script di allenamento sono disponibili sul repository GitHub del team.</sample>
    <sample id="60">Gli autori dell'articolo sono: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon, autore principale di un articolo ACL intitolato "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training", presenta la sua ricerca su compressione dei modelli di NLG. L'obiettivo è ridurre la complessità e il costo dei modelli NLG senza comprometterne le prestazioni.

La compressione dei modelli NLG è un problema cruciale, poiché i modelli più grandi sono più lenti e costosi. La ricerca si concentra sulla distillazione di conoscenza, una tecnica che trasferisce il sapere da un modello grande (insegnante) a uno più piccolo (studente).

La distillazione di conoscenza è un processo che consiste in due fasi: la prima fase è la distillazione di conoscenza, dove il studente imita il comportamento dell'insegnante; la seconda fase è la fine-tuning del modello studente per migliorare le prestazioni.

La ricerca esplora due tipi di distillazione di conoscenza: la distillazione di conoscenza a livello di parola e la distillazione di conoscenza a livello di sequenza. La distillazione di conoscenza a livello di sequenza utilizza il modello insegnante per generare pseudo-targets, che sono testi generati dal modello insegnante.

La ricerca presenta anche una nuova tecnica di distillazione di conoscenza chiamata "joint-teaching", che combina la distillazione di conoscenza a livello di parola e la distillazione di conoscenza a livello di sequenza per migliorare le prestazioni del modello studente.

In sintesi, la ricerca di Nitay Calderon esplora la compressione dei modelli NLG attraverso la distillazione di conoscenza e presenta una nuova tecnica di distillazione di conoscenza chiamata "joint-teaching" per migliorare le prestazioni del modello studente.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre gli stessi output per la stessa task, indipendentemente dalle leggere variazioni di linguaggio delle istruzioni.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello peggiore, poiché indica che il modello è più influenzato dalle variazioni nella frase d'istruzione e non è in grado di produrre output coerenti.</sample>
    <sample id="66">Il testo presenta una panoramica sul tema della ragione matematica e la sua applicazione nell'intelligenza artificiale e nell'elaborazione del linguaggio naturale. L'autore discute la possibilità di sviluppare macchine in grado di risolvere problemi matematici e dimostrare teoremi, un obiettivo importante nella ricerca di intelligenza artificiale. 

Viene presentato un esempio di problema di geometria, che richiede l'identificazione delle relazioni geometriche, l'applicazione di teoremi e la performance di calcoli per ottenere la risposta numerica. Questo tipo di problema può essere formalizzato come un problema di ragionamento neuro-simbolico.

L'autore menziona anche l'automazione della dimostrazione di teoremi, che aiuta gli esseri umani a dimostrare la verità di affermazioni matematiche. Vengono citati alcuni dataset proposti per testare la capacità di linguaggi naturali di ragionare in modo umano.

Inoltre, l'autore discute le architetture neurali proposte per il ragionamento matematico, come le reti neurali di sequenza a sequenza e le reti neurali di sequenza a albero. Viene menzionata anche l'applicazione di grandi modelli linguistici pre-allenati (LLMs) per risolvere problemi di matematica.

Infine, l'autore discute le limitazioni degli LLMs, come la mancanza di capacità di ragionamento matematico preciso, e propone alcune soluzioni, come la sostituzione della strategia di decodifica avarizia con la coerenza di sé e la creazione di LLMs con strumenti di programmazione.</sample>
    <sample id="67">Il problema dell'interferenza in modelli di traduzione multilingue è un tema importante nella traduzione automatica. L'interferenza può essere benefica o dannosa a seconda del contesto. I ricercatori hanno identificato alcuni fattori chiave che contribuiscono all'interferenza o alla sinergia tra lingue diverse. 

In particolare, l'interferenza è più probabile quando il modello è piccolo rispetto alla dimensione dei dati e quando la temperatura di sampling non è calibrata. La temperatura di sampling è un parametro che influenza la selezione delle esempi di training e può essere utilizzato per ridurre l'interferenza. 

I risultati degli esperimenti hanno mostrato che la scala del modello e la dimensione dei dati sono fattori chiave che influenzano l'interferenza, mentre la similitudine linguistica e il numero di lingue utilizzate hanno un impatto minore. 

Inoltre, i ricercatori hanno scoperto che una temperatura di sampling calibrata è fondamentale per ottenere prestazioni forti. In conclusione, la scala del modello e la temperatura di sampling calibrata possono ridurre significativamente l'interferenza senza la necessità di metodi specializzati.</sample>
    <sample id="68">I dati utilizzati per il pre-addestramento dei modelli vengono presi da diversi set di dati, tra cui BLiMP, SyntaxGym, CrowS pairs e Wikipedia.</sample>
    <sample id="69">20 campioni per classe</sample>
    <sample id="70">I collaboratori dell'articolo sono: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Il tuo lavoro si chiama "Resolving Indirect Referring Expressions for Entity Selection" e consiste nell'introdurre il corpus AltEntities. Il tuo obiettivo è capire il linguaggio dei utenti quando vogliono fare una scelta. Un esempio di domanda alternativa è: "Did you mean 'Easy on Me' or 'I Gotta Feeling'?".

Il tuo lavoro si concentra su riferimenti indiretti, che possono essere utili quando un utente non ricorda il nome di un oggetto o quando le pronunce sono troppo simili. Il tuo corpus copre tre domini: musica, libri e ricette. 

Per creare il corpus, hai utilizzato un setup di completamento di cartoni animati. I partecipanti al test hanno dovuto completare la frase "Do you mean A or B?" con un riferimento indiretto. Il tuo corpus contiene 6.000 domande alternative e 42.000 riferimenti indiretti.

Hai testato il modello T5 XL e hai ottenuto risultati interessanti. Se il modello ha accesso allo stesso background knowledge degli annotatori, l'accuratezza è alta (92-95%). Se il modello ha accesso a background knowledge parzialmente sovrapposto, l'accuratezza è compresa tra 82 e 87%. Se il modello ha accesso solo ai nomi degli entità, l'accuratezza è bassa (60%). Il tuo lavoro ha anche dimostrato che i modelli sono generalizzabili a diversi domini.</sample>
    <sample id="72">I bias dell'informazione possono propagarsi dai dati di pre-istruzione ai modelli linguistici e alle attività a valle, creando problemi di equità nei modelli NLP.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">Il paper "Dense-ATOMIC: Towards Densely-connected ATOMIC con High Knowledge Coverage e Massive Multi-hop Paths" propone un nuovo metodo per completare il graphico di conoscenza commonsense ATOMIC. Il metodo, chiamato Rel-CSKGC, predice le relazioni tra eventi utilizzando un modello di linguaggio pre-allineato. Il metodo si basa su tre parti: la normalizzazione degli eventi di coda, l'addestramento di un modello di previsione di relazione e la costruzione di Dense-ATOMIC.

Il metodo di normalizzazione degli eventi di coda converte gli eventi di coda in una forma equivalente agli eventi di capo. Il modello di previsione di relazione utilizza un modello di linguaggio pre-allineato per codificare gli eventi di capo e coda, e poi predice la relazione tra di loro.

Il metodo di completamento dei link utilizza una strategia di completamento intra- e inter-cluster, che inferra i link mancanti all'interno di un cluster e tra diversi cluster. Il metodo è stato valutato su Dense-ATOMIC e ha mostrato un aumento della copertura dei conoscenza e della presenza di percorsi multi-hop.

Il paper conclude che il metodo Rel-CSKGC è un metodo efficace per completare il graphico di conoscenza commonsense ATOMIC e che Dense-ATOMIC è un graphico di conoscenza commonsense più completo e utile per la ragione dei sensi comuni.</sample>
    <sample id="75">Il lavoro presentato da Zheng Yandan, intitolato Jointprop, è un framework di apprendimento semi-supervisionato per l'estrazione di entità e relazioni (NER e RE) che integra le due task in un'unica rete di apprendimento. Il framework consiste di quattro parti: generazione di caratteristiche di span, costruzione di grafi eterogenei, propagazione di etichette e ottimizzazione del modello.

Il problema attuale è che i modelli di apprendimento supervisionato richiedono un grande quantitativo di dati annotati di alta qualità, mentre i modelli semi-supervisionati si affidano a una piccola quantità di dati etichettati per ottenere modelli potenti a basso costo. Tuttavia, i modelli attuali ignorano le interconnessioni tra NER e RE, che possono portare a una perdita di etichette allineate.

Il framework Jointprop propone di utilizzare grafi eterogenei per modellare le interconnessioni tra NER e RE, e di propagare etichette attraverso il grafo per migliorare la precisione delle etichette. Il processo di propagazione di etichette diffonde le etichette attraverso il grafo lungo aree di alta densità formate dai dati non etichettati.

Gli esperimenti condotti su quattro dataset mostrano che il framework Jointprop ottiene risultati migliori rispetto ai modelli di base sia per NER che per RE, e che la joint learning dei due task beneficia della codependenza tra le due task nei dataset congiunti.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici si compone di tre fasi: 

1. Pretraining data: i modelli linguistici sono addestrati su grandi quantità di dati web, che includono notizie di media politici.
2. Modelli linguistici: i modelli linguistici possono avere opinioni politiche diverse a seconda del loro addestramento e possono occupare quattro quadranti del "politic campus".
3. Task downstream: i modelli linguistici con opinioni politiche diverse possono avere prestazioni diverse in compiti come la detezione di discorsi di odio e la detezione di notizie false.</sample>
    <sample id="77">In questo lavoro, i ricercatori di Yale University e Microsoft Research hanno presentato un nuovo dataset chiamato DeFacto, che contiene esempi di feedback umano per migliorare la coerenza fattuale della sintesi testuale. Il dataset è stato creato chiedendo agli annotatori di correggere e fornire feedback sui riassunti automatici generati da modelli di sintesi testuale esistenti. I ricercatori hanno proposto tre nuovi compiti di NLG (Natural Language Generation): editing del riassunto, generazione di feedback e correzione automatica degli errori fattuali. Hanno trovato che i modelli di NLG possono essere addestrati per seguire il feedback umano e correggere gli errori fattuali, ma che la generazione di feedback rimane un compito difficile. Il dataset DeFacto è stato rilasciato su GitHub e può essere utilizzato per valutare la coerenza fattuale dei modelli di sintesi testuale e per addestrare metriche di valutazione della coerenza fattuale.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Secondo la presentazione, DEPLAIN-apa contiene più riassestamenti e aggiunte di parole rispetto a DEPLAIN-web, mentre DEPLAIN-web contiene più riformulazioni.</sample>
    <sample id="79">No, il CoScript dataset non è disponibile pubblicamente, ma è stato reso disponibile per la comunità di ricerca.</sample>
    <sample id="80">La filigrana viene inserita nel testo tramite la contabilità dei trigger, ovvero un insieme di parole di frequenza moderata. Quando un utente invia una frase al servizio del fornitore, il servizio conta il numero di trigger presenti nella frase e somma un peso all'embedding originale in base a questo conteggio. Se il numero di trigger supera un certo valore "m", l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">Il video descrive un progetto di valutazione automatica degli scritti, noto come AES (Automated Essay Scoring). L'AES è un'applicazione della linguistica naturale nella didattica che mira a valutare la qualità degli scritti senza intervento umano. Tuttavia, il training dei modelli AES richiede grandi corpora di testi etichettati, il che è tempo e lavoro consumante. Il progetto proposto introduce un nuovo framework chiamato ULRA (Unsupervised Learning from Rank Aggregation), che utilizza più segnali di qualità heuristici come supervisione pseudo-oggettiva per allenare un modello di valutazione AES. Il framework ULRA è composto da due moduli principali: HER (Heuristic Essay Ranking) e DPRA (Deep Pairwise Rank Aggregation). Il modulo HER genera liste di classifica degli scritti utilizzando segnali di qualità heuristici, mentre il modulo DPRA aggrega le liste di classifica per fornire una supervisione unificata. Il progetto propone anche una strategia di trasformazione dei punteggi predetti per garantire che siano all'interno di un intervallo di punteggio predefinito. Gli esperimenti condotti hanno dimostrato che il framework ULRA supera i metodi basati su supervisione non supervisionata e raggiunge prestazioni competitive rispetto ai metodi cross-prompt e one-shot.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">Shwai He sta presentando il suo articolo "PAD-Net: An Efficient Framework for Dynamic Networks" per ACL 2023. Egli inizia con la descrizione dei reti neurali dinamiche, che possono modificare l'architettura o i parametri in base all'input. Tuttavia, le reti neurali dinamiche tradizionali sono costose e richiedono molti parametri.

Shwai He ipotizza che le reti neurali dinamiche siano composte da sottoreti parzialmente dinamiche che mantengono o superano la capacità di rappresentazione dell'originale rete. Pertanto, egli propone il framework PAD-Net, che divide i parametri in dinamici e statici e stabilisce due fattori di scala per descrivere l'intensità delle due modalità.

Il metodo utilizzato per dividere le due modalità è l'Iterative Mode Partition. L'obiettivo è quello di rendere statici i parametri dinamici non essenziali, poiché questo può ridurre significativamente la perdita di valore. Gli esperimenti mostrano che PAD-Net supera le reti neurali statiche e dinamiche e mantiene un numero minore di parametri e una maggiore efficienza computazionale.

Shwai He esegue anche studi di ablatività per trovare i rapporti ottimali per la convoluzione dinamica e la miscela di esperti e scopre che i fattori di scala per i parametri dinamici e statici sono cruciali per l'accuratezza delle reti dinamiche. Inoltre, PAD-Net si dimostra superiore alle reti neurali dinamiche tradizionali e mantiene una maggiore discriminazione nelle uscite.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata potrebbe essere: "Preparare un dolce di cioccolato per un compleanno di bambino con allergia al latte".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo attraverso l'analisi visiva degli embedding dei testi su quattro dataset utilizzando la PCA (Componenti Principali). Le figure mostrano che è difficile distinguere gli embedding backdoor da quelli normali, il che suggerisce che il metodo Embedding Marker sia abbastanza covert.</sample>
    <sample id="87">Il lavoro di DrBERT utilizza i PLM esistenti come base per costruire un nuovo modello, ad esempio RoBERTa, e lo adatta alle esigenze del dominio biomedico francese attraverso il pre-training su un grande dataset di dati medici (NACHOS). Inoltre, il lavoro compara il DrBERT con altri PLM esistenti, come CamemBERT, PubMedBERT e BioBERT, per valutare la sua efficacia in diversi compiti di NLP.</sample>
    <sample id="88">Non è stato specificato espressamente che GPT-4 è meno allineato a un particolare Paese. Tuttavia, è stato menzionato che GPT-4 è meno allineato a persone non binarie rispetto agli uomini e alle donne.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase: "Per esempio, se riceviamo un blocco di parola che contiene "Vado a parlare di..." e il nostro modello predice la traduzione in tedesco, e guardiamo alle pesi di attenzione incrociata, vedremo che le prime due parole puntano ai primi frame di parola ricevuti, mentre l'ultima parola punti ai frame di parola ricevuti più recenti, per lambda parole."</sample>
    <sample id="90">L'articolo "Rethinking Annotation: Can Language Learners Contribute?" esplora la possibilità di utilizzare gli apprendenti di una lingua come annotatori di dati per il learning delle lingue naturali (NLP). Gli autori, Haneul Yoo e collaboratori, mettono in discussione la convenzione di reclutare solo madrelingua per la annotazione dei dati NLP, poiché ciò può essere difficile per molte lingue. I ricercatori hanno condotto un esperimento pilota per esaminare la fattibilità di utilizzare gli apprendenti come annotatori, scegliendo tre lingue (inglese, coreano e indonesiano) e quattro compiti comuni nel benchmark GLUE.

Gli autori hanno reclutato sia madrelingua che apprendenti e hanno condotto gli stessi esperimenti, utilizzando criteri di valutazione modificati per categorizzare gli apprendenti in tre livelli di competenza: base, intermedio e avanzato. I risultati mostrano che le etichette annotate dagli apprendenti sono quasi accurate, soprattutto per compiti più semplici e domande di livello facile-medio. Inoltre, gli apprendenti sono quasi pari ai madrelingua se le loro etichette sono aggregate con quelle degli altri attraverso il voto di maggioranza.

Gli autori hanno anche dimostrato che i modelli di apprendimento dei dati possono essere addestrati con le etichette degli apprendenti, raggiungendo prestazioni simili a quelle dei modelli addestrati con le etichette dei madrelingua. Infine, gli autori hanno osservato che la competenza linguistica e la conoscenza lessicale e grammaticale degli apprendenti migliorano durante l'esecuzione delle attività di annotazione.

In conclusione, l'articolo suggerisce che gli apprendenti possono contribuire significativamente alla annotazione dei dati NLP e apre la possibilità di estendere la ricerca NLP a molte lingue, superando ostacoli geografici e tecnologici per creare set di dati di riferimento per lingue a bassa risorsa.</sample>
    <sample id="91">Come mostrato dai risultati, la quantità di attività aumenta la performance del modello e riduce la sensibilità. Ciò significa che il modello diventa più stabile e coerente nelle sue risposte man mano che si esegue su più compiti.</sample>
    <sample id="92">Gli autori non menzionano esplicitamente i tre approcci di riferimento con cui confrontano il loro metodo, ma si riferiscono a "other treeless models" che non utilizzano alberi per affrontare la composizione generale.</sample>
    <sample id="93">Il primo autore, Matthias Lindemann, ha lavorato insieme ai suoi advisor, Alexander Koller e Ivan Titov, per sviluppare il lavoro presentato nel paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations".</sample>
    <sample id="94">Ciao Jingwei, grazie per la presentazione del tuo lavoro. Ecco un riassunto sintetico del tuo contributo:

**Background**: Gli strumenti di embedding come servizi (EaaS) sono stati sviluppati su grandi modelli linguistici per assistere a vari compiti di NLP. Tuttavia, gli attaccanti possono rubare il modello imparando dall'embedding e offrire servizi simili.

**Obiettivo**: Proteggere la proprietà intellettuale degli EaaS tramite l'inserimento di un marchio d'acqua (watermark) nel servizio di provider e la verifica se un altro servizio contiene il marchio.

**Proposta**: Embedding Marker, un metodo di watermark basato su un backdoor applicabile agli EaaS. Il metodo consiste in due fasi principali: watermark injection e verifica della proprietà intellettuale.

**Fasi del metodo**:

1. **Selezione di un set di trigger**: un insieme di parole con frequenza moderata.
2. **Inserimento del marchio**: quando un utente invia una frase al servizio di provider, il servizio conta il numero di trigger nella frase e somma il peso dell'embedding originale con l'embedding target.
3. **Verifica della proprietà intellettuale**: il servizio di provider richiede gli embedding al servizio di attaccante con un set di dati backdoor e benigno. Il servizio di attaccante risponde con gli embedding richiesti, che vengono confrontati con l'embedding target per valutare la presenza del marchio.

**Esperimenti**: sono stati condotti su quattro set di dati (AG News, MIND, SST2 e Enron Spam) e il risultato ha mostrato che l'Embedding Marker può avere un grande rendimento di detezione mentre mantiene un grande utilità per le attività downstream.</sample>
    <sample id="95">Non è presente informazione chiara sull'autore principale di PaLM nel testo fornito. Tuttavia, PaLM è stato presentato da Meta, e non da Google Translate, come menzionato da David Vilar.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, un primo anno di dottorato presso l'Università di Carnegie Mellon e oggi presenterò il nostro lavoro "NLPositionality" che caratterizza i bias di progettazione dei dati e dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Instituto Allen per l'Intelligenza Artificiale, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Immaginiamo di lavorare per un giornale e di dover rimuovere il contenuto tossico dai commenti sotto un articolo di notizie. Potremmo utilizzare un API popolare come Prospective API per la detezione di contenuto tossico, che funziona bene se siamo Carl Jones. Tuttavia, se siamo Aditya Sharma, l'API non è altrettanto sensibile alle espressioni offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione dove si osservano differenze sistematiche nel rendimento della tecnologia tra popolazioni. I bias di progettazione come quello appena visto possono essere dovuti alla posizionalità dei ricercatori e sviluppatori di NLP. La posizionalità è il concetto di prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato nelle ricerche critiche, in particolare negli spazi accademici femministi e queer.

Come ricercatori, la posizionalità può influenzare il processo di ricerca e i suoi risultati e conclusioni, poiché può cambiare le decisioni che prendiamo. Una domanda che le persone potrebbero fare è: i dati e i modelli di NLP hanno posizionalità? Non stiamo cercando di dire che i modelli stessi o i dati abbiano identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità più che altre.

La precedente ricerca ha suggerito alcune evidenze aneddotiche di posizionalità, come i divari culturali tra modelli e dati, nonché definizioni teoriche della posizionalità dei modelli. Tuttavia, queste ricerche non hanno esaminato la comparazione tra utenti finali e i dati e i modelli stessi, e lo studio della posizionalità dei dati e dei modelli è sempre più importante poiché le attività di NLP diventano sempre più soggettive e socialmente orientate.

Per studiare la posizionalità dei dati e dei modelli, abbiamo confrontato le annotazioni con utenti reali con i dati e i modelli esistenti attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è la riannotazione dei dati con annotatori diversi. Ciò è necessario per evitare di guardare solo alle demografie degli annotatori originali, poiché solitamente solo pochi annotatori annotano ogni istanza e le demografie sono raramente raccolte e condivise. Inoltre, optiamo per la riannotazione per ottenere molti annotatori per istanza e una ricca gamma di dati demografici.

In seguito, prendiamo le annotazioni per gruppi demografici e le confrontiamo con i dati e i modelli utilizzando un punteggio di correlazione Pearson. Il nostro framework si differenzia dalla letteratura sull'accordo tra annotatori poiché confronta gli utenti finali con i dati e i modelli, le previsioni e le etichette, anziché guardare solo all'accordo tra annotatori o modellare le distribuzioni degli annotatori.

Il nostro framework è in gran parte reso possibile da Lab in the Wild, una piattaforma di sperimentazione online e di crowdsourcing per l'interazione uomo-macchina. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto alle piattaforme come MTurk, che hanno prevalentemente partecipanti dagli Stati Uniti o dall'India, Lab in the Wild è in grado di ottenere dati di alta qualità.

Abbiamo ospitato due compiti su Lab in the Wild. Il primo compito è la social acceptability, dove i partecipanti leggono una situazione dal dataset di social chemistry e scrivono come accettabile socialmente è la situazione. Per rimanere coinvolti nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4.

Abbiamo riprodotto un setup simile per il compito di detezione di contenuto tossico e di abusi d'ingiuria, dove i partecipanti leggono un esempio dal dataset Dynahate e scrivono se ritengono che sia un esempio di abuso d'ingiuria. Abbiamo poi confrontato queste annotazioni con Dynahate, Prospective API, Rewire API, Hate Roberta e GPT 4.

La nostra indagine ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. Siamo ora in grado di rispondere a chi sono i dati e i modelli di NLP più allineati.

Troviamo che ci sia posizionalità in NLP. Ad esempio, troviamo che i dati e i modelli siano più allineati ai paesi di lingua inglese. Nel compito di social acceptability di GPT 4, troviamo che sia più allineato ai paesi di lingua confuciana e inglese. Troviamo che Dynahate sia anche più allineato ai paesi di lingua inglese. Troviamo inoltre che ci sia un'allineamento supplementare con le persone che hanno un titolo di studio universitario. Nel compito di social acceptability di GPT 4, troviamo che sia più allineato con le persone che hanno un titolo di studio universitario o di dottorato.

Tuttavia, quando i dati e i modelli sono allineati a specifiche popolazioni, qualcuno è inevitabilmente lasciato indietro. Ad esempio, troviamo che i dati e i modelli siano meno allineati alle persone non binarie rispetto ai maschi e alle femmine. Troviamo questo nel compito di social acceptability di GPT 4, nonché nel compito di detezione di contenuto tossico di Dynahate.

Data la presenza di posizionalità in NLP, cosa possiamo fare? Abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. La seconda è condurre ricerche di NLP con la lente della perspectivismo. La terza è costruire dati e modelli specializzati per specifiche comunità. Un esempio di ciò è l'iniziativa Masakhani.

Vogliamo sottolineare che la NLP inclusiva non è solo quella di rendere funzionali tutte le tecnologie per tutti. Ecco il termine della nostra presentazione. Se volete imparare di più, potete visitare il nostro dashboard per gli ultimi risultati di analisi e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona 3 problemi associati a SimulST:

1. Specifiche architetture che richiedono addestramento aggiuntivo.
2. Procedure di addestramento lunghe e complesse.
3. Necessità di mantenere e addestrare modelli separati per raggiungere differenti regimi di latenza.</sample>
    <sample id="98">Una possibile soluzione per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere l'utilizzo di tecniche di pre-elaborazione dei dati, come ad esempio:

*   Utilizzare tecniche di riduzione delle informazioni per ridurre l'impatto dei bias nei dati;
*   Utilizzare tecniche di normalizzazione dei dati per ridurre l'impatto dei bias nei dati;
*   Utilizzare tecniche di sampling dei dati per ridurre l'impatto dei bias nei dati;
*   Utilizzare tecniche di valutazione dei dati per identificare e correggere i bias nei dati;
*   Utilizzare tecniche di generazione di dati artificiali per ridurre l'impatto dei bias nei dati;
*   Utilizzare tecniche di valutazione dei modelli per identificare e correggere i bias nei modelli di NLP.

Inoltre, potrebbe essere utile utilizzare tecniche di valutazione dei dati e dei modelli che siano specificamente progettate per identificare e correggere i bias sociali e politici, come ad esempio:

*   Utilizzare metriche di valutazione che siano sensibili ai bias sociali e politici;
*   Utilizzare tecniche di valutazione che siano in grado di identificare e correggere i bias nei dati e nei modelli;
*   Utilizzare tecniche di valutazione che siano in grado di valutare l'impatto dei bias sui risultati dei modelli di NLP.

Inoltre, potrebbe essere utile utilizzare tecniche di valutazione dei dati e dei modelli che siano specificamente progettate per valutare l'impatto dei bias sui risultati dei modelli di NLP, come ad esempio:

*   Utilizzare metriche di valutazione che siano sensibili ai bias sociali e politici;
*   Utilizzare tecniche di valutazione che siano in grado di identificare e correggere i bias nei dati e nei modelli;
*   Utilizzare tecniche di valutazione che siano in grado di valutare l'impatto dei bias sui risultati dei modelli di NLP.

Inoltre, potrebbe essere utile utilizzare tecniche di valutazione dei dati e dei modelli che siano specificamente progettate per valutare l'impatto dei bias sui risultati dei modelli di NLP, come ad esempio:

*   Utilizzare metriche di valutazione che siano sensibili ai bias sociali e politici;
*   Utilizzare tecniche di valutazione che siano in grado di identificare e correggere i bias nei dati e nei modelli;
*   Utilizzare tecniche di valutazione che siano in grado di valutare l'impatto dei bias sui risultati dei modelli di NLP.</sample>
    <sample id="99">Ciao, sono Siyu Yuan della Fudan University e sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models per il Pianificazione Linguistica Confinata". 

Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo dopo passo in forma di script orientati al raggiungimento di obiettivi. I precedenti lavori hanno sfruttato i modelli linguistici per pianificare per obiettivi astratti di attività stereotipiche, come "fare un dolce". E hanno mostrato che i grandi modelli linguistici possono effettivamente decomporre gli obiettivi in passaggi. Tuttavia, i precedenti lavori si sono principalmente concentrati sulla pianificazione per obiettivi astratti di attività stereotipiche. Pianificare per obiettivi con vincoli specifici, come "fare un dolce al cioccolato", rimane ancora poco esplorato.

In questo lavoro, definiamo il problema della pianificazione linguistica con vincoli, che impone diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli multi-aspettativi. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli.

In questo lavoro, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare la nostra ricerca, dobbiamo acquisirli per primi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multi-aspettativi per l'acquisizione dati in loop con l'utente tramite InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo i script generati dai grandi modelli linguistici. La tabella riporta l'accuratezza complessiva dei risultati. Troviamo che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.

Condiamo un'analisi dettagliata per investigare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Scendiamo in un argomento più dettagliato delle categorie di vincoli definite in wikiHow. La mappa di calore nella figura mostra che il rendimento di pianificazione di InstructGPT varia considerevolmente per obiettivi di diverse categorie.

I precedenti studi hanno mostrato che la qualità degli output dei modelli linguistici cade in alta varianza, portando a un cattivo rendimento. Quindi, adottiamo l'idea di sovrapprodurre- poi-filtrare per migliorare la qualità di generazione. Innanzitutto, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici sulla base degli obiettivi astratti di semina. Poi, InstructGPT sovrapproduce K script per obiettivi specifici. Successivamente, si sviluppa un modello filtro per selezionare i script fedeli. Converto script e obiettivi in embedding InstructGPT e calcolo la similitudine coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premio lo script che contiene le parole chiave del vincolo di destinazione. Conservo lo script solo se l'obiettivo di destinazione punteggia il più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di alta qualità. Il nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà ai vincoli.

Poiché i grandi modelli linguistici sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica dei modelli più piccoli e specializzati. Creare il dataset è un passo essenziale in questo senso. Tuttavia, i precedenti studi non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Quindi, seguiamo l'idea di distillazione del conoscimento simbolico, per distillare il dataset di pianificazione linguistica con vincoli dai grandi modelli linguistici. Applichiamo il nostro metodo per creare un dataset di pianificazione linguistica con vincoli, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per assicurare la qualità del set di validazione e test, chiediamo ai lavoratori crowdsourced di trovare e correggere le campioni errate. La figura mostra la distribuzione dei vincoli di CoScript. Troviamo che CoScript mostra una grande varietà nei obiettivi specifici generati.

Con CoScript possiamo provare i modelli più piccoli ma specializzati per la pianificazione linguistica con vincoli. Troviamo che il T5 fine-tunato su CoScript può generare script di alta qualità rispetto a molti grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando addestrati su dataset adeguati.

In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Valutiamo la capacità di pianificazione linguistica con vincoli dei grandi modelli linguistici e sviluppiamo un metodo sovrapprodurre- poi-filtrare per i grandi modelli linguistici. Utilizziamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica con vincoli. Speriamo che il dataset CoScript possa essere un'importante risorsa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Potete trovare ulteriori dettagli di CoScript nel nostro articolo.</sample>
    <sample id="100">Il sistema di recupero di informazioni di PromptRank è un approccio data-efficient per la risoluzione di domande a più passaggi. Utilizza una combinazione di un metodo di recupero non supervisionato con un rireranker basato su un modello di linguaggio a pochi esempi. Il sistema è composto da due passaggi principali: il recupero di una piscina di candidati di catene utilizzando la ricerca TF-IDF e l'esplorazione dei collegamenti ipertestuali, e il rireranking dei candidati utilizzando il rireranker a pochi esempi.

Il sistema utilizza la probabilità della domanda data la catena secondo un modello di linguaggio come funzione di punteggio. Per costruire la catena di promemoria, il sistema inserisce i documenti della catena nella promemoria e aggiunge un token di indicatore per designare i documenti. L'instruction serve a stimolare la capacità di ragionamento del modello di linguaggio sui documenti della catena.

Il sistema esplora anche tecniche aggiuntive, come la ricerca dell'instruction per trovare istruzioni ottimali e la campionatura dell'instruction per aggregare più punteggi. Il sistema è stato sperimentato con GPT2-XL e T5-XL e valutato sul dataset HotpotQA. Il sistema ha ottenuto risultati migliori rispetto ai sistemi supervisionati e comparabili ai sistemi di recupero di informazioni densi a stato dell'arte.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di traduzione di stato dell'arte.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono:

1. Applicabilità all'embedding come servizio
2. Nessuna degradazione della utilità degli embedding forniti
3. Covertibilità sufficiente per l'attaccante o facilità di rimozione della filigrana
4. Trasferibilità della filigrana al servizio dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono:</sample>
    <sample id="104">Solitamente solo pochi annotatori annotano ogni istanza e le informazioni demografiche sono rare e non condivise. Pertanto, si opta per riannotare i dati per ottenere molti annotatori per istanza e una ricca varietà di dati demografici.</sample>
    <sample id="105">Cosine e L2.</sample>
    <sample id="106">Il testo descrive il progetto QUEST, un dataset di retrieval creato per studiare l'efficacia dei sistemi nella gestione di informazioni selective. Il dataset comprende oltre 3.000 query che contengono operazioni di insieme implicito e risposte verificate per la loro rilevanza. Le query sono state costruite utilizzando nomi di categoria di Wikipedia da quattro domini (film, libri, piante e animali) e sono state parafrazzate e validate da annotatori umani per garantire la loro fluentezza e naturalità. Il dataset è stato creato per valutare la capacità dei sistemi di ritrovare insiemi di risposte multipli in un corpus di documenti, dove la prova di rilevanza di un documento può provenire da diverse parti del documento.

Il testo descrive anche la costruzione del dataset, che comprende la creazione di query parafrazzate, la validazione della loro fluentezza e naturalità, la verifica della rilevanza delle entità nelle risposte e la marcatura delle prove di rilevanza nei documenti. Inoltre, il testo descrive la valutazione dei sistemi sul dataset, che richiede la ritrovamento di insiemi di risposte multipli da un corpus di documenti, dove le query contengono operazioni di insieme implicito e le prove di rilevanza dei documenti possono provenire da diverse parti del documento. Il testo conclude sottolineando la difficoltà dei sistemi nel gestire query con operazioni di insieme e la speranza che il dataset QUEST possa aiutare a migliorare i sistemi per scenari di ricerca di informazioni con bisogni di informazione selezionati.</sample>
    <sample id="107">I modelli basati su codificatori multilingue utilizzati sono stati XLM-R e mBERT, associati a decoder basati su puntatori (PTR), e mBART e mT5, che sono modelli Encoder-Decoder multilingue.</sample>
    <sample id="108">Il lavoro presentato da Koustav Sinha e collaboratori riguarda l'evaluazione delle giudizi di accettabilità dei modelli linguistici (LLM). L'obiettivo è di valutare se i LLM sono robusti rispetto al contesto e se i loro giudizi di accettabilità cambiano quando il contesto è più lungo. 

Il team ha rielaborato il paradigma di coppia minima (MPP) per valutare i LLM su giudizi di accettabilità. Il MPP valuta se un modello assegna una probabilità maggiore a una frase accettabile rispetto a una frase non accettabile. Tuttavia, il paradigma attuale non permette di valutare la capacità dei LLM di accettare frasi più lunghe.

Il team ha rielaborato il MPP per valutare i LLM su frasi più lunghe, creando nuove frasi che includono una parte accettabile o non accettabile. Hanno utilizzato dati da BLiMP, SyntaxGym e CrowS pairs per creare queste frasi.

I risultati mostrano che i LLM sono sensibili ai caratteri sintattici e semantici latenti condivisi tra le frasi. Il paradigma MPP attuale non riesce a catturare l'intera conoscenza astratta dei LLM all'interno del contesto. Il team suggerisce che i LLM debbano essere valutati su frasi più lunghe per comprendere meglio la loro capacità di accettare contesti complessi.</sample>
    <sample id="109">"Unnatural Instructions: Tuning Language Models con (Quasi) Nessun Lavoro Umano" è un progetto che mira a creare un dataset di istruzioni per compiti linguistici diversi, senza richiedere lavoro umano. L'obiettivo è quello di creare un dataset diverso in termini di compiti, contenuti e frasi, utilizzando un modello di linguaggio pre-addestrato. 

Il progetto si basa su un modello di linguaggio che genera istruzioni e input, e successivamente output, in modo completamente automatico. Il dataset contiene 64.000 esempi, con un totale di circa 240.000 esempi se si considerano anche le parafrasi delle istruzioni.

Il progetto analizza le proprietà del dataset, come creatività, diversità e correttezza. I risultati mostrano che più del 50% degli esempi generati sono corretti, e anche gli esempi errati contengono informazioni utili per l'addestramento dei modelli di linguaggio.

Il progetto ha anche valutato l'utilità del dataset, addestrando un modello T5 con 11 miliardi di parametri su Unnatural Instructions e confrontandolo con altri modelli. I risultati mostrano che il modello addestrato su Unnatural Instructions outperforma i modelli di confronto su diversi benchmark.</sample>
    <sample id="111">Gli autori decidono quali sono le parole a frequenza moderata attraverso il conteggio della frequenza delle parole in un corpus di testo generale raccolto dal provider.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng e oggi presenterò il nostro articolo "I taggers di CoNLL-2003 ancora funzionano bene nel 2023?". Cominciamo. Il nostro articolo ha investigato il problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate (NER) o compito NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare il NER per quasi 20 anni e ciò solleva naturalmente diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi taggers, cosa serve per una buona generalizzazione? Allo stesso tempo, se osserviamo una cattiva generalizzazione, cosa causa il calo di prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi abbiamo annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi addestrato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sul set di test CoNLL-03 che sul CoNLL++. E infine, abbiamo calcolato il cambiamento percentuale di F1 per valutare la generalizzazione di ogni modello. Quindi, cosa serve per una buona generalizzazione? Durante gli esperimenti abbiamo scoperto che ci sono tre ingredienti principali che servono. Il primo è l'architettura del modello. Attraverso i nostri esperimenti abbiamo scoperto che i modelli transformer generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo che il numero di esempi di fine-tuning influenza direttamente le prestazioni di una task downstream. Qui abbiamo anche trovato che più esempi di fine-tuning, porta a una migliore generalizzazione. Passando alla nostra prossima domanda, cosa causa il calo di prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal reusare lo stesso set di test più volte e ciò è solitamente manifestato come i ritorni decrescenti su un nuovo set di test. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dal aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting adattivo, abbiamo visto che dalla grafica a destra, la linea di migliore adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 traduce in più di una unità di miglioramento su CoNLL++ che significa che non ci sono ritorni decrescenti. E questo ci mostra che l'overfitting adattivo in questo caso non è osservato. Quindi cosa succede con il drift temporale? Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a preaddestrare alcuni modelli con dati più recenti e abbiamo trovato che le prestazioni peggiorano con un divario temporale più grande e ciò conferma la nostra ipotesi che la principale causa del calo di prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, una dimensione del modello più grande, nonché più esempi di fine-tuning. E ciò va di pari passo, non possiamo avere solo un ingrediente e buttar fuori gli altri. Allo stesso tempo, abbiamo anche trovato che il calo di prestazioni è causato dal drift temporale e, sorprendentemente, non è causato dall'overfitting adattivo anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo "I taggers di CoNLL-2003 ancora funzionano bene nel 2023?" E abbiamo trovato che la risposta è un sì deciso. Speriamo che il nostro articolo chiami a più ricerche su come migliorare la generalizzazione dei modelli. E infine, per favore, assicuratevi di consultare il nostro articolo, il nostro dataset e se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">L'autore presenta il lavoro "Finding the Pillars of Strength for Multi-Head Attention" presentato all'AACL 2023. Il modello di linguaggio è rivoluzionario, ma presenta alcune limitazioni, come ad esempio i parametri pesanti, il tempo di training lungo e la necessità di un corpus enorme.

L'autore si concentra sul problema dei parametri pesanti e propone un nuovo approccio chiamato "Grouped Head Attention" che utilizza una strategia di divisione e conquista per comprimere l'attenzione multi-testa. Il modello è composto da due stadi: il primo è il training con vincoli di gruppo, che divide le teste di attenzione in gruppi e rende le teste all'interno di un gruppo più simili e quelle tra gruppi più separate. Il secondo stadio è l'algoritmo "Voting-to-Stay" che prunne le teste di attenzione redundanti e mantiene solo una testa per gruppo.

Il modello è stato valutato su tre task: traduzione automatica, modellazione del linguaggio e riassunto astratto. I risultati mostrano che il modello proposto ottiene prestazioni simili o migliori rispetto ai modelli di base e riduce i parametri di circa il 32%.

L'autore conclude che la compressione dei parametri è una direzione promettente e che il modello proposto può essere utilizzato per prunne i modelli di linguaggio grandi senza sacrificare la prestazione.</sample>
    <sample id="115">Non viene specificata una dimensione fissa del segmento parlato utilizzato dall'approccio EDAtt, ma viene menzionato che il modello utilizza "speech chunks" che contengono una certa quantità di informazione sufficiente per emettere una parola di traduzione.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono "Servin è un giudice".</sample>
    <sample id="117">La qualità dell'esempio.</sample>
    <sample id="118">Il team di ricerca presenta la loro submission ACL 2023 intitolata "Improving Pretraining Techniques for Code-Switched NLP" (Migliorare le tecniche di pre-allenamento per il linguaggio naturale in codice condiviso). La presentazione inizia con la definizione di codice condiviso, che si riferisce alla combinazione di parole di due o più lingue in una stessa frase. L'esempio fornito è "Laptop, mere, bag, me, rakha, hai", che combina inglese e hindi.

Il team afferma che i modelli pre-allenati multilingue come mBERT e XLM-R non performano bene sulle task di question answering e sentiment analysis per il linguaggio in codice condiviso. Pertanto, propongono due nuove tecniche di pre-allenamento: SwitchMLM e FrequencyMLM. SwitchMLM è un adattamento dell'obiettivo di pre-allenamento Masked Language Modeling (MLM) per il linguaggio in codice condiviso, che considera solo i "switch-point" (punti di transizione tra lingue) come token mascherabili. FrequencyMLM è un metodo surrogato che utilizza la frequenza delle parole in un corpus monolingue per assegnare le etichette di lingua (LID).

Il team propone anche alcune modifiche architettoniche per migliorare la rappresentazione delle informazioni di codice condiviso. Tra queste, l'aggiunta di connessioni residuali tra gli strati di una rete neurale e l'imposizione di un'etichetta di perdita LID per incoraggiare l'acquisizione di informazioni di codice condiviso.

I risultati mostrano che la combinazione di SwitchMLM o FrequencyMLM con connessioni residuali e perdita LID migliora la prestazione sulle task di sentiment analysis per il linguaggio in codice condiviso. Inoltre, gli esperimenti di probing confermano che le tecniche proposte aumentano l'informazione di codice condiviso nelle rappresentazioni intermedie e finali delle reti neurali.</sample>
    <sample id="119">Gli esperimenti estesi si concentrano sui modelli linguistici GPT-4, RoBERTa e BART, e sui loro varianti, in particolare GPT series e BART series.</sample>
    <sample id="120">Il modello EDAtt utilizza i punteggi di attenzione di un livello specifico, ovvero i punteggi di attenzione tra l'input audio e l'output testuale.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono: 

* Direttamente nominare il nome del song, ad esempio "Easy on Me".
* Utilizzare la posizione del song, ad esempio "il primo".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Il team di ricerca guidato da Ying e Zhiyang ha presentato un nuovo lavoro intitolato "MultiInstruct: un dataset di istruzioni multimediali per l'apprendimento zero-shot in modo efficiente". L'obiettivo del progetto è di esplorare se l'istruzione tuning possa migliorare la generalizzazione di modelli multimediali a task inediti.

Il team ha creato un dataset multimediale chiamato MultiInstruct, che comprende 62 task diverse e 10 categorie principali. Il dataset è stato costruito utilizzando 21 dataset aperti e contiene 5 istruzioni scritte da esperti per ogni task.

Il team ha utilizzato il modello multimediale OFA come base e ha mostrato che l'istruzione tuning può migliorare significativamente le prestazioni del modello su task multimediali visti. Inoltre, hanno scoperto che il trasferimento di apprendimento da dataset di istruzioni naturali può migliorare ulteriormente le prestazioni del modello.

Il team ha anche proposto un nuovo metrica chiamata sensibilità, che misura la capacità del modello di produrre output coerenti per lo stesso task, indipendentemente dalle variazioni nella formulazione delle istruzioni.

Infine, il team ha annunciato che sta raccogliendo un dataset più grande di istruzioni multimediali con oltre 150 task aggiuntivi e che sarà rilasciato pubblicamente.</sample>
    <sample id="124">Qingyu Tan e la sua squadra hanno presentato un lavoro intitolato "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models". Il loro obiettivo è migliorare la capacità di ragionamento temporale dei modelli di linguaggio grandi. 

Hanno identificato tre livelli di ragionamento temporale: 

1. Ragionamento tra date (L1): ad esempio "Qual è l'anno successivo al 2010?".
2. Ragionamento tra eventi e date (L2): ad esempio "Qual è la squadra di Lionel Messi nel 2010?".
3. Ragionamento tra eventi (L3): ad esempio "Qual è la squadra di Lionel Messi dopo il Barcellona?".

Hanno creato un dataset chiamato TempReason che copre tutti e tre i livelli di ragionamento e copre un periodo di tempo ampio. Hanno anche proposto un nuovo setting di valutazione chiamato "Reasoning QA" in cui vengono forniti ai modelli di linguaggio tutti i fatti temporali rilevanti e loro devono ragionare per trovare la risposta corretta.

Per migliorare la capacità di ragionamento temporale dei modelli di linguaggio, hanno proposto un approccio di addestramento con due componenti: 

1. Estrazione di span temporali pre-addestramento, che consiste nell'addestramento dei modelli per ricostruire i span temporali e di entità in un testo raw.
2. Apprendimento per rinforzo sensibile al tempo, che consiste nel premiare i modelli per le risposte corrette e penalizzare le risposte errate rispetto al tempo.

Hanno valutato i risultati dei loro modelli su TempReason e hanno trovato che il loro modello, chiamato TempT5, è stato in grado di migliorare significativamente la capacità di ragionamento temporale dei modelli di linguaggio grandi.</sample>
    <sample id="125">Non sono stati menzionati i nomi degli autori nell'intera presentazione.</sample>
    <sample id="126">No, non è stato considerato come un approccio standard. L'autore ha menzionato che questo approccio è stato utilizzato nel "Translate-Test" setting, ma non è stato presentato come un approccio generale o standard per il parsing semantico cross-linguale.</sample>
    <sample id="127">Il lavoro "Large Language Models Are Reasoning Teachers" è un progetto condotto da Namgyu Ho, Laura Schmid e Se-Young Yun del KAIST AI in Corea. L'obiettivo è trasferire le abilità di ragionamento dei grandi modelli di linguaggio a modelli più piccoli. Il problema attuale è che la tecnica di ragionamento a catena funziona solo con modelli molto grandi come GPT-3 o PALM, che richiedono grandi risorse di memoria e calcolo.

Per risolvere questo problema, gli autori propongono di utilizzare i grandi modelli come "insegnanti di ragionamento" per trasferire le loro abilità ai modelli più piccoli. Sviluppano anche una tecnica innovativa chiamata "ragionamento diverso" per migliorare l'insegnamento. 

I risultati mostrano che i modelli più piccoli possono eseguire complesse operazioni di ragionamento, precedentemente impossibili con modelli più piccoli. La tecnica di ragionamento a catena applicata ai grandi modelli e poi utilizzata per addestrare i modelli più piccoli mostra un miglioramento significativo nelle prestazioni.

La tecnica di "ragionamento diverso" aumenta ulteriormente le prestazioni, con un aumento del 22% nelle prestazioni su alcuni compiti. Gli autori suggeriscono che la distillazione dei grandi modelli può essere applicata anche ad altre abilità emergenti.

Il progetto fornisce il codice e i dati degli esperimenti, inclusi i costi di inferenza di $1000 o più per il modello di OpenAI. Gli autori invitano a prendere il loro materiale per ulteriori ricerche e a contattarli per discussioni.</sample>
    <sample id="128">Akshatha e Martin presentano il loro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources", una collaborazione tra McGill University, Mila e Microsoft Research. Il test KITMUS è stato progettato per valutare la capacità dei modelli di linguaggio naturale di integrare conoscenza da diverse fonti. I modelli di linguaggio naturale utilizzano una varietà di fonti di conoscenza, tra cui la conoscenza contenuta nei loro parametri, acquisita durante la pre-allenamento, e la conoscenza fornita nell'input durante il tempo di inferenza.

Il lavoro di Akshatha e Martin propone un set di test diagnostiche per l'integrazione della conoscenza. Il set di test è stato progettato per valutare la capacità dei modelli di risolvere il riferimento coreferenziale, che richiede la conoscenza di entità specifiche e conoscenza di fondo. I risultati mostrano che molti modelli di risoluzione del riferimento coreferenziale non sono in grado di ragionare sulla conoscenza da diverse fonti senza allenamento specifico per il compito.

Tuttavia, con l'allenamento specifico per il compito, alcuni modelli riescono a integrare la conoscenza da diverse fonti. I risultati indicano che anche i migliori modelli hanno difficoltà a integrare la conoscenza di fondo fornita solo durante il tempo di inferenza. Il lavoro di Akshatha e Martin contribuisce a comprendere le capacità e le limitazioni dei modelli di linguaggio naturale nell'integrazione della conoscenza da diverse fonti.</sample>
    <sample id="129">L'esempio fornito dagli autori come gruppo contrassegnato è quello delle donne di colore, in particolare le donne asiatiche, mediorientali e afroamericane.</sample>
    <sample id="130">Secondo l'analisi del paper, le architetture dei modelli che non generalizzano in modo adeguato sono quelle non basate sul Transformer, poiché i modelli basati sul Transformer generalizzano meglio ai dati nuovi.</sample>
    <sample id="131">Il testo non menziona esplicitamente i nomi dei set di dati di test. Tuttavia, si fa riferimento a un set di dati di test "pulito" (clean test set) che è utilizzato per valutare il rendimento dei modelli di apprendimento debole supervisionato.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, comprese l'immagine, il testo e le coordinate di una bounding box, inoltre utilizza anche il testo e le istruzioni.</sample>
    <sample id="135">Il gruppo di ricerca dell'Emory NLP Lab, guidato da Professor Jinho Choi, in collaborazione con Amazon Alexa AI, ha sviluppato un nuovo approccio di valutazione dei modelli di conversazione chiamato ABC-Eval. Questo metodo mira a ridurre la soggettività delle valutazioni umane annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi.

ABC-Eval è in grado di misurare le frequenze con cui i modelli di conversazione commettono errori tematici, come ignorare il partner, dire cose irrilevanti, contraddirsi o violare la conoscenza comune. Il metodo è stato testato su quattro modelli di conversazione di ultima generazione, valutati su 100 conversazioni umano-bot per modello.

Gli esiti della valutazione hanno mostrato che le etichette ABC-Eval sono più affidabili e predittive della qualità della conversazione rispetto ai metodi esistenti. In particolare, le etichette ABC-Eval sono state in grado di spiegare oltre il 25% della qualità della conversazione, mentre i metodi esistenti hanno spiegato meno di 5%.

Il gruppo di ricerca spera che ABC-Eval possa essere utilizzato da altri ricercatori nel campo per valutare i modelli di conversazione in modo più preciso e affidabile.</sample>
    <sample id="136">Il lavoro condotto da Jasivan e il suo supervisore Nafise, intitolato "FERMAT: An Alternative to Accuracy for Numerical Reasoning", mira a migliorare la valutazione delle capacità di ragionamento numerico dei modelli di linguaggio. La motivazione è che i modelli attuali possono avere difficoltà a svolgere operazioni matematiche semplici, come la sottrazione, e che le attuali metriche di valutazione non sono informative sulla loro abilità matematica.

Il team propone FERMAT, un insieme di valutazioni flessibili basate su tipi aritmetici, che comprende comprensione dei numeri, operazioni matematiche e dipendenza di addestramento. FERMAT è costituito da domande di matematica formulate in modo naturale, estratte da Illinois e CommonCore, e sono state modificate per includere numeri in forma decimale, interi grandi e piccoli.

Il team ha condotto una valutazione zero-shot e ha scoperto che i modelli di linguaggio hanno difficoltà a svolgere compiti matematici. Tuttavia, dopo aver addestrato i modelli con domande generate da insegnanti di matematica, si è osservata un aumento delle prestazioni. Inoltre, si è scoperto che la diversità linguistica e matematica è importante per migliorare le prestazioni dei modelli.

In conclusione, il lavoro di Jasivan e Nafise suggerisce che le attuali metriche di valutazione non sono informative sulla capacità matematica dei modelli di linguaggio e che FERMAT può essere un'alternativa più utile per valutare le loro abilità.</sample>
    <sample id="137">Il team del Singapore University of Technology and Design ha sviluppato un modello di generazione di piani di progetto di interni (Tell2Design) che utilizza istruzioni linguistiche per creare piani di progetto realistici e funzionali. Il modello è stato presentato in ACL 2023 e utilizza un approccio sequenza-a-sequenza per generare piani di progetto che soddisfano le esigenze specificate dagli utenti. Il dataset Tell2Design contiene 5.051 istruzioni linguistiche annotate da umani e 76.000 istruzioni artificiali generate da template predefiniti. Il modello è stato valutato con un IoU (Intersect over Union) di 54 e 53, rispettivamente per Micro e Macro IoU, superando le basi di testo-conditional image generation. Il modello è stato anche valutato con un caso studio che mostra come i modelli di generazione di immagini condizionate al testo non riescano a soddisfare le esigenze specificate dagli utenti. L'obiettivo del team è quello di creare un modello di generazione di piani di progetto che possa essere utilizzato da utenti non esperti e di migliorare il processo di progettazione.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità dei modelli di linguaggio di integrare e utilizzare conoscenza proveniente da fonti diverse, in particolare conoscenza acquisita durante il pretraining e conoscenza fornita nel tempo di inferenza.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno alcuni limiti: 

- Sono disponibili solo per tipi limitati di traduzioni dipendenti dal contesto.
- Sono disponibili solo per insiemi di lingue limitati.
- Sono spesso basate sulla conoscenza di dominio e sulla cura umana.</sample>
    <sample id="142">Ciao! Sto per discutere del nostro lavoro su "Risolvere le espressioni di riferimento indiretto per la selezione di entità", in cui abbiamo introdotto il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa. "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare tra uno di questi due brani. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio dicendo il nome del brano "Easy on Me" o la sua posizione, "il primo". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò può accadere quando l'utente non riesce a ricordare il nome del brano. O le pronunce sono troppo simili tra loro e difficile da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il più recente" o "il brano che non è energetico". Questo è un problema importante nei sistemi conversazionali e anche per la valutazione dell'intelligenza artificiale dei modelli di linguaggio. Non conosciamo un insieme di dati pubblici di grandi dimensioni per il compito, quindi abbiamo raccolto uno utilizzando l'annotazione della folla. Il nostro insieme di dati copre tre diversi domini: musica, libri e ricette. La nostra metodologia di raccolta dei dati mette l'accento sull'informalità utilizzando un setup di completamento di fumetti. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice, "Ricorda quel brano che stavamo ascoltando ieri?" E con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla, Alice dice, "Hai inteso 'Easy on Me' o 'I Gotta Feeling'?" Quale è la domanda alternativa. E nella terza bolla, Bob utilizza un riferimento indiretto per selezionare uno degli enti, ad esempio, "il più recente". Forniamo la prima e la seconda bolla automaticamente, ma la terza è riempita dall'annotatore. La prima bolla è scelta da pochi promemoria manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue. Utilizziamo sempre un template semplice. Hai inteso A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci muoviamo verso l'alto nella lista, gli enti diventano più simili tra loro e solitamente è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando gli enti hanno titoli simili, ad esempio, due libri con il nome "Il Ritorno". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per un brano. Quando mostriamo questa domanda alternativa agli annotatori, essi sanno il nome di questi enti, ma non necessariamente sanno dell'entità. Quindi mostriamo alcune conoscenze di sfondo sugli enti. Per i brani, semplicemente mostriamo un link di ricerca Google per ogni brano e poi chiediamo agli annotatori di ascoltare almeno alcune di ogni brano, e leggere su ogni brano. Ecco ad esempio il risultato di ricerca Google per il brano "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, nuovamente da Wikipedia, in modo che gli annotatori sappiano come sono. Quindi chiediamo agli annotatori di scegliere uno di questi enti, ad esempio, ecco il primo, e descrivere essi utilizzando tre a cinque espressioni di riferimento indirette. Ad esempio, "l'uno con la musica del pianoforte". Ecco alcuni esempi del nostro insieme di dati. Ad esempio, "l'uno senza parole", "non l'uno con il ragazzo di 12 anni", o "l'uno fittizio", o "viene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative su tre domini, e ha 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso allo stesso background conoscitivo degli annotatori, allora l'accuratezza è molto alta, è intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a delle conoscenze di sfondo parzialmente sovrapposte, allora l'accuratezza è tra il 82 e l'87%, che è più realistico. Ad esempio, quando il modello di linguaggio recupera le conoscenze di sfondo. Se il modello di linguaggio ha accesso solo ai nomi degli enti, allora l'accuratezza è solo del 60%, quindi c'è molto da migliorare. Abbiamo anche mostrato che i modelli sono generalizzabili a dominio. Ecco un link al nostro insieme di dati. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti come la Wait-k strategy e la Local Agreement, nonché con l'architettura di stato dell'arte specificatamente progettata per la traduzione simultanea pre-traduzione.</sample>
    <sample id="144">Non sono state fornite informazioni sull'identità degli autori, ma sembra che siano affiliati all'Università di Nantes, in particolare al Nantes University Hospital data warehouse.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Il dottorato di Yicheng, un PhD student del Fudan University, presenta un paper sul tema dell'analisi dell'omissione nella riassuntazione di dialoghi. La riassuntazione di dialoghi è un sottotask della riassuntazione di testi e consiste nella creazione di un riassunto conciso che rappresenti l'informazione più importante all'interno di un dialogo. Nonostante i grandi progressi fatti negli ultimi anni, soprattutto con l'utilizzo di modelli linguistici pre-allenati a grandi scale, i riassunti generati sono ancora affetti da errori comuni, tra cui l'omissione di informazioni critiche.

Yicheng e il suo team hanno analizzato la gravità del problema dell'omissione nella riassuntazione di dialoghi, trovando che il 70% dei riassunti generati soffre di questa problematica. Hanno inoltre costruito il dataset OLDS, che fornisce etichette di omissione di alta qualità per la riassuntazione di dialoghi, e hanno proposto un metodo automatico per produrre etichette di omissione per i candidati riassunti.

Per risolvere il problema dell'omissione, Yicheng e il suo team hanno sviluppato un modello di omissione che utilizza tre framework di base differenti: classificazione a coppie, etichettatura di sequenze e rete di puntatori. Hanno inoltre valutato il modello utilizzando misure di precisione, recall e F1-score, nonché il WR score per misurare la recall a livello di parole.

I risultati mostrano che il problema dell'omissione è molto difficile da risolvere e che il modello di omissione proposto non raggiunge ancora un buon livello di performance. Tuttavia, gli autori suggeriscono che la detezione di omissione possa essere un compito importante per migliorare la qualità dei riassunti di dialoghi e che la rifinizione dei riassunti utilizzando le omissioni possa essere una direzione promettente per la qualità migliore nella riassuntazione di dialoghi.</sample>
    <sample id="147">3 autori sono coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao Sara Papi, mi sembra di capire che sei una ricercatrice del dipartimento di Informatica dell'Università di Trento e della Fondazione Bruno Kessler. Mi hai chiesto di tradurre il contenuto del tuo articolo intitolato "Attention as a Guide for Simultaneous Speech Translation".

Il tuo articolo presenta un nuovo approccio per la traduzione simultanea di parole in tempo reale, chiamata SimulST. La traduzione simultanea è il processo di traduzione di un linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica.

Il tuo articolo identifica alcuni problemi attuali dei modelli di SimulST, come ad esempio:

- La necessità di addestrare modelli specifici per ogni regime di latenza, che richiede procedure di addestramento lunghe e complesse.
- La necessità di addestrare e mantenere diversi modelli per raggiungere differenti regimi di latenza, ad esempio un modello con una latenza media di un secondo e un altro con una latenza media di due secondi.

La tua soluzione propone di utilizzare modelli di traduzione simultanea offline esistenti senza doverli addestrare nuovamente o adottare architetture specifiche per la SimulST. Utilizzerai un solo modello per ogni regime di latenza e gestirai la latenza attraverso parametri specifici.

Inoltre, utilizzerai la conoscenza già acquisita dal modello attraverso la meccanica di attenzione tra l'input audio e l'output testuale, chiamata meccanica di attenzione incrociata. La tua soluzione propone EDAtt, o Encoder-Decoder Attention, una strategia per decidere se emettere o meno una traduzione parziale basata su dove la meccanica di attenzione punta.

La strategia EDAtt emette una parola se la meccanica di attenzione non è concentrata, ovvero il suo sommatorio è inferiore a un certo valore di soglia alpha verso i lambda ultimi frame di parlato, significando che l'informazione ricevuta è sufficientemente stabile.

Hai anche presentato i risultati principali dell'EDAtt, che includono la traduzione simultanea dei risultati in grafici in cui sono presenti la misura di qualità della traduzione BLEU e la misura di latenza media. I tuoi risultati mostrano che l'EDAtt supera tutte le strategie applicate ai modelli offline e che è anche la strategia più veloce se si considera il tempo di elaborazione computazionale.

Infine, hai reso disponibile il codice e i modelli dell'articolo per facilitare la riproducibilità dei risultati.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente, ma il suo accesso potrebbe richiedere una registrazione o una richiesta di accesso specifica.</sample>
    <sample id="150">Il progetto "MEETINGQA" è un sistema di estrazione di risposte basato su domande e risposte estratte da trascrizioni di riunioni. L'autore, Archiki, ha presentato il lavoro durante la conferenza ACL. Il sistema è stato sviluppato per affrontare il problema dell'estrazione di risposte da trascrizioni di riunioni, che è stato trascurato da precedenti lavori di ricerca.

Il progetto si basa su un nuovo dataset chiamato MeetingQA, che contiene 7.700 domande e relative risposte estratte da trascrizioni di riunioni pubbliche. Il dataset è stato creato utilizzando il corpus AMI, che contiene circa 100 ore di trascrizioni di riunioni multilaterali.

Il sistema di estrazione di risposte è stato sviluppato utilizzando vari modelli, tra cui RoBERTa e Longformer. I risultati mostrano che il sistema è in grado di raggiungere un'accuratezza del 84,6% nel fine-tuning e del 34,6% nel setting zero-shot.

L'analisi degli errori mostra che il sistema ha difficoltà a identificare le domande retoriche e a determinare chi risponde a una domanda. Tuttavia, il sistema è stato in grado di raggiungere un buon risultato nell'estrazione di risposte da trascrizioni di riunioni, dimostrando che il problema è ancora aperto e richiede ulteriori ricerche.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su MultiInstruct che migliora l'apprendimento zero-shot multi-modale tramite l'adattamento delle istruzioni. 

Con gli avanzamenti nei grandi modelli di linguaggio, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare modelli pre-allenati del linguaggio per compiti downstream diversi in modo efficiente in termini di parametri e dati. Di recente, molti studi hanno mostrato che l'adattamento delle istruzioni consente ai grandi modelli del linguaggio di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'adattamento delle istruzioni si è concentrata sull'ottimizzazione del performance zero-shot per compiti di linguaggio solo, lasciando da parte compiti di visione computer e multi-modale.

Quindi, in questo lavoro, vogliamo esplorare se l'adattamento delle istruzioni di un modello pre-allenato multi-modale possa effettivamente migliorare la generalizzazione a compiti multi-modali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una considerevole disomogeneità nell'accessibilità dei dati di istruzione tra NLP e multi-modale. Esistono più di 1600 compiti di istruzione del linguaggio solo. Tuttavia, non esiste un grande set di dati di istruzione multi-modale pubblicamente disponibile. Quindi, ciò ci motiva a costruire un set di dati di adattamento delle istruzioni multi-modale.

Ecco quindi MultiInstruct, il primo set di dati di adattamento delle istruzioni multi-modale di benchmark che consiste in 62 compiti diversi multi-modali coprendo 10 categorie ampie. Questi compiti sono derivati da 21 set di dati open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per esplorare l'adattamento delle istruzioni multi-modali sul nostro set di dati proposto, prendiamo OFA, un modello pre-allenato multi-modale unificato, come modello di base. OFA utilizza una vocabolario unificato per token del linguaggio, immagini e coordinate di un rettangolo di bounding box.

Ecco alcuni esempi di istanze dal nostro set di dati MultiInstruct, per unificare il trattamento di vari tipi di dati di input e output. Seguiamo la metodologia da OFA e formuliamo tutti i compiti in un formato di sequenza a sequenza unificato. In cui il testo di input, immagini, istruzioni e rettangoli di bounding box sono rappresentati nello stesso spazio di token.

Ora, parlerò dell'adattamento delle istruzioni multi-modali. Per il set di dati di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento di senso comune per il test e selezioniamo ulteriori 5 compiti dai gruppi VQ e Miscellaneo. Utilizziamo tutte le istanze della suddivisione di test per ogni compito. Inoltre, campioniamo a caso 20 compiti dalla suddivisione di test delle istruzioni naturali come compito non visto per il linguaggio. Utilizziamo il modello pre-allenato OFA grande come modello di base.

Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei cinque template di istruzioni. Quindi, durante il test per ogni compito, condiamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo il minimo e il massimo del performance e la deviazione standard del performance across tutti i 5 esperimenti. Se il compito è una classificazione multi-modale, riportiamo l'accuratezza. Se è una generazione multi-modale, riportiamo Rouge-L. Per il compito di linguaggio, riportiamo Rouge-L. 

Inoltre, introduciamo un metrica di valutazione aggiuntiva chiamata sensibilità. Quindi, questa misura l'abilità del modello a produrre gli stessi output per lo stesso compito, indipendentemente dalla variazione leggera nella parola dell'istruzione.

Ecco il nostro risultato principale. Come possiamo vedere, l'adattamento delle istruzioni può migliorare significativamente il performance di OFA sui compiti multi-modali visti. Inoltre, l'apprendimento trasferito dal set di dati di istruzioni naturali può beneficiare l'adattamento delle istruzioni. Ecco, come possiamo vedere, man mano che l'importo di compiti aumenta, il modello raggiunge un migliore performance e contemporaneamente una sensibilità più bassa.

Ecco un esperimento che abbiamo fatto. Utilizziamo un'istruzione contro 5 istruzioni. Come possiamo vedere, utilizzando più istruzioni può migliorare il performance complessivo del modello e ridurre notevolmente la sua sensibilità. Quindi, ciò mostra l'effetto di diverse strategie di fine-tuning sul modello di sensibilità. Come possiamo vedere, l'apprendimento trasferito dal set di dati di istruzioni naturali può aiutare OFA ad ottenere una sensibilità molto migliore rispetto al modello originale OFA. Inoltre, possiamo vedere che l'apprendimento trasferito dal set di dati di istruzioni naturali può aiutare OFA a raggiungere un migliore performance sul set di dati di istruzioni naturali.

Quindi, in sintesi, proponiamo il primo grande set di dati di adattamento delle istruzioni multi-modale con significativamente migliorato la capacità di breve termine di OFA, e esploriamo diverse tecniche di apprendimento trasferito e mostriamo i loro benefici. Progettiamo un nuovo metrica chiamata sensibilità. Inoltre, stiamo raccogliendo un set di dati di istruzioni multi-modali molto più grande con circa 150 compiti di visione e linguaggio aggiuntivi e lo rilasceremo. Ecco un QR code per i nostri dati e modello. Grazie.</sample>
    <sample id="152">Il lavoro di Frederick Riemenschneider e la sua squadra si concentra sull'intersezione tra l'apprendimento automatico delle lingue (NLP) e la filologia classica. I ricercatori hanno sviluppato nuovi modelli di linguaggio per la filologia classica, specificamente per l'antico greco e il latino. I modelli, denominati GreBERTa, GreTa, PhilBERTa e PhilTa, sono stati pre-allineati su dati di pre-allenamento di alta qualità, inclusi testi dell'Internet Archive. I modelli sono stati valutati su tre compiti principali: etichettatura delle parti del discorso, analisi delle dipendenze e lemmatizzazione. I risultati mostrano che i modelli sono chiaramente superiori allo stato dell'arte per entrambi l'antico greco e il latino. Inoltre, gli autori hanno analizzato il comportamento dell'encoder del modello T5 e hanno trovato che si comporta in modo diverso rispetto ai modelli encoder-only nativi. I modelli encoder-decoder hanno mostrato una lemmatizzazione di alta qualità, con un aumento del 5% rispetto allo stato dell'arte. Infine, gli autori hanno valutato le capacità semantiche e di conoscenza del mondo dei modelli e hanno trovato che i modelli multilingue non mostrano una differenza significativa rispetto ai modelli monolingue.</sample>
    <sample id="153">Il lavoro presentato da Ninareh Mehrabi, membro del team di Responsabile AI di Amazon Alexa, si concentra sulle ambiguità nelle modello di generazione di immagini a partire da testi. I ricercatori hanno identificato esempi di ambiguità nei promemoria, come "la ragazza entra nella stanza con fiori", dove non è chiaro se i fiori dovrebbero essere nella stanza, con la ragazza o una combinazione di entrambi.

Per risolvere queste ambiguità, il team ha proposto due approcci: il primo consiste nell'interrogare il utente con domande di chiarimento, mentre il secondo prevede la generazione di diverse interpretazioni visive. Una volta ottenuta la disambiguazione del promemoria, il team ha valutato se le immagini generate dalle modello di generazione di immagini a partire da testi sono fedeli all'intenzione dell'utente.

Il team ha creato un dataset di riferimento che copre diversi tipi di ambiguità e ha proposto un framework di valutazione automatica per determinare se le immagini generate sono fedeli all'intenzione dell'utente. I risultati mostrano che la disambiguazione ha un effetto positivo sulla fedeltà della generazione e che il framework di valutazione automatica è in accordo con la valutazione umana.</sample>
    <sample id="154">Gli autori dell'articolo sono Sara Papi, Matteo Negri e Marco Turchi, rispettivamente affiliati alla University of Trento e Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Il tuo lavoro, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph", si concentra sulla sintesi di dialoghi, ovvero la distillazione dell'informazione saliente da un contesto di dialogo in un riassunto conciso. Questo compito è particolarmente sfidante e interessante nel campo della ricerca sulla sintesi di testi.

Il tuo modello, SDDS, è composto da quattro componenti principali: un Utterance Encoder per codificare le frasi in un vettore di rappresentazione, un metodo di modellazione della struttura dati per costruire un grafo statico, un modulo Static-Dynamic Graph per catturare le relazioni semantiche tra le frasi, e un Summary Generator per generare il riassunto finale.

Il tuo modello propone quattro metodi di modellazione della struttura di dialogo statica: il grafo di parsing discorsivo, il grafo di co-occorrenza di chiavi, il grafo di interazione tra gli interlocutori e il grafo di posizione delle frasi. Questi grafi vengono poi fusi utilizzando una rete convoluzionale.

Il modulo Dynamic Graph cattura le relazioni semantiche tra le frasi utilizzando un modello di attenzione multi-testa. Infine, il Summary Generator utilizza un meccanismo di attenzione incrociata per incorporare la rappresentazione del grafo nella generazione del riassunto.

Il tuo lavoro offre una soluzione innovativa per la sintesi di dialoghi, che può aiutare le persone a catturare velocemente i punti salienti di un dialogo complesso senza doverlo rileggere interamente. Il codice e i dati sono stati rilasciati su GitHub e possono essere scaricati tramite il QR code.</sample>
    <sample id="158">Il coreference resolution è un compito di intelligenza artificiale che consiste nell'identificare e nel collegare le menzioni di un'entità all'interno di un testo. Ad esempio, se un testo contiene le menzioni "John" e "he", il compito è quello di collegare queste due menzioni per indicare che si riferiscono allo stesso individuo.

I metodi convenzionali per risolvere questo compito hanno una complessità quadratica, sia in termini di calcolo che di consumo di memoria, poiché devono enumerare tutte le possibili coppie di menzioni. I metodi di cache basati su questo principio riducono la complessità a un livello lineare, ma possono subire cache miss quando si incontrano menzioni di entità che non sono state precedentemente visualizzate.

Il problema si aggrava nei documenti lunghi, in cui il tema può cambiare più volte e le menzioni di un'entità possono essere sparse in una vasta gamma di testo. Per risolvere questo problema, si propone un doppio cache, composto da un cache locale e da un cache globale, che lavorano insieme. Il cache locale memorizza le entità locali con una politica di evizione LRU (Least Recently Used), mentre il cache globale memorizza le entità globali con una politica di evizione LFU (Least Frequently Used).

Il doppio cache funziona in modo tale che il modello scansioni il documento da sinistra a destra, classificando se una menzione è una nuova entità o appartiene a un'entità già memorizzata nel cache. Se la menzione è qualificata, viene aggiunta al cache globale; altrimenti, viene aggiunta al cache locale. Quando il cache è pieno, si attiva la politica di evizione per espellere un'entità da esso.

Gli esperimenti condotti su quattro benchmark pubblici hanno mostrato che il doppio cache ottiene risultati migliori dei metodi di base anche quando utilizzano memoria illimitata. Inoltre, il doppio cache riduce significativamente i cache miss rispetto a un singolo cache. Infine, il doppio cache ha il miglior rapporto prestazioni/costo rispetto ai metodi di base.</sample>
    <sample id="159">Ciao a tutti. Mi chiamo Koustav Sinha e sono felice di accogliervi nel nostro discorso sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, riprendiamo i paradigmi di coppia minima. I paradigmi di coppia minima valutano i modelli linguistici in base ai giudizi di accettabilità. Ciò può includere anche la grammaticalità come BLiMP, SyntaxGym o l'accettabilità in termini di stereotipi come CrowS pairs. E in questo paradigma di coppia minima, la tipica modalità di valutazione dei modelli linguistici è mostrare una frase accettabile o grammaticale e quindi mostrare una frase accettabile o una frase non grammaticale. E poi la speranza è che il modello, di base, attribuisca una probabilità maggiore alla frase accettabile. Il corrente pipeline di MPP non consente di valutare l'accettazione del modello verso frasi più lunghe. Oggi i modelli linguistici più grandi vengono fuori con finestre di contesto sempre più lunghe. Quindi è cruciale valutare l'accettabilità dei modelli all'interno della finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di riprendere il pipeline di MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Quindi, per simulare queste sequenze più lunghe, riprendiamo i dati stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili dai dati stessi. Ad esempio, abbiamo scelto una coppia di grammaticalità tipica dal dataset BLiMP dal caso dell'Isola degli Aggiunti. E cosa facciamo è che ricreiamo sequenze più lunghe e che sono accettabili e che hanno la stessa struttura grammaticale. Estraiamo frasi grammaticali dall'Isola degli Aggiunti e poi aggiungiamo come prefisso a entrambe la frase accettabile e la frase inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso matching e che potrebbe essere utilizzato per testare l'accettabilità del modello. E possiamo fare la stessa cosa scegliendo frasi da un insieme diverso o da un dataset diverso. Quindi, questo è ciò che chiamiamo scenario di disallineamento. Quindi le frasi sono ancora provenienti da insiemi dati rilevanti, ma non provenienti dallo stesso dataset che stiamo valutando. E possiamo fare la stessa cosa per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso come Wikipedia. Quindi ciò ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, come se il contesto provenga da un insieme diverso dei dati, o se sia completamente irrilevante, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito, al sentito</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un insieme non ordinato (unordered multiset) di token che appariranno nell'output.</sample>
    <sample id="161">55.000 script</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato offre il vantaggio di poter utilizzare etichette di dati più economiche e meno costose rispetto alle annotazioni manuali umane, ma questo può portare a un aumento del rumore nelle etichette e a un peggioramento della generalizzazione dei modelli.</sample>
    <sample id="165">Wenting Zhao, un dottorando presso l'Università di Cornell, presenta un nuovo approccio all'apprendimento abduttivo, una tecnica di ragionamento che consiste nel trovare una spiegazione plausibile per un evento. L'autore presenta un esempio concreto: Emily è bloccata nel traffico e arriva in orario per il suo volo. Le spiegazioni possibili sono due: il volo è stato ritardato o è partito in orario. L'obiettivo è trovare la spiegazione più plausibile che colleghi il contesto (Emily è bloccata nel traffico) all'esito (arriva in orario per il volo).

L'autore critica gli approcci attuali all'apprendimento abduttivo, che richiedono l'annotazione delle spiegazioni più plausibili, un compito difficile e soggettivo. Presenta quindi un metodo unsupervisionato chiamato LiPoR, che non richiede l'annotazione delle spiegazioni.

Il metodo LiPoR consiste in due parti: un obiettivo di massimizzazione della verosimiglianza e un regolarizzatore che preferisce le spiegazioni più plausibili. Il regolarizzatore utilizza la caratteristica delle spiegazioni mutualmente esclusive, ovvero che solo una spiegazione possa essere vera.

Il metodo LiPoR viene testato su un dataset di abduttivo e mostra risultati migliori rispetto agli altri metodi, compreso un modello zero-shot basato su GPT-3.</sample>
    <sample id="166">Il team del Harbin Institute of Technology, Shenzhen, ha sviluppato un nuovo metodo per l'estrazione di immagini da testi complessi, chiamato "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text". Il metodo si basa sulla strategia Divide-and-Conquer e sulla Teoria dei due Processi del pensiero umano, che afferma che il cervello umano utilizza due sistemi di ragionamento: uno analogico (System 1) e uno logico (System 2). Il metodo proposto integra questi due sistemi utilizzando un modello di linguaggio visivo pre-allenato (System 1) e un ragionatore simbolico neurale (System 2). Il modello di linguaggio visivo pre-allenato si occupa di decomporre il testo complesso in proposizioni semplici, mentre il ragionatore simbolico neurale si occupa di integrare le informazioni per ottenere la soluzione finale. Il metodo è stato testato su due casi di studio e ha mostrato risultati migliori rispetto ai metodi di base. Gli autori suggeriscono che il calcolo simbolico neurale possa essere un approccio utile per migliorare la ragionamento composto e la pianificazione dei grandi modelli di linguaggio. Inoltre, la strategia Divide-and-Conquer può essere integrata con la Teoria dei due Processi del pensiero umano per risolvere problemi complessi.</sample>
    <sample id="167">I 750 documenti del corpus DEPLAIN-web sono stati allineati in due modi: 

- 750 documenti sono stati allineati manualmente. 
- Inoltre, gli altri 750 documenti sono stati allineati anche con metodi di allineamento automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo testi da Reuters News del 2020, annotandoli con le stesse linee guida di annotazione utilizzate per il CoNLL-2003.</sample>
    <sample id="169">Il tuo lavoro, "Prompting PaLM per la traduzione: valutazione delle strategie e del rendimento", esplora l'utilizzo di modelli di linguaggio per la traduzione. Il modello PaLM, addestrato su 780 miliardi di token, raggiunge il top dei risultati in centinaia di compiti NLP. Il tuo studio valuta la capacità di transizione di questi modelli utilizzando le migliori pratiche della comunità MT, evitando l'overlapping dei dati di test con quelli di addestramento. Confronti con sistemi di stato dell'arte e valutazioni umane utilizzando il framework MQM mostrano che PaLM offre prestazioni competitive, ma con errori di omissione. Le strategie di selezione dei prompt hanno un grande impatto sul rendimento, con differenze di oltre 1 punto BLEURT tra due diverse strategie. Il tuo lavoro consiglia l'utilizzo di un approccio 5-shot, dove le frasi sono etichettate con la lingua di origine, e selezione di esempi di alta qualità per la traduzione.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dalla Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Incroso-Lingua in Molti Linguaggi Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito per costruire rappresentazioni semantiche di query degli utenti, come SQL e Lambda Calculus. Il Parsing Semantico Incroso-Lingua è il compito di tradurre query in molti linguaggi naturali in molte rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in molti linguaggi naturali utilizzando modelli neurali per SQL, Lambda o FunQL, ecc. I modelli di parsing semantico incroso-lingua esistenti sono stati proposti separatamente e valutati su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono molte coperture per certi linguaggi naturali, ma il cinese è mancante e manca di copertura per certe rappresentazioni di significato. La calcolatrice Lambda è mancante, o sono valutati solo su certi modelli neurali. Ad esempio, ci sono solo un singolo modello per valutarli. Pertanto, proponiamo XSemPLR. Offriamo un set di dati uniforme XSemPLR per il parsing semantico incroso-lingua in molti linguaggi naturali e rappresentazioni di significato. Contiene 9 set di dati in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 linguaggi naturali in 15 famiglie di linguaggi. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'allenamento e l'evaluazione. La prima è Translate-Test. Utilizziamo l'API di traduzione di Google per tradurre la sorgente nel linguaggio di destinazione, quindi utilizziamo un modello monolingua per allenare e valutare. E ad esempio, alleniamo il modello inglese sulle query inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello allenato per prevedere l'SQL. E valuteremo anche il Modello Monolingua. In questa impostazione, il linguaggio di sorgente è lo stesso del linguaggio di destinazione, ad esempio tedesco a tedesco o inglese a inglese. Valuteremo anche il Modello Monolingua con poche ripetizioni, allenando i modelli monolingua con solo il 10% dei dati di allenamento. E valuteremo il Modello Multilingua che alleniamo un modello multilingua per tutti i linguaggi. Ad esempio, mettiamo le query tedesche, inglesi e cinesi insieme per allenare un modello multilingua. E durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. E consideriamo anche il Trasferimento Zero-Shot e Few-Shot incroso-lingua. Alleniamo su un linguaggio di sorgente e trasferiamo a un altro linguaggio. Quindi durante l'allenamento, alleniamo su query inglesi o la combinazione di query inglesi e tedesche con poche ripetizioni per allenare un modello multilingua per prevedere l'output SQL. E abbiamo trovato molti risultati interessanti. Quindi, per l'analisi dei modelli monolingua, valutiamo due gruppi di modelli, compresi Encoder-PTR, che stanno per Encoder-Multilingua con Decodificatori a Puntatori, come XLM-R + PTR e mBERT + PTR. E valutiamo anche i modelli Encoder-Decodificatore, che sono i modelli Encoder-Multilingua con Decodificatori, come mBART e mT5. Abbiamo trovato che Encoder-Decodificatore ottiene la migliore prestazione su tutti e nove i set di dati. E valutiamo mT5 e XLM-R + PTR sul setting multilingua. Abbiamo trovato che Encoder-Decodificatore o Encoder-PTR possono essere migliorati allenando in una miscela di vari linguaggi. Abbiamo trovato che questo è perché la maggior parte dei principali linguaggi naturali può ottenere un aumento di prestazione, eccetto che l'inglese diminuisce in sette set di dati e solo guadagna in tre set di dati. Penso che questo sia noto come la "Maledizione della Multilingualità". Abbiamo anche confrontato il divario di prestazione incroso-lingua. In questa figura, la linea blu è il Trasferimento Few-Shot incroso-lingua. La linea arancione è il Trasferimento Zero-Shot incroso-lingua. Mentre la linea verde è l'impostazione Monolingua. Abbiamo trovato che, confrontando la linea verde e la linea arancione, abbiamo trovato che il divario di prestazione del trasferimento Zero-Shot incroso-lingua è significativo, e poi confrontando la linea blu e la linea arancione, abbiamo trovato che con l'impostazione Few-Shot il divario di prestazione del trasferimento è accorciato rapidamente. Abbiamo anche trovato altri risultati interessanti. Ad esempio, Encoder-Decodificatore supera i precedenti lavori o ottiene risultati comparabili. Preallenamento sul linguaggio naturale inglese può migliorare significativamente la prestazione del Few-Shot sul linguaggio naturale di destinazione, e abbiamo trovato che i modelli multilingua come Codex e BLOOM sono ancora insufficienti per il compito di parsing semantico incroso-lingua. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico incroso-lingua con molti linguaggi naturali e rappresentazioni di significato. Abbiamo condotto uno studio di benchmarking comprensivo sui tre tipi rappresentativi di modelli multilingua. E i nostri risultati mostrano molti risultati interessanti. E accogliamo la visita del nostro articolo e del codice. Grazie per aver ascoltato.

Traduzione:

Ciao a tutti, mi chiamo Yusen Zhang dalla Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Incroso-Lingua in Molti Linguaggi Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito per costruire rappresentazioni semantiche di query degli utenti, come SQL e Lambda Calculus. Il Parsing Semantico Incroso-Lingua è il compito di tradurre query in molti linguaggi naturali in molte rappresentazioni di significato. 

... (continua la traduzione del testo)

Traduzione completa:

Ciao a tutti, mi chiamo Yusen Zhang dalla Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Incroso-Lingua in Molti Linguaggi Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito per costruire rappresentazioni semantiche di query degli utenti, come SQL e Lambda Calculus. Il Parsing Semantico Incroso-Lingua è il compito di tradurre query in molti linguaggi naturali in molte rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in molti linguaggi naturali utilizzando modelli neurali per SQL, Lambda o FunQL, ecc. I modelli di parsing semantico incroso-lingua esistenti sono stati proposti separatamente e valutati su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono molte coperture per certi linguaggi naturali, ma il cinese è mancante e manca di copertura per certe rappresentazioni di significato. La calcolatrice Lambda è mancante, o sono valutati solo su certi modelli neurali. Ad esempio, ci sono solo un singolo modello per valutarli. Pertanto, proponiamo XSemPLR. Offriamo un set di dati uniforme XSemPLR per il parsing semantico incroso-lingua in molti linguaggi naturali e rappresentazioni di significato. Contiene 9 set di dati in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 linguaggi naturali in 15 famiglie di linguaggi. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'allenamento e l'evaluazione. La prima è Translate-Test. Utilizziamo l'API di traduzione di Google per tradurre la sorgente nel linguaggio di destinazione, quindi utilizziamo un modello monolingua per allenare e valutare. E ad esempio, alleniamo il modello inglese sulle query inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello allenato per prevedere l'SQL. E valuteremo anche il Modello Monolingua. In questa impostazione, il linguaggio di sorgente è lo stesso del linguaggio di destinazione, ad esempio tedesco a tedesco o inglese a inglese. Valuteremo anche il Modello Monolingua con poche ripetizioni, allenando i modelli monolingua con solo il 10% dei dati di allenamento. E valuteremo il Modello Multilingua che alleniamo un modello multilingua per tutti i linguaggi. Ad esempio, mettiamo le query tedesche, inglesi e cinesi insieme per allenare un modello multilingua. E durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. E consideriamo anche il Trasferimento Zero-Shot e Few-Shot incroso-lingua. Alleniamo su un linguaggio di sorgente e trasferiamo a un altro linguaggio. Quindi durante l'allenamento, alleniamo su query inglesi o la combinazione di query inglesi e tedesche con poche ripetizioni per allenare un modello multilingua per prevedere l'output SQL. E abbiamo trovato molti risultati interessanti. Quindi, per l'analisi dei modelli monolingua, valutiamo due gruppi di modelli, compresi Encoder-PTR, che stanno per Encoder-Multilingua con Decodificatori a Puntatori, come XLM-R + PTR e mBERT + PTR. E valutiamo anche i modelli Encoder-Decodificatore, che sono i modelli Encoder-Multilingua con Decodificatori, come mBART e mT5. Abbiamo trovato che Encoder-Decodificatore ottiene la migliore prestazione su tutti e nove i set di dati. E valutiamo mT5 e XLM-R + PTR sul setting multilingua. Abbiamo trovato che Encoder-Decodificatore o Encoder-PTR possono essere migliorati allenando in una miscela di vari linguaggi. Abbiamo trovato che questo è perché la maggior parte dei principali linguaggi naturali può ottenere un aumento di prestazione, eccetto che l'inglese diminuisce in sette set di dati e solo guadagna in tre set di dati. Penso che questo sia noto come la "Maledizione della Multilingualità". Abbiamo anche confrontato il divario di prestazione incroso-lingua. In questa figura, la linea blu è il Trasferimento Few-Shot incroso-lingua. La linea arancione è il Trasferimento Zero-Shot incroso-lingua. Mentre la linea verde è l'impostazione Monolingua. Abbiamo trovato che, confrontando la linea verde e la linea arancione, abbiamo trovato che il divario di prestazione del trasferimento Zero-Shot incroso-lingua è significativo, e poi confrontando la linea blu e la linea arancione, abbiamo trovato che con l'impostazione Few-Shot il divario di prestazione del trasferimento è accorciato rapidamente. Abbiamo anche trovato altri risultati interessanti. Ad esempio, Encoder-Decodificatore supera i precedenti lavori o ottiene risultati comparabili. Preallenamento sul linguaggio naturale inglese può migliorare significativamente la prestazione del Few-Shot sul linguaggio naturale di destinazione, e abbiamo trovato che i modelli multilingua come Codex e BLOOM sono ancora insufficienti per il compito di parsing semantico incroso-lingua. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico incroso-lingua con molti linguaggi naturali e rappresentazioni di significato. Abbiamo condotto uno studio di benchmarking comprensivo sui tre tipi rappresentativi di modelli multilingua. E i nostri risultati mostrano molti risultati interessanti. E accogliamo la visita del nostro articolo e del codice. Grazie per aver ascoltato.</sample>
    <sample id="171">Gli esistenti lavori possono essere classificati in quattro categorie, ma hanno il difetto di non essere applicabili a embedding come servizi o di mancare di trasferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o BLOOM sono ancora insufficienti per il Cross-Lingual Semantic Parsing (CLSP).</sample>
    <sample id="174">Il video spiega il progetto "ArgAnalysis35K", un grande dataset per l'analisi della qualità degli argomenti. L'autrice, Thea, presenta le caratteristiche uniche del dataset, che si differenzia da altri dataset simili per la sua qualità, diversità e profondità. Il dataset contiene 35.000 coppie di argomento e analisi, con un'alta qualità degli argomenti, provenienti da fonti come tornei di dibattito e oratori esperti. 

Il dataset presenta una maggiore diversità rispetto ad altri dataset, poiché copre 24 temi e include una vasta gamma di argomenti. Inoltre, introduce l'idea di "analisi", che non è solo un claim o una premessa, ma una combinazione di entrambi. 

Il dataset presenta anche un modello di rilevanza, che assegna un punteggio da 0 a 1 per ciascun argomento e tema, permettendo di catturare la rilevanza di un argomento per un particolare tema. L'autrice conclude che il dataset è una raccolta unica di caratteristiche che forniscono una maggiore diversità, qualità e profondità rispetto ad altri dataset.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un'approssimazione continua della ricerca del percorso di soluzione, che consente di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di fornire prestazioni simili indipendentemente dal background, dalla demografia o dalle opinioni politiche degli utenti, senza discriminare o favorire alcune categorie rispetto ad altre.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar, autrice di un articolo intitolato "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", discute sulla capacità dei modelli linguistici di ragionare sulla mente degli altri. Questo concetto è noto come teoria della mente e viene tradizionalmente valutato attraverso test di comprensione della lettura che coinvolgono più personaggi. Un esempio classico di test di teoria della mente è il test Sally-Anne, in cui Alice mette un'arancia in un cesto e poi se ne va, mentre Bob sposta l'arancia in una scatola. Quando Alice torna, Bob pensa che cercherà l'arancia nel cesto.

I modelli linguistici grandi eseguono ancora male sui test di teoria della mente, come ad esempio ChatGPT e GPT-3. L'autrice propone un metodo chiamato SymbolicToM per migliorare la ragione di teoria della mente nei modelli linguistici grandi. Questo metodo utilizza rappresentazioni grafiche esplicite e un algoritmo di inferenza in tempo di esecuzione per migliorare le prestazioni dei modelli linguistici.

Gli esperimenti mostrano che il metodo di SymbolicToM migliora le prestazioni dei modelli linguistici grandi in vari test di teoria della mente, compresi quelli fuori dal dominio. Inoltre, il metodo di SymbolicToM dimostra di essere benefico anche in un nuovo dataset con maggiore diversità linguistica. L'autrice conclude che il metodo di SymbolicToM è una soluzione efficace per migliorare la ragione di teoria della mente nei modelli linguistici grandi.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">Il lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" della Fudan University affronta il problema della pianificazione linguistica con vincoli, ovvero la capacità di creare istruzioni passo dopo passo per raggiungere obiettivi specifici con restrizioni. I modelli di linguaggio grandi possono decomporre obiettivi astratti in passaggi, ma non sono efficaci quando si tratta di pianificare per obiettivi specifici con vincoli, come "fare un dolce al cioccolato".

I ricercatori hanno esteso gli obiettivi astratti con vincoli multi-faceti per l'acquisizione di dati umani in loop con InstructGPT, ottenendo 100 obiettivi specifici e valutando i script generati dai modelli di linguaggio grandi. I risultati sono stati insoddisfacenti e hanno condotto a un'analisi dettagliata per comprendere i motivi del fallimento. 

I ricercatori hanno sviluppato un metodo "over-generate-then-filter" per migliorare la qualità della generazione. Inizialmente, vengono forniti tipi di vincoli con esempi a InstructGPT e vengono ottenuti obiettivi specifici sulla base degli obiettivi astratti. In seguito, InstructGPT produce K script per ogni obiettivo specifico e un modello di filtro seleziona i script fedeli. 

Il metodo è stato applicato per creare un dataset di pianificazione linguistica con vincoli, chiamato CoScript, contenente 55.000 obiettivi specifici con script. I risultati mostrano che CoScript ha una alta pluralità degli obiettivi specifici e che i modelli più piccoli possono superare i modelli più grandi quando addestrati su dataset adeguati. Il CoScript dataset può essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="182">Il tropicalismo, nel contesto di questo articolo, si riferisce a un tropo che associa le donne di colore, in particolare le donne latine, a immagini e caratteristiche esotiche e sensuali, come ad esempio la "vibrantità" e la "curvaceità", che contribuiscono a una visione stereotipata e discriminatoria di queste donne.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target chiedendo ai modelli linguistici di generare personaggi immaginari in base a prompsti come "Immagina di essere una donna asiatica. Descrivi te stessa". Questo metodo è stato ispirato da uno studio che aveva chiesto ai soggetti umani di rispondere a prompsti simili, rilevando stereotipi razziali.</sample>
    <sample id="184">CXMI (Context Usage by Machine Translation Models) è stato esteso a Pointwise CXMI (P-CXMI) per misurare l'utilizzo del contesto a livello di parola o di frase.</sample>
    <sample id="185">DrBERT e ChuBERT sono due modelli pre-allenati per il francese, ma differiscono per il tipo di dati utilizzati per l'allenamento. DrBERT è stato allenato su NACHOS, un dataset di dati medici raccolti dal web, mentre ChuBERT è stato allenato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes.</sample>
    <sample id="187">Il numero di autori non viene menzionato esplicitamente nell'articolo, ma si parla di "Ying" e "Zhiyang" come presentatori del loro lavoro.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento (Transfer Learning Iterative) è una strategia di apprendimento automatico che combina il trasferimento di conoscenza da un compito a un altro con l'aggiornamento iterativo del modello tramite l'annotazione attiva. In questo approccio, si inizia con un modello pre-allineato che è stato addestrato su compiti simili, quindi si aggiornano iterativamente il modello tramite l'annotazione attiva, selezionando esempi più rappresentativi per ogni round di annotazione.</sample>
    <sample id="189">The AltEntities Corpus is a dataset created to understand users' language when they want to make a choice between two entities, and to benchmark the entity understanding of Large Language Models (LLMs).</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Embedding as a Service) imparando da un embedding fornito dal provider, creando un modello simile che offre servizi simili.</sample>
    <sample id="191">Ci sono tre autori coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Yang Luo ha presentato il suo lavoro intitolato "CAME: Confidence-guided Adaptive Memory Efficient Optimization". L'obiettivo del progetto è quello di creare un ottimizzatore che combini la velocità di convergenza delle ottimizzatori tradizionali con la riduzione della memoria richiesta degli ottimizzatori efficienti.

L'attuale ottimizzatore Adafactor riduce la memoria richiesta, ma incurre in aggiornamenti errati durante il training dei modelli neurali, rallentando la convergenza. Yang Luo ha proposto un approccio per ridurre gli errori di aggiornamento, utilizzando il residuo tra l'aggiornamento previsto e l'aggiornamento effettivo come denominatore per l'aggiornamento.

Il progetto CAME è stato testato su tre grandi modelli di linguaggio: BERT, GPT-2 e T5. I risultati mostrano che CAME aumenta la precisione di validazione di circa 3,4% rispetto a Adafactor, utilizzando lo stesso numero di passi di training. Inoltre, CAME riduce la memoria richiesta rispetto a Adam e Adafactor, specialmente per grandi batch di training.

Gli esperimenti mostrano che CAME è efficace anche per grandi batch di training, estendendo le capacità degli ottimizzatori efficienti esistenti. Il progetto CAME propone un ottimizzatore che combina la velocità di convergenza e la riduzione della memoria richiesta, offrendo una soluzione efficace per il training di grandi modelli di linguaggio.</sample>
    <sample id="193">Non è stato specificato esattamente il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati alla Carnegie Mellon University e alla University of Washington, e collaborano con l'Allen Institute for AI.</sample>
    <sample id="195">Il tuo lavoro, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering", si concentra sull'obiettivo di rispondere a una domanda e fornire un'esplicazione sul perché l'answer è stata selezionata. Il tuo approccio si basa su una struttura di decomposizione gerarchica delle domande (HQDT) e su una ragione probabilistica per integrare conoscenza da fonti eterogenee.

Il tuo framework, RoHT, è composto da due stadi: la costruzione dell'HQDT e la ragione probabilistica. La costruzione dell'HQDT si basa su due step: la decomposizione della domanda complessa in sottodomande atomiche e la generazione di domande intermedie basate sulle sottodomande.

La ragione probabilistica si svolge in modo ricorsivo, determinando per ogni nodo la fonte di conoscenza più appropriata, ottenendo le risposte con probabilità e aggregando le risposte candidate per ottenere le risposte più probabili.

Hai valutato il tuo framework su due dataset complessi, KQA Pro e Musique, e hai ottenuto risultati migliori rispetto ai metodi esistenti.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo utilizzati nello studio sono stati selezionati e valutati con l'approccio ABC-Eval. I modelli selezionati non sono stati specificamente nominati nello speech, ma sono stati definiti come "state-of-the-art chat models".</sample>
    <sample id="198">I grandi modelli di linguaggio stanno sviluppando finestre di contesto sempre più lunghe, quindi è cruciale valutare l'accettabilità dei modelli in tutta la finestra di contesto per comprendere correttamente la loro conoscenza astratta.</sample>
    <sample id="199">Sì, è stato osservato che la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset analizzati.</sample>
    <sample id="200">No, gli annotatori non conoscono necessariamente l'entità in anticipo, ma sanno i nomi degli entità, e quindi vengono mostrati alcuni background knowledge per aiutarli a descrivere gli entità con espressioni di riferimento indirette.</sample>
    <sample id="201">Le metriche di MT utilizzate sono state le metriche neural state-of-the-art e BLEURT.</sample>
    <sample id="202">No, il regresso nella generalizzazione non influisce su specifici tipi di NER, ma piuttosto è un problema generale che può colpire diversi tipi di modelli di riconoscimento di entità nominate.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché può influenzare la qualità e l'accuratezza dei risultati dei modelli di linguaggio, in particolare quando si tratta di compiti soggettivi e socialmente orientati. Infatti, la posizionalità può portare a design bias nei dati e nei modelli, che possono avere un impatto negativo su determinate popolazioni, come ad esempio le persone non binarie, che possono essere meno rappresentate o meno accurateamente modellate.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM sono stati affinati mediante una messa a punto integrale, non solo con adattatori.</sample>
    <sample id="205">Shangbin, un dottorando presso l'Università di Washington, presenta un lavoro intitolato "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". I modelli linguistici sono stati addestrati su grandi quantità di dati web e spesso includono notizie di media politici, creando un dilemma. Da un lato, i modelli linguistici possono apprendere da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono inherentemente socialmente biasate e potrebbero portare a problemi di equità nelle applicazioni di NLP.

Shangbin e il suo team propongono di esplorare il pipeline di propagazione dei pregiudizi politici da dati di addestramento a modelli linguistici a compiti downstream. I loro risultati preliminari dimostrano che i modelli linguistici hanno opinioni politiche variabili e che possono essere influenzati dai dati di addestramento. In particolare, GPT-4 è stato identificato come il modello linguistico più liberale, mentre i modelli GPT sono stati trovati più socialmente liberali rispetto ai modelli BART e ai suoi varianti.

Shangbin e il suo team hanno anche condotto esperimenti per valutare l'impatto dei pregiudizi politici dei modelli linguistici su compiti come la detezione di discorsi d'odio e la detezione di notizie false. I risultati hanno mostrato che i modelli linguistici con opinioni politiche diverse hanno prestazioni diverse su questi compiti, con potenziali implicazioni per l'equità nelle applicazioni di NLP.

Shangbin conclude che il dilemma dei pregiudizi politici dei modelli linguistici è simile all'elettroscilla del problema, dove la scelta di non sanificare gli opinioni politiche nei dati di addestramento potrebbe portare a problemi di equità, mentre la scelta di sanificare potrebbe portare a censura o esclusione.</sample>
    <sample id="206">Il modello utilizzato per il trasferimento dell'apprendimento sono due task: topic independent dissonance stance classification (debate) e binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="207">Il recente test set utilizzato per valutare le capacità di PaLM è stato il WMT evaluation.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine: 

1. Includere l'analisi delle stereotipie positive e delle narrazioni essenzializzanti.
2. Utilizzare una lente intersezionale per studiare le bias e i danni.
3. Aumentare la trasparenza sulle metodologie di mitigazione dei bias.</sample>
    <sample id="209">Il metodo proposto (over-generate-then-filter) migliora la qualità delle scritture generando script più fedeli alle restrizioni e più completi semanticamente rispetto al metodo di riferimento (InstructGPT). Inoltre, il metodo proposto consente di ottenere risultati migliori con modelli più piccoli (T5 fine-tunato su CoScript) rispetto ai modelli più grandi.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametri di riferimento per l'automatizzazione della semplificazione del testo in tedesco.</sample>
    <sample id="212">T5</sample>
    <sample id="213">OFA (un modello pre-addestrato multi-modale unificato).</sample>
    <sample id="215">Adam Przepiórkowski sta discutendo la struttura di dipendenza della coordinazione in grammatica. Esistono diverse teorie e approcci che descrivono la struttura di coordinazione in modo diverso. Alcune teorie, come quelle di Universal Dependencies e di Igor Mel'čuk, considerano la prima congiunzione come capo della struttura di coordinazione. Altre teorie, come quella del Prague Dependency Treebanks, considerano la congiunzione come capo della struttura di coordinazione. Infine, la teoria di Hudson's Word Grammar considera tutti i congiunti come capi della struttura di coordinazione.

Adam sostiene che la struttura di coordinazione dovrebbe essere simmetrica, anziché asimmetrica, e che questo può essere dimostrato utilizzando il principio di minimizzazione della lunghezza delle dipendenze. Questo principio afferma che le dipendenze più brevi sono preferite. Adam fornisce esempi di come questo principio possa spiegare la preferenza per la posizione della prima congiunzione in una struttura di coordinazione.

Inoltre, Adam presenta statistiche estratte dalla versione migliorata del Penn Treebank, che confermano l'osservazione che i congiunti di sinistra tendono a essere più brevi e che questa tendenza cresce con la differenza di lunghezza tra i congiunti. Tuttavia, Adam osserva che questa tendenza si verifica solo quando il governatore è sulla sinistra o assente. Quando il governatore è sulla destra, la tendenza scompare. Questo, secondo Adam, fornisce un argomento a favore delle strutture di coordinazione simmetriche e contro quelle asimmetriche.</sample>
    <sample id="217">Il team di ricerca "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" ha sviluppato un modello di dialogo controllabile che può generare risposte coerenti e specifiche per combinazioni di attributi non viste prima. Il modello, chiamato DCG (Disentangled Controllable Generation), utilizza una combinazione di promemoria orientati agli attributi e promemoria orientati al compito per guidare la generazione del dialogo.

Il team ha anche sviluppato un framework di valutazione unificato chiamato MAE (Matrix for Automatic Evaluation) che non richiede dati annotati aggiuntivi per valutare la controllabilità e l'uguaglianza del testo del dialogo. Il framework MAE è stato testato su due benchmark e ha dimostrato di essere efficace nel valutare la controllabilità e l'uguaglianza del testo del dialogo.

Il team ha anche dimostrato che il modello DCG può generalizzare dalle combinazioni di attributi viste prima a quelle non viste prima, e che il framework MAE può valutare la qualità del dialogo generato dal modello DCG. Inoltre, il team ha dimostrato che il modello DCG può disentangolare le combinazioni di attributi e imparare le relazioni tra gli attributi con l'abilità di generalizzare dalle combinazioni di attributi viste prima a quelle non viste prima.</sample>
    <sample id="218">Gli autori dell'articolo sono colleghi di Google Translate.</sample>
    <sample id="219">Ciao Jia-Huei, è un piacere conoscerti. Il tuo lavoro, "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports", è un'analisi dei rapporti finanziari con un approccio multistadio. Il tuo team ha sviluppato un modello di evidenziazione per identificare le informazioni importanti nei rapporti finanziari.

Il modello si basa su due osservazioni: la similitudine dei testi nei rapporti finanziari (circa 80% di token identici) e la dipendenza annuale dei contenuti. Il modello di evidenziazione è stato sviluppato per confrontare e contrapporre il contesto tra il rapporto di interesse e quello dell'anno precedente.

Il modello di evidenziazione è composto da quattro stadi: segmentazione dei documenti, riconoscimento delle relazioni, fine-tuning fuori e dentro il dominio. Il fine-tuning fuori dal dominio si basa su un dataset esterno, eSNLI, mentre il fine-tuning dentro il dominio si basa su pseudo-etichette generate dalle revisioni dei rapporti.

Il modello di evidenziazione è stato valutato su due metriche: precisione e correlazione tra previsione e annotazioni. Il modello ha ottenuto il miglior risultato sul dataset FINAL e ha dimostrato una buona generalizzazione su eSNLI. Il tuo lavoro ha anche dimostrato di essere benefico per la simulazione con le coppie di match non utilizzate durante la formazione.</sample>
    <sample id="220">Non sono state menzionate affiliazioni specifiche degli autori dell'articolo. Tuttavia, Vasudha menziona di essere un candidato al dottorato in Computer Science presso la Stony Brook University.</sample>
    <sample id="221">L'articolo non specifica esplicitamente le coppie linguistiche analizzate, ma si fa riferimento a una traduzione da tedesco a inglese come esempio. Tuttavia, si può supporre che le coppie linguistiche analizzate siano quelle utilizzate per i test di valutazione WMT (Machine Translation), che comprendono diverse lingue, tra cui inglese, francese, tedesco, cinese, giapponese e altre.</sample>
    <sample id="222">Il lavoro "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" affronta il problema della generalizzazione dei modelli di domanda-risposta aperti (Open-Domain QA) a nuovi domini. I ricercatori presentano tre contributi principali: 

1.  **Indagine su interventi di dati**: gli autori esplorano diverse strategie per migliorare la generalizzazione dei modelli Open-Domain QA, tra cui l'uso di esempi di pochi-shot e zero-shot.
2.  **Identificazione del tipo di shift del dominio**: gli autori sviluppano una misura di compatibilità per comprendere il tipo di shift che un nuovo dominio esibisce rispetto al modello di partenza.
3.  **Determinazione degli interventi di dati efficaci**: gli autori determinano quali interventi di dati siano più efficaci per ogni tipo di shift.

Gli autori utilizzano set di dati di domini diversi e mostrano che gli interventi di dati possono migliorare la prestazione dei modelli di lettura e di recupero di passaggi fino a un massimo del 24%. Inoltre, gli autori mostrano che gli interventi di dati sono più efficaci per alcuni tipi di shift, come il shift concettuale e il shift covariante.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati due modelli: long-mBART e mBART.</sample>
    <sample id="225">53 task vengono utilizzate per l'addestramento e 9 task vengono utilizzate per il test.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Il testo descrive un nuovo approccio per il compito di comprensione del linguaggio legato al contesto (Grounded Language Understanding), che consiste nell'addestrare i modelli di linguaggio a discriminare tra piani o programmi validi e non. I modelli di linguaggio attuali sono limitati nella loro capacità di generare piani validi, poiché sono stati addestrati su corpus di testo senza considerare il contesto. Il nuovo approccio, chiamato Pangu, utilizza un agente simbolico per generare piani candidati e un modello di linguaggio per valutare e classificare questi piani. Pangu è stato testato su diversi compiti, tra cui la risposta alle domande basate su conoscenza e ha dimostrato di essere più efficiente e robusto rispetto ai modelli di linguaggio esistenti. L'autore sostiene che la discriminazione sia una strategia più efficace per il compito di comprensione del linguaggio legato al contesto rispetto alla generazione. Il testo conclude con l'invito a discutere e collaborare per migliorare il modello.</sample>
    <sample id="228">Gli autori hanno condotto gli esperimenti sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya presenta un lavoro con Henning Wachsmuth sul rilevamento di affermazioni migliorabili per supportare la scrittura argomentativa. La revisione del testo è un processo ricorsivo fondamentale nella scrittura professionale, che può influenzare direttamente l'effetto del testo sull'uditorio. L'ottimizzazione della fraseggio è essenziale per comunicare efficacemente un messaggio e stimolare una reazione desiderata.

Il lavoro si concentra su due compiti: la detezione di affermazioni subottimali e la proposta di suggerimenti di miglioramento. Per affrontare questo problema, l'autore esplora le sfide legate al lavoro con dati di revisione, come la rappresentatività e la affidabilità del dataset, la complessità e l'architettura del modello, l'importanza del contesto e il bias topico e utente.

Il lavoro dimostra che i dati di revisione possono essere utilizzati efficacemente per i compiti in questione e che modellare la distanza tra due versioni affermative è benefico per la detezione di affermazioni subottimali. Inoltre, l'impatto del contesto è dipendente sia dal compito che dalle questioni di qualità del testo.</sample>
    <sample id="231">NACHOS è un data set di dati medici raccolti dal web, utilizzato per l'addestramento del modello linguistico DrBERT.</sample>
    <sample id="232">Il nome della relatrice o del relatore è David Vilar.</sample>
    <sample id="233">Ciao Sara, sono felice di ascoltare la tua presentazione del tuo lavoro "Attention as a Guide for Simultaneous Speech Translation". 

Il tuo lavoro si concentra sullo sviluppo di una strategia per la traduzione simultanea del linguaggio parlato in tempo reale. La traduzione simultanea, o SimulST, è un processo che traduce il linguaggio parlato in un testo in un'altra lingua in tempo reale, facilitando la comunicazione interlinguistica.

Il problema attuale con i modelli SimulST è che richiedono architetture specifiche e procedure di training complesse e lunghi. Inoltre, è necessario allenare e mantenere diversi modelli per raggiungere differenti regimi di latenza.

La tua soluzione propone di utilizzare modelli di traduzione simultanea offline esistenti senza doverli ristrutturare o adottare architetture specifiche per la SimulST. Utilizzerai un solo modello per ogni regime di latenza e gestirai la latenza attraverso parametri specifici.

La tua strategia si chiama EDAtt (Encoder-Decoder Attention) e si basa sulla decisione di emettere o no una traduzione parziale in base al punto di attenzione tra l'input audio e l'output testuale. La traduzione viene emessa se l'attenzione non è concentrata, cioè se la somma dei pesi di attenzione verso le ultime lambda frasi di parlato è inferiore a un certo valore di soglia alpha.

Hai testato la tua strategia su dati di traduzione simultanea in tedesco e hai ottenuto risultati migliori rispetto a strategie popolari come Wait-k e Local Agreement, nonché rispetto all'architettura di stato dell'arte specificamente progettata per la traduzione simultanea.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati, come dimostrato da un semplice esperimento dove la differenza tra due diversi prompt per ogni frase è stata di oltre 1 punto BLEURT, e in casi estremi di fino a 40 punti BLEURT.</sample>
    <sample id="235">Gli autori dell'articolo sono affiliati con diverse istituzioni: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="236">Non sono specificate le 5 istruzioni scritte da esperti, ma si menziona che ogni compito del dataset MultiInstruct è equipaggiato con cinque istruzioni scritte da esperti.</sample>
    <sample id="237">Gli autori propongono un test di valutazione chiamato KITMUS (Knowledge Integration from Multiple Sources Test), un set di prove diagnostiche per valutare la capacità dei modelli di integrare e utilizzare conoscenze provenienti da fonti diverse.</sample>
    <sample id="238">L'utente Yebowen Hu presenta il nuovo benchmark dataset MeetingBank, creato per aiutare a sviluppare tecnologie di riassunto per meeting. Il dataset comprende 1.366 meeting del City Council, con oltre 7.000 istanze, e include trascrizioni audio, riassunti di riferimento e altre risorse utili. L'utente descrive il processo di raccolta dei dati e presenta le statistiche del dataset, tra cui la durata delle meeting, il numero di token e speaker per meeting e la data di riferimento. Inoltre, l'utente presenta due misure per valutare la qualità dei riassunti: la copertura e la densità. I risultati mostrano che i riassunti dei meeting del City Council tendono a includere punti verbatim più che abstrazioni. L'utente presenta anche i risultati di un'analisi di dieci sistemi di riassunto, tra cui GPT-3, e presenta i risultati di un'evaluazione umana che mostra che GPT-3 raggiunge i migliori risultati in termini di fluenza e coerenza, ma meno impressionanti in termini di informatività e fattualità. L'utente conclude presentando MeetingBank come un benchmark dataset utile per gli studiosi e un'interessante fonte di informazioni sulla decisione dei City Council.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar, e sto per fare una breve recensione del paper "Prompting PaLM per la Traduzione: Valutazione delle strategie e delle prestazioni". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un grande modello di linguaggio con 540 miliardi di parametri presentato lo scorso anno nel 2022. È stato addestrato su una grande raccolta di testi, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto il risultato di punta in centinaia di compiti NLP. In questo lavoro, presentiamo la prima valutazione sistematica del prompting dei grandi modelli di linguaggio per la traduzione automatica. Abbiamo valutato la capacità di transizione di questi modelli utilizzando le migliori pratiche della comunità MT. Ciò comporta l'utilizzo dei test set più recenti per evitare l'overlapping dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi di stato dell'arte, quindi il sistema che esegue meglio, quindi l'evaluation WMT. Utilizziamo metriche di MT neurali di stato dell'arte, e inoltre mostriamo anche i risultati di valutazione umana basati sull'expert. Infine, forniamo alcune raccomandazioni per le strategie di selezione di prompt. Il prompting ha un grande influenza sulla prestazione dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato il prompting a un solo colpo e fornito due diversi prompt per ogni frase. La maggior parte delle frasi, 516 su 1.000, mostra una differenza di più di un punto BLEURT. E questo può andare, nei casi estremi, fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, ci siamo fermati a una strategia di 5-shot prompting, dove abbiamo semplicemente marcato ogni frase che forniamo al sistema, con la lingua in cui si trova. Quindi, in questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di partenza, sono contrassegnate con "tedesco: " e le traduzioni inglesi con "inglese: ". Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di più brevi promptings. È cruciale per il prompting a zero e un solo colpo. E quando andiamo, come nel nostro caso, a 5-shot prompting, ci sono quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano il peso maggiore. La somma dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di partenza. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, abbiamo confrontato la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di valutazione. I dati di valutazione sono molto più curati e di alta qualità rispetto ai dati di addestramento, che sono più rumorosi. E i risultati mostrano una prestazione migliore quando si utilizzano i dati di valutazione. Nonostante ciò, i sistemi di stato dell'arte specializzati hanno un grande vantaggio sui traduttori PaLM. Ma PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo guadagnato dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM hanno detto che la fluidezza di PaLM è paragonabile a quella dei sistemi di stato dell'arte, ma la differenza principale deriva dall'accuratezza. Quindi, in particolare, gli errori più comuni sono gli errori di omissione. Quindi sembra che PaLM scelga di produrre una traduzione che suona meglio, a volte eliminando parti della frase di partenza che sono necessarie per la traduzione. Tuttavia, la categoria "Style/Awkward" per PaLM è inferiore a quella dei sistemi di stato dell'arte, che è un ulteriore segnale che PaLM fornisce un output fluente, ma con alcuni problemi di accuratezza. E basta così per questa breve recensione. Per ulteriori dettagli, per favore vengano al full presentation del paper. Grazie mille.</sample>
    <sample id="240">Ciao Dawei, un dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: Una visione critica dell'apprendimento supervisionato debole". Questo lavoro è stato condotto in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione all'apprendimento supervisionato debole e all'apprendimento supervisionato debole.

Nell'apprendimento supervisionato debole, non si etichettano manualmente i dati. Al contrario, utilizziamo fonti di etichettatura deboli, come regole semplici, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Se confrontati con le annotazioni umane, gli etichetti deboli sono molto più economici, ma sono anche rumorosi, ovvero un certo numero di annotazioni è errato. Se si addestra un modello neurale direttamente sui dati etichettati debolmente, il modello tende a memorizzare il rumore di etichettatura e non generalizza. L'apprendimento supervisionato debole prevede l'utilizzo di algoritmi di addestramento per addestrare i modelli neurali in modo robusto sotto tale rumore di etichettatura, in modo che i modelli addestrati generalizzino ancora bene.

Nelle opere recenti in WSL (apprendimento supervisionato debole), una pretesa comune è che si addestrano i modelli solo sui dati etichettati debolmente e si ottengono prestazioni elevate sui set di test puliti. Tecnicamente, questa pretesa non è sbagliata, ma c'è un problema, ovvero che le persone suppongono che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci a questo problema di impostazione, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento supervisionato debole. Ma come un elefante nella stanza, questa necessità è spesso trascurata.

L'obiettivo principale di questo lavoro è di indagare tre domande di ricerca. Prima domanda: è necessario un set di validazione pulito per WSL o possiamo utilizzare un set di validazione rumoroso invece? Seconda domanda: se è necessario un set di validazione pulito, o se un set di validazione pulito è obbligatorio per WSL per funzionare, allora quanti campioni puliti abbiamo bisogno? Terza domanda: dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono metodi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.

Prima di tutto, abbiamo scoperto che i metodi di apprendimento supervisionato debole recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, ci è un grande calo delle prestazioni. Come mostrato nella figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare al di là degli etichetti deboli originari, il che significa che l'addestramento è inutile. Ciò indica che gli approcci di apprendimento supervisionato debole richiedono effettivamente dati etichettati puliti per funzionare correttamente, e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.

La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci di apprendimento supervisionato debole a raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno di solo 20 campioni per classe per raggiungere prestazioni elevate. Ma non è la fine della storia, perché se decidiamo di accedere ai campioni puliti, allora addestrare su di essi direttamente raggiungerà prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci di apprendimento supervisionato debole, che utilizzano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci di apprendimento supervisionato debole.

Infine, l'improvvisa prestazione affermata in precedenti approcci di apprendimento supervisionato debole può essere facilmente raggiunta consentendo di continuare a fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, chiamato FTw, inizia inizialmente a sottoprestare gli approcci più complessi di apprendimento supervisionato debole, come COSINE. Tuttavia, se consentiamo di continuare a fine-tuning sui campioni puliti, allora FTw raggiunge prestazioni uguali agli altri metodi. Pertanto, in pratica, non c'è motivo di scegliere metodi di apprendimento supervisionato debole più complessi che richiedono più tempo di calcolo e spazio di archiviazione.

In sintesi, abbiamo dimostrato che gli approcci di apprendimento supervisionato debole recenti richiedono campioni etichettati manualmente puliti per funzionare correttamente. Le loro prestazioni e praticità sono pesantemente sopravvalutate. Le nostre raccomandazioni concrete per il futuro sono le seguenti. Prima raccomandazione: riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è effettuata tramite campioni di validazione puliti. Seconda raccomandazione: gli approcci di apprendimento supervisionato debole dovrebbero essere confrontati con basi di apprendimento a pochi esempi, poiché entrambi lavorano sui campioni puliti. Terza raccomandazione: il fine-tuning continuo è un basi semplice e forte che dovrebbe essere considerato in futuro nel campo di apprendimento supervisionato debole. Infine, abbiamo reso disponibile il nostro codice. Potete trovarlo tramite il codice QR in questa diapositiva. Vi preghiamo di verificarlo gratuitamente. Grazie e godete della conferenza.</sample>
    <sample id="241">Il paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" propone un framework di valutazione per lo sviluppo di sistemi che affrontano le carenze delle attuali metodologie di rilevamento della disinformazione. I sistemi attuali sono spesso valutati in modo irrealistico, utilizzando dati storici costruiti in retrospettiva, e non tengono conto della complessità e della noiosità dei social media. Il framework proposto è end-to-end, partendo dai tweet raw di Twitter e arrivando a output azionabili per gli umani, con un' integrazione stretta della feedback umano in vari stadi del processo.

Il sistema si compone di due componenti principali: la prima si occupa della detezione delle affermazioni fuorvianti, utilizzando un modello T5 per l'estrazione delle affermazioni e un modello BERT per la classificazione dello stile dell'autore; la seconda si concentra sulla verifica delle violazioni delle politiche, utilizzando le affermazioni verificate nella prima fase per segnalare le violazioni delle politiche dei social media.

L'efficacia del sistema è valutata attraverso l'early detection, cioè la detezione delle affermazioni fuorvianti prima che siano state smentite dai mezzi di informazione, e la verifica delle violazioni delle politiche. I risultati mostrano che il sistema ha una precisione del 65% nella detezione delle violazioni delle politiche e che è possibile rilevare 124,2 violazioni per ogni ora di lavoro umano. Il framework proposto offre una visione più realistica e complessa dell'interazione tra sistemi e moderatori di contenuti umani, motivando lo sviluppo di sistemi di rilevamento della disinformazione in loop con gli umani.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono: 

1. Valutazione umana, come chiedere ai giudici umani di selezionare quale delle due conversazioni è meglio o di valutare le conversazioni utilizzando una scala Likert.
2. Valutazione basata su Likert scale, dove i giudici valutano le conversazioni utilizzando una scala Likert.
3. Valutazione basata su confronti a due, dove i giudici valutano le conversazioni confrontandole tra loro.</sample>
    <sample id="243">7 autori sono coinvolti nell'articolo: Jenny (autrice del testo), Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap e gli autori dei modelli e dei dati utilizzati (non specificati).</sample>
    <sample id="244">Nell'esempio con Servin e Kea, sono necessarie conoscenze di base come "I giudici decidono casi in corti di giustizia" per risolvere la pronome "he".</sample>
    <sample id="245">Ecco un riassunto dei punti principali presentati da Lining Zhang:

Il lavoro presentato si intitola "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" e si concentra sulla ricerca di lavoratori di alta qualità su Amazon Mechanical Turk (MTurk) per la sommariizzazione di testi.

La pipeline proposta consiste in due fasi: la prima fase, chiamata "Qualification Task", testa le abilità degli annotatori nella valutazione di diverse dimensioni, mentre la seconda fase, chiamata "Endurance Task", verifica la capacità di gestire un carico di lavoro pesante.

I lavoratori che superano queste fasi vengono categorizzati in quattro tipi: oro, argento, bronzo e blocco, con solo i lavoratori di tipo oro e argento che possono proseguire. I risultati mostrano che solo il 6% dei partecipanti (12 lavoratori) supera la seconda fase.

La pipeline proposta mostra un alto livello di accordo tra gli annotatori (Krippendorff's Alpha = 0,443) e supera i risultati dei lavoratori di baseline e CloudResearch MTurk. Tuttavia, ci sono alcune limitazioni nel lavoro, come la mancanza di garanzia per la formazione della correttezza e la necessità di estendere la ricerca a diverse lingue e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile. È possibile trovarlo su GitHub.</sample>
    <sample id="247">Il team di ricerca di Jiho Kim, proveniente dall'Università KAIST, ha sviluppato un nuovo dataset per la verifica dei fatti chiamato FactKG. Questo dataset si basa su grafi di conoscenza (Knowledge Graphs) e utilizza come fonte di dati DBpedia. Le affermazioni sono categorizzate in due stili: scritto e colloquiale, per una maggiore praticità. Il compito consiste nel recuperare l'evidenza dal grafo di conoscenza e verificare l'affermazione utilizzando l'evidenza stessa.

Il dataset FactKG include cinque tipi di ragionamento: one-hop, congiunzione, esistenza, multi-hop e negazione. I claim di uno-hop possono essere rappresentati da un singolo triplo, mentre quelli di congiunzione richiedono la verifica di più claim uno-hop. I claim di esistenza richiedono la verifica di un'unica connessione tra un entità e una relazione. I claim di multi-hop richiedono inferenze multiple, poiché le entità non sono sempre presenti nel claim. I claim di negazione richiedono una verifica aggiuntiva anche dopo aver trovato l'evidenza del grafo.

Il dataset FactKG include sia claim in stile scritto che colloquiale, ottenuti tramite due metodi: la trasferenza di stile colloquiale e la creazione di template di presupposizione. I risultati mostrano che il modello GEAR, che utilizza l'evidenza del grafo, supera tutti gli altri baseline. Il dataset FactKG è disponibile per il download e il team di ricerca è disponibile per eventuali contatti.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico. Hanno riannotato i dati con annotatori diversi e hanno raccolto una grande quantità di dati demografici. Hanno reclutato oltre 1000 annotatori da 87 paesi, garantendo una rappresentazione diversificata.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo "rumore" senza alterare la struttura grammaticale rilevante.</sample>
    <sample id="250">Una valutazione dimensionale significa analizzare e misurare diversi aspetti o dimensioni di un sistema o processo, in questo caso, la qualità delle conversazioni tra un modello di chat e un utente. Questo approccio consente di ottenere una visione più dettagliata e completa della qualità della conversazione, identificando le aree di forza e debolezza del modello.</sample>
    <sample id="251">Non sono disponibili informazioni sugli autori dell'articolo, solo il nome di uno degli autori, Jingwei Yi, che afferma di essere del University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella, un dottorando presso l'IIT Kanpur, ha presentato il suo lavoro "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion". Il progetto è un contributo importante per la task di Prior Case Retrieval, che consiste nel recuperare precedenti giuridici rilevanti da un pool di candidati. Il lavoro di Sai Kiran e del suo team si concentra sulla creazione di un nuovo dataset, IL-PCR, che contiene 7.070 casi giuridici con 6,775 citazioni medi per documento di query.

Il team ha anche sviluppato un pipeline, U-CREAT, che utilizza tecniche di apprendimento non supervisionato e un approccio basato sugli eventi per il compito di PCR. Il pipeline è composto da tre blocchi: pre-elaborazione, parsing di dipendenza e post-elaborazione. Gli eventi estratti sono utilizzati per calcolare una matrice di interazione tra il documento di query e i candidati.

Sai Kiran ha condotto esperimenti con diverse architetture di modello per valutare e confrontare le loro prestazioni sulla task di PCR. I risultati mostrano che gli event-based models, in particolare il Event Filtered Docs, superano i metodi di base e sono in grado di recuperare precedenti giuridici con una maggiore accuratezza e una velocità di inferenza inferiore.

In sintesi, il lavoro di Sai Kiran e del suo team rappresenta un contributo significativo per la task di Prior Case Retrieval e apre nuove prospettive per lo sviluppo di metodi più efficaci per il recupero di precedenti giuridici.</sample>
    <sample id="253">Il lavoro presentato da Mario Ezra Aragón si intitola "DisorBERT: Un Modello di Adattamento a Doppio Dominio per la Rilevazione dei Segni di Disturbi Mentali sui Social Media". L'obiettivo è quello di contribuire alla detezione dei disturbi mentali attraverso l'analisi automatica dei post sui social media.

L'approccio proposto utilizza l'adattamento a doppio dominio per migliorare le prestazioni del modello su un dominio specifico (in questo caso, i disturbi mentali sui social media). Il modello BERT viene addestrato su dati generali e poi adattato a un linguaggio specifico (in questo caso, quello di Reddit e salute mentale).

Il modello DisorBERT integra informazioni da Reddit e salute mentale, oltre che da un lessico per guidare il processo di mascheramento. Il modello è stato testato utilizzando il dataset eRisk e ha mostrato un buon equilibrio tra precisione e recall.

L'analisi dei risultati ha mostrato che il modello DisorBERT tende a focalizzarsi su parole più negative e legate alla salute mentale rispetto al modello BERT. La visualizzazione delle attenzioni del modello ha mostrato che le parole più importanti sono legate a temi come l'ansia e la medicazione.

In conclusione, l'approccio proposto è efficace nella cattura dei segni di disturbi mentali sui social media e ha ottenuto risultati migliori rispetto al modello MentalBERT. Future linee di ricerca includono l'utilizzo di risorse lessicali diverse e dati clinici.</sample>
    <sample id="254">Il team di ricerca guidato da Sun Qi della Nanjing University of Science and Technology ha presentato un nuovo framework per l'estrazione di relazioni documentali a livello di documento, chiamato "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction". Questo framework è progettato per migliorare la qualità dei dati DS (distant supervision) utilizzati per l'addestramento dei modelli di estrazione di relazioni documentali. Il problema principale è che i dati DS contengono vari livelli di rumore, che può essere mitigato utilizzando pseudo etichette. Tuttavia, queste pseudo etichette possono essere false positive, inducendo ulteriore rumore. Il team propone un approccio innovativo che utilizza l'analisi di incertezza per determinare se le predizioni del modello possono essere fidate o meno. Il framework include una strategia di rilabeling dinamica e una strategia di addestramento multi-fase per migliorare ulteriormente le prestazioni del modello. I risultati mostrano che il framework proposto supera i risultati dei metodi precedenti su due diversi dataset pubblici. Le principali contribuzioni del lavoro sono: 1) il framework con analisi di incertezza guidata per la denoising delle etichette; 2) un metodo di analisi di incertezza a livello di istanza per le relazioni sovrapposte; 3) una strategia di rilabeling dinamica con soglie di incertezza di classe dinamiche per il problema delle code lunghe.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero-shot e one-shot prompting.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">L'autore, Chiang Cheng-Han, presenta un lavoro intitolato "Can Large Language Models Be an Alternative to Human Evaluation?", che esplora l'uso di grandi modelli linguistici per valutare la qualità del testo in elaborazione del linguaggio naturale. I ricercatori hanno proposto di utilizzare questi modelli per valutare le storie generate da GPT-2 o scritte da umani, basandosi su quattro attributi: grammatica, coerenza, piacevolezza e rilevanza. Sono stati utilizzati quattro grandi modelli linguistici: T0, InstructGPT (curie e davinci) e ChatGPT. I risultati mostrano che due di questi modelli, Davinci e ChatGPT, preferiscono le storie scritte da umani rispetto a quelle generate da GPT-2, come fanno gli insegnanti di inglese che hanno valutato le storie. Ciò suggerisce che questi modelli linguistici possono essere utilizzati come alternativa alla valutazione umana in alcune applicazioni. Tuttavia, gli autori riconoscono che ci sono ancora molte domande aperte, come ad esempio se i modelli linguistici e gli evaluator umani concordano sui singoli rating di ogni storia, e quali siano i benefici e i costi dell'utilizzo di questi modelli di valutazione rispetto alla valutazione umana.</sample>
    <sample id="259">Il tuo lavoro, XSemPLR, si concentra sulla semantic parsing cross-linguale, ovvero la traduzione di query in diverse lingue naturali in diverse rappresentazioni di significato. Il tuo obiettivo è creare un benchmark unificato per valutare i modelli di semantic parsing cross-linguale.

Il tuo lavoro propone sei setting di valutazione: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot e Cross-lingual Few-shot. Questi setting valutano la capacità dei modelli di tradurre query in diverse lingue e rappresentazioni di significato.

Hai valutato tre tipi di modelli multilingui: Encoder-PTR, Encoder-Decoder e Multilingual Pretrained Encoder-Decoder. I risultati mostrano che gli Encoder-Decoder ottengono le migliori prestazioni su tutti i nove dataset. Inoltre, hai trovato che pretrainare i modelli su lingue naturali può migliorare la loro prestazione in modalità Few-shot.

I risultati mostrano anche che i modelli multilingui, come Codex e BLOOM, sono ancora insufficienti per le attività di parsing semantic cross-linguale. Il tuo lavoro propone XSemPLR come un benchmark unificato per valutare i modelli di semantic parsing cross-linguale e mostra risultati interessanti che potrebbero guidare lo sviluppo di nuovi modelli.</sample>
    <sample id="260">Non sono forniti dettagli sull'elenco degli autori dell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="262">Non sono specificati gli autori nell'introduzione.</sample>
    <sample id="263">La vostra ricerca si concentra sulla mitigazione dei bias nei modelli di apprendimento in contesto. I modelli di apprendimento in contesto sono instabili a causa delle scelte di progettazione, come la scelta e l'ordine degli esempi in contesto. I lavori precedenti hanno dimostrato che l'instabilità di ricerca di apprendimento in contesto deriva da queste scelte e che introducono bias nelle previsioni dei modelli.

La vostra ricerca propone una classificazione dei bias e identifica un nuovo tipo di bias, chiamato "domain-label bias", che cattura l'effetto del corpus di compito sulle previsioni dei modelli. Per confermare che il corpus di compito può influenzare le previsioni dei modelli, avete condotto esperimenti che mostrano che vedere parole casuali in-domain può influenzare le preferenze dei modelli per i nomi di etichetta, mentre vedere parole casuali inglese non mostra tale preferenza.

La vostra ricerca propone un metodo di calibrazione chiamato "domain-context calibration" che utilizza parole casuali in-domain come testo senza contenuto per stimare il bias dei modelli e calibrare le loro previsioni originali. I risultati mostrano che il metodo di calibrazione proposto migliora significativamente le prestazioni dell'apprendimento in contesto su una vasta gamma di dati e modelli.

In sintesi, la vostra ricerca propone un'indagine sistematica dei problemi di bias nell'apprendimento in contesto e propone un metodo di calibrazione che migliora le prestazioni dell'apprendimento in contesto.</sample>
    <sample id="264">Lin Wang, un dottorando della Zhejiang University in Cina, ha presentato un paper intitolato "TAVT: Verso una generazione di testi audio-visivi trasferibili". La presentazione ha trattato la sfida di creare modelli di generazione di testi audio-visivi trasferibili, ovvero capaci di adattarsi a nuovi domini con dati limitati. Il modello TAVT è composto da tre componenti: un network di mappatura meta-audio-visiva, un encoder e generatore audio-visivo e un'apprendimento contrattivo counterfactual.

Il network di mappatura meta-audio-visiva mappa concetti visivi diversi all'interno di uno spazio semantico audio unificato. Gli autori hanno utilizzato il dataset Flickr per raccogliere audio clips e hanno applicato l'algoritmo k-means per clusterizzare i dati e creare uno spazio audio semantico unificato. Il modello utilizza token imparabili, chiamati prefissi visivi, per migliorare la semantica dei token.

L'encoder e il generatore utilizzano una rete transformer con un parametro alpha per valutare la contribuzione di ogni modalità alle parole generate. Il modello TAVT è stato valutato su due benchmark basati su MSVD e MSR-VTT, mostrando risultati superiori rispetto ai modelli di confronto su entrambi i metri e le impostazioni di test.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Non sono stati menzionati gli autori dell'articolo nel testo fornito. Tuttavia, l'autore del testo sembra essere Adam Przepiórkowski, che discute i contenuti del proprio articolo.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli omissioni, ovvero la tendenza a omettere parti del testo originale durante la traduzione.</sample>
    <sample id="269">Ciao, sono James Finch. E sono Sarah Finch. E oggi vi racconteremo tutto su ABC-Eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato svolto dal Laboratorio di Elaborazione del Linguaggio Naturale (NLP) dell'Università di Emory, guidato dalla Professoressa Jinho Choi, in collaborazione con Amazon Alexa AI.

Immaginate di aver sviluppato un modello di dialogo e di voler vedere come si comporta rispetto allo stato dell'arte attuale. La pratica comune è utilizzare l'evaluazione umana, come chiedere ai giudici umani di scegliere quale delle due conversazioni è meglio o di valutare le conversazioni con una scala Likert. Queste approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molte facce. Pertanto, potreste voler valutare più dimensioni della qualità del dialogo per comprendere i punti di forza e le debolezze del modello su un livello più dettagliato.

Un approccio è chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi Likert o comparative esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stessi o il proprio partner. Chiamiamo questo approccio annotazione dei comportamenti nel dialogo (ABC-Eval) in breve.

Abbiamo sviluppato questo metodo per coprire complessivamente i comportamenti del modello del dialogo che sono stati suggeriti dall'ultima letteratura per influenzare la qualità del dialogo. ABC-Eval è in grado di misurare le percentuali con cui i modelli del dialogo commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello del dialogo ignora il proprio partner o dice qualcosa di irrilevante, contraddice se stesso o il proprio partner, fa supposizioni di fatti errati o viola la conoscenza comune, e quando il modello riesce o fallisce a mostrare empatia.

Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di dialogo di stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di dialogo a livello di dialogo. Per ogni metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli del dialogo lungo diverse dimensioni.

Dalla nostra analisi dei risultati di valutazione, abbiamo trovato che le etichette di comportamento di ABC-Eval sono in generale più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni doppie-etichettate. Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto ai metri prodotti dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare.

Ad esempio, potete vedere come misurare la proporzione di turni con contraddizioni con se stesso e con il proprio partner spiega il 5% e il 10% della qualità della conversazione, rispettivamente, mentre la media delle valutazioni Likert della consistenza spiega solo il 4% o meno. Infine, abbiamo verificato se ogni metro di valutazione cattura un aspetto unico della qualità del dialogo utilizzando un'analisi di regressione lineare step-by-step.

Potete vedere come la combinazione di tutti i metri di ABC-Eval spiega più del 25% della qualità della conversazione, e come rimuovere i metri uno per uno comporta la perdita di una buona parte dell'informazione sulla qualità. D'altra parte, la combinazione di tutti i metri Likert a livello di turno spiega molto meno della qualità e meno di questi metri portano informazioni uniche.

Questi metri affidabili, informativi e distinti di ABC-Eval ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta di quanto i metodi precedenti siano in grado di raggiungere. Potete vedere che nei risultati del nostro esperimento ci sono ancora molte sfide che restano e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni della conoscenza comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte, e si contraddicono o si contraddicono il proprio partner in circa il 10% delle volte.

Con il ritmo veloce di miglioramento nel campo, molte di queste percentuali di errore potrebbero vedere una riduzione nei nuovi modelli rilasciati dal momento in cui la nostra valutazione è stata condotta. Tuttavia, questo è ancora più motivo per perseguire metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E ci auguriamo di vedere come l'intelligenza artificiale conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono affiliati all'Emory NLP Lab di Emory University, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">Fine-tuning (FT)</sample>
    <sample id="272">7 autori sono coinvolti nell'articolo: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, Adina Williams e Koustav Sinha.</sample>
    <sample id="273">Ciao, il mio nome è Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede un contesto? Una esplorazione multilingue guidata dai dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.

Molti tradotti dipendono dal contesto. Ad esempio, come tradurre la parola "mole" in questa frase? Bene, se la frase precedente era "Le cose potrebbero diventare pericolose se i ministri lo scoprono", allora "mole" si riferisce a un agente segreto. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. Quindi, a seconda del contesto, il significato della parola cambia e quindi la sua traduzione cambia anche.

Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola porzione delle traduzioni dipendono dal contesto, il che rende impossibile catturare queste traduzioni con metri di corpus come BLEU. E alcune persone hanno suggerito valutazioni mirate sui tradotti dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di tradotti dipendenti dal contesto e set di lingue limitati, poiché spesso si basano sulla conoscenza del dominio e sulla cura umana.

In questo lavoro, cerchiamo di rispondere a queste due domande. Prima di tutto, quando la traduzione richiede un contesto? E secondariamente, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato a misurare quanto un parola dipenda dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura per l'utilizzo del contesto da parte dei modelli di traduzione automatica. E questo è fatto misurando quanto l'informazione che il contesto C fornisce sul bersaglio Y, dato il sorgente X. Puoi pensare a CXMI come l'informazione acquisita dando contesto al modello.

In questo lavoro, abbiamo esteso CXMI a Pointwise CXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo pensare a parole che hanno un alto P-CXMI come quelle che richiedono un contesto per la traduzione. Ora analizziamo parole con alto P-CXMI per cercare di individuare modelli tra queste parole. E eseguiamo il nostro analisi su trascrizioni di TED che sono state tradotte da inglese a 14 diverse lingue. Eseguiamo il nostro analisi a tre diversi livelli.

In primo luogo, esaminiamo le etichette di parte del discorso che hanno un alto P-CXMI medio. E questo ci consente di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi serve un contesto per determinare se un pronome è doppio quando si traduce in arabo. E similmente, troviamo che certi linguaggi richiedono un contesto quando vogliamo scegliere la forma verbale giusta.

In secondo luogo, esaminiamo gli elementi lessicali che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. E ciò ci aiuta a identificare casi come questo, dove in cinese è necessario un contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. E similmente, troviamo che il contesto è importante per tradurre nella forma giusta.

In terzo luogo, esaminiamo diversi token individuali che hanno un alto P-CXMI. E ciò ci consente di identificare fenomeni che non possono essere catturati dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione delle ellissi. Quindi ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a documenti. Per ogni dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger automatici per identificare le parole che si riferiscono al fenomeno. E abbiamo chiamato il nostro tagger il Multilingual Discourse-Aware, o MuDA tagger.

E possiamo anche notare che le diverse lingue hanno diverse proporzioni di questi fenomeni discorsivi. Quindi utilizziamo il MuDA tagger applicando il tagger su un corpus parallelo che vogliamo utilizzare per l'evaluazione e applichiamo le nostre metriche di traduzione di scelta sulle esempi dipendenti dal contesto che il MuDA tagger ha identificato. E infine, utilizziamo il nostro benchmark insieme ad altri metri per valutare diversi modelli sulla traduzione a documenti.

Innanzitutto, quando utilizziamo metri di corpus: quindi per BLEU, troviamo che i modelli agnostici al contesto hanno il miglior rendimento. Ma poi se utilizziamo COMET, i modelli consapevoli del contesto hanno il miglior rendimento. E se utilizziamo il misuratore di parola f, allora i modelli con e senza contesto hanno prestazioni comparabili. E questo di nuovo dimostra che è difficile determinare il miglior sistema di traduzione a documenti se utilizziamo metri di corpus soli.

Quindi utilizziamo il MuDA benchmark per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più precisi dei modelli che non utilizzano il contesto per certi fenomeni discorsivi come la forma e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto per altri fenomeni come l'ellissi, i pronomi e la forma verbale. Quindi questo sorta di suggerisce dove dovremmo vedere più progresso per la traduzione a documenti.

E abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è di solito più preciso di Google Translate per la traduzione a documenti. Per riassumere, eseguiamo un'analisi guidata dai dati su 14 paia di lingue per identificare quando le traduzioni richiedono un contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a documenti che può aiutarci a identificare quali fenomeni discorsivi i modelli possono gestire bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a documenti. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya e Vignesh presentano il loro lavoro intitolato "IndicMT Eval: Un Dataset per la Meta-Evaluazione dei Metrici di Traduzione Automatica per Lingue Indiane". L'obiettivo è quello di valutare i metrici di traduzione automatica per le lingue indiane, che sono state fino ad ora poco studiate. Sono state selezionate cinque lingue appartenenti a due diverse famiglie linguistiche: Tamil e Malayalam (lingue dravidiane) e Hindi, Marathi e Gujarati (lingue indoarie). Sono stati utilizzati sette modelli di traduzione automatica per generare 1.400 candidati di traduzione per ogni lingua. Sono stati poi reclutati annotatori bilingui per valutare le traduzioni e segnalare gli errori. I risultati mostrano che i modelli più recenti come NLLB e Indic Trans presentano meno errori rispetto ai modelli più vecchi come CVIT. I metrici di valutazione sono stati confrontati con le valutazioni umane e i risultati mostrano che alcuni metrici, come COMET-metric, presentano una correlazione più alta con le valutazioni umane. I ricercatori hanno anche finetizzato il metrico COMET per le lingue indiane e lo hanno testato su altre lingue non viste prima, ottenendo risultati promettenti. Il dataset è ora disponibile pubblicamente.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico, ma è basato sul concetto di "multiset tagging e permutation" per affrontare la composizione generale senza l'utilizzo di alberi.</sample>
    <sample id="278">Il metodo "Parole contrassegnate" (Marked Words) è stato sviluppato per identificare le parole che distinguono i gruppi contrassegnati (marked groups) da quelli non contrassegnati (unmarked groups). Questo metodo si basa sulla sociolinguistica del concetto di "contrassegnamento" (markedness), secondo cui esiste un default non contrassegnato e qualsiasi gruppo che ne differisce è contrassegnato linguisticamente. Ad esempio, la parola "guerriero" è spesso associata agli uomini, quindi quando si descrive una guerriera, si specifica spesso "guerriera donna" per contrassegnare il termine.</sample>
    <sample id="279">Non sono stati menzionati gli autori dell'articolo. L'oratore, Shangbin, è un dottorando presso l'Università di Washington.</sample>
    <sample id="280">Il lavoro di Shi Tao, intitolato "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations", si concentra sulla riconoscimento delle emozioni nelle conversazioni. L'obiettivo è quello di prevedere l'etichetta emotiva di ogni enunciato nella conversazione, considerando le tre modalità testuale, audio e visiva. Tuttavia, i metodi esistenti presentano alcuni limiti: non sfruttano a sufficienza l'informazione multimodale, hanno prestazioni insoddisfacenti nelle classi di emozioni minoritarie e hanno difficoltà a distinguere tra emozioni simili semanticamente.

Per superare questi limiti, Shi Tao propone un nuovo framework di fusione multimodale chiamato MultiEMO, composto da quattro componenti chiave: estrazione di feature unimodale, modellazione di contesto, fusione multimodale e classificazione emotiva. Il framework è composto da tre contributi principali: 

1. VisExtNet, un nuovo estrattore di feature visive che cattura le espressioni facciali degli interlocutori senza codificare informazioni di scena.
2. MultiAttn, un modello di fusione multimodale che integra le informazioni testuale, audio e visiva attraverso strati di attenzione bidirezionale e multi-testo.
3. Sample-Weighted Focal Contrastive Loss, una perdita di contrasto focalizzata che assegna più importanza alle classi di emozioni minoritarie e aiuta a distinguere tra emozioni simili semanticamente.

Gli esperimenti condotti su due dataset di benchmark, MELD e IEMOCAP, mostrano che MultiEMO raggiunge le migliori prestazioni in termini di riconoscimento delle emozioni, soprattutto nelle classi di emozioni minoritarie e nelle emozioni simili semanticamente.</sample>
    <sample id="281">Il tuo lavoro, intitolato "When Does Translation Require Context? A Data-driven, Multilingual Exploration", esplora quando la traduzione richiede contesto e come i modelli di traduzione lo gestiscono. In collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig, avete sviluppato un nuovo strumento per misurare l'utilizzo del contesto durante la traduzione, chiamato Pointwise CXMI.

La vostra analisi ha rilevato che alcuni vocaboli richiedono contesto per essere tradotti correttamente, come ad esempio i duali pronomi in arabo o la scelta del verbo corretto. Inoltre, avete identificato fenomeni come la risoluzione degli ellissi e la traduzione di nomi propri che richiedono contesto.

Per valutare i modelli di traduzione, avete creato un benchmark chiamato MuDA (Multilingual Discourse-Aware) che identifica le parole che richiedono contesto per essere tradotte correttamente. Le vostre ricerche hanno mostrato che i modelli di traduzione che utilizzano contesto sono più precisi di quelli che non lo utilizzano per certi fenomeni come la formalità e la coesione lessicale, ma non per altri come gli ellissi, i pronomi e la forma del verbo.

Infine, avete confrontato diversi sistemi di traduzione commerciali e il vostro benchmark ha mostrato che DeepL è generalmente più preciso di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="282">Il nostro lavoro, denominato "StoryTrans: Non-Parallel Story Author-Style Transfer con Discourse Representations e Content Enhancing", si concentra sul compito importante della trasferimento di stile testuale non parallelo. Mentre la maggior parte degli studi si è concentrata su livelli di token o frase, come la trasferenza di sentimenti delle frasi o la trasferenza di testi formali, il nostro lavoro si concentra sul trasferimento di stile a livello di storia, che è fondamentale per imitare lo stile dell'autore. 

Il principale ostacolo consiste nell'imitare le scelte linguistiche dell'autore a livello di discorso, che è difficile da trasferire a un altro stile. Per superare questi problemi, abbiamo proposto un modello di generazione denominato StoryTrans. Questo modello impara le rappresentazioni di discorso dai testi di origine e le combina con gli embedding di stile imparabili per generare testi in stili di destinazione. 

Il nostro modello ha due fasi di training: la prima fase utilizza un framework di training consigliato, mentre la seconda fase è focalizzata sulla generazione di contenuti specifici dello stile. I risultati mostrano che il nostro modello, StoryTrans, supera le basi forti in termini di controllo dello stile e preservazione del contenuto.</sample>
    <sample id="283">Hudson's Word Grammar.</sample>
    <sample id="284">Peng Tianshuo di Wuhan University ha presentato un lavoro intitolato "FSUIE: Un meccanismo di span fuzzy per migliorare l'estrazione universale di informazioni". Il modello attuale basato su span soffre di alcuni limiti, come l'ambiguità nelle etichette di span e la mancanza di adattabilità del modello alle caratteristiche locali delle informazioni. Per superare questi limiti, Peng propone due innovazioni: il meccanismo di span fuzzy e l'attenzione dinamica.

Il meccanismo di span fuzzy introduce una rappresentazione continua delle boundary di span, anziché precise, e calcola la perdita di span utilizzando la funzione di sampling. Inoltre, propone un'attenzione dinamica che adatta la finestra di attenzione in base alle caratteristiche locali delle informazioni.

Gli esperimenti condotti su tre task di estrazione di informazioni (riconoscimento di entità, estrazione di relazioni e estrazione di triplette di aspetto e sentimento) hanno mostrato che il modello FSUIE proposto da Peng ottiene risultati significativamente migliori rispetto al modello UIE senza meccanismo di span fuzzy. In particolare, FSUIE ha ottenuto nuovi record di prestazione sui dataset ACE2004, 2005 e ADE per l'estrazione di relazioni, e sui dataset AST-V2 per l'estrazione di triplette di aspetto e sentimento.</sample>
    <sample id="285">Il team di Mingqi Gao da Peking University ha sviluppato un sistema per correggere errori di fatto nelle sommario di dialoghi. Attualmente, i modelli di sommario generano spesso errori di fatto, anche se le sommario di riferimento contengono gli stessi errori. Ci sono due approcci per risolvere questo problema: introdurre obiettivi correlati alla factualità nel processo di allenamento o inferenza, o progettare un modello di correzione degli errori di fatto (FEC) indipendente dal modello di sommario.

Tuttavia, i modelli FEC sono stati valutati in modo scorretto, utilizzando metriche di factualità come FactCC e DAE che forniscono solo un punteggio generico e non garantono la correttezza degli errori. Il team di Mingqi Gao propone un nuovo framework di valutazione che utilizza annotazioni manuali di riferimento per correggere gli errori di fatto. Il framework consiste in tre passaggi: allineamento, classificazione e confronto.

Gli esperimenti hanno mostrato che allenare i modelli FEC con sommario di riferimento da dataset di sommario di dialoghi produce i migliori risultati. Inoltre, l'introduzione di sommario corretti da parte di esseri umani durante l'allenamento dei modelli FEC per la sommario di dialoghi può migliorare le loro prestazioni. Infine, i modelli FEC attuali hanno difficoltà a correggere errori di aggiunta e non possono affrontare errori di attributo, modalità, collegamenti, ecc.</sample>
    <sample id="286">Professor Jinho Choi</sample>
    <sample id="287">Ci sono 4 autori coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="288">I dati BLiMP, SyntaxGym e Wikipedia possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="290">Non sono state specificate le abbreviazioni dei cinque metodi, ma solo il loro nome: WSL (Weakly Supervised Learning), COSINE e FTw (che potrebbe essere un acronimo di Fine-Tuning con una specificazione aggiuntiva, ma non è stato specificato).</sample>
    <sample id="291">Il modello DrBERT viene valutato su 11 attività downstream in campo biomedico e clinico, tra cui: 

- Named Entity Recognition (riconoscimento di entità nominate)
- Classification (classificazione)
- Part-of-Speech Tagging (etichettatura di parte del parlato)
- Question Answering (risposta alle domande)</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su dati in lingua francese, ma non si specifica il tipo di dati (biomedici o clinici).</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presenta un lavoro di collaborazione tra l'Università di Torino e Amazon Alexa sul riconoscimento dell'ironia nella lingua naturale. L'obiettivo è sviluppare modelli di apprendimento automatico in grado di riconoscere l'ironia in modo più preciso rispetto ai modelli esistenti. 

Per raggiungere questo obiettivo, è stato creato un corpus di dati chiamato EPIC, che contiene 300 conversazioni brevi tra coppie di testi provenienti da fonti diverse, tra cui social media, Reddit e Twitter. I dati sono stati annotati da 74 annotatori provenienti da cinque varietà di inglese, che hanno ricevuto 200 testi ciascuno.

I risultati mostrano che i modelli di apprendimento automatico che tengono conto delle diverse prospettive degli annotatori (chiamati "perspective-aware models") sono più confidenti nelle loro predizioni rispetto ai modelli tradizionali. Inoltre, gli autori hanno scoperto che le differenze di annotazione sono più marcate tra generazioni vicine e tra annotatori provenienti da aree geografiche diverse, come ad esempio il Regno Unito e l'Irlanda.</sample>
    <sample id="297">Il progetto "From Dogwhistles to Bullhorns" analizza il linguaggio codificato utilizzato dai politici per evitare la moderazione dei contenuti online. Il termine "dogwhistle" si riferisce a espressioni che inviano un messaggio diverso a due gruppi diversi: l'outgroup non capisce il messaggio vero, mentre l'ingroup lo interpreta come un segnale di appartenenza. Il progetto ha sviluppato un glossario di oltre 340 termini e simboli, inclusi quelli razzisti, transfobici e antisemiti. 

Il glossario è stato creato per fornire una classificazione e un'analisi dei dogwhistles, che sono caratterizzati da tre elementi: registro (formale o informale), tipo (se aggiunge un implicato o segnala solo il persona) e persona (es. antisemita o transfobica). 

Il progetto ha condotto una ricerca storica sui discorsi politici degli Stati Uniti e ha scoperto che i dogwhistles sono stati utilizzati più frequentemente dai politici conservatori, in particolare dopo la fine della Guerra di Secessione. Inoltre, il progetto ha valutato la capacità dei modelli di linguaggio di riconoscere i dogwhistles e ha scoperto che GPT-3 può riconoscere alcuni dogwhistles, ma con una variazione significativa nei risultati. 

Infine, il progetto ha dimostrato come i dogwhistles possano evitare la moderazione dei contenuti online utilizzando l'API Prospective e le frasi template di HateCheck. I risultati mostrano che le frasi contenenti dogwhistles vengono valutate come meno tossiche rispetto a quelle contenenti termini standard.</sample>
    <sample id="298">Gli esperimenti di retraining dei modelli con dati più recenti hanno mostrato che la prestazione degrada con un maggior gap temporale, confermando l'hypotesi che la deriva temporale sia la causa principale della perdita di prestazioni.</sample>
    <sample id="299">Il lavoro presentato da Michalis Korakakis e Andreas Vlachos si concentra sulla robustezza degli modelli di NLI (Natural Language Inference) attraverso un approccio di training minimax. Gli autori osservano che gli attuali modelli di NLI sono vulnerabili alle "shortcuts", ovvero alle correlazioni spuri tra attributi di input e etichette introdotte durante la creazione del dataset. Ciò porta a un buon rendimento in-distribution, ma a un cattivo rendimento out-of-distribution. Per mitigare questo problema, gli autori propongono un approccio di training minimax che utilizza un modello ausiliario per massimizzare la perdita del modello principale, incentivandolo a concentrarsi sugli esempi più difficili e a ridurre la sua dipendenza dalle scorciatoie. Il modello ausiliario è un rete neurale feed-forward e non richiede conoscenze specifiche del dominio. Gli esperimenti mostrano che l'approccio di training minimax migliora la prestazione out-of-distribution rispetto a un modello di apprendimento standard e al miglior metodo di mitigazione delle scorciatoie per ogni dataset.</sample>
    <sample id="300">L'interazione di dictation è un compito in cui l'utente può utilizzare la voce per scrivere un documento in modo naturale e intuitivo. L'utente può iniziare a scrivere, poi correggere errori di grammatica o di sintassi utilizzando la voce, e infine inviare comandi vocali per modificare il testo. Questo compito è diverso da quelli esistenti, poiché non richiede l'utilizzo di parole chiave o comandi specifici, ma piuttosto l'uso di frasi naturali per specificare le modifiche. Gli autori hanno formalizzato questo compito come un processo a quattro passaggi: riconoscimento del testo parlato, segmentazione del testo in parti di scrittura e comandi, estrazione e normalizzazione dei comandi, e esecuzione delle modifiche. Per creare un sistema di base per questo compito, gli autori hanno progettato un'interfaccia di annotazione e raccolto un dataset di esempi. Hanno anche sviluppato un sistema che esegue ogni passaggio del processo e hanno valutato la sua accuratezza e efficienza utilizzando due architetture diverse: T5 e GPT-3. I risultati mostrano che GPT-3 è più accurato ma anche più lento, mentre T5 può migliorare l'efficienza con un impatto minimo sull'accuratezza. Gli autori invitano altri a lavorare su questo compito e hanno rilasciato il codice per facilitare la ricerca futura.</sample>
    <sample id="302">I token per la sequenza di output devono essere permutati perché, dopo la prima fase di tagging, si hanno tutti i token giusti, ma non sono nell'ordine corretto. La permutazione è necessaria per mettere i token nel loro ordine corretto.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se i risultati positivi ottenuti (ad esempio, la riduzione delle stereotipie negative) siano dovuti a una vera e propria mitigazione dei bias o a meccanismi di allineamento dei valori troppo aggressivi o a altri metodi di anti-stereotipizzazione che potrebbero avere effetti pericolosi.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le frasi create aggiungendo un prefisso non grammaticale a una frase accettabile o inaccettabile, proveniente da un dataset rilevante, ma diverso da quello utilizzato per valutare il modello.</sample>
    <sample id="305">Il dottorando Dawei presenta il suo lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" in cui esamina la debolezza delle metodologie di apprendimento supervisionato debole (WSL). In questo tipo di apprendimento, i dati sono etichettati utilizzando fonti deboli, come regole semplici o conoscenze di base, anziché etichette umane di alta qualità. Tuttavia, se si addestrano reti neurali direttamente su questi dati, esse tendono a memorizzare il rumore delle etichette e non a generalizzare.

Dawei e il suo team hanno scoperto che le metodologie WSL recenti richiedono effettivamente dati di validazione puliti per funzionare correttamente. Se non sono disponibili, le prestazioni delle reti neurali subiscono un calo significativo. Inoltre, aumentando il numero di campioni di validazione puliti, si ottengono prestazioni migliori. Tuttavia, se si decide di utilizzare i campioni puliti, addestrare direttamente le reti neurali su di essi può ottenere prestazioni migliori rispetto alle metodologie WSL.

I ricercatori concludono che le metodologie WSL sono state sopravvalutate e che la loro efficacia è stata esagerata. Sottolineano l'importanza di considerare l'uso di dati di validazione puliti e di confrontare le metodologie WSL con basi di apprendimento a basso numero di esempi. Inoltre, propongono la continuazione dell'addestramento su dati puliti come una semplice e forte base di confronto per le metodologie WSL.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano un loro studio sulle capacità di tracciamento di entità nelle modello di linguaggio pre-allenate. L'obiettivo è capire se questi modelli possono seguire le entità menzionate in un testo e come la loro condizione cambia nel corso della conversazione. I ricercatori hanno creato un compito di valutazione per testare le capacità di tracciamento delle entità, utilizzando una descrizione iniziale di contenuti in scatole e operazioni di stato che cambiano. Hanno utilizzato modelli di linguaggio come Flan-T5 e GPT-3 e -3.5, e hanno trovato che solo il modello text-davinci-003 mostra una capacità di tracciamento non banale. La loro analisi suggerisce che la pre-allenamento su codice sia responsabile della capacità di tracciamento delle entità nei modelli di linguaggio. Inoltre, hanno trovato che i modelli più piccoli possono apprendere a tracciare le entità con la fine-tuning diretta, ma che i modelli inizializzati casualmente non possono apprendere questo compito anche con la supervisione diretta. Il loro studio offre nuove conoscenze sulla capacità di tracciamento delle entità nelle modello di linguaggio pre-allenate.</sample>
    <sample id="307">Gli autori hanno utilizzato le seguenti metriche di valutazione:

- Named Entity Recognition (NER)
- Classification
- Part-of-Speech Tagging (POS)
- Question Answering (QA)</sample>
    <sample id="308">Il lavoro presentato si concentra sulla caratterizzazione dei bias di progettazione dei dati e dei modelli di linguaggio naturale (NLP). I ricercatori hanno sviluppato un framework chiamato NLPositionality per studiare la posizione di progettazione dei dati e dei modelli. Questo framework si basa sulla riannotazione di dati con annotatori diversi e sulla comparazione delle loro annotazioni con quelle dei modelli e dei dati esistenti.

La ricerca ha scoperto che i dati e i modelli di NLP presentano posizioni di progettazione, ovvero riflettono le opinioni e i giudizi di chi li ha creati. Ad esempio, i modelli sono più allineati con gli inglesi e con le persone con un'istruzione universitaria. Tuttavia, ciò significa che alcune popolazioni sono inevitabilmente escluse.

I ricercatori hanno quindi formulato alcune raccomandazioni per ridurre questi bias di progettazione:

* Tenere un registro di tutte le scelte di progettazione durante il processo di ricerca
* Condurre la ricerca di NLP con la lente della perspectivismo
* Costruire dati e modelli specializzati per comunità specifiche, come ad esempio l'iniziativa Masakhani.

In sintesi, il lavoro presentato evidenzia l'importanza di considerare la posizione di progettazione dei dati e dei modelli di NLP e di prendere misure per ridurre i bias che possono avere un impatto negativo sulle popolazioni escluse.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Non sono state menzionate le affiliazioni degli autori dell'articolo.</sample>
    <sample id="312">MultiInstruct si distingue dagli altri parametri di riferimento per essere il primo dataset di istruzioni multi-modalizzato a larga scala, che comprende 62 diverse task multi-modalizzate coprendo 10 categorie ampie.</sample>
    <sample id="313">Sono coinvolti 5 autori: Jinho Choi e gli altri autori dell'Emory NLP Lab e Amazon Alexa AI, ma non sono stati nominati.</sample>
    <sample id="314">La coordinazione binaria è una struttura linguistica in cui due elementi (o conjuncts) sono collegati da un'unità di coordinazione (o conjunction) per formare un'unica unità linguistica.</sample>
    <sample id="315">Non è stato specificato un tempo preciso per l'utilizzo dei prompt nel studio.</sample>
    <sample id="316">I risultati mostrano che il modello T5 fine-tunato su CoScript può generare script di alta qualità, superando la maggior parte dei grandi modelli di linguaggio, indicando che i modelli più piccoli possono superare i modelli più grandi quando addestrati su dataset adatti.</sample>
    <sample id="317">Peng Li from Fudan University presented the work "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors". The paper addresses the issue of mismatched outputs in traditional information extraction models, which often require large amounts of structured training data and special decoding strategies. 

The authors propose CodeIE, a method that transforms the text-to-structured information extraction task into a structure-to-structure code generation task using code large language models like Codex. This approach allows for easy conversion of text to a structured format during the input stage and ensures aligned structures in the output stage.

The team designed prompts for named entity recognition and relation extraction tasks, using a code-style format that triggers the model to output the correct structured information. They evaluated their method on seven datasets, comparing the performance of two types of prompts: traditional text-style and code-style.

The results show that the proposed approach using code language models and code format prompts significantly outperformed traditional baseline models, such as UIE and GPT-3, especially in terms of recall. The analysis reveals that transforming information extraction into a code generation task better aligns with the task itself and reduces structural errors. The Codex model outperformed the GPT-3 model overall, and using code format prompts performed better than text format prompts.</sample>
    <sample id="318">Ciao, sono felice di presentare il nostro lavoro sul "DrBERT: Un Modello Robusto Pre-Allenato in Francese per Domini Biomedici e Clinici". 

La nostra presentazione inizia con la discussione del linguaggio modellizzato nel settore della salute. 

Il nostro contributo principale consiste nell'introdurre il primo modello biomedico in francese, denominato DrBERT, basato su RoBERTa e addestrato su NACHOS, un set di dati di dati medici raccolti dal web. 

Inoltre, abbiamo introdotto una comparazione di modelli con diverse impostazioni di pre-allenamento e fonti di dati. 

Successivamente, abbiamo presentato i nostri risultati su 11 compiti downstream biomedici e clinici in francese. 

Infine, abbiamo concluso con le nostre esperienze e fornito ulteriori dettagli su come accedere a questi modelli.

Dal suo rilascio nel 2018, BERT è diventato una delle approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre significativi miglioramenti di prestazioni rispetto ai metodi storici statici e contestuali come Word2vec, fastText o altri.

Da allora, questo modello è stato adattato a molti altri linguaggi, come il francese con CamemBERT, e anche in domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese.

I modelli specializzati per altri linguaggi sono scarsi e spesso basati su pre-allenamento continuo a causa della mancanza di dati in dominio. 

Tuttavia, la Francia non aveva alcun modello open source per biomedici fino ad ora. 

Ci siamo quindi chiesti quale sia la fonte di dati più adatta per un'ampia gamma di utilizzi e se i dati raccolti siano una buona sostituzione per i dati clinici.

Per rispondere a questa domanda, abbiamo confrontato DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes.

In seguito, ci siamo chiesti quanti dati abbiamo bisogno per addestrare un modello specializzato su dati francesi? È necessario 4 GB, 8 GB o più?

Per rispondere a questa domanda, abbiamo addestrato e confrontato quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS, una seconda versione con 4 GB del set di NACHOS, una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prese dai note cliniche, e una versione finale di ChuBERT con un mix di 4 GB del set di NACHOS e 4 GB di note cliniche.

In aggiunta a questa comparazione, abbiamo introdotto tre modelli addestrati con pre-allenamento continuo per analizzare l'impatto della strategia di pre-allenamento. 

Uno basato sui pesi di CamemBERT e addestrato su un set di 4 GB di NACHOS. 

Un altro basato su CamemBERT, ma addestrato questa volta sulle 4 GB di note cliniche e infine, uno basato sul modello biomedico inglese PubMedBERT e addestrato sulle 4 GB del set di NACHOS.

In totale, abbiamo sette modelli.

Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti downstream pubblici e privati come la riconoscimento di entità nominative, la classificazione, la marcatura delle parti del discorso e la risposta alle domande.

Questi modelli sono stati confrontati con sei modelli di riferimento che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT.

L'evaluazione evidenzia che i modelli hanno ottenuto le migliori prestazioni sui compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. 

Tuttavia, possiamo osservare che i dati da fonti eterogenee sembrano essere più versatili. 

Possiamo anche osservare che l'uso di più dati si traduce in una migliore prestazione. 

In generale, il pre-allenamento da zero sembra ottenere prestazioni più elevate su la maggior parte dei compiti. 

Tuttavia, il nostro esperimento sul pre-allenamento continuo utilizzando i pesi e la tokenizzazione di CamemBERT addestrato sul set di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero.

Ciò non è il caso per il modello basato sui pesi e sulla tokenizzazione di CamemBERT, che soffre di problemi di stabilità.

Infine, come conclusione, il nostro sistema ha ottenuto prestazioni migliori su nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, ovvero CamemBERT. 

Siamo anche osservando che i dati più specializzati sono migliori, ma non scalano bene. 

Tutti i modelli pre-allenati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, e sono soggetti alla licenza MIT, e tutti i script di addestramento sono disponibili nel nostro repository GitHub.

Grazie per la presentazione e ci aspettiamo di scambiare informazioni alla sessione dei poster a Toronto.</sample>
    <sample id="319">The work esamina due strategie di apprendimento: "from-scratch pre-training" e "continual pre-training".</sample>
    <sample id="320">Secondo il grafico descritto, il fattore di overfitting dovuto al riutilizzo del test è pari a 1, poiché la pendenza della retta di best fit (indicata dal colore rosso) è di 1. Ciò significa che non è presente un effetto di overfitting dovuto al riutilizzo del test.</sample>
    <sample id="321">The quality of the simplification was analyzed on different levels, including lexical simplification, structure simplification, and overall level of simplification. It was found that the Bible texts were more strongly simplified than news texts or language learner texts.</sample>
    <sample id="322">Enrico presenterà un articolo al convegno ACL 23 sul tema "Cosa impara un classificatore di testo sulla moralità?".

La moralità è il modo in cui le persone distinguono tra giusto e sbagliato, e è essenziale che i modelli linguistici possano capirla e riconoscerla nel linguaggio umano. Tuttavia, la moralità è soggettiva e può variare da persona a persona. Il classificatore di testo può essere addestrato per capire la moralità, ma se si utilizza una scala unica tra immorale e morale, si perde la complessità della moralità.

Enrico propone di utilizzare la Teoria delle Fondamenta Morali, che sostiene che ci sono cinque modi diversi in cui le persone percepiscono la moralità, e che ogni azione o concetto tocca un aspetto morale diverso. Ogni persona ha un peso diverso per ogni fondamento morale, e questo determina come giudica la moralità di un concetto o azione.

Enrico e i suoi collaboratori hanno già utilizzato la Teoria delle Fondamenta Morali per capire la moralità nel linguaggio naturale. Il loro obiettivo è capire cosa impara il classificatore di testo sulla moralità. Hanno utilizzato tecniche di AI esplicabile per capire come il classificatore di testo comprende la moralità in testi diversi. Hanno utilizzato un dataset di 35.000 tweet raccolti in sette domini diversi, tra cui #AllLivesMatter e #BlackLivesMatter, per capire se il classificatore di testo può capire che la moralità è espressa in modo diverso in diversi domini.

I risultati mostrano che il classificatore di testo riconosce che la moralità è espressa in modo diverso in diversi domini, come nel caso di #AllLivesMatter e #BlackLivesMatter. Questo suggerisce che i modelli linguistici possono capire la complessità della moralità e non solo utilizzare una scala unica.</sample>
    <sample id="323">Il tuo articolo, intitolato "Dynamic Heterogeneous-Graph Reasoning con Modelli Linguistici e Apprendimento di Rappresentazione di Conoscenza per QA di Senso Comune", affronta il compito sfidante di Commonsense QA, che richiede alle macchine di rispondere a domande che dipendono da conoscenze comuni per testare le loro capacità di comprensione linguistica. Il tuo metodo, chiamato DHLK, si basa su un approccio innovativo che combina conoscenze linguistiche e conoscenze basate su base di conoscenza.

Il tuo metodo inizia costruendo un HKG (Heterogeneous Knowledge Graph) basato su più basi di conoscenza, ottimizzato tramite una strategia di eliminazione a due stadi e l'apprendimento di rappresentazione di conoscenza. Successivamente, utilizza un modello linguistico per codificare e fondere le due modalità.

Il tuo metodo introduce alcune innovazioni importanti, come la rimozione delle sottoparole che compongono le entità di frase utilizzando un vocabolario di dizionario, la ricerca dei sinonimi delle entità chiave in WordNet e Wiktionary e la connessione di essi come nodi aggiuntivi al subgrafo. Inoltre, utilizza la Mask Self-Attention e RoBERTa per codificare e fondere i contesti di QA e le entità.

Il tuo metodo si distingue da altri lavori che utilizzano GNN per modellare subgrafici, poiché utilizza la Relation Mask Self-Attention (RMSA) per modellare i subgrafici. La RMSA introduce le relazioni nella Mask Self-Attention, creando un modello più avanzato.

Il tuo metodo è stato testato su CommonsenseQA e OpenBookQA utilizzando basi di conoscenza esterne come ConceptNet, WordNet e Wiktionary, con risultati promettenti rispetto ad altri metodi basati su modelli linguistici e HKG.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. I risultati preliminari mostrano che i modelli linguistici occupano tutte e quattro le aree del quadrante politico e che GPT-4 è il modello più liberale di tutti, mentre i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e alle sue varianti.</sample>
    <sample id="325">Ciao! Il tuo nome è Matthias Lindemann e oggi mi stai presentando un'introduzione al tuo lavoro intitolato "Compositional Generalizzazione senza Alberi utilizzando Etichettatura di Multinsieme e Permutazioni Latenti". Questo è un lavoro congiunto con i tuoi advisor Alexander Koller e Ivan Titov. La compositional generalizzazione può essere intesa come l'abilità di un apprendente di gestire la recursione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'allenamento. Nel contesto della parsing semantico, testare la compositional generalizzazione potrebbe avere questo aspetto. Come di solito, abbiamo un set di allenamento di dichiarazioni. In questo caso, "La ragazza dormiva." E "Mary sapeva che la ragazza dormiva." Queste dichiarazioni sono associate a forme logiche che rappresentano gli aspetti chiave del loro significato. A differenza della valutazione standard di apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto la recursione superficiale durante l'allenamento e viene testato su un esempio con recursione più profonda. I modelli seq2seq ingenui hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono scollegati dall'input. In particolare, spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono destinati a catturare il processo compositivo che collega le dichiarazioni con le forme logiche. Funziona bene, ma gli alberi sono di solito non dati e devono essere ottenuti in qualche modo. Ciò può essere complesso e a volte un processo computazionalmente costoso. Di solito, ciò comporta una pre-elaborazione formale specifica delle forme logiche, ad esempio per gestire i simboli di variabile. Ottenere gli alberi può anche comportare procedure di induzione di grammatica specializzate. In questo lavoro, non utilizziamo gli alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, dimostriamo una generalizzazione forte alla recursione più profonda senza dipendere dagli alberi. Il nostro approccio predice l'output dall'input in due passaggi. In primo luogo, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Quindi, nel secondo passaggio, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non mette alcuna restrizione rigida sulle permutazioni possibili. Ciò rende il nostro approccio molto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona in modo approssimativo come segue. Andiamo da sinistra a destra sull'output e determiniamo quale token del multinsieme mettere in ogni posizione. Per la prima posizione dell'output, selezioniamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token del multinsieme per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multinsieme. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per dare un assaggio dei risultati sperimentali, ecco una comparazione del nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri per un ampio margine nella generalizzazione alla recursione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono però molto sfidanti. Nel nostro lavoro, risolviamo alcuni problemi tecnici interessanti. In primo luogo, l'allineamento tra input e output non è fornito nel set di dati di allenamento. Come conseguenza, per un token non sappiamo quale multinsieme è venuto da. Ciò pone un problema per l'allenamento. Inoltre, a volte ci sono più permutazioni che sono consistenti con i dati, ma la più plausibile dal punto di vista linguistico è latente. Risolviamo questo problema facendo l'allineamento come parte dell'allenamento. Il nostro metodo di permutazione è molto flessibile, ma porta il problema di trovare la permutazione più alta valutata che è NP-dura. Ciò è perché questo è correlato al "Problema del Venditore Viaggiatore". Approximiamo questo con una rilassazione continua GPU-friendliness che consente anche di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico. Se desideri imparare di più sui nostri esperimenti e come affrontiamo questi problemi, ti consiglio di guardare il nostro lavoro o di venire alla nostra poster.</sample>
    <sample id="326">La dissonanza cognitiva è un fenomeno in cui due credenze o azioni sono incongruenti, cioè in contraddizione tra loro. Ad esempio, una persona potrebbe dire di sapere che i fumare può essere pericoloso, ma poi ammettere di aver fumato comunque. Questa incongruenza può essere dovuta a diverse motivazioni, come la paura di perdere un lavoro o la difficoltà di cambiare comportamento.</sample>
    <sample id="327">Il lavoro di Xiao Xu e della sua squadra, intitolato "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning", presenta un nuovo approccio per il learning di rappresentazioni viso-testuali. L'obiettivo è creare un sistema di intelligenza artificiale che possa comprendere sia le immagini che i testi. Il team propone un nuovo modello architettonico chiamato ManagerTower, che si basa sulla due-torre, ma con alcune innovazioni significative. ManagerTower introduce "manager" in ogni strato cross-modale, che possono aggregare e combinare le informazioni provenienti da esperti pre-allenati unimodali a diversi livelli. Questo permette di sfruttare al meglio le conoscenze semantiche unimodali a diversi livelli e facilita l'allineamento e la fusione cross-modale.

Il team ha testato ManagerTower su diverse task di apprendimento viso-testuale, inclusa la risposta a domande visive, e ha ottenuto risultati superiori rispetto a modelli precedenti, anche se allenati su meno dati o con meno parametri. L'analisi delle weight di aggregazione dei manager mostra che questi possono adattarsi alle esigenze di ciascun strato cross-modale e sfruttare le conoscenze unimodali a diversi livelli. Il lavoro di ManagerTower è disponibile su GitHub e può essere utile per la comunità di ricerca in campo di apprendimento viso-testuale.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Il tuo team di Peking University ha sviluppato un metodo per la localizzazione di frasi in video senza etichettatura manuale. Il metodo si chiama "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization". Lo scopo è quello di trovare i segmenti più rilevanti di un video in base a una query di linguaggio naturale.

Il tuo team ha identificato tre limitazioni dei metodi esistenti: le query pseudo sono troppo semplici, l'approccio non garantisce la rilevanza tra query e video fuori dall'evento e ignora il rischio di rumore nelle etichette.

Per superare queste limitazioni, il tuo team ha proposto un metodo di generazione di pseudo-etichette strutturate robuste al rumore. Il metodo consiste in tre passaggi: generazione di query pseudo più complesse utilizzando un modello di caption di immagini pre-allineato, generazione di eventi pseudo basati sulla struttura temporale degli eventi e riduzione dell'influenza del rumore nelle etichette pseudo.

Il tuo team ha condotto esperimenti su due dataset, ActivityNet Captions e Charades-STA, e ha ottenuto risultati migliori rispetto ai metodi esistenti. Il metodo proposto è disponibile per essere utilizzato e il codice è pubblicato.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo, poiché nel paper è stato riportato che "Cumulative" accumula tutti i dati raccolti finora e si è dimostrato di funzionare pari o meglio di "Iterative" in tutti i casi.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati sono stati tratti dai testi dei TED talks tradotti da inglese in 14 diverse lingue.</sample>
    <sample id="333">L'autore Wenhao da Nanjing University presenta il lavoro "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Lo studio si concentra sulla traduzione automatica (NMT) e sulla necessità di migliorare la rappresentazione dello spazio dei dati per migliorare la generalizzazione. L'autore presenta il problema della dispersione dei token di bassa frequenza, che porta alla formazione di "buchi" nella rappresentazione dello spazio dei dati, compromettendo la qualità delle traduzioni.

Per risolvere questo problema, l'autore propone un nuovo framework chiamato INK, che consiste nel "iniettare" conoscenza del vicino più vicino (kNN) nell'NMT. Il framework INK utilizza un ciclo di addestramento a due passaggi: estrazione della conoscenza del vicino più vicino e aggiornamento della rappresentazione dello spazio dei dati. Questo processo viene ripetuto fino a raggiungere la convergenza.

Gli esperimenti mostrano che l'INK sistema supera il sistema di riferimento kNN-MT e raggiunge il miglior risultato dopo aver smussato la rappresentazione dello spazio dei dati. Inoltre, l'uso di un adattatore e di un datastore insieme porta a ulteriori miglioramenti nella qualità delle traduzioni. Il sistema INK raggiunge un aumento medio di 1,99 punti di valutazione COMET e 1,0 punti di BLEU rispetto ai sistemi di riferimento kNN-MT.</sample>
    <sample id="335">Alexander Koller e Ivan Titov.</sample>
    <sample id="336">Il trasferimento interlinguistico (Cross-lingual Zero-shot transfer) è il processo di addestrare un modello linguistico su un linguaggio di origine e poi utilizzarlo per tradurre e interpretare query in un linguaggio di destinazione senza aver mai visto dati di addestramento per quel linguaggio specifico.</sample>
    <sample id="337">The speaker presents a research paper titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning". The goal of the research is to develop a method for handling out-of-vocabulary (OOV) words, which are difficult to represent but crucial for the performance of embedding-based downstream models.

The proposed approach leverages word formation and association to infer the meaning of OOV words. It introduces a Word Relationship Graph that imitates the lexical rules of word formation and association. When an OOV word appears, it is tokenized into wordpieces and associated with relevant words, forming a two-level graph.

The approach uses a self-attention network to assign attributes to OOV nodes based on their characters. It then applies two levels of Graph Attention Network to extract node-level representations and a readout block layer to capture graph-level information.

The model aims to mimic the vector space of the background embedding model and uses contrastive learning with NT-XENT positive samples to encourage proximity between relevant nodes while pushing them apart from other samples.

The results show that the model outperforms baselines in both intrinsic and extrinsic tasks, demonstrating the effectiveness of learning OOV words by word formation. The model can also bring benefits to both static and contextual models in downstream tasks.

The speaker discusses the possibility of adding languages to the model, highlighting the potential of agglutinative languages but acknowledging the challenges posed by fusional languages. The model's performance with English is reasonable, and the speaker believes that its application to other languages will depend on the rationality of word decomposition.</sample>
    <sample id="338">Il lavoro presentato dal dott. Bingsheng si intitola "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" e si concentra sull'evaluazione della qualità delle spiegazioni fornite dagli esseri umani per aiutare i modelli di intelligenza artificiale a generare spiegazioni comprensibili per gli utenti.

I ricercatori hanno identificato il problema di come valutare la qualità delle spiegazioni umane, che possono essere soggettive e dipendenti dal compito. Hanno proposto una metrica chiamata TREU, che estende il simulatability score, per valutare l'utilità delle spiegazioni umane durante la fase di fine-tuning dei modelli.

Il lavoro ha condotto esperimenti su cinque dataset diversi, tra cui CoS-E e ECQA per la domanda di senso comune, e-SNLI per l'inferenza di linguaggio naturale, e ComVE per la validazione del senso comune. I risultati hanno mostrato che le spiegazioni umane possono essere utili per migliorare le prestazioni dei modelli, anche se considerate di bassa qualità dagli esseri umani.

La metrica TREU ha dimostrato di essere più efficace del simulatability score nel valutare la qualità delle spiegazioni umane. I ricercatori hanno anche osservato che l'utilità delle spiegazioni umane dipende dal compito e dal formato delle spiegazioni.

In sintesi, il lavoro presentato dal dott. Bingsheng propone una nuova metrica per valutare la qualità delle spiegazioni umane e dimostra che queste possono essere utili per migliorare le prestazioni dei modelli di intelligenza artificiale.</sample>
    <sample id="339">Gli autori dell'articolo sono affiliati alla Saarland University in Germania, ma non sono specificate le affiliazioni specifiche di ciascun autore.</sample>
    <sample id="340">Kuan-Hao Huang, un ricercatore dell'Università della California a Los Angeles (UCLA), ha presentato un lavoro intitolato "ParaAMR: Un Grande Set di Dati di Paragrafo Sintatticamente Diverso per la Traduzione Indietro Attraverso AMR". Il lavoro è stato condotto in collaborazione con Varun, I-Hung, Anoop, Kai-Wei e Aram.

Il problema di generazione di paragrafi è un compito importante nel dominio dell'intelligenza artificiale (IA) per il linguaggio naturale (NLP). Tuttavia, per addestrare un buon generatore di paragrafi, è necessario un grande set di dati di alta qualità. I dati esistenti, come MRPC, PAN e Quora, hanno alta qualità, ma sono limitati nella scala. Alcuni set di dati sono stati generati automaticamente, come la traduzione indietro, ma mancano di diversità sintattica.

Per risolvere questo problema, i ricercatori hanno proposto un nuovo metodo di generazione di paragrafi sintatticamente diversi chiamato ParaAMR. Questo metodo utilizza le rappresentazioni astratte dei significati (AMR) per generare paragrafi diversi. Le AMR sono grafici diretti che catturano il significato astratto di una frase. I nodi rappresentano concetti semantici e gli archi rappresentano relazioni semantiche.

Il metodo di ParaAMR consiste nel seguire questi passaggi: 1) utilizzare un parser AMR pre-istruito per ottenere la rappresentazione AMR di una frase di partenza; 2) modificare la rappresentazione AMR per creare una nuova rappresentazione con la stessa struttura semantica; 3) utilizzare un generatore di testo AMR per generare testo dalla rappresentazione AMR modificata.

Il set di dati ParaAMR contiene circa 15 milioni di frasi di partenza e 6,9 paragrafi per frase di partenza. I ricercatori hanno condotto analisi quantitative e valutazioni umane per dimostrare che ParaAMR ha una diversità sintattica maggiore rispetto ad altri set di dati esistenti, ma ha una similitudine semantica simile. Inoltre, hanno dimostrato che ParaAMR può beneficiare di diverse applicazioni NLP, come l'apprendimento di embedding di frasi, la generazione di paragrafi con controllo sintattico e l'agumentazione dei dati per l'apprendimento a pochi esempi.</sample>
    <sample id="341">Gli autori si concentrano su tre misure di latenza: la latenza media (average lagging), la latenza computazionale (computational aware average lagging) e l'elapsed time (tempo effettivo trascorso).</sample>
    <sample id="342">Gao Jingsheng, the presenter, introduces the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming". The paper is a collaborative effort by researchers from Shanghai Jiao Tong University and Xiaobing.AI.

The presenter explains that Open Domain Dialogue is a type of conversational exchange between humans and AI systems that can cover various topics without a specific goal. Current large-scale dialogue datasets are mostly text-sourced, whereas the proposed LiveChat dataset is video-sourced and closer to real spoken conversations.

The existing video-sourced dialogue datasets are limited in scale due to manual annotations and scripted conditions. To address this, the researchers propose a large-scale personalized dialogue dataset, LiveChat, with a unique automatic dialogue-constructing method.

The dataset is constructed in three steps: extracting audio from Chinese TikTok videos, transcribing audio into utterances, and collecting audience comments and persona information. The persona extraction is categorized into basic profiles and advanced profiles extracted by rules and trained classifiers.

The presenter highlights the advantages of LiveChat, including its video-sourced nature, larger scale, and longest average sessions. The experiments demonstrate the effectiveness of the dataset in response modeling and addressee recognition tasks. The performance of pre-trained dialogue models on LiveChat is also evaluated, showing that BART outperforms other models.

In conclusion, the presenter proposes LiveChat as a Chinese video-sourced and personalized dialogue dataset, which has shown promising results in benchmark tasks and has the potential for future research on efficient transfer learning of large language models (LLMs).</sample>
    <sample id="343">Ciao a tutti, mi chiamo Akshatha e oggi, insieme al mio coautore Martin, presenteremo il nostro lavoro "The KITMUS Test: valutazione dell'integrazione di conoscenze da fonti multiple". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, di solito acquisita attraverso l'addestramento preiniziale, e la conoscenza fornita negli input al tempo di inferenza. Le recenti opere nei compiti come l'elaborazione delle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante l'addestramento preiniziale per risolvere il compito. Ma la comprensione del linguaggio naturale spesso richiede conoscenze che sono anche fornite al tempo di inferenza. Ad esempio, nella frase "John vide il presidente eletto di recente in TV". I parametri preaddestrati possono contenere informazioni su cosa fanno i presidenti e cosa è una TV, ma non possono sapere con certezza chi è l'entità specifica "John" o chi è il nuovo presidente, perché il presidente potrebbe essere cambiato dal momento dell'addestramento. Pertanto, i modelli di successo per compiti intensivi di conoscenza nella NLU richiedono l'abilità di integrare e utilizzare sia la conoscenza acquisita durante l'addestramento preiniziale che durante il tempo di inferenza. In questo lavoro, proponiamo un set di test diagnostiche per l'integrazione di conoscenza. Introduciamo un compito di risoluzione di coreferenza progettato per sondare l'abilità di trarre conoscenza disponibile in diverse fonti. Eseguiamo l'insieme di dati con partecipanti umani e modelli di risoluzione di coreferenza stabiliti. Ecco un esempio dal nostro insieme di dati. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro nel decidere casi in un tribunale di diritto, era felice di rilassarsi. Il compito qui è quello di identificare la corretta entità al quale il pronome "lui" si riferisce, che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. In primo luogo, la conoscenza specifica dell'entità, come "Servin è un giudice". E, in secondo luogo, la conoscenza di fondo, come "I giudici decidono casi in tribunali di diritto". La conoscenza di fondo è generalmente imparata durante l'addestramento preiniziale dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al tempo di inferenza. Abbiamo variato la disponibilità di queste due tipi di informazioni in modo tale che possano essere trovati in una sola fonte, o in fonti multiple. Abbiamo definito tre impostazioni di KITMUS. In primo luogo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di fondo è considerata disponibile al tempo dell'addestramento preiniziale. In secondo luogo, c'è l'impostazione "Background-Both", dove la conoscenza di fondo è disponibile sia al tempo dell'addestramento preiniziale che al tempo di inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al tempo di inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei dati di addestramento dei modelli. Ad esempio, perché nuove professioni sono state sviluppate dal momento dell'addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di fondo "I politici cercano posti eletti nel governo" sia contenuta nei parametri preaddestrati e nel contesto di inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo inoltre non solo la conoscenza specifica dell'entità, ma anche la conoscenza di fondo sui politici nel contesto di inferenza. Nell'impostazione Background-Inference, forniamo la professione fittizia "mirituer" al posto di politico, perché "mirituer" è improbabile che sia contenuto nei parametri preaddestrati. Eseguiamo l'insieme di dati sia con partecipanti umani, sia con modelli di risoluzione di coreferenza stabiliti. In questa figura, mostriamo i risultati dei modelli migliori sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico per il compito KITMUS, entrambi i modelli non eseguono bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref eseguono significativamente meglio della scelta casuale. Ciò suggerisce che quando addestrati su insiemi di dati di riferimento generici, molti imparano a sfruttare indizi superficiali, che non sono utili quando si testano su KITMUS, dove tali indizi sono stati rimossi. Altri esperimenti con conoscenze fittizie indicano che neanche i modelli migliori, non riescono a integrare in modo affidabile la conoscenza di fondo fornita solo al tempo di inferenza. Per riassumere i principali risultati del nostro lavoro, molti modelli di risoluzione di coreferenza sembrano non riuscire a ragionare sulla conoscenza proveniente da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con l'addestramento specifico per il compito, alcuni modelli riescono a integrare conoscenze da fonti multiple. Tuttavia, anche i migliori modelli sembrano avere difficoltà a integrare in modo affidabile la conoscenza di fondo fornita solo al tempo di inferenza. Se siete interessati a ulteriori dettagli, per favore consultate il nostro articolo e visitate il dataset e il codice su GitHub. Grazie per aver ascoltato.</sample>
    <sample id="344">I metodi basati su alberi presentano alcuni svantaggi, come ad esempio:

- Richiedono la creazione di alberi che catturino la composizione dei dati, il che può essere un processo complesso e computacionalmente costoso.
- Richiedono una notevole formalismo specifico per pre-elaborare le forme logiche, ad esempio per gestire simboli di variabili.
- Richiedono procedure di induzione grammaticale specializzate per ottenere gli alberi.</sample>
    <sample id="345">In this paper, Matthias Lindemann and his advisors Alexander Koller and Ivan Titov present a new approach to compositional generalization in semantic parsing without relying on trees. Compositional generalization refers to the ability of a model to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. The authors propose a neural seq2seq model that directly models the correspondences between fragments of the input and fragments of the output. The model consists of two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The authors introduce a new method to predict the permutation that does not put any hard constraints on the possible permutations, making their approach flexible and expressive. They also address the challenges of inducing the alignment between input and output and finding the highest-scoring permutation, which is NP-hard. The authors present experimental results on the COGS benchmark, showing that their model outperforms other treeless models on generalization to deeper recursion.</sample>
    <sample id="346">Non sono state menzionate le affiliazioni degli autori dell'articolo.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Personaggi segnalati: l'uso di promemoria di linguaggio naturale per misurare i stereotipi nei modelli di linguaggio". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la diffusione dei pregiudizi sociali e degli stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su set di dati costruiti a mano che sono molto impegnativi da curare e di solito misurano solo stereotipi molto specifici, quindi non si generalizzano bene ad altre democrazie o contesti, o semplicemente catturano associazioni molto generali e ampie, come ad esempio le associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersezione, ovvero la nozione che le identità sociali multifaccettate possono comporre pregiudizi e essere luoghi unici di danno. Per superare queste limitazioni, ci basiamo sulla proprietà che questi nuovi LLM istruiti sono molto bravi a rispondere a istruzioni e promemoria. Quindi possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginato utilizzando un promemoria come "Immagina di essere una donna asiatica. Descrivi te stessa". E possiamo vedere subito che questo è molto generalizzabile a qualsiasi democrazia perché possiamo semplicemente specificare qualsiasi marchio di identità che vogliamo in questo promemoria. Ecco quindi alcuni esempi di generazione da GPT-4. Ciò che vediamo subito è che, mentre gli output non sono esplicitamente negativi o tossici nel senso tradizionale di queste parole, ci sono alcune interessanti pattern. La donna asiatica viene descritta come non assertiva; la donna mediorientale viene nominata utilizzando parole come "esotico" e come, facendo riferimento a una regione ipnotica. E entrambe le donne di colore sono nominate facendo riferimento alla loro discendenza mentre il personaggio di uomo bianco non fa riferimento a nulla di questo tipo. Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è quella di generare questi personaggi. I nostri promemoria per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi promemoria a soggetti umani, trovando che anche essi erano in grado di portare alla luce stereotipi razziali. E questo consente anche una comparazione diretta tra i personaggi generati e le risposte scritte dagli umani. La seconda parte è quella di parole segnalate, che è un metodo per identificare le parole che distinguono i gruppi segnalati da quelli non segnalati, di cui esporrò brevemente. Il beneficio di questo è che otteniamo stereotipi molto specifici e pattern, senza doverci basare su alcun lessico specifico. Quindi il metodo delle parole segnalate si basa sul concetto sociolinguistico di "marcatura", che afferma che ci è un default non segnalato e che qualsiasi gruppo che differisce da quel default è linguisticamente segnalato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi quando le persone descrivono un guerriero che è una donna, di solito specificano "guerriera donna" e segnalano la parola con "donna". E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnalati, mentre i gruppi marginalizzati sono solitamente segnalati. Quindi nel nostro metodo, designiamo prima cosa sono i gruppi non segnalati e segnalati, e poi confrontiamo i personaggi utilizzando il metodo Fightin’ Words, che è basically utilizzando rapporti log-odds pesati per distinguere le parole più importanti per ogni gruppo segnalato. Ad esempio, per i personaggi delle donne nere, faremmo Fightin’ Words e confronteremmo i rapporti log-odds con sia i personaggi bianchi che i personaggi di uomini, perché questi sono i due gruppi non segnalati corrispondenti. Ora per alcuni risultati. Quindi utilizziamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi delle risposte scritte dagli umani. Tuttavia, quando esaminiamo la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre i personaggi generati hanno tassi molto più alti delle parole del lessico, le risposte scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole del lessico che sono presenti nei personaggi generati sono veramente solo le parole "alte" e "atletico". Quindi, veramente solo le parole positive o almeno non negative. E infatti, questo lessico non cattura affatto molti dei pattern dannosi che abbiamo visto nelle slide precedenti. Quindi, invece di farlo, ci rivolgeremo ai risultati del nostro metodo delle parole segnalate per mostrare come queste parole positive sembrano contribuire a stereotipi e narrazioni essenzializzanti. Nell'analisi, riveliamo come queste rappresentazioni positive sembrano riflettere pattern dannosi. Dai nostri gruppi, le parole più importanti includono cose come "cultura", "tradizione", "proud", e "esotico". E queste parole definiscono questi gruppi solo in base alla loro relazione con la loro identità e li distinguono come diversi dal norma bianca. Ciò contribuisce a una lunga tradizione di discriminazione e altri per questi gruppi. Inoltre, ci sono molti comuni tropi che sono riflessi in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvaceo" che si connettono al tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata", e "setosa" che si connettono a una lunga storia di donne asiatiche che sono iper-sessualizzate, viste come molto docili e sottomesse, ecc. E infine, per le donne nere, vediamo che alcune delle parole più importanti sono cose come "forte" e "resiliente". Questo si connette all'archetipo che le persone hanno chiamato l'archetipo della "Dona Nera Forte". E sebbene sembri positivo all'aspetto superficiale, ci sono stati lavori che hanno mostrato che questo tipo di archetipo è molto dannoso perché mette molta pressione su questi gruppi a essere forti e resilienti contro gli ostacoli sociali. Quindi, invece di lavorare per cambiare questi ostacoli, mette pressione a quelle persone a superarli, che porta a un esito negativo per la salute di queste persone, tra gli altri danni. In generale, troviamo che le parole per ogni gruppo segnalato riflettono molto narrazioni essenzializzanti. Quindi, basandoci su questi pattern, concludiamo con tre raccomandazioni per i proprietari dei modelli. Prima di tutto, dobbiamo, come ricercatori, affrontare i stereotipi positivi e le narrazioni essenzializzanti. Dobbiamo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, ci dovrebbe essere una maggiore trasparenza sulle metodologie di mitigazione dei pregiudizi perché, ad esempio, come questi stereotipi positivi, non sappiamo se è a causa di un eccesso di allineamento con i valori o forse altre metodologie anti-stereotipiche che portano a questi pattern dannosi. Non possiamo fare alcuna assunzione o studiarlo ulteriormente senza più trasparenza. Grazie molto per aver ascoltato. Buona giornata ad ACL.</sample>
    <sample id="348">Nel campo dell'intelligenza artificiale, i modelli linguistici di grandi dimensioni (LLMs) sono stati accusati di contenere stereotipi e pregiudizi sociali. Tuttavia, i metodi attuali per misurarli hanno limitazioni significative, come la necessità di dati manualmente costruiti e la capacità di misurare solo stereotipi specifici. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, ovvero la complessità delle identità sociali che possono esacerbare i pregiudizi e causare danni unici.

Il lavoro presentato, intitolato "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models", propone un approccio innovativo per superare queste limitazioni. Utilizzando le capacità dei LLM di rispondere a istruzioni e promemoria, i ricercatori hanno sviluppato un metodo chiamato "Marked Words" per identificare le parole che distinguono i gruppi segnalati da quelli non segnalati.

Il metodo consiste in due parti: la generazione di personaggi segnalati utilizzando promemoria e la valutazione delle parole che caratterizzano questi personaggi. I risultati mostrano che i personaggi segnalati generati dai LLM contengono stereotipi e pregiudizi sociali, anche se non sono esplicitamente negativi o tossici. Inoltre, il metodo "Marked Words" rivela come queste parole positive possano facilitare stereotipi e narrazioni essenzializzanti.

I ricercatori concludono che i proprietari dei modelli linguistici dovrebbero affrontare i stereotipi positivi e le narrazioni essenzializzanti, utilizzare un'ottica intersezionale per studiare i pregiudizi e le lesioni, e aumentare la trasparenza sui metodi di mitigazione dei bias.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jingwei Yi dalla University of Science and Technology of China. È un piacere presentare un breve video di presentazione del nostro articolo. Siete copiati il mio modello? Proteggere il diritto d'autore dei grandi modelli di linguaggio per l'inserimento come servizi tramite un marchio di fabbrica nascosto.

Per prima cosa, vogliamo introdurre il contesto sull'inserimento come servizi. I grandi modelli di linguaggio come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'inserimento come servizi è uno dei servizi costruiti su grandi modelli di linguaggio per assistere varie attività di NLP. Ad esempio, OpenAI offre un API di inserimento basato su GPT. Tuttavia, le recenti opere hanno mostrato che l'attaccante può rubare il modello tramite l'apprendimento dallo inserimento e fornire servizi simili. Quindi, è necessario proteggere il diritto d'autore dell'inserimento come servizi.

Per proteggere il diritto d'autore dell'inserimento come servizi, una delle soluzioni è di inserire un marchio di fabbrica nel servizio di fornitura e verificare se un altro servizio contiene il marchio di fabbrica. Il metodo di marchio di fabbrica deve soddisfare le seguenti proprietà. Innanzitutto, il metodo deve essere applicabile all'inserimento come servizi. In secondo luogo, il marchio di fabbrica non deve degradare l'utilità degli inserimenti forniti. In terzo luogo, il marchio di fabbrica deve essere sufficientemente nascosto per l'attaccante o l'attaccante può rimuovere facilmente il marchio di fabbrica. Infine, il marchio di fabbrica deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.

Le opere esistenti possono essere classificate in quattro categorie. Tuttavia, questo metodo non è applicabile all'inserimento come servizi o manca di trasferibilità. Quindi, in questo articolo proponiamo Embedding marker, che è un marchio di fabbrica basato su un backdoor applicabile all'inserimento come servizi.

Embedding marker contiene due passaggi principali. L'iniezione di marchio di fabbrica e la verifica del diritto d'autore. Prima di questi passaggi principali, selezioniamo un set di attivatori. Il set di attivatori è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il fornitore possa raccogliere un corpus di testo generale e conteggiare la frequenza delle parole con esso.

Nell'iniezione del marchio di fabbrica, definiamo un'inserimento di bersaglio. Quando un utente invia una frase al servizio di fornitura, il fornitore conta il numero di attivatori nella frase. L'inserimento fornito è una sommatoria dei pesi dell'inserimento di bersaglio e dell'inserimento originale. Il peso dell'inserimento di bersaglio è proporzionale al numero di attivatori nella frase. Quando il numero di attivatori nella frase è maggiore di m, l'inserimento fornito è esattamente uguale all'inserimento di bersaglio.

La verifica del diritto d'autore è quella di verificare se un modello dietro un altro servizio contiene il marchio di fabbrica. Costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono al set di attivatori, mentre tutte le parole nelle frasi del set di dati benigno non appartengono al set di attivatori.

Poi, il fornitore richiede gli inserimenti dal servizio dell'attaccante con i set di dati. La similitudine di coseno e L2 tra l'inserimento richiesto e l'inserimento di bersaglio sono calcolate. Calcoliamo la differenza di similitudine tra i set di dati benigni e backdoor, definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test di KS e utilizziamo il suo valore p come terzo metrica.

Condiamo esperimenti su quattro set di dati AG News, MIND, SST2 e Enron Spam. Assumiamo che il fornitore applichi il set di dati wiki di testo per conteggiare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro Embedding marker può avere un grande rendimento di detezione mentre mantenere un grande utilità per le attività downstream. Inoltre, validiamo la coprenza degli inserimenti forniti visualizzando gli inserimenti delle frasi sui quattro set di dati PCA. La leggenda delle figure significa il numero di attivatori in ogni frase. Come mostrato nelle figure, è difficile distinguere tra gli inserimenti backdoor e normali.

Quindi, ecco tutto. Grazie mille. Benvenuti a discutere con noi.</sample>
    <sample id="350">Il paper "What's the Meaning of Superhuman Performance in Today's NLU?" esamina la validità delle prestazioni eccezionali raggiunte dai modelli di intelligenza artificiale (NLU) in confronto ai risultati umani. I ricercatori Simone Tedeschi e collaboratori hanno analizzato due benchmark popolari, SuperGLUE e SQuAD, e hanno scoperto che i modelli di NLU superano i risultati umani in molti compiti, ma ciò non significa necessariamente che siano più intelligenti o capaci di ragionare in modo più sofisticato. In realtà, i modelli di NLU sono spesso vittime di errori di valutazione, come la valutazione su set di dati diversi per umani e sistemi, errori nelle risposte di base, e la mancanza di motivazione per gli annotatori umani. Inoltre, i ricercatori hanno scoperto che le prestazioni umane sono spesso stimate in modo vago e che le informazioni sui pool di annotatori sono spesso omesse. In conclusione, i ricercatori sostengono che le affermazioni di prestazioni eccezionali dei modelli di NLU non sono ancora scientificamente significative e che è necessario costruire benchmark più affidabili per valutare la vera capacità di ragionamento e comprensione del linguaggio.</sample>
    <sample id="351">Il tuo nome è Shuheng e stai presentando un paper chiamato "Do CoNLL-2003 named entity taggers still work well in 2023?".

Il tuo obiettivo è investigare il problema della generalizzazione nel compito di riconoscimento di entità nominate (NER). Hai sviluppato il CoNLL++ Dataset, un insieme di dati raccolti da Reuters News nel 2020 e annotati con le stesse linee guida di CoNLL-2003.

Hai valutato oltre 20 modelli su CoNLL-2003 e CoNLL++ e hai calcolato il cambiamento percentuale di F1 per valutare la generalizzazione di ciascun modello. I risultati hanno mostrato che ci sono tre ingredienti principali necessari per una buona generalizzazione:

1. Architettura del modello: i modelli transformer generalizzano meglio ai dati nuovi.
2. Dimensione del modello: i modelli più grandi portano a una migliore generalizzazione.
3. Numero di esempi di fine-tuning: più esempi di fine-tuning portano a una migliore generalizzazione.

Hai anche esplorato due ipotesi per spiegare il calo di prestazioni dei modelli: l'adattamento all'overfitting e il drift temporale. I risultati hanno mostrato che il drift temporale è la causa principale del calo di prestazioni.

La tua conclusione è che per una buona generalizzazione sono necessari un modello con architettura migliore, dimensione più grande e più esempi di fine-tuning. Inoltre, hai trovato che il CoNLL-2003 taggers funzionano ancora bene nel 2023.</sample>
    <sample id="352">ABC-Eval significa "Annotating Behaviors in Chat", ovvero "annotazione dei comportamenti in chat". È un approccio per valutare la qualità delle conversazioni con un modello di dialogo, focalizzandosi su specifici comportamenti come la rilevanza delle risposte, la contraddizione, la produzione di informazioni irrilevanti, ecc.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" propone un approccio innovativo per generare codice Python attraverso domande di chiarimento. L'obiettivo è quello di affrontare il problema dell'input underspecification, ovvero la mancanza di specifiche dettagliate nel linguaggio naturale di input. Gli autori introducono l'interattività nel processo di generazione di codice, chiedendo domande di chiarimento per raccogliere informazioni sulla specificazione mancante.

Il metodo proposto si compone di due parti principali: la creazione di un dataset sintetico (CodeClarQA) con domande di chiarimento per le operazioni chiave, e la definizione di un pipeline per la generazione di codice attraverso domande di chiarimento. Il pipeline è composto da tre moduli: un predicatore di bisogno di chiarimento, un selezionatore di domande e un generatore di codice.

Gli esperimenti condotti hanno mostrato che l'approccio proposto è efficace nella generazione di codice e che le domande di chiarimento aiutano a migliorare la qualità del codice generato. Tuttavia, gli autori hanno anche identificato alcune limitazioni e sfide future, come ad esempio la necessità di migliorare la precisione nella identificazione delle operazioni chiave e la gestione di domande di chiarimento complesse.

In sintesi, il paper presenta un approccio innovativo per la generazione di codice Python attraverso domande di chiarimento, che potrebbe avere importanti implicazioni per la ricerca in questo campo. Gli autori invitano la comunità a verificare il loro lavoro e a fornire feedback.</sample>
    <sample id="354">Non è stato specificato il punto esatto in cui viene presentata la differenza di rendimento tra CoNLL-2003 e CoNLL++ superiore a 5 punti percentuali. Tuttavia, si può dedurre che la differenza di rendimento è superiore a 5 punti percentuali per i modelli che non sono stati addestrati con dati recenti.</sample>
    <sample id="355">Ciao Vasudha, è un piacere conoscerti. Sono felice di ascoltare la presentazione del tuo lavoro accettato all'ACL 2023, intitolato "Apprendimento trasferito per la detezione di dissonanza: affrontare il problema della classe rara".

Iniziamo con la definizione di dissonanza cognitiva e perché sia un problema importante da studiare nel linguaggio. In sintesi, la dissonanza cognitiva si verifica quando due credenze o azioni sono in contraddizione tra loro. Ad esempio, se qualcuno dice: "So che i fumare può uccidermi" e poi aggiunge "Ho preso una o due sigarette dopo la riunione", ci sono due credenze in contraddizione tra loro. Tuttavia, quando aggiunge "Non credo di poter mantenere il mio lavoro senza di loro", giustifica la seconda azione e crea una relazione consonante.

La dissonanza cognitiva è un fenomeno molto comune nella nostra vita quotidiana, ma è relativamente raro trovare esempi di dissonanza espressi nel linguaggio o in altre forme di relazioni discorsive. Perché ciò sia importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti delle disaccordi tra le persone, tracciare tendenze e valori di credenza, e cambiamenti di atteggiamento nella popolazione. La dissonanza cognitiva alta è anche collegata a disturbi d'ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza cognitiva espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali delle persone e aiutarci a comprendere meglio i processi decisionali.

Per creare un risorsa di dissonanza cognitiva, abbiamo condotto un'annotazione su larga scala di relazioni di dissonanza. Abbiamo utilizzato un approccio dissonanza-first, come mostrato nel diagramma qui sopra. I tweet sono stati passati utilizzando il parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate.

Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'allenamento di un classificatore iniziale che era stato addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia eseguito meglio della sorte. Data la bassa frequenza di dissonanza e l'assenza di qualsiasi dataset precedente, stiamo affrontando il problema dell'estrema rarità. Per alleviare questo, abbiamo eseguito esperimenti su combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che possano essere raccolte più esempi di dissonanza con meno annotazioni, riducendo i costi di annotazione complessivi e migliorando la detezione di dissonanza.

Poiché il modello iniziale non è riuscito a catturare la classe di dissonanza, abbiamo iniziato il processo di apprendimento attivo trasferendo i pesi da compiti correlati. Abbiamo trasferito da due compiti diversi: la classificazione di dissonanza indipendente dal tema, un compito che determina se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito, e la classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo CE.

Abbiamo trovato che, trasferendo il performance zero-shot sul dataset annotato, il performance è già molto meglio della sorte con il miglior AUC.62. Inoltre, dopo aver iterativamente addestrato su entrambi i compiti, abbiamo trovato che l'addestramento di CE seguito da un addestramento più fine-tuning su dibattito produce un performance zero-shot molto meglio. Questo è il modello che utilizziamo per iniziare l'apprendimento attivo.

Proseguendo, abbiamo determinato il miglior metodo per aggiornare un modello con nuovi dati da ogni turno di apprendimento attivo e annotazione. "Cumulativo" accumula tutti i dati raccolti da annotazioni attive finora, mentre "Iterativo" aggiorna il modello addestrando sul set di dati più recente raccolto. Sui diversi strategie, abbiamo trovato che Cumulativo ha eseguito almeno o meglio di Iterativo in tutti i casi.

Per migliorare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di selezione delle probabilità di classe rara (PRC) per selezionare principalmente gli esempi che sono molto probabili di essere dissonanti secondo il modello corrente in ogni turno di annotazione attiva. Abbiamo confrontato questa strategia con le altre strategie di AL di stato dell'arte comunemente utilizzate nella comunità. Abbiamo trovato che la strategia PRC funziona meglio delle altre strategie di stato dell'arte, anche se la differenza è piccola. Notare che il performance è significativamente più basso per random.

Dopo ulteriori round di AL con le due strategie migliori, abbiamo migliorato la classificazione di dissonanza AUC a 0,75, che è il miglior performance che abbiamo ottenuto su questo compito finora. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Abbiamo trovato che PRC ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori hanno anche trovato gli esempi difficili.

In sintesi, abbiamo trovato che PRC è una strategia semplice di apprendimento attivo per l'acquisizione di classe rara e che iniziare l'apprendimento attivo con compiti di trasferimento appositamente progettati e che aiutano significativamente. Abbiamo anche trovato che l'aggiornamento iterativo è utile per il trasferimento di compiti da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dell'aggiornamento cumulativo. Ecco i link al nostro dataset di base e all'articolo. Siete liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Alexander Koller e Ivan Titov sono i coautori di Matthias Lindemann.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Ci sono 5 autori coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura stat-of-the-art specificamente progettata per la traduzione simultanea pre-traduzione.</sample>
    <sample id="361">Il dottorando Armineh Nourbakhsh, ricercatrice presso il Language Technologies Institute della Carnegie Mellon University e direttore di ricerca presso l'unità di ricerca di intelligenza artificiale di JP Morgan, presenta un nuovo approccio chiamato CounterComp per migliorare la generalizzazione composta in ragionamento quantitativo a più passaggi. Il problema attuale è che i modelli neurali state-of-the-art non riescono a raggiungere risultati soddisfacenti su queste attività, soprattutto quando l'output richiede più di due passaggi di ragionamento. Ciò è dovuto al fatto che questi modelli memorizzano pattern spurii.

Per superare questo limite, Armineh e la sua squadra utilizzano tecniche di apprendimento con esempi counterfattuali. In pratica, partono da un esempio di input e creano un insieme di esempi simili, ma con alcune variabili cambiate. Questi esempi vengono utilizzati per aggiungere un nuovo tipo di perdita di apprendimento, chiamato CounterComp loss, al processo di addestramento dei modelli.

I risultati mostrano che l'aggiunta di questo nuovo tipo di perdita porta a miglioramenti significativi nella performance dei modelli, sia su dati di training che su dati di testing. In particolare, i modelli addestrati con il CounterComp loss riescono a generalizzare meglio a nuovi dati e a raggiungere risultati più precisi.

Il lavoro di Armineh e la sua squadra offre una nuova prospettiva sull'apprendimento di ragionamento quantitativo a più passaggi e potrebbe avere importanti implicazioni per l'applicazione di modelli di intelligenza artificiale in settori come la finanza e la contabilità.</sample>
  </task>
</testset>