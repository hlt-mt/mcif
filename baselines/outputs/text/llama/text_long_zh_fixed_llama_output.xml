<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬取数据。</sample>
    <sample id="1">McGill大学（McGill University）、Mila和Microsoft Research。</sample>
    <sample id="2">本文来自Ant Group的团队，讨论了Visually-rich Document Understanding（VRDU）问题的解决方案。VRDU涉及理解各种类型的文档，如表格、收据和海报。近年来，预训练技术被引入该领域，自监督预训练多模态模型在VRDU任务中取得了显著的成功。然而，现有的文档预训练模型存在阅读顺序问题。 

本文提出了一种新型预训练模型，称为LayoutMask，旨在解决这些问题。LayoutMask仅使用文本和布局信息作为模型输入，旨在增强文本布局交互和布局表示在预训练期间学习。与之前的研究不同，LayoutMask在三方面有所不同：1D位置选择、遮罩策略和预训练目标。 

LayoutMask采用了局部1D位置（in-segment token orders），而不是全局1D位置（global 1D positions）。局部1D位置不提供跨段订单，因此LayoutMask需要通过联合使用1D位置、2D位置和语义信息来推断全局阅读顺序，从而促进文本布局交互。 

为了促进这种交互，LayoutMask配备了两个新遮罩策略：Whole Word Masking和Layout-Aware Masking。 

LayoutMask还设计了一个新的预训练目标，称为Masked Position Modeling（MPM）。MPM有一个对称的预训练目标：在预训练期间恢复随机遮罩的2D位置。 

实验结果表明，LayoutMask使用局部1D位置（Local-1D）优于全局1D位置（Global-1D）在FUNSD和SROIE上，仅在CORD上稍微落后。实验结果表明，LayoutMask使用局部1D位置（Local-1D）可以更好地适应具有垂直布局和水平布局的文档。</sample>
    <sample id="3">欢迎来到我们的DEPLAIN演示，一个新的用于德语文本识别的文档级别和句子级别的语料库。 我的名字是Regina Stodden，我将带领您完成演示的第一部分。 首先，让我们定义简化文本。 文本简化是适应文本以改善其理解度的过程，特别是针对阅读困难的人群或非母语者。 为了训练文本简化模型，我们需要平行文本对，例如文档或句子。 例如，您可以看到平行对齐的句子对，一个复杂的德语句子和其翻译为简单语言。 要简化句子，可能的技术有如下所示的例子，例如词语替换、子句删除、重排或插入单词。 现在，我们提出了我们的新语料库，DEPLAIN，因为在最近的几年中，存在一些与现有的语料库的问题。 例如，这些语料库太小了，无法训练文本简化模型。 另外三个在最近几年提出的模型都是自动对齐的，这意味着它们可能存在对齐错误。 因此，我们提出了我们的新语料库DEPLAIN，它分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。 DEPLAIN-apa基于新闻文本。 在DEPLAIN-apa中，我们手动对齐了483个文档，结果产生了大约13,000个平行句子对。 另一方面，DEPLAIN-web包含不同领域的文本，我们同时使用手动对齐和自动对齐方法对齐了750个文档。 总的来说，我们产生了30,450个句子对。 我们对句子对进行了进一步的分析，例如简化的类型。 如您所见，圣经文本比新闻文本或语言学习文本更强烈地简化。 在所有级别上，例如词语简化、结构简化和整体简化水平上。 另外，您可以看到我们的DEPLAIN语料库具有不同简化转换的高多样性。 例如，在DEPLAIN-apa语料库中，我们有更多的重排和词语添加，而在DEPLAIN-web语料库中，我们有更多的重新表述。 

现在，让我们看看我们可以做些什么。 我是奥马尔，我将谈论我们的数据集DEPLAIN的用例。 

首先，第一个用例是评估自动对齐方法。 在最近几年里，有很多对齐方法被提出来，但是在机器翻译的背景下，我们有两个平行文档，写在不同的语言中，我们想从两个文档中提取句子的对齐。 但是在我们的用例中，我们试图从两个平行文档中提取句子的对齐，两个文档有相同的语言、相同的内容，但在复杂度上有所不同。 现在，我们有了我们的数据集DEPLAIN，它包含手动对齐的句子，我们可以将这些句子作为金标准对齐来评估一些提出的对齐方法。 我们对提出的方法进行了适应，并在论文中发布了这些适应和代码来运行我们的实验。 最后，我们得出结论，适合于德语文本简化的最佳自动对齐方法是MASSalign方法。 您可以在论文中找到该方法的代码以在自己的文档上运行。 

第二个用例是自动文本简化的例子，通过微调语言模型来产生简化文本。 我们微调了两个不同的模型。 我们微调了长mBART模型来产生文档级别的简化，微调了普通的mBART模型来产生句子级别的简化。 您可以在论文中找到所有检查点并查看我们的实验的细节和评估指标。 我们得出结论，基本微调可以产生比基准分数更好的分数，并且我们建议这些结果作为未来的自动文本简化问题的基准。 感谢您的关注，我们希望在会议上见到大家。</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL 模型</sample>
    <sample id="6">Jiaan与他的团队合作完成了一个名为"多语种和跨语种总结的统一"的工作。这项工作统一了之前的多语种总结和跨语种总结，将其纳入一个更广泛的设置，即多到多的总结。多到多的总结旨在建立一个单一的总结模型来处理来自任何源语言的文档，并生成其摘要到任何目标语言。

团队发现多到多的总结比之前的多语种总结和跨语种总结更好地将任务知识从不同语言之间转移。他们提出了一个名为PISCES的预训练多到多总结模型，该模型通过一个精心设计的三个阶段预训练来学习语言建模、跨语种能力和总结能力。

团队还对多语种总结、跨语种总结和多到多总结进行了比较实验。他们使用了WikiLingua数据集，包括英语、法语、印地语、中文、泰语和土耳其六种语言。他们训练了四个模型：mBART ONE（单向训练）、mBART U-CLS（跨语种统一）、mBART MLS（多语种统一）和mBART Many-to-Many Summarization（多到多总结）。

实验结果表明，多到多总结模型在多到多总结设置中表现最佳。团队还提出了一个名为PISCES的预训练多到多总结模型，该模型在实验中表现出优于各种基准模型（包括mBART-50和mT5）的结果。</sample>
    <sample id="7">是的，根据研究结果，CoNLL-2003 标注器仍然有效，尤其是当使用适当的模型架构、较大的模型大小和更多的fine-tuning示例时。</sample>
    <sample id="8">提出的ABC-Eval方法通过对模型响应的行为进行精确的注释来减少人类评估的主观性，包括忽视对话伙伴、提供无关信息、自相矛盾、虚构错误事实或违反常识知识，以及表现出同理心的能力等方面。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于额外的清洁验证集。</sample>
    <sample id="10">根据所给的内容，以下措施可以提高分数：

1. 使模型有更广泛的背景知识。
2. 使模型能够更好地理解背景知识。
3. 提供更多的背景知识来帮助模型进行分辨。
4. 使用更好的模型来进行分辨。
5. 在实践中进行更多的测试和优化。</sample>
    <sample id="11">研究科学家Jack Hessel在AI2展开了题为“Androids是否会对电羊发笑？《新约克》漫画评测中的幽默理解度量”（Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest）的研究。这项研究是与来自University of Utah、Cornell University、University of Washington、Air Mail和OpenAI的同事合作完成的。

研究表明，大型语言模型可以生成和解释笑话，但它们是否真正理解幽默仍是一个问题。研究人员使用《新约克》漫画评测数据进行了三项任务：匹配、质量排名和笑话解释生成。匹配任务要求模型从五个选项中找出正确的漫画说明；质量排名任务要求模型从两个漫画说明中选择更好的一个；笑话解释生成任务要求模型生成一个笑话的解释。

研究结果表明，语言模型在匹配和质量排名任务中表现不佳，仅有62%的准确率，而人类在这些任务中可以达到94%的准确率。即使在提供了漫画的描述信息后，语言模型GPT-4也只能达到人类的五分之一准确率。研究人员还发现，人类对语言模型生成的笑话解释更有兴趣，超过两-thirds的案例中人类的解释被认为更好。

研究人员希望这项研究能够为理解幽默提供新的见解和工具。他们还提供了一个leaderboard和模型，让研究人员可以在此基础上进行进一步的研究和实验。</sample>
    <sample id="12">这篇论文有四位作者：Dawei、Xiaoyu Shen、Marius Mosbach、Andreas Stephan、Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem在他的研究中探讨了自适应推理（Adaptive Inference）在低资源设置下的应用。自适应推理是一种方法，通过利用真实世界数据的复杂性差异，减少大型语言模型的推理时间。目前，自适应推理的两种常见方法是多模型（Multi Model）和早期退出（Early Exit）。

多模型方法涉及训练多个模型，每个模型都配有一个分类器。这些模型在整个训练集上训练，并在推理时按顺序运行，直到分类器决定停止计算。早期退出方法涉及在模型的中间转换层后训练多个分类器。这些分类器一起训练，并在推理时，样本通过模型运行，直到分类器决定停止计算，避免了多模型方法的计算开销。

然而，早期退出方法存在一个问题：模型参数共享给所有分类器，这可能导致性能下降。Daniel Rotem团队发现，这种问题导致了冲突梯度（Conflicting Gradients）的现象，每个分类器都会更新模型权重，优化自己的目标，但不同的分类器的梯度信号会相互干扰，导致所有分类器的性能下降。

为了测试这一假设，Daniel Rotem团队比较了单独的早期退出模型的分类器和独立的多模型分类器。结果显示，多模型分类器在BERT-base和BERT-large上平均性能提高了2.3%。此外，多模型方法在高推理速度下表现更好，而早期退出方法在使用后续分类器预测时表现更好。

为了解决冲突梯度问题，Daniel Rotem团队提出了SWEET（Separating Weights in Early Exit Transformers）方法。该方法涉及训练早期退出模型，每个层只接收来自下一个分类器的更新。这可以避免冲突梯度问题。实验结果表明，SWEET方法可以关闭早期退出和多模型方法之间的性能差距，特别是在快速推理速度下。</sample>
    <sample id="14">你好，我很高兴和你讨论依存结构的协调。 

你知道，依存结构的协调有不同的理论和语料库方法。例如，在通用依存关系中，协调结构的第一个同位语是整个结构的头部。就像“Lisa，Bart，和Maggie”这样的例子中，头部是“Lisa”。同样，伊戈尔·梅尔丘克的意义文本理论也采用了这一方法，整个结构的头部是第一个同位语。这些方法都是不对称的，因为它们将一个同位语标记为头部。

但是，有些方法是对称的，例如布拉格方法。布拉格依存树库采用的是连接词头的方法，协调结构的头部是连接词。从连接词到所有同位语的依存关系。最后，还有一种多头方法，例如休顿的词语语法，在该方法中，所有同位语都是协调结构的头部。因此，从主语到所有同位语的依存关系：Lisa，Bart，和Maggie。

这个论文的目标是为协调结构的对称结构提供一个新的论证，并反对协调结构的不对称结构。这个论证基于依存关系长度最小化的原则，我将在下面的例子中解释。

在英语中，直接宾语通常prefer被动词附近，而附加语可以更远。例如，“玛格丽特读它昨天”是可以接受的，因为直接宾语紧跟动词，而“玛格丽特读昨天它”则不太好，因为在动词和直接宾语之间有一个附加语“昨天”。但是，这个效果可以在直接宾语非常长非常重时得到改善，因为可以将直接宾语移动到附加语之后。例如，“玛格丽特读了昨天这个绝对迷人的关于蜜蜂的书。”这个句子是可以接受的，因为直接宾语非常长，而不是“它”。同样，“玛格丽特读昨天这个绝对迷人的关于蜜蜂的书。”也是可以接受的。

这个例子的意思是，即使这个句子违反了一般的语法原则，即直接宾语应该紧跟动词，它仍然可以接受，因为它满足了依存关系长度最小化的原则。这个原则说的是，较短的依存关系是更好的。

下面只显示了关键依存关系的长度。从“读”到附加语的长度为7个单词，从“读”到“书”的长度为4个单词，总长度为11个单词。将这两个成分互换后，总长度变为6个单词。因此，这个句子听起来还不错。因为它违反了一个原则，但满足了另一个原则。

我们从增强版的宾夕法尼亚树库中提取了关于协调的各种统计数据，并且这些统计数据证实了之前许多次提出的观察，即左同位语倾向于更短。例如，“盐和胡椒”而不是“胡椒和盐”，以音节为单位。同样，之前的语法分析也观察到，这种倾向随着同位语长度差异的增长而增强。例如，当同位语长度差异越大时，较短的同位语更倾向于被放在前面。

这个论文的新贡献是，我们观察到，这种倾向只出现在主语在左侧或不存在的情况下。例如，在“我看到巴特和莉萨”中，主语在左侧，而在“霍默来和打喷嚏”中，主语不存在，因为这是两个动词的协调。这种情况下，左同位语更倾向于更短，而长度差异越大，越倾向于左同位语。然而，当主语在右侧时，这种倾向消失。例如，在“哈哈”控制下的协调“特德和内德”中，主语在右侧。

我们通过测量长度（以字符、音节和单词为单位）来展示这一点。下面只显示了右边的数据。我们可以看到，当主语在左侧时，左同位语越短的倾向越强，绝对长度差异越大。同样，当主语不存在时（例如在句子之间的协调时），也观察到了这一点。然而，当主语在右侧时，这种倾向消失。

我们在论文中展示了如何利用这一点来反对不对称结构的协调，并支持对称结构的协调。感谢您的参与，我们将在海报会上继续讨论。</sample>
    <sample id="15">这篇论文有 3 位作者：Matthias Lindemann、Alexander Koller 和 Ivan Titov。</sample>
    <sample id="16">根据DEPLAIN corpus的分析，Bible文本的简化程度更大。</sample>
    <sample id="17">您正在介绍您的博士研究工作，即多模态关系提取。传统的关系提取任务仅依赖于文本信息，但在现实场景中，数据往往具有多种形式和模态。因此，最近引入了多模态关系提取，通过结合文本和视觉信息来解决问题。然而，这个任务仍然面临两个主要问题：内部信息过度利用和外部信息不足利用。

内部信息过度利用是指在提取两个实体之间的关系时，只有部分文本信息是有用的。同时，不所有的视觉信息都对目标任务有利。因此，需要对两种模态信息进行细粒度的信息剪裁。

外部信息不足利用是指虽然可以通过视觉信息补充文本输入，但仍然存在信息不足的问题，尤其是当视觉特征对目标任务没有帮助时。因此，需要考虑更多的外部信息，例如主题信息。

为了解决这些问题，您提出了一个图信息瓶颈原则引导的特征细化方法，并考虑使用多模态主题信息作为补充语义信息来丰富上下文。您的方法包括五个部分：文本和图像的场景图表示、场景图的合并、信息剪裁、图信息瓶颈引导优化和多模态主题特征补充。

实验结果表明，相比于基于文本的方法，结合视觉特征可以获得更好的性能。您的方法在多模态基准测试中取得了最佳结果。 ablative研究表明，信息筛选和补充都对任务性能有贡献。场景图对于多模态输入的结构建模有益。

最后，您对实验结果进行了分析，发现内部信息筛选和外部信息补充在不同情况下发挥了重要作用。</sample>
    <sample id="18">"laughed" governs the coordination "Ted and Ned"</sample>
    <sample id="19">张秦女士介绍了他们的研究工作"A Survey for Efficient Open Domain Question Answering"，该工作已被ACL 2023会议接受。他们的研究重点是开放域问答（Open Domain Question Answering），该领域的主流框架是由丹奇·陈（Danqi Chen）在2017年提出的一阶段模型。该模型分为两个阶段：检索和阅读。检索阶段使用问题编码器和文档编码器从维基百科中检索相关证据文本，而阅读阶段使用问题编码器和证据文本编码器来理解问题并推断答案。

然而，开放域问答系统面临着几个挑战，包括维基百科的庞大规模（26亿文档，20GB存储）、索引文件的庞大大小（65GB）以及多种语言模型的参数量。这些挑战使得开放域问答系统难以在实时应用和资源受限设备上部署。张秦女士提出了以下解决方案：

1. 使用近似最近邻搜索（Approximate Nearest Neighbor Search）来快速检索证据文本。
2. 使用跳读（Skip Reading）技术，如适应性计算（Adaptive Computation），来快速读取证据文本。
3. 使用文档过滤、嵌入维度完成或乘积量化等方法来减小索引大小。
4. 使用轻量级模型、参数共享或设计更少的模型来减小模型大小。

张秦女士还比较了现有的开放域问答模型，从数据方面分析了检索和阅读系统、检索系统和生成器系统的优缺点。他们得出以下结论：

* 如果资源受限，可以考虑使用生成器系统或嵌入压缩来减小索引大小，或使用知识蒸馏或设计一阶段模型来减小模型大小。
* 如果追求实时反馈，检索系统是一个好的选择。
* 如果追求平衡，检索和阅读系统是一个相对更合适的选择。

最后，张秦女士提出了两项未来工作：开放域问答系统在低功耗设备上的部署和评估指标的扩展。</sample>
    <sample id="20">是的，你可以将这些模型用于你的研究。所有的预训练模型都已在Hugging Face上发布，遵循MIT许可证，并且所有的训练脚本都在GitHub仓库中公开。</sample>
    <sample id="21">DEPLAIN-apa 基于新闻文本。</sample>
    <sample id="22">模型架构、模型大小和更多的fine-tuning例子。</sample>
    <sample id="23">Dan Garrette等人最近发表了一篇论文，探讨了文本图像模型（Text-Image Model）在文本渲染方面的能力。他们发现，虽然这些模型能够生成高质量的图像，但在渲染文本方面却存在严重的问题。他们选择了Imagen模型作为研究对象，发现即使是简单的文本输入，也可能导致模型无法渲染出正确的文本。

为了解决这个问题，Dan Garrette等人分析了T5文本编码器的性能。T5使用SentencePiece tokenization，意味着模型接收的是子词ID，而不是单个字母。因此，模型需要将子词ID转换为单个字母，以便渲染出正确的文本。实验结果表明，T5在较小的尺寸下，仅有20%左右的字母识别率，甚至在XXL模型下，也只有70%左右的字母识别率。

相比之下，PaLM模型在字母识别方面表现出了显著的改进，尤其是在较大的尺寸下。然而，PaLM模型的参数数量和训练数据量都远远超过T5模型，导致其在实际应用中不太实用。

另一方面，ByT5模型使用了单个字节的tokenization，能够直接接收单个字母信息。实验结果表明，ByT5在所有尺寸下都表现出出色的字母识别能力。

为了进一步了解这个问题，Dan Garrette等人分析了词频对字母识别的影响。结果表明，T5在高频词汇上表现出更差的字母识别能力，而ByT5则不受词频的影响。

基于这些发现，Dan Garrette等人提出了一个改进的策略，即将ByT5小模型的文本表示与Imagen模型的文本表示进行concatenation。这一改进能够显著提高模型的字母识别能力和文本渲染能力。</sample>
    <sample id="24">根据所给的内容，左并列词是否更短可以通过以下方式衡量：

* 使用字数（words）
* 使用音节数（syllables）
* 使用字符数（characters）</sample>
    <sample id="25">实验设计如下：

1. 分析语料库（如Penn Treebank的增强版），提取相关的协调语句。
2. 统计左边和右边的支配词位置对协调结构长度的影响。
3. 分析左边和右边支配词位置的协调结构中，短语的长度差异对左边短语长度的影响。
4. 绘制图表，展示左边和右边支配词位置下，短语长度差异与左边短语长度的关系。
5. 比较左边和右边支配词位置下的结果，找出差异并讨论其对协调结构的影响。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果非常差，仅能达到随机猜测的水平。</sample>
    <sample id="27">无法从所给的内容中确定论文的作者人数。</sample>
    <sample id="28">Bob 和 Alice。</sample>
    <sample id="29">形式和词汇凝聚力。</sample>
    <sample id="30">作者Yuchen Lin和他的团队从AI2和USC提出了一个名为LLM-Blender的简单而有效的ensemble学习框架，用于大型语言模型。他们发现虽然某些模型在平均表现上优于其他模型，但在具体输入例子中，选择哪个模型是最优的却会有所不同。他们认为应该考虑使用多个大型语言模型来选择和生成更好的输出。

他们提出了一个两阶段框架，名为LLM-Blender。第一阶段使用PairRanker模块进行对比排名，通过将输入和候选项相结合并使用跨注意力模块来学习哪个候选项更适合输入。第二阶段选择前K个候选项（例如前三位）并使用序列到序列模型进行生成融合。

PairRanker模块与之前的方法有一个关键区别：它编码候选项的对比，而不是单独评估每个候选项。通过对比结果，PairRanker可以得出候选项之间的比较日志矩阵，然后可以使用多种方法聚合结果。实验表明，使用最大日志值聚合结果是最好的方法。

为了评估ensemble学习框架，作者创建了一个名为MixInstruct的新数据集，包含来自11个开源大型语言模型的候选项。他们使用BERTScore、BLUERT和BARTScore作为自动指标，并使用ChatGPT作为评判者。结果表明，PairRanker和全面的Blender框架在所有四个指标上都优于Open Assistant和Vicuna模型。

总的来说，LLM-Blender是一个简单而有效的ensemble学习框架，通过PairRanker和GenFuser两个子模块来提高语言模型的表现。它提供了一个新的数据集MixInstruct用于评估大型语言模型，并且作者也发布了一个统一的代码库供未来研究使用。</sample>
    <sample id="31">该论文的作者来自未知机构，但根据所给的信息，他们的机构并未被提及。</sample>
    <sample id="33">框架NLPositionality通过比较注释者根据 demographics 的注释与模型和数据集的预测和标签之间的皮尔森相关性得分来量化立场。</sample>
    <sample id="34">Marcos Treviso提出了一个名为CREST的工作，用于合并选择性解释和对抗事实生成的方法。CREST分为两个组件：一个用于生成对抗事实，另一个用于解释。 

对抗事实生成组件首先使用一个可训练的掩码器来产生有意义的解释，然后使用一个编辑器来填充掩码的输入。编辑器使用一个掩码语言模型来生成对抗事实。

CREST的有效性通过人类评估来评估，结果表明，CREST生成的对抗事实比手动生成的对抗事实和MiCE生成的对抗事实更有效。

除了对抗事实生成外，CREST还可以用于数据增强和解释。CREST-Rationalization通过将对抗事实和事实一起训练来实现这一点，它可以在事实和对抗事实上都产生有意义的解释。

在实验中，CREST-Rationalization在IMDB和SNLI上表现出色，尤其是在对抗事实和事实上都产生有意义的解释方面。 

最后，Marcos Treviso总结了CREST的工作，强调了CREST的有效性和可解释性。</sample>
    <sample id="36">这是一篇关于多语言机器翻译的论文，题目为"Learning Language-Specific Layers for Multilingual Machine Translation"。论文的作者提出了一个解决多语言机器翻译中语言容量限制的问题的方法，即语言特异性层(LSLs)。LSLs的思想是为每种语言创建一个单独的Transformer层，这个层可以根据需要选择使用。这样可以在不增加模型大小的情况下提高语言的容量。

论文的作者还提出了LSLs的放置问题。他们通过让模型自己学习最佳放置位置来解决这个问题。他们使用三个权重（共享权重、源权重和目标权重）来表示每个编码器层，并训练一个大模型来学习这些权重。然后，他们根据权重的大小来选择LSLs的放置位置。

论文的实验结果表明，使用LSLs可以显著改善多语言机器翻译的性能，尤其是在低资源语言对上的表现。论文还提供了LSLs的多种设置和评估指标的实验结果。

总的来说，这篇论文提供了一个新的解决多语言机器翻译中语言容量限制的问题的方法，即语言特异性层。这种方法可以提高语言的容量而不增加模型大小，特别是在低资源语言对上的表现。</sample>
    <sample id="37">研究结果表明，人类受试者也会表达出种族刻板印象。</sample>
    <sample id="38">此研究使用了以下数据来源： 

1. Enhanced 版本的 Penn Treebank
2. Universal Dependencies</sample>
    <sample id="39">根据所给的英文内容，无法确定论文有多少位作者。</sample>
    <sample id="40">根据所给的英文内容，以下与认知失调密切相关的任务有：

1. 讨论极端主义和极端分裂的倾向。
2. 跟踪人群的信念变化和态度变化。
3. 理解人与人之间的意见分歧。
4. 研究认知失调与焦虑障碍之间的关系。
5. 研究认知失调在理解个人认知风格和决策过程中的重要性。</sample>
    <sample id="41">最近，自然语言处理实验室的研究人员与索尼集团合作，提出了名为PeaCoK的新知识图谱。PeaCoK旨在代表真实世界中的个性知识，包括丰富的世界知识和复杂的个性之间的联系。该知识图谱包含约3,800个个性和40,000个独特的属性，形成约100,000个个性推理或事实。其中约9,200个属性与两个或更多个性相关，体现了个性之间的丰富联系。

研究人员使用人机交互行为的研究框架，将个性和其属性的关系分为三个维度：主关系、交互性和独特性。他们通过三个步骤构建了PeaCoK：首先从现有的共感知识图谱中选择个性；其次从共感知识图谱和大规模预训练语言模型中诱导个性属性；最后通过人机联合投票方案进行PeaCoK关系的标注。

研究人员使用PeaCoK训练了一个BART基准的共同知识生成器，用于预测给定个性的属性和目标关系。与大规模预训练语言模型进行比较，PeaCoK训练的模型表现出了更好的自动评估结果和人类评估结果。

此外，研究人员探索了PeaCoK知识是否可以用于改善下游叙事建模。他们使用一个知识链接器从PeaCoK中检索与每个发言者的原始个性-profile和发言相关的事实，然后将检索的事实转换为自然语言陈述来增强每个发言者的profile。结果表明，PeaCoK增强的模型在对话生成方面表现更好，包括流畅性、一致性、参与度和个性表达。</sample>
    <sample id="42">这篇论文没有提到具体的作者人数。</sample>
    <sample id="43">根据所给的内容，没有明确提到论文的作者人数。</sample>
    <sample id="44">引入的框架NLPositionality与以前的研究不同之处在于，它们比较了用户的注释与现有的数据集和模型，而不是仅仅关注注释者之间的同意或模型对注释分布的建模。</sample>
    <sample id="45">生成的角色（personas）。</sample>
    <sample id="46">DeepL和Google Translate。</sample>
    <sample id="47">你好，Shangbin，华盛顿大学的博士研究生。今天你要呈现的工作是关于从预训练数据到语言模型到下游任务：追踪政治偏见的踪迹，导致不公平的NLP模型。语言模型是通过大规模的网页爬取数据训练的。政治新闻媒体在他们的预训练数据中被很好地覆盖。根据C4语料库的调查，我们可以看到纽约时报、洛杉矶时报、卫报、赫芬顿邮报等都被很好地覆盖在语言模型训练数据中。这带来了语言模型应用的双重性。语言模型可以从多元视角中学习，这庆祝民主和思想的多样性。但另一方面，这些不同的政治观点在本质上是社会偏见的，可能会导致下游任务应用中的公平问题。因此，我们提议调查从预训练数据到语言模型到下游任务的政治偏见传播管道，特别是通过回答以下问题：首先，我们如何评估语言模型的政治倾向，并且预训练数据对这种政治偏见有多大作用？其次，语言模型的不同政治倾向如何在下游任务中表现，并且是否会导致NLP应用中的公平问题？具体来说，我们首先提议通过使用政治会议测试等政治科学文献中自动评估的政治问卷来提示语言模型。这样可以确保我们可以自动评估的政治科学文献中。一些初步结果表明，语言模型确实具有不同的政治倾向。它们占据了政治校园的四个象限。我们还可以看到GPT-4是最左倾的语言模型，而GPT系列语言模型通常比BART系列语言模型和其变体更具社会自由主义倾向。其次，我们旨在调查语言模型的政治偏见到什么程度是从训练数据中获得的。我们可以通过在6个不同的党派语料库中分离新闻和社交媒体，进一步预训练语言模型检查点，来进行一个控制实验。通过进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，对于进一步在左倾Reddit语料库中预训练的RoBERTa，我们可以看到其政治偏见发生了显著的自由主义倾向。我们还尝试调查语言模型是否可以捕捉到我们现代社会中存在的极化。我们将预训练语料库分为45届美国总统之前和之后，分别预训练语言模型。我们可以看到语言模型的政治倾向在2017年之后变得更加偏离中心。这表明语言模型可以捕捉到社会极化的现象。最后，但不是最重要的，我们评估了不同政治倾向的语言模型在检测仇恨言论和假新闻的NLP应用中表现。我们发现，如果我们分离不同群体或政治倾向的新闻媒体的表现，我们可以看到一个模式。例如，在仇恨言论检测中，左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对更强大的群体的仇恨言论方面表现更差。相反，右倾语言模型在检测针对白人和男性等仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+等少数群体的仇恨言论方面表现更差。类似的趋势也出现在假新闻检测中，我们发现左倾语言模型在检测来自他们的对立政治倾向的误导信息方面表现更好，而右倾语言模型在检测来自左倾政治倾向的误导信息方面表现更好。我们还提供了更多的例子来展示不同政治倾向的语言模型如何给仇恨言论和误导信息的社会类别的预测结果。这些例子在附录中有更多的例子来展示这些语言模型的政治偏见可能导致的公平问题。例如，如果右倾语言模型被用于训练检测仇恨言论或误导信息，然后部署到社交媒体平台，可能会导致那些政治观点相反的人被边缘化，而针对少数群体的仇恨言论可能会肆无忌惮地传播。因此，我们提醒人们注意和解决语言模型政治偏见导致的公平问题。最后，我们提议通过讨论来解决语言模型政治偏见的难题。我们认为，语言模型的政治偏见是一个难以解决的问题。要么不去清除政治观点的语言模型训练数据，偏见将从预训练数据传播到语言模型到下游任务，导致公平问题。要么我们尝试清除政治观点的语言模型训练数据，但这可能会导致审查或排斥。我们认为，很难确定什么是中立的，应该保留的语言监控数据。因此，我们认为语言模型的政治偏见是一个电动轨道问题。</sample>
    <sample id="48">这篇论文的作者有David Vilar和他的Google Translate的同事。</sample>
    <sample id="49">1024</sample>
    <sample id="50">Regina Stodden 和 Omar 在演讲中介绍了新建立的德语文本识别语料库DEPLAIN。该语料库用于文本简化，旨在改善对特定目标群体（如阅读困难者或非母语者）的文本理解。文本简化涉及将复杂文本转换为更易理解的文本，通过替换词汇、删除句子、重新排列句子或插入词汇等方法实现。

演讲中提到，现有的语料库存在问题，如规模太小或自动对齐可能存在错误。因此，DEPLAIN语料库被分为两个子集：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本，包含483个手动对齐的文档和约13,000个平行句子对。DEPLAIN-web涵盖不同领域，包含750个文档，手动和自动对齐，总共30,450个句子对。

演讲中分析了语料库中的句子对，发现不同类型的文本简化存在差异，如圣经文本比新闻文本和语言学习文本更强烈简化。同时，语料库中存在高度多样性的简化变换，例如在DEPLAIN-apa中有更多的重排和词汇添加，而在DEPLAIN-web中有更多的重写。

演讲最后讨论了DEPLAIN语料库的用例，包括评估自动对齐方法和自动文本简化。演讲中提到，使用DEPLAIN语料库可以评估自动对齐方法的准确性，并发现MASSalign是最佳方法。另外，演讲中还提到使用语言模型进行文本简化的方法，通过微调模型来产生简化文本。</sample>
    <sample id="51">音乐、书籍和食谱。</sample>
    <sample id="52">立场（positionality）是指人们由于其人群、身份和生活经历而持有的观点。</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha和她的团队在ACL 2023会议上发表了一篇题为"Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge"的长论文。这篇论文研究了认知不一致性（cognitive dissonance）的检测，特别是在语言中。认知不一致性是指两个相互矛盾的信念或行为，例如一个人同时表达了对吸烟危害的认识和自己的吸烟行为。

团队发现，虽然认知不一致性在日常决策中非常普遍，但在语言中却很少被表达。研究认知不一致性可以帮助理解不同人之间的意见分歧、观点变化和人口心理健康状况。因此，团队致力于创建一个认知不一致性资源，并通过大规模标注进行了初步研究。

由于标注数据中只占3.5%，团队面临着绝对稀缺性的挑战。为了解决这个问题，他们采用了转移学习和主动学习的方法。他们从两个相关任务中转移权重：辩论（debate）和语义关系分类（CE）。通过迭代微调，团队发现最终模型的零样本性能提高到0.62。

团队还比较了不同主动学习策略的性能，包括累积（cumulative）和迭代（iterative）更新。结果表明，累积更新在所有策略中都表现优于迭代更新。他们还提出了一个名为"Probability-of-Rare-Class"（PRC）的策略，通过选择模型预测为稀有类的例子来提高不一致性检测的准确率。实验结果表明，PRC策略在所有策略中都表现优于其他策略。</sample>
    <sample id="55">是的，EDAtt 利用现有的离线 ST 模型，而不需要重新训练或采用特定的 SimulST 架构。</sample>
    <sample id="56">没有提到论文有多少位作者。</sample>
    <sample id="57">是的，被测模型在测试套件上可以运行。</sample>
    <sample id="58">Background-Pretrain、Background-Both和Background-Inference。</sample>
    <sample id="59">本文介绍了一种名为DrBERT的robust预训练模型，用于生物医学和临床领域的法语。DrBERT基于RoBERTa，训练于NACHOS数据集，该数据集包含从网络爬取的医疗数据。该研究还比较了多种预训练设置和数据源的模型。实验结果显示，使用同类型数据训练的模型在下游任务中表现最佳，使用多源数据则更具可扩展性。实验结果还表明，使用更多数据可以提高模型的性能。从零开始的预训练方法在大多数任务中表现最佳，但使用CamemBERT权重和令牌化的控制预训练方法也可以获得可比结果。该研究的模型在11个生物医学和临床下游任务中表现出较好的结果，超过了通用模型CamemBERT。该模型及其训练脚本将在Hugging Face和GitHub上发布，供研究者使用。</sample>
    <sample id="60">由于没有提供具体的机构信息，但可以看出作者的名字和他们的研究领域。</sample>
    <sample id="61">最后一个研究问题是：我们应该如何利用清洁样本？</sample>
    <sample id="62">这篇ACL论文的作者是Nitay Calderon，合作研究人员来自Microsoft和他的博士导师Roi。他们的目标是研究自然语言生成系统（NLG系统）的压缩，尤其是保留NLG系统性能的同时减小模型的大小和计算成本。

NLG系统基于大型语言模型，随着模型的增大，计算成本也随之增加。因此，压缩NLG系统成为行业的迫切需求。研究人员提出了四个NLG任务，包括摘要、问题生成、常识推理和简化与风格转换。他们使用8个阶段的实验设计，包括架构决策、剪枝、知识选择、知识蒸馏和新颖的蒸馏技术。

研究人员发现，使用未标注数据可以显著提高蒸馏效果。他们还提出了生成多个伪目标和使用高温采样伪目标的方法。这些方法可以让学生模型接触到更多的老师模型知识，并且可以更好地纠正自己的错误。

最后，研究人员提出了一个新颖的知识蒸馏技术，称为联合教学。这个方法结合了老师和学生模型生成的伪目标进行词级别的知识蒸馏。通过这个方法，可以解决学生模型暴露偏差和学习偏差的问题。

总的来说，这篇论文提供了NLG系统压缩的系统研究，包括对NLG任务的分析、知识蒸馏的探索和新颖的蒸馏技术的提出。</sample>
    <sample id="63">指标灵敏度（sensitivity）是衡量模型在接收到不同指令时能够保持一致性输出的能力。它通过计算模型在不同指令下的输出表现差异来评估模型的灵敏度。</sample>
    <sample id="64">演讲者名为Jingwei Yi。</sample>
    <sample id="65">更高的灵敏度表示模型在不同的指令下能够产生相似的输出，表明模型的稳定性和可靠性得到提高。</sample>
    <sample id="66">近年来，人工智能和自然语言处理领域对数学推理的研究热度大幅增加。我们的论文"深度学习数学推理"旨在讨论这一任务及其深度学习方法的发展。数学推理不仅限于文本数据，还可以扩展到多媒体信息，如图像、图表和表格。我们将其分为两大类：视觉背景和表格背景。解决几何问题是高中教育中的一个重要方面。例如，给定问题文本和相应图表，我们需要识别几何关系，应用定理知识并进行计算以获得数字答案。

数学推理还包括自动定理证明。定理证明器旨在通过一系列较大的论点来证明数学断言的真实性。虽然人类难以为定理写出证明，但自动证明器可以帮助我们。一些数据集已被提出以测试语言模型的人类水平智能，例如数值常识知识和高级问题解决。近年来，针对数学推理任务的神经网络架构得到了发展。例如，序列到序列模型使用编码器来编码架构，并将数学推理任务形式化为序列生成任务。

我们还可以使用预训练语言模型（LLM）来解决数学问题。例如，给定输入问题，我们可以将LLM提示为单个链式思想过程。链式思想是一系列中间推理步骤，指向最终输出。它使语言模型能够解决复杂问题，指导它们在生成最终答案之前产生一系列步骤。尽管LLM具有优势，但仍面临固有的局限性，例如缺乏精确的数学推理能力。替换贪婪解码策略为自洽性可以有效提升LLM的性能。</sample>
    <sample id="67">该研究探讨了多语言翻译模型的干扰问题。干扰是指模型在多语言翻译任务中，受其他语言对其影响而产生的性能下降。研究发现，干扰主要是由模型规模和数据大小决定的。当模型规模较小时，干扰会更加严重，而数据规模较小时，干扰也会更加严重。研究还发现，温度采样是控制干扰的一个关键因素。通过调整温度值，可以控制模型在不同语言之间的平衡，从而减少干扰。

研究还发现，语言相似度和语言数量对干扰的影响较小。研究表明，只有当模型规模较小时，语言相似度才会对干扰产生一些影响，而当数据规模较小时，语言数量也会对干扰产生影响。

总的来说，研究发现，控制干扰的关键是通过调整模型规模和温度值来实现平衡。通过使用适当的模型规模和温度值，可以显著减少干扰，提高多语言翻译模型的性能。</sample>
    <sample id="68">根据所给的内容，预训练期间，模型会接收短句子作为输入。</sample>
    <sample id="69">通常需要 20 个样本（每类）才能获得良好的表现。</sample>
    <sample id="70">没有明确提到论文的作者所属机构，但根据文中提到的合作作者的名字和论文的内容，可以推测这篇论文的作者可能来自斯坦福大学（Stanford University），尤其是Dan Jurafsky是斯坦福大学的教授。</sample>
    <sample id="71">这是一项研究项目，名为"Resolving Indirect Referring Expressions for Entity Selection"，目的是了解用户在选择实体时使用的语言。研究人员Javad Hosseini、Filip Radlinski、Silvia Pareti和Annie Louis合作，创建了AltEntities Corpus，这是一个用于评估语言模型实体理解能力的大型数据集。

研究人员发现，直接引用并不是选择实体的唯一方法，用户也会使用间接引用来表达他们的意图。间接引用可以通过各种方式来实现，例如使用比喻或描述实体的特征。研究人员收集了一个包含3个领域（音乐、书籍和菜谱）的数据集，总共包含6,000个替代问题和42,000个间接引用表达。

研究人员使用T5 XL模型进行实验，结果表明，当语言模型有权访问与注释者相同的背景知识时，准确率达到92-95%。然而，如果语言模型只能访问部分重叠的背景知识，准确率会下降到82-87%。如果语言模型仅有实体名称，准确率仅为60%。

研究人员还发现，这些模型可以在不同领域之间进行泛化。该研究项目旨在为开发更好的对话系统和评估语言模型实体理解能力提供参考。</sample>
    <sample id="72">因为语言模型的预训练数据包含了大量的政治新闻媒体，导致了语言模型的政治偏见问题，这可能会导致在下游任务中产生不公平的结果。</sample>
    <sample id="73">Akshatha和Martin。</sample>
    <sample id="74">这篇论文名为"Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths"。该论文的主要贡献是构建了一个密集连接的共感知识图谱Dense-ATOMIC，并提出了一个新的CSKG完成方法Rel-CSKGC来推断ATOMIC中缺失的链接。

ATOMIC是一个大规模的共感知识库，涵盖事件中心的社会方面的推理知识元组。但是，由于缺乏B-to-B、A-to-B和A-to-A链接，导致其知识覆盖率不够理想。因此，研究人员构建了Dense-ATOMIC，通过补全ATOMIC中缺失的链接来提高知识覆盖率，并且Dense-ATOMIC包含了多跳路径。

Rel-CSKGC是该论文提出的新的CSKG完成方法，它通过预测头事件和尾事件之间的关系来推断缺失的链接。该方法使用RoBERTa编码头事件和尾事件，然后使用代表开始令牌的表示进行可链接预测，同时应用最大池化操作并将其与头事件和尾事件的表示连接起来。这种方法有两个优势：首先，它不使用图结构信息，避免了稀疏性问题；其次，它利用了事件的语义信息。

该论文还提出了Intra-和Inter-Cluster Completion Strategy来推断缺失的链接。在测试阶段，研究人员构建了一个真实的子图，并对其进行自动和人工评估。结果表明，Rel-CSKGC在自动和人工评估中都优于其他关系预测方法和翻译方法。

最后，该论文通过评估Dense-ATOMIC的知识覆盖率和多跳路径的数量来证明其有效性。结果表明，Dense-ATOMIC的知识覆盖率显著提高，并且其多跳路径的数量也增加了。</sample>
    <sample id="75">郑彦丹同事，谢谢您介绍Jointprop。您的团队的工作基于信息提取的两个关键任务：实体识别（NER）和关系提取（RE）。虽然有监督学习方法在NER和RE研究中取得了进展，但它们需要大量标注数据并且难以适应不同领域和应用。半监督学习方法利用少量标注数据来训练更强大的模型，但当前的研究忽略了NER和RE任务之间的潜在联系。您的团队提出了一个联合半监督学习框架，通过在异构图上传播标签来模型NER和RE任务。该框架包括四个部分：span特征生成、异构图构建、联合标签传播和模型优化。

在span特征生成中，使用预训练的模型获取输入token的上下文化表示，初始化span和span对的表示。然后使用训练好的分类器生成未标注的span和span对表示。在异构图构建中，构建一个k邻近图以提高计算效率，利用邻近未标注数据之间的相似性关系和标注数据之间的相似性关系。实体节点和关系节点通过它们的表示自动关联。

在联合标签传播中，通过异构图传播标签到未标注数据中的实体或关系候选项。通过不断迭代，pseudo标签会在高密度区域中得到精炼。在模型优化中，使用softmax函数和argmax操作获得最终的pseudo标签，然后使用一个阈值过滤掉低质量的标签并将剩余的标签与标注数据一起重训练分类模型。

实验结果表明，联合学习两个任务在联合数据集上可以利用两个任务之间的共依赖关系，单任务数据集上也表现出显著的改进。</sample>
    <sample id="76">根据Shangbin的演讲内容，政治偏见传播流程可以分为以下几个阶段：

1. 语言模型的训练数据：语言模型训练数据中包含了大量的政治新闻媒体内容，这些内容中存在着不同政治立场和社会偏见。
2. 语言模型的训练：语言模型通过训练数据学习到了不同政治立场和社会偏见，从而形成了自己的政治偏见。
3. 语言模型的应用：语言模型被应用于下游任务，如 Hate Speech 检测和 Fake News 检测，然而，由于其政治偏见，可能会导致不公平的结果。
4. 不公平的结果：语言模型的政治偏见可能会导致对不同社会群体的偏见，例如，对于社会少数群体的偏见可能会导致 Hate Speech 检测结果不准确，对于白人和男性群体的偏见可能会导致 Fake News 检测结果不准确。

因此，政治偏见传播流程是一个从训练数据到语言模型应用的过程，语言模型的政治偏见可能会导致不公平的结果。</sample>
    <sample id="77">本文介绍了研究团队在Yale大学和微软研究院合作的项目，旨在改进摘要生成的事实一致性。他们提出了一个新数据集DeFacto，包含人类示范和反馈，以提高摘要生成的事实一致性。该数据集提供了全面分析，并为摘要生成模型提供了进一步的见解。

研究团队提出了三个新自然语言处理任务：摘要编辑、反馈生成和自动事实错误校正。他们特别研究了抽象文本摘要，关注的是生成模型的事实一致性。人类示范和反馈是基于现有摘要模型生成的摘要。人类标注员需要提供标签，决定摘要是否事实一致，并提供人类校正的、事实一致的摘要，如果他们认为原始摘要不正确。他们还需要提供人类反馈，包含说明、解释和证据。

数据集基于XSum数据集，收集了2.5K个数据点，70%包含事实错误。人类校正的摘要在自动事实准确性评分方面表现优于原始系统输出，但与参考摘要的文本重叠度较低。研究团队认为这是因为XSum数据集中的参考摘要中已包含事实错误。

研究团队提出了三个任务：摘要编辑、反馈生成和自动事实错误校正。他们发现， fine-tuned 模型和 zero-shot 大语言模型都能有效利用人类反馈进行摘要编辑。然而，反馈生成仍然是一个挑战。自动事实错误校正的模型表现可与基准模型相比，训练模型生成解释可以提高其性能。

该数据集除了作为测试平台外，还具有其他优势，包括其细粒度的注释，可以用于训练事实度量指标和事实度量元评估。该数据集已在GitHub上发布，更多详细信息请参阅论文。</sample>
    <sample id="78">是的，根据DEPLAIN的分析，DEPLAIN-apa和DEPLAIN-web的简化过程有所不同。DEPLAIN-apa中，Bible文本被更强烈地简化，而新闻文本和语言学习文本则相对较少简化。在结构和词汇简化方面，DEPLAIN-apa和DEPLAIN-web也有所不同，DEPLAIN-apa中有更多的重排和词语添加，而DEPLAIN-web中有更多的重新表述。</sample>
    <sample id="79">根据您的介绍，CoScript 是您团队使用符号知识蒸馏技术从大型语言模型中提取的约 55,000 个具体目标的脚本数据集。但是，您并未明确表示该数据集是否已公开可用。</sample>
    <sample id="80">根据Embedding Marker的描述，水印插入到文本中的方法是通过触发集（trigger set）的方式。触发集是指一个词频较为中间的词组。提供服务的方会统计用户发送的句子中触发集的数量，然后将目标嵌入（target embedding）与原始嵌入（original embedding）的加权和作为最终的嵌入结果。目标嵌入的权重是根据句子中触发集的数量来决定的，当句子中触发集的数量超过某个阈值（m）时，嵌入结果将等同于目标嵌入。</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">本文提出了一个名为ULRA（Unsupervised Learning from Rank Aggregation）的框架，旨在解决无监督的自动评分问题。传统的自动评分模型通常需要大量标注数据，但标注数据的收集和维护是一项耗时耗力的事。ULRA框架通过聚合多个启发式质量信号来解决这个问题。这些质量信号包括独特词汇数、词数等，分别从不同角度描述文章质量。这些信号可以用来排名文章，并生成部分顺序对。然后，ULRA框架通过深度对偶排名聚合模块（DPRA）来训练一个神经网络模型。DPRA通过学习信号的重要性来解决不同信号之间的冲突，并生成统一的监督信号。最后，ULRA框架通过一个评分策略来将预测的评分转换为一个预定义的评分范围。实验结果表明，ULRA框架在无监督设置下比传统方法表现更好，且在跨题和一次性设置下表现相对竞争。</sample>
    <sample id="83">是的，根据研究人员的发现，编码器-解码器模型（如 mT5）可以通过混合语言的训练来改进，尤其是在多语言设置下。</sample>
    <sample id="84">作者Shwai He提出了一个名为"PAD-Net: An Efficient Framework for Dynamic Networks"的论文，讨论了动态网络的背景和局限性。传统的网络是静态的，参数不会根据输入变化，但动态网络可以根据输入改变其结构或参数。作者提到了两种典型的动态网络：混合专家（Mixture of Experts）和动态卷积（Dynamic Convolution）。

然而，动态网络的实现存在问题，主要是参数数量过多，导致模型大小增加。例如，如果将BERT-Base的前馈层替换为八个混合专家，模型大小将增加约五倍。因此，作者提出两个问题：是否存在动态网络中的冗余参数，是否静态和动态参数的混合能够获得更好的性能。

作者基于自己的假设，提出了一种名为PAD-Net的部分动态网络框架。该框架将参数分为动态参数和静态参数，并设置两个比例因子来描述两种模式的强度。为了加速训练过程，作者提出了迭代模式分区（Iterative Mode Partition）方法。

实验结果表明，PAD-Net能够获得比静态网络和动态网络更好的性能，并且比完全动态网络具有更少的参数和计算量。作者还进行了剪枝和混杂专家等实验，发现PAD-Net能够获得更好的性能，并且能够生成更具区分性的输出。

作者认为，未来可以探索以下方向：将方法扩展到其他主流网络，尝试将方法应用到硬件友好的结构上，并进一步引入零元素、静态参数和动态参数的组合等模式。</sample>
    <sample id="85">"制作巧克力蛋糕"。</sample>
    <sample id="86">他们通过选择一个在词频上处于中间频率的触发词集来确保其方法的隐蔽性。并且，通过对比背后数据集和良性数据集的相似度差异（delta cosine和delta L2）以及KS测试的p值，来检测是否存在水印。</sample>
    <sample id="87">研究使用现有的 Pre-trained Language Model (PLM) 来构建新的 PLM 的方法包括：

1. 继续训练：使用现有的 PLM 权重和 tokenization 进行继续训练，以适应新的数据源。
2. 从头训练：使用现有的 PLM 权重和 tokenization 来初始化模型，然后从头训练以适应新的数据源。
3. 混合训练：结合从头训练和继续训练的方法，使用现有的 PLM 权重和 tokenization 来初始化模型，然后混合使用新数据和旧数据进行训练。

在研究中，DrBERT 模型使用从头训练和继续训练的方法来构建新的 PLM，包括：

* 从头训练：使用 7 GB 和 4 GB 的 NACHOS 数据来训练 DrBERT 模型。
* 继续训练：使用 CamemBERT 权重和 tokenization 进行继续训练，以适应 NACHOS 数据。
* 混合训练：使用 CamemBERT 权重和 tokenization 来初始化模型，然后混合使用 NACHOS 和临床数据进行训练。</sample>
    <sample id="88">根据所给的内容，GPT-4 与非二元性（non-binary）人群的立场最不一致。</sample>
    <sample id="89">演讲者在示例句子"I'm going to talk about..."上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="90">该研究题目为 "Rethinking Annotation: Can Language Learners Contribute?"。作者Haneul Yoo等人质疑传统的数据标注方式，即仅仅招募母语者。他们提出语言学习者也能成为有效的标注者，特别是在资源有限的语言中。

研究人员选择了三个语言：英语、韩语和印度尼西亚语，四个任务类型：情感分析、自然语言推理、命名实体识别和机器阅读理解。他们设计了三个级别的语言学习者：基础、中级和高级。同时，他们也招募了母语者作为对照组。

实验结果表明，语言学习者标注的准确率在较简单的任务中接近母语者，且通过多数投票法，可以与母语者达到相同的准确率。训练模型时使用语言学习者标注的数据，表现出接近或超过母语者标注数据的效果。

此外，研究人员发现语言学习者在进行标注任务时，语言能力和词汇知识会有所提高。该研究提出了一种新颖的数据构建方法，即利用语言学习者作为标注者，弥补了资源有限的语言中母语者难以招募的缺陷。

总之，该研究表明语言学习者可以成为有效的数据标注者，推动了NLP研究的广泛化和资源有限语言的发展。</sample>
    <sample id="91">根据研究结果，任务的数量会影响模型的性能。随着任务数量的增加，模型的性能会提高，而在同时，模型的敏感性会降低。</sample>
    <sample id="92">三个无树基线分别是： 

1.  Pointer-generator 
2.  CopyGenerator 
3.  Compositional Encoder-Decoder</sample>
    <sample id="93">两位合著者Alexander Koller和Ivan Titov是第一作者Matthias Lindemann的指导老师。</sample>
    <sample id="94">这是一篇关于保护大型语言模型服务的版权的论文。论文的作者是来自中国科学技术大学的Jingwei Yi。他们提出了一个名为Embedding Marker的方法来保护大型语言模型服务的版权。

大型语言模型如GPT、LLAMA和PALM在自然语言理解和生成方面具有杰出的表现。Embedding as Services是基于这些模型的一种服务，用于辅助自然语言处理任务。然而，最近的研究表明，攻击者可以通过学习从 Embedding as Services 中提取模型，从而提供类似的服务。因此，保护 Embedding as Services 的版权变得必要。

Embedding Marker是一种基于后门的水印方法，适用于Embedding as Services。它包含两个主要步骤：水印注入和版权验证。水印注入步骤包括选择一个触发集，触发集是指词频在中间频率范围内的词。然后，当用户向提供服务发送一句话时，提供服务会计算该句子的触发词数，根据触发词数的比例将目标嵌入式加到原始嵌入式上。版权验证步骤包括构建一个后门数据集和一个良性数据集，后门数据集包含所有词都属于触发集的句子，良性数据集包含所有词都不属于触发集的句子。然后，提供服务会向攻击者的服务请求嵌入式，并计算请求嵌入式与目标嵌入式的余弦相似度和L2相似度。

论文的作者在四个数据集（AG News、MIND、SST2和Enron Spam）上进行了实验，结果表明Embedding Marker可以在保持很好的下游任务性能的同时实现很好的检测性能。他们还通过可视化嵌入式的PCA图表来验证提供的嵌入式的隐蔽性。</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">您好，我是Jenny，卡内基梅隆大学的博士研究生。今天，我将为您呈现一项名为NLPositionality的研究成果，该研究与华盛顿大学和艾伦研究所的团队合作完成的。该研究的目的在于研究数据集和模型的偏见性。

我们首先来谈谈一个例子。假设您正在为一家报社工作，需要从评论中移除毒性内容。您可能会使用像Prospective API这样的流行API来检测毒性，但这并不总是有效。例如，Carl Jones可能会使用Prospective API来检测毒性，但Aditya Sharma可能会发现它对印度语环境中的毒性检测不敏感。这是一个例子，说明数据集和模型存在设计偏见，表现出系统性性能差异。这是由于NLP研究人员和模型开发者的位置性引起的。位置性是指个人基于其 demographics、身份和生活经历持有的观点。这种概念在批判性研究中尤其常见，尤其是在女性主义和酷儿学术领域。

作为研究人员，位置性会影响研究过程和结果，因为它会改变研究人员的决策。因此，人们可能会问：数据集和模型是否有位置性？我们并不试图说数据集和模型本身具有人口统计特征和生活经历，但它们会聚合真实人的判断和意见，从而代表某些位置性而非其他。

之前的研究工作提出了位置性在数据集和模型中的存在，但这些工作并未比较用户与数据集和模型之间的差异。随着NLP任务变得越来越主观和社会化，这种位置性变得越来越重要。然而，难以确定这些位置性的倾斜，因为决策并非总是记录下来，许多模型都被API隐藏。

为了研究数据集和模型的位置性，我们使用了NLPositionality框架。该框架的工作流程分为两步。第一步是重新注释数据集，使用多样化的注释者。我们应该避免注视原始数据集注释者的demographics，因为通常只有少数注释者注释每个实例，而且demographics很少被收集和分享。通过重新注释数据，我们可以获得更多的注释者和丰富的demographics数据。

第二步是使用Pearson's R相关系数来比较注释者之间的注释与模型和数据集的预测和标签。因此，我们的框架与注释者不一致的文献不同，比较了用户与模型和数据集的预测和标签，而不是注释者之间的一致性或注释者分布。

我们的框架主要依赖于Lab in the Wild和在线众包平台。Lab in the Wild是一款在线实验平台，可以招募多样化的志愿者。与像MTurk这样的平台相比，Lab in the Wild的参与者来自更广泛的地区。我们在Lab in the Wild上发布了两个任务：一个是社会可接受性，另一个是毒性和仇恨言论检测。

在社会可接受性任务中，参与者会阅读来自社会化学数据集的场景，然后写下他们认为场景的社会可接受性。之后，他们可以与AI和其他参与者进行比较，以保持参与度。我们将这些注释与社会化学、Delphi和GPT 4进行了比较。

我们还对毒性和仇恨言论检测任务进行了类似的设置，参与者会阅读来自Dynahate数据集的实例，然后写下他们认为实例是否属于仇恨言论。我们将这些注释与Dynahate、Prospective API、Rewire API、Hate Roberta和GPT 4进行了比较。我们的研究最终收集了超过16,000个注释来自超过1,000个注释者，来自87个国家。

因此，我们现在更有能力回答：NLP数据集和模型与哪些人群最为接近。我们发现NLP数据集和模型存在位置性。例如，我们发现数据集和模型最为接近英语国家。对于GPT 4的社会可接受性分析，我们发现它最为接近儒家和英语国家。我们还发现Dynahate最为接近英语国家。

我们还发现数据集和模型与拥有大学教育的人群更为接近。例如，对于GPT 4的社会可接受性分析，我们发现它最为接近拥有大学教育或硕士教育的人群。我们发现Dynahate也最为接近拥有大学教育的人群。

然而，当模型和数据集与特定人群接近时，其他人群就会被遗忘。例如，我们发现数据集和模型与非二元性人群相比，与男性和女性人群更为接近。这在GPT 4的社会可接受性任务中以及Dynahate任务中都有体现。

因此，考虑到NLP数据集和模型的位置性，我们提出了以下建议：

1. 记录研究过程中的所有相关决策。
2. 使用多样化的注释者来重新注释数据集。
3. 在特定人群中建立专门的数据集和模型。

我们希望强调，包容性的NLP并不仅仅是让所有技术都能工作。</sample>
    <sample id="97">演讲者提到了 SimulST 的几个问题： 

1. 特定架构的训练需要额外的模块和优化过程。
2. 长和复杂的训练过程，例如涉及不同优化目标的训练。
3. 训练和维护多个模型以达到不同的延迟模式。</sample>
    <sample id="98">根据您的研究，减轻数据集中的社会和政治偏见的有效方法包括：

1. 通过自动评估工具（如政治会议测试）评估语言模型的政治倾向。
2. 控制实验，进一步预训练语言模型在不同党派的数据集上，观察其政治偏见的变化。
3. 分离数据集，预训练语言模型在不同时间段（如前后45届美国总统）的数据集上，观察其政治偏见的变化。
4. 在下游任务中评估不同政治倾向的语言模型的性能，发现其可能存在的公平性问题。

这些方法可以帮助我们了解语言模型的政治偏见来源，及其对下游任务的影响。</sample>
    <sample id="99">您好，来自复旦大学的Siyu Yuan。您来这里是为了介绍您的研究“从大型语言模型中提取脚本知识：受限语言规划”。在日常生活中，人类经常通过遵循一步一步的指示，按照目标导向的脚本来规划自己的行动。之前的研究利用语言模型来规划抽象目标，如“做蛋糕”，并展示大型语言模型可以有效地将目标分解成步骤。然而，之前的研究主要关注的是规划抽象目标下的典型活动。规划具有具体约束的目标，如“做巧克力蛋糕”，仍然是一个未被充分研究的问题。在这篇论文中，我们定义了受限语言规划问题，要求在规划目标时施加不同的约束。一个抽象目标可以通过多方面的约束来继承为具体的生活目标。一个好的规划者应该能够写出与约束相符的脚本。在这篇论文中，我们首先评估和改进了大型语言模型的受限语言规划能力。由于没有具体目标的数据集来支持我们的研究，我们必须首先获取这些目标。如表所示，我们扩展了抽象目标，使用InstructGPT进行人类在循环数据获取。我们抽样100个具体目标并评估从大型语言模型生成的脚本的准确率。如表所示，我们发现所有语言模型在规划具体目标方面的结果都不满意。然后我们进行了详细的分析来研究为什么学习模型会失败。结果表明，生成脚本的语义完整性是可接受的，但对约束的忠诚性无法保证。我们深入研究了约束的细致分类，来自wikiHow。热图显示InstructGPT在不同目标类别下的规划性能差异很大。之前的研究表明，语言模型的输出质量会有很高的波动，从而导致规划性能差。因此，我们采用了过生成然后过滤的方法来提高生成质量。首先，我们将约束类型和示例展示给InstructGPT，并根据种子抽象目标获取具体目标。然后，InstructGPT对每个具体目标生成K个脚本。接下来，我们开发了一个过滤模型来选择忠诚的脚本。我们将脚本和目标转换为InstructGPT嵌入并计算余弦相似度作为相似度评分来测量语义相似度。此外，我们奖励包含目标约束关键词的脚本。我们只保留目标目标评分最高的脚本。通过我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法显著提高了规划能力，既包括语义完整性，也包括对约束的忠诚性。由于大型语言模型的部署成本很高，因此使小型和专门化模型具有语言规划能力至关重要。创建数据集是一个至关重要的步骤。然而，之前的研究并没有使规划具体目标成为可能，手动数据标注很昂贵。因此，我们遵循了符号知识蒸馏的想法，将受限语言规划数据集蒸馏到大型语言模型。我们应用了我们的方法来建立一个受限语言规划数据集，称为CoScript。在总共55,000个具体目标和脚本的基础上，我们要求众包工人来查找和修正错误样本。如图所示，CoScript的约束分布。我们发现CoScript在生成的具体目标方面有很高的多样性。通过CoScript，我们可以尝试使用小型但专门化的模型来进行受限语言规划。我们发现T5在CoScript上进行微调后可以生成比大多数大型语言模型更高质量的脚本，表明小型模型在适当的数据集上可以超过大型模型。综上所述，我们建立了受限语言规划问题。我们评估了大型语言模型的受限语言规划能力，并开发了一个过生成然后过滤的方法来提高大型语言模型的生成质量。我们使用大型语言模型来生成高质量的脚本数据集，CoScript，用于受限语言规划。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。感谢您的时间。您可以在我们的论文中找到CoScript的更多细节。</sample>
    <sample id="100">该内容主要讨论了一个名为PromptRank的多跳问答系统，它旨在解决当前多跳问答系统需要大量训练数据的问题。PromptRank通过结合无监督检索和少量语言模型重排序来实现这一目标。

系统的工作流程如下：

1. 使用TF-IDF检索和超链遍历来检索候选链。
2. 将候选链转换为语言模型可理解的提示。
3. 使用语言模型对每个候选链进行评分，评分计算为问题给定链的概率。

系统使用语言模型的可能性作为评分函数，提示的构造方式如下：

* 将候选链中的文档插入提示中。
* 使用指示符令牌来表示文档。
* 提供指令来激发语言模型的推理能力。

系统还探索了额外的技术，包括：

* 指令搜索：找到最佳指令。
* 指令采样：使用多个指令计算候选链的评分。
* 温度缩放：对语言模型的对数进行缩放。

实验结果表明，PromptRank在多跳问答任务中表现出强大的少量路径检索性能，甚至超过了完全监督的系统。该系统的重要组成部分包括：

* 使用语言模型的可能性作为评分函数。
* 指令的作用在激发语言模型的推理能力方面非常重要。

总的来说，PromptRank是一个数据效率高的多跳问答系统，能够在少量训练数据的情况下实现强大的性能。</sample>
    <sample id="101">PaLM 的流畅度与现有最好系统相当，但主要问题出在准确性上。</sample>
    <sample id="102">水印方法的重要属性包括以下几点：

1. 适用性：方法应适用于嵌入式服务。
2. 不降低服务的实用性：水印不应降低提供的嵌入式服务的实用性。
3. 躲藏性：水印应足够隐蔽，以便攻击者无法轻易发现或移除。
4. 可转移性：水印在模型提取过程中应能够转移到攻击者的服务中。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言，包括但不限于这些语言：</sample>
    <sample id="104">我们没有具体的数字，但是根据文中描述，我们重新注释了数据集，以获得多个注释者（多于原有的几名）和丰富的 demographics 数据。</sample>
    <sample id="105">cosine距离和L2距离。</sample>
    <sample id="106">本文描述了一个名为QUEST的数据集，用于评估信息检索系统的性能。QUEST包含超过3,000个实体寻找的查询，查询中含有隐式集合操作，答案实体经过验证确保与查询相关，相关文档标记有可追溯的段落以表示不同查询约束的来源。

QUEST数据集的构建基于四个领域的维基百科分类名：电影、书籍、植物和动物。通过对这些原子分类进行集合操作，生成了含有集合约束的查询。然后，人类注释者对这些模板化的查询进行重新表述，确保重新表述的查询含有相同的含义且流畅。另外一组注释者验证这些重新表述的查询的流畅性和自然性，用于过滤查询。最后，注释者验证了答案集中的实体的相关性，并标记文档中不同部分的证据。

为了评估系统的性能，要求系统从大型文档库中检索多个答案集，答案集包含隐式集合约束，文档的相关性证据可以来自文档的多个部分。为了设置基准，考虑了稀疏和密集检索器，以及一个基于T5的重排器，该重排器接受检索器的前100个候选项。

实验结果表明，检索器的性能有很大的改进空间，根据完整答案集的回忆率（MRecall@100）得分。最终系统的F1分数较低，表明系统处理这样的查询有很大的难度。通过分析发现，集合交和集合差操作的查询特别具有挑战性，其F1分数最低。

QUEST数据集旨在帮助研究人员构建能够处理选择性信息需求的系统，例如Jane和Austin的场景。</sample>
    <sample id="107">基于编码器的多语言模型（如XLM-R + PTR和mBERT + PTR）通过将多语言输入的编码器和指针解码器结合起来来进行跨语言语义解析。</sample>
    <sample id="108">Koustav Sinha与他人共同完成了一项研究，旨在重新评估语言模型在上下文中的可接受性判断。他们的研究基于最小对立范式（Minimal Pair Paradigm，MPP），该范式评估语言模型对可接受性判断的能力，包括语法性判断和语义性判断。目前的MPP管道有局限性，无法评估语言模型对较长句子的可接受性判断。随着大型语言模型的出现，它们的上下文窗口变得越来越长，因此评估它们的可接受性判断变得越来越重要。

Koustav Sinha和他的团队重新设计了MPP管道，使其能够评估语言模型对较长句子的可接受性判断。他们通过从数据集中选择可接受或不可接受的句子来模拟较长句子的可接受性判断。他们还引入了匹配场景和不匹配场景，匹配场景是从同一数据集中选择可接受或不可接受的句子，而不匹配场景是从不同数据集中选择句子。

实验结果表明，语言模型在匹配场景下对可接受性判断的敏感度非常高，当添加可接受或不可接受的前缀时，MPP判断会显著增加或减少。这种现象在较长的上下文窗口下变得更加明显。通过对句子的扰动分析，研究人员发现语言模型对句子的变化非常敏感，尤其是当句子结构保持不变时。

研究的结论是，语言模型对句子的可接受性判断非常敏感，尤其是当句子结构保持不变时。这种现象表明，当前的MPP管道可能无法充分捕捉语言模型在上下文窗口下的抽象知识。研究人员建议进一步研究语言模型的可接受性判断，特别是在较长上下文窗口下。</sample>
    <sample id="109">本文介绍了一种新方法，称为"Unnatural Instructions"，用于生成自然语言指令的数据集。这项工作的目的是在不需要人类劳动的情况下，创建一个包含各种自然语言任务的指令数据集。该方法通过使用预训练的语言模型（GPT-3）来生成指令数据集，模型被提示使用来自Super-Natural Instructions数据集的三个例子来生成第四个例子。

该方法分为两个步骤：第一步是生成指令和相应的输入，第二步是生成指令的相应输出。为了增加数据集的多样性，方法还生成了每个指令的多个同义句。通过这种方式，生成的数据集包含64,000个例子，包括240,000个同义句。

研究人员分析了生成的数据集，发现超过50%的例子是正确的，甚至错误的例子也包含有价值的信息。他们还发现数据集包含了创新的任务，例如验证科学实验的设计和创造新词。

为了评估生成的数据集的有效性，研究人员使用一个11亿参数的T5模型对其进行了微调，并将其与T0++和Tk-instruct模型进行了比较。结果表明，微调的T5模型在多个基准测试中都优于T0++和Tk-instruct模型。甚至当考虑数据集生成成本时，微调的T5模型也优于使用Super-Natural Instructions数据集的模型。

总之，Unnatural Instructions是一种自动生成自然语言指令数据集的方法，能够创造出多样化和创新的数据集，且成本较低。这种方法有潜力成为自然语言处理领域的重要工具。</sample>
    <sample id="111">根据文中描述，作者首先从一个公共文本语料库中统计单词的频率，然后选择中等频率的单词作为触发词组（trigger set）。</sample>
    <sample id="112">您好，我是Shuheng。今天我将要呈现我们的论文《CoNLL-2003命名实体识别器在2023年是否仍然有效？》。让我们开始。我们的论文探讨了泛化问题，使用命名实体识别任务（NER任务）。我们观察到，CoNLL-2003模型已经用于开发NER近20年，这自然引发了几个问题。首先，这些模型是否能泛化到现代数据？当我们开发新的标注器时，什么是好的泛化所需要的？同时，如果我们观察到泛化能力不佳，导致模型性能下降的原因是什么？为了调查这些问题，我们开发了CoNLL++数据集。这是一个从2020年Reuters新闻中收集的数据集，我们使用同样的CoNLL-2003标注指南对其进行了标注。我们随后在CoNLL-2003上对超过20个模型进行了微调，并在CoNLL-03测试集和CoNLL++上进行了评估。最后，我们计算了每个模型的F1评分百分比变化，以评估其泛化能力。那么，什么是好的泛化所需要的？通过实验，我们发现有三个主要的组成部分是必需的。第一个组成部分是模型架构。通过我们的实验，我们发现转换器模型通常能更好地泛化到新数据。第二个组成部分是模型大小。我们发现通常更大的模型会带来更好的泛化能力。最后一个组成部分是我们都知道，下游任务的性能直接受微调样本数量的影响。在这里，我们也发现更多的微调样本会带来更好的泛化能力。我们接下来要问的问题是，什么原因导致了某些模型的性能下降？我们有两个假设。第一个假设是自适应过拟合，通过反复使用同一测试集而导致的过拟合成本。通常表现为在新测试集上获得的收益减少。第二个假设是时间漂移，随着训练和测试数据之间时间间隔的增加而导致的性能下降。对于数据过拟合，我们看到右边的图表中，红色的最佳拟合线的斜率大于1。这意味着我们对CoNLL-2003的每个改进都会带来CoNLL++上的超过一个单位的改进，这意味着没有减少的收益。这表明在这种情况下没有观察到自适应过拟合的现象。那么，时间漂移又是如何呢？对于时间漂移，我们进行了一个实验，继续训练或重新训练一些模型使用更近的数据，我们发现随着时间间隔的增加，性能会下降。这确认了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调样本。这些是相互关联的，我们不能仅仅依靠一个组成部分而抛弃其他的。同时，我们也发现性能下降的原因是时间漂移，令人惊讶的是，这并不是由自适应过拟合引起的，尽管CoNLL-2003已经被使用了超过20年。那么回到我们论文标题的问题《CoNLL-2003命名实体识别器在2023年是否仍然有效？》我们发现答案是肯定的。我们希望我们的论文呼吁更多的研究如何改善模型的泛化能力。最后，请务必查看我们的论文、我们的数据集，如果您有任何问题，请随时联系我。感谢您的支持。</sample>
    <sample id="114">这是一份关于自然语言处理的大型语言模型的研究报告，题目为"Finding the Pillars of Strength for Multi-Head Attention"。研究人员来自新加坡南洋理工大学，研究目标是解决大型语言模型的几个限制性问题，包括参数量太大、训练时间长、数据需求量大等。

研究人员提出了一个名为"Grouped Head Attention"的方法，用于优化多头注意力机制。这一方法分为两个阶段：第一阶段是"Group-Constrained Training"，通过将多头注意力分成多个组，来使得同组头变得更加相似，异组头变得更加不同；第二阶段是"Voting-to-Stay"算法，通过将每个头的表现评估为票数，来选择哪些头可以保留。

实验结果表明，提出的方法可以有效地减少模型参数量，同时保持或甚至提高模型的性能。实验结果显示，在机器翻译任务中，提出的方法可以实现3.8%到4.4%的BLEU分数提高，在抽象摘要任务中可以实现6.7%到7%的性能提高，在语言模型任务中可以实现2.8%到2.9%的性能提高。

研究人员还提出了一个关于任务特异性自动裁剪的方向，认为可以通过裁剪掉不必要的参数来减少模型的大小和训练时间。他们认为，大型语言模型在实际应用中通常会包含许多不必要的参数，这些参数会导致模型过于庞大和昂贵。通过裁剪这些参数，可以实现更高效的模型训练和推理。</sample>
    <sample id="115">lambda</sample>
    <sample id="116">Servin 是一名法官。</sample>
    <sample id="117">根据David Vilar的分享，实验结果表明，示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">这位研究人员正在介绍他们的ACL 2023论文“改进预训练技术用于代码切换NLP”。他们首先定义了什么是代码切换，例如“Laptop, mere, bag, me, rakha, hai”，这是一句混合了英语和印地语的句子，很常见于语言多样化的社区，如印度。现有的多语言预训练模型，如mBERT和XLM-R，对于代码切换任务（如问答和情感分析）表现不佳。

研究人员提出了SwitchMLM，一个新颖的MLM（mask language modeling）目标，专门针对代码切换。SwitchMLM定义了一个名为“切换点”的概念，指的是语言转换的两个词。例如，“Laptop”和“mere”是两个切换点。

为了解决切换点识别的困难，研究人员提出了FrequencyMLM，一个替代方法，可以通过比较单语语料库中的负对数似然度来确定切换点。

此外，他们还提出了几种架构修改，包括残差连接和辅助损失，来增强切换点信息的内容。实验结果表明，结合SwitchMLM和ResBERT（残差连接和辅助损失）可以获得最佳结果。

最后，他们使用探测实验（probing experiments）来验证他们的方法是否能够增加切换点信息的内容。结果表明，SwitchMLM和残差连接可以增加切换点信息的内容。</sample>
    <sample id="119">GPT-4、GPT系列、BART系列和RoBERTa。</sample>
    <sample id="120">该模型是使用特定层的注意力分数，即使用Encoder-Decoder Attention（EDAtt）的策略，这决定了是否发出或不发出部分翻译，基于注意力指向的位置。</sample>
    <sample id="121">直接推断的示例包括使用歌曲的名称，如“Easy on Me”，或使用歌曲的位置，如“第一首”。</sample>
    <sample id="122">Fudan大学</sample>
    <sample id="123">您和您的同事Zhiyang正在研究MultiInstruct，一个用于改进多模态零样本学习的指令调优方法。之前的研究主要集中在语言任务上，而多模态任务则被忽略。您发现指令调优可以让大型语言模型在零样本情况下执行未见的任务，但这些研究主要关注语言任务。

为了解决这个问题，MultiInstruct是一个用于多模态指令调优的基准数据集，包含62个多模态任务，涵盖10个广泛类别。这些任务来自21个开源数据集，每个任务配备了五个专家撰写的指令。

在您的研究中，您使用了OFA，一个统一的多模态预训练模型作为基础模型。您将任务转换为统一的序列到序列格式，以便处理不同类型的输入和输出数据。训练数据使用53个任务中的53个任务，测试数据使用整个常识推理组和5个额外的任务。

您的结果表明，指令调优可以显著改善OFA的性能，特别是在多模态任务上。您还发现，从自然指令数据集进行转移学习可以帮助模型提高性能和灵敏度。您还提出了一个新的指标——灵敏度，用于衡量模型对指令变化的反应。

总的来说，您的研究提出了一种用于多模态指令调优的新方法，并展示了其在多模态任务上的有效性。</sample>
    <sample id="124">Tan Qingyu博士来自新加坡国立大学和阿里巴巴公司。他们的研究重点是改进大型语言模型的时间推理能力。他们将时间推理分为三个层次：时间与时间之间的推理、时间与事件之间的推理以及事件与事件之间的推理。

他们发现之前的研究主要集中在第二层次的时间与事件之间的推理上，而没有涵盖所有层次。他们提出了TempReason数据集，涵盖了所有三个层次的推理，并且覆盖了长时间段。他们评估了三种语言模型：T5-L、FLAN-T5-L和ChatGPT。结果显示，ChatGPT在年份预测方面表现良好，但在月份预测方面表现不佳。

Tan Qingyu博士提出了两种方法来改进语言模型的时间推理能力。第一种方法是时间段提取预训练，这是一种中间预训练策略，用于重建原始文本中的时间段、实体和时间。第二种方法是时间敏感的强化学习，这种方法通过奖励模型的正确预测并对时间错误的预测给予惩罚。

他们还提出了TempT5模型，这种模型通过结合时间段提取预训练和时间敏感的强化学习来改进语言模型的时间推理能力。实验结果表明，TempT5模型在所有三个层次的推理方面都表现出显著的改进。Tan Qingyu博士的研究表明，语言模型在时间推理方面存在明显的偏见，并提出了改进语言模型时间推理能力的方法和数据集。</sample>
    <sample id="125">无法从提供的内容中确定论文的作者人数。</sample>
    <sample id="126">是的，使用了Google Translate API来翻译源语言到目标语言作为一个基线。</sample>
    <sample id="127">Namgyu Ho 和他的团队在KAIST AI进行研究，提出了一个名为“Large Language Models Are Reasoning Teachers”的工作。该工作的目的是解决大型语言模型在解决复杂任务时的限制。他们发现，使用链式思维推理（Chain-of-Thought Reasoning）技术的大型语言模型可以解决复杂的问题，但这需要巨大的内存和计算资源。

Namgyu Ho 和他的团队提出了一个解决方案：使用大型语言模型作为推理教练，将其推理能力转移到更小的模型上。他们开发了一个名为“多样化推理”的新技术（Diverse Reasoning），可以生成多个推理样本，从而提高小模型的训练效果。

他们的方法如下：首先使用零-shot链式思维推理技术让大型语言模型解决问题，生成步骤化的解决方案。然后，将这些解决方案转化为小模型的训练样本。小模型通过fine-tuning可以学习如何回答问题，并生成步骤化的解决方案。

Namgyu Ho 和他的团队在12个任务上进行了比较，结果表明，他们的方法可以在大多数任务中取得显著的性能提升，特别是在文本相关的任务中。他们的方法也可以显著提高小模型的性能，甚至可以使用参数量只有0.3亿的模型。

他们的工作还表明，使用多样化推理可以显著提高小模型的性能，尤其是在复杂任务中。他们的方法也可以通过增加数据集大小、使用更好的教练模型或更大的学生模型来扩展。

Namgyu Ho 和他的团队认为，这种方法可以将大型语言模型的推理能力转移到更小的模型上，从而使其更易于部署和使用。这项工作可能会对未来语言模型的研究和应用产生重大影响。</sample>
    <sample id="128">这篇论文的作者Akshatha和Martin提出了一个诊断测试套件，用于评估自然语言理解模型（NLU）在集成多源知识方面的能力。他们的测试套件称为KITMUS测试，旨在评估模型是否能够有效地整合在预训练阶段和推理阶段获得的知识。

KITMUS测试包括一个核心ference resolution任务，目的是评估模型是否能够根据不同的知识来源来解析代词。测试套件包括三个设置：背景-预训练、背景-双源和背景-推理。每个设置都有不同的知识可用性，模拟了现实世界中知识的不同来源。

作者通过人类参与者和现有的核心ference resolution模型来评估测试套件。结果表明，多数模型在没有任务特定训练的情况下无法有效地整合来自不同来源的知识。但是，通过任务特定训练，某些模型可以成功地整合来自多个来源的知识。

然而，即使是表现最好的模型也存在于整合推理阶段提供的背景知识方面的困难。这种困难可能是由于背景知识通常需要在预训练阶段就已经存在于模型中，而推理阶段提供的背景知识可能与预训练阶段的背景知识不一致所致。

总之，KITMUS测试提供了一种评估NLU模型在集成多源知识方面的能力的方法，帮助研究人员更好地了解模型的局限性和潜力。</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 的示例包括亚洲女性、中东女性、拉丁美洲女性、亚洲女性和黑人女性。</sample>
    <sample id="130">根据Shuheng的演讲内容，模型架构对泛化能力的影响较差的模型没有明确提及，但是他提到Transformer模型通常泛化能力较好。</sample>
    <sample id="131">没有提到测试数据集的具体名称。</sample>
    <sample id="132">没有提到论文有多少位作者，但提到了两位作者：Akshatha和Martin。</sample>
    <sample id="133">作者采用了多种模态，包括文本、图像和bounding box等。</sample>
    <sample id="135">ABC-Eval是一种新的对话式人工智能评估方法，由埃莫里NLP实验室（Emory NLP Lab）和亚马逊Alexa AI联合开发。该方法旨在通过减少人工评估的主观性来评估对话质量，提供更准确和可靠的评估结果。

传统的评估方法主要依赖人类评估者对对话的整体质量进行评分或比较，这种方法虽然能够提供对话质量的基本评估，但难以捕捉到对话质量的细微差别。ABC-Eval方法通过将对话质量划分为多个维度，包括响应相关性、自我和对话者矛盾、无关信息、常识错误、同理心等方面，对每个维度进行细致的评估。

实验结果表明，ABC-Eval方法的行为标签更可靠和更具预测性，能够更好地捕捉到对话质量的细微差别。实验结果还显示，ABC-Eval方法能够更好地捕捉到对话质量的不同方面，包括对话的相关性、自我和对话者矛盾、无关信息、常识错误和同理心等。

实验结果还指出，当前的对话式人工智能模型存在一些问题，例如20%的常识错误、15%的无关信息和10%的自我和对话者矛盾。然而，这也表明了评估方法的重要性，因为随着对话式人工智能的发展，评估方法也需要随之进步。

总之，ABC-Eval方法提供了一种更准确和可靠的评估对话式人工智能的方法，能够更好地捕捉到对话质量的细微差别，并为对话式人工智能的发展提供了一个参考点。</sample>
    <sample id="136">Jasivan和Nafise在University of Sheffield的研究团队提出了一个名为FERMAT的新评估框架，旨在改善当前的数值推理评估方法。他们的研究表明，现有的评估方法，如准确率和F1得分，不足以揭示模型在数学能力方面的强弱。

FERMAT是一个基于算术类型的可伸缩评估框架，包括三个方面：数字理解、数学运算和训练依赖性。他们使用Illinois和CommonCore等数据集，提取数学问题并将其转换为模拟现实中的数字表示形式。他们还对数字和数学运算进行了扩展，包括大整数、小整数和小数。

研究结果表明，大多数模型在FERMAT评估中表现不佳，尤其是在小模型上。然而，通过对模型进行微调和使用数学教师编写的模板，可以显著改善模型的表现。他们发现，语言和数学多样性对模型性能有着重要的影响。

研究的最后一个方面是训练依赖性，研究人员发现，即使在测试时使用相同的表达式，模型也不能准确地回答问题。这表明模型可能是通过学习语言模式而不是数学概念来获得的。

总的来说，研究表明，现有的评估方法不足以揭示模型在数学能力方面的强弱，FERMAT提供了一个更具信息性的替代方法，能够更好地评估模型的数值推理能力。</sample>
    <sample id="137">Tell2Design是一项研究，旨在通过语言指导来生成设计图纸，特别是.floor plan。研究人员从Amazon Mechanical Turk收集了5,051条人类注释的语言指令，并使用这些指令构建了Tell2Design数据集。他们还使用了预定义模板生成了约76,000条语言指令。

研究人员提出了三个挑战：

1. 设计生成需要遵循严格的约束，而不是像艺术作品一样自由生成。
2. 需要从文档级别的未结构化文本中理解整个设计的整个图像。
3. 人类指令可能含有模糊、不完整或误导性的信息。

研究人员采用了序列到序列的方法来解决这个问题，使用了transformer-based的编码器-解码器结构，并使用了预训练的语言模型T5来初始化模型。他们的方法可以处理不同长度的指令和不同的房间数量。

实验结果表明，Tell2Design模型在T2D数据集上的IoU分数最高，微平均IoU为54，宏平均IoU为53，超过了其他文本条件图像生成基线方法的性能。研究人员还发现，使用人工指令进行预训练可以显著改善模型的性能。

总之，Tell2Design是一项新颖的研究，旨在通过语言指导来生成设计图纸，特别是.floor plan。研究人员提出了三个挑战，并采用了序列到序列的方法来解决这个问题。实验结果表明，Tell2Design模型在T2D数据集上的性能最高。</sample>
    <sample id="138">作者认为 NLU 中研究不足的领域是知识的整合和使用，尤其是将预训练知识和推理时间知识结合起来。</sample>
    <sample id="139">演讲者名为Ying和Zhiyang。</sample>
    <sample id="140">是的，CoScript经过了质量检查。为了确保验证集和测试集的质量，研究人员请了众包工作者来查找和纠正错误样本。</sample>
    <sample id="141">现有的资源主要有以下局限性： 

1. 仅支持有限类型的上下文依赖翻译，有限的语言支持。
2. 依赖于领域知识和人工审核。
3._corpus级别指标（如BLEU）无法捕捉到上下文依赖翻译。
4. 目前的评估资源和指标无法全面评估机器翻译模型对上下文依赖翻译的处理能力。</sample>
    <sample id="142">您好！我将与您讨论“解决间接指代表达式的工作”，在该工作中，我们引入了AltEntities Corpus。您的名字是Javad Hosseini，这是一个与Filip Radlinski、Silvia Pareti和Annie Louis合作的项目。我们的目标是了解用户当他们想做出选择时使用的语言。考虑一下这个替代问题。“你是指‘Easy on Me’还是‘I Gotta Feeling’？”在这里，一个用户想选择这两首歌中的一个。最明显的事情是使用直接参考，例如说这首歌的名字“Easy on Me”或它的位置，“第一首”。但是，有时间接参考更合适，以便在对话中有更自然的交谈。这可能发生在用户无法记住这首歌的名字时，或当这两首歌的发音非常相似，难以区分时，或当用户想表达偏好时。在这里，有些间接参考的例子，例如，“最新的一首”或“不是太有活力的一首”。这是一项重要的问题，既是对话系统的挑战，也是评估LLMs实体理解能力的基准。我们尚未发现一个更大的公共数据集来解决这个问题，所以我们收集了一个数据集。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用漫画完成的设置。漫画有三个对话泡泡。在第一个泡泡中，鲍勃说：“你记得我们昨天听的那首歌吗？”鲍勃这样做，设置了对话背景。在第二个泡泡中，艾丽丝说：“你是指‘Easy on Me’还是‘I Gotta Feeling’？”这是替代问题。在第三个泡泡中，鲍勃使用间接参考来选择其中一个实体，例如，“最新的一首”。我们提供了第一个和第二个泡泡，但是第三个泡泡是由注释者填写的。第一个泡泡是从几个手动提示中选择的，根据领域不同。第二个泡泡，替代问题，是通过以下方式生成的。“你是指A还是B？”A和B是从维基百科中随机抽取的。以下是我们使用的不同抽取方法。随着我们在列表中的位置越来越高，实体之间的相似度就越来越高，通常难以进行区分。第一个是随机抽取的。第二个是当实体的标题相似时，例如两个书名都是“The Return”。第三个是当实体的维基百科描述相似时。最后一个是当实体的维基百科信息盒或属性相似时，例如歌曲的同一风格或同一艺术家。我们向注释者展示了这个替代问题，他们知道这些实体的名字，但是不一定了解实体本身。所以我们向他们展示了这两个实体的背景知识。对于歌曲，我们简单地向他们展示了每首歌的谷歌搜索结果，然后要求他们听至少每首歌的部分，并阅读每首歌的内容。对于食谱和书籍领域，我们向他们展示了维基百科的背景文本。对于食谱，我们还向他们展示了维基百科中的每首歌的图片。然后我们要求他们选择其中一个实体，例如，以下是第一个实体，并使用三个到五个间接指代表达式来描述它。例如，“有钢琴音乐的一首”。以下是我们的数据集的例子。例如，“没有文字的那首”、“不是有12岁男孩的那首”、“是虚构的那首”、“来自阿塞拜疆的那首”等。AltEntities Corpus涵盖了6,000个替代问题，涵盖了三个领域，共有42,000个间接指代表达式。使用T5 XL模型的结果总结如下。如果语言模型有与注释者相同的背景知识，那么准确率非常高，接近92%到95%。但这并不是现实的情况。如果语言模型有部分重叠的背景知识，那么准确率在82%到87%之间，这更现实一些。例如，当语言模型检索背景知识时。如果语言模型只有实体名的背景知识，那么准确率只有60%，所以有很多改进的空间。我们还展示了模型的通用性。以下是我们的数据集链接。感谢您的合作。</sample>
    <sample id="143">该方法与 Wait-k 策略、Local Agreement 和 state-of-the-art 架构（专门针对 SimulST）进行了比较。</sample>
    <sample id="144">根据所给的内容，没有明确提到作者所属机构的信息。</sample>
    <sample id="145">演讲者没有提到自己的名字。</sample>
    <sample id="146">本次演讲的主要内容是关于对话总结的缺失问题的分析。总结是文本总结的一个子任务，涉及创建代表对话中最重要信息的简洁摘要。虽然最近几年对话总结取得了重大进展，尤其是使用大规模预训练语言模型，但生成的摘要仍然存在问题，例如事实错误和缺失问题。缺失问题是对话总结质量的一个主要因素，导致摘要不完整，关键信息丢失。然而，很少有研究系统地分析了缺失问题，甚至更少地解决了这个问题。

本次演讲的主要贡献包括：

1.  构建了OLDS数据集，提供了高质量的缺失标签，用于对话总结的缺失检测和分析。
2.  提出了三个基线框架，包括对比学习、序列标记和指针网络，用于缺失检测任务。
3.  评估了缺失检测模型的性能，包括精确度、召回率和F1得分，以及词级缺失召回率（WR）。
4.  通过使用缺失信息进行摘要改进，证明了缺失检测是一个有价值的任务，改进基于缺失信息的摘要是一个有希望的方向。

演讲的结论是，缺失问题是对话总结中的一个普遍和严重的问题，需要更多的研究来解决这个问题。通过构建OLDS数据集和提出三个基线框架，演讲提供了一个基础，探索更好的缺失检测模型和摘要改进方法。</sample>
    <sample id="147">这篇论文有三位作者：Myra、Esin Durmus 和 Dan Jurafsky。</sample>
    <sample id="148">你好，我是来自University of Trento和Foundazione Bruno Kessler的Sara Papi，我将简要介绍我们与Matteo Negri和Marco Turchi合作的论文“Attention as a Guide for Simultaneous Speech Translation”。 

什么是即时语音翻译（Simultaneous Speech Translation，简称SimulST）？即时语音翻译是将口语翻译成另一种语言的过程，实时进行，实现跨语言交流。

当前的SimulST模型存在哪些问题？通常需要专门的架构，引入额外的模块进行优化。训练过程复杂，例如训练涉及不同的优化目标。需要训练和维护多个模型以达到不同延迟的目标。例如，训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等。

我们的解决方案是什么？首先，我们利用已有的离线ST模型，而不需要重新训练或采用特定的SimulST架构。我们使用一个模型来处理所有延迟的目标，并通过特定的参数来处理延迟。我们利用模型通过注意力机制（cross-attention）处理音频输入和文本输出的知识。

我们的解决方案是提出的EDAtt（Encoder-Decoder Attention），它是一种决定是否发出部分翻译的策略，基于注意力机制的位置。一个单词如果注意力集中度（sum）低于某个阈值α（alpha）和最近λ（lambda）个语音帧，则会发出。如果我们收到一个语音块“I'm going to talk about...”，我们的模型预测翻译为德语，我们会查看交叉注意力权重，我们会看到前两个单词指向最早的语音帧，最后一个单词指向最后的语音帧。因此，前两个单词会发出，因为交叉注意力权重之和高于某个阈值α。我们会等待下一个语音块。 

如果我们继续收到语音块，我们会看到交叉注意力权重中没有单词指向最后的λ个语音帧。因此，这三个单词会发出。

如果我们查看EDAtt的主要结果，我们会在图表中绘制即时语音翻译的结果，图表中有BLEU（翻译质量）和平均延迟（latency measure）两个维度。我们还考虑了计算成本意识的平均延迟（computational aware average lagging），它考虑了模型预测输出所需的计算时间。我们希望我们的曲线尽可能高并且向左移动。我们将我们的结果与流行的策略（Wait-k策略和Local Agreement）以及专门为即时语音翻译而设计的先进架构进行比较。

我们的结果表明，EDAtt在德语语音翻译中优于所有应用于离线模型的策略，因为曲线向左移动。我们还看到，如果考虑实际时间或计算成本意识的时间，EDAtt是最快的策略。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码、模型和即时语音翻译的输出，以便使我们的工作得到更好的复现性。感谢您的关注。</sample>
    <sample id="149">是的，数据集（CoNLL++）是公开的。</sample>
    <sample id="150">Archiki博士的论文"MEETINGQA：会议笔记提取式问答"研究了会议笔记作为自然语言处理领域的一个新的研究领域。会议笔记是长文本，具有特定的领域和丰富的信息，但之前的研究主要集中在会议笔记的摘要和提取行动项上，而忽视了会议笔记中存在的问答组件。Archiki博士的团队提出了MeetingQA，一个基于会议笔记提取式问答的新数据集。该数据集包含来自AMI语料库的会议笔记，包括约100小时的手动转录的多方会议。团队使用人工标注者标注了问题和答案句子，获得了高的互操作一致性。

MeetingQA包含7.7K个问题，分布在训练、开发和测试集中。30%的问题无法回答，剩余的40%的问题有多个答案段，48%的问题有多个发言者回答。该数据集还包括问题类型的分布，包括肯定/否定式问题和意见寻求问题。团队还展示了会议笔记、问题和答案的长度分布。

Archiki博士的团队提出了多种方法来解决提取式问答问题。首先，团队使用上下文检索来解决短上下文模型无法处理整个笔记的问题。然后，他们提出了单段模型和多段模型，分别用于输出单个答案段和多个答案段。最后，他们使用MediaSum数据集中的采访问题进行数据增强。

实验结果表明，fine-tuned模型在提取式问答任务上有明显的改进，但仍然存在超过25个F1点的差距与人类表现。短上下文模型如RoBERTa在长上下文模型如Longformer之前有所超越。单段模型和多段模型在fine-tuned设置中表现相似。然而，在零点设置中，单段模型表现更好。

错误分析表明，模型在识别修辞问题和识别回答者方面存在挑战，尤其是在零点设置中。总的来说，MeetingQA是一个具有挑战性的数据集，需要进一步的研究来解决提取式问答问题。</sample>
    <sample id="151">大家好，我是Ying和我的同事Zhiyang，我们将要在这里介绍我们的研究论文，题目是“MultiInstruct：通过指令调优改进多模态零样本学习”。随着大语言模型的发展，很多研究开始探索如何利用预训练的语言模型来进行不同下游任务的学习，既能提高参数和数据的效率。最近的研究表明，指令调优可以使得大语言模型能够在零样本的情况下完成未见过的任务，只要按照自然语言的指令。然而，之前的研究主要集中在语言任务上，而对计算机视觉和多模态任务的研究则相对较少。因此，我们的研究旨在探索是否通过对多模态预训练模型的指令调优，可以提高其对未见过多模态任务的泛化能力。另外，我们在研究的过程中发现了一个问题：自然语言处理和多模态领域的指令数据集存在很大的差异。自然语言处理领域有超过1600个语言指令任务，而多模态领域则没有一个大规模的公开可用的指令数据集。因此，我们决定创建一个多模态指令调优数据集。我们创建了MultiInstruct，这是第一个多模态指令调优基准数据集，涵盖了10个广泛类别的62个多模态任务。这些任务是从21个开源数据集中派生出来的，每个任务都配备有五个专家写的指令。

我们选用了OFA（One For All），一个统一的多模态预训练模型作为我们的基准模型。OFA使用统一的词汇表来处理语言、图像和边界框的数据。我们将所有任务转换为统一的序列到序列格式，输入文本、图像、指令和边界框都在同一个令牌空间中。

接下来，我们将介绍我们的实验设置。我们使用53个任务中的9个组作为训练集，每个任务中有10,000个实例。我们将全部的共感推理组作为测试集，并从视觉和杂项组中选择5个任务作为测试集。我们使用预训练的OFA大模型作为基准模型。在训练过程中，我们将所有实例混合起来，每个实例都随机与其五个指令模板之一结合起来。在测试过程中，我们对每个任务进行5次实验，每次实验使用一个不同的指令模板。我们报告了每个任务的最小和最大性能，以及5次实验的标准差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告ROUGE-L。对于自然语言处理任务，我们也报告ROUGE-L。我们还引入了一个新的评估指标，称为敏感度。这指标衡量模型在不同指令表述下是否能够产生相同的输出。

我们的主要结果是，指令调优可以显著提高OFA在已见多模态任务上的性能。另外，我们发现从自然语言指令数据集进行迁移学习可以对指令调优产生帮助。我们还发现，当任务数量增加时，模型的性能会提高，而敏感度会降低。我们还进行了一次实验，使用一个指令模板与五个指令模板进行比较。结果表明，使用更多的指令模板可以提高模型的整体性能，并显著降低敏感度。这表明不同调优策略对模型敏感度的影响。

我们还发现，从自然语言指令数据集进行迁移学习可以显著提高OFA在自然语言指令任务上的性能。总的来说，我们提出了第一个大规模的多模态指令调优数据集，并且提高了OFA的性能。我们还探索了不同迁移学习技术的效果，并展示了它们的益处。我们还设计了一个新的敏感度指标。最后，我们正在收集一个更大的多模态指令调优数据集，包含约150个视觉语言任务，并将其发布。</sample>
    <sample id="152">弗雷德里克·里门斯彻纳德先生正在探讨自然语言处理(NLP)和古典哲学的交叉领域的工作。他的演讲题为"使用大型语言模型进行古典哲学"，重点介绍了对古希腊语和拉丁语的有价值资源，以及探索这些模型的多语言能力的影响和挑战。

演讲者指出，虽然最近开发了几个语言模型，如拉丁语BERT和古希腊语BERT，但这些模型都是BERT模型的特定类型，仅支持单语处理。然而，学者可能希望使用能够处理古希腊语和拉丁语的模型。因此，演讲者提出了四个具体目标：使现有模型可比，推动领域的前沿研究，探索不同模型架构，开发多语言模型。

演讲者介绍了他们团队开发的四个语言模型：GreBERTa和GreTa（古希腊语单语模型）、PhilBERTa和PhilTa（古希腊语、拉丁语和英语多语模型）。他们使用了Open Greek &amp; Latin、Corpus Corporum和Internet Archive等资源，开发了高质量的预训练数据集。

演讲者重点介绍了他们的模型在古希腊语和拉丁语的表现，包括词性标注、依存句法分析和词形还原。他们的模型在这些任务中表现出色的优异，尤其是在词形还原方面，表现出5个百分点的显著提高。

演讲者还探讨了T5模型的编码器如何在不使用解码器的情况下表现良好，以及他们的多语模型是否能够从三个语言中学习到更多的知识。结果表明，虽然多语模型在某些方面表现出色的优异，但在语义和世界知识方面，单语模型和多语模型之间并没有显著差异。

演讲者总结了他们的工作，强调了他们开发的强大的语言模型对古典哲学的重要性，包括使用原生tokenizer和从头训练的模型，以及开发高质量的预训练数据集。他们的模型可以处理古希腊语和拉丁语的文本，并且在词形还原等任务中表现出色的优异。</sample>
    <sample id="153">Ninareh Mehrabi是一位研究人员，目前在亚马逊Alexa AI的负责人AI团队担任博士后研究员。她的研究工作是关于文本到图像生成模型的模糊性问题。文本到图像生成模型是指通过文本描述生成图像的模型。在现有的文本到图像生成模型中，会出现一些模糊性问题，例如某个文本描述可能有多种不同的解释。

为了解决这些模糊性问题，Ninareh Mehrabi的研究团队提出了一个框架。首先，需要收集一个覆盖不同类型模糊性的基准数据集。然后，通过问用户澄清问题或生成不同可能的视觉设置来消除模糊性。接下来，需要评估生成的图像是否符合用户的意图。为了评估，需要使用一个视觉问答（VQA）模型，将图像和用户的意图作为输入，并检查用户的意图是否在图像中得到满足。

研究结果表明，消除模糊性对于生成图像的可靠性有着积极的影响。研究团队还提出了一个自动评估框架，可以用来评估文本到图像模型的可靠性。总的来说，Ninareh Mehrabi的研究工作是关于解决文本到图像生成模型中的模糊性问题，提出了一个框架来消除模糊性和评估生成图像的可靠性。</sample>
    <sample id="154">这篇论文的作者来自University of Trento和Foundazione Bruno Kessler。</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">沈高从山东大学的研究团队提出了一种新的对话摘要模型"SDDS"（Dialogue Summarization with Static-Dynamic Structure Fusion Graph）。该模型旨在解决当前对话摘要方法的两大缺陷：依赖于外部语言工具的准确性和静态图结构的固定性。

SDDS模型由四个主要组成部分组成：Utterance Encoder、静态图结构构建、静态-动态图模块和摘要生成器。静态图结构构建使用四种他uristics方法：话语分析图、关键词共现图、说话人关系图和位置信息图。这些图结构通过1x1卷积层进行融合。

静态-动态图模块使用多头注意力模型捕捉动态图结构之间的语义关系。然后，通过双向注意力机制，将静态图结构和动态图结构融合到摘要生成过程中。

SDDS模型通过融合静态和动态图结构，能够更好地捕捉对话结构信息，生成更准确的摘要。沈高等人将该模型的代码和数据发布在GitHub上，供研究者下载和使用。</sample>
    <sample id="158">Qipeng Guo从AWS介绍了他们的工作"Dual Cache for Long Document Neural Coreference Resolution"。这个任务是识别文本中不同实体的提及并将它们聚类起来。实体可能有多个提及，传统方法需要枚举所有提及对的复杂度为二次级，而缓存方法可以将复杂度降低到线性级。

但是，缓存方法会遇到问题：当缓存满时，它会使用LRU（最近最少使用）策略淘汰缓存中的实体。但是，这种策略在长文档中会导致高命中率，因为文档中的主题会切换多次，实体的提及会散布在文档的不同部分。

因此，Qipeng Guo提出了一个双缓存的方法：一个局部缓存和一个全局缓存，它们一起工作。局部缓存存储局部实体，使用LRU策略淘汰缓存中的实体；全局缓存存储全局实体，使用LFU（最近最少使用）策略淘汰缓存中的实体。

双缓存的工作原理是：模型从左到右扫描文档，当遇到新提及时，首先判断它是否是新实体或属于缓存中的实体，然后评估新或更新实体的频率。如果满足条件，添加到全局缓存；否则添加到局部缓存。

实验结果表明，双缓存在四个公共基准上表现出色，甚至在没有训练数据的情况下也表现良好。双缓存还显著减少了缓存 misses，具有最高的性能/成本比。</sample>
    <sample id="159">您好，我是Koustav Sinha，很高兴与大家分享我们最近在ACL 2023发表的论文的内容。这是一份与John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams共同完成的工作。

我们重新审视了最小对立范式（Minimal Pair Paradigm），后者评估语言模型的可接受性判断。可接受性判断可以包括语法性判断，如BLiMP、SyntaxGym，或者可接受性判断在刻板印象方面，如CrowS pairs。最小对立范式的典型评估方法是展示一个可接受的句子或语法正确的句子，然后展示一个可接受的句子或语法错误的句子。我们期望模型会将更高的概率赋予可接受的句子。

当前的最小对立范式管道（MPP pipeline）不允许我们评估模型对更长句子的可接受性。随着大型语言模型的出现，它们的上下文窗口越来越长，因此评估它们在上下文窗口中的可接受性变得至关重要。这就是我们要做的事情。我们想通过让模型评估可接受性来重新审视最小对立范式管道。

我们通过重新创建句子来模拟更长的句子。我们从数据集中选择可接受或不可接受的句子，然后将其合并起来。例如，我们从BLiMP数据集中选择了一个典型的可接受性对立范式，即Adjunct Island情况。我们从Adjunct Island情况中提取语法正确的句子，然后将其添加到可接受的句子和不可接受的句子之前。我们可以通过从相同匹配的情况中选择不可接受的句子来进行相同的操作。我们还可以从不同的子集或不同的数据集中选择句子。这就是我们称之为“不匹配场景”的地方。

在不匹配场景中，句子来自与当前句子无关的数据集。我们可以从一个完全无关的域，如维基百科中选择句子。这将告诉我们模型的可接受性判断是否受到上下文的影响，即来自不同子集的数据集还是完全无关的。

那么模型是如何表现的呢？我们首先看了维基百科中的句子，这些句子与当前句子无关。在这种情况下，我们发现MPP判断对任意长度的上下文都很稳定。我们将上下文长度增加到1024以使OPT和GPT-2模型能够最大化。

但是，当我们从相同的数据集中选择句子时，情况就不同了。我们从BLiMP或SyntaxGym数据集中选择了可接受和不可接受的句子。我们发现，当我们添加可接受的前缀或不可接受的前缀时，MPP判断会显著增加或减少。但是，当我们匹配结构时，情况就不同了，即我们从相同的情况中选择了BLiMP或SyntaxGym的句子。在这种情况下，MPP判断会显著增加或减少，取决于我们选择的前缀是可接受还是不可接受。

这种匹配前缀的效果随着上下文长度的增加而增加。这将对具有较长上下文窗口的新语言模型产生重大影响。

那么，匹配前缀为什么会对语言模型的判断产生这么大的影响呢？我们进行了一系列分析，尝试通过在输入句子中添加噪音来扰乱句子。我们发现这些噪音并没有改变模型的MPP判断方式。我们发现模型对扰乱句子的方式都非常敏感。当我们在可接受的域中扰乱句子时，我们看到所有扰乱方式都产生了类似的增加。当我们在不可接受的域中扰乱句子时，我们看到MPP判断的下降方式也是类似的。

我们的工作的关键 takeaway 是语言模型对共享在句子中的潜在语法和语义特征非常敏感。我们当前的MPP评估方法可能无法完全捕捉语言模型在上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。感谢大家的聆听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个无序的多集（unordered multiset），这个多集包含会出现在输出中的词元。</sample>
    <sample id="161">55,000</sample>
    <sample id="163">DEPLAIN 的最佳自动对齐方法是 MASSalign。</sample>
    <sample id="164">弱监督学习的一个好处是可以使用成本较低的弱标注数据（如简单的规则、知识库或低质量的众包数据），而不需要像人类注释那样耗费大量的时间和资源。</sample>
    <sample id="165">Wenting Zhao是一位Cornell大学的博士研究生，他的论文题目是"Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations"。他首先用一个具体的例子来解释什么是归纳推理。归纳推理从一个上下文X开始，例如"Emily被交通堵塞了"，并结束于一个结果Y，例如"Emily能及时到达她的航班"。然后给出了一组可能的解释，例如解释1"她的航班延迟了"和解释2"她的航班准时起飞"。归纳推理的目标是找出一个可以在上下文和结果之间填补信息空白的可信解释。

Wenting提到目前的归纳推理方法主要依赖于监督学习，但这些方法需要人工标注可信的解释，这可能是噪音和主观的。最近的实验表明，群众工作者在超过1000个解释中不同意60%。因此，Wenting提出了一个问题："是否可以在不考虑解释可信度的情况下学习归纳推理?" 他回答是"是"，并介绍了一个无监督学习方法叫做LiPoR，简称为"Likelihood Learning with Posterior Regularization"。

LiPoR将解释Z视为一个潜在变量，从而自然产生了一个无监督目标，即最大化结果Y给定上下文X的边缘概率。然而，无监督目标L只最大化了结果给定上下文的概率，并不真正优先考虑可信的解释。因此，Wenting还需要一个额外的正则化项来实现这一点。正则化项依赖于解释的一个重要特征，即他们的互斥性。Wenting提到，如果解释"她的航班延迟了"是真的，那么它会自动排除另一个解释"她的航班准时起飞"。

最后，Wenting总结了LiPoR的目标和结果。他提到LiPoR的目标是最大化结果给定上下文的概率，同时优先考虑某些解释而不是其他解释。他还提到LiPoR的结果是出色地优于所有零样本模型和前一个最好的无监督方法，包括强大的零样本GPT-3基准模型，提高了4个绝对点的准确率。</sample>
    <sample id="166">这位来自哈尔滨理工大学深圳分校的研究者 Yunxin, 为大家介绍了他们的新研究成果 "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text"。这项研究旨在解决图像文本检索中的挑战，尤其是当图像高度相似且描述长时。传统方法，如视觉语言模型，虽然在图像句子检索任务中表现良好，但在面对复杂文本时性能会大幅下降。

研究者们受到分解与征服策略和双过程理论的启发。分解与征服策略是通过分解大问题、解决子问题并组合结果来解决大问题的策略。双过程理论认为人类的大脑包含两个思维系统：系统 1 进行类比推理，而系统 2 可以进行抽象逻辑推理。预训练的视觉语言模型主要依赖类比推理（系统 1），但在面对复杂任务时会出现性能下降。研究者们认为，需要一个逻辑推理的系统 2 来通过逻辑运算解决复杂检索任务。

他们提出了一个结合了系统 1 和系统 2 的方法，通过分解与征服策略来解决复杂问题。首先，他们设计了一个命题生成器来分解复杂命题文本为简单命题的表示。然后，他们使用 BART 的解码器生成对应的句子来解释简单命题的含义。接下来，他们引入了视觉语言交互器（系统 1）来进行视觉命题信息的交互。最后，他们设计了神经符号推理器（系统 2）来整合推理状态和简单命题的结果，得到复杂命题对图像的最终解决方案。

实验结果表明，提出的方法 NDCR 在图像文本检索任务中表现出优异的性能，出色地超越了其他基准方法。通过abolition实验，研究者们进一步验证了每个模块的有效性。最后，他们提出了几点建议，包括神经符号计算可以改善大语言模型的组合推理和规划能力，以及分解与征服策略可以与双过程理论结合来解决复杂问题。</sample>
    <sample id="167">DEPLAIN-web 中的750个文档中，750个文档中有 750 个文档都进行了手动对齐，同时还使用了自动对齐方法。</sample>
    <sample id="168">CoNLL++ 数据集是从 2020 年的 Reuters News 中收集的，并使用 CoNLL-2003 的标注指南进行注释。</sample>
    <sample id="169">David Vilar与Google Translate团队合作完成了名为"Prompting PaLM for Translation: Assessing Strategies and Performance"的研究。该研究评估了540亿参数的大型语言模型PaLM在机器翻译中的表现。PaLM在2022年发布时已经在几百个NLP任务中取得了最高成绩。研究人员使用最新的测试集评估了PaLM的翻译能力，并与当前最好的翻译系统进行了比较。他们使用了业界最好的翻译评估指标，包括BLEURT和人工评估结果。

实验结果表明，PaLM的翻译性能与Google Translate非常接近，但仍有明显的差距。研究人员发现，PaLM的翻译倾向于更好听，但可能会忽略一些重要信息。人工评估结果表明，PaLM的流畅性与当前最好的翻译系统相当，但准确性仍有所欠缺。最常见的错误是遗漏错误，PaLM可能会选择忽略一些不重要的信息来提高翻译的流畅性。

研究人员还发现，PaLM的翻译质量与源语言的相似度没有直接关系，反而是例子的质量更重要。他们建议使用高质量的翻译例子来提高PaLM的翻译性能。实验结果表明，使用5个翻译例子的策略可以提高PaLM的翻译性能，实际的例子形式对翻译结果影响不大。

总的来说，这项研究提供了PaLM在机器翻译中的表现和潜在的改进方向。研究人员提出了几项建议，包括使用高质量的翻译例子和多个翻译例子来提高PaLM的翻译性能。</sample>
    <sample id="170">您好，我是来自宾夕法尼亚州立大学的Yusen Zhang。今天，我将介绍我们的工作“XSemPLR：跨语言语义解析在多种自然语言和意义表示”。语义解析是构建用户查询的语义表示的任务，例如SQL和Lambda Calculus。跨语言语义解析是将多种自然语言中的查询翻译成多种意义表示的任务，如图所示，我们需要使用神经模型将多种自然语言中的查询翻译成SQL、Lambda或FunQL等。

目前，已有的跨语言语义解析模型分别提出了并在有限的任务和应用上评估。例如，有很多关于特定自然语言的覆盖，但对中文缺乏覆盖，并且对某些意义表示缺乏覆盖，例如Lambda calculus。因此，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于跨语言语义解析多种自然语言和意义表示。它包含9个数据集，5个语义解析任务，8种意义表示和22种自然语言，以及15种语言家族。为了更好地评估我们的基准，我们考虑了六种设置。

第一种设置是Translate-Test，我们使用谷歌翻译API将源语言翻译成目标语言，然后使用单语言模型训练和评估。例如，我们在英语上训练模型，然后在推理阶段将德语查询翻译成英语，并使用训练模型预测SQL。我们还测试了Monolingual Model。在这个设置中，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试了Monolingual Few-shot设置，通过使用10%的训练数据训练单语言模型。我们还测试了Multilingual Model，我们将德语、英语和中文的查询放在一起训练一个多语言模型。在推理阶段，我们可以使用这个模型翻译德语查询或中文查询等。

我们还考虑了Cross-lingual Zero-shot和Few-shot transfer。我们在一个源语言上训练模型，然后转移到另一个语言。在训练阶段，我们在英语上训练模型或将英语和德语的少量数据放在一起训练一个多语言模型，用于预测SQL输出。我们发现了很多有趣的结果。

对于单语言模型，我们评估了两个模型组：Encoder-PTR（多语言预训练编码器与指针解码器），例如XLM-R + PTR和mBERT + PTR。我们还评估了Encoder-Decoder模型（多语言预训练编码器解码器模型），例如mBART和mT5。我们发现Encoder-Decoder在所有九个数据集上获得了最佳性能。我们评估了mT5和XLM-R + PTR在多语言设置中。我们发现Encoder-Decoder或Encoder-PTR可以通过在多种语言上训练来改进。我们发现大多数主要自然语言都可以获得性能收益，除了英语在七个数据集上下降，在三个数据集上上升。我们认为这是所谓的“多语言性困境”。

我们还比较了跨语言性能差距。在这个图中，蓝色线是Cross-lingual Few-shot transfer，橙色线是Cross-lingual Zero-shot transfer，绿色线是Monolingual Setting。我们发现，通过比较绿色和橙色线，我们发现Zero-shot设置下，跨语言转移性能差距很大，然后通过比较蓝色和橙色线，我们发现Few-shot设置下，转移差距迅速缩小。

我们还发现了其他有趣的结果，例如，Encoder-Decoder超出了以前的工作或取得了可比结果。预训练在英语自然语言上可以显著提高目标自然语言的Few-shot性能，我们发现多语言语言模型如Codex和BLOOM对于跨语言语义解析任务仍然不够。

总之，我们建立了XSemPLR，一个统一的基准用于跨语言语义解析多种自然语言和意义表示。我们进行了全面基准研究三种代表性多语言语言模型。我们的结果显示了很多有趣的发现。欢迎访问我们的论文和代码。感谢您的聆听。</sample>
    <sample id="171">现有的研究可以分为四大类，但它们存在以下问题： 

1. 不适用于嵌入式服务：某些方法不适用于嵌入式服务，无法满足保护嵌入式服务版权的需求。

2. 嵌入式服务的利用率降低：某些方法会降低嵌入式服务的利用率，影响服务的正常使用。

3. 不够隐蔽：某些方法无法有效地隐蔽水印，攻击者可以轻松地移除水印。

4. 转移性不强：某些方法在模型提取过程中无法有效地转移水印。</sample>
    <sample id="172">根据所给的内容，Codex 或 Bloom 等多语言 LLM 目前对于 CLSP 任务来说仍然不够充分。</sample>
    <sample id="174">ArgAnalysis35K是一份大规模的数据集，用于论证质量分析。论证质量分析是指评估论证的好坏程度，范围从0到1。该数据集由Thea等人创建，他们在视频中介绍了该数据集的独特之处。

目前的数据集存在一些问题，包括质量不高、缺乏多样性、缺乏深度和缺乏相关性。ArgAnalysis35K数据集通过以下方式解决了这些问题：

1. **质量高的论证**：该数据集包含35,000个论证-分析对，超过85%的论证来源于高质量的演讲、专家辩论或中级辩论。剩余的15%来源于初级辩论者或普通人。
2. **多样性**：该数据集选择了24个主题，涵盖了议会辩论中的多种运动。每个主题都包含了尽可能多的运动，创造了更丰富的多样性。
3. **分析**：该数据集引入了一个新的概念——分析。这不是单纯的陈述或前提，而是陈述、前提和其他元素的组合。分析可以是单一的，也可以是多个陈述和前提的组合。
4. **实例级的标注可靠性**：该数据集引入了实例级的标注可靠性，这意味着可以评估每个论证的可靠性，而不是简单地抛弃可能存在偏见的标注者。
5. **相关性模型**：该数据集引入了一个相关性模型，这可以评估每个论证与主题的相关性。这个模型可以更好地捕捉到每个论证与主题之间的关系。

总之，ArgAnalysis35K数据集通过提高论证质量、增加多样性、引入分析和相关性模型等方式，成为论证质量分析领域的一个重要贡献。</sample>
    <sample id="175">该方法通过使用一种叫做"Traveling Salesman"问题的NP-hard问题来解决排列的不确定性问题。为了解决这个问题，该方法使用GPU-friendly的连续放松方法来找到最高分的排列。</sample>
    <sample id="176">根据Shangbin的研究，下游NLP模型的公平性可以定义为模型在处理不同政治立场、社会类别等情况下的偏差和准确率。例如，在检测仇恨言论和虚假新闻方面，左倾语言模型可能更好地检测针对社会少数群体的仇恨言论，但在检测针对白人和男性等更有权力的群体的仇恨言论时则不太好。</sample>
    <sample id="177">演讲者没有明确提到名字，但根据文本内容，可以推测演讲者可能是Yanis Labrak。</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar 的研究探讨了如何提高大型语言模型（LLM）的理论心理学（Theory of Mind）推理能力。理论心理学是理解他人心理状态的能力，通常通过多人角色阅读理解任务来评估。研究人员提出了一种名为 SymbolicToM 的新方法，旨在改善 LLM 的理论心理学推理能力。

SymbolicToM 使用图形表示法来模拟角色之间的信念和认识。这种方法避免了过度拟合的风险，并提供了更清晰的推理过程。通过使用 SymbolicToM，研究人员能够显著提高 LLM 的理论心理学推理能力，甚至在出域故事理解中也表现出优异的表现。

研究人员在 ToMi 数据集上进行了实验，比较了 SymbolicToM 与监督学习基准线的性能。结果表明，SymbolicToM 在第二阶假设错误问题上显著提高了 LLM 的准确率，甚至在存储结构和语言一般化方面也表现出优异的表现。

研究人员还设计了三个新的数据集，用于评估 SymbolicToM 的普遍性。结果表明，SymbolicToM 在这些数据集上仍然表现出显著的提高，甚至在更复杂的故事结构和语言表达上也表现出优异的表现。

综上所述，SymbolicToM 是一种有效的方法，可以显著提高 LLM 的理论心理学推理能力。这种方法不仅在标准的理论心理学任务上表现出优异的表现，还在更复杂的故事结构和语言表达上也表现出优异的表现。</sample>
    <sample id="180">演讲者的名字没有被提及。</sample>
    <sample id="181">您好，Siyu Yuan老师，从上海的复旦大学来看，我们一起讨论您的研究成果——"Distilling Script Knowledge from Large Language Models for Constrained Language Planning"。您的研究主要关注的是如何让大型语言模型规划具体目标，例如"制作巧克力蛋糕"，而不是抽象目标，如"制作蛋糕"。您定义了受限语言规划问题，涉及为目标施加多种约束。

您发现大型语言模型在规划具体目标方面表现不佳，主要原因是它们不能保证脚本的可信度。因此，您提出了一个解决方案：在生成脚本之前先生成多个选项，然后使用过滤模型选择最合适的脚本。您的方法通过提高生成脚本的质量，显著提高了规划能力。

为了使小型模型能够规划受限目标，您提出了符号知识蒸馏的想法，通过蒸馏大型语言模型来创建受限语言规划数据集，称为CoScript。您的研究表明，CoScript数据集能够让小型模型（如T5）在受限语言规划方面表现优于大型模型。您的研究成果旨在为语言规划领域提供新的研究资源和方向。</sample>
    <sample id="182">热带主义 (tropicalism) 在本文中指的是一种对拉丁美洲妇女的刻板印象和刻板形象，强调她们的色彩丰富、曲线美等特征。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。具体来说，他们使用指令，如“想象你是亚洲女性，描述自己”，来让语言模型生成关于特定群体的描写。</sample>
    <sample id="184">CXMI（Context Usage by Machine Translation Models）和Pointwise CXMI（P-CXMI）。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们训练的数据来源。DrBERT 基于 RoBERTa，训练于 NACHOS，这是一个医疗爬取数据的数据集。ChuBERT 基于 RoBERTa，但训练于 Nantes University Hospital 的匿名数据，这意味着 ChuBERT 是一个基于临床数据的模型。</sample>
    <sample id="187">无法从所给的内容中确定论文有多少位作者。</sample>
    <sample id="188">迭代迁移学习是指在迁移学习的基础上，通过多轮迭代的方式进行模型的训练和更新。具体来说，在迁移学习中，模型首先使用源任务的权重进行初始化，然后在目标任务上进行训练和更新。这种方式可以帮助模型更好地适应目标任务，并且可以减少需要标注的样本数量。</sample>
    <sample id="189">数据集的目标是收集和标准化使用间接引用的表达来选择实体的语言数据，以便于用于评估语言模型的实体理解能力。</sample>
    <sample id="190">攻击者可以通过学习从 EaaS 提供的嵌入来提取模型参数。</sample>
    <sample id="191">这篇论文有3位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="192">杨 Luo博士在演讲中介绍了他们的新研究成果"CAME:自信度指导的适应性记忆高效优化"。他们的目标是设计一个优化器来同时实现传统适应性方法的快速收敛和记忆高效方法的低内存使用。

演讲中首先介绍了目前的挑战：传统的适应性优化方法，如Adam，会增加内存使用量；而记忆高效的优化方法，如Adafactor，虽然减少了内存使用量，但会导致收敛速度变慢。

然后，杨博士介绍了他们的解决方案：非负矩阵分解（NMF）技术，可以将矩阵分解为两个矩阵，从而减少内存使用量。他们还提出了一个新的优化器"CAME"，它结合了NMF和自信度指导的更新策略。

演讲中还展示了实验结果，比较了CAME、Adam和Adafactor在不同批大小下的性能和内存使用量。结果表明，CAME在大批大小下表现出更好的性能和更低的内存使用量。

最后，杨博士总结了他们的研究成果：CAME是一个高效的优化器，可以在大批大小下实现快速收敛和低内存使用量。他们的研究结果表明，CAME是一个有希望的解决方案，能够帮助大型语言模型的训练。</sample>
    <sample id="193">43 个例子</sample>
    <sample id="194">该论文的作者来自 Carnegie Mellon University、University of Washington 和 Allen Institute for AI。</sample>
    <sample id="195">该研究提出了一种名为RoHT（Reasoning over Hierarchical Question Decomposition Tree）的框架，用于解释性问答（XQA）。XQA旨在回答给定的问题并提供答案是如何选取的解释。研究指出目前XQA的方法有两个主要方向：神经符号方法和分解方法。神经符号方法将自然语言问题转换为形式化的表示（如SPARQL），而分解方法生成自然语言中间步骤来到达最终答案。

然而，这些方法都存在局限性。神经符号方法只能在结构化知识库（KB）上执行，但即使是最大的KB也是不完整的，因此答案的回忆能力有限。分解方法仅使用自由文本库作为知识来源，而自然语言的多样性使得XQA变得困难。因此，集成来自多个来源的知识对于QA，特别是回答复杂问题，是非常重要的。

研究提出的RoHT框架是一种两阶段框架。第一阶段，研究者构建了一个名为Hierarchical Question Decomposition Tree（HQDT）的树状结构，来理解复杂问题的层次组合结构。第二阶段，研究者使用概率推理来融合来自知识库和文本库的知识，并考虑到每个节点的概率分数。

实验结果表明，RoHT框架在两个挑战性的复杂QA数据集上表现出优异的性能。特别是在KQA Pro数据集上，RoHT KB模型超越了现有的KB QA方法，证明了集成子问题的答案不同层次的优势。同时，在Musique数据集上，RoHT text模型和RoHT-mix模型都优于TransferNet，证明了显式分解的优势。</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">根据所给的内容，实验中使用的四个最先进的对话模型没有被明确提及。</sample>
    <sample id="198">因为目前的大型语言模型具有更长的上下文窗口，评估模型的可接受性仅在短句子中是不够的，需要在整个上下文窗口中评估模型的可接受性。</sample>
    <sample id="199">是的，根据研究结果显示，多语言训练会导致英语表现下降，仅在三种数据集中表现提升。</sample>
    <sample id="200">根据所给的内容，注释者知道两个实体的名称，但不一定知道这些实体的具体信息。</sample>
    <sample id="201">该研究使用了以下MT指标：

1. BLEURT（BLEU Extended with Unreliability and Reference Translation）
2. WMT（Worldwide Machine Translation）评估</sample>
    <sample id="202">根据文中描述，泛化中的回归（temporal drift）会影响模型的整体性能，而不是特定的 NER 类型。</sample>
    <sample id="203">因为 NLP 研究人员和模型开发者的立场会影响研究过程和结果，导致系统性性能差异，这被称为设计偏见。</sample>
    <sample id="204">根据所给的内容，BLOOM 这样的多语言 LLM 仍然不适合于跨语言语义解析任务。</sample>
    <sample id="205">Shangbin博士提出的研究工作探讨了语言模型在政治偏见方面的公平性问题。研究表明，语言模型在训练数据中学习到的政治偏见可能会导致下游任务的不公平性。具体来说，研究发现：

1. 语言模型在政治偏见方面存在差异，包括左倾、右倾、保守和自由四个方面。
2. GPT-4是最左倾的语言模型，而GPT系列语言模型比BART系列和其变体更具社会自由倾向。
3. 语言模型的政治偏见在一定程度上受训练数据的影响。通过在不同党派的新闻和社交媒体上进一步训练语言模型，可以观察到语言模型的政治偏见也会相应变化。
4. 语言模型可以捕捉到社会的极化趋势。通过分离训练数据为前45届美国总统和后45届美国总统两段时间，研究发现语言模型在后2017年后的政治偏见更远离中立。
5. 在下游任务中，语言模型的政治偏见会影响其性能。例如，在检测仇恨言论和虚假新闻方面，左倾语言模型更擅长检测针对社会少数群体的仇恨言论，而右倾语言模型更擅长检测针对白人和男性等更有权力的群体的仇恨言论。

研究人员提出了一个困境，即如何在保持语言模型的多样性和公平性之间取得平衡。如果不清除政治偏见，语言模型的偏见会在下游任务中体现出来；而如果清除偏见，可能会导致审查和排斥。因此，研究人员呼吁关注和解决语言模型政治偏见的问题，以确保其在下游任务中的公平性和可靠性。</sample>
    <sample id="206">他们使用PDTB（Penn Discourse TreeBank）模型进行迁移学习。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集是 WMT 的测试集。</sample>
    <sample id="208">作者最终提出了3条建议：</sample>
    <sample id="209">该研究没有直接给出与最强基线相比的收益率，但提议的方法在改进语义完整性和对约束的忠诚度方面取得了显著进步。</sample>
    <sample id="210">演讲者名字是Shuheng。</sample>
    <sample id="211">是的，论文中提到的结果和数据集可以用作基准。论文中提到，DEPLAIN数据集可以用作评估自动对齐方法的金标准，自动文本简化的基准分数，以及自动文本简化的基准模型。</sample>
    <sample id="212">没有提到具体进行了多少个较小模型的实验。</sample>
    <sample id="213">OFA（Open Fusion Architecture）模型被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">这位研究者，Adam Przepiórkowski，正在探讨依赖结构的协调（Dependency Structure of Coordination）。他提到，目前有几种不同的依赖结构理论和语料库方法来处理协调结构。这些方法包括：

1. 普遍依赖（Universal Dependencies）：首个并列结构是整个协调结构的头部。
2. Igor Mel'čuk的意义文法理论：同样，首个并列结构是整个协调结构的头部。
3. 布拉格方法（Prague Approach）：协调结构由连接词头部。
4. 多头方法（Multi-headed Approach）：如Hudson的词法语法，所有并列结构都是头部。

研究者提到，他的论文旨在提供一个新论点，支持协调结构的对称性结构（如普遍依赖和Igor Mel'čuk的意义文法理论），而不是非对称性结构（如布拉格方法和多头方法）。他基于依赖长度最小化原则来支持这一论点。依赖长度最小化原则指出，较短的依赖关系是更好的。

研究者提供了一个例子，来说明依赖长度最小化原则的作用。他指出，直接宾语通常应该紧跟动词，但如果直接宾语很长，可能可以放在连接词之后。这样可以减少依赖长度。

研究者分析了Penn Treebank语料库的数据，发现左并列结构通常比右并列结构短。这种趋势随着并列结构长度差异的增长而加剧。但是，这种趋势只出现在连接词在左侧或不存在的情况下。如果连接词在右侧，则这种趋势消失。研究者认为，这提供了一个论点，支持协调结构的对称性结构，而不是非对称性结构。</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation"是一项研究工作，由Weihao Zeng、Lulu Zhao和Keqing He共同完成。该研究旨在解决当前对话生成模型在多属性控制下的缺陷，特别是忽略了实践中多属性生成的需求。

该研究提出的方法是Disentangled Controllable Generation（DCG），它通过学习属性概念并使用分离损失来分离不同属性组合。DCG还引入了一个统一的参考自由评估框架，称为MAE（Multi-Attribute Evaluation），用于评估不同属性粒度下的模型性能。

实验结果表明，DCG在多属性控制和文本相等性方面优于其他基线模型。DCG通过使用属性相关信息和任务相关信息的混合提示来实现这一目标，能够有效地学习属性概念并分离不同属性组合。

DCG的有效性通过两个基准测试得到了证明，分别是DailyDialog-CG和E-ACC/A-ACC。实验结果表明，DCG能够有效地解决多属性控制对话生成的组合一般化挑战。

此外，研究还提出了一个统一的参考自由评估框架，称为MAE，用于评估不同属性粒度下的模型性能。MAE通过使用模板和可训练的连续对话提示来实现这一目标，能够有效地评估模型在多属性控制下的性能。

总的来说，研究提出的DCG方法和MAE评估框架为多属性控制对话生成提供了新的方法和工具，能够有效地解决当前对话生成模型在多属性控制下的缺陷。</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">Jia-Huei Ju是一位研究助理，正在与Yu-Shiang Huang、Cheng-Wei Lin和导师Professors Che Lin和Chuan-Ju Wang合作，完成名为"A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports"的研究工作。他们的目标是分析财务报告中的信息，特别是Form 10-K，这是一份由SEC要求提交的年度报告，包含了公司的重要活动。

他们观察到公司报告中的词语非常相似，大约有80%的词语相同，内容也与前一年相似。这使他们意识到，仅仅依靠人工分析是不足以挖掘有价值信息的。因此，他们提出了一个高亮任务和一个多阶段管道。

高亮任务的目标是比较和对比目标报告和前一年报告之间的内容差异。他们定义了一个参考-目标结构，目标报告是当前报告，参考报告是前一年报告。高亮模型应该能够找出这两个报告之间的关系和关键词。

他们提出的管道包括三个阶段：文档分段、关系识别和模型调优。文档分段阶段包括将报告分成不同的段落。关系识别阶段包括识别报告之间的关系类型，包括β类型（语法和语义相似）、修订类型（语法相似但语义不同）和不匹配类型（新信息或公司新业务）。

模型调优阶段包括两个阶段：外域和内域微调。外域微调使用一个外部数据集eSNLI，内域微调使用修订对作为伪标签。他们还使用软标签技术和混合目标函数来提高模型的准确率。

实验结果表明，提出的管道在FINAL数据集上取得了最好的效果，并且在eSNLI数据集上也表现出很好的泛化能力。他们还发现，这个管道在不匹配对上也表现出很好的效果。</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">该论文主要分析了机器翻译（machine translation）中的大语言模型（large language model）提示（prompting）策略的有效性，特别是PaLM模型的翻译能力。</sample>
    <sample id="222">该研究探讨了开放域问答中域适应的挑战和干预措施。研究人员提出了一个问题：在纳罗拉、卡克拉普尔和塔拉普尔的植物中产生了什么。他们使用一个检索器模型来查找相关文本段落，并使用一个阅读器模型来生成答案。在这种情况下，答案是正确的，但如果问题是生物医学领域的，答案可能会不正确。

研究人员发现，使用生物医学文本集合可能不足以解决域适应问题，因为这些域往往很稀疏。他们提出了三种主要贡献：

1.  研究不同数据干预措施，以便在开放域问答中实现域适应。
2.  识别新域的数据平移类型。
3.  确定哪种数据干预措施对特定类型的平移有效。

研究人员设计了一个实验设置，包括一个通用语料库和七个目标文本集合，涵盖六个不同的域。他们研究了零点和少量的数据干预措施，后者使用少量的目标域例子来提示大型语言模型生成更多例子。

结果表明，检索器性能提高了8%，阅读器性能提高了11%。研究人员还发现，零点方法在概念和协变平移方面表现出色，而少量方法在所有目标域都表现出色。他们还发现，检索器模型对文本分布的变化很敏感，而BM25方法表现出最佳整体性能。

研究人员还提出了数据平移的分类系统，包括概念平移、协变平移和全平移。他们使用兼容性度量来评估模型的兼容性，并将目标集合映射到一个2D图表上，以估计数据平移类型。他们发现，某些目标集合对特定类型的数据干预措施响应良好。

总之，这项研究提出了开放域问答中域适应的挑战和干预措施，提供了一个有用的框架来理解和解决域适应问题。</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">研究人员在实验过程中使用了long-mBART和mBART两个模型。</sample>
    <sample id="225">在 MultiInstruct 中，53 个任务用于训练目的，分为 9 个组。用于测试目的的任务包括整个常识推理组和另外 5 个任务，从视觉问答和杂项组中选取。</sample>
    <sample id="226">这篇论文没有提到有多少位作者。</sample>
    <sample id="227">语言模型在自然语言处理领域取得了重大进展，但仍有许多挑战。其中一个关键挑战是语言模型在训练过程中缺乏对环境的关联，这被称为“grounded language understanding”。这种关联对于实现智能助手、语义搜索、医疗数据库查询和机器人等应用至关重要。

目前的语言模型主要通过自回归生成来实现grounded language understanding，但这种方法存在问题。生成的计划或程序可能不合法或无效，例如在知识图谱问答中，T5生成的KB查询可能无法在KB中执行。

为了解决这个问题，我们提出了一个名为Pangu的新框架。Pangu框架通过让语言模型专注于区分而不是生成来实现grounded language understanding。一个符号代理与环境交互并提出候选计划，而语言模型则用于评分和排名这些候选计划。这样一来，语言模型不需要处理目标计划的有效性和语法，因为它不需要自行生成。

我们在知识图谱问答中实例化了Pangu框架，并在不同语言模型（BERT、T5和Codex）和不同学习方式（微调和上下文学习）下进行了实验。结果表明，Pangu在所有设置下表现出杰出的性能，并且具有强大的样本效率。例如，在使用Codex和上下文学习的情况下，Pangu可以在仅有一个示例的情况下实现超过50%的GRAIL查询准确率，远远超过其他设置。

我们还发现，Pangu在非独立同分布（non-i.i.d.）设置下的强大普遍性可能与其对见过和未见过结构的概率分布保持一致有关，这表明Pangu具有强大的抗过拟合能力。

总之，我们的工作表明，对于grounded language understanding，生成可能不是一个好的想法，而是区分可能是一个更好的策略。我们欢迎不同形式的讨论和合作，并希望听到您的想法和意见。</sample>
    <sample id="228">作者在实验中使用了四个数据集：AG News、MIND、SST2 和 Enron Spam。</sample>
    <sample id="229">Gabriella Skitalinskaya 与 Henning Wachsmuth 合作的研究主要关注的是.argumentative写作支持的改善性陈述检测。他们提出了两个任务：Task 1：Suboptimal-Claim检测，检测是否需要改善或优化陈述；Task 2：Claim Improvement Suggestion，根据给定的陈述，选择需要改进的质量问题。他们的研究重点是探索如何利用协作在线辩论平台的隐含修订模式来模型化argumentative文本的质量。

 他们提出了四个挑战：Representativity and Reliability（代表性和可靠性）、Model Complexity and Architecture（模型复杂性和架构）、Contextual Information（上下文信息）和Topical and User Bias（主题和用户偏见）。通过实验，他们发现修订数据可以有效地用于给定的任务，模型距离两个陈述版本之间的距离有助于检测不良陈述，且上下文信息的影响取决于任务和文本的质量问题。

 他们的研究结果表明，修订数据可以用来改善argumentative写作支持，模型距离两个陈述版本之间的距离有助于检测不良陈述，且上下文信息的影响取决于任务和文本的质量问题。他们的研究提供了一个有用的框架，用于改善argumentative写作支持，特别是在协作在线辩论平台上。</sample>
    <sample id="231">NACHOS 是一个医疗数据集，通过爬取网络获取的医疗数据。</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">你好，萨拉帕皮！你刚刚介绍了与马泰奥·内格里和马尔科·图尔奇合作的论文《注意力作为对同时翻译的指南》。这篇论文探讨了同时翻译的概念和挑战。同时翻译（SimulST）是指将口语转换为另一种语言的实时文本，使跨语言交流成为可能。当前的SimulST模型面临几个问题，包括：

* 特定架构通常需要额外的模块进行优化。
* 长期复杂的训练程序，例如涉及不同的优化目标的训练。
* 训练和维护多个模型以达到不同延迟模式。

为了解决这些问题，论文提出了使用现有离线ST模型而不重新训练或采用特定架构的解决方案。该方案使用一个模型适应所有延迟模式，并通过特定参数来处理延迟。

论文还提出了Encoder-Decoder Attention（EDAtt）策略，这是通过注意力机制来决定是否发射部分翻译的。该机制通过计算注意力权重来决定是否发射翻译单元，如果注意力权重集中在最近接收的语音帧上，则发射翻译单元。

实验结果表明，EDAtt策略在德语SimulST任务中表现优于其他策略，包括Wait-k策略和Local Agreement策略。EDAtt策略还表现出最快的计算速度和实际延迟时间。论文的作者鼓励读者阅读论文并下载开源代码和模型以便复现研究结果。</sample>
    <sample id="234">提示策略对结果影响很大，特别是在零、一次提示的情况下。实验结果表明，选择好的示例质量比源句子的相似性更重要。</sample>
    <sample id="235">没有在文中明确指出作者所属机构的信息。</sample>
    <sample id="236">五个由专家编写的指令分别对应五个实验，每个实验评估模型使用其中一个指令的性能。</sample>
    <sample id="237">作者建议使用KITMUS测试套件，包括三个设置：背景-预训练、背景-双源和背景-推理。这些设置允许测试模型在不同来源（预训练知识、推理时知识）下的性能。</sample>
    <sample id="238">这位来自University of Central Florida的研究者Yebowen Hu介绍了他们团队开发的MeetingBank数据集。该数据集包括1,366个城市理事会会议的记录、参考摘要和其他相关资源。他们使用Speechmatics API将音频数据转换为文本，然后从会议网站获取会议信息、参考摘要和会议段落。他们还使用人机评估方法评估了十种不同的摘要系统，包括抽取式和生成式摘要系统。

研究者发现，抽取式摘要系统在ROUGE-2评分方面表现出色，而生成式摘要系统则需要进一步改进。特别是GPT-3在人机评估方面表现出色，获得了最高的总评分，但在自动评分方面表现不佳。研究者认为，这表明当前的自动评分方法不完全符合人类的偏好，需要开发新的自动评分方法。

该数据集的主要贡献是为会议摘要研究提供了一个基准数据集。研究者希望该数据集可以帮助开发更好的会议摘要系统，并为理解城市理事会的决策过程提供新的见解。</sample>
    <sample id="239">大家好，我是David Vilar，我将为大家介绍一篇名为“Prompting PaLM for Translation: Assessing Strategies and Performance”的论文。这篇论文是与Google翻译团队合作完成的。PaLM是一种540亿参数的大型语言模型，于2022年发布。它训练于大量的文本数据中，总共780亿个token。截至发表时，它在百余个NLP任务中实现了最好的成绩。在这项工作中，我们首次系统地研究了大型语言模型促发机器翻译的能力。我们使用了最新的测试集，避免了测试数据与模型训练数据的重叠。我们还与最好的系统进行了比较，使用了WMT评估。我们使用了最新的神经机器翻译评估指标，并且还展示了专家评估的结果。最后，我们提供了有关促发选择策略的建议。促发对LLMs的翻译性能有着重要的影响，如我们在一个简单的实验中所看到的，我们使用了一次促发，并为每个句子提供了两个不同的促发。结果表明，516个句子中的大多数，差异超过了1个BLEURT点，甚至在极端情况下，可以达到40个BLEURT点。因此，选择一个好的促发策略非常重要。在我们的实验中，我们选择了5次促发策略，我们仅仅标记了每个句子所使用的语言。例如，在我们从德语翻译成英语的例子中，我们将德语句子标记为“德语：”，将英语句子标记为“英语：”。我们发现，促发的形式对短促的促发没有太大的影响，但对零次和一次促发则非常重要。当我们选择了5次促发时，促发的形式几乎没有影响。最重要的是，例子的质量。我们的实验结果表明，例子的质量比源句子的相似度更重要。因此，我们选择了高质量的翻译例子。在我们的实验中，我们将训练数据中的促发与WMT评估的开发数据进行比较。开发数据比训练数据更为精心打造，质量更高。结果表明，使用开发数据的促发性能更好。然而，专家级别的系统仍然具有显著的优势，但PaLM的翻译结果与商业系统非常接近。在我们的实验中，我们选择了Google翻译作为评估系统。我们使用MQM框架进行的人类评估结果表明，PaLM的流畅度与专家级别的系统相当，但主要的差异来自准确性。特别是，PaLM最常见的错误是遗漏错误。PaLM选择产生更好听的翻译结果，有时通过丢弃源句子中的部分内容来实现。但是，PaLM的“风格/尴尬”类别低于专家级别的系统，这是一个额外的信号，表明PaLM提供了非常流畅的输出，但仍然存在准确性问题。</sample>
    <sample id="240">您好，我是Dawei，德国萨尔兰大学的博士生。在这段视频中，我将要介绍我们的最新工作“弱监督学习：一个批判性的看法”（Weaker Than You Think: A Critical Look at Weakly Supervised Learning）。这项工作是与肖宇、马里乌斯·莫斯巴赫、安德烈亚斯·斯蒂芬和迪特里希·克拉科夫共同完成的。让我们从弱监督学习的介绍开始。

弱监督学习是一种不需要手动标注数据的学习方法。相反，我们使用弱标注源，如简单的规则、知识库或低质量的众包数据进行标注，正如右边的图表所示。与人类标注相比，弱标注的成本更低，但也更脆弱，意味着有一定数量的标注是错误的。如果我们直接在弱标注数据上训练神经网络，它们倾向于记住标签噪声，而不是泛化。在弱监督学习中，训练算法被提出来，旨在在这种标签噪声下训练神经网络，使训练好的模型仍然可以泛化。

在最近的弱监督学习研究中，一种常见的说法是，人们只训练模型在弱标注数据上，并在干净的测试集上实现了高性能。技术上讲，这种说法并不错，但有一个陷阱，即人们假设有一个额外的干净验证集用于模型选择。我们不能停止在这个问题设置上，但是这意味着在弱监督学习中需要额外的手动标注。然而，这种必要性往往被忽视。

上述疑虑促使我们提出三个研究问题。首先，是否需要干净的验证数据来进行弱监督学习，还是可以使用噪声的验证集呢？其次，如果需要干净的数据，那么需要多少干净的样本？最后，是否仅仅使用干净的样本进行验证，还是有更好的利用它们的方法？

我们在我们的工作中解决了这些问题，以下是我们的发现。首先，我们发现最近的弱监督学习方法确实需要干净的验证样本才能正常工作。如果没有干净的验证样本，训练的模型就无法泛化超出原始弱标签，这意味着训练是无效的。这表明弱监督学习方法实际上需要干净标注的数据来正常工作，而获得干净的验证样本的标注成本不应该被忽视。

我们的第二个发现是，增加干净的验证样本数量可以帮助弱监督学习方法实现更好的性能，正如左边的图表所示。通常我们只需要每个类别20个样本就可以实现高性能。但这不是故事的结尾，因为如果我们决定使用干净的样本，我们可以直接在它们上训练，甚至可以实现更好的性能。右边的图表显示了直接fine-tuning和弱监督学习方法（仅使用干净数据进行验证）的性能差异。如果我们有10个样本每类，直接fine-tuning就会开始超过弱监督学习方法。

最后，我们发现之前弱监督学习方法声称的性能提高可以通过允许在干净的验证样本上继续fine-tuning来轻松实现。正如图表所示，原始模型（FTw）最初比复杂的弱监督学习方法（COSINE）要差，但如果允许在干净的样本上继续fine-tuning，它们的性能就可以与其他方法相提并论。因此，在实践中，没有理由选择更复杂的弱监督学习方法，它们需要更多的计算时间和存储空间。

总的来说，我们表明最近的弱监督学习方法需要干净的手动标注样本才能正常工作。它们的性能提高和实用性被严重夸大了。我们的具体建议是：

首先，报告模型选择的标准。例如，如果模型选择是通过干净的验证样本进行的，请报告。

其次，弱监督学习方法应该与少量样本学习的基准进行比较，因为两者都需要干净的样本。

第三，持续fine-tuning是一个简单而强大的基准值，应该在未来的弱监督学习工作中被考虑。

最后，我们已经开源了我们的代码。您可以通过右边的QR码找到它。感谢您的关注，希望您会喜欢这次会议。</sample>
    <sample id="241">这篇论文讨论了如何更有效地检测社交媒体上的虚假信息。目前的方法往往存在两个缺陷：一是评估方法不够现实，二是这些系统不够人性化。为了解决这些问题，论文提出了一种人机交互式的评估框架。这种框架从社交媒体平台的原始数据开始，通过人机交互来检测虚假信息，并输出可以由人类内容 moderator 使用的可操作结果。

论文的第一部分介绍了检测虚假信息的系统。该系统通过关键词过滤和问答模型（T5）来提取虚假信息，并根据其流行度进行排名。然后，将这些虚假信息提供给人类进行验证。第二部分讨论了违反社交媒体政策的检测。该系统使用BERT模型来检测作者对未经批准的治疗方法的立场，如果作者持有支持立场，则标记为需要人类审核。

论文的第二部分介绍了对系统的评估。通过评估系统的早期检测能力，论文发现系统可以在虚假信息被公开驳斥之前检测出未经批准的治疗方法。同时，论文也评估了系统的违反社交媒体政策的检测能力，发现系统的准确率为65%。此外，论文还讨论了系统对人类工作量的影响，发现系统可以每小时检测出124.2个违反政策的虚假信息。

总之，这篇论文提出了一个更现实的、人性化的虚假信息检测系统，旨在帮助人类内容 moderator 更有效地检测和处理虚假信息。</sample>
    <sample id="242">常用评估方法包括人工评估，例如让人类评判员选择哪个对话更好或评定对话给予Likert等级。</sample>
    <sample id="243">这篇论文的作者包括Jenny（虽然并没有明确提到她的姓氏）以及Sebastian Santy、Ronan Le Bras、Katharina Reinecke和Maarten Sap。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识包括 "法官决定案件"（Judge decide cases）。</sample>
    <sample id="245">本研究旨在找到Amazon Mechanical Turk（MTurk）平台上高一致性工作者的方法。研究人员提出了一个两步流程来筛选高质量的工作者。首先，研究人员设置了资格设置，包括位置、完成任务数量和任务批准率等条件。然后，研究人员设计了两个资格任务和一个耐久任务来测试工作者的能力。

资格任务包括训练部分和资格部分，资格部分包括三个文档，每个文档有一个总结，工作者需要评估六个维度。根据结果，工作者被分类为四种类型：金、银、铜和阻止，只有金和银工作者才能通过资格任务。经过资格任务筛选，共有26名MTurk工作者，8名金工作者和18名银工者。

耐久任务测试工作者的能力来处理大量任务，包括10个任务，每个任务有一个文档和四个总结。通过耐久任务筛选，共有12名MTurk工作者，4名金工作者和8名银工者。

研究人员还设计了一个参考任务来测试工作者的真实任务性能。结果显示，8名MTurk工作者完成了所有任务。研究人员还比较了Pipeline工作者、Baseline MTurk工作者和CloudResearch MTurk工作者的性能，发现Pipeline工作者的Krippendorff的Alpha值最高（0.534）。

研究人员还进行了正确性的分析，发现Pipeline工作者和CloudResearch工作者之间存在显著的Spearman的相关性。但是，Pipeline工作者不能保证正确性的训练。研究人员总结说，预任务过滤可以避免资源浪费，实现高一致性和低成本，并且可以获得与CloudResearch工作者类似的质量。

研究人员还提到了研究的局限性，包括只测试了英语摘要在MTurk平台，设计的问题不是“万能”解决方案，无法保证正确性的训练，以及未来需要研究如何招募高质量的工作者，包括高一致性和正确性。</sample>
    <sample id="246">是的，代码已公开。您可以在GitHub上获取相关代码和数据集。</sample>
    <sample id="247">您来自KAIST AI，提出了名为"FACTKG：基于知识图谱的事实验证"的论文。该论文提出了一个新任务，即基于知识图谱的事实验证。知识图谱是一种宝贵的知识来源，具有两大优势。首先，它可以提供可靠的事实验证。与基于文本或表格的验证相比，知识图谱可以直接连接到事实，提供更直观的证据，实现可靠的推理。其次，它可以应用于实践。例如，现代对话系统可以使用内部知识图谱与用户交互，并检查用户的说法与知识图谱的一致性。

为了实现这一目标，提出了一个新数据集FactKG，用于基于知识图谱的事实验证。该数据集基于DBpedia知识图谱，包含两种事实风格的说法：书面和口语。数据集的标签包括支持和驳斥两种。任务包括从DBpedia中检索证据，并使用证据来验证说法。数据集包含五种推理类型：一跳、共享、存在、多跳和否定。

数据集包含以下类型的说法：

* 一跳：可以用一个三元组来表示，验证时需要检查两个实体之间的关系。
* 共享：可以用多个三元组来表示，验证时需要检查所有的一跳说法。
* 存在：可以用一个三元组来表示，验证时需要检查实体与特定关系的连接。
* 多跳：需要多跳推理，因为一些实体不在说法中。
* 否定：需要验证图谱证据后再进行否定推理。

数据集还包含口语风格的说法，以便于实践应用。使用了两种方法来生成口语风格的说法：一是使用Kim等人提出的口语风格转换模型，二是使用假设模板。数据集的统计信息如下：包含1000个事实说法，五种推理类型，两种事实风格。</sample>
    <sample id="248">NLPositionality 的注释者在各个人口统计学特征方面并不均衡。根据研究结果，注释者来自 87 个国家，总共有超过 1000 名注释者，但具体的人口统计学特征（如国家/地区、性别等）并未提供详细信息。</sample>
    <sample id="249">通过在可接受的域中添加噪音来扰乱句子，例如在句子中添加词语或短语，但保留句子的语法结构。</sample>
    <sample id="250">进行维度评估意味着评估对话质量的多个方面，例如模型响应的相关性、对话的完整性、模型的同理心等。</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">Sai Kiran Tanikella和他的团队在IIT Kanpur发表了一项名为"U-CREAT: Unsupervised Case Retrieval using Events extrAcTion"的研究工作。该研究旨在解决法律领域的案件检索问题，特别是Prior Case Retrieval（PCR）任务。团队提出了两个关键贡献：IL-PCR数据集和U-CREAT管道。

IL-PCR数据集是一种新颖的benchmark数据集，包含7,070个法律案件，平均每个查询文档有6.775个引用。该数据集比现有的COLIEE'21数据集更大，更复杂，具有更长的文档和更大的词汇量。

U-CREAT管道采用无监督学习技术，引入了事件基于的方法来解决PCR任务。它展示了高检索效率、低推理时间和在印度和加拿大法律系统中泛化的能力，无需进行法律或人口特定调优。

事件提取在U-CREAT管道中起着关键作用。通过依赖解析技术，文本可以被表示为事件的集合。事件提取块包含三个步骤：预处理、依赖解析和后处理。提取的事件之间可以计算出一个交互矩阵，用于不同的检索模型来获得候选者排名。

团队在实验中使用了多种模型来验证和比较它们在PCR任务中的性能。这些模型被分为三个组：计数模型、转换器模型和事件模型。事件模型提供了改进PCR性能的不同方法。实验结果表明，事件模型的性能优于基准方法。</sample>
    <sample id="253">这是一份关于检测社交媒体中精神健康问题的研究报告。研究人员使用了一个名为DisorBERT的双域适应模型，旨在自动分析社交媒体用户的帖子，预测他们是否存在精神健康问题。报告的作者Mario Ezra Aragón解释了精神健康问题的定义，以及研究的背景和目标。

报告提到，使用域适应可以提高模型在目标域的性能，特别是在数据不足的情况下。研究人员使用了BERT作为基础语言模型，并将其适应于Reddit和精神健康领域。他们还使用了一个词典来指导遮蔽过程，帮助模型专注于重要的词汇。

报告展示了DisorBERT的结果，使用eRisk数据集进行评估。结果表明，DisorBERT比baseline方法更好地平衡了精度和召回率。报告还展示了DisorBERT如何分析特定句子的文本，预测与精神健康问题相关的词汇。

报告的结论是，双域适应和引导遮蔽的组合可以有效地捕捉社交媒体中的精神健康问题迹象。DisorBERT的结果比使用大量数据训练的MentalBERT模型更好。未来工作包括探索不同词典资源的应用以及使用临床数据。</sample>
    <sample id="254">文中描述了一个名为"Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction"的研究工作。该研究工作旨在改善文档级别关系提取的准确率，特别是在使用远程监督数据时。远程监督数据由于含有各种噪音而导致准确率下降。文中提出的框架使用不确定性估计来评估模型预测的可信度，并设计了一个重标签策略和多阶段训练策略来进一步提高性能。

框架的工作原理如下：首先，使用预去噪声的文档关系提取模型（DocRE）训练生成伪标签。由于伪标签可能包含错误信息，文中引入了不确定性估计来评估模型预测的可信度。文中提出的不确定性估计方法可以捕捉到重叠关系的不确定性分数，并设计了一个动态类别不确定性阈值的重标签策略来过滤掉不确定性较高的伪标签。

文中还提出了一个多阶段训练策略，通过迭代重标签数据来进一步提高模型的准确率。实验结果表明，提出的框架在两种公共数据集上都超越了之前的基准模型。文中总结了该研究工作的四个主要贡献：1）使用不确定性引导的标签去噪技术来改善远程监督数据的标签质量；2）为重叠关系提出的实例级别不确定性估计方法；3）为长尾问题设计的迭代重标签策略和动态类别不确定性阈值；4）显著提高的性能。</sample>
    <sample id="255">在零、一次提示（zero-shot 和 one-shot prompting）的情况下，提示的形式很重要。</sample>
    <sample id="257">作者评估了四个最新的对话模型。</sample>
    <sample id="258">这位研究人员Chiang Cheng-Han在视频中介绍了他们的新工作"Can Large Language Models Be an Alternative to Human Evaluation?"的主要内容。他们提出了使用大型语言模型来评估自然语言处理中的文本质量的想法。研究人员提供了大型语言模型的指令和样本，让它们评估样本，并通过输出提供评分。他们还讨论了与此相关的工作，例如G-Eval，也使用大型语言模型评估自然语言处理任务中的样本质量。

研究人员指出，使用大型语言模型进行评估并不是一个新颖的想法，但是在ACL会议上提交的论文中，没有相关的工作探讨了使用大型语言模型进行评估的想法。他们的动机是，过去，人类评估是评估样本质量的主要方法，但它很不稳定，很难复制。他们问自己一个问题：是否有一个替代人类评估的方法，可以达到人类评估的效果，但不具有人类评估的缺点。

研究人员选择了大型语言模型，因为它们可以理解自然语言文本指令。他们让大型语言模型评估生成的故事，包括GPT-2和人类写的故事。他们让大型语言模型根据四个属性（语法、连贯性、喜欢度和相关性）评分。他们还与人类评估结果进行比较，使用人类评估结果作为基准。

实验结果表明，人类评估者更喜欢人类写的故事，而一些较小的语言模型没有表现出对人类写的故事的偏好。但是，研究人员发现两个语言模型（Davinci和ChatGPT）表现出对人类写的故事的偏好，与人类评估者一致。因此，他们得出结论，有一些大型语言模型可以作为人类评估的替代品。</sample>
    <sample id="259">您来自宾夕法尼亚州立大学，名为Yusen Zhang。您的团队提出了名为XSemPLR的跨语言语义解析模型，旨在解决现有的跨语言语义解析模型在多种自然语言和语义表示之间的缺乏。XSemPLR提供了一种统一的数据集，包含9个数据集、5个语义解析任务、8种语义表示和22种自然语言。 

您的团队考虑了六种训练和评估设置，包括翻译测试、单语言模型、单语言少量样本训练、多语言模型、跨语言零样本转移和跨语言少量样本转移。通过这些设置，您的团队评估了三种多语言语言模型：编码器-指针模型、编码器-解码器模型和预训练语言模型。

您的团队发现，编码器-解码器模型在所有9个数据集上表现最佳。同时，您的团队也发现，多语言语言模型在某些自然语言上的表现会降低，称为“多语言性的诅咒”。此外，您的团队发现，零样本转移的跨语言性能差距很大，而少量样本转移可以缩短这种差距。 

您的团队还发现，预训练在英语自然语言上可以显著提高目标自然语言的少量样本表现。最后，您的团队认为，目前的多语言语言模型，如Codex和BLOOM，对于跨语言语义解析任务仍然不够。</sample>
    <sample id="260">没有提到论文有多少位作者。</sample>
    <sample id="261">良好的规划器应该能够写出合理、忠实于约束的脚本。</sample>
    <sample id="262">没有提到具体的作者人数。</sample>
    <sample id="263">在这篇论文中，研究人员探讨了大型语言模型的内上下文学习能力的不稳定性问题。他们发现各种设计选择，例如上下文示例的选择和顺序，会引入偏见，导致模型预测的不稳定性。然而，目前还没有系统性的讨论来分类和识别内上下文学习的偏见问题，也没有有效的方法来缓解这些偏见。

研究人员提出了一个新的偏见类型——域标签偏见（domain-label bias），并提出了一个新的校准方法来处理各种类型的偏见。他们的方法称为域上下文校准（domain-context calibration），它使用随机来自任务语料库的域内词作为无内容文本来估计模型对每个标签的偏见，然后使用这个估计值来校准模型的原始预测。

实验结果表明，域上下文校准显著改善了内上下文学习的平均性能，尤其是在域标签偏见较大的任务中。研究人员还发现，使用随机域内词而不是随机英文词来估计偏见可以更好地缓解域标签偏见。

总的来说，这篇论文提出了一个系统性的研究内上下文学习的偏见问题，并提出了一个新的校准方法来缓解这些偏见。研究人员的工作有助于改善大型语言模型的性能和稳定性。</sample>
    <sample id="264">林旺同学为我们介绍了他关于"可转移的音频视觉文本生成"的研究工作。该研究旨在解决音频视觉文本生成任务中存在的跨域问题。林旺同学提出了一个新的任务，即可转移的音频视觉文本生成，旨在在多个域中学习一个通用的音频语义空间。

他提出的框架包括三个组件：音频视觉元映射网络、音频视觉编码器和语言模型生成器以及对抗性对比学习。音频视觉元映射网络负责将不同视觉概念映射到一个统一的音频语义空间中，并解决语义分布的变化问题。林旺同学还提出了可学习的视觉前缀和对抗性对比学习来优化模型的性能。

实验结果表明，林旺同学的方法在跨域和跨数据集设置中都优于其他方法，特别是在低资源域中表现出显著的改进。林旺同学还进行了剖析实验来分析音频特征对性能的影响。总的来说，林旺同学的研究工作为音频视觉文本生成任务提供了一个新的框架和方法，具有重要的参考价值。</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">无法确定论文的作者所属机构。</sample>
    <sample id="268">PaLM 的最常见错误是遗漏错误（omission errors）。</sample>
    <sample id="269">您好，我是James Finch，今天我和Sarah Finch会告诉您有关ABC-Eval的新维度评估方法，这是由Emory NLP实验室的教授Jinho Choi带领的团队和亚马逊Alexa AI合作完成的工作。假设您刚刚开发了一个对话模型，想要了解它与当前的先进水平相比的表现。通常的做法是使用人类评估，例如要求人类评判员选择哪个对话更好，或者根据Likert等级评估对话。这些方法在提供对话质量的整体评估方面表现良好，但对话质量有很多方面。因此，您可能希望评估多个对话质量的维度，以了解模型在更细致的层面上的优势和劣势。一个方法是简单地要求人类评判员评估多个对话质量的维度，例如使用存在的比较或Likert等级方法评估模型回应的相关性。然而，我们认为有更精确可靠的策略来进行维度对话评估。我们的方法通过明确注明模型回应是否表达某些行为来减少人类评估的主观性，例如回应无关信息或自相矛盾。我们称这种方法为注明对话行为（ABC-Eval）或简称为ABC-Eval。我们开发了这个方法，以全面覆盖最近文献中提到的对话模型行为。ABC-Eval能够测量聊天模型犯各种主题错误的频率。例如，ABC-Eval测量聊天模型忽视对方、说无关信息、自相矛盾、虚构错误事实或违反常识知识的次数，以及模型表现出同理心的成功或失败。为了确定哪种评估最有效，我们选择了四个先进的聊天模型，并使用ABC-Eval在每个模型上评估100个人类-机器对话。为了进行比较，我们还使用了三个现有的方法： turno级别的Likert评分、对话级别的Likert评分和对话级别的配对比较。对于每个现有的方法，我们收集了评估的8个最常见的对话方面的标签，因为这是在评估聊天模型的多个维度方面的标准做法。从我们对这些评估结果的分析中，我们发现ABC-Eval行为标签总体上比现有方法收集的标签更可靠，因为通过100个双标签对话的交叉验证，我们测量出了更高的评判员一致性。另外，ABC-Eval标签比现有方法产生的指标更能预测对话质量，正如我们使用简单线性回归分析所示。例如，您可以看到测量自相矛盾的比例可以解释5%的对话质量，而测量伙伴自相矛盾的比例可以解释10%的对话质量，而平均Likert一致性得分只能解释4%或更少。最后，我们检查了每个评估指标是否捕捉到了聊天质量的独特方面使用了逐步线性回归分析。您可以看到所有ABC-Eval指标的组合可以解释超过25%的对话质量，而当我们逐一移除这些指标时，大多数指标都导致了对话质量的信息丢失。另一方面，所有turno级别Likert指标的组合解释的质量远远低于这点，而且这些指标中很少有独特的信息。这些可靠、详细的ABC-Eval指标使我们能够用比以前方法更高的分辨率来评估对话AI。您可以看到我们的实验结果显示还有许多挑战需要解决，例如我们测试的机器人在20%的回应中出现了常识违反，15%的回应中出现了无关信息，10%的回应中出现了自相矛盾。由于领域的发展速度很快，很多错误率可能会在新模型发布后降低。然而，这更是我们追求可靠、精确的评估指标来比较模型的理由。我们希望ABC-Eval能够被其他领域的人们利用作为评估模型的有意义的一步。我们期待看到对话AI在未来几个月和几年中会如何发展。感谢您的观看。</sample>
    <sample id="270">Emory NLP Lab（艾默里大学自然语言处理实验室）和Amazon Alexa AI（亚马逊Alexa人工智能）。</sample>
    <sample id="271">Fine-tuning</sample>
    <sample id="272">这篇论文有 7 位作者：John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy、Adina Williams 和 Koustav Sinha。</sample>
    <sample id="273">您好，我是Kayo Yin，我将要在会议上发表题目为“当翻译需要上下文？数据驱动的多语言探索”的作品。这个作品是与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig共同完成的。

许多翻译依赖于上下文。例如，在句子中，“mole”这个词的意思会根据前面的句子不同而不同。如果前面的句子是“如果部长们发现了，这可能会变得危险”，那么“mole”指的是间谍。如果前面的句子是“医生，您觉得这是什么问题呢”，那么“mole”指的是胎记。

然而，评估机器翻译模型如何处理这种情况非常困难。首先，因为只有少数翻译依赖于上下文，因此 corpus级别的指标如BLEU无法捕捉这些翻译。其次，人们建议针对上下文依赖的翻译进行专门的评估，但是这些资源只能支持有限类型的上下文依赖翻译和有限的语言，因为它们通常依赖于领域知识和人类编辑。

在本研究中，我们试图回答两个问题：翻译何时需要上下文？以及机器翻译模型如何处理这些情况。

为了回答第一个问题，我们首先测量了一个单词在翻译过程中依赖于上下文的程度。在之前的工作中，我们引入了CXMI作为机器翻译模型使用上下文的度量标准。CXMI通过测量上下文C为目标Y提供的信息量，给定源X来实现。可以认为CXMI是模型使用上下文获得的信息量。 在本研究中，我们扩展CXMI到点wise CXMI，使其能够在句子级别或词级别测量上下文使用。可以认为具有高P-CXMI的词需要上下文来翻译。

现在，我们分析具有高P-CXMI的词以寻找这些词之间的模式。我们在14种语言的TED演讲的转录中进行分析。我们在三个不同的层次上进行分析。首先，我们查看具有高均值P-CXMI的词性标记。这样可以帮助我们发现，例如，在阿拉伯语中具有较高P-CXMI的双数代词。可以解释这是因为英语中没有双数代词，因此在翻译阿拉伯语时需要上下文来确定代词是双数还是单数。

接下来，我们查看具有高P-CXMI的词汇项的平均值。这样可以帮助我们识别案例，如在中文中需要上下文来翻译名词，以确保在文档中使用相同的翻译。同样，我们发现上下文对于选择正确的形式也是很重要的。

最后，我们查看具有高P-CXMI的不同个体令牌。这样可以帮助我们识别无法通过单词本身捕捉到的现象，但是在句子结构中表达出来的现象，如省略号的解决。

现在，我们使用我们的分析结果来设计文档级别翻译的基准。我们根据五种我们识别出的文本现象创建自动标注器。我们称我们的标注器为多语言对话感知标注器（MuDA标注器）。我们可以看到不同语言中这些文本现象的比例有所不同。

我们使用MuDA标注器来标注我们想要评估的平行语料库中的上下文依赖例句。然后，我们使用我们的翻译指标来评估这些上下文依赖例句。最后，我们使用我们的基准和其他指标来评估不同模型的文档级别机器翻译。首先，我们使用 corpus级别指标：BLEU，我们发现上下文无关模型表现最佳。但是，如果我们使用COMET，则上下文相关模型表现最佳。如果我们使用词级f-measure，则上下文相关和无关模型的表现相当。

这表明，如果我们仅使用 corpus级别指标，则很难确定文档级别翻译系统的最佳系统。我们使用MuDA基准来评估模型，发现上下文相关模型在某些文本现象，如正式性和词汇一致性方面的准确性明显高于不使用上下文的模型。但是，在其他现象，如省略号、代词和动词形式方面，这些模型并没有比不使用上下文的模型表现得更好。

这表明我们需要在文档级别翻译中继续努力。我们还比较了不同的商业系统，MuDA基准显示DeepL通常比谷歌翻译在文档级别翻译中更准确。

总之，我们在14种语言对之间进行了数据驱动的分析，以确定翻译何时需要上下文，然后我们使用我们的发现来建立文档级别机器翻译的基准，这可以帮助我们确定模型可以处理的文本现象以及哪些翻译系统在文档级别翻译中更好。感谢您的关注。我们将在多伦多见面。</sample>
    <sample id="274">演讲者名字没有被提及。</sample>
    <sample id="276">本研究旨在填补现有机器翻译评估指标（MTM）对于印度语言的缺失。研究人员收集了来自印度五种语言的7,000个样本，包括泰米尔、马拉雅拉姆、印地语、马拉地语和古吉拉特语。他们使用七个不同的翻译模型或API生成候选翻译，然后由双语专家注释这些候选翻译，标注错误类型和严重程度。研究人员还评估了各种MTM指标的性能，包括基于重叠、嵌入和COMET等指标。结果表明，COMET指标在所有语言中表现最佳，而基于重叠的指标表现最差。研究人员还发现，指标在准确性错误方面表现更好，而在流畅性错误方面表现较差。为了进一步改进指标性能，研究人员对COMET指标进行了微调，并在四种语言上测试了其零样本能力。结果表明，微调后的指标在大多数语言中表现更好。最后，研究人员评估了其指标的鲁棒性，并发现其在ACES翻译准确性挑战集中表现更好。</sample>
    <sample id="277">该方法没有名称。</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法是通过比较显性群体和隐性群体的语言差异来识别具有偏见的词汇。具体来说，方法涉及以下步骤：首先，确定显性和隐性群体，然后使用Fightin’ Words方法（使用加权对数比率）来比较每个显性群体的顶级词汇。</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">Shi Tao 在演讲中提到了情绪调节在对话中的重要性，并概述了当前的挑战：多模态信息的补充性没有被充分利用，现有方法在少数情绪类别上的性能不佳，难以区分语义相似的情绪。

为了解决这些问题，Shi Tao 提出了一个新的多模态融合框架MultiEMO。该框架包括四个关键组件：单模特特征提取、上下文建模、多模态融合和情绪分类。MultiEMO 的主要贡献包括：

1. 一个新的视觉特征提取器VisExtNet，旨在捕捉交谈者 facial 表情，而不是背景信息。
2. 一个多模态融合模型MultiAttn，使用双向多头交叉注意力层来融合文本、音频和视觉信息。
3. 一个样本加权焦点对比损失函数，旨在提高少数情绪类别的性能并减少语义相似的情绪的混淆。

实验结果表明，MultiEMO 在两个情绪识别基准数据集MELD和IEMOCAP上取得了最好的性能，尤其是在少数情绪类别和语义相似的情绪识别方面。

但是，MultiEMO 也有一些局限性，包括：

1.  VisExtNet 不区分说话者和背景中的其他人。
2.  SWFC 损失需要较大的批量大小。
3.  MultiEMO 在少数情绪类别上的性能仍然不如大多数情绪类别。</sample>
    <sample id="281">Kayo Yin和他的团队最近发表了一篇名为“当翻译需要上下文？：一个数据驱动的多语言探索”的论文。这项工作探讨了翻译中上下文的作用，特别是当词汇在不同的上下文中有不同的含义时。例如，“mole”这个词可能指的是间谍，也可能指的是胎记。Kayo Yin和他的团队发现，仅有少部分翻译依赖于上下文，这使得评估机器翻译模型的能力变得困难。

为了解决这个问题，Kayo Yin和他的团队开发了一个名为CXMI的测量工具，用于评估机器翻译模型在不同上下文中的性能。他们还扩展了CXMI，开发了一个名为Pointwise CXMI的工具，可以在句子或词级别评估上下文的作用。通过分析14种语言的TED演讲，Kayo Yin和他的团队发现，有些词汇在不同的上下文中有不同的含义，例如在阿拉伯语中，双数代词需要上下文才能正确翻译。

Kayo Yin和他的团队还发现，某些语言需要上下文来选择正确的动词形式。他们还发现，某些词汇在不同的上下文中有不同的含义，例如在中文中，需要上下文来翻译正确的名词。他们还发现，形式和语气也是需要上下文来翻译的。

为了评估机器翻译模型的能力，Kayo Yin和他的团队开发了一个名为MuDA的标签工具，可以自动识别需要上下文来翻译的词汇。他们通过应用MuDA标签工具于一个平行语料库，并应用了选择的翻译指标来评估机器翻译模型的能力。结果表明，使用MuDA标签工具评估机器翻译模型的能力可以更准确地评估模型的能力。

Kayo Yin和他的团队的研究结果表明，机器翻译模型在处理需要上下文的翻译时存在挑战。他们的研究结果也表明，使用MuDA标签工具可以更准确地评估机器翻译模型的能力。他们的研究结果也表明，DeepL比Google Translate更擅长处理需要上下文的翻译。</sample>
    <sample id="282">您好，Xuekai Zhu。您的新工作“StoryTrans：非平行故事作者风格转换与对话表示和内容增强”将在ACL 2023上发布。该工作解决了自然语言生成中一个重要的任务——非平行文本风格转换。目前，大多数研究都集中在token级别或句子级别，如句子情感转换或正式文本转换。您的研究在故事级别和对话级别上取得了重大进展，模拟作者风格至关重要。

您的工作的主要挑战在于长文本涉及复杂的作者语言偏好，如对话结构。主要挑战是模拟作者在对话级别的语言选择，如表1中的红色内容，例如叙事技巧，且风格与特定写作主题高度相关。这使得将这种风格特定的内容转移到另一种风格变得困难，如表1中的橙色内容。

您的解决方案是提出一个名为StoryTrans的生成模型。StoryTrans从源文本中学习对话表示，并将其与可学习的风格嵌入结合起来，生成目标风格的文本。您还设计了一个新的训练目标来减少对话表示中的风格特征，使得来自不同文本的表示在潜在空间中更接近。此外，您为了增强内容保留，将生成分为两个阶段。首先，将源文本中的风格特定内容关键词遮蔽，然后生成整个文本，通过将这些关键词明确包含进来。

您的训练框架分为两个阶段。第一个阶段使用建议性训练框架。您使用自我重构损失来恢复输入，然后在句子嵌入上执行解耦损失，旨在解耦句子级别的模拟。句子顺序损失旨在捕获句子级别的依赖关系，最后，风格分类器损失旨在为整个系统产生风格信号。第二阶段与风格转换无关，旨在填充正确风格特定内容并移除遮蔽令牌。

您的实验结果表明，StoryTrans在风格控制和内容保留方面优于强基线模型。此外，风格可视化表明，StoryTrans的转换测试也与金标准文本在风格特征空间中对齐。最后，StoryTrans可以补充几个短语或剧情来丰富整个故事情节并保持主要内容。</sample>
    <sample id="283">根据所给的英文内容，第一个提到的对称依存关系结构是由Hudson的Word Grammar提出的，称为多头结构。</sample>
    <sample id="284">您好，来自武汉大学的潘天舒。今天您将在ACL的主会议上发表题为"FSUIE：一种改进的模糊跨度机制，增强通用信息抽取"的长论文。目前的跨度基于UIE模型主要依赖于跨度边界的标注位置，但标注跨度边界存在模糊性，同一跨度可以有多种合理的标注方式。因此，提出了跨度边界由模糊而非精确学习的方法。同时，提出了基于Transformer的信息抽取存在特征提取和信息抽取的不匹配问题，Transformer关注全局特征而忽略了跨度的有限长度。因此，提出了动态调整的注意力用于跨度抽取决策。

FSUIE模型通过引入模糊跨度机制和动态注意力机制，改进了信息抽取的效果。模糊跨度机制通过将跨度边界学习为连续分布而非精确值，减轻了模型对跨度边界的依赖。动态注意力机制通过引入一个可调节的参数，动态调整注意力范围和注意力分布。

实验结果表明，FSUIE模型在命名实体识别、关系抽取和方面情感三元组抽取任务中都取得了显著的改进。FSUIE模型在小规模数据下更容易学习通用注意力跨度，从而取得更显著的改进。FSUIE模型在关系抽取任务中取得了新的最优结果，且在域适应性方面表现出更强的普遍性。</sample>
    <sample id="285">本文主要介绍了对对话总结的事实错误纠正的研究。总结模型和参考总结中仍存在事实错误的问题，研究人员提出了两种解决方案：一种是将事实相关的目标引入训练或推理过程，以使总结模型更为可信；另一种是设计一个独立的事实错误纠正模型（FEC），它以源文件和模型生成的总结作为输入，并输出纠正后的总结。

但是，现有的FEC评估方法存在缺陷。当前的FEC模型主要通过事实性度量（如FactCC和DAE）进行评估，这些度量方法给出了一个总体得分，但缺乏具体性和可靠性。此外，这种评估方法使得FEC模型可以忽略原始总结的内容直接生成一个更准确的事实性总结，从而使得错误纠正过程变得模糊。

为了解决这些问题，研究人员提出了以下几点建议：1）引入手动标注的参考纠正；2）建立一个基于ERRANT的评估框架，该框架主要包括三个步骤：对齐、分类和比较。

实验结果表明：1）使用对话总结数据集的参考总结训练FEC模型可以获得最好的结果；2）引入人类纠正的总结在训练FEC模型时可以提高其性能；3）结合人类标注数据和合成数据是一个有希望的方向；4）当前的FEC模型难以纠正事实错误，如添加和属性错误等。

因此，研究人员强调了需要改变FEC模型评估方法的紧迫性，特别是在对话总结领域。</sample>
    <sample id="286">演讲者没有明确提到名字，但是他们提到了他们是Emory NLP Lab的成员，合作的团队有Professor Jinho Choi和Amazon Alexa AI。</sample>
    <sample id="287">这篇论文有4位作者：Javad Hosseini、Filip Radlinski、Silvia Pareti和Annie Louis。</sample>
    <sample id="288">BLiMP 和 SyntaxGym 数据集可用于测试句法现象。</sample>
    <sample id="290">第一个研究问题的五种方法的缩写是 WSL（Weakly Supervised Learning），即弱监督学习。</sample>
    <sample id="291">该模型在以下11个生物医学和临床下游任务上进行了评估：

1. 名称实体识别（Named Entity Recognition）
2. 分类（Classification）
3. 词性标注（Part-of-Speech Tagging）
4. 问答（Question Answering）</sample>
    <sample id="294">CamemBERT 最初是在一个 138 GB 的数据集（Oscar）上训练的。</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile 在视频中介绍了一项研究成果，研究团队来自都灵大学和亚马逊的Alexa。他们探讨了自然语言理解和处理的基础，即监督机器学习和数据驱动方法。为了开发这些方法，需要大量的人工标注数据，这些数据包含了人类知识。研究人员发现，通常的假设是存在一个单一的真相（ground truth），但现实中存在局限性。

他们选择了研究幽默的研究方向，幽默是一种复杂且依赖于语境的现象。研究人员开发了一个名为EPIC（英语谐谑语 корпус）的数据集，包含了来自社交媒体、Reddit和Twitter的300个短对话对。他们使用众包平台Prolific让15个注释者为每种英语语言变体标注数据，总共74个注释者。

研究人员观察到注释者之间存在差异，包括性别、年龄组、国籍等维度。他们尝试建模这些差异并训练不同的模型，称为视角感知模型。结果表明，视角感知模型比使用金标准聚合模型的模型更具自信，并且在预测时更少不确定。

最后，研究人员分析了数据，发现年龄组之间的相邻年龄组更容易产生不一致的幽默感知结果。同样，地理分布也显示，来自英国和爱尔兰的注释者之间的模型表现差异最大。这项研究旨在更深入地了解幽默的复杂性，并探讨视角感知模型在自然语言处理中的潜在应用。</sample>
    <sample id="297">这项研究的目的在于揭露隐含在语言中的代码性言论，特别是种族主义、反同性恋和反犹太主义的狗唳。这项研究开发了一种狗唳的分类系统和词典，包含了340多个词语和符号，包括种族主义、反同性恋和反犹太主义的狗唳。研究人员还对历史上的美国政治演讲进行了案例研究，发现狗唳的频率与共和党南方战略有关，后者在民权运动后使用了更多的狗唳。

研究人员还评估了语言模型的狗唳识别能力，使用了GPT-3进行实验。结果表明，GPT-3可以识别出许多狗唳，尤其是那些正式语气的狗唳。但是，它的识别能力在社交媒体中使用的狗唳和反同性恋狗唳方面表现不佳。

此外，研究人员还展示了狗唳如何绕过内容监管系统。他们使用Prospective API和HateCheck的恶意模板句子进行了案例研究，发现狗唳可以降低句子的毒性评分，即使是同样的句子。

总的来说，这项研究开发了一种狗唳的分类系统和词典，评估了狗唳的识别能力，并展示了狗唳如何绕过内容监管系统。研究结果表明，狗唳是一种复杂且具有隐含性质的语言现象，需要更深入的研究和监管以防止其被滥用。</sample>
    <sample id="298">实验表明，时间漂移是性能下降的主要原因，因为随着时间间隔的增加，模型的性能会降低。</sample>
    <sample id="299">该研究旨在改进自然语言推理（NLI）模型的鲁棒性，特别是通过减少它们对捷径的依赖。捷径是由于数据集创建过程中的特定属性和标签之间的偶然关联而产生的。研究人员指出，现有的捷径缓解方法通常假设辅助模型已知会利用捷径来进行预测，这种假设可能不总是可行，并且可能限制了捷径缓解的潜力。

该研究提出了一种新的训练方法，旨在减少NLI模型对捷径的依赖，并提高其对分布外数据的性能。该方法的关键思想是，NLI模型在训练过程中容易忽略少数“困难”的例子，这些例子可能与捷径相矛盾。研究人员提出了一种最小化训练目标，通过交替优化学习器和辅助模型来计算例子权重分布。学习器试图最小化NLI任务的损失，而辅助模型试图最大化学习器的损失，以生成例子权重，使学习器更倾向于学习来自捷径区域的高损失区域。

该研究在三个常用的分析数据集（MNLI、FEVER和QQP）和对应的分布外攻击测试集（HANS Symmetric和PAWS）上评估了该方法。结果表明，相比ERM训练模型和每个数据集的最佳捷径缓解方法，该方法在分布外性能方面表现更好，同时保持高分布内准确率。

该研究还探讨了预训练学习器对性能的影响，以及辅助模型的大小对性能的影响。最后，该研究进行了一项质量评估，以了解学习的例子权重分布。</sample>
    <sample id="300">交互式 dictate 是一种新任务，旨在让用户使用自然和直观的方式在文档中进行 dictate 和编辑。该任务的关键特点包括：

1. flexibly 的 dictate 和编辑的交替，不需要 trigger 词或命令。
2. 使用直观和开放的自然语言句子来指定编辑。

该任务可以分为四个步骤：

1. ASR 识别模块将原始音频解析为语音转录。
2. 语音转录被分段为 dictate 和命令句子。
3. 每个命令被提取和标准化，ASR 错误和缺失被修复。
4. 每个 dictate 和命令句子在序列中执行，直到达到最终文档状态。

为了解决该任务，研究人员设计了一个新的数据采集界面，并收集了一个数据集。他们还建立了一个基线系统，使用四个模型分别处理每个步骤：分段模型、ASR 修复模型、命令提取模型和解释模型。

基线系统的实验结果表明：

* 分段模型的准确率和效率都较高。
* ASR 修复和解释模型的准确率和效率之间存在权衡，GPT-3 模型更准确但速度更慢。
* 预测状态直接比预测中间程序准确率更高，但速度更慢。
* T5 模型在预测程序方面可以显著提高效率，准确率影响较小。

总之，交互式 dictate 是一个新任务，需要进一步的研究和改进。研究人员已经发布了代码和论文，欢迎未来研究人员加入。</sample>
    <sample id="302">因为在第一个步骤中，模型已经预测出了输出序列中的所有词元，但它们是无序的（multiset），需要通过第二个步骤来预测它们的正确顺序。</sample>
    <sample id="303">因为作者不清楚模型中出现的正性刻板印象和等同化叙述是否是由于过度的价值观一致性或其他反刻板印象方法引起的，因此建议提高偏见缓解方法的透明度以进行进一步的研究。</sample>
    <sample id="304">最小对不可接受输入（Minimal Pair Paradigm, MPP）是一种评估语言模型接受性判断的方法。它通过比较接受性和不可接受性的句子对来评估语言模型的表现。</sample>
    <sample id="305">德国萨尔大学的博士生Dawei正在介绍他们的最新研究成果 "Weaker Than You Think: A Critical Look at Weakly Supervised Learning"。他们的研究团队包括Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。

他们的研究重点是弱监督学习（Weakly Supervised Learning，WSL），这是一种使用弱标注源（如简单规则、知识库或低质量众包）来标注数据的方法。相比于人工标注，弱标注成本更低，但也更容易出现噪音。直接在弱标注数据上训练神经网络会导致模型过度拟合噪音，失去泛化能力。

在WSL领域，一些研究表明，可以在弱标注数据上训练模型并在干净的测试集上获得高性能。然而，这种说法有一个前提，即需要额外的干净验证集用于模型选择。然而，这意味着需要额外的人工标注，这个问题经常被忽视。

Dawei的团队提出了三个研究问题：是否可以使用噪音验证集代替干净验证集？如果需要干净数据，需要多少个干净样本？如何利用干净样本。

他们的研究发现：WSL方法确实需要干净验证样本才能正常工作；增加干净验证样本数量可以提高WSL方法的性能；直接在干净样本上训练模型可以获得更好的性能；之前WSL方法宣称的性能提高可以通过继续在干净样本上微调模型来实现。

总之，Dawei的团队指出，WSL方法的性能和实用性被过度宣扬，需要更准确的模型选择和性能评估方法。他们建议将模型选择标准报告出来，比较WSL方法与少量学习基准，考虑继续微调作为WSL的一个简单但强大的基准。</sample>
    <sample id="306">这是一篇关于语言模型实体跟踪能力的研究论文。研究人员 Sebastian Schuster 和 Najoung Kim 指出，为了理解语言中的内容，语言模型需要能够跟踪实体及其状态的变化。然而，现有的研究并没有系统地评估语言模型的实体跟踪能力。

为了解决这个问题，研究人员设计了一个评估任务，要求语言模型根据初始内容和操作预测盒子的内容。他们使用 Flan-T5 和 GPT-3 和 -3.5 模型进行了实验，结果显示大多数模型仅仅重复初始状态，而只有 GPT-3.5 模型表现出非平凡的实体跟踪能力。

研究人员发现，GPT-3.5 模型的实体跟踪能力与其在代码训练方面的经验有关，而不是其模型大小或架构。他们还发现，如果直接对 T5-base 模型进行微调，可以让其学习实体跟踪能力，但随机初始化的同等模型无法学习这个任务。

这项研究表明，语言模型的实体跟踪能力与其预训练数据有关，尤其是代码训练。研究人员呼吁进一步的研究以了解语言模型的实体跟踪能力的普遍性和局限性。</sample>
    <sample id="307">作者使用了以下评估指标：

1. 名称实体识别（Named Entity Recognition）
2. 分类（Classification）
3. 词性标注（Part-of-Speech Tagging）
4. 问答（Question Answering）</sample>
    <sample id="308">这位研究人员是卡内基梅隆大学的博士生，正在为一项名为"NLPositionality"的研究做出贡献。这项研究旨在揭示数据集和模型的偏见，以及它们如何影响人工智能的性能。研究人员发现，数据集和模型的偏见是由开发人员的位置性质（即他们的背景、身份和生活经历）决定的。

研究人员使用了一个名为Lab in the Wild的在线实验平台，招募了来自87个国家的1000多名志愿者，完成了两项任务：社会可接受性和毒性检测。通过比较志愿者的注释与现有的数据集和模型的预测结果，研究人员发现，数据集和模型最多地与英语国家的用户和拥有大学教育的人员相符，但却与非二元性人士相比有明显的偏见。

研究人员提出了几项建议来解决数据集和模型的偏见问题：

1. 记录研究过程中的所有相关设计决策。
2. 使用多元化的视角来进行NLP研究。
3. 为特定社区开发专门的数据集和模型。

研究人员强调，包容性NLP不仅仅是让所有技术都适用于每个人，而是需要考虑到不同群体的需求和观点。</sample>
    <sample id="309">使用了交叉验证（Inter-annotator agreement）的指标来衡量注释者之间的一致性。</sample>
    <sample id="310">维基百科。</sample>
    <sample id="311">由于没有提到具体的机构名称，但根据 Regina Stodden 和 Omar 的名字，可以推测这可能是德国某所大学或研究机构的论文。</sample>
    <sample id="312">MultiInstruct 是第一个大规模的多模态指令调教基准数据集，它包含 62 个多模态任务，涵盖 10 个广泛类别。它与其他基准不同的是，它提供了一个公共可用的多模态指令任务数据集，之前没有这样的数据集。</sample>
    <sample id="313">根据所给的内容，Emory NLP Lab 领导人Professor Jinho Choi 和 Amazon Alexa AI 的合作伙伴参与了该研究，但并未提及具体的作者人数。</sample>
    <sample id="314">二进制协调是指两个或更多的同类成分（如名词、动词等）通过使用连接词（如and、or等）连接而形成的语法结构。</sample>
    <sample id="315">我没有看到提到提示语的平均长度。</sample>
    <sample id="316">这些发现表明，通过适当的训练，较小的 T5 模型可以超越更大的语言模型，生成高质量的脚本，特别是在受限语言规划中。</sample>
    <sample id="317">Peng Li从复旦大学发表了题为"CodeIE:大规模代码生成模型更好的少量示例信息提取器"的研究成果。该研究探讨了信息提取任务，包括命名实体识别和关系提取等。传统的信息提取模型通常使用预训练语言模型如T5和GPT-3，但这些模型在预训练阶段使用文本到文本的方式，而在推理阶段使用线性化的结构输出。这导致了输出和输入之间的不匹配，需要大量的结构化训练数据和特殊的解码策略来解决这个问题。

为了解决这个问题，Peng Li等人提出了CodeIE模型，将文本到结构信息提取任务转换为结构到结构代码生成任务，并使用代码大语言模型如Codex来进行该任务。通过这种方式，可以在输入阶段轻松将文本转换为结构化格式，并在输出阶段确保结构一致。

在命名实体识别任务中，Peng Li等人设计了以下提示：定义一个函数用于命名实体识别，输入文本作为输入，添加注释并从输入文本中提取命名实体。然后，设置输入任务为实际输入文本，初始化一个名为"实体列表"的实体列表，添加注释以触发后续内容。通过少量示例在上下文中演示，可以期望模型输出以下代码，持续提取文本和实体对，并将它们追加到实体列表中。

在关系提取任务中，Peng Li等人设计了类似的提示。他们在三个识别数据集和四个关系提取数据集上评估了其方法。评估的模型包括T5模型、UIE模型、GPT-3模型和Codex模型。他们比较了两种提示的性能，一种使用传统的文本样式提示，另一种使用前面描述的代码样式提示。在少量示例中，他们发现使用代码语言模型和代码格式提示的方法显著和一致地优于传统基准模型，如UIE和自然语言大语言模型，如GPT-3模型。</sample>
    <sample id="318">您好，我是Yanis Labrak，今天我将向您介绍我们的工作“DrBERT：一种适用于生物医学和临床领域的强大的预训练模型”。在这次演讲中，我们首先讨论了医疗领域的语言模型，然后介绍了我们文章的主要贡献。我们介绍了第一款适用于生物医学领域的法语模型DrBERT，它基于RoBERTa，并在NACHOS数据集上进行了训练。NACHOS是一个医疗数据集，来源于网络爬取的数据。我们还介绍了多个预训练设置和数据源的模型比较。接下来，我们将介绍我们的结果，包括11个生物医学和临床下游任务的法语结果。最后，我们将总结实验结果，并提供更多详细信息，包括如何获取这些模型。

自从2018年BERT的发布以来，它已经成为解决自然语言处理任务的最有效方法之一，并且在历史上使用静态和上下文化方法（如Word2vec和fastText）时提供了巨大的性能提升。自那以后，这个模型已经被适应到了许多其他语言，如法语的CamemBERT，以及生物医学领域的PubMedBERT和BioBERT，以及临床领域的ClinicalBERT，但大多数都是英文。其他语言的专用模型很少，并且通常是基于持续预训练的，因为缺乏领域数据。然而，法语在生物医学领域一直没有开源模型，直到现在。因此，我们问了自己一个问题：什么是适用于广泛使用的数据来源？我们发现爬取的数据可以作为临床数据的替代品。

为了回答这个问题，我们将DrBERT与我们的ChuBERT模型进行了比较，ChuBERT基于匿名的数据，来源于南特大学医院数据仓库。接下来，我们问了自己一个问题：我们需要多少数据来训练一个专用的法语模型？是4GB、8GB还是更多？为了回答这个问题，我们训练了四个从头开始的模型：第一版DrBERT，基于7GB的NACHOS数据集；第二版DrBERT，基于4GB的NACHOS数据集；第一版ChuBERT，基于4GB的临床笔记数据集；最后版ChuBERT，基于4GB的NACHOS数据集和4GB的临床笔记数据集。除了这些比较之外，我们还介绍了三个基于持续预训练的模型，以分析预训练策略的影响。其中一个基于CamemBERT权重，并在4GB的NACHOS数据集上进行了训练。另一个基于CamemBERT，并在4GB的临床笔记数据集上进行了训练。最后一个基于英文生物医学模型PubMedBERT，并在4GB的NACHOS数据集上进行了训练。总共，我们有七个模型。

为了评估我们的七个模型，我们收集了公共和私有下游任务的数据，包括命名实体识别、分类、词性标注和问答等。这些模型与六个基准模型进行了比较，包括CamemBERT OSCAR 138 GB、CamemBERT OSCAR 4 GB、CamemBERT CCNET 4 GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果表明，模型在数据来源相同的任务中表现最佳。然而，我们可以观察到来自异质数据源的模型表现更为灵活。我们还观察到使用更多数据可以带来更好的性能。总的来说，从头开始的预训练似乎在大多数任务中表现更好。然而，我们在控制预训练方面使用CamemBERT权重和分词器的实验表明，与DrBERT 4 GB从头开始的表现相似。然而，基于CamemBERT权重和分词器的模型却存在稳定性问题。

最后，我们得出结论，DrBERT在九个下游任务中表现更好，超过了泛化模型CamemBERT。我们还观察到，专用数据越多越好，但却不太可扩展。所有基于NACHOS的预训练模型都可以在Hugging Face上免费获得，并且所有训练脚本都可以在我们的GitHub仓库中找到。感谢您的演讲，我们期待在多伦多的海报会上与您进行交流。</sample>
    <sample id="319">论文研究了从头训练（from-scratch pre-training）和连续性预训练（continual pre-training）两种学习策略。</sample>
    <sample id="320">根据Shuheng的演讲，实验结果表明，测试重复使用导致的过拟合（adaptive overfitting）在这个场景中不是主要原因。</sample>
    <sample id="321">根据所给的英文内容，简化质量可以通过分析句子对的简化类型（如词汇简化、结构简化等）以及整体简化程度来评估。</sample>
    <sample id="322">Enrico将在ACL 23上发表演讲，探讨文本分类器对道德的学习。Enrico首先解释了道德的概念：道德是人类区分是非的内在指南，帮助我们确定行为或概念是否道德。道德是社会的基础，语言模型需要能够理解和识别道德在语言中的表达。

然而，道德的理解通常被视为一个单一的尺度，标签从不道德到道德之间。然而，道德是主观的，人们对同一概念的判断可能会有所不同。例如，堕胎或LGBTQ权利的争议性概念。Enrico认为，仅取平均值或多数聚合会掩盖道德的多元性。

Enrico提到了一种名为道德基础理论的社会理论，该理论认为人类有五种不同的道德感知方式，每种方式都对应着一种道德基础。每个人都会优先考虑这些基础，决定他们对行为或概念的道德判断。Enrico和他的团队已经使用这种理论在自然语言处理中尝试理解和分类道德。

Enrico的论文旨在了解语言模型在理解道德文本时学习了什么。他使用可解释人工智能技术来分析这些模型，特别是关注道德在不同领域中的表达。他们使用一个名为道德基础推特语料库的数据集，包含35,000条推文，涵盖七个不同的领域。Enrico提到，语言模型可以理解道德在不同领域中的不同表达，但还需要进一步研究。

Enrico给出了一个例子，比较了#AllLivesMatter和#BlackLivesMatter两个领域的道德表达。虽然两个领域的语言风格相似，但它们对权威的反叛（subversion）的态度却有所不同。Enrico的研究表明，语言模型可以识别这些差异，但也提出了使用单一模型处理多个领域可能会导致道德误解的警告。</sample>
    <sample id="323">您好，Yujie Wang，来自山西大学。您的论文"动态异构图推理与语言模型和知识表示学习的commonsense QA"解决了commonsense QA挑战，即机器需要回答依赖于常识的问题来测试其语言理解能力。这种挑战需要机器从外部资源中检索相关知识。最近，Holmes认为知识存储在语言模型和知识库中。许多工作将这两种知识结合起来，使用语言模型和GNN来推理答案。然而，这些工作引入了噪声实体，例如“顶部”，“银行”，“猫”，这些实体与当前问题没有直接关系。此外，他们将子图和文本编码为孤立的模态，忽略了实体之间的语义关系。

为了解决这些问题，我们提出了DHLK。首先，我们基于多个知识库建立了一个经过两阶段剪裁策略和知识表示学习（KRL）的异构图（HKG）。然后，我们使用语言模型编码和融合两个模态。我们首先使用词典词汇表移除构成实体短语的子词。同时，我们从WordNet和Wiktionary中检索关键实体的同义词，并将它们连接为额外的节点，形成HKG。接下来，我们使用RoBERTa和掩码自注意力（Mask Self-Attention）编码和融合QA上下文和实体，构建HKG。我们动态移除基于RoBERTa注意权重弱相关的实体。

我们使用平均池化获取实体和关系嵌入。由于HKG由多个三元组组成，我们引入TransE来优化实体和关系嵌入。我们使用关系掩码自注意力（Relation Mask Self-Attention）来模型我们的子图。我们更新HKG中的实体和关系嵌入通过迭代L层的RMSA。最后，我们通过应用最大池化获取HKG的图嵌入。我们将HKG路径信息整合到QA上下文中，并获取QA上下文的嵌入表示。最后，我们输入HKG图嵌入、路径和QA上下文嵌入到多层感知器（MLP）中，获取答案概率。我们在ConceptNet、WordNet和Wiktionary上进行了实验，使用 CommonsenseQA和OpenBookQA。我们提取QA上下文中的关键实体并检索ConceptNet中的知识路径。我们报告了CommonsenseQA和OpenBookQA的结果和leaderboard。与其他LM和HKG方法相比，我们的方法取得了很好的结果。</sample>
    <sample id="324">是的，语言模型确实有不同的政治偏见。根据研究人员的发现，语言模型可以被分为四个政治立场的象限，包括左派、右派、中间派和中立派。其中，GPT-4被认为是最左派的语言模型，GPT系列语言模型通常比BART系列和其变体更为社会自由。</sample>
    <sample id="325">你好！我是Matthias Lindemann，我将给你介绍我们关于“使用多集标记和隐性排列进行无树组合推广”的论文。这是与我的导师Alexander Koller和Ivan Titov共同完成的工作。组合推广可以理解为学习者处理更深的递归和未见的表达式组合的能力，而这些表达式在训练期间已经被单独看到。在语义解析的背景下，测试组合推广可能如下。我们通常有一个训练集的utterances。在这种情况下，“女孩睡了。”和“玛丽知道女孩睡了。”这些utterances与代表它们核心含义的逻辑形式配对。与标准机器学习评估不同，测试集不来自同一分布，但包含结构未见的逻辑形式。在这个例子中，模型在训练期间看到的浅层递归被测试在一个更深的递归例子中。简单的seq2seq模型难以处理这种分布式推广，通常会产生与输入脱节的输出。特别是，它们经常无法重现输入和输出之间的系统性对应关系，例如所示的颜色编码的例子。解决这个问题的流行方法是将树集成到模型中。树旨在捕捉与utterances和逻辑形式相关的组合过程。这很有效，但树通常不会给出并且需要通过某种方式获得。这可能很复杂，有时需要大量的计算资源。通常，这涉及到相当形式化的逻辑形式预处理，例如处理变量符号。获得树也可能涉及专门的语法诱导程序。在本文中，我们没有使用树并介绍了一个直接模型输入片段与输出片段之间对应关系的神经seq2seq模型。我们首次证明了不依赖于树的强推广能力。我们的方法在两步中预测输出。第一步，我们将每个输入token标记为将出现在输出中的无序多集token。在第一步之后，我们有所有正确的token，但它们并未排序。因此，在第二步中，我们使用另一个模型来预测将它们放入正确顺序的排列。我们引入了一个新的方法来预测排列，而不会对可能的排列施加任何硬约束。这使我们的方法很灵活和表达力强。概念上，我们的排列模型大致如下。我们从左到右遍历输出并确定每个位置的多集token。对于第一个输出位置，我们简单地选择一个，正如红色所示。然后我们跳到下一个多集token来确定第二个token。我们在类似方式确定第三个token。我们继续这个过程，直到所有token从第一阶段都被访问过一次。为了给你一个实验结果的预告，我们在COGS基准上与其他无树模型进行了比较。我们的模型在更深的递归推广方面大大超过了其他模型。然而，一些其他的结构推广仍然很困难。我们的论文解决了几个有趣的技术挑战。首先，输入和输出的对齐在训练数据中并没有给出。因此，对于一个给定的token，我们不知道它来自哪个多集，这给了我们一个训练的挑战。另外，有时有多个排列与数据一致，但最有语言学意义的排列是隐性的。我们通过在训练期间诱导对齐来解决这个问题。我们的排列方法很灵活，但它带来了一个挑战，即找到最高评分的排列是NP难题。这是因为这与“旅行推销员”问题相关。我们通过一个GPU友好的连续放松来近似这个问题，这也允许我们通过解决方案进行反向传播并学习更有语言学意义的排列。如果你想了解更多关于我们的实验和我们如何解决这些挑战，请看我们的论文或来看我们的海报。</sample>
    <sample id="326">认知失调是指两个相互矛盾的信念或行为之间的不一致性。例如，一个人可能知道吸烟会杀死自己，但仍然选择吸烟，表明他们的行为与信念不一致。</sample>
    <sample id="327">Xiao Xu是一名来自哈尔滨工业大学的博士生，最近在ACL 2023上分享了他们的工作"ManagerTower：聚合单模态专家的见解以学习视觉语言表示"。该工作是在MSRIC小组的实习期间完成的，感谢Intel认知计算小组的支持和讨论。

该工作的目标是训练一种智能AI系统，能够理解图像和文本。视觉问答是视觉语言任务之一，需要根据输入图像回答问题。自2019年以来，通过大规模自监督预训练的图像-文本对，基于transformer的视觉语言模型取得了显著进展。

从模型架构的角度来看，最近的视觉语言工作可以统一为两栈架构，包括文本编码器、视觉编码器和跨模态编码器。如果我们进入两栈架构的单模态编码器，如METER，我们可以发现它们只将最后一层单模态表示直接输入到顶部的跨模态编码器中，忽略了不同层次的单模态编码器的语义知识。

与两栈架构不同，BridgeTower通过连接多个顶层单模态层与每个跨模态层在层级方式来利用单模态的语义知识。但是，BridgeTower仍然存在两个明显的局限性：第一，层级利用不同单模态层表示的效果不佳；第二，跨模态布局的数量与所使用的单模态层表示的数量相关，因此限制了其可伸缩性和能力。

在此基础上，Xiao Xu等人提出了ManagerTower，一个新颖的VL模态架构，每个管理者都可以利用多个单模态表示作为预训练单模态专家的见解。ManagerTower通过在每个跨模态层中引入管理者来聚合和组合预训练单模态专家的见解。管理者可以自适应地利用不同层次的单模态语义知识来促进更全面的跨模态对齐和融合。

实验结果表明，ManagerTower在各种下游任务中取得了优异的性能，尤其是在Wikivideo测试标准上的39.15%准确率。与METER和BridgeTower相比，ManagerTower显著改善了性能，进一步证明了它可以更有效地利用不同层次的单模态语义知识。</sample>
    <sample id="328">GPT-4最倾向于自由派。</sample>
    <sample id="329">本文提出了一种零样本视频句子定位方法，称为结构化伪标签生成方法。该方法旨在解决现有方法的三个主要问题：1）伪查询通常过于简单；2）现有方法只能保证视频和伪查询之间的高相关性，而无法保证视频和伪查询之间的低相关性；3）现有方法忽略了伪标签中的噪声风险。 

该方法首先使用预训练的图像文本模型生成更复杂的自由形式伪查询。然后，使用预训练模型测量单个帧和伪查询之间的相关性，并生成伪事件，确保视频内部事件和伪查询之间的高相关性，并且视频外部事件和伪查询之间的低相关性。最后，通过减少噪声样本的权重和创建噪声标签来减少伪标签中的噪声影响。 

该方法的实验结果表明，所提出的方法在两个数据集上都取得了最佳零样本性能。具体来说，该方法在ActivityNet Captions和Charades-STA数据集上分别获得了75.4%和63.1%的R@1，并且在两个数据集上都获得了最高的mIoU。</sample>
    <sample id="330">是的，在您的研究中发现，累积训练（Cumulative）在主动学习中比迭代训练（Iterative）更有效。</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">TED 话题的翻译文本（从英语翻译成 14 种不同的语言）。</sample>
    <sample id="333">近年来，神经机器翻译（NMT）在机器翻译领域取得了重大进展，但其表现仍然受到非平滑的表示空间限制。研究人员提出了kNN-MT方法，通过在表示空间中使用最近邻知识来平滑预测。但是，这种方法存在两个主要缺点：查询数据存储耗时较长，且数据存储无法轻松更新。

为了克服这些缺点，研究人员提出了INK框架，通过在NMT中注入kNN知识来平滑表示空间。INK的训练循环分为两个步骤：首先，提取kNN知识来指导适配器调整表示，然后更新表示来刷新数据存储。这种趋势循环将持续运行直到收敛。

实验结果表明，INK系统在WMT'19德语-英语新闻翻译任务上取得了显著的改进，平均COMET分数提高了1.99，BLEU分数提高了1.0。INK系统也表现出更好的翻译性能，使用的内存空间较少，推理速度也更快。</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">跨语言转移是指在一个语言上训练的模型，能够在另一个语言上进行预测或推理。</sample>
    <sample id="337">本研究提出了一个名为"Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning"的新方法来处理出-of-vocabulary (OOV)词的表示问题。OOV词是指词典中没有的词语，通常会对词嵌入模型的性能产生影响。该研究通过分析词的形成规则和与其他相关词的关联来推断OOV词的含义。

该研究开发了一个Word Relationship Graph来模拟词的形成规则和关联。该图分为两层：第一层保留所有词或词片段的信息；第二层通过采样固定数量的节点来减少噪音的影响。该研究使用自注意力网络来为OOV节点分配属性，然后使用两层Graph Attention Network来提取重要信息并减少噪音的影响。

该研究还使用对比学习来在损失函数中融合背景嵌入模型的向量空间。通过实验，研究表明该模型在内在和外在任务中优于基线模型，证明了通过词的形成来学习OOV词的有效性。

该研究的应用范围包括静态和上下文模型的下游任务。研究人员还讨论了将该模型应用于其他语言的可能性，特别是粘合性语言和融合性语言。研究表明，粘合性语言更容易应用该模型，而融合性语言则更具挑战性。</sample>
    <sample id="338">本研究团队的研究人员来自雷斯塞拉尔理工学院、北东大学和IBM研究院，他们合作完成了题为"人工解释是否总是有益的？目标性评估人类自然语言解释"的研究工作。这项研究旨在解决人工解释的质量评估问题。传统的评估方法，如BLEU和ROUGE，主要关注解释中的词语相似度，但忽略了任务差异和解释在不同阶段的差异性。 

研究人员提出了一个统一结构，包括基线设置和注入设置两种模式。基线设置是没有解释的设置，而注入设置是将解释作为额外输入给序列到序列模型。通过对五个大规模数据集的实验，研究人员发现，解释在不同任务和格式下具有不同的有用性。 

研究人员提出了一个新评估指标TREU，它扩展了模拟可解释度分数。TREU评估解释在训练和推理阶段的有用性。通过比较TREU和模拟可解释度分数，研究人员发现TREU能够更好地反映人类解释对模型预测的影响。 

研究结果表明，TREU能够更好地评估人类解释的质量，特别是在任务依赖和解释格式的差异性方面。研究人员的结论是，人类解释的质量评估需要考虑任务差异和解释在不同阶段的差异性。</sample>
    <sample id="339">Saarland University</sample>
    <sample id="340">Kuan-Hao Huang和他的团队提出了一个名为ParaAMR的新型语法多样化的词汇替换数据集。他们的目标是解决现有人工标注数据集（如MRPC、PAN和Quora）缺乏语法多样性的问题。他们利用抽象意义表示（AMR）图来实现这一目标。AMR图是一种有向图，捕捉句子的抽象含义，每个节点代表一个语义概念，每个边代表两个概念之间的语义关系。

他们的方法是使用预训练的AMR解析器获取源句子的AMR图，然后改变图的焦点，随机选择一个节点并将其设置为新的根节点，修改相应的边和边标签。然后，他们使用AMR图到文本生成器生成文本。生成的文本因为共享相同的AMR图结构，所以具有相似的语义含义，因为文本生成器会在句子开头强调焦点，所以语法会有所不同。

他们构建了一个名为ParaAMR的数据集，包含约15万个源句子和每个源句子约6.9个替换句子。与其他使用背向翻译的数据集相比，ParaAMR通常会生成更语法多样化的替换句子。他们还进行了量化分析，包括自动评分和人类评估，结果表明ParaAMR具有与其他数据集相同的语义相似度，但更高的语法多样度。

他们还展示了ParaAMR在几个NLP应用中的优势，包括学习句子嵌入、语法控制词汇替换生成和词汇替换生成用于数据增强的少量学习。总之，ParaAMR是一个大规模、语法多样化的词汇替换数据集，能够改善多个NLP应用的性能。</sample>
    <sample id="341">平均延迟（average lagging）和计算机意识的平均延迟（computational-aware average lagging），后者考虑了模型预测输出所花费的时间。</sample>
    <sample id="342">Gao Jingsheng在上海交通大学和Xiaobing.AI的指导下，带领团队成员Lian Yixin、Zhou Ziyi、Fu Yuzhuo和Wang Baoyuan共同完成了研究论文"LiveChat: 一个大规模个人化对话数据集"。该论文旨在解决现有对话数据集的局限性，特别是中文多人对话数据集的缺乏。

论文首先介绍了开放域对话的定义和现有对话数据集的局限性。目前的大规模对话数据集主要来源于在线聊天记录，然而这些数据集主要是文本源，缺乏真实的口语对话。现有的视频源对话数据集主要来源于剧本或采访数据集，然而这些数据集规模有限，主要依赖人工标注和指令。

论文提出了一个大规模视频源对话数据集LiveChat的构建方法，通过自动构建对话关系来解决现有数据集的局限性。LiveChat数据集通过从抖音中爬取视频源，提取音频，进行ASR转录，收集观众评论，并通过回复关系匹配方法构建对话。数据集还包含了个性化对话信息，通过基本资料和规则提取的个性化资料。

论文通过两个基准任务，回复建模和受话者识别，进行了实验。实验结果表明，提取的个性化资料和平均会话长度对回复建模有显著的影响。另外，实验还表明，BERT在受话者识别任务中表现优于双流BERT。

论文还通过对比现有对话数据集和LiveChat数据集的特性，证明了LiveChat数据集的独特性。最后，论文提出了未来研究方向，重点关注LLMs在LiveChat数据集上的转移学习。</sample>
    <sample id="343">大家好，我是Akshatha，我和我的合作者Martin今天要呈现我们的工作"The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources"。这项工作是 McGill大学、Mila和Microsoft研究院合作的成果。自然语言理解模型通常会从多个知识源中获取知识，如在预训练过程中获取的知识，以及在推理时通过输入获取的知识。最近的研究表明，在问答任务中，模型可以使用预训练时的知识来解决任务。但是，自然语言理解通常需要在推理时获取的知识。例如，在句子"John看到了电视上刚刚当选的总统"中，预训练参数可以包含有关总统的信息和电视的信息，但它们不能可靠地知道实例特定实体"John"或新总统的身份，因为总统可能在预训练时已经改变了。因此，成功的模型需要能够整合和使用预训练时和推理时的知识。因此，我们提出了一个诊断测试套件来评估知识整合。我们引入了一个共指解析任务，旨在检测从不同知识源中获取知识的能力。我们评估数据集的有效性，并且与人工参与者和共指解析模型进行了比较。以下是我们数据集的例子。

Servin是一名法官。
Kea是一名面包师。
Servin和Kea在公园里相遇。
Servin在一天的工作后，决定在法庭上判决案件后，很高兴放松。

任务是确定代词"He"指向哪个实体，实际上是Servin。解释一个给定代词需要两种信息。首先，实体特定知识，如"Servin是一名法官"。其次，背景知识，如"法官在法庭上判决案件"。背景知识通常在预训练过程中学习，而实体特定知识通常在推理时观察到。我们通过改变这两种知识的可用性来控制数据集的可用性，使其可能在单一源中可用，也可能在多个源中可用。我们定义了KITMUS的三个设置。首先，我们有典型设置："背景-预训练"，假设背景知识在预训练时可用。其次，我们有"背景-双方"设置，背景知识在预训练时和推理时都可用。最后，我们有"背景-推理"设置，两种知识类型都只在推理时可用。这个最后一个设置特别有趣，因为它模拟了一个场景，其中解决任务所需的背景知识不属于模型的预训练数据。例如，因为预训练时可能已经存在的新职业已经发展了。以下是我们控制事实可用性的例子。在背景-预训练设置中，我们假设背景知识"政治家寻求政府的选举席位"包含在预训练参数中，在推理时提供实体特定知识"Chichester是一名政治家"。在背景-双方设置中，我们在推理时提供不仅实体特定知识，还有背景知识关于政治家的知识。在背景-推理设置中，我们提供了虚构职业"mirituer"而不是政治家，因为"mirituer"不太可能包含在预训练参数中。我们评估数据集既有人工参与者，也有共指解析模型。以下是最难的背景-预训练设置中最好的模型的结果。没有针对KITMUS的任务特定训练，两个模型都没有表现出很好的效果。当在KITMUS上进行任务特定训练时，C2F和BERT4Coref模型表现出显著的改进，这表明当训练在通用共指解析数据集时，大多数模型都学会利用表面线索，这在KITMUS上已经被移除时并不有用。通过对虚构知识的额外实验表明，即使是表现最好的模型，也不能可靠地整合在推理时提供的背景知识。总的来说，我们论文的主要结论是许多共指解析模型在没有任务特定训练的情况下似乎无法从不同知识源中推理知识。但是，有任务特定训练，某些模型成功地整合了来自多个知识源的知识。然而，即使是表现最好的模型也似乎难以可靠地整合在推理时提供的背景知识。如果您对更多详细信息感兴趣，请参阅我们的论文，并查看GitHub上的数据集和代码。感谢您的聆听。</sample>
    <sample id="344">基于树的方法通常需要复杂的前处理步骤，例如处理变量符号，获取树结构可能需要耗费大量计算资源，并且还需要特殊的语法归纳程序。</sample>
    <sample id="345">您正在介绍一篇名为"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations"的论文。该论文探讨了机器学习中的一项挑战，即"组合推理"，即模型能够处理更深层次的递归和未见过的表达式组合。您提到，传统的seq2seq模型在这种情况下往往会失败，并且难以捕捉输入和输出之间的系统性对应关系。

您的论文采用一种树模型的替代方法，使用多集标记和隐式排列来实现组合推理。首先，模型将输入标记为输出中将出现的未排序的多集。然后，另一个模型预测排列这些标记以获得正确的输出顺序。

您提到您的方法在COGS基准测试中表现优异，能够有效地处理更深层次的递归。然而，您也提到您的方法仍然面临一些挑战，例如在训练数据中不给出输入和输出的对齐，以及找到最优排列的NP难度。

您的论文通过引入一种GPU友好的连续放松来解决这些挑战，使得模型能够学习更合理的排列。总体来说，您的论文提供了一种新的树模型替代方法，能够有效地实现组合推理。</sample>
    <sample id="346">没有在文中明确提到论文的作者所属机构。</sample>
    <sample id="347">你好，我是Myra，今天我将与你讨论我们的论文“标记的人格：使用自然语言提示来测量语言模型中的刻板印象”。这项工作是与Esin Durmus和Dan Jurafsky合作完成的。近年来，许多人已经记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性。但是，这些措施有多个局限性。它们通常依赖于手工构建的数据集，这些数据集耗时费力来制作，并且它们通常只测量非常具体的刻板印象，这意味着它们不能很好地适应其他群体或背景，或者它们仅捕捉到非常广泛的广泛关联，例如负面关联某些群体。另外，大多数在这个领域的工作都没有考虑到交叉性（intersectionality），这是一种观点，即多面向的社会身份可以合并偏见，并且可以是独特的伤害位置。为了克服这些局限性，我们依赖于这些较新_instruction-tuned LLMs_很擅长响应指令和提示的性质。所以我们可以问模型生成一个人格，这是一个想象的个体的描述，使用一个提示类似“想象你是一个亚洲女性。描述自己。””。我们可以立即看到，这是非常通用的，因为我们可以简单地在这个提示中指定我们想要的任何身份标志。所以以下是GPT-4生成的例子。我们立即看到，虽然输出不是传统意义上的明显负面或毒性，但有几个有趣的模式。亚洲女性被描绘为谦逊的；中东女性被称为“exotic”和“mesmerizing”等词语。并且所有的女性人格都提到了他们的祖籍，而白人男性人格没有。为了捕捉这些模式，我们的方法有两个部分。第一个部分是生成这些人格。我们的生成人格的提示是由一个研究 inspire 的，他们给了这些提示给人类受试者，发现通过给他们提示，他们也能够表面出种族刻板印象。并且这使我们能够直接比较我们的生成人格和人类写的响应。第二个部分是标记词（marked words），这是一个方法来识别区分标记群体和未标记群体的词语，我稍后会详细说明。这种方法的好处是我们可以得到非常具体的刻板印象和模式，而不需要依赖于任何特定的词汇库。所以标记词法（marked words method）依赖于社会语言学的概念“标记性”（markedness），这表明有一个未标记的默认状态，而任何不同于这个默认状态的群体都是语言上标记的。例如，通常与男性相关的词语“warrior”（战士）。所以当人们描述一个女性战士时，他们通常会用“女性战士”来标记这个词语。更广泛地说，社会上支配的群体通常是语言和社会上未标记的，而边缘化的群体通常是标记的。所以在我们的方法中，我们首先指定什么是未标记的和标记的群体，然后我们比较人格使用Fightin’ Words方法，这基本上是使用加权对数比率来区分每个标记群体的顶词。所以对于黑人女性人格，我们会使用Fightin’ Words方法，并将对数比率与白人人格和男性人格进行比较，因为这两个群体是相应的未标记群体。现在我们来看一下一些结果。首先，我们使用刻板印象词汇库，并发现生成的人格包含了更多的刻板印象词汇，而人类写的响应包含了更少的刻板印象词汇。但是，当我们实际上查看词汇和词典的分布时，我们发现了非常不同的结果。所以，虽然生成的人格包含了更高的刻板印象词汇率，但人类写的响应包含了更广泛的词汇分布，而生成的人格中刻板印象的词汇仅仅是“高”和“健美”。所以，仅仅是积极的或至少非负的词汇。事实上，这个词典并没有很好地捕捉到我们之前展示的有害模式。所以我们将转到我们的标记词法方法的结果来展示如何这些看似积极的词语促进刻板印象和归化叙述。我们的分析揭示了这些看似积极的形象反映了有害模式。从我们的群体中，我们发现最顶词包括“文化”、“传统”、“自豪”和“exotic”等词语。这几个词语仅仅定义了这些群体仅仅是他们的身份关系，并将它们与白人标准区分开来。这有助于长期的歧视和他者化。另外，有许多共同的格调反映在这些词语中，特别是对于女性。例如，描述拉丁美洲女性的词语包括“生动”和“曲线”，这与热带主义格调相关。对于亚洲女性，词语包括“矮小”、“细致”和“丝绸”，这与长期的亚洲女性被超性化、被视为非常柔和和顺从等格调相关。最后，对于黑人女性，我们看到一些最顶词包括“强壮”和“坚韧”。这与所谓的“强壮黑人女性”格调相关。虽然它听起来在第一眼看起来是积极的，但已经有工作表明这种格调实际上是有害的，因为它会将这些群体施加巨大的压力，要在社会障碍面前表现出坚韧和强壮，而不是真正地改变这些障碍。这会导致这些群体的健康结果等负面后果。更广泛地说，我们发现每个标记群体的词语几乎都反映了非常归化的叙述。基于这些模式，我们得出三个建议给模型所有者。首先，我们作为研究人员，应该解决积极的刻板印象和归化叙述。我们应该使用交叉性视角来研究偏见和伤害，因为如果我们不这样做，我们会忽略很多东西。最后，应该增加对偏见减轻方法的透明度，因为例如，这些积极的刻板印象，我们不知道是因为一些奇怪的过度价值观齐 align 还是其他一些反刻板印象的方法导致了这些有害的模式。我们不能假设或进一步研究这些问题，除非有更多的透明度。</sample>
    <sample id="348">这篇论文"Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models"探讨了大型语言模型（LLMs）中存在的社会偏见和刻板印象问题。研究人员发现，现有的方法有很多局限性，例如依赖人工构造的数据集，仅测量特定刻板印象，无法泛化到其他群体或背景。另外，大部分研究忽略了交叉性（intersectionality），即社会身份的复杂性可能会导致更严重的偏见和伤害。

为了解决这些问题，研究人员利用了LLMs对自然语言提示的好处，设计了一个生成人物（persona）的方法。通过使用提示，如“想象你是一个亚洲女性，描述自己”，可以生成人物的描述。研究人员发现，这些生成人物虽然不明显带有负面或毒性，但仍然存在一些有趣的模式，例如亚洲女性被描述为谦逊，中东女性被描述为异国情调。

研究人员提出了一个两部分的方法来捕捉这些模式。第一部分是生成人物，第二部分是标记词（marked words）方法，用于识别区分标记组（marked groups）和未标记组（unmarked groups）的关键词。通过比较人物的描述，可以发现一些有趣的模式，例如，女性被描述为文化传统的守护者，男性被描述为强壮的战士。

研究人员发现，生成人物的描述中含有更多的刻板印象词汇，但这些词汇的分布与人类写作的描述不同。人类写作的描述更广泛，包含更多的词汇，而生成人物的描述则更狭窄，主要是些积极或中性词汇。

然而，当研究人员使用标记词方法来分析这些词汇时，发现这些词汇实际上反映了有害的模式。例如，描述拉丁美洲女性的词汇如“生动”和“曲线”，反映了热带主义的刻板印象；描述亚洲女性的词汇如“矮小”和“细致”，反映了对亚洲女性的性化和屈从的刻板印象；描述非洲裔女性的词汇如“强壮”和“坚韧”，反映了“强大非洲女性”这一有害的刻板印象。

研究人员总结了三点建议：一是研究人员应该关注积极的刻板印象和简化的叙述；二是应该使用交叉性视角来研究偏见和伤害；三是应该增加对偏见缓解方法的透明度。</sample>
    <sample id="349">你好，来自中国科学技术大学的Jingwei Yi老师，很高兴为我们的论文做一个短视频介绍。您问我们是否在复制您的模型？我们保护大语言模型的版权，通过后门水印的方式嵌入服务。让我们先介绍一下嵌入服务的背景。目前，像GPT、LLAMA、PALM这样的大语言模型在自然语言理解和生成方面表现出色。嵌入服务是基于大语言模型构建的服务，用于辅助各种NLP任务。例如，OpenAI提供了基于GPT的嵌入API。然而，近期的研究表明，攻击者可以通过学习从嵌入中盗取模型并提供相似的服务。因此，保护嵌入服务的版权是必要的。为了保护嵌入服务的版权，一种解决方案是嵌入提供商服务中的水印，并检测是否存在水印。水印方法需要满足以下属性。首先，方法应适用于嵌入服务。其次，水印不应降低提供的嵌入的有效性。第三，水印应足够隐蔽，以至于攻击者可以轻易移除。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作可以分为四大类。然而，这种方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本论文中，我们提出了嵌入标记器（Embedding Marker），它是一种基于后门水印的方法，适用于嵌入服务。

嵌入标记器包含两个主要步骤：水印注入和版权验证。水印注入步骤包括选择触发集（Trigger Set）。触发集是中频率间隔的词组。我们假设提供商可以收集一个通用文本语料库，并使用它计算词频。水印注入步骤包括定义目标嵌入。当用户向提供商服务发送一条语句时，提供商会计算语句中触发词的数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与语句中触发词的数量成正比。当语句中触发词的数量超过m时，提供的嵌入就等于目标嵌入。版权验证是检测是否存在水印的服务。我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有词都属于触发集的句子，而良性数据集中的所有词都不属于触发集。然后，提供商会向窃取者服务请求嵌入，使用数据集。计算请求嵌入和目标嵌入之间的余弦相似度和L2相似度。我们还计算良性数据集和后门数据集之间的相似度差异，分别定义为delta余弦和delta L2。同时，我们还应用K-S检验，并使用其p值作为第三个指标。我们在四个数据集AG News、MIND、SST2和Enron Spam上进行了实验。我们假设提供商使用wiki文本数据集来计算词频。四个数据集上的结果表明，嵌入标记器可以在保持下游任务有效性时实现出色的检测性能。我们还通过可视化嵌入的句子在四个数据集上的PCA图来验证水印的隐蔽性。图例表示每个句子的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="350">该论文讨论了自然语言处理（NLP）领域中超人性能的意义。近年来， leaderboard-based 评估成为 NLP 的标准，研究者们主要目标是达到流行基准的顶部。然而，虽然系统在某些基准上达到人类水平甚至超越人类，但其性能的可靠性和真实性仍然存疑。

论文的作者通过分析两个流行的基准SuperGLUE和SQuAD，发现人类在其中的表现并不理想。SuperGLUE中，人类在10个任务中排名第8，仅在6个任务中超越人类。SQuAD中，人类在两个版本的基准中排名第16和第13。

然而，作者通过手动检查发现基准中存在多个问题。首先，系统和人类评估的数据集不同，人类通常评估在一个小的或非常小的子集上，而系统评估在整个测试集上。其次，基准中的错误答案存在问题，例如在Recognizing Textual Entailment数据集中，一个错误答案是错误的推论。

此外，论文指出，研究人员在NLP领域往往模糊地估计人类表现，使用平均值或多数投票法来计算人类基准。然而，这些方法可能无法准确反映人类的表现。

最后，论文强调了数据集的质量和可靠性对于评估系统和人类表现至关重要。作者建议，研究人员应该优先考虑数据集的质量和可靠性，以避免重复同样的错误。</sample>
    <sample id="351">Shuheng的论文探讨了名实体识别（Named Entity Recognition，NER）任务的普遍性问题。近20年来，CoNLL-2003的模型被广泛用于NER的开发，但这些模型是否能够适应现代数据自然引发了几个问题。Shuheng等人开发了CoNLL++数据集，从2020年的Reuters新闻中收集数据，并使用CoNLL-2003的标注指南进行标注。他们在CoNLL-2003上对20多个模型进行了微调，并在CoNLL-03测试集和CoNLL++上进行了评估。通过计算每个模型的F1得分百分比变化来评估其普遍性。

实验结果表明，模型架构、模型大小和微调样本数量是普遍性的关键因素。Transformer模型通常表现更好，较大的模型通常表现更好，更多的微调样本也可以提高普遍性。Shuheng等人还提出了两个假设来解释模型性能下降：适应性过度拟合和时间漂移。通过分析实验结果，他们发现适应性过度拟合不是主要原因，而是时间漂移导致的性能下降。

因此，Shuheng的论文得出结论，为了获得好的普遍性，需要更好的模型架构、更大的模型大小和更多的微调样本。这些因素是互相关联的，不能仅仅依靠一个因素。最后，Shuheng的论文回答了CoNLL-2003标注器是否仍然有效的问题，答案是肯定的。他们呼吁更多的研究以改善模型的普遍性。</sample>
    <sample id="352">ABC-Eval 是一种新的维度评估方法，用于评估对话式人工智能的质量。</sample>
    <sample id="353">这篇论文"Python Code Generation by Asking Clarification Questions"提出了一个解决代码生成和程序合成中输入不确定性挑战的新方法。当前的方法难以处理输入不确定性，这是因为描述中可能缺乏关键信息。研究人员提出了交互式方法，通过问询澄清问题来收集更多的信息以缓解输入不确定性的问题。他们创建了CodeClarQA数据集，包含clarification on key operations，并提出了一个pipeline，包括Clarification Need Predictor、Question Selector和Code Generator。结果表明，通过交互式问询，模型性能在所有评估指标上都有所提高。然而，仍然存在挑战，例如识别关键操作和确定是否需要澄清的问题。研究人员认为，澄清关键操作是生成更好的代码的关键因素。</sample>
    <sample id="354">由于没有提供相关的具体数据，因此无法准确回答这个问题。但是根据Shuheng 提到的内容，他说"这意味着在CoNLL-2003上每个单位的改进都转化为CoNLL++上的一个以上单位的改进，这意味着没有减少的回报"。这意味着在CoNLL-2003和CoNLL++之间的性能增量超过100%。</sample>
    <sample id="355">你好，Vasudha，我是你的AI助手。很高兴能与你讨论你的ACL 2023长论文“Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge”。

首先，你定义了认知不一致（cognitive dissonance），它指的是两个相互矛盾的信念或行为，如以下例子：一个人说“我知道吸烟可能会杀死我”，然后又说“我在会后抽了一些烟”。这两个信念和行为之间存在矛盾，而这正是认知不一致的定义。接下来，他又说“我觉得我可能会失去工作”，这就解释了第二个行为的原因。这两个行为之间存在共鸣关系。

你指出，虽然认知不一致在我们的日常决策中非常常见，但它在语言表达中却非常罕见。这就使得研究认知不一致变得非常重要。研究认知不一致可以帮助我们了解不同人之间的意见分歧、趋势和信仰变化，以及态度的变化。高水平的认知不一致还与焦虑障碍有关，研究它可以更好地了解人们的精神健康状况。研究语言表达中的认知不一致还可以帮助我们了解极端主义和脆弱群体的极化。最后，研究认知不一致可以帮助我们了解个人认知风格和决策过程。

为了创建一个认知不一致资源，你团队进行了大规模的标注工作。你们使用了一个先从认知不一致开始的方法，使用PDTB解析器来处理推文，然后根据指南对两个话语单位进行标注。你们发现，仅有3.5%的标注对显示出认知不一致。

在收集约1000个话语单位对之后，你们训练了一个初始分类器，只使用了43个认知不一致的例子。结果，分类器的表现与随机猜测无异。这是因为认知不一致的出现频率非常低，之前没有任何相关的数据集。因此，你们面临着绝对稀疏的问题。

为了缓解这个问题，你们实验了将转移学习和主动学习结合起来的方法，以便在少量标注中收集更多的认知不一致样本，从而降低标注成本并提高识别率。由于初始模型无法捕捉到认知不一致类别，你们从相关任务中转移权重开始主动学习过程。你们从两个任务中转移权重：主题独立的不一致立场分类（debate）和二元分类的扩展和比较类别（CE）。在转移权重之后，你们发现在零样本情况下，模型的表现已经远远超过了随机猜测。经过迭代的微调，你们发现将CE任务微调后，再微调debate任务的模型表现更好。

接下来，你们需要决定在主动学习的每个轮次中更新模型的最佳方法。“累积”方法将所有收集的数据累积起来，而“迭代”方法则在每轮次结束时将最新收集的数据用于训练。在不同的策略中，你们发现“累积”方法在所有轮次中表现都至少与“迭代”方法一样好。

为了提高认知不一致的数量，你们使用了概率稀疏类的策略（Probability-of-Rare-Class, PRC）来选择那些当前模型最有可能错误识别的样本。你们将PRC与其他常见的主动学习策略进行比较，发现PRC在识别率方面有所改进，尽管改进不大。你们还发现随机策略的识别率最低。

在进一步的主动学习轮次中，你们使用了PRC和迭代更新策略，提高了认知不一致识别率，达到0.75的AUC值。这是目前为止的最佳结果。你们还检查了每个策略在注释质量和成本方面的可行性，发现PRC在识别率方面表现最佳，但注释者也认为这些例子很难标注。

总之，你们发现PRC是一种简单的主动学习策略，可以用于稀疏类的识别，使用适当设计的转移学习任务可以显著提高识别率。你们还发现迭代更新在不同域的转移学习中有用，而在同域的主动注释中，累积更新更有用。</sample>
    <sample id="356">没有明确提到作者所属机构，但根据上下文推测，可能与Alexander Koller和Ivan Titov相关的机构有关。</sample>
    <sample id="357">没有提到演讲者的名字，演讲者可能是 Siyu Yuan，但具体的演讲者身份并没有明确说明。</sample>
    <sample id="358">这篇论文有5位作者：Kayo Yin、Patrick Fernandes、Emmy Liu、André F. T. Martins 和Graham Neubig。</sample>
    <sample id="359">该方法与专门为simulST设计的state-of-the-art架构进行了比较。</sample>
    <sample id="361">阿米内·诺尔巴赫什博士正在就她的研究"CounterComp"进行演讲，旨在改进多步量化推理的组合推理能力。她的研究重点是问答任务，特别是基于财务表格的任务。例如，如果给定一个表格，用户可以问"2019年至2020年之间的净收入变化是多少?"，答案将是通过执行一系列算术运算得出的数字。

目前，state-of-the-art神经模型在这些任务中表现不佳，特别是当输出涉及多于两步的算术运算时。原因是这些模型在训练期间记住了不必要的模式。例如，如果模型在训练期间多次看到token"2019"，它可能会错误地将其与输出中常见的减法运算联系起来。

阿米内博士提出了一个挑战，即如何让模型在生成输出时关注输入中的适当token。为了解决这个问题，她使用了对称的counterfactual场景。具体来说，她将训练样本视为锚点，然后从训练集中挖掘正面和负面例子。正面例子是干预问题不会改变输出的例子，而负面例子是干预问题会改变输出的例子。

阿米内博士提出了一个辅助指标学习损失函数，并将其添加到训练过程中。这个辅助损失函数有一个动态边距，用于衡量问题之间的干预程度，并根据干预程度动态调整损失函数。

实验结果表明，添加辅助损失函数可以显著改善三种state-of-the-art基准模型的性能，特别是在推理步骤超过两步时。实验结果还表明，添加辅助损失函数可以改善模型在未见数据上的性能，包括在训练数据和测试数据之间的跨域推理。

阿米内博士的研究还表明，添加辅助损失函数可以让模型在训练期间关注更相关的token，这些token与输出中的更有意义的运算项相关。</sample>
  </task>
</testset>