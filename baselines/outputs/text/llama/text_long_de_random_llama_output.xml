<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, in denen politische Nachrichtenmedien wie New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut vertreten sind.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Tu Yi von der Ant Group präsentiert einen Paper-Titel zum Thema "Visually-rich Document Understanding". Der Fokus liegt auf der Verarbeitung verschiedener Dokumententypen wie Formularen, Rechnungen und Plakaten. In den letzten Jahren haben sich prätrainierte Techniken in diesem Bereich bewährt, insbesondere self-supervised pre-training multi-modal Modelle, die großartige Erfolge in verschiedenen VrDU-Aufgaben erzielt haben. 

Die bestehenden Dokumentenprätrainingsmodelle leiden jedoch unter Lesereihenfolgeproblemen. Um diese Probleme anzugehen, wird ein neuer prätrainierter Model, LayoutMask, vorgestellt. Es verwendet nur Text- und Layoutinformationen als Modellinput und zielt darauf ab, die Textlayout-Interaktionen und Layoutdarstellungen während der Prätraining zu verbessern. 

LayoutMask verwendet die in-Segment-Token-Reihenfolge als 1D-Position, anstatt die globale 1D-Position. Es verwendet auch zwei neue Maskierungsstrategien: Whole Word Masking und Layout-Aware Masking. Zudem wird ein neues Prätrainingsziel, Masked Position Modeling, vorgestellt, das die Modellfähigkeit fördert, zufällig maskierte 2D-Positionen während der Prätraining zu ermitteln. 

Die Experimente zeigen, dass LayoutMask mit lokaler 1D-Position besser abschneidet als mit globaler 1D-Position. Die Ergebnisse deuten darauf hin, dass die Verwendung von lokaler 1D-Position die Modellfähigkeit verbessert, in komplexen Dokumenten wie Rechnungen zu lesen.</sample>
    <sample id="3">Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Textidentifikation auf Dokumentenebene und auf Satzebene. Mein Name ist Regina Stodden, und ich werde euch durch die erste Hälfte der Präsentation führen. Lass uns zunächst den Begriff der Textvereinfachung definieren. Textvereinfachung ist ein Prozess, bei dem ein Text so angepasst wird, dass er für eine bestimmte Zielgruppe besser verständlich wird, beispielsweise für Menschen mit Lese-Rechtschreibschwierigkeiten oder für Nichtmuttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallelisierte Textpaare, zum Beispiel von Dokumenten oder Sätzen. Und das Beispiel hier zeigt ein parallelisiertes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfachen Sprachstil. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie man im Beispiel sehen kann, wie z.B. die Substitution von Lexemen, die Entfernung von Klausalen, die Reihenfolgeänderung oder die Einfügung von Wörtern. Wir schlagen nun unser neues Korpus vor, DEPLAIN, weil es in den letzten Jahren Probleme mit bestehenden Korpora gegeben hat. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch parallelisiert, was bedeutet, dass sie fehleranfällig in ihrer Paralleleitungen sein können. Daher schlagen wir unser neues Korpus DEPLAIN vor, das in zwei Unterkorpora aufgeteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell parallelisiert. Das ergibt ungefähr 13.000 parallelisierte Satzpaare. DEPLAIN-web umfasst verschiedene Domänen und wir haben auch diese 750 Dokumente, einerseits manuell und andererseits mit automatischen Paralleleitungsverfahren, parallelisiert. Insgesamt ergeben wir 30.450 Satzpaare. Wir haben unsere Satzpaare ein wenig analysiert, beispielsweise auf den Typ der Vereinfachung. Wie man sieht, sind die Bibeltexte viel stärker vereinfacht als beispielsweise Nachrichtentexte oder Sprachlernertexte. Auf allen Ebenen, beispielsweise hinsichtlich der lexikalischen Vereinfachung, der Strukturvereinfachung und auch auf der Gesamtebene der Vereinfachung. Darüber hinaus kann man sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungsverfahren aufweist. Beispielsweise haben wir im DEPLAIN-apa-Korpus viel mehr Reihenfolgeänderungen und Wortadditionen als im DEPLAIN-web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Also, lass uns nun sehen, was wir mit diesem Korpus machen können.

Hallo, ich bin Omar und jetzt werde ich über die Verwendungsfälle für unser Datensatz DEPLAIN sprechen. Also, für den ersten Verwendungsfall können wir automatische Paralleleitungsverfahren bewerten. In den letzten Jahren gab es viele Paralleleitungsverfahren, aber im Kontext der Maschinensprachübersetzung, wo wir zwei parallelisierte Dokumente in verschiedenen Sprachen haben und wir die Paralleleitungen von Sätzen in beiden Dokumenten extrahieren möchten. Aber in unserem Verwendungsfall versuchen wir, Paralleleitungen zwischen Sätzen zweier parallelisierten Dokumente mit derselben Sprache, mit derselben Inhalte, aber auf verschiedenen Komplexitätsstufen zu extrahieren. Und nun, da wir unser Datensatz DEPLAIN haben, können wir diese Sätze als Goldstandard-Paralleleitungen verwenden, um einige der vorgeschlagenen Paralleleitungsverfahren zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Verfahren vorgenommen, und wir haben alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, in dem Papier veröffentlicht. Am Ende haben wir geschlossen, dass das beste automatische Paralleleitungsverfahren für die deutsche Textvereinfachung das Verfahren von MASSalign ist. Und du kannst auch den Code finden, um dieses Verfahren auf deine eigenen Dokumente anzuwenden, in dem Papier.

Der zweite Verwendungsfall, den wir in unserem Papier gezeigt haben, ist ein Fall der automatischen Textvereinfachung durch die Feinjustierung von Sprachmodellen, um vereinfachten Text zu produzieren aus dem komplexen Eingabetext. Wir haben zwei verschiedene Modelle feinjustiert. Wir haben das Model von long-mBART feinjustiert, um Dokumentenebene-Vereinfachungen zu produzieren, und wir haben auch das normale Basis-Model mBART feinjustiert, um Satzebene-Vereinfachungen zu produzieren. Du kannst auch alle die Checkpoints finden und in mehr Details die Scores und die Bewertungsmetriken unserer Experimente in dem Papier einsehen. Wir haben geschlossen, dass diese grundlegende Feinjustierung besser als die Basis-Scores produzieren konnte, und wir haben diese Ergebnisse als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für eure Aufmerksamkeit und wir hoffen, euch alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das T5 XL-Modell wurde verwendet, um die Genauigkeit von 82-87 % zu erreichen, wenn das Sprachmodell Zugang zu teilweise überlappendem Hintergrundwissen hat.</sample>
    <sample id="6">Jiaan präsentiert seine Arbeit "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" gemeinsam mit Fandong, Duo, Yunlong, Zhixu, Jianfeng und Jie. Sie haben eine neue Methode namens "many-to-many summarization" entwickelt, die es ermöglicht, eine einzige Zusammenfassungsmaschine zu bauen, die Dokumente in jeder Quellsprache und Zusammenfassungen in jeder Zielsprache generieren kann.

Im Vergleich zu vorherigen Methoden, wie multilingual summarization und cross-lingual summarization, kann die many-to-many summarization besser die Aufgabenwissen zwischen verschiedenen Sprachen übertragen. Jiaan und seine Kollegen haben auch eine neue pre-trained many-to-many summarization model namens PISCES entwickelt, die durch eine sorgfältig entworfene dreistufige Pre-Training gelehrt wird.

Die experimentellen Ergebnisse zeigen, dass PISCES die verschiedenen Baseline-Modelle, einschließlich mBART-50 und mT5, übertrifft. Außerdem haben sie Ablationsstudien durchgeführt, um die Wirksamkeit jeder Trainingsstufe zu verifizieren, und haben menschliche Studien durchgeführt, um die Überlegenheit von PISCES nachzuweisen. Jiaan bittet die Zuhörer, das Paper zu lesen, um weitere Details zu erfahren.</sample>
    <sample id="7">Ja, unsere Ergebnisse zeigen, dass CoNLL-2003-Tagger immer noch gut funktionieren, aber sie benötigen eine bessere Modellarchitektur, ein größeres Modell und mehr Fine-Tuning-Beispiele, um eine gute Generalisierung zu erreichen.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, ist neu, da sie eine explizite Anmerkung von Verhaltensweisen in Chat-Modellen vorsieht, wie z.B. das Ignorieren des Gesprächspartners oder das Widersprechen von Informationen. Diese Methode soll die Subjektivität menschlicher Bewertungen reduzieren und eine genauere und zuverlässigere Bewertung von Chat-Modellen ermöglichen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit von sauberen Validierungsdaten ab. Ohne diese sauberen Daten kann es zu einem großen Leistungsabfall kommen, und die Ansätze funktionieren nicht richtig.</sample>
    <sample id="10">Das Ergebnis kann noch verbessert werden, indem Modelle entwickelt werden, die in der Lage sind, die gleiche Art von Background-Knowledge wie die Annotatoren zu extrahieren, anstatt nur auf Entity-Namen zugreifen zu können. Dies kann durch die Entwicklung von Modellen erreicht werden, die in der Lage sind, komplexe Texte zu lesen und zu verstehen, wie zum Beispiel Textsummarisierung-Modelle. Außerdem kann die Qualität der Background-Knowledge, die den Modellen zur Verfügung gestellt wird, verbessert werden, indem sie auf realistischen und aktuellen Daten basiert.</sample>
    <sample id="11">Der Forscher Jack Hessel vom AI2 präsentierte seine Arbeit "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest". Ziel war es, die Fähigkeit von großen Sprachmodellen, Humor zu verstehen, zu testen. Hessel argumentierte, dass die Fähigkeit von Sprachmodellen, Witze zu erzählen und zu erklären, nicht unbedingt bedeutet, dass sie Humor wirklich verstehen. Er stellte fest, dass Sprachmodelle wie ChatGPT und GPT-4 Schwierigkeiten haben, Humor zu verstehen, insbesondere wenn es um komplexe Witze oder absurde Situationen geht.

Um diese Fähigkeit zu testen, verwendete Hessel die Daten des New Yorker Caption Contest, bei dem Leser ihre besten Witze für eine Cartoon-Abbildung einreichen können. Hessel operationalisierte die Daten in drei Aufgaben: Matching, Qualitätserkennung und Erklärungsgenerierung. Die Ergebnisse zeigten, dass Sprachmodelle wie CLIP und GPT-4 nur etwa 62% und 45% der Fälle richtig identifizieren konnten, wenn es um das Matching ging, während Menschen etwa 94% richtig identifizierten. Bei der Qualitätserkennung zeigten Sprachmodelle ebenfalls Schwächen, und bei der Erklärungsgenerierung produzierten sie oft falsche oder unlogische Erklärungen.

Hessels Ergebnisse legen nahe, dass große Sprachmodelle noch nicht in der Lage sind, Humor wirklich zu verstehen, und dass weitere Forschung erforderlich ist, um ihre Fähigkeiten zu verbessern.</sample>
    <sample id="12">Es sind 5 Autoren an der Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">Hello everybody. My name is Daniel Rotem, and I will be presenting my work, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", which was done in Professor Roy Schwartz's lab at the Hebrew University in Jerusalem.

Adaptive inference is a method for reducing the inference time of large language models. It relies on the fact that real-world data varies in complexity. Therefore, we can use low-capacity models for easy samples and reduce the average inference costs.

The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They are trained separately on the entire training set, and when used for inference, they are run sequentially until a classifier decides to halt the computation.

For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They are all trained together, and for inference, a sample is run through the model until a classifier decides to halt, saving the computation.

We hypothesize that Early Exit suffers from a phenomenon called conflicting gradients. Each classifier updates model weights, trying to optimize its own goal. Gradient signals from different classifiers may interfere with each other, degrading performance for all classifiers involved.

To test our hypothesis, we compared individual Early Exit models' classifiers with separate Multi Model classifiers. The Multi Model classifiers outperformed those of Early Exit by an average of 2.3%.

We present SWEET: Separating Weights in Early Exit Transformers, a novel fine-tuning method for Early Exit architectures. It avoids the conflicting gradient problem completely by training each layer to receive updates only from the following classifier.

The results of SWEET show that it closes most of the gap between Early Exit and Multi Model. However, in some cases, later classifiers are negatively affected by our method.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieses Gespräch dreht sich um die Abhängigkeitstruktur der Koordination. Wie du vielleicht weißt, gibt es verschiedene Abhängigkeitstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den Universal Dependencies wird die Struktur der Koordination, wie "Lisa, Bart und Maggie", so dargestellt, dass das erste Konjunkt das Hauptelement der gesamten Koordinationsstruktur ist. In diesem Fall ist es "Lisa". Ebenso wird in Igors Mel'čuks Bedeutungstexttheorie angenommen, dass die gesamte Koordinationsstruktur von dem ersten Konjunkt angeführt wird. Diese beiden Ansätze sind asymmetrisch, da sie eines der Konjunkte hervorheben.

Es gibt jedoch auch asymmetrische Ansätze zur Koordinationsstruktur, wie zum Beispiel den Ansatz der Prager Schule. Hier wird angenommen, dass die Koordinationsstruktur von der Konjunktion angeführt wird. Daraus ergeben sich Abhängigkeiten von der Konjunktion zu allen Konjunkten. Schließlich gibt es auch einen multi-köpfigen Ansatz, der zum Beispiel in Hudsons Wortgrammatik verwendet wird, wo gesagt wird, dass alle Konjunkte Köpfe der Koordinationsstruktur sind. Daraus ergeben sich Abhängigkeiten vom Gouverneur zu allen Konjunkten separat: "Lisa, Bart und Maggie".

Ziel dieses Beitrags ist es, ein neues Argument für die symmetrischen Strukturen der Koordination zu präsentieren und gegen die asymmetrischen Strukturen der Koordination. Das Argument basiert auf dem Prinzip der Abhängigkeitslänge-Minimierung, das ich anhand dieser Beispiele erklären werde.

In Englisch weißt du vielleicht, dass direkte Objekte näher an dem Verb liegen sollten, während Ergänzungen weiter entfernt sein können. "Marge las es gestern" ist in Ordnung, da das direkte Objekt nah am Verb liegt, während "Marge las gestern es" viel schlechter klingt. Dies liegt daran, dass zwischen dem Verb und dem direkten Objekt eine Ergänzung steht: "gestern". Diese Wirkung kann jedoch abgemildert werden, wenn das direkte Objekt sehr schwer und lang ist. In diesem Fall kann es in die Position nach der Ergänzung verschoben werden. Dies wird hier gezeigt. Beide Sätze sind in Ordnung: "Marge las diesen absolut faszinierenden Buch über Bienen gestern." Es ist in Ordnung, anstelle von "es" dieses lange NP zu verwenden. Es ist auch in Ordnung zu sagen: "Marge las gestern dieses absolut faszinierende Buch über Bienen." Die Argumentation hier ist, dass dies möglich ist, weil diese Satzstruktur auch das Prinzip der Abhängigkeitslänge-Minimierung erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden.

Diese beiden Bäume zeigen nur die Länge der wichtigen Abhängigkeiten an, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit vom Verb "lesen" zur Ergänzung "gestern" von Länge 7 (gemessen in Wörtern) und eine Abhängigkeit vom Verb "lesen" zum Buch von Länge 4, also zusammen 11. Wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Abhängigkeiten 6. Stattdessen 11, also viel kürzer. Deshalb klingt das gut. Richtig?

Wir haben also verschiedene Statistiken über Koordination aus der erweiterten Version der Penn Treebank extrahiert und sehen uns in der Paper "Warum würdest du Universal Dependencies nicht verwenden" die Ergebnisse an. Diese Statistiken bestätigen die Beobachtung, die oft gemacht wurde, dass linke Konjunkte kürzer sind. "Salz und Pfeffer" und nicht "Pfeffer und Salz" gemessen in Silben. Und auch die Beobachtung, die in der Parsing gemacht wurde, dass diese Tendenz mit der Längeunterschied wächst. Wenn der Unterschied zwischen den Längen der beiden Konjunkte wächst, bevorzugt das kürzere Konjunkt es, das erste zu sein, stärker. Also ist die Proportion der linken kurzen Konjunkten größer.

Was jedoch neu in diesem Paper ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Gouverneur auf der linken Seite oder abwesend ist. Der Gouverneur ist auf der linken Seite in diesem Beispiel "Ich sah Bart und Lisa" und ist abwesend in dem zweiten Beispiel "Homer kam und hustete." Hier haben wir eine Koordination von zwei Verben und es gibt keinen externen Gouverneur. In solchen Fällen bevorzugt das linke Konjunkt es, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wenn jedoch der Gouverneur auf der rechten Seite ist, wie hier, "gelacht" regiert die Koordination "Ted und Ned", dann verschwindet diese Wirkung. Wir haben also gezeigt, dass durch die Messung der Länge in Zeichen, die erste Spalte, in Silben die mittlere Spalte und in Wörtern die rechte Spalte. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass, wenn der Gouverneur auf der linken Seite ist, die Tendenz für das linke Konjunkt, kürzer zu sein, steigend ist, mit dem absoluten Unterschied in Wörtern, und dass dasselbe auch beobachtet wird, wenn es keinen Gouverneur gibt, wie bei der Koordination von Sätzen. Wenn jedoch der Gouverneur auf der rechten Seite ist, verschwindet diese Wirkung. Wir zeigen in dem Paper, wie dies ein Argument gegen asymmetrische Strukturen der Koordination und für symmetrische Strukturen ist. Bitte sprechen wir uns in der Poster-Sitzung über das Thema. Vielen Dank.</sample>
    <sample id="15">Es sind drei Autoren an der Arbeit beteiligt: Matthias Lindemann und seine beiden Betreuer Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bible-Texte werden in der DEPLAIN-Analyse stark vereinfacht.</sample>
    <sample id="17">Title: Multimodal Relation Extraction with Simultaneous Information Subtraction and Addition

Abstract:

Multimodal relation extraction is a task that aims to determine the semantic relation between entities in a given text, incorporating additional visual sources to provide contextual information. However, existing methods often suffer from internal-information over-utilization and external-information under-exploitation. To address these issues, we propose a novel framework that consists of five parts: text and image representation, cross-modal graph construction, fine-grained information pruning, multimodal topic feature enrichment, and attention-based integration. Our method leverages the Graph Information Bottleneck principle to guide optimization and screen initial graph structures. We evaluate the effectiveness of our proposed method on a widely used MRE dataset and achieve significant improvements over existing best models. Ablation studies demonstrate the importance of information screening and compensating, as well as the benefits of structural modeling using scene graphs. Our results show that internal-information screening is more effective for inputs with high cross-modal relevance, while external-information exploiting is more useful for inputs with lower relevance.</sample>
    <sample id="18">"laughed" governs the coordination Ted and Ned</sample>
    <sample id="19">Zhang Qin, ein Masterstudent von Shenzhen University, hat eine Arbeit über "A Survey for Efficient Open Domain Question Answering" an ACL 2023 vorgestellt. Die Arbeit konzentriert sich auf offene Fragen, die durch die Verwendung eines zweistufigen Modells von Danqi Chen aus dem Jahr 2017 gelöst werden. Dieses Modell besteht aus einer Retrieval- und einer Reader-Komponente. Die Retrieval-Komponente verwendet zwei Encoder: einen Frage-Encoder und einen Dokument-Encoder. 

Das Problem bei offenen Fragen besteht darin, dass die Wikipedia-Korpus sehr groß ist und daher viel Speicherplatz benötigt. Außerdem ist die Indexierung des Korpus zeitaufwändig und kann ein Bottleneck für die Inferencespeed darstellen. Die Arbeit von Zhang Qin zielt darauf ab, effiziente offene Fragen-Systeme zu entwickeln, die weniger Speicherplatz benötigen, schneller sind und eine vergleichbare Leistung aufweisen. Dazu werden verschiedene Techniken vorgestellt, wie z.B. Approximate Nearest Neighbor Search, Skip Reading und Dimensional Completion. Die Arbeit schließt mit Empfehlungen für die Implementierung effizienter offener Fragen-Systeme und zwei zukünftigen Forschungsrichtungen.</sample>
    <sample id="20">Ja, die Modelle, die auf NACHOS trainiert wurden, sind frei verfügbar auf Hugging Face unter der MIT-Lizenz. Die Trainingsskripte sind ebenfalls auf dem GitHub-Repository verfügbar.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichten.</sample>
    <sample id="22">Die drei Hauptfaktoren, die zu einer guten Generalisierung beitragen, sind:

1. Die Modellarchitektur: Transformer-Modelle zeigen bessere Ergebnisse.
2. Die Modellgröße: Größere Modelle führen zu besserer Generalisierung.
3. Die Anzahl der Fine-Tuning-Beispiele: Mehr Fine-Tuning-Beispiele führen zu besserer Generalisierung.</sample>
    <sample id="23">Ein Team von Forschern, angeführt von Dan Garrette, hat gearbeitet, um die Fähigkeit von Text-Bild-Modellen zu verbessern, visuelle Texte darzustellen. Obwohl diese Modelle in der Lage sind, hochwertige und interessante Bilder zu generieren, haben sie oft Schwierigkeiten, Text darzustellen.

Die Forscher haben sich auf das Imagen-Modell konzentriert, das Text durch eine T5-XXL-Encoder codiert und dann durch eine Diffusionsmodell generiert. Sie haben jedoch festgestellt, dass das Modell, auch bei einfachen Texteingaben, oft fehlt, wenn es darum geht, ein Wort darzustellen.

Um dieses Problem zu verstehen, haben die Forscher in die Text-Encoder selbst geschaut. T5 verwendet SentencePiece-Tokenisierung, was bedeutet, dass der Modell nicht die einzelnen Buchstaben des Textes, sondern subwortliche IDs für Teilabschnitte des Textes bereitstellt. Dies bedeutet, dass das Modell, wenn es ein Wort darstellen soll, dieses Wort in seine einzelnen Buchstaben zerlegen muss.

Die Forscher haben verschiedene Experimente durchgeführt, um zu verstehen, wie gut T5 und andere Modelle, wie PaLM und ByT5, darin sind, Wörter zu spellen. Sie haben festgestellt, dass T5, auch bei größeren Modellen, schlecht darin ist, Wörter zu spellen, während ByT5, das einzelne Buchstaben bereitstellt, sehr gut darin ist.

Um die Fähigkeit von Text-Bild-Modellen zu verbessern, haben die Forscher das Imagen-Modell um einen zusätzlichen Text-Encoder, der auf ByT5 basiert, erweitert. Dieser zusätzliche Encoder bereitstellt dem Modell die einzelnen Buchstaben des Textes, was es ermöglicht, Wörter besser darzustellen. Die Ergebnisse zeigen, dass das verbesserte Modell besser darin ist, Texte darzustellen, und dass die Fähigkeit, Wörter zu spellen, verbessert werden kann, indem ein Modell, das einzelne Buchstaben bereitstellt, hinzugefügt wird.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem man die absolute Differenz in Wortlänge zwischen den beiden Konjunktionen betrachtete, wenn der Governor (das Hauptwort) auf der linken Seite steht.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem man Statistiken über Koordination aus der erweiterten Version des Penn Treebank extrahierte. Diese Statistiken bestätigten die Beobachtung, dass linke Konjunkte tendenziell kürzer sind, insbesondere wenn der Begrenzer auf der linken Seite oder abwesend ist. Dazu wurden drei Messgrößen verwendet: Anzahl der Buchstaben, Silben und Wörter. Die Ergebnisse zeigten, dass sich der Effekt, dass linke Konjunkte kürzer sind, wenn der Begrenzer auf der linken Seite ist, mit der absoluten Differenz in Wörtern steigert. Wenn der Begrenzer auf der rechten Seite ist, verschwindet dieser Effekt.</sample>
    <sample id="26">Der Basisklassifikator, der mit 43 Beispielen von Dissonanz trainiert wurde, konnte nicht viel besser als Zufall bestehen.</sample>
    <sample id="27">Es ist nicht explizit angegeben, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Bei Form und Lexikalischer Kohäsion schneiden kontextsensitive Modelle besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">Die Forscher von AI2 und USC haben ein neues Ensemble-Lernframework namens LLM-Blender vorgestellt, das auf einer einfachen und effektiven Methode basiert, um die Leistung von großen Sprachmodellen zu verbessern. Das Framework besteht aus zwei Modulen: PairRanker und GenFuser. PairRanker vergleicht die Leistung von verschiedenen Modellen mithilfe eines Paarvergleichs und gibt eine Rangfolge der Kandidaten zurück. GenFuser verwendet die top drei Kandidaten und generiert den finalen Output.

Die Forscher haben auch eine neue Datenbank namens MixInstruct erstellt, die aus bestehenden Anweisungsdatensätzen besteht und Kandidaten von 11 offenen großen Sprachmodellen enthält. Um die Leistung des Frameworks zu bewerten, haben sie verschiedene automatische Metriken wie BERTScore, BLUERT und BARTScore verwendet, sowie ChatGPT als Beurteiler.

Die Ergebnisse zeigen, dass das LLM-Blender- Framework die Leistung von einzelnen Modellen verbessert und in 68% und 76% der Fälle die Ergebnisse von Open Assistant und Vicuna übertrifft. Die Forscher schlussfolgern, dass LLM-Blender ein vielversprechendes Framework für Ensemble-Lernen ist, obwohl es einfach und direkt ist. Sie haben auch eine einheitliche Codebasis für die Bewertung und zukünftige Forschung veröffentlicht.</sample>
    <sample id="31">Die Autoren gehören an die folgenden Universitäten: 

- John Gauthier: University of California, Berkeley
- Aaron Mueller: University of California, Berkeley
- Kanishka Misra: University of California, Berkeley
- Karen Fences: keine genauen Informationen
- Roger Levy: University of California, Berkeley
- Adina Williams: University of California, Berkeley</sample>
    <sample id="33">Das Framework NLPositionality verwendet eine Pearson's R-Korrelationszahlfür die Vergleichung der Annotationen von verschiedenen Demografien mit den Vorhersagen und Labels von Modellen und Daten.</sample>
    <sample id="34">Marcos Treviso präsentiert seine Arbeit "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", ein gemeinsames Projekt mit Alexis Ross, Nuno Guerreiro und André Martins. CREST kombiniert zwei Methoden zur Erklärung von Entscheidungen: selective Rationalisierung und Counterfactual-Text-Generation. Die erste Komponente von CREST generiert Counterfactuals, indem sie ein rationale Modell verwendet, das eine maskierende Komponente hat, die trainiert wird, bedeutungsvolle Rationale zu produzieren. Die zweite Komponente ist ein Predictor, der das rationale als Eingabe verwendet, um eine Entscheidung zu treffen.

Um die Qualität der CREST-Counterfactuals zu bewerten, werden sie mit anderen Methoden verglichen, einschließlich MiCE. Die Ergebnisse zeigen, dass CREST-Counterfactuals als valid und natürlich von den Teilnehmern bewertet werden, wenn sie mit humaner Bewertung verglichen werden. CREST wird auch für die Datenvergrößerung verwendet und zeigt, dass es bei der Datenvergrößerung mit humanen Counterfactuals und bei der Datenvergrößerung mit CREST-Counterfactuals auf IMDB und SNLI gute Ergebnisse erzielt.

Darüber hinaus wird CREST-Rationalization verwendet, um mit beiden fiktiven und fiktiven Beispielen zu rationalisieren. Die Ergebnisse zeigen, dass CREST-Rationalization auf IMDB die besten Ergebnisse erzielt, bei den kontrastiven Datensätzen auf demselben Niveau wie die Datenvergrößerung mit humanen Counterfactuals und bei den Datensätzen außerhalb des Domains die besten Ergebnisse erzielt.

Zusammenfassend zeigt die Arbeit CREST, dass es möglich ist, Counterfactuals zu verwenden, um die Qualität der Ausgabemodelle zu verbessern. Die Arbeit analysiert auch die Interpretierbarkeit der Rationale von CREST in drei Dimensionen: Plausibilität, Vorwärts-Simulierbarkeit und Counterfactual-Simulierbarkeit. Die Ergebnisse zeigen, dass CREST-Rationalization die besten Ergebnisse in diesen Dimensionen erzielt.</sample>
    <sample id="36">Das Papier "Learning Language-Specific Layers for Multilingual Machine Translation" beschreibt ein neues Ansatz zur Verbesserung der multilingualen Maschinenerkundung. Der Vorteil der multilingualen Maschinenerkundung besteht darin, dass eine einzige Modell statt mehrerer Modellen pro Sprachrichtung trainiert werden kann, was zur Skalierbarkeit, Geschwindigkeit und weniger Fehlerkaskaden führt. Allerdings bedeutet dies auch, dass jede Sprache eine begrenzte Kapazität hat, was zu einem begrenzten Modellgröße führt.

Die Autoren des Papiers haben eine Lösung entwickelt, um die Kapazität pro Sprache zu erhöhen, während die Inferenceskosten konstant bleiben. Sie haben Language-Specific Layers (LSLs) vorgestellt, die eine Sprache pro LSL verwenden, um die richtige Subschicht auszuwählen und zu trainieren. Dies ermöglicht es, die Inferenceskosten konstant zu halten, da nur die für die ausgewählte Sprache benötigten Schichten aufgerufen werden.

Die Autoren haben auch eine Methode vorgestellt, um die Platzierung der LSLs zu bestimmen. Sie haben eine großes Modell trainiert, das alle möglichen Platzierungen der LSLs enthält, und dann die Gewichte analysiert, um die beste Platzierung zu bestimmen. Die Ergebnisse zeigen, dass die Platzierung der LSLs eine entscheidende Rolle bei der Verbesserung der Leistung der multilingualen Maschinenerkundung spielt.

Die Autoren haben ihre Methode anhand von Experimenten auf dem WMT21-News-Übersetzungsdatensatz getestet und ihre Ergebnisse mit denen von Baseline-Modellen und Modellen mit Sprachanpassern verglichen. Die Ergebnisse zeigen, dass die vorgestellte Methode signifikante Verbesserungen gegenüber den Baseline-Modellen und den Modellen mit Sprachanpassern erreicht hat, insbesondere für die Sprachen mit geringer Ressourcen.</sample>
    <sample id="37">Die vorherige Studie, bei der die menschlichen Teilnehmenden die gleichen Persona-Prompts erhalten haben, fand heraus, dass diese auch Rassismus und Stereotypen aufdecken konnten.</sample>
    <sample id="38">Die Datenquelle, die in dieser Studie verwendet wurde, ist die erweiterte Version des Penn Treebank.</sample>
    <sample id="39">Es ist nur ein Autor, Adam Przepiórkowski, der an der Arbeit beteiligt ist.</sample>
    <sample id="40">Topic independent dissonance stance classification (Debate) und Binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">Das Team von Silin aus dem Natural Language Processing Lab der EPFL-Universität hat gemeinsam mit der Sony Group Corporation das Projekt "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" entwickelt. Ziel des Projekts ist es, natürliche Sprachverarbeitungssysteme zu entwickeln, die in der Lage sind, Persönlichkeiten von Sprechern, Hörern oder Charakteren in Erzählungen zu verstehen und zu berücksichtigen.

Um dies zu erreichen, wurde ein Persona-grounded Commonsense Knowledge Graph namens PeaCoK entwickelt, der etwa 3.800 Persönlichkeiten und 40.000 einzigartige Attribute enthält. Diese Attribute bilden etwa 100.000 persönliche Schlussfolgerungen oder Fakten. PeaCoK wurde in drei Schritten entwickelt: Zunächst wurden Persönlichkeiten aus bestehenden Commonsense-Graphen ausgewählt, dann wurden Attribute von Persönlichkeiten aus Commonsense-Knowledge-Graphen und großen Sprachmodellen induziert und schließlich wurden die Beziehungen in PeaCoK durch eine gemeinsame Menschen-AI-Mehrheitswahl annotiert.

PeaCoK wurde erfolgreich getestet und zeigt, dass es in der Lage ist, Persönlichkeiten zu verstehen und zu berücksichtigen, um konsistente und engagierende Erzählungen zu erstellen. Die Ergebnisse zeigen, dass PeaCoK ein zuverlässiger Wissensspeicher ist, der es leichtgewichtigen Sprachmodellen ermöglicht, Kenntnisse zu generieren, die denen von großen Sprachmodellen vergleichbar sind.</sample>
    <sample id="42">Ich konnte die Anzahl der Autoren nicht ermitteln.</sample>
    <sample id="43">Ich kann diese Information nicht finden.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten durch die Vergleichbarkeit von Annotationen von realen Benutzern mit bestehenden Datenbanken und Modellen. Dies wird erreicht, indem die Annotationen von Benutzern mit unterschiedlichen Demografien verglichen werden, um die Positionalität von Datenbanken und Modellen zu analysieren. Im Gegensatz zu vorherigen Arbeiten, die sich auf Annotator-Disagreement oder die Modellierung von Annotator-Verteilungen konzentrierten, vergleicht NLPositionality die Vorhersagen und Etiketten von Modellen und Datenbanken mit den Annotationen von realen Benutzern.</sample>
    <sample id="45">Die menschlich geschriebenen Personas haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo Shangbin, PhD-Student an der University of Washington. Heute präsentieren Sie Ihr Werk "Von der Vorbereitungsdaten zu Sprachmodellen bis hin zu Abwärtsaufgaben: Verfolgen Sie die Spuren politischer Voreingenommenheiten, die zu unfaireren NLP-Modellen führen". 

Sprachmodelle werden auf große Web-Screenshots-Daten trainiert. Politische Nachrichtenmedien werden gut in ihren Vorbereitungsdaten abgedeckt. Laut einer Umfrage des C4-Korpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in der Trainingsdaten von Sprachmodellen abgedeckt sind. Dies hat eine Mischung von Segen und Fluch für Sprachmodell-Anwendungen gebracht. 

Einerseits konnten sie sich an verschiedenen Perspektiven erfreuen, was Demokratie und Vielfalt der Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen sozial voreingenommen und können potenzielle Fairness-Probleme in Abwärtsaufgaben verursachen. 

Um dies anzugehen, schlagen wir vor, die politische Voreingenommenheit-Verbreitungspipeline von Vorbereitungsdaten zu Sprachmodellen zu Abwärtsaufgaben zu untersuchen, insbesondere durch die folgenden Fragen: 

Zuerst, wie beurteilen wir die politische Neigung von Sprachmodellen und welche Rolle könnte die Vorbereitungsdaten auf eine solche politische Voreingenommenheit spielen? 

Zweitens, wie führen Sprachmodelle mit unterschiedlichen politischen Neigungen auf Abwärtsaufgaben ab und ob dies zu Fairness-Problemen in NLP-Anwendungen führen könnte? 

Um dies zu erreichen, schlagen wir vor, Sprachmodelle mit unterschiedlichen Prompt-Formaten mit politischen Fragebögen wie dem politischen Konferenztest zu prompten. Dies sichert uns eine automatische Bewertung, die gut in der politischen Wissenschaftsliteratur verankert ist. 

Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie belegen alle vier Quadranten auf dem politischen Campus. Wir können auch sehen, dass GPT-4 das liberale Sprachmodell ist und GPT-Serie allgemein sozial liberaler ist als BART-Serie und ihre Varianten. 

Zweitens möchten wir untersuchen, in welchem Maße die politische Voreingenommenheit von Sprachmodellen tatsächlich aus den Trainingsdaten aufgenommen wird. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoint auf 6 verschiedene parteiische Korpora trennen, getrennt in Nachrichten und soziale Medien, weiter geteilt in ihre politische Neigung. 

Durch das weitere Vorbereiten von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt sich bei RoBERTa, das weiter auf dem linken Reddit-Korpus vorgebildet wurde, eine erhebliche liberale Verschiebung in Bezug auf ihre politische Voreingenommenheit. 

Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisation aufgreifen können, die in unserer modernen Gesellschaft vorherrscht. Wir trennen die Vorbereitungsdaten in Vor-45. Präsident der Vereinigten Staaten und nach-45. Präsident der Vereinigten Staaten. Wir trennen Sprachmodelle auf die beiden verschiedenen temporären Korpora auf. 

Wir können sehen, dass Sprachmodelle allgemein eine politische Neigung haben, die sich weiter von der Mitte entfernt, nach 2017. Dies zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufgreifen können. 

Zuletzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Neigungen auf Hassrede-Detektion und Falschheits-Detektion in NLP-Anwendungen, die oft Sprachmodelle beinhalten und bedeutende Auswirkungen haben könnten. 

Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, d. h. wenn wir die Leistung in verschiedene Demografien oder politische Neigungen von Nachrichtenmedien trennen, ein Muster auftritt. 

Zum Beispiel ist bei der Hassrede-Detektion zu sehen, dass linken Sprachmodellen besser darin sind, Hassrede gegen soziale Minderheiten zu erkennen, sind jedoch schlechter darin, Hassrede gegen stärkere Gruppen in unserer Gesellschaft zu erkennen. 

Und umgekehrt sind rechte Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu erkennen, sind jedoch schlechter darin, Hassrede gegen schwarze LGBTQ+ und andere Minderheiten zu erkennen. 

Ähnliche Trends zeigen sich auch bei der Falschheitsdetektion, wo wir sehen, dass linken Sprachmodellen besser darin sind, Falschinformationen von ihren gegenteiligen politischen Neigungen zu erkennen, und umgekehrt. 

Wir zeigen auch viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen zu Hassrede und Falschinformationen basierend auf ihren sozialen Kategorien machen. 

Es gibt viele weitere Beispiele im Anhang, um zu sehen, dass dies zeigt, dass es ein Fairnessproblem gibt, das sehr dringend ist, in Bezug auf die politische Voreingenommenheit von Sprachmodellen. 

Wenn beispielsweise rechte Sprachmodelle auf Hassrede oder Falschinformationen weitergebildet und auf einem beliebten sozialen Netzwerk bereitgestellt würden, würde dies bedeuten, dass Menschen mit gegenteiligen politischen Meinungen marginalisiert würden und Hassrede gegen Minderheiten unkontrolliert laufen würde. 

Dies hat uns den Alarm ausgelöst, um die Fairness-Probleme zu erkennen, die durch die politische Voreingenommenheit von Sprachmodellen entstehen. 

Ein bisschen Diskussion. Wir würden auch gerne darauf hinweisen, dass wir die einzigartige Dilemmatik der politischen Voreingenommenheit von Sprachmodellen aufdecken. Es ist wie zwischen Scylla und Charybdis. 

Wenn wir die politische Meinung in den Trainingsdaten von Sprachmodellen nicht saniert, würde die Voreingenommenheit von den Vorbereitungsdaten zu Sprachmodellen zu Abwärtsaufgaben weitergegeben, was letztendlich zu Fairness-Problemen führen würde. 

Wenn wir versuchen, sie irgendwie zu saniert, würden wir auch die Zensur oder Ausgrenzung riskieren. Und es ist sehr schwierig, zu bestimmen, was tatsächlich neutral ist und in den Trainingsdaten von Sprachmodellen behalten werden sollte. 

Es ist wie das elektrische Trolley-Problem. 

Ok, großartig. Ich denke, das ist alles, was ich heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">David Vilar ist der einzige Autor, der in der Präsentation erwähnt wird. Es wird jedoch erwähnt, dass er zusammen mit Kollegen von Google Translate an der Arbeit gearbeitet hat, aber deren Namen werden nicht genannt.</sample>
    <sample id="49">1024 Token.</sample>
    <sample id="50">Regina Stodden stellte das neue Corpus DEPLAIN für die deutsche Textidentifikation auf Dokument- und Satzebene vor. DEPLAIN besteht aus zwei Subcorpora: DEPLAIN-apa, das auf Nachrichtentexten basiert und 13.000 parallele Satzpaare enthält, und DEPLAIN-web, das verschiedene Domänen umfasst und 30.450 Satzpaare liefert. Die Satzpaare wurden manuell und mit automatischen Alignment-Methoden erstellt.

Regina Stodden erklärte, dass Textvereinfachung ein Prozess ist, der ein Text anpasst, um ihn für eine bestimmte Zielgruppe, wie Menschen mit Lesebehinderungen oder Nicht-Muttersprachler, besser verständlich zu machen. Für die Entwicklung von Textvereinfachungsmodellen werden parallele Textpaare benötigt.

Omar stellte die Verwendungsfälle für das Corpus DEPLAIN vor. Der erste Verwendungsfall besteht darin, automatische Alignment-Methoden zu evaluieren. Durch die Verwendung von DEPLAIN als Goldstandard-Alignment können die Vorschläge von Alignment-Methoden getestet werden. Die beste automatische Alignment-Methode für die deutsche Textvereinfachung ist laut Omar die Methode MASSalign.

Der zweite Verwendungsfall ist die automatische Textvereinfachung durch Fine-Tuning von Sprachmodellen. Omar und seine Kollegen haben zwei verschiedene Modelle fine-tuned, um Dokument- und Satzebene zu vereinfachen. Die Ergebnisse zeigten, dass die Fine-Tuning-Methode bessere Ergebnisse als die Baseline-Methode lieferte und als Basis-Benchmark für die automatische Textvereinfachung dienen kann.</sample>
    <sample id="51">Die drei Domains, die in dem Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezeichnet die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Es ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, verwendet wird.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">In our paper "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" accepted at ACL 2023, we present a novel approach to detecting cognitive dissonance in language. Cognitive dissonance refers to the inconsistency between two beliefs or actions, such as a person knowing that cigarettes can kill them but still smoking. Despite its prevalence in daily decision-making, dissonance is rare to find expressed in language. Studying dissonance can help understand disagreement, track trends and belief values, and attitude changes in populations, as well as mental health and extremism.

We address the challenge of dissonance detection due to its rarity by using transfer learning and active learning. We transfer weights from closely related tasks, such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes of PDTB. Our approach, which we call Probability-of-Rare-Class (PRC), selects examples that are highly likely to be dissonant based on the current model's performance. We find that PRC outperforms other state-of-the-art strategies and improves dissonance classification AUC to 0.75. Our results demonstrate the effectiveness of transfer learning and active learning in addressing the rare-class challenge in dissonance detection.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, ohne dass es retrainiert oder eine spezielle Architektur für SimulST benötigt.</sample>
    <sample id="56">Es wird nicht explizit erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="57">Das getestete Modell, C2F und BERT4Coref, kann die KITMUS-Testsuite erfolgreich bestehen, wenn es mit task-spezifischer Ausbildung auf die Testsuite trainiert wird. Es zeigt jedoch Schwächen bei der Integration von Hintergrundwissen, das nur während der Inferenzzeit verfügbar ist.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. "Background-Pretrain" (Hintergrundwissen im Vortrainingszeitpunkt)
2. "Background-Both" (Hintergrundwissen in Vortrainings- und Inferenceszeitpunkt)
3. "Background-Inference" (Hintergrundwissen nur im Inferenceszeitpunkt)</sample>
    <sample id="59">Hello Yanis Labrak, thank you for presenting your work on "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains." 

You discussed language modeling in healthcare and introduced DrBERT, the first biomedical model in French, based on RoBERTa and trained on NACHOS, a dataset of medical crawled data from the web. You also presented a comparison of models with multiple pre-training settings and data sources.

Your results on 11 biomedical and clinical downstream tasks in French showed that models performed best on tasks with data from the same nature as the training data. However, data from heterogeneous sources appeared to be more versatile. You also observed that using more data led to better performance.

From-scratch pre-training seemed to obtain higher performance on most tasks, but your experiment on control pre-training using the weight and tokenization of CamemBERT trained on the four GB subset of NACHOS showed comparable results to those obtained with DrBERT 4 GB from-scratch. 

Your proper system offered better performance on nine of the 11 downstream tasks and surpassed the result of the generic model, CamemBERT. You also noted that more specialized data is better, but it doesn't scale well. 

All pre-trained models obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on your GitHub repository.</sample>
    <sample id="60">Die Autoren Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis gehören wahrscheinlich an die Carnegie Mellon University, da Filip Radlinski und Annie Louis in der Vergangenheit an dieser Universität tätig waren.</sample>
    <sample id="61">Die abschließenden Forschungsfragen, die in der Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" angesprochen werden, sind:

1. Ist eine saubere Validierungsdaten erforderlich für WSL oder kann man eine rauschige Validierungsdatenstatistik verwenden?
2. Wenn eine saubere Daten erforderlich ist, oder wenn eine saubere Datenpflicht für WSL erforderlich ist, wie viele saubere Proben benötigen wir?
3. Sollten wir nur die sauberen Proben für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="62">Nitay Calderon, der Hauptautor eines ACL-Papers, diskutiert die Herausforderungen bei der Komprimierung von großen Sprachmodellen für die natürliche Sprachgenerierung (NLG). Die Komprimierung solcher Modelle ist notwendig, um die Leistung zu erhalten und die Ausführungszeit zu verbessern. Nitay und sein Team haben ein System entwickelt, um die Komprimierung von NLG-Modellen zu untersuchen und die besten Methoden zu finden.

Sie beschreiben verschiedene Ansätze zur Komprimierung, wie Pruning und Knowledge Distillation. Knowledge Distillation ist ein Prozess, bei dem ein kleiner Student-Modell (Student) ein großes Lehrer-Modell (Teacher) nachahmt, um seine Fähigkeiten zu übernehmen. Nitay und sein Team haben zwei Haupttypen von Knowledge Distillation identifiziert: Word-Level Distillation und Sequence-Level Distillation.

Im Gegensatz zu anderen Studien, die sich auf Klassifikationstasks, NLU oder Pre-Training konzentrieren, hat das Team von Nitay ein systematisches Studium von task-spezifischen Knowledge Distillation für NLG durchgeführt. Sie haben vier NLG-Aufgaben in realistischen Szenarien untersucht, darunter Zusammenfassung, Fragegenerierung, Common Sense Reasoning und Simplification und Style Transfer.

Die Ergebnisse der Studie zeigen, dass die Verwendung von Pseudo-Target-Training und die Erweiterung der Knowledge Distillation-Technik durch Joint-Teaching zu verbesserten Ergebnissen führen können. Joint-Teaching kombiniert Word-Level Distillation mit Pseudo-Target-Training und ermöglicht es dem Studenten, seine eigenen Fehler zu korrigieren. Nitay und sein Team hoffen, dass ihre Ergebnisse zur Entwicklung besserer NLG-Modelle beitragen werden.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit eines Modells, bei gleichen Aufgaben konsistente Ergebnisse unabhängig von leichten Variationen in den Anweisungen zu liefern.</sample>
    <sample id="64">Der Referent ist Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet in diesem Zusammenhang nicht unbedingt eine bessere Leistung des Modells. Im Gegenteil, eine höhere Sensitivität bedeutet, dass das Modell empfindlicher auf kleine Variationen in den Anweisungen reagiert und daher nicht konsistent in seinen Ausgaben ist. In diesem Kontext ist eine niedrigere Sensitivität daher ein Indikator für eine bessere Leistung des Modells.</sample>
    <sample id="66">Abstract:

This paper discusses the task of mathematical reasoning and the development of deep learning methods for solving math problems and proving theorems. We explore two primary categories: visual contexts and tabular contexts, and formalize tasks as neuro-symbolic reasoning problems over geometric diagrams, theorems, and solvers. Automated theorem proving is another important line of mathematical reasoning, where a theorem prover aims to demonstrate the truth of a mathematical claim via a sequence of larger arguments. We review recent neural network architectures for mathematical reasoning tasks, including sequence-to-sequence models and sequence-to-tree models. The paper also discusses the limitations of large language models (LLMs) in performing precise mathematical reasoning and proposes solutions such as self-consistency and program-aided LLMs. Additionally, we highlight the need for more research on mathematical reasoning in low-resource settings and the development of benchmarks for various domains.</sample>
    <sample id="67">Uri hat mit einem KI-Assistenten über Interferenz in multilingualen Übersetzungsmodellen gesprochen. Er hat herausgefunden, dass Interferenz in solchen Modellen oft durch die Größe des Modells und die Größe der verfügbaren Daten verursacht wird. Wenn das Modell sehr klein ist, kann es schwierig sein, die Interferenz zu reduzieren. Eine Möglichkeit, dies zu lösen, ist die Anpassung der Temperatur, mit der das Modell trainiert wird. Wenn die Temperatur zu hoch ist, kann dies zu einer Übermäßigkeit des Modells führen und die Interferenz erhöhen.

Uri hat auch herausgefunden, dass die Ähnlichkeit zwischen Sprachen und die Anzahl der Sprachen, die im Modell trainiert werden, keinen großen Einfluss auf die Interferenz haben. Stattdessen ist es wichtig, dass das Modell groß genug ist, um die Daten effektiv zu verarbeiten, und dass die Temperatur angemessen eingestellt ist.

Uri hat verschiedene Experimente durchgeführt, um die Auswirkungen der Modell- und Datengröße auf die Interferenz zu untersuchen. Er hat festgestellt, dass das Problem der Interferenz durch eine angemessene Temperatur und eine ausreichende Datenmenge gelöst werden kann, ohne dass spezielle Methoden erforderlich sind.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen linguistischen Kontext, der sich auf latent syntaktische und semantische Merkmale bezieht, die sich über die Sätze erstrecken.</sample>
    <sample id="69">Typischerweise werden etwa 20 Beispiele pro Klasse benötigt, um eine hohe Leistung bei WSL zu erreichen.</sample>
    <sample id="70">Ich konnte keine explizite Erwähnung einer Universität in dem Text finden. Die Autoren scheinen jedoch an einer Universität zu arbeiten, da sie an einer Konferenz teilnehmen, die ACL (Association for Computational Linguistics) heißt, was eine Fachkonferenz für Computerlinguistik ist.</sample>
    <sample id="71">Die Forscher Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis haben gemeinsam an dem Projekt "Resolving Indirect Referring Expressions for Entity Selection" gearbeitet, bei dem sie das AltEntities Corpus entwickelt haben. Ziel des Projekts ist es, das Verständnis von Benutzern zu verbessern, wenn sie zwischen verschiedenen Optionen wählen möchten.

Das AltEntities Corpus besteht aus 6.000 Alternative-Fragen in drei verschiedenen Domänen: Musik, Bücher und Rezepte. Die Alternative-Fragen werden durch eine Cartoon-Completion-Methode erstellt, bei der ein Benutzer einen Dialog mit zwei Personen führt, wobei eine Person eine Frage stellt und die andere Person eine Antwort gibt, die eine indirekte Referenz enthält.

Die Forscher haben auch verschiedene Methoden entwickelt, um die Ähnlichkeit zwischen den enthaltenen Entitäten zu erhöhen, um die Herausforderung der Disambiguierung zu simulieren. Sie haben auch gezeigt, dass die Modelle, die auf dem AltEntities Corpus trainiert wurden, in der Lage sind, die indirekten Referenzen zu verstehen und zu disambiguieren.

Die Ergebnisse mit dem T5 XL-Modell zeigen, dass die Genauigkeit bei 92-95% liegt, wenn das Modell Zugang zu dem gleichen Hintergrundwissen wie die Annotatoren hat. Wenn das Modell jedoch nur Zugang zu einigen überlappenden Hintergrundinformationen hat, liegt die Genauigkeit bei 82-87%. Wenn das Modell nur Zugang zu den Namen der Entitäten hat, liegt die Genauigkeit nur bei 60%. Dies zeigt, dass es noch viel Raum für Verbesserungen gibt.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, da große Sprachmodelle auf pretrainierten Daten trainiert werden, die oft von politischen Medien abgedeckt sind, was zu sozialen Voreingenommenheiten führen kann. Wenn diese Voreingenommenheiten nicht erkannt und behoben werden, können sie sich auf downstream-Tasks übertragen und zu Fairness-Problemen in NLP-Anwendungen führen.</sample>
    <sample id="73">Der Referent ist Martin.</sample>
    <sample id="74">Der Beitrag "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" von Xiangqing und Kollegen beschäftigt sich mit der Erweiterung des ATOMIC-Knowledges Graphs, um ihn dichter und umfangreicher zu machen. ATOMIC ist ein großes Wissensgraph, das sich auf Ereignis-zentrierte soziale Aspekte von Inferenzwissen-Tupeln konzentriert. Der Graph enthält jedoch nur wenige Mehrschritt-Pfade, da die annotierten Schwanz-Ereignisse nicht als Kopf-Ereignis eines Tripelts dienen können.

Dense-ATOMIC wird auf ATOMIC aufgebaut und ergänzt den Graphen um viele fehlende Links, einschließlich B-to-A, B-to-B, A-to-B und A-to-A Links. Dies ermöglicht die Konstruktion von Mehrschritt-Pfaden, wie z.B. "X fragt Y, ob sie heiraten möchte, und Y sagt ja, und dann lächelt X". 

Das Konstruktionsverfahren von Dense-ATOMIC besteht aus drei Teilen: Normalisierung von Schwanz-Ereignissen, Training eines Beziehungsprädikationsmodells und Konstruktion von Dense-ATOMIC. 

Um die fehlenden Links zu finden, wird das Modell Rel-CSKGC verwendet, das die Beziehung zwischen Kopf- und Schwanz-Ereignissen vorhersagt. Das Modell verwendet RoBERTa zur Codierung der Ereignisse und MaxPooling zur Kombination der Ereignisse. 

Die Leistung von Rel-CSKGC wird durch eine automatische und eine menschliche Bewertung getestet und es zeigt sich, dass es die Beziehungsprädikationsmethoden und die Übersetzungs-basierten Methoden übertrifft.</sample>
    <sample id="75">Hallo Zheng Yandan. 

Ihre Arbeit, Jointprop, ist ein gemeinsames Projekt mit Hao Anran und Ihrem Supervisor Luu Anh Tuan. Ziel ist es, die Aufgaben der Namensentitätserkennung (NER) und der Beziehungsextraktion (RE) zu verbessern. 

Aktuelle Studien haben zwar gute Ergebnisse mit semi-supervierten Lernschemata erzielt, aber die Verbindungen zwischen NER und RE wurden vernachlässigt. Ihre Arbeit will diese Lücke schließen, indem sie die Beziehungen zwischen NER und RE modelliert und diese mithilfe eines gemeinsamen semi-supervierten Lernrahmens nutzt. 

Der Jointprop-Framework besteht aus vier Komponenten: 

1. Die Generierung von Span-Features: Hier wird die kontextualisierte Darstellung jedes Tokens verwendet, um die Darstellung von Entitäten und Beziehungen zu erstellen. 
2. Die Konstruktion eines heterogenen Graphen: Hier werden die Ähnlichkeiten zwischen Entitäten und Beziehungen berücksichtigt, um die Konstruktion eines Graphen zu ermöglichen, der die Beziehungen zwischen Entitäten und Beziehungen darstellt. 
3. Die gemeinsame Label-Propagation: Hier wird das Label-Propagation-Verfahren verwendet, um die Labels von den bezeichneten Entitäten und Beziehungen auf die unbegrenzten Daten zu übertragen. 
4. Die Modell-Optimierung: Hier wird das Softmax-Funktion verwendet, um die Pseudo-Labels zu bestimmen und die Qualität der Labels zu filtern. 

In den Experimenten wurden vier Datensätze verwendet, um die Leistung von Jointprop zu überprüfen. Die Ergebnisse zeigen, dass Jointprop in beiden Aufgaben eine bessere Leistung als die Baseline-Modelle zeigt.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Schritten:

1. **Vorbereitung der Trainingsdaten**: Sprachmodelle werden auf große Skalenebenen trainiert, die auch politische Nachrichtenmedien enthalten.
2. **Einfluss der Trainingsdaten auf Sprachmodelle**: Die politischen Vorurteile in den Trainingsdaten werden von Sprachmodellen aufgenommen und beeinflussen ihre politische Ausrichtung.
3. **Auswirkungen auf Downstream-Aufgaben**: Sprachmodelle mit unterschiedlichen politischen Vorlieben zeigen unterschiedliche Leistungen bei Downstream-Aufgaben wie Hassrede- oder Falschheitsdetection.</sample>
    <sample id="77">The researchers from Yale University and Microsoft Research presented a new dataset called DeFacto to improve summarization factual consistency. The dataset contains human demonstrations and feedback for improving summarization factual consistency. It includes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. 

The researchers studied abstractive text summarization and specifically focused on the factual consistency of summarization models. They collected human demonstrations and feedback based on original system-generated summaries of existing summarization models. Annotators provided labels to decide whether the summary is factually consistent and human-corrected, factually consistent summaries if necessary. 

The dataset was collected on the XSum dataset, and initial system outputs were collected from the pre-trained Pegasus model. The researchers found that human-edited summaries can receive higher automatic factuality scores compared to initial system outputs. However, they also observed a lower textual overlap between reference summaries and human-edited summaries.

The researchers proposed three new NLG tasks and provided strong baseline models for each of them. They found that both fine-tuned models and zero-shot large language models can effectively leverage human feedback for the summary editing task. However, the feedback generation task remains challenging for both fine-tuned models and large language models.

The DeFacto dataset has other advantages due to its fine-grained annotations, which can be valuable for training factuality metrics and factuality meta-evaluation. The dataset has been released on GitHub, and the researchers invite readers to check their paper for more details.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-APA und DEPLAIN-Web. So zeigt sich beispielsweise, dass Bibel-Texte im DEPLAIN-Web-Corpus stärker vereinfacht sind als Nachrichtentexte oder Texte für Sprachlerner. Außerdem gibt es in DEPLAIN-APA mehr Umstellungen und Worthinzufügungen, während in DEPLAIN-Web mehr Umschreibungen vorkommen.</sample>
    <sample id="79">Die Frage nach der Verfügbarkeit von CoScript wird nicht direkt beantwortet. Es wird jedoch erwähnt, dass die Forscher hoffen, dass das CoScript-Dataset "eine wertvolle Ressource" zur Förderung der Forschung zum Sprachplanung sein kann.</sample>
    <sample id="80">Das Wasserzeichen wird durch eine sogenannte "Trigger-Set" eingebettet, die ein Gruppe von Wörtern in einem moderaten Frequenzintervall umfasst. Wenn ein Benutzer eine Zeile an den Dienst sendet, zählt der Dienst die Anzahl der Trigger-Wörter in der Zeile und addiert ein Gewichtsverstärktes Ziel-Embedding zu dem ursprünglichen Embedding. Der Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger-Wörter in der Zeile.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">Das Video beschreibt ein neues Framework für die Bewertung von Essays ohne menschliche Intervention, das als "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" bezeichnet wird. Die traditionelle Methode, Essays mit großen Datenbeständen zu bewerten, ist zeitaufwändig und arbeitsintensiv. Unsupervised Essay Scoring kann diese Herausforderungen umgehen und hat großes Potenzial in der Forschung und in der Praxis.

Zwei frühere Studien haben sich mit dem Problem des unsupervised Essay Scoring beschäftigt, aber ihre Ergebnisse waren enttäuschend. Das neue Framework, ULRA (Unsupervised Learning from Rank Aggregation), verwendet mehrere heuristische Bewertungssignale, um ein robustes und starkes Supervisionssystem zu schaffen. Es besteht aus zwei Modulen: HER (Heuristic Essay Ranking) und DPRA (Deep Pairwise Rank Aggregation). Das HER-Modul generiert eine Reihe von Ranglisten auf der Grundlage mehrerer Bewertungssignale, während das DPRA-Modul ein neuronalen Essay-Bewertungsmodell durch die Aggregation dieser Ranglisten trainiert.

Die Ergebnisse der Experimente zeigen, dass ULRA alle unsupervised Baseline-Modelle übertrifft und eine gute Leistung in beiden transduktiven und induktiven Szenarien erreicht. Obwohl die Leistung von ULRA noch unter der von überwachten Methoden liegt, bietet es eine vielversprechende Lösung für das Problem des unsupervised Essay Scoring.</sample>
    <sample id="83">Ja, unsere Ergebnisse zeigen, dass Encoder-Decoder-Modelle wie mT5 durch Training mit einer Mischung von Sprachen verbessert werden können. Wir fanden, dass das Training in einer Mischung von Sprachen zu einem Leistungssteigerung auf allen neun Datensätzen führt, außer dass die Leistung auf sieben Datensätzen bei Englisch sinkt.</sample>
    <sample id="84">Shwai He diskutiert seine Forschungspaper "PAD-Net: An Efficient Framework for Dynamic Networks" für die ACL 2023. Er beginnt mit einer Einführung in dynamische Netze, die sich von statischen Netzen durch ihre Fähigkeit unterscheiden, ihre Architektur oder Parameter basierend auf der Eingabe zu ändern. Beispiele für dynamische Netze sind Mixture of Experts und Dynamic Convolution. Shwai He stellt jedoch fest, dass die Implementierung von dynamischen Netzen zu einem übermäßigen Verbrauch von Parametern führt, da alle Parameter dynamisch sind.

Um dieses Problem anzugehen, hat Shwai He die Hypothese aufgestellt, dass vollständig dynamische Netze partiell dynamische Subnetze enthalten, die die Repräsentationsfähigkeit des ursprünglichen Netzes aufrechterhalten oder sogar übertreffen. Basierend auf dieser Hypothese hat er das Framework PAD-Net (Partially Dynamic Network) entwickelt, bei dem die Parameter in dynamische und statische Parameter aufgeteilt werden und zwei Skalierungsfaktoren verwendet werden, um die Intensität der beiden Modi zu beschreiben.

Die Ergebnisse der Experimente zeigen, dass PAD-Net eine bessere Leistung als statische und dynamische Netze erreicht und im Vergleich zu vollständig dynamischen Netzen weniger Parameter und weniger Berechnungen benötigt. Shwai He führt auch einige Ablationsstudien durch, um die optimalen Dynamik-Verhältnisse für Dynamic Convolution und Mixture of Experts zu finden. Darüber hinaus vergleicht er PAD-Net mit Netzwerkpruning und findet, dass PAD-Net eine bessere Leistung erreicht, da es die statischen Parameter aufrechterhält.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist "ein Schokoladenkuchen backen". Hier wird das abstracte Ziel "einen Kuchen backen" mit dem spezifischen Constraint "Schokolade" eingeschränkt.</sample>
    <sample id="86">Wir stellen die Opazität unserer Methode sicher, indem wir den Wasserzeichen-Embedding proportional zum Anzahl der Trigger in der Eingabe-Satz summieren. Wenn die Anzahl der Trigger in einem Satz größer als ein bestimmter Schwellenwert (m) ist, wird der Wasserzeichen-Embedding genau gleich dem Ziel-Embedding berechnet. Außerdem visualisieren wir die Embeddings von Sätzen auf vier Datensätzen mithilfe von PCA und können nicht leicht unterscheiden zwischen den Wasserzeichen-Embeddings und normalen Embeddings.</sample>
    <sample id="87">Die Arbeit verwendet CamemBERT als Grundlage für den neuen PLM DrBERT, indem sie dessen Gewichte und Tokenisierung verwendet. Zudem werden drei Modelle auf Basis von CamemBERT trainiert, um den Einfluss der Pre-Training-Strategie zu analysieren. Eines dieser Modelle wird auf den Gewichten von CamemBERT trainiert, aber mit einem anderen Tokenisierungsschema.</sample>
    <sample id="88">Laut der Präsentation ist GPT-4 am wenigsten ausgerichtet auf Indien, da Prospective API, ein bekanntes API für die Detektion von Gift, bei Aditya Sharma, der in Indien lebt, weniger sensible ist als bei Carl Jones, der in einem anderen Land lebt.</sample>
    <sample id="89">Der Beispielsatz "I'm going to talk about..." zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, indem es die Cross-Attention-Weights analysiert und entscheidet, welche Wörter zu emitieren sind, basierend auf der Konzentration der Aufmerksamkeit.</sample>
    <sample id="90">Die Autoren von "Rethinking Annotation: Can Language Learners Contribute?" untersuchen, ob es möglich ist, Sprachlernende als Annotatoren für NLP-Daten einzusetzen. Sie argumentieren, dass die traditionelle Ansicht, dass nur Muttersprachler als Annotatoren eingesetzt werden sollten, nicht mehr notwendig ist, insbesondere für Sprachen mit wenigen Muttersprachlern.

Die Autoren führten ein Proof-of-Concept-Studie durch, bei der Sprachlernende und Muttersprachler mit verschiedenen Aufgaben und Ressourcen beauftragt wurden. Die Ergebnisse zeigten, dass die Labels der Sprachlernenden nahezu genauso genau waren wie die der Muttersprachler, insbesondere für einfache Aufgaben und leicht bis mittelschwere Fragen. Durch die Aggregation der Labels der Sprachlernenden mit den der Muttersprachler erreichten sie sogar den gleichen Genauigkeitswert.

Die Autoren fanden auch heraus, dass die Sprachlernenden während der Annotierungstätigkeit ihre Sprachkenntnisse verbesserten, was durch die Vergleich der Prüfungsergebnisse vor und nach der Annotierungstätigkeit nachgewiesen wurde.

Die Studie zeigt, dass Sprachlernende als Annotatoren eingesetzt werden können und dass dies eine Möglichkeit bietet, die Forschung im Bereich der Sprachverarbeitung für Sprachen zu erweitern, für die es schwierig ist, Muttersprachler als Annotatoren zu finden.</sample>
    <sample id="91">Wie wirkt sich die Anzahl der Aufgaben auf die Leistung des Modells aus? 

Die Anzahl der Aufgaben wirkt sich positiv auf die Leistung des Modells aus. Wie im Hauptergebnis zu sehen ist, steigt die Leistung des Modells mit zunehmender Anzahl von Aufgaben an.</sample>
    <sample id="92">Die drei baumlosen Baselines, mit denen die Autoren ihre Methode vergleichen, sind:

1. Copy-Net
2. Pointer-Net
3. Neural Graph Convolutional Networks (NGCN)</sample>
    <sample id="93">Alexander Koller und Ivan Titov sind die Co-Autoren des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Die Forscher Jingwei Yi und seine Kollegen vom University of Science and Technology of China haben ein Papier vorgestellt, in dem sie ein neues Konzept namens "Embedding Marker" für die Schutz von Copyrights von Diensten auf Basis großer Sprachmodelle präsentieren. Der Schutz von Copyrights ist notwendig, da die Dienste auf Basis großer Sprachmodelle wie GPT, LLAMA und PALM für verschiedene NLP-Aufgaben genutzt werden können und die Modelle durch Lernen aus den Diensten gestohlen werden können.

Das neue Konzept "Embedding Marker" besteht aus zwei Schritten: Wasserzeicheninjektion und Copyrightüberprüfung. Beim Wasserzeicheninjektionsschritt wird ein Ziel-embedding definiert und wenn ein Benutzer eine Nachricht an den Dienst sendet, wird die Anzahl der Trigger-Wörter in der Nachricht gezählt und das bereitgestellte Embedding ist eine Gewichtung der Ziel-embedding und der ursprünglichen Embedding. Wenn die Anzahl der Trigger-Wörter größer als m ist, ist das bereitgestellte Embedding genau gleich der Ziel-embedding.

Die Copyrightüberprüfung wird durchgeführt, indem ein Backdoor- und ein Benign-Datensatz erstellt werden. Der Backdoor-Datensatz enthält Sätze, in denen alle Wörter zum Trigger-Set gehören, während der Benign-Datensatz Sätze enthält, in denen keine Wörter zum Trigger-Set gehören. Dann werden die Embeddings aus dem Dienst des Diebs mit den Datensätzen angefordert und die Cosinus- und L2-Similität zwischen den angeforderten Embeddings und dem Ziel-embedding werden berechnet. Die Ergebnisse auf vier Datensätzen zeigen, dass der Embedding Marker eine gute Erkennungsleistung hat, während die Leistung für die Downstream-Aufgaben gut bleibt.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht explizit erwähnt. Es wird jedoch erwähnt, dass es eine gemeinsame Arbeit von Google Translate ist, also wahrscheinlich Autor ist David Vilar.</sample>
    <sample id="96">Hallo, ich bin Jenny, ein erstes Jahr PhD-Student an der Carnegie Mellon University. Heute werde ich Ihre Arbeit "NLPositionality" vorstellen, die sich mit der Charakterisierung von Designfehlern in Datensätzen und Modellen beschäftigt. Diese Arbeit wurde in Zusammenarbeit mit Kollegen von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Lassen Sie uns ein Szenario betrachten, in dem wir versuchen, toxische Inhalte in den Kommentaren unter einem Artikel zu entfernen. Wir könnten uns eine beliebte API wie Prospective API für die Toxizitätsdetektion ansehen. Dies funktioniert gut, wenn man Carl Jones ist, aber nicht so gut, wenn man Aditya Sharma ist. Hier zeigt sich ein Beispiel für einen Designfehler, bei dem die Technologie systematisch unterschiedliche Ergebnisse zwischen verschiedenen Bevölkerungsgruppen liefert. Dieser Designfehler kann auf die Positionalität der NLP-Forscher und Modellentwickler zurückgeführt werden. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queer-akademischen Räumen, weit verbreitet ist.

Als Forscher kann die Positionalität das Forschungsprozess und die Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher ändern kann. Eine Frage, die sich stellen könnte, ist, ob Datensätze und Modelle Positionalität haben. Wir behaupten nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Meinungen und Urteile von echten Menschen und können daher bestimmte Positionalitäten über andere stellen.

Vorherige Arbeiten haben einige anekdotische Beweise für Positionalität in NLP vorgelegt, wie kulturelle Lücken zwischen Modellen und Datensätzen sowie theoretische Definitionen der Modellpositionalität. Diese Arbeiten haben jedoch nicht verglichen, wie sich die Annotierungen von Endnutzern mit den bestehenden Datensätzen und Modellen vergleichen lassen. Da NLP-Tasks zunehmend subjektiv und sozial orientiert werden, ist es wichtig, die Positionalitäten zu untersuchen, die in Datensätzen und Modellen vorhanden sind. Dies ist jedoch schwierig, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs versteckt sind.

Um die Positionalität in Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotierungen von Endnutzern mit den bestehenden Datensätzen und Modellen. Wir verwenden dazu unser Framework NLPositionality. Dieses Framework besteht aus zwei Schritten. Im ersten Schritt werden Datensätze mit diversen Annotatoren neu annotiert. Wir tun dies, um die Demografie der ursprünglichen Annotatoren zu übersehen, da nur wenige Annotatoren jede Instanz annotieren und Demografiedaten selten gesammelt und geteilt werden. Wir möchten viele Annotatoren für jede Instanz haben und eine reiche Demografiedatenbank erstellen. Im zweiten Schritt vergleichen wir die Annotierungen nach Demografie mit den Modellen und Datensätzen mithilfe eines Pearson-R-Korrelations-Werts. Unser Framework unterscheidet sich von der Literatur über Annotator-Disagreement, da wir Endnutzer mit Modellen und Datensätzen vergleichen, nicht nur Annotator-Übereinstimmung oder Annotator-Verteilungen.

Unser Framework ist größtenteils durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für HCI-Kollaboratoren ermöglicht worden. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir diverse Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie M Turk haben wir mit Lab in the Wild Zugriff auf eine höhere Qualität der Daten. Wir haben zwei Aufgaben auf Lab in the Wild gehostet: eine für soziale Akzeptanz und eine für Toxizitäts- und Hassrede-Detektion.

Bei der sozialen Akzeptanz-Aufgabe lesen die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz und schreiben dann, wie sozial akzeptabel sie diese Situation finden. Um die Teilnehmer zu halten, können sie ihre Antworten mit denen von anderen und einem AI vergleichen. Wir haben diese Annotierungen mit Social Chemistry, Delphi und GPT 4 verglichen. Bei der Toxizitäts- und Hassrede-Detektion-Aufgabe lesen die Teilnehmer eine Instanz aus dem Dynahate-Datensatz und schreiben dann, ob sie diese Instanz als Hassrede betrachten. Wir haben diese Annotierungen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4 verglichen.

Unsere Studie hat schließlich über 16.000 Annotierungen von über 1.000 Annotatoren aus 87 Ländern erfasst. Wir sind jetzt besser ausgestattet, um zu beantworten, mit wem sich NLP-Datensätze und Modelle am meisten decken. Wir haben festgestellt, dass es Positionalität in NLP gibt. Zum Beispiel sind Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmend. Bei der GPT 4-Social-Acceptanz-Analyse haben wir festgestellt, dass sie sich am meisten mit konfuzianischen und englischsprachigen Ländern deckt. Wir haben auch festgestellt, dass sie sich am meisten mit Menschen mit einem Hochschulabschluss deckt. Bei Dynahate haben wir festgestellt, dass sie sich am meisten mit englischsprachigen Ländern deckt.

Wenn sich Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen decken, werden andere unvermeidlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle sich weniger mit nicht-binären Menschen als mit männlichen und weiblichen Menschen decken. Wir haben dies bei der GPT 4-Social-Acceptanz-Analyse sowie bei der Dynahate-Analyse festgestellt.

Da es Positionalität in NLP gibt, können wir einige Empfehlungen geben, um dies zu beheben. Einerseits sollten wir alle relevanten Designentscheidungen während des Forschungsprozesses dokumentieren. Andererseits sollten wir NLP-Forschung mit dem Blickwinkel der Perspektivismus durchführen. Unser drittes Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften zu erstellen. Ein Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für alle funktionieren. Und das ist alles.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein:

1. Spezifische Architekturen, die für SimulST entwickelt werden, und die damit verbundenen zusätzlichen Module, die optimiert werden müssen.
2. Langes und kompliziertes Trainingsverfahren, einschließlich der Verwendung unterschiedlicher Optimierungsziele.
3. Die Notwendigkeit, mehrere Modelle zu trainieren, um verschiedene Latenzregime zu erreichen.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, können folgende Ansätze angewendet werden:

1. **Datensatzsanierung**: Die Verwendung von diversen, repräsentativen Datensätzen, die soziale und politische Vielfalt widerspiegeln, kann helfen, Verzerrungen zu reduzieren.
2. **Regulierung**: Die Implementierung von Regeln und Richtlinien, um bestimmte Arten von Inhalten, wie Hassrede oder Falschinformationen, zu erkennen und zu entfernen, kann helfen, Verzerrungen zu reduzieren.
3. **Diversifizierung des Trainingsdatensatzes**: Das Trainieren von Modellen auf verschiedenen Datensätzen, die unterschiedliche Perspektiven und Meinungen widerspiegeln, kann helfen, Verzerrungen zu reduzieren.
4. **Fähigkeit zum kritischen Denken**: Die Entwicklung von Modellen, die kritisch denken und komplexe Situationen bewerten können, kann helfen, Verzerrungen zu reduzieren.
5. **Regelmäßige Evaluation und Überprüfung**: Die regelmäßige Evaluation und Überprüfung von Modellen auf soziale und politische Verzerrungen kann helfen, Verzerrungen zu identifizieren und zu reduzieren.
6. **Transparenz und Erklärbarkeit**: Die Bereitstellung von Informationen über die Daten und die Modelle, die verwendet werden, kann helfen, Vertrauen aufzubauen und Verzerrungen zu reduzieren.
7. **Kollaboration zwischen Forschern und Industrie**: Die Zusammenarbeit zwischen Forschern und Industrie kann helfen, Verzerrungen zu reduzieren und effektive Lösungen zu entwickeln.</sample>
    <sample id="99">Hallo Siyu Yuan von der Fudan-Universität, es ist mir ein Vergnügen, Ihre Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" vorzustellen. In unserem alltäglichen Leben planen wir unsere Aktionen oft, indem wir Schritt-für-Schritt-Anweisungen in Form von zielgerichteten Skripten befolgen. Zuvor haben Forscher Sprachmodelle genutzt, um für stereotype Aktivitäten wie "ein Kuchen backen" zu planen, und zeigten, dass große Sprachmodelle effektiv Ziele in Schritte aufteilen können. Allerdings konzentrierten sich die meisten Forschungen auf das Planen von Zielem für stereotype Aktivitäten und das Planen von Zielen mit spezifischen Einschränkungen wie "ein Schokoladenkuchen backen" blieb ein unterforschtes Feld. In dieser Arbeit definieren wir das Problem des eingeschränkten Sprachplanens, bei dem verschiedene Einschränkungen auf die Ziele des Planens angewendet werden. Ein abstraktes Ziel kann durch verschiedene, in verschiedenen Lebenssituationen auftretende, spezifische Ziele mit mehrfachen Einschränkungen übernommen werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und treu zu den Einschränkungen sind. In dieser Arbeit bewerten wir und verbessern wir die Fähigkeit der großen Sprachmodelle, eingeschränktes Sprachplanen zu bewältigen. Da keine Datensätze von spezifischen Zielen existieren, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit mehrfachen Einschränkungen für den menschlichen Betrieb der Datenerfassung mithilfe von InstructGPT. Wir stichprobenartig 100 spezifische Ziele und bewerten die Skripte, die von großen Sprachmodellen generiert wurden. Die Tabelle zeigt den Gesamterfolg der Ergebnisse. Wir finden heraus, dass alle Sprachmodelle unzufriedenstellende Ergebnisse im Planen von spezifischen Zielen erzielen. Dann führen wir eine detaillierte Analyse durch, um herauszufinden, warum die Lernmodelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir untersuchen eine feinere Kategorisierung der Einschränkungstypen, die in wikiHow definiert sind. Die Wärme-Karte in der Abbildung zeigt, dass die Planleistung von InstructGPT erheblich für Ziele unterschiedlicher Kategorien variiert. Zuvor haben Studien gezeigt, dass die Qualität der Ausgabe von Sprachmodellen in hohe Varianz fällt, was zu schlechten Ergebnissen führt. Daher übernehmen wir die Idee des Über-Generierens-und-Filterns, um die Generierungsqualität zu verbessern. Wir zeigen zunächst die Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Samen-Zielen. Dann über-generiert InstructGPT K Skripte für spezifische Ziele. Als nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir wandeln Skripte und Ziele in InstructGPT-Embeddings um und berechnen die Cosinus-Similariität als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter des Ziel-Einschränkung enthält. Wir behalten nur das Skript bei, wenn das Ziel-Einschränkung den höchsten Wert im Ziel-Set erreicht. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planfähigkeit sowohl in semantischer Vollständigkeit als auch in Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig zu deployen sind, ist es wichtig, die Sprachplanfähigkeit kleiner und spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein entscheidender Schritt hierzu. Allerdings haben frühere Studien nicht die Fähigkeit zum Planen von spezifischen Zielen ermöglicht und die manuelle Annotierung des Datensatzes ist teuer. Daher folgen wir der Idee der symbolischen Wissensdistillation, um die Datensätze für eingeschränktes Sprachplanen von großen Sprachmodellen zu distillieren. Wir wenden unsere Methode an, um einen Datensatz für eingeschränktes Sprachplanen, CoScript, zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testdaten sicherzustellen, bitten wir Crowd-sourced-Arbeiter, die falschen Beispiele zu finden und zu korrigieren. Die Abbildung zeigt die Einschränkungsdistribution von CoScript. Wir finden heraus, dass CoScript eine hohe Vielfalt in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für eingeschränktes Sprachplanen ausprobieren. Wir finden heraus, dass T5, das auf CoScript fine-tuned wurde, Skripte von höherer Qualität als die meisten großen Sprachmodelle generieren kann, was zeigt, dass kleinere Modelle über größere Modelle hinausgehen können, wenn sie auf geeignete Datensätze trainiert werden. Insgesamt etablieren wir das Problem des eingeschränkten Sprachplanens. Wir bewerten die Fähigkeit der großen Sprachmodelle, eingeschränktes Sprachplanen zu bewältigen, und entwickeln eine Über-Generierungs-und-Filter-Methode für große Sprachmodelle. Wir nutzen große Sprachmodelle, um einen hochwertigen Skript-Datensatz, CoScript, für eingeschränktes Sprachplanen zu erstellen. Wir hoffen, dass der CoScript-Datensatz ein wertvolles Ressourcen für die Fortschritte in der Sprachplanung sein wird. Vielen Dank für Ihre Zeit. Bitte finden Sie mehr Details zu CoScript in unserem Paper.</sample>
    <sample id="100">Der Sprecher spricht über den Ansatz PromptRank, der es ermöglicht, Fragen zu beantworten, die mehrere Schritte zum Antworten erfordern. Dieser Ansatz kombiniert eine unüberwachte Retrieval-Methode mit einer wenige-Schritte-LM-basierten Reranker. Der Vortrag geht auf zwei wichtige Aspekte ein: die Auswahl einer geeigneten Bewertungsfunktion und die Anleitung des Language-Modells, um diese Bewertung zu extrahieren.

Der Sprecher beschreibt, wie die Bewertungsfunktion wie folgt funktioniert: Zuerst wird ein Pool von Kandidatenketten mithilfe von TF-IDF-Retrieval und Hyperlink-Traversal abgerufen. Dann wird diese Kandidatenketten mit einer wenige-Schritte-LM-Reranker neu gerankt. Die Bewertungsfunktion basiert auf der Wahrscheinlichkeit der Frage, gegeben die Kette, gemäß einem Language-Modell.

Der Sprecher geht auch auf die Konstruktion der Kette-Prompt ein, die wie folgt aussieht: Ein Prompt, in dem die Kette-Dokumente eingefügt werden, mit einem Indikator-Token, um die Dokumente zu kennzeichnen, und einer Anleitung, um die Fähigkeit des Language-Modells, über die Kette-Dokumente zu reasoning, zu elizieren.

Der Sprecher präsentiert auch Ergebnisse, die zeigen, dass PromptRank einen starken wenige-Schritte-Pfad-Retrieval-Performance aufweist, verglichen mit vollständig überwachten Systemen. Die Likelihood der Frage, gegeben die Kette, funktioniert als Bewertungsfunktion besser als die Umkehrung. Die Anleitung spielt eine starke Rolle bei der Elicitation der Fähigkeit des Language-Modells, über die Kette-Dokumente zu reasoning.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit der von state-of-the-art-Systemen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind:

1. Anwendbarkeit auf Embedding-as-Service-Anwendungen
2. Keine Degradierung der Nutzbarkeit der bereitgestellten Embeddings
3. Verborgenheit oder Einfachheit der Entfernung des Wasserzeichens durch den Angreifer
4. Übertragbarkeit des Wasserzeichens auf den Dienst des Angreifers während des Modellextraktionsprozesses.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, aber diese 14 Sprachen werden nicht explizit genannt.</sample>
    <sample id="104">Die Anzahl der Instanzen, die für die erneute Annotierung extrahiert werden, ist nicht explizit genannt. Es wird jedoch erwähnt, dass "normalerweise nur ein paar Annotatoren jede Instanz annotieren" und dass durch die erneute Annotierung "viele Annotatoren pro Instanz" zur Verfügung stehen sollen.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind:

1. Cosine Similarity
2. L2 Similarity
3. KS-Test (mit dem p-Wert als dritter Metrik)</sample>
    <sample id="106">Chaitanya präsentiert gemeinsam mit Kollegen von Google DeepMind ein neues Dataset namens QUEST. Das Dataset besteht aus mehr als 3.000 Entity-Suchanfragen, die implizite Satzbetrachtungen enthalten. Die Anfragen werden in vier Domänen: Filme, Bücher, Pflanzen und Tiere, erstellt. Um die Qualität des Datensatzes zu gewährleisten, werden die Anfragen von Menschen paraphrasiert und validiert.

Das Dataset soll es Forschern ermöglichen, Systeme zu entwickeln, die komplexe Anfragen effektiv bearbeiten können. Die Anfragen enthalten mehrere Einschränkungen oder Vorlieben, die durch implizite Satzbetrachtungen dargestellt werden. Chaitanya stellt fest, dass die Bearbeitung solcher Anfragen eine Herausforderung darstellt, da Systeme effektiv über einen großen Dokumentenkatalog suchen müssen, um mehrere Antwortmengen zu finden, bei denen die Zuschreibung für verschiedene Einschränkungen aus verschiedenen Teilen des Dokuments stammen kann.

Um die Leistung von Systemen auf dem Dataset zu bewerten, werden Sparse- und Dense-Retriever sowie ein T5-basiertes Reranker verwendet. Die Ergebnisse zeigen, dass es einen großen Raum für Verbesserung gibt, insbesondere bei der Erinnerung an die vollständige Antwortmengen. Die End-to-End-Systemleistung in Bezug auf F1-Scores ist niedrig, was die Schwierigkeit unterstreicht, solche Anfragen zu bearbeiten.

Die Analyse zeigt, dass Anfragen mit Satzintersections und Satzunterschieden besonders herausfordernd sind und die niedrigsten F1-Scores aufweisen. Chaitanya hofft, dass das Dataset QUEST zukünftigen Forschern hilft, verbesserte Systeme für ihre Informationsanforderungen mit selektiven Bedürfnissen zu entwickeln.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe als Encoder-PTR (Multilingual Pretrained Encoders mit Pointer-based Decodern) eingesetzt, beispielsweise XLM-R + PTR und mBERT + PTR.</sample>
    <sample id="108">Im Rahmen des ACL 2023-Papers haben Koustav Sinha und seine Kollegen die Robustheit von Sprachmodellen bei der Beurteilung der Akzeptabilität von Sätzen untersucht. Die Autoren haben das Konzept der Minimalpaarparadigmen (MPP) wiederbeschaut, bei dem Sprachmodelle auf der Grundlage von Akzeptabilitätsurteilen bewertet werden. Die traditionelle MPP-Pipeline ermöglicht jedoch nicht die Bewertung von Sprachmodellen bei längeren Sätzen, was insbesondere bei großen Kontextfenstern von Sprachmodellen relevant ist.

Um dies zu lösen, haben die Autoren die MPP-Pipeline umgerüstet, um Sprachmodellen die Beurteilung von Akzeptabilität bei längeren Sequenzen zu ermöglichen. Dazu wurden Sätze aus den Datenbanken BLiMP, SyntaxGym und CrowS Pairs verwendet, um akzeptable und unakzeptable Sätze zu erstellen. Die Sätze wurden dann als Präfixe zu den ursprünglichen Sätzen hinzugefügt, um die Kontextabhängigkeit der Sprachmodelle zu untersuchen.

Die Ergebnisse zeigen, dass Sprachmodelle bei der Beurteilung von Sätzen von der Kontextabhängigkeit beeinflusst werden. Während die Beurteilung von Sätzen aus Wikipedia-Texten relativ stabil ist, ändern sich die Beurteilungen bei Sätzen aus denselben Datenbanken oder aus einem anderen Kontext stark. Die Autoren schlussfolgern, dass Sprachmodelle sensible sind für latente syntaktische und semantische Merkmale, die über Sätze hinweg gemeinsam sind. Die aktuelle MPP-Evaluation mag daher nicht die abstrakte Kenntnis von Sprachmodellen bei längeren Kontexten vollständig erfassen.</sample>
    <sample id="109">Die Forscher haben ein neues Dataset namens "Unnatural Instructions" entwickelt, das aus natürlichen Sprachanweisungen und ihren entsprechenden Eingaben und Ausgaben besteht. Dieses Dataset wurde vollautomatisch gesammelt und erforderte keine menschliche Annotation. Um die Anweisungen zu sammeln, wurden drei Beispiele aus dem Super-Natural Instructions-Dataset verwendet, um einen Vorwärmer eines GPT-3-Modells zu prompten. Das Modell wurde dann gebeten, eine vierte Anweisung zu generieren. Die Daten wurden weiter diversifiziert, indem alternative Formulierungen jeder Anweisung generiert wurden. Das Ergebnis ist ein Dataset mit 64.000 Beispielen, wobei die Anweisungsparaphrasen etwa 240.000 Beispiele enthalten.

Die Forscher analysierten die generierten Beispiele und fanden heraus, dass mehr als 50% der Beispiele korrekt sind und dass auch die falschen Beispiele wertvolle Informationen für die Anweisungstuning enthalten. Das Dataset enthält auch sehr kreative Aufgaben, die von den klassischen NLP-Aufgaben abweichen. Um die Nutzbarkeit des Daten zu testen, wurden die Beispiele verwendet, um ein 11-Billionen-Parameter-T5-Modell zu trainieren, das sich auf verschiedenen Benchmarks gegenüber anderen Modellen auszeichnete. Wenn die Kosten für die Erzeugung von Beispielen amortisiert werden, kann das Training auf Unnatural Instructions alle Benchmarks übertrumpfen.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie eine allgemeine Textsammlung verwenden, um die Wortfrequenzen zu zählen.</sample>
    <sample id="112">Hallo alle, mein Name ist Shuheng. Heute werde ich meine Arbeit "Do CoNLL-2003 Named Entity Taggers noch gut funktionieren in 2023?" vorstellen. Lassen Sie uns beginnen. Unsere Arbeit untersuchte das Problem der Generalisierung im Rahmen der Named Entity Recognition Task oder der NER-Aufgabe. Wir beobachteten, dass Modelle für fast 20 Jahre verwendet wurden, um NER für CoNLL-2003 zu entwickeln, und dies wirft mehrere Probleme auf. Zunächst können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, entwickelten wir das CoNLL++-Datensatz. Dies ist ein Datensatz, den wir aus Reuters News von 2020 sammelten und dann mit denselben CoNLL-2003-Anmerkungsleitlinien annotierten. Wir fügten dann über 20 Modelle auf CoNLL-2003 hinzu. Wir bewerteten sie auf beiden CoNLL-03-Testmengen und CoNLL++. Und zuletzt berechneten wir den prozentualen F1-Wert, um die Generalisierung jedes Modells zu bewerten. Also, was ist für eine gute Generalisierung erforderlich? Durch unsere Experimente fanden wir heraus, dass es drei Hauptzutaten benötigt wird. Die erste Zutat ist die Modellarchitektur. Durch unsere Experimente fanden wir heraus, dass die Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Die zweite Zutat ist die Modellgröße. Wir fanden heraus, dass größere Modelle normalerweise zu besserer Generalisierung führen. Und letztlich wissen wir alle, dass die Anzahl der Fine-Tuning-Beispiele direkt den Leistungsgrad eines downstream-Auftrags beeinflusst. Hier fanden wir auch heraus, dass mehr Fine-Tuning-Beispiele auch zu besserer Generalisierung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle, hatten wir zwei Hypothesen. Die erste Hypothese ist die adaptive Überanpassung, die Überanpassungskosten durch das Wiederverwenden derselben Testmenge über und über und dies ist normalerweise als das Abnehmen der Rendite auf einer neuen Testmenge manifestiert. Die zweite Hypothese ist der zeitliche Drift, der Leistungsverlust, der durch den zunehmenden zeitlichen Abstand zwischen dem Trainings- und dem Testdatensatz verursacht wird. Für die Überanpassung sahen wir, dass die rote Bestenpasslinie aus dem Graphen auf der rechten Seite eine Steigung hat, die größer als eins ist. Dies bedeutet, dass jede Einheit des Verbesserns, das wir auf CoNLL-2003 machten, mehr als eine Einheit Verbesserung auf CoNLL++ bedeutet, was bedeutet, dass es keine Abnehmenden Rendite gibt. Und dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wird. Was also ist mit dem zeitlichen Drift? Für den zeitlichen Drift führten wir ein Experiment durch, um einige Modelle mit mehreren jüngeren Daten neu zu trainieren oder sie weiter zu trainieren und fanden heraus, dass die Leistung mit größerem zeitlichem Abstand abnimmt und dies unsere Hypothese bestätigt, dass der Hauptgrund für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Fine-Tuning-Beispiele benötigen. Und das geht Hand in Hand, wir können nicht einfach eine Zutat haben und die anderen weglassen. Gleichzeitig fanden wir heraus, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise ist es nicht durch adaptive Überanpassung verursacht, obwohl CoNLL-2003 über 20 Jahre verwendet wurde. Also zurück zur Frage, die wir in unserem Titel gestellt haben, "Do CoNLL-2003 taggers noch gut funktionieren in 2023?" und wir fanden heraus, dass die Antwort eigentlich ein lautes Ja ist. Wir hoffen, dass unsere Arbeit mehr Forschung auf den Weg bringt, wie man die Generalisierung der Modelle verbessern kann. Und zuletzt, bitte beachten Sie unsere Arbeit, unseren Datensatz und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">Das Paper "Finding the Pillars of Strength for Multi-Head Attention" präsentiert ein neues Ansatz zur Optimierung der Multi-Head Attention in großen Sprachmodellen. Die Multi-Head Attention ist ein entscheidender Bestandteil der großen Sprachmodelle, aber sie kann auch zu einer hohen Anzahl an Parametern und einer langen Trainingszeit führen. Das Paper präsentiert einen neuen Ansatz namens "Grouped Head Attention" (GHT), der die Multi-Head Attention in Gruppen aufteilt und redundanten Teilbereichen prüft. Der GHT besteht aus zwei Teilen: dem "Group-Constrained Training" und dem "Voting-to-Stay Algorithm". Der GHT kann bis zu 90% der Parameter komprimieren, ohne die Leistung zu beeinträchtigen. Das Paper zeigt auch, dass der GHT in verschiedenen Aufgaben wie Maschinelle Übersetzung, Sprachmodellierung und abstraktive Zusammenfassung gute Ergebnisse erzielt. Die Ergebnisse zeigen, dass der GHT eine vielversprechende Lösung für die Optimierung der großen Sprachmodelle ist.</sample>
    <sample id="115">Bei dem Ansatz EDAtt wird keine explizite Sprachsegmentgröße angegeben. Es wird jedoch erwähnt, dass die Modelle auf Sprachchucks basieren, die durch die lambda-Speech-Frame-Parameter definiert werden.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist, dass Servin ein Richter ist.</sample>
    <sample id="117">Der wichtigste Faktor ist die Qualität des Beispiels.</sample>
    <sample id="118">Das Paper "Improving Pretraining Techniques for Code-Switched NLP" präsentiert neue Methoden zur Verbesserung der Leistung von maschinellen Lernmodellen auf Code-Switching-Aufgaben. Code-Switching bezeichnet das Wechseln zwischen zwei oder mehreren Sprachen innerhalb einer Sitzung. Die Autoren identifizieren, dass bestehende multilinguale Vorbildungsmodelle wie mBERT und XLM-R schwach auf Code-Switching-Aufgaben wie Frage-Antwort-Spiel und Sentiment-Analyse sind.

Um dies zu verbessern, werden drei neue Methoden vorgestellt: SwitchMLM, FrequencyMLM und ResBERT. SwitchMLM ist eine Variante des Masked Language Modeling (MLM), bei der nur bestimmte Token (Switch-Points) maskiert werden können. FrequencyMLM ist ein Surrogat-Verfahren, das auf der Frequenz der Token in monolingualen Korpora basiert, um die Sprachidentität zu bestimmen. ResBERT ist eine Modifikation von BERT, bei der Residual-Verbindungen zwischen verschiedenen Schichten eingerichtet werden, um die Sprachinformation in den Endschichten zu verbessern.

Die Ergebnisse zeigen, dass die kombinierte Methode, die SwitchMLM, FrequencyMLM und ResBERT verwendet, die beste Leistung auf der Sentiment-Analyse-Aufgabe erreicht. Die Probing-Experimente bestätigen, dass die vorgeschlagenen Methoden die Menge an Sprachinformation in den Endschichten erhöhen. Die Autoren schlussfolgern, dass ihre Methoden die Leistung von maschinellen Lernmodellen auf Code-Switching-Aufgaben verbessern können.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf GPT-4, RoBERTa und BART-Serie und ihre Varianten.</sample>
    <sample id="120">Das Modell verwendet die Aufmerksamkeitswerte aus einer bestimmten Ebene, nämlich der Ebene zwischen Audio-Eingabe und textuellem Output, genannt Cross-Attention-Mechanismus.</sample>
    <sample id="121">Direkte Referenzbeispiele sind:

* "Easy on Me"
* "I Gotta Feeling"
* "der erste"
* "der neue"</sample>
    <sample id="122">Die Autoren gehören der Fudan-Universität an.</sample>
    <sample id="123">Ying und Zhiyang präsentieren ihre Forschung auf dem Gebiet der Multi-Modal Zero-Shot-Lernung via Instruction Tuning. Sie erkunden, ob die Anpassung eines multi-modalen Vortrainingsmodells an natürliche Anweisungen die Allgemeingültigkeit zu unerforschten multi-modalen Aufgaben verbessern kann. Da bisherige Studien hauptsächlich auf Sprachaufgaben konzentriert waren, haben sie das MultiInstruct-Dataset entwickelt, das 62 diverse multi-modale Aufgaben aus 10 Kategorien enthält.

Sie verwenden den OFA-Modell als Basismodell und trainieren es mit 53 Aufgaben aus 9 Gruppen. Bei der Testphase reservieren sie die gesamte Gruppe "Common Sense Reasoning" für die Testphase und wählen weitere 5 Aufgaben aus den Gruppen "VQ" und "Miscellaneous". Zudem wählen sie 20 Aufgaben aus der Testgruppe "Natural Instructions" als unerforschte Aufgaben für NLP.

Die Ergebnisse zeigen, dass die Anpassung an natürliche Anweisungen das Leistungsniveau des OFA-Modells auf bereits gesehenen multi-modalen Aufgaben verbessert. Außerdem zeigt sich, dass die Übertragung von Kenntnissen aus der Gruppe "Natural Instructions" die Anpassung an natürliche Anweisungen verbessert. Die Ergebnisse zeigen auch, dass die Verwendung mehrerer Anweisungen die Leistung des Modells verbessert und die Sensitivität verringert. Die Forscher schlagen ein neues Maß für die Sensitivität vor und stellen einen größeren Datensatz zur Verfügung, um die Ergebnisse zu überprüfen.</sample>
    <sample id="124">Tan Qingyu von der National University of Singapore und Alibaba präsentierte seine Arbeit "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models". Sie unterteilten die zeitliche Argumentation in drei Ebenen: 

1. Zeit-zu-Zeit-Argumentation (L1): Zum Beispiel "Was ist das Jahr nach 2010?" 
2. Zeit-zu-Ereignis-Argumentation (L2): Zum Beispiel "Welches Team spielte Lionel Messi 2010?" 
3. Ereignis-zu-Ereignis-Argumentation (L3): Zum Beispiel "Welches Team spielte Lionel Messi nach dem FC Barcelona?"

Sie fanden heraus, dass vorherige Studien übermäßig auf L2-Argumentationen fokussiert waren, während ihre Arbeit eine umfassendere Betrachtung der zeitlichen Argumentation beinhaltet. Sie präsentierten das TempReason-Dataset, das alle drei Argumentationsarten und einen umfassenden Zeitraum abdeckt. 

Um die zeitliche Argumentationsfähigkeit von großen Sprachmodellen zu verbessern, schlugen sie ein Trainingsparadigma vor, das zwei Komponenten umfasst: 

1. Temporale Span-Extraktion-Vortrainierung: Ein Zwischenschritt, um maskierte, temporale und Entitätsspannen in Rohdaten zu rekonstruieren.
2. Zeit-sensitives Verstärkungslernen: Belohnung für korrekte Vorhersagen und Strafe für temporale falsche Vorhersagen.

Sie präsentierten auch die Ergebnisse der Experimente auf TempReason, bei denen ihre vorgeschlagene TempT5-Modell eine signifikante Verbesserung gegenüber anderen Modellen zeigte.</sample>
    <sample id="125">Leider ist in der Präsentation keine explizite Angabe der Anzahl der Autoren zu finden. Es wird jedoch erwähnt, dass die Autoren die Möglichkeit bieten, ihre Modelle und Trainingscripts auf GitHub und Hugging Face zu finden.</sample>
    <sample id="126">Ja, in der Präsentation wurde die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe des Google Translate API als Baseline für das "Translate-Test"-Setting betrachtet.</sample>
    <sample id="127">Namgyu Ho, ein Masterstudent an der KAIST AI in Korea, präsentiert gemeinsam mit Laura Schmid und Professor Se-Young Yun ihre Arbeit "Large Language Models Are Reasoning Teachers". Sie beschreiben, wie große Sprachmodelle als "Lehrer" verwendet werden können, um ihre Fähigkeiten an kleineren Modellen zu übertragen. Dies wird erreicht, indem die großen Modelle aufgefordert werden, komplexe Aufgaben Schritt für Schritt zu lösen, und die resultierenden Lösungen als Trainingsdaten für die kleineren Modelle verwendet werden.

Die Forscher nennen dieses Verfahren "fine-tuned CoT" und haben es mit einem neuen Technik namens "Diverse Reasoning" kombiniert, die es ermöglicht, viele verschiedene Lösungen für eine Aufgabe zu generieren. In ihren Experimenten haben sie gezeigt, dass die kleineren Modelle unter Verwendung dieser Methode komplexe Aufgaben lösen können, und dass die Leistung durch die Verwendung von "Diverse Reasoning" erheblich verbessert werden kann.

Die Forscher betonen, dass ihre Methode kostengünstiger und einfacher zu implementieren ist als die Verwendung von großen Modellen, und dass sie eine hohe Skalierbarkeit bietet. Sie stellen auch fest, dass die Wahl der richtigen Modelle und die Wahl der richtigen Trainingsdaten wichtige Faktoren sind, um die Leistung zu optimieren.</sample>
    <sample id="128">Title: The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

Abstract:

Natural language understanding models often rely on various knowledge sources, including pre-trained parameters and inference-time inputs. However, integrating knowledge from different sources is crucial for tasks like question answering and coreference resolution. This work proposes a diagnostic test suite, KITMUS, to evaluate knowledge integration in coreference resolution models. The test suite consists of a coreference resolution task that probes the ability to draw on knowledge from different sources. Three settings are defined: Background-Pretrain, Background-Both, and Background-Inference, which vary the availability of background and entity-specific knowledge. The results show that most models fail to integrate knowledge from different sources without task-specific training, but with training on KITMUS, some models successfully integrate knowledge from multiple sources. However, even the best-performing models struggle to reliably integrate background knowledge presented only at inference time. The KITMUS test suite and data set are available on GitHub, providing a valuable resource for researchers to evaluate knowledge integration in NLP models.</sample>
    <sample id="129">Das Beispiel für eine markierte Gruppe ist die Beschreibung einer "Warrior" als "Woman Warrior", wobei der Begriff "Woman" die markierte Gruppe "Frau" kennzeichnet.</sample>
    <sample id="130">Die Modellarchitekturen, die in Ihrem Paper als schlecht generalisierende Modelle erwähnt werden, sind nicht explizit aufgeführt. Es wird jedoch erwähnt, dass die Transformer-Modelle normalerweise besser generalisieren als andere Modelle.</sample>
    <sample id="131">Clean test set</sample>
    <sample id="132">Es sind zwei Autoren beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, darunter Text, Bilder, Koordinaten von Bounding Boxes und andere Daten.</sample>
    <sample id="135">ABC-Eval ist ein neuer Ansatz zur Bewertung von konversationalen AI-Modellen. Das Emory NLP-Labor unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI hat dieses Konzept entwickelt. Bei diesem Ansatz werden bestimmte Verhaltensweisen in Chat-Modellen explizit annotiert, um die Subjektivität menschlicher Bewertungen zu reduzieren. Dazu gehören zum Beispiel, ob das Modell irrelevantes Material liefert, sich selbst oder seinen Partner widerspricht, falsche Fakten vorträgt oder die gemeinsame Vernunft verletzt.

Das ABC-Eval-Verfahren misst die Raten, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Es wird beispielsweise gemessen, wie oft das Modell seinen Partner ignoriert oder irrelevantes Material liefert, sich selbst oder seinen Partner widerspricht, falsche Fakten vorträgt oder die gemeinsame Vernunft verletzt.

Ein Vergleich mit drei bestehenden Bewertungsmethoden zeigte, dass ABC-Eval-Verhaltenslabels im Allgemeinen zuverlässiger sind als Labels, die mit bestehenden Methoden gesammelt wurden. Darüber hinaus sind ABC-Eval-Labels auch besser vorhersagbar für die Gesamtkonversationsqualität. Die Kombination aller ABC-Eval-Metriken erklärt über 25% der Konversationsqualität, während die Kombination aller Turn-Level-Likert-Metriken viel weniger erklärt.

Das ABC-Eval-Verfahren ermöglicht eine höhere Auflösung bei der Bewertung konversationaler AI-Modelle als bestehende Methoden. Es bietet auch einen Einblick in die Herausforderungen, mit denen Chat-Modelle konfrontiert sind, wie zum Beispiel das Fehlen von gemeinsamem Verständnis in 20% der Antworten.</sample>
    <sample id="136">Jasivan, ein Student der University of Sheffield, präsentiert seine Forschung "FERMAT: An Alternative to Accuracy for Numerical Reasoning" in einer Präsentation. Die Forschung zielt darauf ab, die Leistung von Sprachmodellen bei numerischen Aufgaben zu verbessern. Die aktuellen Benchmarks wie Accuracy und F1-Maß bieten keine ausreichende Informationen über die Stärken und Schwächen der Modelle bei mathematischen Aufgaben.

Jasivan und sein Supervisor Nafise haben FERMAT entwickelt, ein flexibles Evaluationsset, das sich auf arithmetische Typen konzentriert. FERMAT besteht aus mathematischen Wortfragen, die aus Illinois und CommonCore extrahiert wurden. Die Fragen werden in verschiedene Kategorien eingeteilt, wie z.B. Zahlverständnis, mathematische Operationen und Trainingsabhängigkeit.

Die Forschung zeigt, dass die meisten Modelle bei der Zero-Shot-Evaluation schlecht abschneiden. Durch Fine-Tuning mit mathematischen Lehrern geschriebenen Vorlagen, die mit Nummern und Ausdrücken gefüllt sind, kann die Leistung jedoch verbessert werden. Die Ergebnisse zeigen, dass die Modelle bei der Verwendung von Sprach- und mathematischer Vielfalt besser abschneiden.

Die Forschung legt auch nahe, dass die Kodierung und Tokenisierung von Zahlen als Bereiche für Verbesserungen identifiziert werden sollten. Insgesamt bietet FERMAT eine informativere Alternative zu den aktuellen Benchmarks, um die Leistung von Sprachmodellen bei numerischen Aufgaben zu verbessern.</sample>
    <sample id="137">Die Forscher von der Singapore University of Technology and Design haben ein neues Projekt namens "Tell2Design" vorgestellt, das darauf abzielt, eine Maschine zu entwickeln, die es ermöglicht, durch Sprachanweisungen Gebäudepläne zu erstellen. Das Projekt wurde auf der Konferenz ACL 2023 vorgestellt.

Die Forscher haben eine große Datenbank namens "Tell2Design" erstellt, die über 5.000 menschlich annotierte Sprachanweisungen und über 76.000 künstlich generierte Sprachanweisungen enthält. Die Sprachanweisungen beschreiben die Schlüsselkomponenten eines Gebäudeplans, wie z.B. die Raumtypen, die Form und die Größe der Räume sowie die Beziehungen zwischen den Räumen.

Das Ziel des Projekts ist es, ein Modell zu entwickeln, das es ermöglicht, durch Sprachanweisungen Gebäudepläne zu erstellen, die den Anforderungen entsprechen. Die Forscher haben ein Sequenz-zu-Sequenz-Modell entwickelt, das die Sprachanweisungen als Eingabe und die Raumkoordinaten als Ziel verwendet. Das Modell wurde mit einem prädefinierten Sprachmodell T5 initialisiert und erreichte eine IoU-Score von 54 und 53, was eine Verbesserung gegenüber anderen Text-bedingten Bildgenerierungsmodellen darstellt.

Das Projekt zeigt, dass es möglich ist, durch Sprachanweisungen Gebäudepläne zu erstellen, die den Anforderungen entsprechen. Die Forscher hoffen, dass ihr Projekt als Grundlage für zukünftige Forschung auf dem Gebiet der Sprach-bedingten Designgenerierung dienen wird.</sample>
    <sample id="138">Die Autoren sehen das Gebiet der Integration von Wissen aus verschiedenen Quellen, insbesondere der Kombination von vorab gelerntem Wissen (pretrain-time) und Wissen, das während der Inferenzzeit bereitgestellt wird, als zu wenig erforscht an.</sample>
    <sample id="139">Ying und Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Die Autoren haben crowd-sourced workers beauftragt, falsche Beispiele in der Validierungs- und Testmenge zu finden und zu korrigieren.</sample>
    <sample id="141">Die bestehenden Ressourcen für kontextbasierte Übersetzung haben folgende Grenzen:

- Sie unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen.
- Sie unterstützen nur begrenzte Sprachen, da sie häufig auf Domänwissen und menschliche Kuration angewiesen sind.
- Sie sind nicht in der Lage, alle Arten von kontextabhängigen Übersetzungen abzudecken.</sample>
    <sample id="142">Hallo! Ich werde mich mit Ihnen über unser gemeinsames Werk "Die Lösung indirekter Referenzausdrücke für die Entity-Auswahl" unterhalten, in dem wir das AltEntities Corpus einführen. Mein Name ist Javad Hosseini und dies ist ein gemeinsames Projekt mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Wahl treffen möchten. Überlegen wir uns zum Beispiel die alternative Frage: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Nutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Lösung ist eine direkte Referenz, zum Beispiel durch den Namen des Liedes "Easy on Me" oder durch seine Position, "das erste Lied". Manchmal ist jedoch eine indirekte Referenz eine natürlichere Wahl. Dies kann passieren, wenn der Nutzer den Namen des Liedes nicht mehr weiß. Oder wenn die Aussprachen zu ähnlich sind und schwer zu unterscheiden sind. Oder wenn der Nutzer eine Vorliebe ausdrücken möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "das neuere Lied" oder "das Lied, das nicht energisch ist". Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung der Entity-Verständnis von LLMs. Wir wissen nicht von einem größeren öffentlichen Datensatz für diese Aufgabe, daher haben wir einen gesammelt, indem wir eine Crowd-Annotation durchgeführt haben. Unser Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatz-Sammlungsmethode betont die Informalität, indem sie ein Cartoon-Completion-Setup verwendet. Das Cartoon hat drei Sprachblasen. In der ersten Blase sagt Bob: "Denkst du an das Lied, das wir gestern Abend gehört haben?" Und mit dieser Aussage setzt Bob den Dialogkontext. In der zweiten Sprachblase sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Das ist die alternative Frage. Und in der dritten Sprachblase verwendet Bob eine indirekte Referenz, um eine der beiden Entities auszuwählen, zum Beispiel "das neuere Lied". Wir liefern die ersten beiden Sprachblasen automatisch, aber die dritte Blase wird von dem Annotator ausgefüllt. Die erste Sprachblase wird aus wenigen manuellen Vorschlägen pro Domäne ausgewählt. Die zweite Blase, die alternative Frage, wird wie folgt generiert: Wir verwenden immer eine einfache Vorlage. "Meinst du A oder B?" Wo A und B Proben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methode, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entities immer ähnlicher und es ist schwieriger, die Entscheidung zu treffen. Die erste Methode ist die gleichmäßige Zufälligkeit. Die zweite Methode ist, wenn die Entities ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "Die Rückkehr". Die dritte Methode ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel das gleiche Genre oder der gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen dieser Entities, aber sie wissen nicht unbedingt etwas über die Entities. Also zeigen wir ihnen einige Hintergrundwissen über die beiden Entities. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und fragen die Annotatoren, sie sollen mindestens einige von jedem Lied hören und sich über jedes Lied informieren. Hier ist zum Beispiel der Google-Suchresultat für das Lied "Easy on Me". Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wiederum aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann fragen wir die Annotatoren, sie sollen eine dieser Entities auswählen und sie mit drei bis fünf indirekten Referenzausdrücken beschreiben. Zum Beispiel "das Lied mit dem Klaviermusik". Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "das Lied ohne Worte", "nicht das Lied mit dem 12-jährigen Jungen", oder "das fiktive Lied", oder "kommt aus Aserbaidschan", und so weiter. Das AltEntities Corpus hat 6.000 alternative Fragen in drei Domänen und hat 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugriff auf das gleiche Hintergrundwissen hat wie die Annotatoren, dann ist die Genauigkeit sehr hoch, um die 92 bis 95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugriff auf ein teilweise überlappendes Hintergrundwissen hat, dann ist die Genauigkeit zwischen 82 bis 87%, was realistischer ist. Zum Beispiel wenn das Sprachmodell das Hintergrundwissen abruft. Wenn das Sprachmodell nur Zugriff auf die Entity-Namen hat, dann ist die Genauigkeit nur 60%, also gibt es noch viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle domänenübergreifend sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den folgenden bestehenden SimulST-Richtlinien verglichen: 

1. Wait-k-Strategie
2. Lokale Übereinstimmung (Local Agreement)
3. State-of-the-art-Architektur, die speziell für simultanes Vorschlagsübersetzen (simultaneous pre-translation) entwickelt wurde.</sample>
    <sample id="144">Die Autoren gehören der Universität von Nantes an.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, ein PhD-Studierender der Fudan-Universität, hat einen Vortrag über das Paper "Analyse der Omission in der Dialogsummarisierung" gehalten. Die Dialogsummarisierung ist ein Teilgebiet der Textsummarisierung, bei der ein kurzer Überblick über wichtige Informationen in einem Dialog erstellt wird. Trotz Fortschritten in der Dialogsummarisierung, insbesondere mit großen prätrainierten Sprachmodellen, gibt es noch viele Fehler, darunter Omissionen, die wichtige Fakten verpassen. Omissionen sind ein ernstes Problem in der Dialogsummarisierung, da etwa 70% der generierten Zusammenfassungen davon betroffen sind.

Um dieses Problem anzugehen, hat Yicheng ein neues Dataset, OLDS, erstellt, das Omissionen in Dialogsummarisierung unterstützt. Das Dataset wurde auf fünf existierenden Benchmarks aufgebaut und enthält 10 verschiedene Kandidaten-Zusammenfassungen pro Dialog. Um das Omission-Detection-Problem anzugehen, wurden drei Basismodelle entwickelt, die jeweils eine andere Eingabeformat und Struktur haben.

Die Ergebnisse zeigen, dass die Omission-Detection ein herausforderndes Problem ist, bei dem die F1-Score um 50% liegt. Allerdings verbessert sich die Qualität der Zusammenfassung, wenn die Omissionen verwendet werden, um sie zu refinieren. Dies deutet darauf hin, dass das Omission-Detection ein wertvolles Ziel ist und die Verwendung der Omissionen zur Qualitätsoptimierung in der Dialogsummarisierung vielversprechend ist.</sample>
    <sample id="147">Es sind drei Autoren an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler und werde mich kurz auf den "Attention as a Guide for Simultaneous Speech Translation" Artikel einstellen, der ein gemeinsames Werk mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SimulST, ist der Prozess der Übersetzung gesprochener Sprache in eine andere Sprache in Echtzeit, um eine Kreuzsprachenkommunikation zu ermöglichen. Und welche Probleme haben die aktuellen SimulST-Modelle? Besondere Architekturen werden normalerweise trainiert, wodurch zusätzliche Module optimiert werden müssen. Längliche und komplexe Trainingsverfahren, zum Beispiel Trainingsverfahren, die verschiedene Optimierungsziele beinhalten. Und das Training und Warten auf mehrere Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen Modells mit zwei Sekunden Latenz usw. Also, was ist unsere Lösung? Zuerst verwenden wir bereits bestehende offline-Übersetzungsmodelle ohne erneutes Training oder die Anwendung spezieller Architekturen für SimulST. Verwenden Sie nur ein Modell für jedes Latenzregime und handeln Sie die Latenz durch spezifische Parameter ab. Und nutzen Sie die bereits erworbenen Kenntnisse des Modells durch die Aufmerksamkeitsmechanik zwischen dem Audiostream und dem Textoutput. Das ist die Quer-Aufmerksamkeitsmechanik, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung besteht darin, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob eine Teilübersetzung ausgegeben wird oder nicht, basierend auf dem, wohin sich die Aufmerksamkeit richtet. Ein Wort wird ausgegeben, wenn sich die Aufmerksamkeit nicht konzentriert, d. h. wenn ihr Summe unter einem bestimmten Schwellenwert alpha gegenüber dem letzten lambda-Sprechframes liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Zum Beispiel wenn wir einen Sprechblöcke erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung in Deutsch vorhersagt, und wir uns die Quer-Aufmerksamkeitsgewichte ansehen, sehen wir, dass die ersten zwei Wörter auf die frühesten empfangenen Sprechframes zeigen, während das letzte Wort auf die letzten empfangenen Sprechframes zeigt. Dies bedeutet, dass die ersten zwei Wörter ausgegeben werden, da der Summe der Quer-Aufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, werden wir das letzte Wort nicht ausgeben und warten auf einen anderen Sprechblöcke. Wenn wir weitermachen und einen anderen Sprechblöcke erhalten, und unser Modell andere drei Wörter vorhersagt, und wir uns die Quer-Aufmerksamkeitsgewichte ansehen, sehen wir, dass kein Wort auf die letzten lambda-Sprechframes zeigt. Dies bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, sehen wir die simultane Sprachübersetzungsergebnisse in Grafiken, in denen wir auf einer Seite den BLEU-Wert haben, der die Übersetzungsqualität misst, und die durchschnittliche Verzögerung, die als Latenzmaß angegeben wird, und wir berücksichtigen auch die rechnerische durchschnittliche Verzögerung, die die Modellrechenzeiten berücksichtigt, um das Output vorherzusagen. Wir möchten, dass unsere Kurven so hoch wie möglich auf dieser Grafik sind. Aber wir möchten auch, dass sie nach links verschoben sind. Und wir vergleichen sie mit populären Strategien, die auch auf offline-Modellen angewendet werden, wie der Wait-k-Strategie und der Local Agreement. Und wir vergleichen sie auch mit dem aktuellen Stand der Technik, der speziell für simultane Vorschläge entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachübersetzungsstrategie auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche Verzögerungszeit oder die rechnerische Verzögerungszeit berücksichtigen, sie die schnellste Strategie ist. Wenn Sie mehr über die Ergebnisse erfahren möchten, lesen Sie bitte unseren Artikel. Wir haben auch den Code, die Modelle und die simultane Ausgabe offengelegt, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Nein, der CoNLL++-Datensatz, den Shuheng erwähnt, ist nicht explizit als öffentlich zugänglich angekündigt. Es wird jedoch empfohlen, den Paper und die Daten zu kontaktieren, um mehr Informationen zu erhalten.</sample>
    <sample id="150">Archiki präsentiert in seiner ACL-Paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" ein neues Dataset namens MeetingQA, das sich auf die Extraktive Frage-Antwort-Beziehung in Meeting-Transkripten konzentriert. Das Dataset enthält 7,7K Fragen, die in Train, Dev und Test-Sets aufgeteilt sind, wobei 30% der Fragen unantwortbar sind. Die verbleibenden Fragen haben oft mehrere Antwort-Sätze oder mehrere Sprecher, die zur Antwort beitragen.

Archiki beschreibt die Daten-Sammlung-Prozesse, die auf öffentlichen Meeting-Transkripten aus dem AMI-Korpus basieren. Die Fragen werden durch Punktuation und Filterung kurzer Fragen selektiert, während die Antworten von Annotatoren markiert werden. Das Dataset zeigt eine hohe Inter-Annotator-Übereinstimmung mit einer Krippendorff's Alpha von 0.73.

Archiki präsentiert auch die Ergebnisse seiner Arbeit, bei denen sie verschiedene Modelle verwenden, um die Frage-Antwort-Beziehung in Meeting-Transkripten zu lösen. Die Ergebnisse zeigen, dass die Modelle in beiden fein-tunten und zero-shot-Einstellungen Schwierigkeiten haben, die Frage-Antwort-Beziehung zu lösen. Die Modelle haben Schwierigkeiten, rhetorische Fragen zu erkennen, irrelevantes Text zu entfernen und zu bestimmen, welcher Sprecher eine Frage beantwortet.

Zusammenfassend zeigt MeetingQA, dass die Extraktive Frage-Antwort-Beziehung in Meeting-Transkripten ein interessantes und herausforderndes Gebiet ist, das von bestehenden QA-Modellen nicht vollständig gelöst werden kann. Archiki hofft, dass MeetingQA ein Beitrag zum Fortschritt in diesem Bereich sein wird.</sample>
    <sample id="151">Hallo, mein Name ist Ying und mein Kollege Zhiyang und wir werden unsere Forschung über MultiInstruct präsentieren, mit der das Multi-Modal Zero-Shot-Lernen über Anweisungstuning verbessert wird. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen erforscht, bei denen vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben in einer parametrisch- und daten-effizienten Weise wiederverwendet werden können. In jüngster Zeit haben viele Studien gezeigt, dass das Anweisungstuning große Sprachmodelle ermöglicht, auf unerfahrene Aufgaben in einem Zero-Shot-Manier durch natürliche Anweisungen zu folgen. Allerdings konzentrierten sich die meisten vorherigen Arbeiten auf die Verbesserung der Zero-Shot-Leistung auf Sprachaufgaben nur, während Computer Vision und Multi-Modalaufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob das Anweisungstuning eines Multi-Modell-vortrainierten Modells die allgemeine Fähigkeit zur Generalisierung auf unerfahrene Multi-Modalaufgaben verbessern kann. Zudem haben wir bei unserer Forschung eine beträchtliche Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und Multi-Modal festgestellt. Es gibt mehr als 1600 Sprach-only-Anweisungsaufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren Multi-Modal-Anweisungsdatensatz. Dies motiviert uns, einen Multi-Modal-Anweisungstuning-Datensatz zu erstellen. Hier stellen wir MultiInstruct vor, den ersten Multi-Modal-Anweisungstuning-Benchmark-Datensatz, der 62 diverse Multi-Modalaufgaben umfasst, die 10 breite Kategorien abdecken. Diese Aufgaben sind aus 21 bestehenden offenen Quellcodedatensätzen abgeleitet und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Hier zeigen wir einige Beispiele aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einer vereinheitlichten Sequenz-zu-Sequenz-Format. In diesem Format werden Text, Bilder, Anweisungen und Bounding Boxes im gleichen Token-Raum dargestellt. 

Okay, jetzt gehe ich auf Multi-Modal-Anweisungstuning ein. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für die Ausbildung und wir stichprobenartig 10.000 Instanzen pro Aufgabe. Für die Testung reservieren wir den gesamten Gruppe der allgemeinen Vernunft für die Testung und wir wählen zusätzlich 5 Aufgaben aus der Gruppe VQ und Miscellaneous aus. Wir verwenden alle Instanzen in der Test-Split für jede Aufgabe. Zudem stichprobenartig 20 Aufgaben aus der Test-Split der natürlichen Anweisungen als unerfahrene Aufgabe für NLP aus. Wir verwenden das prätrainierte OFA-Großmodell als Basismodell. Während der Ausbildung mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisungstemplate kombiniert. Während der Testung führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell unter Verwendung einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über den Mindest- und Maximalwert der Leistung und die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine Multi-Modalklassifikationsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine Multi-Modalerzeugungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch eine zusätzliche Bewertungsmetrik namens Empfindlichkeit ein. Dies misst die Fähigkeit des Modells, konsequent die gleichen Ausgaben für die gleiche Aufgabe zu produzieren, unabhängig von den geringen Variationen in der Wortwahl der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann das Anweisungstuning die Leistung von OFA auf gesehene Multi-Modalaufgaben signifikant verbessern. Zudem kann die Übertragung von natürlichen Anweisungsdatensätzen die Leistung von Anweisungstuning verbessern. Hier können wir sehen, dass sich das Modell mit zunehmender Anzahl von Aufgaben verbessert und gleichzeitig die Empfindlichkeit verringert. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegenüber 5 Anweisungen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Empfindlichkeit stark verringern. Dies zeigt die Auswirkungen verschiedener Fine-Tuning-Strategien auf die Empfindlichkeit des Modells. Wie wir sehen können, kann die Übertragung von natürlichen Anweisungsdatensätzen das Modell dazu bringen, eine viel bessere Empfindlichkeit zu erreichen, verglichen mit dem ursprünglichen OFA-Modell. Zudem kann die Übertragung von natürlichen Anweisungsdatensätzen OFA dazu bringen, auf dem natürlichen Instrukt-Datensatz eine viel bessere Leistung zu erreichen. Insgesamt schlagen wir den ersten großen Skalenauftrag für Multi-Modell-Anweisungstuning vor und verbessern die Leistungsfähigkeit von OFA. Wir erforschen auch verschiedene Übertragungstechniken und zeigen ihre Vorteile. Wir entwerfen eine neue Metrik namens Empfindlichkeit. Eines mehr, wir sammeln einen viel größeren Multi-Modell-Anweisungstuning-Datensatz mit etwa 150 zusätzlichen Vision- und Sprachaufgaben und werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider hat in seinem Vortrag "Exploring Large Language Models for Classical Philology" die aktuelle Situation bei der Verwendung von Sprachmodellen für die Klassische Philologie vorgestellt. Er hat festgestellt, dass die meisten bestehenden Modelle BERT-basierte, monolinguale Modelle sind, die nur für eine bestimmte Sprache trainiert wurden. Da es jedoch realistisch ist, dass Forscher Modelle benötigen, die in mehreren Sprachen miteinander interagieren können, hat Riemenschneider gemeinsam mit seinem Team neue Sprachmodelle entwickelt, die speziell für die Klassische Philologie konzipiert sind.

Das Team hat zwei monolinguale Modelle für Altgriechisch entwickelt, GreBERTa und GreTa, sowie zwei multilinguale Modelle, PhilBERTa und PhilTa, die auf Altgriechisch, Latein und Englisch trainiert wurden. Die Modelle wurden mit einem neuen, hochwertigen Trainingskorpus für Altgriechisch und zusätzlichen Ressourcen für Latein und Englisch trainiert. Die Ergebnisse zeigen, dass die neuen Modelle die aktuelle State-of-the-Art für Altgriechisch und Latein übertreffen.

Ein besonderer Schwerpunkt lag auf der Untersuchung des Verhaltens des Encoders des T5-Modells, das als besonders schlecht abschnitt, bevor es nach mehreren Trainingsphasen das Niveau eines nativen Encoder-only-Modells erreichte. Die Ergebnisse zeigen auch, dass die Encoder-Decoder-Modelle besonders stark in der Lemmatisierung abschneiden, wobei sie die aktuelle State-of-the-Art um 5 Prozentpunkte übertrafen. Die Untersuchung der semantischen und weltlichen Kenntnisse der Modelle zeigte, dass sie die aktuellen Modelle in diesen Bereichen übertreffen, jedoch kein signifikanter Unterschied zwischen den monolingualen und multilingualen Modellen besteht.</sample>
    <sample id="153">Ninareh Mehrabi, eine Postdoktorandin am Amazon Alexa AI's Responsible AI-Team, präsentierte ihre Arbeit "Resolving Ambiguities in Text-to-Image Generative Models". Das Ziel dieser Forschung ist es, bestehende Ambiguitäten in Text-bild-Modellen zu untersuchen und Rahmenwerke zur Milderung und Bewertung dieser Ambiguitäten zu entwickeln.

Die Forscher haben ein Benchmark-Dataset erstellt, das verschiedene Arten von Ambiguitäten abdeckt. Dazu wurde ein bereits existierender Corpus namens LAVA modifiziert. Das Dataset enthält verschiedene Arten von Ambiguitäten, wie zum Beispiel die Frage "The girl enters the room with flowers", die unterschiedlich interpretiert werden kann.

Um die Ambiguitäten zu mildern, wird ein Rahmenwerk verwendet, das externe Signale sammelt, um die Ambiguität zu klären. Dazu werden entweder klärende Fragen an den Benutzer gestellt oder verschiedene mögliche visuelle Szenarien generiert. Der Benutzer kann dann die Antwort geben, die seinem Absicht entspricht.

Um die Treffsicherheit der generierten Bilder zu bewerten, wird ein VQA-Modell verwendet. Dieses Modell wird mit den Bildern und der Absicht des Benutzers als Input versehen und bewertet, ob die Absicht erfüllt ist oder nicht. Die Forscher haben gezeigt, dass ihre Rahmenwerke eine positive Auswirkung auf die Treffsicherheit der Bildgenerierung haben und dass ihre Bewertungsmethode mit der Bewertung der Menschen übereinstimmt.</sample>
    <sample id="154">Die Autoren gehören der Universität Trento an.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Shen Gao von Shandong University stellt die Arbeit "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" vor, eine gemeinsame Arbeit mit Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao und Rui Yan. Das Ziel der Arbeit ist es, saliente Informationen aus einem Dialogkontext in eine kurze Zusammenfassung zu überführen. 

Die bestehenden Methoden zur Dialogzusammenfassung nutzen vorab berechnete statische Graphenstruktur, die jedoch zwei fundamentale Nachteile haben: Sie hängen stark von der Genauigkeit externer linguistischer Werkzeuge ab und die statische Graphenkonstruktion ist getrennt von der Graphdarstellungslernphase. 

Die vorgestellte SDDS-Modell besteht aus vier Komponenten: Utterance Encoder, statische Graphenkonstruktion, statisch-dynamischer Graph und Summary Generator. Das Modell nutzt eine Kombination aus statischen und dynamischen Graphen, um die Dialogstruktur zu erfassen. Es verwendet vier Heuristiken zur Modellierung der statischen Dialogstruktur, darunter Diskursparsen, Key Co-occurrence und eine Methode zur Modellierung der Sprecherbeziehungen. 

Zum dynamischen Graphen wird eine Multi-Head-Attention-Modell verwendet, um die Beziehungen zwischen den Utterances zu erfassen. Die statische und dynamische Graphen werden dann kombiniert, um die Dialogstrukturinformationen in die Generationsprozess einzubeziehen. Die Arbeit wurde auf GitHub veröffentlicht.</sample>
    <sample id="158">Der Sprecher, Qipeng Guo von AWS, stellt die Aufgabe der Coreferenz-Resolution vor. Dabei geht es darum, in einem Dokument mehrfach erwähnte Entitäten zu identifizieren und zu gruppieren, wenn sie auf dieselbe Entität hinweisen. Conventionele Methoden für diese Aufgabe haben eine quadratische Komplexität in Bezug auf Berechnung und Speicherbedarf, während neuere cache-basierte Methoden eine lineare Komplexität erreichen, indem sie einen festen-Größe-Cache verwenden.

Der Sprecher beschreibt, dass cache-basierte Methoden bei der Eviction von Entitäten, wenn der Cache voll ist, die Least Recently Used (LRU)-Politik verwenden, was bei langen Dokumenten zu hohen Cache-Misses führen kann. Um dieses Problem anzugehen, wurde der Dual-Cache vorgestellt, der aus einem lokalen Cache und einem globalen Cache besteht, die gemeinsam arbeiten. Der lokale Cache speichert lokale Entitäten mit der LRU-Politik, während der globale Cache globale Entitäten mit der Least Frequently Used (LFU)-Politik speichert.

Der Sprecher stellt die Ergebnisse der Evaluation des Dual-Caches auf vier öffentlichen Benchmarks dar und zeigt, dass es sich bei der Verwendung von unbeschränktem Speicher gegenüber Baseline-Modellen besser verhält. Außerdem zeigt er, dass der Dual-Cache bei der Verwendung von begrenztem Speicher schneller ist und die Cache-Misses reduziert. Der Sprecher schlussfolgert, dass der Dual-Cache eine kosteneffektive Lösung für die Coreferenz-Resolution ist.</sample>
    <sample id="159">Hallo, ich bin Koustav Sinha und ich freue mich, dass Sie zu unserer Diskussion über unser ACL 2023-Papier gekommen sind. In diesem Papier untersuchen wir, wie Sprachmodelle bei der Beurteilung von Akzeptabilität robust gegenüber Kontext sind. Dies ist ein gemeinsames Werk mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.

Wir gehen dabei auf die minimalen Paare-Paradigmen zurück. Diese Paradigmen beurteilen Sprachmodelle anhand von Akzeptabilitätsurteilen. Dies kann auch die Grammatikalität wie bei BLiMP, SyntaxGym oder die Akzeptabilität in Bezug auf Stereotypen wie bei CrowS Pairs umfassen. In diesem minimalen Paare-Paradigma wird normalerweise wie folgt evaluiert: Zuerst wird eine akzeptable oder grammatische Sätze angezeigt, gefolgt von einem ungrammatischen Satz. Das Ziel ist es, dass das Modell mehr Wahrscheinlichkeit auf den akzeptablen Satz legt.

Die aktuelle MPP-Pipeline erlaubt es uns nicht, die Akzeptanz eines Modells bei längeren Sätzen zu bewerten. Große Sprachmodelle haben jedoch immer längere Kontextfenster. Es ist daher wichtig, die Akzeptanz der Modelle über den Kontextfenster hinweg zu bewerten. Wir versuchen dies mit unserem Ansatz, indem wir die MPP-Pipeline erneut besuchen und dem Modell die Akzeptanz bei längeren und längeren Sequenzen bewerten.

Um diese längeren Sequenzen zu simulieren, besuchen wir die Datenbanken selbst und erstellen Sätze, indem wir akzeptable oder unakzeptable Sätze aus diesen Datenbanken wählen. Beispielsweise haben wir ein typisches Paar von Grammatikalität aus der BLiMP-Datenbank ausgewählt, nämlich das Adjunct-Island-Fall. Wir fügen dann einen akzeptablen oder unakzeptablen Satz als Präfix zu beiden Sätzen hinzu. Wir können dasselbe tun, indem wir unakzeptable Sätze aus derselben Struktur auswählen, um das Modell zu testen. Wir können auch dasselbe tun, indem wir Sätze aus einer anderen Teilmenge oder einer anderen Datenbank wählen. Wir nennen dies das Mismatch-Szenario.

Hier werden die Sätze immer noch aus relevanten Datenbanken stammen, aber sie stammen nicht aus derselben Datenbank, die wir bei der Bewertung verwenden. Wir können dasselbe auch für den Fall der Unakzeptabilität tun. Schließlich können wir Sätze aus einem völlig unabhängigen Bereich wie Wikipedia wählen. Das zeigt uns, ob die Akzeptabilitätsurteile des Modells durch den Kontext beeinflusst werden, ob der Kontext aus einer anderen Teilmenge der Datenbank stammt oder völlig irrelevant ist.

Wie macht sich das Modell? Zuerst sehen wir uns die Wikipedia-Sätze an, die völlig irrelevant zu den aktuellen Query-Paaren sind. Hier finden wir heraus, dass die MPP-Urteile für beliebige Kontextlänge robust sind. Wir erhöhen die Kontextlänge bis hin zu 1024, um die OPT- und GPT-2-Modelle auszutesten. Wir sehen hier im orangefarbenen Punktliniendesign, dass die MPP-Urteile relativ stabil sind.

Was passiert, wenn wir Sätze aus derselben Datenbank wählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und unakzeptablen Bereichen aus derselben BLiMP- oder SyntaxGym-Datenbank. Hier sehen wir, dass die MPP-Urteile erheblich zunehmen oder abnehmen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Wenn wir jedoch die Struktur matchen, d. h. wenn wir Sätze aus derselben Phänomen aus BLiMP oder SyntaxGym wählen, sehen wir eine massive Zunahme oder Abnahme der MPP-Urteile für das Modell, je nachdem, ob wir einen akzeptablen oder unakzeptablen Präfix gewählt haben.

Dieser Effekt ist sehr stark und zunimmt mit der Kontextlänge. Dies würde wahrscheinlich auch auf neue Sprachmodelle mit großen Kontextfenstern zutreffen. Warum wirkt sich der matchende Präfix so stark auf die Sprachmodellurteile aus?

Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabensatz zu stören, indem wir den relevanten Struktur, aber Lärm in den Eingabensatz hinzufügten. Nachdem wir mehrere dieser Störungen durchgeführt hatten, fanden wir heraus, dass diese Störungen das Modell nicht dazu bringen, seine MPP-Urteile zu ändern. Das Modell reagiert auf die gestörten Sätze in ähnlicher Weise. Wenn wir den Eingabensatz in einem akzeptablen Bereich stören, sehen wir eine ähnliche Zunahme in allen Störungen. Wenn wir den Eingabensatz in einem unakzeptablen Bereich stören, sehen wir eine Abnahme der MPP-Urteile in ähnlicher Weise.

Die Schlüsselergebnisse unserer Arbeit sind, dass Sprachmodelle auf latente syntaktische und semantische Merkmale reagieren, die sich über die Sätze hinweg erstrecken. Die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satzeingaben durchführen, erfasst möglicherweise nicht das abstrakte Wissen der Sprachmodelle über den Kontextfenster hinweg. Bitte lesen Sie unser Papier für weitere Details unserer Experimente. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Die Input-Token werden im ersten Schritt der Methode mit einem unsortierten Multiset von Tokenen zugeordnet, die in der Ausgabe auftreten werden.</sample>
    <sample id="161">55.000</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist laut Omar die Methode MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen (WSL) ist, dass es günstiger ist als traditionelles Lernen, da es keine manuellen Annotationen der Daten erfordert. Stattdessen können schwache Labelquellen wie einfache Heuristiken oder Low-Quality-Crowdsourcing verwendet werden.</sample>
    <sample id="165">Wenting Zhao, ein PhD-Student an der Cornell University, präsentiert seinen Paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". Das Paper beschäftigt sich mit abduktiver Vernunft, einer Form der logischen Schlussfolgerung, bei der ein plausible Erklärung für einen bestimmten Kontext und ein bestimmtes Ergebnis gefunden wird.

Das Paper stellt ein unüberwachtes Lernalgorithmus namens LiPoR vor, das ohne die Annotation von plausible Erklärungen funktioniert. LiPoR maximiert die Wahrscheinlichkeit des Ergebnisses, gegeben den Kontext, und bevorzugt plausible Erklärungen. Der Algorithmus basiert auf der Idee, dass Erklärungen gegenseitig ausschließen, also müssen sie entweder alle wahr oder alle falsch sein.

Die Ergebnisse des Papers zeigen, dass LiPoR über 4 Punkte in der Genauigkeit besser abschneidet als andere unüberwachte Ansätze und sogar ein starkes Zero-Shot-Modell. Dies zeigt, dass LiPoR ein effektives unüberwachtes Lernalgorithmus für abduktive Vernunft ist.</sample>
    <sample id="166">Title: A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text

Abstract:

Image retrieval from linguistically complex text is a challenging task due to the high similarity of images and long descriptions. Existing methods, such as visual language models, perform well on image sentence retrieval tasks but struggle with complex text. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, we propose a neural divide-and-conquer reasoning (NDCR) framework that integrates analogical reasoning (System 1) and abstract logical reasoning (System 2). The framework consists of three modules: Proposition Generator, Visual-Linguistic Interactor, and Neural-Symbolic Reasoner. The Proposition Generator decomposes complex propositions into simple representations, while the Visual-Linguistic Interactor performs visual-propositions information interaction. The Neural-Symbolic Reasoner integrates reasoning states and results to obtain the final solution. Experimental results show that NDCR outperforms other baselines and verifies the effectiveness of each module. This work suggests that neural symbolic calculation and Divide-and-Conquer strategy can improve compositional reasoning and planning of large language models.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden auf 750 Dokumente verteilt. Von diesen wurden 483 Dokumente vollständig manuell ausgerichtet, während die restlichen Dokumente mit einer Kombination aus manueller und automatischer Alignmentmethode ausgerichtet wurden.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde aus Reuters News von 2020 erstellt und dann mit den gleichen CoNLL-2003-Annotierungsrichtlinien annotiert.</sample>
    <sample id="169">Das Papier "Prompting PaLM for Translation: Assessing Strategies and Performance" präsentiert eine systematische Untersuchung der Verwendung von großen Sprachmodellen (LLMs) für Übersetzungen. Dabei wird die PaLM-Maschine verwendet, ein 540 Milliarden Parameter großes LLM, das auf einer großen Sammlung von Texten trainiert wurde. Die Forscher evaluieren die Übersetzungsleistung der PaLM-Maschine unter Verwendung der besten Praktiken der Übersetzungscommunity und vergleichen sie mit den Leistungen von state-of-the-Art-Systemen.

Die Ergebnisse zeigen, dass die Auswahl der Prompte (Eingabemeldungen) einen großen Einfluss auf die Leistung der LLMs für Übersetzungen hat. Im Gegensatz zu state-of-the-Art-Systemen, die eine Substantielle Vorsprung haben, liegt die PaLM-Maschine sehr nahe an einem kommerziellen System. Die Ergebnisse der menschlichen Bewertung zeigen, dass die PaLM-Maschine eine vergleichbare Flüssigkeit wie state-of-the-Art-Systeme aufweist, aber bei der Genauigkeit hinterherhinkt. Die häufigsten Fehler sind Omissionen, bei denen die PaLM-Maschine Teile des Quelltextes weglässt, um eine bessere klingende Übersetzung zu erzeugen.

Die Forscher empfehlen, die Qualität der Beispiele (Examples) für die Prompte zu priorisieren und sie von der Ähnlichkeit zu den Quellsätzen zu trennen. Sie stellen fest, dass die Auswahl von Beispielen aus der Entwicklungsdatenbank (dev data) zu besseren Ergebnissen führt als die Auswahl von Beispielen aus der Trainingsdatenbank. Die Ergebnisse dieses Papiers bieten wertvolle Einblicke in die Verwendung von LLMs für Übersetzungen und geben Empfehlungen für die Auswahl von Prompten und Beispielen.</sample>
    <sample id="170">Hallo, ich bin Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" vorstellen. Semantic Parsing ist eine Aufgabe, bei der semantische Darstellungen von Benutzeranfragen wie SQL und Lambda-Kalkül erstellt werden. Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mithilfe von neuronalen Modellen in SQL, Lambda oder FunQL und andere übersetzen. Bestehende cross-linguale semantische Parsing-Modelle wurden separat vorgeschlagen und auf Datensätzen begrenzter Aufgaben und Anwendungen evaluiert. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen. Aber Chinesisch fehlt und es gibt eine geringe Abdeckung für bestimmte Bedeutungsrepräsentationen. Die Lambda-Kalkül fehlt oder sie wurden nur auf bestimmte neuronale Modelle evaluiert. Zum Beispiel gibt es nur ein einziges Modell, das sie bewertet. Daher schlagen wir XSemPLR vor. Wir bieten ein einheitliches Datensatz XSemPLR für cross-linguales semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen an. Er enthält 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Und um unsere Benchmarks besser zu bewerten, betrachten wir sechs Einstellungen für Training und Bewertung. Die erste ist Translate-Test. Wir verwenden die Google-Übersetzung-API, um die Quell- in die Ziel-Sprache zu übersetzen, dann trainieren wir und bewerten wir ein monolinguales Modell. Zum Beispiel trainieren wir das englische Modell auf englischen Anfragen und während der Vorhersage übersetzen wir die deutsche Anfrage mithilfe der API in Englisch und verwenden dann das trainierte Modell, um die SQL-Vorhersage zu treffen. Wir testen auch das Monolingual-Modell. In dieser Einstellung ist die Quell-Sprache gleich der Ziel-Sprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch das Monolingual-Few-Shot-Setting, indem wir ein monolinguales Modell mit nur 10% des Trainingsdatums trainieren. Und wir testen das Multilingual-Modell, indem wir ein einziges Multilingual-Modell für alle Sprachen trainieren. Zum Beispiel setzen wir deutsche, englische und chinesische Anfragen zusammen, um ein Multilingual-Modell zu trainieren. Und während der Vorhersage können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen zu übersetzen. Wir berücksichtigen auch Cross-Lingual-Zero-Shot- und Few-Shot-Übertragung. Wir trainieren auf einer Quell-Sprache und übertragen auf eine andere Sprache. Also während des Trainings trainieren wir es auf englischen Anfragen oder der Kombination von englischen und deutschen Few-Shot-Anfragen, um ein Multilingual-Modell zu trainieren, um die SQL-Vorhersage zu treffen. Wir finden auch viele interessante Ergebnisse. 

Was die Analyse von monolingualen Modellen betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, das für Multilingual-Prätrainierte Encoder mit Pointer-basierten Decodern steht, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, die Multilingual-Prätrainierte Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder-Modelle die besten Ergebnisse auf allen neun Datensätzen erzielen. Wir bewerten mT5 und XLM-R + PTR im Multilingual-Setting. Wir fanden heraus, dass Encoder-Decoder-Modelle oder Encoder-PTR durch das Training in einer Mischung verschiedener Sprachen verbessert werden können. Wir fanden heraus, dass dies aufgrund der Leistungssteigerung der meisten bedeutenden natürlichen Sprachen erfolgt, mit Ausnahme der englischen Sprache, die in sieben Datensätzen abnimmt und nur in drei Datensätzen zunimmt. Ich denke, das ist bekannt als "Fluch der Multilingualität". Wir vergleichen auch die Leistungsunterschiede zwischen Sprachen. In diesem Bild ist die blaue Linie die Cross-Lingual-Few-Shot-Übertragung. Die orangefarbene Linie ist die Cross-Lingual-Zero-Shot-Übertragung. Während die grüne Linie ist der Monolingual-Setting. Wir fanden heraus, dass, indem wir die grüne und orangefarbene Linie vergleichen, wir herausfinden, dass die Zero-Shot-Einstellung ein bedeutender Leistungsunterschied bei der Sprachübertragung aufweist. Und indem wir die blaue und orangefarbene Linie vergleichen, finden wir heraus, dass mit der Few-Shot-Einstellung der Leistungsunterschied bei der Sprachübertragung schnell abnimmt. Wir finden auch andere interessante Ergebnisse. Zum Beispiel übertrifft Encoder-Decoder das vorherige Werk oder erreicht vergleichbare Ergebnisse. Die Prätraining auf Englisch kann die Leistung der Few-Shot-Übertragung auf der Ziel-Sprache erheblich verbessern. Und wir fanden heraus, dass Multilingual-Modell wie Codex und BLOOM für cross-linguale semantische Parsing-Aufgaben immer noch unzureichend sind. Zusammenfassend schaffen wir XSemPLR, eine einheitliche Benchmarks für cross-linguales semantisches Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmarks-Studie zu drei repräsentativen Typen von Multilingual-Modellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse. Und bitte besuchen Sie unsere Paper und Code. Vielen Dank, dass Sie zugehört haben.</sample>
    <sample id="171">Bisherige Arbeiten können in vier Kategorien eingeteilt werden. Diese Methoden erfüllen jedoch entweder nicht die Anforderungen an Embedding-as-Services oder fehlen an Transferierbarkeit.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex oder BLOOM sind noch nicht ausreichend für Cross-Lingual Semantic Parsing (CLSP).</sample>
    <sample id="174">Thea ist eine der Autorenin des Papiers "ArgAnalysis35K: Ein großes Dataset für die Argumentqualitätsanalyse". Sie erklärt, dass das Dataset einzigartig ist, da es eine umfassendere und qualitativ hochwertigere Sammlung von Argumenten enthält.

Ein wichtiger Aspekt ist die Argumentqualitätsanalyse, die darin besteht, die Güte eines Arguments auf einer Skala von 0 bis 1 zu bewerten. Das Dataset enthält 35.000 Argument-Analyse-Paare, was es zum größten Dataset in diesem Bereich macht. Die Argumente stammen hauptsächlich aus hochwertigen Debattenturnieren, Experten- und Intermediärdiskussionen.

Ein weiterer wichtiger Aspekt ist die Vielfalt der Argumente. Anstatt auf bestimmte Themen zu fokussieren, wurden 24 Themen basierend auf Erfahrung und Expertenmeinungen ausgewählt. Jedes Thema enthält eine Vielzahl von Motiven, was eine breitere Vielfalt an Argumenten ermöglicht.

Das Dataset enthält auch eine neue Analyse-Kategorie, die eine Kombination von Argumenten, Behauptungen und Prämissen ist. Dies ermöglicht eine tiefergehende Analyse der Argumente und bietet eine umfassendere Sicht auf die Argumentqualität.

Darüber hinaus wurde ein Modell der Annotator-Reliabilität eingeführt, das es ermöglicht, die Unzuverlässigkeit von Annotatoren auf der Ebene jedes Arguments zu bewerten. Dies ermöglicht eine bessere Nutzung der verfügbaren Annotations.

Zusammenfassend ist ArgAnalysis35K ein umfassendes und qualitativ hochwertiges Dataset, das eine Vielzahl von Aspekten der Argumentqualitätsanalyse abdeckt. Es bietet eine breitere Vielfalt an Argumenten, eine tiefergehende Analyse der Argumente und eine bessere Annotator-Reliabilität.</sample>
    <sample id="175">Die Methode geht mit der Mehrdeutigkeit der Permutationen um, indem sie eine kontinuierliche Relaxation verwendet, die das Problem der NP-Härte des "Traveling Salesman"-Problems umgeht. Diese Relaxation ermöglicht es, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu lernen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird in diesem Zusammenhang definiert als die Fähigkeit des Modells, gleichwertige Ergebnisse für verschiedene Demografien oder politische Überzeugungen zu liefern, ohne von vornherein voreingenommen zu sein. In anderen Worten, ein fairer NLP-Modell sollte unabhängig von der politischen Überzeugung oder der Demografie der Person, für die es verwendet wird, gleichwertige Ergebnisse liefern.</sample>
    <sample id="177">Der/die Referent*in heißt Yanis Labrak.</sample>
    <sample id="178">Der Referent heißt Koustav Sinha.</sample>
    <sample id="179">In der vorliegenden Forschung wird ein neues Verfahren vorgestellt, um die Theorie der geistigen Zustände (Theory of Mind) bei großen Sprachmodellen (Large Language Models, LLM) zu verbessern. Das Verfahren, SymbolicToM, verwendet explizite grafische Darstellungen, um die geistigen Zustände der Charaktere in einem Gespräch oder einer Geschichte zu modellieren. Es kann auf verschiedenen Ebenen arbeiten, um die geistigen Zustände der Charaktere zu verfolgen, und ermöglicht es, Fragen zu beantworten, die auf die geistigen Zustände der Charaktere abzielen.

Die Forscher haben SymbolicToM mit verschiedenen LLMs getestet und verglichen es mit Supervisierungsbaselines. Die Ergebnisse zeigen, dass SymbolicToM die Leistung der LLMs in der Theorie der geistigen Zustände verbessert, insbesondere bei Fragen, die auf die geistigen Zustände der Charaktere abzielen. Es zeigt auch eine gute Robustheit gegenüber neuen Szenarien und linguistischer Vielfalt. Die Forscher schlussfolgern, dass SymbolicToM ein nützliches Werkzeug ist, um die Theorie der geistigen Zustände bei LLMs zu verbessern und ihre Fähigkeit, komplexe Gespräche oder Geschichten zu verstehen, zu erhöhen.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">Title: Distilling Script Knowledge from Large Language Models for Constrained Language Planning

Abstract:

Constrained language planning involves generating step-by-step instructions for specific goals with multiple constraints. While previous work has focused on abstract goals, this paper addresses the under-studied problem of planning for specific goals with constraints. We evaluate and improve the constrained language planning ability of large language models using InstructGPT for data acquisition and find that they achieve unsatisfactory results. We conduct a detailed analysis and discover that the faithfulness to constraints cannot be guaranteed. To address this issue, we develop an over-generate-then-filter method that first generates multiple scripts and then filters them based on their faithfulness to the constraints. Our method greatly improves the planning ability of large language models. To enable language planning ability of smaller models, we create a dataset of constrained language planning, named CoScript, by distilling knowledge from large language models using symbolic knowledge distillation. CoScript contains 55,000 specific goals with scripts and shows high pluralism in the generated specific goals. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="182">Tropikalismus bezeichnet in diesem Zusammenhang eine stereotype Darstellung von Frauen aus Lateinamerika, die sie als "vibrant", "curvaceous" und ähnlich beschreibt, was auf eine übertriebene und stereotype Vorstellung von Exotik und Sexualität zurückzuführen ist.</sample>
    <sample id="183">Die Autoren haben ihre Methodik anhand eines Studies inspiriert, bei dem Menschen ähnliche Anfragen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." erhalten haben. Dadurch konnten sie direkte Vergleiche zwischen den von Menschen verfassten Beschreibungen und den von LLMs generierten Personas anstellen.</sample>
    <sample id="184">CXMI (Context Usage by Machine Translation Models) wurde in der vorherigen Arbeit als Maß für die Kontextnutzung von Übersetzungsmodellen eingeführt. In dieser Arbeit wurde CXMI zu Pointwise CXMI erweitert, um Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="185">Der Unterschied zwischen DrBERT und ChuBERT besteht darin, dass DrBERT auf NACHOS, einer Datenbank mit medizinischen Informationen, trainiert wurde, während ChuBERT auf anonymisierten Daten des Nantes University Hospital Data Warehouse trainiert wurde.</sample>
    <sample id="187">Es ist nur eine Person, Ying, und ihr Kollege Zhiyang, die an der Arbeit beteiligt sind.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Ansatz, bei dem zunächst ein Modell durch Transferlernen aus einer anderen Aufgabe trainiert wird und dann auf einer neuen, spezifischeren Aufgabe weiter trainiert wird, um ihre Leistung zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist es, die Verständigung von Benutzern in natürlichen Gesprächen zu verbessern, insbesondere wenn sie sich auf bestimmte Entitäten (z.B. Songs, Bücher, Rezepte) beziehen und diese nicht direkt nennen können.</sample>
    <sample id="190">Der Angreifer kann Modellparameter über einen EaaS (Embedding as a Service) extrahieren, indem er von der Dienstleister-Service eine große Menge an Anfragen sendet, um die Modellparameter zu lernen und dann ähnliche Dienstleistungen anzubieten.</sample>
    <sample id="191">Es sind drei Autoren an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Yang Luo, Forscher, hat eine Präsentation über das Projekt "CAME: Confidence-guided Adaptive Memory Efficient Optimization" gehalten. Das Ziel des Projekts ist es, ein Optimierer zu entwickeln, der gleichzeitig eine schnelle Konvergenz und eine geringe Speicherverbrauch erreicht.

Yang Luo hat erwähnt, dass viele adaptive Optimierer wie Adam ein hohes Speicherverbrauch benötigen, um die ersten und zweiten Momenten der Gradienten zu speichern. Um dies zu überwinden, wurden bereits Optimierer wie Adafactor entwickelt, die jedoch eine Leistungseinbuße mit sich bringen.

Yang Luo hat eine neue Methode vorgestellt, die Confidence-guided Adaptive Memory Efficient Optimization (CAME) genannt wird. CAME basiert auf der Non-Negative Matrix Factorization (NMF) und reduziert den Speicherverbrauch von O(mn) auf O(m + n). Außerdem verwendet CAME eine confidence-gesteuerte Aktualisierung, die auf der Residuum zwischen dem vorhergesagten Update und dem tatsächlichen Update basiert.

Yang Luo hat anhand von Experimenten gezeigt, dass CAME eine bessere Leistung als Adam und Adafactor erreicht, insbesondere bei der Vorverarbeitung großer Modelle. CAME reduziert den Speicherverbrauch und erreicht eine höhere Genauigkeit als die bestehenden Optimierer. Die Experimente wurden auf verschiedenen großen Sprachmodellen wie BERT, GPT-2 und T5 durchgeführt.

Insgesamt hat Yang Luo gezeigt, dass CAME ein effektiver Optimierer für die Vorverarbeitung großer Sprachmodelle ist und eine bessere Leistung als die bestehenden Optimierer erreicht. CAME reduziert den Speicherverbrauch und erreicht eine höhere Genauigkeit, was es zu einem vielversprechenden Ansatz für die Entwicklung von künftigen Optimierern macht.</sample>
    <sample id="193">Es ist nicht explizit erwähnt, wie viele Annotatoren verwendet wurden, um den ursprünglichen Datensatz zu erstellen. Es wird jedoch erwähnt, dass um den Datensatz herum 1.000 Beispiele von Diskurs-Einheiten-Paaren gesammelt wurden.</sample>
    <sample id="194">Die Autoren gehören an die University of Washington und an das Allen Institute for AI, sowie an die Carnegie Mellon University.</sample>
    <sample id="195">Das Paper "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" behandelt das Thema Explainable Question Answering (XQA), bei dem es darum geht, eine Frage zu beantworten und die Gründe für die Auswahl der Antwort zu erklären. Die Autoren identifizieren zwei Hauptchancen im Bereich XQA: Neuro-symbolische Methoden und dekompositionsbasierte Methoden. Neuro-symbolische Methoden können jedoch nur auf strukturierten Knowledge Bases (KB) ausgeführt werden, was ihre Recall begrenzt. Dekompositionsbasierte Methoden hingegen nutzen freie Textkorpora als Kenntnisquelle, was die Vielfalt der natürlichen Sprache und die Komplexität von Fragen erhöht.

Um diese Herausforderungen zu überwinden, präsentieren die Autoren das Framework RoHT (Reasoning over Hierarchical Question Decomposition Tree), das sich in zwei Stufen aufteilt. Zunächst wird ein Hierarchisches Fragenzerlegungsbäum (HQDT) erstellt, der die Komposition der komplexen Frage darstellt. Dann wird über das HQDT hinweg probabilistische Gründe gezogen, um Kenntnisse aus einem Knowledge Base und einem Textkorpus zu fusionieren. Die Autoren bewerten das Framework auf zwei herausfordernden komplexen QA-Datasets, KQA Pro und Musique, und zeigen, dass RoHT eine verbesserte Leistung erreicht, wenn es um die Integration von Kenntnissen aus verschiedenen Quellen geht.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">Die Technik für Dialogsysteme ist in ständiger Entwicklung. Laut den Angaben in der Präsentation sind die Bots, die getestet wurden, noch von einigen Herausforderungen geprägt. So treten bei etwa 20% der Antworten Common Sense-Violationen auf, bei etwa 15% werden irrelevante Informationen produziert und bei etwa 10% werden sich selbst oder den Partner widersprechende Antworten gegeben. Es wird jedoch angenommen, dass sich diese Fehlerraten aufgrund der schnellen Fortschritte im Bereich der künstlichen Intelligenz in Zukunft verringern könnten.</sample>
    <sample id="198">Da große Sprachmodelle immer längere Kontextfenster haben, ist es wichtig, ihre Akzeptanz über das gesamte Kontextfenster zu bewerten, um sicherzustellen, dass sie bei längerem Text korrekt funktionieren.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in sieben von neun Datensätzen zu einem Leistungsabfall des englischen Modells geführt.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität nicht unbedingt. Sie kennen jedoch den Namen der Entität. Um ihnen die notwendige Hintergrundinformation zu liefern, zeigen wir ihnen Hintergrundwissen über die beiden Entitäten.</sample>
    <sample id="201">Wir verwendeten state-of-the-art, neuronale MT-Metriken sowie zusätzlich Ergebnisse einer Experten-basierten, menschlichen Bewertung mit dem MQM-Framework.</sample>
    <sample id="202">Nein, in der Präsentation wird nicht erwähnt, dass die Regression bei der Generalisierung auf bestimmte NER-Typen auswirkt. Die Ergebnisse basieren auf den F1-Werten für die Gesamtleistung der Modelle.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Vorurteile der Forscher und Entwickler einer Technologie widerspiegelt und somit systematische Leistungsdifferenzen zwischen verschiedenen Bevölkerungsgruppen hervorrufen kann. Durch die Berücksichtigung der Positionalität können NLP-Modelle und -Daten besser auf die Bedürfnisse und Erfahrungen unterschiedlicher Bevölkerungsgruppen zugeschnitten werden und somit ein inklusiveres und gerechteres Ergebnis erzielt werden.</sample>
    <sample id="204">Es wird nicht explizit erwähnt, ob die mehrsprachigen LLMs wie BLOOM durch Adapter oder eine vollständige Feinabstimmung angepasst wurden. Es wird jedoch erwähnt, dass die multilingualen LLMs wie Codex und BLOOM noch nicht ausreichend für die Aufgabe des cross-lingualen semantischen Parsing sind, ohne dass spezifische Anpassungsmaßnahmen erwähnt werden.</sample>
    <sample id="205">Shangbin, ein Doktorand an der University of Washington, präsentiert seine Forschung "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". Die Arbeit untersucht die Auswirkungen politischer Vorurteile in Sprachmodellen auf NLP-Anwendungen.

Shangbin zeigt, dass Sprachmodelle auf großen Web-Crawldaten trainiert werden, die politische Nachrichtenmedien gut abdecken. Dies hat eine Mischung aus Vorteilen und Nachteilen. Einerseits können Sprachmodelle von diversen Perspektiven lernen, was Demokratie und Vielfalt fördert. Andererseits können diese Vorurteile soziale Ungerechtigkeiten in NLP-Anwendungen verursachen.

Die Forschung fragt nach der politischen Ausrichtung von Sprachmodellen und der Rolle der Trainingsdaten dabei. Shangbin zeigt, dass Sprachmodelle unterschiedliche politische Neigungen haben und dass diese Neigungen von den Trainingsdaten beeinflusst werden. Die Forschung zeigt auch, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedlich auf Downstream-Tasks wie Hate-Speech-Detektion und Fake-News-Detektion reagieren.

Die Ergebnisse zeigen, dass Sprachmodelle mit linken Neigungen besser bei der Erkennung von Hassreden gegen soziale Minderheiten sind, aber schlechter bei der Erkennung von Hassreden gegen dominante Gruppen. Ähnliche Trends werden für die Fake-News-Detektion beobachtet. Shangbin betont, dass dies ein wichtiger Fairness-Aspekt bei der Entwicklung von Sprachmodellen ist und dass die Forschung ein Alarmzeichen für die Bedeutung der Berücksichtigung politischer Vorurteile in Sprachmodellen setzt.</sample>
    <sample id="206">Das Modell, das wir verwenden, um das Transferlernen durchzuführen, ist ein Klassifikator, der anhand von zwei verschiedenen Aufgaben trainiert wird: "debate" (Topic-Independent Dissonance Stance Classification) und "CE" (Binary Classification von Expansion und Comparison-Klassen von PDTB).</sample>
    <sample id="207">Die neuesten Testsets wurden verwendet, um eine Überschneidung mit dem Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen: 

1. Positive Stereotypen und essentialisierende Narrative sollten von Forschern angegangen werden.
2. Ein intersectionaler Blickwinkel sollte bei der Untersuchung von Vorurteilen und Schäden verwendet werden.
3. Es sollte mehr Transparenz über Methoden zur Verringerung von Vorurteilen herrschen.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt nicht explizit angegeben. Es wird jedoch erwähnt, dass die vorgeschlagene Methode "over-generate-then-filter" die Planungsfähigkeit sowohl in Bezug auf semantische Vollständigkeit als auch in Bezug auf Treue zu den Einschränkungen verbessert, aber keine genaue Zahlenangabe wird gemacht.</sample>
    <sample id="210">Der Referent ist Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden, um die Leistung von automatischen Textvereinfachungsmodellen zu bewerten. Die Forscher haben bereits einige Ergebnisse veröffentlicht, darunter die besten automatischen Alignierungsmethoden und die Ergebnisse von Fine-Tuning von Sprachmodellen für Textvereinfachung.</sample>
    <sample id="212">Die Arbeit experimentiert mit einem T5-Modell, das auf CoScript fine-tuned wurde.</sample>
    <sample id="213">Das Basismodell, das für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet wird, ist OFA (One For All), ein einheitlicher multimodaler prätrainierter Model.</sample>
    <sample id="215">Adam Przepiórkowski diskutiert die Abhängigkeitsstruktur der Koordination im Rahmen eines Beitrags. Er präsentiert verschiedene Ansätze zur Darstellung von Koordination in der Universal Dependencies, der Bedeutungstexttheorie von Igor Mel'čuk und der Prager Ansicht. Diese Ansätze sind asymmetrisch, da sie eine der Konjunktionen als Kopf der Koordinationsstruktur auswählen.

Adam argumentiert, dass die Koordinationsstruktur symmetrisch sein sollte, da dies durch das Prinzip der Minimierung der Abhängigkeitslänge unterstützt wird. Er zeigt, dass bei der Verwendung dieses Prinzips die Koordinationsstruktur "Marge read this absolutely fascinating book about bees yesterday" akzeptabel ist, obwohl sie die allgemeine Grammatikregel verletzt, dass direkte Objekte direkt nach dem Verb stehen sollten.

Adam analysiert Statistiken über Koordination aus der erweiterten Version des Penn Treebank und zeigt, dass die linke Konjunktion tendenziell kürzer ist als die rechte. Dieses Phänomen wird stärker, wenn die Längeunterschied zwischen den beiden Konjunktionen größer ist. Allerdings tritt dieses Phänomen nur auf, wenn der Gouverneur auf der linken Seite oder fehlt. Wenn der Gouverneur auf der rechten Seite steht, tritt dieses Phänomen nicht auf.

Adam argumentiert, dass diese Ergebnisse ein Argument gegen asymmetrische Koordinationsstrukturen und für symmetrische Koordinationsstrukturen liefern. Er lädt dazu ein, sich mit ihm im Poster-Session über diese Ergebnisse zu unterhalten.</sample>
    <sample id="217">Die Forscher Weihao Zeng, Lulu Zhao und Keqing He von der Beijing University of Posts and Telecommunications haben ein neues Modell für die kontrollierte Dialoggenerierung vorgestellt, das "Disentangled Controllable Generation" (DCG) genannt wird. Dieses Modell kann mehrere Attribute gleichzeitig kontrollieren und generiert Dialoge, die auf verschiedenen Ebenen kontrolliert werden können.

Das DCG-Modell verwendet eine disentanglement-Losung, um die verschiedenen Attribute-Kombinationen zu trennen und zu lernen, wie sich die Attribute gegenseitig beeinflussen. Es verwendet auch zwei Arten von Prompten: Attribute-orientierte Prompten, die sich auf spezifische Attribute konzentrieren, und Aufgaben-orientierte Prompten, die die globalen Merkmale eines Dialogs berücksichtigen.

Um die Leistung des Modells zu bewerten, haben die Forscher ein neues Bewertungskriterium namens "MAE" (Multi-Attribute Evaluation) entwickelt, das auf einer Referenzfreien Basis arbeitet. Dieses Kriterium wird verwendet, um die Kontrollierbarkeit und die Textgleichheit von Dialogen zu bewerten.

Die Ergebnisse der Experimente zeigen, dass das DCG-Modell in der Lage ist, mehrere Attribute gleichzeitig zu kontrollieren und Dialoge zu generieren, die auf verschiedenen Ebenen kontrolliert werden können. Es zeigt auch, dass das Modell in der Lage ist, die Beziehungen zwischen verschiedenen Attributen zu lernen und sich auf neue Attribute-Kombinationen zu generalisieren.

Insgesamt bietet das DCG-Modell eine neue Möglichkeit für die kontrollierte Dialoggenerierung und kann in verschiedenen Anwendungen verwendet werden, wie z.B. in Chatbots, Sprachassistenten und anderen Sprachanwendungen.</sample>
    <sample id="218">Es wird nicht erwähnt, dass die Autoren einer bestimmten Universität angehören. Sie arbeiten jedoch gemeinsam mit Kollegen von Google Translate.</sample>
    <sample id="219">Jia-Huei Ju, ein Forschungsassistent am Academia Sinica, präsentiert ihre Arbeit "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports". Das Ziel der Arbeit ist die Analyse von Finanzberichten, insbesondere des Form 10-K, das von der SEC verlangt wird. Die Analyse dieser Berichte erfordert viel menschliche Arbeit, da sie viele Details über die Aktivitäten von Unternehmen enthalten.

Die Forscher beobachteten, dass etwa 80% der Wörter in den Berichten ähnlich sind und die Inhalte jahresabhängig sind. Basierend auf dieser Beobachtung wurde ein Highlighting-Tasks und ein mehrstufiges Pipeline vorgestellt. Das Highlighting-Task besteht darin, die Bedeutung von Wörtern in einem Bericht im Vergleich zu einem Referenzbericht aus dem Vorjahr zu bestimmen.

Die Forscher präsentieren ein mehrstufiges Pipeline, das aus vier Stufen besteht: Dokumentsegmentierung, Beziehungsrecognition, out-of-domain-Fine-Tuning und in-domain-Fine-Tuning. Sie verwenden ein externes Dataset, eSNLI, für das out-of-domain-Fine-Tuning und das FINAL-Dataset für die in-domain-Fine-Tuning. Die Leistung des Modells wird mit zwei Metriken bewertet: Precision und PCC (Pearson-Korrelation-Koeffizient).

Die Ergebnisse zeigen, dass das Modell die besten Ergebnisse auf dem FINAL-Dataset erreicht und auch die Fähigkeit aufrechterhält, sich auf neue Daten zu generalisieren. Die Forscher schlagen auch vor, dass ihre Methode sich auf die Analyse von Mismatched-Paaren anwenden lässt, die während der Ausbildung nicht verwendet wurden.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte verschiedene Sprachpaare, jedoch wird speziell die Übersetzung von Deutsch ins Englisch als Beispiel erwähnt.</sample>
    <sample id="222">Die Arbeit "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" beschäftigt sich mit der Herausforderung, Frage-Antwort-Systeme auf neue Domänen anzuwenden. Die Autoren stellen fest, dass die Verwendung eines allgemeinen Korpus wie Wikipedia nicht ausreicht, um auf neue Domänen wie die Biomedizin umzustellen. Sie erkennen drei Hauptkontributionen an:

1. Untersuchung von Dateninterventionsmethoden, die die Ausführung von Frage-Antwort-Systemen in neuen Domänen ermöglichen.
2. Identifizierung des Art des Datenverschiebungs, der bei der Anwendung auf eine neue Domäne auftritt.
3. Bestimmung der Art der Dateninterventionsmethoden, die für einen bestimmten Arten von Datenverschiebung wirksam sind.

Die Autoren präsentieren drei Arten von Dateninterventionsmethoden: Zero-Shot, Few-Shot und eine Mischung aus beiden. Sie testen diese Methoden anhand von sieben Zielpassagen und -Datenbanken, die sich über sechs verschiedene Domänen erstrecken. Die Ergebnisse zeigen, dass die Few-Shot-Methoden durchschnittlich eine Verbesserung von 8% im Retriever und 11% im Reader erzielen, während die Zero-Shot-Methoden durchschnittlich eine Verbesserung von 6% im Retriever und 8% im Reader erzielen.

Die Autoren identifizieren auch drei Arten von Datenverschiebungen: Konzeptverschiebung, kovariante Verschiebung und volle Verschiebung. Sie entwickeln ein Kompatibilitätsmaß, um die Kompatibilität zwischen dem Quellmodell und dem Zielmodell zu messen. Anhand dieses Maßes können die Autoren die Ziel-Datenbanken auf einem 2D-Grid abbilden und den Arten der Datenverschiebung zuordnen.

Die Ergebnisse zeigen, dass die Few-Shot-Methoden für alle Ziel-Datenbanken wirksam sind, während die Zero-Shot-Methoden für die Ziel-Datenbanken mit Konzept- und kovarianten Verschiebungen wirksam sind. Die Autoren schlussfolgern, dass die Verwendung von Dateninterventionsmethoden die Leistung von Frage-Antwort-Systemen in neuen Domänen verbessern kann.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Während der Experimente wurden zwei Modelle untersucht: long-mBART und mBART (basisches Modell).</sample>
    <sample id="225">53 Aufgaben werden für das Training verwendet und 8 Aufgaben (1 aus dem Common Sense Reasoning-Gruppe und 7 aus den VQ- und Miscellaneous-Gruppen) werden für das Testen verwendet.</sample>
    <sample id="226">Es wird nicht explizit angegeben, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="227">Das Forschungsteam hat festgestellt, dass die aktuelle Sprachmodell-Forschung einige Herausforderungen bei der Verständnis von natürlicher Sprache in bestimmten Umgebungen hat. Sie argumentieren, dass die Grundierung von natürlicher Sprache in eine bestimmte Umgebung, bekannt als "Grounded Language Understanding", ein wichtiger Aspekt ist, der bisher nicht ausreichend erforscht wurde. 

Die Grundierung von natürlicher Sprache in eine bestimmte Umgebung ist wichtig für verschiedene Anwendungen wie intelligente Assistenten, semantische Suchmaschinen und Roboter, die natürliche Sprache verstehen können. Das Team argumentiert, dass die Grundierung von natürlicher Sprache in eine bestimmte Umgebung besonders herausfordernd ist, weil die meisten Sprachmodelle während der Vorbereitung nicht mit der Grundierung von natürlicher Sprache in eine bestimmte Umgebung trainiert werden.

Das Team hat daher ein neues Framework namens "Pangu" entwickelt, das Sprachmodelle auf die Grundierung von natürlicher Sprache in eine bestimmte Umgebung trainiert. Im Gegensatz zu anderen Ansätzen, die Sprachmodelle auf die Generierung von Plänen oder Programmen trainieren, verwendet Pangu ein Symbolisches Agent, das Kandidatenpläne vorschlägt und ein Sprachmodell, das die Kandidaten bewertet und priorisiert. Dieser Ansatz ist effizienter und erlaubt es Sprachmodellen, die Grundierung von natürlicher Sprache in eine bestimmte Umgebung zu lernen. 

Die Ergebnisse der Experimente zeigen, dass Pangu eine überzeugende Leistung in verschiedenen Szenarien zeigt, einschließlich der Fragestellungsbasierten Fragestellung und der semantischen Suchmaschine. Pangu zeigt auch eine starke Effizienz bei der Verwendung von Sprachmodellen unterschiedlicher Größe und Typen.</sample>
    <sample id="228">Die Autoren haben an vier Datenbanken experimentiert: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">In the presentation, Gabriella Skitalinskaya discusses a joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. The authors focus on the revision process in professional writing, which is essential for effectively communicating a message. They introduce two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. The first task involves deciding whether a claim needs revisions, while the second task requires selecting the types of quality issues to be improved.

The authors explore the challenges of working with revision-based data, including different domains, goals, and notions of quality. They focus on argumentative text and use collaborative online debate platforms like Kialo to extract information on claim quality. Four challenges are identified: Representativity and Reliability, Model Complexity and Architecture, Contextual Information, and Topical and User Bias.

To address these challenges, the authors present a detailed analysis of strategies and approaches for the introduced tasks. Their experiments show that revision-based data can be effectively employed for the tasks, and modeling the distance between two claimed versions is beneficial for detecting suboptimal claims. The impact of contextual information depends on the task and the quality issues a text is suffering from.</sample>
    <sample id="231">NACHOS ist ein Datenbestand medizinischer Webdaten, die von Crawling-Techniken gesammelt wurden.</sample>
    <sample id="232">Der Referent ist David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) ist der Prozess, gesprochene Sprache in Echtzeit in eine andere Sprache zu übersetzen, um die Kommunikation zwischen Sprachen zu ermöglichen. Aktuelle SimulST-Modelle haben jedoch Probleme wie:

* Zusätzliche Module, die optimiert werden müssen
* Lange und komplexe Trainingsprozeduren
* Trainierung und Wartung mehrerer Modelle für verschiedene Latenzregime

Um diese Probleme zu lösen, wird vorgeschlagen, bestehende Offline-Übersetzungsmodelle ohne erneutes Training oder spezielle Architekturen für SimulST zu verwenden. Dies wird durch die Implementierung des Encoder-Decoder-Attention-Modells (EDAtt) ermöglicht, das eine Strategie zur Entscheidung darstellt, ob ein Teilübersetzung abgesendet werden soll oder nicht, basierend auf der Aufmerksamkeitsmechanik zwischen Audioeingabe und Textausgabe. Die Ergebnisse zeigen, dass EDAtt die Leistung von aktuellen SimulST-Modellen übertrifft und die schnellste Strategie für SimulST ist.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse, insbesondere bei einer geringen Anzahl an Prompten (Zero-Shot oder One-Shot). Bei einer höheren Anzahl an Prompten (z.B. 5-Shot) hat die Form der Prompte jedoch keinen großen Einfluss, sondern vielmehr die Qualität der Beispiele.</sample>
    <sample id="235">Die Autoren gehören an die Carnegie Mellon University, die University of Edinburgh und die University of São Paulo.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen für jede Aufgabe im MultiInstruct-Dataset sind nicht explizit im Text angegeben. Es wird jedoch erwähnt, dass jede Aufgabe mit "fünf Experten geschriebenen Anweisungen" ausgestattet ist.</sample>
    <sample id="237">Die Autoren schlagen vor, ein diagnostisches Test-Suite namens "KITMUS" zu verwenden, um die Fähigkeit von Modellen, Informationen aus mehreren Quellen zu nutzen, zu testen. Dazu wird ein Coreferenz-Resolution-Aufgaben erstellt, bei der Modelle aufgefordert werden, die richtige Entität zu identifizieren, auf die ein Pronomen verweist. Die Verfügbarkeit von Informationen wird in drei verschiedenen Szenarien variiert: "Background-Pretrain", "Background-Both" und "Background-Inference".</sample>
    <sample id="238">Yebowen Hu von der University of Central Florida hat ein neues Benchmark-Dataset namens MeetingBank vorgestellt. Das Dataset enthält Transkripte von City-Council-Meetings, Referenzsummarien und andere nützliche Ressourcen. Um das Dataset zu erstellen, verwendete man den Speechmatics-API, um die Audio-Daten in Transkripte zu konvertieren, und suchte dann die entsprechenden Referenzsummarien und Meeting-Segmente auf der Website der Boston City Council.

Das Dataset enthält 1.366 City-Council-Meetings und etwa 7.000 Instanzen. Es gibt Statistiken über die Anzahl der Meetings, die Dauer, die Anzahl der Tokens und die Anzahl der Sprecher pro Meeting. Es gibt auch eine Bewertung der Zusammenfassungsinhalte mit zwei Messungen: Coverage und Density.

Für die Bewertung der Zusammenfassungssysteme wurden top-tiere Summarization-Systeme auf dem Testset von MeetingBank getestet. Die Ergebnisse zeigen, dass Extr-Oracle ein hohes ROUGE-2-Score erreicht, während GPT-3 in den automatischen Metriken schlecht abschneidet, aber in der menschlichen Bewertung eine hohe Gesamtpunktzahl erreicht. Die Ergebnisse deuten darauf hin, dass die Zusammenfassungssysteme weiterhin auf die Hauptdiskussionsthemen fokussieren sollten und dass neue automatische Bewertungsmetriken entwickelt werden sollten, die besser mit den menschlichen Vorlieben übereinstimmen.</sample>
    <sample id="239">Hallo, ich bin David Vilar und werde eine kurze Besprechung des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist ein gemeinsames Werk mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im vergangenen Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zu diesem Zeitpunkt erreichte es den Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit stellen wir die erste systematische Untersuchung der großsprachmodell-Prompting für die maschinelle Übersetzung vor. Wir evaluierten die Übergabe-Fähigkeit solcher Modelle mithilfe der besten Praktiken der MT-Gemeinschaft. Dazu gehören die Verwendung der neuesten Testmengen, um eine Überschneidung der Testdaten mit der Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen es auch mit den aktuellen Systemen, also dem bestperformierenden System, also der WMT-Evaluation. Wir verwenden aktuelle, neuronale MT-Metriken und zeigen zusätzlich auch Ergebnisse aus der Expertenbewertung. Schließlich geben wir einige Empfehlungen für die Auswahl von Prompt-Strategien ab. Die Prompting hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, in dem wir ein einmaliges Prompting verwendeten und zwei verschiedene Prompt für jede Aussage bereitstellten. Die Mehrheit der Aussagen (516 von 1.000) zeigte einen Unterschied von mehr als einem BLEURT-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkten erreichen. Es ist also wichtig, eine gute Prompt-Strategie auszuwählen. In unseren Experimenten wählten wir eine 5-Schuss-Prompt-Strategie, bei der wir jedes Satz, den wir dem System bereitstellen, mit der Sprache markierten, aus der es stammt. Beispielsweise, wenn wir eine Übersetzung von Deutsch ins Englisch durchführen, markieren wir die deutschen Sätze (die Quellsätze) mit "Deutsch: " und die englischen Übersetzungen mit "Englisch: ". Wir sahen, dass die tatsächliche Form der Prompting keinen großen Einfluss hat, wenn es um mehrere kurze Promptings geht. Es ist jedoch wichtig bei einem ein- oder Null-Schuss-Prompting. Und wenn wir, wie in unserem Fall, zu fünf Schuss-Prompting übergehen, besteht fast keine Unterschied zu der tatsächlichen Form der Prompting. Die Beispiele tragen den größten Teil des Gewichts. Zusammenfassend sind unsere experimentellen Ergebnisse, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellsatz. Es ist also wichtig, Beispiele von hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompt aus der Trainingsdaten für die WMT-Evaluation auf der Dev-Daten. Die Dev-Daten sind viel mehr gekürte und von höherer Qualität als die Trainingsdaten, die eher ungenau sind. Und ihre Ergebnisse zeigen eine bessere Leistung, wenn sie auf der Dev-Daten verwendet werden. Dennoch haben spezialisierte Systeme, die auf dem Stand der Technik sind, einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt den kommerziellen Systemen ziemlich nahe. In unserem Fall wählten wir, um zu evaluieren, Google Translate. Die Erkenntnisse, die wir aus der Bewertung durch Menschen gewannen, die wir mit dem MQM-Framework durchführten, sagten, dass die Flüssigkeit von PaLM mit den Systemen auf dem Stand der Technik vergleichbar ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die am häufigsten vorkommenden Fehler Omissionsfehler. Es scheint, dass PaLM sich entscheidet, eine bessere klingende Übersetzung zu produzieren, indem es manchmal Teile des Quellsatzes, die in der Übersetzung fehlen, weglässt. Der "Style/Awkward"-Kategorie von PaLM ist jedoch niedriger als bei den Systemen auf dem Stand der Technik, was ein weiterer Hinweis darauf ist, dass PaLM wirklich flüssige Ausgaben liefert, aber mit einigen Genauigkeitsproblemen. Und das ist es für diese kurze Besprechung. Für weitere Details, bitte besuchen Sie die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Promotionsstudent an der Saarland University in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in die schwache Supervision und die schwach supervisierte Lernung beginnen. In der schwachen Supervision werden die Daten nicht manuell gekennzeichnet. Stattdessen werden die Daten mit schwachen Kennzeichnungsquellen gekennzeichnet, wie z.B. einfachen heuristischen Regeln, Wissensbasen oder geringwertigen Crowdsourcing-Ansätzen, wie im Bild rechts gezeigt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotations viel günstiger, aber auch lauter, d.h. ein bestimmter Anteil der Annotations ist falsch. Wenn wir direkt neuronale Netze auf schwach gekennzeichnete Daten trainieren, tendieren die Netze dazu, die Kennzeichnungsgeräusche zu memorieren und nicht zu generalisieren. In der schwach supervisierten Lernung werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchen Kennzeichnungsgeräuschen zu trainieren, so dass die trainierten Modelle noch gut generalisieren. In den letzten Arbeiten in WSL (Weakly Supervised Learning) ist ein gemeinsamer Anspruch, dass man nur Modelle auf schwach gekennzeichneten Daten trainiert und trotzdem eine hohe Leistung auf sauberen Testsets erreicht. Technisch ist dies nicht falsch, aber es gibt einen Haken, nämlich dass man annehmen muss, dass ein zusätzlicher sauberer Validierungssatz verfügbar ist, um das Modell zu wählen. Wir können uns nicht auf dieses Problemsetting beschränken, aber dies impliziert, dass zusätzliche manuelle Annotationen erforderlich sind in der schwach supervisierten Lernung. Diese Notwendigkeit wird jedoch oft übersehen. Die vorherige Frage wirft drei Forschungsfragen auf. Erstens ist ein sauberer Validierungssatz für WSL erforderlich oder kann man einen lauten Validierungssatz stattdessen verwenden? Zweitens, wenn ein sauberer Datensatz erforderlich ist oder wenn ein sauberer Datensatz für WSL erforderlich ist, dann wie viele saubere Beispiele benötigen wir? Schließlich sollten wir nur die sauberen Beispiele für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angegangen und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass die jüngsten WSL-Methoden tatsächlich einen sauberen Validierungssatz benötigen, um richtig zu funktionieren. Ansonsten gibt es einen großen Leistungsabfall. Wie im Bild gezeigt, wenn es keinen sauberen Validierungssatz gibt, dann können die trainierten Modelle nicht über die ursprünglichen schwachen Kennzeichnungen hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich einen sauberen Datenpunkt benötigen, um richtig zu funktionieren, und der Kosten für die Erlangung von sauberen Validierungssätzen sollte nicht übersehen werden. Unsere zweite Feststellung ist, dass die Zunahme der Anzahl der sauberen Validierungssätze WSL-Ansätze dazu bringt, eine bessere Leistung zu erreichen, wie im Bild auf der linken Seite gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erreichen. Aber das ist nicht das Ende der Geschichte, denn wenn wir entweder entscheiden, dass wir saubere Beispiele haben, dann kann das Training darauf direkt führen, eine bessere Leistung zu erreichen. Das Bild auf der rechten Seite zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur zur Validierung verwenden. Wie wir sehen, wenn wir 10 Beispiele pro Klasse haben, beginnen die direkten Fine-Tuning-Ansätze, WSL-Ansätze zu übertreffen. Schließlich kann die Leistungsverbesserung in den vorherigen WSL-Ansätzen leicht durch die Erlaubnis erreicht werden, Fine-Tuning auf den sauberen Validierungssätzen fortzusetzen. Wie wir aus den Bildern sehen, ist das Vanilla-Modell, FTw, anfangs weniger gut als andere, komplexere WSL-Ansätze, wie COSINE. Aber wenn wir Fine-Tuning auf den sauberen Beispielen fortsetzen, dann erreicht FTw die gleiche Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Ansätze zu wählen, die mehr Rechenzeit und Speicherplatz benötigen. Insgesamt haben wir gezeigt, dass die jüngsten WSL-Ansätze einen sauberen, manuell annotierten Datensatz benötigen, um richtig zu funktionieren. Ihre Leistung und Praktikabilität werden stark übertrieben. Unsere konkreten Empfehlungen für zukünftige Arbeit sind wie folgt. Erstens sollten die Auswahlkriterien für das Modell gemeldet werden. Zum Beispiel sollte gemeldet werden, ob die Auswahl des Modells über den sauberen Validierungssatz erfolgt. Zweitens sollten WSL-Ansätze mit Few-Shot-Learning-Baselines verglichen werden, da beide auf sauberen Beispielen arbeiten. Drittens ist kontinuierliches Fine-Tuning ein einfacher, aber starker Baseline, der in zukünftigen Arbeiten in WSL berücksichtigt werden sollte. Schließlich haben wir unser Code geöffnet. Sie können ihn über den QR-Code auf dieser Slides finden. Bitte fühlen Sie sich frei, ihn auszuprobieren. Vielen Dank und genießen Sie die Konferenz.</sample>
    <sample id="241">Das Papier "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" beschreibt ein Framework für die Entwicklung von Systemen zur Frühwarnung von Fehlinformationen auf sozialen Medien. Die Autoren argumentieren, dass bestehende Ansätze oft unrealistisch evaluiert werden und nicht menschenzentriert sind. Sie schlagen vor, ein Framework für die Entwicklung von Systemen vorzuschlagen, die die Komplexität der sozialen Medien und die Rolle von Menschen bei der Überwachung von Fehlinformationen berücksichtigen.

Das Framework besteht aus zwei Hauptkomponenten: der Erkennung von irreführenden Ansprüchen und der Überprüfung von Verstößen gegen die Richtlinien der sozialen Medien. Die erste Komponente verwendet ein T5-Modell für die Extraktion von Ansprüchen und eine BERT-basierte Stance-Klassifikation für die Überprüfung der Einstellung des Autors. Die zweite Komponente verwendet eine BERT-basierte Klassifikation, um die Einstellung des Autors zu bestimmen und die Tweets zu flaggen, die von Menschen überprüft werden müssen.

Die Autoren evaluierten ihre Framework mit einer Fallstudie über COVID-19-Behandlungen und fanden heraus, dass ihr System 65% der Fälle von Richtlinienverstößen genau detektierte. Sie berechneten auch die menschliche Arbeitslast und fanden heraus, dass ihr System 124,2 Fälle von Richtlinienverstößen pro menschlicher Arbeitsstunde detektierte. Das Papier schließt mit der Hoffnung, dass ihre Arbeit die Entwicklung von Zukunftssystemen zur Frühwarnung von Fehlinformationen anregt.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind unter anderem:

1. Human Evaluation: Durch die Bewertung von Menschen, die zwei Konversationen vergleichen oder auf einer Likert-Skala bewerten.
2. Likert-Ratings auf der Turn-Ebene: Die Bewertung von einzelnen Antworten auf einer Likert-Skala.
3. Likert-Ratings auf der Dialog-Ebene: Die Bewertung der Gesamtkonversation auf einer Likert-Skala.
4. Dialog-Ebene Pairwise-Vergleiche: Der Vergleich von zwei Konversationen, um zu bestimmen, welche besser ist.</sample>
    <sample id="243">Es sind 5 Autoren an der Arbeit beteiligt: Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap und der Autor selbst (wobei letzterer nicht explizit erwähnt wird).</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass "Judges decide cases in law courts", um den Pronomen "he" zu korrekt identifizieren.</sample>
    <sample id="245">Lining Zhang und ihre Kollegen präsentieren ihre Arbeit "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization". Das Ziel dieser Studie ist es, eine effiziente Methode zu entwickeln, um hochqualifizierte Annotatoren auf der Plattform Amazon Mechanical Turk (MTurk) zu finden.

Die Forscher entwickelten ein zwei-stufiges Pipeline-Modell, um hochagierende Annotatoren zu identifizieren. Im ersten Schritt werden die Qualifikationseinstellungen für die Annotatoren festgelegt, darunter Standort, Anzahl der abgeschlossenen Aufgaben und Anzahl der abgeschlossenen Aufgaben mit einer bestimmten Qualität. Im zweiten Schritt werden zwei Qualifikationsaufgaben durchgeführt: eine "Qualifikationsaufgabe" und eine "Haltbarkeitsaufgabe". Die erste Aufgabe testet die Fähigkeit der Annotatoren, mehrere Dimensionen korrekt zu bewerten, während die zweite Aufgabe die Fähigkeit der Annotatoren testet, einen hohen Arbeitslast zu bewältigen.

Die Ergebnisse zeigen, dass 26 Annotatoren auf MTurk, davon 8 Gold- und 18 Silber-Annotatoren, die Qualifikationsaufgabe bestanden. Im zweiten Schritt wurden 12 Annotatoren ausgewählt, davon 4 Gold- und 8 Silber-Annotatoren, die Haltbarkeitsaufgabe bestanden. Die Ergebnisse zeigen, dass diese Annotatoren eine hohe Übereinstimmung (IAA) erreichen, die besser ist als die von Experten.

Die Forscher vergleichen ihre Ergebnisse mit den Ergebnissen von Baseline-Annotatoren und CloudResearch-Annotatoren. Die Ergebnisse zeigen, dass die Pipeline-Annotatoren ähnliche Qualität wie CloudResearch-Annotatoren erreichen, aber mit einem höheren Aufnahmeprozentsatz. Die Forscher schließen, dass ihre Pipeline-Methodik eine effiziente Möglichkeit bietet, hochagierende Annotatoren auf MTurk zu finden und dass sie als Best Practice für die Annotation von Texten auf einer großen Skala und zu einem niedrigeren Preis dienen kann.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Er kann auf GitHub abgerufen werden.</sample>
    <sample id="247">Die Forscher aus KAIST AI haben ein neues Konzept für die Tatsachenprüfung vorgestellt, das sich auf Kenntnissgraphen (Knowledge Graphs) stützt. Die bestehenden Datenbanken für Tatsachenprüfung, wie FEVER und TabFact, nutzen Texte oder Tabellen als Beweise, während die neue Datenbank FactKG Kenntnissgraphen verwendet. Die Forscher argumentieren, dass Kenntnissgraphen eine zuverlässige Methode für die Tatsachenprüfung bieten, da die Beweise intuitiv sind und direkt auf die Ansprüche verweisen.

Die Forscher haben ein neues Dataset FactKG erstellt, das auf der Kenntnissgraph-Datenbank DBpedia basiert. Die Ansprüche in der Datenbank sind in zwei Stilen vorhanden: geschrieben und kolloquial. Die Ansprüche können in fünf Arten von Argumentationsarten eingeteilt werden: Einhopf, Konjunktion, Existenz, Mehrhopschließen und Negation.

Die Forscher haben auch mehrere Baseline-Modelle erstellt, um die Leistung der neuen Datenbank zu bewerten. Die Ergebnisse zeigen, dass die Modelle, die auf den Kenntnissgraphen basieren, besser abschneiden als die Modelle, die nur auf den Ansprüchen basieren. Die Forscher hoffen, dass ihre Arbeit die Entwicklung von Systemen zur Tatsachenprüfung unterstützen wird, die auf Kenntnissgraphen basieren.

Die Forscher haben ihre Datenbank und ihre Ergebnisse öffentlich zugänglich gemacht, damit andere Forscher ihre Arbeit weiterentwickeln können. Sie laden auch alle Interessierten ein, Kontakt aufzunehmen, um ihre Arbeit zu diskutieren.</sample>
    <sample id="248">Ja, die Annotatoren für NLPositionality sind in Bezug auf jede demographische Gruppe, d.h. Land, Geschlecht usw., ausgewogen. Die Studie hat über 16.000 Annotationen von über 1000 Annotatoren aus 87 Ländern gesammelt.</sample>
    <sample id="249">Die Sätze innerhalb der akzeptablen Domain wurden durcheinander gebracht, indem "Noise" (Rauschen) hinzugefügt wurde, um die relevanten Strukturen zu erhalten.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Bewertung von mehreren Aspekten oder Dimensionen einer Konversation, wie zum Beispiel Relevanz, Widersprüche, Irrelevanz, Halluzinationen, Empathie usw. Diese Art der Bewertung ermöglicht es, die Stärken und Schwächen eines Chatmodells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="251">Die Universität, an der die Autoren der Paper angehören, ist die Universität der Wissenschaft und Technologie in China (University of Science and Technology of China).</sample>
    <sample id="252">Sai Kiran Tanikella, ein Masterstudent an der IIT Kanpur, präsentiert gemeinsam mit Abhinav Joshi, Akshat Sharma und Ashutosh Modi ihre Arbeit "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion". Das Projekt zielt darauf ab, relevantes vorheriges Urteil zu finden, das in einem juristischen Dokument zitiert wird. Die Aufgabe des Prior Case Retrieval ist in der Juristischen Domäne von großer Bedeutung, da die Anzahl der Fälle zunimmt.

Die Forscher haben zwei wichtige Beiträge zum Feld des Prior Case Retrieval geleistet: das IL-PCR-Dataset und den U-CREAT-Pipeline. Das IL-PCR-Dataset ist ein neuer Benchmark für PCR-Aufgaben und enthält 7.070 juristische Fälle mit durchschnittlich 6,775 Zitierungen pro Abfrage-Dokument. Der U-CREAT-Pipeline verwendet unsupervisierte Lernalgorithmen und eine Ereignis-basierte Ansatz für PCR-Aufgaben und zeigt eine hohe Rückgewinnungsleistung, eine geringe Inferenceszeit und eine allgemeine Anwendbarkeit in der indischen und kanadischen Rechtssystematik.

Das Ereignis-Extraktion spielt eine entscheidende Rolle in der Arbeit. Die Forscher haben ein Ereignis-basiertes Modell vorgestellt, das Ereignisse aus juristischen Dokumenten extrahiert und verwendet, um eine Interaktionsmatrix zwischen Abfrage- und Kandidaten-Ereignissen zu erstellen. Diese Matrix wird dann in verschiedenen Retrieval-Modellen verwendet, um eine Rangfolge der Kandidaten zu erstellen.

Die Forscher haben verschiedene Modelle getestet, um ihre Leistung auf der PCR-Aufgabe zu validieren und zu vergleichen. Die Ergebnisse zeigen, dass die Ereignis-basierten Modelle eine höhere Rückgewinnungsleistung und eine geringere Inferenceszeit als die anderen Modelle aufweisen. Das Ereignis-basierte Dokumenten-Filter-Modell zeigt die beste Leistung und übertrifft die anderen Modelle um einiges.</sample>
    <sample id="253">Mario Ezra Aragón präsentiert den Forschungsbeitrag "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media". Das Ziel des Projekts ist es, ein Modell zu entwickeln, das automatisch mentale Gesundheitszustände in sozialen Medien erkennen kann. Hierfür wird das BERT-Modell verwendet und an die spezifischen Domänen von Reddit und mentaler Gesundheit angepasst.

Das Konzept des Domain-Adaptierens wird verwendet, um das Modell an die spezifischen Bedürfnisse der mentalen Gesundheit anzupassen. Dabei wird das Modell zunächst auf soziale Medien und dann auf die mentale Gesundheit spezialisiert. Die Ergebnisse zeigen, dass das DisorBERT-Modell eine gute Balance zwischen Genauigkeit und Recall erreicht, während andere Modelle entweder eine hohe Genauigkeit oder einen hohen Recall, aber nicht beide auf einmal.

Die Ergebnisse zeigen auch, dass das DisorBERT-Modell in der Lage ist, wichtige Wörter in Texten zu erkennen, die mit mentalen Gesundheitszuständen zusammenhängen. Die Ergebnisse werden durch die Visualisierung von Texten und die Gewichtung von Wörtern durch ihre Häufigkeit unterstützt.

Zusammenfassend zeigt der Beitrag, dass das DisorBERT-Modell eine effektive Methode zur Erkennung von Zeichen von mentalen Gesundheitszuständen in sozialen Medien darstellt. Die Ergebnisse sind besser als die von MentalBERT, einem Modell, das mit einer großen Menge an Daten trainiert wurde. In der Zukunft soll das Modell weiterentwickelt werden, indem verschiedene lexikalische Ressourcen und klinische Daten verwendet werden.</sample>
    <sample id="254">Die Forscher Sun Qi von der Nanjing University of Science and Technology präsentieren ein neues Framework für die Dokument-basierte Distanzbeziehungsextraktion (DocRE). Ziel ist es, Beziehungen zwischen Entitäten in einem Dokument zu extrahieren. Bisherige Methoden basieren auf großen Mengen an manuell annotierten Daten, was zeitaufwändig und kostenintensiv ist. Daher werden distanzierte überwachte Daten verwendet, um die Leistung der DocRE-Modelle zu verbessern. Diese Daten enthalten jedoch verschiedene Arten von Lärm, was die Extraktion von Beziehungen erschwert.

Um dieses Problem anzugehen, wird ein Framework vorgestellt, das die Unsicherheit für die Labeleingabe bei der DS-Daten verwendet. Dabei wird zuerst ein vorbereinendes DocRE-Modell mit DS- und manuell annotierten Daten trainiert, um Pseudo-Labels zu generieren. Um die Unsicherheit bei den Pseudo-Labels zu bewerten, wird ein Verfahren zur Unsicherheitsabschätzung verwendet, das es ermöglicht, zu bestimmen, ob die Vorhersage des Modells vertrauenswürdig ist oder nicht.

Das Framework enthält auch eine Strategie zur Wiederausgabe von Labels mit dynamischen Klassenunsicherheitsgrenzwerten und eine mehrphasige Trainingsstrategie, um die Leistung zu verbessern. Die Forscher haben ihre Methode auf öffentlichen Datensätzen getestet und zeigen, dass sie bessere Ergebnisse als andere Methoden liefert. Die Hauptbeiträge des Forschungsprojekts sind:

1. Ein Framework mit Unsicherheitsleitender Labeleingabe, das die Qualität der DS-Daten verbessert.
2. Eine Methode zur Unsicherheitsabschätzung auf der Ebene der Instanz für überlappende Beziehungen.
3. Eine Strategie zur Wiederausgabe von Labels mit dynamischen Klassenunsicherheitsgrenzwerten für das Problem der langen Schwanz.
4. Eine signifikante Verbesserung der Leistung.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei:

- Zero-Shot-Promping
- Ein-Shot-Promping
- Kurzen Prompting-Strategien</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Chatmodelle evaluiert.</sample>
    <sample id="258">Chiang Cheng-Han präsentiert ein neues Werk, das untersucht, ob große Sprachmodelle (Large Language Models, LLMs) als Alternative zur menschlichen Bewertung in der natürlichen Sprachverarbeitung verwendet werden können. Die Forscher geben den LLMs Anweisungen und verwenden diese, um die Qualität von Texten zu bewerten. Sie vergleichen die Ergebnisse der LLMs mit denen menschlicher Bewertungen und stellen fest, dass einige LLMs, wie Davinci und ChatGPT, eine klare Vorliebe für menschgeschriebenen Texten zeigen.

Das Ziel der Forscher besteht darin, die Unzuverlässigkeit und die Schwierigkeit der Wiederholbarkeit menschlicher Bewertungen zu überwinden. Sie stellen fest, dass die menschlichen Bewertungen in ihrem Experiment eine Vorliebe für menschgeschriebene Texte zeigen, während einige kleinere LLMs keine bedeutsamen Vorlieben zeigen.

Die Forscher führen verschiedene Experimente durch, um die Ergebnisse der LLMs zu überprüfen und die Auswirkungen von Änderungen in den Anweisungen oder der Ausgabe der LLMs zu untersuchen. Sie stellen fest, dass die LLMs in bestimmten Fällen bedeutsame Ergebnisse liefern können und dass die Verwendung von LLMs zur Bewertung von Texten einige Vorteile gegenüber menschlicher Bewertung hat, wie z.B. die Möglichkeit, die Bewertung zu automatisieren und zu reproduzieren.</sample>
    <sample id="259">Yusen Zhang von der Penn State University hat ein Projekt namens XSemPLR vorgestellt, das sich mit der Aufgabe beschäftigt, Benutzeranfragen wie SQL und Lambda Calculus in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen zu übersetzen. Bisherige Modelle konzentrierten sich auf begrenzte Aufgaben und Sprachen, wie zum Beispiel die chinesische Sprache fehlte oder die Lambda-Calculus-Übersetzung war nicht vorhanden. 

Um dies zu beheben, hat Yusen Zhang ein einheitliches Datenbank-Set namens XSemPLR erstellt, das 9 Datenbanken in verschiedenen Domänen, 5 semantische Übersetzungsaufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien enthält. 

Um die Leistung der Modelle zu testen, haben die Forscher 6 verschiedene Einstellungen vorgesehen: Übersetzung-Test, Monolingual-Modell, Monolingual-Few-Shot, Multilingual-Modell, Cross-Lingual-Zero-Shot und Cross-Lingual-Few-Shot. 

Die Ergebnisse zeigen, dass das Encoder-Decoder-Modell die besten Ergebnisse liefert, dass die Prätrainierung auf Englisch die Leistung bei Few-Shot-Übersetzungen verbessert und dass multilinguale Sprachmodelle wie Codex und BLOOM für die semantische Übersetzungsarbeit noch nicht ausreichend sind. 

Die Forscher hoffen, dass ihre Ergebnisse dazu beitragen werden, die semantische Übersetzungsarbeit in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen zu verbessern.</sample>
    <sample id="260">Der englische Inhalt beschreibt eine Forschungsarbeit, in der ein neues Verfahren, namens "Embedding Marker", vorgestellt wird, um die Urheberrechte von Diensten, die auf großen Sprachmodellen basieren, zu schützen.

Es ist nicht möglich, die Anzahl der Autoren direkt aus dem Text zu ermitteln.</sample>
    <sample id="261">Ein guter Planer sollte schriftliche Anleitungen erstellen, die vernünftig und treu zu den Einschränkungen sind.</sample>
    <sample id="262">Die Frage nach der Anzahl der Autoren wird nicht in dem Text beantwortet.</sample>
    <sample id="263">Die Forscher haben sich mit dem Problem auseinandergesetzt, dass große Sprachmodelle bei der in-Context-Learning-Fähigkeit instabil sind. Sie haben herausgefunden, dass verschiedene Designentscheidungen, wie die Wahl und Reihenfolge der in-Context-Beispiele, Biased sind und die Vorhersagen des Modells beeinflussen. Um dies zu lösen, haben sie ein neues Typologie von Label-Biases entwickelt, das drei Arten von Label-Biases umfasst: Vanilla-Label-Bias, Context-Label-Bias und Domain-Label-Bias. Sie haben auch festgestellt, dass das Task-Corpus die Vorhersagen des Modells beeinflussen kann.

Um dies zu überwinden, haben sie eine neue Kalibrierungsmethode vorgeschlagen, die Domain-Context-Kalibrierung. Diese Methode verwendet zufällige in-Domain-Wörter aus dem Task-Corpus, um die Bias des Modells zu schätzen und dann die ursprünglichen Vorhersagen des Modells zu kalibrieren. Die Forscher haben ihre Methode auf verschiedenen Modellen und Datenbanken getestet und festgestellt, dass sie die Leistung des in-Context-Lernens verbessert. Sie haben auch festgestellt, dass die Domain-Context-Kalibrierung besonders effektiv ist, wenn das Domain-Label-Bias groß ist.</sample>
    <sample id="264">Lin Wang, ein Absolvent des Zhejiang University in China, hat eine Präsentation über sein Forschungsprojekt "TAVT: Towards Transferable Audio-Visual Text Generation" gehalten. Das Projekt zielt darauf ab, eine neue Aufgabe namens Transferable Audio-Visual Text Generation zu entwickeln, die es ermöglicht, multimodale Textgenerierungsaufgaben wie Audio-Visual Text Generation zu bewältigen, die durch schwierige Datenanmerkung und Domänenverschiebungen gekennzeichnet sind.

Lin Wangs Forschungsgruppe hat ein neues Framework entwickelt, das aus drei Komponenten besteht: einem Audio-Visual Meta-Mapper Network, einem Audio-Visual Encoder und Language Model Generator sowie einem Dual Counterfactual Contrastive Learning (DCLL). Das Framework soll es ermöglichen, ein Modell zu trainieren, das schnell an neue multimodale Domänen anpassen kann, auch mit begrenzten etikettierten Daten.

Die Ergebnisse der Experimente zeigen, dass das TAVT-Modell alle verglichenen Modelle in Bezug auf alle Metriken um einen großen Vorsprung auf beiden cross-Datasets- und cross-Domain-Einstellungen übertroffen hat. Insbesondere in den Domänen mit wenigen etikettierten Daten, wie "Kids" und "Beauty", leistete TAVT eine gute Leistung, während andere Modelle stark abnahmen.

Die Forschungsgruppe hat auch Ablationsversuche durchgeführt, um den Einfluss von Audio-Features auf die Leistung zu analysieren. Die Ergebnisse zeigen, dass das TAVT-Modell eine effiziente Lösung für die Transferable Audio-Visual Text Generation bietet und potenzielle Anwendungen in verschiedenen Bereichen wie der Computer Vision, der Sprachverarbeitung und der multimodalen Textgenerierung hat.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Es ist nicht erwähnt, welche Universität die Autoren an gehören.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, also das Fehlen von Teilen des Quellensatzes im Übersetzungsoutput.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationalen AI, erzählen. Diese Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. 

Lassen Sie uns sagen, dass Sie gerade ein Dialogmodell entwickelt haben und möchten wissen, wie gut es sich im Vergleich zum aktuellen Stand der Forschung verhält. Die übliche Praxis besteht darin, human-evaluative Methoden zu verwenden, wie zum Beispiel, indem man Menschen bitt, zwei Gespräche auszuwählen oder Gespräche mit einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesprächsqualität zu liefern, aber die Gesprächsqualität hat viele Aspekte. Deshalb möchten Sie möglicherweise mehrere Dimensionen der Gesprächsqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.

Ein Ansatz besteht darin, einfach Menschen zu bitten, mehrere Dimensionen der Gesprächsqualität zu bewerten, wie zum Beispiel die Relevanz der Modellantworten mit existierenden Vergleichs- oder Likert-Skala-Methoden. Es gibt jedoch einen genaueren und zuverlässigeren Ansatz für die dimensionale Gesprächsbewertung. Unsere Methode versucht, die Subjektivität der humanen Bewertung zu reduzieren, indem sie explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel, wenn das Modell irrelevantes Material oder sich selbst widerspricht. Wir nennen diesen Ansatz Annotieren von Verhaltensweisen in Gesprächen oder ABC-Eval für die Kurzform.

Wir haben diese Methode entwickelt, um eine umfassende Abdeckung von Verhaltensweisen in Gesprächen zu erreichen, die in der jüngsten Literatur als Gesprächsqualität beeinflussend empfohlen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Gesprächsmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Gesprächsmodell seine Partner ignoriert oder irrelevantes Material sagt, sich selbst widerspricht oder widerspricht, falsche Fakten erfindet oder gegen das allgemeine Wissen verstößt und wenn das Modell Erfolg oder Misserfolg bei der Darstellung von Empathie zeigt.

Um zu bestimmen, welcher Bewertungsansatz am wirksamsten ist, haben wir vier state-of-the-Art-Gesprächsmodelle ausgewählt und sie auf 100 mensch-bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir auch diese Gespräche mit drei existierenden Methoden bewertet: Likert-Ratings auf der Runden-Ebene, Likert-Ratings auf der Gesprächsebene und Gesprächsebene-paarweiser Vergleiche. Für jede der existierenden Methoden haben wir Bewertungen für acht der am häufigsten gemessenen Aspekte der Gespräche gesammelt, da dies die Standardpraxis zur Bewertung von Gesprächsmodellen auf mehreren Dimensionen ist.

Aus unserer Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Verhaltensetiketten insgesamt zuverlässiger sind als Etiketten, die mit existierenden Methoden gesammelt wurden, gemessen an der inter-annotator-Einigung auf 100 doppelt-gelabelten Gesprächen. Darüber hinaus sind ABC-Eval-Etiketten mehr vorhersagbar für die Gesamtkonversationsqualität im Vergleich zu Metriken, die von existierenden Methoden erzeugt wurden, wie gezeigt durch diese einfache lineare Regressionsanalyse. Zum Beispiel können Sie sehen, wie das Messen des Prozentsatzes der Runden mit Selbst- und Partnerwidersprüchen 5% bzw. 10% der Konversationsqualität erklären, während der Durchschnitt der Likert-Konsistenz-Scores nur 4% oder weniger erklären.

Schließlich haben wir überprüft, ob jede Bewertungsmetrik eine einzigartige Aspekt der Gesprächsqualität erfassst, indem wir eine Schritt-für-Schritt-lineare Regressionsanalyse durchgeführt haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken mehr als 25% der Konversationsqualität erklärt und wenn Sie die Metriken nacheinander entfernen, resultiert dies in einem erheblichen Verlust an Informationen über die Qualität. Im Gegensatz dazu erklärt die Kombination aller Runden-Ebene-Likert-Metriken viel weniger der Qualität und weniger dieser Metriken tragen einzigartige Informationen bei.

Diese zuverlässigen, informierenden und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationale AI mit einer höheren Auflösung als vorherige Methoden zu bewerten. Sie können sehen, dass in den Ergebnissen unserer Experimente noch einige Herausforderungen verbleiben und präzise quantifiziert wurden. Zum Beispiel begehen die von uns getesteten Bots in etwa 20% ihrer Antworten Verstöße gegen das allgemeine Wissen, produzieren in etwa 15% der Antworten irrelevantes Material und widersprechen sich selbst oder ihrem Partner in etwa 10% der Zeit.

Mit dem schnellen Tempo der Verbesserung in diesem Feld könnten viele dieser Fehlerraten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Es ist jedoch umso wichtiger, zuverlässige und genaue Bewertungsmetriken für das Vergleichen von Modellen zu entwickeln. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als ein bedeutender Schritt in diese Richtung genutzt werden kann. Wir freuen uns darauf, sehen zu können, wie konversationale AI in den nächsten Monaten und Jahren vorankommt. Vielen Dank für das Zuschauen.</sample>
    <sample id="270">Die Emory NLP Lab ist Teil der Emory University.</sample>
    <sample id="271">CFT steht in dieser Arbeit für Continuous Fine-tuning, was ins Deutsche übersetzt als kontinuierliche Fine-tuning übersetzt werden kann.</sample>
    <sample id="272">Es sind 7 Autoren an der Arbeit beteiligt: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams, sowie der Sprecher Koustav Sinha.</sample>
    <sample id="273">Hallo, ich bin Kayo Yin und ich werde unsere Arbeit "When Does Translation Require Context? A Data-driven, Multilingual Exploration" (Wenn eine Übersetzung Kontext benötigt. Eine datengetriebene, multilinguale Erforschung) präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt.

Viele Übersetzungen hängen von Kontext ab. Zum Beispiel, wie würde man "Mole" in diesem Satz übersetzen? Wenn das vorherige Satz "Dinge könnten gefährlich werden, wenn die Minister davon erfahren" lautet, dann bezieht sich "Mole" auf einen Spion. Aber wenn das vorherige Satz "Könnte es etwas Ernstes sein, Doktor?", dann bezieht sich "Mole" auf eine Geburtsfehlstelle. Abhängig vom Kontext ändert sich also der Bedeutung des Wortes und daher auch seine Übersetzung. Allerdings ist es schwierig, zu bewerten, wie gut Modelle solche Fälle übersetzen können. Zunächst liegt nur ein kleiner Teil der Übersetzungen von Kontext abhängig, was bedeutet, dass corpus-basierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Einige Menschen haben vorgeschlagen, gezielte Bewertungen auf Kontext-abhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von Kontext-abhängigen Übersetzungen und begrenzte Sprachen, da sie normalerweise auf Fachwissen und menschliche Kuratierung angewiesen sind.

In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten: Erstens, wann benötigt eine Übersetzung Kontext? Und zweitens, wie gut können Modelle diese Fälle übersetzen? Um die erste Frage zu beantworten, haben wir begonnen, indem wir messen, wie viel ein Wort von Kontext während der Übersetzung abhängt. In unserer vorherigen Arbeit haben wir CXMI als Maß für den Kontextgebrauch durch maschinelle Übersetzungsmodelle eingeführt. Dies wird erreicht, indem die Information, die der Kontext C über das Ziel Y gibt, gegeben die Quelle X, gemessen wird. Man kann CXMI als die Information denken, die durch die Bereitstellung von Kontext für das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI auf Pointwise CXMI, das Kontextgebrauch auf Satzebene oder auf Wortebene messen kann. Wörter mit hoher P-CXMI können als solche betrachtet werden, die für die Übersetzung Kontext benötigen.

Wir analysieren nun Wörter mit hoher P-CXMI, um Muster zwischen diesen Wörtern zu finden. Wir führen unsere Analyse auf Transkripte von TED-Vorträgen durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zunächst betrachten wir die Teile von Sätzen, die hohe mittlere P-CXMI aufweisen. Dies ermöglicht es uns, beispielsweise Dualpronomen in Arabisch zu finden, die eine relativ hohe P-CXMI aufweisen. Dies kann erklärt werden, weil Englisch keine Dualpronomen hat, daher benötigt man Kontext, um zu bestimmen, ob ein Pronomen Dual ist, wenn man es in Arabisch übersetzt. Ebenso finden wir heraus, dass bestimmte Sprachen auch Kontext benötigen, wenn man die richtige Verbform wählen möchte.

Dann betrachten wir Vokabularitems, die eine hohe P-CXMI aufweisen, wenn man sie über alle ihrer verschiedenen Vorkommen durchschnittlich nimmt. Dies hilft uns, Fälle wie den hier zu identifizieren, wo in Chinesisch Kontext benötigt wird, um Eigennamen richtig zu übersetzen, damit man im Dokument die gleiche Übersetzung verwendet. Ebenso finden wir heraus, dass Kontext wichtig ist, um die richtige Formel zu übersetzen.

Schließlich betrachten wir verschiedene einzelne Token, die hohe P-CXMI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern vielmehr durch die Satzstruktur ausgedrückt werden, wie zum Beispiel Ellipsenresolution.

Jetzt verwenden wir unsere Ergebnisse aus der Analyse, um einen Benchmark für Dokumentenübersetzung zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, die automatisch Wörter identifizieren, die sich auf das Phänomen beziehen. Wir nennen unseren Tagger Multilingual Discourse-Aware (MuDA-Tagger).

Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Proportionen dieser Diskursphänomene aufweisen. Wir verwenden den MuDA-Tagger, indem wir ihn auf ein paralleles Korpus anwenden, das wir für die Bewertung verwenden möchten, und wir wenden unsere bevorzugten Übersetzungs-Metriken auf die kontext-abhängigen Beispiele an, die der MuDA-Tagger identifiziert hat.

Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokumentenübersetzung zu bewerten. Zunächst verwenden wir corpus-basierte Metriken: Wenn wir BLEU verwenden, finden wir heraus, dass kontext-agnostische Modelle die beste Leistung zeigen. Aber wenn wir COMET verwenden, finden wir heraus, dass kontext-bewusste Modelle die beste Leistung zeigen. Und wenn wir das Wort-f-Maß verwenden, zeigen Modelle mit und ohne Kontext vergleichbare Leistung. Dies zeigt wieder einmal, dass es schwierig ist, das beste Dokumentenübersetzungs-System zu bestimmen, wenn man allein corpus-basierte Metriken verwendet.

Wir verwenden den MuDA-Benchmark, um Modelle zu bewerten, und finden heraus, dass kontext-bewusste Modelle signifikant genauere Ergebnisse liefern als Modelle, die keinen Kontext verwenden, insbesondere bei Diskursphänomenen wie Formel und lexikaler Kohäsion. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext verwenden, bei Phänomenen wie Ellipsen, Pronomen und Verbform.

Zusammenfassend: Wir führen eine datengetriebene Analyse durch, die sich auf 14 Sprachpaare erstreckt, um herauszufinden, wann Übersetzungen Kontext benötigen, und verwenden dann unsere Ergebnisse, um einen Benchmark für Dokumentenübersetzung zu erstellen, der uns hilft, herauszufinden, welche Diskursphänomene Modelle gut oder schlecht handhaben und welche Übersetzungs-Systeme gut auf Dokumentenübersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Bis Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya und Vignesh präsentieren ihre Arbeit "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". Sie haben sich auf die Bewertung von Übersetzungen in die indischen Sprachen konzentriert, da bisher nur Übersetzungen in die englische Sprache untersucht wurden. Sie haben ein Dataset mit 7.000 Beispielen aus fünf indischen Sprachen erstellt und es von sieben Übersetzungsmodellen übersetzt. Die Übersetzungen wurden von bilingualen Experten bewertet, die Fehler markierten und eine Gesamtpunktzahl vergeben.

Die Ergebnisse zeigen, dass die Übersetzungsmodelle NLLB und Indic Trans besser abschneiden als ältere Modelle wie CVIT. Die Bewertung von Übersetzungen in die indischen Sprachen zeigt, dass die Metrik COMET die höchste Korrelation mit den Bewertungen der Experten aufweist. Die Ergebnisse sind jedoch gemischt, da einige Metriken eine hohe Korrelation aufweisen, aber auch eine starre Verteilung von Punkten haben.

Ananya und Vignesh haben auch die Robustheit von IndicCOMET getestet und gefunden, dass es eine höhere Korrelation mit den Bewertungen der Experten aufweist als COMET. Sie haben ihr Dataset öffentlich verfügbar gemacht, damit andere Forscher es nutzen können.</sample>
    <sample id="277">Die neue Methode wird im Text nicht explizit einen Namen erhalten.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Methode, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Sie basiert auf dem sociolinguistischen Konzept der "Markedness", das besagt, dass es einen unmarkierten Standard gibt, und jede Gruppe, die von diesem Standard abweicht, als markiert gilt. Die Methode verwendet die Fightin' Words-Methode, um die top-Wörter für jede markierte Gruppe zu ermitteln, indem sie die log-odds-Verhältnisse mit den unmarkierten Gruppen vergleicht.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">Die Emotionserkennung in Gesprächen (ERC) ist ein wichtiger Aspekt der Emotionsregulierung. Shi Tao präsentiert in seinem Paper "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations" eine neue Methode, um Emotionen in Gesprächen zu erkennen. Das Paper konzentriert sich auf die Integration von multimodalem Informationen, wie Text, Audio und Visuell, um die Emotionserkennung zu verbessern.

Das vorgestellte Framework namens MultiEMO besteht aus vier Komponenten: unimodales Feature-Extrahieren, Kontextmodellierung, multimodale Fusion und Emotionsklassifikation. Die drei Hauptkontributionen von MultiEMO sind:

1. Ein neues visuelles Feature-Extractor namens VisExtNet, das nur die Gesichtsausdrücke der Interaktanten und nicht die Umgebungsinformationen berücksichtigt.
2. Ein multimodales Fusion-Modell namens MultiAttn, das drei Komponenten umfasst: MultiAttn-Text, MultiAttn-Audio und MultiAttn-Visuell. Jede Komponente verwendet bidirektionale Multi-Head-Cross-Attention-Schichten, um die Modalitäten zu fusionieren.
3. Ein Sample-Weighted Focal Contrastive Loss, der mehr Gewicht auf die Minderheitsklassen legt und die Modelle dazu bringt, die semantisch ähnlichen Emotionen besser zu unterscheiden.

Die Experimente zeigen, dass MultiEMO auf den beiden Benchmark-Datasets MELD und IEMOCAP state-of-the-art-Ergebnisse erreicht, insbesondere in Minderheitsklassen und bei semantisch ähnlichen Emotionen.</sample>
    <sample id="281">Kayo Yin und ihre Kollegen haben ein Forschungsprojekt durchgeführt, um herauszufinden, wann Übersetzungen Kontext benötigen und wie gut Maschinen diese Fälle handhaben. Sie haben eine Messgröße namens CXMI entwickelt, um den Kontextbedarf von Wörtern bei der Übersetzung zu messen. Sie haben dann CXMI auf die Pointwise CXMI (P-CXMI) erweitert, um Kontextbedarf auf Satz- oder Wortebene zu messen.

Durch die Analyse von Transkripten von TED-Vorträgen, die aus dem Englischen in 14 Sprachen übersetzt wurden, haben sie herausgefunden, dass bestimmte Sprachen Kontext benötigen, um bestimmte grammatische Strukturen oder Formen zu verwenden. Sie haben auch herausgefunden, dass bestimmte Sprachen Kontext benötigen, um die richtige Formalität oder die richtigen Pronomen zu verwenden.

Basierend auf ihren Ergebnissen haben sie ein Benchmark für Dokumentenübersetzung entwickelt, das die sogenannte MuDA-Tagger enthält. Mit diesem Tagger können sie automatisch Wörter identifizieren, die bestimmte Diskursphänomene aufweisen, wie z.B. Formalität oder Lexikalische Kohäsion.

Die Ergebnisse ihrer Studie zeigen, dass Kontextbewusste Modelle bei bestimmten Diskursphänomenen wie Formalität und Lexikalischer Kohäsion besser abschneiden als Modelle, die keinen Kontext verwenden. Allerdings sind Kontextbewusste Modelle nicht immer besser als Modelle, die keinen Kontext verwenden. Die Ergebnisse ihrer Studie haben auch gezeigt, dass bestimmte kommerzielle Übersetzungssysteme wie DeepL besser sind als Google Translate bei Dokumentenübersetzung.</sample>
    <sample id="282">Title: StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Abstract:

StoryTrans is a novel approach to non-parallel text style transfer, addressing the challenge of imitating author style at the discourse level. Traditional studies have focused on token or sentence level, whereas StoryTrans operates at the story level, capturing author linguistic preferences such as discourse structures. The primary challenge lies in imitating style-specific content, which is highly associated with specific writing topics. To alleviate this issue, StoryTrans proposes a generation model that learns discourse representations and combines them with learnable style embeddings. A new training objective is designed to reduce stylistic features and enhance content preservation through a two-stage generation process. Extensive experiments on collected Chinese and English datasets demonstrate the efficiency of StoryTrans, outperforming strong baselines in terms of style control and content preservation. Style visualization and case studies show that StoryTrans can effectively transfer styles while maintaining source semantics, making it a valuable tool for natural language generation.</sample>
    <sample id="283">Lisa</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented his paper "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference 4,915. The current span-based UIE models rely heavily on precise boundary positions of annotated spans, which can be ambiguous. To address this issue, Peng proposed a fuzzy span mechanism, where the span boundary learned by the module is fuzzy instead of precise.

Additionally, Peng noted that there is a mismatch between transformer feature extraction and information extraction. Basic Transformers focus on global features, ignoring the prior hypothesis that spans have limited lengths. To address this, Peng proposed an adaptive attention mechanism that models the furthest span boundary as a continuous distribution of correct probabilities.

The proposed model, FSUIE, uses a fuzzy span loss function that calculates Binary Cross Entropy with the golden boundary as BCE loss and adds KL-divergence between the predicted boundary and the fuzzy span boundary. FSUIE also uses a fuzzy span attention mechanism that dynamically changes the attention span and linearly decays the attention distribution on the attention span boundary.

Experiments were conducted on three main information extraction tasks: named entity recognition, relationship extraction, and aspect sentiment triplet extraction. The results showed that FSUIE achieved significant performance improvements compared to UIE-base without a fuzzy span mechanism, especially on small-scale data sizes. FSUIE also achieved new state-of-the-art results on several datasets and demonstrated competitive performance on others. The ablation study showed that the proposed mechanisms improved convergence speed and information extraction capability.</sample>
    <sample id="285">Die Forscher von Peking University, angeführt von Mingqi Gao, haben ein neues Framework für die Bewertung von Factual Error Correction (FEC) vorgestellt. Sie argumentieren, dass die bisherige Bewertung von FEC-Modellen, insbesondere die Verwendung von FactCC und DAE, zwei Hauptfehler hat: 

1. Diese Metriken liefern nur eine globale Bewertung, die nicht genau genug ist und
2. sie verschwimmen die Grenzen zwischen der Korrektur von Fehlern in der Zusammenfassung und der direkten Erzeugung einer korrekteren Zusammenfassung ohne Fehlerkorrektur.

Um diese Probleme zu lösen, haben die Forscher ein neues Taxonomie für Factual Errors vorgestellt, das zwei Kategorien umfasst: content-based und form-based. Sie haben auch ein neues Bewertungsframework entwickelt, das auf der Basis von ERRANT, einem Bewertungsmetriken für Grammatikfehlerkorrektur, basiert. 

Die Forscher haben verschiedene FEC-Modelle in verschiedenen Trainingsmodi getestet und folgende Schlussfolgerungen gezogen:

* Die Verwendung von Referenzzusammenfassungen aus Dialogsummarisierungsdaten führt zu den besten Ergebnissen.
* Es besteht ein dringender Bedarf an einer Änderung der Bewertungsmethoden für FEC-Modelle.
* Die Kombination von human-annotierten Daten mit synthetischen Daten ist eine vielversprechende Richtung.
* Aktuelle FEC-Modelle haben Schwierigkeiten, Fehlerrkorrekturen wie Hinzufügen zu korrigieren und können auch keine Attributefehler, Modalfehler, Linkfehler usw. korrigieren.</sample>
    <sample id="286">Der Referent ist Professor Jinho Choi.</sample>
    <sample id="287">Es sind vier Autoren an der Arbeit beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">BLiMP (Branching, Linearization, and Movement Preferences) und SyntaxGym können verwendet werden, um syntaktische Phänomene zu testen.</sample>
    <sample id="290">Die Abkürzungen der Methoden, die im Video erwähnt werden, sind:

1. FTw (Vanilla-Modell)
2. COSINE (eine komplexe WSL-Methode)</sample>
    <sample id="291">Das Modell wird anhand von 11 Biomedizinischen und klinischen Downstream-Aufgaben evaluiert, darunter:

- Named Entity Recognition
- Classification
- Part-of-Speech Tagging
- Fragebeantwortung</sample>
    <sample id="294">Ich habe keine Informationen darüber, mit welchen Daten CamemBERT ursprünglich trainiert wurde.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile präsentiert eine Studie, die gemeinsam mit der Universität Turin und Amazon Alexa durchgeführt wurde. Das Ziel war es, ein Corpus zu erstellen, das sich mit der Detektion von Ironie in natürlicher Sprache beschäftigt. Die Studie zeigt, dass die Annahme, dass es eine eindeutige Wahrheit (Ground Truth) gibt, die durch die Annotationen erreicht werden kann, Grenzen hat. Daher entwickelten die Forscher ein Corpus namens EPIC (English Perspectivist Irony Corpus), das aus 300 kurzen Gesprächen besteht, die aus verschiedenen Quellen wie Social Media, Reddit und Twitter stammen.

Die Gespräche wurden von 74 Annotatoren aus verschiedenen Ländern und Sprachvarietäten annotiert. Jeder Annotator erhielt 200 Texte, um die Annotationen zu überprüfen. Die Ergebnisse zeigten, dass es zwischen den Annotatoren unterschiedliche Meinungen gab, insbesondere zwischen verschiedenen Altersgruppen und geografischen Regionen.

Die Forscher entwickelten auch Modelle, die die Perspektiven der Annotatoren berücksichtigen, die sogenannten "Perspektivistischen Modelle". Diese Modelle zeigten eine höhere Zuverlässigkeit und geringere Unsicherheit im Vergleich zu den traditionellen Modellen. Die Studie zeigt auch, dass die Annotatoren aus verschiedenen Generationen und geografischen Regionen unterschiedliche Meinungen über Ironie haben.

Die Ergebnisse dieser Studie haben wichtige Implikationen für die Entwicklung von Modellen für die natürliche Sprachverarbeitung. Sie zeigen, dass die Perspektiven der Annotatoren und der Menschen, die die Daten nutzen, wichtig sind, um die Qualität der Modelle zu verbessern. Die Studie legt auch nahe, dass die Entwicklung von Modellen, die die Perspektiven der Menschen berücksichtigen, wichtig ist, um die Grenzen der traditionellen Methoden zu überwinden.</sample>
    <sample id="297">Die Forscher haben ein Projekt entwickelt, um "Hundehörner" (dogwhistles) in Sprache zu untersuchen, die eine verborgene Botschaft transportieren, die nur bestimmten Gruppen bekannt ist. Diese Botschaften können rassistische, antisemitische oder transphobe Inhalte beinhalten und werden oft verwendet, um eine bestimmte Gruppe zu diskriminieren, ohne explizit zu werden. Die Forscher haben ein Glossar mit über 340 Begriffen und Symbolen erstellt, die als Hundehörner gelten, und haben ein Typologien-Modell entwickelt, um diese Begriffe zu klassifizieren.

Ein Beispiel für ein Hundehörnchen ist der Begriff "kosmopolitisch", der in einem Speech von Senator Josh Hawley verwendet wurde, um eine bestimmte Gruppe zu diskriminieren, ohne explizit zu werden. Die Forscher haben auch ein Fallstudie über historische US-amerikanische politische Reden durchgeführt und gefunden, dass die Häufigkeit von Hundehörnchen in diesen Reden eng mit dem republikanischen Southern Strategy zusammenhängt.

Außerdem haben die Forscher untersucht, wie Sprachmodelle wie GPT-3 Hundehörnchen erkennen und interpretieren. Sie fanden heraus, dass GPT-3 in der Lage ist, viele Hundehörnchen zu erkennen, insbesondere solche, die in einem formalen Register verwendet werden. Allerdings war die Leistung von GPT-3 sehr variabel und es fiel besonders schwer, Hundehörnchen in sozialen Medien und in einem informellen Register zu erkennen.

Die Forscher haben auch gezeigt, dass Hundehörnchen Inhalte, die als toxisch oder hassend gelten, in den Augen von automatischen Inhaltsfiltern weniger toxisch erscheinen lassen, wenn sie mit Hundehörnchen ersetzt werden. Dies zeigt, wie wichtig es ist, Hundehörnchen zu erkennen und zu verstehen, um Inhalte, die als diskriminierend gelten, zu erkennen und zu entfernen.</sample>
    <sample id="298">Die Ergebnisse, die zu der Schlussfolgerung führten, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war, sind die folgenden Experimente:

- Retrain oder fortsetzen Sie das Vortrainieren einiger Modelle mit neueren Daten.
- Die Leistung degradierte mit größerem zeitlichen Abstand zwischen der Trainings- und der Testdaten.

Diese Ergebnisse bestätigten die Hypothese, dass die zeitliche Verzögerung (temporal drift) die Hauptursache für den Leistungsverlust ist.</sample>
    <sample id="299">The speaker, Michalis Korakakis, discusses the limitations of current NLI models, which have achieved state-of-the-art results but rely on shortcuts, such as spurious correlations between input attributes and labels. These shortcuts lead to poor out-of-distribution performance. Existing shortcut mitigation methods require domain- and dataset-specific knowledge, assume the learner will exploit the same shortcuts as the auxiliary, and incur additional computational overhead.

To address these limitations, the speaker proposes a minimax training method that reduces the reliance of NLI models on shortcuts and improves their out-of-distribution performance. The key insight is that NLI models suffer from poor performance on under-represented "hard" training instances that contradict the shortcuts in dominant "easy" examples. The proposed method computes an example weight distribution that places emphasis on these hard examples.

The minimax training objective involves a learner and an auxiliary model. The learner minimizes the loss of the NLI task, while the auxiliary maximizes the learner's loss by generating example weights that incentivize the learner to focus on ranges of the input space where it incurs high losses. Both models are optimized in an alternating fashion using standard optimization algorithms.

The proposed method does not make assumptions about the type of shortcuts contained in a dataset and relies on the learner's own training dynamics to generate example weights. The speaker evaluates their method on three commonly used analytic datasets and observes that it consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">Belinda präsentiert ein Projekt, das sich mit dem Task der interaktiven Diktation beschäftigt. Dabei soll es möglich sein, Dokumente mit der Stimme zu diktierten und gleichzeitig zu bearbeiten. Das System soll in Echtzeit reagieren und die Änderungen anwenden, ohne dass der Benutzer die Änderungen manuell durchführen muss.

Das Projekt wurde von Semantic Machines in Zusammenarbeit mit Jason Eisner, Adam Pauls und Sam Thomson durchgeführt. Die Arbeit besteht aus drei Teilen: Der Formalisierung des Tasks, der Erstellung eines Datenkollektionsinterfaces und der Entwicklung eines Basissystems.

Das Basissystem besteht aus vier Schritten: Die ASR-Erkennung, die Segmentierung des Sprachtranskripts, die Extraktion und Normalisierung der Befehle und die Ausführung der Befehle und Diktate. Die Arbeit enthält auch eine Beschreibung der Datenkollektion und der Bewertung des Basissystems.

Die Ergebnisse zeigen, dass das System in der Lage ist, die Befehle und Diktate zu erkennen und zu bearbeiten. Es gibt jedoch auch einen Trade-off zwischen Genauigkeit und Laufzeit. Die GPT-3-Modelle sind genauer, aber langsamer, während die T5-Modelle effizienter sind, aber weniger genau.

Das Projekt ist ein wichtiger Schritt zur Entwicklung von Systemen, die es ermöglichen, Dokumente mit der Stimme zu diktierten und zu bearbeiten. Es bietet auch eine Plattform für weitere Forschung und Entwicklung in diesem Bereich.</sample>
    <sample id="302">Die Token für die Ausgabesequenz müssen permutiert werden, weil sie nach dem ersten Schritt des Modells noch nicht in der richtigen Reihenfolge vorhanden sind.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, weil sie nicht genau wissen, ob positive Stereotypen auf eine übermäßige Werteverbindung oder auf andere anti-stereotypische Methoden zurückzuführen sind. Ohne Transparenz können sie diese Fragen nicht beantworten und die Methoden nicht weiterentwickeln.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind solche, die von der Sprachmodellbewertung als ungrammatisch oder unakzeptabel eingestuft werden.</sample>
    <sample id="305">Dawei, ein PhD-Student an der Saarland University in Deutschland, präsentiert in diesem Video seine jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Gemeinsam mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow hat er ein Konzept entwickelt, das als "Weakly Supervised Learning" (WSL) bezeichnet wird.

In WSL werden Daten nicht manuell markiert, sondern mit Hilfe von schwachen Markierungsquellen wie einfachen Heuristiken, Wissensbasen oder low-quality Crowdsourcing. Diese Markierungen sind zwar günstiger als menschliche Annotationen, aber auch fehlerhaft. Wenn man Neural Networks direkt auf dieser Daten trainiert, tendieren sie dazu, die Label-Nois zu memorieren und nicht zu generalisieren.

Dawei und seine Kollegen stellen fest, dass die meisten WSL-Methoden eine bestimmte Anzahl an sauberen Validierungsbeispielen benötigen, um zu funktionieren. Sie finden, dass die Anzahl der sauberen Validierungsbeispielen erheblich ist und dass die meisten WSL-Methoden nicht so effektiv sind, wie sie behaupten. Sie schlagen vor, dass die WSL-Methoden mit few-shot-Learning-Baselines verglichen werden sollten und dass kontinuierliche Fine-Tuning ein einfacher und starker Baseline ist, der in der WSL-Forschung berücksichtigt werden sollte.

Zusammenfassend zeigt Dawei, dass die meisten WSL-Methoden nicht so effektiv sind, wie sie behaupten, und dass die Anzahl der sauberen Validierungsbeispielen erheblich ist. Er schlägt vor, dass die WSL-Methoden mit few-shot-Learning-Baselines verglichen werden sollten und dass kontinuierliche Fine-Tuning ein einfacher und starker Baseline ist, der in der WSL-Forschung berücksichtigt werden sollte.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim haben ein Forschungspapier vorgestellt, in dem sie die Fähigkeit von Sprachmodellen untersuchen, Entitäten in einem Diskurs zu verfolgen. Sie argumentieren, dass dies eine entscheidende Fähigkeit ist, um längere Diskurse zu verstehen. Die Autoren stellen fest, dass es bisher keine systematischen Untersuchungen gab, um herauszufinden, inwieweit große Sprachmodelle diese Fähigkeit besitzen.

Um diese Frage zu beantworten, haben sie ein Task-Design entwickelt, bei dem ein Sprachmodell die Inhalte von Boxen vorhergesagt werden muss, nachdem bestimmte Operationen auf sie durchgeführt wurden. Sie haben verschiedene Maßnahmen ergriffen, um sicherzustellen, dass das Modell nicht auf Kurzschlüsse wie Heuristiken oder Memorierung von Sequenzen zurückgreift.

Die Ergebnisse ihrer Experimente zeigen, dass die meisten Sprachmodelle, die sie getestet haben, nur die ursprüngliche Zustände wiederholen, anstatt die Entitäten tatsächlich zu verfolgen. Nur der Text-davinci-003-Modell zeigt eine nicht-triviale Verfolgungsfähigkeit. Die Autoren finden heraus, dass das Vorhandensein von Code in der Vortrainingsdaten ein wichtiger Faktor für die Entstehung dieser Fähigkeit ist.

Sie schlussfolgern, dass die Ergebnisse ihrer Studie zeigen, dass Sprachmodelle, die auf Code trainiert wurden, eine bessere Fähigkeit haben, Entitäten in einem Diskurs zu verfolgen. Sie betonen jedoch, dass es noch unklar ist, ob diese Fähigkeit in anderen Szenarien generalisieren kann.</sample>
    <sample id="307">Die Autoren haben verschiedene Bewertungsmetriken verwendet, um die Leistung ihrer Modelle zu bewerten, darunter:

* Named Entity Recognition (NER)
* Classification
* Part-of-Speech (POS) Tagging
* Frage-Antwort-Modell (Question Answering)

Diese Metriken wurden verwendet, um die Leistung der Modelle auf verschiedenen Downstream-Aufgaben zu bewerten.</sample>
    <sample id="308">Die Präsentation dreht sich um das Konzept der "Positionalität" in der NLP-Forschung. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Die Forscher behaupten, dass NLP-Daten und -Modelle bestimmte Positionalitäten widerspiegeln, indem sie die Meinungen und Urteile von Menschen aggregieren, die in bestimmten Kontexten leben.

Um dies zu untersuchen, entwickelten die Forscher das Framework NLPositionality, das es ermöglicht, die Annotierungen von Benutzern mit den Annotierungen von Daten und Modellen zu vergleichen. Die Forscher re-annotierten Daten mit einer Vielzahl von Annotatoren aus verschiedenen Demografien und verglichen diese Annotierungen mit den Vorhersagen und Labels von Modellen wie GPT-4, Delphi und Perspective API.

Die Ergebnisse zeigten, dass NLP-Daten und -Modelle tendenziell mit englischsprachigen Ländern und Menschen mit höherer Bildung übereinstimmen, aber weniger mit nicht-binären Menschen. Die Forscher empfehlen, dass NLP-Forscher mehr Aufmerksamkeit auf die Perspektiven von Menschen mit verschiedenen Hintergründen richten und dass es wichtig ist, die Designentscheidungen während des Forschungsprozesses zu dokumentieren.

Die Forscher betonen, dass inklusive NLP nicht nur darum geht, dass alle Technologien für alle funktionieren, sondern auch darum, dass NLP-Forschung mit dem Blickwinkel der Perspektivität betrieben wird. Sie empfehlen, dass NLP-Forscher sich auf die Bedürfnisse von spezifischen Gemeinschaften konzentrieren und dass sie sich für die Entwicklung von spezialisierten Daten und Modellen einsetzen.</sample>
    <sample id="309">Die Übereinstimmung zwischen den Kommentatoren wurde mit der Inter-Annotator-Agreement-Methode gemessen.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Der englische Inhalt bezieht sich auf die Präsentation des neuen Korpus "DEPLAIN" für die deutsche Textidentifikation auf Dokument- und Satzebene. Die Autoren Regina Stodden und Omar gehören wahrscheinlich der Universität Leipzig an.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks durch die Kombination von Sprache und Bildern in einer einzigen Sequenz-zu-Sequenz-Format, was eine einheitliche Verarbeitung verschiedener Eingabe- und Ausgabedatentypen ermöglicht.</sample>
    <sample id="313">Die Anzahl der Autoren ist nicht explizit angegeben. Es wird jedoch erwähnt, dass die Arbeit von der Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt wurde.</sample>
    <sample id="314">Die binäre Koordination ist eine syntaktische Struktur, bei der zwei oder mehrere Elemente (meist Subjekte, Objekte oder Adjunkte) mit einem Konjunktionswort (wie "und", "oder", "aber") verbunden werden. In der binären Koordination gibt es zwei Haupttypen: die asymmetrische Koordination und die symmetrische Koordination.

In der asymmetrischen Koordination wird ein Element als Kopf des Koordinationsbaus betrachtet, während die anderen Elemente als seine Söhne. Dies ist z.B. der Fall in der Universal-Dependency-Ansicht, bei der das erste Konjunkt den Kopf des Koordinationsbaus bildet.

In der symmetrischen Koordination hingegen wird kein einziges Element als Kopf des Koordinationsbaus betrachtet. Stattdessen werden alle Elemente gleichberechtigt als Kopf oder Sohn des Koordinationsbaus behandelt. Dies ist z.B. der Fall in der multi-headed-Ansicht, bei der alle Konjunkte Kopf des Koordinationsbaus sind.</sample>
    <sample id="315">Die Länge der in dieser Studie verwendeten Prompts ist nicht explizit angegeben. Es wird jedoch erwähnt, dass die Prompts, die an den LLMs gesendet wurden, um Personas zu generieren, inspiriert waren von einem Studie, bei dem Prompts an menschliche Teilnehmer gesendet wurden, um Rassismustereotype zu erfassen. Die genaue Länge dieser Prompts ist jedoch nicht bekannt.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das T5-Modell, wenn es auf CoScript (dem erstellten Dataset) fine-tuned wird, Scripts von höherer Qualität generieren kann als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere Modelle, wenn sie auf geeigneten Daten trainiert werden, größere Modelle in bestimmten Anwendungsbereichen überbieten können.</sample>
    <sample id="317">Peng Li von der Fudan University hat ein Forschungspapier namens "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors" vorgestellt. In diesem Papier wird die Idee vorgestellt, die Informationsextraktion als eine Struktur-zu-Struktur-Code-Generierungsaufgabe umzusetzen, anstatt wie bisher als Text-zu-Text-Aufgabe. Hierzu wird der Code-Large-Language-Model Codex verwendet, um die Informationsextraktion aus unstrukturiertem Text zu erledigen.

Die Autoren haben die Probleme der bisherigen Ansätze aufgezeigt, bei denen die Ausgaben während des Lernprozesses nicht mit den Eingaben übereinstimmen. Dazu gehören die Verwendung von pre-trained Language-Modellen wie T5 und GPT-3, die während der Lernphase in einem Text-zu-Text-Modus operieren, aber während der Vorhersagephase lineare Strukturen erzeugen müssen. Dies führt zu einer Mismatch zwischen den Ausgaben während des Lernprozesses und den Ausgaben während der Vorhersagephase.

Um dieses Problem zu lösen, haben die Autoren CodeIE vorgestellt, einen Ansatz, der die Informationsextraktion als Struktur-zu-Struktur-Code-Generierungsaufgabe umsetzt. Dazu wird ein Code-Format verwendet, das es ermöglicht, den Text in eine strukturierte Form umzuwandeln. Die Ergebnisse zeigen, dass CodeIE mit Codex und Code-Format-Prompts signifikant und konsistent besser abschneidet als die traditionellen Baseline-Modelle wie UIE und GPT-3.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und werde Ihnen unsere Arbeiten über "DrBERT: Ein robust prätrainiertes Modell in Französisch für biomedizinische und klinische Domänen" vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann stellen wir die Hauptbeiträge unseres Artikels vor. Wir führen das erste biomedizinische Modell in Französisch namens DrBERT ein, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die von der Web crawlen wurden. Wir führen auch eine Vergleichsuntersuchung mit verschiedenen Vortrainierungssettings und Datenquellen durch. Anschließend präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Abschließend ziehen wir eine Schlussfolgerung über die Experimente und geben Ihnen mehr Details darüber, wie Sie Zugriff auf diese Modelle haben können.

Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung geworden und bietet erhebliche Leistungssteigerungen gegenüber historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder anderen. Seitdem wurde dieses Modell auf viele andere Sprachen adaptiert, wie Französisch mit CamemBERT, und auch auf biomedizinische Domänen mit PubMedBERT und BioBERT und auf klinische Domänen mit ClinicalBERT, aber hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind rar und werden oft auf kontinuierliche Vortrainierung basieren, da es an in-Domain-Daten mangelt. Französisch jedoch hatte bis jetzt keine offene Quelle für biomedizinische Modelle. Wir stellen uns daher die Frage, welche Datenquellen am besten geeignet sind für eine breite Palette von Anwendungen und ob die durch Crawling gewonnenen Daten eine gute Substitution für klinische Daten sind.

Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Data-Warehouse des Nantes University Hospital stammen. Danach fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf französische Daten zu trainieren? Ist es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier von-Scratch-Modelle: Eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version von 4 GB von NACHOS; eine erste Version von ChuBERT, das ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen ist; und eine letzte Version von ChuBERT mit einer Mischung von 4 GB von NACHOS und 4 GB von klinischen Notizen. Neben dieser Vergleichsuntersuchung führen wir drei Modelle, die auf kontinuierliche Vortrainierung basieren, um den Einfluss der Vortrainierungsstrategie zu analysieren. Eines basiert auf den Gewichten von CamemBERT und wurde auf 4 GB von NACHOS trainiert. Ein anderes basiert ebenfalls auf CamemBERT, aber wurde auf die 4 GB von klinischen Notizen trainiert. Schließlich basiert eines auf dem englischen biomedizinischen Modell PubMedBERT und wurde auf 4 GB von NACHOS trainiert. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Namenserkennung, Klassifikation, Teilwort-Tags-Identifizierung und Frage-Antwort-Aufgaben. Diese Modelle werden mit sechs Baseline-Modellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass Modelle am besten auf der Aufgabe abschneiden, auf der die Daten der gleichen Natur sind wie die, auf denen das Modell trainiert wurde. Wir beobachten jedoch, dass Daten aus heterogenen Quellen mehr Vielseitigkeit aufweisen. Wir beobachten auch, dass das Verwenden mehrerer Daten zu besseren Leistungen führt. Insgesamt scheint die Vortrainierung von Grund auf höhere Leistungen auf den meisten Aufgaben zu erzielen. Unsere Experimente auf kontinuierliche Vortrainierung, die auf den Gewichten und Tokenisierung von CamemBERT trainiert wurden, zeigten jedoch vergleichbare Ergebnisse mit denen, die mit DrBERT 4 GB von Grund auf trainiert wurden. Dies ist jedoch nicht der Fall für das Modell, das auf den Gewichten und Tokenisierung von CamemBERT basiert und unter Schwierigkeiten leidet. Schließlich ziehen wir als Fazit, dass unser System auf neun der elf Downstream-Aufgaben bessere Leistungen erzielte und globale Ergebnisse des generischen Modells, hier CamemBERT, übertraf. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht skaliert werden. Alle prätrainierten Modelle, die aus NACHOS stammen, sind auf Hugging Face frei verfügbar und unter der MIT-Lizenz. Alle Trainings-Skripte sind auf unserem GitHub-Repository verfügbar. Vielen Dank für diese Präsentation, und wir freuen uns darauf, Sie bei der Poster-Sitzung in Toronto zu treffen.</sample>
    <sample id="319">Die in der Arbeit untersuchten Lernstrategien sind:

1. Von-Scratch-Vorbereitung (Vorbereitung von Grund auf)
2. Kontinuierliche Vorbereitung (Fortsetzung der Vorbereitung eines bereits existierenden Modells)
3. Kontinuierliche Vorbereitung mit Gewichten und Tokenisierung eines anderen Modells (in diesem Fall CamemBERT)</sample>
    <sample id="320">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, ist in diesem Fall nicht beobachtet worden. Dies wird durch den Graphen auf der rechten Seite unterstützt, in dem der rote Best-Fit-Linienabschnitt einen Steigungsabschnitt von mehr als 1 hat, was bedeutet, dass jede Einheit Verbesserung auf CoNLL-2003 zu mehr als einer Einheit Verbesserung auf CoNLL++ führt.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde analysiert, indem auf die Art der Vereinfachung, wie z.B. lexikalische Vereinfachung, Strukturvereinfachung und der Gesamtgrad der Vereinfachung, hingewiesen wurde.</sample>
    <sample id="322">Enrico wird auf dem ACL 23-Kongress einen Vortrag halten, in dem er die Frage "Was lernen Textklassifizierer über Moral?" beantwortet. Er beginnt mit einer Erklärung von Moralität, die als internes Kompass verstanden wird, das Menschen dabei hilft, zwischen richtig und falsch zu unterscheiden. Moralität ist die Grundlage unserer Gesellschaften und es ist wichtig, dass Sprachmodelle sie verstehen und erkennen können.

Enrico kritisiert die traditionelle Ansätze in der NLP-Komunität, die Moralität als ein einziges Maß zwischen moralisch und unmoralisch behandeln. Dies führt zu einem Verlust der Vielfalt in der Moral, da Menschen unterschiedliche Bewertungen von Konzepten wie Abtreibung oder LGBTQ-Rechten haben.

Enrico schlägt vor, die Moral Foundation Theory (MFT) zu verwenden, die besagt, dass es fünf verschiedene Wege gibt, wie Menschen Moralität wahrnehmen. Jeder Mensch priorisiert diese Grundlagen unterschiedlich, was seine Moralbewertungen beeinflusst. Enrico und sein Team haben bereits erfolgreich MFT in der NLP angewendet und zeigen, dass Sprachmodelle Moralität in Texten verstehen können.

In seinem Vortrag wird Enrico die Frage beantworten, was Sprachmodelle über Moralität lernen. Er verwendet erklärbare AI-Techniken und analysiert, wie Moralität in verschiedenen Domänen ausgedrückt wird. Enrico zeigt, dass Sprachmodelle die feinmaschigen Unterschiede in der Ausdrucksweise von Moralität in verschiedenen Domänen erkennen können, wie z.B. zwischen #AllLivesMatter und #BlackLivesMatter. Dies ist ein wichtiger Schritt, um sicherzustellen, dass Sprachmodelle Moralität in verschiedenen Kontexten richtig verstehen und erkennen können.</sample>
    <sample id="323">Yujie Wang von der Shanxi University in China hat ein Paper mit dem Titel "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" veröffentlicht. Das Commonsense QA ist ein herausforderndes Problem, bei dem Maschinen auf Fragen antworten müssen, die auf allgemeinem Wissen basieren, um ihre Sprachverständniskompetenz zu testen. Um dieses Problem zu lösen, müssen Maschinen relevante Kenntnisse aus externen Quellen abrufen. 

Einige Forscher denken, dass Kenntnisse in beiden Sprachmodellen und Wissensbasen gespeichert sind. Um Commonsense QA zu lösen, werden diese beiden Arten von Wissen kombiniert. Diese Arbeiten suchen relevante Kenntnisse in der Wissensbasis durch Entitätssuche, Erstellen eines Untergraphen und dann verwenden Sprachmodelle und GNNs, um Antworten abzuleiten. 

Allerdings führen diese Arbeiten einige ungenaue Entitäten ein, wie "Top", "Bank" und "Cat", die sich stark von der aktuellen Frage lösen. Darüber hinaus werden der Untergraph und der Text isoliert kodiert, was zu begrenzter Interaktion zwischen den beiden Modalitäten führt. 

Um diese Probleme anzugehen, wurde DHLK vorgeschlagen. Zuerst wird ein HKG auf der Grundlage mehrerer Wissensbasen erstellt, die durch eine zweistufige Prüfung und KRL optimiert werden. Schließlich wird das Sprachmodell verwendet, um die beiden Modalitäten zu kodieren und zu fuzen. 

Durch DHLK kann die Präzision von Commonsense QA verbessert werden, indem ungenaue Entitäten entfernt und die Interaktion zwischen den beiden Modalitäten gesteigert wird. Die Ergebnisse zeigen, dass DHLK bessere Ergebnisse als andere LM- und HKG-Methode liefert.</sample>
    <sample id="324">Ja, unsere Forschung zeigt, dass Sprachmodelle tatsächlich unterschiedliche politische Vorurteile haben. Wir fanden heraus, dass GPT-4 der liberalste Sprachmodell unter den getesteten ist, und dass GPT-Serie generell sozial liberaler ist als BART-Serie und ihre Varianten.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unser Papier zu "Kompositionelle Generalisierung ohne Bäume mit Multiset-Tagging und latenten Permutationen" geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tieferen Rekursion und unerfahrenen Kompositionen von Phrasen zu bewältigen, die während der Ausbildung einzeln gesehen wurden. Im Kontext der semantischen Parsen kann die Überprüfung der kompositionellen Generalisierung wie folgt aussehen. 

Wir haben wie gewohnt ein Trainingsset von Aussagen. In diesem Fall "Die Mädchen schliefen" und "Mary wusste, dass das Mädchen schlief". Diese Aussagen werden mit logischen Formen verbunden, die die wichtigsten Aspekte ihres Sinns darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernerevaluation wird das Testset nicht aus der gleichen Verteilung stammen, sondern enthält strukturschonende logische Formen. In diesem Beispiel hat der Model während der Ausbildung nur flache Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der Distribution-Überprüfung und produzieren oft Ausgaben, die sich vom Eingabewert lösen. Insbesondere produzieren sie oft Ausgaben, die nicht die systematischen Korrespondenzen zwischen Eingabe und Ausgabe wiedergeben, wie sie im Beispiel farbig hervorgehoben sind. 

Eine beliebte Methode, um dieses Problem anzugehen, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der die Aussagen mit den logischen Formen verbindet. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal eine rechenintensive Prozedur sein. Typischerweise ist dies eine formalspezifische Vorbereitung der logischen Formen erforderlich, zum Beispiel zur Behandlung von Variablenymbolem. Die Gewinnung von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren erfordern. In unserem Papier verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferen Rekursionen ohne auf Bäume angewiesen zu sein. 

Unsere Herangehensweise prädiziert die Ausgabe aus der Eingabe in zwei Schritten. Zuerst werden jedem Eingabetoken Unordnete Multisets von Token zugeordnet, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Das ist der Grund, warum wir in dem zweiten Schritt einen anderen Model verwenden, um eine Permutation vorherzusagen, die die richtige Reihenfolge bringt. Wir führen eine neue Methode zur Permutationsvorhersage ein, die keine harten Einschränkungen auf die möglichen Permutationen auflegt. Dies macht unsere Herangehensweise sehr flexibel und ausdrucksstark. 

Konzeptionell funktioniert unsere Permutationsvorhersage ungefähr wie folgt. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welcher Multiset-Token in jede Position gehört. Für die erste Ausgabeposition wählen wir einfach einen, wie in Rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um den zweiten Token in der Ausgabe zu bestimmen. Wir bestimmen den dritten Token in der Ausgabe in ähnlicher Weise, indem wir zum nächsten Multiset-Token springen. Wir setzen diesen Prozess fort, bis jeder Token aus der ersten Stufe genau einmal besucht wurde. 

Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen Modelle um einen großen Vorsprung auf der Generalisierung zu tieferen Rekursionen. Einige andere Arten der strukturalen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. 

Zunächst ist die Ausrichtung zwischen Eingabe und Ausgabe nicht in den Trainingsdaten gegeben. Als Folge davon wissen wir für einen gegebenen Token nicht, welcher Multiset er stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch richtige ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. 

Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung mit sich, dass die höchstbewertete Permutation gefunden werden muss, was NP-Schwer ist. Dies liegt daran, dass dies mit dem "Reiseverkäufer"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, die linguistisch plausiblere Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, möchten, schauen Sie bitte in unser Papier oder besuchen Sie unsere Poster.</sample>
    <sample id="326">Cognitive dissonance ist eine Situation, in der zwei Überzeugungen oder Handlungen unvereinbar sind, wie zum Beispiel, wenn jemand sagt, dass Zigaretten tödlich sein könnten, aber trotzdem Zigaretten raucht, weil er glaubt, ohne sie seinen Job nicht behalten zu können.</sample>
    <sample id="327">In dem Paper "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" wird ein neues Vision-Language-Architektur vorgestellt, das die Einschränkungen der bestehenden Architekturen überwindet. Die ManagerTower- Architektur ermöglicht es, dass mehrere unimodale Repräsentationen in jedem Cross-Modal-Schicht kombiniert werden, um die unterschiedlichen Ebenen des semantischen Wissens zu nutzen. Durch die Einführung von adaptiven Managern kann die ManagerTower-Architektur die unterschiedlichen Bedürfnisse der verschiedenen Cross-Modal-Schichten abdecken und somit eine umfassendere Cross-Modal-Alignierung und Fusion ermöglichen. Die ManagerTower-Architektur zeigt sich in verschiedenen Downstream-Aufgaben als effektiver als die bestehenden Architekturen und kann auch mit weniger Daten oder Parametern auskommen. Die Ergebnisse zeigen, dass die ManagerTower-Architektur ein wichtiger Schritt in Richtung einer umfassenden Vision-Language-Modellierung ist.</sample>
    <sample id="328">GPT-4 ist das am meisten linke Sprachmodell, wie aus den Ergebnissen der Studie hervorgeht.</sample>
    <sample id="329">Minghang Zheng and his team from Peking University presented a paper on zero-shot video sentence localization. This task involves finding the most relevant segments of a video based on a given natural language query. The team aimed to develop a method that can train video sentence localization models without manual annotations.

The existing zero-shot methods have three main drawbacks: simple pseudo-queries, unalignment between pseudo-queries and pseudo-events, and ignoring label noise. To address these issues, the team proposed a noise-resistant Structured Pseudo-Label generation method.

The method consists of three steps: generating free-form pseudo-queries using a pre-trained image caption model, generating pseudo-events based on event temporal structure, and reducing the influence of label noise. The team used a sliding window to enumerate all possible pseudo-events and selected the one with the highest event quality.

To reduce the influence of label noise, the team estimated the label noise based on the model's predicted confidence and the IoU between the prediction and the pseudo-label. They weighted the samples using these weights to reduce the contribution of noisy samples.

The team performed experiments on two datasets, ActivityNet Captions and Charades-STA, and achieved the best zero-shot performance on both datasets. They outperformed existing methods on most metrics and demonstrated the robustness of their method to label noise. The team's code is available for further research.</sample>
    <sample id="330">Ja, im Rahmen deines Papers "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" findest du heraus, dass kumulatives Training ("Cumulative") gleich oder besser als iteratives Training ("Iterative") ist, insbesondere wenn es um aktives Lernen geht.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Vorträgen, die in 14 verschiedenen Sprachen übersetzt wurden.</sample>
    <sample id="333">Wenhao von Nanjing University präsentiert das Projekt "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Das Ziel ist es, das Generalisierungsvermögen von NMT-Modellen zu verbessern. Das Problem liegt in der nicht glatten Darstellung des Wortschatzes im Modell, was zu ungenauen Übersetzungen führt. Die Lösung ist das kNN-MT-Modell, das Vorhersagen durch die Anwendung von kNN-Know-how im Darstellungsspielraum verbessert.

Das kNN-MT-Modell hat jedoch zwei Nachteile: Es ist zeitaufwändig und die Darstellung kann nicht leicht aktualisiert werden. Um diese Probleme zu lösen, wird das INK-Modell vorgestellt. Es besteht aus zwei Schritten: Das erste ist das Extrahieren von kNN-Know-how aus dem Datenspeicher, um die Darstellung anzupassen. Das zweite ist die Aktualisierung der Darstellung, um den Datenspeicher zu aktualisieren.

Das INK-Modell verwendet drei Arten von Darstellung, um die Darstellung zu verbessern. Es handelt sich um die Anpassung von Kontextualisierung und Token-Embeddings, die Anpassung von Kontextualisierung und kNN-Token-Embeddings und die Anpassung von Kontextualisierung von Ziel-Token. Das Modell wird mit einem kombinierten Lernalgorithmus trainiert und der Datenspeicher wird nach dem Training entfernt.

Die Experimente zeigen, dass das INK-Modell bessere Übersetzungsleistungen erzielt als das kNN-MT-Modell und dass die Verwendung eines Adapters und eines Datenspeichers zusätzliche Verbesserungen bringt. Das INK-Modell erzielt auch bessere Übersetzungsleistungen mit weniger Speicherplatz und schnellerem Inferencespeed.</sample>
    <sample id="335">Der Referent ist Matthias Lindemann.</sample>
    <sample id="336">Der sprachübergreifende Transfer bezieht sich auf die Fähigkeit eines Modells, die Fähigkeiten, die es in einer Sprache gelernt hat, auf eine andere Sprache übertragen zu können. In diesem Zusammenhang wird er in zwei Formen beschrieben: 

- Cross-lingual Zero-shot transfer: Hier wird das Modell auf einer Sprache trainiert und dann ohne weitere Anpassung auf eine andere Sprache übertragen.
- Cross-lingual Few-shot transfer: Hier wird das Modell auf einer Sprache trainiert und dann mit einem kleinen Datensatz (weniger als 10%) der Ziel-Sprache weiter trainiert.</sample>
    <sample id="337">Die Forscher haben ein neues Modell entwickelt, um Wörter, die nicht im Vokabular eines Sprachmodells enthalten sind (OOV-Wörter), zu verarbeiten. Das Modell basiert auf einem Graphen, der die Beziehungen zwischen Wörtern und ihre Bildung darstellt. Wenn ein OOV-Wort auftaucht, wird es in kleinere Einheiten (Wordpieces) zerlegt und mit anderen relevanten Wörtern assoziiert. Der Graph besteht aus zwei Ebenen: Die erste Ebene enthält alle Wordpieces und die zweite Ebene enthält eine Auswahl von Wordpieces, die für die Verarbeitung verwendet werden.

Das Modell verwendet Graph-Neuronale Netze (GNNs), um den Graphen zu verarbeiten. Um die Attribute der OOV-Wörter zu bestimmen, wird ein Selbst-Attention-Netzwerk verwendet. Die GNNs werden verwendet, um die Beziehungen zwischen den Wordpieces zu modellieren, und ein Readout-Block ist verwendet, um die Gesamtinformation des Graphen zu summarisieren.

Die Forscher haben ihre Methode anhand von Experimenten getestet und gezeigt, dass sie besser als Baseline-Modelle in intrinsischen und extrinsischen Aufgaben abschneidet. Die Methode kann auch auf andere Sprachen angewendet werden, vorausgesetzt, dass die Wörter rational zerlegt werden können.</sample>
    <sample id="338">Bingsheng hat eine Präsentation über das Forschungspapier "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" gehalten. Das Papier ist ein gemeinsames Werk von Forschern aus Rensselaer Polytechnic Institute, Northeastern University und IBM Research. Die Forscher haben sich mit der Frage beschäftigt, wie die Qualität von menschlichen Erklärungen evaluiert werden kann, insbesondere wenn diese subjektiv und task-abhängig sind.

Um dies zu erreichen, haben die Forscher ein vereinheitlichtes Datenformat entwickelt, das verschiedene Aufgaben in eine einheitliche Multiple-Choice-Aufgabe umwandelt. Sie haben auch ein neues Bewertungsmetriken namens TREU vorgestellt, das die Hilfeleistung von Erklärungen bei der Feinjustierung von Modellen berücksichtigt. Die Forscher haben TREU anhand von fünf großen Datensätzen und zwei Modellen (T5 und BART) getestet und gezeigt, dass es simulatabilitäts-Scores übertrifft.

Die Ergebnisse der Forscher zeigen, dass menschliche Erklärungen, auch wenn sie als niedrigwertig eingestuft werden, noch immer nützlich für die Vorhersagen von Modellen sein können. Die Forscher betonen, dass ihre Arbeit die Grundlage für hochwertige Zusammenarbeit zwischen Menschen und Maschinen bei der Annotation legt und empfehlen, ähnliche Qualitätstests in der Zukunft durchzuführen.</sample>
    <sample id="339">Die Autoren gehören an die Saarland University in Deutschland.</sample>
    <sample id="340">Kuan-Hao Huang von der UCLA präsentiert sein Werk "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation". Das Ziel des Projekts ist es, ein großes, syntaktisch vielfältiges Paraphrase-Dataset zu erstellen, das die bestehenden human-annotierten Datensätze wie MRPC, PAN und Quora übertrifft. Die Arbeit basiert auf der Idee, Abstract Meaning Representations (AMR) zu verwenden, um syntaktisch vielfältige Paraphrasen zu generieren.

Die Methode besteht darin, zunächst einen AMR-Graph eines Quellsatzes zu erstellen, indem ein vorgebildeter AMR-Parser verwendet wird. Dann wird der Fokus des Graphen geändert, indem ein zufällig gewählter Knoten als neuer Wurzelknoten gesetzt und die entsprechenden Kanten und Kantenbezeichnungen modifiziert werden. Anschließend wird ein AMR-Graph-to-Text-Generator verwendet, um Text aus dem modifizierten Graphen zu generieren.

Das Ergebnis ist ein Dataset namens ParaAMR, das etwa 15 Millionen Quellsätze enthält und etwa 6,9 Paraphrasen pro Quellsatz. Die Ergebnisse zeigen, dass ParaAMR syntaktisch vielfältiger ist als andere Datensätze, die auf Back-Translation basieren, und dass es ähnliche semantische Ähnlichkeitsscores wie andere Datensätze hat. ParaAMR wird auch in verschiedenen NLP-Anwendungen demonstriert, wie zum Beispiel der Lernung von Satzbedeutungsvektoren, syntaktischem Kontrollparaphrasengenerierung und der Verwendung von Paraphrasen für die Datenverstärkung bei wenigen-Schuss-Lernen.</sample>
    <sample id="341">Die Autoren verwenden zwei Latenzmessungen: 

1. Average lagging (durchschnittliche Verzögerung)
2. Computational aware average lagging (computational bewusste durchschnittliche Verzögerung), die auch die Modellrechenzeit berücksichtigt.</sample>
    <sample id="342">Hello everyone, my name is Gao Jingsheng. Today, I'm presenting our paper, "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming". This paper was conducted by me, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI.

The first part is the introduction. Open Domain Dialogue refers to a type of conversational exchange between a human and an artificial intelligence system that can cover a range of topics and doesn't have a specific goal. It relies on pre-trained models and large-scale datasets. The existing large-scale corpora mainly consist of online chat conversations, which can be divided into text sources and video sources. Currently, the large-scale pre-trained dialogue datasets are mostly text-sourced. Therefore, it is of great significance to construct a large-scale video-sourced dialogue dataset that is closer to real spoken conversation.

To address these barriers, we propose a large-scale personalized dialogue dataset, LiveChat, with a unique automatic dialogue-constructing method. We conduct sufficient experiments on two benchmark tasks: Response Modeling and Addressee Recognition. We further investigate transfer learning of generation models to LiveChat.

The second part is our dataset, LiveChat, which is conducted in three steps. Firstly, we scrape origin streaming videos from Chinese TikTok, Douyin. Then we extract audio from videos and transcribe audio into utterances through ASR. Secondly, we collect audience comments and construct dialogues by our reply-to-whom matching method. Thirdly, we collect the persona information for personalized dialogue generation.

The third part is our experiments. We train retrieval baselines for two tasks. The first task is response modelling. We conclude that our extracted persona and longer average sessions are beneficial to the final result. The second task is Addressee Recognition, which shows that single-stream BERT outperforms double-stream BERT, although persona is beneficial to address recognition.</sample>
    <sample id="343">Hallo, ich bin Akshatha und heute präsentiere ich gemeinsam mit meinem Co-Autor Martin unser Werk "The KITMUS Test: Evaluierung der Wissensintegration aus mehreren Quellen". Dieses Werk ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. NaturSprachverstehende Modelle ziehen sich aus verschiedenen Wissensquellen, wie z.B. dem in ihren Parametern enthaltenen Wissen, das normalerweise durch eine Vorbereitung erworben wird, und dem im Zeitpunkt der Inference bereitgestellten Wissen. In den letzten Jahren haben sich in Aufgaben wie Fragen beantworten gezeigt, dass Modelle das im Vorbereitungstempo erhaltene Wissen verwenden können, um die Aufgabe zu lösen. Allerdings erfordern NaturSprachverstehende oft Wissen, das auch im Zeitpunkt der Inference bereitgestellt wird. Zum Beispiel in der Sätze "John sah den neu gewählten Präsidenten auf dem Fernseher". Die vorausbereiteten Parameter können Informationen über die Tätigkeiten von Präsidenten und was ein Fernseher ist enthalten, können aber nicht zuverlässig wissen, wer die spezifische Instanz "John" ist oder wer der neue Präsident ist, da der Präsident sich seit der Vorbereitung geändert haben könnte. Daher erfordern erfolgreiche Modelle für Wissensintensive NLU-Aufgaben die Fähigkeit, Wissen aus verschiedenen Quellen zu integrieren und zu verwenden. In diesem Werk schlagen wir ein Diagnosetest-Kit für Wissensintegration vor. Wir stellen ein Referenzierungsaufgaben-Kit vor, das die Fähigkeit testet, auf Wissen aus verschiedenen Quellen zuzugreifen. Wir bewerten das Daten-Set mit Studienteilnehmern und etablierten Referenzierungsaufgaben-Modellen. Hier ist ein Beispiel aus unserem Daten-Set: Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich im Park. Nach einer langen Arbeitstage im Gericht, war er glücklich, sich zu entspannen. Die Aufgabe hier ist es, den richtigen Entität zu identifizieren, auf den sich der Pronomen "er" bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomen erfordert zwei Arten von Informationen. Erstens Entitäts-spezifisches Wissen wie "Servin ist ein Richter" und zweitens Hintergrundwissen wie "Richter entscheiden in Gerichten". Hintergrundwissen wird normalerweise während der Vorbereitung von großen Sprachmodellen erworben, während Entitäts-spezifisches Wissen normalerweise im Zeitpunkt der Inference bereitgestellt wird. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen, sodass sie entweder in einer Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen für KITMUS definiert. Zuerst haben wir die typische Einstellung: "Hintergrund-Vorbereitung", wo Hintergrundwissen als verfügbar im Vorbereitungstempo angenommen wird. Zweitens haben wir die Einstellung "Hintergrund-Beide", wo Hintergrundwissen in beiden Quellen verfügbar ist. Letztendlich haben wir die Einstellung "Hintergrund-Inference", wo beide Arten von Wissen nur im Zeitpunkt der Inference verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, wo das notwendige Hintergrundwissen zur Lösung einer Aufgabe nicht Teil der Vorbereitungdaten von Modellen ist. Zum Beispiel, da neue Berufe seit der Vorbereitung entwickelt wurden. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den wahren Quellen kontrollieren. In der Einstellung "Hintergrund-Vorbereitung" nehmen wir an, dass das Hintergrundwissen "Politiker suchen nach gewählten Sitzen in der Regierung" in den vorausbereiteten Parametern enthalten ist und im Zeitpunkt der Inference bereitgestellte Entitäts-spezifisches Wissen "Chichester ist ein Politiker" bereitgestellt wird. In der Einstellung "Hintergrund-Beide" bereitstellen wir nicht nur Entitäts-spezifisches, sondern auch Hintergrundwissen über Politiker im Zeitpunkt der Inference. In der Einstellung "Hintergrund-Inference" bereitstellen wir die fiktive Berufsbezeichnung "mirituer" anstatt Politiker, da "mirituer" unwahrscheinlich in den vorausbereiteten Parametern enthalten ist. Wir bewerten das Daten-Set sowohl mit Studienteilnehmern als auch mit etablierten Referenzierungsaufgaben-Modellen. In diesem Bild zeigen wir die Ergebnisse der besten Modelle im schwierigsten Varianten der Einstellung "Hintergrund-Vorbereitung". Ohne spezifische Ausbildung auf KITMUS, können beide Modelle nicht gut abschneiden. Wenn jedoch auf KITMUS trainiert, schneiden C2F und BERT4Coref jedoch besser ab als die zufällige Auswahl. Dies deutet darauf hin, dass wenn auf generische Referenzierungsaufgaben-Daten-Set trainiert wird, die meisten Modelle lernen, Oberflächenmerkmale auszunutzen, die bei der Bewertung auf KITMUS nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigen, dass selbst die besten Modelle Schwierigkeiten haben, Hintergrundwissen, das nur im Zeitpunkt der Inference bereitgestellt wird, zuverlässig zu integrieren. Zusammenfassend sind die Haupt-Ergebnisse unseres Papiers, dass viele Referenzierungsaufgaben-Modelle nicht in der Lage sind, ohne spezifische Ausbildung auf KITMUS, Wissen aus verschiedenen Quellen zu nutzen. Allerdings können einige Modelle mit spezifischer Ausbildung auf KITMUS erfolgreich Wissen aus mehreren Quellen integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, Hintergrundwissen, das nur im Zeitpunkt der Inference bereitgestellt wird, zuverlässig zu integrieren. Wenn Sie mehr Details erfahren möchten, bitte sehen Sie unser Papier und überprüfen Sie das Daten-Set und die Code auf GitHub. Vielen Dank für die Aufmerksamkeit.</sample>
    <sample id="344">Die baumbasierten Methoden haben den Nachteil, dass sie häufig eine formale Vorverarbeitung der logischen Formen erfordern, um Variable-Symbole zu handhaben, und dass der Aufbau der Bäume oft komplex und computenationale kostenintensiv ist.</sample>
    <sample id="345">In der Forschungssprache wird der Begriff "Compositional Generalization" verwendet, um die Fähigkeit eines Lernenden zu beschreiben, tiefere Rekursionen und unerforschte Kombinationen von Ausdrücken zu handhaben, die einzeln während der Trainingsphase gesehen wurden. In der Semantikparsing wird dies getestet, indem ein Trainingsdatensatz von Aussagen mit ihren entsprechenden logischen Formen verwendet wird. Im Gegensatz zu standardmäßigen Machine-Learning-Evaluierungen enthält der Testdatensatz jedoch nicht dieselbe Verteilung und enthält strukturell unerforschte logische Formen. 

Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der Generalisierung und produzieren oft Ausgaben, die vom Input abgetrennt sind. Eine populäre Methode, um dieses Problem zu lösen, besteht darin, Bäume in die Modelle zu integrieren. Bäume sollen den kompositionellen Prozess erfassen, der Aussagen mit logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume sind normalerweise nicht gegeben und müssen aufwendig ermittelt werden. 

In diesem Papier wird ein neuraler seq2seq-Model vorgestellt, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Dieser Ansatz zeigt erstmals eine starke Generalisierung zur tiefen Rekursion ohne Abhängigkeit von Bäumen. Der Ansatz besteht aus zwei Schritten. Zuerst werden die Eingabe-Token mit einem unsortierten Multiset von Tokenen versehen, die in der Ausgabe auftreten. Danach wird ein weiterer Modell verwendet, um eine Permutation vorherzusagen, um die Token in die richtige Reihenfolge zu bringen. 

Der Permutationsmodell wird eine neue Methode vorgestellt, die keine harten Einschränkungen auf die möglichen Permutationen auflegt, was den Ansatz sehr flexibel und ausdrucksstark macht. Der Ansatz wird auf dem COGS-Benchmark getestet und zeigt eine deutliche Überlegenheit gegenüber anderen baumlosen Modellen in Bezug auf Generalisierung zur tiefen Rekursion.</sample>
    <sample id="346">Es wird nicht erwähnt, welcher Universität die Autoren angehören.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Papier "Markierte Personas: Verwendung von natürlichen Sprachanweisungen, um Stereotypen in Sprachmodellen zu messen." sprechen, das in Zusammenarbeit mit Esin Durmus und Dan Jurafsky entstanden ist. In den letzten Jahren haben viele die Präsenz sozialer Voreingenommenheit und Stereotypen in großen Sprachmodellen, also LLMs, dokumentiert. Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie beruhen meist auf handgefertigten Datensätzen, die sehr zeitaufwändig zu erstellen sind und messen nur sehr spezifische Stereotypen, was sie schlecht auf andere Demografien oder Kontexte anwendbar macht. Zudem erfassen sie oft nur sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigen die meisten Arbeiten in diesem Bereich die Intersektionalität nicht, die die Idee darstellt, dass soziale Identitäten mit mehreren Facetten sich gegenseitig verstärken können und einzigartige Schwerpunkte der Schädigung darstellen. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neuen, durch Anweisungen trainierten LLMs sehr gut auf Anweisungen und Anfragen reagieren. Wir können also das Modell auffordern, eine Personas zu generieren, also eine Darstellung eines imaginären Individuums, indem wir eine Anfrage wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." stellen. Und wir können sofort erkennen, dass dies sehr allgemein anwendbar ist, da wir einfach die gewünschte Identitätsmarke in diese Anfrage einfügen können.

Hier sind einige Beispiele von GPT-4-Generationen. Sofort sehen wir, dass, während die Ausgaben nicht offen negativ oder toxisch sind, wie man es traditionell versteht, es doch einige interessante Muster gibt. Die asiatische Frau wird als zurückhaltend dargestellt; die mittelöstliche Frau wird mit Worten wie "exotisch" und "wie, beziehungsweise, auf eine faszinierende Region" bezeichnet. Und beide Frauen der Farbe machen Bezug auf ihre Abstammung, während der weiße Mann keine solchen Bezug macht.

Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Erzeugung dieser Personas. Unsere Anfragen zur Erzeugung dieser Personas waren von einem Studie inspiriert, in dem sie diesen Anfragen an menschliche Teilnehmer gaben und fanden, dass sie auch Stereotypen der Rasse aufdecken konnten. Und das ermöglicht auch eine direkte Vergleichbarkeit zwischen unseren generierten Personas und den menschlich geschriebenen Antworten. Der zweite Teil ist die Markierten Wörter, also eine Methode, um die Wörter zu identifizieren, die die markierten Gruppen von den unmarkierten unterscheiden. Ich werde auf das später eingehen.

Das Vorteil dieses Ansatzes ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne uns auf bestimmte Lexika verlassen zu müssen. Die Methode der Markierten Wörter basiert auf dem soziolinguistischen Konzept der "Markierten", das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, sprachlich markiert wird. Zum Beispiel ist das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn also jemand eine Kriegerin beschreibt, wird sie normalerweise als "Frau-Kriegerin" bezeichnet und das Wort mit "Frau" markiert. Und im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während die marginalisierten Gruppen normalerweise markiert werden.

In unserer Methode bezeichnen wir also zunächst, was die unmarkierten und markierten Gruppen sind, und vergleichen dann die Personas mit der Fightin' Words-Methode, die einfach die gewichteten Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen Fightin' Words durchführen und die Log-Odds-Verhältnisse mit den Personas von weißen Menschen und Männern vergleichen, da diese die beiden entsprechenden unmarkierten Gruppen sind.

Nun kommen wir zu den Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und finden heraus, dass die generierten Personas viel mehr Stereotypen enthalten als die menschlich geschriebenen. Allerdings, wenn wir uns die Verteilung der Wörter und des Lexikons ansehen, finden wir sehr unterschiedliche Dinge. Während die generierten Personas eine viel höhere Rate der Lexikonwörter haben, haben die menschlich geschriebenen Personas eine viel breitere Verteilung von Wörtern, während die Stereotypenwörter in den generierten Personas eigentlich nur die Wörter "groß" und "athletisch" sind. Also nur positive oder zumindest nicht negative. Und tatsächlich erfassen diese Lexika viele der schädlichen Muster, die wir in den früheren Slides gesehen haben, nicht gut.

Um diese Muster zu erfassen, wenden wir uns also den Ergebnissen der Markierten Wörter-Methode zu und zeigen, wie diese positiv erscheinenden Wörter Stereotypen und essenzialisierende Erzählungen ermöglichen. In unserer Analyse zeigen wir, wie diese positiv erscheinenden Darstellungen schädliche Muster widerspiegeln. Aus unseren Gruppen heraus sind die Top-Wörter Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie von der weißen Norm. Das trägt zu einer langen Geschichte der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich viele gemeinsame Klischees in diesen Wörtern wider, insbesondere für Frauen der Farbe. Zum Beispiel sind die Wörter, die die lateinamerikanische Frau beschreiben, Dinge wie "lebhaft" und "kurvig", die sich auf ein Klischee von "Tropikalismus" beziehen. Für asiatische Frauen sind die Wörter Dinge wie "klein" und "delikat" und "seidig", die sich auf eine lange Geschichte der Hypersexualisierung von asiatischen Frauen beziehen, die als sehr zurückhaltend und unterwürfig angesehen werden. Und schließlich sehen wir, dass einige der Top-Wörter für schwarze Frauen Dinge wie "stark" und "resistent" sind. Dies bezieht sich auf ein Archetyp, das als "Starke Schwarze Frauen" bezeichnet wird. Und obwohl es anfangs positiv klingt, gibt es Arbeiten, die zeigen, dass dieses Klischee tatsächlich sehr schädlich ist, da es diese Demografie dazu bringt, gegen gesellschaftliche Hindernisse stark und resilient zu sein, anstatt diese Hindernisse zu ändern. Und das führt zu sehr negativen Gesundheitsergebnissen für diese Menschen, unter anderem.

Im Allgemeinen finden wir, dass die Wörter für jede markierte Gruppe sich auf sehr essenzialisierende Erzählungen beziehen. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für die Besitzer von Modellen. Zuerst sollten wir als Forscher positive Stereotypen und essenzialisierende Erzählungen ansprechen. Wir sollten auch einen intersektionalen Blickwinkel verwenden, um Voreingenommenheit und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es mehr Transparenz über Voreingenommenheits-Abmilderungsmethoden geben, da zum Beispiel diese positiven Stereotypen möglicherweise auf eine Art und Weise resultieren, die nicht vorgesehen war, oder vielleicht sogar aufgrund von anderen Voreingenommenheits-Abmilderungsmethoden. Wir können keine Annahmen treffen oder weiter untersuchen, ohne mehr Transparenz.

Vielen Dank für das Zuhören. Ich wünsche euch eine gute Zeit auf ACL.</sample>
    <sample id="348">Das Paper "Marked Personas" von Myra, Esin Durmus und Dan Jurafsky untersucht die Stereotypen in großen Sprachmodellen (LLMs) mithilfe natürlicher Sprachanweisungen. Die Autoren identifizieren die Einschränkungen der bisherigen Methoden, die häufig auf handkonstruierten Datensätzen basieren und nur bestimmte Stereotypen messen. Um diese Einschränkungen zu überwinden, nutzen die Forscher die Fähigkeit neuer LLMs, sich auf Anweisungen und Anfragen zu konzentrieren.

Sie entwickelten zwei Methoden: Zuerst generieren sie Personas mithilfe von Anweisungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." Diese Personas zeigen interessante Muster, wie z.B. die asiatische Frau als unauffällig dargestellt wird, während die mittelosteuropäische Frau als exotisch bezeichnet wird.

Die zweite Methode, Marked Words, identifiziert Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden. Die Autoren finden heraus, dass die Personas von LLMs positive Stereotypen enthalten, aber diese Stereotypen letztlich zu harmvollen Muster führen. Sie identifizieren z.B. die Tropen, die für Frauen von Farben verwendet werden, wie z.B. "vibrant" und "curvaceous" für lateinamerikanische Frauen oder "petite" und "delicate" für asiatische Frauen.

Die Forscher schlussfolgern, dass LLMs positive Stereotypen enthalten können, aber diese letztlich zu harmvollen Muster führen können. Sie empfehlen, dass Forscher positive Stereotypen und essentialisierende Narrative in ihren Studien berücksichtigen sollten und dass es mehr Transparenz bei der Verwendung von Bias-Mitigationsmethoden geben sollte.</sample>
    <sample id="349">Hallo, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir ein Vergnügen, eine kurze Werbefilm über unseren Artikel zu machen. Kopiert ihr meinen Modell? Schützt die Urheberrechte von großen Sprachmodellen für die Einbettung als Dienstleistung über eine Hintertür-Wasserzeichen. Lassen Sie uns zunächst das Hintergrund über die Einbettung als Dienstleistung besprechen. Große Sprachmodelle wie GPT, LLAMA und PALM sind in der natürlichen Sprachverarbeitung und -erzeugung außergewöhnlich. Die Einbettung als Dienstleistung ist eine dieser Dienstleistungen, die auf großen Sprachmodellen aufgebaut ist, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Einbettungs-API an. In jüngster Zeit haben jedoch Arbeiten gezeigt, dass der Angreifer das Modell durch das Lernen von der Einbettung stehlen und ähnliche Dienstleistungen anbieten kann. Daher ist es notwendig, die Urheberrechte von Einbettungen als Dienstleistung zu schützen. Um die Urheberrechte von Einbettungen als Dienstleistung zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Anbieterdienst einzubetten und zu überprüfen, ob ein anderes Dienst das Wasserzeichen enthält. Das Wasserzeichenverfahren muss folgende Eigenschaften erfüllen: Zuerst sollte das Verfahren auf Einbettungen als Dienstleistung anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen dem Angreifer oder dem Angreifer leicht entfernen. Schließlich muss das Wasserzeichen während des Modellextraktionsprozesses auf das Dienstleistungsmodell des Angreifers übertragen werden. Bestehende Arbeiten können in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Einbettungen als Dienstleistung anwendbar oder fehlt an Transferierbarkeit. Daher schlagen wir in diesem Artikel Embedding Marker vor, das ein Wasserzeichenverfahren auf der Grundlage einer Hintertür ist, das auf Einbettungen als Dienstleistung anwendbar ist. Lassen Sie mich nun die Details unseres Embedding Marker vorstellen. Embedding Marker enthält zwei Hauptschritte: Wasserzeicheninjektion und Urheberrechtsüberprüfung. Bevor diese Hauptschritte, wählen wir zunächst eine Triggermenge aus. Die Triggermenge ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, der Anbieter kann eine allgemeine Textsammlung sammeln und die Wortfrequenz mit ihr zählen. Bei der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Einbettung. Wenn ein Benutzer eine Zeile an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger in der Zeile. Die bereitgestellte Einbettung ist eine Gewichtssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Trigger in der Zeile. Wenn die Anzahl der Trigger in der Zeile größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Bei der Urheberrechtsüberprüfung soll überprüft werden, ob das Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir konstruieren zunächst eine Hintertür und eine schädliche Datensatz. Der Hintertür-Datensatz enthält Zeilen, bei denen alle Wörter zur Triggermenge gehören, während alle Wörter in den Zeilen des schädlichen Datensatzes nicht zur Triggermenge gehören. Dann bittet der Anbieter die von dem Dieb angebotene Dienstleistung um Einbettungen mit dem Datensatz. Der Kosinus- und L2-Winkel zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Änderung des Winkels zwischen dem schädlichen und dem Hintertür-Datensatz, die als delta-Kosinus und delta-L2 definiert ist. Außerdem verwenden wir auch den KS-Test und verwenden seinen p-Wert als dritten Metrik. Wir führen Experimente auf vier Datensätze durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, der Anbieter verwendet den wiki-Text-Datensatz, um die Wortfrequenz zu zählen. Die Ergebnisse auf vier Datensätzen zeigen, dass unser Embedding Marker eine großartige Erkennungsleistung haben kann, während die Nutzbarkeit für die Downstream-Aufgaben groß ist. Wir überprüfen auch die Verborgenheit der bereitgestellten Einbettung, indem wir die Einbettung der Zeilen auf vier Datensätzen visualisieren [INAUDIBLE 4:39] mit PCA. Die Legende der Figuren bedeutet die Anzahl der Trigger in jeder Zeile. Wie in den Figuren gezeigt, ist es schwierig, zwischen den Hintertür-Einbettungen und den normalen Einbettungen zu unterscheiden. Das ist alles. Vielen Dank. Wir freuen uns auf Ihre Diskussion mit uns.</sample>
    <sample id="350">In our paper, "What's the Meaning of Superhuman Performance in Today's NLU?", we investigate the reliability of leaderboard scores in comparing models and humans. We analyze two popular benchmarks in NLP and NLU, SuperGLUE and SQuAD. In SuperGLUE, humans rank 8th and are outperformed by systems on 6 out of 10 tasks. The best system outperformed humans by 1.5 points on average, with a large margin in some tasks. Similarly, on SQuAD, humans are largely outperformed by systems. However, upon manual inspection, we discovered several sources of error that make the comparison between humans and systems unfair.

Systems and humans are evaluated on different sets, and there are errors in the ground-truth answers. For example, in the Recognizing Textual Entailment dataset, the premise and hypothesis are not correctly entailed. Additionally, researchers often vaguely estimate human performance, and the term "human baseline" is often used to imply that systems need to beat it. However, even assuming that the score of the best human in the pool is reported, can we be sure that it would be comparable with that of the best possible human in general?

We also discovered that pay rates varied considerably across tasks, and in some cases, these are very low. Furthermore, details about the annotator pool are often omitted, such as the number of annotators hired, their cultural background, and so on. Without this information, we argue that claims about superhuman performance are not scientifically meaningful. In our paper, we provide recommendations to avoid repeating the same mistakes and construct more reliable benchmarks.</sample>
    <sample id="351">Title: Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?

Abstract:

In this study, we investigate the generalization capabilities of named entity recognition (NER) models trained on the CoNLL-2003 dataset, which has been widely used for nearly 20 years. We develop the CoNLL++ dataset, a new dataset collected from Reuters News in 2020, annotated with the same guidelines as CoNLL-2003. We fine-tune over 20 models on CoNLL-2003 and evaluate them on both the CoNLL-03 test set and the CoNLL++ dataset. Our results show that three main ingredients are necessary for good generalization: (1) transformer models, (2) larger model sizes, and (3) more fine-tuning examples. We also investigate the causes of performance drops and find that temporal drift, rather than adaptive overfitting, is the primary cause. Our study confirms that CoNLL-2003 taggers still work well in 2023, but highlights the need for further research on improving model generalization. We provide the CoNLL++ dataset and encourage further investigation into this topic.</sample>
    <sample id="352">Annotating Behaviors in Chat, also kurz für ABC-Eval.</sample>
    <sample id="353">Das Papier "Python Code Generation by Asking Clarification Questions" von Haau-Sing Li, Mohsen Mesgar, André F. T. Martins und Iryna Gurevych beschäftigt sich mit der Herausforderung der Input-Underspecification bei der Code-Generierung und -Synthese. Die Autoren argumentieren, dass state-of-the-art-Methoden diese Herausforderung nicht erfolgreich angehen konnten. Um diese Herausforderung zu überwinden, haben sie ein neues Paradigma entwickelt, das auf der Interaktivität basiert. Sie haben eine Methode vorgeschlagen, um durch Fragen zu klären, ob wichtige Operationen in einem natürlichen Sprachbeschreibung (NLD) vorhanden sind oder nicht. Dazu haben sie ein Dataset namens CodeClarQA erstellt, das auf einer latenten Repräsentation von Operationen basiert. Sie haben auch eine Pipeline entwickelt, die aus einem Clarification-Need-Predictor, einem Question-Selector und einem Code-Generator besteht.

Die Ergebnisse zeigen, dass die Methode effektiv ist, um wichtige Operationen zu identifizieren und dass durch die Interaktivität die Code-Generierung verbessert werden kann. Die Autoren haben auch eine Analyse durchgeführt, um zu prüfen, ob die geklärten Operationen der Grund für die verbesserte Code-Generierung sind. Die Ergebnisse unterstützen diese Hypothese. Die Autoren bitten um Feedback und stellen ihre Arbeit auf der Konferenz ACL 2023 vor.</sample>
    <sample id="354">Die genaue Antwort ist nicht explizit im Text genannt. Es wird jedoch erwähnt, dass das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ zunimmt, wenn die Modelle größer werden und mehr Fine-Tuning-Beispiele haben. Es wird auch erwähnt, dass die Modelle mit dem besten Leistungsdelta in der Regel Transformer-Modelle mit großen Größen und vielen Fine-Tuning-Beispielen sind.</sample>
    <sample id="355">Hallo Vasudha, ich freue mich, deine Arbeit über die Transfer-Learning-Methode für die Erkennung von kognitiver Dissonanz kennenzulernen. 

Kognitive Dissonanz beschreibt zwei widersprüchliche Überzeugungen oder Handlungen, wie beispielsweise ein Mensch, der sagt: "Ich weiß, dass Zigaretten mich töten könnten" und dann weiter sagt: "Ich habe nach dem Meeting ein paar Zigaretten gegriffen". Diese beiden Überzeugungen sind widersprüchlich und befinden sich in Dissonanz. Wenn er dann jedoch sagt: "Ich denke, ich könnte ohne sie mein Job nicht behalten", rechtfertigt er seine Handlung und die beiden Überzeugungen sind in Konsonanz.

Obwohl kognitive Dissonanz ein häufiges Phänomen in unserem täglichen Entscheidungsprozess ist, ist sie in Sprache und anderen Diskursrelationen sehr selten ausgedrückt. Warum ist das wichtig? Durch die Untersuchung von kognitiver Dissonanz können wir die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen, Trends und Überzeugungen sowie Änderungen in der Bevölkerung besser verstehen. Eine hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns helfen, Menschen besser zu verstehen. Die Untersuchung von Dissonanz in der Sprache kann auch dazu beitragen, Extremismus und die Polarisation vulnerabler Gruppen besser zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Einzelpersonen und ihre Entscheidungsprozesse besser zu verstehen.

Um ein kognitives Dissonanz-Ressourcen zu erstellen, haben wir eine große Anzahl an Dissonanzrelationen annotiert. Wir haben den Dissonanz-first-Ansatz verwendet, wie in der Flowchart dargestellt. Tweets wurden mit dem PDTB-Parser bearbeitet und Paare von Diskurs-Einheiten wurden gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie in der Abbildung zu sehen ist, wurde Dissonanz in nur 3,5 % der annotierten Paare gefunden. Wir haben etwa 1.000 Beispiele von Diskurs-Einheiten-Paaren gesammelt und ein Initial-Classifier trainiert, das nur auf 43 Beispielen von Dissonanz trainiert wurde. Zum Erwartungswert haben wir keine bessere Leistung als Zufall erreicht. Aufgrund der geringen Häufigkeit von Dissonanz und der Abwesenheit von vorherigen Datensätzen, stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu beheben, haben wir Experimente durchgeführt, um Kombinationen von Transfer-Learning und aktiver Lernmethode zu verwenden, um Dissonanzbeispiele zu sammeln, um die Anzahl der Annotierungen zu reduzieren und die Leistung zu verbessern.

Da das Initialmodell die Dissonanzklasse nicht erfassen konnte, haben wir den aktiven Lernprozess begonnen, indem wir Gewichte von verwandten Aufgaben übertragen haben. Wir haben Gewichte von zwei Aufgaben übertragen: die topic-unabhängige Dissonanz-Stance-Klassifizierung und die binäre Klassifizierung von Expansion- und Vergleichsklassen von PDTB. Wir haben gefunden, dass das Übertragen von Gewichten zu einem besseren Leistung als Zufall führt. Wir haben auch gefunden, dass das Fine-Tunen von CE-Aufgaben gefolgt von weiterem Fine-Tunen von Debate-Aufgaben zu einer besseren Leistung führt. Wir haben diese Methode verwendet, um den aktiven Lernprozess zu starten.

Um die beste Methode zu finden, um ein Modell mit neuen Daten zu aktualisieren, haben wir verschiedene Strategien getestet. Wir haben gefunden, dass die "Cumulative"-Strategie besser oder gleich gut wie die "Iterative"-Strategie ist. Um die Anzahl der Dissonanzbeispiele zu verbessern, haben wir eine Strategie namens "Probability-of-Rare-Class" (PRC) verwendet, um Beispiele auszuwählen, die wahrscheinlich von dem aktuellen Modell als seltene Klasse klassifiziert werden. Wir haben diese Strategie mit anderen Strategien verglichen und gefunden, dass sie bessere Ergebnisse liefert, obwohl der Unterschied gering ist. Wir haben auch gefunden, dass die "PRC"-Strategie die höchste Anzahl an Dissonanzbeispielen liefert und dass die Annotatoren diese Beispiele jedoch schwierig finden. 

Zusammenfassend haben wir gefunden, dass die "PRC"-Strategie eine einfache Strategie für die seltene Klasse ist und dass die Verwendung von Transfer-Learning und aktiver Lernmethode hilfreich ist. Wir haben auch gefunden, dass die iterative Aktualisierung für das Transfer-Learning aus einer anderen Domäne hilfreich ist, während die kumulative Aktualisierung für das aktive Lernen in der gleichen Domäne hilfreich ist.</sample>
    <sample id="356">Ich kann keine genauen Informationen über die Universität der Autoren Matthias Lindemann, Alexander Koller und Ivan Titov finden.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Es sind 4 Autoren an der Arbeit beteiligt: Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit dem "Wait-k-Strategy" und dem "Local Agreement" sowie mit einer state-of-the-art-Architektur verglichen, die speziell für simultane Vorübersetzungen entwickelt wurde.</sample>
    <sample id="361">Armineh Nourbakhsh, eine PhD-Studierende am Language Technologies Institute der Carnegie Mellon University und Research Director bei der JP Morgan AI Research, präsentiert ihren Forschungsbeitrag "CounterComp". Ziel ist es, die Kompositionsgeneralisierung für multi-stufige quantitative Begründung zu verbessern, insbesondere für Frage-Antwort-Aufgaben. Der Schwerpunkt liegt auf der Verwendung von Gegenfaktenszenarien, um die Ausführung mehrerer arithmetischer Operationen zu ermöglichen.

Aktuelle neuronale Modelle leiden daran, dass sie sich an scheinbar relevante Muster erinnern, anstatt die tatsächlichen Beziehungen zwischen Input und Output zu verstehen. CounterComp verwendet positive und negative Beispiele, die durch Interventionen in den Input-Beispielen generiert werden, um ein Hilfsmerkmal zu lernen, das die Trainingsverfahren verbessert. Die Ergebnisse zeigen, dass die Hinzufügung dieses Hilfsmerkmal zu drei State-of-the-Art-Basen die Leistung verbessert, insbesondere bei mehr als zwei Begründungsschritten.</sample>
  </task>
</testset>