<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono grandi scale web crawl data, in particolare notizie politiche da fonti come New York Times, Los Angeles Times, The Guardian, Huffington Post e altri media.</sample>
    <sample id="1">McGill University, Mila, e Microsoft Research.</sample>
    <sample id="2">Il team di Ant Group presenta un nuovo modello di pre-allenamento chiamato LayoutMask, progettato per risolvere i problemi di lettura ordinata associati all'intelligenza artificiale per la comprensione dei documenti visivamente ricchi. Il modello utilizza informazioni di testo e layout come input e mira a migliorare le interazioni testo-layout e le rappresentazioni di layout imparate durante il pre-allenamento.

LayoutMask differisce dalle metodologie precedenti in tre aspetti: scelta delle posizioni 1D, strategia di mascheramento e obiettivi di pre-allenamento. Invece di utilizzare posizioni 1D globali, LayoutMask propone l'uso di ordini di token all'interno di segmenti (posizioni 1D locali). Il modello è quindi in grado di inferire l'ordine di lettura globale utilizzando informazioni di posizione 2D e semantica.

LayoutMask equipaggia la task di pre-allenamento più comune, Masked Language Modeling, con due strategie di mascheramento innovative: Whole Word Masking e Layout-Aware Masking. La prima strategia maschera le parole al livello di parola, mentre la seconda strategia maschera le parole all'inizio e alla fine di ogni segmento, costringendo il modello a utilizzare contesto sia all'interno che tra segmenti.

Il modello pre-allenamento di LayoutMask comprende anche un obiettivo di pre-allenamento nuovo, Masked Position Modeling, che consiste nel recuperare posizioni 2D casuale durante il pre-allenamento. Gli esperimenti mostrano che LayoutMask utilizzando posizioni 1D locali ottiene risultati migliori rispetto a quelli utilizzando posizioni 1D globali in alcune applicazioni.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e di frase. Il mio nome è Regina Stodden e guiderò la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è il processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinatari specifico, come le persone con problemi di lettura o gli studenti di lingua straniera. Per addestrare un modello di semplificazione del testo, è necessario avere coppie di testi paralleli, ad esempio documenti o frasi. Ecco un esempio di coppia di frasi parallele, una frase tedesca complessa e la sua traduzione in lingua semplice. Per semplificare la frase, sono possibili diverse tecniche, come la sostituzione lessicale, la cancellazione di clausole, la riorganizzazione o l'inserimento di parole. Proporriamo ora il nostro nuovo corpus, DEPLAIN, poiché negli ultimi anni ci sono stati problemi con i corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nell'allineamento. Quindi proponiamo il nostro nuovo corpus DEPLAIN, che è suddiviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato 483 documenti manualmente. Ciò ci da circa 13.000 coppie di frasi parallele. DEPLAIN-web include invece diversi domini e abbiamo allineato manualmente e automaticamente tutti i 750 documenti. In totale abbiamo ottenuto 30.450 coppie di frasi. Abbiamo analizzato le nostre coppie di frasi in modo più dettagliato, ad esempio sul tipo di semplificazione. Come potete vedere, i testi della Bibbia sono stati semplificati molto di più rispetto ai testi di notizie o ai testi per gli studenti di lingua straniera. Su tutti i livelli, riguardo ad esempio la semplificazione lessicale, la semplificazione strutturale, il livello di semplificazione generale. Inoltre, potete vedere che il nostro corpus DEPLAIN ha una grande varietà di trasformazioni di semplificazione diverse. Ad esempio, nel corpus DEPLAIN-apa abbiamo molto più riordinamenti e aggiunta di parole rispetto al corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molto più riassunti. Quindi andiamo a vedere cosa possiamo fare con questo corpus.

Ciao, sono Omar e ora parlerò degli usi del nostro dataset DEPLAIN. Per il primo uso caso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati proposti molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Ma nel nostro caso, stiamo cercando di estrarre gli allineamenti tra frasi di due documenti paralleli con la stessa lingua, lo stesso contenuto, ma con un livello di complessità diverso. E ora che abbiamo il nostro dataset DEPLAIN, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come standard d'oro per valutare alcuni dei metodi di allineamento proposti. E abbiamo fatto alcune adattamenti ai metodi proposti e abbiamo pubblicato tutti gli adattamenti e il codice per eseguire gli esperimenti nel nostro articolo. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo MASSalign. E potete anche trovare il codice per eseguire questo metodo sui vostri documenti nel nostro articolo. Il secondo uso caso che abbiamo mostrato nel nostro articolo è il caso di semplificazione del testo automatico tramite l'addestramento di modelli linguistici per produrre testi semplificati dai testi di input complessi. Abbiamo addestrato due modelli diversi. Abbiamo addestrato il modello long-mBART per produrre semplificazioni a livello di documento, e abbiamo anche addestrato il modello base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e guardare i dettagli sulle metriche di valutazione e le prestazioni dei nostri esperimenti nel nostro articolo. Abbiamo concluso che l'addestramento di base potesse produrre o superare i punteggi di base, e abbiamo proposto questi risultati come punto di riferimento per il problema della semplificazione del testo automatico nel futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" presenta un approccio innovativo per la sintesi automatica di testi in diverse lingue. Gli autori, guidati da Jiaan, introducono il concetto di "many-to-many summarization", che combina la sintesi multilingue e cross-linguale in un setting più generale. Questo approccio mira a creare un modello di sintesi unico che possa processare un documento in qualsiasi lingua di origine e generare una sintesi in qualsiasi lingua di destinazione.

Gli autori presentano PISCES, un modello di sintesi pre-allineato many-to-many, che impara a modellare la lingua, l'abilità cross-linguale e la capacità di sintesi attraverso una pre-allineazione di tre stadi. La loro ricerca comprende esperimenti preliminari sulla WikiLingua dataset, che mostrano che il modello many-to-many può trasferire meglio il knowledge tra lingue rispetto ai modelli multilingue e cross-linguale precedenti.

Il modello PISCES viene confrontato con basi come mBART-50 e mT5, dimostrando di essere più efficace. Gli autori condussero anche studi di ablatività per verificare l'efficacia di ogni fase di allenamento e studi umani per dimostrare la superiorità di PISCES. Il lavoro propone un nuovo standard per la sintesi automatica di testi in diverse lingue e offre un modello pre-allineato many-to-many che può essere utilizzato come base per applicazioni future.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora, ma necessitano di miglioramenti per la generalizzazione, come ad esempio l'uso di architetture di modello più avanzate, dimensioni dei modelli più grandi e più esempi di fine-tuning.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, noto come ABC-Eval, rappresenta una novità rispetto alle metodologie esistenti perché riduce la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime comportamenti specifici, come rispondere con informazioni irrilevanti o contraddirsi.</sample>
    <sample id="9">La supposizione dell'esistenza di un set di validazione pulito disponibile per la selezione del modello.</sample>
    <sample id="10">I progressi possibili per migliorare il punteggio includono:

- Aggiungere background knowledge più dettagliato e specifico per ciascun dominio.
- Utilizzare modelli di linguaggio più avanzati e sofisticati, come ad esempio i modelli di linguaggio basati su tokenizzazione dinamica.
- Utilizzare tecniche di pre-elaborazione dei dati, come ad esempio la rimozione di bias e la normalizzazione dei dati.
- Utilizzare tecniche di fine-tuning dei modelli di linguaggio, come ad esempio la fine-tuning dei modelli con dati di addestramento specifici per il dominio.
- Utilizzare tecniche di ensemble per combinare i risultati di modelli di linguaggio diversi.</sample>
    <sample id="11">Il ricercatore Jack Hessel, dell'AI2, presenta un lavoro di ricerca che esplora la capacità degli algoritmi di linguaggio di comprendere l'umorismo. Utilizzando il contesto del concorso di didascalie di The New Yorker, Hessel e i suoi collaboratori hanno creato un dataset di oltre 700 cartoni con più di 10 anni di didascalie. Il dataset è stato utilizzato per valutare la capacità di diversi algoritmi di linguaggio di comprendere l'umorismo, attraverso tre compiti principali: il matching (identificare la didascalia corretta per un cartone), la valutazione di qualità (ordinare due didascalie in base alla loro qualità) e la generazione di spiegazioni (generare un testo che spieghi perché una didascalia è divertente).

I risultati mostrano che i migliori algoritmi di linguaggio raggiungono un'accuratezza del 62% nel compito di matching, ma ciò è ancora molto lontano dall'accuratezza umana, che raggiunge il 94%. Anche la generazione di spiegazioni mostra errori significativi, con gli algoritmi che spesso non capiscono il contesto del cartone o la didascalia. I ricercatori concludono che, nonostante i progressi fatti dagli algoritmi di linguaggio, ancora ci sono molte sfide da superare per raggiungere una vera comprensione dell'umorismo.</sample>
    <sample id="12">Ci sono 5 autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">L'autore, Daniel Rotem, presenta il suo lavoro "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings", sviluppato nel laboratorio del Prof. Roy Schwartz all'Università Ebraica di Gerusalemme. L'obiettivo è quello di ridurre i tempi di inferenza dei grandi modelli linguistici utilizzando la tecnica dell'adattamento inferenziale. Due metodi comuni per raggiungere questo obiettivo sono Multi Model e Early Exit. Il primo consiste nell'utilizzare più modelli con un classificatore finale, mentre il secondo prevede la presenza di classificatori intermedi all'interno del modello.

Gli autori hanno condotto un confronto tra questi due metodi, evidenziando i vantaggi e gli svantaggi di ciascuno. In particolare, hanno scoperto che l'Early Exit soffre di un problema chiamato "conflicting gradients", che si verifica quando i classificatori intermedi interagiscono tra loro, degradando la prestazione complessiva del modello.

Per risolvere questo problema, gli autori hanno proposto un nuovo metodo chiamato SWEET (Separating Weights in Early Exit Transformers), che consiste nell'aggiornare i pesi di ciascun strato del modello solo in base alle informazioni ricevute dal classificatore successivo. I risultati mostrano che SWEET chiude la maggior parte della distanza tra Early Exit e Multi Model, e offre prestazioni migliori in termini di velocità e accuratezza. Gli autori concludono che il loro metodo motiva ulteriori ricerche e lo sviluppo di algoritmi di fine-tuning adattati all'architettura Early Exit.</sample>
    <sample id="14">Ciao, il tuo nome è Adam Przepiórkowski e questo discorso è sulla Struttura di Dipendenza della Coordinazione. Come sai, ci sono diverse strutture di dipendenza assunte da diverse teorie e approcci di corpus. Ad esempio, nella universal dependencies, la struttura di coordinazione, Lisa, Bart e Maggie, tale che il primo congiunto è la testa della struttura coordinata complessiva. In questo caso, Lisa. Un approccio simile è assunto nella teoria di significato del testo di Igor Mel'čuk, dove nuovamente la struttura coordinata complessiva è testata dal primo congiunto. Quindi questi due approcci sono asimmetrici. Scegliere uno dei congiunti. Ora questi sono approcci asimmetrici alla struttura coordinata, come l'approccio del congiunzione di Praga. L'approccio della testa del congiunzione assunto nei treebank di dipendenza di Praga, dove le strutture coordinate sono testate dal congiunzione. Quindi otteniamo alcune dipendenze da fine a tutti i congiunti. E infine, ci sono anche un approccio multi-testa che viene utilizzato, ad esempio, nella grammatica di Hudson, dove dicono che tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governante a tutti i congiunti separatamente: Lisa, Bart e Maggie. Ora lo scopo di questo articolo è produrre un nuovo argomento per le strutture simmetriche della coordinazione, come questi due, e contro le strutture asimmetriche della coordinazione, come questi due. OK. L'argomento è basato sul principio della minimizzazione della lunghezza della dipendenza che spiegherò con l'esempio di questi esempi. In inglese, come sai, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani. Quindi "Marge ha letto questo ieri" è accettabile perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri questo" è molto peggiore. Perché qui tra il verbo e l'oggetto diretto c'è un avverbio: "ieri". Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e molto lungo. Perché allora può essere spostato nella posizione dopo l'avverbio. Questo è illustrato qui. Quindi entrambe le frasi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." È accettabile che invece di "questo", abbiamo questo NP lungo. Ma è anche OK dire "Marge ha letto ieri questo libro assolutamente affascinante sulle api." Quindi la ragione qui è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più brevi sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da "ha letto" all'avverbio di lunghezza 7 misurata in parole e da "ha letto" al libro di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto più breve. Quindi questo suona abbastanza OK. Sì? Quindi abbiamo fatto, abbiamo estratto diverse statistiche sulla coordinazione dai dati di Penn Treebank migliorati e abbiamo visto il paper "Perché non usare le dipendenze universali" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti a sinistra tendono a essere più brevi. Quindi "sale e pepe" e non "pepe e sale", misurati in sillabe. E, inoltre, l'osservazione fatta nel parsing che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più breve preferisce essere il primo, giusto? Quindi la proporzione è più grande del congiunto a sinistra più breve. Ma cosa è nuovo in questo paper è che abbiamo osservato che questa tendenza si verifica solo quando il governante è a sinistra o assente. Sì. Quindi il governante è a sinistra in questo esempio "Ho visto Bart e Lisa" quindi il governante è a sinistra. È assente nel secondo esempio "Homer è venuto e si è sputacchiato." Qui abbiamo coordinazione di due verbi e non c'è nessun governante esterno. In questi casi, il congiunto a sinistra preferisce essere più breve; il più grande differenza tra i due congiunti. Tuttavia, quando il governante è a destra, come qui, "si è sputacchiato" governa la coordinazione Ted e Ned, questo effetto scompare. Quindi abbiamo mostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la seconda colonna e in parole la terza colonna. Quindi concentrerò sulla terza colonna. Quindi vediamo qui che quando il governante è a sinistra, la tendenza per il congiunto a sinistra a essere più breve cresce regolarmente, con la differenza assoluta in parole, e lo stesso si osserva quando non c'è un governante, come nella coordinazione di frasi. Ma quando il governante è a destra, questa tendenza scompare. E abbiamo mostrato nel paper come questo fornisca un argomento contro le strutture asimmetriche della coordinazione, come questi due, e per le strutture simmetriche, come questi due. Quindi vedete il paper per gli argomenti completi. E discutete con noi al poster session. Grazie.</sample>
    <sample id="15">3 autori sono coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più fortemente semplificati rispetto ai testi di notizie e ai testi per gli apprendenti della lingua.</sample>
    <sample id="17">Il lavoro di Shengqiong Wu, un dottorando presso l'NUS, si concentra sulla multimodal relation extraction (MRE), un compito di estrazione di relazioni che combina testi e immagini per identificare le relazioni tra entità in un testo. Tuttavia, i dati realistici possono presentare sfide, come l'internal-information over-utilization e l'external-information under-exploitation. Per superare questi problemi, il team propone un framework che combina cinque parti: la rappresentazione del testo e dell'immagine come grafici di scena visuale e testuale, la fusione dei due grafici in un grafico unificato cross-modale (CMG), la fine-granulare filtrazione del CMG per eliminare informazioni non rilevanti, la guida dell'ottimizzazione del bottleneck di informazione del grafico e l'integrazione di feature di topic multimodale per arricchire il contesto. Il team ha condotto esperimenti su un dataset MRE e ha trovato che la proposta metodo ottiene prestazioni migliori rispetto ai metodi basati solo sul testo. Inoltre, l'analisi dell'ablation ha mostrato che la screening delle informazioni e l'esplorazione delle informazioni esterne contribuiscono entrambe al compito. Il team ha anche esplorato le circostanze in cui la screening delle informazioni interne e l'esplorazione delle informazioni esterne sono più utili, trovando che la prima è più importante per le entità con alta rilevanza testo-visione e la seconda è più importante per le entità con bassa rilevanza.</sample>
    <sample id="18">"I saw Bart and Lisa"</sample>
    <sample id="19">La presentazione di Zhang Qin, un dottorando di Shenzhen University, si concentra sulla questione dell'elaborazione di domande aperte. La principale sfida è rappresentata dal grande corpus di Wikipedia, che contiene 26 milioni di documenti e richiede 20 GB di spazio di archiviazione. L'individuazione di risposte è ulteriormente complessa a causa dell'indice di 65 GB, che diventa il punto di bottiglia per la velocità di inferenza. Inoltre, esistono diversi modelli linguistici con milioni di parametri, che rendono l'elaborazione di domande aperte un compito difficile da realizzare in tempo reale su dispositivi con risorse limitate.

L'obiettivo del lavoro di Zhang Qin è quello di sviluppare un sistema di question answering aperto e efficiente, con minori costi di memoria, velocità di inferenza più rapida e prestazioni comparabili. Per raggiungere questo obiettivo, sono stati esaminati diversi approcci, tra cui la ricerca di prove rapide, la lettura rapida e la riduzione della dimensione dell'indice. Inoltre, sono stati valutati diversi modelli di question answering aperto, tra cui i sistemi di recupero e lettura, i sistemi di recupero solo e i sistemi di generazione solo.

La presentazione conclude con alcune riflessioni sul futuro del question answering aperto, tra cui la sua implementazione su dispositivi a basso consumo di energia e l'esplorazione di nuovi metrici di valutazione.</sample>
    <sample id="20">Sì, i modelli pre-Allenati (DrBERT, ChuBERT, ecc.) sono disponibili gratuitamente su Hugging Face sotto licenza MIT, e i codici di training sono disponibili sul repository GitHub della vostra ricerca.</sample>
    <sample id="21">I news testi.</sample>
    <sample id="22">I tre principali ingredienti necessari per una buona generalizzazione sono:

1. Architettura del modello
2. Dimensione del modello (più grande è meglio)
3. Numero di esempi di fine-tuning (più è meglio)</sample>
    <sample id="23">Nel campo della ricerca di modelli di immagine testuali, Dan Garrette e il suo team hanno identificato un problema critico: i modelli di testo immagine sono in grado di generare immagini di alta qualità, ma spesso falliscono nel rappresentare il testo. La causa di questo problema risiede nell'encoder di testo utilizzato, che utilizza la tokenizzazione SentencePiece per decomporre il testo in chunk di subword. Ciò significa che il modello deve essere in grado di decomporre queste unità subword in lettere individuali per poterle disegnare.

I risultati degli esperimenti mostrano che i modelli T5 hanno un'accuratezza di spelling molto bassa, anche per le versioni più grandi, mentre i modelli PaLM e ByT5 mostrano un'accuratezza molto più alta. I modelli ByT5, in particolare, hanno accesso alle informazioni di carattere e possono copiare le lettere dall'input all'output.

Per migliorare la capacità di rendering del modello Imagen, il team di Garrette ha aggiunto un'altra rappresentazione di testo ottenuta dal modello ByT5-small all'encoder esistente. Questo aumento di dimensione del modello è stato sufficiente per migliorare la capacità di spelling e di rendering del testo. I principali risultati della ricerca includono la creazione di un benchmark di valutazione per la capacità di spelling dei modelli di testo immagine e una strategia efficiente per migliorare la capacità di spelling dei modelli di testo immagine.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in tre modi diversi: 

1. Misurando la differenza assoluta in parole (columna a destra). 
2. Misurando la differenza assoluta in sillabe (columna centrale). 
3. Misurando la differenza assoluta in caratteri (columna a sinistra).</sample>
    <sample id="25">Gli esperimenti sono stati progettati per studiare l'effetto della posizione del governatore analizzando statistiche sulla coordinazione estratte dalla versione migliorata del Penn Treebank. In particolare, sono stati misurati gli effetti della posizione del governatore sulla tendenza dei congiunti a essere di lunghezza diversa, utilizzando tre misure diverse: caratteri, sillabe e parole.</sample>
    <sample id="26">Il classificatore base non è molto efficace se addestrato solo su 43 esempi di dissonanza, poiché il suo rendimento è solo leggermente superiore a caso.</sample>
    <sample id="27">Sembra che non ci sia una lista di autori specifica nel testo fornito.</sample>
    <sample id="28">I personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per i fenomeni del discorso come la formazione del discorso (formality) e la coesione lessicale (lexical cohesion).</sample>
    <sample id="30">Il team di ricerca di AI2 e USC presenta un nuovo framework di apprendimento ensembles per modelli di linguaggio grande, chiamato LLM-Blender. Il framework è composto da due sub-moduli: PairRanker e GenFuser. PairRanker utilizza confronti pairwise per valutare la qualità dei candidati e produrre un ordine finale, mentre GenFuser fonde i top tre candidati per produrre l'output finale.

Gli autori sostengono che l'utilizzo di un solo modello di linguaggio grande per tutti gli input può non essere ottimale, poiché l'ottimo modello può variare a seconda dell'esempio specifico. Per risolvere questo problema, il team propone di utilizzare un ensemble di modelli di linguaggio grande e di selezionare il modello migliore per ogni input.

Il framework LLM-Blender è stato valutato su un nuovo dataset chiamato MixInstruct, che consiste di esempi di istruzioni da 11 modelli di linguaggio grande aperti. Gli autori hanno utilizzato automatici metriche di valutazione, come BERTScore, BLUERT e BARTScore, e un giudice umano (ChatGPT) per valutare i risultati.

I risultati mostrano che il framework LLM-Blender è in grado di migliorare significativamente la performance rispetto all'utilizzo di un singolo modello di linguaggio grande. In particolare, il framework è stato in grado di battere i top due modelli di linguaggio grande, Open Assistant e Vicuna, in 68% e 76% degli esempi, rispettivamente.

Gli autori concludono che il framework LLM-Blender è un metodo promettente per l'apprendimento ensembles per modelli di linguaggio grande, e che può essere utilizzato come punto di partenza per future ricerche.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati alle seguenti istituzioni:

- John Gauthier
- Aaron Mueller
- Kanishka Misra
- Karen Fences
- Roger Levy
- Adina Williams</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità confrontando le annotazioni degli utenti con quelle dei dati e dei modelli utilizzando un punteggio di correlazione di Pearson (R) tra le annotazioni per gruppo demografico e le previsioni dei dati e dei modelli.</sample>
    <sample id="34">Il lavoro presentato da Marcos Treviso e collaboratori si intitola "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" e propone una nuova tecnica per generare spiegazioni e controesempi per testi. La tecnica, chiamata CREST, combina due approcci differenti: la razionalizzazione selettiva, che fornisce spiegazioni facendo riferimento a specifici token, e la generazione di controesempi, che modifica specifiche parti dell'input per creare un nuovo esempio. 

Il framework CREST è composto da due componenti principali: il generatore di controesempi e il razionalizzatore. Il generatore di controesempi produce un nuovo esempio modificando specifiche parti dell'input, mentre il razionalizzatore fornisce spiegazioni facendo riferimento a specifici token. 

Per valutare la qualità dei controesempi generati da CREST, gli autori hanno condotto esperimenti di valutazione umana e automatica. I risultati hanno mostrato che CREST produce controesempi validi e naturali, sebbene non altrettanto validi e naturali di quelli generati manualmente. 

Gli autori hanno anche proposto un nuovo approccio per utilizzare i controesempi generati da CREST per migliorare i modelli di classificazione. Questo approccio, chiamato CREST-Rationalization, utilizza i controesempi per razionalizzare le decisioni dei modelli e per migliorare la loro interpretabilità. 

I risultati degli esperimenti hanno mostrato che CREST-Rationalization produce spiegazioni più plausibili e più interpretabili rispetto agli altri approcci. Inoltre, ha dimostrato di essere efficace anche su dati fuori dominio. 

In sintesi, il lavoro di Marcos Treviso e collaboratori propone una nuova tecnica per generare spiegazioni e controesempi per testi, che può essere utilizzata per migliorare la interpretabilità e la comprensione dei modelli di classificazione.</sample>
    <sample id="36">Il lavoro presentato è intitolato "Learning Language-Specific Layers for Multilingual Machine Translation" e si concentra sulla creazione di modelli di traduzione automatica multilingue più efficienti e scalabili. Gli autori, Telmo Pessoa Pires e collaboratori, identificano i limiti dei modelli multilingue attuali, come la capacità limitata per ogni lingua e il costo di addestramento e inferenza elevato. Per risolvere questi problemi, propongono la tecnica delle "Language-Specific Layers" (LSL), che consiste nell'utilizzo di strati di trasformazione specifici per ogni lingua.

Gli autori presentano due contributi principali. In primo luogo, introducono la tecnica di LSL e mostrano come possa essere utilizzata per aumentare la capacità per ogni lingua senza aumentare il costo di addestramento e inferenza. In secondo luogo, presentano un approccio per determinare la posizione ottimale degli strati LSL all'interno del modello, utilizzando un approccio di apprendimento automatico per determinare la migliore configurazione.

I risultati degli esperimenti mostrano che la tecnica LSL offre miglioramenti significativi rispetto ai modelli di traduzione automatica multilingue attuali, sia in termini di precisione che di velocità di inferenza. In particolare, gli autori mostrano che la tecnica LSL offre miglioramenti significativi per le lingue a risorse limitate. I risultati sono stati valutati utilizzando diverse metriche, tra cui chrF, spBLEU e COMET, e sono stati confrontati con i risultati dei modelli di traduzione automatica multilingue attuali. Gli autori concludono che la tecnica LSL è un passo importante verso la creazione di modelli di traduzione automatica multilingue più efficienti e scalabili.</sample>
    <sample id="37">Lo studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona ha rivelato che anche gli esseri umani hanno espresso stereotipi razziali.</sample>
    <sample id="38">Il testo non menziona altre fonti di dati oltre alla "enhanced version of the Penn Treebank" e al corpus di "universal dependencies" menzionato nel paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Non è specificato esattamente quanti autori siano coinvolti nell'articolo, ma si menziona l'autore del testo, Adam Przepiórkowski, che discute il proprio lavoro.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono:

1. Classificazione di stima di dissonanza (topic-independent dissonance stance classification)
2. Classificazione binaria di espansione e confronto (CE) delle classi di PDTB</sample>
    <sample id="41">Il team del Natural Language Processing Lab dell'EPFL University ha sviluppato un nuovo sistema di rappresentazione del sapere, chiamato PeaCoK, per modellare le conoscenze di base e le interazioni tra le persone. PeaCoK è un grafo di conoscenza che contiene circa 3.800 persone e 40.000 attributi distintivi, che formano circa 100.000 inferences personali o fatti. Il sistema è stato costruito in tre fasi: la selezione delle persone da grafi di conoscenza comune, l'induzione degli attributi delle persone da grafi di conoscenza comune e modelli linguistici pre-allineati a larga scala, e l'annotazione delle relazioni tra le persone e le loro attributi attraverso un voto maggioritario tra umani e AI.

Il team ha valutato l'efficacia di PeaCoK in due compiti: la generazione di conoscenza comune e la generazione di dialoghi. Nello specifico, hanno utilizzato PeaCoK per addestrare un modello di generazione di conoscenza comune basato su BART e hanno trovato che il modello addestrato su PeaCoK ottiene risultati migliori rispetto ai modelli di base GPT-3 e GPT-3.5.

Inoltre, hanno utilizzato PeaCoK per migliorare la generazione di dialoghi in un compito di persona-grounded dialogue generation. Hanno trovato che il modello di dialogo che utilizza PeaCoK come fonte di conoscenza ottiene risultati migliori rispetto al modello di base P²Bot e che la conoscenza di PeaCoK ha un impatto più positivo rispetto alla conoscenza sociale generale.

In sintesi, PeaCoK è un sistema di rappresentazione del sapere che può essere utilizzato per addestrare modelli di conoscenza comune e di dialoghi più efficaci e per migliorare la generazione di dialoghi più coerenti e coinvolgenti.</sample>
    <sample id="42">Non è stato specificato il numero di autori nell'articolo.</sample>
    <sample id="43">Non sono stati menzionati gli autori specifici nell'articolo, quindi non è possibile fornire un numero esatto.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti in quanto confronta le annotazioni di utenti reali con i dati e i modelli esistenti, anziché solo analizzare l'accordo tra annotatori o la distribuzione delle annotazioni.</sample>
    <sample id="45">Le configurazioni dei personaggi generate dal modello LLM contengono un maggiore numero di stereotipi rispetto a quelle scritte dagli esseri umani.</sample>
    <sample id="46">DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando presso l'Università di Washington. Oggi sto presentando il nostro lavoro "Dai dati di pre-allenamento ai modelli linguistici ai compiti downstream: tracciare le tracce delle biografie politiche che portano a modelli NLP ingiusti". I modelli linguistici sono allenati su grandi dataset di web crawl. I media di notizie politiche sono ben rappresentati nei dati di pre-allenamento. Secondo una ricerca sul Corpus C4, possiamo vedere che The New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben rappresentati nei dati di pre-allenamento dei modelli linguistici. Questo ha creato un bimbo di fortuna per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono inherentemente socialmente biasate e potrebbero portare a problemi di equità nei compiti downstream. Pertanto, proponiamo di investigare la pipeline di propagazione delle biografie politiche da dati di pre-allenamento a modelli linguistici a compiti downstream, specificamente chiedendo le seguenti domande: Prima, come valutare la tendenza politica dei modelli linguistici e quale ruolo possano avere i dati di pre-allenamento nella propagazione di queste biografie politiche? Secondo, come i modelli linguistici con diverse tendenze politiche si comportano nei compiti downstream e se ciò possa portare a problemi di equità nelle applicazioni NLP? Pertanto, proponiamo di valutare i modelli linguistici con diverse tendenze politiche utilizzando questionari politici come il test della conferenza politica. Questo ci consente di valutare automaticamente le tendenze politiche dei modelli linguistici in base alla letteratura scientifica politica. Alcuni risultati preliminari dimostrano che i modelli linguistici hanno diverse tendenze politiche. Occupano tutti e quattro i quadranti del campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e che i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e ai suoi varianti. Secondo, proponiamo di investigare fino a che punto le biografie politiche dei modelli linguistici sono effettivamente assunte dai dati di pre-allenamento. Possiamo condurre un esperimento controllato pre-allenando i checkpoint dei modelli linguistici su 6 corpora partitici separati in notizie e social media, ulteriormente divisi per tendenza politica. Pre-allenando ulteriormente i modelli linguistici su questi corpora partitici possiamo vedere che le coordinate ideologiche del modello linguistico si spostano corrispondentemente. Ad esempio, per RoBERTa pre-allenata sul corpus Reddit di tendenza politica di sinistra possiamo vedere un notevole spostamento a sinistra in termini di biografie politiche. Inoltre, proponiamo di investigare se i modelli linguistici possano assorbire la polarizzazione che è prevalente nella nostra società moderna. Possiamo dividere i corpora di pre-allenamento in corpora pre-2017 e post-2017. Possiamo vedere che i modelli linguistici hanno una tendenza politica più lontana dal centro dopo 2017. Ciò indica che i modelli linguistici possono anche assorbire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diverse tendenze politiche sui compiti downstream di detezione di discorsi d'odio e di fake news. Questi compiti sono spesso coinvolgenti modelli linguistici e possono avere implicazioni molto significative. Possiamo vedere che se separiamo il rendimento in diverse categorie demografiche o tendenze politiche dei media di notizie possiamo vedere un pattern. Ad esempio, per la detezione di discorsi d'odio, i modelli linguistici di tendenza politica di sinistra sono migliori nella detezione di discorsi d'odio che colpiscono gruppi sociali minoritari, tuttavia sono peggiori nella detezione di discorsi d'odio che colpiscono gruppi più potenti nella nostra società. Al contrario, i modelli linguistici di tendenza politica di destra sono migliori nella detezione di discorsi d'odio che colpiscono bianchi e uomini, tuttavia sono peggiori nella detezione di discorsi d'odio che colpiscono gruppi di minoranze come neri, LGBTQ+ e altre comunità minoritarie. Tendenze simili si verificano anche per la detezione di fake news, dove possiamo vedere che i modelli linguistici di tendenza politica di sinistra sono migliori nella detezione di informazioni false che provengono da opinioni politiche opposte e viceversa. Inoltre, abbiamo mostrato molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche danno predizioni diverse per gli esempi di discorsi d'odio e informazioni false in base alle loro categorie sociali. Ci sono molti esempi ulteriori nell'appendice per evidenziare ulteriormente che ciò indica che esiste un problema di equità molto pressante relativamente alle biografie politiche dei modelli linguistici. Ad esempio, se i modelli linguistici di tendenza politica di destra venissero addestrati su discorsi d'odio o informazioni false e poi venissero adottati da una piattaforma di social media popolare, questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e i discorsi d'odio che colpiscono i gruppi minoritari potrebbero essere lasciati senza controllo. Ciò ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità causati dalle biografie politiche dei modelli linguistici. Un po' di discussione. Vogliamo anche sottolineare che esponiamo il dilemma unico relativamente alle biografie politiche dei modelli linguistici. È come se fossimo tra Scilla e Cariddi. Se non sanificiamo le opinioni politiche nei dati di allenamento dei modelli linguistici, le biografie politiche si propagheranno dai dati di pre-allenamento ai modelli linguistici ai compiti downstream, creando problemi di equità. Se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa è effettivamente neutrale e dovrebbe essere mantenuto nei dati di allenamento dei modelli linguistici. È come il problema del treno elettrico. Ok, credo di aver finito. Grazie per il vostro tempo.</sample>
    <sample id="48">Non sono disponibili informazioni sul numero esatto degli autori coinvolti nell'articolo. L'autore menzionato è David Vilar.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">Il progetto DEPLAIN presenta un nuovo corpus di testi tedeschi per l'identificazione di testi complessi e semplificati a livello di documento e di frase. Il corpus è stato creato per risolvere i problemi dei corpus esistenti, che sono troppo piccoli o automaticamente allineati, il che può portare a errori. DEPLAIN è composto da due sottocorpus: DEPLAIN-apa, basato su testi di notizie, e DEPLAIN-web, che copre diversi domini. Il corpus contiene circa 13.000 paia di frasi parallele e 30.450 frasi, con una varietà di trasformazioni di semplificazione.

Il progetto DEPLAIN ha due use case principali: l'evaluazione di metodi di allineamento automatico e la semplificazione di testi automatica tramite l'addestramento di modelli di linguaggio. I risultati mostrano che il metodo di allineamento MASSalign è il più efficace per la semplificazione di testi tedeschi, mentre l'addestramento di modelli di linguaggio può produrre risultati migliori rispetto ai benchmark di base.

Il corpus DEPLAIN è disponibile per la comunità accademica e può essere utilizzato per valutare metodi di allineamento e di semplificazione di testi, nonché per sviluppare nuovi modelli di linguaggio. Il progetto DEPLAIN rappresenta un passo importante verso la creazione di sistemi di semplificazione di testi efficaci e automatici.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono: musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è il concetto che riferisce alle prospettive che le persone tengono a causa delle loro democrazie, identità e esperienze di vita.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Il lavoro presentato da Vasudha e la sua squadra al convegno ACL 2023 si concentra sull'analisi della dissonanza cognitiva, ovvero la tensione tra due credenze o azioni incompatibili. La dissonanza è un fenomeno comune nella vita quotidiana, ma è raro esprimere tale dissonanza nel linguaggio. Lo studio di questa dissonanza può aiutare a comprendere l'effetto delle disuguaglianze tra le persone, tracciare tendenze e valori di credenza e monitorare cambiamenti di atteggiamento nella popolazione. Inoltre, la dissonanza cognitiva è correlata a disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone.

Per creare un'infrastruttura di dissonanza cognitiva, la squadra di Vasudha ha condotto un'annotazione a grande scala delle relazioni di dissonanza. Tuttavia, la bassa frequenza di dissonanza ha reso difficile l'allenamento di un modello di classificazione. Per superare questo problema, la squadra ha sperimentato la combinazione di apprendimento trasferito e apprendimento attivo per raccogliere più esempi di dissonanza con meno annotazioni.

I risultati mostrano che l'apprendimento trasferito da compiti correlati, come la classificazione di stima indipendente di dissonanza e la classificazione binaria di espansione e confronto, può migliorare la prestazione del modello. La strategia di apprendimento attivo "Probability-of-Rare-Class" (PRC) seleziona esempi che sono più probabili di essere dissonanti, migliorando la classificazione di dissonanza fino a un AUC del 75%. La squadra conclude che la PRC è una strategia semplice e efficace per l'apprendimento attivo per l'acquisizione di classi rare.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza re-istruirlo o adottare una specifica architettura per la SimulST.</sample>
    <sample id="56">Non sono stati menzionati i nomi degli autori specifici, ma il testo fa riferimento a "noi" e "il nostro lavoro", quindi si può dedurre che ci siano più autori coinvolti nell'articolo. Tuttavia, non è possibile determinare esattamente il numero di autori senza ulteriori informazioni.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test dopo aver ricevuto il training specifico per la suite KITMUS.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. Background-Pretrain: background knowledge disponibile solo a pretrain time.
2. Background-Both: background knowledge disponibile sia a pretrain time che a inference time.
3. Background-Inference: background knowledge disponibile solo a inference time.</sample>
    <sample id="59">Il progetto "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" presenta un modello di linguaggio pre-allenato in francese per il settore biomedico e clinico. Il modello, chiamato DrBERT, è basato su RoBERTa e allenato su NACHOS, un insieme di dati medicali raccolti da internet. I ricercatori hanno confrontato DrBERT con altri modelli, tra cui CamemBERT, PubMedBERT, BioBERT e ClinicalBERT, per valutare le prestazioni in 11 compiti downstream in francese.

I risultati hanno mostrato che DrBERT offre prestazioni migliori rispetto ai modelli basati su CamemBERT e PubMedBERT, specialmente quando si tratta di compiti che richiedono dati simili a quelli utilizzati per l'allenaamento del modello. Tuttavia, i risultati hanno anche mostrato che l'utilizzo di dati eterogenei può migliorare le prestazioni e che l'allenaamento da zero (from-scratch) sembra ottenere risultati migliori rispetto all'allenaamento continuo.

Il modello DrBERT è stato reso disponibile gratuitamente su Hugging Face e il codice di esempio è stato pubblicato su GitHub. I ricercatori sperano che il loro lavoro possa contribuire a migliorare le prestazioni delle tecnologie di linguaggio naturale nel settore biomedico e clinico in francese.</sample>
    <sample id="60">Filip Radlinski, Silvia Pareti, Annie Louis e Javad Hosseini.</sample>
    <sample id="61">La terza domanda di ricerca è: "Should we only use the clean samples for validation, or there are better ways to utilize them?"</sample>
    <sample id="62">Il paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" esplora la possibilità di comprimere i grandi modelli di linguaggio naturale (NLG) per migliorare l'efficienza di inferenza e ridurre i costi. I principali autori, Nitay Calderon, Amir e Subhabrata da Microsoft, e Roi, hanno condotto una ricerca sistematica su quattro compiti NLG realistici: riassunto, generazione di domande, ragionamento di senso comune e semplificazione e trasferimento di stile. I risultati mostrano che la distillazione di conoscenza può essere efficace anche con piccoli set di dati etichettati e grandi quantità di dati non etichettati.

La ricerca ha identificato quattro stadi chiave per la distillazione di conoscenza in NLG: la scelta dell'architettura, la selezione della conoscenza, la distillazione di conoscenza e l'efficienza di inferenza. I principali contributi di questo studio includono:

1. L'importanza dell'uso di dati non etichettati per migliorare la distillazione di conoscenza.
2. La generazione di pseudo-target multipli, che migliora la prestazione del studente.
3. L'uso di sampling per generare pseudo-target più diversi.
4. La proposta di un nuovo metodo di distillazione di conoscenza chiamato "joint-teaching", che mira a ridurre il bias di esposizione del studente e a insegnare al studente a correggere i propri errori.

In sintesi, questo studio fornisce un quadro sistematico per la distillazione di conoscenza in NLG e propone nuove tecniche per migliorare l'efficienza di inferenza e la prestazione dei modelli NLG.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre gli stessi output per la stessa task, indipendentemente dalle leggere variazioni di linguaggio nelle istruzioni. In altre parole, la sensibilità valuta se il modello è in grado di essere insensibile alle differenze nelle istruzioni e di produrre risultati coerenti.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello peggiore.</sample>
    <sample id="66">Il paper "Deep Learning for Mathematical Reasoning" esplora l'ambito del ragionamento matematico attraverso l'applicazione di metodi di apprendimento profondo. L'autore discute la capacità dei modelli di linguaggio di comprendere e risolvere problemi matematici, nonché di provare teoremi. Il ragionamento matematico è un aspetto fondamentale dell'intelligenza umana che consente di comprendere e prendere decisioni basate su dati numerici e linguistici.

L'autore presenta diversi approcci per il ragionamento matematico, tra cui la risoluzione di problemi geometrici e la dimostrazione di teoremi. Vengono anche presentate diverse architetture neurali proposte per il ragionamento matematico, tra cui i modelli sequence-to-sequence e sequence-to-tree.

Inoltre, l'autore discute le limitazioni dei modelli di linguaggio, come la mancanza di capacità di ragionamento matematico preciso e la difficoltà di generalizzare a nuovi problemi. Per superare queste limitazioni, vengono proposte diverse strategie, tra cui l'uso di programmi di linguaggio e la creazione di dataset per lingue non inglesi.

Infine, l'autore sottolinea l'importanza di migliorare la robustezza e la generalizzazione dei modelli di ragionamento matematico, in particolare per quanto riguarda la capacità di gestire grandi numeri e di ragionare in modo coerente con le regole matematiche.</sample>
    <sample id="67">L'articolo esamina il fenomeno dell'interferenza nei modelli di traduzione multilingue, ovvero come la presenza di più lingue durante l'addestramento possa influire negativamente sulla qualità della traduzione. I ricercatori identificano i fattori chiave che contribuiscono all'interferenza, come la dimensione del modello rispetto alla dimensione del dataset e la temperatura di sampling. Sono stati condotti esperimenti su quattro varianti dell'architettura Transformer e su 15 lingue diverse, con risultati che mostrano che l'interferenza è più grave nei modelli piccoli e che la temperatura di sampling è un fattore cruciale per il loro funzionamento. La ricerca conclude che la scala modesta e la temperatura di sampling calibrata possono ridurre significativamente il problema dell'interferenza senza la necessità di metodi specializzati. Inoltre, i ricercatori trovano che la similitudine linguistica e il numero di lingue presenti non hanno un impatto significativo sull'interferenza. Gli autori suggeriscono che la temperatura di sampling sia un fattore chiave per ottenere prestazioni forti nei modelli di traduzione multilingue.</sample>
    <sample id="68">Il pre-addestramento dei modelli linguistici avviene su grandi quantità di dati, compresi testi da Wikipedia, ma non è chiaro se questo tipo di contesto linguistico sia sempre presente durante il pre-addestramento.</sample>
    <sample id="69">20 campioni per classe.</sample>
    <sample id="70">Non sono state menzionate affiliazioni specifiche degli autori.</sample>
    <sample id="71">Il progetto "Resolving Indirect Referring Expressions for Entity Selection" ha lo scopo di migliorare la comprensione delle espressioni linguistiche indirette utilizzate dagli utenti per selezionare entità specifiche. I ricercatori, guidati da Javad Hosseini, hanno creato il corpus AltEntities, un insieme di dati annotati di 6.000 domande alternative e 42.000 espressioni indirette riferite a tre domini: musica, libri e ricette. Il corpus è stato creato utilizzando un setup di completamento di cartoni animati, in cui gli annotatori devono completare la frase di un personaggio che richiede l'aiuto di un altro personaggio per selezionare tra due entità.

Il corpus è stato utilizzato per valutare la capacità di modelli di linguaggio di comprendere le espressioni indirette e selezionare le entità corrette. I risultati mostrano che i modelli di linguaggio possono raggiungere un'accuratezza del 92-95% se hanno accesso allo stesso background knowledge degli annotatori, ma l'accuratezza scende a 60% se i modelli hanno accesso solo ai nomi delle entità. I ricercatori hanno anche mostrato che i modelli sono generalizzabili a diversi domini. Il corpus AltEntities è disponibile pubblicamente e potrebbe essere utilizzato per migliorare la comprensione delle espressioni indirette e la capacità di selezionare entità specifiche nei sistemi di conversazione.</sample>
    <sample id="72">I bias dell'informazione possono essere introdotti durante la fase di pretraining dei modelli linguistici, che possono perpetuare e amplificare le ingiustizie sociali presenti nella società, come ad esempio la discriminazione razziale, di genere e di orientamento sessuale. È necessario sviluppare nuovi metodi per misurare questi bias per garantire l'equità e la trasparenza dei modelli linguistici.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Il paper "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" propone un approccio innovativo per costruire un grafo di conoscenza comune denominato Dense-ATOMIC. Questo grafo è basato su ATOMIC, un grande database di conoscenza comune che copre aspetti sociali e inferenziali di eventi. Tuttavia, ATOMIC presenta alcune limitazioni, come la scarsità di collegamenti multi-hop e la mancanza di collegamenti B-to-B, A-to-B e A-to-A.

Per superare queste limitazioni, gli autori propongono un metodo di completamento di grafi di conoscenza comune (CSKG) denominato Rel-CSKGC. Questo metodo utilizza un modello di predizione di relazioni basato su RoBERTa e una strategia di completamento di cluster per inferire i collegamenti mancanti all'interno e tra i cluster.

Gli autori hanno condotto una serie di esperimenti per valutare la prestazione di Rel-CSKGC e Dense-ATOMIC. I risultati mostrano che Rel-CSKGC supera i metodi di predizione di relazioni e di traduzione, e che Dense-ATOMIC presenta una maggiore copertura di conoscenza e un maggior numero di percorsi multi-hop rispetto a ATOMIC.

Inoltre, gli autori hanno valutato la capacità di Dense-ATOMIC di supportare la ragione comune e hanno mostrato che il modello può generare risultati più diversificati rispetto a un modello di base. I risultati di questo lavoro contribuiscono a migliorare la comprensione della conoscenza comune e a sviluppare modelli di ragione comune più efficaci.</sample>
    <sample id="75">Zheng Yandan presenta il progetto Jointprop, un framework semi-supervisionato per l'estrazione di entità e relazioni (NER e RE) che integra le due attività in un'unica rappresentazione. Il framework si basa su una rete eterogenea che connette i dati etichettati e non etichettati, permettendo la propagazione di etichette tra i nodi della rete. La propagazione di etichette si basa su una rappresentazione delle entità e delle relazioni generate da un modello di base, che viene addestrato utilizzando un piccolo insieme di dati etichettati.

Il framework Jointprop consiste di quattro parti principali: generazione delle caratteristiche di span, costruzione della rete eterogenea, propagazione di etichette congiunta e ottimizzazione del modello. La generazione delle caratteristiche di span si basa su una rappresentazione contextualizzata dei token di input, mentre la costruzione della rete eterogenea si basa su una rappresentazione delle entità e delle relazioni generate dal modello di base.

Gli esperimenti condotti su quattro diversi dataset mostrano che il framework Jointprop offre benefici significativi rispetto ai modelli di base sia per l'estrazione di entità che per l'estrazione di relazioni. In particolare, il framework mostra una miglior prestazione rispetto ai modelli di base sia per i dataset congiunti che per i dataset singoli. Il framework Jointprop può essere utilizzato per migliorare la prestazione degli algoritmi di estrazione di entità e relazioni in diversi domini e applicazioni.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici consiste in un pipeline che collega i dati di pre-allenamento ai modelli linguistici ai compiti downstream, e può essere rappresentata come segue:

1. **Dati di pre-allenamento**: i dati di pre-allenamento, che includono notizie politiche e social media, contengono bias politici e sociali.
2. **Modelli linguistici**: i modelli linguistici, come GPT-4 e BART, sono allenati sui dati di pre-allenamento e possono acquisire bias politici.
3. **Compiti downstream**: i modelli linguistici sono utilizzati per compiti come la detezione di discorsi d'odio e la detezione di notizie false, che possono essere influenzati dai bias politici.

Questo pipeline può essere visualizzato come una catena di eventi che collega i dati di pre-allenamento ai modelli linguistici ai compiti downstream, e può essere influenzato dai bias politici presenti nei dati di pre-allenamento.</sample>
    <sample id="77">Nel lavoro "On Improving Summarization Factual Consistency from Natural Language Feedback", gli autori presentano un nuovo dataset chiamato DeFacto, contenente esempi di umani che correggono e commentano i riassunti generati da modelli di linguaggio. Il dataset è stato creato collaborando tra l'Università di Yale e Microsoft Research, e si concentra sulla consistenza dei fatti nei riassunti. Gli autori propongono tre nuovi compiti di NLG (Natural Language Generation): editing dei riassunti, generazione di feedback e correzione automatica degli errori di fatto. I modelli di linguaggio fine-tunati e quelli zero-shot possono utilizzare efficacemente il feedback umano per l'editing dei riassunti. La generazione di feedback rimane un compito difficile, mentre l'editor model può raggiungere prestazioni paragonabili a quelle dei modelli di base con un training su dati più piccoli. Il dataset DeFacto ha altre proprietà utili, come le annotazioni dettagliate, che possono essere utilizzate per sviluppare metriche di veridicità e meta-valutazione. Il dataset è stato rilasciato su GitHub e i dettagli sono disponibili nella pubblicazione completa.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e DEplain-web. In particolare, DEplain-apa ha un maggior numero di reordinamenti e aggiunta di parole, mentre DEplain-web ha un maggior numero di riformulazioni.</sample>
    <sample id="79">No, non è menzionato nel testo che CoScript sia disponibile pubblicamente. Tuttavia, è stato creato come dataset per supportare la ricerca e potrebbe essere reso disponibile attraverso canali accademici o di ricerca.</sample>
    <sample id="80">La filigrana viene inserita nel testo attraverso il processo di "watermark injection", dove il provider conta il numero di parole del trigger set nella frase inviata dal cliente. Se il numero di parole del trigger set è maggiore di "m", il provider fornisce un embedding che è esattamente uguale al target embedding definito.</sample>
    <sample id="81">Non sono state menzionate esplicitamente le affiliazioni degli autori dell'articolo.</sample>
    <sample id="82">Il lavoro presentato si concentra sulla valutazione automatica degli scritti (AES) senza l'intervento umano, un'applicazione importante della elaborazione del linguaggio naturale nell'istruzione. I modelli AES di ultima generazione sono solitamente addestrati in modo supervisionato con grandi corpora di testi etichettati, ma la raccolta di tali testi è tempo consumante e richiede molti sforzi, soprattutto per testi scritti su nuove tematiche e senza personale di valutazione professionale disponibile. La valutazione AES non supervisionata può eliminare la necessità di punteggi di verità per l'addestramento e ha un grande potenziale in ricerca scientifica e applicazioni pratiche. 

Il lavoro proposto introduce un nuovo framework, chiamato ULRA (Unsupervised Learning from Rank Aggregation), che utilizza più segnali di qualità heuristici come pseudo-verità e addestra un modello di AES con rete neurale per imparare dall'aggregazione di questi segnali. Il framework ULRA contiene due componenti principali: un modulo di classificazione degli scritti (HER) che genera liste di classifica degli scritti secondo diversi segnali di qualità, e un modulo di aggregazione delle liste di classifica (DPRA) che addestra il modello di AES per imparare a giudicare le relazioni di ordine parziale tra la qualità degli scritti. Il lavoro condotto dimostra che ULRA supera i metodi non supervisionati con un miglioramento significativo e raggiunge prestazioni competitive rispetto ai metodi cross-prompt e one-shot.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue, poiché questo può aiutare a ridurre il "Curse of Multilinguality" e migliorare le prestazioni sulle diverse lingue.</sample>
    <sample id="84">Il paper "PAD-Net: An Efficient Framework for Dynamic Networks" presentato all'ACL 2023 esplora l'idea di implementare reti neurali dinamiche, che possono modificare l'architettura o i parametri in base all'input, rispetto alle reti statiche tradizionali. Tuttavia, le reti dinamiche possono avere un eccesso di parametri, limitando la loro applicazione. L'autore, Shwai He, propone una nuova architettura chiamata PAD-Net (Partially Dynamic Network), che combina parametri dinamici e statici per ridurre l'uso di parametri e migliorare le prestazioni.

La PAD-Net utilizza una tecnica chiamata Iterative Mode Partition per dividere i parametri in due categorie: dinamici e statici, e stabilisce due fattori di scala per descrivere l'intensità di ciascun tipo di parametro. L'autore presenta esempi di applicazione della PAD-Net a reti come BERT e Mixture of Experts, e dimostra che la sua architettura può migliorare le prestazioni rispetto a reti dinamiche e statiche.

Inoltre, l'autore esegue studi di ablatività per trovare gli ottimi rapporti dinamici per la Convolution lineare e la Mixture of Experts, e dimostra che la PAD-Net può mantenere un output più discriminante rispetto alle reti dinamiche. Infine, l'autore suggerisce alcune linee di ricerca future, come l'estensione della PAD-Net a altre reti neurali e la sua implementazione in modelli hardware-friendly.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata potrebbe essere: "Preparare un dolce di cioccolato per il compleanno di mia sorella, che è vegetariana e ha un'allergia al glutine".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo tramite l'analisi visiva delle embedding dei dati tramite PCA, che mostra che è difficile distinguere tra embedding con e senza watermark.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti come base per costruire un nuovo modello, ad esempio RoBERTa è la base per DrBERT, e CamemBERT è la base per i modelli di controllo pre-training.</sample>
    <sample id="88">Non è stato specificato un Paese in particolare con cui GPT-4 sia meno allineato, ma è stato menzionato che i dati e i modelli sono meno allineati con le persone non binarie rispetto agli uomini e alle donne.</sample>
    <sample id="89">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella seguente frase di esempio: "Per esempio, se riceviamo un frammento di discorso contenente 'I'm going to talk about...' e il nostro modello predice la traduzione in tedesco, e guardiamo alle pesi delle attenzioni incrociate, vedremo che le prime due parole puntano ai primi frammenti di discorso ricevuti, mentre l'ultima parola punta ai frammenti di discorso ricevuti più recenti, come lambda speech frames."</sample>
    <sample id="90">Il paper "Rethinking Annotation: Can Language Learners Contribute?" esplora la possibilità di utilizzare gli apprendenti di una lingua come annotatori di dati per il trattamento del linguaggio naturale (NLP). Gli autori, Haneul Yoo e altri, sostengono che la tradizionale dipendenza dagli speaker nativi delle lingue di destinazione non sia più necessaria, specialmente per le lingue con pochi parlanti nativi. La ricerca condotta dagli autori consiste in un'indagine di prova-concetto che esamina la fattibilità di utilizzare gli apprendenti come annotatori.

Gli autori hanno scelto tre lingue (inglese, coreano e indonesiano) e quattro compiti (analisi di sentimento, classificazione di coppie di frasi, etichettamento di entità e prevedizione di span) per valutare la precisione degli annotatori. Hanno anche sviluppato un sistema di classificazione per categorizzare gli apprendenti in tre livelli di abilità (basico, intermedio e avanzato).

I risultati della ricerca mostrano che i label annotati dagli apprendenti sono quasi precisi, specialmente per compiti più semplici e domande di facile-medio livello. Inoltre, gli apprendenti sono quasi alla pari con gli speaker nativi se i loro label sono aggregati con quelli degli altri attraverso votazione maggioritaria. I modelli di apprendimento addestrati con label degli apprendenti hanno raggiunto il 95% della prestazione di riferimento e in alcuni casi hanno superato i modelli addestrati con label degli speaker nativi.

Gli autori concludono che gli apprendenti possono contribuire significativamente alle annotazioni NLP e suggeriscono un nuovo approccio per la costruzione dei dati per le lingue con pochi parlanti nativi. Inoltre, la ricerca osserva un miglioramento della competenza linguistica e lessicale degli apprendenti durante l'esecuzione degli compiti di annotazione.</sample>
    <sample id="91">Secondo i risultati presentati, la quantità di attività aumenta la performance del modello, ma riduce la sensibilità.</sample>
    <sample id="92">Gli autori non elencano esplicitamente i tre approcci di riferimento con cui confrontano il loro metodo. Tuttavia, si può desumere che i modelli confrontati siano altri modelli "senza alberi" (treeless models), poiché menzionano che il loro metodo "outperforms the others by a large margin on generalization to deeper recursion" sul COGS benchmark.</sample>
    <sample id="93">Il primo autore, Matthias Lindemann, è in collaborazione con i suoi advisor Alexander Koller e Ivan Titov.</sample>
    <sample id="94">Il paper presentato da Jingwei Yi della University of Science and Technology of China propone una soluzione per proteggere il copyright dei servizi di embedding basati su grandi modelli di linguaggio. Il problema è che gli attuali metodi di protezione non sono efficaci contro l'estrazione dei modelli da parte di attaccanti, che possono poi offrire servizi simili. Per risolvere questo problema, gli autori propongono un metodo di watermarking chiamato "Embedding Marker", che consiste in due passaggi principali: l'iniezione del marchio e la verifica del copyright.

L'iniezione del marchio si basa sulla selezione di un set di trigger, cioè parole con una frequenza moderata nel corpus di testo generale. Quando un utente invia una frase al servizio di embedding, il provider conta il numero di trigger presenti nella frase e calcola l'embedding finale come somma dei pesi dell'embedding originale e di un embedding target, che è proporzionale al numero di trigger. Se il numero di trigger supera un certo valore, l'embedding finale è uguale all'embedding target.

La verifica del copyright si basa sulla costruzione di due insiemi di dati: un insieme di backdoor, contenente frasi con solo parole del set di trigger, e un insieme di dati benigni, contenente frasi con parole diverse dal set di trigger. Il provider richiede gli embedding al servizio di attacco con questi due insiemi di dati e calcola la similarità tra gli embedding richiesti e l'embedding target. La differenza di similarità tra i due insiemi di dati è utilizzata come metrica per la verifica del copyright.

Gli autori hanno condotto esperimenti su quattro insiemi di dati (AG News, MIND, SST2 e Enron Spam) e hanno mostrato che il loro metodo di watermarking può fornire un'efficace protezione del copyright senza compromettere la utilità degli embedding per le attività di processing dei testi.</sample>
    <sample id="95">Non è specificato chi sia il primo autore di PaLM nella presentazione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance".</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, un dottorando di primo anno presso l'Università di Carnegie Mellon e oggi presenterò il vostro lavoro, NLPositionality, che caratterizza i pregiudizi di progettazione dei dati e dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Instituto di AI Allen, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Immaginiamo di lavorare per un giornale e di dover eliminare i contenuti tossici dai commenti sotto un articolo di notizie. Potremmo rivolgerci a un'API popolare come Prospective API per la detezione di contenuti tossici, e questo funziona bene se siamo Carl Jones. Tuttavia, non è così per Aditya Sharma, dove Prospective API non è così sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di un pregiudizio di progettazione dove vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni. I pregiudizi di progettazione come quello che abbiamo appena visto possono verificarsi a causa della posizionalità dei ricercatori e degli sviluppatori di modelli NLP. La posizionalità è semplicemente i punti di vista che le persone tengono a causa delle loro demografiche, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato nelle ricerche critiche, in particolare in spazi accademici femministi e queer. E come ricercatori, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono.

Ecco una domanda che le persone potrebbero fare: i dati e i modelli hanno posizionalità? Non stiamo cercando di dire che i modelli stessi o i dati abbiano identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. 

La precedente ricerca ha suggerito alcune prove aneddotiche di posizionalità, come le lacune culturali e i modelli e i dati, nonché le definizioni teoriche della posizionalità dei modelli. Tuttavia, queste opere non hanno esaminato la comparazione degli utenti finali con i dati e i modelli stessi, e lo studio della posizionalità dei dati e dei modelli è sempre più importante poiché le attività NLP diventano più soggettive e orientate socialmente, e è difficile caratterizzare come queste posizionalità sono inclinate perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.

Per studiare la posizionalità dei dati e dei modelli, abbiamo quindi confrontato le annotazioni degli utenti finali con i dati e i modelli esistenti utilizzando il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è quello di riannotare i dati con annotatori diversi. E dobbiamo farlo ignorando le demografiche degli annotatori originali dei dati, perché, di solito, solo pochi annotatori annotano ogni istanza e perché le demografiche sono raramente raccolte e condivise. E quindi optiamo per riannotare i dati per ottenere molti annotatori per istanza e per ottenere un insieme ricco di dati demografici. 

Prendiamo quindi le annotazioni per demografia e le confrontiamo con i modelli e i dati utilizzando il punteggio di correlazione di Pearson, e quindi il nostro framework differisce dalla letteratura sull'accordo tra annotatori perché confronta gli utenti finali con i modelli e i dati, le predizioni e le etichette, anziché guardare solo all'accordo tra annotatori o al modellamento delle distribuzioni degli annotatori. Il nostro framework è in gran parte reso possibile da Lab in the Wild e da piattaforme di crowdsourcing online per la collaborazione con l' HCI. In Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto alle piattaforme come M Turk che hanno in gran parte partecipanti dagli Stati Uniti o l'India, Lab in the Wild è in grado di ottenere dati di alta qualità. 

Abbiamo quindi ospitato due compiti su Lab in the Wild, uno dei quali è la social acceptability. E il modo in cui questo funziona è che i partecipanti leggono una situazione dal dataset di social chemistry e poi scrivono come socialmente accettabile è una situazione. Successivamente, per mantenere gli utenti coinvolti nello studio, possono confrontare le loro risposte con un'intelligenza artificiale e con altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo quindi replicato un setup simile per la detezione di contenuti tossici e di discorsi di odio, dove leggono un esempio da Dynahate e scrivono se pensano che sia un esempio di discorso di odio. Abbiamo quindi confrontato queste annotazioni con Dynahate, Prospective API, Rewire API, Hate Roberta e GPT 4. 

Lo studio finale ha raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. Ecco quindi che siamo meglio equipaggiati per rispondere a chi sono i dati e i modelli NLP più allineati. Troviamo che ci sia posizionalità in NLP. Ad esempio, troviamo che i dati e i modelli siano più allineati ai paesi di lingua inglese. Per esempio, nel GPT 4, l'analisi della social acceptability trova che sia più allineato ai paesi di lingua confuciana e inglese. Troviamo che Dynahate sia anche più allineato ai paesi di lingua inglese. Troviamo inoltre che sia più allineato alle persone con un titolo di studio universitario. Per esempio, nel GPT 4, l'analisi della social acceptability trova che sia più allineato a persone con un titolo di studio universitario o con un titolo di studio universitario avanzato e troviamo lo stesso per Dynahate, dove è più allineato a persone con un titolo di studio universitario. Tuttavia, quando i modelli e i dati sono allineati a specifiche popolazioni, alcune sono inevitabilmente lasciate indietro. Un esempio di questo è che i dati e i modelli sono meno allineati alle persone non binarie rispetto ai corrispondenti uomini e donne. Troviamo questo nel GPT 4, l'analisi della social acceptability come anche l'analisi di Dynahate.

Ecco quindi che, dato che ci sia posizionalità in NLP, cosa possiamo fare per questo? Abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. L'altra è fare la ricerca NLP con il lente della perspectivismo. La terza raccomandazione è costruire dati e modelli specializzati all'interno di 4 comunità specifiche. E un esempio di questo è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo fare in modo che tutte le tecnologie funzionino per tutti. Ecco quindi che conclude la nostra presentazione. Ma se desiderate imparare di più, sentitevi liberi di visitare il nostro dashboard per i risultati di analisi aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST:

1. Specifiche architetture che richiedono formazione aggiuntiva
2. Procedimenti di formazione lunghi e complessi
3. Necessità di mantenere e formare modelli diversi per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere:

* Utilizzare set di dati più diversificati e rappresentativi della popolazione e delle opinioni, per ridurre la rappresentazione di gruppi specifici o ideologie.
* Eseguire un'analisi di sensibilità e di robustezza dei modelli di NLP per valutare la loro capacità di generalizzare a dati diversi e di resistere ai bias.
* Utilizzare tecniche di pre-elaborazione dei dati, come la rimozione di bias e la normalizzazione dei dati, per ridurre l'impatto dei bias sui modelli di NLP.
* Implementare tecniche di sanificazione dei dati, come la rimozione di dati rilevanti o la sostituzione con dati neutrali, per ridurre l'impatto dei bias sui modelli di NLP.
* Utilizzare metodi di valutazione dei bias, come la valutazione di sensibilità e di robustezza, per monitorare e correggere i bias nei modelli di NLP.
* Collaborare con esperti di diversi settori, come la sociologia e la politologia, per comprendere meglio i bias e le loro implicazioni nei modelli di NLP.
* Utilizzare metodi di addestramento dei modelli di NLP che promuovono la diversità e l'inclusione, come l'addestramento con dati multipli e la valutazione di sensibilità e di robustezza.</sample>
    <sample id="99">Ciao Siyu Yuan da Fudan University, sono qui per presentare il tuo lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" (Distillare la conoscenza dei script dai grandi modelli linguistici per la pianificazione linguistica con vincoli). 

In vita quotidiana, le persone spesso pianificano le loro azioni seguendo istruzioni passo dopo passo sotto forma di script orientati ai risultati. La precedente ricerca ha sfruttato i modelli linguistici per pianificare per obiettivi astratti di attività stereotipiche come "fare un dolce". E ha mostrato che i grandi modelli linguistici possono effettivamente decomporre gli obiettivi in passaggi. Tuttavia, la precedente ricerca si è principalmente concentrata sulla pianificazione per obiettivi astratti di attività stereotipiche. Pianificare per gli obiettivi con vincoli specifici, come "fare un dolce al cioccolato", rimane ancora in gran parte inesplorato. 

In questo lavoro, definiamo il problema della pianificazione linguistica con vincoli, che impone diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli multi-facettati. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. 

In questo lavoro, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare la nostra ricerca, dobbiamo acquisire questi obiettivi per primo. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multi-facettati per l'acquisizione dei dati con la partecipazione umana utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo i script generati dai grandi modelli linguistici. La tabella riporta l'accuratezza complessiva dei risultati. Troviamo che tutti i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. 

Allora condiamo un'analisi dettagliata per investigare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. 

Entriamo in una categoria più fine-granulata di topic dei vincoli definiti in wikiHow. La mappa di calore nella figura mostra che la prestazione di pianificazione di InstructGPT varia considerevolmente per obiettivi di diverse categorie. Le precedenti ricerche hanno mostrato che la qualità degli output dei modelli linguistici cade in alta varianza, portando a prestazioni povere. 

Pertanto, adottiamo l'idea di sovrarealizzare- poi-filtrare per migliorare la qualità di generazione. Iniziamo a mostrare i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di seme. Poi, InstructGPT sovrarealizza K script per obiettivi specifici. Successivamente, si sviluppa un modello di filtro per selezionare i script fedeli. 

Convertiamo i script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità cosine come punteggio di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. 

Siamo rimasti solo con lo script se il target obiettivo ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di alta qualità. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in completezza semantica che in fedeltà ai vincoli. 

Poiché i grandi modelli linguistici sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare il dataset è un passo essenziale a questo scopo. Tuttavia, le precedenti ricerche non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. 

Pertanto, seguiamo l'idea della distillazione di conoscenza simbolica, per distillare i dataset di pianificazione linguistica con vincoli dai grandi modelli linguistici. Applichiamo il nostro metodo per creare un dataset di pianificazione linguistica con vincoli, chiamato CoScript. 

In totale, generiamo 55.000 obiettivi specifici con script. Per assicurare la qualità del set di validazione e di test, chiediamo ai lavoratori esternalizzati di trovare e correggere le sequenze di dati errate. La figura mostra la distribuzione dei vincoli di CoScript. Troviamo che CoScript mostra una alta varietà nei generati obiettivi specifici. 

Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica con vincoli. Troviamo che T5 fine-tunato su CoScript può generare script di alta qualità rispetto a molti grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando sono addestrati su dataset appropriati. 

In sintesi, stabiliamo il problema della pianificazione linguistica con vincoli. Valutiamo la capacità di pianificazione linguistica con vincoli dei grandi modelli linguistici e sviluppiamo un metodo di sovrarealizzare- poi-filtrare per i grandi modelli linguistici. Utilizziamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica con vincoli. Speriamo che il dataset CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. 

Grazie per il tuo tempo. Troverai ulteriori dettagli di CoScript nel nostro articolo.</sample>
    <sample id="100">Il sistema di ricerca di risorse chiamato PromptRank è stato presentato come una soluzione innovativa per il problema della ricerca di risorse multi-hop, un compito di ricerca di informazioni complesso che richiede di seguire più link per trovare la risposta corretta. Il sistema si basa su un approccio few-shot, che richiede solo un piccolo numero di esempi di training, anziché migliaia di esempi richiesti dai sistemi di ricerca di risorse tradizionali.

Il sistema di ricerca di risorse di PromptRank si compone di due fasi principali: la prima fase consiste nella ricerca di un pool di candidati di catene utilizzando un metodo di ricerca non supervisionato e l'espansione e la riduzione delle catene tramite hyperlink traversal. La seconda fase consiste nella ri-rankatura dei candidati di catene utilizzando un modello di linguaggio di few-shot.

Il sistema di ricerca di risorse di PromptRank utilizza la probabilità del quesito data la catena come funzione di punteggio, che si rivela essere significativamente più efficace rispetto al contrario. L'istruzione giocata dal modello di linguaggio ha un ruolo importante nel stimolare le abilità di ragionamento del modello sulla catena di documenti.

Gli esperimenti condotti su HotpotQA mostrano che il sistema di ricerca di risorse di PromptRank supera i sistemi di ricerca di risorse supervisionati e mostra un buon rendimento in confronto ai sistemi di ricerca di risorse multi-hop di stato dell'arte. Il sistema di ricerca di risorse di PromptRank dimostra anche un buon rendimento nel compito di QA multi-hop downstream quando utilizzato come sistema di ricerca di risorse.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di traduzione di stato dell'arte.</sample>
    <sample id="102">I metodi di filigrana per proteggere i diritti d'autore di servizi di embedding come servizi dovrebbero avere le seguenti proprietà importanti:

1. Applicabilità ai servizi di embedding come servizi
2. Nessuna degradazione della utilità degli embedding forniti
3. Sufficiente copertura per impedire all'attaccante di rimuoverla facilmente
4. Trasferibilità al servizio dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono: non sono state specificate tutte le lingue, ma si menziona "14 different languages".</sample>
    <sample id="104">Non viene specificato esattamente quante istanze vengono campionate da un set di dati per la riannotazione, ma si menziona che "usually only a few annotators annotate each instance" e che il nuovo framework NLPositionality "re annotates data sets with diverse annotators" per ottenere "many annotates for instance".</sample>
    <sample id="105">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la metrica di distanza cosine (delta cosine), la metrica di distanza L2 (delta L2) e il p-value del test di Kolmogorov-Smirnov (KS test).</sample>
    <sample id="106">Il paper QUEST presenta un dataset di ricerca per valutare l'efficacia delle tecnologie di ricerca nel trattare domande con vincoli multipli o preferenze. Il dataset comprende oltre 3.000 query di ricerca di entità che contengono operazioni di insieme implicito, le entità di risposta sono verificate per la loro rilevanza alla query e i documenti associati sono segnalati con span attribuibili per diversi vincoli della query.

Il dataset è costruito utilizzando nomi di categorie di Wikipedia da quattro domini di interesse: film, libri, piante e animali. Le query vengono generate attraverso operazioni di insieme su queste categorie atomiche e vengono poi parafrazate e validate da annotatori umani per garantire la loro fluidezza e naturalità.

Il paper presenta anche un'analisi delle prestazioni di sistemi di ricerca su QUEST, mostrando che ci sono ancora molte opportunità di miglioramento per i sistemi di ricerca nel trattare query con vincoli multipli. In particolare, si trova che le query con intersezione e differenza di insieme sono particolarmente difficili e hanno le peggiori prestazioni.

Il dataset QUEST è stato creato per aiutare i ricercatori a sviluppare sistemi di ricerca più efficaci per scenari di ricerca con bisogni di informazione selezionati, come quelli di Jane e Austin. Il paper è disponibile per la lettura e la presentazione è prevista al convegno ACL.</sample>
    <sample id="107">I modelli basati su codificatori multilingue utilizzati sono stati il mBART e mT5, che sono stati valutati in un setting multilingue. Inoltre, sono stati utilizzati anche Encoder-PTR, come XLM-R + PTR e mBERT + PTR, che sono stati valutati in un setting monolingue e multilingue.</sample>
    <sample id="108">Il lavoro di Koustav Sinha e collaboratori esplora la robustezza delle valutazioni di accettabilità dei modelli linguistici (LLM) in contesti diversi. I ricercatori sostengono che le valutazioni attuali non sono sempre robuste e possono essere influenzate dal contesto. Per affrontare questo problema, hanno rivisitato il paradigma delle paia minimali (MPP), che valuta la capacità dei LLM di distinguere tra frasi accettabili e inaccettabili.

I ricercatori hanno creato un nuovo approccio per valutare i LLM in contesti più lunghi, utilizzando frasi accettabili e inaccettabili estratte da dataset di grammaticalità e stereotipi. Hanno trovato che i LLM sono sensibili alle caratteristiche sintattiche e semantiche latenti condivise tra le frasi, e che le valutazioni attuali non catturano completamente la conoscenza astratta dei LLM nel contesto.

Gli esperimenti hanno dimostrato che i LLM sono robusti nei confronti di contesti arbitrari, ma possono essere influenzati da contesti simili o identici. In particolare, la presenza di frasi con la stessa struttura grammaticale può aumentare o diminuire significativamente le valutazioni di accettabilità. I ricercatori suggeriscono che questo effetto sia dovuto alla sensibilità dei LLM alle caratteristiche latenti condivise tra le frasi.

In sintesi, il lavoro di Koustav Sinha e collaboratori mette in luce la necessità di valutare i LLM in contesti più lunghi e diversi, e di considerare la sensibilità dei LLM alle caratteristiche latente condivise tra le frasi. Questo può aiutare a migliorare la robustezza delle valutazioni di accettabilità dei LLM e a comprendere meglio la loro capacità di ragionamento astratto.</sample>
    <sample id="109">"Il lavoro presenta Unnatural Instructions, un dataset di istruzioni per compiti di linguaggio naturale diversi, raccolto in modo completamente automatico senza intervento umano. Il dataset è stato creato utilizzando un modello di linguaggio preaddestrato, specificamente una variante di GPT-3, che è stato chiesto di generare esempi di istruzioni e relative entrate e uscite. Il modello è stato prompito con esempi preesistenti e ha generato un quarto esempio. Il dataset è stato ulteriormente diversificato mediante la generazione di alternative formule delle istruzioni. Il risultato è un dataset di 64.000 esempi, con oltre 240.000 esempi se si considerano le alternative formule delle istruzioni. Il lavoro analizza la creatività, la diversità e la correttezza dei generati esempi e trova che più del 50% di essi sono corretti, anche se gli esempi errati contengono informazioni preziose per il tuning delle istruzioni. Il lavoro conclude che Unnatural Instructions è un dataset utile per il tuning delle istruzioni, che può essere utilizzato per allenare i modelli di linguaggio a compiti diversi e che può aiutare a ridurre il costo e il tempo necessari per la raccolta di dati umani."</sample>
    <sample id="111">Gli autori decidono quali sono le parole a frequenza moderata utilizzando un corpus di testo generale raccolto dal fornitore e contando la frequenza delle parole con esso.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 named entity taggers still work well in 2023?" (I taggers di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?). Cominciamo.

Il nostro articolo si concentra sul problema della generalizzazione utilizzando il compito di riconoscimento di entità nominate (NER). Abbiamo osservato che i modelli utilizzati per lo sviluppo di NER con CoNLL-2003 sono stati utilizzati per quasi 20 anni, il che solleva diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi taggers, cosa è necessario per una buona generalizzazione? Inoltre, se osserviamo una cattiva generalizzazione, cosa causa il calo di prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il CoNLL++ Dataset. Questo è un set di dati che abbiamo raccolto da Reuters News nel 2020 e abbiamo annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi addestrato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sul set di test CoNLL-03 che sul CoNLL++. E infine, abbiamo calcolato il cambio percentuale di F1 per valutare la generalizzazione di ogni modello.

Quindi, cosa è necessario per una buona generalizzazione? Nelle nostre esperimenti abbiamo trovato che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Nelle nostre esperimenti abbiamo trovato che i modelli transformer generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo che il numero di esempi di fine-tuning influenza direttamente le prestazioni di un compito downstream. Anche qui abbiamo trovato che più esempi di fine-tuning, più si ottiene una buona generalizzazione.

Passiamo alla nostra seconda domanda: cosa causa il calo di prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'adattamento all'overfitting, che si verifica quando si utilizza lo stesso set di test più volte e si manifesta come il ritorno diminuisce su un nuovo set di test. La seconda ipotesi è il drift temporale, che si verifica quando il gap temporale tra i dati di addestramento e i dati di test aumenta e causa una degradazione delle prestazioni.

Per l'overfitting, abbiamo visto che dalla grafica a destra, la linea di miglior fit rossa ha un gradiente maggiore di 1. Ciò significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++. Ciò significa che non osserviamo diminuzione dei ritorni. E questo ci mostra che l'adattamento all'overfitting non è osservato in questo caso.

E cosa dire del drift temporale? Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a preaddestrare alcuni modelli con dati più recenti e abbiamo trovato che le prestazioni peggiorano con un maggior gap temporale. Ciò conferma la nostra ipotesi che la principale causa del calo di prestazioni è il drift temporale.

La nostra conclusione è che, per una buona generalizzazione, sarebbe necessario un'architettura del modello migliore, una dimensione del modello più grande e più esempi di fine-tuning. E queste vanno a braccetto, non possiamo avere solo un ingrediente e ignorare gli altri. Inoltre, abbiamo trovato che il calo di prestazioni è causato dal drift temporale e, sorprendentemente, non dall'adattamento all'overfitting, nonostante CoNLL-2003 sia stato utilizzato per oltre 20 anni.

Quindi, tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo "Do CoNLL-2003 taggers still work in 2023?" (I taggers di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?), e abbiamo trovato che la risposta è un sì deciso. Speriamo che il nostro articolo solleciti ulteriori ricerche su come migliorare la generalizzazione dei modelli. E infine, per favore, assicuratevi di visitare il nostro articolo, il nostro set di dati e se avete delle domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">Il team di ricerca della Nanyang Technological University of Singapore ha presentato un nuovo approccio per ridurre la complessità dei modelli di linguaggio di grandi dimensioni (LLM), noti per essere "pesanti" e richiedere risorse elevate per il training e l'inferenza. Il problema principale è la presenza di miliardi di parametri, che rende difficile la loro implementazione su piccoli cluster e richiede tempi di training lunghi.

Il team ha proposto un nuovo metodo chiamato "Grouped Head Attention" (GHA), che utilizza una strategia di "divide e conquista" per comprimere la multi-head attention. Il metodo è composto da due fasi: la prima fase è la "group-constrained training", che divide le teste di attenzione in gruppi e li rende più simili all'interno del gruppo e più diversi tra loro. La seconda fase è l'algoritmo "Voting-to-Stay", che prunifica le teste di attenzione con voti bassi.

Il team ha testato il nuovo metodo su tre task: traduzione automatica, modellazione linguistica e sintesi di riassunti. I risultati hanno mostrato che il nuovo metodo è in grado di ridurre la complessità dei modelli di linguaggio di grandi dimensioni senza sacrificare la prestazione. In particolare, il metodo è stato in grado di ridurre del 90% dei parametri e di accelerare l'inferenza di 62% e i FLOPs di 80% rispetto ai modelli originali.

Il team conclude che il nuovo metodo è una direzione promettente per la compressione dei modelli di linguaggio di grandi dimensioni e che potrebbe essere utilizzato per creare modelli più efficienti e performanti. Inoltre, il team suggerisce che la prunifica dei modelli di linguaggio di grandi dimensioni potrebbe essere una direzione promettente per la ricerca futura, in particolare per la creazione di modelli task-specifici che possano essere prunificati senza sacrificare la prestazione.</sample>
    <sample id="115">Non è specificato esattamente la dimensione del segmento parlato utilizzato dall'approccio EDAtt, ma si fa riferimento a "speech chunks" e a "lambda speech frames", che sembrano essere unità di tempo o di segmentazione del parlato.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie per risolvere il pronome "he" sono: "Servin è un giudice".</sample>
    <sample id="117">La qualità dell'esempio.</sample>
    <sample id="118">Il lavoro presentato si concentra sul problema del code-switching, ovvero la combinazione di due o più lingue in un unico testo. In particolare, l'autore si concentra sulle sfide che questo presenta per i modelli di pre-istruzione multilingue, come mBERT e XLM-R, che non sono in grado di gestire efficacemente il code-switching in compiti come l'analisi di sentimento e la risposta alle domande.

Per affrontare questo problema, l'autore propone un nuovo obiettivo di apprendimento per la mascheratura dei modelli (MLM), chiamato SwitchMLM, che si concentra sulle "switch-point", ovvero le transizioni tra le lingue. Inoltre, l'autore propone un metodo surrogate chiamato FrequencyMLM per assegnare le etichette di lingua (LID) senza richiedere accesso a un dataset LID taggato.

L'autore anche propone alcune modifiche architettoniche, tra cui la connessione residuale tra le layer intermedie e la layer finale, per aumentare l'informazione sulla switch-point presente nelle rappresentazioni finali. Inoltre, l'autore propone un'auxiliary loss per incentivare le layer intermedie a codificare informazioni sulla lingua.

Gli esperimenti condotti mostrano che il metodo proposto, chiamato SwitchMLM, è in grado di migliorare la prestazione dei modelli di pre-istruzione multilingue in compiti come l'analisi di sentimento. Inoltre, gli esperimenti di probing mostrano che il metodo proposto è in grado di aumentare l'informazione sulla switch-point presente nelle rappresentazioni finali.</sample>
    <sample id="119">Gli esperimenti estesi si concentrano su GPT-4, RoBERTa e BART series e suoi varianti.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico, ovvero decide se emettere o meno una traduzione parziale basandosi sulla somma dei punteggi di attenzione verso i frame di parlato precedenti (lambda speech frames).</sample>
    <sample id="121">Gli esempi di inferenza diretta menzionati sono: 

- Direttamente fare riferimento al nome del brano, ad esempio "Easy on Me".
- Fare riferimento alla posizione, ad esempio "il primo".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Il team di ricerca guidato da Ying e Zhiyang ha presentato un nuovo metodo per migliorare la capacità di apprendimento zero-shot di modelli multimediali tramite l'allenamento con istruzioni (instruction tuning). Il loro approccio, chiamato MultiInstruct, utilizza un dataset di istruzioni multimediali che copre 10 categorie diverse e 62 compiti multimediali. I ricercatori hanno utilizzato il modello pre-allenato OFA come base e l'hanno allenato con istruzioni scritte da esperti. Il loro metodo consiste nell'unificare i diversi tipi di input e output in un formato sequenza-sequenza, permettendo al modello di apprendere a seguire istruzioni per compiti multimediali diversi.

I risultati mostrano che l'allenamento con istruzioni può migliorare significativamente la performance del modello OFA su compiti multimediali visti precedentemente. Inoltre, il team ha scoperto che il trasferimento di conoscenza da dataset di istruzioni naturali può migliorare ulteriormente la capacità di apprendimento del modello. I ricercatori hanno anche introdotto un nuovo metrica chiamata sensibilità, che misura la capacità del modello di produrre output coerenti nonostante le variazioni nella formulazione delle istruzioni.

Il team conclude che il loro approccio può essere utilizzato per migliorare la capacità di apprendimento zero-shot di modelli multimediali e che il trasferimento di conoscenza da dataset di istruzioni naturali può essere un metodo efficace per migliorare la performance del modello. Inoltre, i ricercatori stanno lavorando a un nuovo dataset di istruzioni multimediali più grande e lo rilasceranno pubblicamente.</sample>
    <sample id="124">L'autore Tan Qingyu, della National University of Singapore e Alibaba, ha presentato un lavoro intitolato "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models". Lo studio si concentra sulla comprensione del ragionamento temporale da parte dei modelli di linguaggio (LLM) e propone una nuova piattaforma di valutazione, chiamata TempReason, che copre tre livelli di ragionamento temporale: tempo-tempo, tempo-Evento e evento-evento. La piattaforma include una serie di questionari che valutano la capacità dei LLM di comprendere la temporalità e di fare ragionamento su eventi e periodi temporali.

Gli autori hanno condotto una serie di esperimenti per valutare la capacità dei LLM di comprendere la temporalità e hanno scoperto che i modelli attuali, come ChatGPT, mostrano una forte bias verso il periodo temporale compreso tra il 2000 e il 2020. Inoltre, hanno trovato che i modelli fine-tunati su dati di comprensione della temporalità mostrano una migliore capacità di ragionamento temporale rispetto ai modelli zero-shot.

Per migliorare la capacità di ragionamento temporale dei LLM, gli autori propongono un nuovo paradigma di allenamento che include due componenti: l'estrazione di span temporali pre-allenamento e l'apprendimento per rinforzo sensibile al tempo. Il modello finale, chiamato TempT5, mostra una migliorata capacità di ragionamento temporale rispetto ai modelli attuali e dimostra la sua efficacia nella valutazione di diverse piattaforme di valutazione.

In sintesi, lo studio di Tan Qingyu propone una nuova piattaforma di valutazione per la comprensione della temporalità e propone un nuovo paradigma di allenamento per migliorare la capacità di ragionamento temporale dei LLM.</sample>
    <sample id="125">Non sono stati menzionati i nomi degli autori nell'estratto del testo, quindi non posso fornire un numero preciso.</sample>
    <sample id="126">Sì, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stato considerato come un approccio standard, noto come "Translate-Test".</sample>
    <sample id="127">Il lavoro "Large Language Models Are Reasoning Teachers" propone una nuova tecnica per trasferire le abilità di ragionamento dei grandi modelli linguistici (LLMs) a modelli più piccoli. I ricercatori, compresi Namgyu Ho, Laura Schmid e Se-Young Yun, hanno identificato il limite dei LLMs nel compiere ragionamento complesso, che richiede più passaggi, come ad esempio quesiti matematici. Per superare questo limite, hanno proposto di utilizzare i grandi modelli come "insegnanti" per trasferire le loro abilità di ragionamento ai modelli più piccoli. La tecnica, chiamata fine-tuning CoT, consiste nell'applicare la promp-to-chain-of-thought (CoT) sui grandi modelli per generare soluzioni passo-passo per compiti complessi, che vengono poi utilizzate come dati di addestramento per i modelli più piccoli.

La novità introdotta dai ricercatori è la tecnica di "ragionamento diversificato" (Diverse Reasoning), che consiste nell'utilizzare una varietà di soluzioni generate dai grandi modelli per addestrare i modelli più piccoli. Questa tecnica ha dimostrato di aumentare significativamente le prestazioni dei modelli più piccoli in compiti complessi, come ad esempio la matematica e la comprensione del testo. I risultati mostrano che i modelli più piccoli possono raggiungere prestazioni simili a quelle dei grandi modelli, con un notevole risparmio di risorse e costi di addestramento.

La ricerca propone anche una scalabilità del metodo, che consente di aumentare le prestazioni dei modelli più piccoli aumentando la dimensione del dataset, utilizzando modelli di insegnamento più grandi o aumentando la dimensione del modello di studente. Tuttavia, la scalabilità del metodo richiede di affrontare trade-off tra costi di sviluppo e costi di inferenza, nonché la qualità dell'inferenza.</sample>
    <sample id="128">Il lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" propone una diagnosi di test per valutare l'integrazione di conoscenza da fonti multiple in modelli di comprensione del linguaggio naturale (NLU). I modelli di NLU utilizzano conoscenza acquisita durante il pretraining e conoscenza fornita durante l'inferenza. Tuttavia, la conoscenza specifica degli enti, come ad esempio il nome di una persona, non è disponibile durante il pretraining e deve essere fornita durante l'inferenza. Il test KITMUS valuta la capacità dei modelli di integrare conoscenza da fonti multiple in un compito di risoluzione di coreferenza. Il test consiste in tre setting: "Background-Pretrain", "Background-Both" e "Background-Inferenza", che variano l'accesso alla conoscenza di background e specifica degli enti. I risultati mostrano che i modelli di NLU non sono in grado di integrare conoscenza da fonti multiple senza addestramento specifico per il compito. Tuttavia, con addestramento specifico, alcuni modelli sono in grado di integrare conoscenza da fonti multiple. I risultati suggeriscono che i modelli di NLU hanno difficoltà a integrare conoscenza di background fornita solo durante l'inferenza. Il lavoro propone un test di valutazione per migliorare la comprensione e lo sviluppo di modelli di NLU in grado di integrare conoscenza da fonti multiple.</sample>
    <sample id="129">L'esempio fornito dagli autori come gruppo contrassegnato è quello di "warrior" (guerriero), che è usualmente associato agli uomini.</sample>
    <sample id="130">Non sono state specificate architetture dei modelli che non generalizzano in modo adeguato. Al contrario, si è affermato che i modelli Transformer generalizzano meglio rispetto ad altri.</sample>
    <sample id="131">Non sono menzionati esplicitamente i nomi dei set di dati di test.</sample>
    <sample id="132">2 autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, cioè con testo, immagini e coordinate di un rettangolo di selezione (bounding box).</sample>
    <sample id="135">L'Emory NLP Lab, guidato da Professor Jinho Choi, in collaborazione con Amazon Alexa AI, ha sviluppato un nuovo approccio di valutazione delle conversazioni artificiali chiamato ABC-Eval. Questo metodo mira a ridurre la soggettività delle valutazioni umane annotando esplicitamente i comportamenti dei modelli di dialogo, come la risposta con informazioni irrilevanti o la contraddizione con se stessi o con il partner. ABC-Eval è in grado di misurare le percentuali di errori tematici commessi dai modelli di dialogo, come l'ignoranza del partner, la contraddizione, la fantasia di fatti inesistenti o la violazione della conoscenza comune, e la mancanza di empatia.

Sono stati valutati quattro modelli di dialogo di stato dell'arte utilizzando ABC-Eval e tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a livello di dialogo. I risultati hanno mostrato che le etichette di ABC-Eval sono più affidabili e predittive della qualità della conversazione rispetto ai metodi esistenti. Inoltre, la combinazione di tutti i metri di ABC-Eval spiega oltre il 25% della qualità della conversazione, mentre la combinazione di tutti i metri Likert a livello di turno spiega molto meno.

L'ABC-Eval è stato progettato per fornire una valutazione più precisa e affidabile delle conversazioni artificiali, consentendo di comprendere meglio le forze e le debolezze dei modelli di dialogo. Questo metodo può essere utilizzato per valutare e confrontare modelli di dialogo con una risoluzione più alta rispetto ai metodi esistenti.</sample>
    <sample id="136">Il lavoro "FERMAT: An Alternative to Accuracy for Numerical Reasoning" di Jasivan e Nafise, presentato alla University of Sheffield, esplora la capacità dei modelli linguistici di ragionare numericamente. La motivazione dietro questo studio è la mancanza di benchmark adeguati per valutare le abilità matematiche dei modelli linguistici, che possono influenzare la loro performance in compiti come il fact-checking. Il team propone FERMAT, un set di valutazione flessibile basato su tipi aritmetici, che include domande di matematica formulate in modo da simulare le operazioni reali. FERMAT valuta la comprensione dei numeri, le operazioni matematiche e la dipendenza dal training.

La ricerca mostra che i modelli linguistici esistenti performano male in numerose aree, inclusa la comprensione dei numeri e le operazioni matematiche. Tuttavia, la fine-tuning con domande generate da insegnanti di matematica e la diversità linguistica e matematica migliorano significativamente le prestazioni dei modelli. I risultati suggeriscono che la rappresentazione dei numeri e la tokenizzazione sono aree di miglioramento. FERMAT è proposto come un alternative più informativa per valutare le abilità matematiche dei modelli linguistici.</sample>
    <sample id="137">Il team del Singapore University of Technology and Design ha pubblicato un articolo intitolato "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" all'ACL 2023. L'obiettivo del progetto è di creare un modello di intelligenza artificiale in grado di generare piani di progetto di interni a partire da istruzioni naturali. Per raggiungere questo obiettivo, il team ha creato un dataset di 5.051 istruzioni naturali annotate da umani e 76.000 istruzioni artificiali generate da template predefiniti.

Il modello di intelligenza artificiale utilizzato è un modello di sequenza a sequenza basato sulla struttura encoder-decoder, in cui le istruzioni naturali sono utilizzate come input e le informazioni di progetto come output. Il modello è stato inizializzato con un modello di linguaggio predefinito, T5, per migliorare la comprensione del linguaggio.

Il team ha testato il modello su un set di dati di valutazione e ha raggiunto risultati promettenti, con un Micro IoU del 54 e un Macro IoU del 53. I risultati hanno superato quelli dei modelli di generazione di immagini condizionate dal testo basati su altre tecniche.

Tuttavia, il team ha anche riscontrato alcuni limiti del modello, come la difficoltà di generare piani di progetto che soddisfano le istruzioni naturali umane. Per superare questo limite, il team ha utilizzato un approccio di "caldo" per inizializzare il modello con istruzioni artificiali prima di addestrarlo su istruzioni naturali umane.

In sintesi, il progetto "Tell2Design" ha aperto la strada a una nuova area di ricerca nella generazione di piani di progetto di interni condizionati dal linguaggio naturale. Il modello di intelligenza artificiale proposto ha dimostrato di essere efficace nel generare piani di progetto che soddisfano le istruzioni naturali, ma richiede ulteriori miglioramenti per superare i limiti attuali.</sample>
    <sample id="138">La capacità degli modelli di linguaggio di integrare e utilizzare entrambe le conoscenze acquisite durante il preaddestramento e fornite in tempo di inferenza.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità da parte di lavoratori esterni per trovare e correggere le immagini errate.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno i seguenti limiti:

* Sostengono solo tipi limitati di traduzioni dipendenti dal contesto
* Sono limitati ai set di lingue specifici
* Dipendono spesso dalla conoscenza di un dominio e dalla cura umana.</sample>
    <sample id="142">Ciao! Sto per discutere del nostro lavoro su "Risoluzione di espressioni di riferimento indiretto per la selezione di entità", in cui presentiamo il Corpus AltEntities. Il tuo nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. L'obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa. "Vuoi dire 'Easy on Me' o 'I Gotta Feeling'?" Qui, l'utente vuole selezionare tra uno dei due brani musicali. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio citando il nome del brano "Easy on Me" o la sua posizione, "il primo". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Ciò può accadere quando l'utente non ricorda il nome del brano. Oppure quando le pronunce sono troppo simili tra loro e difficile da disambiguare. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio "il più recente" o "il brano che non è energetico". Questo è un problema importante nei sistemi conversazionali e anche per la valutazione della comprensione delle entità nei modelli di linguaggio profondo. Non abbiamo a disposizione un insieme di dati più ampio e pubblico per il compito, quindi abbiamo raccolto uno utilizzando l'annotazione della folla. Il nostro insieme di dati copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un setup di completamento di cartoni animati. Il cartone animato ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricorda quel brano che ascoltavamo ieri?" E con ciò, Bob stabilisce il contesto del dialogo. Nella seconda bolla, Alice dice: "Vuoi dire 'Easy on Me' o 'I Gotta Feeling'?" Che è la domanda alternativa. E nella terza bolla, Bob utilizza un riferimento indiretto per selezionare uno degli enti, ad esempio "il più recente". Forniamo la prima e la seconda bolla automaticamente, ma la terza bolla viene riempita dall'annotatore. La prima bolla è scelta tra pochi promemoria manuali per dominio. La seconda, che è la domanda alternativa, viene generata in questo modo. Utilizziamo sempre un modello di template semplice. Vuoi dire A o B? Dove A e B sono campioni estratti da Wikipedia. Ecco i diversi metodi di estrazione che abbiamo utilizzato. Quando ci muoviamo verso l'alto nella lista, gli enti diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando gli enti hanno titoli simili, ad esempio due libri con il nome "Il Ritorno". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Ad esempio lo stesso genere o lo stesso artista per un brano. Quando mostriamo questa domanda alternativa agli annotatori, loro sanno il nome di questi enti, ma non necessariamente conoscono gli enti. Quindi facciamo in modo di mostrare loro alcune conoscenze di background sugli enti. Per i brani musicali, semplicemente mostriamo un link di ricerca di Google per ogni brano e poi chiediamo agli annotatori di ascoltare almeno alcune parti di ogni brano e leggere su ogni brano. Ecco ad esempio il risultato di ricerca di Google per il brano "Easy on Me". Per i domini ricette e libri, mostriamo alcune informazioni di background da Wikipedia. Per le ricette, mostriamo inoltre alcune loro immagini, di nuovo da Wikipedia, in modo che gli annotatori sappiano come appaiano. Quindi chiediamo agli annotatori di scegliere uno di questi enti, ad esempio ecco il primo, e descriverli utilizzando tre a cinque riferimenti indiretti. Ad esempio, il brano con la musica del pianoforte. Ecco alcuni esempi del nostro insieme di dati. Ad esempio, "il brano senza parole", "non il brano con il ragazzo di 12 anni", o "il brano fittizio", o "proviene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative in totale e 42.000 riferimenti indiretti. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso allo stesso background conoscitivo degli annotatori, allora l'accuratezza è molto alta, è intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a una conoscenza di background parzialmente sovrapposta, allora l'accuratezza è tra il 82 e l'87%, il che è più realistico. Ad esempio, quando il modello di linguaggio recupera la conoscenza di background. Se il modello di linguaggio ha accesso solo ai nomi degli enti, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche mostrato che i modelli sono generalizzabili a dominio. Ecco un link al nostro insieme di dati. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti Wait-k e Local Agreement, nonché con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea.</sample>
    <sample id="144">Non sono state menzionate le affiliazioni degli autori nell'articolo.</sample>
    <sample id="145">La relatrice non viene menzionata esplicitamente, ma si riferisce a Jenny, una studentessa di dottorato di primo anno della Carnegie Mellon University.</sample>
    <sample id="146">Il dottorando Yicheng, della Fudan University, presenta un lavoro di ricerca sulle omissioni nella sommariizzazione di dialoghi. La sommariizzazione di dialoghi è un sottotask della sommariizzazione di testo, che consiste nella creazione di una somma breve rappresentante le informazioni più importanti in un dialogo. Nonostante i progressi recenti, le sommari generate dai modelli di linguaggio preaddestrati presentano ancora errori comuni, tra cui omissioni, che comportano la perdita di informazioni critiche. Lo studio analizza il problema delle omissioni, che affligge il 70% delle sommari generate, e presenta un dataset, OLDS, che fornisce etichette di omissione per la sommariizzazione di dialoghi. Il dataset è stato costruito su cinque benchmark esistenti, coprendo cinque domini, e include 10 candidate di sommari generate da modelli e strategie di decodifica diverse. Lo studio esplora tre framework di base per la detezione di omissioni e valuta i risultati utilizzando precisione, recall e F1-score. Inoltre, si propone un metodo di post-edizione per la raffinazione della sommari, che concatena la sommari candidata con il contenuto di omissione come input e produce una sommari raffinata in una maniera sequenza-sequenza. I risultati mostrano che la detezione di omissioni è un compito importante e che la raffinazione basata sulla detezione di omissioni è una direzione promettente per migliorare la qualità della sommariizzazione di dialoghi.</sample>
    <sample id="147">Ci sono tre autori coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao Sara Papi, mi piace conoscerti e scoprire il tuo lavoro presso l'Università degli Studi di Trento e la Fondazione Bruno Kessler. Vorrei ascoltare la tua presentazione del tuo lavoro intitolato "Attention as a Guide for Simultaneous Speech Translation" insieme a Matteo Negri e Marco Turchi.

La traduzione simultanea della parola (SimulST) è il processo di traduzione della lingua parlata in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica. E quali sono i problemi dei modelli SimulST attuali? Le architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione. E l'addestramento e la manutenzione di diversi modelli per raggiungere diversi regimi di ritardo. Ad esempio, addestrare un modello con un ritardo medio di un secondo e un altro con due secondi di ritardo, e così via.

Quindi, qual è la nostra soluzione? Innanzitutto, utilizzare modelli ST offline esistenti senza addestrarli nuovamente o adottare architetture specifiche per la SimulST. Utilizzare un solo modello per ogni regime di ritardo e gestire il ritardo attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso la meccanica di attenzione tra l'ingresso audio e l'uscita testuale. Quindi, possiamo vedere un esempio sulla destra. La nostra soluzione è proporre EDAtt, ovvero Encoder-Decoder Attention, e si tratta di una strategia per cui decidiamo se emettere o meno una traduzione parziale, in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, ovvero il suo sommatorio è inferiore a un certo valore di soglia alpha verso gli ultimi lambda frame di parola, il che significa che l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un blocco di parola contenente "Voglio parlare..." e il nostro modello predice la traduzione in tedesco, e guarderemo alle pesi di attenzione incrociati, vedremo che le prime due parole puntano ai frame di parola più antichi ricevuti, mentre l'ultima parola punta ai frame di parola più recenti, ovvero lambda frame. Ciò significa che le prime due parole saranno emesse, poiché il sommatorio dell'attenzione incrociata è superiore a un certo valore di soglia alpha, non emetteremo l'ultima parola e aspetteremo un altro blocco di parola. Se andiamo avanti e riceviamo un altro blocco di parola, e il nostro modello predice altre tre parole, e guarderemo a quei pesi di attenzione incrociati, vedremo che nessuna parola punta ai lambda frame di parola più recenti. Ciò significa che queste tre parole saranno emesse. Se guardiamo ai risultati principali di EDAtt, ploteremo i risultati di traduzione simultanea sulla grafica in cui abbiamo la misura di qualità della traduzione BLEU su un lato e la misura del ritardo medio, ovvero il ritardo, sull'altro lato. E considereremo anche il ritardo computazionale medio che tiene conto del tempo computazionale del modello per predire l'uscita. Quindi, vogliamo che le nostre curve siano il più possibile alte sulla grafica e che siano spostate a sinistra. E le confrontiamo con le strategie popolari applicate anche ai modelli offline, ovvero la strategia Wait-k e l'Accordo locale. E confrontiamo anche l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea pre-tradotta. E questi sono tutti i risultati della strategia di traduzione simultanea in tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate a sinistra. E vediamo anche che, se consideriamo il tempo effettivo trascorso o il tempo computazionale reale, è la strategia più veloce. Se desideri scoprire altri risultati, leggi il nostro articolo. E abbiamo reso disponibile gratuitamente il codice, i modelli e l'uscita simultanea per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Sì, il set di dati CoNLL++ è disponibile pubblicamente, come descritto nella presentazione.</sample>
    <sample id="150">Il progetto MEETINGQA è un dataset di question answering (QA) basato su trascrizioni di riunioni che mira a sfruttare il potenziale di questo dominio inedito per la ricerca di intelligenza artificiale (IA) e linguaggio naturale (NL). Il dataset MeetingQA è stato creato utilizzando trascrizioni di riunioni pubbliche del corpus AMI, che coprono quasi 100 ore di riunioni multilaterali. I dati sono stati annotati manualmente per identificare le domande e le risposte, con un alto livello di accordo tra gli annotatori (Krippendorff's alpha di 0.73). Il dataset contiene 7.700 domande suddivise in tre set di training, valutazione e test, con il 30% delle domande non rispondibili e il 70% delle risposte multiple. Il progetto presenta un modello di question answering (QA) che utilizza metodi di retrieval del contesto, modelli a span unico e multi-span, e annotazioni d'oro per l'aumento dei dati. I risultati mostrano un gap di oltre 25 punti F1 tra i modelli finemente addestrati e la prestazione umana, con un gap di oltre 50 punti F1 tra la prestazione zero-shot e la prestazione umana. L'analisi degli errori mostra che i modelli hanno difficoltà ad identificare le domande retoriche e a determinare chi risponde a una domanda. Il progetto MeetingQA è un contributo importante per la comunità di ricerca di IA e NL, offrendo un nuovo dominio di applicazione per la QA e sollecitando ulteriori ricerche per migliorare le prestazioni dei modelli di QA.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca sul MultiInstruct che migliora l'apprendimento zero-shot multi-modale tramite l'adattamento delle istruzioni. 

Con gli avanzamenti nei grandi modelli di linguaggio, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento che riutilizzano i modelli di linguaggio pre-allenati per compiti downstream in modo efficiente rispetto ai parametri e ai dati. Di recente, molti studi hanno mostrato che l'adattamento delle istruzioni consente ai grandi modelli di linguaggio di eseguire compiti non visti in modo zero-shot seguendo le istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'adattamento delle istruzioni si è concentrata sull'incremento del rendimento zero-shot nei compiti di linguaggio solo, mentre i compiti di visione e multi-modale sono stati lasciati fuori. 

Quindi, in questo lavoro, vogliamo esplorare se l'adattamento delle istruzioni di un modello pre-allenato multi-modale possa effettivamente migliorare la generalizzazione ai compiti multi-modali non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una considerevole discordanza nella disponibilità dei set di istruzioni tra l'NLP e i compiti multi-modali. Esistono più di 1600 compiti di istruzioni linguaggio solo. Tuttavia, non esiste un grande set di istruzioni multi-modali pubblicamente disponibile. 

Quindi, questo ci motiva a costruire un set di istruzioni di adattamento multi-modale. Ecco che presentiamo MultiInstruct, il primo set di istruzioni di adattamento multi-modale benchmark che consiste in 62 compiti multi-modali diversi che coprono 10 categorie ampie. Questi compiti sono derivati da 21 set di dati aperti e ogni compito è dotato di cinque istruzioni scritte esperte. 

Per esplorare l'adattamento delle istruzioni multi-modali sul nostro set di dati proposto, prendiamo OFA, un modello pre-allenato multi-modale unificato, come modello di base. OFA utilizza un vocabolario unificato per i token di linguaggio, immagine e le coordinate di un rettangolo di bounding box. Ecco alcuni esempi di istanze dal nostro set di dati MultiInstruct, per unificare il trattamento dei dati di input e output diversi. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato di sequenza a sequenza unificato. 

Nel quale il testo di input, le immagini, le istruzioni e le caselle di bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'adattamento delle istruzioni multi-modali. 

Quindi, per il set di dati di training, utilizziamo 53 compiti da 9 gruppi per l'allenamento e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento comune per il testing e selezioniamo 5 compiti aggiuntivi dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nella suddivisione di testing per ogni compito. 

Inoltre, campioniamo 20 compiti casuali dalla suddivisione di testing di istruzioni naturali come compito non visto per l'NLP. Quindi utilizziamo il modello pre-allenato OFA grande come modello di base. Durante l'allenamento, mescoliamo tutte le istanze per tutti i compiti. 

Ogni istanza è combinata casualmente con uno dei cinque template di istruzioni. Quindi, durante il testing per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. 

In ogni esperimento, riportiamo il minimo e il massimo rendimento e la deviazione standard del rendimento across tutti i 5 esperimenti. Se il compito è una classificazione multi-modale, riportiamo l'accuratezza. 

Se è un compito di generazione multi-modale, riportiamo Rouge-L. Per i compiti di NLP, riportiamo Rouge-L. 

Introduciamo anche un metro di valutazione aggiuntivo chiamato sensibilità. Questo misura l'abilità del modello a produrre gli stessi output per lo stesso compito, indipendentemente dalle variazioni leggere nella parola dell'istruzione. 

Ecco i nostri risultati principali. Come possiamo vedere, l'adattamento delle istruzioni può migliorare significativamente il rendimento di OFA sui compiti multi-modali visti. 

Inoltre, l'apprendimento trasferito dai set di istruzioni naturali può beneficiare l'adattamento delle istruzioni. Ecco che possiamo vedere, come l'incremento del compito aumenta il rendimento del modello e, nel contempo, diminuisce la sensibilità. 

Quindi, abbiamo fatto un esperimento. Utilizziamo una istruzione contro 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare il rendimento generale del modello e ridurre la sua sensibilità molto. 

Quindi, questo mostra l'effetto di diverse strategie di fine-tuning sulle sensibilità del modello. Come possiamo vedere, l'apprendimento trasferito dai set di istruzioni naturali può fare in modo che il modello raggiunga una sensibilità molto migliore rispetto al modello originale OFA. 

Inoltre, possiamo vedere che l'apprendimento trasferito dai set di istruzioni naturali può aiutare OFA a raggiungere un rendimento molto migliore sul set di istruzioni naturali. 

Quindi, in generale, proponiamo il primo grande set di istruzioni di adattamento multi-modale con un rendimento significativamente migliorato rispetto a OFA, e esploriamo diverse tecniche di apprendimento trasferito e mostrano i loro benefici. 

Progettiamo un nuovo metro di valutazione chiamato sensibilità. Quindi, un'altra cosa, stiamo raccogliendo un grande set di istruzioni di adattamento multi-modale con circa 150 compiti di visione e linguaggio aggiuntivi e li rilasceremo. 

Quindi, ecco un codice QR per i nostri dati e modello. Grazie.</sample>
    <sample id="152">Il progetto "Exploring Large Language Models for Classical Philology" si concentra sulla creazione di modelli linguistici avanzati per la filologia classica. Gli autori hanno sviluppato quattro modelli linguistici: GreBERTa e GreTa per il greco antico, e PhilBERTa e PhilTa per il greco antico, il latino e l'inglese. Questi modelli sono stati pre-istruttinati con dati di alta qualità, tra cui un nuovo corpus creato dall'Internet Archive, e hanno superato il state-of-the-art per il greco antico e il latino in tre compiti principali: etichettatura di parte del discorso, analisi di dipendenza e lemmatizzazione. I modelli encoder-decoder, in particolare, hanno mostrato una prestazione eccezionale nella lemmatizzazione, migliorando di 5 punti percentuali il state-of-the-art per il greco antico. I modelli multilingue hanno anche superato i modelli monolingue in alcune prove di conoscenza semantica e di mondo, ma non hanno mostrato una differenza significativa. Gli autori hanno anche analizzato come il codificatore di T5 si comporta e hanno investigato le implicazioni della multilinguità nei loro modelli. Il progetto fornisce nuove risorse per la filologia classica e apre nuove prospettive per l'analisi e l'apprendimento di testi antichi.</sample>
    <sample id="153">Ninareh Mehrabi, ricercatrice post-dottorale presso il team di Responsabile AI di Amazon Alexa, ha presentato il lavoro "Resolving Ambiguities in Text-to-Image Generative Models" (Risolvere ambiguità in modelli generativi di testo-immagine). L'obiettivo del lavoro è studiare le ambiguità presenti nelle richieste fornite ai modelli di testo-immagine e proporre framework per mitigare e valutare tali ambiguità.

Il team ha creato un dataset di benchmark, modificato da un corpus esistente chiamato LAVA, che copre diversi tipi di ambiguità. Il framework proposto prevede due approcci: il primo consiste nel generare domande di chiarimento utilizzando l'apprendimento in contesto, mentre il secondo prevede la generazione di diverse possibili visualizzazioni. In entrambi i casi, l'utente fornisce risposte che consentono di ottenere una richiesta disambiguata.

Una volta ottenuta la richiesta disambiguata, il team valuta se le immagini generate sono fedeli all'intenzione dell'utente utilizzando un modello di valutazione automatica basato su VQA (Visual Question Answering). I risultati mostrano una disparità nella risoluzione delle ambiguità per diversi tipi di ambiguità, ma anche che la disambiguazione utilizzando il framework proposto ha un effetto positivo sulla fedeltà della generazione. Inoltre, il modello di valutazione automatica è in accordo con l'evaluazione umana, rendendolo una soluzione affidabile per valutare i modelli di testo-immagine.</sample>
    <sample id="154">Sara Papi, Matteo Negri e Marco Turchi sono rispettivamente:

- Sara Papi: University of Trento e Foundazione Bruno Kessler
- Matteo Negri: University of Trento e Foundazione Bruno Kessler
- Marco Turchi: University of Trento e Foundazione Bruno Kessler</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Il team di ricerca della Shandong University, guidato da Shen Gao, ha sviluppato un modello di dialogo chiamato "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS). Questo modello è progettato per distillare le informazioni più importanti da un contesto di dialogo in un riassunto conciso. La sommersione dei dialoghi è un compito difficile e interessante nel campo della ricerca di sommersione di testi, poiché aiuta le persone a catturare velocemente i punti salienti di un dialogo semi-strutturato e multi-partecipante senza dover esaminare il contesto di dialogo complesso.

Il modello SDDS si differenzia dagli altri metodi di sommersione dei dialoghi poiché utilizza una struttura di grafo statico dinamico, che combina la rappresentazione statica del grafo costruita con metodi di modellazione dei dati esistenti con la rappresentazione dinamica del grafo appresa tramite un modello di attenzione multi-testa. Il modello SDDS ha quattro componenti principali: un encoder di frase per codificare le frasi nel contesto di dialogo, un modulo di grafo statico-dinamico per catturare le relazioni semantiche tra le frasi, un generatore di sommario pre-allineato per combinare la struttura di grafo statica e dinamica e produrre un riassunto finale.

Il modello SDDS propone anche quattro metodi di modellazione della struttura del dialogo statica, tra cui un grafo di analisi del discorso, un grafo di co-occorrenza di parole chiave e un grafo di interazione tra gli utenti. Il modello SDDS è stato testato su diverse basi di dati di dialogo e ha mostrato risultati promettenti nella sommersione dei dialoghi. Il codice e i dati sono stati rilasciati su GitHub e possono essere scaricati tramite il QR code.</sample>
    <sample id="158">Il problema della risoluzione delle coreferenze consiste nell'identificare le menzioni di entità diverse in un testo e nel clusterizzare le menzioni che si riferiscono alla stessa entità. I metodi convenzionali per risolvere questo problema hanno una complessità quadratica, che può essere un problema per i documenti lunghi. I metodi di caching recentemente proposti hanno ridotto la complessità a livello lineare, ma quando il cache è pieno, l'evizione di un'entità con una politica di evizione come LRU (Least Recently Used) può portare a una grande quantità di errori di cache.

Per superare questo problema, si propone un sistema di cache doppio, che consiste in un cache locale e in un cache globale che lavorano insieme. Il cache locale memorizza le entità locali con una politica di evizione LRU, mentre il cache globale memorizza le entità globali con una politica di evizione LFU (Least Frequently Used). Il sistema di cache doppio funziona scandendo il documento da sinistra a destra e classificando le menzioni come nuove entità o come appartenenti a entità già presenti nel cache.

I risultati dei test mostrano che il sistema di cache doppio outperforma i metodi di caching singoli, riduce la quantità di errori di cache e offre il miglior rapporto prestazioni/costo. Il sistema di cache doppio è stato testato su quattro benchmark pubblici e su un libro di 30.000 parole, mostrando risultati promettenti in termini di efficienza e prestazioni.</sample>
    <sample id="159">Ciao, a tutti. Sono Koustav Sinha e sono felice di accogliervi nella nostra discussione sul nostro lavoro presentato all'ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo i paradigmi delle coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici sulla base dei giudizi di accettabilità. Ciò può anche includere grammaticalità come BLiMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS pairs. E nel paradigma delle coppie minime, la tipica modalità di valutazione dei modelli linguistici è quella di mostrare una frase accettabile o grammaticale e poi mostrare una frase accettabile o una frase inaccettabile. E poi la speranza è che il modello, in pratica, assegna una probabilità maggiore alla frase accettabile. Il pipeline attuale dei paradigmi delle coppie minime non ci consente di valutare l'accettazione del modello verso frasi più lunghe. Oggi i modelli linguistici a grandi dimensioni stanno emergendo con finestre di contesto sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli all'interno della finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline dei paradigmi delle coppie minime chiedendo al modello di valutare l'accettabilità su sequenze più lunghe e più lunghe. Quindi, per simulare queste sequenze più lunghe, rivediamo i set di dati stessi e poi ricreiamo le frasi scegliendo frasi accettabili o inaccettabili da quei set di dati. Quindi, per esempio, abbiamo scelto una coppia tipica di grammaticalità dal set di dati BLiMP dall'Isola degli Aggettivi. E ciò che facciamo è che per ricreare sequenze più lunghe e accettabili e inaccettabili con la stessa struttura grammaticale. Estraiamo le frasi grammaticali dall'Isola degli Aggettivi e poi le aggiungiamo come prefisso a entrambe la frase accettabile e la frase inaccettabile. Possiamo fare lo stesso scegliendo frasi inaccettabili dallo stesso matching e ciò potrebbe anche essere utilizzato per testare l'accettabilità del modello. E possiamo fare lo stesso scegliendo frasi da un insieme diverso o da un set di dati diverso. Quindi, questo è ciò che chiamiamo scenario di disallineamento. Quindi, le frasi sono ancora provenienti da set di dati pertinenti, ma non dallo stesso set di dati con cui stiamo valutando. E possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente irrilevante come Wikipedia. Quindi, ciò ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati dal contesto, come, se il contesto proviene da un insieme diverso dei dati, o se è completamente irrilevante, rispetto alla frase corrente che stiamo esaminando. Quindi, come si comporta il modello? Quindi, prima, esaminiamo le frasi di Wikipedia, che sono completamente irrilevanti alla coppia di query corrente e lì troviamo che i giudizi di MPP sono per lo più robusti per la lunghezza arbitraria del contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT 2 e vediamo che nella linea tratteggiata in arancione, i giudizi di MPP sono relativamente stabili. Quindi, cosa accade quando scegliamo frasi dallo stesso set di dati? Quindi, qui stiamo scegliendo o creando frasi da domini accettabili e inaccettabili dallo stesso set di dati BLiMP o SyntaxGym e lì vediamo che i giudizi di MPP aumentano o diminuiscono significativamente quando si aggiunge un prefisso accettabile o inaccettabile. Ma quando corrispondiamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno nel BLiMP o nel SyntaxGym, vediamo un aumento o una diminuzione massiccio dei giudizi di MPP del modello, a seconda del prefisso scelto sia accettabile o inaccettabile. E questo effetto aumenta notevolmente lungo la lunghezza del contesto e ciò potrebbe influire sui modelli linguistici più recenti che hanno finestre di contesto sempre più lunghe. Quindi, perché il prefisso di corrispondenza influisce tanto sui giudizi del modello? Quindi, abbiamo fatto una serie di analisi in cui abbiamo provato a perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore alla frase. E dopo aver fatto queste perturbazioni, abbiamo trovato che nessun rumore stava effettivamente cambiando il corso del modello in termini di come mostra i giudizi di MPP. In pratica, abbiamo trovato che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi dal dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi dal dominio inaccettabile, vediamo una diminuzione dei giudizi di MPP in modo simile. Quindi, i principali punti chiave del nostro lavoro è che i modelli linguistici sono sensibili alle caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E l'evaluazione di MPP, come la facciamo attualmente con l'input di frase singola e breve, non cattura completamente la conoscenza astratta dei modelli linguistici all'interno della finestra di contesto. Leggete il nostro lavoro per maggiori dettagli degli esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Nel primo passaggio del metodo, i token di input vengono mappati in un insieme non ordinato (unordered multiset) di token che appariranno nell'output.</sample>
    <sample id="161">55.000 specifici script sono rappresentati in CoScript.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato offre il vantaggio di utilizzare etichette di qualità inferiore, come regole heuristiche semplici, basi di conoscenza o etichettature di crowdsourcing di bassa qualità, che sono più economiche rispetto alle annotazioni umane, ma possono essere noiose.</sample>
    <sample id="165">Il lavoro di ricerca presentato da Wenting Zhao, PhD student presso l'Università di Cornell, si concentra sull'abduzione ragionamento, un processo che consiste nell'identificare una spiegazione plausibile per un evento o un risultato partendo da un contesto e un insieme di spiegazioni possibili. Lo studio propone una nuova metodologia chiamata LiPoR (Likelihood Learning with Posterior Regularization), un approccio non supervisionato che può apprendere l'abduzione ragionamento senza richiedere l'annotazione delle spiegazioni plausibili. LiPoR utilizza un obiettivo non supervisionato che massimizza la probabilità di un risultato dato un contesto, marginalizzando le altre spiegazioni possibili. Tuttavia, per preferire le spiegazioni più plausibili, LiPoR introduce un regolarizzatore che punta a mantenere la mutualità esclusività tra le spiegazioni. Il lavoro di ricerca presenta risultati promettenti su un dataset di abduzione ragionamento, AlphaNLI, dove LiPoR supera i modelli zero-shot e il precedente miglior approccio non supervisionato con un aumento di accuratezza di oltre 4 punti. La ricerca di Zhao ha importanti implicazioni per l'apprendimento di abduzione ragionamento e potrebbe avere un impatto significativo sulle applicazioni della ragionamento abduttivo in vari campi.</sample>
    <sample id="166">Il lavoro presentato da Yunxin e i suoi colleghi si concentra sul problema di retrieval di immagini da testo linguistico complesso. Questo compito è particolarmente sfidante a causa della presenza di immagini simili e di descrizioni lunghe. I metodi tradizionali, come i modelli di linguaggio visivo, mostrano buoni risultati nei task di retrieval di immagini e frasi, ma la loro prestazione cala drasticamente quando si affrontano testi linguistici complessi.

Per superare questo limite, gli autori si ispirano alla strategia Divide-and-Conquer e alla Teoria dei Due Processi. La strategia Divide-and-Conquer consiste nel dividere un problema complesso in sottoproblemi più semplici, risolverli e combinare le soluzioni per ottenere il risultato desiderato. La Teoria dei Due Processi descrive due sistemi di pensiero umano: il Sistema 1, che si occupa della ragione analogica, e il Sistema 2, che è capace di ragionamento logico astratto.

Gli autori propongono un modello di ragionamento complesso che combina i vantaggi dei due sistemi. Il modello consiste in tre componenti: il Generator di Propositioni, che decomposta il testo linguistico complesso in rappresentazioni di proposizioni semplici; l'Interattore Visuo-Linguistico, che si occupa dell'interazione tra le informazioni visive e le proposizioni; e il Razonatore Simbolico Neurale, che integra le informazioni per ottenere la soluzione finale.

Gli esperimenti mostrano che il modello proposto, chiamato NDCR, supera i risultati dei metodi di base e conferma l'efficacia di ciascuna componente. Inoltre, gli autori presentano due casi di studio per verificare la prestazione del modello e suggeriscono che la combinazione di Divide-and-Conquer e Teoria dei Due Processi possa essere una via promettente per risolvere problemi complessi.</sample>
    <sample id="167">I 750 documenti in DEPLAIN-web sono stati allineati in due modi: 

- 750 documenti sono stati allineati manualmente
- 750 documenti sono stati allineati anche con metodi di allineamento automatici</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo testi da Reuters News del 2020 e annotandoli con le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">Il paper "Prompting PaLM for Translation: Assessing Strategies and Performance" è un'indagine sistematica sulle strategie di promtting per la traduzione automatica utilizzando il modello di linguaggio PaLM, un grande modello di linguaggio addestrato su 780 miliardi di token. L'obiettivo è valutare la capacità di traduzione di PaLM utilizzando le migliori pratiche della comunità di MT e confrontandolo con i sistemi di traduzione automatica di stato dell'arte. Gli autori hanno valutato l'influenza del promtting sulla prestazione di PaLM e hanno identificato alcune strategie di promtting efficaci, come l'utilizzo di esempi di alta qualità e la riduzione del numero di esempi. I risultati mostrano che PaLM è in grado di raggiungere prestazioni competitive con i sistemi di traduzione automatica di stato dell'arte, sebbene abbia difficoltà con l'accuratezza. La valutazione umana ha confermato che la fluentezza di PaLM è simile a quella dei sistemi di traduzione automatica di stato dell'arte, ma che la principale differenza risiede nell'accuratezza. Gli autori forniscono anche alcune raccomandazioni per la selezione delle strategie di promtting e suggeriscono che l'utilizzo di esempi di alta qualità sia fondamentale per migliorare le prestazioni di PaLM. In generale, il paper offre una panoramica completa delle strategie di promtting per la traduzione automatica e fornisce spunti per future ricerche in questo campo.</sample>
    <sample id="170">Ciao a tutti, il mio nome è Yusen Zhang della Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Incroso-Lingua in Molti Linguaggi Naturali e Rappresentazioni di Significato". Il parsing semantico è un compito per costruire rappresentazioni semantiche di query degli utenti come SQL e Lambda Calculus. Il Parsing Semantico Incroso-Lingua è il compito di tradurre query in più linguaggi naturali in più rappresentazioni di significato. Come mostrato in questa figura, abbiamo bisogno di tradurre la query in più linguaggi naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. I modelli di parsing semantico incroso-lingua esistenti sono stati proposti e valutati separatamente su dati limitati di compiti e applicazioni. Ad esempio, ci sono molte coperture su certi linguaggi naturali, ma il cinese manca e non ci sono coperture su certe rappresentazioni di significato. La Lambda calculus manca, o vengono valutate solo su certi modelli neurali. Ad esempio, esiste solo un singolo modello per valutarli. Pertanto, proponiamo XSemPLR. Offriamo un insieme di dati uniformi XSemPLR per il parsing semantico incroso-lingua in più linguaggi naturali e rappresentazioni di significato. Contiene 9 insiemi di dati in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 linguaggi naturali in 15 famiglie di linguaggi. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e l'evaluazione. La prima impostazione è Translate-Test. Utilizziamo l'API di Traduzione di Google per tradurre la fonte in linguaggio target, poi addestriamo e valutiamo il modello monolingua. E ad esempio, addestriamo il modello inglese sulle query inglesi e durante l'inferenza traduciamo la query tedesca con l'API in inglese e poi utilizziamo il modello addestrato per prevedere il SQL. E valuteremo anche il modello Monolingua. In questa impostazione, il linguaggio di fonte è lo stesso del linguaggio target, ad esempio tedesco a tedesco o inglese a inglese. Valuteremo anche il modello Monolingua Few-shot addestrandolo con solo l'1% dei dati di addestramento. E valuteremo il modello Multilingua che addestriamo un modello multilingua per tutti i linguaggi. Ad esempio, mettiamo le query tedesche, inglesi e cinesi insieme per addestrare un modello multilingua. E durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, ecc. E consideriamo anche il trasferimento incroso-lingua Zero-shot e Few-shot. Addestriamo su un linguaggio di fonte e trasferiamo su un altro linguaggio. Quindi durante l'addestramento addestriamo su query inglesi o la combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingua per prevedere l'output SQL. E abbiamo trovato molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolingui, valutiamo due gruppi di modelli compresi Encoder-PTR che rappresenta gli encoder multilingui con decoder a puntatore, come XLM-R + PTR e mBERT + PTR. E valutiamo anche modelli Encoder-Decoder, che rappresentano gli encoder multilingui con decoder, come mBART e mT5. Abbiamo trovato che Encoder-Decoder ottiene la migliore prestazione su tutti e nove i dati di insieme. E valutiamo mT5 e XLM-R + PTR sulle impostazioni multilingua. Abbiamo trovato che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrandoli in una miscela di vari linguaggi. Abbiamo trovato che è perché la maggior parte dei principali linguaggi naturali possono ottenere un guadagno di prestazione, eccetto che la prestazione inglese scende in sette dati e solo guadagna in tre dati. Penso che questo sia noto come la "Maledizione della Multilinguismo". Abbiamo anche confrontato il divario di prestazione incroso-lingua. In questa figura, la linea blu è il trasferimento incroso-lingua Few-shot. La linea arancione è il trasferimento incroso-lingua Zero-shot. Mentre la linea verde è la impostazione Monolingua. Abbiamo trovato che, confrontando la linea verde e arancione, abbiamo trovato che il divario di prestazione di trasferimento incroso-lingua Zero-shot è significativo, e poi confrontando le linee blu e arancione, abbiamo trovato che con l'impostazione Few-shot il divario di prestazione di trasferimento è ridotto rapidamente. Abbiamo anche trovato altri risultati interessanti. Ad esempio, Encoder-Decoder supera i risultati precedenti o raggiunge risultati comparabili. L'addestramento sul linguaggio naturale inglese può aumentare significativamente la prestazione Few-shot sul linguaggio naturale target, e abbiamo trovato che i modelli multilingua come Codex e BLOOM sono ancora insufficienti per compiti di parsing semantico incroso-lingua. Per riassumere, abbiamo costruito XSemPLR, un benchmark unificato per il parsing semantico incroso-lingua con più linguaggi naturali e rappresentazioni di significato. Abbiamo condotto uno studio di benchmarking comprensivo su tre tipi rappresentativi di modelli multilingua. E i nostri risultati mostrano molti risultati interessanti. E ecc. E accogliamo il vostro visitare il nostro articolo e codice. Grazie per ascoltare.</sample>
    <sample id="171">I lavori connessi in tal senso sono stati classificati in quattro categorie:

1. Metodi di watermarking basati su attacco (Attack-based watermarking)
2. Metodi di watermarking basati su difesa (Defense-based watermarking)
3. Metodi di watermarking basati su approccio misto (Hybrid approach)
4. Metodi di watermarking basati su approccio di watermarking distribuito (Distributed watermarking)

Tuttavia, questi metodi hanno dei limiti, come ad esempio la mancanza di applicabilità per le servizi di embedding o la mancanza di trasferibilità.</sample>
    <sample id="172">No, i LLM multilingue come Codex o BLOOM sono ancora insufficienti per il CLSP (Cross-Lingual Semantic Parsing).</sample>
    <sample id="174">Il paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" propone un nuovo approccio per l'analisi della qualità degli argomenti utilizzati nelle discussioni e dibattiti. Il dataset, che contiene 35.000 coppie argomento-analisi, è il più grande e di alta qualità in questo campo. Le principali caratteristiche del dataset sono:

* Qualità degli argomenti: i dati sono stati raccolti da fonti di alta qualità, come tornei di dibattito e oratori esperti.
* Diversità degli argomenti: i dati sono stati raccolti su 24 temi diversi, piuttosto che su pochi argomenti specifici.
* Analisi: il dataset introduce un nuovo concetto di "analisi", che rappresenta la combinazione di affermazioni, premesse e ragionamenti utilizzati per sostenere un argomento.
* Rilevanza: il dataset include un modello di rilevanza che assegna un punteggio da 0 a 1 per ogni argomento e tema, per catturare la sua pertinenza rispetto al tema in discussione.
* Affidabilità degli annotatori: il dataset utilizza un approccio di affidabilità degli annotatori basato su istanze, che elimina solo le annotazioni che sono ritenute biasate per un argomento specifico.

In sintesi, il dataset ArgAnalysis35K offre una rappresentazione più completa e dettagliata degli argomenti utilizzati nelle discussioni e dibattiti, fornendo un quadro più preciso per l'analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni mediante una continua approssimazione del problema, che consente di backpropagare e apprendere le permutazioni più plausibili linguisticamente.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di fornire prestazioni simili indipendentemente dalle caratteristiche demografiche o politiche dei dati di input, senza discriminare o marginalizzare alcun gruppo.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Il paper "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker" affronta il problema della mancanza di capacità di ragionamento sul pensiero degli altri (Theory of Mind) negli modelli di linguaggio (LLM). I ricercatori presentano SymbolicToM, un metodo di inferenza in tempo reale che utilizza rappresentazioni grafiche esplicite per migliorare le capacità di ragionamento sul pensiero degli altri negli LLM. Il metodo utilizza un algoritmo di inferenza in tempo reale che combina modelli di NLI e OpenIE off-the-shelf per generare rappresentazioni grafiche delle credenze degli individui in un contesto narrativo. Il paper presenta esperimenti che testano la metodologia su una varietà di LLM e confrontano i risultati con basi di apprendimento supervisionate. I risultati mostrano che SymbolicToM migliora notevolmente le prestazioni degli LLM su domini di apprendimento e fuori dal dominio, incluso il ragionamento sul pensiero degli altri in contesti narrativi complessi. Il metodo si rivela anche efficace nel generalizzare a nuove strutture narrative e a nuove forme linguistiche. In conclusione, il paper presenta un contributo significativo alla comprensione delle capacità di ragionamento sul pensiero degli altri negli LLM e fornisce un metodo innovativo per migliorare le prestazioni degli LLM in questo ambito.</sample>
    <sample id="180">Non è specificato il nome della relatrice o del relatore, ma solo il nome dell'autrice del testo, Myra.</sample>
    <sample id="181">Il lavoro presentato da Siyu Yuan e la sua squadra di Fudan University si concentra sul problema della pianificazione linguistica vincolata, ovvero la capacità di pianificare azioni seguendo istruzioni passo dopo passo in forma di script con obiettivi specifici e vincoli. I grandi modelli linguistici sono stati utilizzati per pianificare obiettivi astratti di attività stereotipiche, ma il piano per obiettivi con vincoli specifici, come "fare una torta al cioccolato", è ancora poco studiato.

L'obiettivo del lavoro è valutare e migliorare la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. I ricercatori hanno esteso gli obiettivi astratti con vincoli multi-aspettuali per l'acquisizione di dati umani-in-loop utilizzando InstructGPT. Hanno campionato 100 obiettivi specifici e valutato i script generati dai grandi modelli linguistici. I risultati mostrano che i modelli linguistici raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.

Per migliorare la qualità delle generazioni, i ricercatori hanno adottato l'idea di "over-generate-then-filter", che consiste nell'over-generare K script per obiettivi specifici e poi utilizzare un modello di filtro per selezionare i script fedeli. Il filtro utilizza la similitudine semantica tra script e obiettivi specifici per misurare la fedeltà ai vincoli.

I ricercatori hanno creato un dataset di pianificazione linguistica vincolata, chiamato CoScript, utilizzando la distillazione di conoscenza simbolica per distillare i dati dei grandi modelli linguistici. CoScript contiene 55.000 obiettivi specifici con script e mostra alta pluralità nella generazione di obiettivi specifici.

Il lavoro dimostra che i modelli linguistici più piccoli possono superare i modelli più grandi quando sono addestrati su dataset adatti. La squadra di Fudan University spera che il dataset CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="182">Il tropicalismo nel contesto di questo articolo si riferisce a un tropo culturale che associa le donne di colore, in particolare le donne latine, a caratteristiche esotiche e sensuali, come ad esempio "vibrant" e "curvaceous", che perpetuano un'immagine stereotipata e discriminatoria di queste donne.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target attraverso una serie di passaggi: 

1.  hanno creato un insieme di promemoria per generare le rappresentazioni dei gruppi target, ispirati a uno studio che aveva già utilizzato questi promemoria con soggetti umani per scoprire stereotipi razziali. 
2.  hanno utilizzato queste rappresentazioni per confrontarle con quelle generate dai modelli di linguaggio, consentendo un confronto diretto tra le due.
3.  hanno identificato le parole chiave che distinguono i gruppi target dagli altri, utilizzando il metodo "Fightin' Words" che calcola le log-odds ratio pesate per identificare le parole più significative per ogni gruppo.</sample>
    <sample id="184">CXMI (Context Usage by Machine Translation Models) e Pointwise CXMI (P-CXMI) sono stati utilizzati per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="185">DrBERT e ChuBERT sono due modelli di pre-allenamento, ma differiscono nella fonte dei dati utilizzati per l'allenamento. DrBERT è stato allenato su NACHOS, un set di dati di dati medici raccolti dal web, mentre ChuBERT è stato allenato su dati clinici anonimizzati provenienti dal data warehouse dell'ospedale universitario di Nantes.</sample>
    <sample id="187">Non è specificato il numero di autori nell'articolo, ma solo il nome dei due autori, Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento si riferisce al processo di aggiornamento del modello di apprendimento tramite l'aggiunta di nuove informazioni e l'adattamento dell'architettura del modello stessa. In questo contesto, il modello viene addestrato iterativamente, cioè viene aggiornato in base alle nuove informazioni e alle nuove annotazioni, e l'architettura del modello viene modificata per adattarsi alle nuove informazioni.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities Corpus è di comprendere il linguaggio dei utenti quando vogliono selezionare un entità tra due o più opzioni, utilizzando riferimenti indiretti.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS (Embedding as a Service) imparando dai dati di embedding forniti dal servizio.</sample>
    <sample id="191">3 autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Yang Luo presenta un nuovo ottimizzatore per il training di grandi modelli di linguaggio, chiamato CAME (Confidence-guided Adaptive Memory Efficient Optimization). L'obiettivo è raggiungere due obiettivi simultaneamente: una rapida convergenza come nei metodi di ottimizzazione tradizionali e un basso consumo di memoria come nei metodi di ottimizzazione efficienti.

L'attuale ottimizzatore Adafactor riduce il consumo di memoria, ma presenta un problema di aggiornamento errato che limita la sua applicazione. Yang Luo propone un approccio per ridurre gli errori di aggiornamento, utilizzando la differenza tra il momento storico e l'aggiornamento corrente come instabilità nel momento storico.

Il nuovo ottimizzatore CAME utilizza questa instabilità come denominatore per l'aggiornamento del momento storico, riducendo gli errori di aggiornamento. Gli esperimenti mostrano che CAME raggiunge una maggiore accuratezza di valutazione rispetto a Adafactor e Adam, con un consumo di memoria ridotto. Inoltre, CAME lavora bene anche con grandi batch di training, estendendo l'applicazione dei metodi di ottimizzazione efficienti.

Gli esperimenti sono condotti su tre grandi modelli di linguaggio: BERT, GPT-2 e T5, e mostrano che CAME raggiunge una maggiore accuratezza di valutazione rispetto ai metodi di ottimizzazione tradizionali e efficienti. Inoltre, CAME riduce il consumo di memoria rispetto ai metodi di ottimizzazione tradizionali, come Adam e LAMB.</sample>
    <sample id="193">Non è stato specificato il numero esatto di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">I collaboratori menzionati sono affiliati alle seguenti istituzioni:

- Sebastian Santy: non specificato
- Ronan Le Bras: non specificato
- Katharina Reinecke: non specificato
- Maarten Sap: non specificato
- Jenny (autore del testo): Carnegie Mellon University</sample>
    <sample id="195">Il team di ricerca ha presentato un nuovo framework chiamato RoHT (Reasoning over Hierarchical Question Decomposition Tree) per rispondere a domande complesse e fornire una spiegazione del perché l'answer è stata selezionata. Il framework si concentra sulla decomposizione di domande complesse in sottodomande più semplici, utilizzando un albero di decomposizione (HQDT) per rappresentare la struttura composta della domanda. RoHT è un framework a due stadi: nella prima fase, si costruisce l'albero di decomposizione della domanda complessa, identificando le sottodomande e le loro relazioni; nella seconda fase, si esegue un ragionamento probabilistico sull'albero per integrare conoscenza da diverse fonti, come un knowledge base e un corpus di testo.

Il team ha valutato il framework RoHT su due set di dati complessi, KQA Pro e Musique, e ha ottenuto risultati promettenti. Sull'insieme di dati KQA Pro, RoHT ha superato i metodi KB QA esistenti, dimostrando l'efficacia dell'integrazione di risposte di sottodomande di diversi livelli. Sull'insieme di dati Musique, RoHT ha migliorato la misura F1 di 11,9 rispetto al metodo SOTA EX(SA), dimostrando la superiorità del decomposizione esplicita.

Il framework RoHT offre diverse novità rispetto ai metodi esistenti, come la capacità di integrare conoscenza da diverse fonti e di fornire spiegazioni del perché l'answer è stata selezionata. Il team spera che RoHT possa essere utilizzato per migliorare la comprensione e la risoluzione di domande complesse in diversi campi, come la medicina, la giurisprudenza e la scienza.</sample>
    <sample id="196">"I saw Bart and Lisa".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo utilizzati nell'esperimento sono stati selezionati tra quelli di stato dell'arte, ma non sono stati specificati nel testo.</sample>
    <sample id="198">Le grandi modello di linguaggio stanno venendo fuori con finestre di contesto sempre più lunghe, quindi è cruciale valutare l'accettabilità dei modelli lungo tutta la finestra di contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in 7 dei 9 dataset, mentre ha portato a un aumento delle prestazioni solo in 3 dei 9 dataset.</sample>
    <sample id="200">No, gli annotatori non conoscono necessariamente l'entità in anticipo, ma sono forniti di background knowledge su entrambe le entità.</sample>
    <sample id="201">Le metriche di MT utilizzate per la valutazione sono state le metriche neurali di stato dell'arte, nonché gli esiti della valutazione umana basati sul framework MQM.</sample>
    <sample id="202">No, il regresso nella generalizzazione non influenza specifici tipi di NER, ma piuttosto è causato da un fattore generale chiamato "temporal drift", ovvero la performance degrada a causa dell'incremento del gap temporale tra i dati di allenamento e quelli di test.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché può influenzare la ricerca e i risultati, e può comportare una rappresentazione sistematica di alcune posizioni rispetto ad altre. Inoltre, con l'aumento di compiti NLP soggettivi e orientati alla società, è cruciale comprendere come le posizioni siano distorte, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM sono stati affinati mediante una messa a punto integrale, non solo tramite adattatori.</sample>
    <sample id="205">Il lavoro presentato da Shangbin, PhD student presso l'Università di Washington, esplora le politiche di bias nei modelli di linguaggio. I modelli di linguaggio vengono addestrati su grandi dataset di web crawl, che comprendono notizie politiche da fonti come il New York Times, Los Angeles Times, The Guardian e Huffington Post. Ciò crea un dilemma: da un lato, i modelli di linguaggio possono imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee; dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente biasate e possono portare a problemi di equità in applicazioni di NLP.

Lo studio propone di esplorare la catena di propagazione del bias politico dai dati di addestramento ai modelli di linguaggio alle applicazioni downstream. I ricercatori hanno utilizzato questionari politici per valutare la tendenza politica dei modelli di linguaggio e hanno scoperto che questi occupano tutti e quattro i quadranti del "campus politico". I modelli GPT-4 sono stati identificati come i più liberali, mentre i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e alle sue varianti.

Gli autori hanno condotto esperimenti controllati per valutare l'impatto dei dati di addestramento sui modelli di linguaggio. Hanno trovato che la tendenza politica dei modelli di linguaggio può essere influenzata dai dati di addestramento e che questi possono anche catturare la polarizzazione presente nella società moderna.

Infine, gli autori hanno valutato i modelli di linguaggio con diverse tendenze politiche nelle applicazioni di detezione di discorsi d'odio e notizie false. I risultati hanno mostrato che i modelli con tendenze politiche diverse possono avere prestazioni diverse nelle diverse categorie demografiche e politiche. Ciò suggerisce che ci sono problemi di equità legati alle politiche di bias nei modelli di linguaggio e che è necessario affrontarli per garantire l'uso etico di questi modelli in applicazioni di NLP.</sample>
    <sample id="206">Il modello utilizzato per il trasferimento dell'apprendimento è un modello di classificazione zero-shot, con un AUC di 0,62, che trasferisce pesi da due task correlate: "debate" (classificazione di dissonanza indipendente da tema) e "CE" (classificazione binaria di espansione e confronto di PDTB).</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i test set più aggiornati della comunità MT, che evitano l'overlapping con i dati di training del modello.</sample>
    <sample id="208">Gli autori hanno proposto 3 suggerimenti alla fine del loro lavoro.</sample>
    <sample id="209">Il metodo proposto (over-generate-then-filter) migliora la qualità delle scritture rispetto al metodo di riferimento (InstructGPT) sia in termini di completezza semantica che di fedeltà alle restrizioni, con una maggiore precisione e una riduzione dell'incertezza.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametri di riferimento per valutare metodi di allineamento automatico e per sviluppare modelli di semplificazione del testo automatica.</sample>
    <sample id="212">T5.</sample>
    <sample id="213">OFA (unified multi-modal pre-trained model).</sample>
    <sample id="215">La discussione verte sulla struttura di dipendenza della coordinazione, un tema fondamentale nella linguistica. L'autore, Adam Przepiórkowski, esamina le diverse teorie e approcci utilizzati per rappresentare la coordinazione in linguistica, tra cui l'approccio universale delle dipendenze, la teoria del significato testuale di Igor Mel'čuk e l'approccio di Praga. Questi approcci possono essere asimmetrici o simmetrici, ovvero possono dare priorità a un congiunto rispetto agli altri o considerare tutti i congiunti come uguali.

L'autore presenta un nuovo argomento a sostegno delle strutture simmetriche della coordinazione, basato sul principio di minimizzazione della lunghezza delle dipendenze. Questo principio afferma che le dipendenze più brevi sono preferite rispetto a quelle più lunghe. L'autore utilizza esempi per illustrare come questo principio possa spiegare la preferenza per la posizione dei congiunti in coordinazioni.

Inoltre, l'autore presenta statistiche estratte dal Penn Treebank che confermano l'osservazione che i congiunti a sinistra tendono a essere più brevi rispetto a quelli a destra. Tuttavia, questo effetto si verifica solo quando il governatore è a sinistra o assente, mentre non si verifica quando il governatore è a destra. Questo risultato fornisce un argomento contro le strutture asimmetriche della coordinazione e a favore delle strutture simmetriche.

In sintesi, la discussione verte sulla rappresentazione della coordinazione in linguistica e presenta un nuovo argomento a sostegno delle strutture simmetriche della coordinazione, basato sul principio di minimizzazione della lunghezza delle dipendenze.</sample>
    <sample id="217">Il progetto "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" è un'innovativa ricerca condotta da Weihao Zeng, Lulu Zhao e Keqing He presso la Beijing University of Posts and Telecommunications. L'obiettivo è sviluppare un modello di generazione di dialoghi controllabili per multi-attributi, che superi le limitazioni delle metodologie precedenti che si concentravano su singoli attributi.

Il modello proposto, chiamato DCG (Disentangled Controllable Generation), utilizza una combinazione di tecniche di apprendimento di attributi disentangolati e di valutazione automatica (MAE) per generare dialoghi controllabili per multi-attributi. Il DCG è basato sulla piattaforma DialoGPT e utilizza due tipi di promp: orientati agli attributi e orientati alla task. Questi promp sono progettati per guidare il modello a focalizzarsi sulla specifica informazione nel dialogo e a generare risposte coerenti.

Il DCG è stato testato su due benchmark e ha dimostrato di superare le basi in termini di controllabilità degli attributi e di qualità del testo. La valutazione automatica MAE ha mostrato una correlazione significativa con le valutazioni umane, confermando l'efficacia del modello. Il progetto ha anche dimostrato l'importanza delle promp nella composizione generaleizzata e ha proposto un metodo per disentangolare attributi combinati e apprendere le relazioni tra attributi diversi.

In sintesi, il progetto "Seen to Unseen" ha contribuito a sviluppare un modello innovativo di generazione di dialoghi controllabili per multi-attributi, che supera le limitazioni delle metodologie precedenti e dimostra una maggiore controllabilità e qualità del testo. Il modello proposto può essere utilizzato per applicazioni di dialoghi controllabili in diversi settori, come la customer service, la formazione e la comunicazione.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">Il lavoro presentato da Jia-Huei Ju e collaboratori si concentra sulla analisi dei rapporti finanziari (Form 10-K) richiesti dalla Securities and Exchange Commission (SEC). L'obiettivo è quello di automatizzare il processo di estrazione di informazioni utili da questi rapporti, che attualmente richiede molti sforzi umani. I ricercatori hanno osservato che i rapporti finanziari di una stessa azienda sono molto simili da un anno all'altro (circa l'80% delle parole sono identiche) e che i contenuti sono dipendenti dall'anno.

Per risolvere questo problema, gli autori introducono un compito di evidenziazione (highlighting) e un pipeline a più stadi. Il compito di evidenziazione consiste nel confrontare e contrapporre il contesto tra due rapporti finanziari consecutivi (target e riferimento). Il modello di evidenziazione deve identificare le parole più importanti (raziocinio) che giustificano le relazioni tra i due rapporti.

Il pipeline a più stadi prevede la segmentazione del documento, la riconoscimento delle relazioni, la fine-tuning fuori e dentro il dominio. I ricercatori hanno proposto un modello di evidenziazione che utilizza un dataset esterno (eSNLI) per la fine-tuning fuori dominio e un dataset intermedio (revised pairs) per la fine-tuning dentro dominio. Il modello è stato valutato utilizzando due metriche: precisione e correlazione tra le previsioni e le annotazioni. I risultati mostrano che il modello di evidenziazione proposto raggiunge le migliori prestazioni sul dataset finale (FINAL) e mantiene la capacità di generalizzazione.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Non viene specificato esplicitamente nell'articolo, ma si fa riferimento alla traduzione da tedesco (German) a inglese.</sample>
    <sample id="222">L'articolo "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" esplora il problema della generalizzazione dei modelli di domanda-risposta (QA) a nuovi domini. I ricercatori si concentrano sulla questione di come migliorare la prestazione dei modelli QA in ambienti di domanda-risposta aperti, dove il modello deve essere in grado di adattarsi a nuovi domini senza avere accesso a un grande numero di dati di addestramento specifici.

Gli autori presentano tre contributi principali. In primo luogo, esplorano diverse interventioni sui dati per abilitare la generalizzazione fuori dal dominio in QA aperto. In secondo luogo, identificano il tipo di shift dei dati che un nuovo dominio esibisce. Infine, determinano quali interventioni sui dati sono efficaci per un tipo specifico di shift.

Gli autori propongono due metodi di interventione sui dati: zero-shot e few-shot. I metodi zero-shot non richiedono esempi del dominio target, mentre i metodi few-shot utilizzano esempi del dominio target per generare nuovi esempi. Gli autori mostrano che i metodi few-shot migliorano la prestazione del modello di 8% in media, mentre i metodi zero-shot migliorano la prestazione del modello di 11% in media.

Inoltre, gli autori presentano una misura di compatibilità per valutare la compatibilità del modello di base con il dominio target. Utilizzano questa misura per mappare i dati di target su una griglia 2D e stimare il tipo di shift dei dati.

Gli autori concludono che solo certi tipi di interventioni sui dati sono efficaci a seconda del tipo di shift dei dati del dominio target. Sottolineano l'importanza di comprendere il tipo di shift dei dati per migliorare la prestazione dei modelli QA in ambienti di domanda-risposta aperti.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">I modelli studiati durante gli esperimenti sono stati due: long-mBART e base mBART.</sample>
    <sample id="225">53 task vengono utilizzate per l'addestramento e 9 vengono utilizzate per il test.</sample>
    <sample id="226">Non sono indicati autori nell'articolo. Sono menzionati solo due presentatori: Regina Stodden e Omar.</sample>
    <sample id="227">Il team di ricerca ha identificato la principale sfida nella comprensione del linguaggio legata alla mancanza di grounding durante l'allenamento dei modelli di linguaggio. I modelli di linguaggio, inclusi quelli di grandi dimensioni, sono stati allenati principalmente su corpora testuali senza grounding, il che rende difficile la mappatura di una espressione linguistica su una rappresentazione specifica in un ambiente. Questo problema è stato evidenziato dall'assenza di grammaticalità e validità dei piani generati dai modelli di linguaggio per compiti di comprensione del linguaggio legata alla grounding.

Per risolvere questo problema, il team ha proposto un framework innovativo chiamato Pangu, che separa la generazione di piani da una componente di discriminazione. Il framework utilizza un agente simbolico per proporre piani candidati e un modello di linguaggio per valutare e classificare questi piani. In questo modo, il modello di linguaggio non è più responsabile della generazione di piani grammaticalmente corretti.

I risultati sperimentali mostrano che Pangu raggiunge prestazioni eccezionali in vari scenari di comprensione del linguaggio legata alla grounding, inclusa la questione di base di conoscenza e la ricerca semantica su Google. Inoltre, Pangu dimostra una grande efficienza di apprendimento e una robustezza notevole in ambienti non i.i.d. Il team conclude che la discriminazione potrebbe essere una strategia più efficace rispetto alla generazione per compiti di comprensione del linguaggio legata alla grounding.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Il lavoro presentato da Gabriella Skitalinskaya e Henning Wachsmuth si concentra sulla detezione di affermazioni migliorabili per supportare la scrittura argomentativa. L'autore esamina il processo di revisione di un affermazione, come "I cellulari causano cancro al cervello", e come essa possa essere migliorata attraverso la specificazione di dettagli, come ad esempio la radiazione cellulare, o la precisazione della possibilità di una connessione causale. Il problema affrontato è quello di determinare quando un affermazione è formulata in modo ottimale e non richiede ulteriori revisioni.

Il lavoro propone due nuove attività: la detezione di affermazioni subottimali e la suggerzione di miglioramenti per le affermazioni. Per risolvere questo problema, gli autori esplorano la possibilità di apprendere dalle pattern di revisione umane, anziché definire esplicitamente cosa rende un affermazione buona o cattiva. Tuttavia, questo approccio presenta alcuni sfide, come la diversità dei domini, delle qualità e delle revisioni.

Gli autori presentano un'analisi dettagliata delle strategie per affrontare queste sfide e una comparazione sistematica delle approcci per le attività introdotte. I risultati mostrano che i dati di revisione possono essere utilizzati efficacemente per le attività proposte e che il modello della distanza tra due versioni affermative è utile per la detezione di affermazioni subottimali. Inoltre, l'impatto dell'informazione contestuale dipende sia dalla task che dalle qualità delle affermazioni da migliorare.</sample>
    <sample id="231">NACHOS è un dataset di dati medici tratti da pagine web, costituito da un insieme di dati medici raccolti tramite crawling web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Il paper "Attention as a Guide for Simultaneous Speech Translation" propone una soluzione innovativa per la traduzione simultanea di parole in tempo reale, denominata SimulST. La traduzione simultanea è un processo che consente la comunicazione cross-linguistica, ma le attuali architetture di SimulST presentano alcuni problemi, come la necessità di addestrare modelli specifici, procedure di addestramento complesse e la necessità di mantenere diversi modelli per raggiungere differenti regimi di latenza.

La proposta del paper è di utilizzare modelli offline di traduzione simultanea esistenti senza doverli rieducare o adottare architetture specifiche per SimulST. Inoltre, si propone di utilizzare un solo modello per ogni regime di latenza e di gestire la latenza attraverso parametri specifici. La strategia proposta si basa sull'uso della meccanica di attenzione tra l'input audio e l'output testuale, noto come cross-attenzione.

Il paper introduce EDAtt, un'abbinata Encoder-Decoder Attention, che decide se emettere o meno una traduzione parziale in base a dove l'attenzione si concentra. La strategia di EDAtt emette una parola se l'attenzione non si concentra troppo sulla parte finale dell'input audio, indicando che la informazione ricevuta è sufficientemente stabile. I risultati mostrano che EDAtt supera le strategie applicate ai modelli offline e le architetture specifiche per la traduzione simultanea, sia in termini di qualità della traduzione che di latenza. Inoltre, la strategia proposta è la più veloce tra quelle considerate.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati, in particolare per la traduzione. Un esperimento semplice ha mostrato che la differenza tra due diversi prompt per ogni frase può essere di oltre 1 punto BLEURT, e in casi estremi può arrivare a 40 punti BLEURT.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, e Graham Neubig.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti per ogni compito sono state utilizzate per testare la capacità del modello di generalizzare a compiti diversi.</sample>
    <sample id="237">Gli autori propongono un test di valutazione, chiamato KITMUS Test, per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da fonti diverse, come ad esempio conoscenza acquisita durante l'addestramento e conoscenza fornita in tempo di inferenza.</sample>
    <sample id="238">L'autore Yebowen Hu, del University of Central Florida, presenta il dataset MeetingBank, un benchmark per lo sviluppo di tecnologie di riassunto per riunioni. Il dataset è stato creato per affrontare due sfide principali: la disponibilità di riassunti di alta qualità e la raccolta di dati da fonti attendibili per riunioni pubbliche. Il processo di raccolta dei dati include la conversione di audio in trascrizioni utilizzando Speechmatics API, l'identificazione dei meeting ID e la raccolta di riassunti di riferimento e segmenti di meeting. Il dataset contiene 1.366 riunioni di City Council e quasi 7.000 istanze.

Il team ha valutato il performance di diversi sistemi di riassunto, inclusi Oracle, LEAD, LexRank, TextRank e modelli neurali abstrattivi come BART-Large, Pagasus, Longformer, DialogLM e HMNet. La valutazione ha evidenziato che l'estrattivo Extr-Oracle ha ottenuto un alto punteggio ROUGE-2, mentre GPT-3 non ha ottenuto risultati soddisfacenti secondo i metrici automatici. Tuttavia, la valutazione umana ha mostrato che GPT-3 ha ottenuto i risultati migliori in termini di fluenza e coerenza.

Il contributo principale del lavoro è la creazione del dataset MeetingBank, che può essere utilizzato per lo sviluppo di tecnologie di riassunto avanzate per riunioni e offre anche una comprensione degli aspetti decisionali delle riunioni di City Council.</sample>
    <sample id="239">Ciao a tutti, il mio nome è David Vilar e sto per fare una breve recensione del paper "Prompting PaLM per la traduzione: valutazione delle strategie e delle prestazioni". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un grande modello di linguaggio con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una grande raccolta di testi, comprensiva di 780 miliardi di token. Al momento della pubblicazione, ha raggiunto i risultati di punta in centinaia di compiti NLP. In questo lavoro, presentiamo la prima valutazione sistematica del prompting dei grandi modelli di linguaggio per la traduzione automatica. Abbiamo valutato la capacità di trasposizione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò include l'utilizzo delle ultime set di test per evitare un sovrapposizione dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi di stato dell'arte, quindi il miglior sistema, quindi l'evaluazione WMT. Utilizziamo metriche di stato dell'arte, neurali MT, e inoltre mostriamo anche risultati di valutazione umana a cura di esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompting ha un grande influenza sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato il prompting a un-shot e fornito due diversi prompt per ogni frase. La maggior parte delle frasi, 516 su 1.000, mostra una differenza di più di un punto BLEURT. E questo può andare, in casi estremi, fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di prompting. Negli esperimenti, ci siamo fermati a una strategia di prompting a 5-shot, dove abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema, con la lingua in cui si trova. Quindi, in questo esempio qui, dove eseguiamo la traduzione da tedesco a inglese, le frasi tedesche, le frasi di partenza, sono contrassegnate con tedesco colon e le traduzioni inglesi con inglese colon. Abbiamo visto che la forma effettiva del prompting non ha un grande influenza nel caso di più prompt brevi. È cruciale per il prompting a zero e uno-shot. E quando andiamo, come nel nostro caso, a 5-shot, non c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano il peso maggiore. La somma dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di partenza. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, abbiamo confrontato la selezione dei prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di alta qualità rispetto ai dati di addestramento, che sono più rumorosi. E i risultati sono migliori quando si utilizzano i dati di sviluppo. Nonostante ciò, i sistemi di stato dell'arte specializzati hanno un grande vantaggio rispetto alle traduzioni di PaLM. Ma PaLM si avvicina molto a un sistema commerciale. Nello specifico, abbiamo scelto di valutare con Google Translate. Le informazioni che abbiamo acquisito dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM dicono che la fluidezza di PaLM è paragonabile a quella dei sistemi di stato dell'arte, ma la principale differenza deriva dall'accuratezza. Quindi, in particolare, gli errori più comuni sono gli errori di omissione. Quindi sembra che PaLM scelga di produrre una traduzione che suona meglio, a volte lasciando fuori parti della frase di partenza che sono necessarie per la traduzione. Tuttavia, la categoria "Stile/Awkward" per PaLM è inferiore a quella dei sistemi di stato dell'arte, che è un ulteriore segnale che PaLM fornisce una traduzione molto fluente, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per ulteriori dettagli, per favore andate alla presentazione completa del paper. Grazie molto.</sample>
    <sample id="240">Ciao, sono Dawei, un dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debole. La supervisione debole non prevede l'etichettatura manuale dei dati. Invece, utilizziamo fonti di etichettatura deboli, come regole di etichettatura semplici, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Se paragonato alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, ovvero una certa quantità di annotazioni è errata. Se addestriamo reti neurali direttamente sui dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore di etichettatura e non generalizzano. L'apprendimento supervisionato debole consiste nell'adeguare gli algoritmi di addestramento per addestrare le reti neurali in modo robusto sotto tale rumore di etichettatura, in modo che i modelli addestrati generalizzino ancora bene. Negli ultimi lavori in WSL, un comune affermazione è che le persone dicono di addestrare solo i modelli sui dati etichettati debolmente e di raggiungere prestazioni elevate sui set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un caveat, ovvero che le persone suppongono che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci a questo problema di impostazione, ma ciò implica che ulteriori annotazioni manuali sono necessarie nell'apprendimento supervisionato debole. Ma come un elefante nella stanza, questa necessità è spesso trascurata. La menzione precedente solleva tre domande di ricerca. Primo, è necessario un set di validazione pulito per WSL o possiamo utilizzare un set di validazione rumoroso al suo posto? Secondo, se è necessario un set di dati pulito, o se è necessario un set di dati pulito per WSL per funzionare, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono metodi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, abbiamo scoperto che i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo di prestazioni. Come mostrato nella figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare al di là delle etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che aumentando il numero di campioni di validazione puliti aiuterà gli approcci WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Di solito, abbiamo bisogno di 20 campioni per classe per raggiungere prestazioni elevate. Ma non è la fine della storia, perché se decidiamo di accedere ai campioni puliti, allora addestrare su di essi direttamente raggiungerà prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci WSL, che utilizzano i dati puliti per la validazione solo. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a superare gli approcci WSL. Infine, l'incremento di prestazioni affermato in precedenti approcci WSL può essere facilmente raggiunto consentendo di continuare a fine-tunare sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello base, chiamato FTw, inizialmente sottoprende gli approcci più complessi WSL, come COSINE. Tuttavia, se consentiamo di continuare a fine-tunare sui campioni puliti, FTw raggiunge prestazioni uguali agli altri metodi. Quindi, in pratica, non c'è ragione di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio di archiviazione. Per riassumere, abbiamo mostrato che gli approcci WSL recenti richiedono campioni manualmente annotati puliti per funzionare correttamente. Il loro guadagno di prestazioni e praticità è enormemente sopravvalutato. Le nostre raccomandazioni concrete per future ricerche sono le seguenti. Primo, si dovrebbero riportare le criteri di selezione del modello. Ad esempio, si dovrebbe riportare se la selezione del modello è effettuata tramite campioni di validazione puliti. Secondo, gli approcci WSL dovrebbero essere confrontati con basi di apprendimento a pochi esempi, poiché entrambi lavorano sui campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma forte che dovrebbe essere considerato in future ricerche in WSL. Infine, abbiamo reso disponibile il nostro codice. Potete trovarlo tramite il codice QR sulla slide. Per favore, sentitevi liberi di verificarlo. Grazie e godetevi la conferenza.</sample>
    <sample id="241">Il lavoro presentato, intitolato "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments", affronta il problema della detezione dell'informazione falsa su piattaforme social come Twitter. I metodi attuali di detezione dell'informazione falsa presentano due principali limitazioni: sono spesso valutati in modo irrealistico, utilizzando dati storici anziché dati in tempo reale, e non tengono conto della complessità e della noia dei dati reali. Inoltre, questi metodi spesso non coinvolgono i moderatori umani nel processo di detezione dell'informazione falsa.

Per superare queste limitazioni, gli autori propongono un framework di valutazione per lo sviluppo di sistemi che coinvolgano i moderatori umani in vari stadi del processo. Il sistema proposto è un sistema end-to-end che va dalla raccolta dei dati in tempo reale a output azionabili per i moderatori umani. Il sistema è composto da due componenti principali: la prima componente si occupa della detezione delle affermazioni fuorvianti, utilizzando un modello T5 per l'estrazione delle affermazioni e un modello BERT per la classificazione dello stato d'animo dell'autore; la seconda componente si occupa della verifica delle violazioni delle politiche social, utilizzando le affermazioni verificate nella prima componente per flaggare le violazioni delle politiche.

Gli autori hanno valutato l'efficacia del loro framework utilizzando un set di dati di tweet realistici e hanno trovato che il sistema ha un tasso di detezione delle violazioni delle politiche del 65%. Inoltre, hanno valutato la quantità di lavoro umano richiesta per confermare le violazioni delle politiche e hanno trovato che il sistema può confermare 124,2 violazioni delle politiche per ogni ora lavorata da un moderatore umano. Il lavoro proposto contribuisce a sviluppare sistemi di detezione dell'informazione falsa più realistici e umanizzati.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo sono:

1. Valutazione umana: chiedere ai giudici umani di selezionare quale delle due conversazioni è migliore o di assegnare un punteggio utilizzando una scala Likert.
2. Valutazione a livello di turno: chiedere ai giudici umani di valutare ogni turno di conversazione utilizzando una scala Likert.
3. Valutazione a livello di dialogo: chiedere ai giudici umani di valutare l'intera conversazione utilizzando una scala Likert.
4. Comparazione a livello di dialogo: chiedere ai giudici umani di scegliere quale delle due conversazioni è migliore.</sample>
    <sample id="243">Sono coinvolti 6 autori: Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap e 2 autori non menzionati esplicitamente (Jenny e l'autore dell'articolo che Jenny sta presentando).</sample>
    <sample id="244">Nell'esempio con Servin e Kea, sono necessarie conoscenze di base come "Giudici decidono casi in corti di legge" per risolvere la pronome "he" correttamente.</sample>
    <sample id="245">L'articolo "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" esplora la possibilità di identificare lavoratori affidabili su Amazon Mechanical Turk (MTurk) per la comprensione dei testi. I ricercatori presentano un approccio a due fasi per selezionare lavoratori affidabili, denominato "pipeline". La prima fase consiste in un compito di qualificazione che valuta l'abilità dei lavoratori nell'evaluare diverse dimensioni di un testo. I lavoratori che superano questo compito vengono categorizzati in quattro tipi: oro, argento, bronzo e blocco. Solo i lavoratori oro e argento possono proseguire alla seconda fase, che consiste in un compito di endurance che valuta la capacità dei lavoratori di gestire un carico di lavoro pesante.

Il pipeline seleziona 26 lavoratori MTurk, di cui 8 oro e 18 argento, che mostrano un alto livello di accordo tra loro (IAA). I risultati mostrano che il pipeline può selezionare lavoratori affidabili in modo efficace e a basso costo, e che i lavoratori selezionati possono produrre risultati di alta qualità. L'articolo conclude che il pipeline può essere utilizzato come best practice per la selezione di lavoratori affidabili su larga scala e a basso costo, e che è necessario ulteriori studi per migliorare la selezione di lavoratori affidabili e per estendere l'approccio a diverse applicazioni e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Il team di ricerca di Jiho Kim da KAIST AI ha presentato un nuovo paper intitolato "FACTKG: Fact Verification via Reasoning on Knowledge Graphs". Il lavoro si concentra sulla creazione di un nuovo dataset, FactKG, che utilizza le conoscenze grafiche (KG) come fonte di evidenza per la verifica dei fatti. Il dataset è stato creato utilizzando il grafo di conoscenza DBpedia e include due stili di affermazioni: scritte e colloquiali. Le affermazioni sono etichettate come SUPPORTED o REFUTED e il compito consiste nel recuperare l'evidenza dal grafo di conoscenza e verificare l'affermazione utilizzando l'evidenza.

Il dataset FactKG include cinque tipi di ragionamento: one-hop, congiunzione, esistenza, multi-hop e negazione. Il team di ricerca ha anche sviluppato basi di dati per confrontare le prestazioni dei modelli di verifica dei fatti. I risultati mostrano che il modello GEAR, che utilizza l'evidenza grafica, supera le altre basi di dati, compresa la maggioranza dei classi. Il lavoro di FactKG offre una nuova prospettiva sulla verifica dei fatti utilizzando le conoscenze grafiche e può essere utilizzato in vari compiti che richiedono la verifica dei fatti, come la verifica della coerenza tra il linguaggio naturale e i grafi di conoscenza.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono stati selezionati per essere diversi e rappresentativi di vari gruppi demografici, come ad esempio il Paese, il genere, l'età e il livello di istruzione, grazie all'utilizzo di piattaforme di crowdsourcing come Lab in the Wild.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore alle frasi, con l'obiettivo di preservare la struttura rilevante, ma senza alterare la loro accettabilità.</sample>
    <sample id="250">Eseguire una valutazione dimensionale significa analizzare e misurare diversi aspetti o dimensioni di una cosa, in questo caso la qualità della conversazione. In altre parole, si cerca di comprendere le forze e le debolezze di un modello di dialogo esaminando diverse caratteristiche specifiche, come la rilevanza delle risposte, la coerenza, l'empatia, ecc.</sample>
    <sample id="251">Non sono state menzionate esplicitamente le affiliazioni degli autori dell'articolo, ma Jingwei Yi è un studente della University of Science and Technology of China.</sample>
    <sample id="252">Il lavoro "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion" presenta un nuovo approccio per il retrieval di precedenti giuridici (Prior Case Retrieval, PCR) utilizzando tecniche di apprendimento non supervisionato e estrazione di eventi. Gli autori, Sai Kiran Tanikella e collaboratori, hanno creato un nuovo dataset, IL-PCR, che contiene 7.070 casi giuridici indiani con 6,775 citazioni medie per documento di query. Il dataset è stato utilizzato per valutare il performance di diversi modelli di retrieval, tra cui quelli basati su eventi.

Il metodo U-CREAT prevede la creazione di un modello di retrieval che utilizza l'estrazione di eventi per rappresentare i documenti giuridici. L'estrazione di eventi è stata effettuata utilizzando una tecnica di parsing di dipendenza. Il modello di retrieval è stato valutato su diversi set di dati, tra cui IL-PCR e COLIEE'21, e ha mostrato risultati significativamente migliori rispetto ai metodi di retrieval tradizionali.

Il metodo U-CREAT ha anche mostrato una buona generalizzazione across diversi sistemi giuridici, senza richiedere adattamenti specifici per legge o demografia. Inoltre, il metodo ha mostrato una bassa complessità computazionale e una bassa influenza del tempo di inferenza.

In sintesi, il lavoro "U-CREAT" presenta un nuovo approccio innovativo per il retrieval di precedenti giuridici utilizzando tecniche di apprendimento non supervisionato e estrazione di eventi. Il metodo ha mostrato risultati significativamente migliori rispetto ai metodi di retrieval tradizionali e ha la potenzialità di essere applicato in diversi contesti giuridici.</sample>
    <sample id="253">Il lavoro "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media" è stato presentato da un gruppo di ricercatori messicani e spagnoli. L'obiettivo del progetto è sviluppare un modello di intelligenza artificiale in grado di rilevare segni di disturbi mentali nelle interazioni di social media. Il modello, chiamato DisorBERT, utilizza la tecnica di adattamento di dominio per trasferire il conoscimento acquisito da un modello di linguaggio generale (BERT) a un dominio specifico (Reddit e salute mentale). Il modello è stato testato utilizzando i dati del set di eRisk e ha dimostrato di avere un buon equilibrio tra precisione e recall rispetto ai metodi di base.

Il DisorBERT utilizza una tecnica di mascheratura guidata per far concentrare il modello sulle parole importanti durante il processo di apprendimento. Il modello è stato testato su un insieme di 21 item del Beck's Depression Inventory (BDI) e ha dimostrato di generare risposte più negative e orientate alla salute mentale rispetto al modello BERT.

Il lavoro conclude che il DisorBERT è un modello efficace per rilevare segni di disturbi mentali nelle interazioni di social media e suggerisce future linee di ricerca per migliorare il modello e applicarlo a dati clinici.</sample>
    <sample id="254">Il lavoro di ricerca "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" presentato da Sun Qi della Nanjing University of Science and Technology affronta il problema della denoising dei dati di etichettatura distanti utilizzati per l'estrazione di relazioni a livello di documento. I dati distanti contengono livelli di rumore variabili, che possono causare errori di etichettatura false positive. I metodi precedenti per alleviare il problema del rumore utilizzano etichette pseudo, ma questi possono indurre ulteriori errori. Per risolvere questo problema, l'autore propone un framework per l'estrazione di relazioni a livello di documento con denoising di etichette guidato da incertezza. Il framework utilizza l'estimazione dell'incertezza per determinare se le previsioni del modello possono essere fidate o meno. Viene introdotta una strategia di rilabeling dinamica con soglie di incertezza di classe dinamiche per filtrare le etichette pseudo con alta incertezza. Inoltre, viene proposto un approccio di re-labeling iterativo per migliorare ulteriormente le prestazioni del modello. I risultati mostrano che il framework proposto supera le basi di confronto su due dataset pubblici. Le principali contribuzioni del lavoro sono la proposta di un framework con denoising di etichette guidato da incertezza, un metodo di estimazione dell'incertezza a livello di istanza per relazioni sovrapposte, una strategia di rilabeling iterativa con soglie di incertezza di classe dinamiche e miglioramenti significativi delle prestazioni.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero-shot e one-shot prompting, mentre in presenza di più di un prompt (come nel 5-shot prompting) la forma del prompting non ha un grande impatto.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">Nel video "Can Large Language Models Be an Alternative to Human Evaluation?", Chiang Cheng-Han presenta un lavoro che esplora l'idea di utilizzare modelli linguistici di grandi dimensioni per valutare la qualità del testo in elaborazione del linguaggio naturale. I ricercatori hanno proposto di utilizzare questi modelli per valutare le storie generate da GPT-2 o scritte da esseri umani, basandosi su quattro attributi: grammatica, coerenza, affascinamento e rilevanza. Hanno condotto un esperimento utilizzando quattro modelli linguistici di grandi dimensioni (T0, InstructGPT e ChatGPT) e hanno confrontato i risultati con le valutazioni umane. I risultati hanno mostrato che due dei modelli, Davinci e ChatGPT, hanno preferito le storie scritte dagli esseri umani rispetto a quelle generate da GPT-2, come fatto dai valutatori umani. Ciò suggerisce che i modelli linguistici di grandi dimensioni potrebbero essere utilizzati come alternativa alle valutazioni umane in questo compito. Tuttavia, gli autori riconoscono che ci sono ancora molte domande aperte, come ad esempio se i modelli linguistici e i valutatori umani concordano sui singoli voti di ciascuna storia e quali siano i benefici e i costi dell'utilizzo di valutazioni di grandi modelli linguistici rispetto a quelle umane. Il lavoro è stato presentato in un paper e un poster al convegno ACL.</sample>
    <sample id="259">Il team di ricerca di Yusen Zhang della Penn State University ha presentato il progetto XSemPLR, un benchmark unificato per l'analisi semantica cross-linguale con multiple lingue naturali e rappresentazioni di significato. Lo scopo del progetto è di tradurre query in diverse lingue naturali in rappresentazioni di significato multiple, come SQL, Lambda calculus e FunQL.

Il team ha identificato una lacuna nella ricerca esistente, che non copre abbastanza lingue naturali e rappresentazioni di significato. Pertanto, hanno creato un dataset XSemPLR che contiene 9 set di dati in vari domini, 5 compiti di analisi semantica, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche.

Il team ha condotto una valutazione approfondita di tre tipi di modelli di linguaggio multilingue rappresentativi: Encoder-PTR, Encoder-Decoder e multilingue. I risultati hanno mostrato che gli Encoder-Decoder ottengono le migliori prestazioni su tutti e nove i set di dati. Inoltre, si è scoperto che pre-istruire i modelli su lingue naturali inglesi può migliorare significativamente le prestazioni dei modelli few-shot su lingue naturali di destinazione.

Il team ha anche valutato il gap di prestazioni tra le lingue e ha trovato che il trasferimento zero-shot presenta un gap significativo, mentre il trasferimento few-shot riduce rapidamente il gap. I risultati hanno anche mostrato che i modelli di linguaggio multilingue come Codex e BLOOM sono ancora insufficienti per compiti di analisi semantica cross-linguale.

In sintesi, il progetto XSemPLR fornisce un benchmark unificato per l'analisi semantica cross-linguale e offre una valutazione approfondita di modelli di linguaggio multilingue rappresentativi. I risultati mostrano molte scoperte interessanti e possono contribuire a migliorare la ricerca in questo campo.</sample>
    <sample id="260">Non sono in grado di trovare il numero esatto degli autori.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="262">Non sono stati menzionati gli autori specifici, quindi non è possibile fornire un numero preciso.</sample>
    <sample id="263">Il lavoro presentato si concentra sul problema delle biasse dei etichetti (label biases) nell'apprendimento in contesto (in-context learning), un paradigma popolare per l'utilizzo di grandi modelli di linguaggio. L'apprendimento in contesto è noto per essere instabile a causa di varie scelte di design, come la scelta e l'ordine degli esempi in contesto. La ricerca precedente ha dimostrato che l'instabilità della ricerca in contesto deriva da queste scelte e che introducono biasse nelle predizioni dei modelli. Tuttavia, non esisteva una discussione sistematica per categorizzare le scoperte precedenti sui problemi di bias e per individuare nuovi tipi di bias e, più importantemente, per mitigare l'effetto di diversi tipi di bias.

Il lavoro propone una tipologia delle biasse dei etichetti e identifica un nuovo tipo di bias importante, noto come bias dei etichetti di dominio (domain-label bias). Questo tipo di bias si verifica quando il modello è influenzato dalle parole del dominio del compito. Per affrontare questo problema, il lavoro propone un metodo di calibrazione innovativo, noto come calibrazione del contesto di dominio (domain-context calibration), che utilizza parole casuali del dominio del compito per stimare la biasse dei etichetti e calibrare le predizioni originali del modello.

Gli esperimenti condotti dimostrano che la calibrazione del contesto di dominio migliora significativamente il rendimento dell'apprendimento in contesto su una varietà di dataset e modelli di linguaggio. La ricerca conclude che la calibrazione del contesto di dominio è un metodo efficace per mitigare l'effetto di diversi tipi di bias e migliorare il rendimento dell'apprendimento in contesto.</sample>
    <sample id="264">Il lavoro di Lin Wang, intitolato "TAVT: Towards Transferable Audio-Visual Text Generation", si concentra sulla generazione di testi multimodali in risposta a stimoli audio-visivi. La principale sfida di questo compito è la variazione dei domini, che può influire sulla comprensione del contenuto. Per superare questo ostacolo, l'autore propone un nuovo compito chiamato Transferable Audio-Visual Text Generation, che consiste nel mappare concetti visivi diversi in uno spazio semantico unificato.

Il modello proposto da Lin Wang è composto da tre componenti: un network di mappatura meta-audio-visiva, un encoder e generator audio-visivi e un'apprendimento contrattivo counterfactual. Il primo modello mappa concetti visivi diversi in uno spazio semantico unificato, mentre il secondo modello utilizza un encoder e generator basati su trasformatori per generare testi in risposta a stimoli audio-visivi. Il terzo modello propone un'apprendimento contrattivo counterfactual per ottimizzare la allineamento visivo-testuale.

Gli esperimenti condotti da Lin Wang mostrano che il modello proposto, chiamato TAVT, outperisce i modelli di riferimento in tutti i metri e in entrambi i setting di test (cross-dataset e cross-domain). Inoltre, gli esperimenti di ablatività mostrano che le caratteristiche audio hanno un impatto significativo sulla prestazione del modello. Il lavoro di Lin Wang rappresenta un importante passo avanti nella generazione di testi multimodali e offre una nuova prospettiva per la comprensione del contenuto audio-visivo.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Non sono state menzionate specifiche affiliazioni degli autori dell'articolo.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli omissioni, ovvero il modello tende a omettere parti del testo originale durante la traduzione per produrre una traduzione che suoni meglio.</sample>
    <sample id="269">Ciao, sono James Finch e Sarah Finch. E oggi vi parleremo del nuovo approccio dimensionale per l'evaluazione dell'intelligenza artificiale conversazionale, noto come ABC-Eval. Questo lavoro è stato condotto dal laboratorio di NLP dell'Università di Emory, guidato dal professor Jinho Choi, in collaborazione con Amazon Alexa AI.

Quando si sviluppa un modello di dialogo e si vuole vedere come si confronta con la tecnologia attuale, la pratica comune è utilizzare l'evaluazione umana, come chiedere ai giudici umani di selezionare quale delle due conversazioni è migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità del dialogo per comprendere le abilità e le debolezze del modello su un livello più dettagliato.

Una possibile approccio è chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi di confronto o scala Likert esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluazione dimensionale del dialogo.

Il nostro approccio cerca di ridurre la soggettività dell'evaluazione umana annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrilevanti o contraddittorie. Chiamiamo questo approccio annotazione di comportamenti in chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello chat che sono stati suggeriti di influenzare la qualità del dialogo nella letteratura recente.

ABC-Eval è in grado di misurare le percentuali con cui i modelli chat commettono errori tematici vari. Ad esempio, ABC-Eval misura il numero di turni in cui un modello chat ignora il suo partner o dice qualcosa di irrilevante, contraddittorio con sé stesso o con il suo partner, inventa fatti falsi o viola la conoscenza del senso comune, e quando il modello riesce o fallisce a mostrare empatia.

Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli chat di stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bottone per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti di dialogo a livello di dialogo.

Per ogni metodo esistente, abbiamo raccolto valutazioni su otto delle dimensioni di dialogo più comuni, poiché questa è la pratica standard per valutare i modelli chat lungo più dimensioni. Dalla nostra analisi dei risultati di valutazione, abbiamo trovato che le etichette di comportamento di ABC-Eval sono in generale più affidabili delle etichette raccolte dai metodi esistenti, misurate dall'accordo tra gli annotatori su 100 conversazioni doppie-annotate.

Inoltre, le etichette di ABC-Eval sono più predittive della qualità della conversazione complessiva rispetto ai metri prodotti dai metodi esistenti, come mostrato da questo semplice analisi di regressione lineare. Ad esempio, potete vedere come misurare la proporzione di turni con contraddizioni con sé stesso e con il partner spiega il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi Likert medi spiegano solo il 4% o meno.

Infine, abbiamo controllato se ogni metro di valutazione cattura un aspetto unico della qualità del dialogo utilizzando una regressione lineare step-by-step. Potete vedere come la combinazione di tutti i metri di ABC-Eval spiega oltre il 25% della qualità della conversazione, e mentre rimuovi i metri uno alla volta, la maggior parte di loro risultano nella perdita di una buona quantità di informazioni sulla qualità.

Questi metri affidabili, informativi e distinti di ABC-Eval ci consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Potete vedere che nei risultati del nostro esperimento rimangono ancora alcune sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il loro partner in circa il 10% delle volte.

Con il ritmo veloce di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati dal momento in cui è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E ci auguriamo di vedere come l'intelligenza artificiale conversazionale avanzerà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono affiliati all'Emory NLP Lab dell'Emory University e collaborano con Amazon Alexa AI.</sample>
    <sample id="271">FTw (F)</sample>
    <sample id="272">7 autori sono coinvolti nell'articolo: John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, Adina Williams e Koustav Sinha.</sample>
    <sample id="273">Ciao, il mio nome è Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede un contesto? Una esplorazione multilingue guidata dai dati". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.

Molte traduzioni dipendono dal contesto. Ad esempio, come tradurre la parola "mole" in questa frase? Se la frase precedente era "Le cose potrebbero diventare pericolose se i ministri scoprissero", allora "mole" si riferisce a un agente segreto. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un segno di nascita. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la sua traduzione cambia.

Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile catturare queste traduzioni con metriche a livello di corpus come BLEU. Alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insiemi di lingue limitati, poiché di solito dipendono dalla conoscenza del dominio e dalla cura umana.

Nel nostro lavoro, cerchiamo di rispondere a queste due domande. In primo luogo, quando la traduzione richiede un contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato a misurare quanto una parola dipenda dal contesto durante la traduzione. In precedenti lavori, abbiamo introdotto CXMI come misura per l'utilizzo del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quanto informazione il contesto C fornisce per il bersaglio Y, dato il sorgente X. Potresti pensare a CXMI come l'informazione guadagnata dando contesto al modello.

In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto P-CXMI come quelle che richiedono un contesto per la traduzione. Ora analizziamo le parole con alto P-CXMI per cercare pattern tra queste parole. E eseguiamo il nostro analisi su trascrizioni dei discorsi TED che sono state tradotte dall'inglese in 14 lingue diverse. Eseguiamo il nostro analisi a tre livelli diversi.

In primo luogo, guardiamo le etichette di parte del discorso che hanno un alto P-CXMI medio. E questo ci consente di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi serve un contesto per determinare se un pronome è duplice quando si traduce in arabo. E in modo simile, troviamo che certe lingue richiedono un contesto anche quando si vuole scegliere la forma verbale appropriata.

Poi guardiamo gli elementi lessicali che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. E questo ci aiuta a identificare casi come questo, dove in cinese serve un contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. E in modo simile, troviamo che il contesto è importante per tradurre con la giusta formalità.

Infine, guardiamo diversi token individuali che hanno un alto P-CXMI. E questo ci consente di identificare fenomeni che non possono essere catturati dal significato della parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione degli ellissi.

Quindi, usiamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo etichettatori per identificare automaticamente le parole che si riferiscono al fenomeno. E chiamiamo il nostro etichettatore il Multilingual Discourse-Aware, o MuDA tagger. E possiamo anche notare che diverse lingue hanno diverse proporzioni di questi fenomeni del discorso.

Quindi, utilizziamo il MuDA tagger applicando l'etichettatore su un corpus parallelo che vogliamo utilizzare per la valutazione, e applichiamo le nostre metriche di traduzione di scelta sulle esemplificazioni dipendenti dal contesto identificate dal MuDA tagger. E infine, utilizziamo il nostro benchmark insieme ad altre metriche per valutare diversi modelli per la traduzione automatica a livello di documento.

Innanzitutto, quando utilizziamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli agnostici al contesto hanno la migliore prestazione. Ma poi se utilizziamo COMET, i modelli consapevoli del contesto hanno la migliore prestazione. E se utilizziamo la misura di precisione a livello di parola, allora i modelli con e senza contesto hanno prestazioni comparabili. E questo di nuovo dimostra che è difficile determinare il sistema di traduzione a livello di documento migliore se si utilizzano sole le metriche a livello di corpus.

Quindi, utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più precisi dei modelli che non utilizzano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto per altri fenomeni come gli ellissi, i pronomi e la forma verbale. Quindi, questo tipo di suggerisce dove dovremmo vedere più progressi per la traduzione a livello di documento.

Inoltre, abbiamo confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è di solito più preciso di Google Translate per la traduzione a livello di documento.

Per riassumere, eseguiamo un'analisi guidata dai dati su 14 paia di lingue per identificare quando le traduzioni richiedono un contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento che ci aiuti a identificare quali fenomeni del discorso i modelli possono gestire bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Il lavoro di Ananya e Vignesh, intitolato "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages", si concentra sulla valutazione delle metriche di traduzione automatica per le lingue indiane. La traduzione automatica è un campo in continua evoluzione, ma la valutazione delle metriche utilizzate per misurarne la qualità è ancora un argomento poco esplorato, specialmente per le lingue non inglesi. Il team di Ananya e Vignesh si propone di colmare questo divario, focalizzandosi sulle lingue indiane, in particolare sulle cinque lingue: Tamil, Malayalam, Hindi, Marathi e Gujarati.

Il lavoro consiste nella creazione di un dataset di 7.000 esempi di traduzione automatica, generati da sette modelli di traduzione diversi, per ciascuna delle cinque lingue. I dati sono stati annotati da esperti bilingui, che hanno valutato la qualità delle traduzioni e segnalato gli errori, classificandoli in tre categorie: errori di accuratezza, errori di fluenza e errori speciali. I risultati mostrano che le metriche di valutazione automatica possono essere efficaci per le lingue indiane, ma richiedono una fine-tuning per ogni lingua specifica. Il team ha anche creato un metrica personalizzata, chiamata IndicCOMET, che mostra un miglioramento significativo rispetto alle metriche di base.

In sintesi, il lavoro di Ananya e Vignesh contribuisce a migliorare la valutazione delle metriche di traduzione automatica per le lingue indiane, fornendo un dataset e una metrica personalizzata che possono essere utilizzati da ricercatori e sviluppatori di tecnologie di traduzione automatica.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel testo.</sample>
    <sample id="278">Il metodo "Parole contrassegnate" (Marked Words) è stato descritto come un metodo per identificare le parole che distinguono i gruppi contrassegnati (marked) da quelli non contrassegnati (unmarked). Questo metodo si basa sul concetto sociolinguistico di "contrassegnamento" (markedness), secondo cui esiste un default non contrassegnato e qualsiasi gruppo che si discosti da questo default è contrassegnato linguisticamente. Ad esempio, la parola "guerriero" è comunemente associata agli uomini, quindi quando si descrive una guerriera, si specifica spesso "guerriera donna" per contrassegnare il termine. Il metodo "Parole contrassegnate" designa i gruppi contrassegnati e non contrassegnati e confronta le parole utilizzate nelle descrizioni dei gruppi contrassegnati con quelle utilizzate nei gruppi non contrassegnati, utilizzando le weighted log-odds ratios per identificare le parole più significative per ogni gruppo contrassegnato.</sample>
    <sample id="279">Non sono state menzionate specifiche affiliazioni degli autori dell'articolo.</sample>
    <sample id="280">Shi Tao ha presentato il suo lavoro "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations", un framework innovativo per la rilevazione delle emozioni in conversazioni. L'obiettivo è quello di prevedere l'etichetta emotiva di ogni enunciato nella conversazione, considerando le tre modalità di testo, audio e video. Tuttavia, i metodi esistenti hanno dei limiti, come la mancanza di sfruttamento delle informazioni multimodali e la difficoltà di classificare le classi di emozioni minoritarie e simili.

Per superare questi problemi, Shi Tao propone il framework MultiEMO, composto da quattro componenti chiave: estrazione di caratteristiche unimodal, modellazione di contesto, fusione multimodale e classificazione emotiva. Il contributo principale di questo lavoro consiste nella proposta di un nuovo estrattore di caratteristiche visive chiamato VisExtNet, che cattura le espressioni facciali degli interlocutori senza includere informazioni di scena irrilevanti.

Inoltre, Shi Tao propone un modello di fusione multimodale chiamato MultiAttn, che utilizza attenzione multi-testo e multi-audio per integrare le informazioni delle tre modalità. Infine, è stato proposto un nuovo tipo di perdita di apprendimento chiamato Sample-Weighted Focal Contrastive Loss, che assegna più peso alle classi di emozioni minoritarie e aiuta a distinguere le emozioni simili.

Gli esperimenti condotti su due dataset ERC, MELD e IEMOCAP, hanno dimostrato che MultiEMO raggiunge le migliori prestazioni rispetto ai metodi esistenti, con miglioramenti significativi nelle classi di emozioni minoritarie e simili.</sample>
    <sample id="281">Nel lavoro "When Does Translation Require Context? A Data-driven, Multilingual Exploration", gli autori Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig esplorano l'importanza del contesto nella traduzione. Utilizzando un corpus di 14 lingue, hanno sviluppato un metodo per misurare l'utilizzo del contesto (CXMI) e lo hanno esteso a livello di parola (P-CXMI). L'analisi ha rivelato che certi tipi di parole, come i pronomi duali in arabo e i verbi in forma appropriata, richiedono contesto per essere tradotti correttamente. I ricercatori hanno anche identificato fenomeni di discorso come la risoluzione degli ellissi e la forma di formalità che richiedono contesto per essere tradotti correttamente. Per valutare l'efficacia dei modelli di traduzione, gli autori hanno creato un benchmark, chiamato MuDA (Multilingual Discourse-Aware), che identifica esempi di traduzione che richiedono contesto. I risultati mostrano che i modelli che utilizzano contesto sono più precisi di quelli che non lo utilizzano per certi fenomeni di discorso, ma non per altri. Il lavoro conclude che la valutazione dei modelli di traduzione richiede un approccio più approfondito e che il contesto è un fattore importante nella traduzione documentale.</sample>
    <sample id="282">Il lavoro presentato da Xuekai Zhu alla conferenza ACL 2023 si intitola "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing". Questo studio si concentra sulla trasferimento di stile non parallelo in testi, un compito importante nella generazione di linguaggio naturale. La maggior parte degli studi precedenti si sono concentrati sul livello di token o frase, come il trasferimento di sentimento o la forma di testo. Invece, questo lavoro prende un passo in avanti trasferendo lo stile a livello di storia e a livello di discorso, che è fondamentale per imitare lo stile dell'autore.

La principale sfida in questo compito è imitare le scelte linguistiche dell'autore a livello di discorso, come tecniche narrative e stili che sono spesso associati a specifici argomenti di scrittura. Ciò rende difficile trasferire questo contenuto specifico dello stile a un altro stile.

Per risolvere questi problemi, viene proposto un modello di generazione chiamato StoryTrans, che impara rappresentazioni di discorso dai testi di origine e combina questo con embedding di stile imparabili per generare testi in stili di destinazione. Il modello è stato progettato per ridurre le caratteristiche stilistiche dalle rappresentazioni di discorso e per preservare il contenuto.

Il modello StoryTrans è stato valutato su due stadi di training e ha dimostrato di poter trasferire con successo lo stile di autori noti in testi di storia e di ogni giorno. Le valutazioni automatiche e manuali confermano l'efficacia del modello e mostrano che StoryTrans supera i modelli di base in termini di controllo dello stile e di preservazione del contenuto.</sample>
    <sample id="283">Hudson's Word Grammar.</sample>
    <sample id="284">Il lavoro di Peng Tianshuo, intitolato "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction", presenta un nuovo approccio per migliorare l'estrazione di informazioni (IE) tramite l'uso di una meccanismo di span fuzzy. La proposta si concentra sul problema di precisione delle span boundary, che può essere soggettiva e influenzare negativamente i risultati dell'estrazione di informazioni. Il modello FSUIE introduce due componenti principali: il fuzzy span loss e la fuzzy span attention.

Il fuzzy span loss consente di convertire la distribuzione continua della span boundary in valori discreti, riducendo la dipendenza dalle span boundary. La fuzzy span attention, invece, è un meccanismo di maschera che adatta la distribuzione di attenzione del modello per ridurre l'errore di troncamento delle span boundary.

Gli esperimenti condotti su tre compiti di IE (riconoscimento di entità, estrazione di relazioni e estrazione di triplette di sentimento) hanno mostrato che FSUIE raggiunge risultati significativamente migliori rispetto ai modelli basati su span fixed. In particolare, FSUIE raggiunge nuovi record di prestazioni su dataset come ACE2004, 2005 e ADE, e dimostra una maggiore capacità di generalizzazione per compiti di IE domain-specific. L'analisi ablativa conferma l'efficacia dei due componenti principali del modello FSUIE.</sample>
    <sample id="285">Il team di ricerca di Mingqi Gao da Peking University ha presentato un nuovo approccio per la correzione di errori factual in riassunti di dialoghi. Lo studio, intitolato "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework", mette in luce i limiti delle attuali metodologie di valutazione per i modelli di correzione di errori factual (FEC). I ricercatori hanno identificato due principali problemi: la vaghezza delle metriche di factualità e la mancanza di una valutazione approfondita della performance dei modelli FEC.

Per superare questi limiti, il team propone un nuovo framework di valutazione che prevede la creazione di annotazioni di riferimento corrette a mano. Questo approccio offre due vantaggi principali: fornisce dati più preziosi per l'addestramento dei modelli FEC e consente una valutazione più completa e accurata della loro performance.

Inoltre, i ricercatori hanno proposto una nuova taxonomia degli errori factual che distingue due classi di errori: quelli basati sul contenuto e quelli basati sulla forma. Il framework di valutazione proposto si basa sull'ERRANT, un metro di valutazione per la correzione di errori grammaticali, e prevede tre fasi principali: allineamento, classificazione e confronto.

Gli esperimenti condotti dal team hanno mostrato che l'addestramento dei modelli FEC con riassunti di riferimento corretti a mano da dataset di riassunti di dialoghi porta i migliori risultati in termini di metriche di factualità non affidabili. Inoltre, l'introduzione di dati annotati da umani durante l'addestramento dei modelli FEC per la riassumazione di dialoghi può migliorare la loro performance. Infine, i modelli FEC attuali hanno difficoltà a correggere errori di aggiunta e non riescono a risolvere errori di attributo, modalità, collegamento, ecc.</sample>
    <sample id="286">Professor Jinho Choi</sample>
    <sample id="287">Ci sono 4 autori coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="288">BLiMP, SyntaxGym, Wikipedia e dataset di corrispondenza stereotipica come CrowS pairs.</sample>
    <sample id="290">WSL (Weakly Supervised Learning), COSINE (non specificato), FTw (Fine-Tuning), WSL (di nuovo), few-shot learning (non specificato)</sample>
    <sample id="291">Il modello DrBERT viene valutato su 11 attività biomedicali e cliniche downstream in francese, tra cui: 

- Named entity recognition
- Classification
- Part-of-speech tagging
- Question answering</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Il lavoro presentato da Valerio Basile è il risultato di una collaborazione tra l'Università di Torino e Amazon Alexa. L'obiettivo è di investigare il problema della detezione dell'ironia nel linguaggio naturale, un compito difficile per i modelli di elaborazione del linguaggio. Per studiare questo problema, è stato creato un corpus chiamato EPIC (English Perspectivist Irony Corpus), che contiene 300 conversazioni brevi da fonti diverse, tra cui social media, Reddit e Twitter, raccolte su un periodo di 1,5 anni. Il corpus è stato annotato da 74 annotatori, selezionati tramite il piattaforma di crowdsourcing Prolific, che hanno fornito 5 annotazioni per ogni conversazione. I risultati mostrano che i modelli di detezione dell'ironia che tengono conto delle diverse prospettive degli annotatori (chiamati "perspectivist models") sono più sicuri e meno incerti rispetto ai modelli tradizionali. Inoltre, gli autori hanno trovato che le differenze di opinione tra gli annotatori sono più marcate tra le generazioni più vicine e tra gli annotatori provenienti da aree geografiche diverse. Questi risultati suggeriscono che la detezione dell'ironia è un compito complesso che richiede di considerare le diverse prospettive e il contesto in cui si esprime il linguaggio.</sample>
    <sample id="297">Il progetto "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" si concentra sull'analisi delle "dog whistle", termini che inviano messaggi differenti a diversi gruppi di persone. L'autore presenta un glossario di oltre 340 termini e simboli, in particolare per dog whistle razzisti, transfobici e antisemiti, e propone una tipologia e una classificazione di questi termini. La ricerca esplora come i dog whistle siano utilizzati nella retorica politica, come ad esempio nel discorso di Senator Josh Hawley, e come possano evadere la moderazione dei contenuti online.

Il progetto consiste in tre parti principali: la creazione di un glossario e di una tipologia di dog whistle, la valutazione della riconoscimento dei dog whistle nei modelli linguistici, in particolare GPT-3, e la verifica di come i dog whistle possano evadere la moderazione dei contenuti online. I risultati mostrano che i dog whistle possono essere riconosciuti dai modelli linguistici, ma solo in parte e con variabilità. Inoltre, la ricerca dimostra che i dog whistle possono evadere la moderazione dei contenuti online, rendendo più difficile la detezione della loro presenza.

Il progetto ha importanti implicazioni per la comprensione della retorica politica e della moderazione dei contenuti online, e può aiutare a sviluppare strategie per rilevare e contrastare la propaganda e l'odio online.</sample>
    <sample id="298">L'analisi dei risultati ha portato a una serie di esperimenti per valutare l'ipotesi della deriva temporale. In particolare, è stato condotto un esperimento di retraining o continuazione della pre-Allenamento di alcuni modelli con dati più recenti. I risultati hanno mostrato che la prestazione degrada con un maggior gap temporale, confermando l'ipotesi che la deriva temporale sia la causa principale della perdita di prestazioni.</sample>
    <sample id="299">Il lavoro presentato da Michalis Korakakis e Andreas Vlachos mira a migliorare la robustezza dei modelli di NLI (Natural Language Inference) mediante un approccio di training minimax. I modelli di NLI hanno ottenuto risultati eccellenti su diversi benchmark, ma recenti studi hanno dimostrato che la loro successo è in parte dovuto alla scoperta di scorciatoie (shortcuts) nella creazione dei dati di addestramento. Queste scorciatoie sono correlazioni spuri tra attributi di input e etichette che possono essere sfruttate dai modelli per migliorare le prestazioni in-distribution, ma che possono portare a risultati instabili quando si verificano test fuori-distribution.

L'obiettivo del lavoro è quello di ridurre la dipendenza dei modelli di NLI da scorciatoie e migliorare le loro prestazioni fuori-distribution. Per raggiungere questo obiettivo, gli autori propongono un metodo di training che utilizza una minimax training objective tra un modello principale (learner) e un modello ausiliario (auxiliary). Il modello ausiliario ha il compito di massimizzare la perdita del modello principale, incentivandolo a concentrarsi su esempi difficili che contraddicono le scorciatoie presenti negli esempi facili. Il metodo proposto non richiede conoscenze specifiche del dominio o del dataset e non assume che il modello principale utilizzi le stesse scorciatoie del modello ausiliario.

Gli autori valutano il metodo proposto su tre dataset analitici (MNLI, FEVER e QQP) e i relativi test fuori-distribution (HANS Symmetric e PAWS). I risultati mostrano che il metodo proposto migliora le prestazioni fuori-distribution mentre mantiene alta la precisione in-distribution. Inoltre, gli autori esaminano la trasferibilità delle prestazioni migliorate a modelli più grandi, scorciatoie sintetiche e test fuori-dominio.</sample>
    <sample id="300">L'articolo presenta un nuovo task chiamato "interactive dictation" che consiste nel creare un sistema di trascrizione vocale che permetta agli utenti di correggere i propri errori di parola in modo naturale e intuitivo. Questo task è caratterizzato da due principali funzionalità: l'intercalare in modo flessibile la trascrizione e l'edizione, senza l'utilizzo di parole d'ordine o comandi prefissati, e l'utilizzo di linguaggio naturale aperto per specificare le correzioni. 

Per implementare questo task, gli autori hanno formalizzato un procedimento a quattro passaggi: riconoscimento vocale, segmentazione delle trascrizioni in parti di trascrizione e comando, normalizzazione dei comandi e esecuzione delle trascrizioni e dei comandi. Per raccogliere dati, gli autori hanno progettato un'interfaccia di annotazione che consente agli utenti di riprodurre un documento e di correggere gli errori di trascrizione vocale.

Gli autori hanno anche sviluppato un sistema di base per questo task, che utilizza modelli di apprendimento automatico per eseguire ciascuno dei quattro passaggi. I risultati mostrano che i modelli GPT-3 sono più precisi ma anche più lenti, mentre i modelli T5 consentono di migliorare l'efficienza senza compromettere la precisione. Gli autori invitano altri a continuare a lavorare su questo task e hanno reso disponibile il codice per facilitare la ricerca futura.</sample>
    <sample id="302">I token non sono ordinati dopo la prima fase di etichettatura, quindi è necessario permutarli per ottenere la sequenza corretta di output.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché, nonostante i modelli siano stati addestrati per evitare stereotipi negativi, hanno scoperto che alcuni metodi di mitigazione potrebbero essere responsabili di stereotipi positivi pericolosi, come ad esempio l'archetipo della "Strong Black Women". Inoltre, non è chiaro se questi stereotipi positivi siano dovuti a un eccesso di valore di allineamento o a metodi di anti-stereotipizzazione non efficaci, e quindi è necessaria maggiore trasparenza per studiare ulteriormente questo problema.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le frasi create aggiungendo un prefisso inaccettabile a una frase originale inaccettabile, o aggiungendo un prefisso accettabile a una frase originale inaccettabile.</sample>
    <sample id="305">Il lavoro di Dawei e dei suoi colleghi, intitolato "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", esplora le limitazioni delle approcci di apprendimento supervisionato debole (WSL). I ricercatori hanno scoperto che le recenti metodologie di WSL richiedono dati di valutazione puliti per funzionare correttamente, altrimenti si verifica una significativa caduta di prestazioni. Inoltre, hanno trovato che aumentare il numero di campioni di valutazione puliti porta a prestazioni migliori, ma che potrebbe essere sufficiente utilizzare solo 20 campioni per classe per raggiungere prestazioni elevate.

I ricercatori hanno anche scoperto che l'approccio di fine-tuning diretto su dati puliti può superare le prestazioni delle metodologie di WSL, specialmente se si utilizzano solo 10 campioni per classe. Inoltre, hanno mostrato che la prestazione migliorata reclamata dalle precedenti metodologie di WSL può essere facilmente raggiunta permettendo la continuazione del fine-tuning sui dati di valutazione puliti.

I ricercatori hanno formulato alcune raccomandazioni per il futuro, tra cui:

* Rendere noti i criteri di selezione del modello
* Comporre le metodologie di WSL con basi di apprendimento a pochi esempi
* Considerare il fine-tuning continuo come una base semplice ma forte per le metodologie di WSL

Il lavoro di Dawei e dei suoi colleghi ha aperto la strada a una riconsiderazione delle metodologie di WSL e delle loro limitazioni, offrendo nuove prospettive per lo sviluppo di approcci più efficaci di apprendimento supervisionato debole.</sample>
    <sample id="306">Il lavoro presentato da Sebastian Schuster e Najoung Kim si concentra sull'abilità degli agenti linguistici di tracciare le entità e le loro condizioni all'interno di un discorso. L'obiettivo è valutare l'estensione con cui i modelli linguistici pre-allenati possono eseguire questa funzione. I ricercatori hanno identificato tre sfide nel progettare un compito di valutazione: le entità di stato possono essere comuni nel dati di pre-allenamento, i modelli possono imparare associazioni semplici tra parole e stati di entità e i modelli possono memorizzare sequenze di stato o imparare heuristici come il riempimento di slot.

Per superare queste sfide, i ricercatori hanno progettato un compito di valutazione che coinvolge la tracciatura degli stati di entità all'interno di scatole e oggetti. Il compito consiste nel completare una descrizione iniziale di contenuti di scatole con le informazioni relative agli stati di entità dopo aver applicato operazioni di stato. I risultati mostrano che i modelli Flan-T5 e GPT-3 e -3.5 non sono in grado di tracciare gli stati di entità, tranne il modello text-davinci-003, che esibisce un comportamento di tracciamento non banale.

I ricercatori hanno trovato che la pre-allenamento sul codice è responsabile della capacità di tracciamento degli stati di entità, e che i modelli più piccoli possono imparare a tracciare gli stati di entità se vengono direttamente fine-tunati. Tuttavia, rimane incerto se le abilità di tracciamento degli stati osservate generalizzino oltre lo scenario di valutazione.</sample>
    <sample id="307">Gli autori hanno utilizzato le seguenti metriche di valutazione:

- Named Entity Recognition (NER)
- Classificazione
- Part-of-speech tagging
- Question answering</sample>
    <sample id="308">Il lavoro presentato da Jenny, un dottorando presso la Carnegie Mellon University, esplora il concetto di "posizionalità" nei modelli di linguistica computazionale (NLP). La posizionalità si riferisce alle prospettive e alle esperienze che gli individui hanno a causa della loro demografia, identità e vita. I ricercatori hanno creato un framework chiamato NLPositionality per studiare la posizionalità dei dati e dei modelli NLP. Il framework consiste in due passaggi principali: la riannotazione dei dati con annotatori diversi e la comparazione delle annotazioni con i modelli e i dati esistenti.

Gli autori hanno condotto una ricerca con oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. I risultati hanno mostrato che i dati e i modelli NLP sono più allineati con le popolazioni di lingua inglese e con persone con un'istruzione universitaria. Tuttavia, alcuni gruppi sono stati lasciati indietro, come le persone non binarie.

Gli autori forniscono alcune raccomandazioni per affrontare la posizionalità nei modelli NLP, tra cui la registrazione delle scelte di progettazione, la conduzione di ricerche con la lente della perspectivismo e la creazione di dati e modelli specializzati per comunità specifiche. La ricerca ha importanti implicazioni per lo sviluppo di tecnologie NLP più inclusive e equilibrate.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Non sono state menzionate specifiche affiliazioni degli autori nell'articolo.</sample>
    <sample id="312">MultiInstruct è il primo dataset di istruzioni multi-modal pubblicamente disponibile, che consiste in 62 task diverse e copre 10 categorie ampie.</sample>
    <sample id="313">Non sono indicati i nomi degli autori dell'articolo, ma solo il nome del capo del laboratorio Emory NLP, ovvero Professor Jinho Choi.</sample>
    <sample id="314">La coordinazione binaria è un tipo di coordinazione in cui due elementi (congiunti) sono collegati da un congiunzione (ad esempio "e", "o", "ma") e sono trattati come pari, senza che uno dei due sia designato come capo della struttura coordinata.</sample>
    <sample id="315">Non è stato specificato il tempo esatto durante il quale sono stati utilizzati i prompt nel studio.</sample>
    <sample id="316">I risultati mostrano che il modello T5, fine-tunato su CoScript, può generare script di alta qualità, superando la maggior parte dei grandi modelli di linguaggio, dimostrando che i modelli più piccoli possono raggiungere prestazioni eccellenti quando addestrati su dataset adeguati.</sample>
    <sample id="317">Il team di ricerca di Fudan University, guidato da Peng Li, ha presentato un nuovo approccio per l'estrazione di informazioni, chiamato CodeIE, che utilizza modelli di linguaggio per la generazione di codice per risolvere il problema di mismatch tra input e output. I modelli di linguaggio tradizionali, come T5 e GPT-3, operano in modo testo-testo durante la fase di pre-allenamento, ma richiedono una grande quantità di dati strutturati e strategie di decoding speciali per generare output strutturati. CodeIE trasforma l'estrazione di informazioni in un compito di generazione di codice strutturato, utilizzando modelli di linguaggio per la generazione di codice come Codex. Il team ha valutato CodeIE su tre dataset di riconoscimento e quattro dataset di estrazione di relazioni, confrontandolo con modelli di linguaggio tradizionali come UIE e GPT-3. I risultati mostrano che CodeIE supera i modelli di linguaggio tradizionali in termini di prestazioni, con una precisione e un recall più alti. L'analisi dettagliata ha rivelato che la trasformazione dell'estrazione di informazioni in un compito di generazione di codice strutturato aiuta a migliorare la precisione e la recall. Inoltre, il modello Codex ha mostrato di essere più efficace del modello GPT-3 in termini di estrazione di informazioni.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e presenterò i nostri lavori su "DrBERT: Un Modello Robusto Pre-allenato in Francese per Domini Biomedici e Clinici". 

In questa presentazione, parleremo inizialmente del linguaggio di modellazione in sanità. 

Il nostro contributo principale è la presentazione del primo modello biomedico in francese chiamato DrBERT, che si basa su RoBERTa e è stato allenato su NACHOS, un set di dati di dati medici raccolti dal web. 

Inoltre, presenteremo una comparazione di modelli con diversi setting di allenamento e fonti di dati. 

Poi, presenteremo i nostri risultati su 11 compiti downstream biomedici e clinici in francese. 

Infine, concluderemo sugli esperimenti e forniremo maggiori dettagli su come accedere a questi modelli. 

Dal suo rilascio nel 2018, BERT è diventato una delle soluzioni più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre notevoli guadagni di prestazioni rispetto ai metodi storici e contestuali come Word2vec, fastText o altri. 

Da allora, questo modello è stato adattato a molti altri linguaggi, come il francese con CamemBERT, e anche in domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. 

I modelli specializzati per altri linguaggi sono rari e spesso si basano su pre-allenamento continuo a causa della mancanza di dati in dominio. 

Tuttavia, la Francia non aveva alcun modello open source per biomedici fino ad ora. 

Quindi ci siamo chiesti cosa sia la fonte di dati più appropriata per un uso ampio e quali dati raccolti siano una buona sostituzione per i dati clinici. 

Per rispondere a questa domanda, abbiamo confrontato DrBERT con il nostro modello ChuBERT, che si basa su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes. 

In seguito, ci siamo chiesti quanta quantità di dati occorra per allenare un modello specializzato sui dati francesi. È di 4 GB, 8 GB o più? 

Per rispondere a questa domanda, abbiamo allenato e confrontato quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB del set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prese dai registri clinici; e una versione finale di ChuBERT con un mix di 4 GB del set di NACHOS e 4 GB di registri clinici. 

Inoltre, abbiamo introdotto tre modelli allenati su pre-allenamento continuo per analizzare l'impatto della strategia di pre-allenamento. 

Uno basato sul peso di CamemBERT e allenato su un set di 4 GB di NACHOS. 

Un altro basato su CamemBERT, ma allenato questa volta sul 4 GB di registri clinici e infine, uno basato sul modello biomedico inglese PubMedBERT, e allenato su 4 GB del set di NACHOS. 

In totale, abbiamo sette modelli. 

Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti downstream pubblici e privati come riconoscimento di entità nominative, classificazione, etichettatura di parti del discorso e risposta alle domande. 

Questi modelli sono stati confrontati con sei modelli di riferimento, ovvero CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. 

L'evaluazione mette in luce che i modelli hanno ottenuto i risultati migliori con compiti di dati della stessa natura di quelli su cui il modello è stato allenato. 

Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee sembrano essere più versatili. 

Possiamo anche osservare che l'uso di più dati si traduce in prestazioni migliori. 

In generale, il pre-allenamento da zero sembra ottenere prestazioni migliori su la maggior parte dei compiti. 

Tuttavia, il nostro esperimento di controllo pre-allenamento utilizzando i pesi e la tokenizzazione di CamemBERT allenati sul subset di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. 

Ciò non è accaduto per il modello basato sui pesi e sul tokenizzatore di CamemBERT, che ha subito problemi di stabilità. 

Infine, come conclusione, il nostro sistema appropriato ha ottenuto prestazioni migliori su nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, ovvero CamemBERT. 

Possiamo anche osservare che i dati specializzati sono meglio, ma non scalano bene. 

Tutti i modelli pre-allenati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face e sono coperti dalla licenza MIT, e tutti i script di allenamento sono disponibili nel nostro repository GitHub. 

Grazie per questa presentazione, e ci aspettiamo di scambiare informazioni alla sessione dei poster a Toronto.</sample>
    <sample id="319">Il lavoro esamina le seguenti strategie di apprendimento:

1. Pre-training da zero (from-scratch) con dati in dominio (NACHOS e note cliniche)
2. Pre-training continuo utilizzando i pesi e la tokenizzazione di CamemBERT come base
3. Pre-training continuo utilizzando PubMedBERT come base</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è pari a 1, come indicato dalla pendenza della retta di best fit (1) nel grafico a destra.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i tipi di semplificazione, come ad esempio la semplificazione lessicale, la semplificazione della struttura e il livello di semplificazione complessivo. Inoltre, è stata analizzata la varietà delle trasformazioni di semplificazione presenti nel corpus.</sample>
    <sample id="322">Enrico presenterà un contributo al convegno ACL 23 sul tema "Cosa impara un classificatore di testo sulla moralità?". La moralità è un concetto complesso che aiuta gli esseri umani a distinguere il giusto dal torto, e la sua comprensione è essenziale per le società. Tuttavia, la moralità è soggettiva e può essere interpretata in modi diversi. Il classificatore di testo può imparare a comprendere la moralità in testo, ma è importante che non si limiti a una scala unica tra morale e immorale.

Enrico propone l'uso della Teoria delle Fondamenta Morali, che sostiene che ci sono cinque modi diversi in cui gli esseri umani percepiscono la moralità, ognuno dei quali è prioritizzato in modo diverso da ogni persona. Il contributo di Enrico esplora come i classificatori di testo imparano a comprendere la moralità in testo, utilizzando tecniche di AI esplicabile su un dataset di 35.000 tweet raccolti in sette domini diversi. I risultati mostrano che i classificatori di testo possono riconoscere che la moralità è espressa in modo diverso in diversi domini, come ad esempio tra #AllLivesMatter e #BlackLivesMatter.

Il contributo di Enrico avverte che l'uso di un unico modello per molti domini diversi può portare a misunderstanding della moralità, e che è importante comprendere come la moralità è espressa in modo diverso in diversi contesti. Enrico spera di presentare i risultati del suo contributo al convegno ACL in Toronto.</sample>
    <sample id="323">Il titolo del tuo lavoro, "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA", si concentra sulla risoluzione delle domande di conoscenza comune che richiedono l'accesso a conoscenze esterne. Recentemente, è stato proposto che la conoscenza sia memorizzata sia nei modelli linguistici che nelle basi di conoscenza. Tuttavia, le combinazioni di queste due fonti di conoscenza presentano alcuni limiti, come la presenza di entità rumorose e l'incapacità di interagire tra le due modalità.

Per superare questi limiti, il tuo lavoro propone un approccio chiamato DHLK, che si compone di tre fasi principali. Inizialmente, si costruisce un grafo eterogeneo (HKG) basato su più basi di conoscenza, che viene ottimizzato tramite una strategia di riduzione a due stadi e l'apprendimento della rappresentazione della conoscenza (KRL). Successivamente, si utilizza un modello linguistico per codificare e fondere le due modalità. Infine, si utilizza un modello di Mask Self-Attention per modellare i subgrafi e si aggiorna le rappresentazioni degli entità e delle relazioni del HKG.

Il tuo lavoro si concentra sulla risoluzione delle domande di conoscenza comune, utilizzando un approccio dinamico e eterogeneo per integrare conoscenze esterne. Il metodo proposto, chiamato DHLK, sembra offrire risultati promettenti rispetto ad altri metodi basati su modelli linguistici e grafi eterogenei.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. I risultati preliminari mostrano che i modelli linguistici occupano tutti e quattro i quadranti del "politic campus" e che GPT-4 è il modello linguistico più liberale di tutti, mentre i modelli GPT sono generalmente più socialmente liberali rispetto ai modelli BART e alle sue varianti.</sample>
    <sample id="325">Ciao! Il tuo nome è Matthias Lindemann e oggi ti presenterai brevemente al nostro lavoro intitolato "Compositional Generalization senza alberi utilizzando Multiset Tagging e Permutazioni Latenti". Questo è un lavoro congiunto con i tuoi advisor Alexander Koller e Ivan Titov. La composizione generale può essere intesa come l'abilità di un apprendente di gestire la recursione più profonda e composizioni non viste di frasi che sono state viste individualmente durante l'allenamento. Nel contesto della parsing semantico, testare la composizione generale potrebbe essere fatto in questo modo. Come di solito, abbiamo un insieme di allenamento di enunciati. In questo caso, "La ragazza dormiva." e "Mary sapeva che la ragazza dormiva." Questi enunciati sono associati a forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza dell'evaluazione standard di apprendimento automatico, l'insieme di test non proviene dalla stessa distribuzione, ma contiene forme logiche non viste strutturalmente. In questo esempio, il modello ha visto la recursione superficiale durante l'allenamento e viene testato su un esempio con recursione più profonda. I modelli seq2seq ingenui hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono scissi dall'input. In particolare, spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo è integrare gli alberi nei modelli. Gli alberi sono destinati a catturare il processo compositivo che collega gli enunciati con le forme logiche. Funziona bene, ma gli alberi sono solitamente non dati e devono essere ottenuti in qualche modo. Ciò può essere complesso e a volte un processo computazionalmente costoso. Di solito, ciò comporta un pre-elaborazione formale-specifica degli schemi logici, ad esempio per gestire i simboli di variabili. Ottenere gli alberi può anche comportare procedure di induzione grammaticale specializzate. In questo lavoro, non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, dimostriamo una generalizzazione forte alla recursione più profonda senza dipendere dagli alberi. Il nostro approccio predice l'output dall'input in due passaggi. In primo luogo, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. È per questo che nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone alcuna restrizione rigida sulle possibili permutazioni. Ciò rende il nostro approccio molto flessibile ed espressivo. Concezionalmente, il nostro modello di permutazione funziona in modo simile. Andiamo da sinistra a destra sull'output e determiniamo quale token del multiset mettere in ogni posizione. Per la prima posizione dell'output, semplicemente selezioniamo uno, come evidenziato in rosso. Poi saltiamo al prossimo token del multiset per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multiset. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per dare un assaggio dei risultati sperimentali, ecco una comparazione del nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri per una grande marcia nella generalizzazione alla recursione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto sfidanti, però. Nel nostro lavoro, risolviamo alcuni interessanti problemi tecnici. In primo luogo, l'allineamento tra input e output non è dato nel set di allenamento. Come conseguenza, per un token non sappiamo quale multiset è stato utilizzato, il che pone un problema per l'allenamento. Inoltre, a volte ci sono più permutazioni che sono consistenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'allenamento. Il nostro metodo di permutazione è molto flessibile, ma porta il problema di trovare la permutazione più alta-punteggio, che è NP-dura. Ciò è perché si tratta del "Problema del Venditore Itinerante". Abbiamo approssimato questo con una rilassazione continua GPU-frienly che consente anche di propagare il backprop attraverso la soluzione e imparare le permutazioni più plausibili linguisticamente. Se desideri imparare di più sui nostri esperimenti e come affrontiamo questi problemi, per favore guarda il nostro lavoro o visita il nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è la condizione in cui due credenze o azioni sono inconsistenti tra loro, come ad esempio una persona che sostiene di sapere che i fumare può essere letale, ma poi continua a fumare, giustificando la propria azione con motivi come la necessità di mantenere il proprio lavoro.</sample>
    <sample id="327">Il lavoro presentato da Xiao Xu, un dottorando del Harbin Institute of Technology, si concentra sulla Vision-Language Representation Learning, un campo di ricerca che mira a sviluppare sistemi di intelligenza artificiale in grado di comprendere sia immagini che testi. La presentazione descrive il modello di rete neuronale ManagerTower, un'architettura innovativa che si basa sulla due-torre, ma introduce una nuova tecnica chiamata "manager" per aggregare le informazioni provenienti da esperti unimodali (testuali o visivi) a diversi livelli di rappresentazione.

Il modello ManagerTower utilizza due tipi di encoder: RoBERTa e CLIP-ViT base, e introduce i "manager" in ogni livello di cross-modalità per combinare le informazioni provenienti da esperti unimodali a diversi livelli. I manager possono adattarsi per sfruttare differenti livelli di conoscenza semantica unimodale, facilitando una maggiore allineamento e fusione cross-modale.

La presentazione mostra che ManagerTower supera le prestazioni di altri modelli, tra cui METER e BridgeTower, anche quando si utilizza un piccolo set di dati (4 milioni di immagini). I risultati mostrano che ManagerTower può sfruttare efficacemente differenti livelli di conoscenza semantica unimodale, migliorando le prestazioni sulle varie attività di fine-allestimento. La presentazione conclude con la disponibilità del codice e dei modelli su GitHub e Archive, e la speranza che il lavoro possa essere utile alla comunità di ricerca.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale di tutti.</sample>
    <sample id="329">Il lavoro presentato da Minghang Zheng e collaboratori si concentra sulla localizzazione di frasi in video senza etichette (zero-shot video sentence localization). Questa task consiste nel trovare i segmenti di video più rilevanti per una query di linguaggio naturale in video lunghi, con applicazioni in retrieval di video, riassunti e altri campi. Tuttavia, molti metodi esistenti richiedono un gran numero di etichettature manuali per il training, che è costoso e inefficiente.

Il team propone un metodo di generazione di pseudo-etichette strutturate per superare questi limiti. Il metodo consiste in tre passaggi: (1) generazione di pseudo-domande basate su video tramite un modello di caption pre-allineato, (2) generazione di pseudo-eventi basati sulla struttura temporale degli eventi, e (3) riduzione dell'influenza del rumore nelle pseudo-etichette tramite pesatura dei campioni e rifinizione delle etichette.

Il metodo proposto supera i limiti dei metodi esistenti, come la semplicità delle pseudo-domande e la mancanza di garanzia di irrilevanza tra query e video esterni agli eventi. Inoltre, il metodo riduce l'influenza del rumore nelle pseudo-etichette tramite pesatura dei campioni e rifinizione delle etichette.

Il team ha condotto esperimenti su due dataset, ActivityNet Captions e Charades-STA, e ha ottenuto risultati migliori rispetto ai metodi esistenti su quasi tutti i metrici di valutazione. Il codice del metodo è disponibile su richiesta.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo, poiché "Cumulative" accumula tutti i dati raccolti fino a quel momento e produce risultati migliori o equivalenti a quelli di "Iterative" in tutti i casi.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati utilizzati per il parametro di riferimento MuDa sono stati tratti dai transcripts di TED talks tradotti da inglese in 14 diverse lingue.</sample>
    <sample id="333">Il team di ricerca guidato da Wenhao da Nanjing University ha proposto un nuovo framework per migliorare la rappresentazione dello spazio di traduzione automatica con reti neurali (NMT). Il framework, chiamato INK, si concentra sulla gestione delle conoscenze vicine (kNN) per raffinare la rappresentazione dello spazio di traduzione. La proposta di INK consiste nell'injectare la conoscenza kNN nel modello NMT tramite un adattatore, che consente di migliorare la rappresentazione dello spazio di traduzione. L'adattatore è progettato per essere piccolo e leggero, quindi può essere facilmente integrato nel modello NMT esistente.

Il team di ricerca ha condotto esperimenti su un dataset di traduzione news tedesco-inglese e ha ottenuto risultati promettenti. I risultati mostrano che l'uso dell'adattatore INK porta a un miglioramento significativo della rappresentazione dello spazio di traduzione, con un aumento del 1,99 di punteggio COMET e del 1,0 di punteggio BLEU rispetto ai sistemi kNN-MT esistenti. Inoltre, l'uso dell'adattatore INK porta a una riduzione della memoria necessaria e a un aumento della velocità di inferenza.

Il team di ricerca ha anche esplorato tre questioni di ricerca principali:

1. Se è possibile migliorare la rappresentazione dello spazio di traduzione con un adattatore piccolo e leggero.
2. Quanto miglioramento può essere portato dall'uso della conoscenza kNN per raffinare la rappresentazione dello spazio di traduzione.
3. Se l'uso dell'adattatore e del datastore insieme porta a ulteriori miglioramenti.

I risultati mostrano che l'uso dell'adattatore INK porta a un miglioramento significativo della rappresentazione dello spazio di traduzione e che l'uso dell'adattatore e del datastore insieme porta a ulteriori miglioramenti. Il team di ricerca conclude che il framework INK rappresenta un passo avanti significativo nella gestione delle conoscenze vicine per migliorare la rappresentazione dello spazio di traduzione.</sample>
    <sample id="335">Alexander Koller e Ivan Titov.</sample>
    <sample id="336">Il trasferimento interlinguistico (Cross-lingual Zero-shot transfer) è il processo di addestrare un modello linguistico su una lingua di origine e poi applicarlo per tradurre e interpretare query in altre lingue senza necessità di addestramento supplementare.</sample>
    <sample id="337">Il lavoro presentato, intitolato "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning", si concentra sulla rappresentazione delle parole fuori vocabolario (OOV) utilizzando un approccio basato su reti grafiche. L'obiettivo è sviluppare un modello che possa imparare le relazioni tra parole e utilizzarle per inferire il significato delle OOV. Il modello proposto utilizza una rete di wordpiece, che rappresenta le parole come insieme di sottounità e relazioni tra di esse. Per affrontare il problema delle OOV, il modello utilizza una rete di self-attenzione per assegnare attributi alle parole OOV e una rete di Graph Attention Network per estrarre informazioni rilevanti. Il modello è stato valutato in diverse attività intrinseche ed estrinseche e ha dimostrato di essere superiore alle basi. Il modello può essere applicato anche a modelli statici e contestuali per migliorarne le prestazioni. L'autore discute anche la possibilità di estendere il modello a diverse lingue, evidenziando che le lingue agglutinative sono più adatte alla rappresentazione delle parole utilizzata dal modello. In sintesi, il lavoro presenta un approccio innovativo per la rappresentazione delle parole fuori vocabolario utilizzando reti grafiche e mostra buone prestazioni in diverse attività.</sample>
    <sample id="338">Il lavoro presentato da Bingsheng si concentra sull'evaluazione della qualità delle spiegazioni naturali fornite dagli esseri umani ai modelli di linguaggio. I ricercatori hanno identificato il problema di come valutare la qualità di queste spiegazioni, che possono essere soggettive e dipendenti dal compito. La simulatability score, una metrica comunemente utilizzata, non considera le differenze di compito e la differenza di utilità delle spiegazioni durante le fasi di fine-tuning e inferenza.

Per risolvere questo problema, gli autori hanno proposto una struttura dati unificata basata su template, che converte diversi compiti in un compito di scelta multipla unificato. Questa struttura include un setting di base senza spiegazione e un setting di infusione in cui la spiegazione serve come input aggiuntivo ai modelli sequenza-sequenza.

Gli autori hanno condotto esperimenti per analizzare l'utilità delle spiegazioni e hanno proposto una nuova metrica chiamata TREU, che estende la simulatability score. La TREU score valuta la bontà delle spiegazioni non solo durante l'inferenza, ma anche durante il fine-tuning.

Gli esperimenti condotti sugli autori hanno dimostrato che la TREU score può riflettere meglio l'utilità delle spiegazioni rispetto alla simulatability score. Inoltre, gli autori hanno osservato che la bontà delle spiegazioni può variare a seconda del compito e della forma delle spiegazioni.</sample>
    <sample id="339">Gli autori dell'articolo sono affiliati alla Saarland University in Germania.</sample>
    <sample id="340">Il lavoro presentato da Kuan-Hao Huang e collaboratori, intitolato "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", si concentra sulla creazione di un grande dataset di parafrasi sintatticamente diverso. Il problema attuale è che i dataset di parafrasi esistenti, come MRPC, PAN e Quora, hanno una buona qualità ma sono limitati nella scala. Al contrario, i dataset generati automaticamente, come la traduzione inversa, possono produrre un gran numero di parafrasi, ma mancano di diversità sintattica.

Per risolvere questo problema, i ricercatori propongono l'utilizzo delle rappresentazioni semantiche astratte (AMR) per generare parafrasi sintatticamente diverse. L'idea chiave è di utilizzare la traduzione inversa di AMR per creare un grande dataset di parafrasi. Il processo consiste nel generare un grafo AMR per una frase di input, modificare il focus del grafo e utilizzare un generatore di testo da grafo AMR per generare una parafrasi.

Il dataset proposto, chiamato ParaAMR, contiene circa 15 milioni di frasi di input e 6,9 parafrasi per frase. I risultati mostrano che ParaAMR è più sintatticamente diverso rispetto ai dataset esistenti, mentre mantiene una buona similitudine semantica. I ricercatori dimostrano che ParaAMR può beneficiare diverse applicazioni NLP, come l'apprendimento di embedding di frasi, la generazione di parafrasi con controllo sintattico e l'agumentazione di dati per l'apprendimento a pochi esempi. Il dataset è disponibile online e può essere utilizzato da altri ricercatori per sviluppare nuove applicazioni NLP.</sample>
    <sample id="341">Gli autori fanno ricorso a due misure di latenza: BLEU (che misura la qualità di traduzione) e average lagging (che misura la latenza) e, inoltre, considerano anche il computational-aware average lagging (che tiene conto del tempo di elaborazione del modello).</sample>
    <sample id="342">Il presente studio, intitolato "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming", presenta un nuovo dataset di conversazioni personalizzate e video-sourced, chiamato LiveChat, costruito automaticamente da video di streaming cinesi. Il dataset è stato creato attraverso tre passaggi: estrazione di audio e transcrizione di video, costruzione di dialoghi tramite un metodo di matching risposte e raccolta di informazioni di persona per la generazione di dialoghi personalizzati.

Il LiveChat è stato confrontato con altri dataset esistenti di dialoghi aperti e ha mostrato una maggiore scala e una struttura di dialogo più complessa. Sono stati condotti esperimenti su due task di valutazione: il modello di risposta e la riconoscimento dell'interlocutore, con risultati che confermano l'importanza delle informazioni di persona e delle sessioni di dialogo più lunghe.

Inoltre, è stata valutata la performance di modelli di linguaggio pre-allenati (LLMs) su LiveChat, con risultati che mostrano la distinzione del dominio di LiveChat rispetto agli altri dataset esistenti. Gli esperimenti di apprendimento in contesto hanno mostrato che l'aumento del numero di esempi di demo può migliorare la performance dei LLMs, ma che un numero eccessivo di esempi può introdurre rumore.

In sintesi, il presente studio presenta un nuovo dataset di conversazioni personalizzate e video-sourced, LiveChat, e conferma l'importanza delle informazioni di persona e delle sessioni di dialogo più lunghe per la generazione di dialoghi personalizzati.</sample>
    <sample id="343">Ciao a tutti, mi chiamo Akshatha e oggi, insieme al mio coautore Martin, presenteremo il nostro lavoro "The KITMUS Test: valutazione dell'integrazione delle conoscenze da diverse fonti". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a diverse fonti di conoscenza, come quella contenuta nei loro parametri, solitamente acquisita durante il preaddestramento, e quella fornita negli input al tempo dell'inferenza. Gli ultimi lavori in compiti come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il preaddestramento per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze che sono anche fornite al tempo dell'inferenza. Ad esempio, nella frase "John vide il presidente eletto di recente in TV", i parametri preaddestrati possono contenere informazioni su cosa fanno i presidenti e cosa è una TV, ma non possono essere in grado di conoscere in modo affidabile chi è l'entità specifica "John" o chi è il presidente nuovo, perché il presidente potrebbe essere cambiato dal momento del preaddestramento. Quindi, i modelli di successo per compiti di comprensione del linguaggio naturale intensivi nella conoscenza richiedono la capacità di integrare e utilizzare entrambe la conoscenza acquisita durante il preaddestramento e quella fornita al tempo dell'inferenza. In questo lavoro proponiamo un set di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di trarre conoscenza da diverse fonti. Valutiamo il set di dati con partecipanti umani e modelli di risoluzione della coreferenza stabiliti. Ecco un esempio dal nostro set di dati. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, era felice di rilassarsi. Il compito qui è quello di identificare la corretta entità a cui il pronome "egli" si riferisce, che in questo caso è Servin. La risoluzione di un pronome dato richiede due tipi di informazioni. In primo luogo, la conoscenza specifica dell'entità come "Servin è un giudice." E in secondo luogo, la conoscenza di fondo come "I giudici decidono casi in tribunali". In generale, la conoscenza di fondo è appresa durante il preaddestramento dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al tempo dell'inferenza. Abbiamo variato la disponibilità di queste due informazioni in modo che possa essere trovata in una sola fonte o in diverse fonti. Abbiamo definito tre impostazioni di KITMUS. In primo luogo, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di fondo è stata assunta essere disponibile al tempo del preaddestramento. In secondo luogo, c'è un'impostazione "Background-Both", dove la conoscenza di fondo è disponibile sia al tempo del preaddestramento che dell'inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al tempo dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non fa parte dei dati di preaddestramento dei modelli. Ad esempio, perché nuove occupazioni sono state sviluppate dal momento del preaddestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di fondo "I politici cercano seggi eletti nel governo" sia contenuta nei parametri preaddestrati e nel contesto dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico." Nell'impostazione Background-Both forniamo non solo la conoscenza specifica dell'entità, ma anche la conoscenza di fondo sui politici nel contesto dell'inferenza. Nell'impostazione Background-Inference forniamo la professione fittizia "mirituer" al posto di politico, perché "mirituer" è improbabile di essere contenuto nei parametri preaddestrati. Valutiamo il set di dati sia con partecipanti umani, sia con modelli di risoluzione della coreferenza stabiliti. In questa figura, mostriamo i risultati dei modelli migliori che hanno ottenuto il punteggio più alto nella variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico per il compito KITMUS, entrambi i modelli non hanno un buon risultato. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref hanno ottenuto risultati significativamente migliori rispetto alla scelta casuale. Ciò suggerisce che quando addestrati su set di dati di riferimento generici, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove questi indizi sono stati rimossi. Altre esperimenti con conoscenza fittizia indicano che anche i migliori modelli non sono in grado di integrare in modo affidabile la conoscenza di fondo fornita solo al tempo dell'inferenza. Per riassumere i principali risultati del nostro lavoro, molti modelli di risoluzione della coreferenza sembrano non essere in grado di ragionare sulla conoscenza da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli riescono a integrare conoscenza da diverse fonti. Tuttavia, anche i migliori modelli sembrano avere difficoltà a integrare in modo affidabile la conoscenza di fondo fornita solo al tempo dell'inferenza. Se siete interessati a ulteriori dettagli, per favore vedete il nostro articolo e controllate il set di dati e il codice su GitHub. Grazie per aver ascoltato.</sample>
    <sample id="344">I metodi basati su alberi hanno alcuni svantaggi, tra cui:

* richiedono la creazione di alberi che possono essere complicati e computacionalmente costosi da ottenere;
* richiedono un pre-elaborazione formale specifica delle forme logiche;
* possono richiedere procedure di induzione di grammatica specializzate.</sample>
    <sample id="345">Il lavoro presentato da Matthias Lindemann e collaboratori si concentra sulla composizione generale senza l'utilizzo di alberi, affrontando il problema della generalizzazione ai livelli di recursione più profondi in campo di parsing semantico. La composizione generale si riferisce alla capacità di un modello di apprendimento di gestire la composizione di frasi non viste durante l'allenamento. Il team propone un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti di input e output senza l'ausilio di alberi. Il modello si compone di due fasi: la prima consiste nell'etichettare ogni token di input con un insieme multiset di token che appariranno nell'output, mentre la seconda fase utilizza un altro modello per prevedere la permutazione dei token per ottenere l'output corretto.

Il team affronta due sfide tecniche principali: l'allineamento tra input e output non è disponibile nel training dati, e la ricerca della permutazione più alta è NP-dura. Per superare queste sfide, il team induce l'allineamento come parte del training e utilizza una rilassazione continua per approssimare la ricerca della permutazione più alta. I risultati sperimentali mostrano che il modello proposto supera i modelli senza alberi esistenti nella valutazione di generalizzazione ai livelli di recursione più profondi.</sample>
    <sample id="346">Non sono presenti informazioni sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="347">Ciao, sono Myra e oggi parleremo del nostro articolo "Personaggi segnati: l'uso di promemoria di linguaggio naturale per misurare i stereotipi nei modelli di linguaggio". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la diffusione di bias e stereotipi sociali nei grandi modelli di linguaggio, o LLM. Tuttavia, questi metodi hanno diverse limitazioni. Di solito si basano su set di dati costruiti a mano che sono molto impegnativi da curare e si concentrano solo su stereotipi specifici, quindi non si generalizzano bene ad altre demografie o contesti o semplicemente catturano associazioni molto generali e ampie, come le associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, ovvero la nozione che le identità sociali multifacetiche possono accumulare bias e essere luoghi unici di danno.

Per superare queste limitazioni, ci basiamo sulla proprietà dei nuovi LLM di essere molto bravi a rispondere a istruzioni e promemoria. Quindi possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginato utilizzando un promemoria come "Immagina di essere una donna asiatica. Descrivi te stessa." E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marchio di identità che vogliamo in questo promemoria.

Ecco alcuni esempi di generazioni da GPT-4. Subito si vedono alcuni interessanti pattern. La donna asiatica è descritta come non asserente; la donna del Medio Oriente viene descritta utilizzando parole come esotica e come, facendo riferimento a una regione ipnotica. E entrambi i personaggi delle donne di colore fanno riferimento all'ascendenza, mentre il personaggio del uomo bianco non fa nulla del genere.

Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è la generazione di questi personaggi. I promemoria per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi promemoria a soggetti umani, trovando che anche loro fossero in grado di portare alla luce i stereotipi razziali. E questo consente una comparazione diretta tra i personaggi generati e le risposte scritte dagli umani.

La seconda parte è le parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, che spiegherò più avanti. Il vantaggio di questo è che otteniamo stereotipi molto specifici e pattern, senza dover dipendere da alcun lessico specifico.

Il metodo delle parole segnate si basa sul concetto sociolinguistico della "marcatura", che afferma che c'è un default non segnato e qualsiasi gruppo che differisce da quel default è linguisticamente segnato. Quindi per esempio la parola "guerriero" è solitamente associata agli uomini. Quindi quando le persone descrivono un guerriero che è una donna, solitamente specificano "guerriero donna" e segnano la parola con "donna". E più in generale, i gruppi dominanti nella società sono sia linguistici che sociali non segnati, mentre i gruppi marginalizzati sono solitamente segnati.

Quindi nel nostro metodo, designiamo inizialmente cosa sono i gruppi non segnati e segnati, e poi confrontiamo i personaggi utilizzando il metodo Fightin' Words, che è sostanzialmente utilizzare rapporti log-odds pesati per distinguere le parole più importanti per ogni gruppo segnato. Quindi per esempio per i personaggi delle donne nere, faremmo Fightin' Words e confronteremmo i rapporti log-odds con entrambi i personaggi dei bianchi e dei uomini perché sono i due gruppi non segnati corrispondenti.

Ecco alcuni risultati. Quindi utilizziamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti dagli umani. Tuttavia, quando guardiamo la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre i personaggi generati hanno un tasso molto più alto di parole del lessico, i personaggi scritti dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipiche dei personaggi generati sono davvero solo le parole "alto" e "atletico". Quindi davvero solo le parole positive o almeno non negative. E in effetti, questo lessico non cattura molti dei pattern dannosi che abbiamo visto nelle slide precedenti affatto bene.

Quindi per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole segnate per mostrare come queste parole positive sembrano facilitare stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni positive sembrano riflettere pattern dannosi. Innanzitutto dai nostri gruppi, le parole più importanti includono cose come "cultura", "tradizione", "proud", e "esotico". E queste parole definiscono questi gruppi solo in base alla loro relazione con la loro identità e li distinguono come diversi dal normale bianco. Ciò contribuisce a una lunga eredità di discriminazione e altri per questi gruppi.

Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore. Quindi per esempio le parole che descrivono le donne latine includono cose come "vibrante" e "curvacea" che si connettono a un tropo del tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata" e "silkata" che si connettono a una lunga storia di donne asiatiche essere iper-sessualizzate, viste come molto docili e sottomesse, ecc.

E infine, per le donne nere, vediamo che alcune delle parole più importanti sono cose come "forte" e "resiliente". Questo si connette a un archetipo che le persone hanno chiamato l'"archetipo della donna nera forte". E sebbene sembri positivo a prima vista, ci sono stati lavori che mostrano che questo tipo di archetipo è veramente dannoso perché mette una grande pressione su queste demografie per essere forti e resilienti contro gli ostacoli sociali. Quindi invece di lavorare per cambiare questi ostacoli, mette la pressione su quelle persone per superarli, il che porta a esiti negativi per la salute di queste persone, tra gli altri danni.

Più in generale, troviamo che le parole per ogni gruppo segnato riflettono sostanzialmente narrazioni essenzializzanti.

Quindi sulla base di questi pattern, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, dobbiamo, come ricercatori, affrontare i stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare i bias e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei bias, perché per esempio, come questi stereotipi positivi, non sappiamo se ci sia una sorta di allineamento valoriale eccessivo o forse altri metodi anti-stereotipici che portano a questi pattern dannosi. Non possiamo fare alcuna assunzione o studiare ulteriormente senza più trasparenza.

Grazie mille per aver ascoltato. Buona giornata a ACL.

Traduzione:

Ciao, sono Myra e oggi parleremo del nostro articolo "Personaggi segnati: l'uso di promemoria di linguaggio naturale per misurare i stereotipi nei modelli di linguaggio". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky.

Negli ultimi anni, molti hanno documentato la diffusione di bias e stereotipi sociali nei grandi modelli di linguaggio, o LLM. Tuttavia, questi metodi hanno diverse limitazioni.

Per superare queste limitazioni, ci basiamo sulla proprietà dei nuovi LLM di essere molto bravi a rispondere a istruzioni e promemoria.

Ecco alcuni esempi di generazioni da GPT-4. Subito si vedono alcuni interessanti pattern.

Per catturare questi pattern, il nostro metodo ha due parti.

La prima parte è la generazione di questi personaggi. I promemoria per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi promemoria a soggetti umani.

La seconda parte è le parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati.

Il metodo delle parole segnate si basa sul concetto sociolinguistico della "marcatura".

Quindi nel nostro metodo, designiamo inizialmente cosa sono i gruppi non segnati e segnati, e poi confrontiamo i personaggi utilizzando il metodo Fightin' Words.

Ecco alcuni risultati. Quindi utilizziamo un lessico di stereotipi e troviamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti dagli umani.

Tuttavia, quando guardiamo la distribuzione delle parole e del lessico, troviamo cose molto diverse.

Quindi per farlo, ci rivolgeremo ai risultati del nostro metodo delle parole segnate per mostrare come queste parole positive sembrano facilitare stereotipi e narrazioni essenzializzanti.

Nella nostra analisi, riveliamo come queste rappresentazioni positive sembrano riflettere pattern dannosi.

Inoltre, ci sono molti tropi comuni riflessi in queste parole, soprattutto per le donne di colore.

E infine, per le donne nere, vediamo che alcune delle parole più importanti sono cose come "forte" e "resiliente".

Più in generale, troviamo che le parole per ogni gruppo segnato riflettono sostanzialmente narrazioni essenzializzanti.

Quindi sulla base di questi pattern, concludiamo con tre raccomandazioni per i proprietari dei modelli.

Innanzitutto, dobbiamo, come ricercatori, affrontare i stereotipi positivi e le narrazioni essenzializzanti.

Dovremmo anche utilizzare una lente intersezionale per studiare i bias e i danni.

E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei bias.

Grazie mille per aver ascoltato. Buona giornata a ACL.</sample>
    <sample id="348">Il lavoro "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" esplora la presenza di stereotipi e pregiudizi nei modelli di linguaggio (LLMs). I ricercatori, guidati da Myra, hanno identificato limitazioni nelle misure attuali, che spesso dipendono da set di dati costruiti a mano, non generalizzabili e che non considerano l'intersezionalità. Per superare questi limiti, gli autori hanno utilizzato la capacità degli LLM di rispondere a istruzioni e promemoria per creare "personae" immaginarie.

Il metodo "Marked Words" identifica le parole che distinguono i gruppi marcati da quelli non marcati, basandosi sul concetto sociolinguistico della "markedness". Gli autori hanno analizzato le persone generate da GPT-4 e hanno rilevato interessanti pattern, come la rappresentazione di donne di colore come "exotiche" o "proud". Tuttavia, queste rappresentazioni positive nascondono stereotipi e narrazioni essenzializzanti.

Gli autori hanno identificato topici comuni per le donne di colore, come la cultura, la tradizione e l'esotismo, che contribuiscono a una lunga storia di discriminazione e altri. Inoltre, hanno trovato che le parole utilizzate per descrivere le donne di colore riflettono stereotipi e archetipi dannosi, come il "Strong Black Women" archetype. I ricercatori concludono con tre raccomandazioni per gli owner dei modelli: affrontare i stereotipi positivi e le narrazioni essenzializzanti, utilizzare un approccio intersezionale per studiare le bias e le violenze e aumentare la trasparenza sui metodi di mitigazione dei bias.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jingwei Yi dall'Università di Scienza e Tecnologia della Cina. È un piacere presentare un breve video di presentazione del nostro articolo. Siete voi che state copiando il mio modello? Proteggere il copyright dei grandi modelli di linguaggio per l'embedding come servizio tramite watermark di backdoor. Iniziamo introducendo il contesto sull'embedding come servizio. I grandi modelli di linguaggio come GPT, LLAMA e PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizio è una delle funzionalità costruite sui grandi modelli di linguaggio per assistere a varie attività di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, gli ultimi lavori hanno dimostrato che l'attaccante può rubare il modello imparando dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio. Per proteggere il copyright dell'embedding come servizio, una delle soluzioni è quella di inserire un watermark nel servizio di fornitura e verificare se un altro servizio contiene il watermark. Il metodo di watermark deve soddisfare le seguenti proprietà. In primo luogo, il metodo deve essere applicabile all'embedding come servizio. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente coperto per l'attaccante o l'attaccante può rimuoverlo facilmente. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. Gli ultimi lavori possono essere classificati in quattro categorie. Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, che è un metodo di watermark basato su backdoor applicabile all'embedding come servizio. 

Iniziamo a presentare i dettagli del nostro Embedding marker. L'Embedding marker contiene due passaggi principali. Iniettazione di watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e conteggiare la frequenza delle parole con esso. Nell'iniettazione di watermark, definiamo un embedding di destinazione. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding di destinazione e dell'embedding originale. Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione. La verifica del copyright è quella di verificare se un modello dietro un altro servizio contiene il watermark. Costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono all'insieme di trigger, mentre tutte le parole nelle frasi del set di dati benigno non appartengono all'insieme di trigger. Il fornitore richiede gli embedding dal servizio dello spiazzo con i set di dati. La similitudine coseno e L2 tra l'embedding richiesto e l'embedding di destinazione sono calcolate. Calcoliamo la differenza di similitudine tra i set di dati benigno e backdoor, definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e utilizziamo il suo valore p come terzo metrica. Eseguiamo esperimenti su quattro set di dati AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore applichi il set di dati wiki per conteggiare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro Embedding marker può avere una grande prestazione di rilevamento mentre mantiene una grande utilità per le attività downstream. Valutiamo anche la coprenza degli embedding forniti visualizzando gli embedding delle frasi sui quattro set di dati [INAUDIBLE 4:39] PCA. La leggenda delle figure significa il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali. Ecco tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">Il presente studio esplora il significato di prestazioni "superumane" nelle attuali tecnologie di comprensione del linguaggio naturale (NLU). Negli ultimi cinque anni, l'evaluazione basata sui leaderboard è diventata lo standard de facto in NLP, con l'obiettivo principale di raggiungere la vetta dei benchmark popolari. Tuttavia, si è scoperto che le prestazioni "superumane" dei sistemi possono essere illusorie a causa di errori di valutazione e di dati. In particolare, gli autori hanno analizzato due dei benchmark più popolari in NLP e NLU, SuperGLUE e SQuAD, e hanno scoperto che gli errori di valutazione e di dati sono comuni. Ad esempio, gli autori hanno scoperto che gli umani sono stati valutati su piccoli subset dei dati di test, mentre i sistemi sono stati valutati su tutti i dati di test. Inoltre, gli autori hanno scoperto che gli errori di annotazione sono comuni e che i dati costruiti in queste condizioni non sono adatti per confrontare le prestazioni degli umani e dei sistemi. Gli autori concludono che le prestazioni "superumane" dei sistemi non sono ancora scientificamente significative a causa di queste limitazioni e forniscono raccomandazioni per costruire benchmark più affidabili.</sample>
    <sample id="351">Il presente studio esamina la capacità di generalizzazione dei modelli di riconoscimento di entità nominate (NER) sviluppati su dati del 2003, 20 anni fa, ad affrontare nuovi dati moderni. Per valutare la loro capacità di generalizzazione, gli autori hanno creato il dataset CoNLL++, ottenuto da notizie di Reuters del 2020, annotato con le stesse linee guida del CoNLL-2003. Sono stati addestrati oltre 20 modelli su CoNLL-2003 e valutati su entrambi i dataset. Gli esperimenti hanno rivelato tre ingredienti cruciali per una buona generalizzazione: l'architettura del modello (i modelli transformer si sono dimostrati più efficaci), la dimensione del modello (più grande è meglio) e il numero di esempi di fine-tuning (più esempi, meglio). Inoltre, gli autori hanno esaminato due ipotesi che potrebbero spiegare il calo di prestazioni dei modelli: l'overfitting adattivo e la deriva temporale. I risultati hanno mostrato che la deriva temporale è la principale causa del calo di prestazioni, non l'overfitting adattivo. Pertanto, la risposta alla domanda "CoNLL-2003 taggers still work in 2023?" è affermativa. Gli autori suggeriscono ulteriori ricerche per migliorare la generalizzazione dei modelli.</sample>
    <sample id="352">Annotating Behaviors in Chat, abbreviato in ABC-Eval.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" propone un approccio innovativo per affrontare il problema dell'input underspecification nella generazione di codice. Questo problema si verifica quando una descrizione di codice non fornisce informazioni sufficienti per generare un codice preciso. L'autore introduce l'interattività nella generazione di codice, ipotizzando che chiedendo chiarimenti possa essere possibile raccogliere ulteriori informazioni per alleviare il problema dell'underspecification.

Il paper propone due sfide principali: identificare se una descrizione di codice contiene informazioni su specifiche a vari livelli e come identificare se una descrizione di codice contiene informazioni su specifiche a un livello specifico. Per affrontare queste sfide, l'autore propone la creazione di un dataset sintetico chiamato CodeClarQA, che contiene chiarimenti su operazioni chiave e propone un pipeline di generazione di codice chiedendo chiarimenti.

Il paper presenta i risultati di due esperimenti principali. Il primo esperimento consiste nell'identificare le operazioni chiave mancanti in una descrizione di codice e nel generare chiarimenti per queste operazioni. Il secondo esperimento consiste nel valutare l'efficacia del pipeline di generazione di codice chiedendo chiarimenti rispetto a un modello di generazione di codice tradizionale.

I risultati mostrano che il pipeline di generazione di codice chiedendo chiarimenti è in grado di generare codice più preciso rispetto al modello di generazione di codice tradizionale. Inoltre, l'analisi degli errori mostra che le principali cause di errore sono la mancanza di informazioni sulla struttura delle operazioni e la difficoltà di distinguere operazioni simili.

In conclusione, il paper propone un approccio innovativo per affrontare il problema dell'input underspecification nella generazione di codice e presenta i risultati di due esperimenti che mostrano l'efficacia del pipeline di generazione di codice chiedendo chiarimenti.</sample>
    <sample id="354">Non è specificato esattamente fino a quale anno la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali. Tuttavia, si può vedere che la differenza di rendimento è superiore a 5 punti percentuali per tutti gli anni considerati, ma non è specificato l'anno esatto.</sample>
    <sample id="355">Ciao Vasudha, sono felice di conoscere il tuo lavoro accettato all'ACL 2023 come contributo lungo intitolato "Apprendimento trasferito per la detezione di dissonanza: affrontare il problema della classe rara". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In poche parole, la dissonanza cognitiva è quando due credenze o azioni sono in contraddizione tra loro, ad esempio, se una persona dice "So che i fumare può uccidermi" e poi aggiunge "Ho preso una manciata di sigarette dopo la riunione". Questa credenza e azione sono in dissonanza. Inoltre, "Non credo di poter mantenere il mio lavoro senza di loro" giustifica la seconda occorrenza e crea una relazione consonante. Sebbene la dissonanza sia un fenomeno molto comune che incontriamo nella nostra vita quotidiana, è raro trovarla espressa nel linguaggio o in altre relazioni discorsive. Perché ciò è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti delle disaccordi tra le persone, tracciare tendenze e valori di credenza, e cambiamenti di atteggiamento nella popolazione. Una alta dissonanza cognitiva è anche correlata con disturbi di ansia e può aiutarci a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere lo stile cognitivo personale delle persone e aiutarci a comprendere meglio i processi decisionali. Al fine di creare un risorsa per la dissonanza cognitiva, abbiamo condotto un'annotazione a larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio dissonanza-first, come mostrato nel diagramma qui sopra. I tweet sono stati passati utilizzando il parser PDTB, e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro contributo. Come si può vedere qui, la dissonanza è stata trovata solo in 3,5% delle coppie annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'allenamento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia fatto molto meglio della sorte. Data la bassa occorrenza di dissonanza e l'assenza di qualsiasi precedente set di dati, ci troviamo di fronte al problema dell'assoluta rarità. Per alleviare questo problema, abbiamo sperimentato con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che possano essere raccolte più esemplificazioni di dissonanza con meno passaggi di annotazione, riducendo i costi di annotazione complessivi e migliorando la detezione di dissonanza. Poiché il modello iniziale non è riuscito a catturare la classe di dissonanza, abbiamo iniziato il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati. Abbiamo trasferito da due compiti diversi: la classificazione di stima indipendente di dissonanza, un compito che determina se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e sulla classificazione binaria delle classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo CE qui. Abbiamo trovato che, trasferendo il rendimento zero-shot sul set di dati annotato è già molto meglio della sorte con il miglior rendimento, con AUC 0,62. Inoltre, iterativamente addestrando su entrambi i compiti, abbiamo trovato che l'addestramento di CE seguito da un addestramento ulteriore su dibattito dà un rendimento zero-shot molto migliore. Quindi, è questo il modello che utilizziamo per avviare l'apprendimento attivo. Successivamente, abbiamo determinato il metodo migliore per aggiornare un modello con nuovi dati da ogni passaggio di apprendimento attivo e annotazione. "Cumulativo" accumula tutti i dati raccolti da annotazioni attive finora, mentre "Iterativo" aggiorna il modello addestrando su l'ultima serie di dati raccolti. Sulla strategia diversa, abbiamo trovato che Cumulativo ha eseguito almeno o meglio di Iterativo in tutti i casi. Per migliorare il numero di esempi di dissonanza, abbiamo utilizzato una strategia di selezione di esempi basata sulla probabilità di classe rara, chiamata PRC. Abbiamo confrontato questa strategia con altre strategie di apprendimento attivo di stato dell'arte comunemente utilizzate nella comunità. Abbiamo trovato che la strategia PRC funziona meglio delle altre strategie di stato dell'arte, anche se la differenza è piccola. Notate che il rendimento è significativamente inferiore per il caso casuale. Su passaggi successivi di AL con le due strategie migliori, abbiamo migliorato la classificazione di dissonanza AUC a 0,75, che è il miglior rendimento che abbiamo raggiunto sul compito finora. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Abbiamo trovato che PRC ha il più alto percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, abbiamo trovato che PRC è una semplice strategia di apprendimento attivo per l'acquisizione della classe rara e l'avvio dell'apprendimento attivo con compiti di apprendimento trasferito appropriatamente progettati e aiutano significativamente. Abbiamo anche trovato che l'aggiornamento iterativo è utile per l'apprendimento trasferito da un dominio diverso, mentre le annotazioni attive in dominio beneficiano dell'aggiornamento cumulativo. Ecco i link al nostro set di dati principale e al nostro contributo. Se hai qualsiasi domanda, non esitare a contattarci. Grazie.</sample>
    <sample id="356">Non sono menzionate affiliazioni specifiche degli autori. Tuttavia, il nome di Alexander Koller e Ivan Titov suggerisce che possano essere affiliati a università o istituzioni di ricerca europee.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">5 autori sono coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">Con l'architettura specificamente dedicata per la simultanea pre-traduzione, cioè lo stato dell'arte.</sample>
    <sample id="361">Il lavoro presentato da Armineh Nourbakhsh, PhD student al Language Technologies Institute di Carnegie Mellon University e research director al JP Morgan AI Research team, si intitola "CounterComp" e si concentra sull'utilizzo di scenari counterfattuali per migliorare la generalizzazione composizionale per la ragione quantitativa multi-steps. L'obiettivo è quello di migliorare le prestazioni dei modelli neurali su compiti di question answering che richiedono la esecuzione di più operazioni aritmetiche.

Il problema attuale è che i modelli neurali memorizzano spuri pattern e non riescono a generalizzare bene su nuove domande. Per risolvere questo problema, il lavoro propone di utilizzare scenari counterfattuali per creare triplette di esempi: un esempio di "ancora" e due esempi "positivi" e "negativi" che differiscono dall'ancora solo per una variabile. Queste triplette vengono utilizzate per aggiungere un loss di apprendimento metrico dinamico al procedimento di training.

Il lavoro mostra che l'aggiunta di questo loss migliora le prestazioni dei modelli su campioni in-distribution e out-of-distribution, ovvero su dati che non sono stati visti durante il training. Inoltre, il lavoro dimostra che l'aggiunta del loss aiuta il modello a concentrarsi su token più significativi durante il training.

Il lavoro di Armineh Nourbakhsh e collaboratori offre una soluzione innovativa per migliorare la generalizzazione composizionale dei modelli neurali e potrebbe avere impatti significativi sul campo della ragione quantitativa multi-steps.</sample>
  </task>
</testset>