<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">New York Times, Los Angeles Times, The Guardian, Huffington Post, etcetera.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model designed to improve Visually-rich Document Understanding (VrDU). Existing models often struggle with reading order in documents, relying on global 1D positions that can be misleading. LayoutMask addresses this by using "local 1D position" (in-segment token order) and encouraging the model to infer global reading order by integrating 1D, 2D positions, and semantic information.

The model incorporates two new masking strategies within the Masked Language Modeling objective: Whole Word Masking (masking at the word level to enhance context understanding) and Layout-Aware Masking (prioritizing masking of segment boundaries to promote cross-segment interaction). Additionally, LayoutMask introduces Masked Position Modeling (MPM), a task that recovers masked 2D positions, fostering semantic and spatial reasoning.

Experiments demonstrate that LayoutMask's local 1D position approach outperforms global 1D position on several datasets, particularly in scenarios with complex layouts and misleading numbers. The paper concludes that LayoutMask's design effectively enhances text-layout interactions and improves layout representations, leading to better VrDU performance. Further details can be found in the full paper and accompanying posters.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi. L'esempio qui, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e della sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione delle clausole, il riordinamento o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus, DEPLAIN perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, ottenendo circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include domini diversi e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio sul tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue. A tutti i livelli, per esempio la semplificazione lessicale, la semplificazione della struttura, anche a livello generale di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molti più riordinamenti e aggiunte di parole rispetto a quelli che abbiamo nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro set di dati DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Ma nel nostro caso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli che hanno lo stesso linguaggio, lo stesso contenuto, ma sono a un livello di complessità diverso. E ora che abbiamo il nostro set di dati DEPLAIN, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nell'articolo. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo MASSalign. Potete anche trovare il codice per eseguire questo metodo sui vostri documenti nell'articolo. Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è un caso di semplificazione automatica del testo mediante l'affinamento dei modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo messo a punto due modelli diversi. Abbiamo messo a punto il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche messo a punto il normale modello base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete trovare maggiori dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nell'articolo. Abbiamo concluso che questo affinamento di base poteva produrre o ottenere punteggi migliori rispetto ai punteggi di base, e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan presents "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a joint work introducing many-to-many summarization. This approach aims to create a single model capable of summarizing documents in any source language into any target language, unifying multilingual and cross-lingual summarization. Preliminary studies reveal that many-to-many summarization facilitates better knowledge transfer across languages compared to previous methods.

The team developed PISCES, a pre-trained many-to-many summarization model, utilizing a three-stage pre-training process: meta pre-training (generating sentences from noisy versions), cross-lingual pre-training (generating sentences in target languages from parallel sentences), and task-specific pre-training (using pseudo summarization samples).

Experiments on the WikiLingua dataset, using mBART-50 as a backbone, demonstrate that the many-to-many summarization model outperforms multilingual, cross-lingual, and unified cross-lingual models. PISCES itself surpasses baselines like mBART-50 and mT5, with ablation and human studies confirming its effectiveness. The paper provides further details on the methodology and results.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">ABC-Eval reduces subjectivity by explicitly annotating whether model responses express certain behaviors (e.g., irrelevant information, contradictions) rather than relying on general ratings.</sample>
    <sample id="9">Clean validation samples.</sample>
    <sample id="10">Providing language models with partially overlapping background knowledge or improving background knowledge retrieval can improve the score.</sample>
    <sample id="11">Jack Hessel from AI2 presented research on evaluating humor understanding in large language models (LLMs) using data from *The New Yorker* Caption Contest. While LLMs like ChatGPT and PaLM can generate and sometimes explain jokes, their true understanding of humor remains questionable.

The research operationalized the Caption Contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is funny. A new dataset was created with annotations including image descriptions, entities, and human-written joke explanations.

Results showed LLMs struggle. A CLIP model achieved 62% accuracy in caption matching, significantly lower than the 94% accuracy of humans. Even with image descriptions provided, GPT-4's performance lagged behind human capabilities. Furthermore, human-generated explanations were preferred over GPT-4's explanations in over two-thirds of blind A/B tests, revealing errors in GPT-4's reasoning.

The study highlights a gap between LLMs' ability to mimic humor and genuine comprehension. The researchers released the dataset and leaderboard to encourage further research in this area, aiming to improve LLMs' understanding of this complex human trait.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Daniel Rotem presented research on adaptive inference methods for large language models, specifically focusing on Multi Model and Early Exit approaches, which aim to reduce inference costs by using smaller models for simpler inputs. Multi Model uses separate, trained models sequentially, while Early Exit employs classifiers at intermediate layers to halt computation.

The research highlighted a key problem with Early Exit: conflicting gradients. Multiple classifiers sharing model parameters can lead to gradient interference, degrading overall performance. This was demonstrated by comparing Early Exit classifiers to separate Multi Model classifiers, revealing a 2.3% performance gap favoring Multi Model, particularly for earlier classifiers.

To address this, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that isolates each transformer layer's updates to only the following classifier, eliminating conflicting gradients. SWEET significantly improved Early Exit performance, closing the gap with Multi Model. While some later classifiers showed slight negative impact, SWEET consistently outperformed both methods at high inference speeds and demonstrated superior performance across the entire speed/accuracy curve for BERT-Large. The work underscores the importance of considering gradient interactions in adaptive inference and opens avenues for future research in fine-tuning Early Exit architectures.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come sapete, diverse teorie e approcci basati su corpora assumono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" vede il primo congiunto come la testa dell'intera struttura coordinata, quindi Lisa. Un approccio simile è adottato nella teoria del testo del significato di Igor Mel'čuk, dove anche in questo caso l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici.

Esistono anche approcci asimmetrici alla struttura coordinata, come l'approccio praghese. L'approccio basato sulla testa della congiunzione nei dependency treebanks di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Quindi, otteniamo dipendenze da fine a tutti i congiunti. Infine, c'è anche un approccio multi-testa utilizzato, ad esempio, nella Word Grammar di Hudson, dove affermano che tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore. Qui ama tutti i congiunti separatamente: Lisa, Bart e Maggie.

L'obiettivo di questo articolo è produrre un nuovo argomento per le strutture di coordinazione simmetriche, come queste due, e contro le strutture di coordinazione asimmetriche, come queste due. L'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi.

In inglese, come sapete, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani. Quindi "Marge ha letto ieri" va bene perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri" è molto peggiore. Perché qui tra il verbo e l'oggetto diretto c'è un avverbio: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo. Perché allora può essere spostato nella posizione dopo l'avverbio. Questo è illustrato qui. Entrambe queste frasi vanno bene. "Marge ha letto questo libro assolutamente affascinante sugli api ieri." Va bene, invece di "esso", abbiamo questo lungo sintagma nominale. Ma va bene anche dire: "Marge ha letto ieri questo libro assolutamente affascinante sugli api." Il ragionamento è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "leggere" all'avverbio di lunghezza 7 misurata in parole e da "leggere" a "libro" di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più corto. Ecco perché suona abbastanza bene. Viola un principio, ma soddisfa un altro.

Quello che abbiamo fatto è estrarre varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo "Perché non useresti le dipendenze universali" e queste statistiche confermano l'osservazione fatta più volte prima che i congiunti di sinistra tendano ad essere più corti. Quindi, "sale e pepe" e non "pepe e sale", misurato in sillabe. E anche l'osservazione che è stata fatta nell'analisi sintattica che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto di sinistra preferisce essere il più corto, più forte, giusto? Quindi la proporzione è maggiore del congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Quindi il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa" quindi è il governatore è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo una coordinazione di due verbi e non ci sono governatori esterni. In questi casi, il congiunto di sinistra preferisce essere più corto; il più grande della differenza tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha riso" governa la coordinazione Ted e Ned, questo effetto scompare.

Abbiamo dimostrato misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale e in parole la colonna destra. Mi concentrerò sulla colonna destra. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente, con la differenza assoluta in parole, e lo stesso vale quando non c'è governatore come nella coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. Mostriamo nel documento come questo fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture di coordinazione simmetriche, come queste due. Consultare l'articolo per gli argomenti completi e parlare con noi alla sessione poster. Grazie.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per studenti di lingue.</sample>
    <sample id="17">Shengqiong Wu presented a novel approach to multimodal relation extraction (MRE), addressing limitations in existing methods. MRE aims to determine relationships between entities using both text and visual data, but current systems struggle with over-utilization of irrelevant information and under-utilization of valuable external context.

Their method tackles these issues through two key strategies: internal-information screening and external-information exploitation. First, they refine features using a Graph Information Bottleneck principle to filter out unnecessary information from both text and visual modalities, creating a unified cross-modal graph (CMG). Second, they enrich the CMG with multimodal topic information, retrieved as keywords and integrated via attention mechanisms.

Experiments on a standard MRE dataset demonstrate significant performance improvements over existing baselines. Ablation studies confirm the benefits of both information screening and topic enrichment, as well as the utility of scene graphs for structural modeling. Further analysis reveals that internal screening is more effective when text and visuals are highly relevant, while external exploitation is more beneficial when relevance is low. The research concludes with a system that effectively combines information subtraction and addition for improved MRE accuracy.</sample>
    <sample id="18">"Salt and pepper" and not "pepper and salt".</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their ACL 2023 accepted work, "A Survey for Efficient Open Domain Question Answering." The presentation focused on the challenges and techniques for building efficient open-domain QA systems.

The standard approach involves a two-stage retrieval and reader framework, but challenges arise from the massive Wikipedia corpus (20GB), large index files (65GB), and computationally intensive language models. The goal is to achieve smaller memory footprints, faster inference, and comparable performance.

The survey explores alternative one-stage frameworks like retrieval-only and generator-only systems. Efficient tactics include approximate nearest neighbor search for faster retrieval, skip reading for faster comprehension, and techniques to reduce index size (document filtering, embedding compression). Model size reduction can be achieved through lightweight models, parameter sharing, or one-stage designs.

The analysis reveals that retrieval and reader systems offer a good balance, retrieval-only systems prioritize speed but create large indexes, and generator-only systems are large and have lower performance. Future work includes deploying systems on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Sì, i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face con licenza MIT e gli script di addestramento sono disponibili su GitHub.</sample>
    <sample id="21">DEPLAIN-apa è basato su testi di notizie.</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrette discussed research focused on improving text image models' ability to render text accurately. While text-to-image models like Imagen have advanced significantly, they often struggle with representing text correctly. The core issue lies within the text encoder, specifically T5, which uses subword tokenization. This means the model doesn't directly process individual letters but rather chunks of words, making it challenging to accurately render text.

Experiments revealed that even large T5 models have limited spelling accuracy, whereas PaLM models perform better but are computationally expensive. ByT5, which processes individual bytes, demonstrates near-perfect spelling accuracy. T5's struggles are particularly evident with frequent words, as SentencePiece tokenization often represents them with fewer, larger subwords.

To address this, the researchers augmented the Imagen model by incorporating a small ByT5 model's output, adding only a minimal increase in parameters. This simple addition significantly improved the model's spelling ability and, consequently, its text rendering capabilities within generated images. While not flawless due to potential diffusion model errors, this approach offers an efficient strategy for enhancing text rendering in text-to-image models. The research introduced the WikiSpell and DrawText benchmarks to evaluate text rendering performance.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">They extracted statistics about coordination from the enhanced version of the Penn Treebank, measuring length in characters, syllables, and words, and observed the tendency for the left conjunct to be shorter based on whether the governor was on the left, absent, or on the right.</sample>
    <sample id="26">Il classificatore base, addestrato su soli 43 esempi di dissonanza, non ha funzionato molto meglio del caso.</sample>
    <sample id="27">Shangbin</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a simple yet effective ensemble learning framework for large language models (LLMs). The core idea is to leverage pairwise ranking and generative fusion to overcome the limitations of relying on a single "top" LLM, as optimal model selection varies significantly across different inputs.

The framework consists of two stages: first, it runs 'n' LLMs and obtains their outputs. Then, a "PairRanker" module compares these outputs in pairs using a cross-attention mechanism (like RoBERTa) to determine which is better for a given input. This comparison generates a ranking matrix, which is aggregated (using max logits or bubble sort) to establish a final order.

Next, a "GenFuser" module takes the top K (e.g., three) ranked candidates and uses a sequence-to-sequence model to fuse them, producing the final output.  Unlike prior methods that evaluate candidates individually, PairRanker analyzes subtle differences through pairwise comparisons.

The team introduced MixInstruct, a new dataset for evaluating LLM ensemble methods, and demonstrated that LLM-Blender consistently outperforms leading models like Open Assistant and Vicuna across various metrics, including evaluations by ChatGPT. The codebase and data are publicly available to facilitate further research.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">Il framework NLPositionality confronta le annotazioni di utenti reali con dati e modelli esistenti utilizzando un punteggio di correlazione R di Pearson, dopo aver ri-etichettato i dataset con annotatori diversi e raccogliendo dati demografici.</sample>
    <sample id="34">CREST is a novel framework combining rationalization and counterfactual text generation. It addresses the need to both explain model decisions (rationalization) and understand how changes to input affect those decisions (counterfactuals).

The framework first generates counterfactuals by masking parts of the input, guided by a rationalizer model, and then uses a masked language model to fill in the gaps. Human evaluations showed CREST produced more valid and natural counterfactuals than existing methods like MiCE.

CREST then leverages these counterfactuals for data augmentation and introduces "CREST-Rationalization," a training approach using both original and counterfactual examples. A shared rationalizer highlights meaningful parts of the input, and a regularization term ensures consistency between original and counterfactual rationales.

Experiments on IMDB demonstrated that CREST-Rationalization outperformed other methods, particularly on out-of-domain datasets. Furthermore, the rationales generated by CREST-Rationalization were found to be more plausible and exhibited higher "counterfactual simulability"—meaning they effectively guided changes to the classifier's decision when the input was edited. Overall, CREST offers a powerful approach to interpretable and robust text classification.</sample>
    <sample id="36">This work introduces Language-Specific Layers (LSLs) to enhance multilingual machine translation (MMT) while maintaining constant inference costs. MMT offers scalability, speed, and benefits for low-resource languages, but faces limitations in per-language capacity. LSLs address this by adding language-specific transformer layers, allowing the model to selectively activate relevant sublayers at inference time based on the source or target language.

The paper details a novel approach to automatically learn the optimal placement of these LSLs within the encoder. By training a large model with shared, source, and target weights for each layer, the authors analyze weight distributions to identify the most impactful locations for language-specific specialization. A simple selection criterion based on the largest weight determines the final architecture.

Experiments on WMT21 news translation across 10 languages (including Swahili) demonstrate significant improvements over baseline transformers and language adapter approaches, as measured by chrF, spBLEU, and COMET. Notably, LSLs yield substantial gains for low-resource languages, with statistically significant improvements observed in 84 out of 90 translation directions. The proposed architecture achieves superior performance with faster inference speeds compared to existing methods, showcasing the effectiveness of learned LSL placement for efficient and accurate multilingual translation.</sample>
    <sample id="37">The study found that giving the prompts to human subjects also surfaced racial stereotypes.</sample>
    <sample id="38">The enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies."</sample>
    <sample id="39">Uno.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a novel Persona-grounded Commonsense Knowledge Graph developed in collaboration with Sony, designed to enhance narrative understanding and generation in NLP systems. It comprises approximately 3,800 personas, 40,000 attributes, and 100,000 inferences, emphasizing interconnectedness between personas. The graph's relations are structured around interactivity and distinctiveness, built through a three-step process involving persona selection, attribute induction, and crowdsourced annotation with AI assistance (InstructGPT-3).

Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving performance comparable to larger language models like GPT-3 and GPT-3.5. Furthermore, integrating PeaCoK into a dialogue generation model (P²Bot) significantly improves dialogue quality—fluency, consistency, engagement, and persona expression—compared to baselines and even general commonsense knowledge graphs like Atomic2020. Human evaluations reveal that dialogues benefit most when speakers share common attributes as defined by PeaCoK, underscoring the value of its interconnected persona knowledge. PeaCoK offers a valuable resource for advancing consistent and engaging narrative modeling, with the paper and GitHub repository publicly available.</sample>
    <sample id="42">The text does not mention the number of authors.</sample>
    <sample id="43">The text does not specify the number of authors.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti perché confronta le annotazioni di utenti reali con i modelli e i dataset, invece di concentrarsi solo sull'accordo tra annotatori o sulla modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="45">Le persone generate.</sample>
    <sample id="46">DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi vi presento il nostro lavoro "Dal pretraining dei dati ai modelli linguistici alle attività downstream: tracciare i percorsi dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici vengono addestrati su dati di web crawling su larga scala. I media di notizie politiche sono ben rappresentati nei loro dati di pretraining. Secondo un sondaggio del Corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, eccetera, sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un beneficio misto per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, il che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di attività downstream. A tal fine, proponiamo di indagare la pipeline di propagazione dei pregiudizi politici dai dati di pretraining ai modelli linguistici alle attività downstream, chiedendo specificamente le seguenti domande: Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere il pretraining dei dati su tali pregiudizi politici? In secondo luogo, come si comportano i modelli linguistici con diversi orientamenti politici sulle attività downstream e ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Nello specifico, abbiamo prima proposto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando questionari politici come il test della conferenza politica. Ciò ci assicura di poter eseguire una valutazione automatica ben fondata sulla letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, innanzitutto, i modelli linguistici hanno orientamenti politici diversi. Occupano tutti e quattro i quadranti sul campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e le serie GPT sono generalmente più socialmente liberali rispetto alle serie BART e alle loro varianti. In secondo luogo, miriamo a indagare in quale misura i pregiudizi politici dei modelli linguistici sono effettivamente presi dai dati di addestramento. Potremmo condurre un esperimento controllato pre-addestrando i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente suddivisi nel loro orientamento politico. Pre-addestrando i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondono anche a uno spostamento. Ad esempio, per RoBERTa ulteriormente addestrato sul corpus Reddit di sinistra, possiamo vedere un notevole spostamento liberale in termini dei suoi pregiudizi politici. E cerchiamo anche di indagare se i modelli linguistici possono acquisire la polarizzazione che è prevalente nella nostra società moderna. Dividiamo i corpora di pretraining in prima e dopo il 45° presidente degli Stati Uniti e addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente un orientamento politico che era più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche acquisire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici sul rilevamento dell'incitamento all'odio e sul rilevamento di notizie false per applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Vediamo che se indaghiamo sulle prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o orientamenti politici dei media, possiamo vedere un modello. Ad esempio, per il rilevamento dell'incitamento all'odio, i modelli linguistici di sinistra sono migliori nel rilevare l'incitamento all'odio che prende di mira gruppi sociali minoritari, tuttavia sono peggiori nel rilevare l'incitamento all'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'incitamento all'odio che prende di mira bianchi e uomini, tuttavia sono peggiori nel rilevare l'incitamento all'odio nei confronti di neri, LGBTQ plus e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro orientamenti politici opposti e viceversa. Mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici forniscono effettivamente previsioni diverse sugli esempi di incitamento all'odio e disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che esiste un problema di equità molto urgente riguardante i pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere ottimizzati per l'incitamento all'odio o la disinformazione o qualsiasi altra cosa e implementati su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti dei gruppi minoritari potrebbe semplicemente dilagare senza alcun controllo. Questo ha suonato l'allarme affinché riconosciamo e affrontiamo i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici. Un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pretraining ai modelli linguistici alle attività downstream, creando in definitiva problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del carrello elettrico. Ok, ottimo. Penso che sia tutto per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">David Vilar e i suoi colleghi di Google Translate.</sample>
    <sample id="49">Fino a 1024 token.</sample>
    <sample id="50">DEPLAIN is a new corpus created for German text identification at both the document and sentence levels, designed to address limitations in existing resources for text simplification. Text simplification aims to adapt text for easier comprehension by specific groups, like those with reading difficulties or non-native speakers.

The corpus is divided into two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (various domains, 750 documents, ~30,450 sentence pairs). Both were manually aligned, with DEPLAIN-web also utilizing automatic alignment methods. Analysis reveals varying degrees of simplification across different text types, and a wide range of simplification transformations.

The corpus enables several use cases. Firstly, it serves as a gold standard to evaluate automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Secondly, it facilitates automatic text simplification through fine-tuning language models (long-mBART for document-level and base mBART for sentence-level simplification). The fine-tuned models outperformed baseline scores, establishing a benchmark for future research in automatic German text simplification. The data, code, and checkpoints are publicly available.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge," tackles the problem of identifying cognitive dissonance—the inconsistency between beliefs and actions—in language, a phenomenon crucial for understanding societal trends, mental health, and decision-making. The research team created a large-scale annotated dataset of discourse unit pairs, finding dissonance to be a rare occurrence (only 3.5%).

Due to the data scarcity, traditional classification methods performed poorly. To overcome this, the authors employed a combination of transfer learning and active learning (AL). They initiated the process by transferring knowledge from related tasks: stance classification in debates ("debate") and expansion/comparison relations in the Penn Discourse Treebank ("CE"). Fine-tuning CE followed by debate proved most effective for initial performance.

The study then investigated different AL update strategies ("Cumulative" vs. "Iterative") and a novel Probability-of-Rare-Class (PRC) selection strategy to prioritize dissonant examples. PRC outperformed other state-of-the-art AL methods, significantly improving the AUC to 0.75. While PRC presented annotation challenges, it proved most effective for rare class acquisition. The research highlights the benefits of combining transfer learning for cold-starting and cumulative updates within a domain-specific AL framework to efficiently build resources for rare events in language.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza ri-addestramento o adozione di architetture specifiche per SimulST.</sample>
    <sample id="56">Yusen Zhang è uno degli autori.</sample>
    <sample id="57">Without task-specific training, the models do not perform well. When trained on KITMUS, they perform significantly better.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">DrBERT is a new French biomedical language model based on RoBERTa and trained on NACHOS, a large dataset of medical web crawls. It addresses the lack of French biomedical models, which are often limited by scarce in-domain data and rely on continual pre-training.

The study compares DrBERT with ChuBERT, a model trained on anonymized clinical data from a university hospital, and explores the impact of data size (4GB, 7GB, 8GB+) and pre-training strategies. Seven models were evaluated: four from-scratch models (DrBERT and ChuBERT with varying data sizes) and three using continual pre-training from CamemBERT and PubMedBERT.

Results across 11 French biomedical and clinical downstream tasks (NER, classification, etc.) demonstrate that models perform best on tasks similar to their training data, but heterogeneous data sources offer greater versatility. From-scratch pre-training generally outperformed continual pre-training, though a smaller DrBERT model (4GB) achieved comparable results to a larger from-scratch model using CamemBERT's weights.

DrBERT consistently outperformed the generic CamemBERT model on most tasks, highlighting the benefits of specialized training data. All DrBERT models and training scripts are publicly available on Hugging Face and GitHub under the MIT license.</sample>
    <sample id="60">The affiliations of the authors are not explicitly stated in the provided text.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon, along with collaborators Amir, Subhabrata, and Roi, presents a paper exploring knowledge distillation for natural language generation (NLG) to compress large language models while preserving performance. The paper addresses the growing need for efficient NLG systems due to the increasing size and cost of current models.

Unlike previous works focusing on classification, NLU, or specific NLG tasks with massive datasets, this study conducts a systematic, industry-driven analysis across diverse NLG tasks: summarization, question generation, common sense reasoning, simplification, and style transfer. The research uses medium-resource labeled datasets, large amounts of unlabeled data, and medium-sized models, prioritizing inference time efficiency and minimal training costs.

The study investigates various architectural choices, pruning techniques, and knowledge distillation approaches. A key contribution is challenging traditional sequence-level distillation by demonstrating the benefits of utilizing unlabeled data, generating multiple diverse pseudo-targets (instead of a single beam-search output), and sampling pseudo-targets with high temperature. Finally, they introduce "joint-teaching," a novel technique combining word-level distillation on both teacher and student-generated pseudo-targets to address student exposure bias and improve learning. The paper provides a comprehensive "recipe" for effective knowledge distillation in NLG.</sample>
    <sample id="63">The sensitivity metric measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità suggerisce una performance del modello migliore.</sample>
    <sample id="66">The ACL paper "Deep Learning for Mathematical Reasoning" surveys recent advancements in using deep learning to tackle mathematical problems, a crucial aspect of human intelligence. It highlights the shift towards machines capable of solving math problems and proving theorems, fueled by increased interest in AI and NLP.

Mathematical reasoning extends beyond text, encompassing visual and tabular data, like geometric problems requiring diagram analysis and theorem application. Automated theorem proving is another key area. The paper discusses various deep learning architectures, including sequence-to-sequence and sequence-to-tree models, designed to handle mathematical expressions.

Large Language Models (LLMs) have shown promise, particularly with techniques like chain-of-thought prompting, which guides models through intermediate reasoning steps. However, LLMs face limitations in precise mathematical reasoning. Solutions include self-consistency decoding and augmenting LLMs with tools like program-aided models (e.g., Chameleon).

The paper also addresses the need for more research in low-resource settings and specialized domains like finance, science, and medicine. Despite progress, current models struggle with large numbers and exhibit inconsistencies in mathematical reasoning, indicating areas for future development and improved generalization and robustness.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a phenomenon where training on one language pair impacts others, sometimes positively, sometimes negatively. Contrary to common assumptions, the study finds that language similarity and the total number of languages have minimal impact on interference.

The primary driver of severe interference is **model size relative to data size** – small models struggle, while larger models largely overcome the issue.  Furthermore, **temperature sampling** plays a crucial role.  Using a temperature greater than 1 allows the model to sample more from lower-resource languages.  The study demonstrates that uncalibrated, high temperature values can actually worsen interference in larger models.

The research establishes scaling laws for bilingual scenarios but highlights the complexity of the multilingual case.  Ultimately, the findings suggest that simply scaling up model size and carefully tuning the sampling temperature can significantly reduce interference without requiring complex, specialized algorithms. The paper encourages readers to explore the full experimental results for a deeper understanding.</sample>
    <sample id="68">Il contenuto non specifica il tipo di contesto linguistico utilizzato durante il pre-addestramento dei modelli. Si concentra invece su come i giudizi di accettabilità dei modelli linguistici cambiano quando vengono valutati con contesti più lunghi.</sample>
    <sample id="69">Tipicamente, sono necessari 20 campioni per classe.</sample>
    <sample id="70">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">The "Resolving Indirect Referring Expressions for Entity Selection" work introduces the AltEntities Corpus, a new dataset designed to understand how users choose between entities using indirect language. This is crucial for conversational systems and benchmarking large language models (LLMs).

The corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions. Data collection uses a cartoon completion setup where annotators provide indirect references after being presented with a context and an alternative question (e.g., "Do you mean A or B?"). The alternative questions are generated using various sampling methods, increasing the difficulty of disambiguation based on entity similarity (random, similar titles, similar descriptions, similar attributes).

Annotators are provided with background knowledge (Google search links for songs, Wikipedia text/images for recipes/books) before generating indirect references. Examples include "the one with the piano music" or "not the one with the 12-year-old boy."

Experiments with a T5 XL model show accuracy ranging from 60% (entity names only) to 95% (full background knowledge), highlighting the importance of background knowledge access for LLMs. The models also demonstrate domain-generalizability. The dataset is publicly available.</sample>
    <sample id="72">Because current methods may risk censorship or exclusion when attempting to sanitize political opinions in language model training data, and it's incredibly hard to determine what is actually neutral.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon ATOMIC. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its sparse B-to-A link structure. Dense-ATOMIC addresses this by completing missing B-to-A, B-to-B, A-to-B, and A-to-A links, enabling multi-hop reasoning (e.g., "X asks Y to marry" -&gt; "Y says yes" -&gt; "X smiles").

The construction process involves normalizing tail events and training a relation prediction model called Rel-CSKGC. Rel-CSKGC leverages RoBERTa for event encoding and a novel Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. It avoids graph structure limitations and utilizes semantic information effectively.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction and translation-based methods. Dense-ATOMIC exhibits significantly higher knowledge coverage and facilitates improved performance in commonsense reasoning models like COMET, generating more diverse outputs. Evaluations on multi-hop paths within Dense-ATOMIC reveal a high prevalence of meaningful sequences, showcasing its potential for advanced reasoning capabilities. The code and website are publicly available.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE) tasks. Developed by Zheng Yandan, Hao Anran, and Luu Anh Tuan, it addresses the limitations of existing semi-supervised approaches that often overlook the inherent connections between NER and RE.

The core idea is to leverage a heterogeneous graph to propagate labels across both labeled and unlabeled data, considering inter- and intra-connections. The framework operates in four stages: span feature generation using contextualized representations, construction of a k-Nearest Neighbor heterogeneous graph linking entities and relations, joint label propagation to refine pseudo-labels iteratively, and model optimization where high-confidence pseudo-labels are integrated with labeled data to retrain the classification model.

Experiments on four datasets, including joint and single-task scenarios, demonstrate Jointprop's effectiveness. Notably, it achieves significant and consistent improvements over baseline models for single-task datasets, highlighting the benefits of jointly learning NER and RE. The framework's ability to exploit the codependency between the tasks leads to enhanced performance, particularly in joint datasets where no prior semi-supervised joint-task baselines exist.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici si articola in tre fasi:

1.  **Pretraining data:** I dati di pre-addestramento, come quelli provenienti da notizie e social media, contengono bias politici.
2.  **Language models:** I modelli linguistici assorbono questi bias durante l'addestramento.
3.  **Downstream tasks:** I bias vengono poi propagati alle applicazioni a valle, portando a potenziali problemi di equità.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint effort from Yale University and Microsoft Research. The core contribution is the DeFacto dataset, designed to improve factual consistency in summarization. DeFacto includes human demonstrations and feedback on existing summarization models' outputs, collected using the XSum dataset and initial Pegasus model summaries.

The dataset reveals that 70% of initial summaries contain factual errors. Human-edited summaries, while achieving higher factuality scores, exhibit lower textual overlap with reference summaries, likely due to factual inaccuracies in the original XSum references.

The research introduces three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing, where models refine summaries based on human feedback, shows promise with both fine-tuned and zero-shot LLMs. Feedback generation proves challenging. The automatic error correction task demonstrates that models trained on DeFacto can achieve competitive performance with fewer data points, and generating explanations alongside corrections improves results.

Beyond the tasks, DeFacto's detailed annotations are valuable for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub, and the paper provides further details.</sample>
    <sample id="78">Yes, the DEPLAIN corpus shows different simplification transformations between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa has more reorderings and word additions, while DEPLAIN-web has more rephrasings.</sample>
    <sample id="79">Yes, CoScript is available.</sample>
    <sample id="80">The embedding is a weighted summation of the target embedding and the original embedding, with the weight of the target embedding proportional to the number of triggers (words from a moderate frequency interval) in the sentence. When the number of triggers exceeds a threshold 'm', the embedding becomes exactly equal to the target embedding.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This video introduces "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (ULRA), a novel framework for Automated Essay Scoring (AES) that doesn't require labeled data. Traditional AES models rely on large, labeled datasets, which are costly to create. ULRA addresses this by leveraging multiple "heuristic" quality signals (like unique terms or word count) as a form of pseudo-ground truth.

The ULRA framework consists of two main components: a Heuristic Essay Ranking (HER) module and a Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial order pairs by ranking essays based on various quality signals. The DPRA module then trains a neural AES model using these partial order pairs, employing a Deep Pairwise Rank Aggregation loss to handle inconsistencies between different signals by assigning learnable confidence weights. A scoring strategy is also used to map the model's output to a predefined score range.

Experiments in both transductive and inductive settings show that ULRA outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches. While still lagging behind fully supervised methods due to the lack of strong supervision, ULRA represents a significant advancement in unsupervised essay scoring.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore o codificatore-PTR possono essere migliorati con l'addestramento in una combinazione di varie lingue.</sample>
    <sample id="84">Shwai He presented "PAD-Net: An Efficient Framework for Dynamic Networks" at ACL 2023, addressing the limitations of fully dynamic networks. While dynamic networks, which adapt architecture or parameters based on input, often outperform static networks, fully dynamic approaches suffer from excessive parameter usage. Replacing static layers with dynamic ones can significantly increase model size, hindering practical application.

PAD-Net proposes a solution by introducing partially dynamic networks, partitioning parameters into dynamic and static components. The framework utilizes Iterative Mode Partition to identify and convert redundant dynamic parameters into static ones, minimizing parameter count without sacrificing performance. Two scale factors control the intensity of dynamic and static modes, and constraints are applied to accelerate training.

Experiments demonstrate PAD-Net achieves superior performance compared to both static and fully dynamic networks, while significantly reducing parameters and computation. Ablation studies optimized dynamic ratios for specific components like Dynamic Convolution and Mixture of Experts. Compared to network pruning, PAD-Net maintains static parameters, leading to better results and more discriminating outputs. Future work includes extending PAD-Net to other networks, hardware-friendly structures, and incorporating additional parameter modes.</sample>
    <sample id="85">"Make a chocolate cake."</sample>
    <sample id="86">By visualizing the embeddings of sentences on four datasets using PCA, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">DrBERT was built using RoBERTa as its base and leveraging continual pre-training strategies, including models based on CamemBERT and PubMedBERT.</sample>
    <sample id="88">GPT-4 is less aligned with non-binary people.</sample>
    <sample id="89">"For example, if we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the conventional reliance on native speakers for NLP data annotation, particularly crucial with the advancement of language models. We investigate the feasibility of utilizing language learners, who are abundant even for low-resource languages, as annotators. Our proof-of-concept study involved English, Korean, and Indonesian, across four GLUE benchmark tasks (sentiment analysis, NLI, NER, and MRC). We categorized learners into proficiency levels (basic, intermediate, advanced) and compared their annotations to those of native speakers. Experiments included pre-tests, annotation tasks with varying difficulty levels and resource support (dictionaries, machine translation), and post-tests to assess learning effects.

Results demonstrate that learner-annotated labels are highly accurate, especially for simpler tasks, and achieve near-native speaker accuracy when aggregated via majority voting. Training simulations revealed that language models trained on learner annotations achieved 95% of ground truth performance, sometimes even surpassing models trained on native speaker data. Furthermore, annotation tasks demonstrably improved learners' language proficiency and vocabulary. This work proposes a novel data construction approach for low-resource languages, moving beyond translation-based methods, and highlights the potential of language learners to significantly contribute to NLP research, broadening accessibility and facilitating benchmark dataset creation.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">Other treeless models on the COGS benchmark.</sample>
    <sample id="93">They are the first author's advisors.</sample>
    <sample id="94">This paper introduces "Embedding Marker," a novel backdoor-based watermark method designed to protect the copyright of embedding as services (EaaS) powered by large language models. EaaS, like OpenAI's GPT embedding API, are vulnerable to model theft via embedding learning.

Embedding Marker addresses this by embedding a watermark during service provision. It involves two key steps: watermark injection and copyright verification. First, a "trigger set" of moderately frequent words is selected. During injection, the provider modifies embeddings based on the number of triggers in a user's input, proportionally weighting a "target embedding."

Verification involves constructing backdoor and benign datasets. The provider requests embeddings from a potentially stolen service using these datasets and calculates cosine and L2 similarities between the requested embeddings and the target embedding. A statistical test (KS test) is also applied. Experiments on four datasets (AG News, MIND, SST2, Enron Spam) demonstrate high detection performance with minimal impact on embedding utility. Visualization using PCA confirms the covertness of the watermark, making it difficult to distinguish between watermarked and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato del primo anno al Carnegie Mellon University e oggi vi presenterò il nostro lavoro NLPositionality, che caratterizza i bias di progettazione dei dataset e dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Immaginiamo che tu stia lavorando per un giornale e stai esaminando i commenti sotto il tuo articolo per rimuovere contenuti tossici. Potresti rivolgerti a un'API popolare come Prospective API per il rilevamento della tossicità, e questo funziona bene se sei Carl Jones. Ma non è lo stesso per Aditya Sharma. Prospective API non è così sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione in cui vediamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni.

Questi bias di progettazione, come quello che abbiamo appena visto, potrebbero derivare dalla posizione degli scienziati dell'elaborazione del linguaggio naturale e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati perché può modificare le decisioni che i ricercatori prendono.

Quindi, una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione? E non stiamo dicendo che i modelli stessi o i dataset stessi hanno identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre.

I lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come le lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizione del modello. Tuttavia, questi lavori non confrontano gli utenti finali con i dataset e i modelli stessi e lo studio della posizione dei dataset e dei modelli è sempre più importante man mano che le attività di elaborazione del linguaggio naturale diventano più soggettive e socialmente orientate. È difficile caratterizzare come questi posizionamenti siano distorti perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.

Per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni con utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due fasi principali. La prima fase è quella di ri-annotare i dataset con annotatori diversi. Dobbiamo farlo guardando la demografia degli annotatori dei dataset originali, perché di solito solo pochi annotatori annotano ogni istanza e la demografia viene raramente raccolta e condivisa. Quindi, optiamo per ri-annotare i dati per ottenere molte annotazioni per istanza e per ottenere un ricco set di dati demografici.

Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R, e quindi il nostro framework differisce dalla letteratura sulla discrepanza degli annotatori confrontando gli utenti finali con i modelli e i dataset, previsioni ed etichette, invece di guardare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori.

Il nostro framework è ampiamente abilitato tramite Lab in the Wild e una piattaforma di crowdsourcing online per collaboratori HCI. In Live in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi. Rispetto alle piattaforme come M Turk, che hanno in gran parte partecipanti dagli Stati Uniti o dall'India, Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Ospitiamo 2 attività su lab in the wild, una delle quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di chimica sociale e quindi scriveranno quanto una situazione sia socialmente accettabile. In seguito, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con gli altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4.

Quindi replicare un setup molto simile per l'attività di rilevamento della tossicità e dell'odio, dove leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di odio. Quindi abbiamo confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4.

Il nostro studio ha accumulato alla fine oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi.

Ora siamo meglio attrezzati per rispondere a chi si allineano di più i dataset e i modelli di elaborazione del linguaggio naturale. Scopriamo che c'è una posizione nell'elaborazione del linguaggio naturale. Ad esempio, scopriamo che i dataset e i modelli si allineano maggiormente ai paesi di lingua inglese.

Per l'analisi dell'accettabilità sociale di GPT 4, scopriamo che si allinea maggiormente ai paesi di lingua confuciana e inglese. Scopriamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria.

Tuttavia, quando i modelli e i dataset si allineano a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli si allineano meno alle persone non binarie rispetto alle controparti maschili e femminili. Lo troviamo anche nell'analisi dell'attività di accettabilità sociale di GPT 4 e nell'analisi di Dynahate.

Quindi, dato che c'è una posizione nell'elaborazione del linguaggio naturale, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è quella di tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. La nostra seconda raccomandazione è quella di condurre ricerche sull'elaborazione del linguaggio naturale con la lente del perspectivismo. La nostra terza raccomandazione è quella di costruire dataset e modelli specializzati all'interno di comunità specifiche. Un buon esempio di questo è l'iniziativa Masakhani.

Vogliamo sottolineare che l'elaborazione del linguaggio naturale inclusiva non significa semplicemente fare in modo che tutte le tecnologie funzionino per tutti.

Questo conclude la nostra presentazione. Ma se vuoi saperne di più, non esitare a consultare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">Tre.</sample>
    <sample id="98">The presentation highlights the dilemma: sanitizing training data to remove political opinions risks censorship and determining neutrality is difficult.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". Nella vita di tutti i giorni, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo dopo passo sotto forma di script orientati all'obiettivo. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare una torta". E hanno dimostrato che i modelli linguistici di grandi dimensioni possono scomporre efficacemente gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane ancora in gran parte inesplorata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diversi vincoli agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli sfaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto le capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati con il coinvolgimento umano utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi conduciamo un'analisi dettagliata per indagare sul motivo per cui i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma non è possibile garantire la fedeltà ai vincoli. Approfondiamo un argomento più granulare di categorie di vincoli definiti in wikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta un'elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di sovra-generare e poi filtrare per migliorare la qualità della generazione. Mostriamo innanzitutto i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti iniziali. Quindi, InstructGPT sovra-genera K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo di destinazione. Conserviamo solo lo script se l'obiettivo di destinazione ottiene il punteggio più alto nell'insieme di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente le capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare le capacità di pianificazione di modelli più piccoli e specializzati. La creazione del set di dati è un passo essenziale a questo scopo. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale del set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica, per distillare set di dati di pianificazione linguistica vincolata dai modelli linguistici di grandi dimensioni. Applichiamo il nostro metodo per creare un set di dati di pianificazione linguistica vincolata, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dell'insieme di validazione e test, chiediamo a lavoratori provenienti da fonti esterne di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un elevato pluralismo negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 addestrato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi quando vengono addestrati correttamente su set di dati adatti. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo le capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppiamo un metodo di sovra-generazione e poi filtraggio per i modelli linguistici di grandi dimensioni. Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il set di dati CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Troverete maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering, requiring only 128 training examples—a significant improvement over existing methods that need thousands. It combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a few-shot language model-based reranker.

The core idea is to score candidate chains based on the likelihood of the question given a specially constructed prompt. This prompt includes chain documents marked with indicator tokens and an instruction designed to elicit reasoning from the language model (e.g., "Read the previous documents and ask a question"). Techniques like instruction search and sampling, and temperature scaling are explored to optimize the prompt and scoring.

Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers. Ablation studies confirm the importance of each component. When paired with a reader model (ELECTRA-Large), PromptRank achieves strong downstream multi-hop QA performance, closely matching the performance of MDR. The research highlights the effectiveness of using language models for ranking candidate paths and emphasizes the crucial role of the instruction in guiding the model's reasoning.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile a quella dei sistemi all'avanguardia.</sample>
    <sample id="102">Applicability to embedding as services, no degradation of embedding utility, covertness, and transferability.</sample>
    <sample id="103">14</sample>
    <sample id="104">Over 16,000 annotations from over 1000 annotators were amassed.</sample>
    <sample id="105">Cosine similarity, L2 similarity, and a KS test p-value.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to address the challenge of information seeking with multiple constraints or preferences, often expressed implicitly as set operations. The dataset is motivated by scenarios like a zoologist identifying an unknown reptile species and a reader seeking their next book.

QUEST comprises over 3,000 entity-seeking queries derived from Wikipedia categories (films, books, plants, and animals) using set operations (intersection, complement). Human annotators paraphrase and validate queries for fluency, then verify entity relevance and mark evidence spans within documents corresponding to different query constraints.

The dataset poses a difficult retrieval problem, requiring systems to find multi-answer sets where evidence for relevance can be scattered throughout a document. Initial evaluations using sparse and dense retrievers, and a T5 reranker, reveal significant room for improvement, with particularly low performance on queries involving set intersection and difference.

QUEST aims to facilitate research into building more effective systems for handling selective information needs, mirroring real-world scenarios where users express complex information requirements. The authors encourage researchers to explore the dataset and attend their ACL presentation.</sample>
    <sample id="107">Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) and Encoder-Decoder models (Multilingual Pretrained Encoder-Decoder Models) were evaluated. Training in a mixture of various languages improved Encoder-Decoder and Encoder-PTR performance.</sample>
    <sample id="108">The ACL 2023 paper by Koustav Sinha and colleagues investigates the robustness of language model acceptability judgments when considering longer contexts. Traditional Minimal Pair Paradigm (MPP) evaluations, which compare acceptable and unacceptable sentences, often use short sentences and don't account for how context influences a model's judgment.

The researchers revisited MPP by creating longer sequences, simulating extended context windows. They constructed sentences by adding prefixes (acceptable or unacceptable) from the same or different datasets to the original query pairs. They found that judgments are largely robust with irrelevant context (like Wikipedia sentences) even up to 1024 tokens. However, when prefixes from the same dataset are used, judgments significantly shift based on whether the prefix is acceptable or unacceptable, especially when the grammatical structure matches.

This sensitivity to shared syntactic and semantic features suggests that current MPP evaluations might not fully capture a language model's broader understanding. The study highlights that language models are influenced by latent features across sentences, and longer context windows can amplify these effects, potentially impacting the reliability of acceptability judgments. The paper encourages a re-evaluation of how language models are assessed for acceptability, particularly as models gain larger context windows.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large dataset of instructions, inputs, and outputs entirely automatically, without human annotation. The method leverages a pre-trained GPT-3 model, initially seeded with examples from the Super-Natural Instructions dataset. The model then generates new instructions and corresponding inputs, followed by outputs based on those instructions. To enhance diversity, the system also creates paraphrases of the generated instructions.

The resulting dataset comprises 64,000 examples, expanding to 240,000 with paraphrases. Analysis reveals a correctness rate exceeding 50%, with even incorrect examples proving valuable for training. The dataset showcases remarkable creativity and diversity, encompassing tasks beyond traditional NLP benchmarks, such as verifying scientific experiments and inventing new words.

The utility of "Unnatural Instructions" is demonstrated by fine-tuning an 11 billion-parameter T5 model, which outperforms existing instruction-tuned models (T0++ and Tk-instruct) across multiple benchmarks. Considering the cost-effectiveness of automated generation, training on "Unnatural Instructions" proves superior to training on the manually curated Super-Natural Instructions dataset. This work highlights the potential of language models to autonomously generate high-quality, diverse training data, offering a faster and cheaper alternative to human annotation.</sample>
    <sample id="111">Gli autori presumono che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi vi presenterò il nostro articolo "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di Riconoscimento di Entità Nominate, o NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione CoNLL-2003. Abbiamo quindi ottimizzato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E ultimo ma non meno importante, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalmente generalizzano meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che, di solito, modelli più grandi portano a una migliore generalizzazione. E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di ottimizzazione fine influenza direttamente le prestazioni di un compito downstream. Anche qui abbiamo scoperto che più esempi di ottimizzazione fine portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte e questo si manifesta solitamente come rendimenti decrescenti su un nuovo set di test. La seconda ipotesi è il temporal drift, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo visto che dal grafico a destra, la linea di migliore adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, il che significa che non si osserva un overfitting adattivo in questo caso. E il temporal drift? Per il temporal drift, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni diminuiscono con un divario temporale più ampio e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il temporal drift. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande e più esempi di ottimizzazione fine. Questi vanno di pari passo, non possiamo avere un solo ingrediente ma scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni è causato dal temporal drift e, sorprendentemente, non è causato dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?". E abbiamo scoperto che la risposta è in realtà un sì deciso. Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare la generalizzazione dei modelli. E infine, controllate il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The work presented at ACL 2023, "Finding the Pillars of Strength for Multi-Head Attention," from Nanyang Technological University, addresses the parameter inefficiency of large language models (LLMs). While LLMs are revolutionary, their massive size (billions of parameters, vast training data, and long training times) poses deployment challenges.

The research focuses on optimizing multi-head attention, which often contains redundant heads. Existing approaches (homogenization, diversification, and scoring) have limitations. The proposed solution, Grouped Head Attention (GHT), employs a divide-and-conquer strategy.

GHT uses group-constrained training to create distinct groups of attention heads (similar within groups, separate between groups) and a Voting-to-Stay algorithm to prune redundant heads, leaving one per group. This achieves significant parameter compression (up to 90%) without performance loss.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate improvements over state-of-the-art baselines and substantial model compression (32.1% - 16.9%). Further analysis shows a "LITE" model achieving 62% faster inference and 80% reduction in FLOPs. Future work explores task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to further optimize LLMs.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">The ACL 2023 submission "Improving Pretraining Techniques for Code-Switched NLP" addresses the challenge of building computational models for code-switching, a common phenomenon in linguistically diverse communities where sentences mix multiple languages. Existing multilingual models like mBERT and XLM-R struggle with code-switched tasks.

The paper introduces SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching. Unlike standard MLM, SwitchMLM only masks tokens at switch-points (transitions between languages). To overcome the need for language identification (LID) tags, a surrogate method called FrequencyMLM is proposed, using negative log likelihoods to estimate language.

Furthermore, the authors propose architectural modifications, including residual connections from intermediate layers rich in switch-point information to the final layer, and an auxiliary LID-based loss to encourage language encoding in these intermediate layers.

Experiments on sentiment analysis demonstrate that the combined method (Switch/FrequencyMLM + ResBERT + auxiliary loss) outperforms existing approaches. Probing experiments, using linear and conditional probing, confirm that the proposed methods increase the amount of switch-point information in model representations, validating the effectiveness of the approach. The research highlights the importance of tailoring pretraining techniques to the unique characteristics of code-switched data.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">The model uses the cross-attention weights.</sample>
    <sample id="121">Direct references include saying the name of the song ("Easy on Me") or its position ("the first one").</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on MultiInstruct, a new benchmark dataset for multi-modal instruction tuning. Their work addresses the gap in instruction tuning research, which has largely focused on language-only tasks, by investigating its effectiveness in multi-modal settings.

MultiInstruct comprises 62 diverse multi-modal tasks across 10 categories, derived from 21 existing datasets, each with five expert-written instructions. They used OFA, a unified multi-modal pre-trained model, as their base model and trained it on 53 of the tasks. They evaluated performance on seen tasks, unseen NLP tasks, and measured "sensitivity"—the model's consistency across different instruction wordings.

The results demonstrate that instruction tuning significantly improves OFA's performance on multi-modal tasks and that transfer learning from natural instruction datasets enhances both performance and sensitivity. Using multiple instructions (five versus one) also improved performance and reduced sensitivity. The researchers are expanding the dataset to include 150 additional vision-language tasks, which will be publicly released. The work highlights the benefits of multi-modal instruction tuning and introduces a new metric for evaluating model robustness.</sample>
    <sample id="124">This presentation introduces "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The researchers identified three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event, noting that previous research overemphasized the second level.

They created the TempReason dataset, encompassing all three levels and a broad temporal range, to comprehensively evaluate LLMs. Experiments using Closed Book QA, Open Book QA, and a novel Reasoning QA setting revealed biases in existing models like ChatGPT, particularly in month prediction and across different time periods.

To address these limitations, they proposed a training strategy: Temporal span extraction pre-training and time-sensitive reinforcement learning. This resulted in TempT5, which outperformed other models, including ChatGPT and fine-tuned versions of FLAN-T5 and T5, on the TempReason benchmark.

The study highlights the need to overcome temporal reasoning biases in LLMs and suggests future work focusing on addressing data imbalance and improving performance across all time periods. The TempReason dataset and training paradigm offer valuable resources for advancing LLM temporal reasoning capabilities.</sample>
    <sample id="125">The text does not specify the number of authors.</sample>
    <sample id="126">No, it was considered as the Translate-Test setting, one of six settings for training and evaluation.</sample>
    <sample id="127">This presentation introduces "Large Language Models Are Reasoning Teachers," a paper exploring how to transfer reasoning abilities from large language models (LLMs) to smaller, more deployable models. The core problem addressed is that chain-of-thought reasoning, a technique enabling LLMs to solve complex tasks, typically requires massive models like GPT-3, limiting its practicality.

The proposed solution involves using large LLMs as "teacher" models to generate step-by-step solutions for complex tasks. These solutions are then used as training data to fine-tune smaller "student" models. A key innovation is "Diverse Reasoning," which generates multiple, slightly different reasoning paths from the teacher model using stochastic temperature sampling, leading to more robust student training.

Experiments across 12 tasks demonstrate that this "fine-tuned CoT" method significantly outperforms prompt-based baselines and even vanilla fine-tuning, even with student models as small as 0.3 billion parameters. The research highlights the scalability of the approach, noting that performance can be further improved by increasing dataset size, using a better teacher model, or employing a larger student model, while acknowledging the trade-offs between development and inference costs. The authors provide code and data, including OpenAI inference costs, encouraging further research and exploration of this distillation technique for transferring emergent abilities to smaller models.</sample>
    <sample id="128">Akshatha and Martin present "The KITMUS Test," a diagnostic tool for evaluating knowledge integration in natural language understanding models. These models rely on both knowledge acquired during pretraining (pretrain-time knowledge) and knowledge provided during inference. KITMUS focuses on coreference resolution, requiring models to link pronouns to the correct entities, which necessitates both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases").

The test suite features three settings: "Background-Pretrain" (background knowledge in pretraining), "Background-Both" (background and entity-specific knowledge at inference), and "Background-Inference" (both knowledge types only at inference). The "Background-Inference" setting simulates scenarios where necessary background knowledge is absent from pretraining data.

Experiments with human participants and coreference resolution models (C2F and BERT4Coref) reveal that models initially struggle without KITMUS-specific training, relying on superficial cues. Training on KITMUS improves performance, but even the best models struggle to reliably integrate background knowledge provided solely at inference time, particularly with fictional or novel concepts. The paper highlights the need for improved knowledge integration capabilities in NLU models and provides a dataset and code for further research.</sample>
    <sample id="129">Black women.</sample>
    <sample id="130">Transformer models do not generalize well.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Two.</sample>
    <sample id="133">Multi-modal.</sample>
    <sample id="135">ABC-Eval is a new method for evaluating conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of traditional human evaluation methods like Likert scales and pairwise comparisons, which often lack granularity. ABC-Eval focuses on annotating specific behaviors exhibited by dialogue models, such as irrelevance, contradictions, hallucinations, and empathy.

The method was tested on four state-of-the-art models across 100 conversations, comparing ABC-Eval with Likert ratings and pairwise comparisons. Results showed ABC-Eval labels were more reliable (higher inter-annotator agreement) and predictive of overall conversation quality.  Specifically, measuring contradictions proved highly indicative of quality.

Furthermore, ABC-Eval metrics captured unique aspects of chat quality, explaining over 25% of conversation quality compared to less than 4% explained by Likert metrics. The study revealed that current models still struggle with common sense violations (20%), irrelevant information (15%), and contradictions (10%).  The researchers hope ABC-Eval will provide a more precise and reliable tool for evaluating and tracking progress in conversational AI, enabling more meaningful comparisons between models.</sample>
    <sample id="136">Jasivan presented FERMAT, a new evaluation set designed to improve our understanding of numerical reasoning in language models. Current benchmarks relying on accuracy scores are insufficient for diagnosing strengths and weaknesses in mathematical abilities, especially in accessible models (around 3 billion parameters).

FERMAT addresses this by evaluating models across number understanding (integers, decimals), mathematical operations (simple vs. combined), and training dependency. The set consists of math word problems extracted from educational resources, with variations in number representation. Initial zero-shot evaluations revealed poor performance across all aspects, highlighting the limitations of existing benchmarks.

Fine-tuning with 200,000 generated examples improved performance, but even with seen expressions, accuracy remained below 50%, suggesting a lack of memorization and the importance of linguistic nuances. Further investigation showed that diversifying training templates with datasets like GSM8K and AQUA significantly boosted performance, emphasizing the need for both language and mathematical diversity.

FERMAT aims to provide a more informative alternative to traditional benchmarks, pinpointing areas for improvement like number encoding and tokenization, ultimately contributing to better numerical reasoning capabilities in language models.</sample>
    <sample id="137">The paper introduces "Tell2Design," a new dataset and task focused on language-guided floor plan generation. Existing text-conditional generative AI models excel at creating realistic artwork but struggle with design tasks requiring adherence to specific constraints and user requirements. Tell2Design addresses this gap by enabling users to "tell" instructions to generate floor plans.

The dataset comprises 5,051 human-annotated and approximately 76,000 artificially generated language instructions describing floor plan components (semantics, geometry, and topology). The authors propose a sequence-to-sequence model, leveraging a transformer-based encoder-decoder architecture initialized with a pre-trained T5 language model, to generate floor plan layouts from these instructions. This approach treats room bounding boxes as a structured target sequence.

Experiments on the T2D dataset demonstrate that their model significantly outperforms existing text-conditional image generation baselines, achieving high IoU scores. However, a language distribution gap between artificial and human instructions poses a challenge, mitigated by a warm-up phase using artificial data before training on human instructions. The study highlights the potential of language-guided design generation and establishes Tell2Design as a valuable resource for future research in this area.</sample>
    <sample id="138">Reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Crowd-sourced workers were asked to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">They support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation.</sample>
    <sample id="142">Ciao! Vi parlerò del nostro lavoro su "Risoluzione di Riferimenti Indiretti per la Selezione di Entità", in cui presentiamo l'AltEntities Corpus. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa: "Intendevi 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. O le pronunce sono troppo simili e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "la più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking degli LLM per la comprensione delle entità. Non siamo a conoscenza di un dataset pubblico più ampio per questo compito, quindi ne abbiamo raccolto uno utilizzando annotazioni di crowd. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità utilizzando un setup di completamento di un cartone animato. Il cartone animato ha tre fumetti. Nel primo fumetto, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?". E con questo, Bob stabilisce il contesto del dialogo. Nel secondo fumetto, Alice dice: "Intendevi 'Easy on Me' o 'I Gotta Feeling'?", che è la domanda alternativa. E nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "la più recente". Forniamo il primo e il secondo fumetto automaticamente, ma il terzo viene riempito dall'annotatore. Il primo fumetto è scelto da alcuni prompt manuali per dominio. Il secondo, che è la domanda alternativa, viene generato come segue. Usiamo sempre un semplice modello. Intendevi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è solitamente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente qualcosa sulle entità. Quindi, quello che facciamo è mostrare alcune conoscenze di base su queste due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ciascuna canzone e leggere qualcosa su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca di Google per la canzone "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi, abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverle usando tre o cinque espressioni di riferimento indirette. Ad esempio, "quella senza parole", "non quella con il ragazzino di 12 anni", o "la fittizia", o "viene dall'Azerbaigian", e così via. L'AltEntities Corpus ha 6.000 domande alternative su tre domini e 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta, intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, allora l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">The approach is compared with Wait-k strategy, Local Agreement, and state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The affiliation is not explicitly stated in the provided text.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Dialogue summarization, while benefiting from large-scale pre-trained language models, suffers from common errors like factual omissions, hindering real-world application. This paper investigates the prevalence and characteristics of omission in dialogue summarization, revealing that approximately 70% of generated summaries contain omissions across various domains and models. The omitted information is randomly distributed throughout the dialogue, highlighting the difficulty in identifying key information.

To address this gap, the authors introduce the OLDS dataset, the first publicly available dataset with high-quality omission labels for dialogue summarization, built upon five existing benchmarks. The dataset utilizes diverse candidate summaries generated by different models and employs an automatic method, validated by human evaluation, to produce omission labels. Experiments with three baseline architectures for omission detection demonstrate the task's challenging nature, with F1-scores around 50%.

Furthermore, the study explores refining summaries by incorporating detected omissions using a post-editing method. Results show significant performance improvements, validating omission detection as a valuable task and refinement as a promising avenue for enhancing dialogue summarization quality. The work underscores the importance of addressing omission to improve the reliability and usability of dialogue summaries.</sample>
    <sample id="147">Tre.</sample>
    <sample id="148">Simultaneous speech translation, or SimulST, is the process of translating spoken language into text in another language in real time, enabling cross-language communication. Current SimulST models have problems such as requiring specific architectures, long and complicated training procedures, and the need to train and maintain multiple models for different latency regimes. Our solution is to use existing offline ST models without retraining or adopting specific architectures for SimulST, using only one model for each latency regime and managing latency through specific parameters. We leverage the knowledge already acquired by the model through the cross-attention mechanism between audio input and textual output. Our solution is to propose EDAtt, or Encoder-Decoder Attention, a strategy for deciding whether to emit a partial translation based on where attention points. A word is emitted if the attention is not concentrated, meaning its sum is below a certain threshold alpha towards the last lambda speech frames, indicating that the received information is stable enough. We compare EDAtt with popular strategies applied to offline models (Wait-k and Local Agreement) and state-of-the-art architectures specifically designed for simultaneous pre-translation. The results on German show that EDAtt outperforms all strategies applied to offline models and is the fastest when considering actual or computational time. The code and models are open source to facilitate reproducibility.</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">The ACL paper introduces MeetingQA, a new extractive question answering dataset built on meeting transcripts. Millions of meetings generate vast amounts of transcripts, a domain largely untapped by NLP research beyond summarization and action item extraction. MeetingQA addresses this gap by focusing on the inherent question-answer dynamics within meetings, where participants pose questions that spark detailed discussions.

The dataset, derived from the AMI corpus, contains 7.7K questions, with 30% being unanswerable and a significant portion featuring multi-span and multi-speaker answers, as well as rhetorical questions and disagreements. Questions are typically longer and open-ended. Human performance on the test set achieves an F1 score of 84.6.

The paper explores various approaches, including context retrieval for short-context models, single-span and multi-span QA models, and data augmentation using silver annotations from the MediaSum dataset. Results reveal a substantial gap between model performance and human capabilities, particularly in zero-shot settings. Short-context models like RoBERTa outperform long-context models, while single-span models show comparable or slightly better performance than multi-span models. Error analysis highlights challenges in identifying rhetorical questions and speaker attribution, indicating significant room for improvement in meeting QA. MeetingQA presents a challenging and valuable resource for advancing QA research in the meeting domain.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e io e il mio collega Zhiyang presenteremo la nostra ricerca su MultiInstruct, che migliora l'apprendimento zero-shot multimodale tramite l'instruction tuning. Con i progressi dei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di eseguire attività mai viste in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si è concentrata sul miglioramento delle prestazioni zero-shot sulle attività solo linguistiche, mentre i compiti di visione artificiale e multimodali sono stati trascurati. Pertanto, in questo lavoro vogliamo indagare se l'instruction tuning di modelli pre-addestrati multimodali può effettivamente migliorare la generalizzazione a compiti multimodali mai visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra l'elaborazione del linguaggio naturale e i compiti multimodali. Esistono più di 1600 attività solo linguistiche. Tuttavia, non esiste un dataset multimodale di istruzioni di grandi dimensioni disponibile pubblicamente. Pertanto, questo ci motiva a costruire un dataset di instruction tuning multimodale. Qui presentiamo MultiInstruct, il primo benchmark dataset di instruction tuning multimodale che consiste in 62 compiti multimodali diversi che coprono 10 ampie categorie. Questi compiti derivano da 21 dataset open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare sull'instruction tuning multimodale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per linguaggio, token immagine e le coordinate di un bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare l'elaborazione di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequence-to-sequence unificato. In cui il testo di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'instruction tuning multimodale. Per il dataset di training, utilizziamo 53 compiti da 9 gruppi per il training e campioniamo 10.000 istanze per compito. Per il testing, riserviamo l'intero gruppo di ragionamento di buon senso per il testing e selezioniamo 5 compiti aggiuntivi dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nella suddivisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla suddivisione di test delle istruzioni naturali come compito mai visto per l'elaborazione del linguaggio naturale. Utilizziamo il modello OFA large pre-addestrato come modello di base. Durante il training, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione. Durante il test per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo le prestazioni minime e massime e la deviazione standard delle prestazioni su tutti i 5 esperimenti. Se il compito è un compito di classificazione multimodale, riportiamo l'accuratezza. Se è un compito di generazione multimodale, riportiamo Rouge-L. Per i compiti di elaborazione del linguaggio naturale, riportiamo Rouge-L anche noi. Introduciamo anche una metrica di valutazione aggiuntiva chiamata sensibilità. Questo misura la capacità del modello di produrre in modo coerente gli stessi output per lo stesso compito indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'instruction tuning può migliorare significativamente le prestazioni di OFA sui compiti multimodali visti. Inoltre, il transfer learning dai dataset di istruzioni naturali può avvantaggiare l'instruction tuning. Possiamo vedere che, all'aumentare del numero di compiti, il modello ottiene prestazioni migliori e, nel contempo, una sensibilità inferiore. Abbiamo anche fatto un esperimento. Utilizziamo un'istruzione rispetto a 5 istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Possiamo anche vedere che il transfer learning dai dataset di istruzioni naturali può aiutare OFA a ottenere una sensibilità molto migliore. Possiamo anche vedere che il transfer learning dai dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni naturali. Nel complesso, proponiamo il primo dataset di instruction tuning multimodale su larga scala che ha significativamente migliorato la capacità zero-shot di OFA, ed esploriamo diverse tecniche di transfer learning e ne mostriamo i vantaggi. Abbiamo progettato una nuova metrica chiamata sensibilità. Un'altra cosa, stiamo raccogliendo un dataset di instruction tuning multimodale ancora più grande con circa 150 ulteriori attività vision language e lo rilasceremo. Questo è un codice QR per i nostri dati e il nostro modello. Grazie.</sample>
    <sample id="152">Frederick Riemenschneider presented work on new language models for classical philology, addressing limitations of existing models (monolingual, BERT-only, lacking robust evaluation). The project aimed to create comparable, state-of-the-art models, explore architectures, and introduce multilingual capabilities.

Two monolingual Ancient Greek models were developed: GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder). Parallelly, PhilBERTa and PhilTa were created as multilingual equivalents (Greek, Latin, English). A novel pre-training corpus for Ancient Greek was built by identifying and re-OCRing texts from the Internet Archive using stop word analysis.

Benchmarking on part-of-speech tagging, dependency parsing, and lemmatization revealed significant performance improvements over existing models for both Greek and Latin. Notably, GreTa's encoder initially underperformed but eventually approached encoder-only model performance. Encoder-decoder models excelled in lemmatization. Semantic and world knowledge probing showed strong performance, though multilingual models didn't significantly outperform monolingual ones.

The research highlights new models, a high-quality dataset, architectural insights (T5 encoder behavior), and the implications of multilinguality for classical language processing.</sample>
    <sample id="153">Here's a brief summary of the presentation (approximately 200 words):

Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team presented research on addressing ambiguities in text-to-image generative models. The core problem is that ambiguous prompts can lead to images that don't reflect the user's intended meaning.

The research introduces a pipeline to tackle this. First, a benchmark dataset (modified LAVA corpus) was created to cover various ambiguity types. Then, a prompt disambiguation framework is used, employing two methods: generating clarifying questions for the user to answer or generating multiple visual interpretations for the user to choose from. This process creates a disambiguated prompt.

Next, the original and disambiguated prompts are fed into a text-to-image model to generate images. An automatic evaluation framework, utilizing a Visual Question Answering (VQA) model, assesses the faithfulness of the generated images to the user's intention. The VQA model is given the image and a question reflecting the user's intention, determining if the image satisfies that intention.

Findings indicate disparities in resolving different ambiguity types, but overall, the framework improves faithful image generation. The automatic evaluation framework correlates well with human evaluation, demonstrating its reliability. The paper details these findings and further discussions.</sample>
    <sample id="154">University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The paper introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to dialogue summarization that addresses limitations in existing methods. Current techniques often rely on pre-computed static graphs derived from external linguistic tools, which are prone to errors and inflexible.

SDDS overcomes these issues by integrating both static and dynamic graph structures. It begins by encoding utterances using an utterance encoder and constructing static graphs through four heuristic methods: Discourse Parsing Graph, Key Co-occurrence, Speaker Relationship Modeling, and Utterance Position Graph. These methods capture discourse relations, keyword overlap, speaker interactions, and positional information.

The core innovation lies in the Static-Dynamic Graph module, which fuses the static graphs and then employs a dynamic graph module using multi-head attention to learn semantic relationships between utterances. Finally, a pre-trained language model, enhanced with a graph attention layer, generates the summary by fusing the static and dynamic dialogue structures. This dual cross-attention mechanism incorporates graph representation into the generation process. The model aims to improve dialogue summarization accuracy and adaptability by dynamically learning relationships within the dialogue context. The code and data are publicly available on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS presented "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenges of coreference resolution in lengthy texts. Coreference resolution involves linking mentions to their corresponding entities, a task traditionally hampered by quadratic computational complexity. Cache-based methods offer a linear solution, but standard LRU eviction policies struggle with topic shifts in long documents, leading to high cache misses.

The proposed Dual Cache tackles this by employing two caches: a local cache (LRU) for local entities and a global cache (LFU) for globally relevant entities. The model classifies mentions, adding them to the appropriate cache based on frequency. When caches are full, LRU and LFU policies trigger evictions.

Evaluations on benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate Dual Cache's superior performance compared to baselines, even those with unbounded memory. Notably, the performance gap widens significantly in book-level documents. Dual Cache also reduces cache misses substantially. The research highlights a favorable performance/cost ratio, establishing Dual Cache as a cost-effective solution for coreference resolution in long documents.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla nostra presentazione del paper di ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, riprendiamo il paradigma delle coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere la grammaticalità come in BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come le coppie CrowS. In questo paradigma, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi mostrare una frase accettabile o non grammaticale. E si spera che il modello, in sostanza, assegni una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettabilità del modello verso frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno generando frasi sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli attraverso l'intera finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di riprendere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Questo è l'approccio. Ciò che facciamo è rivedere i dataset stessi e ricreare frasi scegliendo frasi accettabili o non accettabili da quei dataset per simulare queste sequenze più lunghe. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset BLiMP dal caso dell'Isola Adverbiale. E quello che facciamo è ricreare sequenze più lunghe che siano accettabili e che abbiano la stessa struttura grammaticale. Estraiamo frasi grammaticali dall'Isola Adverbiale e le aggiungiamo come prefisso sia alla query accettabile che alla query non accettabile. Possiamo fare lo stesso scegliendo frasi non accettabili dalla stessa struttura corrispondente, e questo può essere utilizzato per testare l'accettabilità del modello. Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è quello che chiamiamo scenario di mismatch. Quindi le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset con cui si sta valutando. Possiamo fare lo stesso per il caso di non accettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, ad esempio se il contesto proviene da un sottoinsieme diverso del dataset, o se è completamente irrilevante rispetto alla frase che stiamo esaminando. Come se la cava il modello? Innanzitutto, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti rispetto alla coppia di query corrente, e lì scopriamo che i giudizi MPP sono per lo più robusti per una lunghezza del contesto arbitraria. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT 2. E qui vediamo nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili. Ma quando si corrisponde la struttura, cioè quando si scelgono le frasi dalla stessa fenomenologia in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o non accettabile. Questo e questo è molto grande, questo effetto aumenta attraverso la lunghezza del contesto e probabilmente influenzerà i modelli linguistici più recenti che hanno una finestra di contesto ampia. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico? Abbiamo condotto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. E dopo aver eseguito diverse di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare al modello il suo corso in termini di come ci mostra il giudizio MPP. In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, nel modo in cui la facciamo attualmente con input brevi e a singola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico attraverso l'intera finestra di contesto. Si prega di leggere il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">I token di input vengono mappati a un multiset non ordinato di token che appariranno nell'output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">Weak supervision uses cheaper, noisy labels (like heuristic rules or crowdsourcing) instead of manual annotations.</sample>
    <sample id="165">The paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" introduces LiPoR, an unsupervised learning method for abductive reasoning. Abductive reasoning aims to find plausible explanations that bridge the gap between a context and an outcome. Current supervised approaches struggle with noisy and subjective annotation of explanations, with crowd workers often disagreeing.

LiPoR addresses this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context. This unsupervised objective doesn't require knowing which explanations are plausible. To further refine the selection, LiPoR incorporates a regularizer based on the principle of mutual exclusivity – explanations in abductive reasoning are typically mutually exclusive. The regularizer penalizes scenarios where too many explanations receive significant probability mass.

The LiPoR objective combines likelihood maximization and mutual exclusivity enforcement. Experiments on the AlphaNLI dataset demonstrate that LiPoR significantly outperforms existing zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement over a strong GPT-3 baseline. The paper's findings suggest that effective abductive reasoning can be achieved without relying on labeled data regarding explanation plausibility.</sample>
    <sample id="166">This paper introduces NDCR, a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Addressing the limitations of existing visual language models that struggle with complex descriptions, NDCR draws inspiration from Divide-and-Conquer strategies and Dual-Process Theory. The framework mimics human cognition by integrating an analogical reasoning "System 1" (Visual-Linguistic Interactor) with a logical reasoning "System 2" (Neural-Symbolic Reasoner).

NDCR first decomposes complex text into simpler propositions using a Proposition Generator. System 1 then interacts visual and propositional information, producing matching scores and reasoning states. System 2, comprising a negation executor and conjunction operation, integrates these states to derive final inferences. The framework combines the outputs of both systems to achieve a comprehensive solution.

Experimental results demonstrate that NDCR significantly outperforms existing baselines. Ablation studies confirm the effectiveness of each module. Case studies showcase the framework's ability to present intermediate inference states, highlighting its interpretable and interoperable processing. The authors suggest that neural-symbolic computation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for enhancing compositional reasoning in large language models.</sample>
    <sample id="167">DEPLAIN-web documents were aligned both manually and with automatic alignment methods.</sample>
    <sample id="168">It was created by collecting data from Reuters News in 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar presented a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," a joint work with Google Translate colleagues. The paper explores the use of PaLM, a 540 billion-parameter language model, for machine translation through systematic prompting strategies.

The study evaluated PaLM's translation capabilities using established MT practices, including current test sets and comparisons against state-of-the-art systems and neural MT metrics, alongside human evaluations. Prompt selection significantly impacts performance; a simple experiment showed BLEURT score differences of up to 40 points based on different prompts.

The research found that a 5-shot prompting strategy, where sentences are marked with their language, proved effective. Example quality outweighed similarity to the source sentence, with examples from curated development data yielding better results than training data. While PaLM nearly matched the performance of Google Translate, specialized MT systems still held a substantial advantage.

Human evaluations revealed PaLM's fluency was comparable to state-of-the-art systems, but accuracy was a key differentiator, with frequent omission errors. Despite accuracy issues, PaLM demonstrated lower "Style/Awkward" scores, indicating fluent output. The paper concludes with recommendations for prompt selection and highlights the potential of LLMs in machine translation.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università della Pennsylvania. Oggi vi presenterò il nostro lavoro "XSemPLR: Semantic Parsing Cross-Linguale in Molteplici Lingue Naturali e Rappresentazioni del Significato". Il semantic parsing è un compito volto a costruire rappresentazioni semantiche di query utente come SQL e Lambda Calculus. Il Semantic Parsing Cross-Linguale è il compito di tradurre query in molteplici lingue naturali in molteplici rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in molteplici lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, eccetera. I modelli di semantic parsing cross-linguale esistenti sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio, ci sono molte risorse per determinate lingue naturali, ma la lingua cinese è assente e manca la copertura per determinate rappresentazioni del significato. La lambda calculus è assente, o vengono valutati solo su determinati modelli neurali. Ad esempio, c'è un solo modello per valutarli. A tal fine, proponiamo XSemPLR. Forniamo un dataset unificato XSemPLR per il semantic parsing cross-linguale in molteplici lingue naturali e rappresentazioni del significato. Contiene 9 dataset in vari domini, 5 compiti di semantic parsing, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. La prima è Translate-Test. Utilizziamo l'API di Google Translate per tradurre dalla lingua di origine alla lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e, durante l'inferenza, traduciamo la query in tedesco utilizzando l'API in inglese e quindi utilizziamo il modello addestrato per prevedere il SQL. Testeremo anche il Modello Monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testeremo anche l'impostazione Monolingue Few-shot addestrando modelli monolingui con solo il 10% dei dati di addestramento. E testeremo il Modello Multilingue che addestriamo con un unico modello multilingue per tutte le lingue. Ad esempio, mettiamo query in tedesco, inglese e cinese insieme per addestrare un modello multilingue. E durante l'inferenza possiamo utilizzare questo modello per tradurre query in tedesco o cinese, eccetera. Consideriamo anche il Cross-lingual Zero-shot e Few-shot transfer. Addestriamo su una lingua di origine e trasferiamo in un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query in inglese o su una combinazione di query in inglese e tedesco Few-shot per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, tra cui Encoder-PTR, che sta per Multilingual Pretrained Encoders with Pointer-based Decoders, come XLM-R + PTR e mBERT + PTR. E valutiamo anche i modelli Encoder-Decoder, come Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che l'Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i dataset. E valutiamo su mT5 e XLM-R + PTR nell'impostazione multilingue. Abbiamo scoperto che l'Encoder-Decoder o l'Encoder-PTR possono essere migliorati addestrandoli in una miscela di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni in inglese diminuiscono in sette dataset e ottengono miglioramenti solo in tre dataset. Credo che questo sia noto come la "Maledizione della Multilinguità". Abbiamo anche confrontato il divario di prestazioni cross-lingua. In questa figura, la linea blu è il Cross-lingual Few-shot transfer. La linea arancione è il Cross-lingual Zero-shot transfer. La linea verde è l'impostazione Monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo scoperto che l'impostazione Zero-shot, il divario di prestazioni nel trasferimento cross-linguale è significativo, e confrontando le linee blu e arancione, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si riduce rapidamente. Abbiamo anche scoperto alcuni altri risultati interessanti. Ad esempio, l'Encoder-Decoder supera i lavori precedenti o ottiene risultati comparabili. Il pre-addestramento sulla lingua naturale inglese può migliorare significativamente le prestazioni del Few-shot sulle lingue naturali di destinazione e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di semantic parsing cross-linguale. In sintesi, abbiamo costruito XSemPLR, un benchmark unificato per il semantic parsing cross-linguale con molteplici lingue naturali e rappresentazioni del significato. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti. E altro ancora. Vi invitiamo a visitare il nostro articolo e il codice. Grazie per aver ascoltato.</sample>
    <sample id="171">Existing works can be broadly classified into four categories.</sample>
    <sample id="172">No, multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset for argument quality analysis, distinguished from existing datasets by its scale, diversity, depth, and reliability. It comprises 35,000 argument-analysis pairs, significantly larger than previous datasets, with a high proportion of arguments sourced from high-quality debate tournaments and expert debaters.

Unlike datasets limited to a small set of pre-selected motions, ArgAnalysis35K utilizes 24 themes identified through expert consultation and circuit experience, capturing a wider range of motions. A key innovation is the introduction of "analysis," a concept encompassing claims, premises, and their connections, providing a more comprehensive explanation of an argument's reasoning.

To address annotator bias, the dataset employs instance-based annotator reliability, selectively removing judgments deemed biased on a per-argument basis. Finally, a relevance model assigns scores indicating how well an argument applies to different themes, recognizing that arguments often have broad applicability beyond a single motion.

The dataset aims to provide a more diverse, reliable, and nuanced resource for argument quality analysis, encouraging further research and feedback.</sample>
    <sample id="175">It addresses this by inducing the alignment as part of the training and approximating the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita esaminando le prestazioni per categoria, ovvero separando le prestazioni in base a diverse demografie o orientamenti politici dei media di notizie.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presented "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, the ability to understand others' mental states, is traditionally tested with false-belief questions like the Sally-Anne test. Despite progress in LLMs, they still struggle with these tasks.

Sclar introduced SymbolicToM, a novel inference-time method using explicit graphical representations of characters' beliefs. These graphs, like BBob and BBob,Alice, represent what different characters believe about the world and each other's beliefs. SymbolicToM leverages existing Natural Language Inference (NLI) and OpenIE models to compute these graphs and efficiently answer ToM questions by converting them into factual queries over the graph.

Experiments demonstrated significant performance gains across various LLMs (GPT-3, Macaw, Flan-T5-XXL) compared to supervised baselines. Notably, SymbolicToM showed strong generalization capabilities on newly created datasets (D₁, D₂, D₃) testing story structure and linguistic diversity, where supervised models struggled. The method even enabled stronger models like GPT-4 to fully solve these challenging datasets, highlighting its effectiveness and interpretability in enhancing LLMs' ToM reasoning abilities.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints, such as "make a chocolate cake" versus the more common "make a cake." The authors define constrained language planning as generating scripts that are both reasonable and faithful to multifaceted constraints. They first evaluate and improve large language models' constrained planning abilities, finding limitations in faithfulness to constraints despite acceptable semantic completeness. To overcome this, they propose an "over-generate-then-filter" method, leveraging InstructGPT to generate multiple scripts and a filter model based on semantic similarity and keyword matching to select the most faithful.

Recognizing the cost of large language models, the authors then introduce CoScript, a dataset of 55,000 specific goals and scripts generated using their method and refined through crowd-sourced validation. CoScript demonstrates high diversity in constraint types.  Experiments show that a smaller T5 model fine-tuned on CoScript outperforms larger language models in constrained language planning, highlighting the value of specialized datasets. The authors believe CoScript will serve as a valuable resource for advancing research in language planning and enabling smaller models to achieve strong performance in this area.</sample>
    <sample id="182">Tropicalismo si riferisce a un tropo che collega le donne latine a immagini "vibranti" e "curvacee".</sample>
    <sample id="183">The authors were inspired by a study that gave prompts to human subjects to surface racial stereotypes, enabling a direct comparison between generated personas and human-written responses.</sample>
    <sample id="184">CXMI (Contextual Mutual Information) and its extension, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is based on crawled medical data (NACHOS), while ChuBERT is based on anonymized data from the Nantes University Hospital.</sample>
    <sample id="187">Two.</sample>
    <sample id="188">Iterative updating involves training the model on only the latest set of data collected in each round of active learning.</sample>
    <sample id="189">The goal of the dataset is to understand users’ language when they want to make a choice.</sample>
    <sample id="190">An attacker may steal the model by learning from the embedding.</sample>
    <sample id="191">Tre.</sample>
    <sample id="192">Yang Luo presented "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of balancing fast convergence and low memory usage in training large language models. Traditional adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from slower convergence due to update errors.

CAME builds upon Non-negative Matrix Factorization (NMF) used in Adafactor, but introduces a novel approach to mitigate these errors. It calculates an "instability matrix" based on the difference between predicted and actual updates, using its square root as a denominator to adaptively adjust the update step. This confidence-guided mechanism aims to reduce the impact of erroneous updates.

Experiments on BERT, GPT-2, and T5 models demonstrated CAME's effectiveness. It achieved significant improvements in validation accuracy compared to Adam and Adafactor, particularly with large batch sizes (8K to 32K). CAME also exhibited comparable performance to baselines on downstream tasks while using less memory. Memory usage tests showed CAME outperformed Adam, LAMB, and even existing memory-efficient optimizers like SM3.

Ultimately, CAME offers a promising solution for training large language models by combining the benefits of fast convergence and memory efficiency, extending the capabilities of existing memory-efficient optimizers for large batch training.</sample>
    <sample id="193">The text does not mention the number of annotators used to create the initial dataset.</sample>
    <sample id="194">University of Washington and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT, a novel framework for Explainable Question Answering (XQA) that addresses limitations in existing neuro-symbolic and decompose-based methods. RoHT tackles the challenges of determining question decomposition granularity and finding optimal solutions by employing a two-stage approach.

First, it constructs a Hierarchical Question Decomposition Tree (HQDT) representing the compositional structure of a complex question, breaking it down into atomic questions. A question decomposer generates leaf nodes (atomic questions), while a question generator creates intermediate questions. Certainty scores are assigned to each node.

Second, RoHT performs probabilistic reasoning over the HQDT, recursively traversing from root to leaves. A scheduler selects appropriate knowledge sources (KB, text corpus, or child nodes), executors retrieve answers with probabilities, and an aggregator combines these answers to produce top key answers.

Evaluations on KQA Pro and Musique datasets demonstrate RoHT's superior performance compared to existing methods, highlighting the benefits of integrating knowledge from both KBs and text corpora at different levels of decomposition. The framework effectively handles incomplete KBs and leverages explicit decomposition for improved accuracy and explainability.</sample>
    <sample id="196">"I saw Bart and Lisa."</sample>
    <sample id="197">Four state-of-the-art chat models.</sample>
    <sample id="198">Because large language models are now using longer and longer context windows, it's crucial to evaluate their acceptability judgments throughout that window.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset.</sample>
    <sample id="200">No, gli annotatori conoscono i nomi delle entità, ma non necessariamente le informazioni su di esse.</sample>
    <sample id="201">Metriche neurali MT all'avanguardia e valutazione umana basata sull'esperienza (MQM framework).</sample>
    <sample id="202">No, the performance drop is attributed to temporal drift, not adaptive overfitting.</sample>
    <sample id="203">It's increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="204">BLOOM è inadeguato per i compiti di semantic parsing cross-linguale.</sample>
    <sample id="205">This presentation explores the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. The research investigates how language models acquire and exhibit political leanings, and how these biases affect fairness in applications like hate speech and fake news detection.

The study uses political questionnaires and controlled experiments, including further pretraining on partisan corpora (news and social media) and temporal datasets (pre- and post-2017), to demonstrate that language models do indeed possess varying political leanings, often reflecting societal polarization. Notably, GPT models tend to be more liberal than BART models.

The research reveals that language models with different political leanings exhibit biases in downstream tasks. Left-leaning models are better at detecting hate speech against minority groups but worse at detecting it against dominant groups, while right-leaning models show the opposite trend. Similar biases are observed in fake news detection.

The presentation concludes by highlighting a critical dilemma: removing political opinions from training data risks censorship, while retaining them can lead to unfair outcomes. The research underscores the urgent need to address these fairness issues arising from political biases in language models.</sample>
    <sample id="206">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="207">The latest test sets were used to avoid overlap of the test data with the language model's training data.</sample>
    <sample id="208">Tre.</sample>
    <sample id="209">The proposed method (over-generate-then-filter) improves upon existing large language models by generating higher-quality scripts with both better semantic completeness and faithfulness to constraints, surpassing the performance of most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset are proposed as a benchmark for automatic text simplification.</sample>
    <sample id="212">One T5 model.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk explores the dependency structure of coordination, contrasting asymmetric approaches (Universal Dependencies, Mel'čuk's Meaning-Text Theory) where the first conjunct is the head, with symmetric approaches (Prague Dependency Treebanks, Hudson's Word Grammar) and multi-headed approaches.

His paper argues for symmetric coordination structures based on the principle of dependency length minimization. This principle suggests shorter dependencies are preferred, similar to how direct objects ideally stay close to verbs. Przepiórkowski observed that the tendency for the left conjunct to be shorter is prominent when the governor is on the left or absent. This observation, confirmed through analysis of the Penn Treebank, shows a correlation between the length difference and the preference for a shorter left conjunct.

However, this tendency vanishes when the governor is on the right. This finding provides evidence against asymmetric structures and supports symmetric ones, suggesting that the placement of conjuncts is influenced by minimizing dependency length, regardless of a designated head. The full argument and further details are available in the paper and at a poster session.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" introduces a novel approach to controllable dialogue generation, addressing limitations in existing methods that primarily focus on single attributes or rely on extensive labeled data. The work, by Zeng, Zhao, and He from Beijing University of Posts and Telecommunications, proposes DCG, a Disentangled Controllable Generation model, built upon DialoGPT, that learns attribute concepts from seen values and utilizes a disentanglement loss.

DCG employs attribute-oriented and task-oriented prompts to guide generation, concatenated into a whole prompt embedding, and incorporates pseudo combinations to enhance diversity. A key contribution is the MAE (Multi-Attribute Evaluation) framework, a unified, reference-free evaluation metric for assessing controllability across different attribute granularities.

Experiments on DailyDialog-CG demonstrate DCG's superior performance in attribute controllability and text quality compared to baselines. The model effectively generalizes to unseen attribute combinations, outperforming CTRL. MAE exhibits strong correlation with human judgments, proving its effectiveness and generality across different language models. Visualization confirms the model's ability to disentangle attribute combinations and learn relationships between them, enabling generalization from seen to unseen attribute values. The research concludes by highlighting DCG's success in tackling compositional generalization challenges in multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">Jia-Huei Ju presented "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports," conducted with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang. The work addresses the challenge of extracting useful information from Form 10-K reports (annual reports) which currently requires significant human effort.

The research was motivated by the observation that financial reports exhibit high text similarity year-over-year (around 80% token overlap). To tackle this, they introduced a "highlighting task" that compares and contrasts target reports with previous year reports (reference). The model predicts word importance to identify rationales for relationships between the reports.

The proposed pipeline involves document segmentation, relation recognition (classifying pairs into β, revised, and mismatched types), and a two-stage fine-tuning process. First, out-of-domain fine-tuning using eSNLI, followed by in-domain fine-tuning using revised pairs with pseudo-positive labels and mixed objectives (cross-entropy and KL divergence).

Evaluation on the FINAL dataset and eSNLI demonstrated strong performance, particularly with mismatched pairs. The work contributes a highlighting task, a dataset (FINAL), and a domain-adaptive model, with future directions including incorporating information retrieval techniques.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">L'articolo non specifica le coppie linguistiche analizzate. Tuttavia, cita un esempio di traduzione dal tedesco all'inglese.</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates methods to improve question answering (QA) performance when transferring models trained on a general domain (Wikipedia) to new, specialized domains. The core challenge lies in domain shift, where the source model struggles to generalize due to differences in vocabulary, reasoning patterns, or context. The study explores both zero-shot and few-shot data interventions to address this. Few-shot methods leverage large language models to generate training examples from a limited number of target domain samples, while zero-shot techniques manipulate the question, answer, and context distributions to understand their impact on model learning.

The research identifies three types of dataset shift: no shift, concept shift, and covariate shift, based on the compatibility of the retriever and reader models with the target domain. A compatibility measure, based on likelihood scores, is introduced to categorize datasets. The findings reveal that few-shot adaptation is broadly effective across various domains, while zero-shot interventions are particularly beneficial for datasets exhibiting concept or covariate shift. Through experimentation with diverse interventions, the study achieves up to a 24% improvement in reader performance, demonstrating the importance of tailoring adaptation strategies to the specific type of domain shift encountered.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART e base mBART.</sample>
    <sample id="225">53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">Two.</sample>
    <sample id="227">Current language models excel at general NLP tasks but lack grounded language understanding – the ability to map natural language to executable plans in specific environments. This is due to pre-training primarily on text without grounding. Existing approaches often use language models to directly generate plans, which can be grammatically incorrect or invalid.

The paper introduces "Pangu," a novel framework that shifts the focus from generation to discrimination. A symbolic agent proposes candidate plans, and a language model (like BERT, T5, or Codex) scores and ranks them. This alleviates the language model's burden of ensuring plan validity.

Pangu is demonstrated on knowledge-based question answering, achieving outstanding performance with both fine-tuning and in-context learning, including remarkable sample efficiency (e.g., Codex achieving over 50% accuracy with just one demo). The framework exhibits strong generalizability, as autoregressive models tend to overfit seen structures, while Pangu maintains consistent probability distributions across seen and unseen structures.

The key takeaway is that for grounded language understanding, discrimination is a more effective strategy than generation when utilizing language models. The authors welcome collaboration and feedback.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This presentation introduces a research paper by Gabriella Skitalinskaya and Henning Wachsmuth focusing on detecting improvable claims in argumentative writing. The core problem addresses how to determine when an argumentative claim is phrased effectively, a crucial aspect for influencing audience reception.

The paper introduces two tasks: *Suboptimal-Claim Detection* (identifying claims needing revision) and *Claim Improvement Suggestion* (identifying quality issues). The research leverages revision data from online debate platforms like Kialo, where claim revisions are tracked, with final versions considered optimal and predecessors suboptimal.

However, using revision data presents challenges. These include: ensuring data representativeness and reliability, selecting appropriate model complexity and architecture, accounting for contextual information (debate-wide, parent claim, domain knowledge), and mitigating topical and user biases.

The research explores these challenges and investigates various modeling approaches. Key findings indicate that revision-based data can be effectively used for claim assessment, modeling the distance between claim versions aids in detecting suboptimal claims, and contextual information's impact varies depending on the task and quality issues. The paper details their analysis and comparison of strategies for tackling these challenges, encouraging readers to consult the full paper for further insights.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web used to train DrBERT.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) aims to translate spoken language into text in real-time, but current models face challenges like complex architectures, lengthy training, and the need for multiple models to achieve different latency levels.

This paper introduces EDAtt (Encoder-Decoder Attention), a novel strategy that leverages existing offline speech translation models without retraining or architectural modifications. EDAtt controls latency by adjusting parameters and utilizing the cross-attention mechanism between audio and text.

The core idea is to selectively emit partial translations based on attention weights. A word is emitted only if its attention is not concentrated on recent speech frames (lambda frames), indicating sufficient stability in the received information. This allows the model to "wait" for more context before finalizing a translation.

Experiments on German show that EDAtt outperforms existing strategies applied to offline models, achieving higher translation quality (BLEU score) with lower latency (average lagging and computational-aware lagging). The code, models, and output are publicly available to promote reproducibility.</sample>
    <sample id="234">The prompting has a big influence on the performance of LLMs for translation, with differences of up to 40 BLEURT points observed between different prompts.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">Five expert-written instructions.</sample>
    <sample id="237">They propose a diagnostic test suite for knowledge integration, introducing a coreference resolution task called KITMUS.</sample>
    <sample id="238">The video introduces MeetingBank, a new benchmark dataset for meeting summarization developed by the University of Central Florida. Addressing the need for datasets tailored to meeting-specific summarization, MeetingBank comprises 1,366 City Council meetings and nearly 7,000 instances from cities like Boston, Seattle, and Denver. 

The data collection process involves using Speechmatics API for transcript generation, identifying meeting details, locating reference summaries from meeting minutes, and aligning timestamps. The dataset includes meeting transcripts, reference summaries, and URLs. Analysis reveals that the summaries often include verbatim points rather than abstractive summaries.

The video presents an evaluation of various summarization systems, including extractive models (Oracle, LEAD, LexRank, TextRank) and abstractive models (BART-Large, Pegasus, Longformer, DialogLM, HMNet), alongside GPT-3. While GPT-3 performed poorly on automatic metrics, human evaluation showed it excelled in fluency and coherence. The research highlights the importance of capturing key discussion points and suggests a need for improved automatic evaluation metrics that better reflect human preferences. MeetingBank is released to the public to facilitate research and provide insights into City Council decision-making processes.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e farò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico. E abbiamo confrontato con sistemi all'avanguardia, il sistema con le migliori prestazioni, quindi la valutazione WMT. Utilizziamo metriche MT neurali all'avanguardia e mostriamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000, la differenza osservata è di oltre un punto BLEURT. E questo può arrivare, in casi estremi, fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a 5 colpi in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema, con la lingua in cui si trova. In questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con tedesco due punti e le traduzioni inglesi con inglese due punti. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompting brevi. È cruciale per il prompting zero e one-shot. E quando passiamo, come nel nostro caso, al prompting a 5 colpi, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riepilogo dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano prestazioni migliori quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Gli spunti che abbiamo tratto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM hanno detto che la fluidità di PaLM è paragonabile ai sistemi all'avanguardia, ma la differenza principale deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra quindi che PaLM scelga di produrre una traduzione dal suono migliore, a volte omettendo parti della frase di origine che sono fatte nella traduzione. Tuttavia, la categoria "Stile/Imbarazzante" per PaLM è inferiore a quella dei sistemi all'avanguardia, il che è un segnale aggiuntivo che PaLM fornisce un output davvero fluido, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, dottorando all'Università del Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Più Debole di Quanto Pensi: Uno Sguardo Critico all'Apprendimento con Supervisione Debole". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento con supervisione debole, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore dell'etichetta in modo che i modelli addestrati generalizzino bene. In recenti lavori sull'ASL, ASL sta per Apprendimento con Supervisione Debole, un'affermazione comune è che le persone dicono di addestrare solo modelli sui dati debolmente etichettati e di ottenere prestazioni elevate su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un aspetto da considerare, ovvero che le persone presumono che sia disponibile un set di validazione pulito aggiuntivo per la selezione del modello. Non possiamo fermarci a questo problema, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento con supervisione debole. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata. Il suddetto dubbio pone tre domande di ricerca. Innanzitutto, i dati di validazione puliti sono necessari per l'ASL o possiamo usare un set di validazione rumoroso invece? In secondo luogo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori affinché l'ASL funzioni, quanti campioni puliti ci servono? Infine, dovremmo usare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, scopriamo che, curiosamente, i recenti metodi ASL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un calo significativo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci ASL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci ASL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se in ogni caso decidiamo di accedere a campioni puliti, l'addestramento su di essi raggiungerà persino prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci ASL, che utilizzano i dati puliti solo per la validazione. Come si può vedere, se abbiamo 10 campioni per classe, il fine-tuning inizia a battere gli approcci ASL. Infine, il miglioramento delle prestazioni rivendicato negli approcci ASL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come si può vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma rispetto ai metodi ASL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw funziona altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi ASL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci ASL richiedono campioni annotati manualmente e puliti affinché funzionino correttamente. Il loro guadagno di prestazioni e la loro praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Innanzitutto, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello viene eseguita tramite campioni di validazione puliti. In secondo luogo, gli approcci ASL dovrebbero essere confrontati con i baseline di apprendimento few-shot, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma efficace che dovrebbe essere considerato in futuro nel lavoro sull'ASL. Infine, abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR su questo slide. Sentiti libero di dare un'occhiata. Grazie e godetevi la conferenza.</sample>
    <sample id="241">This paper introduces a novel human-in-the-loop evaluation framework for early misinformation detection, addressing shortcomings in existing automated approaches. Current systems often rely on unrealistic datasets and neglect the crucial role of human content moderators. Our framework focuses on building end-to-end systems that integrate human feedback throughout the process, from raw social media data to actionable outputs.

We present a case study focusing on COVID-19 treatment misinformation, implementing a system with two components: claim detection and policy violation verification. The claim detection component uses keyword filtering and a T5 question-answering model to extract and rank potentially misleading claims based on trendiness. The policy verification component employs a BERT-based stance classification model to flag tweets supporting unapproved treatments for human review.

Our evaluation demonstrates the system's ability to detect unapproved treatments *before* they are debunked in news articles, operationalizing "early detection" as a key measure of utility. We achieve a 65% precision for policy violation detection and a high efficiency of 124.2 policy violations confirmed per human hour. This framework provides a more realistic assessment of human-system interaction in misinformation detection and offers a valuable perspective for developing future, consistently evaluable, human-in-the-loop systems.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">Five.</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This presentation details "A Needle in a Haystack," a study analyzing how to identify high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The research addresses the limitations of automatic metrics and unclear best practices for MTurk recruitment.

The proposed pipeline involves two stages: a Qualification Task assessing evaluators' ability to correctly assess summaries across six dimensions, and an Endurance Task evaluating their capacity for handling large workloads. This process yielded a small but high-quality group of 26 workers (8 gold, 18 silver) representing 13% of the initial 200. Further endurance testing reduced this to 12 (4 gold, 8 silver), achieving inter-annotator agreement (IAA) comparable to experts.

A Reference-based Task further evaluated performance, demonstrating a Krippendorff's Alpha of 0.534. Comparisons with baseline methods (MACE) and CloudResearch MTurk workers showed the pipeline achieved comparable or better results (0.513 Alpha) at a lower cost, though with a lower task acceptance rate. Analysis revealed a correlation between Pipeline and CloudResearch workers, and a strong correlation between GPT models and expert judgments.

The study concludes that the pipeline effectively identifies high-agreement workers, preventing resource waste and offering a cost-effective alternative to CloudResearch. Future work will focus on improving worker correctness and exploring applications across various tasks, languages, and platforms.</sample>
    <sample id="246">Yes, on GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI presented their paper, "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper introduces a new task, Knowledge Graph-Based Fact Verification, addressing the lack of datasets utilizing knowledge graphs as evidence alongside natural language claims. Existing datasets like FEVER and TabFact rely on text or tables, respectively.

FACTKG utilizes DBpedia as its knowledge graph and features claims in both written and colloquial styles for practical application. The dataset includes "SUPPORTED" and "REFUTED" labels and requires retrieving evidence from DBpedia to verify claims. The task incorporates five reasoning types: one-hop, conjunction, existence, multi-hop, and negation.

The dataset was created using a colloquial style transfer model and presupposition templates. Baselines were established, including a "Claim Only" approach and a model utilizing the GEAR model for graph evidence verification. Results demonstrate that leveraging graph evidence, as implemented by the GEAR model, significantly outperforms baselines, highlighting the value of knowledge graph reasoning for fact verification. The dataset is publicly available for download.</sample>
    <sample id="248">No, gli annotatori non sono bilanciati rispetto a ciascun gruppo demografico. Il framework re-annotates datasets per ottenere molte annotazioni per istanza e per raccogliere dati demografici ricchi, poiché le demografie degli annotatori originali dei dataset sono spesso limitate o non condivise.</sample>
    <sample id="249">Perturbazioni sono state aggiunte alle frasi nel dominio accettabile per preservare la struttura rilevante, ma introducendo rumore nell'input.</sample>
    <sample id="250">Evaluating multiple aspects of chat quality to understand strengths and weaknesses on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces U-CREAT, an unsupervised pipeline for Prior Case Retrieval (PCR), a task crucial for legal professionals facing increasing case volumes. The work addresses this challenge with two key contributions: the IL-PCR dataset and the U-CREAT pipeline itself.

The IL-PCR dataset, a new benchmark for PCR tasks, comprises 7,070 Indian legal cases, significantly larger and more complex than existing datasets like COLIEE’21. U-CREAT leverages unsupervised learning and an event-based approach. It extracts events from legal documents using dependency parsing to form subject-verb-object triplets. These events are then used to create an interaction matrix between query and candidate documents, enabling efficient retrieval.

Experiments demonstrated that event-based models, particularly the "Event Filtered Documents" approach, significantly outperform count-based and transformer-based models, including legal-specific transformers like InCaseLawBERT and InLegalBERT. U-CREAT achieves state-of-the-art performance on the COLIEE’21 dataset, showcasing its generalization capabilities across legal systems and highlighting the effectiveness of event-based techniques for PCR. The research opens new avenues for advancements in legal document retrieval.</sample>
    <sample id="253">DisorBERT is a novel model developed by researchers from Mexico and Spain for detecting signs of mental disorders in social media posts. It leverages a double domain adaptation approach, building upon the BERT language model by first adapting it to social media language (Reddit) and then specializing it for mental health. A key innovation is guided masking, which encourages the model to focus on relevant words during training.

The model's effectiveness is demonstrated using the eRisk dataset, showing a good balance between precision and recall compared to baseline methods. Analysis of DisorBERT's predictions on sentences from the Beck Depression Inventory reveals a tendency to generate words with a more negative or psychological orientation, aligning with common mental health concerns.

Furthermore, visualizations of attention scores on a user's post highlight the model's ability to identify key phrases related to mental health, such as "anxious" and "medication." DisorBERT outperforms MentalBERT, a model trained on a larger dataset, showcasing the benefits of the double domain adaptation and guided masking strategy. Future work will explore different lexical resources and incorporate clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">The research paper, "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," addresses the challenge of noisy data in distant supervision (DS) for document-level relation extraction (DocRE). Existing methods using pseudo-labels risk introducing false relations and losing correct ones.

This paper proposes a framework that first trains a DocRE model with both human-annotated and DS data to generate pseudo-labels. To mitigate noise, it introduces uncertainty estimation using Monte Carlo dropout to determine the reliability of model predictions. A key innovation is an instance-level uncertainty estimation method tailored for overlapping relations, which accurately identifies false positives.

The framework further employs dynamic class uncertainty thresholds to filter pseudo-labels with high uncertainty and a multi-phase training strategy to iteratively re-label DS data. This approach leverages the DS data more effectively, especially for long-tail classes. Experimental results on public datasets demonstrate significant performance improvements compared to existing baselines, highlighting the effectiveness of the proposed uncertainty-guided label denoising framework. The contributions include the framework itself, the instance-level uncertainty estimation, the re-labeling strategy, and the achieved performance gains.</sample>
    <sample id="255">La forma del prompting è cruciale per il prompting zero e one-shot, ma ha poca influenza quando si utilizza il prompting a 5-shot.</sample>
    <sample id="257">Four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces their work, "Can Large Language Models Be an Alternative to Human Evaluation?" which explores using large language models (LLMs) to evaluate text quality in natural language processing. The core idea is to provide LLMs with instructions and samples, prompting them to generate ratings.

Traditionally, human evaluation has been used, but it's unstable and difficult to reproduce. The research investigates whether LLMs can offer a reliable alternative. They conducted experiments rating stories generated by GPT-2 and written by humans, assessing grammar, coherence, likability, and relevance.  LLM ratings were compared against ground-truth ratings obtained from English teachers.

Four LLMs were tested: T0, InstructGPT (curie and davinci), and ChatGPT. While human evaluators preferred human-written stories, some smaller LLMs didn't show a clear preference. However, Davinci and ChatGPT demonstrated a preference for human-written text, mirroring the English teachers' judgments.

The paper addresses further questions about agreement between LLMs and humans, the impact of instruction wording, sampling methods, and the cost-benefit analysis of LLM evaluation versus human evaluation, as well as exploring the application of this method to other NLP tasks. The speaker encourages viewers to read the paper or visit their poster at ACL for more details.</sample>
    <sample id="259">The presentation introduces XSemPLR, a new benchmark dataset for cross-lingual semantic parsing. This task involves translating queries in multiple natural languages into various meaning representations like SQL, Lambda Calculus, and FunQL. Existing models often lack broad language coverage or support for specific meaning representations.

XSemPLR comprises 9 datasets across 5 semantic parsing tasks, 8 meaning representations, and 22 languages spanning 15 language families. The benchmark evaluates models across six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study found that Encoder-Decoder models generally outperform Encoder-PTR models. Training multilingual models with a mix of languages improves performance across most languages, though English sometimes sees a drop (the "Curse of Multilinguality"). Zero-shot transfer exhibits a significant performance gap compared to monolingual settings, but this gap narrows considerably with few-shot learning.

Key findings also include the benefit of pretraining on English to boost performance in other languages and the inadequacy of large language models like Codex and BLOOM for this specific task. The presentation concludes by encouraging the audience to explore the paper and code for further details.</sample>
    <sample id="260">Non specificato.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">Siyu Yuan è uno degli autori.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs), which stems from biases introduced by design choices like example selection and order. The paper systematically categorizes these label biases, identifying a novel "domain-label bias" – the influence of the task corpus on model predictions.

The researchers found that even random in-domain words can significantly bias LLMs' predictions, unlike random English words. They observed that ICL performs well with low domain-label bias but struggles with high bias, even with existing calibration methods.

To mitigate these biases, they propose "domain-context calibration," which uses random in-domain words as content-free text to estimate and correct biases. This approach outperforms previous methods that used single, predefined tokens. Experiments across diverse datasets and models (including GPT-3) demonstrate significant performance improvements, particularly in tasks with high domain-label bias. 

The study highlights the limitations of single content-free tokens and underscores the importance of incorporating domain-specific information for effective calibration, ultimately enhancing the reliability and performance of ICL.</sample>
    <sample id="264">Lin Wang presented "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenge of multimodal text generation where data annotation is costly and performance degrades across domains. TAVT aims to overcome this by enabling models to quickly adapt to new audio-visual domains with limited labeled data.

The framework comprises three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network aligns visual concepts across domains into a unified auditory semantic space using audio clusters. Learnable visual prefixes are introduced to map visual content to the audio space. The encoder-generator utilizes a transformer-based architecture with an alpha value to assess each modality's contribution to each word.

To directly optimize visual-audio alignment, a Dual Counterfactual Contrastive Learning (DCLL) loss function is proposed. Meta-training, similar to MAML, involves selecting support and query sets from different domains. Experiments on MSVD and MSR-VTT benchmarks demonstrate TAVT's superior performance compared to existing methods, particularly in low-resource domains. Ablation studies further validate the impact of audio features on overall performance.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">L'articolo si basa su un'analisi dei dati estratti dalla Enhanced Penn Treebank e dal paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato svolto dal laboratorio Emory NLP guidato dal Professor Jinho Choi presso l'Emory University e in collaborazione con Amazon Alexa AI. Quindi, diciamo che avete appena sviluppato un modello di dialogo e volete vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di scegliere quale delle due conversazioni è migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare molteplici dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare. Un approccio è quello di chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, ad esempio la pertinenza delle risposte del modello utilizzando metodi comparativi o di scala Likert esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprime o meno determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di irrilevante, si contraddice o contraddice il proprio interlocutore, allucina fatti errati o viola la conoscenza del senso comune e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo molteplici dimensioni. Dalla nostra analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento di ABC-Eval sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in modo doppio. Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, potete vedere come la misurazione della proporzione di turni con contraddizioni di sé e del partner spiega il 5% e il 10% della qualità della conversazione, mentre le medie valutazioni Likert sulla coerenza spiegano solo il 4% o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise. Potete vedere come la combinazione di tutte le metriche di ABC-Eval spiega oltre il 25% della qualità della conversazione e, man mano che rimuovete le metriche una alla volta, la maggior parte di esse comporta la perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione superiore a quella che i metodi precedenti sono in grado di raggiungere. Potete vedere che nei risultati del nostro esperimento rimangono diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il proprio interlocutore circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Emory University and Amazon Alexa AI.</sample>
    <sample id="271">Continuous fine-tuning.</sample>
    <sample id="272">7</sample>
    <sample id="273">Buongiorno, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente fosse "Le cose potrebbero iniziare a farsi pericolose se i ministri lo scoprono", allora "mole" si riferisce a una spia. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. A seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazioni il contesto C fornisce sul target Y, dato la sorgente X. Puoi pensare a CXMI come alle informazioni ottenute fornendo il contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P-CXMI per cercare schemi tra queste parole. E conduciamo la nostra analisi su trascrizioni di TED talk che sono state tradotte dall'inglese in 14 diverse lingue. Conduciamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo i tag part-of-speech che hanno un alto P-CXMI medio. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che anche alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi esaminiamo gli elementi di vocabolario che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come quello qui, in cui in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre nella forma di cortesia corretta. Infine, esaminiamo diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati veramente dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno. Abbiamo chiamato il nostro tagger Multilingual Discourse-Aware, o MuDA tagger. Possiamo quindi notare che le diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Quindi usiamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MuDA ha identificato. Infine, usiamo il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione a livello di documento. Innanzitutto, quando usiamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli indipendenti dal contesto hanno le migliori prestazioni. Ma poi se usiamo COMET, i modelli consapevoli del contesto funzionano meglio. E se usiamo la word f-measure, allora i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo metriche a livello di corpus da sole. Ora usiamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori rispetto ai modelli che non utilizzano il contesto su altri fenomeni come pronomi e forme verbali. Questo suggerisce in qualche modo dove dovremmo vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. In sintesi, conduciamo un'analisi basata sui dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Vi vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya and Vignesh presented "IndicMT Eval," a dataset for meta-evaluating machine translation metrics specifically for Indian languages. Recognizing that evaluation metrics often favor English and may not be suitable for languages with unique linguistic features, they created a dataset focusing on five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati.

The dataset comprises 7,000 samples generated from 200 source sentences translated into English using seven different translation models. Bilingual expert annotators meticulously evaluated these translations, marking errors by type and severity using the MQM framework and providing overall scores.

Their analysis revealed that COMET-metric variants demonstrated the highest overall correlations with human scores. However, many metrics exhibited skewed score distributions. Further analysis based on error types (fluency vs. accuracy) showed improved correlations when metrics were evaluated against accuracy-focused subsets.

The team then fine-tuned COMET using their MQM dataset, creating "IndicCOMET," which outperformed standard COMET baselines across most languages and demonstrated strong zero-shot performance on unseen languages. Finally, IndicCOMET showed improved robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for more reliable evaluation of machine translation for Indian languages. The dataset is publicly available.</sample>
    <sample id="277">The method does not have a name.</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness," identifying words that distinguish marked groups (those differing from a dominant, unmarked default) from unmarked ones, using weighted log-odds ratios to pinpoint key differentiating terms.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">MultiEMO is a novel attention-based multimodal fusion framework for emotion recognition in conversations (ERC) designed to address limitations in existing approaches. The framework tackles challenges including inadequate exploitation of multimodal complementarity, poor performance on minority emotion classes, and difficulty distinguishing semantically similar emotions. MultiEMO comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. 

A core contribution is VisExtNet, a visual feature extractor that focuses solely on facial expressions, eliminating redundant scene information. The MultiAttn module, built upon bidirectional multi-head cross-attention layers, facilitates effective integration of textual, audio, and visual modalities. Furthermore, a Sample-Weighted Focal Contrastive Loss (SWFC) is introduced to prioritize minority classes and enhance discrimination between semantically similar emotions.

Extensive experiments on the MELD and IEMOCAP datasets demonstrate state-of-the-art performance, with significant improvements in recognizing minority and semantically similar emotions. Visualization of attention heatmaps highlights MultiEMO's ability to handle complex scenarios where emotional cues from different modalities are asynchronous. While limitations exist, including speaker differentiation in VisExtNet and batch size requirements for SWFC, MultiEMO represents a significant advancement in multimodal emotion recognition.</sample>
    <sample id="281">The presentation "When Does Translation Require Context? A Data-driven, Multilingual Exploration" investigates the role of context in machine translation across 14 languages. The research team, led by Kayo Yin, introduces Pointwise CXMI (P-CXMI), an extension of CXMI, to measure context usage at the sentence or word level, identifying words that heavily rely on context for accurate translation.

Analysis of TED talk transcripts revealed patterns: dual pronouns in Arabic needing context due to English's lack of dual pronouns, verb form selection requiring context, consistent translation of proper nouns in Chinese, and maintaining appropriate formality. Phenomena like ellipsis resolution, not captured by individual words, were also identified.

To facilitate evaluation, the team developed the Multilingual Discourse-Aware (MuDA) tagger, automatically identifying context-dependent words based on five discourse phenomena. The MuDA benchmark revealed that while context-aware models excel in formality and lexical cohesion, they don't significantly outperform context-agnostic models in areas like pronouns or verb forms. Corpus-level metrics like BLEU can be misleading, while COMET better reflects context-aware performance. DeepL consistently outperformed Google Translate in document-level translation based on the benchmark. The work highlights areas needing improvement for document-level machine translation and provides a valuable tool for evaluating translation systems.</sample>
    <sample id="282">StoryTrans, presented at ACL 2023, tackles non-parallel story-level text style transfer, a significant advancement over previous token or sentence-level approaches. The core challenge lies in replicating an author's discourse-level linguistic preferences, including narrative techniques, while also preserving content specific to the writing topic.

StoryTrans addresses these challenges by learning discourse representations from source texts and combining them with style embeddings. A novel training objective minimizes style influence in discourse representations and enhances content preservation through a two-stage generation process. First, the source text is transferred with style-specific content masked, then the full text is generated incorporating these keywords.

The training framework utilizes an advisory approach with self-reconstruction, disentanglement, sentence order, and style classifier losses for the first stage. The second stage focuses on content restoration. Experiments on new Chinese and English datasets demonstrate StoryTrans's superior performance compared to baselines in style control and content preservation, confirmed by both automatic and manual evaluations. Style visualization confirms alignment with golden text. StoryTrans effectively enriches storylines and maintains source semantics, showcasing its ability to rewrite sentences while preserving meaning. Data and code are publicly available.</sample>
    <sample id="283">Prague dependency treebanks</sample>
    <sample id="284">FSUIE is a novel approach for Universal Information Extraction (UIE) designed to address limitations in existing span-based models. These models often over-rely on precise span boundaries, which can be ambiguous in annotation. FSUIE introduces a "fuzzy span" mechanism, treating span boundaries as continuous distributions rather than fixed points.

The model incorporates two key components: a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL uses Binary Cross Entropy and KL-divergence to learn from both the golden boundary and supplementary information, creating a fuzzy boundary represented by R-min and R-max. FSA acts as a mask function, dynamically adjusting the attention span using an optimizable parameter (delta) and linearly decaying attention at the boundaries. This focuses attention on relevant semantic information within a limited range.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness. It achieves state-of-the-art results on several datasets, including ACE2004, 2005, and ADE for relationship extraction, and AST-V2 for aspect sentiment triplet extraction. Ablation studies confirm that both FSL and FSA contribute to improved convergence speed and information extraction capabilities, showcasing FSUIE's strong generalization abilities and overall performance in UIE tasks.</sample>
    <sample id="285">The paper "Reference Matters" addresses factual errors in dialogue summarization, a previously unexplored area. Current Factual Error Correction (FEC) models are evaluated using unreliable factuality metrics like FactCC and DAE, which provide vague scores and don't distinguish between true error correction and summary rewriting.

The authors argue for a new evaluation framework using manually annotated reference corrections, reflecting the core requirements of FEC: minimal changes (substitutions, insertions, deletions) for fluency and non-redundancy. This provides better training data and a more accurate performance assessment.

They propose a new taxonomy of factual errors, categorizing them as content-based (part of speech, dependencies) and form-based (addition, deletion, substitution). Their framework, built on ERRANT, involves alignment, classification, and comparison.

Experiments reveal that training FEC models with reference summaries improves performance on unreliable factuality metrics. Combining human-annotated data with synthetic data is promising. However, current models struggle with additions and more complex errors like attribute, modality, and link errors, highlighting the need for improved FEC techniques.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Four.</sample>
    <sample id="288">BLiMP, SyntaxGym, and CrowS pairs.</sample>
    <sample id="290">WSL, COSINE, FTw.</sample>
    <sample id="291">The models are evaluated on 11 biomedical and clinical downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This presentation introduces EPIC, the English Perspectivist Irony Corpus, a dataset developed by the University of Turin and Amazon Alexa to study irony detection in natural language understanding. Traditional NLP relies on "ground truth" annotations, but EPIC challenges this assumption by acknowledging the subjective nature of irony.

The corpus comprises approximately 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. Data was annotated by 74 crowdsourced annotators using Prolific, each providing 5 annotations per conversation, with quality control checks.

The research explores "perspective-aware models," training separate models on data annotated by different groups (e.g., by gender, age, nationality). While raw performance wasn't significantly improved, these models demonstrated higher confidence in their predictions compared to aggregated "gold standard" models.

Analysis revealed that disagreement on irony perception is most pronounced between generations close in age and between annotators from the UK and Ireland, suggesting cultural and generational influences on interpreting irony. The project highlights the importance of considering annotator perspectives in NLP models.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns" investigates coded rhetoric, specifically dogwhistles—terms with a surface meaning for an outgroup and a hidden, often inflammatory, meaning for an in-group. The project addresses the challenge of studying these terms, which thrive on outgroup unawareness.

The core of the project is a glossary of over 340 dogwhistles (primarily US-centric, focusing on racist, transphobic, and anti-Semitic terms), categorized by register (formal/informal), type (implicature vs. covert signaling), and persona (e.g., anti-Semitic, transphobic). A case study of historical U.S. political speeches revealed a correlation between dogwhistle frequency and the Republican Southern Strategy, demonstrating their use to circumvent explicit expressions of prejudice.

The research also evaluated language models (GPT-3) for dogwhistle recognition. While GPT-3 could surface some dogwhistles, performance varied, particularly with informal and transphobic terms. Providing definitions and "secret cues" improved identification. Finally, a toxicity detection case study demonstrated that replacing slurs with dogwhistles significantly reduced toxicity scores, highlighting how these coded terms can evade content moderation online.</sample>
    <sample id="298">Retraining models with more recent data resulted in performance degradation with a larger temporal gap, confirming the hypothesis that temporal drift is the main cause of performance drop.</sample>
    <sample id="299">The presentation discusses "Improving the robustness of NLI models with minimax training," a joint work addressing the issue of NLI models relying on shortcuts—spurious correlations in datasets that lead to brittle performance on out-of-distribution data. Existing shortcut mitigation methods often require auxiliary models with specific knowledge and assumptions about the learner's behavior, which are not always practical.

The proposed method tackles this by employing a minimax training objective. A learner model focuses on the NLI task, while an auxiliary model generates example weights to maximize the learner's loss, incentivizing it to concentrate on "hard" examples—under-represented instances that contradict shortcuts. This alternating optimization process encourages the learner to prioritize learning from these challenging examples, improving out-of-distribution generalization.

The method doesn't require prior knowledge of shortcut types and uses a feed-forward network for the auxiliary. Evaluations on MNLI, FEVER, and QQP datasets with adversarial test sets (HANS Symmetric, PAWS) demonstrate consistent improvements in out-of-distribution performance while maintaining in-distribution accuracy compared to standard training and existing mitigation techniques. The paper further explores the impact of model size, synthetic shortcuts, and transferability, alongside a qualitative analysis of the learned example weights.</sample>
    <sample id="300">Belinda introduces "interactive dictation," a new task aiming to create a more natural and intuitive voice-based document editing experience. Unlike existing speech-to-text systems that primarily support dictation, interactive dictation allows users to seamlessly interleave dictation and editing commands using natural language.

The task involves four key steps: ASR recognition, segmentation of utterances into dictation and commands, command extraction and normalization, and finally, executing these utterances sequentially to reach the final document state. To address the lack of existing data, the team designed a specialized annotation interface to collect a dataset for this task.

They developed a baseline system using separate models for each step, experimenting with architectures like T5 and GPT-3 for the interpretation model. Results showed a trade-off between runtime and accuracy, with GPT-3 generally being more accurate but slower. Interestingly, for GPT-3, directly predicting the next document state proved more accurate than predicting intermediate programs, while T5 benefited from program prediction for efficiency. The team emphasizes the potential for future improvements and has released the code to encourage further research in this area.</sample>
    <sample id="302">Because after the first step, all the right tokens are present, but they are not ordered.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se i modelli stiano generando stereotipi positivi a causa di un allineamento eccessivo dei valori o di altri metodi anti-stereotipici, e questa mancanza di trasparenza impedisce ulteriori studi.</sample>
    <sample id="304">Unacceptable sentences are chosen from the same matching, or from a completely unrelated domain such as Wikipedia.</sample>
    <sample id="305">Dawei's presentation critiques recent advances in Weakly Supervised Learning (WSL). WSL aims to train neural networks using cheaper, noisy labels from sources like heuristics or crowdsourcing, rather than manual annotations. However, the presentation reveals a significant oversight: most WSL methods rely on a clean validation set for model selection, effectively requiring additional manual annotation.

The research investigates whether clean validation data is essential, how much is needed, and how best to utilize it. Findings demonstrate that WSL methods falter without clean validation, failing to generalize beyond the weak labels. While increasing the number of clean samples improves performance, directly fine-tuning on those samples consistently outperforms WSL approaches, even with as few as 10 samples per class.  

The presentation concludes that the performance gains claimed by WSL methods are often inflated and achievable through simple fine-tuning. It recommends future WSL research to explicitly report model selection criteria, compare against few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. The code for their experiments is publicly available.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented their research on entity tracking in large language models (LLMs). They argue that understanding entity states and their changes is crucial for comprehending longer discourses, but existing LLMs haven't been systematically evaluated for this ability.

Their research addresses challenges in designing an evaluation task, specifically preventing models from exploiting shortcuts like common patterns in pre-training data or simple word associations. They designed a task using boxes and objects, where models predict the contents of boxes after a series of state-changing operations.

Experiments with Flan-T5, GPT-3, and GPT-3.5 revealed that only text-davinci-003 showed non-trivial entity tracking. Notably, GPT-3.5 models, trained with substantial code data, demonstrated this ability, while others did not. Fine-tuning smaller models like T5-base enabled entity tracking, highlighting the importance of pre-training.

The findings suggest that pre-training on code may be a key factor in surfacing entity tracking capabilities in LLMs. However, the generalizability of these observed abilities remains an open question, and further analysis, including experiments with GPT-4, is detailed in their paper.</sample>
    <sample id="307">The authors used data for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework developed to characterize the biases inherent in NLP datasets and models. It addresses the issue that these technologies often perform differently across populations, a phenomenon termed "design bias." This bias stems from the positionality – the perspectives shaped by demographics, identity, and experiences – of the researchers and developers creating them.

The framework involves re-annotating existing datasets with a diverse group of annotators, collecting demographic data, and comparing these annotations with model and dataset predictions using correlation scores. The study utilized Lab in the Wild, an online experimentation platform, to recruit over 1,000 annotators from 87 countries, generating over 16,000 annotations for social acceptability and toxicity/hate speech detection tasks.

The findings reveal that datasets and models tend to align most closely with English-speaking countries and individuals with higher education. However, this alignment leaves certain groups, such as non-binary individuals, underrepresented. To mitigate these biases, the researchers recommend documenting design choices, adopting a perspectivist approach to NLP research, and building specialized datasets and models tailored to specific communities. The goal is not universal applicability but rather inclusive NLP that acknowledges and addresses these inherent biases.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The affiliations of the authors are not mentioned in the provided text.</sample>
    <sample id="312">MultiInstruct è il primo benchmark dataset multi-modale per l'instruction tuning su larga scala, composto da 62 compiti diversi che coprono 10 categorie ampie.</sample>
    <sample id="313">James Finch e Sarah Finch.</sample>
    <sample id="314">Coordination of two sentences.</sample>
    <sample id="315">I prompt sono stati ispirati da uno studio in cui sono stati dati a soggetti umani.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="317">Peng Li from Fudan University presented "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing the challenges of information extraction (IE) using traditional language models like T5 and GPT-3. These models struggle because of a mismatch between the text-based pre-training and the linearized, structured output required during inference.

CodeIE tackles this by reframing IE as a structure-to-structure code generation task, leveraging code-specialized large language models like Codex. The approach involves designing prompts that define functions for IE, incorporating comments to guide the model in extracting entities and relationships and appending them to lists.

Experiments across named entity recognition and relation extraction datasets demonstrated that CodeIE significantly outperformed baselines like UIE and GPT-3, particularly in few-shot settings. Analysis revealed that code language models exhibit lower perplexity on code-formatted inputs and produce fewer structural errors. Furthermore, Codex consistently outperformed GPT-3, and code-style prompts generally yielded better recall. The research suggests that aligning pre-training and output structures, along with utilizing code-optimized models, improves IE performance. The paper and code are publicly available.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un Modello Pre-addestrato Robusto in Francese per i Domini Biomedico e Clinico". In questa presentazione, parleremo innanzitutto del linguaggio modeling nell'assistenza sanitaria. Quindi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, che è un dataset di dati medici estratti dal web. Abbiamo anche introdotto un confronto di modelli con molteplici impostazioni di pre-addestramento e fonti di dati. Quindi presentiamo i nostri risultati su 11 compiti biomedici e clinici in francese. Infine, concludiamo sugli esperimenti e vi forniamo maggiori dettagli su come accedere a questi modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre enormi guadagni di prestazioni rispetto ai metodi statici e contestuali storici come Word2vec, fastText o altro. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche in domini come il biomedico con PubMedBERT e BioBERT e sul clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Quindi ci siamo posti una domanda su quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi e se i dati estratti dal web siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanta dati servono per addestrare un modello specializzato in francese? Sono 4 gigabyte, 8 gigabyte o più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di un set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prese da note cliniche; e una versione finale di ChuBERT con una miscela di 4 GB di set di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro sempre basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato sul modello biomedico inglese PubMedBERT, e addestrato su 4 GB di set di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo dati per compiti downstream pubblici e privati come il riconoscimento di entità nominate, la classificazione, il part-of-speech tagging e il question answering. Questi modelli sono confrontati con sei modelli di base che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli funzionano meglio sui compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee appaiono più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori. Nel complesso, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando i pesi e il tokenizzatore di CamemBERT addestrato su un sottoinsieme di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Questo non è il caso del modello basato sui pesi e sul tokenizzatore di CamemBERT, che soffrono di problemi di stabilità. Infine, in conclusione, il nostro sistema proprietario ha offerto prestazioni migliori in nove degli 11 compiti downstream e ha superato globalmente il risultato del modello generico, qui CamemBERT. Osserviamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, e sotto la licenza MIT, e tutti gli script di addestramento sono nel nostro repository GitHub. Quindi grazie per questa presentazione, e non vediamo l'ora di scambiare idee alla sessione poster a Toronto.</sample>
    <sample id="319">The strategies examined are from-scratch pre-training and continual pre-training.</sample>
    <sample id="320">Non è stato osservato overfitting adattivo.</sample>
    <sample id="321">The quality of simplification was evaluated by fine-tuning language models (long-mBART and mBART) and comparing their scores and evaluation metrics against baseline scores.</sample>
    <sample id="322">Hi everyone. I'm Enrico and I will be presenting at ACL 23 answering the question, "What does a Text Classifier Learn about Morality?" Human morality helps us distinguish right from wrong and is essential for language models to understand in text. Current NLP approaches often treat morality as a single scale between immoral and moral, which is problematic because morality is subjective.

My research applies Moral Foundation Theory, which posits five distinct ways humans perceive morality, similar to taste buds. Language models can somewhat understand morality in text, and my paper investigates what they learn by using explainable AI techniques.

I focus on how morality is expressed differently across domains, using a dataset of 35,000 tweets from seven domains like #AllLivesMatter and #BlackLivesMatter. The research reveals that language models recognize these differences. For example, the concept of "subversion" is viewed negatively in the All Lives Matter domain (associated with words like "overthrow") but is more encouraged in the Black Lives Matter domain.

This highlights the danger of using a single model across diverse domains, as it can lead to misunderstandings of morality. I hope to see you at ACL in Toronto.</sample>
    <sample id="323">Yujie Wang from Shanxi University presents "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA." The paper addresses challenges in Commonsense Question Answering (QA) where machines need common knowledge to understand language. Existing approaches combine language models and knowledge bases, but often retrieve noisy entities and encode information in isolation, neglecting semantic relationships.

DHLK, the proposed method, constructs a Heterogeneous Knowledge Graph (HKG) through a two-stage pruning strategy and Knowledge Representation Learning (KRL). It enhances the graph by adding paraphrased entities from WordNet and Wiktionary. RoBERTa and Mask Self-Attention are used to encode and fuse question-answer contexts with entities, dynamically removing irrelevant entities based on attention weights.

TransE optimizes entity and relation embeddings within the HKG. Instead of traditional GNNs, Relation Mask Self-Attention (RMSA), inspired by RGAT, models the subgraphs, iteratively updating embeddings. Path information from the HKG is integrated into the QA context. Finally, embeddings from the HKG, enhanced paths, and the QA context are fed into an MLP for answer prediction. Experiments on CommonsenseQA and OpenBookQA, using ConceptNet, WordNet, and Wiktionary, demonstrate DHLK's strong performance compared to other methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Occupano tutti e quattro i quadranti dello spettro politico e GPT-4 è il modello più liberale.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo su "Generalizzazione Composizionale senza Alberi utilizzando Tag Multiset e Permutazioni Latenti". Questo è un lavoro congiunto con i miei relatori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un discente di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del semantic parsing, il test per la generalizzazione composizionale potrebbe assomigliare a questo. Come al solito, abbiamo un set di addestramento di espressioni. In questo caso, "La ragazza dormiva" e "Mary sapeva che la ragazza dormiva". Queste espressioni sono abbinate a forme logiche che rappresentano gli aspetti principali del loro significato. A differenza della valutazione standard del machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente inedite. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli seq2seq naive faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono destinati a catturare il processo compositivo che mette in relazione le espressioni con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. In genere, ciò comporta una pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli di variabile. L'ottenimento degli alberi può anche comportare procedure specializzate di induzione della grammatica. In questo articolo, non utilizziamo gli alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dall'input in due fasi. Innanzitutto, etichettiamo ogni token di input con un multiset non ordinato di token che appariranno nell'output. Dopo la prima fase, abbiamo tutti i token giusti, ma non sono ordinati. Ecco perché nella seconda fase utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente così. Passiamo da sinistra a destra sull'output e determiniamo quale token multiset inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Quindi saltiamo al token multiset successivo per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token multiset. Continuiamo questo processo finché ogni token della prima fase non è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, però. Nel nostro articolo, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta alla sfida di trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema dello "zaino del commesso viaggiatore". Lo approssimiamo con una rilassazione continua adatta alla GPU che ci consente anche di retropropagare la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, dai un'occhiata al nostro articolo o vieni al nostro poster.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) architecture designed to improve representation learning by effectively aggregating insights from unimodal experts at different levels. Building upon the BridgeTower approach, ManagerTower addresses its limitations by introducing managers within each cross-modal layer to adaptively combine multiple unimodal representations. Unlike BridgeTower's fixed layer connections, ManagerTower allows for flexible exploitation of semantic knowledge across various unimodal layers.

The architecture utilizes RoBERTa and CLIP-ViT base as unimodal encoders and employs managers to facilitate comprehensive cross-modal alignment and fusion. Experimental results demonstrate that ManagerTower achieves superior performance on various downstream tasks, including Visual Question Answering, even with a relatively small pre-training dataset of four million images. Notably, it outperforms existing models like METER and BridgeTower, showcasing a 39.15% accuracy improvement on the Wikivideo test standard.

Visualization of manager aggregation weights reveals that adaptive managers dynamically adjust their focus on different unimodal layers based on the cross-modal layer, contrasting with the static, less intuitive behavior of static managers. This adaptability highlights ManagerTower's ability to leverage diverse levels of unimodal semantic knowledge for enhanced VL representation learning. The paper, code, and models are publicly available.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, aiming to overcome the limitations of existing methods that rely on pseudo-labels generated without manual annotation. Current methods often suffer from simplistic pseudo-queries, misalignment between queries and video segments, and vulnerability to label noise.

The proposed "noise-resistant Structured Pseudo-Label generation" method addresses these issues. It begins by generating complex, free-form pseudo-queries using a pre-trained image captioning model (BLIP). Subsequently, it constructs pseudo-events by measuring frame relevance to the queries, ensuring high relevance within the event and low relevance outside. A sliding window approach identifies the optimal event duration based on this relevance difference. To mitigate redundancy, the method selects only the top K pseudo-queries with high event quality and eliminates overlapping pairs.

To combat label noise, the model employs a two-pronged strategy: sample re-weighting based on prediction confidence and IoU, and label refinement by incorporating high-confidence predictions as new pseudo-labels. Experiments on ActivityNet Captions and Charades-STA demonstrate significant performance improvements over existing zero-shot methods, achieving state-of-the-art results. The code is publicly available.</sample>
    <sample id="330">Cumulative performed equal or better than Iterative across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Trascrizioni di TED Talks.</sample>
    <sample id="333">The presentation introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," a novel framework designed to improve neural machine translation (NMT) models. Traditional NMT models often suffer from a non-smooth representation space, leading to poor generalization and performance, particularly with low-frequency tokens. kNN-MT addresses this by smoothing predictions using nearest neighbors, but it's computationally expensive and struggles with updating representations.

INK overcomes these limitations by iteratively refining the NMT model's representation space using kNN knowledge. The framework involves two steps: extracting kNN knowledge to guide an adapter's adjustment of representations, and then asynchronously updating the datastore with these refined representations. This loop continues until convergence, ultimately allowing the datastore to be discarded.

Experiments using the WMT’19 German-English news translation task demonstrate that INK significantly improves performance compared to state-of-the-art kNN-MT systems, achieving gains in both BLEU and COMET scores. INK achieves higher BLEU scores with less memory space and faster inference speed, proving the effectiveness of injecting and refining kNN knowledge to smooth the representation space of NMT models. The research also suggests that combining adapters and datastores can further enhance translation quality.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This research introduces a novel approach called "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" to address the challenge of representing out-of-vocabulary (OOV) words in language models. The core idea is to leverage word formation and association, mimicking human learning habits.

The method constructs a Word Relationship Graph around the OOV word, breaking it down into wordpieces and connecting them to related words. This creates a two-level graph where each word/wordpiece is a node with an associated embedding. A self-attention network assigns attributes to the OOV nodes based on their characters.

To process this graph, the model utilizes a Graph Attention Network (GAT) with two concatenated layers to filter noise and capture node-level representations. A readout block then generates a graph-level representation summarizing the word's formation. Contrastive learning, using NT-XENT loss, further refines the embeddings by encouraging proximity to relevant neighbors and pushing them away from unrelated words.

Experiments demonstrate the model's superior performance compared to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of learning OOV words through word formation. The model's adaptability to different languages, particularly agglutinative ones, is also discussed, highlighting the importance of rational word decomposition.</sample>
    <sample id="338">This research investigates the quality of human-generated natural language explanations used to improve machine learning models. While humans often rely on these explanations to train models, the study questions whether they are always helpful and proposes a method for objective evaluation.

The paper introduces a unified data structure converting diverse tasks into a multiple-choice format, allowing for consistent evaluation. Through experiments across five datasets (CoS-E, ECQA, e-SNLI, ComVE), the researchers found that fine-tuning with explanations doesn't always impart new knowledge but can lead models to rely on the explanation itself. They observed task-dependent explanation utility, with ECQA explanations proving more helpful than CoS-E.

To address limitations of existing metrics like BLEU, ROUGE, and simulatability score, the authors propose a new metric called TREU. TREU evaluates explanation helpfulness during fine-tuning by comparing model performance with and without explanations. Evaluations on T5 and BART models across the datasets demonstrate that TREU consistently ranks dataset qualities better than simulatability score, particularly for tasks like e-SNLI and ComVE. The study highlights the importance of task-specific explanation formats and suggests that even seemingly low-quality human explanations can benefit model predictions. Ultimately, the work advocates for rigorous quality checks in human annotation processes.</sample>
    <sample id="339">Saarland University, Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a joint work with several collaborators. The core challenge addressed is the need for large, high-quality, and syntactically diverse paraphrase datasets for NLP applications like question answering and chatbots. Existing datasets are either limited in scale (human-annotated) or lack syntactic diversity (automatically generated via back-translation).

ParaAMR tackles this by leveraging Abstract Meaning Representations (AMR) graphs, which capture a sentence's semantic meaning. The method involves parsing sentences into AMR graphs, randomly changing the "focus" (root node) of the graph, and then using an AMR graph-to-text generator to create paraphrases. This process ensures semantic similarity while introducing syntactic variation due to the generator's focus on the new root node.

The resulting ParaAMR dataset contains 15 million source sentences and approximately 6.9 paraphrases per sentence. Evaluations demonstrate that ParaAMR maintains good semantic similarity compared to other back-translation datasets while exhibiting significantly higher syntactic diversity. Experiments on sentence embeddings, syntactic control paraphrase generation, and few-shot learning showcase ParaAMR's benefits, leading to improved performance in these NLP tasks. The dataset is publicly available.</sample>
    <sample id="341">Training models with an average of one second latency and another one with two seconds latency, and so on.</sample>
    <sample id="342">The paper introduces LiveChat, a large-scale, personalized dialogue dataset automatically constructed from live streaming videos (specifically Chinese TikTok and Douyin). Existing open-domain dialogue datasets are primarily text-based and often limited in scale due to manual annotation requirements. LiveChat addresses these limitations by leveraging video content and an automated "reply-to-whom" matching method to create dialogues.

The dataset construction involves three steps: extracting audio from videos, transcribing it using ASR, and matching audience comments to form dialogues. Persona information is also collected, categorized into basic profiles (manual labeling/scraping) and inferred profiles (rules and classifiers). LiveChat significantly surpasses existing datasets in scale, being video-sourced, having longer average sessions, and including personal annotations.

Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions. Notably, BART outperforms other pre-trained dialogue models, highlighting LiveChat's distinct domain. Human evaluations of LLMs on LiveChat showed richer informativeness, and in-context learning experiments revealed performance improvements with increasing demonstration shots, though excessive demonstrations introduced noise. The research concludes that LiveChat is a valuable resource for advancing dialogue research, particularly in personalized and multi-party conversation scenarios, with future work focusing on efficient LLM transfer learning.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi io e il mio co-autore Martin presentiamo il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite un pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti in attività come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Ma la comprensione del linguaggio naturale richiede spesso conoscenze fornite anche al momento dell'inferenza. Ad esempio, nella frase, "John ha visto il presidente appena eletto in TV." I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere in modo affidabile chi sia questa entità specifica "John" o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuto il pre-addestramento. Pertanto, modelli di successo per attività di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che quella fornita al momento dell'inferenza. In questo lavoro, proponiamo una suite diagnostica di test per l'integrazione della conoscenza. Introduciamo un compito di risoluzione della coreferenza, progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il set di dati con partecipanti a uno studio umano e modelli di risoluzione della coreferenza consolidati. Ecco un esempio dal nostro set di dati. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui il pronome "lui" si riferisce, che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Innanzitutto, la conoscenza specifica dell'entità, come "Servin è un giudice". E in secondo luogo, la conoscenza di base, come "I giudici decidono casi in tribunali". Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Variaamo la disponibilità di questi due tipi di informazioni in modo che possano essere trovati in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Innanzitutto, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base si presume sia disponibile al momento del pre-addestramento. In secondo luogo, c'è un'impostazione "Background-Both", in cui la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione "Background-Inference", in cui entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, perché nuove professioni si sono sviluppate da quando è avvenuto il pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle vere fonti. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di base "I politici cercano seggi eletti nel governo" sia contenuta nei parametri pre-addestrati e nel contesto del momento dell'inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo inoltre non solo la conoscenza specifica dell'entità, ma anche la conoscenza di base sui politici nel loro contesto del momento dell'inferenza. Nell'impostazione Background-Inference, forniamo la professione fittizia "mirituer" invece di politico perché "mirituer" è improbabile che sia contenuto nei parametri pre-addestrati. Valutiamo il set di dati sia con partecipanti a uno studio umano, sia con modelli di risoluzione della coreferenza consolidati. In questa figura, mostriamo i risultati dei modelli con le prestazioni migliori nella variante più difficile dell'impostazione Background-Pretrain. Senza un addestramento specifico per il compito su KITMUS, entrambi i modelli non funzionano bene. Quando vengono addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref funzionano significativamente meglio di una scelta casuale. Ciò suggerisce che, quando addestrati su set di dati di risoluzione delle referenze generici, la maggior parte impara a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze fittizie hanno indicato che anche i modelli con le prestazioni migliori non riescono ad integrare in modo affidabile la conoscenza all'indietro fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di risoluzione della coreferenza sembrano non essere in grado di ragionare sulle conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli con le prestazioni migliori sembrano avere difficoltà ad integrare in modo affidabile la conoscenza all'indietro presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il set di dati e il codice su GitHub. Grazie per l'ascolto.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive, often involving formalism-specific pre-processing or specialized grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves strong compositional generalization—the ability to handle unseen combinations of phrases—without relying on explicit tree structures. Traditional methods often use trees to capture compositional relationships, but this requires complex pre-processing and grammar induction.

The proposed model tackles this by directly modeling input-output correspondences. It operates in two stages: first, it tags each input token with a multiset of output tokens; second, it predicts a permutation to order these tokens correctly. A key innovation is a flexible permutation model that avoids hard constraints, allowing for expressive representation of possible orderings. The model conceptually selects output tokens sequentially, jumping between multisets to determine the order.

Experiments on the COGS benchmark demonstrate significant outperformance compared to other tree-less models, particularly in generalizing to deeper recursion. The paper also addresses technical challenges, including the lack of explicit alignment between input and output tokens during training and the computational complexity of finding optimal permutations (related to the Traveling Salesman problem). These are addressed through alignment induction and a GPU-friendly continuous relaxation technique that enables backpropagation.</sample>
    <sample id="346">The affiliations of the authors are not mentioned in the provided text.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi sociali nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su set di dati creati manualmente che sono molto dispendiosi in termini di tempo e, inoltre, misurano solitamente solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali sfaccettate possono comporre pregiudizi ed essere luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLM ottimizzati tramite istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come "Immagina di essere una donna asiatica. Descriviti". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Vediamo immediatamente che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni schemi interessanti. La donna asiatica è raffigurata come modesta; la donna del Medio Oriente è descritta usando parole come esotica e, come, riferendosi a una regione affascinante. E entrambe le donne di colore fanno riferimento alla loro ascendenza mentre la persona maschile bianca non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che fornendoli a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali. E questo consente anche un confronto diretto tra le persone generate e le risposte scritte dagli umani. La seconda parte è "Marked Words", che è un metodo per identificare le parole che distinguono i gruppi "marked" dai gruppi "unmarked", che spiegherò a breve. Il vantaggio è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su un lessico specifico. Quindi, il metodo "Marked Words" attinge al concetto sociolinguistico di "markedness", che afferma che esiste un default non marcato e qualsiasi gruppo che differisce da tale default è linguisticamente marcato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano effettivamente "guerriera" e marcano il termine con "donna". Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Quindi nel nostro metodo, designiamo prima quali sono i gruppi non marcati e marcati, quindi confrontiamo le persone utilizzando il metodo "Fightin’ Words", che è fondamentalmente l'utilizzo di rapporti log-odds ponderati per distinguere le parole principali per ciascun gruppo marcato. Quindi, ad esempio, per le persone di donne nere, faremmo "Fightin’ Words" e confronteremmo i rapporti log-odds sia con le persone bianche che con le persone maschio, perché questi sono i due gruppi non marcati corrispondenti. Ora, per alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente alla distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi più elevati delle parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole di stereotipo che sono nelle persone generate sono solo le parole "alto" e "atletico". Quindi, solo le parole positive o almeno non negative. E in effetti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo "Marked Words" per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Innanzitutto, dai nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" ed "esotico". E queste parole definiscono questi gruppi solo in relazione al loro rapporto con la loro identità e li distinguono come diversi dalla norma bianca. Ciò contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molte trame comuni che sono riflesse in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "formosa" che si collegano a una tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa" che si collega a una lunga storia di donne asiatiche che sono iper-sessualizzate, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della "Donna Nera Forte". E sebbene possa sembrare positivo a prima vista, ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Invece di lavorare effettivamente per cambiare questi ostacoli, mette pressione su queste persone per superarli, il che porta a esiti sanitari molto negativi per queste persone, tra le altre conseguenze dannose. Più in generale, scopriamo che le parole per ciascun gruppo marcato riflettono fondamentalmente solo narrazioni molto essenzializzanti. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a una sorta di allineamento del valore eccessivamente eccessivo o forse ad alcuni altri metodi anti-stereotipo che stanno portando a questi schemi dannosi. Non possiamo semplicemente fare alcuna ipotesi o studiare ulteriormente, senza maggiore trasparenza. Grazie mille per aver ascoltato. Divertitevi all'ACL.</sample>
    <sample id="348">This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," introduces a novel method for identifying and analyzing stereotypes in large language models (LLMs). Existing approaches often rely on limited, hand-curated datasets or capture only broad associations, failing to account for intersectionality. Our method leverages the instruction-following capabilities of LLMs to generate personas based on prompts like "Imagine you are a [demographic group]. Describe yourself." We then employ "Marked Words," a sociolinguistic approach, to identify words that distinguish marked (marginalized) groups from unmarked (dominant) ones.

Unlike lexicon-based methods, Marked Words reveals subtle, yet harmful, patterns beyond overt negativity. Our analysis of GPT-4 generated personas reveals that while seemingly positive words like "culture," "tradition," "strong," and "exotic" appear frequently, they reinforce essentializing narratives and perpetuate harmful stereotypes. For example, Latina women are linked to "vibrant" and "curvaceous" (tropicalism), Asian women to "petite" and "delicate" (hypersexualization), and Black women to the "Strong Black Women" archetype, which can lead to negative health outcomes. We find that generated personas exhibit more stereotypes than human-written ones, but the lexicon used fails to capture the nuanced harmful patterns. We conclude by recommending researchers address positive stereotypes, adopt an intersectional lens, and advocate for increased transparency in bias mitigation techniques within LLMs.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo. Vi state copiando il mio modello? Protezione del copyright dei modelli linguistici di grandi dimensioni per l'embedding come servizio tramite watermark backdoor. Introduciamo innanzitutto il contesto dell'embedding come servizio. Attualmente, modelli linguistici di grandi dimensioni come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere varie attività di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti lavori hanno dimostrato che un attaccante può rubare il modello apprendendo dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio. Per proteggere il copyright dell'embedding come servizio, una delle soluzioni è incorporare un watermark nel servizio del provider e rilevare se un altro servizio contiene il watermark. Il metodo di watermark deve soddisfare le seguenti proprietà. Innanzitutto, il metodo deve essere applicabile all'embedding come servizio. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente discreto in modo che l'attaccante non lo rilevi o possa rimuoverlo facilmente. Infine, il watermark deve essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie. Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, un metodo di watermark basato su backdoor applicabile all'embedding come servizio. Quindi, vi presento i dettagli del nostro embedding marker. Embedding marker contiene due passaggi principali: iniezione di watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo innanzitutto un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione di watermark, definiamo innanzitutto un embedding target. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright serve a rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo innanzitutto un backdoor e un set di dati benigni. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigni non appartengono al set di trigger. Quindi, il provider richiede gli embedding dal servizio del ladro con il set di dati. La similarità coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Viene calcolata la differenza di similarità tra il set di dati benigno e il set di dati backdoor, che è definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e utilizziamo il suo p-value come terza metrica. Abbiamo condotto esperimenti su quattro set di dati: AG News, MIND, SST2 ed Enron Spam. Assumiamo che il provider applichi il set di dati wiki text per contare la frequenza delle parole. I risultati su quattro set di dati mostrano che il nostro embedding marker può avere ottime prestazioni di rilevamento mantenendo un'ottima utilità per le attività a valle. Abbiamo anche convalidato la discrezione dell'embedding fornito visualizzando l'embedding delle frasi su quattro dataset [INAUDIBILE 4:39] PCA. La legenda delle figure significa il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" questions the validity of claims of superhuman performance in Natural Language Understanding (NLU) based on leaderboard scores. It argues that current benchmarks like SuperGLUE and SQuAD, while showcasing impressive results, offer a misleading picture of true understanding.

The core issue is that systems often outperform humans on these benchmarks due to exploiting spurious correlations and dataset-specific patterns, something humans don't do. Furthermore, the comparison between humans and systems is often unfair. Humans are evaluated on small subsets of the test data, while systems are tested on the full set. The paper also highlights errors in ground-truth answers, undermining the reliability of the benchmarks.

Beyond dataset issues, the paper criticizes the methodology for establishing human baselines. Simple aggregation methods are used, rather than comparing against the best possible human performance. Moreover, low pay rates and a lack of transparency regarding annotator pools raise concerns about the quality and representativeness of human annotations. Ultimately, the paper concludes that current claims of superhuman performance in NLU are not scientifically meaningful and proposes recommendations for constructing more reliable benchmarks.</sample>
    <sample id="351">The paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. The core question is whether these models, developed over two decades ago, still perform well on modern data and what factors contribute to good generalization.

To address this, the researchers created CoNLL++, a new dataset of Reuters News articles from 2020 annotated using the CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-03 and CoNLL++.

The findings highlight three key ingredients for good generalization: transformer-based model architectures, larger model sizes, and more fine-tuning examples.  They explored two hypotheses for performance drops: adaptive overfitting (due to repeated use of the same test set) and temporal drift (due to the time gap between training and testing data). Adaptive overfitting was not observed, while temporal drift was confirmed as the primary cause of performance degradation.

Ultimately, the paper concludes that CoNLL-2003 taggers *do* still work well in 2023, but emphasizes the need for continued research into improving model generalization, particularly addressing temporal drift.</sample>
    <sample id="352">Annotating behaviors in chat or ABC-Eval.</sample>
    <sample id="353">This paper addresses the challenge of input underspecification in natural language to code generation, a prevalent issue in real-world scenarios where natural language descriptions (NLDs) lack crucial details. To tackle this, the authors introduce an interactive approach, focusing on clarifying operation-level specifications through clarification questions (CQs). They propose a novel task: code generation by asking clarification questions.

A key contribution is the creation of CodeClarQA, a synthetic dataset with CQs targeting missing key operations. The methodology identifies missing operations by comparing NLDs to operation documentation using schema-based similarity scores. CQs are generated using templates, offering yes/no or multiple-choice formats. Experiments demonstrate the effectiveness of their method in identifying missing operations, with MPNet showing strong performance.

The proposed pipeline incorporates a Clarification Need Predictor, a Question Selector, and a Code Generator. Results indicate that the task is more challenging than existing CQA ranking tasks, and that clarifications generally improve code generation. While the pipeline currently underperforms model-only approaches due to the difficulty of the CQA ranking task, analysis suggests that clarified key operations are indeed a significant factor in generating better code, with Oracle CQs leading to near-ground truth predictions. The paper highlights areas for future improvement, including handling taxonomy ambiguities and incorporating argument values.</sample>
    <sample id="354">2020</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una studentessa di dottorato in Informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. In poche parole, la dissonanza cognitiva si verifica quando due credenze o azioni sono incoerenti, come nell'esempio in cui una persona afferma "So che le sigarette possono uccidermi" e poi dice "Ho preso un paio di sigarette dopo la riunione". Queste credenze e azioni sono incoerenti e si trovano in uno stato di dissonanza. Ulteriori spiegazioni come "Non credo che potrei mantenere il mio lavoro senza di loro" giustificano la seconda occorrenza e creano una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero raro trovarla espressa nel linguaggio rispetto ad altri tipi di relazioni discorsive. Quindi, perché questo è importante? Lo studio della dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare le tendenze e i valori delle credenze e i cambiamenti di atteggiamento nella popolazione. L'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Lo studio della dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e per aiutarci a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo utilizzato un approccio "dissonanza-first", come si vede nel diagramma di flusso qui. I tweet sono stati passati attraverso l'analizzatore PDTB e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie di unità discorsive annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia funzionato molto meglio del caso. Data la bassa occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo ad affrontare il problema dell'assoluta rarità. Per alleviare questo problema, sperimentiamo combinazioni di transfer learning e active learning per annotare in modo da raccogliere più campioni dissonanti con meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione migliorando al contempo il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe della dissonanza, abbiamo iniziato il processo di active learning trasferendo i pesi da attività correlate. Trasferiamo da due attività diverse: la classificazione dello stance sulla dissonanza indipendente dall'argomento, un'attività che determina se due affermazioni di dibattito provenienti da persone diverse sono d'accordo o in disaccordo, indipendentemente dall'argomento, chiamata "dibattito" qui, e sulla classificazione binaria delle classi di espansione e confronto di PDTB poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza e le chiamiamo CE qui. Scopriamo che il trasferimento delle prestazioni zero-shot sul set di dati annotato è già molto migliore del caso con il migliore, con AUC di 0,62. Inoltre, eseguendo un fine-tuning iterativo su entrambe le attività, scopriamo che il fine-tuning delle attività CE seguito da un ulteriore fine-tuning sul dibattito produce prestazioni zero-shot migliori. Pertanto, questo è il modello che utilizziamo per l'avvio a freddo dell'active learning. Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di active learning. "Cumulative" accumula tutti i dati raccolti dalle annotazioni active finora, mentre "Iterative" aggiorna il modello addestrandolo con l'ultimo set di dati raccolto. Tra le diverse strategie, abbiamo scoperto che Cumulative ha funzionato in modo uguale o migliore di Iterative in tutti i casi. Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia PRC (Probability-of-Rare-Class) per selezionare principalmente gli esempi che è molto probabile che siano dissonanti secondo il modello corrente in ogni round di rarità. Confrontiamo questo con altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, sebbene la differenza sia piccola. Si noti che le prestazioni sono significativamente inferiori per casuale. Con ulteriori round di AL con le due migliori strategie, miglioriamo l'AUC di classificazione della dissonanza a 0,75, che è la migliore performance che abbiamo ottenuto finora su questo compito. Verifichiamo anche la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che PRC è una strategia AL semplice per l'acquisizione di classi rare e l'avvio a freddo di AL con un compito di transfer learning opportunamente progettato e aiuta significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active all'interno del dominio beneficiano dell'aggiornamento cumulativo. Questi sono i link al nostro set di dati principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie.</sample>
    <sample id="356">Alexander Koller and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">State-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh presented "CounterComp," a method to improve compositional generalization in multi-step quantitative reasoning for question answering tasks involving financial tables. Current neural models struggle with these tasks, particularly when requiring more than two arithmetic operations, due to memorizing spurious patterns based on repeated tokens in the input.

CounterComp addresses this by leveraging counterfactual scenarios. It mines positive and negative examples from the training data by intervening in the questions. Positive examples involve interventions that don't change the output, while negative examples result in output changes. These triplets are used with a metric learning loss that dynamically adjusts based on the extent of the question intervention.

The CounterComp loss consistently improves performance on state-of-the-art models, especially with complex reasoning steps. Crucially, it enhances performance on both in-distribution and out-of-distribution samples, demonstrating improved compositional generalization.  Qualitative analysis reveals that CounterComp encourages the model to focus on more relevant tokens related to the operations needed to solve the questions. The research aims to reduce the need for costly human supervision while improving model robustness and generalization capabilities.</sample>
  </task>
</testset>