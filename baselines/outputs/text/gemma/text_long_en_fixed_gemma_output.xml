<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawl data, including news media like The New York Times, Los Angeles Times, and The Guardian.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model addressing reading order challenges in Visually-rich Document Understanding (VrDU). Existing models often rely on global 1D positions, which can be problematic for complex document layouts. LayoutMask utilizes "local 1D position" representing in-segment token order, forcing the model to infer global reading order by integrating 1D, 2D position, and semantic information to enhance text-layout interactions.

To further promote these interactions, LayoutMask incorporates two novel masking strategies within the Masked Language Modeling objective: Whole Word Masking, which encourages context-based prediction, and Layout-Aware Masking, which prioritizes masking segment boundaries. Additionally, a new Masked Position Modeling (MPM) objective is introduced, requiring the model to recover masked 2D positions, fostering spatial reasoning and semantic inference.

Experimental results demonstrate that LayoutMask's local 1D position approach outperforms global 1D position on datasets like FUNSD and SROIE, particularly in scenarios with complex layouts and misleading numbers. These findings highlight the effectiveness of LayoutMask in learning robust layout representations and improving VrDU performance. The paper details the model's architecture, training objectives, and experimental evaluation, offering a valuable contribution to the field of document understanding.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL</sample>
    <sample id="6">This paper introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," proposing a novel "many-to-many summarization" framework that generalizes both multilingual and cross-lingual summarization. This approach aims to build a single model capable of summarizing documents in any source language into any target language. Preliminary analyses demonstrate that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional methods.

To realize this, the authors present PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training strategy: meta pre-training for sentence generation from noisy inputs, cross-lingual pre-training for translation, and task-specific pre-training using pseudo many-to-many summarization data. Experiments on the WikiLingua dataset, using a shared mBART-50 backbone, show that PISCES outperforms baselines like mBART-50 and mT5. Ablation studies confirm the effectiveness of each pre-training stage, and human evaluations further validate PISCES's superior performance. The work highlights the benefits of a unified many-to-many approach for cross-lingual summarization and introduces a promising pre-trained model for this task.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">It reduces subjectivity by explicitly annotating whether model responses express certain behaviors (e.g., irrelevant information, contradictions).</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">Providing language models with more or better background knowledge.</sample>
    <sample id="11">This research investigates whether large language models (LLMs) genuinely understand humor by leveraging data from *The New Yorker* Caption Contest. While LLMs can generate and sometimes explain jokes, their true comprehension remains questionable. To probe this, the authors created a benchmark comprising three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is humorous. They annotated over 700 cartoons with detailed descriptions and collected over 650 human-written joke explanations.

Experiments revealed a significant performance gap between LLMs and humans across all tasks. Even with image descriptions provided, models like GPT-4 struggled, achieving only 57% accuracy on matching compared to 94% for humans. Furthermore, human-generated explanations were preferred over GPT-4's explanations in over two-thirds of blind A/B tests, highlighting inaccuracies in the model's reasoning. The study demonstrates that while LLMs can mimic aspects of humor, a deeper understanding remains elusive. The dataset and leaderboard are publicly available to encourage further research into computational humor understanding.</sample>
    <sample id="12">5</sample>
    <sample id="13">Adaptive inference techniques like Multi Model and Early Exit aim to reduce the computational cost of large language models by utilizing smaller models for simpler inputs. While Early Exit offers faster inference and memory efficiency, this work identifies a key limitation: conflicting gradients. Multiple classifiers within an Early Exit model share parameters, leading to gradient interference that degrades overall performance. This is demonstrated through a comparison with Multi Model classifiers, which consistently outperform Early Exit classifiers, particularly in earlier layers. To address this, the authors introduce SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that isolates gradient updates to individual transformer layers, eliminating conflicting gradients. Experimental results show SWEET significantly closes the performance gap between Early Exit and Multi Model, achieving superior speed/accuracy trade-offs, especially at high inference speeds. This research highlights the importance of considering gradient dynamics in adaptive inference architectures and provides a promising direction for future optimization of Early Exit models. The findings are detailed in the paper "Finding the SWEET spot" available on Archive.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">Bible texts are simplified more.</sample>
    <sample id="17">This paper introduces a novel approach to multimodal relation extraction (MRE) that addresses the challenges of internal-information over-utilization and external-information under-exploitation. Existing MRE methods often struggle with irrelevant textual information and fail to fully leverage visual cues, particularly when visual features are less informative. To overcome these limitations, the proposed method employs a Graph Information Bottleneck principle-guided feature refinement strategy for fine-grained information pruning and incorporates multimodal topic information to enrich contextual understanding.

The framework constructs a unified cross-modal graph (CMG) from textual and visual scene graphs, then refines this graph by filtering nodes and edges using the Graph Information Bottleneck principle. Subsequently, it integrates multimodal topic features, derived from textual and visual keywords, to supplement the CMG representation. Experiments on a standard MRE dataset demonstrate significant performance improvements over existing baselines, highlighting the contributions of both information screening and external information enrichment. An ablation study confirms the benefits of scene graphs for structural modeling. Further analysis reveals that internal-information screening is more crucial for inputs with high text-vision relevance, while external-information exploitation is more beneficial for inputs with lower relevance.</sample>
    <sample id="18">"Salt and pepper" instead of "pepper and salt."</sample>
    <sample id="19">This paper surveys recent advances in efficient open-domain question answering (ODQA), a challenging task due to the massive scale of knowledge sources like Wikipedia. The dominant two-stage retrieval-reader framework, while effective, faces bottlenecks related to large index sizes (65GB) and computationally intensive models. This survey explores techniques to address these challenges, focusing on memory reduction, faster inference, and maintaining comparable performance.

The work examines one-stage approaches like retrieval-only and generator-only systems, highlighting their trade-offs. Efficient tactics discussed include approximate nearest neighbor search for faster retrieval, adaptive computation for quicker reading, and index size reduction through document filtering, embedding compression, and product quantization. Model size reduction strategies encompass lightweight models, parameter sharing, and unified retrieval-reader architectures.

The survey analyzes existing ODQA models across data aspects, revealing that retrieval-reader systems offer a balanced approach. It concludes with insights for resource-constrained scenarios, suggesting generator-only or embedding compression for reduced index size, knowledge distillation or one-stage models for smaller models, and retrieval-only systems for real-time feedback. Future work directions include deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">News texts.</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Recent advancements in text-to-image models have yielded impressive image generation capabilities, yet these models often struggle with accurately rendering text within images. This work investigates the root cause of this issue, focusing on the text encoders used in models like Imagen, which rely on subword tokenization (SentencePiece). Our analysis reveals that even large-scale T5 models exhibit surprisingly low spelling accuracy, particularly with frequent words, due to the subword representation requiring decomposition into individual letters. In contrast, models utilizing byte-level tokenization (ByT5) demonstrate near-perfect spelling accuracy by directly accessing character-level information. To address this limitation, we propose a simple yet effective solution: augmenting the existing text representation with an additional representation from a small ByT5 model. This approach, adding only a modest increase in parameters (approximately 5% for ByT5-small), significantly improves both text rendering and overall image generation quality within the Imagen framework. We introduce the WikiSpell and DrawText benchmarks to evaluate text-only and text-to-image models, respectively, and highlight the benefits of character-aware text encoding for enhancing text rendering in generative models.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">The researchers extracted statistics about coordination from the Penn Treebank, measuring length in characters, syllables, and words, and observed the tendency for the left conjunct to be shorter based on the governor's position (left, absent, or right).</sample>
    <sample id="26">The classifier performs not much better than chance.</sample>
    <sample id="27">One.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a novel and effective ensemble learning framework for large language models (LLMs), leveraging pairwise ranking and generative fusion. Recognizing that individual LLMs exhibit varying strengths and weaknesses across different inputs, the framework moves beyond relying on a single "top" model. LLM-Blender first runs *n* LLMs on a given input and then employs a "PairRanker" module to comparatively evaluate the outputs. This module utilizes a cross-attention mechanism to analyze pairs of candidate outputs alongside the input, enabling a nuanced understanding of their relative quality. The PairRanker generates a comparison matrix, which is aggregated to determine a ranking of the candidates. Subsequently, a "GenFuser" module selects the top *K* (e.g., three) ranked candidates and fuses them into a final output using a sequence-to-sequence model. Experiments on the newly created MixInstruct dataset, evaluated using both automatic metrics (BERTScore, BLUERT, BARTScore) and human judgment (ChatGPT), demonstrate that LLM-Blender consistently outperforms leading LLMs like Open Assistant and Vicuna. The framework's simplicity and significant performance improvements highlight its promise for advancing LLM ensemble learning. The codebase and dataset are publicly released to facilitate further research.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">The framework compares annotations from diverse users with existing datasets and models using Pearson's R correlation scores.</sample>
    <sample id="34">CREST is a novel joint framework for rationalization and counterfactual text generation, combining selective rationalization and counterfactual editing to enhance interpretability and model robustness. The system first generates counterfactual examples by masking a rationale derived from an input, then utilizes a masked language model to fill in the masked portions. Human evaluations demonstrate CREST produces counterfactuals that are more valid and natural than existing methods like MiCE. 

Building on this, CREST-Rationalization leverages both factual and counterfactual examples during training. A shared rationalizer highlights meaningful rationales from both input types, which are then fed into a predictor module. A regularization term encourages consistency between original and newly generated rationales, promoting factual and counterfactual reasoning. Experiments on IMDB and SNLI datasets show CREST-Rationalization achieves state-of-the-art performance, particularly on out-of-domain data, and improves downstream model accuracy. 

Furthermore, CREST-Rationalization generates more plausible and interpretable rationales, exhibiting superior counterfactual simulability—the ability of an explanation to influence a classifier's decision when paired with a corresponding edit—compared to alternative approaches. The work highlights the potential of counterfactuals for data augmentation and generating controllable, high-quality explanations.</sample>
    <sample id="36">This work introduces Language-Specific Layers (LSLs) to enhance multilingual machine translation (MMT) while maintaining inference speed. Traditional MMT models face a capacity bottleneck per language, prompting the need for increased language-specific representation without escalating computational costs. LSLs address this by incorporating a dedicated transformer layer per language, allowing the model to selectively activate relevant sublayers during inference.

The research focuses on strategically placing these LSLs within the encoder, employing a novel approach to learn optimal placement rather than relying on manual tuning. By analyzing the weights of shared, source, and target components within each encoder layer, the model identifies which layers benefit most from language-specific specialization. The resulting architecture, determined through weight analysis, dynamically allocates LSLs based on the largest weight component (shared, source, or target).

Experiments on WMT21 news translation across 10 languages (including a low-resource language, Swahili) demonstrate significant improvements over baseline transformer models and language adapter approaches, as measured by chrF, spBLEU, and COMET. Notably, LSLs yield substantial gains for low-resource language pairs, with statistically significant improvements observed in 84 out of 90 translation directions, all while preserving inference efficiency.</sample>
    <sample id="37">The previous study found that giving prompts to human subjects also surfaced racial stereotypes.</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">1</sample>
    <sample id="40">Topic independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives introduces a novel knowledge graph designed to enhance narrative understanding in NLP systems. Addressing the limitations of current models in representing real-world personas and their associated knowledge, PeaCoK comprises approximately 3,800 personas, 40,000 attributes, and 100,000 inferences, emphasizing interconnectedness between personas. Built through a three-step process involving persona selection, attribute induction, and crowdsourced annotation with AI assistance (InstructGPT-3), PeaCoK provides a structured representation of persona-related commonsense knowledge. Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving comparable performance to larger language models like GPT-3 and GPT-3.5. Furthermore, integrating PeaCoK into a dialogue generation system (P²Bot) significantly improves dialogue quality, as evaluated by human raters, surpassing baselines and demonstrating the benefits of persona-centric knowledge over general commonsense knowledge. The study reveals a positive correlation between shared attributes and dialogue consistency/engagement, underscoring the value of interconnected persona knowledge in narratives. PeaCoK's public availability facilitates further research and development in narrative modeling.</sample>
    <sample id="42">One.</sample>
    <sample id="43">The text does not specify the number of authors.</sample>
    <sample id="44">The framework differs from previous works by comparing end users with models and datasets, as opposed to just annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="48">David Vilar and his colleagues from Google Translate.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN is a newly created corpus designed to advance German text simplification research at both the document and sentence levels. Addressing limitations of existing corpora—namely their small size and reliance on error-prone automatic alignments—DEPLAIN comprises two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (diverse domains, 750 documents, ~30,450 sentence pairs), both featuring manually aligned data with automatic alignment methods also employed in DEPLAIN-web. Analysis reveals varying degrees of simplification across domains, with Bible texts exhibiting greater simplification than news or language learner texts, and distinct patterns in simplification transformations between the subcorpora.

The corpus enables several key use cases. First, it provides a gold standard for evaluating automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Second, DEPLAIN facilitates the development of automatic text simplification models through fine-tuning language models (long-mBART for document-level and base mBART for sentence-level simplification). Fine-tuning demonstrated improved performance over baseline scores, establishing a benchmark for future research in this area. The DEPLAIN corpus, along with associated code and checkpoints, is publicly available to support further advancements in German text simplification.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper presents a novel approach to detecting cognitive dissonance in language, a rare but significant phenomenon with implications for understanding disagreement, mental health, and decision-making. Cognitive dissonance, defined as the inconsistency between beliefs and actions, is sparsely expressed in text, posing a substantial challenge for machine learning. To address this "absolute rarity," the authors created a large-scale annotated dataset of discourse unit pairs, finding dissonance in only 3.5% of cases. They employed a combination of transfer learning and active learning (AL) to efficiently annotate data and improve detection performance.

Initial transfer learning from related tasks—dissonance stance classification ("debate") and discourse relation classification ("CE")—significantly boosted zero-shot performance. Subsequently, an active learning strategy, Probability-of-Rare-Class (PRC), was developed to prioritize the selection of potentially dissonant examples for annotation. PRC outperformed other state-of-the-art AL strategies, achieving an AUC of 0.75 after multiple rounds, representing the best performance to date. The study also highlights the importance of cumulative model updates within the domain and iterative updates for cross-domain transfer learning. The findings demonstrate the effectiveness of PRC for rare class acquisition and the value of strategic transfer learning for cold-starting active learning in challenging NLP tasks.</sample>
    <sample id="55">Yes.</sample>
    <sample id="56">One.</sample>
    <sample id="57">Without task-specific training, the models do not perform well, but with training, some models successfully integrate knowledge from multiple sources, though they still struggle with inference-time knowledge.</sample>
    <sample id="58">Background-Pretrain, Background-Both, Background-Inference.</sample>
    <sample id="59">DrBERT is a novel French biomedical language model, addressing the scarcity of specialized language resources for the French healthcare sector. Building upon RoBERTa and trained on NACHOS, a 7GB dataset of medical web crawls, DrBERT represents the first open-source biomedical model in French. This work investigates optimal pre-training strategies and data sources, comparing DrBERT with a clinical model (ChuBERT) trained on anonymized hospital data and several models utilizing continual pre-training from CamemBERT and PubMedBERT. A comprehensive evaluation across 11 French biomedical and clinical downstream tasks (NER, classification, POS tagging, QA) demonstrates that DrBERT consistently outperforms generic French models like CamemBERT. The study reveals that models perform best on tasks aligned with their training data, but heterogeneous data sources yield more versatile models. While increased data generally improves performance, from-scratch pre-training proves superior, with a 4GB DrBERT model achieving comparable results to a larger continual pre-training approach. DrBERT and associated training scripts are freely available on Hugging Face and GitHub under the MIT license, facilitating further research and application in French-speaking healthcare settings.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG), addressing the growing need to compress large, computationally expensive NLG models while preserving performance. Unlike prior work focusing on specific tasks or pre-training, this research explores task-specific knowledge distillation across diverse NLG tasks—summarization, question generation, common sense reasoning, simplification, and style transfer—within realistic, industry-driven settings. These settings prioritize medium-resource labeled datasets, leveraging large amounts of unlabeled data and medium-sized models for practical inference time efficiency.

The study investigates architectural choices, the impact of pruning, and compares various knowledge selection approaches. A key contribution is an exploration of pseudo-target training, challenging the conventional single-mode approximation. The research demonstrates the critical role of unlabeled data and the benefits of generating multiple, diverse pseudo-targets through sampling techniques. Furthermore, a novel "joint-teaching" method is proposed, combining word-level distillation on both teacher and student-generated pseudo-targets to mitigate student exposure bias and improve learning. The findings provide a comprehensive "recipe" for effective knowledge distillation in NLG, offering practical guidance for model compression and performance preservation.</sample>
    <sample id="63">Sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Greater sensitivity suggests improved model performance.</sample>
    <sample id="66">Mathematical reasoning, a core aspect of human intelligence, is increasingly addressed by AI and NLP research. This survey explores the landscape of deep learning approaches for mathematical reasoning, encompassing text, visual, and tabular data. Tasks range from solving math word problems involving arithmetic operations to automated theorem proving and geometric problem-solving, often formalized as neuro-symbolic reasoning. Recent advancements leverage sequence-to-sequence and sequence-to-tree models to represent and generate mathematical expressions. The rise of large language models (LLMs) has shown promise, particularly with techniques like chain-of-thought prompting, but they face limitations in precise mathematical computation. Current research focuses on enhancing LLMs through methods like self-consistency decoding, program-aided architectures (e.g., Chameleon), and tool augmentation. While datasets have expanded beyond English and into specialized domains, challenges remain in low-resource settings and addressing generalization failures, particularly concerning large numbers and inconsistent reasoning. The field continues to evolve, striving to build robust and reliable models capable of human-level mathematical reasoning.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a common challenge where training on one language pair impacts others. While numerous methods aim to mitigate interference, their effectiveness remains unclear, particularly with larger models. Through experiments with Transformer architectures and 15 languages, the study identifies that severe interference primarily occurs when models are small relative to the dataset size—a condition termed "parameter poverty." Surprisingly, factors like language similarity and the total number of languages have minimal impact on interference levels. The research demonstrates that a simple yet effective solution is tuning the sampling temperature. Higher temperatures (T &gt; 1) allow for greater sampling from lower-resource languages, significantly reducing interference. The optimal temperature varies with model size, with smaller models benefiting from higher temperatures and larger models requiring calibration to avoid oversampling. Ultimately, the findings suggest that scaling model and data size, combined with carefully tuned temperature sampling, can substantially reduce interference without the need for complex, specialized algorithms, offering a practical approach to improving multilingual translation quality.</sample>
    <sample id="68">Sentences from relevant datasets, sentences from a different subset, or sentences from a completely unrelated domain (like Wikipedia).</sample>
    <sample id="69">20 samples per class.</sample>
    <sample id="70">Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">This paper introduces the AltEntities Corpus, a novel dataset designed to facilitate research on resolving indirect referring expressions for entity selection in conversational systems and to benchmark large language model (LLM) entity understanding. The corpus addresses the challenge of users employing indirect references (e.g., "the newer one," "the song that's not energetic") when selecting entities, a common occurrence when direct references are impractical or undesirable. The AltEntities Corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions, collected through crowd annotation using a cartoon completion setup. A key feature is the controlled generation of alternative questions, employing various sampling methods to increase entity similarity and disambiguation difficulty. Annotators are provided with background knowledge (Google search links for music, Wikipedia text and images for recipes and books) to inform their generation of indirect references. Experiments with a T5 XL model demonstrate that accuracy significantly improves with access to background knowledge, highlighting the importance of knowledge integration for LLMs. The results also show domain-generalizability and indicate substantial room for improvement, particularly when models rely solely on entity names. The dataset is publicly available to support further research in this area.</sample>
    <sample id="72">To address fairness issues arising from political biases propagating from pretraining data to language models and downstream tasks.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">This paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon the existing ATOMIC knowledge base. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its sparse B-to-A link structure. Dense-ATOMIC addresses this by completing missing B-to-A, B-to-B, A-to-B, and A-to-A links, enabling the creation of multi-hop paths crucial for commonsense reasoning. The construction process involves normalizing tail events and training a novel relation prediction model, Rel-CSKGC. Rel-CSKGC leverages pre-trained language models (RoBERTa) to encode head and tail events, effectively utilizing semantic information and circumventing the sparsity issues of graph-based methods. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links. Experiments demonstrate that Dense-ATOMIC significantly improves knowledge coverage and benefits downstream tasks like COMET, generating more diverse outputs. Evaluations on multi-hop paths reveal high aggregate path quality. The work highlights the potential of Dense-ATOMIC for advancing commonsense reasoning capabilities in machines. Code and a website are publicly available.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE) that addresses the limitations of existing approaches by explicitly modeling the interconnections between these tasks. Recognizing that NER and RE share underlying similarities and dependencies, Jointprop leverages a heterogeneous graph to propagate labels across labeled and unlabeled data, considering both intra- and inter-connections. The framework comprises four key components: span feature generation using contextualized representations, construction of a k-Nearest Neighbor heterogeneous graph to capture data similarity, joint label propagation to refine pseudo-labels iteratively, and model optimization through retraining a base classification model with combined labeled and high-confidence pseudo-labeled data. Experiments on four datasets, including both joint and single-task settings, demonstrate that Jointprop consistently outperforms baseline models, particularly in single-task scenarios, highlighting the benefits of jointly learning NER and RE. The absence of prior semi-supervised joint-task baselines underscores the novelty of this approach, which effectively integrates information from diverse data sources to improve performance in information extraction.</sample>
    <sample id="76">Pretraining data → Language Models → Downstream Tasks</sample>
    <sample id="77">This work introduces DeFacto, a new dataset designed to improve factual consistency in abstractive text summarization, developed collaboratively by Yale University and Microsoft Research. DeFacto comprises human demonstrations and feedback on system-generated summaries from the Pegasus model, collected using the XSum dataset. The dataset reveals that 70% of initial summaries contain factual errors. Annotators provided labels for factual consistency, human-corrected summaries, and detailed feedback including instructions, explanations, and supporting evidence from the source document.

The research proposes three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing showed promise with both fine-tuned models and large language models. Feedback generation proved challenging, while the factual error correction task demonstrated competitive performance with fewer training data, particularly when incorporating explanation generation. DeFacto's fine-grained annotations also offer benefits for training factuality metrics and meta-evaluation. The dataset and paper are publicly available on GitHub, providing a valuable resource for advancing research in factual summarization.</sample>
    <sample id="78">Yes, the types of simplification transformations differ. DEPLAIN-apa has more reorderings and word additions, while DEPLAIN-web has more rephrasings.</sample>
    <sample id="79">Yes, the CoScript dataset is publicly available.</sample>
    <sample id="80">The watermark is inserted by defining a target embedding and creating a weighted summation of the target embedding and the original embedding, where the weight is proportional to the number of triggers (words from a moderate frequency interval) in the sentence.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This paper introduces ULRA (Unsupervised AES by Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to obtain, particularly for new prompts. ULRA addresses this challenge by leveraging multiple heuristic quality signals as a form of pseudo-groundtruth to train a neural AES model without requiring human-annotated scores. The framework incorporates a Heuristic Essay Ranking (HER) module that generates partial order pairs by ranking essays based on various quality signals like unique terms and word count. These partial orders are then aggregated by a Deep Pairwise Rank Aggregation (DPRA) module, which employs a novel loss function that learns confidence weights for each signal to mitigate inconsistencies. A scoring strategy further transforms model predictions to align with predefined score ranges. Experiments in both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches, although it still lags behind fully supervised methods due to the inherent limitations of unsupervised learning. ULRA offers a promising approach for unsupervised essay scoring by effectively aggregating partial-order knowledge from multiple heuristic signals.</sample>
    <sample id="83">Yes, encoder-decoder or encoder-PTR models can be improved by training in a mixture of various languages.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks introduces a novel approach to address the parameter inefficiency of fully dynamic neural networks. While dynamic networks, which adapt their architecture or parameters based on input, often outperform static networks, fully dynamic models can lead to excessive parameter growth, limiting their practical application. This paper investigates whether redundant dynamic parameters exist and if a combination of static and dynamic parameters can achieve superior performance. The authors hypothesize that partially dynamic subnetworks can maintain or exceed the representation power of fully dynamic networks. To this end, they propose PAD-Net, a Partially Dynamic Network framework that partitions parameters into dynamic and static components, utilizing scale factors to control their influence and iterative mode partitioning to optimize this division. PAD-Net aims to identify and convert redundant dynamic parameters into static ones, minimizing parameter count and computational cost without sacrificing accuracy. Experimental results demonstrate that PAD-Net outperforms both static and fully dynamic networks, achieving comparable or better performance with significantly fewer parameters and computations. Ablation studies highlight the importance of dynamic ratios and scale factors. The work also shows PAD-Net's superiority over network pruning and its ability to produce more discriminating outputs. Future research directions include extending PAD-Net to other networks, hardware-friendly implementations, and incorporating additional parameter modes.</sample>
    <sample id="85">"Make a chocolate cake."</sample>
    <sample id="86">By visualizing the embeddings of sentences on four datasets using PCA, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The work uses continual pre-training on existing models like CamemBERT and PubMedBERT, as well as from-scratch pre-training, to build DrBERT.</sample>
    <sample id="88">Non-binary people.</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the traditional reliance on native speakers for NLP data annotation, particularly crucial with the advancement of language models. Recognizing the difficulty in recruiting native speakers for many languages, especially low-resource ones, the study investigates the feasibility of utilizing language learners. A proof-of-concept study was conducted across English, Korean, and Indonesian, employing four tasks from the GLUE benchmark (sentiment analysis, NLI, NER, and MRC). Language learners were categorized by proficiency (basic, intermediate, advanced) and compared to native speakers through experiments involving pre-tests, annotation tasks with varying resource support, and post-tests to assess learning effects.

Results demonstrate that learner-annotated labels achieve near-native accuracy, especially on simpler tasks, and perform comparably to native speakers when aggregated via majority voting. Training simulations further revealed that language models trained on learner annotations achieved 95% of ground truth performance, sometimes even surpassing models trained on native speaker data. The study also observed improvements in learner vocabulary and grammar through annotation. Ultimately, the research proposes a novel approach to data construction for low-resource languages, highlighting the potential of language learners to contribute significantly to NLP annotation and broaden research accessibility.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">The content does not name specific treeless baselines.</sample>
    <sample id="93">Advisors.</sample>
    <sample id="94">This paper addresses the critical issue of copyright protection for embedding as a service (EaaS), a growing application built upon large language models (LLMs). The increasing ability of attackers to replicate EaaS models through embedding extraction necessitates robust copyright safeguards. We propose Embedding Marker, a novel backdoor-based watermark method specifically designed for EaaS. Our approach involves injecting a covert watermark during embedding generation by incorporating a "trigger set" of moderately frequent words. The magnitude of the watermark is proportional to the number of triggers in a user's input, ensuring minimal utility degradation. Copyright verification is achieved by constructing backdoor and benign datasets, querying the suspected service, and analyzing the cosine and L2 similarity between the obtained embeddings and a predefined target embedding. We also utilize a Kolmogorov-Smirnov (KS) test for enhanced detection. Experiments across four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate Embedding Marker's high detection accuracy while preserving embedding utility. Visualization using PCA confirms the covertness of the watermark, making it difficult to distinguish from normal embeddings. This work provides a practical and effective solution for protecting EaaS copyright against model extraction attacks.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="97">Three.</sample>
    <sample id="98">There is no effective way to mitigate these biases; attempting to sanitize training data risks censorship and determining neutrality is incredibly difficult.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering that addresses the need for fewer training examples compared to existing methods. It combines unsupervised retrieval using TF-IDF and hyperlink traversal with a few-shot language model-based reranker. The system first retrieves candidate chains, then scores them by calculating the likelihood of the question given a specifically constructed chain prompt, which includes chain documents and an instruction designed to elicit reasoning. Techniques like instruction search and temperature scaling are explored to optimize performance. Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers, achieving strong downstream QA performance when paired with a reader model like ELECTRA-Large. The study highlights the effectiveness of using the likelihood of the question given the chain as a scoring function and emphasizes the crucial role of the instruction in guiding the language model's reasoning process. Overall, PromptRank offers a promising solution for multi-hop QA in low-resource domains by leveraging the power of language models with minimal training data.</sample>
    <sample id="101">Comparable to state-of-the-art systems.</sample>
    <sample id="102">Applicable to embedding as services, should not degrade utility, covert enough, and needs to be transferable.</sample>
    <sample id="103">The TED talks were translated into 14 different languages.</sample>
    <sample id="104">16,000</sample>
    <sample id="105">Cosine and L2 similarity.</sample>
    <sample id="106">QUEST is a novel retrieval dataset designed to address the challenge of information seeking with selective information needs, where queries involve implicit set constraints. Motivated by real-world scenarios like a zoologist identifying an unknown reptile and a reader seeking their next book, the dataset comprises over 3,000 entity-seeking queries derived from Wikipedia categories across films, books, plants, and animals. These queries incorporate set operations like intersection and complement, requiring systems to retrieve multi-answer sets with evidence attribution spanning different document sections.

The dataset construction process involves generating queries from category sets, paraphrasing them for fluency, validating their naturalness, and rigorously verifying entity relevance and document attribution through human annotation. Evaluation on QUEST reveals a significant performance gap, with existing sparse and dense retrievers, and even a T5-based reranker, struggling to effectively handle the complexity of queries with implicit set constraints. Analysis highlights that queries involving set intersection and difference pose the greatest difficulty. QUEST aims to facilitate the development of improved information retrieval systems capable of understanding and satisfying users' selective information needs, ultimately benefiting scenarios where users express complex preferences and constraints.</sample>
    <sample id="107">They were trained in a mixture of various languages (German, English, Chinese, etc.) and used to translate queries during inference.</sample>
    <sample id="108">This paper investigates the robustness of language model acceptability judgments, revealing that they are surprisingly sensitive to context, particularly when evaluating longer sequences. Traditional Minimal Pair Paradigm (MPP) evaluations, which rely on short, single-sentence inputs, may not accurately reflect a model's broader linguistic understanding. To address this, the authors revisit the MPP pipeline, extending it to assess acceptability across longer contexts by reconstructing sentences from datasets like BLiMP and SyntaxGym.

The study introduces three context scenarios: irrelevant (Wikipedia), same dataset (matched and mismatched structures), and different dataset. Results demonstrate that while judgments remain relatively stable with irrelevant context, acceptability scores fluctuate significantly when incorporating sentences from the same dataset, especially when structural matching occurs. This effect intensifies with increasing context length, potentially impacting newer models with large context windows. Analysis suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, indicating that current MPP evaluations may not fully capture a model's abstract linguistic knowledge. The findings highlight the need for more context-aware evaluation methods to accurately assess language model capabilities.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large, diverse dataset of natural language instructions, inputs, and outputs entirely without human annotation. This dataset addresses the limitations of existing methods that rely on reformulated academic benchmarks or costly user-generated data. The process leverages a pre-trained GPT-3 variant, prompting it with a small seed of examples from the Super-Natural Instructions dataset to generate new instructions and corresponding input-output pairs. Further diversification is achieved through automatic paraphrasing of instructions. The resulting dataset comprises 64,000 examples, expanding to 240,000 with paraphrases, exhibiting high creativity and diversity, including tasks beyond traditional NLP benchmarks like experiment verification and word invention. Analysis reveals that over 50% of generated examples are correct, with even incorrect examples proving valuable for training. Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions demonstrates superior performance compared to baselines trained on Super-Natural Instructions across multiple benchmarks, highlighting the efficiency and effectiveness of this automated data generation method. This work underscores the potential of language models to autonomously create high-quality training data, surpassing the limitations of human annotation in terms of creativity, diversity, and cost.</sample>
    <sample id="111">The provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="114">This work introduces Grouped Head Attention (GHT), a novel approach to compressing multi-head attention in large language models (LLMs), addressing the challenges of their heavy parameter count, long training times, and massive data requirements. Existing methods for multi-head attention redundancy optimization either sacrifice performance (homogenization), lack parameter efficiency (diversification), or leave significant redundancy unaddressed. GHT employs a divide-and-conquer strategy, first grouping attention heads through group-constrained training to promote intra-group similarity and inter-group separation. This is achieved using unsupervised hidden unit discovery and a combined homogenization and diversification loss. Subsequently, a Voting-to-Stay algorithm prunes redundant heads within each group, retaining only one per group.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 7%) and substantial parameter compression (up to 90%) compared to state-of-the-art baselines. Furthermore, a "LITE" model achieves 62% faster inference speed and 80% reduction in FLOPs while maintaining comparable performance. The authors suggest future work focusing on task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to further optimize LLMs by removing unnecessary parameters for specific applications, mirroring the efficient resource management of devices like smartphones.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">Example quality is more important.</sample>
    <sample id="118">This paper introduces SwitchMLM, a novel pretraining technique designed to improve performance on code-switched natural language processing (NLP) tasks. Code-switching, the mixing of languages within a single sentence (e.g., "Laptop, mere, bag, me, rakha, hai"), is common in linguistically diverse communities and poses a challenge for existing multilingual models like mBERT and XLM-R. SwitchMLM addresses this by focusing on "switch-points"—transitions between languages—during masked language modeling (MLM). Unlike standard MLM, SwitchMLM only masks tokens at these switch-points, but requires language identification (LID) tags, which are not always available. To mitigate this, the authors propose FrequencyMLM, a surrogate method using monolingual corpora to estimate LID tags.

Furthermore, the work incorporates architectural modifications, specifically residual connections from intermediate layers rich in switch-point information to the final layer, alongside an auxiliary LID-based loss to enhance language encoding. Probing experiments, utilizing linear and conditional probing, demonstrate that SwitchMLM and the proposed architectural changes effectively increase the representation of switch-point information within the model. Empirical results on sentiment analysis tasks across various language pairs show that the combined SwitchMLM/FrequencyMLM and ResBERT approach achieves state-of-the-art performance.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">The model leverages the cross-attention mechanism.</sample>
    <sample id="121">Saying the name of the song or its position (e.g., "the first one").</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">This research introduces MultiInstruct, a novel benchmark dataset designed to advance multi-modal zero-shot learning through instruction tuning. Addressing the scarcity of large-scale multi-modal instruction datasets, MultiInstruct comprises 62 diverse tasks spanning 10 categories, derived from 21 existing datasets and each equipped with five expert-written instructions. The study investigates the efficacy of instruction tuning on a unified multi-modal pre-trained model, OFA, demonstrating significant performance improvements on seen multi-modal tasks. Furthermore, the research explores transfer learning from natural instruction datasets, revealing benefits for both performance and a newly introduced metric called "sensitivity," which measures consistency in output generation across varying instruction wording. Experiments utilizing multiple instructions (five versus one) highlight the positive impact of instruction diversity on both performance and sensitivity reduction. The findings underscore the effectiveness of instruction tuning and transfer learning strategies in enhancing OFA's zero-shot capabilities. The authors are also expanding the dataset to include approximately 150 additional vision-language tasks, which will be publicly released.</sample>
    <sample id="124">This work introduces "TempReason," a novel benchmark dataset and training paradigm designed to comprehensively evaluate and improve the temporal reasoning capabilities of Large Language Models (LLMs). Recognizing that temporal reasoning is fundamental to understanding the real world, the study breaks it down into three levels: time-to-time, time-to-event, and event-to-event reasoning. Prior research has disproportionately focused on the time-to-event level. The TempReason dataset covers all three levels and spans a broad temporal range, incorporating data from Wikidata and Wikipedia. Experiments reveal biases in existing LLMs like ChatGPT and FLAN-T5-L, particularly regarding a preference for the 2000-2020 timeframe. To address these limitations, the authors propose a two-stage training strategy: temporal span extraction pre-training and time-sensitive reinforcement learning. The resulting "TempT5" model demonstrates significant performance improvements over baseline models, especially in Open Book and Reasoning QA settings. While TempT5 achieves state-of-the-art results, the study also identifies remaining performance fluctuations across different time periods, highlighting areas for future research focused on mitigating temporal reasoning biases within LLMs.</sample>
    <sample id="125">Yanis Labrak</sample>
    <sample id="126">Yes.</sample>
    <sample id="127">This paper introduces "Large Language Models Are Reasoning Teachers," a novel approach to transferring reasoning abilities from large language models (LLMs) to significantly smaller models. Chain-of-thought (CoT) reasoning, a technique enabling LLMs to solve complex tasks, is currently limited to very large models due to computational demands. To address this, the authors propose fine-tuning smaller models using step-by-step solutions generated by larger "teacher" models via CoT prompting. A key contribution is "Diverse Reasoning," a technique that generates multiple, slightly distinct reasoning paths from the teacher model using stochastic temperature sampling, leading to more robust training data for the student model.

Experiments across 12 tasks demonstrate that this "fine-tuned CoT" method enables smaller models (even those with only 0.3 billion parameters) to perform complex reasoning, outperforming prompt-based baselines and vanilla fine-tuning. The results highlight the scalability of the approach, with performance improvements achievable through increased dataset size, better teacher models, or larger student models, albeit with associated trade-offs between development and inference costs. The authors provide code and data, including OpenAI inference logs, encouraging further research into this accessible and effective distillation method for transferring emergent abilities to smaller, more deployable models.</sample>
    <sample id="128">This paper introduces KITMUS, a diagnostic test suite designed to evaluate knowledge integration capabilities in natural language understanding models. These models often rely on both knowledge acquired during pretraining and knowledge provided at inference time, a crucial ability for knowledge-intensive tasks like coreference resolution. KITMUS focuses on coreference resolution, presenting scenarios requiring both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases in law courts"). The test suite features three settings—Background-Pretrain, Background-Both, and Background-Inference—varying the availability of these knowledge types. The Background-Inference setting, particularly, simulates situations where necessary background knowledge is absent from pretraining data.

Evaluations using human participants and established coreference resolution models (C2F and BERT4Coref) reveal that models initially struggle without task-specific training, relying on superficial cues. While training on KITMUS improves performance, even the best models demonstrate difficulty reliably integrating background knowledge provided solely at inference time, especially when using fictional or novel concepts. The findings highlight a gap in current models' ability to effectively reason over diverse knowledge sources and suggest a need for improved knowledge integration techniques. The dataset and code are publicly available.</sample>
    <sample id="129">The word "warrior" is usually associated with men, so when describing a woman warrior, people typically specify "woman warrior" and mark the term with "woman."</sample>
    <sample id="130">Non-transformer models.</sample>
    <sample id="131">The text does not mention the names of the testing datasets.</sample>
    <sample id="132">Two.</sample>
    <sample id="133">Multiple modalities.</sample>
    <sample id="135">ABC-Eval is a novel dimensional evaluation approach for conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI, designed to provide a more precise and reliable assessment than traditional human evaluation methods. Unlike methods relying on Likert scales or pairwise comparisons, ABC-Eval focuses on annotating specific behaviors exhibited by dialogue models, such as irrelevance, contradiction, hallucination, and lack of empathy. This approach aims to reduce subjectivity and comprehensively measure thematic errors in chat models.

The study evaluated four state-of-the-art models across 100 human-bot conversations, comparing ABC-Eval against Likert ratings (turn and dialogue-level) and dialogue-level pairwise comparisons. Results demonstrated that ABC-Eval labels exhibited higher inter-annotator agreement and were more predictive of overall conversation quality, as evidenced by linear regression analysis. Furthermore, ABC-Eval metrics captured unique aspects of chat quality, collectively explaining over 25% of conversation quality, significantly outperforming the predictive power of existing methods. The research quantified persistent challenges, revealing error rates like 20% for common sense violations and 15% for irrelevant information. ABC-Eval offers a higher-resolution evaluation, facilitating more informed comparisons and advancements in conversational AI.</sample>
    <sample id="136">This work introduces FERMAT, a novel evaluation set designed to provide a more informative alternative to traditional accuracy-based benchmarks for assessing numerical reasoning capabilities of language models. Current benchmarks often fail to reveal the specific strengths and weaknesses of models, particularly those with limited parameters (e.g., 3 billion). FERMAT addresses this by evaluating models across arithmetic types, focusing on number understanding, mathematical operations, and training dependency. The dataset, derived from CommonCore and Illinois datasets, incorporates variations in number representation (integers, decimals) and operation complexity.

Baseline zero-shot evaluations revealed poor performance across all aspects, highlighting the limitations of existing benchmarks. Fine-tuning with 200,000 generated examples improved performance, demonstrating the value of targeted training data. Further analysis of training dependency showed that models struggle even with expressions encountered during training, suggesting sensitivity to linguistic variations. Finally, experiments with diversified training templates, incorporating data from GSM8K and AQUA, significantly boosted performance, emphasizing the importance of both language and mathematical diversity. The findings suggest that number encoding and tokenization are key areas for future improvement in numerical reasoning models.</sample>
    <sample id="137">This paper introduces Tell2Design, a novel dataset and task focused on language-guided floor plan generation. Addressing the need for design tools that incorporate user requirements specified in natural language, the research frames floor plan generation as a sequence-to-sequence problem, moving beyond traditional artwork-focused text-conditional image generation. The Tell2Design dataset comprises 5,051 human-annotated and 76,000 artificially generated language instructions describing floor plan semantics, geometry, and topology, associated with corresponding 2D floor plans. A transformer-based sequence-to-sequence model, initialized with T5, is proposed as a baseline, demonstrating significantly improved performance (Micro IoU of 54, Macro IoU of 53) compared to existing text-conditional image generation methods. The study highlights challenges including strict design constraints, understanding document-level text, and handling ambiguous instructions. Results reveal a language distribution gap between artificial and human instructions, but demonstrate that leveraging both types of data improves model performance. The work establishes a foundation for future research in language-guided design generation, enabling users to "tell" instructions and facilitating more interactive and user-centric design processes.</sample>
    <sample id="138">The ability to integrate and use both pretrain-time and inference-time knowledge.</sample>
    <sample id="139">Ying and Zhiyang</sample>
    <sample id="140">Yes, crowd-sourced workers were asked to find and revise incorrect samples in CoScript.</sample>
    <sample id="141">They support limited types of context-dependent translations and limited sets of languages, often relying on domain knowledge and human curation.</sample>
    <sample id="143">Wait-k strategy, Local Agreement, and state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The authors are affiliated with Nantes University Hospital.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This paper addresses the significant issue of omission in dialogue summarization, a prevalent problem hindering the real-world applicability of current large language models despite their fluency. Analysis reveals that approximately 70% of generated summaries exhibit omissions, with critical information randomly distributed throughout dialogues, posing a challenge for models to identify key details. To facilitate research in this area, the authors introduce the OLDS dataset, a novel resource providing high-quality utterance-level omission labels across five domains. The dataset was constructed using diverse candidate summaries generated by various models and validated through human evaluation.

The study explores three baseline architectures for omission detection—pairwise classification, sequence labeling, and pointer networks—achieving an F1-score of around 50%, highlighting the task's difficulty. Furthermore, the research investigates the potential of using detected omissions to refine summaries through a post-editing method. Results demonstrate a substantial performance boost when omission content is incorporated, suggesting that omission detection is a valuable task and that refinement based on detected omissions offers a promising avenue for improving dialogue summarization quality. The OLDS dataset is publicly available to support further research.</sample>
    <sample id="147">Three.</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">Meeting transcripts represent a rich, untapped resource for NLP research, particularly due to their length, domain specificity, and information density. While prior work has focused on summarization and action item extraction, this paper introduces MeetingQA, a novel extractive question answering (QA) dataset leveraging the inherent QA component of meeting discussions. MeetingQA comprises 7.7K questions derived from the AMI corpus, with answers annotated to reflect realistic meeting scenarios, including multi-span answers, contributions from multiple speakers, and rhetorical questions. The dataset exhibits a challenging distribution, with 30% unanswerable questions, 40% multi-span answers, and 48% multi-speaker answers, frequently involving disagreement.

Experiments demonstrate a significant performance gap between current QA models and human performance, even with fine-tuning. Surprisingly, short-context models outperformed long-context models, and multi-span models showed comparable or slightly lower performance than single-span models. Silver data augmentation using automatically annotated interview questions improved zero-shot performance, with instruction-tuned models like FLAN-T5 achieving competitive results. Error analysis revealed difficulties in identifying rhetorical questions and speaker attribution, highlighting the dataset's complexity and potential for future research. MeetingQA provides a valuable benchmark for advancing QA models in the challenging domain of meeting conversations.</sample>
    <sample id="152">This presentation introduces new language models specifically designed for classical philology, addressing limitations in existing models for Ancient Greek and Latin. The project aims to improve model comparability, push state-of-the-art performance, explore diverse architectures, and enable multilingual processing. Two monolingual models, GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder), were developed for Ancient Greek, alongside multilingual equivalents, PhilBERTa and PhilTa, trained on Greek, Latin, and English data. A novel, high-quality pre-training corpus for Ancient Greek was created by leveraging the Internet Archive and employing a unique stop-word identification technique to overcome OCR challenges.

Rigorous benchmarking on part-of-speech tagging, dependency parsing, and lemmatization tasks demonstrates significant performance gains over existing models for both languages. Notably, the encoder-decoder architecture (GreTa) achieved a 5 percentage point improvement in Ancient Greek lemmatization. Semantic and world knowledge probing revealed strong performance, though multilingual models did not show a substantial advantage over monolingual counterparts. The research highlights the distinct behavior of T5 encoders compared to traditional encoder-only models and underscores the potential of these new models for advancing research in classical philology.</sample>
    <sample id="153">This work investigates ambiguities inherent in prompts given to text-to-image generative models, which can lead to images that don't align with user intention. To address this, the authors propose frameworks for both mitigating and evaluating these ambiguities. The research begins by curating a benchmark dataset, a modified version of LAVA, encompassing various ambiguity types. A prompt disambiguation framework is then introduced, utilizing language models to either generate clarifying questions for users or produce alternative visual interpretations. Users select the option reflecting their intended meaning, resulting in a disambiguated prompt. Finally, an automatic evaluation framework assesses image faithfulness by employing a Visual Question Answering (VQA) model, comparing images generated from original and disambiguated prompts against the user's stated intention. Key findings demonstrate disparities in ambiguity resolution across different types, a positive impact of the proposed disambiguation framework on faithful image generation, and strong agreement between the automatic evaluation and human assessments. This research contributes a valuable dataset and evaluation methodology for improving the reliability and user-centricity of text-to-image models.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Dialogue summarization, a challenging task in text summarization, aims to condense multi-participant dialogues into concise summaries. Existing methods often rely on pre-computed static graph structures derived from external linguistic tools, which are prone to errors and lack adaptability. This work introduces SDDS, a novel model addressing these limitations by fusing static and dynamic graph structures.

SDDS employs an utterance encoder to generate vector representations, then constructs static graphs using four heuristic methods: discourse parsing, key co-occurrence, speaker relationship modeling, and utterance position analysis. A Static-Dynamic Graph module combines these static graphs and utilizes a dynamic graph module with multi-head attention to capture semantic relationships based on deep vector representations. Finally, a pre-trained language model, enhanced with a dual cross-attention mechanism incorporating graph attention, generates the summary.

The model effectively integrates static dialogue structure information with dynamically learned semantic relationships, leading to improved summarization performance. The code and data are publicly available, facilitating further research in this area.</sample>
    <sample id="158">This paper introduces "Dual Cache," a novel approach to improve the efficiency and performance of neural coreference resolution, particularly for long documents. Coreference resolution, the task of linking mentions to their corresponding entities, traditionally suffers from quadratic complexity due to exhaustive pairwise mention comparisons. Cache-based methods mitigate this, but standard Least Recently Used (LRU) eviction policies struggle with long documents exhibiting topic shifts and scattered entity mentions, leading to high cache miss rates.

Dual Cache addresses this by employing two caches: a local cache using LRU for local entities and a global cache using Least Frequently Used (LFU) for globally relevant entities. The model classifies mentions, adding qualified entities to the global cache and others to the local cache, triggering eviction policies as needed. Experiments on four benchmarks demonstrate Dual Cache's superior performance compared to baselines, even those with unbounded memory, especially on long, book-level documents. Furthermore, Dual Cache significantly reduces cache misses and achieves the highest performance-to-cost ratio, showcasing its effectiveness in balancing efficiency and accuracy for coreference resolution in lengthy texts.</sample>
    <sample id="160">An unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">Weakly supervised learning uses cheaper, noisy labels (e.g., heuristic rules, crowdsourcing) instead of manual annotations.</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive commonsense reasoning. Traditional approaches rely on supervised training, which is hindered by the subjective and noisy nature of human annotations for plausible explanations. LiPoR addresses this challenge by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, eliminating the need for explicit plausibility labels. To further refine the selection of explanations, LiPoR incorporates a regularization term based on the principle of mutual exclusivity—the understanding that explanations in abductive reasoning are often mutually exclusive. This regularizer encourages the model to favor a subset of explanations, preventing the assignment of probability mass to an excessive number of possibilities. Experiments on the AlphaNLI dataset demonstrate that LiPoR significantly outperforms existing zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement over a strong GPT-3 baseline. The method offers a promising avenue for abductive reasoning without relying on costly and potentially unreliable human annotations.</sample>
    <sample id="166">This paper introduces NDCR, a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Addressing the limitations of existing visual language models that struggle with complex descriptions, NDCR draws inspiration from the Divide-and-Conquer strategy and Dual-Process Theory of human cognition. The framework integrates an analogical reasoning "System 1" (Visual-Linguistic Interactor) with a logical reasoning "System 2" (Neural-Symbolic Reasoner). Initially, a Proposition Generator decomposes complex text into simpler propositions. System 1 then interacts visual and propositional information, generating matching scores and reasoning states. System 2, comprising a negation executor and conjunction operation, integrates these states to derive final inferences. Finally, the outputs of both systems are combined to produce the ultimate retrieval solution. Experimental results demonstrate that NDCR significantly outperforms existing baselines, and ablation studies confirm the effectiveness of each module. Case studies showcase the framework's ability to present intermediate inference states, highlighting its interpretable processing. The work suggests that neural-symbolic computation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for enhancing compositional reasoning in large language models.</sample>
    <sample id="167">Manually and with automatic alignment methods.</sample>
    <sample id="168">It was created by collecting data from Reuters News in 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents the first systematic study of prompting strategies for machine translation using PaLM, a 540 billion-parameter large language model. The research evaluates PaLM's translation capabilities using established machine translation best practices, including current test sets and state-of-the-art neural MT metrics, alongside human evaluation. The study highlights the significant impact of prompting on translation performance, demonstrating that prompt selection is crucial, with differences in prompts leading to substantial BLEURT score variations. A 5-shot prompting strategy, where sentences are marked with language labels, proved effective. The findings emphasize that example quality within prompts is more important than similarity to the source sentence, favoring examples from curated development data over noisy training data. While PaLM achieves translation fluency comparable to state-of-the-art systems, it lags in accuracy, exhibiting a tendency to omit parts of the source sentence. Despite this, PaLM's output demonstrates a lower rate of stylistic awkwardness. The research concludes that PaLM's translation performance is competitive with commercial systems, offering valuable insights for optimizing prompt selection and improving the accuracy of large language models in machine translation.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset designed to advance argument quality analysis, addressing limitations found in existing resources. Unlike datasets primarily sourced from crowdsourcing, ArgAnalysis35K boasts 35,000 argument-analysis pairs, prioritizing high-quality arguments drawn from expert and tournament debaters. This focus ensures greater argument quality and a broader range of perspectives.

The dataset distinguishes itself through thematic diversity, moving beyond pre-selected motions to encompass 24 themes identified through expert consultation and circuit experience. A key innovation is the introduction of "analysis," a concept that combines claims and premises into a cohesive explanatory unit, enriching the dataset's depth. To mitigate annotator bias, ArgAnalysis35K employs instance-based annotator reliability, selectively filtering judgments based on potential topical biases.

Finally, a relevance model assigns scores indicating an argument's applicability across various themes, recognizing that arguments often transcend single motions. This allows for a more nuanced understanding of argument relevance. ArgAnalysis35K represents a significant step forward in argument quality analysis, offering a larger, more diverse, and more reliable resource for NLP research.</sample>
    <sample id="175">It approximates the problem with a GPU-friendly continuous relaxation that allows backpropagation.</sample>
    <sample id="176">Fairness is defined as whether the model's performance varies based on the political leaning of the input data or the demographics/political leaning of the news media it's evaluating.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">This research addresses the challenge of improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs), which currently struggle with false-belief tasks. We introduce SymbolicToM, a novel inference-time method that leverages explicit graphical representations to enhance ToM capabilities without requiring model fine-tuning. SymbolicToM constructs multiple graphs representing the mental states of different characters within a story, enabling the model to reason about beliefs and expectations. These graphs are generated using readily available Natural Language Inference and Open Information Extraction models.

The method efficiently answers ToM questions by detecting entities, retrieving relevant belief graphs, and transforming the question into a factual query over the graph. Experiments across various LLMs (including GPT-3, Macaw, and Flan-T5-XXL) demonstrate significant performance gains compared to supervised baselines, achieving up to a 65-point accuracy increase. Furthermore, SymbolicToM exhibits strong generalization capabilities, maintaining performance on out-of-domain datasets designed to test storage structure and linguistic diversity, while supervised models degrade substantially. This plug-and-play approach offers interpretable reasoning and improves out-of-the-box LLM performance in complex story understanding scenarios.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints in goal-oriented scripts. While large language models (LLMs) effectively decompose abstract goals, planning for specific goals like "make a chocolate cake" remains challenging. The authors define constrained language planning, where abstract goals are realized with multifaceted constraints, and propose an "over-generate-then-filter" method to improve LLM planning quality. This involves generating multiple scripts, then using a filter model based on semantic similarity and keyword matching to select the most faithful ones. To enable training smaller, specialized models, they introduce CoScript, a dataset of 55,000 specific goals and scripts generated using LLMs and refined through crowd-sourced validation. CoScript demonstrates high diversity in constraint types. Notably, fine-tuning a T5 model on CoScript outperforms larger LLMs, highlighting the potential of smaller models when trained on high-quality, constrained planning datasets. The work establishes a new research direction and provides a valuable resource for advancing language planning research.</sample>
    <sample id="182">Tropicalism connects to a trope where Latina women are portrayed as vibrant and curvaceous.</sample>
    <sample id="183">The authors drew inspiration from a study where prompts similar to those used for the LLMs were given to human subjects.</sample>
    <sample id="184">CXMI (and its extension, Pointwise CXMI).</sample>
    <sample id="185">DrBERT is trained on NACHOS, a dataset of medical data crawled from the web, while ChuBERT is trained on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Two.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="189">To understand users’ language when they want to make a choice.</sample>
    <sample id="190">By learning from the embeddings.</sample>
    <sample id="191">Three.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) addresses the challenge of training large language models by simultaneously achieving fast convergence and low memory usage. Existing adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from performance penalties due to errors in their updates. CAME builds upon non-negative matrix factorization (NMF) principles, similar to Adafactor, but introduces a novel confidence-guided approach to mitigate these errors.

Inspired by the discrepancies between momentum and update vectors, CAME calculates an instability matrix based on the residual between predicted and generated updates. This instability matrix is then used to adaptively scale the momentum, effectively reducing the impact of erroneous updates and stabilizing the training process. Experiments on BERT, GPT-2, and T5 models demonstrate that CAME significantly outperforms Adam and Adafactor, achieving higher validation accuracy and improved performance with large batch sizes (8K to 32K). Furthermore, CAME exhibits reduced memory footprint compared to other optimizers, including Adam and SM3, making it a highly effective and memory-efficient solution for training large language models.</sample>
    <sample id="193">The text does not mention the number of annotators used.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">This paper introduces RoHT, a novel framework for Explainable Question Answering (XQA) called "Reasoning over Hierarchical Question Decomposition Tree." RoHT addresses the limitations of existing XQA approaches, namely neuro-symbolic methods constrained by incomplete knowledge bases (KBs) and decompose-based methods struggling with the diversity of natural language. RoHT integrates knowledge from heterogeneous sources—KBs and text corpora—by leveraging question decomposition. The framework tackles two key challenges: determining the optimal granularity of decomposition and navigating the uncertainty inherent in both decomposition and answering.

RoHT operates in two stages: first, it constructs a Hierarchical Question Decomposition Tree (HQDT) representing the compositional structure of a complex question, breaking it down into atomic questions. Second, it performs probabilistic reasoning over the HQDT, recursively traversing from root to leaves, selecting appropriate knowledge sources (KB, text, or child nodes) and aggregating answers with associated probabilities. Evaluations on KQA Pro and Musique datasets demonstrate RoHT's superior performance compared to existing methods, highlighting the benefits of integrating knowledge from both KBs and text corpora, and the effectiveness of explicit question decomposition.</sample>
    <sample id="196">"I saw Bart and Lisa."</sample>
    <sample id="197">Four state-of-the-art chat models.</sample>
    <sample id="198">Large language models are coming up with longer and longer context windows, so it's crucial to evaluate models' acceptability throughout the context window.</sample>
    <sample id="199">Yes, English performance dropped in seven datasets when training in a multilingual fashion.</sample>
    <sample id="200">No, they don't necessarily know about the entities.</sample>
    <sample id="201">State-of-the-art, neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">The presentation does not mention if the performance drop impacts specific NER types.</sample>
    <sample id="203">It's increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="204">The presentation does not specify whether BLOOM and Codex were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="205">This work investigates the propagation of political biases from pretraining data to language models and their downstream applications, highlighting potential fairness issues in NLP. Analyzing prominent news sources within the C4 corpus reveals significant representation of diverse political perspectives in language model training data, a double-edged sword that can both broaden knowledge and introduce bias. The study proposes a novel evaluation framework using political questionnaires to assess language model political leaning, finding that models exhibit varying ideologies, with GPT-4 demonstrating the most liberal stance. Controlled experiments involving further pretraining on partisan corpora demonstrate a direct correlation between training data and model political biases, and reveal that models can capture societal polarization trends. Crucially, the research evaluates the impact of these biases on downstream tasks like hate speech and fake news detection, revealing performance disparities based on political leaning – left-leaning models perform better on detecting hate speech against minority groups, while right-leaning models are better at detecting hate speech against dominant groups, and similar patterns emerge for fake news detection. The findings underscore a critical dilemma: mitigating bias risks censorship, while ignoring it leads to unfair outcomes and potential marginalization.</sample>
    <sample id="206">A model fine-tuned on CE tasks followed by further fine-tuning on debate.</sample>
    <sample id="207">The latest test sets to avoid overlap with the language model's training data.</sample>
    <sample id="208">Three.</sample>
    <sample id="209">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes.</sample>
    <sample id="212">One.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">This paper presents a novel argument for symmetric coordination structures, challenging asymmetric approaches prevalent in theories like Universal Dependencies and Mel'čuk's Meaning-Text Theory. The argument is grounded in the principle of dependency length minimization, which favors shorter dependencies between words. The research leverages data from the enhanced Penn Treebank to analyze coordination patterns in English.

Previous observations confirm that left conjuncts tend to be shorter, a tendency that strengthens with increasing length differences between conjuncts. However, this study reveals a crucial condition: this preference for a shorter left conjunct only occurs when the governing verb is positioned to the left of the coordination or is entirely absent. When the governing verb precedes the coordinated elements, the tendency vanishes.

The findings, measured in characters, syllables, and words, consistently demonstrate this relationship. This observation provides empirical support for symmetric coordination structures, where no single conjunct is inherently prioritized, and challenges asymmetric models that designate a head based on position. The paper concludes by advocating for symmetric approaches and invites further discussion at the poster session.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or rely on labeled data. This work introduces DCG, a Disentangled Controllable Generation model, designed to tackle compositional generalization in multi-attribute dialogue. DCG learns attribute concepts from seen values using a disentanglement loss, enabling generation of novel attribute combinations. A novel, reference-free evaluation framework, MAE, is proposed to assess controllability across varying attribute granularities. Experiments on two benchmarks demonstrate DCG's superior performance in attribute controllability and text quality compared to baselines, including CTRL. The model utilizes attribute-oriented and task-oriented prompts within a DialoGPT framework, enhanced with pseudo combinations and disentanglement learning to improve generation diversity and compositional generalization. MAE’s effectiveness is validated through correlation analysis with human judgments, showcasing its ability to evaluate both discrete and continuous attributes. Visualization confirms the model's capacity to disentangle attribute combinations and generalize from seen to unseen attribute values, highlighting the benefits of shared embedding mapping for learning attribute concepts.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">This paper introduces a novel approach for financial report analysis, specifically targeting Form 10-K filings, to reduce the reliance on manual information extraction. Recognizing the high degree of textual similarity between consecutive annual reports (approximately 80% token overlap), the authors propose a "highlighting" task designed to identify key differences between a target report and its preceding year's report. This task involves a compare-and-contrast multistage pipeline that predicts the importance of words reflecting relationships between the reports. The pipeline consists of document segmentation, relation recognition classifying pairs into syntactic/semantic similarities (β), revised meanings, and mismatched information, and a two-stage fine-tuning process. Initially, the model undergoes out-of-domain fine-tuning using the eSNLI dataset, followed by in-domain fine-tuning leveraging "pseudo-positive" labels derived from revised pairs within the financial reports. The model's performance is evaluated on both eSNLI and a newly released FINAL dataset, demonstrating strong results and generalization capabilities, particularly with mismatched pairs. The work contributes a highlighting task, a dataset, and a domain-adaptive model for efficient financial signal discovery.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">The paper does not specify all language pairs analyzed, but it mentions German to English as one example.</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates methods to improve question answering (QA) performance when transferring models trained on a general domain (Wikipedia) to new, specialized domains. The core challenge lies in domain shift, where the source model struggles to generalize due to differences in vocabulary, reasoning patterns, or context. The study explores both zero-shot and few-shot data interventions to address this. Few-shot methods leverage large language models to generate training examples from a limited number of target domain samples, while zero-shot techniques manipulate question and context formats and answer distributions to control model learning.

The research identifies three types of dataset shift: "No shift," "Concept shift," and "Covariate shift," based on the compatibility of the retriever and reader models with the target domain. A compatibility measure, based on likelihood scores, is developed to categorize datasets. The findings reveal that few-shot adaptations are broadly effective across various domains, while zero-shot interventions are particularly beneficial for datasets exhibiting concept or covariate shift. Ultimately, the study demonstrates significant improvements in reader performance (up to 24%) by strategically applying data interventions tailored to the specific type of domain shift encountered.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART and base mBART.</sample>
    <sample id="225">53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">Two.</sample>
    <sample id="227">Current language models excel at general NLP tasks but struggle with grounded language understanding—mapping natural language to executable plans within specific environments. This challenge stems from a lack of grounding during pre-training, hindering their ability to produce valid and grammatical plans, as seen in applications like smart assistants and robotic control. This paper introduces Pangu, a novel framework that shifts the focus from plan generation to discrimination. Instead of directly generating plans, Pangu utilizes a symbolic agent to propose candidate plans, which are then scored and ranked by a language model. This approach alleviates the language model's burden of ensuring plan validity and grammar. The framework is demonstrated on knowledge-based question answering, showcasing strong performance across various language models (BERT, T5, Codex) and learning paradigms (fine-tuning, in-context learning), including remarkable sample efficiency. Notably, Pangu exhibits robustness under non-i.i.d. settings, attributed to autoregressive models overfitting to training structures, unlike Pangu's consistent probability distributions across seen and unseen structures. The core takeaway is that discrimination, rather than generation, represents a more effective strategy for leveraging language models in grounded language understanding.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This paper investigates the detection of improvable claims in argumentative writing, a crucial aspect of effective communication. We introduce two novel tasks: Suboptimal-Claim detection (identifying claims needing revision) and Claim Improvement Suggestion (classifying quality issues). Our approach leverages revision-based data from collaborative online debate platforms like Kialo, analyzing revision histories to infer claim quality. We explore the challenges inherent in this methodology, including representativity and reliability of data, model complexity for capturing subtle revisions, the importance of contextual information (debate-wide, parent claim, domain knowledge), and biases introduced by users and topics. 

We systematically analyze various strategies to address these challenges, comparing their effectiveness across the two tasks. Our findings demonstrate that revision-based data can be effectively utilized for identifying suboptimal claims, and that modeling the distance between claim versions is particularly beneficial. Furthermore, we observe that the relevance of contextual information varies depending on the task and the specific quality issues present. Ultimately, this work provides valuable insights into harnessing revision patterns to support and improve argumentative writing.</sample>
    <sample id="231">A dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) aims to provide real-time cross-language communication, but current models face challenges including complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency targets. This paper introduces EDAtt (Encoder-Decoder Attention), a novel strategy that leverages existing offline speech translation models without retraining or architectural modifications. EDAtt dynamically controls latency by selectively emitting partial translations based on the cross-attention mechanism between audio input and textual output. Specifically, a word is emitted only when its attention weights are sufficiently stable over the last lambda speech frames, indicating sufficient information has been processed. This approach allows a single model to operate across various latency regimes. Experiments on German demonstrate that EDAtt outperforms existing strategies applied to offline models, such as Wait-k and Local Agreement, achieving higher translation quality (BLEU score) with lower latency, both in terms of average and computationally-aware lagging. The code, models, and simultaneous output are publicly available to promote reproducibility and further research in SimulST.</sample>
    <sample id="234">Prompting has a big influence on performance, with differences of up to 40 BLEURT points observed with different prompts.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">The 5 expert-written instructions are randomly combined with each instance during training.</sample>
    <sample id="237">A diagnostic test suite for knowledge integration, featuring a coreference resolution task.</sample>
    <sample id="238">MeetingBank, a new benchmark dataset for meeting summarization, has been created by researchers at the University of Central Florida. Addressing the need for datasets tailored to meeting-specific summarization, MeetingBank comprises 1,366 City Council meetings and nearly 7,000 instances, including transcripts, reference summaries, and associated URLs. The dataset was constructed using Speechmatics API for transcription and meticulous data collection from city council websites.

Analysis of MeetingBank reveals that meeting summaries often prioritize verbatim points over abstraction, with coverage scores generally between 0.7 and 0.9. Evaluation of various summarization systems, both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet), alongside GPT-3, demonstrates the potential of extractive methods and the strength of DialogLM for long dialogue summaries. While GPT-3 performed poorly on automatic metrics, human evaluation revealed surprisingly high scores for fluency and coherence. 

The research highlights the importance of capturing key discussion points in meeting summaries and suggests a need for improved automatic evaluation metrics that better reflect human preferences. MeetingBank is released to the research community as a valuable resource for developing advanced meeting summarization technologies and gaining insights into civic decision-making processes.</sample>
    <sample id="241">This paper introduces a novel human-in-the-loop evaluation framework for early misinformation detection, addressing shortcomings in existing automated approaches. Current systems often rely on unrealistic datasets and neglect the crucial role of human content moderators in the noisy, real-time environment of social media. Our framework emphasizes end-to-end systems integrating human feedback throughout the detection process, rather than treating humans as a final validation step. We demonstrate this framework with a case study focused on COVID-19 treatment misinformation, building a system with two components: claim detection using a T5 question-answering model and stance classification using a BERT model to identify policy violations. 

Evaluation focuses on "early detection"—identifying unapproved treatments before they appear in debunking news—and assesses policy violation detection accuracy and human workload. Results show our system effectively detects treatments before debunking (demonstrated with examples) and achieves 65% accuracy in identifying policy violations. Crucially, the system enables the confirmation of 124.2 policy violations per human hour, significantly improving efficiency. This work provides a more realistic evaluation methodology and a valuable external perspective on misinformation detection system development, encouraging the creation of future human-centric systems.</sample>
    <sample id="242">Human evaluation (selecting which conversation is better or rating with Likert scales) and evaluating multiple dimensions of chat quality using comparative or Likert scale methods.</sample>
    <sample id="243">5</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">This work, "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization," addresses the challenges of recruiting reliable annotators on Amazon Mechanical Turk (MTurk) for summarization tasks, where automatic metrics can be unreliable and best practices are unclear. We propose a two-step pipeline to identify high-agreement workers: a Qualification Task assessing multi-dimensional evaluation skills and an Endurance Task evaluating workload capacity. This pipeline yielded a small but highly reliable group of 12 workers (4 gold, 8 silver) representing 6% of the initial 200 participants, achieving an inter-annotator agreement (IAA) exceeding that of experts (Krippendorff's Alpha = 0.443). A subsequent reference-based task further validated their performance (Alpha = 0.534). Compared to baseline approaches like MACE and CloudResearch MTurk workers, our pipeline achieved comparable or superior agreement (Alpha = 0.513 and 0.380, respectively) at a lower cost. Analysis revealed a significant correlation between our pipeline workers and CloudResearch workers, and a strong correlation between GPT models and expert judgments. Ultimately, our pipeline demonstrates a cost-effective method for obtaining high-agreement annotations, avoiding wasted resources on unreliable workers, and offers a valuable best practice for large-scale annotation projects. Future work will focus on improving worker correctness and exploring broader applications across tasks, languages, and platforms.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">This paper introduces FACTKG, a novel dataset and task for Knowledge Graph-Based Fact Verification. Existing fact verification datasets primarily rely on text (FEVER, VitaminC) or tables (TabFact, InfoTabs) as evidence, lacking a focus on knowledge graphs and natural language claims. FACTKG addresses this gap by leveraging DBpedia as a knowledge source, presenting claims in both written and colloquial styles to enhance practical applicability. The dataset features two labels: SUPPORTED and REFUTED, and requires retrieving relevant evidence from DBpedia to verify claims.

The task incorporates five reasoning types: one-hop, conjunction, existence, multi-hop, and negation, reflecting diverse reasoning patterns. FACTKG's design emphasizes the intuitive nature of KG evidence, enabling more reliable reasoning compared to text or table-based approaches, and highlights its practical utility in applications like dialogue systems requiring KG consistency checks. The dataset was constructed using colloquial style transfer and presupposition templates. Experimental results demonstrate that models utilizing graph evidence, such as the GEAR model, significantly outperform baselines that rely solely on claims, validating the effectiveness of FACTKG for advancing fact verification research. The dataset is publicly available for download.</sample>
    <sample id="248">No, the annotators come from 87 countries and over 1000 annotators, providing a diverse set of demographics.</sample>
    <sample id="249">Noise was added while preserving the relevant structure.</sample>
    <sample id="250">Evaluating multiple aspects of chat quality to understand strengths and weaknesses on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">U-CREAT is a novel unsupervised pipeline for Prior Case Retrieval (PCR) designed to address the challenges of efficiently identifying relevant past precedents in the increasing volume of legal cases. This work introduces two key contributions: the IL-PCR dataset, a new benchmark for PCR tasks comprising 7,070 Indian legal cases, and the U-CREAT pipeline itself. U-CREAT leverages an event-based approach, extracting events from legal documents using dependency parsing to form subject-verb-object triplets. These events are then used to construct an interaction matrix, enabling retrieval models to rank candidate documents.

Experiments across various models, including count-based, transformer-based (including legal-specific BERT models), and event-based approaches, demonstrate that event-based models, particularly the Event Filtered Documents model, significantly outperform baselines and existing transformer-based methods in terms of both F1 score and inference time. Notably, U-CREAT achieves state-of-the-art performance on the COLIEE’21 dataset, showcasing its generalization capabilities across legal systems. This work highlights the effectiveness of unsupervised event extraction for PCR and paves the way for future advancements in legal information retrieval.</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed to detect signs of mental disorders in social media posts. Addressing the challenge of limited annotated data, DisorBERT leverages knowledge transfer from general language models (like BERT) to specialized domains: social media language and mental health discourse. The model incorporates guided masking, encouraging it to focus on key words during training. This two-stage approach first learns social media language and then specializes in identifying mental disorder indicators.

Evaluated on the eRisk dataset, DisorBERT demonstrates a strong balance between precision and recall, outperforming baseline methods and even MentalBERT, a model trained on a larger dataset. Analysis of predicted words reveals DisorBERT's tendency to generate terms with negative psychological connotations, aligning with mental health concerns. Visualization of attention scores on user posts highlights the model's ability to identify relevant keywords like "anxious" and "medication" in individuals exhibiting signs of depression.

The research suggests that combining double domain adaptation and guided masking is an effective strategy for mental disorder detection in social media. Future work will explore diverse lexical resources and integration of clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">Document-level relation extraction (DocRE) benefits from distant supervision (DS) but suffers from noisy labels inherent in DS data. Existing methods using pseudo-labels risk amplifying this noise. This paper introduces a novel framework, "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," to address this challenge. Our approach trains a pre-denoising DocRE model with both DS and human-annotated data to generate pseudo-labels, then employs uncertainty estimation to filter unreliable predictions. Recognizing the possibility of overlapping relations between entities, we propose an instance-level uncertainty estimation method to capture uncertainty scores for each relation. To handle the long-tail distribution of relation types, we dynamically adjust class uncertainty thresholds for re-labeling pseudo-labels. A multi-phase training strategy iteratively refines the DS data. We leverage Monte Carlo dropout to model uncertainty within the DocRE model, modifying existing uncertainty estimation formulas to better distinguish overlapping relations. Experiments on public datasets demonstrate significant performance improvements over strong baselines, highlighting the effectiveness of our uncertainty-guided denoising framework in enhancing DocRE accuracy.</sample>
    <sample id="255">It's crucial for zero and one-shot prompting.</sample>
    <sample id="257">Four state-of-the-art chat models.</sample>
    <sample id="258">This work explores the potential of large language models (LLMs) as a viable alternative to traditional human evaluation in natural language processing. Recognizing the instability and reproducibility challenges inherent in human evaluation, the researchers investigated whether LLMs, guided by natural language instructions, could provide comparable and reliable ratings. While LLM-based evaluation isn't entirely novel, the authors assert that their approach was unique at the time of submission.

The study focused on evaluating stories generated by GPT-2 and human writers, assessing them across four attributes: grammar, coherence, likability, and relevance. LLMs—T0, InstructGPT (curie and davinci), and ChatGPT—were used to rate the stories, with their outputs parsed to extract ratings. To validate the LLM ratings, they were compared against ground-truth ratings obtained from English teachers, who served as expert human evaluators using identical instructions and stories.

Results indicated that English teachers preferred human-written stories over those generated by GPT-2. Notably, Davinci and ChatGPT demonstrated a clear preference for human-written text, mirroring the human evaluators' judgments. This suggests that certain LLMs can indeed serve as a useful substitute for human evaluation, offering a potentially more stable and reproducible method. The paper further addresses questions regarding agreement with human raters, instruction sensitivity, sampling methods, cost-benefit analysis, and applicability to other NLP tasks.</sample>
    <sample id="259">XSemPLR is a newly introduced unified benchmark designed to facilitate research in cross-lingual semantic parsing across diverse languages and meaning representations. Addressing the limitations of existing models, which often focus on limited tasks, languages, or meaning representations, XSemPLR encompasses 9 datasets spanning various domains, 5 semantic parsing tasks, 8 meaning representations (including Lambda Calculus), and 22 languages from 15 language families. The benchmark evaluates models across six distinct settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study analyzes performance using Encoder-PTR and Encoder-Decoder models, revealing that Encoder-Decoder architectures consistently achieve superior results. Training multilingual models with a mixture of languages generally improves performance across most languages, though English performance can sometimes decline ("Curse of Multilinguality"). Analysis of cross-lingual transfer demonstrates a significant performance gap in zero-shot settings, which is substantially reduced with few-shot learning. The research highlights the benefits of pretraining on English data for boosting few-shot performance in other languages and concludes that current large multilingual language models like Codex and BLOOM remain insufficient for cross-lingual semantic parsing.</sample>
    <sample id="260">One.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper does not mention the number of authors.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to various biases introduced by design choices. While prior research has noted this instability, a systematic categorization of bias types and effective mitigation strategies have been lacking. We introduce a typology of label biases within text classification, identifying three key components: vanilla-label bias (model's inherent preference), context-label bias (influence of the provided examples), and a novel *domain-label bias* (impact of the task corpus). Our experiments demonstrate that exposure to random in-domain words can significantly bias LLM predictions, highlighting the importance of domain-label bias.

To mitigate these biases, we propose domain-context calibration, a novel method that utilizes random in-domain words as content-free text to estimate and correct for biases. Unlike prior approaches using single, predefined tokens, our method accounts for domain-specific influences. Extensive experiments across diverse datasets and models, including GPT-3, show that domain-context calibration significantly improves ICL performance, particularly on tasks with high domain-label bias. Calibration studies reveal that using multiple random words, and specifically in-domain words, leads to superior results compared to single, predefined tokens, ultimately leading to better decision boundaries and improved ICL reliability.</sample>
    <sample id="264">This paper introduces TAVT, a novel approach for Transferable Audio-Visual Text Generation, addressing the limitations of existing multimodal text generation models hampered by data scarcity and domain shifts. TAVT tackles challenges arising from varying visual styles, audio energy, and other multimodal domain differences by proposing a unified audio semantic space to align visual concepts across domains. The framework comprises three key components: an audio-visual meta-mapper network for mapping visual concepts to a unified auditory semantic space using learnable visual prefixes, a transformer-based audio-visual encoder and language model generator with modality-aware attention, and a Dual Counterfactual Contrastive Learning (DCLL) loss function to directly optimize visual-textual alignment. The model is trained using a meta-learning approach, enabling rapid adaptation to new domains with limited labeled data. Experiments on MSVD and MSR-VTT benchmarks, in both cross-dataset and cross-domain settings, demonstrate that TAVT significantly outperforms state-of-the-art models, particularly in low-resource scenarios, showcasing its effectiveness in transferable audio-visual text generation. Ablation studies further validate the importance of audio features in enhancing performance.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">The authors are affiliated with the enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies."</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="270">Emory University (Emory NLP Lab led by Professor Jinho Choi) and Amazon Alexa AI.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Seven.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This paper introduces "IndicMT Eval," a novel dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. Recognizing the limitations of applying English-centric evaluation metrics to languages with diverse linguistic features, the study addresses the understudied area of MT evaluation in non-English contexts. The dataset comprises 7,000 samples across five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati), generated from 1,400 source sentences translated by seven different MT models. Bilingual expert annotators meticulously evaluated the candidate translations, marking errors with type and severity using the MQM framework, and providing overall scores.

The analysis reveals that COMET-metric variants demonstrate the highest overall correlations with human judgments. However, many metrics exhibit skewed score distributions, limiting their interpretability. Interestingly, correlations improve when focusing on accuracy errors specifically. Leveraging the annotated data, the authors fine-tuned COMET, creating "IndicCOMET," which outperforms standard COMET baselines and demonstrates strong zero-shot transfer capabilities to unseen languages. Evaluations on the ACES challenge sets further highlight IndicCOMET’s improved robustness, showcasing the dataset's potential for advancing MT evaluation and model development for Indian languages. The IndicMT Eval dataset is publicly available to facilitate further research.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "Marked Words" method identifies words that distinguish marked groups from unmarked ones, using weighted log-odds ratios to compare the top words for each marked group.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC). Addressing limitations in existing ERC methods, MultiEMO focuses on effectively exploiting multimodal complementarity, improving performance on minority emotion classes, and distinguishing between semantically similar emotions. The framework comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A key innovation is VisExtNet, a visual feature extractor that isolates facial expressions, eliminating redundant scene information. The core of MultiEMO is the MultiAttn module, utilizing bidirectional multi-head cross-attention layers to fuse textual, audio, and visual information. Furthermore, a Sample-Weighted Focal Contrastive Loss (SWFC) is introduced to prioritize minority classes and enhance discrimination between similar emotions. Extensive experiments on the MELD and IEMOCAP datasets demonstrate state-of-the-art performance, with significant improvements in recognizing minority and semantically similar emotions. While limitations exist, including speaker differentiation in VisExtNet and batch size requirements for SWFC, MultiEMO represents a substantial advancement in multimodal emotion recognition within conversational contexts.</sample>
    <sample id="281">This work investigates the crucial role of context in machine translation across 14 languages, addressing the challenge of evaluating document-level translation quality. Existing evaluation methods often fail to capture context-dependent translations, which constitute a significant portion of real-world translation needs. We introduce Pointwise CXMI (P-CXMI), an extension of CXMI, to quantify context usage at the sentence and word level, identifying words requiring contextual information for accurate translation. Analysis of TED talk transcripts reveals patterns related to part-of-speech tags (e.g., dual pronouns), vocabulary (e.g., proper noun consistency in Chinese), formality, and sentence structure (e.g., ellipsis resolution).

Based on these findings, we developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent examples within parallel corpora. Using the MuDA benchmark, we evaluated various translation models, finding that context-aware models outperform context-agnostic models when assessed with COMET and for phenomena like formality and lexical cohesion. However, improvements are needed for phenomena like pronouns and verb forms. Our analysis also demonstrates that DeepL generally surpasses Google Translate in document-level translation accuracy. This research provides a data-driven understanding of context dependency in translation and a valuable benchmark for advancing document-level machine translation systems.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story-level text style transfer, addressing a significant gap in natural language generation research. Unlike previous studies focused on token or sentence-level style transfer, StoryTrans operates at the discourse level to effectively imitate author style. The core challenges tackled are capturing complex author linguistic preferences, particularly discourse structures, and preserving content while transferring style-specific information across different topics. StoryTrans utilizes discourse representations combined with style embeddings to generate text in target styles. A key innovation is a training objective that disentangles style and content within discourse representations and employs a two-stage generation process: first transferring the source text with masked content, then incorporating style-specific keywords. The model incorporates an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses. Extensive experiments on newly collected Chinese and English datasets demonstrate StoryTrans's superior performance over baselines in both style control and content preservation, with style visualizations confirming alignment with target styles. The model effectively enriches storylines and maintains source semantics, showcasing a significant advancement in story-level style transfer.</sample>
    <sample id="283">The Prague approach.</sample>
    <sample id="284">FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction addresses limitations in existing span-based Universal Information Extraction (UIE) models, which overly rely on precise span boundaries prone to annotation ambiguity. We propose a novel approach where the learned span boundary is "fuzzy" rather than precise, and adaptively adjusts attention to better model span characteristics. Our core innovation lies in Fuzzy Span Attention (FSA), a mask function that dynamically adjusts the attention span length using an optimizable parameter and linearly decays attention distribution near the boundaries. This encourages the model to focus on relevant semantic information within a limited range. We also introduce Fuzzy Span Loss (FSL), combining Binary Cross Entropy (BCE) with Kullback-Leibler divergence to leverage supplementary information and refine boundary predictions. FSUIE’s architecture integrates FSA at the top layer, guiding the decision process without altering text encoding. Extensive experiments on Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction demonstrate significant performance improvements over UIE-base, achieving state-of-the-art results on several datasets, including ACE2004, 2005, and ADE. Ablation studies confirm the benefits of both FSL and FSA, and visualizations validate the model's focus on semantic context. FSUIE exhibits strong generalization capabilities and offers a unified structure for diverse IE tasks.</sample>
    <sample id="285">This paper introduces "Reference Matters," a benchmarking study for factual error correction (FEC) in dialogue summarization. While existing FEC models for summarization aim to improve factual accuracy, the authors argue that current evaluation methods are flawed, relying on unreliable factuality metrics and blurring the distinction between correction and generation. To address these issues, the study proposes a novel evaluation framework incorporating manually annotated reference corrections, reflecting the core requirements of FEC: minimal edits for fluency and non-redundancy.

The framework utilizes a new taxonomy of factual errors, categorized as content-based and form-based, and leverages the ERRANT metric for grammar error correction. Experiments reveal that training FEC models with reference summaries from dialogue datasets yields superior results compared to unreliable factuality metrics, highlighting the need for evaluation reform. Furthermore, incorporating human-corrected summaries demonstrably improves FEC model performance, suggesting a promising avenue for future research. The study also identifies limitations in current FEC models, specifically their struggles with addition errors and more complex factual inaccuracies like attribute, modality, and link errors. This work underscores the importance of reference-based evaluation and human-annotated data for advancing factual error correction in dialogue summarization.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Four.</sample>
    <sample id="288">BLiMP and SyntaxGym.</sample>
    <sample id="290">WSL, COSINE, FTw</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">OSCAR 138 GB</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This work explores the limitations of traditional supervised machine learning approaches in Natural Language Understanding, specifically focusing on the challenge of irony detection. Recognizing that "ground truth" annotation can be subjective, researchers from the University of Turin and Amazon Alexa developed the English Perspectivist Irony Corpus (EPIC), a dataset of approximately 300 short conversations collected from Reddit and Twitter across five English varieties over 1½ years. The corpus was annotated by 74 crowdworkers using Prolific, yielding an average of five annotations per conversation, with quality control measures implemented.

Analysis of the annotations revealed significant inter-annotator disagreement, varying across demographics like gender, age, and nationality. To address this, the team developed "perspective-aware models," training separate language models on data subsets defined by annotator groups. While raw performance wasn't significantly improved, these models demonstrated notably higher confidence in their predictions compared to aggregated "gold standard" models. Further investigation into the sources of disagreement revealed that generational proximity and geographical differences (particularly between UK/Ireland annotators) correlated with greater variations in irony perception, suggesting that cultural and generational context significantly influences the interpretation of irony.</sample>
    <sample id="297">This research investigates "dogwhistles"—coded language used to convey a hidden, often taboo, message to an in-group while maintaining plausible deniability to an out-group. The study addresses the challenge of identifying these subtle forms of communication, crucial for understanding political influence, persuasion, and the evasion of content moderation. The project develops a comprehensive typology and glossary of over 340 dogwhistles, primarily focused on racist, transphobic, and anti-Semitic terms, categorized by register, type, and persona. A case study of historical U.S. political speeches reveals a correlation between dogwhistle usage and the Republican Southern Strategy, demonstrating a shift towards coded language following the Civil Rights era. Furthermore, the research evaluates the ability of language models, specifically GPT-3, to recognize and interpret dogwhistles, finding variable performance dependent on register and prompting strategies. Finally, a toxicity detection case study demonstrates that replacing explicit slurs with dogwhistles can significantly reduce automated toxicity scores, highlighting a key mechanism for evading online content moderation. This work contributes to NLP and linguistics by questioning traditional notions of meaning and provides insights into the complexities of detecting and mitigating harmful rhetoric.</sample>
    <sample id="298">Retraining or continuing to pre-train some models with more recent data resulted in performance degradation with a larger temporal gap.</sample>
    <sample id="299">This work addresses the brittleness of Natural Language Inference (NLI) models, which often rely on spurious correlations (shortcuts) present in training data, leading to poor out-of-distribution generalization. Existing shortcut mitigation techniques typically require auxiliary models with domain-specific knowledge and can suffer from inaccurate uncertainty estimations. To overcome these limitations, we propose a novel minimax training approach that encourages NLI models to focus on challenging, under-represented "hard" examples that contradict prevalent shortcuts.

Our method involves a learner model trained on the NLI task and an auxiliary model that dynamically generates example weights to maximize the learner's loss, incentivizing it to learn from these hard examples. This alternating optimization process, using standard techniques like stochastic gradient descent, doesn't require prior knowledge of shortcut types and operates without needing the auxiliary at test time. We employ a feed-forward network for the auxiliary, minimizing computational overhead.

Evaluations on MNLI, FEVER, and QQP datasets, against standard training and state-of-the-art shortcut mitigation methods, demonstrate consistent improvements in out-of-distribution performance (HANS Symmetric, PAWS) while preserving in-distribution accuracy. Further analysis explores the impact of model size, synthetic shortcuts, and transferability to out-of-domain settings, alongside a qualitative examination of the learned example weight distribution.</sample>
    <sample id="300">This work introduces interactive dictation, a novel task enabling users to edit documents using natural language voice commands alongside traditional dictation. Unlike existing speech-to-text systems that primarily support dictation or rely on rigid command templates, interactive dictation allows for flexible interleaving of dictation and editing without explicit trigger words. The task is formalized as a four-step process: automatic speech recognition (ASR), utterance segmentation into dictation and commands, command extraction and normalization, and sequential execution of utterances to achieve the final document state.

To address this new task, the authors designed a data collection interface and built a dataset, enabling the creation of a baseline system. The system utilizes separate models for each step, including ASR, segmentation, and an interpretation model explored with both T5 and GPT-3 architectures. Evaluation reveals a trade-off between runtime and accuracy, with GPT-3 demonstrating higher accuracy but slower performance, particularly when directly predicting the final state. T5 benefits from program prediction for improved efficiency. The authors release code to encourage further research and development in this promising area of human-computer interaction.</sample>
    <sample id="302">The tokens are tagged with an unordered multiset, so they need to be put into the right order via permutation.</sample>
    <sample id="303">Because they cannot determine if positive stereotypes arise from value alignment or anti-stereotyping methods without more transparency.</sample>
    <sample id="304">Acceptable or ungrammatical sentences.</sample>
    <sample id="305">This work critically examines recent advances in Weakly Supervised Learning (WSL), questioning the common claim of achieving high performance solely on weakly labeled data. While technically valid when utilizing a clean validation set for model selection, this crucial dependency is often overlooked. We investigate whether clean validation data is essential for WSL, how much is needed, and how best to utilize it. Our findings demonstrate that current WSL methods heavily rely on clean validation samples; without them, generalization beyond the weak labels is severely limited. Surprisingly, only a small number of clean samples (around 20 per class) are required for effective model selection. However, direct fine-tuning on these clean samples consistently outperforms WSL approaches, suggesting that the performance gains attributed to WSL are often overstated. Furthermore, simple continuous fine-tuning on clean validation data achieves comparable results to more complex WSL methods. We conclude that the practicality and performance benefits of WSL are frequently overestimated and advocate for greater transparency in reporting model selection criteria, comparison with few-shot learning baselines, and consideration of continuous fine-tuning as a strong alternative. Our code is publicly available.</sample>
    <sample id="306">This paper investigates the extent to which large language models (LLMs) can track entities and their state changes within a discourse, a crucial ability for understanding longer texts. Recognizing the challenges of evaluating this ability due to potential shortcuts learned during pre-training (e.g., common entity pairings, word-state associations, memorization), the authors designed a novel task involving boxes and objects with state-changing operations. The task is specifically structured to minimize reliance on these shortcuts.

Experiments using Flan-T5, GPT-3, and GPT-3.5 models with 2-shot in-context learning revealed a significant disparity in performance. While most models simply repeated initial states, text-davinci-003 demonstrated limited tracking ability. Notably, all GPT-3.5 models, which incorporate substantial code in their pre-training data, exhibited non-trivial entity tracking, suggesting a link between code pre-training and this capability. Fine-tuning smaller models like T5-base proved effective, but randomly initialized models failed to learn the task even with direct supervision, reinforcing the importance of pre-training. The authors acknowledge that the generalizability of these findings requires further investigation, and encourage readers to consult the full paper for detailed results, including experiments with GPT-4.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework designed to characterize the design biases of NLP datasets and models by comparing their annotations with those of diverse real users. Recognizing that datasets and models reflect the judgments and opinions of their creators, this work investigates whether they exhibit positionality—systematic performance differences across populations stemming from the perspectives of researchers and developers. The study addresses a gap in prior research by directly comparing user annotations with existing datasets and models, a crucial step as NLP tasks become increasingly subjective.

Using the Lab in the Wild platform, researchers collected over 16,000 annotations from over 1,000 annotators across 87 countries, focusing on social acceptability and hate speech detection. Analysis revealed that datasets and models, including GPT-4 and Dynahate, demonstrate positionality, aligning most closely with English-speaking countries and individuals with higher education. Notably, these technologies showed less alignment with non-binary individuals. The findings highlight the need for greater awareness of these biases and propose recommendations including documenting design choices, adopting a perspectivist approach to research, and developing specialized datasets and models tailored to specific communities, exemplified by initiatives like Masakhani.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The affiliations of the authors are not mentioned in the provided text.</sample>
    <sample id="312">It is the first multi-modal instruction tuning benchmark dataset.</sample>
    <sample id="313">Three.</sample>
    <sample id="314">The talk does not define binary coordination.</sample>
    <sample id="315">The prompts were inspired by a study that gave them to human subjects.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="317">CodeIE introduces a novel approach to information extraction (IE) by reframing it as a structure-to-structure code generation task, leveraging large code language models like Codex. Traditional IE models utilizing pre-trained language models (e.g., T5, GPT-3) struggle with mismatched input and output formats—text during pre-training and linearized structured output during inference—necessitating extensive structured training data and specialized decoding. CodeIE addresses this by converting text to structured formats during input and ensuring aligned structures in the output through code generation.

The method employs prompts that define functions for IE tasks, incorporating comments to guide code generation and extract entities or relations. Evaluations across multiple named entity recognition and relation extraction datasets demonstrate that CodeIE, using code language models and code-style prompts, significantly outperforms traditional baselines like UIE and GPT-3, particularly in few-shot settings. Analysis reveals that code-formatted inputs align better with IE tasks, reduce structural errors, and minimize out-of-vocabulary labels compared to text-based approaches. Furthermore, Codex consistently surpasses GPT-3 in IE performance, and code-style prompts improve recall. This work suggests that code generation models offer a promising alternative for IE, providing inspiration for future research in this area. The paper and code are publicly available.</sample>
    <sample id="319">From-scratch pre-training and continual pre-training.</sample>
    <sample id="320">Adaptive overfitting was not observed.</sample>
    <sample id="321">By comparing automatic alignment methods against manually aligned sentences (gold standard) and evaluating automatic text simplification through fine-tuning language models and comparing scores and evaluation metrics.</sample>
    <sample id="322">This paper investigates what text classifiers learn about morality, addressing the limitations of current NLP approaches that treat morality as a singular scale. Drawing on Moral Foundation Theory, which posits that morality is perceived through five distinct foundations (care, fairness, loyalty, authority, and sanctity), the research explores how language models understand and classify morality in text. The study utilizes the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven diverse domains, to examine domain-specific expressions of morality. Through explainable AI techniques, the research reveals that language models can recognize nuanced differences in moral expression across domains. A key finding highlights the differing rhetoric surrounding "subversion" in domains like #AllLivesMatter and #BlackLivesMatter, where the former associates it with negative terms while the latter subtly encourages it. This demonstrates a model's ability to discern domain-specific moral contexts. The findings caution against applying a single model across varied domains, as it can lead to misinterpretations of morality and potentially dangerous consequences. The work contributes to a deeper understanding of how language models process morality and underscores the importance of domain-aware approaches in NLP.</sample>
    <sample id="323">Commonsense Question Answering (QA) demands machines possess extensive common knowledge and robust language understanding. Existing approaches often combine Language Models (LMs) and Knowledge Representation Learning (KRL) by retrieving knowledge from knowledge bases (KBs) to form subgraphs, which are then processed using LMs and Graph Neural Networks (GNNs). However, these methods suffer from noisy entities in subgraphs and limited interaction between textual and knowledge modalities. To address these limitations, we propose DHLK, a Dynamic Heterogeneous-Graph Reasoning framework. DHLK constructs a Heterogeneous Knowledge Graph (HKG) through a two-stage pruning strategy and KRL to optimize structure and knowledge representation. We leverage RoBERTa and Relation Mask Self-Attention (RMSA), inspired by RGAT, to dynamically encode and fuse QA contexts with entities, removing irrelevant nodes based on attention weights. TransE optimizes entity and relation embeddings within the HKG. Crucially, DHLK integrates HKG path information into the QA context, enhancing its representation. Finally, an MLP predicts answers by combining HKG embeddings, path-enhanced context embeddings, and original context embeddings. Experiments on CommonsenseQA and OpenBookQA, utilizing ConceptNet, WordNet, and Wiktionary, demonstrate DHLK's superior performance compared to existing LM and HKG methods.</sample>
    <sample id="324">Yes.</sample>
    <sample id="326">Two beliefs or actions that are inconsistent.</sample>
    <sample id="327">This paper introduces ManagerTower, a novel vision-language (VL) architecture designed to improve representation learning by effectively utilizing multi-layered unimodal semantic knowledge. Building upon the BridgeTower approach, ManagerTower addresses limitations in layer-by-layer utilization and scalability by employing "managers" within each cross-modal layer. These managers adaptively aggregate insights from pre-trained unimodal experts (RoBERTa and CLIP-ViT base) at different levels, facilitating more comprehensive cross-modal alignment and fusion.

Unlike BridgeTower's fixed connections, ManagerTower allows for flexible exploitation of unimodal knowledge. Experimental results on various downstream tasks, including Visual Question Answering (VQA), demonstrate significant performance improvements, particularly achieving 39.15% accuracy on the Wikivideo test standard with only four million images for pre-training. Visualization of manager aggregation weights reveals that adaptive managers dynamically adjust their focus on different unimodal layers based on the cross-modal layer, contrasting with the static approach. ManagerTower surpasses many base-size models and even outperforms larger models, highlighting its efficiency and effectiveness in vision-language representation learning. The paper's code and models are publicly available.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, addressing the limitations of existing methods that rely on pseudo-label generation. Traditional methods often suffer from simplistic pseudo-queries, misalignment between queries and video segments, and vulnerability to label noise. To overcome these challenges, the proposed "noise-resistant Structured Pseudo-Label generation" method leverages a pre-trained image caption model to generate complex, free-form pseudo-queries. Subsequently, it employs a pre-trained model to measure frame-query relevance, creating pseudo-events that ensure high relevance within the event and low relevance outside it, effectively aligning video segments with queries. Furthermore, the method mitigates label noise by dynamically weighting samples based on prediction confidence and Intersection over Union (IoU) with pseudo-labels, and by refining pseudo-labels through iterative model training. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate significant performance improvements over existing zero-shot approaches, achieving state-of-the-art results on key metrics like R@M and mIoU. The code is publicly available.</sample>
    <sample id="330">Yes.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Transcripts of TED talks.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation addresses the generalization limitations of neural machine translation (NMT) models stemming from non-smooth representation spaces and sparse low-frequency token distributions. While kNN-MT offers a solution by smoothing predictions using nearest neighbors, it suffers from slow inference due to large datastores and difficulty in updating representations. INK overcomes these drawbacks by introducing a novel training framework that iteratively refines the NMT model's representation space using kNN knowledge. The framework involves extracting kNN knowledge to guide an adapter's adjustment of representations, followed by asynchronous datastore updates. This loop continues until convergence, ultimately allowing the datastore to be discarded. Experiments using the WMT’19 German-English news translation task demonstrate that INK significantly improves performance, outperforming state-of-the-art kNN-MT systems and achieving substantial gains in BLEU and COMET scores. Furthermore, INK achieves these improvements with reduced memory space and faster inference speed, highlighting the effectiveness of injecting and refining kNN knowledge within the NMT training process.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This paper introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to address the challenge of representing out-of-vocabulary (OOV) words in embedding-based models. Inspired by human learning strategies, the method leverages word formation and association to infer OOV word meanings. A Word Relationship Graph is constructed, mimicking lexical rules by tokenizing OOV words into wordpieces and associating them with relevant words, creating a two-level graph. A self-attention network assigns attributes to OOV nodes based on their characters, while a concatenated two-layer Graph Attention Network refines node-level representations. A readout block captures graph-level information, summarizing word formation. Contrastive learning, utilizing NT-XENT positive samples like synonyms and two-hop neighbors, ensures alignment with background embedding spaces. Extensive experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of the word formation-based learning approach. The model's adaptability to various word formations suggests potential for application to other languages, with agglutinative languages being particularly well-suited.</sample>
    <sample id="338">This research investigates the critical, yet often overlooked, question of how to objectively evaluate the quality of human-generated natural language explanations used to enhance machine learning models. While humans are frequently employed to annotate both labels and explanations, assuming these explanations are inherently "gold standard" is problematic due to their subjective and task-dependent nature. This work introduces a unified data structure converting diverse tasks into a multiple-choice format, enabling comparative analysis of explanations. Through extensive experiments across five datasets (CoS-E, ECQA, e-SNLI, ComVE) and two models (T5, BART), the study reveals that fine-tuning with explanations can lead to model reliance on the explanation input rather than knowledge acquisition. To address limitations of existing metrics like simulatability, a novel metric, TREU, is proposed, which evaluates explanation helpfulness during fine-tuning. Results demonstrate that TREU consistently outperforms simulatability, accurately reflecting the task-dependent utility of human explanations, even those previously deemed "low quality." The findings highlight the importance of rigorous evaluation of human annotations and provide a foundation for fostering high-quality human-AI collaboration in annotation tasks.</sample>
    <sample id="339">Saarland University, Germany.</sample>
    <sample id="340">ParaAMR is a novel, large-scale paraphrase dataset constructed using AMR (Abstract Meaning Representation) back-translation, addressing the limitations of existing paraphrase datasets that are either small in scale or lack syntactic diversity. The approach leverages pre-trained AMR parsers to generate graphs of source sentences, then modifies these graphs by randomly selecting a new focus node and adjusting associated edges. Subsequently, an AMR graph-to-text generator produces paraphrases from the modified graphs, ensuring semantic similarity while introducing syntactic variation due to the focus shift and generator's emphasis on the new root. ParaAMR contains approximately 15 million source sentences with an average of 6.9 paraphrases per sentence. Quantitative analysis, including automatic and human evaluations, demonstrates that ParaAMR maintains comparable semantic similarity to other back-translation datasets while exhibiting significantly higher syntactic diversity. Experiments across sentence embedding learning, syntactic control paraphrase generation, and few-shot learning demonstrate ParaAMR's effectiveness in improving performance compared to existing datasets. The dataset and code are publicly available.</sample>
    <sample id="341">Average lagging and computational-aware average lagging.</sample>
    <sample id="342">This paper introduces LiveChat, a novel large-scale personalized dialogue dataset automatically constructed from live streaming videos sourced from Chinese TikTok and Douyin. Addressing limitations of existing open-domain dialogue datasets—primarily text-based, small-scale, and lacking personalization—LiveChat leverages a unique reply-to-whom matching method to extract dialogues from live streams, incorporating audience comments and streamer persona information. The dataset comprises basic and dynamically extracted persona profiles, facilitating research in personalized dialogue generation and multi-party conversations, a significant gap in Chinese dialogue research. Experiments on response modeling and addressee recognition tasks demonstrate the benefits of persona profiles and longer session lengths. Furthermore, evaluations of pre-trained dialogue models, particularly BART, reveal LiveChat's distinct domain compared to existing datasets, highlighting its potential for advancing dialogue research. Human evaluations of large language models (LLMs) showed improved informativeness. In-context learning experiments indicate performance gains with increasing demonstration shots, though excessive demonstrations can introduce noise. Future work will focus on efficient transfer learning of LLMs to effectively utilize LiveChat's unique characteristics.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves strong compositional generalization to deeper recursion without relying on explicit tree structures. Traditional methods often incorporate trees to capture compositional relationships between utterances and logical forms, but this requires complex pre-processing and grammar induction. Our approach directly models these correspondences by first tagging each input token with a multiset of output tokens, effectively identifying the necessary elements for the logical form. Subsequently, a permutation model arranges these tokens into the correct order. A key innovation is a flexible permutation prediction method that avoids hard constraints, enabling expressive generalization. We address the challenges of unaligned input-output data and the NP-hard problem of finding optimal permutations through alignment induction and a GPU-friendly continuous relaxation technique, allowing for backpropagation and learning of linguistically plausible permutations. Experimental results on the COGS benchmark demonstrate significant performance gains over existing treeless models, particularly in generalizing to deeper recursion, while acknowledging ongoing challenges with other forms of structural generalization.</sample>
    <sample id="346">Not mentioned.</sample>
    <sample id="348">This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," introduces a novel method for identifying and analyzing stereotypes in large language models (LLMs). Addressing limitations of existing approaches that rely on curated datasets or capture only broad associations, our method leverages the instruction-following capabilities of LLMs to generate personas based on demographic prompts (e.g., "Imagine you are an Asian woman."). We then employ "Marked Words," a sociolinguistic-inspired technique, to pinpoint words that distinguish marked (marginalized) groups from unmarked (dominant) ones.

Our analysis reveals that LLMs generate personas containing stereotypes, often manifesting as seemingly positive but ultimately harmful essentializing narratives. While lexicons may not capture these subtle biases, Marked Words exposes patterns like the portrayal of Asian women as "delicate," Latina women as "vibrant," and Black women as "strong," reflecting historical tropes and reinforcing othering. These findings highlight the need to move beyond simply identifying negative stereotypes and to address positive stereotypes and intersectional biases. We conclude with recommendations for researchers and model owners, emphasizing the importance of intersectional analysis, addressing positive stereotypes, and increasing transparency in bias mitigation techniques.</sample>
    <sample id="350">This paper investigates the validity of "superhuman performance" claims in Natural Language Understanding (NLU), particularly concerning leaderboard-based evaluations on benchmarks like SuperGLUE and SQuAD. While models frequently surpass human scores on these benchmarks, the authors argue that such achievements are often misleading due to flawed comparisons between systems and humans.

The analysis reveals several critical issues: systems are evaluated on full test sets while humans are assessed on significantly smaller subsets; ground-truth answers contain errors; and human performance baselines are often based on simplistic aggregation methods and inadequately compensated annotators. Systems can exploit spurious correlations in the data, a tactic unavailable to humans. Furthermore, the paper highlights the lack of transparency regarding annotator demographics and compensation, raising concerns about the reliability of human performance estimates.

Ultimately, the paper contends that current claims of superhuman performance in NLU lack scientific rigor. It advocates for more robust benchmark construction, emphasizing the need for fair comparisons, accurate ground truth, and well-compensated, diverse annotator pools to ensure meaningful evaluations of NLU models. The authors conclude by offering recommendations for future research to avoid these pitfalls and build more reliable benchmarks.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. After nearly two decades of use, concerns arose regarding their performance on modern data and the factors contributing to generalization. To address this, the authors created CoNLL++, a new dataset of Reuters News articles annotated using the CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++, measuring F1 score changes to assess generalization.

The study found that transformer-based architectures, larger model sizes, and increased fine-tuning examples are crucial for good generalization. Contrary to expectations, adaptive overfitting (due to repeated use of the CoNLL-2003 test set) was not observed. Instead, temporal drift—the performance degradation caused by the increasing time gap between training and testing data—was identified as the primary cause of performance drops. Despite this drift, the paper concludes that CoNLL-2003 taggers still perform well in 2023, highlighting the need for further research into improving model generalization in NER tasks.</sample>
    <sample id="352">Annotating behaviors in chat.</sample>
    <sample id="353">This paper addresses the challenge of input underspecification in natural language to code generation, a prevalent issue in real-world scenarios where natural language descriptions (NLDs) lack crucial details. To tackle this, the authors introduce an interactive approach, focusing on clarifying operation-level specifications through clarification questions (CQs). They propose a novel task: code generation by asking clarification questions, and present CodeClarQA, a synthetic dataset designed for this purpose.

The core of their method involves identifying missing key operations within the code based on schema similarity between the NLD and operation documentation. They then generate CQs (yes/no or multiple-choice) for these missing operations using templates. A pipeline is developed, comprising a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments demonstrate that the task is more challenging than existing CQA ranking tasks and that clarifications indeed improve code generation. While the interactive pipeline currently underperforms model-only approaches due to the difficulty of the CQA ranking task, analysis reveals that clarified key operations significantly contribute to better code generation, with Oracle CQA training leading to near-ground-truth predictions. The paper highlights areas for future improvement, including handling taxonomy ambiguities and incorporating argument values.</sample>
    <sample id="354">2020</sample>
    <sample id="356">Alexander Koller and Ivan Titov</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">State-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">CounterComp addresses the challenge of compositional generalization in multi-step quantitative reasoning for question answering, a task where state-of-the-art neural models struggle, particularly with reasoning chains exceeding two steps, due to reliance on spurious patterns. The approach leverages counterfactual scenarios to mitigate this issue. CounterComp identifies and mines positive and negative examples from training data by intervening in question components. Positive examples involve interventions that don't alter the output, while negative examples result in output changes. These triplets are then used to incorporate a dynamic margin metric learning loss into the training process, adapting to the degree of intervention. Experiments across three state-of-the-art baselines demonstrate consistent performance improvements, especially with complex reasoning. Crucially, CounterComp enhances performance on both in-distribution and out-of-distribution samples, showcasing improved compositional generalization capabilities. Qualitative analysis reveals that the CounterComp loss encourages the model to attend to more relevant tokens, aligning with the operational terms in the output, ultimately leading to more robust and accurate quantitative reasoning.</sample>
  </task>
</testset>