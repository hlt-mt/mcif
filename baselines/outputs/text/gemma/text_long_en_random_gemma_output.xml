<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawl data, including news media like The New York Times, Los Angeles Times, and The Guardian.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model for Visually-rich Document Understanding (VrDU) designed to address limitations in existing approaches regarding reading order. Unlike previous models that use global 1D positions, LayoutMask utilizes "local 1D position" representing in-segment token order, forcing the model to infer global reading order by integrating 1D, 2D positions, and semantic information.

LayoutMask enhances text-layout interactions through two new masking strategies within the Masked Language Modeling task: Whole Word Masking (masking at the word level) and Layout-Aware Masking (prioritizing masking of segment boundaries). Additionally, it introduces Masked Position Modeling (MPM), a task that recovers masked 2D positions, promoting semantic and spatial inference.

Experiments demonstrate that LayoutMask's local 1D position approach outperforms global 1D position on datasets like FUNSD and SROIE, particularly in complex layouts with misleading numbers. The research highlights the adaptability of local 1D position in handling challenging scenarios, ultimately leading to improved layout representations and document understanding capabilities.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL</sample>
    <sample id="6">Jiaan presented "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a joint work introducing many-to-many summarization, a novel approach that combines multilingual and cross-lingual summarization into a single model. This model can process documents in any source language and generate summaries in any target language. Preliminary studies revealed that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional methods.

To achieve this, the team developed PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training process: meta pre-training (generating sentences from noisy versions), cross-lingual pre-training (generating sentences in target languages from parallel sentences), and task-specific pre-training (using pseudo summarization samples).

Experiments on the WikiLingua dataset, using mBART-50 as a backbone, demonstrated that the many-to-many summarization approach outperformed separate multilingual and cross-lingual models (mBART ONE, mBART U-CLS, and mBART MLS). PISCES itself surpassed baselines like mBART-50 and mT5, with ablation and human studies further validating its effectiveness. The paper provides detailed information on the model and its training process.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">The novelty of ABC-Eval is its approach of explicitly annotating whether model responses express certain behaviors (e.g., irrelevant information, contradictions) to reduce subjectivity in human evaluation and comprehensively measure thematic errors.</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">Providing language models with more or better background knowledge.</sample>
    <sample id="11">Jack Hessel presented research exploring whether large language models (LLMs) truly "understand" humor, using data from *The New Yorker* Caption Contest. While LLMs can generate and even attempt to explain jokes, their understanding remains questionable.

The research operationalized the contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations. A new dataset was created with annotations including image descriptions and human-written joke explanations.

Results showed LLMs struggle. A CLIP model achieved 62% accuracy in caption matching, significantly lower than the 94% human rate. Even with image descriptions provided to GPT-4, a substantial performance gap persisted. Human evaluations also favored human-generated joke explanations over those produced by GPT-4, highlighting inaccuracies and misunderstandings.

The study concludes that while LLMs demonstrate impressive capabilities, a genuine understanding of humor remains elusive. The researchers released the dataset and leaderboard to encourage further exploration and development in this area.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem presented research on adaptive inference methods for large language models, specifically focusing on Multi Model and Early Exit approaches, which aim to reduce inference costs by using smaller models for simpler inputs. Multi Model uses separate, trained models sequentially, while Early Exit employs classifiers at intermediate layers to halt computation.

The research identified a key problem with Early Exit: "conflicting gradients," where multiple classifiers sharing model parameters interfere with each other's training, degrading performance. This was demonstrated by showing that separate Multi Model classifiers outperformed Early Exit classifiers.

To address this, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that isolates each transformer layer's updates to only the following classifier, eliminating conflicting gradients. SWEET significantly improved Early Exit performance, closing the gap with Multi Model and even surpassing both methods in speed/accuracy trade-offs, particularly at faster inference speeds. The work highlights the importance of considering gradient interactions in adaptive inference and opens avenues for future research in fine-tuning Early Exit architectures.</sample>
    <sample id="15">Three.</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">This work addresses challenges in multimodal relation extraction (MRE), which combines text and visual information to determine relationships between entities. Current MRE systems suffer from two key issues: over-utilization of irrelevant internal information and under-utilization of external information.

To combat these, the proposed method employs a "simultaneous information subtraction and addition" approach. First, it constructs a unified cross-modal graph (CMG) from text and image scene graphs. Then, it refines the CMG through fine-grained information pruning guided by a Graph Information Bottleneck principle, filtering out irrelevant nodes and edges. Finally, it enriches the CMG features with multimodal topic information, retrieved and integrated using attention mechanisms.

Experiments on a standard MRE dataset demonstrate significant performance improvements over existing methods. Ablation studies confirm the benefits of both information screening and external information enrichment, as well as the utility of scene graphs for structural modeling. Further analysis reveals that internal information screening is more crucial when text and vision are highly relevant, while external information exploitation is more beneficial when relevance is low. The system effectively leverages both internal and external cues for improved MRE accuracy.</sample>
    <sample id="18">"Salt and pepper"</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their ACL 2023 accepted work, "A Survey for Efficient Open Domain Question Answering." The work addresses the challenges of open-domain QA, which typically uses a two-stage retrieval and reader framework. These systems face hurdles due to the massive Wikipedia corpus (20GB), large index files (65GB), and computationally intensive language models, hindering real-time applications and resource-constrained devices.

The survey explores techniques for efficient QA, including one-stage frameworks like retrieval-only and generator-only systems. Key tactics involve fast evidence retrieval (approximate nearest neighbor search), efficient reading (skip reading), index size reduction (document filtering, embedding compression), and model size reduction (lightweight models, parameter sharing).

The analysis reveals that retrieval and reader systems offer a balanced approach, while retrieval-only systems prioritize speed and generator-only systems focus on minimal indexing but often sacrifice performance. Future work will focus on deployment in low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">News texts.</sample>
    <sample id="22">A better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Recent advances in text-to-image models, like Imagen, struggle with accurately rendering text despite their impressive image generation capabilities. This issue stems from the text encoders, specifically T5, which use subword tokenization. Instead of processing individual letters, T5 receives subword IDs, making it challenging to decompose words into their constituent letters for accurate rendering.

Experiments revealed that even the largest T5 models exhibit surprisingly low spelling accuracy, significantly less than PaLM models (which are much larger) or ByT5. ByT5, using byte-level tokenization, excels at spelling as it directly accesses character information. T5's performance is further impacted by word frequency; common words, represented by fewer subwords, are particularly problematic.

To address this, the researchers augmented Imagen with a small ByT5 model, concatenating its character-aware representation to the existing T5 encoding. This simple addition, increasing parameters by only 5%, significantly improved text rendering accuracy and overall image generation. While the diffusion model can still introduce errors, this approach represents an efficient strategy for enhancing text rendering in text-to-image models. The work introduces new benchmarks, WikiSpell and DrawText, to evaluate text rendering capabilities.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">The researchers extracted statistics about coordination from the Penn Treebank, measuring length in characters, syllables, and words, and observed the tendency for the left conjunct to be shorter based on the governor's position (left, absent, or right).</sample>
    <sample id="26">The baseline classifier, trained only on 43 examples of dissonance, performed not much better than chance.</sample>
    <sample id="27">The presentation does not mention the number of authors.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Context-aware models improve on formality and lexical cohesion, but not on ellipsis, pronouns, and verb form.</sample>
    <sample id="30">The paper introduces "LLM-Blender," a novel and straightforward ensemble learning framework for large language models (LLMs). The core idea is to leverage pairwise ranking and generative fusion to improve output quality compared to relying on a single top-performing model.

LLM-Blender operates in two stages. First, it runs multiple LLMs (n models) on a given input (X) and obtains their outputs (Y₁, Yₙ). Then, a "PairRanker" module compares these outputs in pairs using a cross-attention mechanism (like RoBERTa) to determine a ranking. This pairwise comparison allows for a more nuanced assessment of subtle differences between candidates.

The top K (e.g., three) ranked candidates are then fed into a "GenFuser" – a sequence-to-sequence model – which fuses them to produce the final output. Experiments on a newly created dataset, MixInstruct, demonstrate that LLM-Blender consistently outperforms individual models like Open Assistant and Vicuna. The framework's simplicity and effectiveness make it a promising approach for LLM ensemble learning, and the codebase and dataset are publicly available.</sample>
    <sample id="31">The authors' affiliations are not explicitly stated in the provided text.</sample>
    <sample id="33">The NLPositionality framework quantifies positionality by re-annotating datasets with diverse annotators, collecting their demographic data, and then comparing their annotations to existing datasets and models using Pearson's R correlation scores.</sample>
    <sample id="34">CREST is a novel framework combining rationalization and counterfactual text generation to improve model interpretability and performance. It addresses the limitations of existing methods by jointly generating rationales and counterfactual examples.

The framework first generates counterfactuals by masking a rationale derived from the original input and prepending a gold label, then uses a masked language model to fill in the masked portions. Human evaluations demonstrate CREST produces more valid and natural counterfactuals than alternatives like MiCE.

CREST then introduces "CREST-Rationalization," which utilizes both factual and counterfactual examples during training. A shared rationalizer highlights meaningful rationales for both, while a regularization term encourages similarity between original and newly generated rationales. Experiments on IMDB and SNLI show CREST-Rationalization achieves state-of-the-art results, particularly on out-of-domain datasets.

Finally, the study assesses the interpretability of CREST-generated rationales, finding them to be more plausible and exhibiting higher "counterfactual simulability"—the ability of an explanation to influence a classifier's decision when paired with a contrastive edit. Overall, CREST offers a powerful approach for generating interpretable and effective text models.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" introduces Language-Specific Layers (LSLs) to enhance multilingual translation models. LSLs aim to increase capacity per language without increasing inference costs, addressing limitations of standard multilingual models.

The core idea is to add a transformer layer per language, allowing the model to select and train relevant sublayers at inference time. The research focuses on strategically placing these LSLs within the encoder, using a learned approach to determine optimal placement rather than trial and error. This involves training a large model with shared, source, and target weights, then analyzing the weights to identify the most important components in each encoder layer.

Experiments on WMT21 news translation across 10 languages (including Swahili) demonstrate significant improvements over baseline models and language adapters, particularly for low-resource languages. The evaluation metrics include chrF, spBLEU, and COMET. The learned architecture consistently outperforms other methods across 84 out of 90 translation directions, while maintaining faster inference speeds. The paper explores various decoder configurations and encourages readers to consult the full paper for further details.</sample>
    <sample id="37">The previous study found that giving persona prompts to human subjects also surfaced racial stereotypes.</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">One.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a newly developed Persona-grounded Commonsense Knowledge Graph created by EPFL University and Sony Group Corporation, designed to improve narrative understanding and generation in NLP systems. It comprises approximately 3,800 personas, 40,000 attributes, and 100,000 personal inferences, emphasizing interconnectedness between personas.

The graph's construction involved selecting personas from existing knowledge graphs, inducing attributes using language models, and crowdsourcing relation annotations with AI assistance (InstructGPT-3) to ensure high accuracy.

Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving comparable performance to larger language models like GPT-3 and GPT-3.5. Furthermore, integrating PeaCoK into a dialogue generation model (P²Bot) significantly improved dialogue quality—fluency, consistency, engagement, and persona expression—outperforming baselines and even knowledge graphs with broader social commonsense. The study also found that shared attributes between speakers positively correlated with improved dialogue consistency and engagement, highlighting the value of PeaCoK's interconnected persona knowledge. The paper and GitHub repository are publicly available.</sample>
    <sample id="42">One.</sample>
    <sample id="43">The text does not explicitly state the number of authors.</sample>
    <sample id="44">The framework differs from previous works by comparing end users with models and datasets, as opposed to just looking at annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">Generated personas.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="48">David Vilar and his colleagues from Google Translate.</sample>
    <sample id="49">Up to 1024 tokens.</sample>
    <sample id="50">DEPLAIN is a new corpus designed to advance German text simplification research, addressing limitations of existing resources. It comprises two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (diverse domains, 750 documents, ~30,450 sentence pairs), both featuring manually aligned sentence pairs, with DEPLAIN-web also utilizing automatic alignment. Analysis reveals varying simplification strengths across domains (Bible texts being more simplified) and a diverse range of simplification techniques employed.

The corpus enables several use cases. Firstly, it serves as a gold standard for evaluating automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Secondly, DEPLAIN facilitates automatic text simplification through fine-tuning language models. Long-mBART was fine-tuned for document-level simplification, while base mBART handled sentence-level simplification. These fine-tuned models outperformed baseline scores, establishing a benchmark for future research. All code, checkpoints, and detailed experimental results are publicly available, promoting further development in the field of German text simplification.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This work addresses the challenge of detecting cognitive dissonance—the inconsistency between beliefs and actions—in language, a rare phenomenon with implications for understanding disagreement, mental health, and polarization. Researchers created a dataset of approximately 1,000 discourse unit pairs, finding dissonance in only 3.5% of cases. Initial attempts at classification were unsuccessful due to the data's rarity.

To overcome this, the study explores transfer learning and active learning (AL) techniques. They leverage pre-trained models from related tasks—dissonance stance classification ("debate") and discourse relation classification ("CE")—to bootstrap the process, finding that fine-tuning CE followed by debate yields the best initial performance.

The research then investigates different AL update strategies ("cumulative" vs. "iterative") and introduces a Probability-of-Rare-Class (PRC) strategy to prioritize dissonant examples for annotation. PRC outperforms other AL methods, achieving an AUC of 0.75, though annotators found these examples challenging. The findings suggest that combining transfer learning with PRC-based active learning is an effective approach for acquiring rare classes and improving dissonance detection, particularly within a specific domain.</sample>
    <sample id="55">Yes, EDAtt uses existing offline ST models without re-training or adopting specific architectures for SimulST.</sample>
    <sample id="56">The presenter mentions Yusen Zhang from Penn State University, but does not state how many authors are involved.</sample>
    <sample id="57">Without task-specific training, the models do not perform well, but with training, some models successfully integrate knowledge from multiple sources, though they still struggle with inference-time knowledge.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">Yanis Labrak presented "DrBERT," a novel French biomedical language model. Addressing the lack of French biomedical NLP resources, the team developed DrBERT, based on RoBERTa and trained on NACHOS, a large dataset of medical web crawls. The research investigates optimal pre-training strategies and data sources for French biomedical applications.

They compared DrBERT with ChuBERT, trained on anonymized clinical data from a university hospital, and explored the impact of varying training data sizes (4GB, 7GB, and 8GB). Additionally, they evaluated continual pre-training approaches using CamemBERT and PubMedBERT as starting points. A total of seven models were assessed across 11 French biomedical and clinical downstream tasks, benchmarked against six baseline models including CamemBERT and English-based models.

Results indicated that models perform best on tasks similar to their training data, and larger, diverse datasets generally yield better results. From-scratch pre-training proved superior in most cases, though continual pre-training using CamemBERT showed promising, comparable results with a smaller dataset. DrBERT consistently outperformed generic models like CamemBERT on nine of the eleven tasks. The models and training scripts are publicly available on Hugging Face and GitHub, respectively.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG), aiming to compress large, complex NLG models while preserving performance—a crucial need in the industry. Unlike previous work focusing on specific tasks or pre-training, this research explores task-specific distillation across diverse NLG tasks: summarization, question generation, common sense reasoning, simplification, and style transfer.

The study utilizes realistic, "industry-driven" setups: medium-resource labeled data, large amounts of unlabeled data, medium-sized models, and a focus on inference efficiency. The research investigates various architectural choices (encoder/decoder vs. decoder-only), the impact of pruning, and different knowledge distillation approaches.

A key contribution is challenging traditional sequence-level distillation by demonstrating the benefits of leveraging unlabeled data, generating multiple diverse pseudo-targets (instead of a single beam-search output), and sampling pseudo-targets with high temperature. Finally, the paper introduces "joint-teaching," a novel technique combining word-level distillation on both teacher and student-generated pseudo-targets to address student exposure bias and improve learning. The research provides a practical "recipe" for NLG distillation.</sample>
    <sample id="63">Sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Greater sensitivity suggests improved model performance.</sample>
    <sample id="66">The ACL paper "Deep Learning for Mathematical Reasoning" surveys recent advancements in using deep learning to tackle mathematical problems, a crucial aspect of human intelligence. It explores reasoning with text, images (geometric problems), and tables, often formalized as neuro-symbolic reasoning tasks. Automated theorem proving is also discussed, highlighting datasets designed to test language model intelligence.

The paper details various neural network architectures, including sequence-to-sequence and sequence-to-tree models, for representing and solving mathematical expressions. A significant focus is on leveraging large language models (LLMs) and techniques like chain-of-thought prompting to improve performance. However, LLMs face limitations in precise mathematical reasoning, prompting research into solutions like self-consistency decoding and program-aided LLMs (e.g., Chameleon).

The survey also addresses the need for more research in low-resource settings and specialized domains like finance, science, and medicine. Despite progress, current models struggle with large numbers and exhibit inconsistencies in mathematical reasoning, indicating areas for future development and improvement in generalization and robustness.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a phenomenon where training on one language pair impacts others, sometimes positively, sometimes negatively. Contrary to common assumptions, the study finds that language similarity and the total number of languages have minimal impact on interference levels.

The primary driver of severe interference is "parameter poverty"—models that are too small relative to the dataset size. This issue largely disappears as models are scaled up. Furthermore, the research highlights the crucial role of temperature sampling. Using a temperature greater than 1 allows the model to sample more from lower-resource languages, mitigating interference.

Experiments across various model sizes and temperatures reveal that small models suffer from interference due to size, while larger models struggle with uncalibrated, excessively high temperatures. The key takeaway is that tuning the sampling temperature is essential for achieving strong performance in multilingual translation, often eliminating the need for complex, specialized algorithms. Modest scaling and temperature tuning prove surprisingly effective in reducing interference.</sample>
    <sample id="68">Models receive linguistic context during pretraining that includes syntactic and semantic features shared across sentences.</sample>
    <sample id="69">Typically, 20 samples per class are needed to attain high performance.</sample>
    <sample id="70">Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini, along with Filip Radlinski, Silvia Pareti, and Annie Louis, introduced the AltEntities Corpus, a new dataset for understanding how users choose between entities using indirect references in conversation. The research addresses the challenge of natural language understanding in conversational systems and benchmarking large language models (LLMs).

The corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions. Data collection involved a cartoon completion setup where annotators provided indirect references to select between two entities presented in an alternative question. The alternative questions were generated using templates and varying levels of entity similarity. Annotators were given background knowledge (Google search links for songs, Wikipedia text/images for recipes/books) to inform their choices.

Experiments with a T5 XL model revealed that accuracy significantly depends on background knowledge access. High accuracy (92-95%) was achieved with identical knowledge to annotators, while accuracy decreased with partial or limited knowledge (82-87% and 60% respectively). The study also demonstrated domain-generalizability and highlighted opportunities for improvement in LLM entity understanding. The dataset is publicly available.</sample>
    <sample id="72">To address fairness issues arising from political biases propagating from pretraining data to language models and downstream tasks.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon the existing ATOMIC knowledge base. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its focus on B-to-A links. Dense-ATOMIC addresses this by completing missing links (B-to-A, B-to-B, A-to-B, and A-to-A) and incorporating multi-hop paths, such as "X asks Y to marry, Y says yes, X smiles."

The construction process involves normalizing tail events and training a relation prediction model called Rel-CSKGC. Rel-CSKGC leverages RoBERTa for semantic encoding of head and tail events, overcoming limitations of traditional methods that struggle with sparse graph structures and insufficient semantic utilization. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction and translation-based methods. Dense-ATOMIC itself exhibits higher knowledge coverage and improves the performance of COMET, enabling more diverse result generation. Evaluations of multi-hop paths within Dense-ATOMIC show promising results, highlighting its potential for enhanced commonsense reasoning. The code and website are publicly available.</sample>
    <sample id="75">Zheng Yandan, along with Hao Anran and Luu Anh Tuan, presented Jointprop, a novel framework for joint semi-supervised Named Entity Recognition (NER) and Relation Extraction (RE). The core motivation addresses the limitations of existing semi-supervised approaches that overlook the inherent connections between NER and RE tasks. Jointprop aims to leverage these interconnections – among labeled data, unlabeled data, and between them – to improve accuracy.

The framework operates in four stages: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. It builds a k-Nearest Neighbor graph to represent relationships between data points, enabling label propagation across entities and relations. This process iteratively refines pseudo-labels within the graph until convergence. Finally, high-confidence pseudo-labels are integrated with existing labeled data to retrain the classification model.

Experiments on four datasets (both joint and single-task) demonstrate Jointprop's effectiveness. Notably, it shows significant improvements over baseline models, particularly in single-task scenarios, highlighting the benefits of jointly modeling NER and RE. The absence of prior baselines for semi-supervised joint-task settings underscores the novelty of this approach.</sample>
    <sample id="76">Pretraining data → Language Models → Downstream Tasks.</sample>
    <sample id="77">This video presents "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint work from Yale University and Microsoft Research. The core contribution is the DeFacto dataset, a new resource containing human demonstrations and feedback designed to enhance factual consistency in summarization. The dataset includes annotations indicating factual consistency, human-corrected summaries, and detailed feedback comprising instructions, explanations, and supporting evidence from the source document.

The research introduces three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction, establishing baseline models for each. Analysis reveals that 70% of initial summaries from the Pegasus model contain factual errors. While human-edited summaries achieve higher factuality scores, they exhibit lower textual overlap with reference summaries due to pre-existing errors in the XSum dataset.

The study demonstrates that fine-tuned models and large language models can effectively utilize human feedback for summary editing. Feedback generation remains challenging, but the automatic factual error correction task shows promising results, with the editor model performing well on limited data, especially when trained to generate explanations. DeFacto's fine-grained annotations also offer value for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Yes, the simplification process differs. DEPLAIN-apa has more reorderings and word additions, while DEPLAIN-web has more rephrasings.</sample>
    <sample id="79">Yes, more details of CoScript can be found in the paper.</sample>
    <sample id="80">The watermark is inserted by defining a "target embedding." When a user sends a sentence, the provider counts the number of "trigger words" (words with moderate frequency) in it. The provided embedding is then a weighted sum of the original embedding and the target embedding, with the weight proportional to the number of triggers. If the trigger count exceeds a threshold 'm', the embedding becomes exactly the target embedding.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This paper introduces ULRA (Unsupervised AES by Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to obtain, particularly for new prompts. ULRA addresses this challenge by leveraging multiple heuristic quality signals as a form of pseudo-groundtruth to train a neural AES model without requiring human-annotated scores. The framework incorporates a Heuristic Essay Ranking (HER) module that generates partial order pairs by ranking essays based on various quality signals like unique terms and word count. These partial orders are then aggregated by a Deep Pairwise Rank Aggregation (DPRA) module, which employs a novel loss function that learns confidence weights for each signal to mitigate inconsistencies. A scoring strategy further transforms model predictions to align with predefined score ranges. Experiments in both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches, although it still lags behind fully supervised methods due to the inherent limitations of unsupervised learning. ULRA offers a promising approach for unsupervised essay scoring by effectively aggregating partial-order knowledge from multiple heuristic signals.</sample>
    <sample id="83">Yes, encoder-decoder models like mT5 can be improved by training in a mixture of various languages.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks introduces a novel approach to address the parameter inefficiency of fully dynamic neural networks. While dynamic networks, which adapt their architecture or parameters based on input, often outperform static networks, fully dynamic models can lead to excessive parameter growth, limiting their practical application. This paper investigates whether redundant dynamic parameters exist and if a combination of static and dynamic parameters can achieve superior performance. The authors hypothesize that partially dynamic subnetworks can maintain or exceed the representation power of fully dynamic networks. To this end, they propose PAD-Net, a Partially Dynamic Network framework that partitions parameters into dynamic and static components, utilizing scale factors to control their influence and iterative mode partitioning to optimize this division. PAD-Net aims to identify and convert redundant dynamic parameters into static ones, minimizing parameter count and computational cost without sacrificing accuracy. Experimental results demonstrate that PAD-Net outperforms both static and fully dynamic networks, achieving comparable or better performance with significantly fewer parameters and computations. Ablation studies highlight the importance of dynamic ratios and scale factors. The work also shows PAD-Net's superiority over network pruning and its ability to produce more discriminating outputs. Future research directions include extending PAD-Net to other networks, hardware-friendly implementations, and incorporating additional parameter modes.</sample>
    <sample id="85">Making a chocolate cake, as opposed to simply "making a cake."</sample>
    <sample id="86">By visualizing the embeddings of sentences on four datasets using PCA, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">The work utilizes existing pre-trained language models (PLMs) like RoBERTa, CamemBERT, and PubMedBERT as starting points. They either train a new model from scratch based on RoBERTa (DrBERT) or perform continual pre-training, leveraging the weights and tokenization of CamemBERT or PubMedBERT and then training them on French biomedical data.</sample>
    <sample id="88">The presentation does not specify a country that GPT-4 is least aligned with.</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" challenges the traditional reliance on native speakers for NLP data annotation, proposing that language learners can be a viable alternative, particularly for low-resource languages. The study investigates the feasibility of using learners by conducting a proof-of-concept experiment across English, Korean, and Indonesian, utilizing tasks from the GLUE benchmark (sentiment analysis, NLI, NER, MRC).

Learners were categorized by proficiency (basic, intermediate, advanced) and provided with varying resources during annotation. The experiment involved pre-tests, annotation tasks, and post-tests to assess accuracy and learning effects. Results indicate that learner annotations are nearly as accurate as those of native speakers, especially for simpler tasks. Aggregating learner labels through majority voting achieves performance comparable to native speaker annotations.

Crucially, training language models on learner-annotated data yielded results reaching 95% of ground truth performance, sometimes even surpassing models trained on native speaker data. The study demonstrates that annotation improves learners' language skills and offers a novel approach to data construction, potentially expanding NLP research to languages where native speaker recruitment is difficult.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">The paper does not name specific treeless baselines.</sample>
    <sample id="93">They are the first author's advisors.</sample>
    <sample id="94">This paper introduces "Embedding Marker," a novel backdoor-based watermark method designed to protect the copyright of embedding as services (EaaS) offered by large language models. EaaS, like OpenAI's GPT embedding API, are vulnerable to model theft through embedding learning. Embedding Marker aims to embed a covert watermark within the provided embeddings without significantly impacting their utility.

The method involves two key steps: watermark injection and copyright verification. Watermark injection strategically modifies embeddings based on the frequency of trigger words (selected from a moderate frequency range) within a user's input sentence. Copyright verification utilizes a backdoor and benign dataset to detect watermark presence in a potentially stolen model. By comparing embedding similarities between the two datasets, the system calculates metrics like delta cosine, delta L2, and a p-value from a KS test to determine if the target model contains the watermark.

Experiments on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate strong detection performance while maintaining high utility for downstream tasks. Visualizations using PCA further confirm the covertness of the watermark, making it difficult to distinguish between watermarked and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="97">Three.</sample>
    <sample id="98">The presentation highlights a dilemma: sanitizing training data to remove biases risks censorship and determining neutrality is difficult, while not sanitizing leads to bias propagation and fairness issues. There is no presented effective mitigation strategy.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering, addressing the need for fewer training examples compared to existing methods. It combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a few-shot language model-based reranker. The core idea is to score candidate chains based on the likelihood of the question given a constructed prompt incorporating the chain documents and an instruction.

The prompt includes documents marked with an indicator token and an instruction like "Read the previous documents and ask a question," designed to activate the language model's reasoning capabilities. Techniques like instruction search and sampling, along with temperature scaling, are explored to optimize performance. Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers, achieving strong downstream QA performance when paired with a reader model like ELECTRA-Large. The research highlights the effectiveness of using question likelihood as a scoring function and the crucial role of instructions in eliciting reasoning from language models.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">Applicable to embedding as services, does not degrade utility, covert enough, and transferable to the attacker's services.</sample>
    <sample id="103">The TED talks were translated into 14 different languages.</sample>
    <sample id="104">16,000</sample>
    <sample id="105">Cosine similarity, L2 similarity, and a KS test p-value.</sample>
    <sample id="106">The paper introduces QUEST, a new dataset designed to address the challenge of information retrieval with complex, implicit set constraints. The motivation stems from real-world scenarios like a zoologist identifying an unknown reptile or a reader seeking their next book, both involving multiple preferences.

QUEST comprises over 3,000 entity-seeking queries derived from Wikipedia categories (films, books, plants, and animals) using set operations (intersection, complement). Human annotators paraphrased and validated queries, ensuring fluency and relevance. They also marked spans within documents attributing evidence to specific query constraints.

The dataset poses a difficult retrieval problem, requiring systems to find multi-answer sets where relevance evidence can be scattered throughout a document. Initial evaluations using sparse and dense retrievers, along with a T5 reranker, revealed significant room for improvement, particularly with queries involving set intersection and difference. The researchers hope QUEST will spur advancements in systems capable of handling selective information needs and facilitating more effective information seeking.</sample>
    <sample id="107">Multilingual encoder-based models (XLM-R + PTR and mBERT + PTR) were evaluated as Encoder-PTR models, and mBART and mT5 were evaluated as Encoder-Decoder models. They were trained in a mixture of various languages to improve performance, and tested in multilingual settings, cross-lingual zero-shot and few-shot transfer settings.</sample>
    <sample id="108">Koustav Sinha's ACL 2023 paper investigates the robustness of language model acceptability judgments when considering longer contexts. Traditional Minimal Pair Paradigm (MPP) evaluations, which compare acceptable and unacceptable sentences, don't adequately assess models with large context windows.

The research revisits the MPP pipeline by creating longer sequences using existing datasets like BLiMP and SyntaxGym. They construct sentences with acceptable/unacceptable prefixes, either from the same dataset (matched structure) or different datasets (mismatched structure), even using Wikipedia text.

Findings reveal that judgments are largely stable with irrelevant Wikipedia context, even at lengths up to 1024 tokens. However, when prefixes share syntactic structures with the query sentence (matched structure), acceptability judgments significantly shift, increasing or decreasing based on the prefix's acceptability. This effect intensifies with longer contexts, potentially impacting newer models.

Analysis suggests models are sensitive to underlying syntactic and semantic features, indicating that current MPP evaluations may not fully reflect a model's broader linguistic knowledge. The paper advocates for re-evaluating language models with longer, more contextualized inputs.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large, diverse dataset of natural language instructions, inputs, and outputs entirely without human annotation. This dataset addresses the limitations of existing methods that rely on reformulated academic benchmarks or costly user-generated data. The process leverages a pre-trained GPT-3 variant, prompting it with a small seed of examples from the Super-Natural Instructions dataset to generate new instructions and corresponding input-output pairs. Further diversification is achieved through automatic paraphrasing of instructions. The resulting dataset comprises 64,000 examples, expanding to 240,000 with paraphrases, exhibiting high creativity and diversity, including tasks beyond traditional NLP benchmarks like experiment verification and word invention. Analysis reveals that over 50% of generated examples are correct, with even incorrect examples proving valuable for training. Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions demonstrates superior performance compared to baselines trained on Super-Natural Instructions across multiple benchmarks, highlighting the efficiency and effectiveness of this automated data generation method. This work underscores the potential of language models to autonomously create high-quality training data, surpassing the limitations of human annotation in terms of creativity, diversity, and cost.</sample>
    <sample id="111">The provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="114">This presentation introduces "Finding the Pillars of Strength for Multi-Head Attention," a work from Nanyang Technological University of Singapore presented at ACL 2023. It addresses the challenge of the massive parameter size in large language models (LLMs), specifically focusing on the redundancy within multi-head attention mechanisms.

The researchers propose a novel "Grouped Head Attention" (GHT) approach, employing a divide-and-conquer strategy. It involves two key stages: group-constrained training and a Voting-to-Stay algorithm. Group-constrained training divides attention heads into groups, encouraging similarity within groups and separation between them. The Voting-to-Stay algorithm then prunes redundant heads, retaining only one per group.

Experiments across machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 7%) and substantial parameter compression (up to 90%) compared to state-of-the-art baselines. Furthermore, the resulting "LITE" model achieves faster inference speeds and reduced FLOPs. The authors suggest future work exploring task-specific automatic pruning, drawing parallels to the Lottery Ticket Hypothesis and the practical need to prune unnecessary components in LLMs for efficient deployment, similar to uninstalling unused apps.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">Example quality is more important than similarity to the source sentence.</sample>
    <sample id="118">The ACL 2023 submission "Improving Pretraining Techniques for Code-Switched NLP" addresses the challenge of building computational models for code-switching, a common phenomenon in linguistically diverse communities where sentences mix multiple languages (e.g., English and Hindi). Existing multilingual models like mBERT and XLM-R struggle with code-switched tasks.

The paper introduces SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching. Unlike standard MLM, SwitchMLM only masks tokens at switch-points (transitions between languages). To overcome the need for language identification (LID) tags, a surrogate FrequencyMLM method is proposed, using negative log likelihoods to estimate language.

Furthermore, the work proposes architectural modifications, including residual connections from intermediate layers (found to encode more switch-point information) to the final layer, and an auxiliary LID-based loss to encourage language encoding in these intermediate layers.

Experiments, including sentiment analysis and probing classifiers (linear and conditional), demonstrate that the combined SwitchMLM/FrequencyMLM with ResBERT architecture significantly outperforms standard methods, effectively increasing switch-point information representation within the model.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">The model leverages the cross-attention mechanism between audio input and textual output.</sample>
    <sample id="121">Saying the name of the song or its position (e.g., "the first one").</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on MultiInstruct, a novel approach to improve multi-modal zero-shot learning through instruction tuning. Recognizing a gap in multi-modal instruction datasets compared to the abundance in NLP, they created MultiInstruct, a benchmark dataset comprising 62 diverse multi-modal tasks across 10 categories, each with five expert-written instructions.

Their study utilized OFA, a unified multi-modal pre-trained model, and demonstrated that instruction tuning significantly enhances its performance on both seen and unseen multi-modal tasks. They explored transfer learning from natural instruction datasets, finding it beneficial for both performance and a newly introduced metric called "sensitivity," which measures consistency in output regardless of instruction wording variations.

Results showed that using multiple instructions (five versus one) improved overall performance and reduced sensitivity. Transfer learning from natural instructions also boosted performance on NLP tasks. The researchers are expanding MultiInstruct to include 150 additional vision-language tasks, which will be publicly released. The work highlights the effectiveness of instruction tuning for multi-modal models and provides a valuable resource for future research in this area.</sample>
    <sample id="124">Tan Qingyu from NUS and Alibaba presented work on "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The research identifies three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event, finding previous studies overemphasized the second level.

To address this, they introduced the TempReason dataset, encompassing all three levels and a broad temporal range, and evaluated models (T5-L, FLAN-T5-L, ChatGPT) in Closed Book, Open Book, and Reasoning QA settings. Initial findings revealed biases in T5-L towards the 2000-2020 timeframe and significant performance drops in ChatGPT's month prediction and overall temporal reasoning.

To improve temporal reasoning, they proposed a training strategy: Temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. Experiments on TempReason demonstrated TempT5 outperformed other models, including ChatGPT and fine-tuned T5 variants, particularly in Open Book and Reasoning QA. However, performance fluctuations across time periods suggest potential biases related to training data imbalance, an area for future research.</sample>
    <sample id="125">The presenter is Yanis Labrak.</sample>
    <sample id="126">Yes.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" introduces a method for transferring reasoning abilities from large language models (LLMs) to smaller, more deployable models. Traditional chain-of-thought prompting, which enables LLMs to solve complex tasks, is limited by the need for massive models like GPT-3. This work proposes using these large models as "teacher" models to generate step-by-step solutions for complex problems, which are then used as training data to fine-tune smaller "student" models.

A key innovation is "Diverse Reasoning," where multiple reasoning paths are generated from the teacher model using stochastic sampling, creating a richer training dataset for the student. Experiments across 12 tasks demonstrate that this "fine-tuned CoT" approach significantly outperforms prompt-based baselines and even vanilla fine-tuning, even with student models as small as 0.3 billion parameters.

The research highlights the scalability of the method, noting that performance can be further improved by using larger datasets, better teacher models, or bigger student models, while acknowledging the trade-offs between development and inference costs. The authors provide code and data, including OpenAI inference costs, encouraging further research and exploration of this distillation technique for transferring emergent abilities to smaller models.</sample>
    <sample id="128">Akshatha and Martin introduce "KITMUS," a diagnostic test suite designed to evaluate how well natural language understanding models integrate knowledge from various sources. These models typically rely on knowledge learned during pretraining and information provided during inference. KITMUS focuses on coreference resolution, requiring models to link pronouns to the correct entities, which necessitates both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases").

KITMUS presents three settings: "Background-Pretrain" (background knowledge in pretraining), "Background-Both" (knowledge in both pretraining and inference), and "Background-Inference" (knowledge only at inference). The "Background-Inference" setting simulates scenarios with new or evolving knowledge not present in pretraining data.

Experiments with human participants and coreference resolution models (C2F and BERT4Coref) reveal that models often rely on surface cues rather than true knowledge integration. While task-specific training improves performance, even the best models struggle to reliably integrate background knowledge provided solely at inference time, highlighting a limitation in current NLU models' ability to reason with diverse knowledge sources. The dataset and code are available on GitHub.</sample>
    <sample id="129">The word "warrior" is usually associated with men, so when describing a woman warrior, people typically specify "woman warrior" and mark the term with "woman."</sample>
    <sample id="130">Non-transformer models.</sample>
    <sample id="131">The text does not mention the names of the testing datasets.</sample>
    <sample id="132">Two</sample>
    <sample id="133">Multiple modalities.</sample>
    <sample id="135">ABC-Eval is a new method developed by the Emory NLP Lab and Amazon Alexa AI for evaluating conversational AI models. It addresses the limitations of traditional human evaluation methods like Likert scales and pairwise comparisons, which often lack granularity in assessing dialogue quality.

ABC-Eval focuses on annotating specific behaviors exhibited by models during conversations, such as irrelevance, contradictions, hallucinations, and empathy. By measuring the rates of these "thematic errors," ABC-Eval provides a more precise and reliable assessment of a model's strengths and weaknesses.

The researchers tested ABC-Eval against four state-of-the-art models and compared it to existing evaluation techniques. Their findings demonstrate that ABC-Eval labels exhibit higher inter-annotator agreement and are more predictive of overall conversation quality. Furthermore, ABC-Eval metrics capture unique aspects of chat quality, explaining a significant portion of conversation quality variance compared to Likert ratings.

The study highlights ongoing challenges in conversational AI, quantifying error rates like common sense violations and irrelevant responses. Ultimately, ABC-Eval aims to offer a more robust and detailed framework for evaluating and advancing conversational AI.</sample>
    <sample id="136">Jasivan presented FERMAT, a new evaluation set designed to improve our understanding of numerical reasoning in language models. Current benchmarks relying on accuracy scores are insufficient for diagnosing the strengths and weaknesses of these models, particularly those with fewer parameters (around 3 billion).

FERMAT addresses this by categorizing mathematical questions based on number understanding (integers, decimals), mathematical operations (addition, combinations), and training dependency. Initial zero-shot evaluations revealed poor performance across all categories, suggesting existing benchmarks aren't representative of real-world needs.

Fine-tuning with 200,000 generated examples improved performance, but further analysis showed models often fail even when presented with expressions seen during training, highlighting the importance of linguistic nuances. The study also found that diversifying training templates with datasets like GSM8K and AQUA significantly boosted performance, demonstrating the value of both mathematical and linguistic diversity.

Ultimately, FERMAT aims to provide a more informative alternative to traditional accuracy-based benchmarks, identifying areas for improvement like number encoding and tokenization.</sample>
    <sample id="137">The paper introduces "Tell2Design," a new dataset and task focused on language-guided floor plan generation. Unlike existing text-conditional image generation models that prioritize realism and creativity, Tell2Design aims to enable users to design by providing textual instructions, bridging the gap between user requirements and design solutions.

The task involves generating 2D floor plans from natural language instructions detailing room semantics, geometry, and topology. The Tell2Design dataset comprises over 80,000 language instructions, a mix of human-annotated and artificially generated, associated with publicly available floor plans.

The authors propose a sequence-to-sequence model, leveraging a transformer-based encoder-decoder architecture and a pre-trained T5 language model, to address the challenges of constrained design generation, understanding complex instructions, and handling ambiguous information. Experiments demonstrate that their model significantly outperforms existing text-conditional image generation baselines, achieving high IoU scores. The research highlights a language distribution gap between artificial and human instructions but shows that combining both improves performance. The work aims to establish a foundation for future research in language-guided design generation.</sample>
    <sample id="138">The ability to integrate and use both pretrain-time and inference-time knowledge.</sample>
    <sample id="139">Ying and Zhiyang</sample>
    <sample id="140">Yes, crowd-sourced workers were asked to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">Existing resources for evaluating context-dependent translation are limited by supporting only a few types of context-dependent translations and a restricted set of languages, often relying on domain knowledge and human curation.</sample>
    <sample id="143">Wait-k strategy, Local Agreement, and state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The authors are affiliated with Nantes University Hospital.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This paper addresses the significant issue of omission in dialogue summarization, a problem where generated summaries miss crucial information from the original dialogue. Despite advancements using large language models, approximately 70% of summaries still contain omissions, regardless of dialogue length or domain. The research reveals that omitted information is randomly distributed throughout the dialogue, highlighting the difficulty in identifying key details.

To tackle this, the authors introduce the OLDS dataset, a novel resource providing high-quality omission labels for dialogue summarization across five domains. They developed an automatic method, validated by human evaluation, to generate these labels from diverse candidate summaries. They then explored three baseline models for omission detection, finding the task challenging with an F1-score around 50%.

Finally, the study investigates using detected omissions to refine summaries through a post-editing method. Results demonstrate a significant performance boost when omission content is incorporated, suggesting that omission detection is a valuable task and a promising avenue for improving dialogue summarization quality. The OLDS dataset is publicly available to facilitate further research.</sample>
    <sample id="147">Three.</sample>
    <sample id="149">Yes, the CoNLL++ dataset is publicly available.</sample>
    <sample id="150">The ACL paper introduces MeetingQA, a new extractive question answering dataset built from meeting transcripts. Recognizing that meeting discussions are rich in question-answer interactions, the researchers aim to address the gap left by existing summarization and action item extraction approaches. MeetingQA comprises 7.7K questions with corresponding answer spans extracted from the AMI corpus, featuring unique characteristics like longer, open-ended questions, multi-speaker answers, discontinuous answer spans, and rhetorical questions.

The dataset's creation involved question selection and annotation, achieving a high inter-annotator agreement (Krippendorff's alpha of 0.73). Experiments reveal a significant performance gap between current models and human capabilities, with short-context models (RoBERTa) slightly outperforming long-context models (Longformer). Multi-span models showed comparable or slightly lower performance than single-span models. Data augmentation using silver annotations from MediaSum improved zero-shot performance, and larger instruction-tuned models (FLAN-T5) demonstrated competitive zero-shot results. Error analysis highlighted challenges in identifying rhetorical questions and speaker attribution, indicating that MeetingQA presents a complex and largely unsolved challenge for QA models.</sample>
    <sample id="152">Frederick Riemenschneider presented work on new language models for classical philology, specifically Ancient Greek and Latin. Existing models like Latin BERT and Ancient Greek BERT are limited as they are monolingual and primarily BERT-based. To address this, the team developed GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder) for Ancient Greek, and PhilBERTa and PhilTa (multilingual) for Greek, Latin, and English.

A key innovation was creating a new, high-quality Ancient Greek pre-training corpus by identifying and re-OCRing texts from the Internet Archive using incorrectly transcribed Greek stop words. The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, significantly outperforming existing state-of-the-art, particularly in Ancient Greek lemmatization.

Interestingly, the T5 encoder initially performed poorly but eventually approached the performance of dedicated encoder-only models. While multilingual models showed strong performance, the study found no significant advantage over monolingual models in semantic and world knowledge tasks. The project provides valuable resources and insights for NLP applications in classical studies.</sample>
    <sample id="153">Ninareh Mehrabi's research focuses on addressing ambiguities in prompts given to text-to-image generative models. These ambiguities, like "The girl enters the room with flowers," can lead to images that don't reflect the user's intended meaning. The work proposes frameworks to both resolve these ambiguities and evaluate the faithfulness of generated images to user intention.

The research pipeline involves a benchmark dataset (modified LAVA corpus) covering various ambiguity types. A prompt disambiguation framework uses either clarifying questions generated by a language model (answered by a user) or generates visual interpretations for the user to choose from, resulting in a disambiguated prompt.

To evaluate faithfulness, the original and disambiguated prompts are fed into a text-to-image model, and the resulting images are assessed using a Visual Question Answering (VQA) model. The VQA model determines if the image satisfies the user's stated intention.

Key findings include disparities in resolving different ambiguity types, a positive impact of the disambiguation framework on faithful image generation, and strong agreement between the automatic evaluation framework and human evaluation, demonstrating its reliability.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The paper introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to dialogue summarization that addresses limitations in existing methods. Current techniques often rely on pre-computed static graphs derived from external linguistic tools, which are prone to errors and inflexible.

SDDS overcomes these issues by integrating both static and dynamic graph structures. It begins by encoding dialogue utterances and constructing static graphs using four heuristic methods: Discourse Parsing, Key Co-occurrence, Speaker Relationship modeling, and Utterance Position. These static graphs capture various aspects of the dialogue structure.

A Static-Dynamic Graph module then dynamically learns semantic relationships between utterances using a multi-head attention model, creating a dynamic graph. These graphs are fused into a unified representation. Finally, a pre-trained language model, enhanced with a graph attention layer, generates the summary, effectively incorporating both static and dynamic dialogue structure information. The model aims to improve dialogue summarization accuracy and adaptability by combining the strengths of both static and dynamic graph representations. The code and data are publicly available.</sample>
    <sample id="158">Qipeng Guo from AWS presented "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenges of coreference resolution in lengthy texts. Coreference resolution involves linking mentions of the same entity within a document, a task traditionally hampered by quadratic computational complexity. Cache-based methods offer a linear solution, but standard LRU eviction policies struggle with topic shifts in long documents, leading to high cache misses.

The proposed Dual Cache system tackles this by employing two caches: a local cache (LRU) for local entities and a global cache (LFU) for frequently occurring, globally relevant entities. The model classifies mentions, adding them to the appropriate cache based on frequency. When caches are full, LRU and LFU policies trigger evictions.

Evaluations on benchmarks demonstrated Dual Cache's superior performance compared to baselines, even those with unbounded memory, particularly in long-form documents like a 30,000-word book. It significantly reduced cache misses and offered the best performance-to-cost ratio, effectively balancing efficiency and accuracy in coreference resolution.</sample>
    <sample id="160">An unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">Weakly supervised learning is cheaper than manual labeling because it uses weak labeling sources like heuristic rules or low-quality crowdsourcing.</sample>
    <sample id="165">Wenting Zhao presented a paper on "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," introducing an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization). Abductive reasoning aims to find plausible explanations bridging a context and an outcome, like explaining why Emily made her flight despite being stuck in traffic (possible explanations: flight delay or on-time departure).

Current abductive reasoning approaches often rely on supervised methods, which are hindered by noisy and subjective annotation of plausible explanations. LiPoR addresses this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, eliminating the need for labeled data.

However, maximizing likelihood alone isn't enough to prefer plausible explanations. LiPoR incorporates a regularizer based on the principle of mutual exclusivity – explanations in abductive reasoning are typically mutually exclusive. The regularizer penalizes scenarios where too many explanations receive significant probability mass, encouraging the model to select a smaller, more plausible subset.

Experiments on the AlphaNLI dataset demonstrate LiPoR's superior performance compared to zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement. The paper and code are available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">The presentation introduces "NDCR," a new framework for image retrieval from complex text descriptions. Existing visual language models struggle with this task due to the complexity of the language, relying heavily on analogical reasoning (System 1). NDCR addresses this by integrating System 1 and System 2 thinking, inspired by Divide-and-Conquer strategy and Dual-Process Theory.

The framework comprises three key modules: a Proposition Generator that breaks down complex text into simpler propositions, a Visual-Linguistic Interactor (System 1) for initial visual-proposition interaction, and a Neural-Symbolic Reasoner (System 2) that performs logical reasoning using negation and conjunction operations. The system combines the outputs of both systems to achieve a final solution.

Experimental results demonstrate NDCR's superior performance compared to baselines, with ablation studies confirming the effectiveness of each module. The framework's ability to present inference states and results highlights its interoperable processing. The presentation concludes by suggesting that neural-symbolic calculation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for improving complex reasoning in large language models.</sample>
    <sample id="167">750 documents were aligned manually and with automatic alignment methods.</sample>
    <sample id="168">It was created by collecting data from Reuters News in 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents a systematic study of using prompting strategies with Google's PaLM, a 540 billion-parameter large language model, for machine translation. The research evaluates PaLM's translation capabilities using established MT practices, including current test sets and comparisons against state-of-the-art systems and WMT evaluations.

The study highlights the significant impact of prompting on PaLM's performance, with differences in prompts leading to substantial BLEURT score variations. A 5-shot prompting strategy, where sentences are marked with their language, proved effective. Crucially, the quality of the examples used in prompting is more important than their similarity to the source sentence, favoring examples from curated development data over noisy training data.

While PaLM demonstrates fluency comparable to state-of-the-art systems, it lags in accuracy, frequently exhibiting omission errors where parts of the source sentence are dropped. Despite this, PaLM's output is considered stylistically less awkward than current leading systems. The findings suggest PaLM is approaching the performance of commercial translation systems, offering valuable insights for prompt selection and future LLM-based translation development.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset designed to advance argument quality analysis, addressing limitations found in existing resources. Unlike datasets primarily sourced from crowdsourcing, ArgAnalysis35K boasts 35,000 argument-analysis pairs, prioritizing high-quality arguments drawn from expert and tournament debaters. This focus ensures greater argument quality and a broader range of perspectives.

The dataset distinguishes itself through thematic diversity, moving beyond pre-selected motions to encompass 24 themes identified through expert consultation and circuit experience. A key innovation is the introduction of "analysis," a concept that combines claims and premises into a cohesive explanatory unit, enriching the dataset's depth. To mitigate annotator bias, ArgAnalysis35K employs instance-based annotator reliability, selectively filtering judgments based on potential topical biases.

Finally, a relevance model assigns scores indicating an argument's applicability across various themes, recognizing that arguments often transcend single motions. This allows for a more nuanced understanding of argument relevance. ArgAnalysis35K represents a significant step forward in argument quality analysis, offering a larger, more diverse, and more reliable resource for NLP research.</sample>
    <sample id="175">It approximates finding the highest-scoring permutation with a GPU-friendly continuous relaxation that allows backpropagation and learning of more plausible permutations.</sample>
    <sample id="176">Fairness is defined as whether different political leanings of language models lead to marginalization of people with opposite political opinions or allow hate speech targeting minority groups to run rampant.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar presented research on improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs) using a new method called SymbolicToM. ToM is the ability to understand others' mental states, traditionally tested with false-belief questions (e.g., the Sally-Anne test). Current LLMs like ChatGPT struggle with these tasks.

SymbolicToM addresses this by employing explicit graphical representations of characters' beliefs. These graphs, like BBob and BBob,Alice, map out what different characters believe about the world and each other's beliefs, up to a defined level of complexity. The system uses these pre-computed graphs during inference to efficiently answer ToM-related questions by converting them into factual queries over the graph.

Experiments showed significant performance gains across various LLMs (GPT-3, Macaw, Flan-T5-XXL) compared to fine-tuned models and existing ToM-specific models. Notably, SymbolicToM demonstrated strong generalization capabilities on newly created datasets testing story structure and linguistic diversity, where supervised models faltered. The method avoids overfitting and provides more interpretable reasoning, ultimately enhancing LLMs' ability to understand complex narratives involving multiple characters and their beliefs.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints in goal-oriented scripts. While large language models (LLMs) effectively decompose abstract goals, planning for specific goals like "make a chocolate cake" remains challenging. The authors define constrained language planning, where abstract goals are realized with multifaceted constraints, and propose an "over-generate-then-filter" method to improve LLM planning quality. This involves generating multiple scripts, then using a filter model based on semantic similarity and keyword matching to select the most faithful ones. To enable training smaller, specialized models, they introduce CoScript, a dataset of 55,000 specific goals and scripts generated using LLMs and refined through crowd-sourced validation. CoScript demonstrates high diversity in constraint types. Notably, fine-tuning a T5 model on CoScript outperforms larger LLMs, highlighting the potential of smaller models when trained on high-quality, constrained planning datasets. The work establishes a new research direction and provides a valuable resource for advancing language planning research.</sample>
    <sample id="182">In this paper, "tropicalism" is a trope reflected in the words used to describe Latina women, such as "vibrant" and "curvaceous."</sample>
    <sample id="183">The authors were inspired by a study where prompts similar to those used for the LLMs were given to human subjects.</sample>
    <sample id="184">CXMI (Contextual Mutual Information) and its extension, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is based on data crawled from the web (NACHOS), while ChuBERT is based on anonymized data from the Nantes University Hospital data warehouse (clinical notes).</sample>
    <sample id="187">Two.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand users’ language when they want to make a choice and to benchmark LLMs' entity understanding, specifically focusing on resolving indirect referring expressions for entity selection.</sample>
    <sample id="190">An attacker can steal the model by learning from the embeddings provided by the service.</sample>
    <sample id="191">Three.</sample>
    <sample id="192">Yang Luo presented "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of balancing fast convergence and low memory usage in training large language models. Traditional adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often sacrifice performance.

CAME builds upon Non-negative Matrix Factorization (NMF) used in Adafactor, but tackles Adafactor's slow convergence due to update errors. The core idea is to introduce a "confidence-guided" approach, calculating an instability matrix based on the difference between predicted and actual updates. This instability is then used to adaptively adjust the optimization step, mitigating erroneous updates.

Experiments on BERT, GPT-2, and T5 demonstrate CAME's effectiveness. It achieves significant improvements in validation accuracy compared to Adam and Adafactor, particularly with large batch sizes (8K to 32K). CAME also exhibits lower memory footprint than other optimizers, including Adam and LAMB, while maintaining comparable performance on downstream tasks. The research highlights CAME's ability to efficiently train very large models with reduced memory costs.</sample>
    <sample id="193">Around 1,000 examples of discourse unit pairs were collected.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT, a novel framework for Explainable Question Answering (XQA) called "Reasoning over Hierarchical Question Decomposition Tree." RoHT addresses limitations in existing XQA approaches – neuro-symbolic methods struggling with incomplete knowledge bases and decompose-based methods facing challenges due to the diversity of natural language.

RoHT tackles complex questions by first constructing a Hierarchical Question Decomposition Tree (HQDT), breaking down the question into sub-questions of varying granularity, assessed by certainty scores. Then, it employs probabilistic reasoning to fuse knowledge from both knowledge bases (KBs) and text corpora at different levels. A scheduler selects appropriate knowledge sources for each node, executors retrieve answers with probabilities, and an aggregator combines these to produce top answers.

Evaluations on KQA Pro and Musique datasets demonstrate RoHT's effectiveness. It outperforms existing methods, particularly when integrating KBs and text, showcasing the benefits of explicit question decomposition and leveraging heterogeneous knowledge sources. The results highlight RoHT's ability to handle incomplete KBs and improve performance compared to end-to-end approaches.</sample>
    <sample id="196">"I saw Bart and Lisa."</sample>
    <sample id="197">Four state-of-the-art chat models were evaluated.</sample>
    <sample id="198">Large language models are coming up with longer and longer context windows, so it's crucial to evaluate models' acceptability throughout the context window.</sample>
    <sample id="199">Yes, English performance dropped in seven datasets when training in a multilingual fashion.</sample>
    <sample id="200">No, they don't necessarily know about the entities.</sample>
    <sample id="201">State-of-the-art, neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">The paper does not mention whether the performance drop due to temporal drift impacts specific NER types.</sample>
    <sample id="203">It's increasingly important as NLP tasks become more subjective and socially oriented, and because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="204">The paper states that multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks, but does not specify whether they were fine-tuned with adapters or full fine-tuning.</sample>
    <sample id="205">This presentation explores the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. The research investigates how language models acquire and exhibit political leanings, and how these biases affect fairness in applications like hate speech and fake news detection.

The study found that language models, including GPT-4, demonstrate varying political leanings, often shifting based on the partisan data they are trained on. Models trained on left-leaning data become more liberal, while those trained on right-leaning data become more conservative. Furthermore, models trained on data after 2017 exhibited a greater polarization compared to those trained on earlier data.

Crucially, the research revealed that language models with different political leanings perform differently on downstream tasks. Left-leaning models are better at detecting hate speech against minority groups but worse at detecting it against dominant groups, and vice versa for right-leaning models. Similar biases were observed in fake news detection. The presentation concludes by highlighting the dilemma of addressing these biases – avoiding censorship while mitigating fairness issues – and calls for further research and awareness of this pressing problem.</sample>
    <sample id="206">A model fine-tuned on CE tasks followed by further fine-tuning on debate.</sample>
    <sample id="207">The latest test sets were used to avoid overlap of the test data with the language model's training data.</sample>
    <sample id="208">Three.</sample>
    <sample id="209">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset can be used as a benchmark for automatic text simplification.</sample>
    <sample id="212">One.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk explores the dependency structure of coordination, highlighting differing theoretical approaches: asymmetric (Universal Dependencies, Mel'čuk's Meaning-Text Theory – head is the first conjunct) and symmetric (Prague Dependency Treebanks – head is the conjunction, Hudson's Word Grammar – all conjuncts are heads).

The core argument presented favors symmetric structures, grounded in the principle of dependency length minimization. This principle suggests shorter dependencies are preferred, similar to how direct objects ideally reside close to verbs. Przepiórkowski demonstrates that when a direct object is lengthy, it can move after an adjunct to minimize overall dependency length.

Analysis of the Penn Treebank revealed a consistent tendency: the left conjunct in a coordination is typically shorter, especially when the governor (the word governing the coordination) is on the left or absent. However, this preference vanishes when the governor is on the right. This observation provides empirical support for symmetric coordination structures and challenges asymmetric models, suggesting that the position of the governor influences the preferred order of conjuncts based on length. Further details and full arguments are available in the paper and at the poster session.</sample>
    <sample id="217">The work "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" addresses the limitations of existing dialogue generation methods in handling multi-attribute control and generalizing to unseen attribute combinations. The researchers, Weihao Zeng, Lulu Zhao, and Keqing He, introduce DCG, a Disentangled Controllable Generation model, built upon DialoGPT, designed to learn attribute concepts from seen values and disentangle attribute combinations using a disentanglement loss.

DCG utilizes attribute-oriented and task-oriented prompts to guide generation, concatenated into whole prompt embeddings, and incorporates pseudo combinations to enhance diversity. A novel, reference-free evaluation framework, MAE, is proposed to assess controllability across different attribute granularities without extensive labeled data.

Experiments on DailyDialog-CG demonstrate DCG's superior performance in attribute controllability and text quality compared to baselines. The model effectively generalizes to unseen attribute combinations, outperforming CTRL. MAE exhibits strong correlation with human judgments, proving its effectiveness. Visualization confirms the model's ability to disentangle attribute combinations and learn relationships between them, enabling generalization from seen to unseen attribute values.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">Jia-Huei Ju presented research on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports," conducted with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang. The work addresses the challenge of extracting useful information from Form 10-K reports (annual reports required by the SEC), which traditionally requires significant human effort.

The research was motivated by the observation that financial reports exhibit high text similarity year-over-year (around 80% token overlap). To tackle this, they introduced a "highlighting task" that compares and contrasts a target report with its previous year's report (reference). The goal is to identify key words ("rationale") explaining the relationship between the two.

Their pipeline involves document segmentation, relation recognition (classifying pairs into β, revised, and mismatched categories), and a two-stage fine-tuning process. First, out-of-domain fine-tuning uses the eSNLI dataset. Second, in-domain fine-tuning utilizes revised pairs with pseudo-positive labels and mixed objectives (cross-entropy and KL divergence). Evaluation on the FINAL dataset and eSNLI demonstrates strong performance, particularly with mismatched pairs. The research concludes with future directions including improved effectiveness and incorporating information retrieval techniques.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">The paper does not specify all language pairs analyzed, but it mentions German to English as one example.</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," addresses the difficulty of applying models trained on general domains (like Wikipedia) to specialized areas like biomedicine. Simply adding new data can confuse the model, leading to incorrect answers.

The research investigates data interventions to improve out-of-domain generalization. They explore both zero-shot and few-shot techniques. Few-shot involves prompting large language models to generate examples from target domains, while zero-shot focuses on manipulating question and answer formats and distributions. They found that retriever performance improved by 8% and reader performance by 11% on average with few-shot methods.

The study also identifies the type of dataset shift occurring—no shift, concept shift, covariate shift, or full shift—by measuring the compatibility of the source model with the target domain. Compatibility is assessed by calculating the likelihood of contexts (retriever) and answers (reader). Finally, they demonstrate that few-shot adaptations are generally effective, while zero-shot methods are beneficial for concept and covariate shifts. Overall, the research achieved up to a 24% improvement in reader performance by strategically applying data interventions.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART and base mBART.</sample>
    <sample id="225">53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">Two</sample>
    <sample id="227">Current language models excel at general NLP tasks but lack grounded language understanding – the ability to map natural language to executable plans within a specific environment. This is due to pre-training primarily on text without grounding, creating a gap between pre-training and real-world application. Existing approaches often rely on language models to directly generate plans, which can result in invalid or ungrammatical outputs.

The paper introduces "Pangu," a novel framework addressing this challenge by shifting the focus from generation to discrimination. Pangu utilizes a symbolic agent to propose candidate plans, while a language model (like BERT, T5, or Codex) scores and ranks them. This approach alleviates the language model's burden of ensuring plan validity.

Experiments on knowledge-based question answering demonstrate Pangu's superior performance, particularly in sample efficiency, even with limited data. Notably, Pangu exhibits strong generalizability across different data distributions, unlike autoregressive models that tend to overfit. The core takeaway is that for grounded language understanding, discrimination proves to be a more effective strategy than generation when leveraging language models.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth's paper explores how to automatically detect and improve argumentative claims, a crucial aspect of effective writing. They address the question of when a claim is phrased well enough, introducing two tasks: Suboptimal-Claim detection (identifying claims needing revision) and Claim Improvement Suggestion (suggesting quality improvements).

The research leverages revision data from online debate platforms like Kialo, where final, accepted versions are considered optimal and preceding versions suboptimal. However, working with this data presents challenges, including ensuring representativeness and reliability of the dataset, selecting appropriate model complexity, accounting for contextual information (debate-wide, parent claim, or domain knowledge), and mitigating topical and user biases.

The paper details these challenges and analyzes various strategies for addressing them. Experiments demonstrate that revision-based data can be effectively used for claim assessment, and that modeling the difference between claim versions is helpful for identifying suboptimal claims. The impact of contextual information varies depending on the task and the specific quality issues present. The authors encourage readers to consult the full paper for a comprehensive analysis and findings.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Sara Papi, along with Matteo Negri and Marco Turchi, presented "Attention as a Guide for Simultaneous Speech Translation" (SimulST). SimulST aims to translate spoken language into text in real-time for cross-language communication. Current SimulST models face challenges like complex architectures, lengthy training processes, and the need for multiple models to achieve different latency levels.

Their solution, EDAtt (Encoder-Decoder Attention), utilizes existing offline speech translation models without retraining, managing latency through parameters and leveraging the cross-attention mechanism. EDAtt determines when to emit partial translations based on attention weights. A word is emitted only when attention isn't concentrated on recent speech frames, indicating stable information.

Experiments on German demonstrated that EDAtt outperforms other strategies applied to offline models, achieving higher translation quality (BLEU score) with lower latency, both in terms of average and computational-aware lagging. The code, models, and output are publicly available to encourage reproducibility.</sample>
    <sample id="234">Prompting has a big influence on performance; a simple experiment showed differences of up to 40 BLEURT points. Selecting a good prompting strategy is important, and example quality is more important than similarity to the source sentence.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">The 5 expert-written instructions are randomly combined with each instance during training.</sample>
    <sample id="237">A diagnostic test suite for knowledge integration, introducing a coreference resolution task.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset created by the University of Central Florida for developing meeting summarization technologies. Addressing the need for datasets specific to meeting contexts, it comprises 1,366 City Council meetings from various cities (Boston, Seattle, Denver) and nearly 7,000 instances. Data collection involved using Speechmatics API for transcript generation, identifying meeting details, locating reference summaries from meeting minutes, and aligning timestamps.

The dataset includes meeting transcripts, reference summaries, and URLs, allowing for analysis of abstraction levels using coverage and density scores. Analysis reveals that summaries often include verbatim points, with Seattle and Boston exhibiting higher editing density.

Evaluation of both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet) summarization systems, including GPT-3, showed DialogLM performing best with ROUGE-2 scores. While GPT-3 underperformed with automatic metrics, human evaluation revealed it excelled in fluency and coherence, though less so in informativeness and factuality. MeetingBank aims to facilitate research into advanced meeting summarizers and provide insights into City Council decision-making processes.</sample>
    <sample id="241">Ethan presented a paper detailing a new evaluation framework and system for early misinformation detection, specifically focusing on COVID-19 treatments. The core problem addressed is the unrealistic evaluation and lack of human integration in existing automated misinformation detection systems.

Their framework emphasizes a human-in-the-loop approach, creating an end-to-end system that incorporates human feedback throughout the process, rather than relegating humans to a final judgment. The system consists of two components: claim detection (using keyword filtering and a T5 model to extract and rank claims) and policy violation verification (using a BERT-based stance classification model).

The evaluation focused on "early detection"—identifying unapproved treatments before they appear in debunking news—and demonstrated the system's ability to do so. It also showed a 65% accuracy in detecting policy violations and a high efficiency of 124.2 policy violations confirmed per human hour. The research highlights the importance of realistic evaluation and human collaboration in combating misinformation, offering a valuable perspective for developing more effective and practical systems.</sample>
    <sample id="242">Human evaluation (selecting which conversation is better or rating with Likert scales), Likert ratings (turn-level and dialogue-level), and dialogue-level pairwise comparisons.</sample>
    <sample id="243">Four.</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">This work, "A Needle in a Haystack," presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks, addressing issues with automatic metrics and unclear best practices for MTurk recruitment.

The pipeline first uses a "Qualification Task" to assess workers' ability to evaluate summaries across six dimensions, categorizing them as gold, silver, bronze, or blocked. Only gold and silver workers (13% of 200) proceed. A subsequent "Endurance Task" evaluates their capacity for handling workload, resulting in 6% (12 workers) passing. These workers demonstrated high inter-annotator agreement (IAA), exceeding expert levels with a Krippendorff's Alpha of 0.443.

A "Reference-Based Task" further tested performance, achieving a Krippendorff's Alpha of 0.534. Comparisons with baseline MTurk workers (using MACE) and CloudResearch workers showed the pipeline achieved comparable quality (Alpha of 0.513) at a lower cost, though CloudResearch had a lower task acceptance rate. Analysis revealed a correlation between pipeline and CloudResearch workers, and a strong correlation between GPT models and expert judgments.

The pipeline ultimately identified a small group of highly reliable workers (4 gold, 8 silver), offering a cost-effective approach to high-agreement annotations and avoiding wasted resources. Future work will focus on improving worker quality in both agreement and correctness across various tasks and languages.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The paper introduces FactKG, a new dataset and task for knowledge graph-based fact verification. Existing datasets like FEVER and TabFact rely on text or tables, but FactKG leverages knowledge graphs (specifically DBpedia) to provide more reliable and practical evidence for verifying natural language claims.

FactKG claims are presented in both written and colloquial styles to reflect real-world usage. The dataset includes "SUPPORTED" and "REFUTED" labels and focuses on five reasoning types: one-hop, conjunction, existence, multi-hop, and negation. These reasoning types require varying levels of graph traversal and inference. For example, one-hop verification checks for direct relationships, while multi-hop requires navigating paths between entities.

To generate colloquial claims, the researchers utilized a style transfer model and presupposition templates. Experiments demonstrate that models utilizing graph evidence (like the GEAR model) significantly outperform those relying solely on claims, highlighting the value of knowledge graph reasoning for fact verification. The dataset is publicly available, encouraging further research in this area.</sample>
    <sample id="248">The annotators for NLPositionality come from over 1000 individuals across 87 countries, providing a diverse demographic representation.</sample>
    <sample id="249">Noise was added to the input sentence while preserving the relevant structure.</sample>
    <sample id="250">Evaluating multiple aspects of chat quality to understand strengths and weaknesses on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces U-CREAT, a novel unsupervised pipeline for Prior Case Retrieval (PCR), a task crucial for legal professionals. The work addresses the challenge of efficiently finding relevant past precedents given the increasing volume of legal cases.

Two key contributions are presented: the IL-PCR dataset, a new benchmark for PCR tasks featuring 7,070 Indian legal cases, and the U-CREAT pipeline itself. U-CREAT utilizes an event-based approach, extracting events (subject-verb-object triplets) from legal documents using dependency parsing. These events are then compared between query and candidate documents to determine relevance.

Experiments demonstrate that event-based models, particularly the "Event Filtered Documents" approach, significantly outperform traditional count-based and transformer-based models, including legal-specific transformer models like InCaseLawBERT and InLegalBERT. U-CREAT achieves state-of-the-art results on the COLIEE dataset and exhibits high retrieval efficiency and low inference time, generalizing across legal systems without specific tuning. The research highlights the effectiveness of event extraction for PCR and paves the way for future advancements in the field.</sample>
    <sample id="253">"DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media" is a research project by a team from Mexico and Spain focused on automatically analyzing social media posts to detect mental health disorders. The project addresses the challenge of limited annotated data by employing domain adaptation, leveraging knowledge from general language models like BERT and specializing them for social media and mental health contexts.

DisorBERT utilizes a two-stage approach: first learning social media language, then specializing in mental disorder identification through guided masking, which encourages the model to focus on key words. Evaluation using the eRisk dataset demonstrates a strong balance between precision and recall, outperforming MentalBERT. Analysis of predicted words reveals DisorBERT's tendency to generate terms with negative psychological connotations compared to BERT. Visualization of attention scores on user posts highlights relevant keywords like "anxious" and "medication" in relation to depression.

The research concludes that the combination of double domain adaptation and guided masking effectively identifies signs of mental disorders in social media, with future work planned to explore different lexical resources and clinical data integration.</sample>
    <sample id="254">This research introduces a novel framework, "Uncertainty Guided Label Denoising," to improve document-level distant relation extraction (DocRE). Existing methods using distantly supervised (DS) data for DocRE suffer from noise induction due to false-positive pseudo labels. To address this, the proposed framework trains a DocRE model with both DS and human-annotated data, then employs uncertainty estimation to assess the reliability of generated pseudo labels.

A key innovation is an instance-level uncertainty estimation method designed to handle overlapping relations, a common challenge where an entity pair has multiple potential relationships. This method utilizes Monte Carlo dropout to model uncertainty and calculates instance-level scores for each relation. Dynamic class uncertainty thresholds are then applied to filter out high-uncertainty pseudo labels, replacing them with more reliable ones.

A multi-phase training strategy iteratively re-labels DS data, maximizing its utility. Experimental results on public datasets demonstrate significant performance improvements compared to existing baselines, highlighting the effectiveness of the proposed approach in mitigating noise and enhancing DocRE accuracy.</sample>
    <sample id="255">It's crucial for zero and one-shot prompting.</sample>
    <sample id="257">Four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces their work, "Can Large Language Models Be an Alternative to Human Evaluation?" which explores using LLMs to assess text quality in NLP. The core idea is to provide LLMs with instructions and samples, prompting them to generate ratings.

Motivated by the instability and reproducibility issues of human evaluation, the researchers investigated LLMs as a potential alternative. They conducted experiments rating stories generated by GPT-2 and written by humans, evaluating them on grammar, coherence, likability, and relevance. Human evaluators (English teachers) provided ground-truth ratings for comparison.

The study tested four LLMs: T0, InstructGPT (curie and davinci), and ChatGPT. While human raters preferred human-written stories, some smaller LLMs didn't show a clear preference. However, Davinci and ChatGPT demonstrated a preference for human-written text, mirroring the human evaluators' judgments.

The paper addresses further questions regarding agreement between LLMs and humans, the impact of instruction wording, sampling methods, and the cost-benefit analysis of LLM evaluation compared to human evaluation, as well as its applicability to other tasks. The researchers encourage readers to consult their paper or visit their poster at ACL for more details.</sample>
    <sample id="259">The presentation introduces XSemPLR, a new benchmark dataset for cross-lingual semantic parsing. This task involves translating queries from multiple natural languages into various meaning representations like SQL, Lambda Calculus, and FunQL. Existing models often lack broad language and representation coverage, prompting the creation of XSemPLR.

The dataset encompasses 9 datasets across 5 tasks, 8 meaning representations, and 22 languages spanning 15 language families. The study evaluates models across six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot.

Key findings include that Encoder-Decoder models generally outperform Encoder-PTR models. Training multilingual models with a mix of languages improves performance across most languages, though English sometimes sees a decline ("Curse of Multilinguality"). Zero-shot transfer exhibits a significant performance gap compared to monolingual settings, which is substantially reduced with few-shot learning. The research also highlights the limitations of large language models like Codex and BLOOM for this specific task and demonstrates that pretraining on English can significantly improve few-shot performance in other languages.</sample>
    <sample id="260">One. Jingwei Yi is mentioned by name.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper does not mention the number of authors.</sample>
    <sample id="263">This work addresses the instability of in-context learning in large language models (LLMs) due to label biases. It systematically categorizes these biases, identifying three key types: vanilla-label bias (model's inherent preference), context-label bias (influence of the provided examples), and a newly identified domain-label bias (impact of the task's corpus).

The research demonstrates that simply exposing the model to random in-domain words can significantly bias predictions, highlighting the importance of domain-label bias. Traditional calibration methods struggle with tasks exhibiting high domain-label bias.

To mitigate these biases, the authors propose "domain-context calibration," which utilizes random in-domain words as content-free text to estimate and correct biases. This approach outperforms previous methods, particularly on tasks with substantial domain-label bias, leading to improved decision boundaries and overall performance across various datasets and models, including GPT-3.

The study emphasizes that single, predefined content-free tokens are suboptimal and that incorporating random in-domain words is crucial for effectively addressing label biases in in-context learning.</sample>
    <sample id="264">Lin Wang presented "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenge of multimodal text generation where data annotation is expensive and models struggle with domain shifts. TAVT aims to create a model that quickly adapts to new audio-visual domains with limited labeled data.

The framework consists of three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network aligns visual concepts across domains into a unified auditory semantic space using audio clusters. The encoder-generator utilizes a transformer-based architecture with an attention mechanism to evaluate the contribution of each modality.

To directly optimize visual-audio alignment, a Dual Counterfactual Contrastive Learning (DCLL) loss function is introduced. The model is trained using a meta-learning approach, similar to MAML, with support and query sets for adaptation. Experiments on MSVD and MSR-VTT benchmarks demonstrate TAVT's superior performance compared to existing methods, particularly in low-resource domains, highlighting its ability to handle domain shifts effectively.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">The authors are affiliated with the enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies."</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="270">Emory University and Amazon Alexa AI.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Seven.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This work introduces "IndicMT Eval," a dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages, addressing the understudied evaluation of translations beyond English. The dataset comprises 7,000 samples across five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati), generated from 1,400 candidate translations per language using seven different MT models.

Human annotators, fluent in both the source and target languages, meticulously evaluated these translations, marking errors by type and severity using the MQM framework and providing overall scores. Analysis revealed that COMET-metric variants demonstrated the highest overall correlations with human judgments. However, many metrics exhibited skewed score distributions, limiting their interpretability.

Interestingly, metrics showed improved correlation with human scores when focusing on accuracy errors specifically. Leveraging the dataset, the researchers fine-tuned COMET, creating "IndicCOMET," which outperformed standard COMET baselines across multiple languages, even in zero-shot scenarios. IndicCOMET also demonstrated greater robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for improved MT evaluation in diverse linguistic contexts. The dataset is publicly available to facilitate further research.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The Marked Words method identifies words that distinguish marked groups from unmarked ones, using weighted log-odds ratios to compare the top words for each marked group. It draws upon the sociolinguistic concept of "markedness," where dominant groups are unmarked, and marginalized groups are linguistically marked.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">Shi Tao presented "MultiEMO," a novel framework for emotion recognition in conversations (ERC) addressing limitations in existing methods. Current ERC approaches often inadequately utilize multimodal information (text, audio, visual) and struggle with minority emotion classes and distinguishing similar emotions.

MultiEMO tackles these issues with four key components: a specialized visual feature extractor (VisExtNet) focusing solely on facial expressions, a multimodal fusion model (MultiAttn) using bidirectional multi-head cross-attention to integrate modalities, and a Sample-Weighted Focal Contrastive Loss (SWFC) to improve classification of minority and semantically similar emotions. VisExtNet avoids redundant scene information, while MultiAttn facilitates cross-modal correlation learning. The SWFC loss prioritizes difficult samples and maximizes inter-class distances.

Extensive experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in handling minority and similar emotions. Visualization of heatmaps further highlights MultiEMO's ability to discern emotions when modalities provide asynchronous cues. Limitations include VisExtNet's inability to differentiate speakers, SWFC's batch size requirements, and persistent performance gaps between minority and majority emotion classes.</sample>
    <sample id="281">This work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," investigates when and how well machine translation models handle context-dependent translations. Traditional metrics like BLEU struggle to capture these nuances, prompting the need for a more targeted evaluation.

The researchers extended CXMI to Pointwise CXMI (P-CXMI) to measure context usage at the sentence and word level, identifying words requiring context for accurate translation. Analyzing TED talk transcripts across 14 languages revealed patterns related to part-of-speech (e.g., Arabic dual pronouns), vocabulary (e.g., consistent translation of Chinese proper nouns), formality, and sentence structure (e.g., ellipsis resolution).

To facilitate evaluation, they developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words based on five discourse phenomena. Using this benchmark, they found that while context-aware models excel in formality and lexical cohesion, improvements are needed for phenomena like pronouns and verb forms. Notably, DeepL outperformed Google Translate in document-level translation according to their benchmark. The study highlights the limitations of corpus-level metrics and provides a valuable resource for advancing document-level machine translation research.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story-level text style transfer, addressing a significant gap in natural language generation research. Unlike previous studies focused on token or sentence-level style transfer, StoryTrans operates at the discourse level to effectively imitate author style. The core challenges tackled are capturing complex author linguistic preferences, particularly discourse structures, and preserving content while transferring style-specific information across different topics. StoryTrans utilizes discourse representations combined with style embeddings to generate text in target styles. A key innovation is a training objective that disentangles style and content within discourse representations and employs a two-stage generation process: first transferring the source text with masked content, then incorporating style-specific keywords. The model incorporates an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses. Extensive experiments on newly collected Chinese and English datasets demonstrate StoryTrans's superior performance over baselines in both style control and content preservation, with style visualizations confirming alignment with target styles. The model effectively enriches storylines and maintains source semantics, showcasing a significant advancement in story-level style transfer.</sample>
    <sample id="283">The Prague approach.</sample>
    <sample id="284">FSUIE is a novel approach to Universal Information Extraction (UIE) designed to overcome limitations in existing span-based models. These models often over-rely on precise span boundaries, which can be ambiguous in annotation. FSUIE addresses this by introducing a "fuzzy span" mechanism, treating span boundaries as continuous probability distributions rather than fixed points.

The model incorporates two key components: a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL utilizes Binary Cross Entropy and KL-divergence to learn from both golden and fuzzy boundaries, while FSA adaptively adjusts the model's attention span. FSA employs an optimizable parameter (delta) to dynamically control the attention range and linearly decays attention distribution near the boundaries, focusing on relevant semantic information within a limited range.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's effectiveness, achieving state-of-the-art results on several datasets. Notably, FSUIE exhibits improved performance on smaller datasets and stronger generalization capabilities for domain-specific information. Ablation studies confirm the benefits of both FSL and FSA, highlighting their roles in convergence speed and information extraction capability.</sample>
    <sample id="285">Mingqi Gao from Peking University presented "Reference Matters," a study addressing factual errors in dialogue summarization. Existing factual error correction (FEC) models for summarization are flawed due to unreliable evaluation methods. Current metrics like FactCC and DAE provide vague, overall scores and fail to distinguish between true error correction and summary replacement.

The research argues for incorporating manually annotated reference corrections to improve FEC model training and evaluation. This approach aligns with the core requirements of factual error correction: minimal changes (substitutions, insertions, deletions) resulting in fluent, non-redundant summaries.

The study proposes a new taxonomy of factual errors, categorizing them as content-based (part of speech, dependencies) and form-based (addition, deletion, substitution). Their evaluation framework, built on ERRANT, involves alignment, classification, and comparison.

Key findings include the benefit of training FEC models with reference summaries, the need for improved evaluation methods, and the potential of combining human-annotated and synthetic data. Current FEC models struggle with additions and more complex errors like attribute and modality errors.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Four.</sample>
    <sample id="288">BLiMP, SyntaxGym, and CrowS pairs.</sample>
    <sample id="290">WSL, COSINE, FTw</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presented a collaborative work between the University of Turin and Amazon Alexa focused on irony detection in natural language understanding. Traditional approaches rely on "ground truth" annotations, but this research challenges that assumption, recognizing irony's subjective nature.

To investigate, they developed the EPIC corpus, a dataset of 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. Data was crowdsourced using Prolific, with 74 annotators providing five annotations per conversation, assessing whether a reply was ironic.

Analysis revealed variations in annotation agreement based on annotator demographics like gender, age, and nationality. The team developed "perspective-aware models," training separate models on data subsets based on annotator groups. While raw performance didn't show clear trends, these models demonstrated significantly higher confidence in their predictions. Interestingly, disagreement on irony perception was highest between generations close in age and between annotators from the UK and Ireland, suggesting cultural and generational influences on interpreting irony.</sample>
    <sample id="297">This research project, "From Dogwhistles to Bullhorns," investigates coded rhetoric—specifically dogwhistles—and their implications for NLP, linguistics, and online content moderation. Dogwhistles are terms that convey a hidden, often inflammatory, message to an in-group while maintaining plausible deniability for the speaker.

The project developed a typology and glossary of over 340 dogwhistles (primarily US-centric, focusing on racist, transphobic, and anti-Semitic terms), categorized by register, type, and persona. A case study of historical U.S. political speeches revealed a correlation between dogwhistle usage and the Republican Southern Strategy, demonstrating a shift towards coded language after the Civil Rights era.

The research also evaluated language models (GPT-3) for dogwhistle recognition. While GPT-3 could surface some dogwhistles, performance varied, particularly with informal and transphobic terms. Identifying covert meanings improved with specific prompting strategies. Finally, a toxicity detection case study demonstrated that sentences using dogwhistles are often rated as less toxic than those using explicit slurs, highlighting how these coded terms can evade content moderation.</sample>
    <sample id="298">Retraining or continuing to pre-train some models with more recent data resulted in performance degradation with a larger temporal gap.</sample>
    <sample id="299">This work addresses the issue of NLI models relying on shortcuts—spurious correlations in datasets that lead to good in-distribution performance but poor out-of-distribution generalization. Existing shortcut mitigation methods often require auxiliary models with specific knowledge and assumptions about shortcut types, which limits their applicability.

The proposed solution, minimax training, avoids these limitations. It pits a learner NLI model against an auxiliary model. The learner minimizes the standard NLI loss, while the auxiliary maximizes the learner's loss by generating example weights, incentivizing the learner to focus on "hard" examples—under-represented instances that contradict shortcuts. This alternating optimization encourages the learner to prioritize learning from these challenging examples, improving out-of-distribution performance.

The method doesn't require pre-defined shortcut knowledge and uses a lightweight feed-forward network for the auxiliary. Experiments on MNLI, FEVER, and QQP datasets with adversarial test sets demonstrate consistent improvements in out-of-distribution accuracy compared to standard training and existing mitigation techniques, while maintaining high in-distribution accuracy. Further analysis explores the impact of model size, synthetic shortcuts, and transferability.</sample>
    <sample id="300">This presentation introduces "interactive dictation," a new task aiming to create a more natural and intuitive voice-based document editing experience. Unlike existing speech-to-text systems that primarily focus on dictation, interactive dictation allows users to seamlessly interleave dictation and editing commands using natural language, without needing to memorize specific commands.

The research team formalized the task as a four-step process: ASR recognition, utterance segmentation (dictation vs. command), command extraction and normalization, and finally, execution of both dictation and commands to achieve the final document state. To address the lack of existing data, they designed a new annotation interface and built a dataset.

A baseline system was developed, utilizing separate models for each step, including ASR repair and an interpretation model tested with T5 and GPT-3 architectures. Results showed a trade-off between runtime and accuracy, with GPT-3 generally being more accurate but slower. For T5, predicting programs proved more efficient. The researchers emphasize the potential for future improvements and have released the code to encourage further research in this area.</sample>
    <sample id="302">The tokens are tagged with an unordered multiset, so they need to be put into the right order via permutation.</sample>
    <sample id="303">Because they cannot determine if positive stereotypes arise from value alignment or anti-stereotyping methods without more transparency.</sample>
    <sample id="304">Acceptable or ungrammatical sentences used in pairs to evaluate language models' acceptability judgments.</sample>
    <sample id="305">Dawei's presentation critiques recent advances in Weakly Supervised Learning (WSL), highlighting a significant, often overlooked, dependency on clean validation data. While WSL aims to reduce manual labeling by using cheaper, noisy labels, the research reveals that current methods critically rely on a small set of clean, manually annotated samples for model selection.

The study found that without clean validation data, WSL models fail to generalize beyond the weak labels, rendering the training process ineffective. Even a modest number of clean samples (around 20 per class) significantly improves performance, but direct fine-tuning on these clean samples consistently outperforms WSL approaches. Furthermore, allowing continuous fine-tuning on the clean validation data enables simpler models to achieve comparable results to more complex WSL methods.

Dawei concludes that the performance gains of existing WSL methods are often overstated and recommends future research to transparently report model selection criteria, compare WSL with few-shot learning, and consider continuous fine-tuning as a strong baseline. The code for their study is publicly available.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented research on evaluating entity tracking abilities in large language models (LLMs). They argue that understanding how entities change within a discourse is crucial for comprehension, but existing LLMs haven't been systematically tested for this.

Their study addresses challenges in designing such a task, specifically avoiding shortcuts like exploiting common patterns in pre-training data or relying on simple word associations. They created a task using "boxes and objects" where models predict box contents after a series of state-changing operations.

Experiments with Flan-T5, GPT-3, and GPT-3.5 revealed that only text-davinci-003 showed non-trivial entity tracking. Notably, GPT-3.5 models, trained with substantial code data, demonstrated this ability, while others did not. Fine-tuning smaller models like T5-base enabled entity tracking, highlighting the importance of pre-training. 

The researchers acknowledge that the generalizability of these findings remains unclear and encourage readers to consult their paper for further details, including experiments with GPT-4.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework developed to characterize biases in NLP datasets and models, revealing how they reflect the positionality—the perspectives shaped by demographics and experiences—of their creators. The research highlights that existing datasets and models often align with specific populations, primarily English-speaking countries and individuals with higher education.

The study, conducted with collaborators from the University of Washington and Allen Institute for AI, involved re-annotating datasets with over 1,600 diverse annotators from 87 countries using the Lab in the Wild platform. This allowed for a comparison of annotations with models like GPT-4, Perspective API, and Dynahate, uncovering significant alignment patterns.

Findings indicate that while models like GPT-4 and Dynahate demonstrate alignment with certain demographics, they often fall short in representing marginalized groups, such as non-binary individuals. The research advocates for increased transparency in design choices, a perspectivist approach to NLP research, and the creation of specialized datasets and models tailored to specific communities to promote more inclusive and equitable NLP technologies.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The affiliations of the authors are not mentioned in the provided text.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning benchmark dataset, consisting of 62 diverse multi-modal tasks covering 10 broad categories.</sample>
    <sample id="313">Three.</sample>
    <sample id="314">The talk does not define binary coordination.</sample>
    <sample id="315">The prompts used in this study were inspired by those given to human subjects.</sample>
    <sample id="316">Smaller models, like T5, can surpass larger models when trained on a suitable dataset like CoScript, demonstrating the value of specialized datasets for constrained language planning.</sample>
    <sample id="317">Peng Li from Fudan University presented "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing the challenges of information extraction (IE) using traditional language models like T5 and GPT-3. These models struggle because the structured output of IE doesn't align with the plain text format used during pre-training.

CodeIE tackles this by reframing IE as a structure-to-structure code generation task, leveraging code-based large language models like Codex. The approach involves designing prompts that instruct the model to generate code that extracts information, such as named entities and relations, and stores them in a structured format.

Experiments across multiple datasets demonstrated that CodeIE significantly outperformed traditional models (UIE, GPT-3) in few-shot settings. Analysis revealed that code language models exhibit lower perplexity and fewer structural errors compared to text-based models. Furthermore, Codex consistently outperformed GPT-3, and code-style prompts improved recall. The research suggests that aligning pre-training and output formats, along with utilizing code-specialized models, enhances information extraction performance. The paper and code are publicly available.</sample>
    <sample id="319">From-scratch pre-training and continual pre-training.</sample>
    <sample id="320">Adaptive overfitting is not observed.</sample>
    <sample id="321">The quality of simplification was evaluated using evaluation metrics and scores from fine-tuning language models (long-mBART and mBART) to produce simplified text.</sample>
    <sample id="322">Enrico's ACL 23 presentation explores how text classifiers learn about morality, challenging the common NLP approach of treating morality as a single scale between "moral" and "immoral." He argues this simplification is dangerous due to the subjective and pluralistic nature of human morality.

Drawing on Moral Foundation Theory, which posits five distinct moral foundations (fairness, authority, care, loyalty, and sanctity), Enrico's research investigates what language models learn about morality in text. He utilizes the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven domains, to examine how morality is expressed differently.

The presentation highlights a key finding: language models can recognize nuanced differences in moral expression across domains. For example, the concept of "subversion" is viewed negatively within the "All Lives Matter" domain (associated with words like "overthrow" and "mayhem") but is more positively framed within the "Black Lives Matter" domain.

Ultimately, Enrico cautions against using a single model across diverse domains, as this can lead to misinterpretations of morality. His work emphasizes the need for a more sophisticated understanding of morality in language models, acknowledging its complexity and context-dependent nature.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods combining language models and knowledge graphs. Current approaches often retrieve noisy entities and fail to effectively integrate textual and graph information.

DHLK constructs a Heterogeneous Knowledge Graph (HKG) from multiple knowledge bases (ConceptNet, WordNet, Wiktionary) using a two-stage pruning strategy and Knowledge Representation Learning (KRL) to optimize its structure. It then utilizes RoBERTa and a novel Relation Mask Self-Attention (RMSA) mechanism to encode and fuse the QA context with the HKG. RMSA, inspired by RGAT, incorporates relationships into the attention mechanism, allowing for dynamic removal of irrelevant entities based on attention weights.

The method enhances the QA context with HKG path information and employs a multi-layer perceptron (MLP) to predict answers. Experiments on CommonsenseQA and OpenBookQA demonstrate DHLK's effectiveness, achieving competitive results compared to existing language model and HKG-based methods. Key entities are extracted using KeyBERT, and knowledge paths are retrieved within ConceptNet to further improve performance.</sample>
    <sample id="324">Yes, language models exhibit varying political leanings, occupying all four quadrants on the political spectrum. GPT-4 is the most liberal, with the GPT series generally more socially liberal than BART series. These biases are influenced by the political leaning of their pretraining data.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="327">Xiao Xu from Harbin Institute of Technology presented "ManagerTower," a novel vision-language (VL) architecture designed to improve representation learning. Building upon previous models like METER and BridgeTower, ManagerTower addresses limitations in how unimodal semantic knowledge is utilized.

Traditional two-tower architectures often overlook valuable information at different layers within unimodal encoders. BridgeTower attempts to connect these layers but struggles with inefficient utilization and scalability. ManagerTower introduces "managers" within each cross-modal layer to adaptively aggregate insights from multiple unimodal representations (from RoBERTa and CLIP-ViT base).

This allows for more comprehensive cross-modal alignment and fusion. Experiments with only 4 million images demonstrate ManagerTower's superior performance on various downstream tasks, including a 39.15% accuracy on the Wikivideo test standard, significantly outperforming BridgeTower and even surpassing larger models. Visualization of manager weights reveals that adaptive managers effectively tailor the use of unimodal knowledge based on the specific cross-modal layer, unlike static managers. The paper, code, and models are publicly available.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, addressing the limitations of existing methods that rely on pseudo-label generation. Traditional methods often suffer from simplistic pseudo-queries, misalignment between queries and video segments, and vulnerability to label noise. To overcome these challenges, the proposed "noise-resistant Structured Pseudo-Label generation" method leverages a pre-trained image caption model to generate complex, free-form pseudo-queries. Subsequently, it employs a pre-trained model to measure frame-query relevance, creating pseudo-events that ensure high relevance within the event and low relevance outside it, effectively aligning video segments with queries. Furthermore, the method mitigates label noise by dynamically weighting samples based on prediction confidence and Intersection over Union (IoU) with pseudo-labels, and by refining pseudo-labels through iterative model training. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate significant performance improvements over existing zero-shot approaches, achieving state-of-the-art results on key metrics like R@M and mIoU. The code is publicly available.</sample>
    <sample id="330">Yes, cumulative training performed equal or better than iterative across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Transcripts of TED talks.</sample>
    <sample id="333">Wenhao from Nanjing University introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," a framework designed to improve neural machine translation (NMT) models. Traditional NMT models often have non-smooth representation spaces, leading to poor generalization and performance, particularly with low-frequency tokens. kNN-MT addresses this by smoothing predictions using nearest neighbors, but it suffers from slow inference and difficulty updating representations.

INK overcomes these limitations by iteratively refining the NMT model's representation space using kNN knowledge. The framework involves extracting kNN knowledge to guide an adapter, updating representations, and refreshing a datastore asynchronously. Crucially, the datastore is ultimately discarded, reducing memory usage and speeding up inference.

Experiments using the WMT’19 German-English translation task demonstrate that INK significantly outperforms state-of-the-art kNN-MT systems, achieving substantial gains in BLEU and COMET scores while requiring less memory and offering faster inference. The research highlights the benefits of using kNN knowledge to refine representations and suggests that combining adapters and datastores can further enhance translation performance.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This research introduces a novel approach called "Graph-based Relation Mining" to effectively learn embeddings for out-of-vocabulary (OOV) words, a common challenge in natural language processing. The method leverages word formation and association, inspired by human learning, by constructing a "Word Relationship Graph." This graph represents a word (or its wordpieces) as nodes, with word embeddings as node attributes.

The approach uses a self-attention network to assign attributes to OOV nodes based on their characters, followed by a two-level Graph Attention Network to filter noise and create node-level representations. A readout block then captures the overall graph structure, summarizing word formation. Contrastive learning is employed to align the learned embeddings with a background embedding model, ensuring semantic consistency.

Experiments demonstrate the model's superior performance compared to existing methods in both intrinsic and extrinsic tasks, benefiting both static and contextual models. The researchers also explore the model's applicability to other languages, noting that agglutinative languages are particularly well-suited, while fusional languages present greater challenges. Ultimately, the model's ability to handle complex word formations makes it a promising solution for OOV word embedding learning.</sample>
    <sample id="338">This research investigates the quality of human-generated natural language explanations used to improve AI model performance. The core question addressed is: "How do we evaluate the quality of human-annotated explanations?" Traditional metrics like BLEU and ROUGE are inadequate due to the subjective nature of explanations.

The researchers developed a unified data structure converting various tasks into a multiple-choice format, enabling consistent evaluation. Through experiments on five datasets (CoS-E, ECQA, e-SNLI, ComVE), they found that fine-tuning with explanations doesn't always impart new knowledge but can lead to models relying on the explanation input. They introduced a new metric, TREU, which extends the "simulatability score" by evaluating explanation helpfulness during fine-tuning.

Evaluation using TREU and simulatability score on models T5 and BART revealed that even seemingly "low-quality" human explanations can benefit model predictions. TREU consistently ranked dataset qualities better than simulatability, demonstrating its sensitivity to task-specific nuances. The study highlights the task-dependent nature of explanation utility and advocates for quality checks in human annotation processes to foster better human-AI collaboration.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a joint work with several collaborators. The core problem addressed is the need for large, high-quality, and syntactically diverse paraphrase datasets for NLP applications like question answering and chatbots. Existing datasets either lack scale (human-annotated) or syntactic diversity (automatically generated via back-translation).

ParaAMR tackles this by leveraging Abstract Meaning Representations (AMR) – graph-based representations of sentence meaning. The method involves parsing sentences into AMR graphs, randomly changing the "focus" (root node) of the graph, and then using an AMR-to-text generator to create paraphrases. This process ensures semantic similarity while introducing syntactic variation due to the generator's focus on the new root node.

The resulting ParaAMR dataset contains 15 million source sentences and approximately 6.9 paraphrases per sentence. Evaluations demonstrate that ParaAMR maintains good semantic similarity while exhibiting significantly higher syntactic diversity compared to other back-translation datasets. Experiments on sentence embeddings, syntactic control paraphrase generation, and few-shot learning showcase ParaAMR's benefits, and the dataset is publicly available.</sample>
    <sample id="341">Average lagging and computational-aware average lagging.</sample>
    <sample id="342">This presentation introduces LiveChat, a novel large-scale personalized dialogue dataset automatically constructed from live streaming videos on Chinese TikTok and Douyin. Existing open-domain dialogue datasets are primarily text-based and often limited in scale due to manual annotation requirements. LiveChat addresses these limitations by leveraging video content and an automated "reply-to-whom" matching method to create dialogues.

The dataset construction involves three steps: extracting audio from videos, transcribing it using ASR, and matching audience comments to form dialogues. Persona information is also collected, categorized into basic profiles and dynamically extracted profiles using rules and classifiers, enabling personalized dialogue generation. LiveChat significantly surpasses existing datasets in scale, being video-sourced, featuring longer average sessions, and including personal annotations.

Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions. Furthermore, evaluations of pre-trained dialogue models, particularly BART, highlight the distinctiveness of LiveChat's domain. The research also explores in-context learning, showing performance improvements with increasing demonstration shots, though excessive demonstrations can introduce noise. Future work will focus on efficient transfer learning of large language models for LiveChat.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves strong compositional generalization to deeper recursion without relying on explicit tree structures. Traditional methods often incorporate trees to capture compositional relationships between utterances and logical forms, but this requires complex pre-processing and grammar induction. Our approach directly models these correspondences by first tagging each input token with a multiset of output tokens, effectively identifying the necessary elements for the logical form. Subsequently, a permutation model arranges these tokens into the correct order. A key innovation is a flexible permutation prediction method that avoids hard constraints, enabling expressive generalization. We address the challenges of unaligned input-output data and the NP-hard problem of finding optimal permutations through alignment induction and a GPU-friendly continuous relaxation technique, allowing for backpropagation and learning of linguistically plausible permutations. Experimental results on the COGS benchmark demonstrate significant performance gains over existing treeless models, particularly in generalizing to deeper recursion, while acknowledging ongoing challenges with other forms of structural generalization.</sample>
    <sample id="346">This information is not provided in the text.</sample>
    <sample id="348">This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," introduces a novel method for identifying and analyzing stereotypes in large language models (LLMs). Addressing limitations of existing approaches that rely on curated datasets or capture only broad associations, our method leverages the instruction-following capabilities of LLMs to generate personas based on demographic prompts (e.g., "Imagine you are an Asian woman."). We then employ "Marked Words," a sociolinguistic-inspired technique, to pinpoint words that distinguish marked (marginalized) groups from unmarked (dominant) ones.

Our analysis reveals that LLMs generate personas containing stereotypes, often manifesting as seemingly positive but ultimately harmful essentializing narratives. While lexicons may not capture these subtle biases, Marked Words exposes patterns like the portrayal of Asian women as "delicate," Latina women as "vibrant," and Black women as "strong," reflecting historical tropes and reinforcing othering. These findings highlight the need to move beyond simply identifying negative stereotypes and to address positive stereotypes and intersectional biases. We conclude with recommendations for researchers and model owners, emphasizing the importance of intersectional analysis, addressing positive stereotypes, and increasing transparency in bias mitigation techniques.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" questions the validity of claims of superhuman performance in Natural Language Understanding (NLU) based on leaderboard scores. It argues that current benchmarks like SuperGLUE and SQuAD, while showing models outperforming humans, are flawed and lead to misleading conclusions.

The analysis reveals several issues: systems and humans are evaluated on different subsets of data, ground-truth answers contain errors, and human performance baselines are often poorly estimated. Systems exploit spurious correlations in the data, a tactic humans cannot employ. Furthermore, the paper highlights concerns about low annotator pay rates and a lack of transparency regarding annotator demographics, suggesting that poorly motivated or unrepresentative annotators undermine the reliability of human baselines.

The authors contend that current "superhuman" claims are not scientifically meaningful without addressing these issues. They advocate for more rigorous benchmark construction, including using full test sets, correcting ground truth, comparing against the best possible human performance, and ensuring adequate annotator compensation and diversity. The paper ultimately calls for a re-evaluation of how we measure and interpret NLU progress.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. After nearly two decades of use, concerns arose regarding their performance on modern data and the factors contributing to generalization. To address this, the authors created CoNLL++, a new dataset of Reuters News articles annotated using the CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++, measuring F1 score changes to assess generalization.

The study found that transformer-based architectures, larger model sizes, and increased fine-tuning examples are crucial for good generalization. Contrary to expectations, adaptive overfitting (due to repeated use of the CoNLL-2003 test set) was not observed. Instead, temporal drift—the performance degradation caused by the increasing time gap between training and testing data—was identified as the primary cause of performance drops. Despite this drift, the paper concludes that CoNLL-2003 taggers still perform well in 2023, highlighting the need for further research into improving model generalization in NER tasks.</sample>
    <sample id="352">Annotating behaviors in chat.</sample>
    <sample id="353">This paper introduces a novel approach to Python code generation that addresses the challenge of "input underspecification"—when natural language descriptions (NLDs) lack crucial details needed to generate correct code. The authors propose an interactive framework where the model asks clarification questions (CQs) to gather missing specifications.

Their key contribution is the CodeClarQA dataset, synthetically created with clarification questions focused on key operations within code. The process involves identifying missing operations by comparing NLDs to operation documentation using schema analysis and similarity scores. They then generate either yes/no or multiple-choice CQs for these missing operations.

The proposed pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments demonstrate that asking clarification questions increases the difficulty of the task but also improves code generation performance when more high-ranked questions are answered. While the interactive pipeline currently underperforms a model-only approach, analysis suggests that clarified key operations are indeed a significant factor in generating better code, though the task remains challenging, as evidenced by instances of confusion matrices missing classes.</sample>
    <sample id="354">2020</sample>
    <sample id="356">Alexander Koller and Ivan Titov</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">State-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh's research, "CounterComp," addresses the challenge of compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. Current neural models struggle with tasks requiring more than two arithmetic operations, often relying on spurious patterns rather than true reasoning.

CounterComp tackles this by leveraging counterfactual scenarios. The approach identifies "positive" examples where question modifications don't change the answer and "negative" examples where they do. These triplets are then used to create an auxiliary metric learning loss during training, dynamically adjusting based on the degree of question intervention.

The research demonstrates that incorporating this CounterComp loss consistently improves performance across three state-of-the-art baselines, particularly as the number of reasoning steps increases. Crucially, it enhances performance on both in-distribution and, more significantly, out-of-distribution samples, showcasing improved compositional generalization. Qualitative analysis reveals that CounterComp encourages the model to focus on more relevant tokens within the input, aligning with the necessary operations for accurate output. The work aims to reduce the need for costly human supervision while boosting model robustness and reasoning capabilities.</sample>
  </task>
</testset>