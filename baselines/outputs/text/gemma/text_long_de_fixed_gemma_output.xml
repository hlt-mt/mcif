<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Large-scale web crawl data, including news media like The New York Times, Los Angeles Times, and The Guardian.</sample>
    <sample id="1">McGill University.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model addressing reading order challenges in Visually-rich Document Understanding (VrDU). Unlike existing methods that rely on global 1D positions, LayoutMask utilizes "local 1D position" representing in-segment token order, forcing the model to infer global reading order by integrating 1D, 2D positions, and semantic information. To enhance text-layout interactions, LayoutMask incorporates two novel masking strategies within Masked Language Modeling: Whole Word Masking and Layout-Aware Masking. Additionally, a new Masked Position Modeling objective is introduced, requiring the model to predict masked 2D positions. Experiments demonstrate that LayoutMask's local 1D position approach outperforms global 1D position on FUNSD and SROIE datasets, particularly in complex layouts with misleading numbers. These findings highlight the benefits of adaptive reading order inference for improved VrDU performance.</sample>
    <sample id="3">Hallo, ich bin Omar und werde nun über die Anwendungsfälle für unseren Datensatz DEPLAIN sprechen. Der erste Anwendungsfall ist die Evaluierung automatischer Ausrichtungsverfahren. In den letzten Jahren gab es viele Ausrichtungsverfahren, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die die gleiche Sprache und den gleichen Inhalt haben, aber auf einem anderen Komplexitätsniveau stehen. Und jetzt, da wir unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsverfahren zu evaluieren. Wir haben einige Anpassungen an die vorgeschlagenen Verfahren vorgenommen, und wir haben alle Anpassungen und den Code zur Durchführung unserer Experimente in dem Paper veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungs-Methode für die deutsche Textvereinfachung die Methode von MASSalign ist. Sie finden auch den Code, um diese Methode auf Ihren eigenen Dokumenten auszuführen, in dem Paper. Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist der der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um vereinfachungen auf Dokumentenebene zu erzeugen, und wir haben auch das normale base mBART feinabgestimmt, um vereinfachungen auf Satzebene zu erzeugen. Sie finden auch alle Checkpoints und können sich in dem Paper die Details der Scores und Evaluationsmetriken unserer Experimente ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung Ergebnisse erzielen kann, die besser sind als die Baseline-Scores, und wir schlugen diese Ergebnisse als Basis-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">This paper introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," proposing a novel "many-to-many summarization" framework that combines multilingual and cross-lingual summarization into a single model capable of summarizing documents from any source language into any target language. Preliminary analyses demonstrate that this approach facilitates better knowledge transfer across languages compared to traditional methods. The authors present PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. Experiments on the WikiLingua dataset, using mBART-50 as a backbone, show that PISCES outperforms baselines like mBART-50 and mT5. Ablation and human studies further validate the effectiveness of each training stage and the overall superiority of PISCES in cross-lingual summarization tasks.</sample>
    <sample id="7">Ja.</sample>
    <sample id="8">Die Methode annotiert explizit, ob jede Antwort des Modells bestimmte Verhaltensweisen aufweist, anstatt einfach nach Dimensionen der Dialogqualität zu fragen.</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">There's a lot of room for improvement, as accuracy is only 60% when the language model has access only to entity names.</sample>
    <sample id="11">This research investigates whether large language models (LLMs) genuinely understand humor by leveraging data from *The New Yorker* Caption Contest. While LLMs can generate and sometimes explain jokes, their understanding remains questionable. The study introduces three tasks: caption matching (selecting the correct caption for a cartoon), quality ranking (comparing two captions), and explanation generation (explaining why a caption is funny). A new dataset of over 700 cartoons, annotated with descriptions and explanations, was created to support these tasks. Results show that even state-of-the-art models like CLIP and GPT-4 struggle, achieving significantly lower accuracy than humans on matching and ranking tasks. GPT-4's generated explanations also contain errors and are often preferred less by humans. The dataset and leaderboard are publicly available, aiming to foster further research into LLMs' ability to comprehend and reason about humor.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Adaptive inference techniques like Multi Model and Early Exit aim to reduce the inference cost of large language models by utilizing less complex models for simpler inputs. While Early Exit offers faster inference and memory efficiency, it suffers from "conflicting gradients" where multiple classifiers sharing model parameters hinder each other's training. This work investigates this phenomenon, demonstrating that separate Multi Model classifiers outperform Early Exit classifiers by an average of 2.3%. To address this, the authors introduce SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that isolates gradient updates to each transformer layer, preventing conflicting gradients. SWEET significantly closes the performance gap between Early Exit and Multi Model, and achieves superior speed/accuracy trade-offs, particularly at high inference speeds. The findings highlight the importance of considering gradient interference in adaptive inference and pave the way for future research in fine-tuning strategies tailored for Early Exit architectures.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es unterschiedliche Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So zum Beispiel bei den Universal Dependencies wird die Struktur der Koordination „Lisa, Bart und Maggie“ so angenommen, dass das erste Konjunkt das Haupt der gesamten koordinierten Struktur ist. In diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igors Mel’čuks Meaning-Text-Theorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkt abhängig ist. Diese beiden Ansätze sind asymmetrisch. Sie heben eines der Konjunkte hervor.

Das sind asymmetrische Ansätze für Koordinatstrukturen, wie z. B. der Prager Ansatz. Der konjunktionsgeführte Ansatz, der in den Prager Dependency Treebanks angenommen wird, bei dem koordinierte Strukturen vom Konjunkt abhängig sind. So erhalten wir Abhängigkeiten von Ende zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in Hudsons Word Grammar verwendet wird, wo sie sagen, dass alle Konjunkte Köpfe der koordinierten Struktur sind. So erhalten wir Abhängigkeiten vom Regenten zu allen Konjunkten separat: Lisa, Bart und Maggie.

Nun ist das Ziel dieser Arbeit, ein neuartiges Argument für symmetrische Strukturen der Koordination, wie diese beiden, und gegen asymmetrische Strukturen der Koordination, wie diese beiden, zu erbringen. OK. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erläutern werde.

So bevorzugen im Englischen direkte Objekte, nahe am Verb zu sein, während Adverbien weiter entfernt sein können. „Marge las es gestern“ ist in Ordnung, weil das direkte Objekt nahe am Verb ist, während „Marge las gestern es“ viel schlechter ist. Denn hier befindet sich ein Adverb zwischen dem Verb und dem direkten Objekt: „gestern“. Dieses Phänomen kann jedoch gemildert werden, wenn das direkte Objekt sehr lang und schwer ist. Denn dann kann es an die Position nach dem Adverb verschoben werden. Dies wird hier veranschaulicht. Beide Sätze sind in Ordnung. „Marge las dieses absolut faszinierende Buch über Bienen gestern.“ Es ist in Ordnung, anstatt „es“ dieses lange NP zu verwenden. Aber es ist auch in Ordnung zu sagen: „Marge las gestern dieses absolut faszinierende Buch über Bienen.“ Die Begründung hier ist, dass dies möglich ist, weil der Satz zwar ein allgemeines grammatikalisches Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, er aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die nicht konstant unter diesen beiden Strukturen sind. Hier haben wir also eine Abhängigkeit von „lesen“ zum Adverb der Länge 7 (gemessen in Wörtern) und von „lesen“ zum „Buch“ der Länge 4, also zusammen 11. Wenn man diese beiden Konstituenten vertauscht, wird die Summe dieser beiden Abhängigkeiten 6. Anstatt also 11 ist 6 viel kürzer. Deshalb klingt das ziemlich in Ordnung. Es verletzt ein Prinzip, erfüllt aber ein anderes.

OK. Was wir getan haben, ist, verschiedene Statistiken über Koordinationen aus der erweiterten Version des Penn Treebank zu extrahieren und in der Arbeit „Warum Sie Universal Dependencies nicht verwenden sollten“ zu untersuchen. Diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. So „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Und auch die Beobachtung, dass beim Parsen diese Tendenz mit dem Längenunterschied wächst. Wenn sich der Unterschied in der Länge der beiden Konjunkte vergrößert, bevorzugt der kürzere Konjunkt, der erste zu sein, stärker. So ist der Anteil des linken, kurzen Konjunkts größer. Was in dieser Arbeit jedoch neu ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn sich der Regens auf der linken Seite befindet oder fehlt. Der Regens befindet sich also in diesem Beispiel „Ich sah Bart und Lisa“ auf der linken Seite. Es fehlt im zweiten Beispiel „Homer kam und niesste“. Hier haben wir eine Koordination von zwei Verben und es gibt keine äußere, externe Steuerung. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wenn sich der Regens jedoch auf der rechten Seite befindet, wie hier „lachte“ regiert die Koordination Ted und Ned, verschwindet dieser Effekt.

Wir haben dies gemessen, indem wir die Länge in Zeichen (erste Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte) gemessen haben. Ich werde mich hier auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn sich der Regens auf der linken Seite befindet, die Tendenz, dass der linke Konjunkt kürzer ist, mit dem absoluten Unterschied in Wörtern stetig wächst, und dasselbe gilt, wenn kein Regens vorhanden ist, wie bei der Koordination von Sätzen. Aber wenn sich der Regens auf der rechten Seite befindet, verschwindet diese Tendenz. Wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden, liefert. Sehen Sie sich die Arbeit für die vollständigen Argumente an. Und sprechen Sie uns bei der Poster-Session an. Vielen Dank.</sample>
    <sample id="15">Drei.</sample>
    <sample id="16">Bible texts.</sample>
    <sample id="17">This paper introduces a novel approach to multimodal relation extraction (MRE) that addresses the challenges of internal-information over-utilization and external-information under-exploitation. The proposed method employs a Graph Information Bottleneck principle-guided feature refinement and incorporates multimodal topic information to enrich context. The framework constructs cross-modal graphs (CMGs) from text and images, then refines these graphs by filtering nodes and edges to remove irrelevant information. Multimodal topic features are subsequently integrated using an attention mechanism. Experiments on a standard MRE dataset demonstrate significant performance improvements over existing methods, with ablation studies confirming the benefits of both information screening and topic enrichment. Further analysis reveals that internal-information screening is more effective when text and vision are highly relevant, while external-information exploitation is more beneficial when relevance is low. The work highlights the importance of simultaneously subtracting and adding information for effective multimodal relation extraction.</sample>
    <sample id="18">"Salt and pepper"</sample>
    <sample id="19">This paper surveys efficient open-domain question answering (ODQA) systems, addressing the challenges posed by large Wikipedia corpora and computationally intensive models. The dominant two-stage retrieval-reader framework, while effective, faces bottlenecks due to index size and inference speed. This survey explores alternative one-stage approaches like retrieval-only and generator-only systems, analyzing their trade-offs. Key techniques for efficiency include approximate nearest neighbor search for faster retrieval, adaptive computation for quicker reading, index size reduction through filtering and compression, and model size reduction via lightweight models and parameter sharing. The analysis reveals that retrieval-reader systems offer a balanced approach, retrieval-only systems prioritize speed, and generator-only systems struggle with performance and size. The paper concludes with insights for resource-constrained scenarios, suggesting index compression or model reduction, and highlights future research directions including deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">Nachrichtenartikel.</sample>
    <sample id="22">A better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">This paper investigates the challenges text-to-image models face in accurately rendering text, specifically focusing on the Imagen model. While these models excel at generating complex images, they often struggle with representing text due to the subword tokenization used by their text encoders (like T5). The research reveals that even large T5 models exhibit surprisingly low spelling accuracy, particularly with frequent words, because SentencePiece tokenization represents common words with fewer, larger subword units. In contrast, models like ByT5, which operate at the byte level, demonstrate near-perfect spelling accuracy. To address this, the authors propose a simple yet effective solution: augmenting the Imagen model with a character-aware representation from a small ByT5 model. This approach, adding only a minimal number of parameters, significantly improves text rendering capabilities without sacrificing overall image generation quality. The paper introduces the WikiSpell and DrawText benchmarks to evaluate text rendering and highlights a practical strategy for enhancing model spelling ability.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">Durch Messung der Länge in Zeichen, Silben und Wörtern wurde untersucht, ob die Tendenz, dass das linke Konjunkt kürzer ist, mit der Position des Regenten (links oder rechts) variiert.</sample>
    <sample id="26">The classifier performs not much better than chance.</sample>
    <sample id="27">Der Text erwähnt Shangbin als PhD-Student, der die Arbeit präsentiert, aber gibt keine Informationen über die Anzahl der Autoren.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a simple yet effective ensemble learning framework for large language models (LLMs). Recognizing that no single LLM consistently excels across all inputs, LLM-Blender leverages pairwise ranking and generative fusion to dynamically select and combine outputs from multiple models. The framework consists of two stages: a PairRanker module and a GenFuser module. PairRanker uses a cross-attention mechanism (e.g., RoBERTa) to compare pairs of LLM outputs given an input, generating a ranking matrix. This ranking is then aggregated to determine the optimal order of candidates. Subsequently, the GenFuser module takes the top K ranked candidates and fuses them into a final output using a sequence-to-sequence model. Experiments on the newly created MixInstruct dataset, evaluated using automatic metrics and ChatGPT, demonstrate that LLM-Blender consistently outperforms individual top-performing models like Open Assistant and Vicuna. The codebase and dataset are publicly released to facilitate further research in LLM ensemble learning.</sample>
    <sample id="31">Die Autoren gehören verschiedenen Universitäten an, darunter MIT, Stanford und CMU.</sample>
    <sample id="33">Es verwendet einen Pearson's R Korrelationsscore, um die Annotationen von Endbenutzern nach Demografie mit den Vorhersagen von Datensätzen und Modellen zu vergleichen.</sample>
    <sample id="34">CREST is a novel framework combining rationalization and counterfactual text generation to improve model interpretability and performance. It generates counterfactuals by masking rationales derived from an input, then uses a masked language model to fill in the gaps. Human evaluations demonstrate CREST produces more valid and natural counterfactuals than existing methods like MiCE. 

Beyond counterfactual generation, CREST introduces CREST-Rationalization, a training approach that leverages both original and generated counterfactual examples. A shared rationalizer highlights meaningful rationales, which are then fed into a predictor. A regularization term encourages consistency between original and counterfactual rationales. Experiments on IMDB and SNLI show CREST-Rationalization achieves state-of-the-art results, particularly on out-of-domain datasets. 

Furthermore, CREST-Rationalization generates more plausible and simulable rationales compared to other approaches, indicating improved interpretability and the ability to guide decision changes through targeted input modifications. The framework offers a controllable and effective way to enhance both model robustness and explanation quality.</sample>
    <sample id="36">This work introduces Language-Specific Layers (LSLs) to enhance multilingual machine translation (MMT) while maintaining constant inference costs. LSLs increase language-specific capacity within a shared MMT model by incorporating a dedicated transformer layer per language, activated based on the source or target language during inference. The paper addresses the challenge of optimal LSL placement within the encoder by training a large model and analyzing weight distributions to identify layers where language-specific information is most crucial. A simple selection strategy based on the largest weight determines the final architecture. Experiments on WMT21 news translation across 10 languages (including Swahili) demonstrate significant improvements over baseline transformers and language adapters, as measured by chrF, spBLEU, and COMET. Notably, LSLs yield substantial gains for low-resource languages, with statistically significant improvements observed in 84 out of 90 translation directions. The approach maintains fast inference speeds, making it a practical solution for improving MMT performance.</sample>
    <sample id="37">Die Studie ergab, dass die menschlichen Teilnehmenden ebenfalls rassistische Stereotypen aufdeckten.</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">Einer.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives introduces a novel, large-scale knowledge graph designed to enhance narrative understanding in NLP systems. PeaCoK comprises approximately 3,800 personas and 40,000 attributes, representing 100,000 personal inferences and rich interconnections between personas. Built through a three-step process involving persona selection, attribute induction, and crowdsourced relation annotation (leveraging AI assistance), PeaCoK facilitates persona-grounded commonsense reasoning. Experiments demonstrate that training a BART-based model on PeaCoK yields comparable knowledge generation capabilities to larger language models like GPT-3. Furthermore, integrating PeaCoK into a dialogue generation system (P²Bot) significantly improves dialogue quality—fluency, consistency, engagement, and persona expression—compared to baselines and even general commonsense knowledge graphs like Atomic2020. The study reveals that increased shared knowledge between speakers correlates with more consistent and engaging dialogues, underscoring the value of PeaCoK's interconnected persona knowledge for narrative modeling. The paper and associated resources are publicly available.</sample>
    <sample id="42">Die Arbeit wurde von Shuheng und seinem Team verfasst.</sample>
    <sample id="43">Der Text nennt Vasudha als Computer Science PhD-Kandidatin, aber die genaue Anzahl der Autoren wird nicht angegeben.</sample>
    <sample id="44">Das Framework unterscheidet sich von früheren Arbeiten, indem es Endnutzer mit Datensätzen und Modellen vergleicht, anstatt nur die Übereinstimmung zwischen Annotatoren zu untersuchen.</sample>
    <sample id="45">Die generierten Personas.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit mit dem Titel "Von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Verfolgung der Spur von politischen Voreingenommenheiten, die zu unfairen NLP-Modellen führen". Sprachmodelle werden auf großen Web-Crawling-Datensätzen trainiert. Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, der Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle abgedeckt sind. Dies hat einen zwiespältigen Nutzen für Sprachmodellanwendungen geschaffen. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können möglicherweise zu Fairnessproblemen bei Downstream-Aufgabenanwendungen führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben zu untersuchen, indem wir insbesondere die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen Pretraining-Daten bei solchen politischen Voreingenommenheiten? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen tatsächlich bei Downstream-Aufgaben und führt dies möglicherweise zu Fairnessproblemen in NLP-Anwendungen? Konkret schlagen wir zunächst vor, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung politischer Fragebögen wie dem Political Conference Test zu versehen. Dies stellt sicher, dass wir eine automatische Bewertung durchführen können, die gut in der politischen Wissenschaftsliteratur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie nehmen alle vier Quadranten auf der politischen Kampagne ein. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell unter allen ist und GPT-Serien im Allgemeinen sozial liberaler sind als BART-Serien und ihre Varianten. Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedene parteiische Korpora trainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter in ihre politische Ausrichtung unterteilt sind. Durch das weitere Trainieren von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschieben. Zum Beispiel können wir bei RoBERTa, das weiter auf dem linken Reddit-Korpus trainiert wurde, eine deutliche liberale Verschiebung in Bezug auf seine politischen Voreingenommenheiten feststellen. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung erfassen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen Pretraining-Korpora in vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle nach 2017 eine politische Ausrichtung hatten, die weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erfassen können. Zuletzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei der Erkennung von Hassreden und der Erkennung von Falschmeldungen, um NLP-Anwendungen zu bewerten, die häufig Sprachmodelle verwenden und erhebliche Auswirkungen haben könnten. Wir stellen fest, dass, wenn wir die Kategorie-spezifische Leistung untersuchen, d. h. wenn wir die Leistung in verschiedene demografische Gruppen oder politische Ausrichtungen von Nachrichtenmedien aufteilen, wir ein Muster erkennen. Zum Beispiel sind bei der Erkennung von Hassreden sprachmodelle mit linker Ausrichtung besser darin, Hassreden zu erkennen, die auf sozial marginalisierte Gruppen abzielen, aber schlechter darin, Hassreden zu erkennen, die auf mächtigere Gruppen in unserer Gesellschaft abzielen. Umgekehrt sind sprachmodelle mit rechter Ausrichtung besser darin, Hassreden zu erkennen, die sich gegen Weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richten. Ähnliche Trends treten bei der Erkennung von Falschmeldungen auf, wo wir sehen, dass sprachmodelle mit linker Ausrichtung besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um zu sehen, dass sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen zu Hassreden und Falschmeldungen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Reihe weiterer Beispiele im Anhang, um hervorzuheben, dass dies ein dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigt. Zum Beispiel würde bedeuten, wenn sprachmodelle mit rechter Ausrichtung auf Hassreden oder Falschmeldungen oder was auch immer feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würden, dass Menschen mit gegenteiligen politischen Meinungen marginalisiert würden und Hassreden, die auf Minderheitengruppen abzielen, ohne Kontrolle grassieren würden. Dies hat Alarm geschlagen, um die Fairnessprobleme anzuerkennen und anzugehen, die sich aus den politischen Voreingenommenheiten von Sprachmodellen ergeben. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten für Sprachmodelle nicht bereinigen, wird die Voreingenommenheit von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben propagiert, was letztendlich zu Fairnessproblemen führt. Wenn wir jedoch versuchen, sie irgendwie zu bereinigen, riskieren wir auch Zensur oder Ausgrenzung. Es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Sprachüberwachungsdaten beibehalten werden sollte. Es ist also wie das elektrische Trolley-Problem. Gut, ich glaube, das ist alles, was ich heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">David Vilar und seine Kollegen von Google Translate.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN is a new corpus for German text simplification, addressing limitations of existing resources. It comprises two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (diverse domains, 750 documents, ~30,450 sentence pairs), both with manually aligned data and automatic alignment methods for DEPLAIN-web. Analysis reveals varying simplification strengths across domains and a wide range of simplification transformations.

The corpus enables evaluation of automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Furthermore, DEPLAIN facilitates automatic text simplification through fine-tuning language models. Long-mBART was fine-tuned for document-level simplification, while base mBART handled sentence-level simplification. Fine-tuning yielded improved results compared to baselines, establishing a benchmark for future research in automatic German text simplification. The corpus, code, and checkpoints are publicly available.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper presents a novel approach to detecting cognitive dissonance in language, a rare phenomenon with implications for understanding disagreement, mental health, and decision-making. Addressing the challenge of absolute rarity, the authors created a large-scale annotated dataset of dissonance relations found in tweets (only 3.5% of pairs). Initial classifiers performed poorly due to the limited training data. To overcome this, they employed transfer learning from related tasks—dissonance stance classification ("debate") and discourse relation classification ("CE")—demonstrating improved zero-shot performance. Subsequently, they utilized active learning (AL) with a Probability-of-Rare-Class (PRC) strategy to efficiently select dissonant examples for annotation. PRC outperformed other state-of-the-art AL strategies, achieving an AUC of 0.75. The study also found that cumulative model updates were beneficial for in-domain annotation, while iterative updates were useful for transfer learning. The research highlights the effectiveness of combining transfer learning and PRC-based active learning for rare class acquisition and cold-starting AL in cognitive dissonance detection.</sample>
    <sample id="55">Ja.</sample>
    <sample id="56">Yusen Zhang</sample>
    <sample id="57">Ohne aufgabenspezifisches Training funktionieren die Modelle nicht gut.</sample>
    <sample id="58">Background-Pretrain, Background-Both, Background-Inference.</sample>
    <sample id="59">DrBERT is a novel French biomedical language model based on RoBERTa and trained on NACHOS, a large dataset of medical web crawls. This work addresses the scarcity of French biomedical NLP resources, offering the first open-source model for this domain. The study investigates optimal pre-training strategies and data sources, comparing DrBERT with a clinical model (ChuBERT) trained on anonymized hospital data and several models utilizing continual pre-training from CamemBERT and PubMedBERT. Seven models were evaluated across 11 French biomedical and clinical downstream tasks, benchmarked against established models like CamemBERT, PubMedBERT, BioBERT, and ClinicalBERT. Results demonstrate that models perform best on tasks aligned with their training data, with heterogeneous data proving more versatile. From-scratch pre-training generally yielded superior performance, though continual pre-training using CamemBERT's weights achieved comparable results with a smaller dataset. DrBERT consistently outperformed generic models like CamemBERT on nine tasks. The pre-trained DrBERT models and training scripts are publicly available under the MIT license, facilitating further research and development in French biomedical NLP.</sample>
    <sample id="60">Die Autoren gehören keiner bestimmten Universität an.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG) aimed at compressing large language models while preserving performance. Addressing the growing need for efficient NLG systems, the research explores various techniques to transfer knowledge from a large teacher model to a smaller student model. Unlike prior work focusing on specific tasks or pre-training, this study investigates task-specific knowledge distillation across summarization, question generation, common sense reasoning, simplification, and style transfer, using realistic, industry-driven setups with limited labeled data and abundant unlabeled data.

The study examines architectural choices, pruning strategies, and knowledge selection methods. A key contribution is the exploration of pseudo-target training, challenging the conventional single-mode approach. The authors demonstrate the importance of unlabeled data, the benefits of generating multiple diverse pseudo-targets via sampling, and introduce a novel "joint-teaching" technique that combines word-level distillation with student-generated pseudo-targets to mitigate bias and improve learning. The research provides a practical recipe for knowledge distillation in NLG, balancing compression rate and inference efficiency.</sample>
    <sample id="63">It measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine niedrigere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="66">This survey explores the burgeoning field of deep learning for mathematical reasoning, a crucial aspect of human intelligence. It examines the task's evolution, encompassing text-based problems, multimodal data (images, tables), and automated theorem proving. The paper categorizes approaches into neuro-symbolic reasoning (e.g., geometric problem solving) and sequence generation models, including sequence-to-sequence and sequence-to-tree architectures. Recent advancements leveraging large language models (LLMs) and techniques like chain-of-thought prompting are discussed, alongside their limitations in precise mathematical reasoning. Solutions like self-consistency decoding and program-aided LLMs (e.g., Chameleon) are highlighted. The survey also addresses the need for more research in low-resource settings and specialized domains (finance, science, medicine), while acknowledging ongoing challenges related to generalization, robustness, and handling large numbers in mathematical reasoning tasks.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a common challenge where training on one language pair impacts others. Contrary to many existing approaches, the study finds that severe interference primarily occurs in small models relative to the data size. Language similarity and the total number of languages have a surprisingly minimal impact on interference levels. The key to mitigating interference lies in tuning the sampling temperature; higher temperatures (e.g., 5) allow for better sampling from low-resource languages. Experiments across various model sizes, data scales, and temperatures demonstrate that even modest scaling and temperature calibration can significantly reduce interference without requiring specialized algorithms. The research highlights the importance of model and data size, and emphasizes that temperature tuning is crucial for achieving strong performance in multilingual translation.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen linguistischen Kontext aus verschiedenen Datensätzen, einschließlich Wikipedia.</sample>
    <sample id="69">Typically, 20 samples per class are needed.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">The "Resolving Indirect Referring Expressions for Entity Selection" work introduces the AltEntities Corpus, a novel dataset designed to understand user language when making choices in conversational systems and benchmark large language model (LLM) entity understanding. The corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions. Data collection utilizes a cartoon completion setup where annotators provide indirect references to select between two entities presented in an alternative question. Entity pairs are generated using increasing levels of similarity based on titles, descriptions, and attributes. Annotators are provided with background knowledge (Google search links for music, Wikipedia text/images for books/recipes) before generating indirect references. Experiments with a T5 XL model demonstrate high accuracy (92-95%) when models have access to the same background knowledge as annotators, decreasing to 60% with only entity names. The results highlight the importance of background knowledge for accurate entity selection and show domain-generalizability. The dataset is publicly available.</sample>
    <sample id="72">Because existing methods are insufficient to address the fairness issues resulting from language model political biases.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Dense-ATOMIC is introduced as a densely-connected commonsense knowledge graph built upon ATOMIC to address its limitations in knowledge coverage and multi-hop paths. ATOMIC, while high-quality, suffers from sparse graph structure and insufficient links (B-to-A, B-to-B, A-to-B, A-to-A). Dense-ATOMIC completes these missing links, enabling multi-hop reasoning, exemplified by paths like "X asks Y to marry" -&gt; "Y says yes" -&gt; "X smiles." The construction process involves normalizing tail events and training a relation prediction model (Rel-CSKGC) leveraging RoBERTa for semantic encoding. Rel-CSKGC overcomes sparsity issues and utilizes event semantics effectively. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links. Experiments demonstrate that Dense-ATOMIC significantly improves knowledge coverage and benefits commonsense reasoning models like COMET, generating more diverse outputs. Evaluations on multi-hop paths reveal high aggregate path quality, showcasing Dense-ATOMIC's potential for advanced reasoning capabilities.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE). Addressing the limitations of existing approaches that overlook the inherent connections between NER and RE, Jointprop models these tasks by propagating labels across a heterogeneous graph. The framework leverages span feature generation, constructs a k-Nearest Neighbor graph to capture inter- and intra-connections among labeled and unlabeled data, and performs joint label propagation to refine pseudo-labels iteratively. Finally, high-confidence pseudo-labels are integrated with labeled data to retrain a classification model. Experiments on four datasets, including both joint and single-task settings, demonstrate that Jointprop consistently outperforms baseline models, highlighting the benefits of joint learning and the effectiveness of leveraging inter-task dependencies for improved NER and RE performance, particularly in semi-supervised scenarios where labeled data is scarce.</sample>
    <sample id="76">Vom Vortrainingsdatensatz zu Sprachmodellen zu nachgelagerten Aufgaben.</sample>
    <sample id="77">This work introduces DeFacto, a new dataset from Yale University and Microsoft Research designed to improve factual consistency in abstractive text summarization. DeFacto contains human demonstrations and feedback on existing summarization models' outputs, collected using the XSum dataset and initial Pegasus model summaries. The dataset reveals that 70% of initial summaries contain factual errors. Annotators provided labels for factual consistency, human-corrected summaries, and detailed feedback including instructions, explanations, and supporting evidence.

The research proposes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction, establishing baseline models for each. Summary editing showed promise with fine-tuned and zero-shot LLMs. Feedback generation proved challenging. The automatic error correction model achieved competitive performance with less training data, and explanation generation improved results. DeFacto's fine-grained annotations are valuable for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub.</sample>
    <sample id="78">Yes, the simplification process differs. DEPLAIN-apa has more reorderings and word additions, while DEPLAIN-web has more rephrasings.</sample>
    <sample id="79">Ja.</sample>
    <sample id="80">Das Wasserzeichen wird durch eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings erzeugt, wobei das Gewicht des Ziel-Embeddings proportional zur Anzahl der Triggerwörter im Satz ist.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This paper introduces ULRA (Unsupervised AES by Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to obtain. ULRA addresses this by leveraging multiple heuristic quality signals (e.g., unique terms, word count) as pseudo-ground truth. The framework consists of a Heuristic Essay Ranking module (HER) that generates partial order pairs based on these signals and a Deep Pairwise Rank Aggregation module (DPRA) that trains a neural AES model using these pairs. DPRA employs a novel loss function with learnable confidence weights to handle inconsistencies among the signals. A scoring strategy then maps the model's predictions to a predefined score range. Experiments in both transductive and inductive settings demonstrate that ULRA outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches, although it still lags behind supervised methods due to the absence of strong supervision.</sample>
    <sample id="83">Ja, Encoder-Decoder- oder Encoder-PTR-Modelle können durch Training in einer Mischung von verschiedenen Sprachen verbessert werden.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks

Traditional networks are static, with fixed parameters, while dynamic networks adapt their architecture or parameters based on input. Although dynamic networks often outperform static ones, fully dynamic networks suffer from excessive parameter usage, limiting their practicality. This paper introduces PAD-Net, a Partially Dynamic Network framework, addressing this issue. PAD-Net partitions parameters into dynamic and static components, utilizing scale factors to control their influence and iterative mode partitioning to identify and convert redundant dynamic parameters into static ones. The core idea is that fully dynamic networks contain partially dynamic subnetworks that maintain representation power. Experiments demonstrate that PAD-Net achieves superior performance compared to both static and fully dynamic networks while significantly reducing parameters and computation. Ablation studies reveal the importance of dynamic ratios and scale factors. Compared to network pruning, PAD-Net maintains static parameters and produces more discriminating outputs. Future work includes extending PAD-Net to other networks, hardware-friendly structures, and incorporating additional parameter modes.</sample>
    <sample id="85">"make a chocolate cake"</sample>
    <sample id="86">They validate the covertness by visualizing embeddings on PCA, showing it's hard to distinguish between backdoor and normal embeddings.</sample>
    <sample id="87">The work uses continual pre-training, leveraging the weights and tokenization of CamemBERT and PubMedBERT to train new models.</sample>
    <sample id="88">Nicht-binäre Personen.</sample>
    <sample id="89">"For example, if we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the traditional reliance on native speakers for NLP data annotation, particularly for low-resource languages. We investigate the feasibility of utilizing language learners as annotators through a proof-of-concept study across English, Korean, and Indonesian, employing four tasks from the GLUE benchmark. Learners were categorized by proficiency (basic, intermediate, advanced) and provided with varying resources during annotation. Our experiments, involving pre-tests, annotation tasks, and post-tests over six days, demonstrate that learner-annotated labels achieve near-native accuracy, especially for simpler tasks. Aggregating learner labels via majority voting yields performance comparable to native speakers. Crucially, language models trained on learner annotations achieve 95% of ground truth performance and occasionally surpass those trained on native speaker data. We also observed improvements in learner language proficiency and vocabulary. This work proposes a novel data construction approach for low-resource languages, broadening NLP research possibilities and overcoming recruitment barriers.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance.</sample>
    <sample id="92">The paper does not mention three specific tree-less baselines. It states that their model outperforms "other treeless models" on the COGS benchmark.</sample>
    <sample id="93">Sie sind seine Berater.</sample>
    <sample id="94">This paper addresses the copyright protection of embedding as a service (EaaS) offered by large language models (LLMs) like GPT and LLAMA, which are vulnerable to model theft via embedding extraction. To combat this, the authors propose "Embedding Marker," a novel backdoor-based watermark method specifically designed for EaaS. Embedding Marker involves injecting a watermark by subtly modifying embeddings based on the frequency of trigger words within user input. The weight of the modification is proportional to the trigger count, ensuring covertness. Copyright verification is achieved by constructing backdoor and benign datasets, requesting embeddings from a potentially stolen service, and analyzing the similarity between the obtained embeddings and a target embedding using cosine similarity, L2 distance, and a Kolmogorov-Smirnov test. Experiments on four datasets (AG News, MIND, SST2, Enron Spam) demonstrate high detection accuracy while maintaining utility for downstream tasks. Visualization confirms the covertness of the watermark, making it difficult to distinguish between watermarked and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University und heute werde ich eure Arbeit „NLPositionality“ vorstellen, die Design-Bias in Datensätzen und Modellen charakterisiert. Diese Arbeit wurde in Zusammenarbeit mit einigen Kollegen an der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Lasst uns also mit einem Beispiel beginnen: Stellt euch vor, ihr arbeitet für eine Zeitung und durchsucht die Kommentare unter eurem Nachrichtenartikel, um toxische Inhalte zu entfernen. Ihr könntet euch dann an eine beliebte API wie die Prospective API zur Toxizitätserkennung wenden. Das funktioniert wirklich gut, wenn ihr Carl Jones seid. Die Prospective API kann toxische Instanzen korrekt erkennen. Aber das ist nicht der Fall für Aditya Sharma. Die Prospective API ist hier nicht so sensibel für beleidigende Begriffe, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Bevölkerungsgruppen sehen.

Solche Design-Bias, wie der gerade gezeigte, können auf die Positionality der NLP-Forscher und Modellentwickler zurückzuführen sein. Positionality ist einfach die Perspektive, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Dieses Konzept wird in den kritischen Studien, insbesondere in feministischen und queeren akademischen Bereichen, breit verwendet. Als Forscher kann Positionality den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen verändern kann, die Forscher treffen.

Und so stellt sich die Frage: Haben Datensätze und Modelle Positionality? Wir wollen nicht behaupten, dass Modelle oder Datensätze demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können somit bestimmte Positionalities gegenüber anderen repräsentieren.

Vorherige Arbeiten haben bereits einige anekdotische Hinweise auf Positionality geliefert, wie z. B. kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionality. Diese Arbeiten betrachten jedoch nicht, wie Endbenutzer mit Datensätzen und Modellen verglichen werden, und die Untersuchung von Modell- und Datensatz-Positionality wird zunehmend wichtig, da NLP-Aufgaben subjektiver und sozial orientierter werden. Es ist eine Herausforderung, zu charakterisieren, wie diese Positionalities verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind.

Um die Positionality von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen echter Benutzer mit bestehenden Datensätzen und Modellen. Wir tun dies über unser Framework „NLPositionality“. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt ist die erneute Annotation von Datensätzen mit diversen Annotatoren. Wir entscheiden uns dafür, die Demografie der ursprünglichen Datensatz-Annotatoren zu übersehen, da in der Regel nur wenige Annotatoren jede Instanz annotieren und da Demografie selten erfasst und geteilt wird. Wir entscheiden uns also für die erneute Annotation, um viele Annotationen pro Instanz und einen reichen Satz an demografischen Daten zu erhalten.

Wir nehmen dann die Annotationen nach Demografie und vergleichen sie mit den Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten. Unser Framework unterscheidet sich also von der Literatur über Annotatoreinigkeit, indem es Endbenutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleicht, anstatt nur die Annotatoreinigkeit zu betrachten oder Annotatorendistributionen zu modellieren.

Unser Framework wird weitgehend durch Lab in the Wild ermöglicht, eine Online-Experimentierplattform für Human-Computer-Interaction-Kollaborateure. In Live in the Wild können wir diverse Freiwillige rekrutieren. Im Vergleich zu Plattformen wie Mechanical Turk, die größtenteils Teilnehmer aus den USA oder Indien haben, kann Lab in the Wild weiterhin qualitativ hochwertige Daten liefern.

Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Bei dieser Aufgabe lesen die Teilnehmer eine Situation aus dem Social Chemistry-Datensatz und schreiben dann, wie sozial akzeptabel diese Situation ist. Um das Engagement in der Studie aufrechtzuerhalten, können sie ihre Antworten mit einer KI und anderen vergleichen. Wir haben diese Annotationen dann mit Social Chemistry, Delphi und GPT 4 verglichen.

Wir replizieren einen sehr ähnlichen Aufbau für die Aufgabe zur Erkennung von Toxizität und Hassreden, bei der sie eine Instanz aus Dynahate lesen und angeben, ob sie der Meinung sind, dass es sich um eine Hassrede handelt. Wir haben diese Annotationen dann mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4 verglichen.

Unsere Studie umfasste am Ende über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.

Nun sind wir besser gerüstet, um die Frage zu beantworten: Mit wem stimmen NLP-Datensätze und -Modelle am meisten überein? Wir stellen fest, dass es Positionality in NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen.

Für die GPT-4-Analyse zur sozialen Akzeptanz stellen wir fest, dass sie am meisten mit konfuzianisch und englischsprachigen Ländern übereinstimmt. Wir stellen auch eine zusätzliche Übereinstimmung mit Menschen fest, die eine Hochschulausbildung haben. So stellen wir für GPT 4 in der Aufgabe zur sozialen Akzeptanz fest, dass es am meisten mit Menschen mit einer Hochschulausbildung oder einem Hochschulabschluss übereinstimmt, und wir stellen das Gleiche für Dynahate fest, wo es am meisten mit Menschen mit einer Hochschulausbildung übereinstimmt.

Wenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen übereinstimmen, werden einige unweigerlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmen als mit männlichen und weiblichen Personen. Wir stellen dies sowohl in der GPT-4-Aufgabe zur sozialen Akzeptanz als auch in der Dynahate-Analyse fest.

Was können wir angesichts der Positionality in NLP tun? Wir haben ein paar Empfehlungen. Die erste ist, einen Überblick über alle relevanten Designentscheidungen während des gesamten Forschungsprozesses zu führen. Die zweite ist, NLP-Forschung mit dem Blickwinkel des Perspektivismus durchzuführen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur darin besteht, Technologien so zu gestalten, dass sie für jeden funktionieren.

Damit schließt sich unsere Präsentation ab. Wenn Sie mehr erfahren möchten, können Sie gerne unseren Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Drei.</sample>
    <sample id="98">The presentation highlights a dilemma: sanitizing training data to remove political opinions risks censorship, while not doing so propagates biases. There's no easy solution to determine what is truly neutral and should be retained.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan University. Ich möchte unsere Arbeit "Destillation von Skriptwissen aus großen Sprachmodellen für eingeschränkte Sprachplanung" vorstellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweisen Anweisungen in Form von zielorientierten Skripten folgen. Frühere Arbeiten haben Sprachmodelle zur Planung für abstrakte Ziele stereotypischer Aktivitäten wie "einen Kuchen backen" genutzt und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Frühere Arbeiten konzentrierten sich jedoch hauptsächlich auf die Planung für die abstrakten Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z. B. "einen Schokoladenkuchen backen", ist bisher wenig erforscht worden. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele der Planung ausübt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit vielschichtigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig sind und den Einschränkungen entsprechen. In dieser Arbeit evaluieren und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da es keinen Datensatz für spezifische Ziele gibt, der unsere Studie unterstützt, müssen wir diese zuerst beschaffen. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielschichtigen Einschränkungen für die datengestützte Erfassung durch Menschen unter Verwendung von InstructGPT. Wir wählen 100 spezifische Ziele aus und evaluieren die von großen Sprachmodellen generierten Skripte. Die Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Modelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Einhaltung der Einschränkungen jedoch nicht garantiert werden kann. Wir befassen uns mit einer detaillierteren Kategorie von Einschränkungen, die in wikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Qualität der Ausgaben von Sprachmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des "Over-Generate-then-Filter"-Ansatzes, um die Generierungsqualität zu verbessern. Wir zeigen zunächst Constraint-Typen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Seed-Abstrakten Zielen. Als Nächstes generiert InstructGPT K Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen den Cosinus-Ähnlichkeitswert als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Zielziel im Zielset den höchsten Wert erzielt. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in Bezug auf die semantische Vollständigkeit als auch in Bezug auf die Einhaltung der Einschränkung erheblich. Da große Sprachmodelle teuer im Einsatz sind, ist es wichtig, die Sprachplanungsfähigkeit kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt in diese Richtung. Frühere Studien haben jedoch keine Planung für spezifische Ziele ermöglicht und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für eingeschränkte Sprachplanung zu erstellen, der als CoScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität des Validierungs- und Testsets sicherzustellen, bitten wir Crowdsourcing-Mitarbeiter, falsche Beispiele zu finden und zu korrigieren. Diese Abbildung zeigt die Constraint-Verteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen aufweist. Mit CoScript können wir kleinere, aber spezialisierte Modelle für eingeschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt ist, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle bessere Ergebnisse erzielen können, wenn sie mit geeigneten Datensätzen trainiert werden. Zusammenfassend lässt sich sagen, dass wir das Problem der eingeschränkten Sprachplanung definieren. Wir evaluieren die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle und entwickeln eine "Over-Generate-then-Filter"-Methode für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen qualitativ hochwertigen Skriptdatensatz, CoScript, für eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserem Paper.</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering that addresses the need for fewer training examples compared to existing methods. It combines unsupervised retrieval (TF-IDF and hyperlink traversal) with a few-shot language model-based reranker. Chains are scored by the likelihood of the question given a constructed prompt incorporating chain documents and an instruction designed to elicit reasoning. Techniques like instruction search and temperature scaling are explored to optimize performance. Experiments using GPT2-XL and T5-XL on HotpotQA demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers, achieving strong downstream QA performance when paired with a reader model. The study highlights the effectiveness of using question likelihood as a scoring function and the crucial role of instructions in leveraging language model reasoning abilities for multi-hop QA.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit der von modernsten Systemen.</sample>
    <sample id="102">First, it should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, it should be covert enough. Finally, the watermark needs to be transferable.</sample>
    <sample id="103">14 verschiedene Sprachen.</sample>
    <sample id="104">Über 16.000</sample>
    <sample id="105">Cosine similarity, L2 similarity, and a KS test p-value.</sample>
    <sample id="106">QUEST is a novel retrieval dataset designed to address the challenge of information seeking with implicit set constraints, reflecting real-world scenarios like identifying unknown species or finding specific books. The dataset comprises over 3,000 entity-seeking queries derived from Wikipedia categories (films, books, plants, and animals), incorporating set operations like intersection and complement. Human annotators paraphrased and validated queries, ensuring fluency and relevance, and marked attributable spans within documents for each query constraint.

QUEST presents a difficult retrieval problem, requiring systems to identify multi-answer sets and locate evidence scattered across documents. Initial evaluations using sparse and dense retrievers, alongside a T5 reranker, reveal significant room for improvement, with particularly low performance on queries involving set intersection and difference. The dataset aims to facilitate research into building more effective systems capable of handling selective information needs and complex queries with implicit set operations.</sample>
    <sample id="107">Encoder-Decoder- und Encoder-PTR-Modelle wurden eingesetzt, wobei Encoder-Decoder die beste Leistung erbrachte.</sample>
    <sample id="108">This paper revisits the Minimal Pair Paradigm (MPP) for evaluating language model acceptability judgments, addressing limitations in current pipelines that don't account for longer contexts. The authors demonstrate that language models' acceptability assessments are surprisingly sensitive to context, particularly when sentences share underlying syntactic and semantic features.

To investigate this, they extend the MPP by constructing longer sequences using prefixes from the same or different datasets, creating "match" and "mismatch" scenarios. Experiments reveal that models exhibit significant shifts in acceptability judgments when presented with prefixes from the same grammatical phenomenon (match), while judgments remain relatively stable with irrelevant Wikipedia text (mismatch). Perturbation analysis suggests models are sensitive to latent syntactic and semantic features, indicating that current short-sentence MPP evaluations may not fully reflect a model's broader linguistic knowledge. The findings highlight the need for more context-aware evaluation methods for assessing language model understanding.</sample>
    <sample id="109">Unnatural Instructions introduces a novel approach to instruction tuning by generating a large, diverse dataset of instructions, inputs, and outputs entirely without human annotation. Leveraging a GPT-3 variant, the method automatically generates instructions and corresponding outputs based on a small seed of examples from Super-Natural Instructions. The process includes generating paraphrases to further expand the dataset, resulting in 64,000 core examples and 240,000 with paraphrases. Analysis reveals a high degree of creativity and diversity, with over 50% of examples being correct, and even incorrect ones proving valuable. Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions demonstrates superior performance compared to baselines (T0++, Tk-instruct, and Super-Natural Instructions) across multiple benchmarks, showcasing the dataset's utility and cost-effectiveness. This work highlights the potential of language models to autonomously create high-quality training data, surpassing the limitations of human annotation in terms of creativity and efficiency.</sample>
    <sample id="111">Die Autoren nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Häufigkeit der Wörter damit zählen kann.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute werde ich unsere Arbeit „Funktionieren CoNLL-2003-Named-Entity-Tagger immer noch gut im Jahr 2023?“ vorstellen. Lasst uns loslegen. Unsere Arbeit untersuchte das Problem der Verallgemeinerung mithilfe der Named-Entity-Recognition-Aufgabe oder der NER-Aufgabe. Wir stellen fest, dass Modelle in CoNLL-2003 seit fast 20 Jahren zur Entwicklung von NER verwendet werden, was natürlich mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten verallgemeinern? Und was ist für eine gute Verallgemeinerung erforderlich, wenn wir neue Tagger entwickeln? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung feststellen, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anleitungen annotiert haben. Anschließend haben wir über 20 Modelle auf CoNLL-2003 feinabgestimmt. Wir haben sie sowohl auf den CoNLL-03-Testdatensätzen als auch auf dem CoNLL++-Datensatz bewertet. Und zu guter Letzt haben wir den prozentualen Leistungsunterschied berechnet, um die Verallgemeinerung jedes Modells zu bewerten. Was ist also für eine gute Verallgemeinerung erforderlich? Im Laufe der Experimente haben wir festgestellt, dass drei Hauptbestandteile erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle in der Regel besser auf neue Daten verallgemeinern. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu einer besseren Verallgemeinerung führen. Und zu guter Letzt wissen wir alle, dass die Anzahl der Feinabstimmungsexemplare die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsexemplare tatsächlich auch zu einer besseren Verallgemeinerung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptives Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursachte Overfitting ist und sich typischerweise als abnehmende Erträge auf einem neuen Testdatensatz äußert. Die zweite Hypothese ist der zeitliche Drift, der der Leistungsabfall ist, der durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Für Daten-Overfitting haben wir gesehen, dass die rote Best-Fit-Linie auf der rechten Grafik eine Steigung hat, die größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 erzielt haben, zu mehr als einer Einheit der Verbesserung auf CoNLL++ führt, was bedeutet, dass keine abnehmenden Erträge vorliegen. Dies zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird. Was ist aber mit zeitlichem Drift? Für zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder fortzusetzen. Wir haben festgestellt, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt, was unsere Hypothese bestätigt, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Verallgemeinerung eine bessere Modellarchitektur, eine größere Modellgröße und auch mehr Feinabstimmungsexemplare benötigen. Diese gehen Hand in Hand, wir können nicht nur eine Zutat haben, sondern die anderen verwerfen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Wenn wir nun zur Frage zurückkehren, die wir in der Überschrift unseres Papiers gestellt haben: Funktionieren CoNLL-2003-Tagger immer noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Papier mehr Forschung darüber anregt, wie wir die Verallgemeinerung von Modellen verbessern können. Und schließlich schauen Sie bitte unseren Artikel, unseren Datensatz an und kontaktieren Sie mich, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="114">This work from Nanyang Technological University of Singapore introduces "Grouped Head Attention" (GHT), a novel approach to compressing multi-head attention in large language models (LLMs). Addressing the heavy parameter problem of LLMs, GHT employs a divide-and-conquer strategy to group attention heads, promoting similarity within groups and separation between them during training. The method consists of two stages: group-constrained training and a Voting-to-Stay algorithm. Group-constrained training uses unsupervised learning to organize heads into groups, while Voting-to-Stay prunes redundant heads, retaining only one per group. Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 4.4% BLEU, 7% summarization improvement) alongside substantial parameter compression (up to 90%). Furthermore, the resulting "LITE" model achieves a 62% faster inference speed and 80% reduction in FLOPs. The authors suggest task-specific pruning as a future direction, leveraging the Lottery Ticket Hypothesis to further optimize LLMs for real-world applications where only a subset of capabilities are needed.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">Die Beispielqualität ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">This paper introduces SwitchMLM, a novel pretraining technique designed to improve performance on code-switched natural language processing tasks. Code-switching, the mixing of languages within a sentence (e.g., "Laptop, mere, bag, me, rakha, hai"), is common in linguistically diverse communities. Existing multilingual models like mBERT and XLM-R struggle with these tasks. SwitchMLM focuses on switch-points – transitions between languages – by masking only these tokens during pretraining, unlike standard MLM. To address the need for language identification (LID) tags, a surrogate FrequencyMLM method is proposed. Furthermore, the paper introduces architectural modifications, including residual connections from intermediate layers rich in switch-point information and an auxiliary LID-based loss to enhance language encoding. Experiments on sentiment analysis demonstrate superior performance with the combined SwitchMLM/FrequencyMLM and ResBERT approach. Probing experiments, using linear and conditional probing, confirm that the proposed methods increase switch-point information in model representations, validating the design choices and highlighting the effectiveness of the approach.</sample>
    <sample id="119">RoBERTa, GPT-4, GPT series, and BART series.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus der Cross-Attention-Mechanismus zwischen Audioeingabe und Texteausgabe.</sample>
    <sample id="121">Saying the name of the song or its position, for example by saying "Easy on Me" or "the first one".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">MultiInstruct introduces a novel benchmark dataset for multi-modal instruction tuning, addressing the lack of large-scale multi-modal instruction data compared to NLP. The dataset comprises 62 diverse multi-modal tasks across 10 categories, derived from 21 existing datasets and each task is equipped with five expert-written instructions. The research investigates whether instruction tuning multi-modal pre-trained models enhances generalization to unseen tasks. Using OFA as a base model, the study demonstrates significant performance improvements on seen multi-modal tasks through instruction tuning and benefits from transfer learning from natural instruction datasets. A new metric, "sensitivity," is introduced to measure consistency across instruction variations. Results show that utilizing multiple instructions improves performance and reduces sensitivity, and transfer learning enhances both performance and sensitivity. The work highlights the effectiveness of instruction tuning for multi-modal tasks and explores various fine-tuning strategies. A larger dataset with 150 additional vision-language tasks is currently being collected and will be released.</sample>
    <sample id="124">This work introduces "TempReason," a novel benchmark dataset and training paradigm designed to comprehensively evaluate and improve the temporal reasoning capabilities of Large Language Models (LLMs). The study identifies that existing research often overemphasizes a specific type of temporal reasoning (time-to-event). TempReason covers three levels—time-to-time, time-to-event, and event-to-event—with extensive temporal coverage. Experiments reveal biases in models like ChatGPT and FLAN-T5-L, particularly in month prediction and across different time periods. To address these limitations, the authors propose a training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model.  Evaluation on TempReason demonstrates that TempT5 significantly outperforms other models, including ChatGPT and fine-tuned versions of T5, especially in Open Book and Reasoning QA settings. The research highlights the need to address temporal reasoning biases in LLMs and provides a valuable resource for future research in this area.</sample>
    <sample id="125">Yanis Labrak</sample>
    <sample id="126">Ja, die Übersetzung mit der Google Translate API wurde als Baseline (Translate-Test) betrachtet.</sample>
    <sample id="127">Large language models (LLMs) excel at chain-of-thought reasoning, but their size and computational cost limit deployment. This work, "Large Language Models Are Reasoning Teachers," proposes a method to transfer reasoning abilities from large "teacher" LLMs to smaller, more efficient "student" models. The approach involves using the teacher model to generate step-by-step solutions for complex tasks, which are then used as training data to fine-tune the student. A key innovation is "Diverse Reasoning," which generates multiple, slightly different reasoning paths from the teacher using stochastic sampling, leading to improved student performance. Experiments across 12 tasks demonstrate that fine-tuned CoT models, even with as few as 0.3 billion parameters, achieve significant performance gains compared to prompt-based baselines and vanilla fine-tuning. The method is scalable, with performance improving through larger datasets, better teacher models, or larger student models, though trade-offs between development and inference costs must be considered. The research highlights the potential for knowledge distillation to transfer emergent abilities to smaller models.</sample>
    <sample id="128">The KITMUS test is introduced as a diagnostic suite to evaluate knowledge integration in natural language understanding models. These models often require combining knowledge acquired during pre-training with information provided at inference time. KITMUS focuses on coreference resolution, presenting scenarios where pronoun resolution necessitates both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases"). The test features three settings: "Background-Pretrain" (background knowledge in pre-training), "Background-Both" (background and entity-specific knowledge at inference), and "Background-Inference" (only entity-specific knowledge, simulating new concepts not present in pre-training data). Evaluation with human participants and coreference resolution models reveals that models often rely on surface cues rather than true knowledge integration. While task-specific training improves performance, reliably integrating background knowledge provided solely at inference time remains a challenge, even for state-of-the-art models. The KITMUS dataset and code are publicly available.</sample>
    <sample id="129">Eine Frau-Krieger.</sample>
    <sample id="130">Transformer models do not generalize well.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Zwei.</sample>
    <sample id="133">Mehrere Modalitäten.</sample>
    <sample id="135">ABC-Eval is a novel dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of traditional human evaluation methods (Likert scales, pairwise comparisons) by focusing on annotating specific behaviors exhibited by dialogue models. ABC-Eval identifies and measures thematic errors like irrelevance, contradiction, hallucination, and lack of empathy.

The study evaluated four state-of-the-art models using ABC-Eval and compared it to existing methods. Results demonstrate that ABC-Eval labels are more reliable (higher inter-annotator agreement) and predictive of overall conversation quality than Likert ratings. Stepwise regression analysis reveals that ABC-Eval metrics explain a significantly larger proportion of conversation quality and capture unique aspects compared to existing methods. The evaluation highlighted persistent challenges, such as common sense violations (20%), irrelevant information (15%), and contradictions (10%). ABC-Eval offers a higher-resolution evaluation, enabling more precise comparison and tracking of progress in conversational AI.</sample>
    <sample id="136">FERMAT is introduced as a flexible evaluation set designed to provide a more informative alternative to accuracy-based benchmarks for numerical reasoning in language models. Current benchmarks lack granularity in assessing mathematical abilities, particularly for accessible models (around 3 billion parameters). FERMAT, built from CommonCore and Illinois datasets, evaluates models across number understanding (integers, decimals), mathematical operations (complexity), and training dependency. Initial zero-shot evaluations revealed poor performance across all aspects, highlighting the limitations of existing benchmarks. Fine-tuning with 200,000 generated examples improved performance, but analysis showed models often fail even with expressions seen during training, suggesting sensitivity to linguistic variations. Further investigation demonstrated that diversifying training templates with datasets like GSM8K and AQUA significantly boosts performance, emphasizing the importance of both language and mathematical diversity. The study concludes that number encoding/tokenization and linguistic understanding are key areas for improvement in language models' numerical reasoning capabilities.</sample>
    <sample id="137">Tell2Design introduces a novel task: language-guided floor plan generation. Addressing the need for design tools that incorporate user requirements specified in natural language, this work presents a dataset and a sequence-to-sequence model as a baseline. The Tell2Design dataset comprises 5,051 human-annotated and 76,000 artificially generated language instructions paired with corresponding 2D floor plans. The task involves generating floor plan designs from language instructions detailing semantics, geometry, and topology. A transformer-based sequence-to-sequence model, initialized with T5, is proposed to address the challenges of constrained design generation, understanding document-level text, and handling ambiguous instructions. Experiments on the Tell2Design dataset demonstrate that the proposed model significantly outperforms existing text-conditional image generation baselines, achieving high Intersection over Union (IoU) scores. The study also highlights the importance of bridging the language distribution gap between artificial and human instructions for optimal performance. This work aims to establish a foundation for future research in language-guided design generation.</sample>
    <sample id="138">Das zuverlässige Integrieren von Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird.</sample>
    <sample id="139">Ying und Zhiyang.</sample>
    <sample id="140">Yes, crowd-sourced workers were asked to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">Bestehende Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und eine begrenzte Anzahl von Sprachen, da sie oft auf Fachwissen und menschlicher Kuratierung beruhen.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit zum Thema "Auflösung indirekter Bezugsausdrücke für die Entität Auswahl" sprechen, in der wir den AltEntities-Korpus vorstellen. Mein Name ist Javad Hosseini und dies ist eine Gemeinschaftsarbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten Sie diese alternative Frage: "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Benutzer zwischen einem dieser beiden Songs auswählen. Das Offensichtlichste wäre, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Songs "Easy on Me" oder seine Position "die erste" nennt. Aber manchmal ist ein indirekter Bezug angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Benutzer sich den Namen des Songs nicht erinnert. Oder wenn die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind. Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "der neuere Song" oder "der Song, der nicht energiegeladen ist". Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Benchmarking von LLMs' Entitätsverständnis. Wir sind nicht fähig, einen größeren öffentlichen Datensatz für die Aufgabe zu finden, daher erstellen wir einen mit Crowd-Annotationen. Unser Datensatz deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datensatz-Sammlungsmethodik betont Informalität durch einen Cartoon-Abschlussaufbau. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Erinnerst du dich an den Song, den wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Was die alternative Frage ist. Und in der dritten Blase verwendet Bob einen indirekten Bezug, um eine dieser Entitäten auszuwählen, zum Beispiel "der neuere Song". Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Bereich ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer eine einfache Vorlage. Meinten Sie A oder B? Dabei sind A und B Stichproben aus Wikipedia. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir in der Liste höher gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen. Die erste ist uniform zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel das gleiche Genre oder der gleiche Künstler für einen Song. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie die Namen dieser Entitäten, aber sie kennen sie nicht unbedingt. Was wir also tun, ist, dass wir einige Hintergrundkenntnisse über die beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren dann, mindestens einige von jedem Song anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song "Easy on Me". Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann baten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier ist die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Zum Beispiel "der mit der Klaviermusik". Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "der ohne Worte", "nicht der mit dem 12-jährigen Jungen", oder "der fiktive", oder "stammt aus Aserbaidschan" und so weiter. Der AltEntities-Korpus hat 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5 XL-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf die gleichen Hintergrundkenntnisse wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf einige teilweise überlappende Hintergrundkenntnisse hat, liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundkenntnisse abruft. Wenn das Sprachmodell nur Zugriff auf Entitätsnamen hat, beträgt die Genauigkeit nur 60 %, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Wait-k strategy and Local Agreement.</sample>
    <sample id="144">Nantes University Hospital.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This paper investigates the prevalent issue of omission in dialogue summarization, a significant factor hindering the real-world applicability of current large language models. Analysis reveals that approximately 70% of generated summaries exhibit omissions, with critical information randomly distributed throughout dialogues, posing a challenge for models to identify key details. To address this gap, the authors introduce the OLDS dataset, a novel resource providing high-quality omission labels across five domains. The dataset facilitates the development of omission detection models, explored using three baseline frameworks (pairwise classification, sequence labeling, and pointer network). Results demonstrate the task's difficulty, with F1-scores around 50%. Furthermore, the study explores a post-editing refinement method that incorporates detected omissions into the summary generation process, resulting in substantial improvements in summary quality. This work highlights the importance of addressing omission and suggests that omission detection and refinement are promising avenues for enhancing dialogue summarization.</sample>
    <sample id="147">Drei.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und werde kurz das Paper "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachtranslation? Simultane Sprachtranslation, oder SimulST, ist der Prozess der Übersetzung von gesprochener Sprache in Echtzeit in Text in einer anderen Sprache, der die sprachübergreifende Kommunikation ermöglicht. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wodurch zusätzliche Module entstehen, die optimiert werden müssen. Lange und komplizierte Trainingsprozeduren, beispielsweise Trainings mit unterschiedlichen Optimierungszielen. Und das Trainieren und Warten mehrerer Modelle, um unterschiedliche Latenzbereiche zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden usw. Was ist also unsere Lösung? Erstens die Verwendung bereits existierender Offline-ST-Modelle ohne erneutes Training oder die Anpassung spezifischer Architekturen für SimulST. Verwenden Sie nur ein Modell für jeden Latenzbereich und handhaben Sie die Latenz über bestimmte Parameter. Und nutzen Sie das bereits im Modell vorhandene Wissen durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Texteausgabe. Das ist der Cross-Attention-Mechanismus, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist der Vorschlag von EDAtt, oder Encoder-Decoder Attention, und zwar eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben oder nicht ausgegeben wird, basierend darauf, wohin die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. ihre Summe unterhalb einer bestimmten Schwelle Alpha in den letzten Lambda Sprachframes liegt, was bedeutet, dass genügend stabile Informationen empfangen wurden. Zum Beispiel, wenn wir einen Sprachabschnitt erhalten, der "Ich werde über... sprechen" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, dann schauen wir uns die Cross-Attention-Gewichte an, und wir werden sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Sprachframes zeigt, d. h. Lambda Sprachframes. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten, da die Summe der Cross-Attention über einer bestimmten Schwelle Alpha liegt. Wenn wir fortfahren und einen weiteren Sprachabschnitt erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir uns diese Cross-Attention-Gewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, stellen wir die Ergebnisse der simultanen Sprachtranslation in Diagrammen dar, in denen wir auf der einen Seite BLEU haben, das die Übersetzungsqualität misst, und auf der anderen Seite die durchschnittliche Verzögerung, das Latenzmaß. Wir berücksichtigen auch die rechnerisch bewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen also, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind. Aber wir wollen auch, dass sie nach links verschoben sind. Wir vergleichen mit beliebten Strategien, die auch auf Offline-Modelle angewendet werden, nämlich der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit dem hochmodernen Architektur, die speziell für simultane Vorübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachtranslation-Strategie auf Deutsch. Wir sehen, dass sie alle Strategien, die auf Offline-Modelle angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechnerisch bewusste Zeit berücksichtigen, es die schnellste Strategie ist. Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unser Paper. Und wir haben den Code und die Modelle sowie die simultanen Ausgaben Open Source freigegeben, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">MeetingQA introduces a new extractive question answering (QA) dataset based on real-world meeting transcripts, addressing the gap in NLP research that overlooks the significant QA component within meetings. The dataset, built from the AMI corpus, comprises 7.7K questions with corresponding answer spans, exhibiting characteristics like longer, open-ended questions, multi-speaker answers, discontinuous answer spans, and rhetorical questions. MeetingQA presents unique challenges, with 30% unanswerable questions, 40% multi-span answers, and 48% multi-speaker answers, often involving disagreement. Experiments reveal a substantial performance gap between models and human baselines, with short-context models (RoBERTa) slightly outperforming long-context models (Longformer). Multi-span models show comparable or slightly lower performance than single-span models. Data augmentation with silver annotations from MediaSum improves zero-shot performance, and instruction-tuned models (FLAN-T5) demonstrate competitive zero-shot results. Error analysis highlights difficulties in identifying rhetorical questions and speaker attribution, indicating that MeetingQA remains a challenging and valuable resource for advancing QA research in the meeting domain.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung zu MultiInstruct vorstellen, die Multi-Modale Zero-Shot-Lernen durch Instruction Tuning verbessert. Angesichts der Fortschritte bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf parameter- und dateneffiziente Weise wiederzuverwenden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, auf ungesehenen Aufgaben in Zero-Shot-Manier zu arbeiten, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer Vision und multimodale Aufgaben vernachlässigt wurden. Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction Tuning von multimodalen vortrainierten Modellen die Verallgemeinerung auf ungesehene multimodale Aufgaben tatsächlich verbessern kann. Zusätzlich haben wir bei unserer Forschung festgestellt, dass es eine beträchtliche Diskrepanz in der Verfügbarkeit von Instructional Datensätzen zwischen NLP und multimodalen Bereichen gibt. Es gibt mehr als 1600 sprachbasierte Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multimodalen Instruction-Datensatz. Dies motiviert uns zum Aufbau eines multimodalen Instruction-Tuning-Datensatzes. Hier präsentieren wir MultiInstruct, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus 62 vielfältigen multimodalen Aufgaben in 10 breiten Kategorien besteht. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen versehen. Um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein einheitliches multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispiele aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence-to-Sequence-Format. Dabei werden Text, Bilder, Anweisungen und Begrenzungsrahmen im selben Token-Raum dargestellt. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wählen 10.000 Instanzen pro Aufgabe aus. Für das Testen behalten wir die gesamte Gruppe für Common-Sense-Reasoning zurück und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous aus. Wir verwenden alle Instanzen in der Test-Split für jede Aufgabe. Zusätzlich wählen wir zufällig 20 Aufgaben aus der Test-Split von Natural Instructions als ungesehene Aufgabe für NLP aus. Wir verwenden das vortrainierte OFA Large Model als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Instruction-Vorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment melden wir die minimale und maximale Leistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multimodale Klassifikationsaufgabe ist, melden wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, melden wir Rouge-L. Für NLP-Aufgaben melden wir ebenfalls Rouge-L. Wir führen außerdem eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier sind unsere Hauptergebnisse. Wie man sieht, kann Instruction Tuning die Leistung von OFA bei gesehenen multimodalen Aufgaben deutlich verbessern. Auch das Transferlernen von Natural Instruction-Datensätzen kann dem Instruction Tuning zugute kommen. Hier kann man sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität verringert. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung versus 5 Anweisungen. Wie man sieht, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität deutlich reduzieren. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Sensitivität des Modells. Wir können auch sehen, dass das Transferlernen von Natural Instruction-Datensätzen OFA helfen kann, eine viel bessere Sensitivität zu erreichen, im Vergleich zum ursprünglichen OFA-Modell. Wir können auch sehen, dass das Transferlernen von Natural Instruction-Datensätzen OFA helfen kann, auf dem Natural Instruct-Datensatz eine viel bessere Leistung zu erzielen. Insgesamt schlagen wir den ersten großen multimodalen Instruction-Tuning-Datensatz vor, der die Zero-Shot-Fähigkeiten von OFA deutlich verbessert hat, und wir untersuchen verschiedene Transferlerntechniken und zeigen deren Vorteile. Wir haben eine neue Metrik namens Sensitivität entworfen. Nur noch eine Sache, wir sammeln einen noch größeren multimodalen Instruction-Tuning-Datensatz mit rund 150 zusätzlichen Vision-Language-Aufgaben und werden ihn veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">This presentation introduces new language models specifically designed for classical philology, addressing limitations of existing models in Ancient Greek and Latin. The project aimed to improve comparability, push state-of-the-art performance, explore diverse architectures, and enable multilingual processing. Two monolingual models, GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder), were developed for Ancient Greek, alongside multilingual equivalents, PhilBERTa and PhilTa, trained on Greek, Latin, and English data. A novel, high-quality pre-training corpus for Ancient Greek was created by leveraging the Internet Archive and employing a stop-word identification technique to overcome OCR challenges. Benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization demonstrated significant performance gains over existing models for both languages, particularly in lemmatization. Analysis revealed distinct behavior of T5 encoders compared to native encoder-only models. While semantic and world knowledge probing showed strong performance, multilingual models did not significantly outperform monolingual ones. The work provides valuable resources and insights for NLP applications in classical studies.</sample>
    <sample id="153">This work investigates ambiguities in prompts given to text-to-image generative models and proposes frameworks to address them. The research highlights that ambiguous prompts, like "The girl enters the room with flowers," can lead to varied interpretations and hinder faithful image generation. To tackle this, the authors curated a benchmark dataset (modified LAVA corpus) covering different ambiguity types. Their framework employs two disambiguation strategies: generating clarifying questions for users to answer or presenting multiple visual interpretations for users to select. These interactions produce disambiguated prompts.  Faithfulness of generated images is then evaluated using a Visual Question Answering (VQA) model, comparing images generated from original and disambiguated prompts against user intention. Findings demonstrate disparities in ambiguity resolution across different types, a positive impact of the framework on faithful generation, and strong agreement between the automatic VQA-based evaluation and human judgment. The study contributes a benchmark dataset and evaluation framework for improving the reliability and user-intent alignment of text-to-image models.</sample>
    <sample id="154">University of Trento</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Dialogue summarization, a challenging task in text summarization, aims to condense dialogue context into concise summaries. Existing methods rely on pre-computed static graph structures derived from external linguistic tools, which are prone to errors and lack dynamic adaptation. This work introduces SDDS, a novel model that addresses these limitations by fusing static and dynamic graph structures. SDDS employs an utterance encoder, constructs static graphs using four heuristic methods (discourse parsing, key co-occurrence, speaker relationship modeling, and utterance position), and then utilizes a Static-Dynamic Graph module. This module combines static graphs and dynamically learns semantic relationships between utterances via a multi-head attention mechanism. Finally, a pre-trained language model generates the summary, integrating both static and dynamic dialogue structures through a dual cross-attention mechanism. The model effectively captures dialogue structure information, leading to improved summarization performance. Code and data are publicly available.</sample>
    <sample id="158">Dual Cache for Long Document Neural Coreference Resolution introduces a novel caching strategy to address the challenges of coreference resolution in lengthy texts. Traditional methods suffer from quadratic complexity, while existing cache-based approaches using LRU eviction struggle with topic shifts and high cache miss rates, particularly for frequently occurring entities. To mitigate this, we propose Dual Cache, which employs a local cache (LRU) for local entities and a global cache (LFU) for global entities. This dual-cache architecture efficiently manages entities across varying topical contexts. Evaluations on four benchmarks demonstrate that Dual Cache outperforms baselines, even those with unbounded memory, especially in long-form documents like a 30,000-word book. Furthermore, Dual Cache significantly reduces cache misses compared to single-cache methods and achieves the highest performance-to-cost ratio, making it a practical and effective solution for neural coreference resolution in long documents.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserem ACL 2023-Paper begrüßen zu dürfen. Die Urteile von Sprachmodellen über die Akzeptanz sind nicht immer robust gegenüber dem Kontext. Dies ist eine Gemeinschaftsarbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit gehen wir auf das Minimal Pair Paradigm zurück. Das Minimal Pair Paradigm bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paare umfassen können. Im Minimal Pair Paradigm ist die typische Art und Weise, Sprachmodelle zu bewerten, dass man ein akzeptables oder grammatikalisches Satz und dann ein akzeptables oder ungrammatisches Satz präsentiert. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, die Akzeptanz eines Modells für längere Sätze zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern. Daher ist es entscheidend, die Akzeptanz der Modelle im gesamten Kontextfenster zu bewerten, und das ist es, was wir hier versuchen zu tun. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptanz für immer längere Sequenzen zu bewerten. Das ist der Ansatz. Was wir tun, ist, dass wir, um diese längeren Sequenzen zu simulieren, die Datensätze selbst erneut besuchen und Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar der Grammatikalität aus dem BLiMP-Datensatz aus dem Adjunct Island-Fall ausgewählt. Und was wir tun, ist, dass wir, um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur haben, grammatikalische Sätze aus Adjunct Island extrahieren und diese als Präfix sowohl dem akzeptablen Query als auch dem inakzeptablen Query hinzufügen. Wir können das Gleiche tun, indem wir inakzeptable Sätze aus derselben Übereinstimmung auswählen, was auch verwendet werden kann, um die Akzeptanz des Modells zu testen. Und wir können das Gleiche tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze zwar aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem man das Modell bewertet. Und wir können das Gleiche für den Fall der Inakzeptanz tun. Schließlich können wir Sätze aus einem völlig anderen Bereich wie Wikipedia auswählen. Dies zeigt uns, ob die Akzeptanzurteile des Modells tatsächlich durch einen Kontext beeinflusst werden, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder ob er völlig irrelevant für den Satz ist, den wir betrachten. Wie schlägt sich das Modell also? Zuerst betrachten wir die Sätze aus Wikipedia, die völlig irrelevant für das aktuelle Query-Paar sind, und dort stellen wir fest, dass die MPP-Urteile für beliebige Kontextlänge weitgehend robust sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT-2-Modelle maximal auszunutzen. Und wir sehen hier in der orangefarbenen gepunkteten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben BLiMP- oder SyntaxGym-Datensatz aus. Dort sehen wir, dass die MPP-Urteile entweder deutlich steigen oder fallen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, d. h. wenn wir Sätze aus demselben Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das ausgewählte Präfix akzeptabel oder inakzeptabel ist. Dieses und dieses ist sehr groß, dieser Effekt verstärkt sich im Laufe der Kontextlänge und würde wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst das Match-Präfix das Sprachmodellurteil so stark? Wir haben eine Reihe von Analysen durchgeführt, in denen wir versucht haben, den Eingangsatz zu stören, indem wir versuchten, die relevante Struktur beizubehalten, aber dem Eingangsfeld Rauschen hinzufügten. Nachdem wir mehrere dieser Störungen durchgeführt haben, stellen wir fest, dass keine dieser Störungen das Modell dazu bringt, seine MPP-Urteilsausgabe zu ändern. Wir stellen fest, dass die Modelle auf ähnliche Weise auf die gestörten Sätze reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich stören, sehen wir eine ähnliche Erhöhung in allen Störungen, und wenn wir die Sätze im inakzeptablen Bereich stören, sehen wir eine Abnahme der MPP-Urteile auf ähnliche Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über die Sätze hinweg gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satz-Eingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unser Paper für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Ein ungeordnetes Multiset von Token, die im Output erscheinen werden.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">Weak annotations are much cheaper than human annotations.</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive commonsense reasoning. Unlike existing supervised approaches that rely on noisy and subjective human annotations of plausible explanations, LiPoR leverages the inherent mutual exclusivity of explanations. The method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, avoiding the need for explicit plausibility labels. To further prioritize plausible explanations, LiPoR incorporates a regularizer that enforces mutual exclusivity by minimizing the entropy of explanation probabilities. This encourages the model to favor a subset of explanations. Experiments on the AlphaNLI dataset demonstrate that LiPoR significantly outperforms zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement over a strong GPT-3 baseline. The paper proposes a promising direction for abductive reasoning by eliminating the reliance on labeled data and exploiting the structural properties of explanations.</sample>
    <sample id="166">This paper introduces NDCR, a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Addressing the limitations of existing visual language models that struggle with complex descriptions, NDCR draws inspiration from Divide-and-Conquer strategies and Dual-Process Theory. The framework integrates an analogical reasoning "System 1" (Visual-Linguistic Interactor) with a logical reasoning "System 2" (Neural-Symbolic Reasoner). NDCR decomposes complex text into simple propositions using a Proposition Generator, interacts visual and propositional information, and then employs a Neural-Symbolic Reasoner with negation and conjunction operations to derive the final solution. Experimental results demonstrate that NDCR outperforms baselines and ablation studies confirm the effectiveness of each module. The system provides interpretable inference states, showcasing its operational transparency. The authors suggest that neural-symbolic computation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for enhancing compositional reasoning in large language models.</sample>
    <sample id="167">750 documents were aligned manually and with automatic alignment methods.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde aus Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen CoNLL-2003-Anleitungen annotiert.</sample>
    <sample id="169">This paper presents the first systematic study of prompting strategies for machine translation using Google's PaLM, a 540 billion-parameter large language model. The research evaluates PaLM's translation capabilities using state-of-the-art neural MT metrics and human evaluation, comparing its performance to established systems. The study highlights the significant impact of prompting on translation quality, demonstrating differences of up to 40 BLEURT points with varying prompts. A 5-shot prompting strategy, where sentences are marked with their language, proved effective. Crucially, the research finds that example quality outweighs similarity to the source sentence, recommending selection from curated development data over noisy training data. While PaLM achieves fluency comparable to state-of-the-art systems, it exhibits accuracy limitations, particularly with omission errors. Despite this, PaLM's performance approaches that of a commercial translation system like Google Translate, offering promising potential for future advancements in LLM-based machine translation.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" vorstellen. Semantic Parsing ist die Aufgabe, semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda Calculus zu erstellen. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Cross-Lingual Semantic Parsing Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es eine große Abdeckung bestimmter natürlicher Sprachen. Aber Chinesisch fehlt und es gibt eine fehlende Abdeckung bestimmter Bedeutungsrepräsentationen. Die Lambda-Berechnung fehlt oder sie werden nur mit bestimmten neuronalen Modellen bewertet. Zum Beispiel gibt es nur ein einzelnes Modell, um sie zu bewerten. Um dies zu erreichen, schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält 9 Datensätze in verschiedenen Bereichen, 5 Semantic Parsing Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu bewerten, betrachten wir sechs Einstellungen für das Training und die Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell für das Training und die Bewertung. Zum Beispiel trainieren wir das englische Modell mit englischen Abfragen und verwenden während der Inferenz die API, um die deutsche Abfrage ins Englische zu übersetzen, und verwenden dann das trainierte Modell, um die SQL-Ausgabe vorherzusagen. Wir testen auch Monolinguale Modelle. In dieser Einstellung sind die Quellsprache und die Zielsprache gleich, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch Monolinguale Few-shot Einstellungen, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen Multilinguale Modelle, die wir mit allen Sprachen trainieren. Zum Beispiel fügen wir deutsche, englische und chinesische Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten auch Cross-Lingual Zero-shot und Few-shot Transfer. Wir trainieren in einer Quellsprache und übertragen sie in eine andere Sprache. Beim Training trainieren wir es mit englischen Abfragen oder einer Kombination aus englischen und deutschen Few-shot Abfragen, um ein multilinguales Modell zu trainieren, um die SQL-Ausgabe vorherzusagen. Und wir haben auch viele interessante Ergebnisse gefunden. Bezüglich der Analyse von monolingualen Modellen bewerten wir auf zwei Gruppen von Modellen, einschließlich Encoder-PTR, was für Multilingual Pretrained Encoders mit Pointer-basierten Decodern steht, wie z. B. XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder Modelle, die Multilingual Pretrained Encoder-Decoder Modelle sind, wie z. B. mBART und mT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung in allen neun Datensätzen erzielen. Und wir bewerten mT5 und XLM-R + PTR in der Multilingualen Einstellung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PTR verbessert werden können, indem sie in einer Mischung aus verschiedenen Sprachen trainiert werden. Wir haben festgestellt, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen abnimmt und nur in drei Datensätzen eine Steigerung erzielt. Ich denke, dies wird als "Curse of Multilinguality" bekannt. Wir vergleichen auch die Cross-Language Performance Gap. In dieser Abbildung ist die blaue Linie Cross-Lingual Few-shot Transfer. Die orange Linie ist Cross-Lingual Zero-shot Transfer. Die grüne Linie ist die Monolinguale Einstellung. Wir haben festgestellt, dass beim Vergleich der grünen und orangefarbenen Linie der Zero-shot Transfer Performance Gap signifikant ist, und beim Vergleich der blauen und orangefarbenen Linien der Transfer Gap mit der Few-shot Einstellung schnell verkürzt wird. Wir haben auch einige andere interessante Ergebnisse gefunden. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vortrainieren auf natürlicher englischer Sprache kann die Leistung von Few-shot auf Zielsprachen erheblich steigern, und wir haben festgestellt, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für Cross-Lingual Semantic Parsing Aufgaben sind. Zusammenfassend haben wir XSemPLR, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen, erstellt. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Arten von multilinguallen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse. Besuchen Sie unser Papier und unseren Code. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="171">Existing works can be broadly classified into four categories.</sample>
    <sample id="172">Nein, mehrsprachige Sprachmodelle wie Codex und BLOOM sind für Cross-Lingual Semantic Parsing (CLSP) nicht ausreichend.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset for argument quality analysis, distinguished by its scale and unique features. Unlike existing datasets often reliant on crowdsourcing and limited motions, ArgAnalysis35K comprises 35,000 argument-analysis pairs sourced primarily from high-quality debate sources. It addresses limitations in diversity by utilizing 24 themes and capturing numerous motions within each, moving beyond pre-selected topics. A key innovation is the introduction of "analysis," a combined concept of claims and premises, providing deeper context than traditional argument structures. To mitigate annotator bias, the dataset employs instance-based annotator reliability, selectively removing potentially biased judgments. Finally, a relevance model assigns scores to arguments across themes, acknowledging the broad applicability of arguments beyond a single motion. This comprehensive approach results in a more diverse, reliable, and nuanced dataset for advancing argument quality analysis research.</sample>
    <sample id="175">It addresses this by inducing the alignment as part of the training and approximating the NP-hard problem with a GPU-friendly continuous relaxation.</sample>
    <sample id="176">Fairness in downstream NLP models is defined by whether the model's performance varies across different demographics or political leanings of news media.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">This research introduces SymbolicToM, a novel inference-time method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, the ability to reason about others' mental states, is traditionally assessed using false-belief questions. While LLMs like ChatGPT struggle with these tasks, SymbolicToM addresses this by employing explicit graphical representations of characters' beliefs, creating graphs like BBob and BBob,Alice to model different perspectives.

SymbolicToM leverages off-the-shelf Natural Language Inference and Open Information Extraction models to compute these graphs and efficiently answer ToM questions by transforming them into factual queries over the graph. Experiments demonstrate significant performance gains across various LLMs (e.g., 65 accuracy points for GPT-3) compared to supervised baselines. Furthermore, SymbolicToM exhibits strong generalization capabilities on newly designed datasets testing story structure and linguistic diversity, outperforming supervised models and enabling even advanced models like GPT-4 to achieve near-perfect scores. This plug-and-play approach offers interpretable reasoning and avoids overfitting, representing a substantial advancement in LLM ToM capabilities.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints (e.g., "make a chocolate cake" vs. "make a cake"). The authors define constrained language planning as generating scripts faithful to both the overall goal and specific constraints. They first evaluate and improve large language models' constrained planning abilities, finding limitations in faithfulness to constraints despite acceptable semantic completeness. To overcome this, they propose an "over-generate-then-filter" method using InstructGPT, leveraging embeddings and keyword matching to select faithful scripts. Recognizing the cost of large language models, they then introduce CoScript, a dataset of 55,000 constrained language planning examples generated using their method and refined through crowd-sourcing.  Experiments demonstrate that smaller models (T5) fine-tuned on CoScript outperform larger models, highlighting the dataset's potential to enable constrained language planning with more efficient models. CoScript is released as a valuable resource for advancing research in this area.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Wörter "vibrant" und "curvaceous", die zur Beschreibung von Latina-Frauen verwendet werden und auf eine lange Geschichte der Objektivierung und Sexualisierung dieser Gruppe hinweisen.</sample>
    <sample id="183">Die Prompts zur Generierung der Personas wurden von einer Studie inspiriert, in der diese Prompts Menschen gegeben wurden, um rassistische Stereotypen aufzudecken.</sample>
    <sample id="184">CXMI (Contextual Mutual Information) und seine Erweiterung, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is trained on NACHOS, a dataset of medical data crawled from the web, while ChuBERT is based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Zwei.</sample>
    <sample id="188">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="189">To understand users’ language when they want to make a choice.</sample>
    <sample id="190">Durch das Lernen aus den Embeddings.</sample>
    <sample id="191">Drei.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) is a novel optimizer designed to address the challenge of training large language models efficiently. Existing adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from slower convergence. CAME tackles this by drawing inspiration from Non-negative Matrix Factorization (NMF) and addressing the inherent errors in Adafactor's updates.

The core innovation lies in a confidence-guided updating mechanism that adaptively adjusts the optimization step based on the residual between predicted and generated updates. This approach mitigates the instability caused by erroneous updates, leading to faster convergence. Experiments on BERT, GPT-2, and T5 demonstrate that CAME significantly outperforms Adam and Adafactor, achieving higher validation accuracy with reduced memory usage, particularly at large batch sizes (8K-32K). Furthermore, CAME exhibits comparable downstream task performance with BERT-based models while maintaining a smaller memory footprint compared to other optimizers, showcasing its effectiveness and efficiency for large language model training.</sample>
    <sample id="193">Around 1,000 examples of discourse unit pairs were collected.</sample>
    <sample id="194">Carnegie Mellon University und University of Washington.</sample>
    <sample id="195">We introduce RoHT, a novel framework for Explainable Question Answering (XQA) called "Reasoning over Hierarchical Question Decomposition Tree." RoHT addresses the limitations of existing XQA methods by integrating knowledge from both knowledge bases (KBs) and text corpora, particularly for complex questions. Our approach constructs a Hierarchical Question Decomposition Tree (HQDT) to represent the compositional structure of a question, breaking it down into sub-questions with associated certainty scores. We then perform probabilistic reasoning over the HQDT, recursively selecting appropriate knowledge sources (KB, text, or child nodes) and aggregating answers with probabilities.  Evaluations on KQA Pro and Musique datasets demonstrate RoHT's superior performance compared to state-of-the-art methods, highlighting the benefits of explicit question decomposition and the effective fusion of knowledge from heterogeneous sources. RoHT outperforms existing KB QA and end-to-end methods, achieving substantial improvements in accuracy and showcasing the value of supplementing text information with KB knowledge.</sample>
    <sample id="196">I saw Bart and Lisa.</sample>
    <sample id="197">Human evaluation, such as asking judges to select which conversation is better or rate conversations on a Likert scale.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows.</sample>
    <sample id="199">Ja, in sieben Datensätzen sank die Leistung von Englisch, während sie in nur drei Datensätzen stieg.</sample>
    <sample id="200">Nein, sie kennen die Namen der Entitäten, aber nicht unbedingt die Entitäten selbst.</sample>
    <sample id="201">State-of-the-art, neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">No.</sample>
    <sample id="203">NLP tasks are becoming more subjective and socially oriented, making it increasingly important to understand how positionality skews datasets and models.</sample>
    <sample id="204">Vollständige Feinabstimmung.</sample>
    <sample id="205">This work investigates the propagation of political biases from pretraining data to language models and downstream tasks, highlighting potential fairness issues in NLP applications. Analyzing language models like GPT-4 and BART, the study reveals varying political leanings across a political spectrum, with GPT models generally exhibiting greater social liberalism. Controlled experiments involving further pretraining on partisan corpora demonstrate that language models readily adopt the ideological biases of their training data, including reflecting societal polarization post-2017. Evaluating these models on hate speech and fake news detection reveals performance disparities based on political leaning: left-leaning models perform better on detecting hate speech against minority groups but worse against dominant groups, and vice versa for right-leaning models. This bias extends to fake news detection, with models excelling at identifying misinformation from opposing viewpoints. The research underscores a critical dilemma – mitigating political bias risks censorship, while retaining it leads to fairness concerns, posing a challenge akin to the trolley problem.</sample>
    <sample id="206">CE tasks followed by debate.</sample>
    <sample id="207">The latest test sets were used to avoid overlap of the test data with the language model's training data.</sample>
    <sample id="208">Drei.</sample>
    <sample id="209">The method improves both semantic completeness and faithfulness to the constraint. T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results are proposed as a base benchmark for automatic text simplification.</sample>
    <sample id="212">Ein T5-Modell.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">This paper presents a novel argument for symmetric coordination structures, challenging asymmetric approaches found in Universal Dependencies and Meaning-Text Theory. The argument is grounded in the principle of dependency length minimization, which favors shorter dependencies. Analysis of the Penn Treebank reveals a consistent tendency for the left conjunct in a coordination to be shorter, a phenomenon previously observed but now linked to dependency structure. Crucially, this tendency is only observed when the governor is on the left or absent. When the governor is on the right, the preference for a shorter left conjunct disappears. This observation provides empirical support for symmetric coordination structures, where no single conjunct is inherently prioritized, and argues against asymmetric models that designate a head for the entire coordinate structure. The findings highlight the importance of dependency length in understanding coordination patterns and offer a new perspective on linguistic theory.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or rely on labeled data. This work introduces DCG, a Disentangled Controllable Generation model, which leverages compositional generation to handle multi-attribute control. DCG learns attribute concepts from seen values and employs a disentanglement loss to improve generalization to unseen attribute combinations. A novel, reference-free evaluation framework, MAE, is proposed to assess controllability across different attribute granularities. Experiments on two benchmarks demonstrate DCG's superior performance in attribute controllability and text quality compared to baselines. The model utilizes attribute-oriented and task-oriented prompts within a DialoGPT framework, enhanced with disentanglement learning and pseudo combinations. MAE exhibits strong correlation with human judgments, proving its effectiveness. The study highlights DCG's ability to generalize from seen to unseen attribute combinations, advancing multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google</sample>
    <sample id="219">This work introduces a compare-and-contrast multistage pipeline for uncovering financial signals within Form 10-K reports, aiming to reduce the human effort required for financial report analysis. The research observes high text similarity between consecutive annual reports, motivating a novel highlighting task where a model identifies key words (rationale) explaining differences between a target report and its preceding year's report. The pipeline consists of document segmentation, relation recognition (classifying pairs as β, revised, or mismatched), and a two-stage fine-tuning process. Initially, the model undergoes out-of-domain fine-tuning using the eSNLI dataset, followed by in-domain fine-tuning leveraging revised pairs as pseudo-positive labels and incorporating soft labeling techniques. Evaluation on the FINAL dataset and eSNLI demonstrates strong performance, with the model effectively identifying rationale and generalizing well. The study highlights the benefits of the approach on mismatched pairs, suggesting potential for broader application in information retrieval and financial analysis.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Deutsch nach Englisch</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates how to effectively transfer knowledge from general-purpose question answering models (trained on Wikipedia) to specialized domains like biomedicine. The core challenge lies in domain shift, where the source model struggles with new vocabulary and reasoning patterns. The paper explores data interventions—both zero-shot and few-shot—to improve performance. Few-shot methods leverage large language models to generate training examples from target domain passages, while zero-shot techniques manipulate question and context formats to control model learning. Experiments across seven target datasets reveal that retriever performance improves by 8% and reader performance by 11% on average with few-shot adaptation. Furthermore, the study identifies and categorizes dataset shifts (no shift, concept shift, covariate shift, full shift) and demonstrates that specific interventions are more effective for certain shift types, with few-shot methods generally performing well across all domains and zero-shot methods proving useful for concept and covariate shifts.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART and base mBART.</sample>
    <sample id="225">53 werden für das Training und 9 für die Tests verwendet.</sample>
    <sample id="226">Zwei.</sample>
    <sample id="227">Current language models excel at general NLP tasks but struggle with grounded language understanding—mapping natural language to executable plans in specific environments. This challenge stems from a lack of grounding during pre-training. Existing approaches rely on language models to directly generate plans, often resulting in invalid or ungrammatical outputs. This paper introduces Pangu, a novel framework that separates plan generation (handled by a symbolic agent) from plan evaluation (handled by a language model). Pangu leverages language models for discrimination rather than generation, significantly improving performance and robustness. Experiments on knowledge-based question answering demonstrate Pangu's superior performance across various language models (BERT, T5, Codex) and learning paradigms (fine-tuning, in-context learning), exhibiting strong sample efficiency. Notably, Pangu shows greater generalizability in non-i.i.d. settings, likely due to autoregressive models overfitting to seen structures, while Pangu maintains consistent probability distributions across seen and unseen structures. The key takeaway is that discrimination, rather than generation, is a more effective strategy for language models in grounded language understanding.</sample>
    <sample id="228">AG News, MIND, SST2 and Enron Spam.</sample>
    <sample id="229">This paper introduces two novel tasks—Suboptimal-Claim Detection and Claim Improvement Suggestion—aiming to support argumentative writing by identifying claims needing revision. The research leverages revision patterns from collaborative online debate platforms like Kialo to model argument quality implicitly. The study explores challenges inherent in using revision-based data, including representativity, model complexity, contextual dependency, and topical/user bias. Experiments investigate various model architectures and the impact of pre-training and fine-tuning on claim assessment. Findings demonstrate that revision-based data can effectively be used for these tasks, and modeling the distance between claim versions aids in suboptimal claim detection. The research highlights the task- and quality-issue-dependent influence of contextual information, offering valuable insights for developing automated argumentative writing support tools.</sample>
    <sample id="231">A dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) aims to translate spoken language into text in real-time, but current models face challenges like complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency levels. This paper introduces EDAtt (Encoder-Decoder Attention), a novel strategy that leverages existing offline speech translation models without retraining or architectural modifications. EDAtt dynamically decides when to emit partial translations based on the cross-attention mechanism between audio and text. Specifically, a word is emitted only if its attention weights are not concentrated on recent speech frames, indicating sufficient stability in the received information. Experiments on German demonstrate that EDAtt outperforms existing strategies applied to offline models, achieving higher translation quality (BLEU score) with lower latency, both in terms of average and computationally-aware lagging. The code, models, and simultaneous output are publicly available to promote reproducibility.</sample>
    <sample id="234">The prompting strategy has a big influence on the performance of LLMs for translation.</sample>
    <sample id="235">Es wird nicht erwähnt, welcher Universität die Autoren angehören.</sample>
    <sample id="236">Five expert-written instructions.</sample>
    <sample id="237">They propose a diagnostic test suite called KITMUS, featuring a coreference resolution task designed to probe the ability to draw on knowledge from different sources.</sample>
    <sample id="238">MeetingBank, a new benchmark dataset for meeting summarization, has been created by the University of Central Florida. Addressing the need for datasets tailored to meeting-specific summarization, MeetingBank comprises 1,366 City Council meetings and nearly 7,000 instances, including transcripts, reference summaries, and URLs. The dataset was constructed using Speechmatics API for transcription and leveraging meeting minutes for summaries. Analysis reveals that City Council meeting summaries often include verbatim points rather than abstractive content. Evaluations using both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet) summarization systems, alongside GPT-3, demonstrate DialogLM's strong performance with ROUGE-2 scores. While GPT-3 underperforms in automatic metrics, human evaluation reveals exceptional fluency and coherence, though lower informativeness and factuality. MeetingBank provides a valuable resource for researchers to develop advanced meeting summarizers and offers insights into City Council decision-making processes.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Rezension des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine Gemeinschaftsarbeit mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es Spitzenleistungen in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting von großen Sprachmodellen für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle unter Verwendung der Best Practices der MT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überlappung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir haben sie mit modernsten Systemen verglichen, also dem leistungsstärksten System, also der WMT-Bewertung. Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch Ergebnisse der menschlichen Bewertung durch Experten. Abschließend geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwendeten und für jeden Satz zwei verschiedene Prompts bereitstellten. Bei den meisten Sätzen, 516 von 1.000, wurde ein Unterschied von mehr als einem BLEURT-Punkt festgestellt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine 5-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System bereitstellen, einfach mit der Sprache kennzeichnen. In diesem Beispiel hier, wo wir eine Übersetzung vom Deutschen ins Englische durchführen, sind die deutschen, die Quellsätze, mit "Deutsch:" gekennzeichnet und die englischen Übersetzungen mit "Englisch:". Wir haben festgestellt, dass die tatsächliche Form des Promptings keinen großen Einfluss hat, wenn es sich um mehrere kurze Prompts handelt. Es ist entscheidend für Zero- und One-Shot-Prompting. Und wenn wir wie in unserem Fall zu einem 5-Shot-Prompting übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Promptings. Die Beispiele tragen den größten Stellenwert. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Bewertungen auf den Dev-Daten. Die Dev-Daten sind viel besser kuratiert und von höherer Qualität als die Trainingsdaten, die verrauschter sind. Ihre Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte, modernste Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. PaLM kommt aber ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Flüssigkeit von PaLM mit modernsten Systemen vergleichbar ist, aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint also, dass PaLM eine besser klingende Übersetzung erstellen möchte, manchmal indem es Teile des Quellsatzes auslässt, die in der Übersetzung enthalten sind. Die Kategorie "Stil/Holprig" für PaLM ist jedoch niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber dennoch mit Genauigkeitsproblemen zu kämpfen hat. Und das war's für diesen wirklich kurzen Überblick. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein PhD-Student an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine Gemeinschaftsarbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwache Supervision und schwaches überwachtes Lernen beginnen. Bei schwacher Supervision werden die Daten nicht manuell beschriftet. Stattdessen beschriften wir die Daten mit schwachen Beschriftungsquellen, wie z. B. einfachen heuristischen Regeln, Wissensdatenbanken oder minderwertiger Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel billiger, aber auch verrauscht, was bedeutet, dass ein gewisser Teil der Annotationen fehlerhaft ist. Wenn wir neuronale Netze direkt mit schwach beschrifteten Daten trainieren, neigen die neuronalen Netze dazu, das Beschriftungsrauschen zu merken und generalisieren nicht. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Beschriftungsrauschen zu trainieren, so dass die trainierten Modelle dennoch gut generalisieren. In den letzten Arbeiten im WSL, wobei WSL für Weakly Supervised Learning steht, wird oft behauptet, dass Menschen sagen, dass sie Modelle nur mit den schwach beschrifteten Daten trainieren und eine hohe Leistung auf sauberen Testdatensätzen erzielen. Diese Behauptung ist technisch nicht falsch, aber es gibt einen Haken: Man geht davon aus, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellauswahl verfügbar ist. Wir können nicht bei diesem Problemsetting aufhören, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Der oben genannte Zweifel stellt drei Forschungsfragen. Erstens, ist ein sauberer Validierungsdatensatz für WSL erforderlich oder können wir stattdessen einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn saubere Daten erforderlich sind, wie viele saubere Samples benötigen wir dann? Und schließlich, sollten wir die sauberen Samples nur für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungs-Samples benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle ohne saubere Validierungs-Samples nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erlangung sauberer Validierungs-Samples sollten nicht übersehen werden. Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl sauberer Validierungs-Samples dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erzielen, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Samples pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns auf jede Weise saubere Samples beschaffen, erzielen das direkte Training auf diesen sogar eine bessere Leistung. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie man sieht, beginnt das Fine-Tuning mit 10 Samples pro Klasse, WSL-Ansätze zu übertreffen. Schließlich können die in früheren WSL-Ansätzen beanspruchten Leistungsverbesserungen leicht durch die Möglichkeit erzielt werden, die sauberen Validierungs-Samples kontinuierlich zu fine-tunen. Wie aus den Abbildungen ersichtlich, unterschlägt das Vanilla-Modell, das als FTw bezeichnet wird, anfänglich gegenüber komplexeren WSL-Methoden wie COSINE. Wenn wir jedoch die kontinuierliche Fine-Tuning auf den sauberen Samples zulassen, erzielt FTw die gleiche Leistung wie andere Methoden. Daher gibt es in der Praxis keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Samples benötigen, damit sie ordnungsgemäß funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Berichten Sie zunächst über die Modellauswahlkriterien. Berichten Sie beispielsweise, ob die Modellauswahl über saubere Validierungs-Samples erfolgt ist. Zweitens sollten WSL-Ansätze mit Few-Shot-Learning-Baselines verglichen werden, da beide mit sauberen Samples arbeiten. Drittens sollte ein kontinuierliches Fine-Tuning als einfacher, aber starker Baseline in zukünftiger WSL-Arbeit berücksichtigt werden. Schließlich haben wir unseren Code Open Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">This paper introduces a human-in-the-loop evaluation framework for early misinformation detection, addressing shortcomings in existing automated approaches. Current systems often rely on unrealistic datasets and exclude human input, hindering their effectiveness in real-world scenarios. Our framework emphasizes end-to-end systems integrating human feedback throughout the process, rather than treating them as authoritative. We demonstrate this framework with a COVID-19 treatment misinformation detection system, comprising claim detection (using keyword filtering and a T5 question-answering model) and policy violation verification (using a BERT-based stance classification model). Evaluation focuses on early detection—identifying unapproved treatments before debunking—and policy violation identification. Results show our system detects treatments before news debunking and achieves 65% precision for policy violation detection, enabling 124.2 policy violations confirmed per human hour. This work provides a more realistic evaluation of human-system interaction in misinformation detection and offers a valuable perspective for developing future, consistently evaluable, human-in-the-loop systems.</sample>
    <sample id="242">Human evaluation, such as asking judges to select which conversation is better or rate conversations on a Likert scale.</sample>
    <sample id="243">Vier.</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This paper, "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization," presents a two-step pipeline for identifying high-quality Amazon Mechanical Turk (MTurk) workers for summarization tasks. Addressing the limitations of automatic metrics and unclear best practices for MTurk recruitment, the pipeline first assesses annotator evaluation skills through a qualification task (identifying "gold" and "silver" workers), followed by an endurance task to gauge workload capacity. The resulting pipeline yielded 12 workers (4 gold, 8 silver) achieving high inter-annotator agreement (IAA) comparable to experts (Krippendorff's Alpha = 0.443). A reference-based task further demonstrated strong agreement (Alpha = 0.534). Compared to baseline approaches like MACE and CloudResearch MTurk workers, the pipeline achieved comparable or superior results at a lower cost. While Spearman's correlation was observed between pipeline and CloudResearch workers, and GPT models correlated with expert judgments, the pipeline doesn't guarantee training for correctness. The work highlights a cost-effective method for obtaining high-agreement annotations at scale, with future work focusing on improving worker correctness and expanding applications across tasks, languages, and platforms.</sample>
    <sample id="246">Ja, der Code ist auf GitHub verfügbar.</sample>
    <sample id="247">FACTKG introduces a novel Knowledge Graph-Based Fact Verification task and dataset to address the lack of datasets utilizing knowledge graphs as evidence for natural language claims. Leveraging DBpedia, FactKG provides claims in both written and colloquial styles, labeled as SUPPORTED or REFUTED. The task requires retrieving evidence from the KG and verifying claims through five reasoning types: one-hop, conjunction, existence, multi-hop, and negation. The dataset facilitates reliable and practical fact verification, particularly relevant for dialogue systems needing consistency checks with internal knowledge graphs. The study presents FactKG's statistics and establishes baselines, demonstrating that models utilizing graph evidence, such as GEAR, significantly outperform those relying solely on claims. The dataset and code are publicly available for further research.</sample>
    <sample id="248">Nein, die Datensätze und Modelle sind weniger auf nicht-binäre Personen ausgerichtet als auf männliche und weibliche Personen.</sample>
    <sample id="249">Durch das Hinzufügen von Rauschen zur Eingabe unter Beibehaltung der relevanten Struktur.</sample>
    <sample id="250">Evaluating multiple aspects of chat quality to understand strengths and weaknesses on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">U-CREAT is a novel unsupervised pipeline for Prior Case Retrieval (PCR) that leverages event extraction to improve efficiency and generalization across legal systems. This work introduces the IL-PCR dataset, a new benchmark for PCR tasks featuring 7,070 Indian legal cases with a substantial number of citations, surpassing existing datasets like COLIEE’21 in size and complexity. U-CREAT’s core innovation lies in its event-based approach, extracting subject-verb-object triplets from legal documents using dependency parsing. These events are then used to construct an interaction matrix, enabling retrieval models to rank candidate cases. Experiments demonstrate that event-based models, particularly the Event Filtered Documents approach, significantly outperform count-based and transformer-based models, including legal-specific BERT variants, achieving state-of-the-art results on both the IL-PCR and COLIEE datasets. U-CREAT exhibits lower inference times and higher F1 scores, showcasing its potential for advancing PCR research and practical applications.</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed to detect signs of mental disorders in social media posts. Addressing the challenge of limited annotated data, DisorBERT leverages knowledge transfer from general language models (BERT) to social media language (Reddit) and then specializes in the mental health domain. The approach incorporates guided masking, encouraging the model to focus on relevant words during training. Experiments using the eRisk dataset demonstrate a strong balance between precision and recall, outperforming existing models like MentalBERT. Analysis of predicted words reveals DisorBERT's tendency to generate terms with a negative psychological orientation, aligning with mental health concerns. Visualization of attention scores highlights the model's ability to identify key phrases related to specific disorders, such as anxiety and medication in the context of depression. Future work will explore diverse lexical resources and integration of clinical data to further enhance the model's accuracy and applicability.</sample>
    <sample id="254">This paper introduces a novel framework for document-level distant relation extraction (DocRE) that addresses the challenge of noisy data. Existing methods utilizing distant supervision (DS) often suffer from noise induction due to false-positive pseudo-labels. To mitigate this, the proposed framework employs uncertainty-guided label denoising, leveraging both DS and human-annotated data to generate pseudo-labels. A key innovation is an instance-level uncertainty estimation method, utilizing Monte Carlo dropout, specifically designed to handle overlapping relations and differentiate between true and false positives. Dynamic class uncertainty thresholds are then applied to filter pseudo-labels with high uncertainty, replacing noisy labels with more reliable ones. A multi-phase training strategy iteratively re-labels DS data to further enhance performance. Experiments on public datasets demonstrate significant improvements over strong baselines, highlighting the effectiveness of the proposed approach in improving label quality and boosting DocRE performance, particularly in addressing the long-tail problem.</sample>
    <sample id="255">It's crucial for zero- and one-shot prompting.</sample>
    <sample id="257">Vier state-of-the-art Chat-Modelle.</sample>
    <sample id="258">This work explores the potential of large language models (LLMs) as an alternative to traditional human evaluation in natural language processing. Recognizing the instability and reproducibility challenges of human evaluation, the authors investigated whether LLMs could be instructed to rate text samples based on natural language instructions. They evaluated stories generated by GPT-2 and human writers using LLMs (T0, InstructGPT, and ChatGPT) across four attributes: grammar, coherence, likability, and relevance. Human evaluations, conducted by English teachers, served as ground truth for comparison.

The results showed that while smaller LLMs didn't consistently prefer human-written stories, Davinci and ChatGPT demonstrated a clear preference mirroring that of the human evaluators. This suggests that certain LLMs can indeed provide meaningful ratings and potentially serve as a viable substitute for human evaluation. The paper addresses further questions regarding agreement with human raters, instruction sensitivity, sampling methods, cost-benefit analysis, and applicability to other NLP tasks, encouraging readers to consult the full paper or poster presentation for more details.</sample>
    <sample id="259">XSemPLR is a novel benchmark dataset for cross-lingual semantic parsing, encompassing 9 datasets across 5 tasks, 8 meaning representations, and 22 languages from 15 language families. This work addresses the limitations of existing cross-lingual semantic parsing models, which often lack broad language coverage or support for diverse meaning representations. XSemPLR facilitates comprehensive evaluation through six training and evaluation settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot. The study evaluates Encoder-PTR and Encoder-Decoder models, revealing that Encoder-Decoder architectures consistently outperform others. Training multilingual models with a mix of languages generally improves performance across most languages, though English can experience a performance drop ("Curse of Multilinguality"). The benchmark highlights significant performance gaps in zero-shot transfer, which are substantially reduced with few-shot learning. Findings also indicate that pretraining on English data enhances few-shot performance and that current large language models like Codex and BLOOM remain insufficient for cross-lingual semantic parsing.</sample>
    <sample id="260">Der Text nennt nur Jingwei Yi als Sprecher.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">Die Arbeit wurde von Siyu Yuan und ihrem Team von Fudan University durchgeführt.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to label biases. We present a systematic typology of these biases, identifying three key types: vanilla-label bias, context-label bias, and a novel *domain-label bias* arising from the task corpus itself. Experiments demonstrate that exposure to in-domain words can significantly skew LLM predictions, even without specific examples.

To mitigate these biases, we propose *domain-context calibration*, a novel method that utilizes random in-domain words as content-free text to estimate and correct label biases. Unlike prior methods using single, predefined tokens, our approach accounts for domain-specific influences. Extensive evaluations across diverse datasets and models (including GPT-3) show that domain-context calibration substantially improves ICL performance, particularly on tasks with high domain-label bias. Calibration studies reveal that random in-domain words outperform single tokens and random English words, highlighting the importance of domain awareness in bias mitigation.</sample>
    <sample id="264">TAVT: Towards Transferable Audio-Visual Text Generation addresses the challenge of multimodal text generation, specifically audio-visual text generation, where data annotation is costly and existing models struggle with domain shifts. This paper introduces a novel task, Transferable Audio-Visual Text Generation, focusing on aligning visual concepts across domains using a unified audio semantic space. The proposed framework comprises an audio-visual meta-mapper network to map visual concepts into this space, a transformer-based encoder-decoder for text generation with modality-aware attention, and Dual Counterfactual Contrastive Learning (DCLL) for improved visual-textual alignment. The model is trained using a meta-learning approach, enabling rapid adaptation to new domains with limited labeled data. Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art models in both cross-dataset and cross-domain settings, particularly in low-resource scenarios, showcasing its effectiveness in transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Die Autoren gehören der Universität Prag an.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlicher Bewertung, beispielsweise durch die Bitte von menschlichen Bewertern, zu wählen, welche von zwei Gesprächen besser ist, oder durch die Bewertung von Gesprächen anhand einer Likert-Skala. Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der Gesamtqualität des Dialogs zu liefern, aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz ist es, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, beispielsweise die Relevanz der Modellantworten mithilfe bestehender Vergleichs- oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit angegeben wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, z. B. das Antworten mit irrelevanten Informationen oder das Widersprechen an sich selbst. Wir nennen diesen Ansatz die Annotation von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um Chat-Modell-Verhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als wirksam für die Chat-Qualität befunden wurden. ABC-Eval kann die Raten messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten halluziniert oder gesundem Menschenverstand widerspricht und wann das Modell Empathie zeigt oder nicht. Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie anhand von 100 menschlich-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Rundenebene, Likert-Bewertungen auf Dialogebene und Dialogebenen-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu den acht am häufigsten gemessenen Aspekten des Dialogs erhoben, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse der Bewertungsergebnisse stellten wir fest, dass ABC-Eval-Verhaltensbezeichnungen insgesamt zuverlässiger sind als Bezeichnungen, die mit bestehenden Methoden erhoben wurden, gemessen am Inter-Annotator-Agreement auf 100 doppelt beschrifteten Gesprächen. Darüber hinaus sind ABC-Eval-Bezeichnungen prädiktiver für die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die durch bestehende Methoden erzeugt werden, wie diese einfache lineare Regression zeigt. Sie können sehen, wie die Messung des Anteils von Runden mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich überprüften wir, ob jede Bewertungsmessung einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression durchführten. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken einzeln entfernen, geht dabei ein beträchtlicher Teil der Informationen über die Qualität verloren. Andererseits erklärt die Kombination aller turnbasierten Likert-Metriken deutlich weniger der Qualität, und weniger dieser Metriken enthalten einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, Konversations-KI mit einer höheren Auflösung zu bewerten, als dies bisherige Methoden ermöglichten. Sie können in den Ergebnissen unseres Experiments sehen, dass mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20 % ihrer Antworten Verletzungen des gesunden Menschenverstands. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen, und sie widersprechen sich selbst oder ihrem Gesprächspartner in etwa 10 % der Zeit. Mit dem rasanten Fortschritt in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen sinken, die seit unserer Bewertung veröffentlicht wurden. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmesswerte für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich genutzt werden kann, als ein sinnvoller Schritt in diese Richtung. Und wir freuen uns darauf zu sehen, wie sich die Konversations-KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Sieben.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel "Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Erkundung" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig erstellt. Viele Übersetzungen hängen also vom Kontext ab. Zum Beispiel, wie würden wir "mole" in diesem Satz übersetzen? Nun, wenn der vorherige Satz lautet "Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden", dann bezieht sich "mole" auf einen Spion. Aber wenn der vorherige Satz lautet "Könnte es etwas Ernstes sein, Doktor?", dann bezieht sich "mole" auf einen Leberfleck. Je nach Kontext ändert sich die Bedeutung des Wortes, und daher ändert sich auch seine Übersetzung. Die Bewertung, wie gut Modelle solche Fälle übersetzen können, ist jedoch ziemlich schwierig. Erstens, nur ein kleiner Teil der Übersetzungen hängt vom Kontext ab, was dazu führt, dass korpusbasierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Und einige Leute haben gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie in der Regel auf Fachwissen und menschliche Kuratierung beruhen. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle mit diesen Fällen um? Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie stark ein Wort während der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht durch Messung, wie viele Informationen der Kontext C über das Ziel Y gibt, gegeben die Quelle X. Man kann CXMI als die Information betrachten, die man erhält, wenn man dem Modell Kontext gibt. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder Wortebene messen kann. Wir können Wörter mit einem hohen P-CXMI als solche betrachten, die für die Übersetzung Kontext erfordern. Wir analysieren nun Wörter mit einem hohen P-CXMI, um nach Mustern zwischen diesen Wörtern zu suchen. Wir führen unsere Analyse anhand von Transkripten von TED-Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Erstens betrachten wir Wortarten, die einen hohen mittleren P-CXMI aufweisen. Dies ermöglicht es uns beispielsweise, Dualpronomen im Arabischen zu finden, die einen relativ hohen P-CXMI aufweisen. Dies lässt sich erklären, weil Englisch keine Dualpronomen hat, sodass man beim Übersetzen ins Arabische Kontext benötigt, um festzustellen, ob ein Pronomen dual ist. Und ähnlich stellen wir fest, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die passende Verbform auswählen wollen. Wir betrachten dann Vokabeln, die über alle ihre verschiedenen Vorkommnisse hinweg einen hohen P-CXMI aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, bei dem im Chinesischen Kontext benötigt wird, um Eigennamen so zu übersetzen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Und ähnlich stellen wir fest, dass es wichtig ist, im richtigen Formalitätsgrad zu übersetzen. Schließlich betrachten wir einzelne Token mit einem hohen P-CXMI. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie z. B. Ellipsenauflösung. Nun verwenden wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir haben unseren Tagger Multilingual Discourse-Aware oder MuDA Tagger genannt. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben. Wir verwenden dann den MuDA Tagger, indem wir den Tagger auf einen parallelen Korpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere Übersetzungsmetriken der Wahl auf die kontextabhängigen Beispiele an, die der MuDA Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der maschinellen Übersetzung auf Dokumentenebene zu bewerten. Erstens, wenn wir korpusbasierte Metriken verwenden: also für BLEU, finden wir, dass kontextunabhängige Modelle die beste Leistung erbringen. Wenn wir aber COMET verwenden, erzielen kontextbewusste Modelle die besten Ergebnisse. Und wenn wir Word f-measure verwenden, dann haben Modelle mit und ohne Kontext eine vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste System für die Übersetzung auf Dokumentenebene zu bestimmen, wenn wir nur korpusbasierte Metriken verwenden. Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten, und wir stellen fest, dass kontextbewusste Modelle für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Pronomen, Ellipsen und Verbformen nicht wesentlich besser als Modelle, die keinen Kontext verwenden. Dies deutet also darauf hin, wo wir Fortschritte für die Übersetzung auf Dokumentenebene erzielen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die Übersetzung auf Dokumentenebene. Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden dann unsere Ergebnisse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder nicht handhaben können, und welche Übersetzungssysteme gut für die Übersetzung auf Dokumentenebene sind. Vielen Dank für Ihre Aufmerksamkeit. Wir sehen uns in Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This paper introduces "IndicMT Eval," a dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. Recognizing the limitations of applying English-centric metrics to languages with diverse linguistic features, the study focuses on Tamil, Malayalam, Hindi, Marathi, and Gujarati. The dataset comprises 7,000 samples generated from 200 source sentences translated by seven different MT models. Bilingual expert annotators meticulously evaluated these translations, marking errors with type and severity using the MQM framework.

The analysis reveals that COMET-based metrics demonstrate the highest overall correlations with human scores. However, many metrics exhibit skewed score distributions. Interestingly, metrics show improved correlation when evaluated on subsets focusing on accuracy errors. The authors further fine-tuned COMET using the MQM dataset, creating "IndicCOMET," which outperforms standard COMET models across multiple languages and demonstrates strong zero-shot transfer capabilities. Finally, IndicCOMET exhibits improved robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for more reliable MT evaluation in Indian language contexts. The dataset is publicly available.</sample>
    <sample id="277">Sie hat keinen Namen.</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness" to identify words that distinguish marked groups from unmarked ones, using weighted log-odds ratios to compare the top words for each marked group.</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">This paper introduces MultiEMO, a novel attention-based multimodal fusion framework for emotion recognition in conversations (ERC). Addressing limitations in existing ERC methods, MultiEMO focuses on effectively exploiting multimodal complementarity, improving minority emotion classification, and distinguishing semantically similar emotions. The framework comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. Key contributions include VisExtNet, a visual feature extractor that focuses solely on facial expressions, MultiAttn, a multimodal fusion network utilizing bidirectional multi-head cross-attention layers to integrate textual, audio, and visual cues, and a Sample-Weighted Focal Contrastive Loss to enhance classification of minority and semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in challenging scenarios with asynchronous emotional cues. While limitations exist regarding speaker differentiation in VisExtNet and batch size requirements for the SWFC loss, MultiEMO represents a significant advancement in multimodal emotion recognition.</sample>
    <sample id="281">This work investigates when translation requires context and how well current models handle these cases. Traditional evaluation metrics like BLEU struggle to capture context-dependent translations, which are crucial for accurate document-level machine translation. We extend CXMI to Pointwise CXMI (P-CXMI) to measure context usage at the word level across 14 languages using TED talk transcripts. Our analysis reveals patterns related to part-of-speech tags (e.g., dual pronouns), vocabulary (e.g., proper noun consistency in Chinese), formality, and sentence structure (e.g., ellipsis).

To facilitate evaluation, we developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words. Using this benchmark, we evaluated models and found that context-aware models outperform context-agnostic models when assessed with COMET and for phenomena like formality and lexical cohesion. However, improvements are needed for ellipsis, pronouns, and verb forms. Our results also indicate that DeepL generally surpasses Google Translate in document-level translation accuracy. This research provides valuable insights and a benchmark for advancing document-level machine translation.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story-level text style transfer, addressing the limitations of existing token or sentence-level methods. It tackles the challenges of imitating complex authorial linguistic preferences at the discourse level and preserving content across different writing topics. StoryTrans learns discourse representations and combines them with style embeddings to generate text in target styles. A key innovation is a two-stage generation process: first transferring the source text with masked content, then incorporating style-specific keywords. The model utilizes an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses. Extensive experiments on new Chinese and English datasets demonstrate StoryTrans's superior performance over baselines in style control and content preservation, as confirmed by both automatic and manual evaluations. Style visualization confirms alignment with golden text, and case studies highlight StoryTrans's ability to enrich storylines and maintain source semantics while rewriting sentences in the target style. Data and code are publicly available.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction

Span-based universal information extraction (UIE) models often over-rely on precise span boundaries, which can be ambiguous in annotation. To address this, we propose FSUIE, a novel framework incorporating a fuzzy span mechanism. FSUIE introduces a fuzzy span loss, modeling span boundaries as continuous probability distributions rather than discrete points, and a fuzzy span attention mechanism. The attention mechanism adaptively adjusts the attention span, dynamically controlling its length and linearly decaying attention on boundary tokens. This mitigates the mismatch between transformer feature extraction (global focus) and IE (limited span length). Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate significant performance improvements over UIE-base, achieving state-of-the-art results on several datasets. Ablation studies confirm the benefits of both fuzzy span loss and attention, and visualizations reveal focused attention on relevant semantic information. FSUIE exhibits strong generalization capabilities and efficient convergence, offering a simple yet effective approach to enhance UIE performance.</sample>
    <sample id="285">This paper addresses factual errors in dialogue summarization, a previously unexplored area. While existing Factual Error Correction (FEC) models rely on unreliable factuality metrics and blur the line between correction and generation, the authors propose a new evaluation framework using manually annotated reference corrections. This framework emphasizes minimal, fluent, and non-redundant error correction.

They introduce a novel taxonomy of factual errors, categorizing them as content-based (part-of-speech, dependencies) and form-based (addition, deletion, substitution). The evaluation framework leverages ERRANT, a grammar error correction metric, through alignment, classification, and comparison steps. Experiments reveal that training FEC models with reference summaries improves performance on unreliable factuality metrics, highlighting the need for evaluation method changes. Combining human-annotated data with synthetic data proves promising. However, current FEC models struggle with additions and more complex errors like attribute and modality errors, indicating areas for future research.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier.</sample>
    <sample id="288">BLiMP und SyntaxGym.</sample>
    <sample id="290">FTw, COSINE.</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">OSCAR 138 GB</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This work presents EPIC, a novel English Perspectivist Irony Corpus developed through a collaboration between the University of Turin and Amazon Alexa. EPIC addresses the limitations of traditional supervised machine learning approaches in Natural Language Understanding, which often rely on a single "ground truth" annotation. Focusing on the complex phenomenon of irony, the research explores the variability in human perception of irony rather than seeking a definitive label.

EPIC comprises approximately 300 short conversations collected from Reddit and Twitter over 1½ years, annotated by 74 diverse annotators across five English varieties. The annotation task involved identifying whether a reply was ironic within the given context. Analysis revealed significant inter-annotator disagreement, varying across demographics like gender, age, and nationality. To account for these differences, "perspective-aware" models were developed by fine-tuning pre-trained language models on data subsets representing different annotator groups. These models demonstrated increased confidence in their predictions compared to aggregated models, suggesting a better capture of nuanced perspectives on irony. The study further found that disagreement was highest between generations and geographically proximate regions (UK and Ireland).</sample>
    <sample id="297">This project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," investigates coded language used to convey hidden meanings to an in-group while maintaining plausible deniability to an out-group. The research develops a typology and glossary of over 340 dogwhistles (primarily US-centric, focusing on racist, transphobic, and anti-Semitic terms), categorized by register, type, and persona. A case study of historical U.S. political speeches reveals a correlation between dogwhistle frequency and the Republican Southern Strategy. Experiments with GPT-3 demonstrate its ability to surface and identify dogwhistles, though performance varies significantly based on register and topic. Crucially, the study demonstrates how dogwhistles can evade content moderation, as sentences containing dogwhistles are often rated as less toxic by automated systems compared to those using explicit slurs. This work highlights the challenges of detecting and mitigating coded rhetoric in NLP and its implications for online content moderation.</sample>
    <sample id="298">Das Experiment, bei dem einige Modelle mit neueren Daten erneut trainiert oder vorab trainiert wurden, zeigte, dass die Leistung mit zunehmender zeitlicher Differenz abnahm.</sample>
    <sample id="299">This work addresses the brittleness of Natural Language Inference (NLI) models, which often rely on spurious correlations (shortcuts) present in training data, leading to poor out-of-distribution performance. Existing shortcut mitigation methods require auxiliary models and dataset-specific knowledge, limiting their applicability. We propose a novel minimax training approach that eliminates these limitations. Our method leverages the observation that NLI models struggle with "hard" examples that contradict shortcuts. A learner model is trained to minimize NLI loss, while an auxiliary model dynamically generates example weights to maximize the learner's loss, forcing it to focus on these hard examples. This alternating optimization encourages the learner to prioritize learning from under-represented data that counteracts shortcut exploitation. Crucially, our method doesn't require pre-defined shortcuts or a pre-trained auxiliary, and operates without the auxiliary at test time. Experiments on MNLI, FEVER, and QQP with adversarial test sets (HANS, PAWS) demonstrate consistent improvements in out-of-distribution performance while maintaining in-distribution accuracy, showcasing the method's effectiveness and generalizability.</sample>
    <sample id="300">Interactive dictation is a novel task that combines speech dictation and editing through natural language commands, moving beyond traditional speech-to-text systems. This work introduces and formalizes interactive dictation, characterized by flexible interleaving of dictation and editing using intuitive, open-ended utterances. The authors designed a data collection interface and built a dataset to support this task, and developed a baseline system comprising four steps: ASR recognition, utterance segmentation, command extraction/normalization, and sequential execution of dictation and commands. The system utilizes separate models for each step, experimenting with T5 and GPT-3 architectures for command interpretation, with both program prediction and direct state prediction approaches. Results indicate a trade-off between runtime and accuracy, with GPT-3 demonstrating higher accuracy but slower performance, while T5 benefits from program prediction for efficiency. The code and dataset are publicly released to encourage further research in this promising area.</sample>
    <sample id="302">Because the first step only provides an unordered multiset of tokens, and they need to be put in the correct order.</sample>
    <sample id="303">Die Autoren empfehlen mehr Transparenz, weil sie nicht wissen, ob positive Stereotypen auf eine übermäßige Ausrichtung auf Werte oder andere Anti-Stereotyp-Methoden zurückzuführen sind, und dies ohne weitere Transparenz nicht weiter untersucht werden kann.</sample>
    <sample id="304">Unacceptable sentences from the same matching, or sentences from a completely unrelated domain such as Wikipedia.</sample>
    <sample id="305">This work critically examines recent advances in Weakly Supervised Learning (WSL), questioning the common claim of achieving high performance solely on weakly labeled data. We demonstrate that current WSL methods crucially rely on clean validation data for effective generalization, a factor often overlooked. Our research addresses three key questions: the necessity of clean validation data, the required amount, and optimal utilization strategies. We find that WSL performance significantly degrades without clean validation samples and that only a small number (around 20 samples per class) are typically needed. However, direct fine-tuning on these clean samples consistently outperforms WSL approaches. Furthermore, allowing continuous fine-tuning on clean validation data enables simpler models to achieve comparable results to complex WSL methods. We conclude that the performance gains of WSL are often overestimated and advocate for transparent reporting of model selection criteria, comparison with few-shot learning baselines, and consideration of continuous fine-tuning as a strong baseline. Our code is publicly available.</sample>
    <sample id="306">This paper investigates the extent to which large language models (LLMs) can track entities and their state changes within a discourse, a crucial ability for understanding longer texts. Recognizing challenges in evaluating this ability due to potential shortcuts like pre-training biases and heuristic associations, the authors designed a novel task involving boxes and objects with state-changing operations. They employed 2-shot in-context learning with Flan-T5, GPT-3, and GPT-3.5 models.

Results revealed that most models simply repeated initial states, performing poorly even below a random baseline. Notably, text-davinci-003 showed some non-trivial tracking. Further analysis of the GPT series indicated that models trained on substantial amounts of code exhibited significantly better entity tracking capabilities. While fine-tuning smaller models like T5-base proved successful, randomly initialized models struggled even with direct supervision, highlighting the importance of pre-training. The authors acknowledge the need for further research to determine the generalizability of these findings, with additional results from GPT-4 detailed in the full paper.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework for characterizing design biases in NLP datasets and models by comparing their annotations with those of diverse real users. The study addresses the issue that existing NLP technologies often exhibit systematic performance differences across populations due to the positionality—the perspectives shaped by demographics and experiences—of model developers.

The framework re-annotates datasets with a large, demographically diverse group of annotators recruited through Lab in the Wild, an online experimentation platform. By comparing these annotations with existing datasets and models (including GPT-4, Perspective API, and others) using correlation scores, the research reveals that NLP datasets and models tend to align most closely with English-speaking countries and individuals with higher education. Conversely, certain populations, such as non-binary individuals, are often left behind.

The findings highlight the importance of acknowledging and mitigating positionality in NLP. Recommendations include documenting design choices, adopting a perspectivist approach to research, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The text does not mention the authors' university.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning benchmark dataset.</sample>
    <sample id="313">Drei.</sample>
    <sample id="314">Coordination structures are headed by the conjunction.</sample>
    <sample id="315">Die Prompts wurden von einer Studie inspiriert, in der sie menschlichen Probanden gegeben wurden.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="317">CodeIE reframes information extraction (IE) as a structure-to-structure code generation task, leveraging large code language models like Codex to overcome limitations of traditional text-to-text approaches (e.g., T5, GPT-3). These models struggle with mismatched input/output structures during inference, requiring extensive structured training data. CodeIE addresses this by converting IE into code generation, ensuring aligned structures between input and output.

The approach utilizes prompts that define functions for IE tasks (NER, RE), guiding the model to extract information and append it to structured lists. Experiments on multiple datasets demonstrate that CodeIE significantly outperforms baselines like UIE and GPT-3, particularly in few-shot settings. Analysis reveals that code language models exhibit lower perplexity on code-formatted inputs and generate fewer structural errors. Furthermore, CodeIE reduces the occurrence of extraneous labels and consistently outperforms GPT-3, highlighting the benefits of code-style prompts and code-pretraining for IE. The paper's findings offer insights into improving IE performance and provide publicly available code and paper.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu "DrBERT: Ein robustes vortrainiertes Modell für Französisch in den Bereichen Biomedizin und Klinik" vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir stellen DrBERT vor, das erste biomedizinische Modell in Französisch, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web gecrawlt wurden. Wir haben auch einen Vergleich von Modellen mit verschiedenen Vortrainingsszenarien und Datenquellen vorgestellt. Anschließend präsentieren wir unsere Ergebnisse für 11 biomedizinische und klinische Downstream-Aufgaben in Französisch. Und schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 hat sich BERT zu einem der effektivsten Ansätze zur Lösung von Natural Language Processing-Aufgaben entwickelt und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder ähnlichen enormen Leistungssteigerungen. Seitdem wurde dieses Modell in andere Sprachen wie Französisch mit CamemBERT und auch in Bereiche wie Biomedizin mit PubMedBERT und BioBERT und in den klinischen Bereich mit ClinicalBERT adaptiert, hauptsächlich aber auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an In-Domain-Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für den biomedizinischen Bereich. Wir haben uns also die Frage gestellt, welche die am besten geeigneten Datenquellen für eine breite Palette von Anwendungen sind und ob gecrawlte Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager des Universitätsklinikums Nantes gewonnen wurden. Anschließend fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell in französischer Sprache zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir zunächst vier Modelle von Grund auf: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB von NACHOS; eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen; und eine finale Version von ChuBERT mit einer Mischung aus 4 GB von NACHOS und 4 GB von klinischen Notizen. Ergänzend zu diesem Vergleich haben wir drei Modelle vorgestellt, die mit kontinuierlichem Vortraining trainiert wurden, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von CamemBERT und wurde auf einem 4-GB-Datensatz von NACHOS trainiert. Ein weiteres Modell basiert ebenfalls auf CamemBERT, wurde aber diesmal auf den 4 GB klinischer Notizen trainiert, und schließlich ein Modell, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB von NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu evaluieren, haben wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech-Tagging und Question Answering gesammelt. Diese Modelle werden mit sechs Basislinienmodellen verglichen, nämlich CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluierung zeigt, dass die Modelle am besten auf Aufgaben funktionieren, deren Daten der Natur der Daten entsprechen, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir stellen auch fest, dass die Verwendung von mehr Daten zu einer besseren Leistung führt. Insgesamt scheinen Modelle, die von Grund auf vortrainiert wurden, in den meisten Aufgaben eine höhere Leistung zu erzielen. Unser Experiment mit kontrolliertem Vortraining unter Verwendung der Gewichte und der Tokenisierung von CamemBERT, das auf einem 4-GB-Teilmengen-Datensatz von NACHOS trainiert wurde, zeigte vergleichbare Ergebnisse wie die von DrBERT 4 GB von Grund auf. Dies ist nicht der Fall für das Modell, das auf CamemBERT-Gewichten und -Tokenisierer basiert, das unter Stabilitätsproblemen leidet. Abschließend bietet unser proprietäres System in neun von elf Downstream-Aufgaben eine bessere Leistung und übertrifft global das Ergebnis des generischen Modells, hier CamemBERT. Wir stellen auch fest, dass spezialisiertere Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS gewonnen wurden, sind unter der MIT-Lizenz auf Hugging Face frei verfügbar, und alle Trainingsskripte finden Sie in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf den Austausch auf der Poster-Session in Toronto.</sample>
    <sample id="319">From-scratch pre-training and continual pre-training.</sample>
    <sample id="320">Adaptive overfitting was not observed.</sample>
    <sample id="321">The quality of simplification was assessed by analyzing the types of simplification transformations present in the corpus.</sample>
    <sample id="322">This paper investigates what text classifiers learn about morality, moving beyond the traditional binary "moral vs. immoral" classification. Drawing on Moral Foundation Theory, which posits five distinct moral foundations, the research explores how language models understand and express morality across different domains. Using the Moral Foundation Twitter Corpus (35,000 tweets across seven domains), the study applies explainable AI techniques to analyze language model behavior. The findings reveal that language models can recognize nuanced differences in moral expression across domains, even when the rhetoric is similar. For example, the concept of "subversion" is perceived differently in domains like #AllLivesMatter and #BlackLivesMatter, with language models associating it with negative connotations in the former and more positive ones in the latter. The research highlights the danger of applying a single model across diverse domains, cautioning that it can lead to misinterpretations of morality. Ultimately, the work underscores the importance of domain-specific understanding in moral reasoning for language models.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods combining Language Models (LMs) and Knowledge Representation Learning (KRL). Existing approaches often retrieve noisy entities and encode subgraphs and text in isolation, hindering performance. DHLK constructs a Heterogeneous Knowledge Graph (HKG) from multiple knowledge bases, employing a two-stage pruning strategy and KRL for optimized structure and knowledge representation. It leverages RoBERTa and Relation Mask Self-Attention (RMSA), inspired by RGAT, to dynamically remove irrelevant entities and model the HKG, incorporating semantic relationships. The method enhances QA context with HKG path information and utilizes a multi-layer perceptron (MLP) for final answer prediction. Experiments on CommonsenseQA and OpenBookQA, using ConceptNet, WordNet, and Wiktionary, demonstrate DHLK's superior performance compared to existing LM and HKG methods.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unser Papier über "Kompositionelle Verallgemeinerung ohne Bäume unter Verwendung von Multiset-Tagging und latenten Permutationen" geben. Dies ist eine Gemeinschaftsarbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und ungesehene Kompositionen von Phrasen zu verarbeiten, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte das Testen der kompositionellen Verallgemeinerung so aussehen. Wie üblich haben wir einen Trainingsdatensatz mit Äußerungen. In diesem Fall "Das Mädchen schlief." Und "Maria wusste, dass das Mädchen schlief." Diese Äußerungen sind mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Machine-Learning-Evaluierung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Seq2Seq-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Verallgemeinerung und erzeugen oft Ausgaben, die vom Input abgekoppelt sind. Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Input und Output wiederzugeben, wie sie in dem Beispiel farblich hervorgehoben sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht bereitgestellt und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal rechenintensiv sein. Typischerweise beinhaltet dies eine beträchtliche vorformelspezifische Vorverarbeitung der logischen Formen, beispielsweise zur Behandlung von Variablen. Das Erhalten von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren erfordern. In diesem Papier verwenden wir keine Bäume und führen ein neuronales Seq2Seq-Modell ein, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein. Unser Ansatz sagt die Ausgabe in zwei Schritten aus. Zuerst taggen wir jedes Input-Token mit einer ungeordneten Multimenge von Tokens, die im Output erscheinen werden. Nach dem ersten Schritt haben wir also alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation zu berechnen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage der Permutation ein, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell in etwa so: Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jede Position gehört. Für die erste Output-Position wählen wir einfach eines aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Vorgang fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumfreien Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen um einen großen Abstand bei der Verallgemeinerung auf tiefere Rekursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Erstens wird die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht angegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation NP-hart ist. Das liegt daran, dass dies mit dem "Traveling Salesman"-Problem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie sich bitte unser Papier an oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) architecture designed to improve representation learning by effectively utilizing multi-level unimodal semantic knowledge. Building upon BridgeTower, ManagerTower addresses its limitations by introducing "managers" in each cross-modal layer that adaptively aggregate insights from pre-trained unimodal experts (RoBERTa and CLIP-ViT base). This allows for more comprehensive cross-modal alignment and fusion compared to previous approaches that statically assign unimodal layers.

The architecture’s key innovation lies in its ability to dynamically weigh different levels of unimodal semantic knowledge, as demonstrated through visualizations showing distinct aggregation weight distributions for adaptive managers compared to static ones.  Evaluated on various downstream tasks with only 4 million images, ManagerTower achieves state-of-the-art performance, significantly outperforming BridgeTower and even surpassing larger models trained on more data. The code and models are publicly available.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper introduces a novel noise-resistant zero-shot video sentence localization method. Existing approaches generate pseudo-labels for training, but suffer from simplistic queries, misalignment between queries and video segments, and ignoring label noise. To address these issues, the proposed method generates structured pseudo-labels by first creating complex, free-form pseudo-queries using a pre-trained image captioning model (BLIP). Then, it generates pseudo-events by measuring frame relevance to queries, ensuring high relevance within events and low relevance outside. To mitigate label noise, the method weights samples based on prediction confidence and IoU, and refines pseudo-labels by incorporating high-confidence predictions. Experiments on ActivityNet Captions and Charades-STA demonstrate significant performance improvements over existing zero-shot methods, achieving state-of-the-art results. The approach leverages structured pseudo-label generation, event temporal structure modeling, and noise reduction techniques to achieve robust zero-shot video sentence localization.</sample>
    <sample id="330">Kumulatives Training hat in diesem Fall entweder gleich gut oder besser abgeschnitten als iteratives Training.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Transkripte von TED-Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">INK is a novel framework for Neural Machine Translation (NMT) that injects kNN knowledge to enhance generalization and performance. Addressing the limitations of traditional kNN-MT, which suffers from slow inference and static data stores, INK iteratively refines the NMT model's representation space. The framework employs a training loop where kNN knowledge guides an adapter to adjust representations, and updated representations asynchronously refresh a data store. This process aligns contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token using KL-divergence, effectively smoothing the representation space and mitigating sparsity. Crucially, INK eliminates the need for the data store during inference. Experiments on the WMT’19 German-English translation task demonstrate that INK outperforms state-of-the-art kNN-MT systems, achieving significant BLEU and COMET score improvements while requiring less memory and offering faster inference speeds.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This paper introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to represent out-of-vocabulary (OOV) words. Inspired by human learning, the method leverages word formation and association to infer OOV meaning. It constructs a Word Relationship Graph, mimicking lexical rules, where nodes represent words or wordpieces with corresponding embeddings. A two-level graph is formed around the OOV word, preserving wordpiece information while sampling neighbors to mitigate noise. A self-attention network assigns attributes to OOV nodes based on characters, followed by a concatenated Graph Attention Network to capture node-level representations. A readout block then generates a graph-level representation summarizing word formation. Contrastive learning, using NT-XENT, encourages proximity to relevant neighbors and background embeddings. Experiments demonstrate superior performance compared to baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models. The model's adaptability to various languages depends on the rationality of word decomposition, showing promise for agglutinative languages.</sample>
    <sample id="338">This research investigates the quality of human-annotated natural language explanations used to enhance model performance and reasoning. Challenging the assumption that these explanations are inherently beneficial, the work introduces a unified data structure and a novel evaluation metric, TREU, to objectively assess their helpfulness. TREU extends the simulatability score by evaluating the impact of explanations during fine-tuning. Experiments across five diverse datasets (CoS-E, ECQA, e-SNLI, ComVE) and two models (T5, BART) reveal that while explanations can improve model predictions, their utility is task-dependent and fine-tuning with explanations can lead to reliance on the explanation input itself. Notably, even explanations previously deemed "low quality" can still benefit models. TREU consistently ranks dataset qualities better than the simulatability score, demonstrating its sensitivity to task nuances like negation and counterfactual reasoning. The findings highlight the need for rigorous quality checks in human annotation and lay the groundwork for more effective human-AI collaboration in explanation generation.</sample>
    <sample id="339">Saarland University</sample>
    <sample id="340">ParaAMR is a novel, large-scale paraphrase dataset constructed using AMR (Abstract Meaning Representation) back-translation. Addressing the limitations of existing paraphrase datasets—high quality but limited scale (human-annotated) or large scale but lacking syntactic diversity (back-translation)—ParaAMR leverages AMR graphs to generate diverse paraphrases. The method involves parsing sentences into AMR graphs, randomly selecting a new focus node, modifying edges, and then generating text from the altered graph. This process ensures semantic similarity while introducing syntactic variation. ParaAMR contains 15 million source sentences with approximately 6.9 paraphrases per sentence. Evaluations demonstrate that ParaAMR maintains comparable semantic similarity to other back-translation datasets while exhibiting significantly higher syntactic diversity. Experiments on sentence embedding learning, syntactic control paraphrase generation, and few-shot learning demonstrate ParaAMR's effectiveness in improving performance across various NLP applications. The dataset and code are publicly available.</sample>
    <sample id="341">BLEU, average lagging und computational aware average lagging.</sample>
    <sample id="342">LiveChat is a novel, large-scale personalized dialogue dataset automatically constructed from live streaming videos on Chinese TikTok and Douyin. Addressing limitations of existing open-domain dialogue datasets—primarily text-sourced, small-scale, and lacking personalization—LiveChat leverages a unique reply-to-whom matching method to extract dialogues from audience comments. The dataset incorporates persona information, extracted through both manual labeling and rule-based/classifier approaches, enabling personalized dialogue generation and supporting multi-party conversations. LiveChat significantly surpasses existing datasets in scale, video sourcing, persona annotations, and average session length. Experiments on response modeling and addressee recognition demonstrate the benefits of persona profiles and longer sessions. Furthermore, evaluations of pre-trained dialogue models, particularly BART, reveal LiveChat's distinct domain. In-context learning experiments highlight the positive impact of demonstrations, though performance plateaus beyond 8 shots due to potential noise. LiveChat offers a valuable resource for advancing dialogue research, particularly in personalized and multi-party conversation scenarios.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natural Language Understanding Modelle greifen auf eine Vielzahl von Wissensquellen zurück, wie z. B. Wissen, das in ihren Parametern enthalten ist, typischerweise durch Vortraining erworben, und Wissen, das bei der Inferenz in den Eingaben bereitgestellt wird. Jüngste Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle vortrainingsbezogenes Wissen nutzen können, um die Aufgabe zu lösen. Natürliches Sprachverständnis erfordert jedoch oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel im Satz: "John sah den neu gewählten Präsidenten im Fernsehen." Die vortrainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität "John" ist, oder wer der neue Präsident ist, da sich der Präsident seit dem Vortraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainings- als auch Inferenzzeitwissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Kerneferenzauflösung aufgabe ein, die darauf abzielt, die Fähigkeit zu untersuchen, Wissen aus verschiedenen Quellen zu nutzen. Wir evaluieren den Datensatz mit menschlichen Studien Teilnehmern und etablierten Kerneferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag bei der Arbeit, Fälle in einem Gerichtshof entscheidend, war er glücklich, sich zu entspannen. Die Aufgabe hier ist es, die korrekte Entität zu identifizieren, auf die das Pronomen "er" sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätsspezifisches Wissen wie "Servin ist ein Richter". Und zweitens, Hintergrundwissen wie "Richter entscheiden Fälle in Gerichtshöfen". Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Wissensarten, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMUS definiert. Erstens haben wir die typische Einstellung: "Hintergrund-Vortraining", wobei angenommen wird, dass Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es eine "Hintergrund-Beide"-Einstellung, bei der Hintergrundwissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar ist. Letztlich gibt es eine "Hintergrund-Inferenz"-Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind. Letztere Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das zur Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der Vortrainingsdaten der Modelle ist. Zum Beispiel, weil seit der Zeit des Vortrainings neue Berufe entstanden sind. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den wahren Quellen steuern. In der "Hintergrund-Vortraining"-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker streben nach gewählten Sitzen in der Regierung" in den vortrainierten Parametern und im Inferenzzeit-Kontext enthalten ist, den wir das entitätsspezifische Wissen "Chichester ist ein Politiker" bereitstellen. In der "Hintergrund-Beide"-Einstellung stellen wir zusätzlich zum entitätsspezifischen Wissen auch Hintergrundwissen über Politiker in ihrem Inferenzzeit-Kontext bereit. In der "Hintergrund-Inferenz"-Einstellung stellen wir stattdessen den fiktiven Beruf "mirituer" bereit, anstatt Politiker, da "mirituer" unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir evaluieren den Datensatz sowohl mit menschlichen Studien Teilnehmern als auch mit etablierten Kerneferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der am besten abschneidenden Modelle in der schwierigsten Variante der "Hintergrund-Vortraining"-Einstellung. Ohne aufgabenspezifisches Training auf KITMUS leisten beide Modelle keine gute Arbeit. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref deutlich besser ab als bei einer zufälligen Wahl. Dies deutet darauf hin, dass die meisten beim Training auf generischen Kerneferenzauflösungsdatensätzen lernen, Oberflächenhinweise auszunutzen, die bei Tests auf KITMUS, wo solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die am besten abschneidenden Modelle nicht zuverlässig Hintergrundwissen integrieren können, das nur zur Inferenzzeit bereitgestellt wird. Zusammenfassend lässt sich sagen, dass viele Kerneferenzauflösungsmodelle nicht in der Lage sind, über Wissen aus verschiedenen Quellen zu schlussfolgern, ohne aufgabenspezifisches Training. Mit aufgabenspezifischem Training integrieren jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen. Selbst die am besten abschneidenden Modelle haben jedoch Schwierigkeiten, Hintergrundwissen, das nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren. Wenn Sie an weiteren Details interessiert sind, sehen Sie bitte unser Papier und den Datensatz sowie den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for compositional generalization in semantic parsing, achieving strong results without relying on explicit tree structures. The model addresses the challenge of generating logical forms for unseen, structurally deeper compositions, a task that often perplexes standard sequence-to-sequence models. Our approach operates in two stages: first, it tags each input token with a multiset of output tokens; second, it predicts a permutation to order these tokens correctly. A key innovation is a flexible permutation model that avoids hard constraints, enabling it to capture subtle linguistic nuances. We tackle the challenges of implicit input-output alignment and the NP-hard problem of finding optimal permutations through induced alignment during training and a GPU-friendly continuous relaxation technique, respectively. Experiments on the COGS benchmark demonstrate significant performance gains over existing tree-less models, particularly in generalizing to deeper recursion, while acknowledging ongoing challenges with other forms of structural generalization.</sample>
    <sample id="346">Das wird im Text nicht erwähnt.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren wurde vielfach dokumentiert, wie weit verbreitet soziale Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, sind. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich meist auf manuell erstellte Datensätze, die sehr zeitaufwändig zu kuratieren sind. Außerdem messen sie meist nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere demografische Gruppen oder Kontexte generalisieren oder einfach nur sehr allgemeine, breite Assoziationen erfassen, wie z. B. negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich keine Intersektionalität, d. h. die Vorstellung, dass facettenreiche soziale Identitäten Vorurteile verstärken und einzigartige Schadensursachen darstellen können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, durch Anweisungen optimierten LLMs sehr gut darin sind, Anweisungen und Prompts zu beantworten. Wir können das Modell also bitten, eine Persona zu generieren, d. h. eine Darstellung einer imaginären Person, indem wir einen Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede demografische Gruppe generalisiert werden kann, da wir einfach die gewünschte Identitätsmarkierung in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Man sieht sofort, dass die Ausgaben nicht offen negativ oder toxisch im herkömmlichen Sinne dieser Wörter sind, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie "exotisch" und "wie, Bezug nehmend auf eine faszinierende Region" bezeichnet. Und beide Frauen mit Farbhaut-Personas beziehen sich auf ihre Abstammung, während die Persona des weißen Mannes nichts dergleichen enthält. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts menschlichen Probanden gegeben wurden, was dazu führte, dass auch hier rassische Stereotypen aufgedeckt wurden. Dies ermöglicht auch einen direkten Vergleich zwischen den generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist "Marked Words", eine Methode zur Identifizierung der Wörter, die markierte Gruppen von nicht-markierten Gruppen unterscheiden, was ich gleich näher erläutern werde. Der Vorteil ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Marked Words-Methode greift auf das soziolinguistische Konzept der "Markierung" zurück, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie normalerweise tatsächlich den Begriff "Frau-Krieger" spezifizieren und den Begriff mit "Frau" markieren. Und im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen als unmarkiert und welche als markiert gelten, und vergleichen dann die Personas mithilfe der Fightin’ Words-Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen Fightin’ Words verwenden und die Log-Odds-Verhältnisse sowohl mit den weißen Personas als auch mit den männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter und des Lexikons betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas eine viel höhere Rate der Wörter aus dem Lexikon aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung der Wörter. Die Stereotyp-Wörter, die in den generierten Personas enthalten sind, sind wirklich nur die Wörter "groß" und "sportlich". Es sind also wirklich nur die positiven oder zumindest nicht-negativen Wörter. Und in der Tat erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht gut. Stattdessen werden wir uns daher auf die Ergebnisse der Marked Words-Methode stützen, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Narrative erleichtern. In unserer Analyse zeigen wir auf, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst umfassen die Top-Wörter unserer Gruppen Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Tropen wider, insbesondere für Frauen mit Farbhaut. Zum Beispiel umfassen die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebendig" und "kurvig", die sich auf einen Tropismus beziehen. Für asiatische Frauen sind die Wörter Dinge wie "zart" und "delikat" und "seidig", die sich auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen beziehen, die als sehr gefügig und unterwürfig angesehen werden und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies bezieht sich auf das "Strong Black Women"-Archetyp, das von einigen als schädlich angesehen wird, da es diese demografische Gruppe unter Druck setzt, angesichts gesellschaftlicher Hindernisse widerstandsfähig und stark zu sein. Anstatt also daran zu arbeiten, diese Hindernisse zu beseitigen, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen und anderen Schäden führt. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch eine intersektionale Perspektive verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir dies nicht tun. Und schließlich sollte es eine verstärkte Transparenz über Methoden zur Voreingenommenheitsminderung geben, da wir zum Beispiel nicht wissen, ob es sich um eine Art von seltsamer, übermäßig exzessiver Werteausrichtung oder vielleicht um einige andere Anti-Stereotyp-Methoden handelt, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank für Ihre Aufmerksamkeit. Ich wünsche Ihnen einen schönen Aufenthalt auf der ACL.</sample>
    <sample id="348">This paper, "Marked Personas," introduces a novel method for measuring stereotypes in large language models (LLMs) by leveraging their instruction-following capabilities. Unlike existing approaches relying on curated datasets or broad association measures, our method generates personas using prompts like "Imagine you are an [identity]. Describe yourself," enabling broad demographic coverage and intersectional analysis. We then employ "Marked Words," a sociolinguistic approach, to identify words that distinguish marked (minority) groups from unmarked (dominant) ones.

Our findings reveal that LLMs generate personas containing stereotypes, often masked as seemingly positive attributes. While lexicons fail to capture these subtle harms, Marked Words expose patterns like defining groups solely by their identity ("culture," "tradition") and perpetuating tropes (e.g., "exotic" for Middle-Eastern women, "strong" for Black women). These positive portrayals contribute to essentializing narratives and reinforce historical discrimination. We conclude by recommending researchers address positive stereotypes, adopt an intersectional lens, and advocate for increased transparency in bias mitigation techniques within LLMs.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo für unser Paper zu präsentieren: "Schutz des Urheberrechts großer Sprachmodelle für Embedding als Services durch Backdoor-Wasserzeichen". Lassen Sie uns zunächst den Hintergrund von Embedding als Services vorstellen. Aktuelle große Sprachmodelle wie GPT, LLAMA, PALM sind außergewöhnlich im Bereich des Verständnisses und der Generierung natürlicher Sprache. Embedding als Services ist einer der Services, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Embedding-API. Jüngste Arbeiten haben jedoch gezeigt, dass ein Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Services anbieten kann. Daher ist es notwendig, das Urheberrecht von Embedding als Services zu schützen. Um das Urheberrecht von Embedding als Services zu schützen, ist eine Lösung, ein Wasserzeichen in den Provider-Service einzubetten und zu erkennen, ob ein anderer Service das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Embedding als Services anwendbar sein. Zweitens sollte das Wasserzeichen die Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt sein oder der Angreifer kann das Wasserzeichen leicht entfernen. Schließlich muss das Wasserzeichen während des Modell-Extraktionsprozesses auf die Services des Angreifers übertragen werden. Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Embedding als Services anwendbar oder es fehlt ihnen an Übertragbarkeit. Daher schlagen wir in diesem Paper Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die auf Embedding als Services anwendbar ist. Lassen Sie mich nun die Details unseres Embedding Markers vorstellen. Embedding Marker besteht aus zwei Hauptschritten: Wasserzeicheneinfügung und Urheberrechtsverifizierung. Vor diesen Hauptschritten wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzbereich. Wir gehen davon aus, dass der Provider einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheneinfügung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Provider-Service sendet, zählt der Provider die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifizierung dient dazu, zu erkennen, ob ein Modell hinter einem anderen Service das Wortmark enthält. Wir konstruieren zunächst eine Hintertür und einen benignen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des benignen Datensatzes nicht zum Trigger-Satz gehören. Anschließend fordert der Provider die Embeddings vom Service des Angreifers mit dem Datensatz an. Die Cosine- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem benignen und dem Backdoor-Datensatz, die als Delta Cosine und Delta L2 definiert ist. Gleichzeitig wenden wir auch einen KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Provider den Wiki Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielen kann, während er gleichzeitig eine hervorragende Nutzbarkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf den vier Datensätzen [INAUDIBLE 4:39] PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie aus den Abbildungen hervorgeht, ist es schwierig, zwischen den Backdoor-Embeddings und den normalen Embeddings zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf eine Diskussion mit Ihnen.</sample>
    <sample id="350">This paper investigates the validity of "superhuman performance" claims in Natural Language Understanding (NLU) benchmarks like SuperGLUE and SQuAD. While models frequently surpass human scores on leaderboards, the authors argue that these achievements are often misleading due to flawed comparisons. Key issues include evaluating systems on full test sets while humans are assessed on small subsets, the presence of errors in ground-truth answers, and the reliance of models on spurious correlations that humans avoid. Furthermore, the paper highlights concerns about the methodology used to establish human baselines, including inadequate annotator compensation and a lack of transparency regarding annotator demographics and hiring processes. The authors contend that these factors undermine the scientific meaning of superhuman performance claims and propose recommendations for constructing more reliable benchmarks that accurately reflect true progress in NLU.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. The study addresses whether these models still perform well on modern data and identifies factors contributing to good generalization. To assess this, the authors created CoNLL++, a new dataset annotated with CoNLL-2003 guidelines but sourced from 2020 Reuters News. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++.

Findings indicate that transformer-based architectures, larger model sizes, and more fine-tuning examples are crucial for good generalization. The research explored two hypotheses for performance drops: adaptive overfitting and temporal drift. Adaptive overfitting was not observed, while temporal drift, caused by the increasing time gap between training and testing data, was confirmed as the primary cause of performance degradation. Despite this drift, the study concludes that CoNLL-2003 taggers still function effectively in 2023, highlighting the need for further research into improving model generalization.</sample>
    <sample id="352">Annotating behaviors in chat.</sample>
    <sample id="353">This paper addresses the challenge of input underspecification in natural language to code generation by introducing an interactive approach. Existing methods struggle when natural language descriptions lack crucial details. The authors propose a novel task: code generation through clarification questions (CodeClarQA). They create a synthetic dataset focusing on operation-level specifications, identifying missing operations using schema similarity analysis. Clarification questions are generated using templates (yes/no or multiple-choice).

The proposed pipeline includes a Clarification Need Predictor, Question Selector, and Code Generator. Experiments demonstrate that the task is more challenging than existing clarification question ranking tasks, and that clarifications generally improve code generation. While the interactive pipeline currently underperforms model-only approaches due to the difficulty of the clarification ranking task, analysis suggests that clarified key operations contribute to better code generation, with oracle clarifications leading to near-ground-truth predictions. The paper highlights areas for future improvement, including handling taxonomy ambiguities and incorporating argument values.</sample>
    <sample id="354">2020</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin in Informatik an der Stony Brook University. Ich möchte Ihnen unsere Arbeit vorstellen, die für ACL 2023 akzeptiert wurde und als Long Paper erscheint: "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem ist, das man in der Sprache untersuchen sollte. Einfach ausgedrückt ist kognitive Dissonanz eine Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, wie in diesem Beispiel, in dem eine Person sagt: "Ich weiß, dass Zigaretten mich töten können" und dann hinzufügt: "Ich habe nach dem Meeting ein paar Zigaretten geraucht." Diese Überzeugung und Handlung sind inkonsistent und stehen in einem Dissonanzverhältnis. Wenn die Person dann hinzufügt: "Ich glaube, ich könnte meinen Job nicht ohne sie behalten", rechtfertigt dies das zweite Vorkommnis und schafft eine Konsonanzbeziehung. Obwohl Dissonanz ein sehr häufiges Phänomen bei alltäglichen Entscheidungsprozessen ist, findet man sie selten in der Sprache im Vergleich zu anderen Arten von Diskursbeziehungen. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Wertvorstellungen zu verfolgen und Einstellungen in der Bevölkerung zu beobachten. Hohe kognitive Dissonanz steht auch in Zusammenhang mit Angststörungen und kann uns helfen, die psychische Gesundheit besser zu verstehen. Die Untersuchung von Dissonanz, die in der Sprache ausgedrückt wird, kann auch hilfreich sein, um Extremismus und die Polarisierung schutzbedürftiger Gruppen zu verstehen. Schließlich ist es wichtig, kognitive Dissonanz zu verstehen, um persönliche kognitive Stile von Individuen zu verstehen und Entscheidungsprozesse besser zu verstehen. Um das Ziel zu erreichen, eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir haben einen Dissonanz-First-Ansatz verwendet, wie in der Flussdiagramm hier zu sehen ist. Tweets wurden mithilfe des PDTB-Parsers verarbeitet, und Paare von Diskursbausteinen wurden gemäß den in unserem Paper beschriebenen Richtlinien annotiert. Wie man hier sehen kann, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1.000 Beispiele für Paare von Diskursbausteinen gesammelt hatten, führten wir ein Training für einen anfänglichen Klassifikator durch, der nur auf 43 Beispiele für Dissonanz trainiert wurde. Nicht überraschenderweise schnitt der Klassifikator nicht wesentlich besser als zufällig ab. Angesichts der geringen Häufigkeit von Dissonanz und des Fehlens eines vorherigen Datensatzes stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und Active Learning, um Daten zu annotieren, sodass mehr dissonante Beispiele mit weniger Annotationen gesammelt werden können, wodurch die Gesamtkosten der Annotation gesenkt und gleichzeitig die Dissonanzerkennung verbessert wird. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den Active-Learning-Prozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: Dissonanz-Stands-Klassifikation ohne Themenbezug, eine Aufgabe, die feststellt, ob zwei Debattenaussagen verschiedener Personen übereinstimmen oder nicht übereinstimmen, unabhängig vom Thema (genannt "Debate"), und binäre Klassifikation von Erweiterungs- und Vergleichsklassen von PDTB, da diese eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind (genannt "CE"). Wir stellen fest, dass die Übertragung der Zero-Shot-Performance auf dem annotierten Datensatz bereits deutlich besser ist als zufällig, wobei die beste AUC bei .62 liegt. Darüber hinaus stellen wir fest, dass das iterative Feintuning sowohl der CE- als auch der Debate-Aufgaben zu einer deutlich besseren Zero-Shot-Performance führt. Dies ist das Modell, das wir zur Kaltstartphase des Active Learning verwenden. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des Active Learning zu aktualisieren. "Kumulativ" akkumuliert alle Daten, die von den Active-Annotationen bisher gesammelt wurden, während "Iterativ" das Modell trainiert, indem es auf den neuesten Datensatz trainiert wird, der gesammelt wurde. Bei den verschiedenen Strategien stellten wir fest, dass Kumulativ bei allen Strategien gleich gut oder besser abschneidet als Iterativ. Als Nächstes verwenden wir eine Probability-of-Rare-Class-Strategie – PRC – um hauptsächlich die Beispiele auszuwählen, bei denen das aktuelle Modell höchstwahrscheinlich dissonant ist. Wir vergleichen dies mit anderen State-of-the-Art-AL-Strategien, die in der Community üblicherweise verwendet werden. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser funktioniert als andere State-of-the-Art-Strategien, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei weiteren AL-Runden mit den beiden besten Strategien auf 0,75 AUC für die Dissonanzklassifikation verbessert wird, was die beste Leistung ist, die wir bisher in dieser Aufgabe erzielt haben. Wir überprüfen auch die Durchführbarkeit jeder Strategie für die Annotationsqualität und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für die seltene Klasse geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Seltenheitsklassenakquise ist und das Kaltstarten von AL mit einer angemessen gestalteten Transfer-Learning-Aufgabe hilft und deutlich verbessert. Wir stellen auch fest, dass iterative Updates für das Transfer Learning aus einem anderen Bereich nützlich sind, während bei domänenspezifischen Active-Annotationen kumulative Updates von Vorteil sind. Hier sind die Links zu unserem Kern-Datensatz und unserem Paper. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität an, die von Alexander Koller und Ivan Titov besucht wird.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf.</sample>
    <sample id="359">Wait-k strategy, Local Agreement, and state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">CounterComp addresses the challenge of compositional generalization in multi-step quantitative reasoning for question answering, where state-of-the-art neural models struggle, particularly with complex arithmetic operations. The approach identifies that models often memorize spurious patterns based on repeated tokens in the input. CounterComp leverages the interchangeability of question components to mine counterfactual scenarios – positive examples where interventions don't change the output, and negative examples where they do. An auxiliary metric learning loss, incorporating a dynamic margin based on the intervention extent, is then added to the training procedure. Experiments with three baselines demonstrate consistent performance improvements, especially for reasoning steps exceeding two, both in-distribution and, crucially, out-of-distribution. CounterComp also encourages the model to attend to more relevant tokens, leading to better operational understanding.</sample>
  </task>
</testset>