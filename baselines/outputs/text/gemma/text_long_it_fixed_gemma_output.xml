<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Large-scale web crawl data, including news media such as The New York Times, Los Angeles Times, and The Guardian.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model addressing reading order challenges in Visually-rich Document Understanding (VrDU). Existing models often rely on global 1D positions, which can be problematic. LayoutMask utilizes "local 1D position" representing in-segment token order, forcing the model to infer global reading order by integrating 1D, 2D positions, and semantic information to enhance text-layout interactions.

LayoutMask incorporates two novel masking strategies within the Masked Language Modeling (MLM) objective: Whole Word Masking, which promotes context understanding by eliminating semantic relations within words, and Layout-Aware Masking, which prioritizes masking the first and last words of segments to encourage cross-segment context analysis. Additionally, a new Masked Position Modeling (MPM) objective is introduced, requiring the model to predict masked 2D positions, fostering spatial reasoning and text-layout interaction.

Experiments demonstrate that LayoutMask's local 1D position outperforms global 1D position on FUNSD and SROIE datasets, particularly in scenarios with complex layouts and misleading numbers. These results highlight the effectiveness of LayoutMask in learning robust layout representations and improving VrDU performance by promoting deeper text-layout interactions. The paper details these findings and encourages further exploration through the full paper and accompanying posters.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo target specifico, come persone con problemi di lettura o non madrelingua. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi. L'esempio qui, potete vedere una coppia di frasi parallele allineate di una frase tedesca complessa e della sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche come potete vedere nell'esempio, come la sostituzione lessicale, l'eliminazione delle clausole, il riordinamento o l'inserimento di parole. Ora proponiamo il nostro nuovo corpus DEPLAIN perché negli ultimi anni ci sono stati alcuni problemi con i corpus esistenti. Ad esempio, questi corpus sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nei loro allineamenti. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è diviso in due sottocorpus: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti. Ciò si traduce in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include domini diversi e allineiamo anche tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di allineamento automatico. In totale otteniamo 30.450 coppie di frasi. Abbiamo analizzato ulteriormente le nostre coppie di frasi, ad esempio sul tipo di semplificazione. Come potete vedere qui, i testi biblici sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per studenti di lingue. A tutti i livelli, per esempio la semplificazione lessicale, la semplificazione della struttura, anche a livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN ha un'elevata varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molti più riordinamenti e aggiunte di parole rispetto a quelli che abbiamo nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molte più riformulazioni. Vediamo ora cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro set di dati DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma nel contesto della traduzione automatica, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre gli allineamenti delle frasi in entrambi i documenti. Ma nel nostro caso, stiamo cercando di estrarre gli allineamenti tra le frasi di due documenti paralleli che hanno lo stesso linguaggio, lo stesso contenuto, ma sono a un livello di complessità diverso. E ora che abbiamo il nostro set di dati DEPLAIN, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcuni dei metodi di allineamento proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e il codice per eseguire i nostri esperimenti nell'articolo. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco è il metodo MASSalign. Potete anche trovare il codice per eseguire questo metodo sui vostri documenti nell'articolo. Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è un caso di semplificazione automatica del testo mediante l'affinamento dei modelli linguistici per produrre testo semplificato dal testo di input complesso. Abbiamo messo a punto due modelli diversi. Abbiamo messo a punto il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche messo a punto il normale modello base mBART per produrre semplificazioni a livello di frase. Potete anche trovare tutti i checkpoint e potete trovare maggiori dettagli sui punteggi e sulle metriche di valutazione dei nostri esperimenti nell'articolo. Abbiamo concluso che questo affinamento di base poteva produrre o ottenere punteggi migliori rispetto ai punteggi di base, e abbiamo proposto questi risultati come benchmark di base per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">This paper introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," proposing a novel many-to-many summarization framework that generalizes both multilingual and cross-lingual summarization. This approach trains a single model to summarize documents from any source language into any target language. Preliminary analyses demonstrate that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional methods.

To realize this framework, the authors present PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training strategy. This includes meta pre-training for sentence generation from noisy inputs, cross-lingual pre-training for translation from noisy parallel sentences, and task-specific pre-training using pseudo many-to-many summarization samples.

Experiments on the WikiLingua dataset, using English, French, Hindi, Chinese, Thai, and Turkish, show that PISCES outperforms baselines like mBART-50 and mT5. Ablation studies confirm the effectiveness of each pre-training stage, and human evaluations further validate PISCES's superior performance. The work highlights the benefits of the many-to-many summarization paradigm and provides a strong foundation for future research in multilingual and cross-lingual summarization.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">ABC-Eval reduces subjectivity by explicitly annotating whether model responses express certain behaviors (e.g., irrelevant information, contradictions) rather than relying solely on overall ratings.</sample>
    <sample id="9">Clean validation samples.</sample>
    <sample id="10">Providing language models with more or better background knowledge.</sample>
    <sample id="11">This research investigates whether large language models (LLMs) genuinely understand humor by leveraging data from *The New Yorker* Caption Contest. While LLMs can generate and sometimes explain jokes, their true comprehension remains questionable. To probe this, the authors created a benchmark with three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is funny.

The dataset comprises over 700 cartoons, annotated with descriptions, entities, and human-written joke explanations. Experiments reveal a significant performance gap between LLMs and humans across all tasks. Even with image descriptions provided to models like GPT-4, accuracy on matching and ranking remains substantially lower than human performance. Furthermore, human evaluations consistently favored human-generated joke explanations over those produced by GPT-4, highlighting inaccuracies and misinterpretations.

The study demonstrates that while LLMs can mimic aspects of humor, they struggle with nuanced understanding and contextual reasoning. The released dataset and leaderboard aim to foster further research into humor understanding in AI, ultimately contributing to more sophisticated and reliable language models.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Adaptive inference techniques like Multi Model and Early Exit aim to reduce the computational cost of large language models by utilizing smaller models for simpler inputs. While Multi Model offers versatility, it suffers from overhead. Early Exit, though memory-efficient and faster, faces a performance bottleneck due to conflicting gradients—where multiple classifiers sharing model parameters interfere with each other during training. This work investigates this phenomenon, demonstrating that separate Multi Model classifiers outperform Early Exit classifiers by an average of 2.3%, particularly for early classifiers.

To address this, we introduce SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that isolates gradient updates, preventing conflicting gradients by ensuring each transformer layer receives updates only from its subsequent classifier. Experimental results show SWEET significantly closes the performance gap between Early Exit and Multi Model, achieving superior speed/accuracy trade-offs, especially at high inference speeds. While some later classifiers may experience minor performance regressions, SWEET overall represents a substantial improvement for Early Exit architectures, motivating further research into specialized fine-tuning algorithms for adaptive inference.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come sapete, diverse teorie e approcci basati su corpora assumono strutture di dipendenza differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione, Lisa, Bart e Maggie, fa sì che il primo congiunto sia la testa dell'intera struttura coordinata. In questo caso, Lisa. Un approccio simile è assunto nella teoria del testo del significato di Igor Mel'čuk, dove anche in questo caso l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici. Giusto. Singolano uno dei congiunti. Ora, questi sono approcci asimmetrici alle strutture di coordinazione, come l'approccio praghese. L'approccio guidato dalla congiunzione, assunto nei treebank di dipendenza di Praga, dove le strutture di coordinazione sono guidate dalla congiunzione. Quindi, otteniamo delle dipendenze da fine a tutti i congiunti. E infine, c'è anche un approccio multi-testa che viene utilizzato, ad esempio, nella Word Grammar di Hudson, dove dicono che tutti i congiunti sono teste della struttura coordinata. Quindi otteniamo dipendenze dal governatore. Qui ama tutti i congiunti separatamente: Lisa, Bart e Maggie. Ora, l'obiettivo di questo articolo è produrre un argomento nuovo per le strutture di coordinazione simmetriche, come queste due, e contro le strutture di coordinazione asimmetriche, come queste due. OK. L'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò sulla base di questi esempi. Quindi, in inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani. Quindi "Marge ha letto ieri" va bene perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri" è molto peggiore. Perché qui tra il verbo e l'oggetto diretto c'è un avverbio: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo. Perché allora può essere spostato nella posizione dopo l'avverbio. Questo è illustrato qui. Quindi entrambe queste frasi vanno bene. "Marge ha letto questo libro assolutamente affascinante sulle api ieri." Va bene, invece di "esso", abbiamo questo lungo NP. Ma va bene anche dire: "Marge ha letto ieri questo libro assolutamente affascinante sulle api." Quindi il ragionamento è che questo è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Quindi questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Quindi qui abbiamo una dipendenza da "leggere" all'avverbio di lunghezza 7 misurata in parole e da "leggere" a "libro" di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6 è molto più corto. Ecco perché suona abbastanza bene. Giusto? Viola un principio, ma soddisfa un altro. Ok. Quindi abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedere l'articolo "Perché non useresti le dipendenze universali" e queste statistiche confermano l'osservazione fatta più volte prima che i congiunti di sinistra tendano ad essere più corti. Quindi, "sale e pepe" e non "pepe e sale", misurato in sillabe. E, anche l'osservazione che è stata fatta nell'analisi sintattica che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto di sinistra preferisce essere il primo, più forte, giusto? Quindi la proporzione è maggiore del congiunto di sinistra corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Giusto? Quindi il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa" quindi è il governatore è a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo una coordinazione di due verbi e non ci sono esterni, governatore esterno. In tali casi, il congiunto di sinistra preferisce essere più corto; la differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "risero" governa la coordinazione Ted e Ned, questo effetto scompare. Quindi abbiamo mostrato misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale e in parole la colonna destra. Quindi mi concentrerò sulla destra. Ciò che vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente, con la differenza assoluta in parole, e lo stesso è osservato quando non c'è governatore come nella coordinazione di frasi. Ma quando il governatore è a destra questa tendenza scompare. E dimostriamo nel documento come questo fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture di coordinazione simmetriche, come queste due. Quindi vedi l'articolo per gli argomenti completi. E parla con noi alla sessione poster. Grazie.</sample>
    <sample id="15">Tre.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per studenti di lingue.</sample>
    <sample id="17">Multimodal Relation Extraction (MRE) addresses the limitations of traditional text-based relation extraction by incorporating visual information, particularly relevant in scenarios like social media where textual context can be ambiguous. However, current MRE methods suffer from internal-information over-utilization (relying on irrelevant text parts) and external-information under-exploitation (insufficient context even with visuals). This work proposes a novel framework to tackle these issues through simultaneous information subtraction and addition.

The framework constructs a unified cross-modal graph (CMG) from text and image scene graphs, then employs a Graph Information Bottleneck principle to refine features by pruning irrelevant nodes and edges (internal-information screening). To compensate for remaining information gaps, multimodal topic features are integrated via an attention mechanism (external-information exploitation). Experiments on a standard MRE dataset demonstrate significant performance improvements over existing baselines, highlighting the contributions of both information screening and topic enrichment. Further analysis reveals that internal-information screening is more beneficial when text and vision are highly relevant, while external-information exploitation is more effective when relevance is low, demonstrating the framework's adaptability to varying input conditions. The proposed approach offers a robust solution for MRE by strategically managing information flow across modalities.</sample>
    <sample id="18">"Salt and pepper" not "pepper and salt".</sample>
    <sample id="19">Open-domain question answering (ODQA) typically employs a two-stage retrieval and reader framework, facing challenges due to the massive Wikipedia corpus (26 million documents, 20GB), large index files (65GB), and computationally intensive language models. This work surveys recent advancements aimed at achieving efficient ODQA systems with reduced memory footprint, faster inference, and comparable performance.

The survey explores one-stage frameworks like retrieval-only and generator-only systems, alongside efficient tactics across several aspects. These include approximate nearest neighbor search for faster retrieval, adaptive computation (skip reading) for efficient reading, and techniques like document filtering, embedding compression, and product quantization to reduce index size. Model size reduction strategies involve lightweight models, parameter sharing, and one-stage architectures combining retrieval and reading.

Analysis reveals that retrieval and reader systems offer a balanced trade-off, while retrieval-only systems prioritize speed at the cost of index size, and generator-only systems struggle with model size and performance. The survey concludes with insights for resource-constrained scenarios, suggesting index reduction or model size optimization, and highlights future research directions including deployment on low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">DEPLAIN-apa è basato su testi di notizie.</sample>
    <sample id="22">A better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Recent advances in text-to-image models have yielded impressive image generation capabilities, yet these models often struggle with accurately rendering text. This work investigates the root cause of this issue, focusing on the text encoders used in models like Imagen. We find that standard subword tokenization methods, such as those employed by T5, hinder accurate spelling due to their inability to directly access character-level information. Specifically, T5 exhibits surprisingly low spelling accuracy, particularly for frequent words, as the SentencePiece algorithm represents them with fewer, larger subword units. In contrast, Byte-level models like ByT5, which operate on individual bytes, demonstrate near-perfect spelling accuracy. To address this, we propose a simple yet effective solution: augmenting the existing text representation with a character-aware representation from a smaller ByT5 model. This approach, adding only approximately 5% to the text encoder's parameter count, significantly improves both text rendering and overall image generation quality within the Imagen framework. While the diffusion model can still introduce errors, this strategy represents a practical and efficient method for enhancing text rendering in text-to-image models. We introduce the WikiSpell and DrawText benchmarks to evaluate text-only and text-to-image models, respectively.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">They extracted statistics about coordination from the enhanced version of the Penn Treebank, measuring length in characters, syllables, and words, and observed the tendency for the left conjunct to be shorter based on whether the governor was on the left, absent, or on the right.</sample>
    <sample id="26">Il classificatore base, addestrato su soli 43 esempi di dissonanza, non ha funzionato molto meglio del caso.</sample>
    <sample id="27">Non specificato.</sample>
    <sample id="28">Bob e Alice.</sample>
    <sample id="29">Formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a novel and effective ensemble learning framework for large language models (LLMs), leveraging pairwise ranking and generative fusion. Recognizing that the optimal LLM selection varies significantly across different input examples, despite leaderboard rankings, LLM-Blender aims to improve performance beyond relying on a single top-performing model. The framework operates in two stages: first, it runs *n* LLMs and uses a *PairRanker* module to compare all candidate outputs. PairRanker employs a cross-attention mechanism to analyze subtle differences between candidate pairs alongside the input, outperforming methods that evaluate candidates individually. The PairRanker generates a comparison matrix, which is aggregated (using max logits or bubble sort) to determine a ranking of candidates. Second, a *GenFuser* module takes the top *K* (e.g., three) ranked candidates and fuses them into a final output using a sequence-to-sequence model.

To facilitate evaluation, the authors introduce MixInstruct, a new dataset comprising existing instruction datasets and candidate outputs from 11 open-source LLMs, evaluated using automatic metrics (BERTScore, BLUERT, BARTScore) and human judgment (ChatGPT). Experiments demonstrate that LLM-Blender consistently outperforms leading models like Open Assistant and Vicuna across all metrics, achieving significant improvements in 68-76% of examples. The codebase and dataset are publicly released to encourage further research in LLM ensemble learning.</sample>
    <sample id="31">John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="33">Il framework NLPositionality confronta le annotazioni di utenti reali con dati e modelli esistenti utilizzando un punteggio di correlazione R di Pearson, analizzando come le previsioni e le etichette dei modelli e dei dataset si allineano con diversi gruppi demografici di annotatori.</sample>
    <sample id="34">CREST is a novel joint framework for rationalization and counterfactual text generation. It combines selective rationalization, which highlights key tokens, with counterfactual generation, which edits inputs to alter decisions. CREST first generates counterfactuals by masking a rationale derived from an input and prepending a gold label, then uses a masked language model to fill in the masked portions. Human evaluations demonstrate CREST produces more valid and natural counterfactuals than existing methods like MiCE.

Beyond counterfactual generation, CREST introduces CREST-Rationalization, a training approach that leverages both factual and counterfactual examples. This involves a shared rationalizer that highlights meaningful rationales for both input types, guided by a regularization term encouraging similarity to original CREST-generated rationales. Experiments on IMDB and SNLI datasets show CREST-Rationalization achieves state-of-the-art results, particularly on out-of-domain data, demonstrating improved downstream model performance.

Finally, CREST-Rationalization generates interpretable rationales, exhibiting high plausibility and, crucially, strong counterfactual simulability – the ability of an explanation to influence a classifier's decision when paired with a corresponding contrastive edit. This work highlights the potential of combining rationalization and counterfactual generation for improved model interpretability and robustness.</sample>
    <sample id="36">This work introduces Language-Specific Layers (LSLs) to enhance multilingual machine translation (MMT) while maintaining constant inference costs. MMT offers scalability, speed, and benefits for low-resource languages, but faces limitations in per-language capacity. LSLs address this by adding language-specific transformer layers, allowing the model to selectively activate relevant sublayers at inference time based on the source or target language.

The paper details a novel approach to automatically learn the optimal placement of these LSLs within the encoder. By training a large model with shared, source, and target weights for each layer, the authors analyze weight distributions to identify the most impactful locations for language-specific specialization. A simple selection criterion based on the largest weight determines the final architecture.

Experiments on WMT21 news translation across 10 languages (including Swahili) demonstrate significant improvements over baseline transformers and language adapter approaches, as measured by chrF, spBLEU, and COMET. Notably, LSLs yield substantial gains for low-resource languages, with statistically significant improvements observed in 84 out of 90 translation directions. The proposed architecture achieves superior performance with faster inference speeds compared to existing methods, showcasing the effectiveness of learned LSL placement for efficient and accurate multilingual translation.</sample>
    <sample id="37">Lo studio precedente con i soggetti umani ha anche fatto emergere stereotipi razziali.</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">Uno.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a novel, large-scale Persona Commonsense Knowledge Graph developed in collaboration with Sony Group Corporation, designed to enhance narrative understanding and generation in NLP systems. It comprises approximately 3,800 personas and 40,000 attributes, forming over 100,000 personal inferences and demonstrating rich interconnections between personas. The graph's relations are structured across three dimensions, incorporating interactivity and distinctiveness. Constructed through a three-step process involving persona selection, attribute induction from commonsense graphs and language models, and crowdsourced annotation with AI assistance (InstructGPT-3), PeaCoK achieves high-quality relation annotations.

Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving comparable performance to larger language models like GPT-3 and GPT-3.5 in persona attribute inference. Furthermore, integrating PeaCoK into a dialogue generation system (P²Bot) significantly improves dialogue quality, as evaluated by human raters, surpassing baselines and demonstrating the benefits of persona-centric commonsense knowledge over general social knowledge. The study reveals that dialogue consistency and engagement increase with greater shared attributes between speakers, underscoring the value of PeaCoK’s interconnected persona knowledge for creating more compelling narratives. The paper and associated resources are publicly available.</sample>
    <sample id="42">Non specificato.</sample>
    <sample id="43">Non specificato.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti perché confronta le annotazioni di utenti reali con i modelli e i dataset, invece di concentrarsi solo sull'accordo tra annotatori o sulla modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="45">Le persone generate.</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando all'Università di Washington. Oggi vi presento il nostro lavoro "Dal pretraining dei dati ai modelli linguistici alle attività downstream: tracciare i percorsi dei pregiudizi politici che portano a modelli NLP ingiusti". I modelli linguistici vengono addestrati su dati di web crawling su larga scala. I media di notizie politiche sono ben rappresentati nei loro dati di pretraining. Secondo un sondaggio del Corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, eccetera, sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un beneficio misto per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di imparare da diverse prospettive, il che celebra la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di attività downstream. A tal fine, proponiamo di indagare la pipeline di propagazione dei pregiudizi politici dai dati di pretraining ai modelli linguistici alle attività downstream, chiedendo specificamente le seguenti domande: Innanzitutto, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbe avere il pretraining dei dati su tali pregiudizi politici? In secondo luogo, come si comportano i modelli linguistici con diversi orientamenti politici sulle attività downstream e ciò potrebbe portare a problemi di equità nelle applicazioni NLP?

Nello specifico, abbiamo prima proposto di sollecitare i modelli linguistici con diversi formati di sollecitazione utilizzando questionari politici come il test della conferenza politica. Ciò ci assicura di poter eseguire una valutazione automatica ben fondata sulla letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, innanzitutto, i modelli linguistici hanno orientamenti politici diversi. Occupano tutti e quattro i quadranti sul campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e le serie GPT sono generalmente più socialmente liberali rispetto alle serie BART e alle loro varianti.

In secondo luogo, miriamo a indagare in quale misura i pregiudizi politici dei modelli linguistici provengano effettivamente dai dati di addestramento. Potremmo condurre un esperimento controllato pre-addestrando ulteriormente i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente suddivisi nel loro orientamento politico. Pre-addestrando ulteriormente i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico corrispondono anche a uno spostamento. Ad esempio, per RoBERTa ulteriormente addestrato sul corpus Reddit di sinistra, possiamo vedere un notevole spostamento liberale in termini dei suoi pregiudizi politici. E cerchiamo anche di indagare se i modelli linguistici possono cogliere la polarizzazione che è prevalente nella nostra società moderna. Dividiamo i corpora di pretraining in prima e dopo il 45° presidente degli Stati Uniti e addestriamo separatamente i modelli linguistici sui due diversi corpora temporali. Possiamo vedere che i modelli linguistici avevano generalmente un orientamento politico che era più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società.

Infine, valutiamo i modelli linguistici con diversi orientamenti politici sul rilevamento dell'incitamento all'odio e sul rilevamento di notizie false per applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Vediamo che se indaghiamo sulle prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o orientamenti politici dei media, possiamo vedere un modello. Ad esempio, per il rilevamento dell'incitamento all'odio, i modelli linguistici di sinistra sono migliori nel rilevare l'incitamento all'odio che prende di mira gruppi sociali minoritari, ma sono peggiori nel rilevare l'incitamento all'odio che prende di mira gruppi più potenti nella nostra società. E viceversa, i modelli linguistici di destra sono migliori nel rilevare l'incitamento all'odio che prende di mira bianchi e uomini, ma peggiori nel rilevare l'incitamento all'odio nei confronti di neri, LGBTQ plus e altre comunità minoritarie. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli linguistici di sinistra sono migliori nel rilevare la disinformazione dai loro orientamenti politici opposti e viceversa. Mostriamo ulteriormente molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici forniscono effettivamente previsioni diverse sugli esempi di incitamento all'odio e disinformazione in base alle loro categorie sociali. Ci sono un sacco di altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che esiste un problema di equità molto urgente per quanto riguarda i pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di destra dovessero essere ottimizzati per l'incitamento all'odio o la disinformazione o qualsiasi altra cosa e implementati su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere emarginate e l'incitamento all'odio nei confronti dei gruppi minoritari potrebbe semplicemente dilagare senza alcun controllo. Questo ha suonato l'allarme affinché riconosciamo e affrontiamo i problemi di equità derivanti dai pregiudizi politici dei modelli linguistici.

Un po' di discussione. Vorremmo anche evidenziare che esponiamo il dilemma unico per quanto riguarda i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di addestramento dei modelli linguistici, il pregiudizio si propagherà dai dati di pretraining ai modelli linguistici alle attività downstream, creando in definitiva problemi di equità. Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione. È incredibilmente difficile determinare cosa sia effettivamente neutro e debba essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del carrello elettrico.

Ok, penso che sia tutto per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">David Vilar e i suoi colleghi di Google Translate.</sample>
    <sample id="49">Fino a 1024 token.</sample>
    <sample id="50">DEPLAIN is a new corpus designed to address limitations in existing resources for German text simplification. It comprises two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (diverse domains, 750 documents, ~30,450 sentence pairs). Both were created with manual alignment, addressing the error-proneness of automatically aligned corpora. Analysis reveals varying simplification strengths across domains (Bible texts being more simplified) and a diverse range of simplification transformations, with DEPLAIN-apa exhibiting more reorderings and additions, while DEPLAIN-web features more rephrasings.

The corpus enables two key use cases. First, it serves as a gold standard for evaluating automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Second, DEPLAIN facilitates automatic text simplification through fine-tuning language models. Long-mBART was fine-tuned for document-level simplification, and base mBART for sentence-level simplification. These fine-tuned models outperformed baseline scores, establishing a benchmark for future research in automatic German text simplification. The corpus, along with adaptation codes and model checkpoints, is publicly available, promoting further advancements in the field.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper addresses the challenge of detecting cognitive dissonance in language, a rare phenomenon with implications for understanding disagreement, belief shifts, mental health, and polarization. Cognitive dissonance, defined as the inconsistency between beliefs and actions, is sparsely expressed in text, posing a significant hurdle for automated detection. To create a resource for this task, we conducted a large-scale annotation of discourse unit pairs, finding dissonance in only 3.5% of cases.

Facing the problem of absolute rarity, we explored transfer learning and active learning (AL) to efficiently annotate dissonant samples. We leveraged transfer learning from related tasks—dissonance stance classification ("debate") and discourse relation classification ("CE")—finding that fine-tuning CE followed by debate significantly improved initial performance. Subsequently, we employed a Probability-of-Rare-Class (PRC) strategy for AL, which prioritizes examples likely to be dissonant. Compared to other state-of-the-art AL methods, PRC demonstrated superior performance, achieving an AUC of 0.75 after multiple rounds. While PRC examples proved challenging for annotators, it proved most effective for rare class acquisition. Our findings highlight the utility of combining transfer learning for cold-starting and PRC for efficient rare class annotation, demonstrating a practical approach to tackling low-resource NLP problems.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza riaddestramento o adozione di architetture specifiche per SimulST.</sample>
    <sample id="56">Non specificato.</sample>
    <sample id="57">Senza un addestramento specifico su KITMUS, i modelli non funzionano bene. Con l'addestramento specifico, alcuni modelli funzionano bene, ma hanno ancora difficoltà con le conoscenze fornite solo al momento dell'inferenza.</sample>
    <sample id="58">Background-Pretrain, Background-Both, Background-Inference.</sample>
    <sample id="59">This work introduces DrBERT, the first open-source biomedical language model in French. Addressing the scarcity of French biomedical NLP resources, we trained DrBERT on NACHOS, a large dataset of medical web crawls, building upon the RoBERTa architecture. We conducted a comprehensive comparison of DrBERT against several models, including a clinical model (ChuBERT) trained on anonymized hospital data, and models utilizing continual pre-training strategies based on CamemBERT and PubMedBERT. Our evaluation across 11 French biomedical and clinical downstream tasks (NER, classification, POS tagging, QA) revealed that models perform best on tasks aligned with their training data, but heterogeneous data sources offer greater versatility. Notably, from-scratch pre-training generally outperformed continual pre-training, although a smaller DrBERT model (4GB) achieved comparable results to a CamemBERT-based approach. DrBERT consistently surpassed the performance of generic French models like CamemBERT on nine out of eleven tasks. The findings highlight the value of specialized data, while also demonstrating the effectiveness of from-scratch pre-training. All DrBERT models and training scripts are publicly available under the MIT license on Hugging Face and GitHub, respectively, facilitating further research and development in French biomedical NLP.</sample>
    <sample id="60">Non specificato nel testo.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG), addressing the growing need to compress large, computationally expensive NLG models while preserving performance. Unlike prior work focusing on specific tasks or pre-training, this research explores task-specific knowledge distillation across diverse NLG tasks: summarization, question generation, common sense reasoning, simplification, and style transfer. The study adopts realistic, industry-driven setups characterized by medium-resource labeled datasets, abundant unlabeled data, medium-sized models, and a focus on inference efficiency.

The research investigates architectural choices (encoder-decoder vs. decoder-only), the impact of pruning, and compares various knowledge selection approaches. A key contribution is an exploration of pseudo-target training, challenging the conventional single-mode approximation. The study demonstrates the crucial role of unlabeled data and the benefits of generating multiple, diverse pseudo-targets through sampling techniques. Furthermore, a novel "joint-teaching" method is proposed, combining word-level distillation on both teacher and student-generated pseudo-targets to mitigate student exposure bias and improve learning. The findings provide a practical "recipe" for effective knowledge distillation in NLG, offering a pathway to compress models without significant performance degradation.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nella formulazione dell'istruzione.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità suggerisce una performance del modello migliore.</sample>
    <sample id="66">Mathematical reasoning, a core aspect of human intelligence, is gaining significant traction in AI and NLP. This survey explores the task of mathematical reasoning and the application of deep learning methods, encompassing text, images, and tables. Key areas include solving geometric problems via neuro-symbolic reasoning and automated theorem proving, often evaluated using datasets designed to assess human-level intelligence.

Recent advancements leverage sequence-to-sequence and sequence-to-tree models to represent and generate mathematical expressions. Large language models (LLMs) have shown promise, particularly with chain-of-thought prompting, but struggle with precise mathematical operations. To address this, techniques like self-consistency and program-aided LLMs are being developed to enhance performance.

Current research also focuses on expanding mathematical reasoning to low-resource languages and specialized domains like finance, science, and medicine. Despite progress, models still exhibit weaknesses in handling large numbers and maintaining consistency in reasoning. The survey highlights the ongoing challenges and future directions in this rapidly evolving field, emphasizing the need for improved generalization and robustness in mathematical reasoning models.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a phenomenon where training on one language pair impacts the quality of others. While existing mitigation methods often fall short, this study identifies key factors influencing interference. Contrary to expectations, language similarity and the total number of languages have minimal impact. Instead, severe interference primarily occurs when models are small relative to the dataset size—a condition termed "parameter poverty." The research demonstrates that this interference largely disappears with increased model scale. Furthermore, the study highlights the crucial role of temperature sampling. Using a temperature greater than 1 allows for increased sampling from lower-resource languages, and careful tuning of this parameter proves to be a simple yet effective solution for mitigating interference, particularly in larger models where uncalibrated high temperatures can be detrimental. The findings establish scaling laws for bilingual scenarios but reveal a more complex interplay of factors in the multilingual setting. Ultimately, the study concludes that modest scaling and tuned temperature can significantly reduce interference without requiring specialized algorithms, offering a practical approach to improving multilingual translation quality.</sample>
    <sample id="68">The paper doesn't specify the type of linguistic context provided during pre-training.</sample>
    <sample id="69">Tipicamente, sono necessari 20 campioni per classe.</sample>
    <sample id="70">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">The "Resolving Indirect Referring Expressions for Entity Selection" work introduces the AltEntities Corpus, a novel dataset designed to facilitate research in conversational systems and benchmark large language model (LLM) entity understanding. The corpus addresses the challenge of users employing indirect references when selecting entities, such as songs, books, or recipes, instead of direct mentions. This often occurs due to memory limitations, pronunciation ambiguities, or preference specification.

The AltEntities Corpus comprises 6,000 alternative questions across three domains, generating 42,000 indirect referring expressions. Data collection utilizes a cartoon completion setup, presenting annotators with a dialogue context and an alternative question (e.g., "Do you mean A or B?") followed by a prompt to provide an indirect reference.  Entities in the alternative questions are sampled based on increasing similarity (random, similar titles, similar descriptions, similar attributes), creating a spectrum of disambiguation difficulty. Annotators are provided with background knowledge (Google search links for songs, Wikipedia text/images for recipes/books) to inform their indirect reference generation.

Experiments with a T5 XL model demonstrate that accuracy significantly improves with access to background knowledge, reaching 92-95% when knowledge is identical to that provided to annotators, and 82-87% with partial overlap. Performance drops to 60% with only entity names, highlighting the importance of knowledge integration. The results also indicate domain-generalizability of the models. The dataset is publicly available to foster further research in this area.</sample>
    <sample id="72">Because current language models exhibit political biases that can lead to fairness issues in downstream tasks like hate speech and fake news detection, potentially marginalizing certain groups or allowing harmful content to spread unchecked.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Dense-ATOMIC is introduced as a densely-connected commonsense knowledge graph built upon the ATOMIC knowledge base to address its limitations in knowledge coverage and multi-hop paths. ATOMIC, while high-quality, suffers from sparse graph structure and insufficient links (B-to-B, A-to-B, A-to-A), hindering its utility for commonsense reasoning. Dense-ATOMIC expands upon ATOMIC by completing these missing links and incorporating multi-hop paths, exemplified by sequences like "X asks Y to marry" -&gt; "Y says yes" -&gt; "X smiles."

The construction process involves normalizing tail events and training a relation prediction model, Rel-CSKGC. Rel-CSKGC leverages RoBERTa to encode head and tail events, utilizing semantic information and bypassing the sparsity issues of graph-based methods. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction and translation-based methods. Dense-ATOMIC exhibits significantly higher knowledge coverage and facilitates improved performance in commonsense reasoning models like COMET, generating more diverse outputs. Evaluation of multi-hop paths within Dense-ATOMIC reveals a high prevalence of meaningful sequences, showcasing its potential for advanced reasoning capabilities. The code and website are publicly available.</sample>
    <sample id="75">Jointprop is a novel joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE) that addresses the limitations of existing approaches by explicitly modeling the interconnections between these tasks. Current semi-supervised methods often overlook the inherent relationships between NER and RE, potentially missing valuable label alignment opportunities. Jointprop constructs a heterogeneous graph leveraging span features and contextualized representations to capture inter- and intra-connections among labeled and unlabeled data. This graph facilitates label propagation across entity and relation nodes, iteratively refining pseudo-labels until convergence. The framework incorporates a k-Nearest Neighbor graph for efficiency and utilizes similarity relations to enforce smoothness constraints.  The refined pseudo-labels, filtered by a confidence threshold, are then integrated with existing labeled data to retrain a baseline classification model. Experiments on four datasets, including both joint and single-task settings, demonstrate that Jointprop consistently outperforms baseline models, showcasing the benefits of joint learning and the effectiveness of leveraging task interdependencies for improved NER and RE performance, particularly in semi-supervised scenarios where labeled data is scarce.</sample>
    <sample id="76">The political bias propagation pipeline goes from pretraining data to language models to downstream tasks.</sample>
    <sample id="77">This work introduces DeFacto, a new dataset designed to improve factual consistency in abstractive text summarization. Developed collaboratively by Yale University and Microsoft Research, DeFacto comprises human demonstrations and feedback on system-generated summaries from existing models (specifically, Pegasus on the XSum dataset). The dataset reveals that 70% of initial summaries contain factual errors. Annotators provided labels for factual consistency, human-corrected summaries, and detailed feedback including instructions, explanations, and supporting evidence from the source document.

The research proposes three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing, leveraging human feedback, showed effectiveness with both fine-tuned models and large language models. Feedback generation proved more challenging. The automatic error correction task demonstrated comparable performance with fewer training data, and explanation generation further improved results. DeFacto’s fine-grained annotations also offer value for training factuality metrics and meta-evaluation. The dataset and paper are publicly available on GitHub, providing a valuable resource for advancing research in factual summarization.</sample>
    <sample id="78">Sì, il tipo di semplificazione varia tra DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa mostra una semplificazione più forte, soprattutto a livello lessicale e strutturale, mentre DEPLAIN-web presenta più riformulazioni. Inoltre, DEPLAIN-apa ha più riordinamenti e aggiunte di parole, mentre DEPLAIN-web ha più riformulazioni.</sample>
    <sample id="79">Sì, CoScript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita sommando pesati gli embedding target e originali. Il peso dell'embedding target è proporzionale al numero di trigger (parole a frequenza moderata) nella frase. Quando il numero di trigger supera una soglia 'm', l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper introduces ULRA (Unsupervised AES by Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to obtain. ULRA addresses this limitation by leveraging multiple heuristic quality signals as pseudo-ground truth to train a neural AES model. The framework incorporates a Heuristic Essay Ranking (HER) module that generates partial order pairs by ranking essays based on various quality signals like unique terms and word count. These partial orders are then aggregated by a Deep Pairwise Rank Aggregation (DPRA) module, which employs a novel loss function that learns confidence weights for each signal to handle inconsistencies. A scoring strategy maps the model's output to a predefined score range. Experiments in both transductive and inductive settings demonstrate that ULRA significantly outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches. While still lagging behind fully supervised methods due to the absence of strong ground truth labels, ULRA effectively utilizes heuristic signals to achieve robust unsupervised essay scoring, offering a promising solution for scenarios with limited labeled data.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore o codificatore-PTR possono essere migliorati con l'addestramento in una combinazione di varie lingue.</sample>
    <sample id="84">PAD-Net: An Efficient Framework for Dynamic Networks introduces a novel approach to address the parameter inefficiency of fully dynamic neural networks. While dynamic networks, which adapt their architecture or parameters based on input, often outperform static networks, fully dynamic models suffer from excessive parameter usage, limiting their practical application. PAD-Net (Partially Dynamic Network) tackles this by partitioning parameters into dynamic and static components, guided by the hypothesis that fully dynamic networks contain partially dynamic subnetworks capable of maintaining representation power.

The framework employs Iterative Mode Partition to identify and convert redundant dynamic parameters into static ones, minimizing impact on the loss function. This process significantly reduces model size and computational cost while preserving performance. Experiments demonstrate that PAD-Net surpasses both static and fully dynamic networks in accuracy, achieving substantial parameter reduction compared to fully dynamic models. Ablation studies reveal the importance of dynamic ratios and scale factors for optimal performance. Furthermore, PAD-Net outperforms network pruning techniques and enhances output discrimination. Future work includes extending PAD-Net to other network architectures, exploring hardware-friendly implementations, and incorporating additional parameter modes.</sample>
    <sample id="85">"Make a chocolate cake."</sample>
    <sample id="86">Visualizzando gli embedding delle frasi su quattro dataset tramite PCA, è difficile distinguere tra gli embedding backdoor e quelli normali.</sample>
    <sample id="87">Il lavoro utilizza modelli linguistici pre-addestrati esistenti, come CamemBERT e PubMedBERT, come punto di partenza per il pre-addestramento continuo o per l'addestramento da zero, per creare DrBERT.</sample>
    <sample id="88">Non specificato.</sample>
    <sample id="89">"For example, if we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the conventional reliance on native speakers for NLP data annotation, particularly crucial with the advancement of language models. We investigate the feasibility of utilizing language learners, who are abundant even for low-resource languages, as annotators. Our proof-of-concept study involved English, Korean, and Indonesian, across four GLUE benchmark tasks (sentiment analysis, NLI, NER, and MRC). We categorized learners into proficiency levels (basic, intermediate, advanced) and compared their annotations to those of native speakers. Experiments included pre-tests, annotation tasks with varying difficulty levels and resource support (dictionaries, machine translation), and post-tests to assess learning effects.

Results demonstrate that learner-annotated labels are highly accurate, especially for simpler tasks, and achieve near-native speaker accuracy when aggregated via majority voting. Training simulations revealed that language models trained on learner annotations achieved 95% of ground truth performance, sometimes even surpassing models trained on native speaker data. Furthermore, annotation tasks demonstrably improved learners' language proficiency and vocabulary. This work proposes a novel data construction approach for low-resource languages, moving beyond translation-based methods, and highlights the potential of language learners to significantly contribute to NLP research, broadening accessibility and facilitating benchmark dataset creation.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">*   Treeless models
*   Models that integrate trees
*   Seq2seq models</sample>
    <sample id="93">Sono i suoi relatori.</sample>
    <sample id="94">This paper addresses the copyright protection of embedding as a service (EaaS), a critical concern as attackers can potentially steal models by learning from embeddings. We propose Embedding Marker, a novel backdoor-based watermark method specifically designed for EaaS. Our approach embeds a covert watermark during embedding generation without significantly degrading embedding utility.

Embedding Marker involves two key steps: watermark injection and copyright verification. Watermark injection defines a target embedding and modifies the output embedding based on the number of triggers (words within a moderate frequency range) present in the user's input sentence. Copyright verification utilizes a backdoor and benign dataset to detect watermark presence in a potentially stolen model. By comparing the cosine and L2 similarities between embeddings generated by the suspected model and the target embedding, alongside a Kolmogorov-Smirnov (KS) test, we determine the likelihood of watermark inclusion.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate Embedding Marker's strong detection performance while maintaining high utility for downstream tasks. Visualization of embeddings using PCA confirms the covertness of the watermark, making it difficult to distinguish between watermarked and normal embeddings. This work provides a practical and effective solution for protecting the copyright of EaaS.</sample>
    <sample id="95">David Vilar.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una studentessa di dottorato del primo anno al Carnegie Mellon University e oggi vi presenterò il nostro lavoro NLPositionality, che caratterizza i bias di progettazione dei dataset e dei modelli. Questo lavoro è stato svolto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Cominciamo immaginando che tu stia lavorando per un giornale e stai esaminando i commenti sotto il tuo articolo, cercando di rimuovere contenuti tossici. Potresti rivolgerti a un'API popolare come Prospective API per il rilevamento della tossicità, e questo funziona davvero bene se sei Carl Jones. Ma non è lo stesso per Aditya Sharma. Prospective API non è così sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione in cui vediamo differenze sistematiche nelle prestazioni della tecnologia tra le popolazioni.

Questi bias di progettazione, come quello che abbiamo appena visto, potrebbero derivare dalla posizione degli scienziati dell'elaborazione del linguaggio naturale e degli sviluppatori di modelli. La posizione è semplicemente la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati perché può modificare le decisioni che i ricercatori prendono.

E quindi una domanda che le persone potrebbero porsi è: i dataset e i modelli hanno una posizione? E non stiamo dicendo che i modelli stessi o i dataset stessi hanno identità demografiche ed esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni rispetto ad altre.

Lavori precedenti hanno suggerito alcune prove aneddotiche di avere una posizione, come le lacune culturali nei modelli e nei dataset, nonché definizioni teoriche della posizione del modello. Tuttavia, questi lavori non confrontano gli utenti finali con i dataset e i modelli stessi e lo studio della posizione dei dataset e dei modelli è sempre più importante man mano che le attività di elaborazione del linguaggio naturale diventano più soggettive e socialmente orientate, ed è difficile caratterizzare come queste posizioni siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.

Per studiare la posizione dei dataset e dei modelli, confrontiamo effettivamente le annotazioni con utenti reali con dataset e modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due fasi principali. La prima fase è quella di ri-annotare i dataset con annotatori diversi. E scegliamo di farlo guardando la demografia degli annotatori dei dataset originali, perché di solito solo pochi annotatori annotano ogni istanza e perché la demografia viene raramente raccolta e condivisa. Quindi scegliamo di ri-annotare i dati per ottenere molte annotazioni per istanza e per ottenere un ricco set di dati demografici.

Quindi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R, e quindi il nostro framework differisce dalla letteratura sulla discrepanza degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, invece di guardare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori.

Il nostro framework è ampiamente abilitato attraverso Lab in the Wild e una piattaforma di crowdsourcing online per collaboratori HCI. In Live in the Wild è una piattaforma di sperimentazione online in cui possiamo reclutare volontari diversi. Rispetto alle piattaforme come M Turk, che hanno in gran parte partecipanti provenienti dagli Stati Uniti o dall'India, Lab in the Wild è ancora in grado di ottenere dati di alta qualità.

Ospitiamo 2 attività su lab in the wild, una delle quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset di social chemistry e quindi scriveranno quanto una situazione sia socialmente accettabile. Successivamente, per rimanere impegnati nello studio, possono confrontare le loro risposte con un'IA e con gli altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4.

Quindi replicare un setup molto simile per l'attività di rilevamento della tossicità e dell'odio, in cui leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di odio. Quindi abbiamo confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4.

Il nostro studio ha accumulato alla fine oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi.

Quindi ora siamo meglio attrezzati per rispondere a chi si allineano di più i dataset e i modelli di elaborazione del linguaggio naturale. Scopriamo che c'è una posizione nell'elaborazione del linguaggio naturale. Ad esempio, scopriamo che i dataset e i modelli si allineano maggiormente ai paesi di lingua inglese.

Quindi, per l'analisi dell'accettabilità sociale di GPT 4, scopriamo che si allinea maggiormente ai paesi di lingua confuciana e inglese. Scopriamo anche che Dynahate si allinea maggiormente ai paesi di lingua inglese. Scopriamo anche un ulteriore allineamento con le persone che hanno un'istruzione universitaria.

Quindi, per GPT 4, nell'attività di accettabilità sociale, scopriamo che si allinea maggiormente con le persone con un'istruzione universitaria o un'istruzione post-laurea e troviamo lo stesso per Dynahate, dove si allinea maggiormente con le persone con un'istruzione universitaria.

Tuttavia, quando i modelli e i dataset si allineano a popolazioni specifiche, alcuni vengono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli si allineano meno alle persone non binarie rispetto alle controparti maschili e femminili. Troviamo questo nell'attività di accettabilità sociale di GPT 4 e nell'analisi di Dynahate.

Quindi, dato che c'è una posizione nell'elaborazione del linguaggio naturale, cosa possiamo fare al riguardo? Quindi abbiamo alcune raccomandazioni per questo. La prima è quella di tenere traccia di tutte le scelte di progettazione rilevanti durante il processo di ricerca. E l'altra è quella di condurre ricerche sull'elaborazione del linguaggio naturale con la lente del perspectivismo. La nostra terza raccomandazione è quella di costruire dataset e modelli specializzati all'interno di comunità specifiche. E un buon esempio di questo è l'iniziativa Masakhani.

Vogliamo sottolineare che l'elaborazione del linguaggio naturale inclusiva non significa semplicemente fare in modo che tutte le tecnologie funzionino per tutti.

E questo conclude la nostra presentazione. Ma se vuoi saperne di più, sentiti libero di controllare il nostro dashboard per i risultati dell'analisi più aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">Tre.</sample>
    <sample id="98">The presentation highlights the dilemma: sanitizing training data to remove political opinions risks censorship and determining neutrality is difficult.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università Fudan. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". Nella vita di tutti i giorni, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo dopo passo sotto forma di script orientati all'obiettivo. Lavori precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare una torta". E hanno dimostrato che i modelli linguistici di grandi dimensioni possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipate. La pianificazione per obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane ancora in gran parte inesplorata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata che impone diversi vincoli sugli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli sfaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo innanzitutto le capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni. Poiché non esiste un set di dati di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli sfaccettati per l'acquisizione di dati con il coinvolgimento umano utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta la precisione complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Quindi conduciamo un'analisi dettagliata per indagare sul perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma non è possibile garantire la fedeltà ai vincoli. Approfondiamo un argomento più granulare di categorie di vincoli definiti in wikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta un'elevata varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di sovra-generare e poi filtrare per migliorare la qualità della generazione. Mostriamo innanzitutto i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti iniziali. Quindi, InstructGPT sovra-genera K script per obiettivi specifici. Successivamente, viene sviluppato un modello di filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Conserviamo solo lo script se l'obiettivo target ottiene il punteggio più alto nell'insieme di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente le capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i modelli linguistici di grandi dimensioni sono costosi da implementare, è essenziale abilitare le capacità di pianificazione di modelli più piccoli e specializzati. La creazione del set di dati è un passo essenziale a questo scopo. Tuttavia, studi precedenti non consentono la pianificazione per obiettivi specifici e l'annotazione manuale del set di dati è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica, per distillare set di dati di pianificazione linguistica vincolata dai modelli linguistici di grandi dimensioni. Applichiamo il nostro metodo per creare un set di dati di pianificazione linguistica vincolata, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dell'insieme di validazione e test, chiediamo a lavoratori provenienti da fonti esterne di trovare e rivedere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un elevato pluralismo negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Scopriamo che T5 addestrato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei modelli linguistici di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi quando vengono addestrati correttamente su set di dati adatti. In sintesi, stabiliamo il problema della pianificazione linguistica vincolata. Valutiamo le capacità di pianificazione linguistica vincolata dei modelli linguistici di grandi dimensioni e sviluppiamo un metodo di sovra-generazione e poi filtraggio per i modelli linguistici di grandi dimensioni. Utilizziamo modelli linguistici di grandi dimensioni per generare un set di dati di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il set di dati CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Troverete maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering that addresses the need for fewer training examples compared to existing methods. It combines unsupervised retrieval with a few-shot language model-based reranker, achieving strong performance with as few as 128 examples. The system first retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks them using a language model. A key innovation is the scoring function, which utilizes the likelihood of the question given a chain prompt constructed by inserting chain documents into a prompt with an indicator token and an instruction. The instruction, such as "Read the previous documents and ask a question," elicits the language model's reasoning ability. Techniques like instruction search and temperature scaling are explored to optimize performance. Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers. Further evaluation with an ELECTRA-Large reader model shows excellent downstream multi-hop QA performance. The findings highlight the effectiveness of language models for few-shot path retrieval and emphasize the importance of both the scoring function and the instruction in eliciting reasoning abilities.</sample>
    <sample id="101">La fluidità di PaLM è paragonabile a quella dei sistemi all'avanguardia.</sample>
    <sample id="102">Applicabilità ai servizi di embedding, nessuna degradazione dell'utilità degli embedding forniti, sufficiente occultamento e trasferibilità all'estrazione del modello da parte dell'attaccante.</sample>
    <sample id="103">14 diverse lingue.</sample>
    <sample id="104">Ogni istanza viene riannotata.</sample>
    <sample id="105">Cosine similarity, L2 similarity, and a KS test p-value.</sample>
    <sample id="106">QUEST is a novel retrieval dataset designed to address the challenge of information seeking with selective information needs, where queries contain implicit set constraints. The dataset comprises over 3,000 entity-seeking queries across films, books, plants, and animals, constructed using Wikipedia categories and paraphrased by human annotators to ensure fluency and naturalness. Each query requires retrieving a multi-answer set, with annotators verifying entity relevance and marking attributable spans within associated documents for different query constraints.

QUEST presents a difficult retrieval problem, demanding systems to effectively search large document corpora and identify multi-answer sets where evidence for relevance can be distributed across multiple document sections. Initial evaluations using sparse and dense retrievers, alongside a T5-based reranker, reveal significant room for improvement, with end-to-end F1 scores indicating the difficulty of handling these complex queries. Analysis highlights that queries involving set intersection and set difference operations are particularly challenging. QUEST, inspired by scenarios like a zoologist identifying a reptile and a reader seeking their next book, aims to facilitate the development of improved information retrieval systems capable of handling selective information needs and implicit set constraints. The dataset and findings are detailed in a paper and will be presented at ACL.</sample>
    <sample id="107">Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) and Encoder-Decoder (Multilingual Pretrained Encoder-Decoder Models) were used. Encoder-Decoder models achieved the best performance overall. Training in a mixture of various languages improved Encoder-Decoder and Encoder-PTR models.</sample>
    <sample id="108">This work revisits the Minimal Pair Paradigm (MPP) for evaluating language model acceptability judgments, highlighting a critical limitation: current MPP pipelines don't assess acceptability within longer contexts, increasingly relevant with the rise of large language models (LLMs). We address this by extending the MPP to evaluate models across varying context lengths.

Our approach involves recreating minimal pairs—acceptable vs. unacceptable sentences—by incorporating prefixes from different sources: the same dataset (matched), a different subset of the same dataset (mismatch), or an entirely unrelated domain like Wikipedia. We observed that judgments are largely robust to irrelevant Wikipedia context, even at lengths up to 1024 tokens. However, when prefixes are drawn from the same dataset, acceptability judgments significantly shift based on whether the prefix is acceptable or unacceptable, and this effect intensifies with longer contexts. Notably, matching grammatical structures (e.g., Adjunct Island phenomena) leads to the most pronounced shifts in model judgments.

Further analysis revealed that LLMs are sensitive to latent syntactic and semantic features shared across sentences, suggesting that traditional MPP evaluations with short, single-sentence inputs may not fully capture the models' broader linguistic knowledge. Our findings underscore the need for more comprehensive evaluation methods that account for contextual influences on language model acceptability.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large, diverse dataset of instructions, inputs, and outputs entirely without human annotation. Leveraging a GPT-3 variant, the method automatically expands upon a small seed of examples from the Super-Natural Instructions dataset, prompting the model to generate new instructions and corresponding outputs in a two-step process. Further diversification is achieved through automatic instruction paraphrasing, resulting in a dataset of 64,000 examples, scaling to 240,000 with paraphrases. Analysis reveals a correctness rate exceeding 50%, with even incorrect examples proving valuable for training. The dataset exhibits remarkable creativity and diversity, encompassing tasks beyond traditional NLP benchmarks, such as scientific experiment verification and novel word invention. Fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions demonstrates superior performance compared to T0++ and Tk-instruct across multiple benchmarks, and even outperforms a baseline trained on Super-Natural Instructions when considering the cost of data generation. This work highlights the potential of language models to autonomously generate high-quality training data, offering a faster and more cost-effective alternative to human annotation while avoiding common annotation biases.</sample>
    <sample id="111">Gli autori presumono che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di Riconoscimento di Entità Nominate o NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per sviluppare NER per quasi 20 anni e questo solleva naturalmente diversi problemi. Innanzitutto, questi modelli possono generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione CoNLL-2003. Abbiamo quindi messo a punto oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E ultimo ma non meno importante, abbiamo calcolato la variazione percentuale dell'F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli transformer generalmente generalizzano meglio a nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che in genere modelli più grandi portano a una migliore generalizzazione. E ultimo ma non meno importante, sappiamo tutti che il numero di esempi di messa a punto influisce direttamente sulle prestazioni di un compito downstream. Anche qui abbiamo scoperto che più esempi di messa a punto portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Avevamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte e questo si manifesta solitamente come rendimenti decrescenti su un nuovo set di test. La seconda ipotesi è il temporal drift, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e i dati di test. Per l'overfitting dei dati, abbiamo visto che dal grafico a destra, la linea di migliore adattamento rossa ha un gradiente maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo ottenuto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, il che significa che non si osserva un overfitting adattivo in questo caso. E il temporal drift? Per il temporal drift, abbiamo condotto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni diminuiscono con un divario temporale più ampio e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è il temporal drift. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, una dimensione del modello più grande e più esempi di messa a punto. Questi vanno di pari passo, non possiamo avere un solo ingrediente ma scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni è causato dal temporal drift e, sorprendentemente, non è causato dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per oltre 20 anni. Tornando alla domanda che abbiamo posto nel titolo del nostro articolo "I tagger di entità nominate CoNLL-2003 funzionano ancora bene nel 2023?". E abbiamo scoperto che la risposta è in realtà un sonoro sì. Speriamo che il nostro articolo inviti a ulteriori ricerche su come migliorare la generalizzazione dei modelli. E infine, controllate il nostro articolo, il nostro dataset e, se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">This work introduces Grouped Head Attention (GHT), a novel approach to compressing multi-head attention in large language models (LLMs), addressing the challenges of their heavy parameter count, long training times, and massive data requirements. Existing methods for multi-head attention redundancy optimization either sacrifice performance (homogenization), lack parameter efficiency (diversification), or leave considerable redundancy unaddressed. GHT employs a divide-and-conquer strategy, first grouping attention heads through group-constrained training to promote intra-group similarity and inter-group separation. This is achieved using unsupervised hidden unit discovery and a combined homogenization and diversification loss. Subsequently, a Voting-to-Stay algorithm prunes redundant heads within each group, retaining only one per group.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 7%) and substantial parameter compression (up to 90%) compared to state-of-the-art baselines. Furthermore, a LITE model achieves 62% faster inference speed and 80% reduction in FLOPs while maintaining comparable performance. The authors argue that LLMs are inherently redundant, particularly in real-world applications where only a subset of their capabilities are needed, drawing an analogy to app management on smartphones. Future work will explore task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to further optimize LLMs without sacrificing performance.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">This paper introduces SwitchMLM, a novel pretraining technique designed to improve performance on code-switched natural language processing (NLP) tasks. Code-switching, the mixing of languages within a single sentence (e.g., "Laptop, mere, bag, me, rakha, hai" - English and Hindi), is common in linguistically diverse communities. Existing multilingual models like mBERT and XLM-R struggle with these tasks.

SwitchMLM focuses on "switch-points"—transitions between languages—and masks only these tokens during pretraining, unlike standard Masked Language Modeling (MLM). To address the need for language identification (LID) tags, the authors propose FrequencyMLM, a surrogate method using monolingual corpora. Furthermore, they introduce architectural modifications, specifically residual connections from intermediate layers rich in switch-point information to the final layer, alongside an auxiliary LID-based loss to enhance language encoding.

Experiments on sentiment analysis demonstrate that the combined SwitchMLM/FrequencyMLM and ResBERT approach outperforms baselines across various language pairs. Probing experiments, using linear and conditional probing, confirm that the proposed methods increase the representation of switch-point information in both intermediate and final layers, validating the effectiveness of the approach. The work highlights the importance of tailoring pretraining objectives and architectures to the unique characteristics of code-switched data.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">The model leverages the cross-attention mechanism.</sample>
    <sample id="121">Saying the name of the song "Easy on Me" or its position, "the first one".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">MultiInstruct introduces a novel benchmark dataset and investigates instruction tuning for multi-modal zero-shot learning. Building on the success of instruction tuning in natural language processing, this work explores its efficacy in computer vision and multi-modal tasks, addressing the scarcity of large-scale multi-modal instruction datasets. MultiInstruct comprises 62 diverse multi-modal tasks spanning 10 categories, derived from 21 existing datasets and augmented with five expert-written instructions per task. 

The research utilizes OFA, a unified multi-modal pre-trained model, and demonstrates that instruction tuning significantly enhances its performance on both seen and unseen multi-modal tasks. Transfer learning from natural instruction datasets further improves performance and reduces sensitivity to instruction wording variations. Experiments reveal that utilizing multiple instruction templates during training and testing improves overall performance and robustness. A new sensitivity metric is introduced to quantify the model's consistency across different instruction phrasings. 

The findings highlight the benefits of instruction tuning for multi-modal models and the importance of diverse instruction sets. The authors are also expanding the dataset to include 150 additional vision-language tasks, which will be publicly released. The MultiInstruct dataset and model are available for download via the provided QR code.</sample>
    <sample id="124">This work introduces "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models," addressing the critical need for comprehensive temporal reasoning evaluation in LLMs. The study breaks down temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event, finding that prior research overemphasized the second level. To facilitate a more thorough assessment, the authors created the TempReason dataset, encompassing all three reasoning types and a broad temporal scope, extending beyond year prediction to include month prediction and complex event relationships.

Experiments using Closed Book QA, Open Book QA, and a novel Reasoning QA setting revealed biases in existing LLMs like ChatGPT, particularly in month prediction and across different time periods. To mitigate these limitations, the authors propose a training strategy consisting of Temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model.  Evaluation on TempReason demonstrates that TempT5 significantly outperforms baseline models, including ChatGPT and other fine-tuned versions of T5, especially in Open Book and Reasoning QA scenarios. The study highlights remaining performance fluctuations linked to training data imbalances and suggests future research directions to address these biases, ultimately contributing to improved temporal reasoning capabilities in LLMs.</sample>
    <sample id="125">The text does not specify the number of authors.</sample>
    <sample id="126">No.</sample>
    <sample id="127">Large language models (LLMs) excel at complex reasoning tasks when prompted with chain-of-thought (CoT) techniques, but this capability is typically limited to very large models, hindering deployment due to high computational costs. This work, "Large Language Models Are Reasoning Teachers," introduces a method to transfer reasoning abilities from these large "teacher" models to significantly smaller "student" models. The core idea is to leverage large LLMs to generate step-by-step solutions for complex tasks and use these as training data to fine-tune smaller models. A key innovation is "Diverse Reasoning," which generates multiple, slightly varied reasoning paths from the teacher model using stochastic sampling, leading to more robust student training. Experiments across 12 tasks demonstrate that fine-tuned CoT models, even with as few as 0.3 billion parameters, achieve notable performance, often surpassing prompt-based baselines and vanilla fine-tuning. The approach is highly scalable, with performance improvements achievable through larger datasets, better teacher models, or increased student model size, though these choices involve trade-offs between development and inference costs. The research highlights the potential for knowledge distillation to transfer emergent abilities to smaller models and provides open-source code and data for further exploration.</sample>
    <sample id="128">The KITMUS test is a novel diagnostic suite designed to evaluate knowledge integration capabilities in natural language understanding models. These models often rely on both knowledge acquired during pretraining and knowledge provided at inference time, a crucial ability for knowledge-intensive tasks like coreference resolution. KITMUS introduces a coreference resolution task that probes a model's ability to combine entity-specific knowledge (e.g., "Servin is a judge") with background knowledge (e.g., "Judges decide cases in law courts"). The test suite features three settings—Background-Pretrain, Background-Both, and Background-Inference—varying the availability of background knowledge across pretraining and inference. The Background-Inference setting, particularly, simulates scenarios where necessary background knowledge is absent from pretraining data. Evaluations using human participants and established coreference resolution models (C2F, BERT4Coref) reveal that models initially struggle without task-specific training, relying on superficial cues. While training on KITMUS improves performance, even the best models demonstrate difficulty reliably integrating background knowledge provided solely at inference time, highlighting a limitation in current knowledge integration approaches. The KITMUS dataset and code are publicly available to facilitate further research in this area.</sample>
    <sample id="129">Donne nere.</sample>
    <sample id="130">Transformer models generally generalize better. The presentation does not specify which architectures do not generalize well.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Due.</sample>
    <sample id="133">Multi-modal.</sample>
    <sample id="135">ABC-Eval is a novel dimensional evaluation approach for conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. Addressing the limitations of traditional human evaluation methods (Likert scales, pairwise comparisons), ABC-Eval aims to reduce subjectivity by explicitly annotating specific behaviors exhibited by dialogue models. These behaviors, informed by recent literature, include irrelevance, contradiction, hallucination, common sense violations, and empathy.

The method was tested on four state-of-the-art models across 100 human-bot conversations, comparing ABC-Eval against Likert ratings (turn and dialogue-level) and dialogue-level pairwise comparisons. Results demonstrate that ABC-Eval behavior labels exhibit higher inter-annotator agreement and are more predictive of overall conversation quality than existing methods, as evidenced by linear regression analysis. Furthermore, ABC-Eval metrics capture unique aspects of chat quality, collectively explaining over 25% of conversation quality variance, significantly outperforming turn-level Likert metrics.

The study quantifies persistent challenges in current models, such as common sense violations (20%), irrelevant information (15%), and contradictions (10%). ABC-Eval provides a more reliable and precise evaluation framework, facilitating a higher-resolution assessment of conversational AI and enabling more effective comparison and improvement of models.</sample>
    <sample id="136">FERMAT is introduced as a novel evaluation framework designed to address limitations in current numerical reasoning benchmarks for language models. Existing benchmarks often rely on accuracy scores, which fail to provide granular insights into a model's mathematical strengths and weaknesses, particularly concerning smaller, more accessible models (around 3 billion parameters). FERMAT, a flexible evaluation set based on arithmetic types, dissects numerical reasoning into three key areas: number understanding, mathematical operation proficiency, and training dependency.

The framework utilizes math-worded questions sourced from established curricula (Illinois and CommonCore), manipulating number representations (integers, decimals) and varying mathematical operations to assess model capabilities. Initial zero-shot evaluations revealed suboptimal performance across all aspects, highlighting the inadequacy of current benchmarks. Fine-tuning with generated data (200,000 examples) improved performance, but further analysis demonstrated that models struggle even with expressions encountered during training, suggesting sensitivity to linguistic variations. Experiments with diversified training templates, incorporating datasets like GSM8K and AQUA, significantly boosted performance, emphasizing the importance of both language and mathematical diversity. The study concludes that existing benchmarks are unrepresentative and proposes FERMAT as a more informative alternative, identifying number encoding and tokenization as areas ripe for improvement.</sample>
    <sample id="137">Tell2Design introduces a novel task: language-guided floor plan generation. Unlike existing text-conditional image generation models focused on artistic realism, this task requires generating designs that strictly adhere to natural language instructions specifying semantics, geometry, and topology of rooms. To facilitate this research, we present Tell2Design, a large-scale dataset comprising 76,000 language instructions (5,051 human-annotated, and the rest artificially generated) paired with corresponding floor plans. The dataset features complex instructions averaging over 200 words per floor plan. We address the challenges of constrained design generation, document-level understanding, and ambiguous instructions by framing the task as a sequence-to-sequence problem using a transformer-based encoder-decoder model initialized with T5. Our model reconstructs room bounding boxes into a structured target sequence, enabling effective handling of varying floor plan complexities. Experiments on the T2D dataset demonstrate that our sequence-to-sequence approach significantly outperforms existing text-conditional image generation baselines, achieving a Micro IoU of 54 and a Macro IoU of 53. While a language distribution gap exists between artificial and human instructions, incorporating artificial data for initial training improves performance. This work establishes a foundation for future research in language-guided design generation.</sample>
    <sample id="138">L'integrazione di conoscenze a ritroso fornite solo al momento dell'inferenza.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Crowd-sourced workers were asked to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana.</sample>
    <sample id="142">Ciao! Vi parlerò del nostro lavoro su "Risoluzione di Riferimenti Indiretti per la Selezione di Entità", in cui presentiamo l'AltEntities Corpus. Mi chiamo Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considerate questa domanda alternativa. "Intendevi 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. O le pronunce sono troppo simili e difficili da disambiguare. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "la più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking degli LLM per la comprensione delle entità. Non siamo a conoscenza di un dataset pubblico più ampio per questo compito, quindi ne abbiamo raccolto uno utilizzando l'annotazione di crowd. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La nostra metodologia di raccolta dati enfatizza l'informalità utilizzando un setup di completamento del cartone animato. Il cartone animato ha tre fumetti. Nel primo fumetto, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?". E con questo, Bob stabilisce il contesto del dialogo. Nel secondo fumetto, Alice dice: "Intendevi 'Easy on Me' o 'I Gotta Feeling'?", che è la domanda alternativa. E nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "la più recente". Forniamo il primo e il secondo fumetto automaticamente, ma il terzo viene riempito dall'annotatore. Il primo fumetto è scelto da alcuni prompt manuali per dominio. Il secondo, che è la domanda alternativa, viene generato come segue. Usiamo sempre un semplice modello. Intendevi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro ed è solitamente più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio, due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente qualcosa sulle entità. Quindi, quello che facciamo è mostrare alcune conoscenze di base su queste due entità. Per le canzoni, mostriamo semplicemente un link di ricerca di Google a ciascuna canzone e poi chiediamo agli annotatori di ascoltare almeno una parte di ciascuna canzone e leggere qualcosa su ciascuna canzone. Ecco, ad esempio, il risultato della ricerca di Google per la canzone "Easy on Me". Per i domini delle ricette e dei libri, mostriamo alcuni testi di background da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Quindi, abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverle usando tre o cinque espressioni di riferimento indirette. Ad esempio, "quella senza parole", "non quella con il ragazzino di 12 anni", o "la fittizia", o "viene dall'Azerbaigian", e così via. L'AltEntities Corpus ha 6.000 domande alternative su tre domini e 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alle stesse conoscenze di base degli annotatori, allora l'accuratezza è davvero alta, intorno al 92-95%. Ma questo non è realistico. Se il modello linguistico ha accesso ad alcune conoscenze di base parzialmente sovrapposte, allora l'accuratezza è compresa tra l'82 e l'87%, il che è più realistico. Ad esempio, quando il modello linguistico recupera le conoscenze di base. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili per dominio. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">Wait-k strategy, Local Agreement, and state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">Non specificato nel testo.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Dialogue summarization, while benefiting from large-scale pre-trained language models, suffers from common errors like factual omissions, hindering real-world application. This paper investigates the prevalence and characteristics of omission in dialogue summarization, revealing that approximately 70% of generated summaries contain omissions across various domains and models. The omitted information is randomly distributed throughout the dialogue, highlighting the difficulty in identifying key information.

To address this gap, the authors introduce the OLDS dataset, the first publicly available dataset with high-quality omission labels for dialogue summarization, built upon five existing benchmarks. The dataset utilizes diverse candidate summaries generated by different models and employs an automatic method, validated by human evaluation, to produce omission labels. Experiments with three baseline architectures for omission detection demonstrate the task's challenging nature, with F1-scores around 50%.

Furthermore, the study explores refining summaries by incorporating detected omissions using a post-editing method. Results show significant performance improvements, validating omission detection as a valuable task and refinement as a promising avenue for enhancing dialogue summarization quality. The work underscores the importance of addressing omission to improve the reliability and usability of dialogue summaries.</sample>
    <sample id="147">Tre.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e introdurrò brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", un lavoro congiunto con Matteo Negri e Marco Turchi. Cos'è la traduzione simultanea del parlato? La traduzione simultanea del parlato, o SimulST, è il processo di traduzione del linguaggio parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione interlinguistica. E quali sono i problemi dei modelli SimulST attuali? Architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio addestramenti che coinvolgono diversi obiettivi di ottimizzazione. E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con una latenza di due secondi, e così via. Quindi, qual è la nostra soluzione? Innanzitutto, utilizzare modelli ST offline esistenti senza riaddestramento o adottare architetture specifiche per SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale. Cioè, il meccanismo di cross-attenzione, e potete vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, ed è una strategia per cui decidiamo se emettere o meno una traduzione parziale, in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè la sua somma è inferiore a una certa soglia alpha verso gli ultimi lambda frame di parlato, il che significa che le informazioni ricevute sono sufficientemente stabili. Ad esempio, se riceviamo un chunk di parlato contenente "I'm going to talk about..." e il nostro modello prevede la traduzione in tedesco, guarderemo i pesi della cross-attenzione e vedremo che le prime due parole puntano ai frame di parlato ricevuti più all'inizio, mentre l'ultima parola punta agli ultimi frame di parlato ricevuti, come lambda frame di parlato. Ciò significa che le prime due parole verranno emesse poiché la somma della cross-attenzione è superiore a una certa soglia alpha, non emetteremo l'ultima parola e aspetteremo un altro chunk di parlato. Se continuiamo e riceviamo un altro chunk di parlato e il nostro modello prevede altre tre parole e guardiamo quei pesi della cross-attenzione, vedremo che nessuna parola punta agli ultimi lambda frame di parlato. Ciò significa che queste tre parole verranno emesse. Se guardiamo i risultati principali di EDAtt, tracciamo i risultati della traduzione simultanea del parlato su grafici in cui abbiamo BLEU da un lato che misura la qualità della traduzione e la latenza media, che è la misura della latenza, e consideriamo anche la latenza media consapevole del calcolo che tiene conto dei tempi di calcolo del modello per prevedere l'output. Quindi vogliamo che le nostre curve siano il più alte possibile su questo grafico. Ma vogliamo anche che siano spostate a sinistra. E confrontiamo con strategie popolari che vengono applicate anche ai modelli offline, ovvero la strategia Wait-k e l'Accordo Locale. E confrontiamo anche con l'architettura all'avanguardia specificamente progettata per la pre-traduzione simultanea. Questi sono tutti i risultati della strategia di traduzione simultanea del parlato in tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline poiché le curve sono spostate a sinistra. E vediamo anche che, considerando il tempo trascorso effettivo o il tempo consapevole del calcolo, è la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro paper. E abbiamo anche rilasciato open source il codice e i modelli e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="149">Yes, the dataset is available.</sample>
    <sample id="150">MeetingQA introduces a novel extractive question answering (QA) dataset built on real-world meeting transcripts, addressing a gap in NLP research that primarily focuses on summarization and action item extraction. The dataset comprises 7.7K questions derived from the AMI corpus, reflecting the unique characteristics of meeting discussions: long, domain-specific documents with open-ended questions eliciting detailed responses. MeetingQA features complex answer scenarios, including multiple speakers, discontinuous sentences, and rhetorical questions, with 30% of questions being unanswerable.

The paper explores various QA approaches, including context retrieval for short-context models, single-span and multi-span models, and data augmentation using silver annotations. Results reveal a significant performance gap between models and human baselines, with short-context models (RoBERTa) slightly outperforming long-context models (Longformer). Multi-span models demonstrate comparable or slightly lower performance than single-span models. Zero-shot performance highlights the dataset's difficulty, but silver data augmentation and larger instruction-tuned models (FLAN-T5) show promise. Error analysis indicates challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot settings. MeetingQA presents a valuable resource for advancing QA research in the challenging domain of meeting transcripts, demonstrating that current models are far from achieving human-level performance.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e io e il mio collega Zhiyang presenteremo la nostra ricerca su MultiInstruct, che migliora l'apprendimento zero-shot multimodale tramite l'instruction tuning. Con i progressi dei modelli linguistici di grandi dimensioni, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli linguistici pre-addestrati per diverse attività a valle in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'instruction tuning consente ai modelli linguistici di grandi dimensioni di eseguire attività mai viste in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'instruction tuning si è concentrata sul miglioramento delle prestazioni zero-shot sulle attività solo linguistiche, mentre i compiti di visione artificiale e multimodali sono stati trascurati. Pertanto, in questo lavoro vogliamo indagare se l'instruction tuning di modelli pre-addestrati multimodali può effettivamente migliorare la generalizzazione a compiti multimodali mai visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra l'NLP e il multimodale. Esistono più di 1600 attività solo linguistiche. Tuttavia, non esiste un dataset multimodale di istruzioni di grandi dimensioni disponibile pubblicamente. Pertanto, questo ci motiva a costruire un dataset di instruction tuning multimodale. Qui presentiamo MultiInstruct, il primo benchmark dataset di instruction tuning multimodale che consiste in 62 attività multimodali diverse che coprono 10 ampie categorie. Queste attività derivano da 21 dataset open-source esistenti e ogni attività è dotata di cinque istruzioni scritte da esperti. Per indagare sull'instruction tuning multimodale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multimodale unificato, come modello di base. OFA utilizza un vocabolario unificato per linguaggio, token immagine e le coordinate di un bounding box. Qui mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, per unificare l'elaborazione di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutte le attività in un formato sequence-to-sequence unificato. In cui il testo di input, le immagini, le istruzioni e i bounding box sono rappresentati nello stesso spazio di token. Ok, ora parlerò dell'instruction tuning multimodale. Per il dataset di training, utilizziamo 53 attività da 9 gruppi per il training e campioniamo 10.000 istanze per attività. Per il testing, riserviamo l'intero gruppo di ragionamento di buon senso per il testing e selezioniamo 5 attività aggiuntive dai gruppi VQ e Miscellaneous. Utilizziamo tutte le istanze nella suddivisione di test per ogni attività. Inoltre, campioniamo casualmente 20 attività dalla suddivisione di test delle istruzioni naturali come attività mai vista per l'NLP. Utilizziamo il modello OFA large pre-addestrato come modello di base. Durante il training, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione. Durante il test per ogni attività, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo le prestazioni minime e massime e la deviazione standard delle prestazioni su tutti i 5 esperimenti. Se l'attività è un'attività di classificazione multimodale, riportiamo l'accuratezza. Se è un'attività di generazione multimodale, riportiamo Rouge-L. Per le attività NLP, riportiamo Rouge-L anche noi. Introduciamo anche una metrica di valutazione aggiuntiva chiamata sensibilità. Questo misura la capacità del modello di produrre in modo coerente gli stessi output per la stessa attività indipendentemente dalla leggera variazione nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'instruction tuning può migliorare significativamente le prestazioni di OFA sulle attività multimodali viste. Inoltre, il transfer learning dal dataset di istruzioni naturali può avvantaggiare l'instruction tuning. Possiamo vedere che, all'aumentare del numero di attività, il modello ottiene prestazioni migliori e, nel contempo, una sensibilità inferiore. Abbiamo anche fatto un esperimento. Utilizziamo un'istruzione rispetto a 5 istruzioni. Come possiamo vedere, l'utilizzo di più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Questo mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Possiamo anche vedere che il transfer learning dai dataset di istruzioni naturali può aiutare OFA a ottenere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il transfer learning dai dataset di istruzioni naturali può aiutare OFA a ottenere prestazioni molto migliori sul dataset di istruzioni naturali. Nel complesso, proponiamo il primo dataset di instruction tuning multimodale su larga scala che ha significativamente migliorato la capacità zero-shot di OFA, ed esploriamo diverse tecniche di transfer learning e ne mostriamo i vantaggi. Abbiamo progettato una nuova metrica chiamata sensibilità. Un'altra cosa, stiamo raccogliendo un dataset di instruction tuning multimodale ancora più grande con circa 150 ulteriori attività vision language e lo rilasceremo. Questo è un codice QR per i nostri dati e il nostro modello. Grazie.</sample>
    <sample id="152">This presentation introduces new language models specifically designed for classical philology, addressing limitations in existing models for Ancient Greek and Latin. The project aims to improve comparability, push state-of-the-art performance, explore diverse architectures, and enable multilingual processing. Two monolingual models, GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder), were developed for Ancient Greek, alongside multilingual counterparts, PhilBERTa and PhilTa, trained on Greek, Latin, and English data. A novel, high-quality pre-training corpus for Ancient Greek was created by leveraging the Internet Archive and employing a unique stop-word identification technique to overcome OCR challenges.

Rigorous benchmarking on part-of-speech tagging, dependency parsing, and lemmatization tasks demonstrates significant performance gains over existing models for both languages. Notably, the encoder-decoder architecture (GreTa) excels in lemmatization, achieving a 5 percentage point improvement for Ancient Greek. Semantic and world knowledge probing reveals strong performance, though multilingual models did not show a significant advantage over monolingual ones. The study also investigates the distinct behavior of T5 encoders compared to native encoder-only models. The work provides valuable resources and insights for NLP applications in classical philology, with a detailed paper available for further exploration.</sample>
    <sample id="153">This work investigates ambiguities in prompts given to text-to-image generative models and proposes frameworks to address them. The core challenge lies in the varied interpretations of ambiguous prompts, hindering the generation of images aligned with user intention. To tackle this, the authors curated a benchmark dataset based on LAVA, encompassing diverse ambiguity types. Their framework employs two disambiguation strategies: generating clarifying questions for users to answer or presenting multiple visual interpretations for users to select. These interactions yield disambiguated prompts, which are then fed into text-to-image models.

To evaluate faithfulness, an automatic evaluation framework leverages a Visual Question Answering (VQA) model. The VQA model assesses whether generated images satisfy the user's intended meaning, comparing images generated from original and disambiguated prompts. Findings reveal disparities in ambiguity resolution across different types and demonstrate that the proposed disambiguation framework generally improves image faithfulness. Crucially, the automatic evaluation framework exhibits strong agreement with human evaluations, establishing its reliability for assessing text-to-image model performance. The study highlights the importance of addressing prompt ambiguities for faithful image generation and provides valuable tools for both mitigation and evaluation.</sample>
    <sample id="154">University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Dialogue summarization, a challenging task in text summarization, aims to condense dialogue context into concise summaries. Existing methods rely on pre-computed static graph structures derived from external linguistic tools, which are prone to errors and lack adaptability. This work introduces SDDS, a novel model addressing these limitations by fusing static and dynamic graph structures.

SDDS employs an utterance encoder to generate vector representations, then constructs static graphs using four heuristic methods: Discourse Parsing Graph, Key Co-occurrence, Speaker Relationship Modeling, and Utterance Position Graph. A Static-Dynamic Graph module combines these static graphs and utilizes a dynamic graph module with multi-head attention to capture semantic relationships based on deep vector representations. Finally, a pre-trained language model acts as the Summary Generator, fusing both static and dynamic structures for summary generation.

The model integrates static and dynamic graph information through cross-graph fusion using convolutional layers and a unified graph representation. A dual cross-attention mechanism, incorporating a graph attention layer, further enhances the generation process. SDDS effectively leverages dialogue structure information, improving summary quality and addressing the drawbacks of relying solely on pre-computed static graphs. The code and data are publicly available on GitHub.</sample>
    <sample id="158">"Dual Cache for Long Document Neural Coreference Resolution" addresses the computational challenges of coreference resolution in lengthy texts. Traditional methods suffer from quadratic complexity, while cache-based approaches offer linear efficiency but struggle with topic shifts and scattered entity mentions, leading to high cache miss rates. This work introduces a dual cache architecture, combining a local cache (LRU eviction) for local entities and a global cache (LFU eviction) for frequently occurring, globally relevant entities.

The model classifies mentions as new or existing, evaluating their frequency to determine cache placement. Experiments on four benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate that dual cache outperforms baselines, even those with unbounded memory, when training data is available. Notably, on a 30,000-word book-level document, the performance gap widens significantly. The dual cache also substantially reduces cache misses compared to single cache methods. The research highlights a favorable performance/cost ratio, establishing dual cache as a cost-effective solution for efficient and accurate coreference resolution in long documents.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla nostra presentazione del paper di ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, riprendiamo il paradigma delle coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici in base ai giudizi di accettabilità, che possono includere la grammaticalità come BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come le coppie CrowS. E in questo paradigma, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi mostrare una frase accettabile o non grammaticale. E la speranza è che il modello, fondamentalmente, assegni una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettabilità del modello verso frasi più lunghe. Al giorno d'oggi, i modelli linguistici di grandi dimensioni stanno generando frasi sempre più lunghe. Quindi è fondamentale valutare l'accettabilità dei modelli attraverso l'intera finestra di contesto e questo è ciò che stiamo cercando di fare qui. Stiamo cercando di riprendere la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Questo è l'approccio. Quindi, cosa facciamo? Per simulare queste sequenze più lunghe, riprendiamo i dataset stessi e ricreiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto una tipica coppia di grammaticalità dal dataset BLiMP dal caso dell'Isola Adverbiale. E quello che facciamo è ricreare sequenze più lunghe che siano accettabili e che abbiano la stessa struttura grammaticale. Estraiamo frasi grammaticali dall'Isola Adverbiale e le aggiungiamo come prefisso sia alla query accettabile che alla query non accettabile. Possiamo fare lo stesso scegliendo frasi non accettabili dalla stessa struttura corrispondente, e questo può anche essere utilizzato per testare l'accettabilità del modello. Possiamo anche fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è quello che chiamiamo scenario di mismatch. Quindi, le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset con cui si sta valutando. E possiamo fare lo stesso per il caso di non accettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, ad esempio se il contesto proviene da un sottoinsieme diverso del dataset, o se è completamente irrilevante rispetto alla frase che stiamo esaminando. Quindi, come se la cava il modello? Innanzitutto, guardiamo le frasi di Wikipedia, che sono completamente irrilevanti rispetto alla coppia di query corrente, e lì scopriamo che i giudizi MPP sono per lo più robusti per una lunghezza del contesto arbitraria. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT 2. E qui vediamo nella linea tratteggiata arancione, i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili. Ma quando si fa la corrispondenza della struttura, cioè quando si scelgono le frasi dalla stessa fenomenologia in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o non accettabile. Questo e questo è molto grande, questo effetto aumenta attraverso la lunghezza del contesto e questo probabilmente influenzerà i modelli linguistici più recenti che hanno una finestra di contesto ampia. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico? Abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. E dopo aver fatto diverse di queste perturbazioni, scopriamo che nessuno di questi rumori sta effettivamente facendo cambiare al modello il suo corso in termini di come ci mostra la stampa del giudizio MPP. Fondamentalmente, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Cioè, quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Quindi, i punti chiave del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E la valutazione MPP, nel modo in cui la facciamo attualmente con input brevi e a singola frase, potrebbe non catturare appieno la conoscenza astratta del modello linguistico attraverso l'intera finestra di contesto. Si prega di leggere il nostro paper per maggiori dettagli sui nostri esperimenti. Grazie per aver ascoltato.</sample>
    <sample id="160">Each input token is mapped to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">Le annotazioni più deboli sono molto più economiche rispetto alle annotazioni umane.</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive commonsense reasoning. Traditional approaches rely on supervised training, which suffers from noisy and subjective annotation of plausible explanations, often exhibiting significant disagreement among annotators. LiPoR addresses this limitation by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, eliminating the need for explicit plausibility labels.

To ensure the selection of plausible explanations, LiPoR incorporates a regularization term based on the principle of mutual exclusivity—the understanding that explanations in abductive reasoning are typically mutually exclusive. This regularizer penalizes models that assign probability mass to more explanations than a predefined threshold, effectively encouraging the selection of a concise subset of plausible explanations.

Experiments on the AlphaNLI dataset demonstrate that LiPoR significantly outperforms existing zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement over a strong GPT-3 baseline. This work demonstrates the feasibility of learning abductive reasoning without relying on labeled plausibility data, paving the way for more robust and scalable abductive reasoning systems. The paper and code are available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces NDCR, a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Addressing the limitations of existing visual language models that struggle with complex descriptions, NDCR draws inspiration from Divide-and-Conquer strategies and Dual-Process Theory. The framework mimics human cognition by integrating an analogical reasoning "System 1" (Visual-Linguistic Interactor) with a logical reasoning "System 2" (Neural-Symbolic Reasoner).

NDCR first decomposes complex text into simpler propositions using a Proposition Generator. System 1 then interacts visual and propositional information, producing matching scores and reasoning states. System 2, comprising a negation executor and conjunction operation, integrates these states to derive final inferences. The framework combines the outputs of both systems to achieve a comprehensive solution.

Experimental results demonstrate that NDCR significantly outperforms existing baselines. Ablation studies confirm the effectiveness of each module. Case studies showcase the framework's ability to present intermediate inference states, highlighting its interpretable and interoperable processing. The authors suggest that neural-symbolic computation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for enhancing compositional reasoning in large language models.</sample>
    <sample id="167">I 750 documenti in DEPLAIN-web sono stati allineati sia manualmente che con metodi di allineamento automatici.</sample>
    <sample id="168">It was created by collecting data from Reuters News in 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents the first systematic study of prompting strategies for machine translation using PaLM, a 540 billion-parameter large language model. The research evaluates PaLM's translation capabilities using established machine translation best practices, including state-of-the-art test sets and neural MT metrics, alongside human evaluation via the MQM framework. The study highlights the significant impact of prompting on translation performance, demonstrating differences of up to 40 BLEURT points with varying prompts. A 5-shot prompting strategy, where sentences are marked with their language, proved most effective. Crucially, the research finds that example quality outweighs similarity to the source sentence, recommending selection from curated development data over noisy training data. While PaLM achieves translation fluency comparable to state-of-the-art systems, it lags in accuracy, exhibiting a tendency to omit parts of the source sentence. Despite this, PaLM's output demonstrates a lower rate of stylistic awkwardness compared to existing systems. The findings suggest that PaLM holds promise as a competitive translation model, nearing the performance of commercial systems like Google Translate, but further improvements in accuracy are needed.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università della Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Quindi, il semantic parsing è un compito per costruire rappresentazioni semantiche di query utente come SQL e Lambda Calculus. E il Cross-Lingual Semantic Parsing è il compito di tradurre query in più lingue naturali in più rappresentazioni del significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. I modelli di semantic parsing cross-linguale esistenti sono stati proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono molte coperture su determinate lingue naturali. Ma il cinese è assente e manca la copertura su determinate rappresentazioni del significato. Il Lambda calculus è assente, o vengono valutati solo su determinati modelli neurali. Ad esempio, c'è un solo modello per valutarli. Quindi, a questo punto, proponiamo XSemPLR. Forniamo un set di dati uniforme XSemPLR per il semantic parsing cross-linguale in più lingue naturali e rappresentazioni del significato. Contiene 9 set di dati in vari domini, 5 compiti di semantic parsing, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per l'addestramento e la valutazione. Il primo è Translate-Test. Utilizziamo l'API di Google Translate per tradurre la sorgente nella lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione. Ad esempio, addestriamo il modello inglese su query inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e quindi utilizziamo il modello addestrato per prevedere il SQL. Testeremo anche il modello monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche il modello monolingue Few-shot addestrando modelli monolingui con solo il 10% dei dati di addestramento. E testiamo il modello Multilingue che addestriamo con un modello multilingue per tutte le lingue. Ad esempio, mettiamo query tedesche, inglesi e cinesi insieme per addestrare un modello multilingue. E durante l'inferenza possiamo utilizzare questo modello per tradurre query tedesche o cinesi, ecc. Consideriamo anche il Cross-lingual Zero-shot e Few-shot transfer. Addestriamo su una lingua di origine e trasferiamo in un'altra lingua. Quindi, durante l'addestramento, lo addestriamo su query inglesi o sulla combinazione di query inglesi e tedesche Few-shot per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche trovato molti risultati interessanti. Per quanto riguarda l'analisi dei modelli monolingui, valutiamo su due gruppi di modelli, tra cui Encoder-PTR, che sta per Multilingual Pretrained Encoders with Pointer-based Decoders, come XLM-R + PTR e mBERT + PTR. E valutiamo anche i modelli Encoder-Decoder, che sono Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che l'Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i set di dati. E valutiamo su mT5 e XLM-R + PTR nell'impostazione multilingue. Abbiamo scoperto che l'Encoder-Decoder o l'Encoder-PTR possono essere migliorati addestrandosi in una miscela di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un miglioramento delle prestazioni, tranne che le prestazioni dell'inglese diminuiscono in sette set di dati e ottengono guadagni solo in tre set di dati. Credo che questo sia noto come la "Maledizione della Multilinguità". Confrontiamo anche il divario di prestazioni cross-lingua. In questa figura, la linea blu è il Cross-lingual Few-shot transfer. La linea arancione è il Cross-lingual Zero-shot transfer. La linea verde è l'impostazione Monolingue. Abbiamo scoperto che confrontando la linea verde e la linea arancione, abbiamo scoperto che l'impostazione Zero-shot, il divario di prestazioni del trasferimento cross-linguale è significativo, e confrontando le linee blu e arancione, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si riduce rapidamente. Troviamo anche altri risultati interessanti. Ad esempio, l'Encoder-Decoder supera i lavori precedenti o ottiene risultati comparabili. Il pre-addestramento sulla lingua naturale inglese può migliorare significativamente le prestazioni del Few-shot sulle lingue naturali di destinazione e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di semantic parsing cross-linguale. In sintesi, abbiamo costruito XSemPLR, un benchmark unificato per il semantic parsing cross-linguale con più lingue naturali e rappresentazioni del significato. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molti risultati interessanti. E altro ancora. Benvenuti a visitare il nostro articolo e il codice. Grazie per aver ascoltato.</sample>
    <sample id="171">Existing works can be broadly classified into four categories.</sample>
    <sample id="172">No, i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di cross-lingual semantic parsing.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset designed to advance argument quality analysis, addressing limitations found in existing resources. Unlike datasets primarily sourced from crowdsourcing, ArgAnalysis35K boasts 35,000 argument-analysis pairs, the largest in the field, with 85% originating from high-quality sources like expert and tournament debaters. This ensures a higher baseline argument quality.

The dataset prioritizes diversity by moving beyond pre-selected motions. Instead, it utilizes 24 themes identified through expert consultation and circuit experience, capturing a broader range of motions within each theme. A key innovation is the introduction of "analysis," a concept encompassing claims, premises, and their interconnected reasoning, providing a more nuanced understanding of argument structure than simply isolating claims or premises.

To mitigate annotator bias, ArgAnalysis35K employs instance-based annotator reliability, selectively removing judgments deemed biased on a per-argument basis. Finally, a relevance model assigns scores (0-1) to each argument within each theme, reflecting its applicability across various contexts, recognizing that arguments often transcend single motions. This comprehensive approach results in a more diverse, reliable, and relevant dataset for argument quality analysis research.</sample>
    <sample id="175">It addresses this by inducing the alignment as part of the training and approximating the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita esaminando le prestazioni per categoria, ovvero separando le prestazioni in base a diverse demografie o orientamenti politici dei media di notizie.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">This research addresses the challenge of improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs), which currently struggle with false-belief tasks. We introduce SymbolicToM, a novel inference-time method that leverages explicit graphical representations to enhance ToM capabilities without requiring model fine-tuning. SymbolicToM constructs multiple graphs representing the mental states of different characters, enabling reasoning about beliefs up to a predefined level. These graphs are generated using readily available Natural Language Inference and Open Information Extraction models.

The method efficiently answers ToM questions by detecting entities, retrieving relevant belief graphs, performing recursive questioning over the graph, and then using a language model to generate the final answer based on the graph's content. Experiments across various LLMs (including GPT-3, Macaw, and Flan-T5-XXL) demonstrate significant performance gains compared to supervised baselines, achieving up to a 65-point accuracy increase. Furthermore, SymbolicToM exhibits strong generalization capabilities, maintaining performance improvements on out-of-domain datasets designed to test storage structure and linguistic diversity, while supervised models degrade substantially. This plug-and-play approach offers interpretable reasoning and improves out-of-the-box LLM performance in ToM tasks, paving the way for more robust and nuanced language understanding.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints, such as "make a chocolate cake" versus the more common "make a cake." The authors define constrained language planning as generating scripts that are both reasonable and faithful to multifaceted constraints. They first evaluate and improve large language models' constrained planning abilities, finding limitations in faithfulness to constraints despite acceptable semantic completeness. To overcome this, they propose an "over-generate-then-filter" method, leveraging InstructGPT to generate multiple scripts and a filter model based on semantic similarity and keyword matching to select the most faithful.

Recognizing the cost of large language models, the authors then introduce CoScript, a dataset of 55,000 specific goals and scripts generated using their method and refined through crowd-sourced validation. CoScript demonstrates high diversity in constraint types.  Experiments show that a smaller T5 model fine-tuned on CoScript outperforms larger language models in constrained language planning, highlighting the value of specialized datasets. The authors believe CoScript will serve as a valuable resource for advancing research in language planning and enabling smaller models to achieve strong performance in this area.</sample>
    <sample id="182">Tropicalismo si riferisce a un tropo che collega le donne latine a immagini esotiche e vibranti.</sample>
    <sample id="183">Gli autori si sono ispirati a uno studio in cui sono stati forniti agli esseri umani gli stessi prompt per generare descrizioni di persona, al fine di confrontare direttamente le rappresentazioni generate dal modello con le risposte scritte dagli esseri umani.</sample>
    <sample id="184">CXMI (Contextual Mutual Information) e la sua estensione, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is based on data crawled from the web (NACHOS), while ChuBERT is based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Due.</sample>
    <sample id="188">Iterative updating trains the model on the latest set of data collected in each round of active learning.</sample>
    <sample id="189">To understand users’ language when they want to make a choice.</sample>
    <sample id="190">Attraverso l'apprendimento dagli embedding forniti dal servizio.</sample>
    <sample id="191">Tre.</sample>
    <sample id="192">CAME (Confidence-guided Adaptive Memory Efficient Optimization) addresses the challenge of training large language models (LLMs) by simultaneously achieving fast convergence and low memory usage. Existing adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from performance penalties due to inaccurate updates stemming from non-negative matrix factorization (NMF). CAME builds upon Adafactor's NMF approach but introduces a novel confidence-guided mechanism to mitigate these errors.

Inspired by the discrepancies between momentum and update vectors, CAME calculates an instability matrix based on the residual between predicted and generated updates. This instability matrix is then used to adaptively scale the momentum, effectively reducing the impact of erroneous updates and stabilizing the training process.

Experiments on BookCorpus and English Wikipedia with BERT, GPT-2, and T5 demonstrate CAME's superior performance compared to Adam and Adafactor. CAME achieves significant improvements in validation accuracy and exhibits better performance with large batch sizes (8K to 32K). Furthermore, BERT models trained with CAME demonstrate comparable downstream task performance to baselines while using less memory. Memory usage analysis reveals that CAME significantly reduces memory footprint compared to Adam, LAMB, and other memory-efficient optimizers, making it a practical solution for training very large LLMs.</sample>
    <sample id="193">Il testo non specifica il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and Allen Institute for AI.</sample>
    <sample id="195">We introduce RoHT, a novel framework for Explainable Question Answering (XQA) that addresses the limitations of existing neuro-symbolic and decompose-based methods. RoHT, Reasoning over Hierarchical Question Decomposition Tree, tackles the challenges of determining optimal question decomposition granularity and navigating the uncertainties inherent in question decomposition and answering. Our two-stage framework first constructs a Hierarchical Question Decomposition Tree (HQDT) representing the compositional structure of a complex question, with leaf nodes representing atomic questions. Subsequently, it performs probabilistic reasoning over the HQDT, fusing knowledge from both knowledge bases (KBs) and text corpora at different levels. This involves a scheduler to select appropriate knowledge sources, executors to retrieve answers with probabilities, and an aggregator to combine candidate answers. We evaluate RoHT on KQA Pro and Musique datasets, demonstrating significant improvements over state-of-the-art methods, including TransferNet and EX(SA). Results show the benefits of integrating knowledge from both KBs and text, particularly when KBs are incomplete, and highlight the superiority of explicit question decomposition. RoHT achieves substantial performance gains, showcasing its effectiveness in complex question answering and explainability.</sample>
    <sample id="196">"I saw Bart and Lisa."</sample>
    <sample id="197">Four state-of-the-art chat models.</sample>
    <sample id="198">Because large language models are now using longer and longer context windows, it's crucial to evaluate their acceptability judgments throughout that window.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset, con guadagni solo in tre.</sample>
    <sample id="200">No, gli annotatori conoscono i nomi delle entità, ma non necessariamente le informazioni su di esse.</sample>
    <sample id="201">Neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">No, the performance drop is caused by temporal drift, not adaptive overfitting.</sample>
    <sample id="203">Studiare la posizionalità di dataset e modelli è sempre più importante poiché i compiti di NLP diventano più soggettivi e socialmente orientati, e perché non tutti i processi decisionali sono documentati e molti modelli sono nascosti dietro API.</sample>
    <sample id="204">BLOOM e Codex sono risultati inadeguati per i compiti di semantic parsing cross-linguale.</sample>
    <sample id="205">This work investigates the propagation of political biases from pretraining data to language models and their downstream applications, highlighting potential fairness issues in NLP. Analyzing prominent news sources in the C4 corpus reveals significant representation of diverse political perspectives in language model training data, a double-edged sword that can lead to biased outcomes. The study proposes a novel evaluation framework using political questionnaires to assess language model political leaning, finding that models exhibit varying biases across a political spectrum, with GPT-4 demonstrating the most liberal leaning.

Controlled experiments involving further pretraining on partisan corpora demonstrate that language models readily adopt the ideological biases of their training data, and that models trained after 2017 reflect increased societal polarization. Evaluating models with different political leanings on hate speech and fake news detection reveals performance disparities: left-leaning models are better at detecting hate speech against minority groups but worse at detecting it against dominant groups, and vice versa for right-leaning models. This highlights a critical fairness concern, where biased models deployed in real-world applications could marginalize certain groups or allow harmful content to proliferate. The research concludes by discussing the challenging dilemma of balancing bias mitigation with the risk of censorship, emphasizing the need for careful consideration and further research in this area.</sample>
    <sample id="206">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="207">I latest test sets to avoid overlap with the language model's training data.</sample>
    <sample id="208">Tre.</sample>
    <sample id="209">The proposed method (over-generate-then-filter) improves generation quality in semantic completeness and faithfulness to constraints compared to directly using large language models. Furthermore, fine-tuning smaller models (like T5) on the generated CoScript dataset can achieve higher quality scripts than larger language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset are proposed as a benchmark for automatic text simplification.</sample>
    <sample id="212">Uno.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">This talk presents a novel argument for symmetric coordination structures, challenging asymmetric approaches prevalent in Universal Dependencies and Meaning-Text Theory. The argument is grounded in the principle of dependency length minimization, which favors shorter dependencies between words. The research leverages data from the Penn Treebank to analyze coordination patterns in English.

A key observation is that the tendency for the left conjunct to be shorter than the right conjunct is consistently observed when the governor (the word governing the coordination) is located to the left of the coordinated elements or is absent entirely. However, this preference vanishes when the governor is positioned to the right. This phenomenon is quantified by measuring the length difference between conjuncts in characters, syllables, and words, demonstrating a clear correlation with governor position.

The findings suggest that asymmetric coordination structures, which privilege one conjunct as the head, are less optimal from a dependency length perspective. Conversely, symmetric structures, where both conjuncts are treated equally, align better with the principle of minimizing dependency length, particularly when the governor is on the left or absent. The paper concludes that these empirical observations provide compelling evidence supporting symmetric coordination structures and questioning the validity of asymmetric models.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or rely on labeled data. This work introduces DCG, a Disentangled Controllable Generation model, designed to tackle compositional generalization in multi-attribute dialogue. DCG learns attribute concepts from seen values and employs a disentanglement loss to handle diverse attribute combinations. A novel, reference-free evaluation framework, MAE, is proposed to assess controllability across varying attribute granularities.

The model, built upon DialoGPT, utilizes attribute-oriented and task-oriented prompts to guide generation, concatenated into a comprehensive prompt embedding. Pseudo combinations and disentanglement learning further enhance generation ability and compositional generalization. Experiments on two established benchmarks demonstrate DCG's superior performance in attribute controllability and text quality compared to baselines, including CTRL. MAE exhibits strong correlation with human judgments, outperforming traditional metrics. Visualization of prompt embeddings confirms the model's ability to disentangle attribute combinations and generalize from seen to unseen attribute values, showcasing the effectiveness of the shared embedding mapping for learning attribute concepts.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">This work introduces a novel approach for financial report analysis, specifically targeting Form 10-K filings, which are traditionally labor-intensive to analyze. Recognizing the high degree of textual similarity between consecutive annual reports (approximately 80% token overlap), we propose a "highlighting" task designed to identify key changes and differences between reports from consecutive years. Our method employs a compare-and-contrast multistage pipeline to uncover financial signals.

The pipeline consists of document segmentation, relation recognition (classifying pairs of text segments into β, revised, and mismatched categories), and a two-stage fine-tuning process. Initially, the model undergoes out-of-domain fine-tuning using the eSNLI dataset. Subsequently, it's fine-tuned on "revised" pairs from our dataset, leveraging pseudo-positive labels and a combination of cross-entropy and KL divergence for robust training.

Evaluated on both eSNLI and our newly released FINAL dataset, the proposed domain-adaptive highlighting model demonstrates superior performance, measured by precision and Pearson correlation coefficient (PCC). Notably, the model exhibits strong generalization capabilities and benefits from simulating mismatched pairs, which were not explicitly used during training. This work provides a valuable framework for automated financial report analysis and lays the groundwork for future enhancements incorporating information retrieval techniques.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">The article does not specify all language pairs analyzed, but it mentions German to English as one example.</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates methods to improve question answering (QA) performance when transferring models trained on a general domain (Wikipedia) to new, specialized domains. The core challenge lies in domain shift, where the source model struggles to generalize due to differences in vocabulary, reasoning patterns, or context. The study explores both zero-shot and few-shot data interventions to address this. Few-shot methods leverage large language models to generate training examples from a limited number of target domain samples, while zero-shot techniques manipulate the question, answer, and context distributions to understand their impact on model learning.

The research identifies three types of dataset shift: no shift, concept shift, and covariate shift, based on the compatibility of the retriever and reader models with the target domain. A compatibility measure, based on likelihood scores, is introduced to categorize datasets. The findings reveal that few-shot adaptation is broadly effective across various domains, while zero-shot interventions are particularly beneficial for datasets exhibiting concept or covariate shift. Through experimentation with diverse interventions, the study achieves up to a 24% improvement in reader performance, demonstrating the importance of tailoring adaptation strategies to the specific type of domain shift encountered.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART e base mBART.</sample>
    <sample id="225">53 tasks are used for training and 9 tasks are used for testing.</sample>
    <sample id="226">Due.</sample>
    <sample id="227">Current language models excel at general NLP tasks but struggle with grounded language understanding—mapping natural language to executable plans in specific environments. This challenge stems from a lack of grounding during pre-training, leading to generated plans that are often invalid or ungrammatical. This work introduces Pangu, a novel framework that shifts the focus from generation to discrimination. Instead of directly generating plans, Pangu utilizes a symbolic agent to propose candidate plans, which are then scored and ranked by a language model. This approach alleviates the language model's burden of ensuring plan validity and grammar.

The framework is demonstrated on knowledge-based question answering, showcasing strong performance across various language models (BERT, T5, Codex) and learning paradigms (fine-tuning, in-context learning). Pangu exhibits exceptional sample efficiency, achieving over 50% accuracy with Codex and a single demonstration example. Furthermore, the framework demonstrates robustness under non-i.i.d. settings, attributed to autoregressive models overfitting to seen structures, while Pangu maintains consistent probability distributions across both seen and unseen structures. The core takeaway is that for grounded language understanding, discrimination offers a more effective strategy than generation when leveraging language models.</sample>
    <sample id="228">AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">This paper introduces two novel tasks—Suboptimal-Claim Detection and Claim Improvement Suggestion—aiming to support novice writers in crafting effective argumentative claims. The research investigates how to leverage revision patterns from collaborative online debate platforms like Kialo to automatically assess claim quality. The core idea is to learn from human revision behaviors rather than relying on explicit quality definitions.

However, utilizing revision-based data presents several challenges. The study delineates four key challenges: Representativity and Reliability of the data, Model Complexity and Architecture for capturing subtle revisions, the importance of Contextual Information (debate-wide, parent claim, or domain knowledge), and Topical and User Bias inherent in collaborative environments.

The paper explores various modeling approaches to address these challenges, systematically comparing their performance on the defined tasks. Experiments demonstrate that revision-based data can be effectively utilized for claim assessment, and that modeling the distance between claim versions is beneficial for suboptimal claim detection. Furthermore, the impact of contextual information varies depending on the task and the specific quality issues present in the text. The findings highlight the potential of leveraging revision histories to provide valuable feedback and support for argumentative writing.</sample>
    <sample id="231">A dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) aims to translate spoken language into text in real-time, but current models face challenges including complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency targets. This paper introduces EDAtt (Encoder-Decoder Attention), a novel strategy that leverages existing offline speech translation models without retraining or architectural modifications. EDAtt dynamically controls latency by selectively emitting partial translations based on the cross-attention mechanism between audio input and textual output. Specifically, a word is emitted only when its attention weights are sufficiently dispersed across recent speech frames, indicating stable information reception. This approach allows a single model to operate across various latency regimes. Experiments on German demonstrate that EDAtt outperforms existing strategies applied to offline models, achieving superior translation quality (BLEU score) and lower latency, both in terms of average and computationally-aware lagging. The results highlight the effectiveness of utilizing attention mechanisms to guide SimulST without requiring specialized architectures. To promote reproducibility and further research, the code, models, and simultaneous output are publicly available.</sample>
    <sample id="234">The prompting has a big influence on the performance of LLMs for translation, with differences of up to 40 BLEURT points observed between different prompts.</sample>
    <sample id="235">Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">Each task is equipped with five expert-written instructions.</sample>
    <sample id="237">KITMUS, a diagnostic test suite for knowledge integration, featuring a coreference resolution task.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for meeting summarization, addressing the need for resources tailored to this specific domain. Created by the University of Central Florida, it comprises 1,366 City Council meetings from various cities (Boston, Seattle, Denver) and nearly 7,000 instances, including transcripts, reference summaries, and associated URLs. The data collection process involves using Speechmatics API for transcription, identifying meeting details, locating reference summaries, and aligning timestamps.

The dataset's statistics reveal a mix of abstraction levels in the summaries, with coverage scores generally high (0.7-0.9), indicating a prevalence of verbatim points. Analysis of density scores highlights variations in editing practices across cities. Model evaluations using both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet) summarization systems, alongside GPT-3, demonstrate the potential of extractive methods and the strength of DialogLM for long dialogue summaries.

Human evaluation, conducted with experienced annotators, revealed an unexpected finding: GPT-3 achieved the highest overall scores, particularly in fluency and coherence, despite lower performance in informativeness and factuality according to automatic metrics. This underscores the need for improved automatic evaluation metrics that better reflect human preferences in meeting summarization and highlights the importance of capturing key discussion points. MeetingBank is released to facilitate research and provide insights into City Council decision-making processes.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e presenterò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo il primo studio sistematico del prompting di modelli linguistici di grandi dimensioni per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità MT. Ciò implica l'utilizzo degli ultimi set di test per evitare una sovrapposizione dei dati di test con i dati di addestramento del modello linguistico. E abbiamo confrontato con sistemi all'avanguardia, il sistema con le migliori prestazioni, quindi la valutazione WMT. Utilizziamo metriche MT neurali all'avanguardia e mostriamo anche i risultati della valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione, come possiamo vedere in un semplice esperimento, in cui abbiamo utilizzato il prompting one-shot e fornito due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000, la differenza osservata è di oltre un punto BLEURT. E questo può arrivare, in casi estremi, fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per una strategia di prompting a 5-shot in cui abbiamo semplicemente contrassegnato ogni frase che forniamo al sistema con la lingua in cui si trova. In questo esempio qui, dove eseguiamo la traduzione dal tedesco all'inglese, le frasi tedesche, le frasi di origine, sono contrassegnate con tedesco due punti e le traduzioni inglesi con inglese due punti. Abbiamo visto che la forma effettiva del prompting non ha una grande influenza nel caso di diversi prompting brevi. È cruciale per il prompting zero e one-shot. E quando passiamo, come nel nostro caso, al prompting a 5-shot, c'è quasi nessuna differenza nella forma effettiva del prompting. Sono gli esempi che portano la maggior parte del peso. Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. Quindi è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano prestazioni migliori quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Gli spunti che abbiamo tratto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM hanno detto che la fluidità di PaLM è paragonabile ai sistemi all'avanguardia, ma la differenza principale deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Sembra quindi che PaLM scelga di produrre una traduzione dal suono migliore, a volte omettendo parti della frase di origine che sono fatte nella traduzione. Tuttavia, la categoria "Stile/Imbarazzante" per PaLM è inferiore a quella dei sistemi all'avanguardia, il che è un segnale aggiuntivo che PaLM fornisce un output davvero fluido, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per maggiori dettagli, si prega di partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, dottorando all'Università del Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Più Debole di Quanto Pensi: Uno Sguardo Critico all'Apprendimento con Supervisione Debole". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore dell'etichetta e non generalizzano. Nell'apprendimento con supervisione debole, vengono proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore dell'etichetta in modo che i modelli addestrati generalizzino bene. In recenti lavori sull'ASL, ASL sta per Apprendimento con Supervisione Debole, un'affermazione comune è che le persone dicono di addestrare solo i modelli sui dati debolmente etichettati e di ottenere prestazioni elevate su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un aspetto da considerare, ovvero che le persone presumono che sia disponibile un set di validazione pulito aggiuntivo per la selezione del modello. Non possiamo fermarci a questo problema, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento con supervisione debole. Ma, come un elefante nella stanza, questa necessità viene spesso trascurata. Il suddetto dubbio pone tre domande di ricerca. Innanzitutto, i dati di validazione puliti sono necessari per l'ASL o possiamo usare un set di validazione rumoroso invece? In secondo luogo, se i dati puliti sono necessari, o se i dati puliti sono obbligatori affinché l'ASL funzioni, quanti campioni puliti ci servono? Infine, dovremmo usare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Innanzitutto, scopriamo che, curiosamente, i recenti metodi ASL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, si verifica un calo significativo delle prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci ASL richiedono effettivamente dati etichettati puliti per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato. La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci ASL a ottenere prestazioni migliori, come mostrato nella figura a sinistra. Tipicamente, abbiamo bisogno solo di 20 campioni per classe per ottenere prestazioni elevate. Ma non è finita qui, perché se decidiamo comunque di accedere a campioni puliti, l'addestramento su di essi raggiungerà persino prestazioni migliori. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci ASL, che utilizzano i dati puliti solo per la validazione. Come si può vedere, se abbiamo 10 campioni per classe, il fine-tuning inizia a battere gli approcci ASL. Infine, il miglioramento delle prestazioni rivendicato negli approcci ASL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come si può vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma rispetto ai metodi ASL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw funziona altrettanto bene degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi ASL più complessi che richiedono più tempo di calcolo e spazio su disco. In sintesi, abbiamo dimostrato che i recenti approcci ASL richiedono campioni annotati manualmente e puliti affinché funzionino correttamente. Il loro guadagno di prestazioni e la loro praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Innanzitutto, segnalare i criteri di selezione del modello. Ad esempio, segnalare se la selezione del modello viene eseguita tramite campioni di validazione puliti. In secondo luogo, gli approcci ASL dovrebbero essere confrontati con i baseline di apprendimento few-shot, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un baseline semplice ma efficace che dovrebbe essere considerato in futuro nel lavoro sull'ASL. Infine, abbiamo reso il nostro codice open source. Puoi trovarlo tramite il codice QR su questo slide. Sentiti libero di dare un'occhiata. Grazie e godetevi la conferenza.</sample>
    <sample id="241">This paper introduces a novel human-in-the-loop evaluation framework for early misinformation detection, addressing shortcomings in existing automated approaches. Current systems often rely on unrealistic datasets and neglect the crucial role of human content moderators. Our framework focuses on building end-to-end systems that integrate human feedback throughout the process, from raw social media data to actionable outputs.

We present a case study focusing on COVID-19 treatment misinformation, implementing a system with two components: claim detection and policy violation verification. The claim detection component uses keyword filtering and a T5 question-answering model to extract and rank potentially misleading claims based on trendiness. The policy verification component employs a BERT-based stance classification model to flag tweets supporting unapproved treatments for human review.

Our evaluation demonstrates the system's ability to detect unapproved treatments *before* they are debunked in news articles, operationalizing "early detection" as a key measure of utility. We achieve a 65% precision for policy violation detection and a high efficiency of 124.2 policy violations confirmed per human hour. This framework provides a more realistic assessment of human-system interaction in misinformation detection and offers a valuable perspective for developing future, consistently evaluable, human-in-the-loop systems.</sample>
    <sample id="242">Human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">5</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This paper, "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization," presents a two-step pipeline for identifying reliable Amazon Mechanical Turk (MTurk) workers for summarization tasks, addressing the limitations of automatic metrics and unclear best practices for MTurk recruitment. The pipeline first assesses workers' ability to evaluate summaries across multiple dimensions through a Qualification Task, categorizing them as gold, silver, bronze, or blocked, with only gold and silver workers progressing. Subsequently, an Endurance Task evaluates their capacity for handling large workloads. This rigorous process yielded a small but highly reliable group of 12 workers (6% of 200), achieving inter-annotator agreement (IAA) comparable to experts (Krippendorff's Alpha of 0.443).

A reference-based task further validated their performance, demonstrating a Krippendorff's Alpha of 0.534. Comparisons with baseline MTurk workers (using MACE) and CloudResearch workers revealed comparable quality with the pipeline approach, offering a cost-effective alternative. Analysis indicated a strong correlation between pipeline and CloudResearch workers, and a positive correlation between GPT models and expert judgments. The study concludes that this pipeline effectively filters out unreliable workers, saving resources while maintaining high agreement, and suggests future research should focus on improving worker correctness and expanding the pipeline's applicability across tasks, languages, and platforms.</sample>
    <sample id="246">Sì, sul GitHub.</sample>
    <sample id="247">FACTKG introduces a novel Knowledge Graph-Based Fact Verification task and dataset to address the lack of datasets utilizing knowledge graphs as evidence for natural language claims. Unlike existing datasets like FEVER and TabFact, FACTKG leverages DBpedia as its knowledge source, offering more reliable and practical reasoning capabilities. The dataset comprises claims in both written and colloquial styles, labeled as SUPPORTED or REFUTED, and requires retrieving evidence from DBpedia to verify claims.

FACTKG incorporates five reasoning types: one-hop, conjunction, existence, multi-hop, and negation, reflecting diverse reasoning patterns. The dataset employs techniques like style transfer models and presupposition templates to generate colloquial claims. Experiments demonstrate the effectiveness of utilizing graph evidence, with the GEAR model significantly outperforming baselines that rely solely on claims. FACTKG's practical utility extends to tasks requiring consistency checks between knowledge graphs and natural language, such as dialogue systems that interact with internal knowledge graphs. The dataset and code are publicly available, encouraging further research in knowledge graph-based fact verification.</sample>
    <sample id="248">No, gli annotatori non sono bilanciati rispetto a ciascun gruppo demografico. Il framework re-annotates i dataset per ottenere molte annotazioni per istanza e per ottenere un ricco set di dati demografici.</sample>
    <sample id="249">The input sentences were perturbed by adding noise while attempting to preserve the relevant structure.</sample>
    <sample id="250">Evaluating multiple aspects of chat quality to understand strengths and weaknesses on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">U-CREAT is a novel unsupervised pipeline for Prior Case Retrieval (PCR) that leverages event extraction to improve efficiency and generalization across legal systems. This work addresses the challenge of legal professionals efficiently identifying relevant past precedents from a growing volume of cases. Our key contributions are the IL-PCR dataset, a new benchmark for PCR tasks comprising 7,070 Indian legal cases, and the U-CREAT pipeline itself. IL-PCR significantly expands upon existing datasets like COLIEE’21 in terms of case volume, document length, vocabulary size, and citation density.

U-CREAT employs an event-based approach, extracting subject-verb-object triplets from legal documents using dependency parsing. These events are then used to construct an interaction matrix between query and candidate documents, facilitating retrieval. Experiments across various models—count-based, transformer-based, and event-based—demonstrate that event-based models, particularly the Event Filtered Documents approach, significantly outperform baselines and even specialized legal transformer models. U-CREAT achieves state-of-the-art results on the COLIEE’21 dataset, showcasing its effectiveness and potential for advancing PCR research. The pipeline’s low inference time and high F1 scores highlight its practical utility for legal professionals.</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model designed to detect signs of mental disorders in social media posts. Addressing the challenge of limited annotated data, DisorBERT leverages knowledge transfer from general language models (like BERT) to specialized domains: social media language and mental health. The approach incorporates guided masking, encouraging the model to focus on relevant keywords during training.

The model's effectiveness is demonstrated using the eRisk dataset, showing a balanced precision and recall compared to baseline methods. Analysis of predicted words reveals DisorBERT's tendency to generate terms with a negative psychological orientation, aligning with mental health concerns. For instance, when predicting words related to depression, DisorBERT favors terms like "focus," "talk," and "sleep," while BERT generates more general words. 

Visualization of attention scores on posts from users with high depression scores highlights key terms like "anxious" and "medication," further validating the model's ability to identify relevant content. DisorBERT outperforms MentalBERT, a model trained on a larger dataset, showcasing the benefits of double domain adaptation and guided masking for mental disorder detection. Future work will explore diverse lexical resources and integration of clinical data to enhance performance.</sample>
    <sample id="254">This paper introduces a novel framework, "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction," addressing the noise challenge in distantly supervised (DS) data used for training document-level relation extraction (DocRE) models. Existing methods relying on pseudo-labels risk amplifying noise through false positives. To mitigate this, the proposed framework leverages uncertainty estimation to assess the trustworthiness of model predictions. A key innovation is an instance-level uncertainty estimation method designed to handle overlapping relations, a common issue where multiple relations exist between entity pairs. This method is implemented using Monte Carlo dropout, modified to provide instance-level uncertainty scores for each positive pseudo-label. Furthermore, dynamic class uncertainty thresholds are introduced to filter noisy pseudo-labels, and a multi-phase training strategy iteratively re-labels DS data, capitalizing on its potential while mitigating noise. Experiments on public datasets demonstrate significant performance improvements compared to strong baselines, highlighting the effectiveness of the proposed approach in enhancing DocRE model quality and accuracy. The contributions include the uncertainty-guided framework, the instance-level uncertainty estimation, the dynamic thresholding strategy, and the resulting performance gains.</sample>
    <sample id="255">The form of prompting is crucial for zero-shot and one-shot prompting. When using five-shot prompting, the form has nearly no influence.</sample>
    <sample id="257">Quattro modelli di dialogo all'avanguardia.</sample>
    <sample id="258">This work explores the potential of large language models (LLMs) as an alternative to traditional human evaluation in natural language processing. Recognizing the instability and reproducibility challenges inherent in human evaluation, the researchers investigated whether LLMs could be instructed to assess text quality, mirroring the human evaluation process. While LLM-based evaluation isn't entirely novel, the paper highlights that its application as a direct substitute for human evaluation was unexplored at the time of submission.

The study focuses on evaluating stories generated by GPT-2 and human authors, using LLMs to rate them based on grammar, coherence, likability, and relevance. Ground truth ratings were obtained through human evaluation by English teachers, considered experts in essay scoring. Four LLMs—T0, InstructGPT (curie and davinci), and ChatGPT—were tested.

Results showed that English teachers preferred human-written stories over GPT-2 generated ones. While smaller LLMs lacked clear preference, Davinci and ChatGPT demonstrated a significant preference for human-written text, aligning with the human evaluators. This suggests that certain LLMs can effectively serve as a viable alternative to human evaluation, offering a potentially more stable and reproducible method. The paper addresses further questions regarding agreement with human ratings, instruction sensitivity, sampling methods, cost-benefit analysis, and applicability to other NLP tasks, all detailed within the full paper.</sample>
    <sample id="259">XSemPLR is a novel benchmark dataset for cross-lingual semantic parsing, designed to evaluate models translating natural language queries into various meaning representations (e.g., SQL, Lambda Calculus) across multiple languages. The dataset encompasses 9 datasets across diverse domains, 5 semantic parsing tasks, 8 meaning representations, and 22 languages spanning 15 language families. To facilitate thorough evaluation, XSemPLR introduces six distinct training and evaluation settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study analyzes performance using Encoder-PTR and Encoder-Decoder models, revealing that Encoder-Decoder architectures consistently achieve superior results. Training multilingual models with a mixture of languages generally improves performance across most languages, though English performance can sometimes decline ("Curse of Multilinguality"). Analysis of cross-lingual transfer demonstrates a significant performance gap in zero-shot settings, which rapidly diminishes with few-shot learning. The research highlights the benefits of pretraining on English for boosting few-shot performance in other languages and identifies limitations in current large multilingual language models like Codex and BLOOM for cross-lingual semantic parsing. XSemPLR provides a valuable resource for advancing research in this area.</sample>
    <sample id="260">Non specificato.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">Non specificato.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to various design biases. While prior research has highlighted search instability, a systematic categorization of bias types and mitigation strategies has been lacking. We introduce a typology of label biases in text classification, identifying three key components: vanilla-label bias (model's inherent preference), context-label bias (influence of the provided examples), and a novel *domain-label bias* (impact of the task corpus). Our experiments demonstrate that exposure to random in-domain words can significantly bias LLM predictions, even without specific examples.

We propose *domain-context calibration*, a novel method to mitigate these biases. Unlike existing calibration techniques that use predefined content-free tokens, our approach utilizes random in-domain words to account for domain-label bias. Extensive experiments across diverse datasets and models (including GPT-3) show that domain-context calibration significantly improves ICL performance, particularly on tasks with high domain-label bias. Calibration studies reveal that single, predefined tokens are themselves biased, and using multiple random words, especially in-domain ones, leads to superior results. This work provides a systematic investigation of label biases in ICL and offers a practical calibration method to enhance the reliability and performance of LLMs.</sample>
    <sample id="264">TAVT: Towards Transferable Audio-Visual Text Generation addresses the challenge of multimodal text generation, specifically audio-visual text generation, where data annotation is costly and existing models struggle with domain shifts. This paper introduces a novel task, Transferable Audio-Visual Text Generation, designed to overcome these limitations by focusing on multi-modal domain shifts in visual style, audio energy, and other characteristics. The core idea is to leverage a unified audio semantic space to align visual concepts across domains, recognizing that audio content is less susceptible to domain variations than visual content.

The proposed framework comprises three key components: an audio-visual meta-mapper network for mapping visual concepts to a unified audio semantic space, an audio-visual encoder and language model generator utilizing modality-aware attention, and a Dual Counterfactual Contrastive Learning (DCLL) loss function for direct visual-textual alignment. The model is trained using a meta-learning approach, adapting quickly to new domains with limited labeled data. Experiments on MSVD and MSR-VTT benchmarks demonstrate that TAVT significantly outperforms state-of-the-art models in both cross-dataset and cross-domain settings, particularly in low-resource scenarios, showcasing its effectiveness in transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">The authors' affiliations are mentioned in the paper "Why wouldn't you use universal dependencies."</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. E oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato svolto dal laboratorio Emory NLP guidato dal Professor Jinho Choi presso l'Emory University e in collaborazione con Amazon Alexa AI. Quindi, diciamo che hai appena sviluppato un modello di dialogo e vuoi vedere quanto si confronta con lo stato dell'arte attuale. La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di scegliere quale delle due conversazioni è migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare molteplici dimensioni della qualità della chat per comprendere i punti di forza e di debolezza del modello a un livello più granulare. Un approccio è quello di chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello utilizzando metodi comparativi o di scala Likert esistenti. Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ciascuna risposta del modello esprime o meno determinati comportamenti, come rispondere con informazioni irrilevanti o contraddire se stesso. Chiamiamo questo approccio l'annotazione dei comportamenti nella chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti per influenzare la qualità della chat nella letteratura recente. ABC-Eval è in grado di misurare i tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il proprio partner o dice qualcosa di irrilevante, si contraddice o contraddice il proprio partner, allucina fatti errati o viola la conoscenza del senso comune e quando il modello riesce o fallisce nel mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati della qualità del dialogo, poiché questo è lo standard per valutare i modelli di chat lungo molteplici dimensioni. Dalla nostra analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento di ABC-Eval sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni etichettate in modo doppio. Inoltre, le etichette di ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, puoi vedere come misurare la proporzione di turni con contraddizioni di sé e del partner spiega il 5% e il 10% della qualità della conversazione, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare a stepwise. Puoi vedere come la combinazione di tutte le metriche di ABC-Eval spiega oltre il 25% della qualità della conversazione e, man mano che rimuovi le metriche una alla volta, la maggior parte di esse comporta la perdita di una discreta quantità di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità e meno di queste metriche trasportano informazioni uniche. Queste metriche ABC-Eval affidabili, informative e distinte ci consentono di valutare l'IA conversazionale con una risoluzione superiore a quella che i metodi precedenti sono in grado di raggiungere. Puoi vedere che nei risultati del nostro esperimento rimangono diverse sfide e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune in circa il 20% delle loro risposte. Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o contraddicono il proprio partner circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per perseguire metriche di valutazione affidabili e precise per il confronto dei modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel settore come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale progredirà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Emory NLP Lab (Emory University) and Amazon Alexa AI.</sample>
    <sample id="271">Fine-tuning.</sample>
    <sample id="272">7</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente fosse "Le cose potrebbero iniziare a diventare pericolose se i ministri lo scoprono", allora "mole" si riferisce a una spia. Ma se la frase precedente fosse "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un neo. A seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. Innanzitutto, solo una piccola parte delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata delle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano sulla conoscenza del dominio e sulla curatela umana. In questo lavoro, cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E secondo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. E questo viene fatto misurando quanto informazioni il contesto C fornisce sul target Y, dato la sorgente X. Puoi pensare a CXMI come alle informazioni ottenute fornendo il contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI che può misurare l'utilizzo del contesto a livello di frase o a livello di parola. Possiamo pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con un alto P-CXMI per cercare schemi tra queste parole. E conduciamo la nostra analisi su trascrizioni di TED talk che sono state tradotte dall'inglese in 14 lingue diverse. Conduciamo la nostra analisi a tre livelli diversi. Innanzitutto, esaminiamo i tag part-of-speech che hanno un alto P-CXMI medio. Questo ci permette di trovare, ad esempio, pronomi duali in arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato perché l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che anche alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Quindi esaminiamo gli elementi di vocabolario che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come quello qui, in cui in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre nella forma di cortesia corretta. Infine, esaminiamo diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati veramente dalla parola stessa, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno. E abbiamo chiamato il nostro tagger Multilingual Discourse-Aware, o MuDA tagger. Possiamo quindi notare che le diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Quindi usiamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il tagger MuDA ha identificato. Infine, usiamo il nostro benchmark, così come altre metriche, per valutare diversi modelli sulla traduzione a livello di documento. Innanzitutto, quando usiamo metriche a livello di corpus: quindi per BLEU, troviamo che i modelli indipendenti dal contesto hanno le migliori prestazioni. Ma poi se usiamo COMET, i modelli consapevoli del contesto funzionano meglio. E se usiamo la word f-measure, allora i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo metriche a livello di corpus da sole. Ora usiamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati rispetto ai modelli che non utilizzano il contesto per determinati fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori rispetto ai modelli che non utilizzano il contesto su altri fenomeni come pronomi e forme verbali. Questo suggerisce in qualche modo dove dovremmo vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. In sintesi, conduciamo un'analisi basata sui dati in 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e quindi utilizziamo i nostri risultati per costruire un benchmark per la traduzione a livello di documento che può aiutarci a identificare quali fenomeni del discorso i modelli possono gestire bene o meno e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Vi vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This paper introduces "IndicMT Eval," a novel dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. Recognizing the limitations of applying English-centric evaluation metrics to languages with diverse linguistic features, the study focuses on five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati. The dataset comprises 7,000 samples generated from 200 source sentences translated into English by seven different MT models. Bilingual expert annotators meticulously evaluated these translations, marking errors with type and severity using the MQM framework, and providing overall scores.

The analysis reveals that COMET-metric variants exhibit the highest overall correlations with human judgments. However, many metrics demonstrate skewed score distributions, limiting their interpretability. Interestingly, correlations improve when focusing on accuracy errors specifically. Leveraging the annotated data, the authors fine-tuned COMET, creating "IndicCOMET MQM," which outperforms standard COMET baselines across multiple languages and demonstrates improved zero-shot performance on unseen languages. Furthermore, IndicCOMET MQM exhibits greater robustness on the ACES Translation Accuracy Challenge Sets. The publicly available dataset aims to facilitate more accurate and nuanced evaluation of MT systems for Indian languages, addressing a significant gap in current MT evaluation practices.</sample>
    <sample id="277">Il metodo non ha un nome.</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness" to identify words that distinguish marked groups from unmarked ones, using weighted log-odds ratios to compare the top words for each marked group.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">MultiEMO is a novel attention-based multimodal fusion framework for emotion recognition in conversations (ERC) designed to address limitations in existing approaches. The framework tackles challenges including inadequate exploitation of multimodal complementarity, poor performance on minority emotion classes, and difficulty distinguishing semantically similar emotions. MultiEMO comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. 

A core contribution is VisExtNet, a visual feature extractor that focuses solely on facial expressions, eliminating redundant scene information. The MultiAttn module, built upon bidirectional multi-head cross-attention layers, facilitates effective integration of textual, audio, and visual modalities. Furthermore, a Sample-Weighted Focal Contrastive Loss (SWFC) is introduced to prioritize minority classes and enhance discrimination between semantically similar emotions.

Extensive experiments on the MELD and IEMOCAP datasets demonstrate state-of-the-art performance, with significant improvements in recognizing minority and semantically similar emotions. Visualization of attention heatmaps highlights MultiEMO's ability to handle complex scenarios where emotional cues from different modalities are asynchronous. While limitations exist, including speaker differentiation in VisExtNet and batch size requirements for SWFC, MultiEMO represents a significant advancement in multimodal emotion recognition.</sample>
    <sample id="281">This work investigates when translation requires context and how well current models handle these cases. Traditional evaluation metrics like BLEU struggle to capture context-dependent translations, which represent a small portion of overall translations. To address this, we extend CXMI to Pointwise CXMI (P-CXMI), a measure of context usage at the sentence or word level, analyzing English-to-14-language TED talk transcripts. Our analysis reveals patterns in context dependency across part-of-speech tags (e.g., dual pronouns in Arabic), vocabulary items (e.g., proper noun consistency in Chinese), and sentence structure (e.g., ellipsis resolution).

Based on these findings, we developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words related to five discourse phenomena. We then created a benchmark for document-level translation, evaluating models using MuDA and other metrics. Results show that corpus-level metrics like BLEU favor context-agnostic models, while COMET favors context-aware models. Context-aware models outperform context-agnostic models on formality and lexical cohesion but not on ellipsis, pronouns, or verb forms. DeepL consistently demonstrates superior document-level translation accuracy compared to Google Translate. This research highlights areas for improvement in document-level machine translation and provides a valuable benchmark for future development.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story-level text style transfer, addressing a significant gap in natural language generation. Unlike previous studies focused on token or sentence-level style transfer, StoryTrans operates at the discourse level to effectively imitate author style. The core challenges lie in capturing complex authorial linguistic preferences, particularly discourse structures, and preserving content while transferring style, especially when styles are topic-specific.

To tackle these challenges, StoryTrans learns discourse representations and combines them with style embeddings for generation. A key innovation is a training objective that disentangles style and content within discourse representations and enhances content preservation through a two-stage generation process: first transferring the source text with masked content keywords, then generating the full text incorporating these keywords. The training framework utilizes an advisory approach with self-reconstruction, disentanglement, sentence order, and style classifier losses.

Evaluations on newly collected Chinese and English datasets demonstrate StoryTrans's superior performance over baselines in both style control and content preservation, confirmed by automatic and manual evaluations. Style visualization further validates the alignment of transferred texts with target styles. StoryTrans effectively enriches storylines and maintains source semantics, showcasing its ability to rewrite sentences while preserving meaning. Data and code are publicly available.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction addresses limitations in existing span-based Universal Information Extraction (UIE) models, which overly rely on precise span boundaries prone to annotation ambiguity. We propose a novel approach where the learned span boundary is "fuzzy" rather than precise, and adaptively adjusts attention to better model span characteristics.

Our core contributions are a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL incorporates Binary Cross Entropy and KL-divergence to model the boundary as a continuous probability distribution, while FSA dynamically adjusts the attention span using an optimizable parameter and linearly decays attention at the boundaries. This mask function guides the model's decision process without impacting text encoding.

Experiments on Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction demonstrate FSUIE's effectiveness. FSUIE-base significantly outperforms UIE-base, particularly on small datasets. It achieves state-of-the-art results on ACE2004, 2005, and ADE datasets for relationship extraction, showcasing superior generalization capabilities. Furthermore, FSUIE achieves state-of-the-art results on AST-V2 datasets. Ablation studies confirm that FSL and FSA improve convergence and information extraction capabilities, respectively. Visualization of attention distributions reveals a focus on relevant semantic information within a limited range, validating our design. FSUIE offers a unified structure for diverse IE tasks, achieving excellent performance through a novel fuzzy span mechanism.</sample>
    <sample id="285">This paper introduces "Reference Matters," a study addressing factual errors in dialogue summarization and critiquing existing Factual Error Correction (FEC) model evaluation methods. While current approaches utilize factuality metrics like FactCC and DAE, the authors argue these are vague, unreliable, and conflate error correction with summary rewriting. To overcome these limitations, the paper proposes a novel evaluation framework grounded in manually annotated reference corrections, emphasizing minimal, fluent, and non-redundant error rectification.

The framework employs a new taxonomy of factual errors, categorizing them as content-based (part-of-speech, dependencies) and form-based (addition, deletion, substitution). Leveraging ERRANT, the evaluation process involves alignment, classification, and comparison against reference corrections. Experiments reveal that training FEC models with dialogue-specific reference summaries improves performance on unreliable factuality metrics, highlighting the need for evaluation reform. Furthermore, combining human-annotated data with synthetic data proves promising. The study also identifies current FEC models' limitations in correcting additions and addressing complex errors like attribute, modality, and link errors, underscoring areas for future research and development in factual error correction for dialogue summarization.</sample>
    <sample id="286">James Finch e Sarah Finch.</sample>
    <sample id="287">Four.</sample>
    <sample id="288">BLiMP, SyntaxGym, CrowS pairs.</sample>
    <sample id="290">WSL, COSINE, FTw.</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This work presents EPIC, a novel English Perspectivist Irony Corpus developed through a collaboration between the University of Turin and Amazon Alexa. EPIC addresses the limitations of traditional supervised machine learning approaches in Natural Language Understanding, which often rely on a single "ground truth" annotation. Focusing on the complex and pragmatic phenomenon of irony, the research explores the variability in human perception of irony rather than aiming for a definitive classification.

EPIC comprises approximately 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. Data was annotated by 74 crowdworkers via Prolific, with each annotator providing five annotations per conversation. Analysis revealed significant inter-annotator disagreement, varying across demographics like gender, age, and nationality.

To account for these differences, the researchers developed "perspective-aware models" – multiple models fine-tuned on data subsets representing different annotator groups. While raw performance wasn't significantly improved, these models demonstrated notably higher confidence in their predictions compared to aggregated "gold standard" models. Further investigation into annotation discrepancies revealed that disagreement was highest between generations close in age and between annotators from the UK and Ireland, suggesting nuanced cultural and generational influences on irony perception.</sample>
    <sample id="297">This project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," investigates the phenomenon of dogwhistles—terms that convey a hidden, often inflammatory, message to an in-group while maintaining plausible deniability for the speaker. The research addresses the challenge of studying these subtle forms of communication, which are crucial for understanding political influence, persuasion, and the evasion of content moderation.

The project developed a comprehensive typology and glossary of over 340 dogwhistles, primarily focused on racist, transphobic, and anti-Semitic terms, drawing from diverse sources. Dogwhistles are categorized by register (formal/informal), type (implicature vs. covert signaling), and persona (e.g., anti-Semitic, transphobic). A case study of historical U.S. political speeches revealed a correlation between dogwhistle usage and the Republican Southern Strategy, demonstrating a shift towards coded language following the Civil Rights era.

Furthermore, the study evaluated language models (GPT-3) for dogwhistle recognition, finding variable performance—particularly poor recognition of informal and transphobic dogwhistles. Finally, a toxicity detection case study demonstrated that replacing explicit slurs with dogwhistles significantly reduces automated toxicity scores, highlighting their effectiveness in evading content moderation systems. This work contributes to NLP and linguistics by questioning traditional notions of meaning and provides insights into the challenges of detecting and mitigating harmful online rhetoric.</sample>
    <sample id="298">Retraining or pre-training some models with more recent data resulted in performance degradation with a larger temporal gap, confirming the hypothesis that temporal drift is the main cause of the performance drop.</sample>
    <sample id="299">This work addresses the brittleness of Natural Language Inference (NLI) models, which often rely on spurious correlations (shortcuts) present in training data, leading to poor out-of-distribution generalization. Existing shortcut mitigation techniques typically require auxiliary models with domain-specific knowledge and can suffer from inaccurate uncertainty estimations. To overcome these limitations, we propose a novel minimax training approach that encourages NLI models to focus on challenging, under-represented "hard" examples that contradict shortcuts.

Our method involves a learner model trained on the NLI task and an auxiliary model that dynamically generates example weights to maximize the learner's loss, incentivizing it to learn from hard examples. This alternating optimization process, using standard techniques like stochastic gradient descent, doesn't require prior knowledge of shortcut types. The auxiliary is modeled as a lightweight feed-forward network.

Evaluations on MNLI, FEVER, and QQP datasets, against standard training and state-of-the-art shortcut mitigation methods, demonstrate consistent improvements in out-of-distribution performance (HANS Symmetric, PAWS) while preserving in-distribution accuracy. Further analysis explores the impact of model size, synthetic shortcuts, and out-of-domain testing, alongside a qualitative examination of the learned example weight distribution. This approach offers a robust and adaptable solution for enhancing NLI model generalization.</sample>
    <sample id="300">Interactive dictation is a novel task that combines speech dictation and editing through natural language commands, moving beyond traditional speech-to-text systems. This work introduces and formalizes interactive dictation, characterized by flexible interleaving of dictation and editing using intuitive, open-ended utterances. The research team developed a data collection interface and built a dataset to support this task, and created a baseline system comprised of four key steps: automatic speech recognition (ASR), utterance segmentation, command extraction and normalization, and sequential execution of dictation and commands.

The system utilizes separate models for each step, experimenting with architectures like T5 and GPT-3 for the interpretation model, exploring both program prediction and direct state prediction. Evaluation reveals a trade-off between runtime and accuracy, with GPT-3 demonstrating higher accuracy but slower performance. Notably, direct state prediction proves more effective with GPT-3, while program prediction enhances efficiency for T5 models. Despite promising initial results, significant room for improvement remains. To encourage further research, the code and dataset have been released publicly. This work represents a crucial step towards more natural and intuitive voice-based document editing interfaces.</sample>
    <sample id="302">Because after the first step, all the right tokens are present, but they are not ordered.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias perché non è chiaro se i modelli stiano generando stereotipi positivi a causa di un allineamento eccessivo dei valori o di altri metodi anti-stereotipici, e questa mancanza di trasparenza impedisce ulteriori studi.</sample>
    <sample id="304">Sentences chosen from the same matching but deemed unacceptable.</sample>
    <sample id="305">This work critically examines recent advances in Weakly Supervised Learning (WSL), questioning the common claim of achieving high performance solely on weakly labeled data. While technically valid when a clean validation set is used for model selection, this crucial dependency is often overlooked. Our research investigates whether clean validation data is essential for WSL, how much is needed, and how best to utilize it.

We demonstrate that current WSL methods heavily rely on clean validation samples; without them, generalization beyond the weak labels is severely limited. Surprisingly, only a small number of clean samples (around 20 per class) are typically required for effective model selection. However, we find that directly fine-tuning a model on these clean samples consistently outperforms WSL approaches. Furthermore, allowing continuous fine-tuning on the clean validation set enables a simple baseline (FTw) to achieve comparable performance to more complex WSL methods.

Our findings suggest that the performance gains attributed to WSL are often overestimated and that the annotation cost of clean validation samples should be carefully considered. We recommend future WSL research to explicitly report model selection criteria, compare against few-shot learning baselines, and incorporate continuous fine-tuning as a strong baseline. The code for our experiments is publicly available.</sample>
    <sample id="306">This paper investigates the extent to which large language models (LLMs) can track entities and their state changes within a discourse, a crucial ability for understanding longer texts. Existing LLMs haven't been systematically evaluated for this capability, and challenges arise due to potential shortcuts like reliance on common patterns in pre-training data or simple word-state associations. To address these, the authors designed a novel evaluation task involving boxes and objects, incorporating state-changing operations while actively preventing heuristic exploitation.

Experiments with Flan-T5, GPT-3, and GPT-3.5 using 2-shot in-context learning revealed a significant difference in performance. Only text-davinci-003 demonstrated non-trivial entity tracking, while others performed below a random baseline. Further analysis focusing on the GPT series highlighted a correlation between pre-training on code and the emergence of entity tracking abilities. Fine-tuning smaller models like T5-base enabled entity tracking, but randomly initialized models failed even with direct supervision, reinforcing the importance of pre-training. While promising, the generalizability of these observed tracking abilities remains an open question, and the paper encourages further exploration, including results from GPT-4, detailed in the full paper.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework for characterizing design biases in NLP datasets and models by comparing their annotations with those of diverse real users. Existing work has hinted at positionality—the aggregation of judgments reflecting specific perspectives—in NLP, but lacked systematic comparison. This study addresses this gap by re-annotating datasets (Social Chemistry and Dynahate) with over 1,600 annotators from 87 countries via the Lab in the Wild platform. The annotations were then compared to existing datasets and models (Perspective API, Rewire API, Hate Roberta, GPT-4) using correlation scores.

The findings reveal significant positionality: datasets and models demonstrate strong alignment with English-speaking countries and individuals with college education. Conversely, they exhibit less alignment with non-binary individuals. These biases highlight the risk of leaving certain populations underserved by NLP technologies. To mitigate these issues, the authors recommend meticulous documentation of design choices, adopting a perspectivist approach to NLP research, and developing specialized datasets and models tailored to specific communities, exemplified by initiatives like Masakhani. Ultimately, NLPositionality underscores the importance of moving beyond universal applicability and striving for inclusive NLP that considers diverse perspectives.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The affiliations of the authors are not mentioned in the provided text.</sample>
    <sample id="312">MultiInstruct è il primo benchmark dataset multi-modale per l'instruction tuning, composto da 62 compiti diversi che coprono 10 categorie ampie.</sample>
    <sample id="313">Three.</sample>
    <sample id="314">Coordination of two sentences.</sample>
    <sample id="315">I prompt sono stati ispirati da uno studio in cui sono stati dati a soggetti umani.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="317">CodeIE introduces a novel approach to information extraction (IE) by reframing it as a structure-to-structure code generation task, leveraging large code language models like Codex. Traditional IE models utilizing pre-trained language models (e.g., T5, GPT-3) face challenges due to mismatched input-output formats between pre-training and inference, requiring extensive structured training data and specialized decoding. CodeIE addresses this by converting text into structured code representations, ensuring alignment between input and output structures.

The method employs carefully designed prompts that define code functions for IE tasks (e.g., named entity recognition, relation extraction), guiding the model to extract information and append it to structured lists. Experiments across multiple datasets demonstrate that CodeIE, using code language models and code-style prompts, significantly outperforms traditional text-based approaches like UIE and GPT-3, particularly in few-shot settings. Analysis reveals that code models exhibit lower perplexity on code-formatted inputs and generate fewer structural errors. Furthermore, CodeIE mitigates issues like generating unsupported labels and consistently outperforms GPT-3, highlighting the benefits of code-centric IE and the effectiveness of code-style prompts, especially for recall. The publicly available paper and code aim to inspire further research in this area.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un Modello Pre-addestrato Robusto in Francese per i Domini Biomedico e Clinico". In questa presentazione, parleremo innanzitutto del linguaggio modeling nell'assistenza sanitaria. Quindi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, che è un set di dati medici raccolti dal web. Abbiamo anche introdotto un confronto di modelli con molteplici impostazioni di pre-addestramento e fonti di dati. Quindi presentiamo i nostri risultati su 11 compiti biomedici e clinici in francese. Infine, concludiamo sugli esperimenti e vi forniamo maggiori dettagli su come accedere a questi modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre enormi guadagni di prestazioni rispetto a metodi statici e contestuali storici come Word2vec, fastText o altro. Da allora, questo modello è stato adattato a molte altre lingue, come in francese con CamemBERT, e anche in domini come il biomedico con PubMedBERT e BioBERT e sul clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altre lingue sono scarsi e spesso si basano su pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Quindi ci siamo posti una domanda su quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi e se i dati raccolti siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanta dati servono per addestrare un modello specializzato in francese? Sono 4 gigabyte, 8 gigabyte o più? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di un set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi prese da note cliniche; e una versione finale di ChuBERT con una combinazione di 4 GB di set di NACHOS e 4 GB di note cliniche. Oltre a questo confronto, abbiamo introdotto tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro sempre basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato sul modello biomedico inglese PubMedBERT, e addestrato su 4 GB di set di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo dati per compiti downstream pubblici e privati come il riconoscimento di entità nominate, la classificazione, il part-of-speech tagging e il question answering. Questi modelli sono confrontati con sei modelli di base che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli funzionano meglio sui compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti eterogenee appaiono più versatili. Osserviamo anche che l'utilizzo di più dati si traduce in prestazioni migliori. Nel complesso, il pre-addestramento da zero sembra ottenere prestazioni più elevate nella maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento di controllo utilizzando i pesi e il tokenizzatore di CamemBERT addestrato su un sottoinsieme di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Questo non è il caso del modello basato sui pesi e sul tokenizzatore di CamemBERT, che soffrono di problemi di stabilità. Infine, in conclusione, il nostro sistema proprietario ha offerto prestazioni migliori in nove degli 11 compiti downstream e ha superato globalmente il risultato del modello generico, qui CamemBERT. Osserviamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, e sotto la licenza MIT, e tutti gli script di addestramento sono nel nostro repository GitHub. Quindi grazie per questa presentazione, e siamo ansiosi di scambiare idee durante la sessione poster a Toronto.</sample>
    <sample id="319">From-scratch pre-training, continual pre-training, and comparison of models with multiple pre-training settings and data sources.</sample>
    <sample id="320">Non è stato osservato overfitting adattivo.</sample>
    <sample id="321">The quality of simplification was evaluated by fine-tuning language models (long-mBART and mBART) and comparing their scores and evaluation metrics against baseline scores.</sample>
    <sample id="322">This paper investigates what text classifiers learn about morality, moving beyond the traditional binary "moral vs. immoral" classification. Drawing on Moral Foundation Theory, which posits that morality is perceived through five distinct foundations (Care, Fairness, Loyalty, Authority, and Sanctity), the research explores how language models understand and represent morality in text. The study utilizes the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven diverse domains, to examine domain-specific expressions of morality.

The research employs explainable AI techniques to analyze language models trained on moral text classification tasks. A key finding demonstrates that language models can recognize nuanced differences in moral expression across domains. For example, the concept of "subversion" is associated with negative connotations (overthrow, mayhem) within the "All Lives Matter" domain, while it is viewed more favorably within the "Black Lives Matter" domain. This highlights the importance of considering domain-specific contexts when evaluating moral judgments.

The findings caution against applying a single model across diverse domains, as it can lead to misinterpretations of morality. The paper underscores the need for more sophisticated approaches that account for the pluralistic and context-dependent nature of human morality, ultimately contributing to safer and more accurate language model applications.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods combining Language Models (LMs) and Knowledge Representation Learning (KRL). Current approaches often retrieve noisy entities from knowledge bases, encode text and subgraphs in isolation, and neglect semantic relationships between entities. DHLK constructs a Heterogeneous Knowledge Graph (HKG) from multiple knowledge bases, employing a two-stage pruning strategy and KRL to optimize its structure and knowledge representation. Key entities are expanded with paraphrases from WordNet and Wiktionary. RoBERTa and Relation Mask Self-Attention (RMSA), inspired by RGAT, are used to encode and fuse QA contexts and entities, dynamically removing irrelevant entities based on attention weights. TransE optimizes entity and relation embeddings within the HKG. Path information from the HKG is integrated into the QA context, and a multi-layer perceptron (MLP) predicts the final answer. Experiments on CommonsenseQA and OpenBookQA, utilizing ConceptNet, WordNet, and Wiktionary, demonstrate DHLK's superior performance compared to existing LM and HKG methods, achieving state-of-the-art results.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Occupano tutti e quattro i quadranti dello spettro politico e GPT-4 è il modello più liberale.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi vi darò una breve introduzione al nostro articolo su "Generalizzazione Composizionale senza Alberi utilizzando Tagging di Multinsiemi e Permutazioni Latenti". Questo è un lavoro congiunto con i miei relatori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere compresa come la capacità di un discente di gestire una ricorsione più profonda e composizioni inedite di frasi che sono state viste individualmente durante l'addestramento. Nel contesto del semantic parsing, il test per la generalizzazione composizionale potrebbe assomigliare a questo. Come al solito, abbiamo un insieme di addestramento di espressioni. In questo caso, "La ragazza dormiva" e "Mary sapeva che la ragazza dormiva". Queste espressioni sono abbinate a forme logiche che rappresentano gli aspetti principali del loro significato. A differenza della valutazione standard del machine learning, l'insieme di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente inedite. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento ed è testato su un esempio con una ricorsione più profonda. I modelli seq2seq naive faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono distaccati dall'input. In particolare, spesso non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle colorate nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono destinati a catturare il processo composizionale che mette in relazione le espressioni con le forme logiche. Questo funziona bene, ma gli alberi di solito non sono forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. In genere, ciò comporta una pre-elaborazione specifica del formalismo delle forme logiche, ad esempio per gestire i simboli di variabile. L'ottenimento di alberi può anche comportare procedure specializzate di induzione della grammatica. In questo articolo, non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede la previsione dell'output dall'input in due fasi. Innanzitutto, etichettiamo ogni token di input con un multinsieme non ordinato di token che appariranno nell'output. Dopo la prima fase, abbiamo tutti i token giusti, ma non sono ordinati. Ecco perché nella seconda fase utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente così. Passiamo da sinistra a destra sull'output e determiniamo quale token del multinsieme inserire in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Quindi saltiamo al token successivo del multinsieme per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token del multinsieme. Continuiamo questo processo finché ogni token della prima fase non è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto impegnativi, però. Nel nostro articolo, risolviamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è fornito nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multinsieme proviene, il che pone una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta alla sfida di trovare la permutazione con il punteggio più alto, che è NP-difficile. Questo perché è correlato al problema del "commesso viaggiatore". Lo approssimiamo con una rilassazione continua adatta alla GPU che ci consente anche di retropropagare la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, dai un'occhiata al nostro articolo o vieni al nostro poster.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) architecture designed to improve representation learning by effectively aggregating insights from unimodal experts at different levels. Building upon the BridgeTower approach, ManagerTower addresses its limitations by introducing managers within each cross-modal layer to adaptively combine multiple unimodal representations. Unlike BridgeTower's fixed layer connections, ManagerTower allows for flexible exploitation of semantic knowledge across various unimodal layers.

The architecture utilizes RoBERTa and CLIP-ViT base as unimodal encoders and employs managers to facilitate comprehensive cross-modal alignment and fusion. Experimental results demonstrate that ManagerTower achieves superior performance on various downstream tasks, including Visual Question Answering, even with a relatively small pre-training dataset of four million images. Notably, it outperforms existing models like METER and BridgeTower, showcasing a 39.15% accuracy improvement on the Wikivideo test standard.

Visualization of manager aggregation weights reveals that adaptive managers dynamically adjust their focus on different unimodal layers based on the cross-modal layer, contrasting with the static, less intuitive behavior of static managers. This adaptability highlights ManagerTower's ability to leverage diverse levels of unimodal semantic knowledge for enhanced VL representation learning. The paper, code, and models are publicly available.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, addressing the limitations of existing methods that rely on pseudo-label generation. Traditional methods often suffer from simplistic pseudo-queries, misalignment between queries and video segments, and vulnerability to label noise. To overcome these challenges, we propose a noise-resistant Structured Pseudo-Label generation method. Our approach first leverages a pre-trained image captioning model (BLIP) to generate complex, free-form pseudo-queries. Subsequently, we utilize a pre-trained model to assess frame relevance to these queries, creating pseudo-events that ensure high relevance within the event and low relevance outside it. To mitigate label noise, we implement sample re-weighting based on prediction confidence and IoU, and refine pseudo-labels by incorporating high-confidence predictions. We densely sample video frames, model temporal event structure, and filter redundant pseudo-query-event pairs. Experiments on ActivityNet Captions and Charades-STA demonstrate that our method significantly outperforms existing zero-shot approaches, achieving state-of-the-art results on both datasets using metrics like R@M and mIoU. The code is publicly available.</sample>
    <sample id="330">Yes, cumulative training performed equal or better than iterative training.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Trascrizioni di TED Talks.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation addresses the generalization limitations of neural machine translation (NMT) models stemming from non-smooth representation spaces and sparse low-frequency token distributions. While kNN-MT offers a solution by smoothing predictions using nearest neighbors, it suffers from slow inference due to large datastores and difficulty in updating representations. INK overcomes these drawbacks by introducing a novel training framework that iteratively refines the NMT model's representation space using kNN knowledge.

The INK training loop extracts kNN knowledge to guide an adapter in adjusting representations, followed by asynchronous datastore updates. This process aligns contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token via KL-divergence, enriching semantic meaning and addressing sparsity. Crucially, INK allows for datastore removal during inference, reducing memory footprint and accelerating speed.

Experiments on the WMT’19 German-English news translation task demonstrate that INK significantly outperforms state-of-the-art kNN-MT systems, achieving an average gain of 1.99 COMET score and 1.0 BLEU score. Results also highlight the benefits of jointly using adapters and datastores for further representation smoothing, suggesting potential for even greater improvements with optimized frameworks. INK achieves superior performance with reduced memory and faster inference, showcasing its effectiveness in enhancing NMT generalization.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This paper introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to address the challenge of representing out-of-vocabulary (OOV) words in embedding-based models. Inspired by human learning strategies, the method leverages word formation and association to infer OOV word meanings. A Word Relationship Graph is constructed, mimicking lexical rules by tokenizing OOV words into wordpieces and connecting them to relevant words, forming a two-level graph. A self-attention network assigns attributes to OOV nodes based on their characters, while a concatenated two-layer Graph Attention Network refines node-level representations. A readout block captures graph-level information, summarizing word formation. Contrastive learning, utilizing NT-XENT positive samples like two-hop neighbors and synonyms, ensures alignment with background embeddings. Extensive experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of the word formation-based learning approach. The model's adaptability to various word formations suggests potential for application to other languages, with agglutinative languages being particularly well-suited. The research highlights the model's ability to handle complex word formations and its potential to benefit both static and contextual models in downstream applications.</sample>
    <sample id="338">This research investigates the critical question of how to objectively evaluate the quality of human-generated natural language explanations used to enhance machine learning models. While humans often provide explanations to improve model understanding and performance, their subjective and task-dependent nature makes traditional evaluation methods inadequate. This work challenges the assumption that human annotations represent a gold standard and introduces a novel evaluation metric, TREU, which extends the simulatability score.

The study proposes a unified data structure converting diverse tasks into a multiple-choice format, enabling comparative analysis of explanations. Through extensive experiments across five datasets (CoS-E, ECQA, e-SNLI, ComVE) and two models (T5, BART), the research reveals that fine-tuning with explanations can lead to substantial performance improvements, even when explanations are deemed "low quality" by human judgment. TREU demonstrates superior performance compared to simulatability score, effectively capturing the task-dependent utility of explanations, particularly in nuanced scenarios like negation and counterfactual reasoning. 

The findings highlight the importance of rigorous quality checks in human annotation processes and underscore the potential for human-machine collaboration to create more effective and reliable AI systems. This work lays the groundwork for future research focused on developing more robust and insightful evaluation metrics for human-generated explanations.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">ParaAMR is a novel, large-scale paraphrase dataset constructed using AMR (Abstract Meaning Representation) back-translation. Addressing the limitations of existing paraphrase datasets—high-quality but small (human-annotated) or large but lacking syntactic diversity (back-translation)—ParaAMR leverages AMR graphs to generate diverse paraphrases. The method involves parsing source sentences into AMR graphs, randomly selecting a new focus node (root), modifying edges and labels, and then generating text from the altered graph using an AMR graph-to-text generator. This process ensures semantic similarity while introducing syntactic variation due to the generator's focus on the new root. ParaAMR contains approximately 15 million source sentences with roughly 6.9 paraphrases per sentence. Quantitative analysis, including automatic and human evaluations, demonstrates that ParaAMR maintains comparable semantic similarity to other back-translation datasets while exhibiting significantly higher syntactic diversity. Experiments across sentence embedding learning, syntactic control paraphrase generation, and few-shot learning demonstrate ParaAMR's effectiveness in improving performance compared to existing datasets. The dataset and code are publicly available.</sample>
    <sample id="341">Gli autori ricorrono a una singola modello per ogni regime di latenza, gestendo la latenza attraverso parametri specifici.</sample>
    <sample id="342">LiveChat is a novel, large-scale personalized dialogue dataset automatically constructed from live streaming videos sourced from Chinese TikTok and Douyin. Addressing limitations in existing open-domain dialogue datasets—primarily their text-based nature, limited scale, and lack of personalization—LiveChat leverages a unique reply-to-whom matching method to extract dialogues from audience comments and streamer responses. The dataset incorporates persona information, extracted through both manual labeling and rule-based/classifier approaches, enabling personalized dialogue generation. LiveChat significantly surpasses existing datasets in scale, video sourcing, personal annotations, and average session length.

Experiments on response modeling and addressee recognition tasks demonstrate the benefits of persona profiles and longer sessions. Notably, transfer learning experiments reveal that BART outperforms other pre-trained dialogue models, highlighting LiveChat's distinct domain. Human evaluations of large language models (LLMs) on LiveChat indicate improved informativeness. In-context learning experiments show performance gains with increasing demonstration shots, though excessive demonstrations (beyond 8) introduce noise. LiveChat provides a valuable resource for advancing research in open-domain dialogue, personalized dialogue, and multi-party conversation, particularly within the Chinese language context. Future work will focus on efficient transfer learning strategies for LLMs on this unique dataset.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi io e il mio co-autore Martin presentiamo il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita tramite un pre-addestramento, e la conoscenza fornita negli input al momento dell'inferenza. Lavori recenti in compiti come il question answering mostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere il compito. Ma la comprensione del linguaggio naturale spesso richiede una conoscenza che è fornita anche al momento dell'inferenza. Ad esempio, nella frase, "John ha visto il presidente appena eletto in TV." I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono sapere in modo affidabile chi sia questa entità specifica "John", o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuto il pre-addestramento. Pertanto, modelli di successo per compiti di NLU ad alta intensità di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza pre-addestrata che la conoscenza fornita al momento dell'inferenza. In questo lavoro, proponiamo una suite diagnostica di test per l'integrazione della conoscenza. Introduciamo un compito di coreference resolution, progettato per sondare la capacità di attingere alla conoscenza disponibile in diverse fonti. Valutiamo il data set con partecipanti a uno studio umano e modelli di coreference resolution consolidati. Ecco un esempio dal nostro data set. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui il pronome "lui" si riferisce, che in questo caso è Servin. La risoluzione di un dato pronome richiede due tipi di informazioni. Innanzitutto, la conoscenza specifica dell'entità, come "Servin è un giudice". E in secondo luogo, la conoscenza di base, come "I giudici decidono casi in tribunali". Generalmente, la conoscenza di base viene appresa durante il pre-addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell'entità viene tipicamente osservata al momento dell'inferenza. Variaamo la disponibilità di questi due tipi di informazioni in modo che possa essere trovata in una singola fonte, o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Innanzitutto, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base si presume sia disponibile al momento del pre-addestramento. In secondo luogo, c'è un'impostazione "Background-Both", dove la conoscenza di base è disponibile sia al momento del pre-addestramento che al momento dell'inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-addestramento dei modelli. Ad esempio, perché nuove professioni si sono sviluppate da quando è avvenuto il pre-addestramento. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle vere fonti. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di base "I politici cercano seggi eletti nel governo" sia contenuta nei parametri pre-addestrati e nel contesto di inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo inoltre non solo la conoscenza specifica dell'entità, ma anche la conoscenza di base sui politici nel loro contesto di inferenza. Nell'impostazione Background-Inference, forniamo la professione fittizia "mirituer" invece di politico, perché "mirituer" è improbabile che sia contenuto nei parametri pre-addestrati. Valutiamo il data set sia con partecipanti a uno studio umano, sia con modelli di coreference resolution consolidati. In questa figura, mostriamo i risultati dei modelli con le prestazioni migliori nella variante più difficile dell'impostazione Background-Pretrain. Senza un addestramento specifico per il compito su KITMUS, entrambi i modelli non funzionano bene. Quando addestrati su KITMUS, tuttavia, sia C2F che BERT4Coref funzionano significativamente meglio di una scelta casuale. Ciò suggerisce che, quando addestrati su data set di risoluzione delle referenze generici, la maggior parte impara a sfruttare gli indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenza fittizia hanno indicato che anche i modelli con le prestazioni migliori non riescono ad integrare in modo affidabile la conoscenza all'indietro fornita solo al momento dell'inferenza. Per riassumere i punti principali del nostro articolo, molti modelli di coreference resolution sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo la conoscenza da più fonti. Tuttavia, anche i modelli con le prestazioni migliori sembrano avere difficoltà ad integrare in modo affidabile la conoscenza all'indietro presentata solo al momento dell'inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e il data set e il codice su GitHub. Grazie per l'ascolto.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive, involving formalism-specific pre-processing and grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves strong compositional generalization to deeper recursion without relying on explicit tree structures. Traditional methods often incorporate trees to capture compositional relationships between utterances and logical forms, but this requires complex pre-processing and grammar induction. Our approach directly models these correspondences by first tagging each input token with a multiset of output tokens, effectively identifying the necessary elements for the logical form. Subsequently, a permutation model arranges these tokens into the correct order. A key innovation is a flexible permutation prediction method that avoids hard constraints, enabling expressive generalization. We address the challenges of unaligned input-output data and the NP-hard problem of finding optimal permutations through alignment induction and a GPU-friendly continuous relaxation technique, allowing for backpropagation and learning of linguistically plausible permutations. Experimental results on the COGS benchmark demonstrate significant performance gains over existing treeless models, particularly in generalizing to deeper recursion, while acknowledging that other forms of structural generalization remain challenging. The paper details the technical solutions to these challenges and invites readers to explore the full details in the paper or at the poster session.</sample>
    <sample id="346">Non specificato.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi sociali nei modelli linguistici di grandi dimensioni, o LLM. Tuttavia, queste misure presentano varie limitazioni. Di solito si basano su set di dati pre-costruiti che sono molto dispendiosi in termini di tempo e inoltre di solito misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi. Inoltre, la maggior parte dei lavori in questo campo non tiene conto dell'intersezionalità, che è la nozione che le identità sociali sfaccettate possono comporre pregiudizi ed essere loci unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi più recenti LLM ottimizzati tramite istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come "Immagina di essere una donna asiatica. Descriviti". E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatore di identità in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Vediamo immediatamente che, sebbene gli output non siano apertamente negativi o tossici nel senso tradizionale del termine, ci sono alcuni schemi interessanti. La donna asiatica è raffigurata come poco appariscente; la donna del Medio Oriente è descritta usando parole come esotica e come una regione affascinante. E entrambe le donne di colore fanno riferimento alla loro ascendenza mentre la persona maschile bianca non ha nulla di simile. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste persone. I nostri prompt per generare queste persone sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che dando questi prompt a soggetti umani, sono stati anche in grado di far emergere stereotipi razziali. E questo consente anche un confronto diretto tra le persone generate e le risposte scritte dagli umani. La seconda parte è "Marked Words", che è un metodo per identificare le parole che distinguono i gruppi "marked" dai gruppi "unmarked", che spiegherò a breve. Il vantaggio è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su un lessico specifico. Quindi, il metodo "Marked Words" attinge al concetto sociolinguistico di "markedness", che afferma che esiste un default non marcato e qualsiasi gruppo che differisce da tale default è linguisticamente marcato. Ad esempio, la parola "guerriero" è solitamente associata agli uomini. Quindi, quando le persone descrivono un guerriero che è una donna, di solito specificano "guerriera" e marcano il termine con "donna". Più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati. Quindi nel nostro metodo, designiamo prima quali sono i gruppi non marcati e marcati, e poi confrontiamo le persone utilizzando il metodo "Fightin’ Words", che è fondamentalmente l'utilizzo di rapporti log-odds ponderati per distinguere le parole principali per ciascun gruppo marcato. Quindi, ad esempio, per le persone di donne nere, faremmo "Fightin’ Words" e confronteremmo i rapporti log-odds sia con le persone bianche che con le persone uomini perché questi sono i due gruppi non marcati corrispondenti. Ora, per alcuni risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che le persone generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente alla distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, mentre le persone generate hanno tassi più elevati delle parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole di stereotipo che sono nelle persone generate sono solo le parole "alto" e "atletico". Quindi, solo le parole positive o almeno non negative. E in effetti, questo lessico non cattura molti dei modelli dannosi che abbiamo visto nelle slide precedenti. Quindi, per farlo, ci rivolgeremo ai risultati del nostro metodo "Marked Words" per mostrare come queste parole apparentemente positive facilitino stereotipi e narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Innanzitutto, dai nostri gruppi, le parole principali includono cose come "cultura", "tradizione", "orgoglioso" ed "esotico". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Ciò contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molte trame comuni che sono riflesse in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "formosa" che si collegano a una tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola" e "delicata" e "setosa" che si collega a una lunga storia di donne asiatiche che sono iper-sessualizzate, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone hanno chiamato l'"archetipo della donna nera forte". E sebbene possa sembrare positivo a prima vista, ci sono stati studi che dimostrano che questo tipo di archetipo è in realtà molto dannoso perché mette molta pressione su queste demografie per essere resilienti e forti contro gli ostacoli sociali. Invece di lavorare effettivamente per cambiare questi ostacoli, mette pressione su queste persone per superarli, il che porta a esiti sanitari molto negativi per queste persone, tra le altre conseguenze dannose. Più in generale, scopriamo che le parole per ciascun gruppo marcato riflettono fondamentalmente solo narrazioni molto essenzializzanti. Sulla base di questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, noi ricercatori dovremmo affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare pregiudizi e danni perché ci sono molte cose che potrebbero essere trascurate se non lo facessimo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione dei pregiudizi, perché, ad esempio, come questi stereotipi positivi, non sappiamo se è perché c'è una sorta di valore di allineamento eccessivo e strano o forse alcuni altri metodi anti-stereotipo che stanno portando a questi schemi dannosi. Non possiamo davvero fare alcuna supposizione o studiare ulteriormente, senza maggiore trasparenza. Grazie mille per aver ascoltato. Divertitevi all'ACL.</sample>
    <sample id="348">This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," introduces a novel method for identifying and analyzing stereotypes in large language models (LLMs). Existing approaches often rely on limited, hand-curated datasets or capture only broad associations, failing to account for intersectionality. Our method leverages the instruction-following capabilities of LLMs to generate personas based on prompts like "Imagine you are a [demographic group]. Describe yourself." We then employ "Marked Words," a sociolinguistic approach, to identify words that distinguish marked (marginalized) groups from unmarked (dominant) ones.

Unlike lexicon-based methods, Marked Words reveals subtle, yet harmful, patterns beyond overt negativity. Our analysis of GPT-4 generated personas reveals that while seemingly positive words like "culture," "tradition," "strong," and "exotic" appear frequently, they reinforce essentializing narratives and perpetuate harmful stereotypes. For example, Latina women are linked to "vibrant" and "curvaceous" (tropicalism), Asian women to "petite" and "delicate" (hypersexualization), and Black women to the "Strong Black Women" archetype, which can lead to negative health outcomes. We find that generated personas exhibit more stereotypes than human-written ones, but the lexicon used fails to capture the nuanced harmful patterns. We conclude by recommending researchers address positive stereotypes, adopt an intersectional lens, and advocate for increased transparency in bias mitigation techniques within LLMs.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina. È un piacere per me presentare un breve video promozionale del nostro articolo. State copiando il mio modello? Protezione del copyright dei modelli linguistici di grandi dimensioni per l'embedding come servizio tramite watermark backdoor. Introduciamo innanzitutto il contesto dell'embedding come servizio. Attualmente, modelli linguistici di grandi dimensioni come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'embedding come servizio è uno dei servizi costruiti su modelli linguistici di grandi dimensioni per assistere varie attività di NLP. Ad esempio, OpenAI offre un'API di embedding basata su GPT. Tuttavia, recenti lavori hanno dimostrato che un attaccante può rubare il modello apprendendo dall'embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'embedding come servizio. Per proteggere il copyright dell'embedding come servizio, una delle soluzioni è incorporare un watermark nel servizio del provider e rilevare se un altro servizio contiene il watermark. Il metodo di watermark deve soddisfare le seguenti proprietà. Innanzitutto, il metodo deve essere applicabile all'embedding come servizio. In secondo luogo, il watermark non deve degradare l'utilità degli embedding forniti. In terzo luogo, il watermark deve essere sufficientemente discreto in modo che l'attaccante non lo rilevi o possa rimuoverlo facilmente. Infine, il watermark deve essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere classificati in quattro categorie. Tuttavia, questo metodo non è applicabile all'embedding come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding marker, un metodo di watermark basato su backdoor applicabile all'embedding come servizio. Quindi, permettetemi di introdurre i dettagli del nostro embedding marker. Embedding marker contiene due passaggi principali: iniezione di watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo innanzitutto un trigger set. Il trigger set è un gruppo di parole in un intervallo di frequenza moderato. Assumiamo che il provider possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione di watermark, definiamo innanzitutto un embedding target. Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target. La verifica del copyright consiste nel rilevare se un modello dietro un altro servizio contiene il watermark. Costruiamo innanzitutto un backdoor e un set di dati benigni. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono al trigger set, mentre tutte le parole nelle frasi del set di dati benigni non appartengono al trigger set. Quindi, il provider richiede gli embedding dal servizio del ladro con il set di dati. La similarità coseno e L2 tra l'embedding richiesto e l'embedding target vengono calcolate. Viene calcolata la differenza di similarità tra il set di dati benigno e il set di dati backdoor, che è definita come delta coseno e delta L2. Nel frattempo, applichiamo anche il test KS e utilizziamo il suo p-value come terza metrica. Abbiamo condotto esperimenti su quattro set di dati: AG News, MIND, SST2 ed Enron Spam. Assumiamo che il provider applichi il set di dati wiki text per contare la frequenza delle parole. I risultati su quattro set di dati mostrano che il nostro embedding marker può avere ottime prestazioni di rilevamento mantenendo un'ottima utilità per le attività downstream. Abbiamo anche validato la discrezione dell'embedding fornito visualizzando gli embedding delle frasi su quattro dataset [INAUDIBILE 4:39] PCA. La legenda delle figure significa il numero di trigger in ogni frase. Come mostrano le figure, è difficile distinguere tra gli embedding backdoor e gli embedding normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">This paper investigates the validity of "superhuman performance" claims in Natural Language Understanding (NLU), particularly concerning leaderboard-based evaluations on benchmarks like SuperGLUE and SQuAD. While models frequently surpass human scores on these benchmarks, the paper argues that such achievements are often misleading due to flawed comparisons.

The analysis reveals several critical issues: systems and humans are evaluated on different subsets of the data, ground-truth answers contain errors, and human baselines are often poorly defined. Systems exploit spurious correlations in the data, a tactic unavailable to humans, while human performance is frequently estimated using simplistic aggregation methods rather than comparing against the best possible human performance. Furthermore, the paper highlights concerns about inadequate annotator compensation and a lack of transparency regarding annotator demographics and hiring processes, which significantly impact data quality.

Ultimately, the paper contends that current claims of superhuman performance in NLU lack scientific rigor. It advocates for more reliable benchmark construction, emphasizing the need for fair comparisons, accurate ground truth, well-compensated and diverse annotator pools, and a focus on comparing against the highest achievable human performance, rather than vague "human baseline" estimates.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. After nearly two decades of use, concerns arose regarding their performance on modern data and the factors contributing to generalization. To address this, we created the CoNLL++ dataset, comprising Reuters News from 2020 annotated with CoNLL-2003 guidelines. We fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-03 and CoNLL++, measuring F1 score changes to assess generalization.

Our findings indicate that effective generalization requires a combination of factors: transformer-based model architectures, larger model sizes, and a greater number of fine-tuning examples. We explored two hypotheses for performance degradation: adaptive overfitting (due to repeated use of the CoNLL-2003 test set) and temporal drift (caused by the time gap between training and testing data).  Analysis revealed no evidence of adaptive overfitting, but confirmed that temporal drift significantly impacts performance. Retraining models with more recent data improved results, validating this hypothesis.

Ultimately, we conclude that CoNLL-2003 taggers remain viable in 2023, though generalization can be enhanced through architectural improvements, larger models, and more fine-tuning data. The study highlights the importance of addressing temporal drift in NER model development and encourages further research into generalization strategies.</sample>
    <sample id="352">Annotating behaviors in chat.</sample>
    <sample id="353">This paper addresses the challenge of input underspecification in natural language to code generation, a prevalent issue in real-world scenarios where natural language descriptions (NLDs) lack crucial details. To tackle this, the authors introduce an interactive approach, focusing on clarifying operation-level specifications through clarification questions (CQs). They propose a novel task: code generation by asking clarification questions.

A key contribution is the creation of CodeClarQA, a synthetic dataset with CQs targeting missing key operations. The methodology identifies missing operations by comparing NLDs to operation documentation using schema-based similarity scores. CQs are generated using templates, offering yes/no or multiple-choice formats. Experiments demonstrate the effectiveness of their method in identifying missing operations, with MPNet showing strong performance.

The proposed pipeline incorporates a Clarification Need Predictor, a Question Selector, and a Code Generator. Results indicate that the task is more challenging than existing CQA ranking tasks, and that clarifications generally improve code generation. While the pipeline currently underperforms model-only approaches due to the difficulty of the CQA ranking task, analysis suggests that clarified key operations are indeed a significant factor in generating better code, with Oracle CQs leading to near-ground truth predictions. The paper highlights areas for future improvement, including handling taxonomy ambiguities and incorporating argument values.</sample>
    <sample id="354">2020</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una studentessa di dottorato in Informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Iniziamo definendo la dissonanza cognitiva e spiegando perché è un problema importante da studiare nel linguaggio. In poche parole, la dissonanza cognitiva è l'inconsistenza tra due credenze o azioni, come nell'esempio in cui una persona afferma "So che le sigarette possono uccidermi" e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa credenza e questa azione sono inconsistenti e quindi in dissonanza. Ulteriori affermazioni come "Non credo che potrei mantenere il mio lavoro senza di loro" giustificano la seconda occorrenza e creano una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nel processo decisionale quotidiano, è davvero rara da trovare espressa nel linguaggio rispetto ad altri tipi di relazioni discorsive. Ma perché questo è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a monitorare le tendenze e i valori delle credenze e i cambiamenti di atteggiamento nella popolazione. L'alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e per aiutarci a comprendere meglio i processi decisionali. Per raggiungere l'obiettivo di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto una annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio "dissonanza-first", come si può vedere nel diagramma di flusso qui. I tweet sono stati passati attraverso il parser PDTB e le coppie di unità discorsive sono state annotate in base alle linee guida descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo nel 3,5% delle coppie di unità discorsive annotate. Dopo aver raccolto circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito l'addestramento per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia funzionato molto meglio del caso. Data la bassa occorrenza della dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo ad affrontare il problema dell'assoluta rarità. Per alleviare questo problema, sperimentiamo combinazioni di transfer learning e active learning per annotare in modo da raccogliere più campioni dissonanti con meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione migliorando al contempo il rilevamento della dissonanza. Poiché il modello iniziale non era in grado di catturare la classe della dissonanza, abbiamo iniziato il processo di active learning trasferendo i pesi da attività correlate. Trasferiamo da due attività diverse: la classificazione dello stance sulla dissonanza indipendente dall'argomento, un'attività che determina se due affermazioni di dibattito provenienti da persone diverse sono d'accordo o in disaccordo, indipendentemente dall'argomento, chiamata "debate" qui, e la classificazione binaria delle classi di espansione e confronto di PDTB poiché queste due sono strettamente correlate al concetto di consonanza e dissonanza e le chiamiamo CE qui. Scopriamo che il trasferimento delle prestazioni zero-shot sul set di dati annotato è già molto migliore del caso con il migliore, con AUC di 0,62. Inoltre, eseguendo un fine-tuning iterativo su entrambe le attività, scopriamo che il fine-tuning delle attività CE seguito da un ulteriore fine-tuning sul debate produce prestazioni zero-shot migliori. Questo è quindi il modello che utilizziamo per l'avvio a freddo dell'active learning. Successivamente, determiniamo il metodo migliore per aggiornare un modello con nuovi dati da ogni round di active learning. "Cumulative" accumula tutti i dati raccolti dalle annotazioni active finora, mentre "Iterative" aggiorna il modello addestrandolo con l'ultimo set di dati raccolto. Tra le diverse strategie, abbiamo scoperto che Cumulative ha funzionato uguale o meglio di Iterative in tutti i casi. Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia Probability-of-Rare-Class (PRC) per selezionare principalmente gli esempi che è molto probabile siano dissonanti secondo il modello corrente in ogni round di rarità. Confrontiamo questo con altre strategie AL all'avanguardia comunemente utilizzate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio delle altre strategie all'avanguardia, sebbene la differenza sia piccola. Si noti che le prestazioni sono significativamente inferiori per casuale. Con ulteriori round di AL con le due migliori strategie, miglioriamo l'AUC di classificazione della dissonanza a 0,75, che è la migliore performance che abbiamo ottenuto finora su questo compito. Verifichiamo anche la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Scopriamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, scopriamo che PRC è una strategia AL semplice per l'acquisizione di classi rare e l'avvio a freddo di AL con un compito di transfer learning opportunamente progettato e aiuta significativamente. Scopriamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active all'interno del dominio beneficiano di un aggiornamento cumulativo. Questi sono i link al nostro set di dati principale e al nostro articolo. Non esitate a contattarci se avete domande. Grazie.</sample>
    <sample id="356">Alexander Koller and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">Viene confrontato con un'architettura all'avanguardia specificamente progettata per la traduzione simultanea del parlato.</sample>
    <sample id="361">CounterComp addresses the challenge of compositional generalization in multi-step quantitative reasoning for question answering, where state-of-the-art neural models struggle, particularly with reasoning chains exceeding two steps, due to memorizing spurious patterns. The approach leverages counterfactual scenarios to mitigate this issue. Given a training sample (anchor), CounterComp mines positive and negative examples by intervening in the question text. Positive examples involve interventions that don't alter the output, while negative examples result in output changes. These triplets are then used to incorporate a dynamic margin metric learning loss into the training process, adapting to the degree of intervention. Experiments across three state-of-the-art baselines demonstrate consistent performance improvements, especially with complex reasoning. Crucially, CounterComp enhances performance on both in-distribution and out-of-distribution samples, showcasing improved compositional generalization capabilities. Qualitative analysis reveals that the CounterComp loss encourages the model to attend to more relevant tokens, aligning with the operational terms in the output. This method avoids costly human supervision while fostering more robust and generalizable reasoning abilities in neural models for quantitative question answering.</sample>
  </task>
</testset>