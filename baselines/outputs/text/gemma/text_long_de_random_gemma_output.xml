<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Large-scale web crawl data, including news media like The New York Times, Los Angeles Times, and The Guardian.</sample>
    <sample id="1">McGill University.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model designed to improve visually-rich document understanding (VrDU). Existing models often struggle with reading order due to reliance on global 1D positions, which represent the sequential order of tokens. LayoutMask addresses this by using "local 1D positions" – the order of tokens within a segment – and encouraging the model to infer the global reading order by integrating 1D, 2D positions, and semantic information.

LayoutMask incorporates two new masking strategies within the Masked Language Modeling (MLM) objective: Whole Word Masking (masking at the word level to enhance context understanding) and Layout-Aware Masking (prioritizing masking of segment boundaries to promote cross-segment interaction). Additionally, it introduces Masked Position Modeling (MPM), a task that recovers randomly masked 2D positions, fostering semantic and spatial reasoning.

Experiments demonstrate that LayoutMask's use of local 1D positions outperforms global 1D positions on datasets like FUNSD and SROIE, particularly in scenarios with complex layouts and misleading numbers. The paper concludes that LayoutMask's approach to text-layout interaction leads to improved document understanding capabilities. Further details can be found in the full paper and accompanying posters.</sample>
    <sample id="3">Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Textidentifikation auf Dokumentenebene und auf Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lass uns zunächst Textvereinfachung definieren. Textvereinfachung ist ein Prozess der Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie z. B. Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. Und im Beispiel hier können Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z. B. lexikalische Substitution, Klausurstreichung, Umordnung oder Einfügung von Wörtern. Wir schlagen nun unseren neuen Korpus DEPLAIN vor, weil es in den letzten Jahren einige Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Subkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente alle manuell ausgerichtet. Das ergibt etwa 13.000 parallele Satzpaare. Für DEPLAIN-web umfasst dieser Korpus verschiedene Bereiche und wir richten auch alle 750 Dokumente aus, einerseits manuell und andererseits mit automatischen Ausrichtungsmethoden. Insgesamt erhalten wir 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Texte für Sprachlernende. Auf allen Ebenen, z. B. hinsichtlich lexikalischer Vereinfachung, Strukturvereinfachung und des gesamten Vereinfachungsgrads. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im DEPLAIN-apa-Korpus viel mehr Umordnungen und Wortzusätze als im DEPLAIN-web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Sehen wir uns nun an, was wir mit diesem Korpus machen können. Hallo, ich bin Omar und werde nun über die Anwendungsfälle für unser Dataset DEPLAIN sprechen. Für den ersten Anwendungsfall können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten zu extrahieren, die die gleiche Sprache und den gleichen Inhalt haben, aber ein unterschiedliches Komplexitätsniveau aufweisen. Und jetzt, da wir unser Dataset DEPLAIN haben, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und haben alle diese Anpassungen und den Code zur Durchführung unserer Experimente in dem Paper veröffentlicht. Abschließend kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode von MASSalign ist. Sie finden den Code, um diese Methode auf Ihren eigenen Dokumenten auszuführen, ebenfalls in dem Paper. Der zweite Anwendungsfall, den wir in unserem Paper gezeigt haben, ist der der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabetext zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um vereinfachungen auf Dokumentenebene zu erzeugen, und wir haben auch das normale Base mBART feinabgestimmt, um vereinfachungen auf Satzebene zu erzeugen. Sie finden alle Checkpoints und können sich in dem Paper die Details der Scores und der Evaluationsmetriken unserer Experimente ansehen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Scores erzielen oder Baseline-Scores übertreffen kann, und wir schlugen diese Ergebnisse als Basen-Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model with partially overlapping background knowledge.</sample>
    <sample id="6">Jiaan presented their work, "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," introducing a new framework called many-to-many summarization. This approach aims to create a single model capable of summarizing documents in any source language and generating summaries in any target language, unifying previous multilingual and cross-lingual summarization methods.

Their research indicates that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional approaches. To demonstrate this, they conducted preliminary experiments on the WikiLingua dataset using mBART-50, comparing it against separate multilingual models, unified cross-lingual models, and a many-to-many model. The results showed improved performance with the many-to-many setting.

Furthermore, they introduced PISCES, a pre-trained many-to-many summarization model, utilizing a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. Experimental results demonstrate that PISCES outperforms existing models like mBART-50 and mT5. Ablation studies and human evaluations further validate the effectiveness of PISCES and its training stages. The paper provides detailed information for those interested in exploring the methodology and results.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">ABC-Eval reduziert die Subjektivität menschlicher Bewertungen, indem es explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen aufweist.</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">Durch Verbesserung des Zugriffs der Sprachmodelle auf Hintergrundwissen.</sample>
    <sample id="11">Jack Hessel from AI2 presented research on evaluating humor understanding in large language models (LLMs) using data from *The New Yorker* Caption Contest. The study questions whether LLMs truly "understand" humor, despite their ability to generate and explain jokes.

The research operationalized the contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is funny. A new dataset was created with annotations including image descriptions, entity links, and human-written joke explanations.

Results showed LLMs, even advanced models like GPT-4, struggle with these tasks. While CLIP fine-tuned on the dataset achieved 62% accuracy in caption matching (compared to a 20% baseline and 94% for humans), GPT-4's performance remained significantly lower even with image descriptions. Human evaluations also favored human-written joke explanations over those generated by GPT-4 in over two-thirds of cases, highlighting errors in understanding the cartoon's context.

The researchers released the dataset and leaderboard to encourage further research into LLM humor understanding. The work suggests that while LLMs can mimic humor, a deeper comprehension remains elusive.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Daniel Rotem presented "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," exploring methods to reduce large language model inference time. Adaptive inference leverages varying data complexity, using smaller models for simpler samples. The two main approaches are Multi Model and Early Exit. Multi Model uses sequentially run, separately trained models with classifiers, while Early Exit employs classifiers at intermediate layers, halting computation when appropriate.

The research highlighted a key issue with Early Exit: conflicting gradients. Multiple classifiers sharing model parameters can lead to gradient interference, degrading performance. This was demonstrated by comparing Early Exit classifiers to separate Multi Model classifiers, revealing a 2.3% performance gap favoring Multi Model, particularly for earlier classifiers.

To address this, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method where each layer receives updates only from its subsequent classifier, eliminating conflicting gradients. SWEET significantly closed the performance gap between Early Exit and Multi Model, and even outperformed both methods in fast inference scenarios, especially with BERT-Large. The work demonstrates the existence of conflicting gradients, provides a fair comparison of adaptive inference methods, and introduces SWEET as a promising solution for Early Exit architectures.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es unterschiedliche Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So zum Beispiel bei den Universal Dependencies wird die Struktur der Koordination „Lisa, Bart und Maggie“ so angenommen, dass das erste Konjunkt das Haupt der gesamten koordinierten Struktur ist. In diesem Fall also Lisa. Ein ähnlicher Ansatz wird in Igors Mel’čuks Meaning-Text-Theorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkt abhängig ist. Diese beiden Ansätze sind asymmetrisch. Sie heben eines der Konjunkte hervor.

Das sind asymmetrische Ansätze für Koordinatstrukturen, wie zum Beispiel der Prager Ansatz. Der konjunktionsgeführte Ansatz, der in den Prager Dependency Treebanks angenommen wird, bei dem koordinierte Strukturen vom Konjunkt abhängig sind. So erhalten wir Abhängigkeiten von Ende zu allen Konjunkten. Und schließlich gibt es auch einen mehrköpfigen Ansatz, der beispielsweise in Hudsons Word Grammar verwendet wird, wo sie sagen, dass alle Konjunkte Köpfe der koordinierten Struktur sind. So erhalten wir Abhängigkeiten vom Regenten zu allen Konjunkten einzeln: Lisa, Bart und Maggie.

Nun ist das Ziel dieser Arbeit, ein neuartiges Argument für symmetrische Strukturen der Koordination, wie diese beiden, und gegen asymmetrische Strukturen der Koordination, wie diese beiden, zu liefern. OK. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erläutern werde.

So, in Englisch, wie Sie vielleicht wissen, bevorzugen direkte Objekte, nahe am Verb zu sein, während Adverbien weiter entfernt sein können. „Marge las es gestern“ ist in Ordnung, weil das direkte Objekt nahe am Verb ist, während „Marge las gestern es“ viel schlechter ist. Denn hier befindet sich ein Adverb zwischen dem Verb und dem direkten Objekt: „gestern“. Dieses Phänomen kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und lang ist. Denn dann kann es an die Position nach dem Adverb verschoben werden. Dies wird hier veranschaulicht. Beide Sätze sind in Ordnung. „Marge las dieses absolut faszinierende Buch über Bienen gestern.“ Es ist in Ordnung, anstatt „es“ dieses lange NP zu verwenden. Aber es ist auch in Ordnung zu sagen: „Marge las gestern dieses absolut faszinierende Buch über Bienen.“ Die Begründung hier ist, dass dies möglich ist, weil der Satz zwar ein allgemeines grammatikalisches Prinzip verletzt, dass direkte Objekte neben dem Verb stehen sollten, er aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die nicht konstant unter diesen beiden Strukturen sind. Hier haben wir also eine Abhängigkeit von „lesen“ zum Adverb der Länge 7 (gemessen in Wörtern) und von „lesen“ zum „Buch“ der Länge 4, also zusammen 11. Wenn Sie diese beiden Konstituenten vertauschen, wird die Summe dieser beiden Abhängigkeiten 6. Anstatt also 11 ist 6 viel kürzer. Deshalb klingt das ziemlich in Ordnung. Es verletzt ein Prinzip, erfüllt aber ein anderes.

OK. Was wir getan haben, ist, verschiedene Statistiken über Koordinationen aus der erweiterten Version des Penn Treebank zu extrahieren und in der Arbeit „Warum Sie Universal Dependencies nicht verwenden sollten“ zu untersuchen. Diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. So „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Und auch die Beobachtung, dass beim Parsen diese Tendenz mit dem Längenunterschied wächst. Wenn sich der Unterschied zwischen den Längen der beiden Konjunkte vergrößert, bevorzugt der kürzere Konjunkt, der erste zu sein, stärker. So ist der Anteil des linken, kurzen Konjunkts größer. Was in dieser Arbeit jedoch neu ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn sich der Regens auf der linken Seite befindet oder fehlt. Der Regens befindet sich also in diesem Beispiel „Ich sah Bart und Lisa“ auf der linken Seite. Es fehlt im zweiten Beispiel „Homer kam und niesste“. Hier haben wir eine Koordination von zwei Verben und es gibt keine externen Regenten. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wenn sich der Regens jedoch auf der rechten Seite befindet, wie hier „lachte“ regiert die Koordination Ted und Ned, verschwindet dieser Effekt.

Wir haben dies gemessen, indem wir die Länge in Zeichen (erste Spalte), Silben (mittlere Spalte) und Wörtern (rechte Spalte) gemessen haben. Ich werde mich hier auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn sich der Regens auf der linken Seite befindet, die Tendenz, dass der linke Konjunkt kürzer ist, mit dem absoluten Unterschied in Wörtern stetig wächst, und dasselbe gilt, wenn kein Regens vorhanden ist, wie bei der Koordination von Sätzen. Aber wenn sich der Regens auf der rechten Seite befindet, verschwindet diese Tendenz. Wir zeigen in der Arbeit, wie dies ein Argument gegen asymmetrische Strukturen der Koordination liefert, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden. Sehen Sie sich die Arbeit für die vollständigen Argumente an. Und sprechen Sie uns bei der Poster-Session an. Vielen Dank.</sample>
    <sample id="15">Drei.</sample>
    <sample id="16">Bible texts are much more strongly simplified.</sample>
    <sample id="17">This paper introduces a novel approach to multimodal relation extraction (MRE) that addresses the challenges of internal-information over-utilization and external-information under-exploitation. The proposed method employs a Graph Information Bottleneck principle-guided feature refinement and incorporates multimodal topic information to enrich context. The framework constructs cross-modal graphs (CMGs) from text and images, then refines these graphs by filtering nodes and edges to remove irrelevant information. Multimodal topic features are subsequently integrated using an attention mechanism. Experiments on a standard MRE dataset demonstrate significant performance improvements over existing methods, with ablation studies confirming the benefits of both information screening and topic enrichment. Further analysis reveals that internal-information screening is more effective when text and vision are highly relevant, while external-information exploitation is more beneficial when relevance is low. The work highlights the importance of simultaneously subtracting and adding information for effective multimodal relation extraction.</sample>
    <sample id="18">"Salt and pepper" statt "pepper and salt".</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their ACL 2023 accepted work, "A Survey for Efficient Open Domain Question Answering." The paper addresses the challenges of open-domain QA, which typically uses a two-stage retrieval and reader framework. This framework, while effective, faces hurdles due to the massive Wikipedia corpus (20GB), large index files (65GB), and computationally intensive language models, hindering real-time applications and resource-constrained devices.

The survey explores techniques for efficient QA, including one-stage frameworks like retrieval-only and generator-only systems. Key tactics involve fast evidence retrieval (approximate nearest neighbor search), efficient reading (skip reading), index size reduction (document filtering, embedding compression), and model size reduction (lightweight models, parameter sharing, one-stage models).

The analysis reveals that retrieval and reader systems offer a balanced approach, while retrieval-only systems prioritize speed at the cost of index size, and generator-only systems struggle with model size and performance. The paper concludes with insights for resource-limited scenarios, suggesting index reduction or model size optimization. Future work focuses on deployment in low-power devices and the development of more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">Nachrichtenartikel.</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrette's presentation focuses on improving text rendering in text-to-image models, specifically addressing the common issue of these models struggling to accurately depict text within generated images. The research centers on Google's Imagen model, which utilizes a T5-XXL text encoder to generate image prompts.

The core problem identified is that T5's subword tokenization hinders its ability to accurately spell words. Instead of receiving individual letters, T5 processes chunks of text, requiring it to decompose these chunks into letters for rendering. Experiments revealed that even the largest T5 models exhibit relatively low spelling accuracy, particularly with frequent words due to SentencePiece's tokenization strategy.

In contrast, models like PaLM (larger and data-intensive) and ByT5 (character-level input) demonstrate significantly better spelling capabilities. ByT5's direct access to character information allows it to effectively copy characters, regardless of word frequency.

To address this, the researchers augmented the Imagen model by incorporating a small ByT5 model's output alongside the existing T5 representation. This minimal addition (only a 5% increase in parameters) substantially improved text rendering accuracy and overall image generation quality. While the diffusion model can still introduce errors, this approach represents a practical and efficient strategy for enhancing text rendering in text-to-image models. The work introduces the WikiSpell and DrawText benchmarks to evaluate text rendering capabilities.</sample>
    <sample id="24">In characters, syllables, and words.</sample>
    <sample id="25">Die Experimente wurden mit einer erweiterten Version des Penn Treebank durchgeführt, um Statistiken über die Koordination zu extrahieren. Die Forscher untersuchten, ob die Tendenz, dass linke Konjunkte kürzer sind, auftritt, wenn sich der Governor (derjenige, der die Koordination regiert) auf der linken Seite befindet, fehlt oder sich auf der rechten Seite befindet. Die Länge der Konjunkte wurde in Zeichen, Silben und Wörtern gemessen.</sample>
    <sample id="26">The classifier performs not much better than chance.</sample>
    <sample id="27">Der Text erwähnt Shangbin als PhD-Student, der die Arbeit präsentiert, aber gibt keine Informationen über die Anzahl der Autoren.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Formality und lexikalische Kohäsion.</sample>
    <sample id="30">LLM-Blender is a novel and straightforward ensemble learning framework for large language models (LLMs) developed by AI2 and USC. The core idea revolves around pairwise ranking and generative fusion, addressing the limitation of relying solely on a single, top-performing LLM, as optimal model selection varies significantly across different inputs.

The framework operates in two stages. First, it runs 'n' LLMs on a given input and obtains their outputs. Then, a "PairRanker" module compares these outputs in pairs using a cross-attention mechanism (like RoBERTa) to determine which candidate is superior for the input. This comparison generates a ranking matrix, which is aggregated (ideally using max logits) to establish a final order.

The second stage, the "GenFuser," selects the top K (e.g., three) ranked candidates and uses a sequence-to-sequence model to fuse them, producing the final output. Unlike prior methods that evaluate candidates individually, PairRanker analyzes subtle differences through pairwise comparisons.

To facilitate evaluation, the authors created MixInstruct, a new dataset comprising existing instruction datasets and candidate outputs from 11 open-source LLMs, evaluated using metrics like BERTScore, BLUERT, BARTScore, and ChatGPT. Experiments demonstrate that LLM-Blender consistently outperforms leading models like Open Assistant and Vicuna, highlighting its potential as a simple yet effective ensemble learning approach. The codebase and dataset are publicly available.</sample>
    <sample id="31">Die Universitäten der Autoren werden im Text nicht genannt.</sample>
    <sample id="33">The framework quantifies positionality by re-annotating datasets with diverse annotators, collecting demographic data, and then comparing annotator responses with existing datasets and models using Pearson's R correlation scores.</sample>
    <sample id="34">CREST is a novel framework combining rationalization and counterfactual text generation to improve model interpretability and performance. It addresses the limitations of existing methods by jointly generating rationales and counterfactual examples.

The framework first generates counterfactuals by masking a rationale derived from the original input and prepending a gold label, then uses a masked language model to fill in the masked portions. Human evaluations show CREST produces more valid and natural counterfactuals than other automatic approaches like MiCE.

CREST then introduces "CREST-Rationalization," which leverages both factual and counterfactual examples during training. A shared rationalizer highlights meaningful rationales for both input types, and a regularization term encourages similarity between original and new rationales. Experiments on IMDB and SNLI demonstrate that CREST-Rationalization achieves state-of-the-art results, particularly on out-of-domain datasets.

Finally, the study assesses the interpretability of CREST-generated rationales, finding them to be more plausible and exhibiting higher "counterfactual simulability" – the ability of an explanation to influence the classifier's decision when a contrastive edit is made – compared to other methods. CREST offers a promising approach for building more interpretable and robust NLP models.</sample>
    <sample id="36">This presentation introduces "Language-Specific Layers" (LSLs), a novel approach to enhance multilingual machine translation (MMT) models. MMT offers scalability and speed advantages, but often suffers from limited capacity per language. LSLs address this by adding language-specific transformer layers, allowing the model to focus resources where needed without increasing overall inference costs.

The key innovation lies in *learning* the optimal placement of these LSLs within the encoder. The authors train a large model with shared, source, and target weights for each encoder layer, then analyze the weights to determine the best layer configuration. They observed that source weights are consistently important, while target weights shift in importance depending on the encoder layer. The final architecture is then determined by selecting the component (shared, source, or target) with the largest weight in each layer.

Experiments on WMT21 news translation across 10 languages (including Swahili) demonstrate significant improvements over baseline transformer models and language adapter approaches, particularly for low-resource languages. The improvements are statistically significant across a majority of translation directions. The authors emphasize that LSLs achieve these gains while maintaining fast inference speeds, making them a promising solution for efficient and effective multilingual translation. Further details, including decoder configurations and ablation studies, can be found in the full paper and poster session.</sample>
    <sample id="37">Die Studie mit menschlichen Teilnehmern ergab, dass sie ebenfalls rassistische Stereotypen aufdeckten.</sample>
    <sample id="38">The enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Ein Autor.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a novel Persona-grounded Commonsense Knowledge Graph developed by EPFL University and Sony Group Corporation to enhance narrative understanding and generation in NLP systems. It addresses the limitation of current systems in representing real-world personas and their associated knowledge.

PeaCoK comprises approximately 3,800 personas, 40,000 attributes, and 100,000 inferences, with a focus on interconnectedness between personas. The graph's relations are structured around interactivity and distinctiveness, and built through a three-step process involving persona selection, attribute induction, and crowdsourced annotation with AI assistance (InstructGPT-3).

Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving comparable performance to larger language models like GPT-3 and GPT-3.5. Furthermore, integrating PeaCoK into a dialogue generation system (P²Bot) significantly improves dialogue quality—fluency, consistency, engagement, and persona expression—compared to baselines and even general commonsense knowledge graphs like Atomic2020. The study reveals that dialogue quality improves with greater shared knowledge between speakers, underscoring the value of PeaCoK's interconnected persona knowledge. The paper and associated resources are publicly available.</sample>
    <sample id="42">The text does not mention the number of authors.</sample>
    <sample id="43">Nicht angegeben.</sample>
    <sample id="44">The framework differs from annotator disagreement literature by comparing end users with models and datasets, predictions and labels, as opposed to looking at just annotator agreement or modelling annotator distributions.</sample>
    <sample id="45">Die generierten Personas haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">DeepL und Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit mit dem Titel "Von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Verfolgung der Spur von politischen Voreingenommenheiten, die zu unfairen NLP-Modellen führen". Sprachmodelle werden auf großen Web-Crawling-Datensätzen trainiert. Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, der Huffington Post usw. gut in den Trainingsdaten für Sprachmodelle abgedeckt sind. Dies hat einen zwiespältigen Nutzen für Sprachmodellanwendungen geschaffen. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können möglicherweise zu Fairnessproblemen bei Downstream-Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben zu untersuchen, indem wir die folgenden Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen Pretraining-Daten bei solchen politischen Voreingenommenheiten? Zweitens, wie verhalten sich Sprachmodelle mit unterschiedlichen politischen Ausrichtungen tatsächlich bei Downstream-Aufgaben und führt dies möglicherweise zu Fairnessproblemen in NLP-Anwendungen?

Konkret haben wir zunächst vorgeschlagen, Sprachmodelle mit verschiedenen Prompt-Formaten unter Verwendung politischer Fragebögen wie dem Political Conference Test zu versehen. Dies stellt sicher, dass wir eine automatische Bewertung durchführen können, die gut in der politischen Wissenschaftsliteratur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie nehmen alle vier Quadranten auf der politischen Kampagne ein. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell unter allen ist und die GPT-Reihe im Allgemeinen sozial liberaler ist als BART und seine Varianten.

Zweitens wollen wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedene parteiische Korpora trainieren, die in Nachrichten und soziale Medien unterteilt sind und weiter in ihre politische Ausrichtung unterteilt sind. Durch das weitere Trainieren von Sprachmodellen auf solchen parteiischen Korpora können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells ebenfalls entsprechend verschieben. Zum Beispiel können wir bei RoBERTa, das weiter auf dem linken Reddit-Korpus trainiert wurde, eine deutliche liberale Verschiebung in Bezug auf seine politischen Voreingenommenheiten feststellen. Und wir versuchen auch herauszufinden, ob Sprachmodelle die Polarisierung erfassen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Pretraining-Korpora in vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle nach 2017 eine politische Ausrichtung hatten, die weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft erfassen können.

Zu guter Letzt bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen auf die Erkennung von Hassreden und die Erkennung von Falschmeldungen, um NLP-Anwendungen zu bewerten, die häufig Sprachmodelle verwenden und erhebliche Auswirkungen haben könnten. Wir stellen fest, dass wir, wenn wir die Leistung pro Kategorie untersuchen, d. h. wenn wir die Leistung in verschiedene demografische Gruppen oder die politische Ausrichtung von Nachrichtenmedien aufteilen, ein Muster erkennen können. Zum Beispiel sind bei der Erkennung von Hassreden sprachmodelle mit linker Ausrichtung besser darin, Hassreden zu erkennen, die auf sozial marginalisierte Gruppen abzielen, aber schlechter darin, Hassreden zu erkennen, die auf mächtigere Gruppen in unserer Gesellschaft abzielen. Umgekehrt sind sprachmodelle mit rechter Ausrichtung besser darin, Hassreden zu erkennen, die sich gegen Weiße und Männer richten, aber schlechter darin, Hassreden zu erkennen, die sich gegen Schwarze, LGBTQ+ und andere Minderheitengruppen richten. Ähnliche Trends treten bei der Erkennung von Falschmeldungen auf, wo wir sehen, dass sprachmodelle mit linker Ausrichtung besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen, und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um zu sehen, dass sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen zu Hassreden und Falschmeldungen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Reihe weiterer Beispiele im Anhang, um hervorzuheben, dass dies ein dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigt. Zum Beispiel würde bedeuten, wenn sprachmodelle mit rechter Ausrichtung auf Hassreden oder Falschmeldungen oder was auch immer feinabgestimmt und auf einer beliebten Social-Media-Plattform eingesetzt würden, dass Menschen mit gegenteiligen politischen Meinungen marginalisiert würden und Hassreden, die auf Minderheitengruppen abzielen, ohne Kontrolle grassieren würden. Dies hat Alarm geschlagen, um die Fairnessprobleme anzuerkennen und anzugehen, die sich aus den politischen Voreingenommenheiten von Sprachmodellen ergeben.

Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten für Sprachmodelle nicht bereinigen, wird die Voreingenommenheit von Pretraining-Daten zu Sprachmodellen zu Downstream-Aufgaben propagiert, was letztendlich zu Fairnessproblemen führt. Wenn wir jedoch versuchen, sie irgendwie zu bereinigen, riskieren wir auch Zensur oder Ausgrenzung. Es ist unglaublich schwierig zu bestimmen, was tatsächlich neutral ist und in den Sprachüberwachungsdaten beibehalten werden sollte. Es ist also wie das elektrische Trolley-Problem.

Das war's im Wesentlichen für heute. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">David Vilar and his colleagues from Google Translate.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN is a newly created corpus designed to advance German text simplification research at both the document and sentence levels. Text simplification aims to make text more accessible for specific audiences like those with reading difficulties or non-native speakers, requiring parallel text pairs of varying complexity. Existing corpora faced limitations, including small size and alignment errors from automated methods.

DEPLAIN addresses these issues with two subcorpora: DEPLAIN-apa (news texts, 483 documents, ~13,000 sentence pairs) and DEPLAIN-web (diverse domains, 750 documents, ~30,450 sentence pairs). Both were manually aligned, with DEPLAIN-web also utilizing automatic alignment techniques. Analysis reveals varying simplification strengths across domains, with Bible texts showing greater simplification than news or language learner texts. The corpus also exhibits a diverse range of simplification transformations.

The corpus enables several use cases. Firstly, it serves as a gold standard for evaluating automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Secondly, DEPLAIN facilitates automatic text simplification through fine-tuning language models (long-mBART for document-level, mBART for sentence-level simplification). Fine-tuning demonstrated improved results compared to baselines, establishing a benchmark for future research in this area. The models, checkpoints, and code are publicly available.</sample>
    <sample id="51">Music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper presents a novel approach to detecting cognitive dissonance in language, a rare phenomenon with implications for understanding disagreement, mental health, and decision-making. Addressing the challenge of absolute rarity, the authors created a large-scale annotated dataset of dissonance relations found in tweets (only 3.5% of pairs). Initial classifiers performed poorly due to the limited training data. To overcome this, they employed transfer learning from related tasks—dissonance stance classification ("debate") and discourse relation classification ("CE")—demonstrating improved zero-shot performance. Subsequently, they utilized active learning (AL) with a Probability-of-Rare-Class (PRC) strategy to efficiently select dissonant examples for annotation. PRC outperformed other state-of-the-art AL strategies, achieving an AUC of 0.75. The study also found that cumulative model updates were beneficial for in-domain annotation, while iterative updates were useful for transfer learning. The research highlights the effectiveness of combining transfer learning and PRC-based active learning for rare class acquisition and cold-starting AL in cognitive dissonance detection.</sample>
    <sample id="55">Ja.</sample>
    <sample id="56">Yusen Zhang</sample>
    <sample id="57">Without task-specific training, the models do not perform well.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">Yanis Labrak presented "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains." The work addresses the lack of French biomedical language models, filling a gap compared to English models like PubMedBERT and ClinicalBERT. DrBERT, based on RoBERTa and trained on the NACHOS dataset (medical web crawls), is introduced as the first open-source French biomedical model.

The research investigates optimal data sources and training data volume. DrBERT is compared to ChuBERT (trained on anonymized hospital data) and several models with varying pre-training strategies (from-scratch vs. continual pre-training using CamemBERT and PubMedBERT weights). Seven models were evaluated across 11 French biomedical and clinical downstream tasks against six baselines.

Results indicate that models perform best on tasks similar to their training data, but heterogeneous data sources offer greater versatility. More training data generally improves performance, and from-scratch pre-training often yields superior results. Notably, a DrBERT model trained on a smaller (4GB) NACHOS subset achieved comparable performance to a larger from-scratch model.

DrBERT consistently outperformed the generic CamemBERT model on most tasks. The pre-trained DrBERT models and training scripts are freely available on Hugging Face and GitHub, respectively, under the MIT license.</sample>
    <sample id="60">Die Autoren gehören keiner bestimmten Universität an.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">Nitay Calderon and his team (Amir, Subhabrata, and Roi) present a systematic study on compressing Natural Language Generation (NLG) models through knowledge distillation. The core problem addressed is preserving performance while reducing the size and computational cost of large NLG models, a growing industry need.

Their research departs from existing work by focusing on task-specific knowledge distillation across diverse NLG tasks (summarization, question generation, common sense reasoning, simplification, and style transfer) within realistic, industry-driven setups. These setups involve medium-sized labeled datasets (1:4 labeled-to-unlabeled ratio), large amounts of unlabeled data, medium-sized models, and a focus on inference efficiency.

The study explores various architectural choices (encoder/decoder vs. decoder-only), the impact of pruning, and different knowledge distillation approaches. A key contribution is challenging traditional sequence-level distillation by demonstrating the benefits of utilizing unlabeled data, generating multiple diverse pseudo-targets (instead of a single beam-search approximation), and employing sampling techniques with high temperatures.

Finally, they introduce "joint-teaching," a novel technique combining word-level distillation on both teacher- and student-generated pseudo-targets to address student exposure bias and improve learning. The paper provides a comprehensive recipe for knowledge distillation in NLG, aiming to bridge the gap between research and practical industry applications.</sample>
    <sample id="63">It measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine niedrigere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="66">This survey explores the burgeoning field of deep learning for mathematical reasoning, a crucial aspect of human intelligence. It examines the task's evolution, encompassing text-based problems, multimodal data (images, tables), and automated theorem proving. The paper categorizes approaches into neuro-symbolic reasoning (e.g., geometric problem solving) and sequence generation models, including sequence-to-sequence and sequence-to-tree architectures. Recent advancements leveraging large language models (LLMs) and techniques like chain-of-thought prompting are discussed, alongside their limitations in precise mathematical reasoning. Solutions like self-consistency decoding and program-aided LLMs (e.g., Chameleon) are highlighted. The survey also addresses the need for more research in low-resource settings and specialized domains (finance, science, medicine), while acknowledging ongoing challenges related to generalization, robustness, and handling large numbers in mathematical reasoning tasks.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a phenomenon where training on one language pair impacts others, sometimes positively, sometimes negatively. Contrary to common assumptions, the study finds that language similarity and the total number of languages have a surprisingly small impact on interference levels.

The primary driver of severe interference is identified as "parameter poverty"—models that are too small relative to the dataset size. This issue largely disappears as models are scaled up. Furthermore, the research highlights the crucial role of temperature sampling. Using a temperature greater than 1 allows the model to sample more from lower-resource languages, mitigating interference.

The study demonstrates that a simple, tuned temperature setting (rather than complex algorithms) is key to achieving strong performance. Baseline interference is weak due to model size in smaller models, and weak due to uncalibrated temperature in larger ones. Ultimately, the research suggests that modest scaling and careful temperature tuning can significantly reduce interference in multilingual translation without requiring specialized techniques.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen linguistischen Kontext aus verschiedenen Datensätzen, einschließlich Wikipedia.</sample>
    <sample id="69">Typically, 20 samples per class are needed to attain high performance.</sample>
    <sample id="70">Die Frage kann anhand des bereitgestellten Textes nicht beantwortet werden.</sample>
    <sample id="71">The "Resolving Indirect Referring Expressions for Entity Selection" work introduces the AltEntities Corpus, a new dataset designed to understand how users naturally select entities in conversation. The corpus addresses the challenge of indirect references—like "the newer one" instead of a direct name—which arise when users struggle to recall names or need to express preferences.

The AltEntities Corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions. Data collection uses a cartoon completion setup where annotators provide indirect references after seeing a context-setting dialogue and an alternative question generated using varying similarity levels between entities (random, similar titles, similar descriptions, similar attributes). Annotators are provided with background knowledge (Google search links for songs, Wikipedia text/images for recipes/books) to inform their choices.

Experiments with a T5 XL model reveal that accuracy significantly improves with access to background knowledge, reaching 92-95% when knowledge matches annotators' and 82-87% with partial overlap. Performance drops to 60% with only entity names, highlighting the need for better knowledge integration. The dataset demonstrates domain-generalizability and is publicly available.</sample>
    <sample id="72">Because existing methods are insufficient to address the fairness issues resulting from language model political biases.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon the existing ATOMIC knowledge base. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its focus solely on B-to-A links. Dense-ATOMIC addresses this by incorporating missing B-to-B, A-to-B, and A-to-A links, creating richer, multi-hop paths like "X asks Y to marry, Y says yes, X smiles."

The construction process involves normalizing tail events and training a relation prediction model called Rel-CSKGC. Rel-CSKGC leverages RoBERTa to encode head and tail events, utilizing semantic information without relying on graph structure, overcoming the sparsity issues of ATOMIC. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction and translation-based methods. Dense-ATOMIC itself exhibits significantly higher knowledge coverage and benefits commonsense reasoning models like COMET, enabling more diverse and coherent outputs. Evaluations of multi-hop paths within Dense-ATOMIC reveal a high prevalence of meaningful sequences, such as "X misses Y's opportunity, X goes home sadly, X is melancholy." The paper concludes by highlighting the potential of Dense-ATOMIC for advancing commonsense reasoning capabilities.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE). Developed by Zheng Yandan, Hao Anran, and Luu Anh Tuan, it addresses the limitations of existing semi-supervised approaches that often overlook the inherent connections between NER and RE tasks.

The core idea is to model these tasks jointly by propagating labels across a heterogeneous graph. This graph incorporates inter- and intra-connections among labeled and unlabeled data, leveraging similarities between entities and relations. The framework consists of four key components: span feature generation, heterogeneous graph construction (using a k-Nearest Neighbor approach), joint label propagation, and model optimization.

During label propagation, pseudo-labels are iteratively refined across the graph, diffusing labels to unlabeled data points. A confidence threshold filters low-quality pseudo-labels, and the remaining labels are combined with the original labeled data to retrain the classification model.

Experiments on four datasets (both joint and single-task) demonstrate that Jointprop consistently outperforms baseline models, particularly on single-task datasets. The results highlight the benefits of jointly learning NER and RE, capitalizing on the codependency between the two tasks to achieve significant improvements in both NER and relation extraction performance.</sample>
    <sample id="76">From pretraining data to language models to downstream tasks.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint effort from Yale University and Microsoft Research. The core contribution is the DeFacto dataset, designed to improve factual consistency in summarization. DeFacto includes human demonstrations and feedback on system-generated summaries, collected using the XSum dataset and initial outputs from the Pegasus model.

The dataset reveals that 70% of initial summaries contain factual errors. Human-edited summaries, while achieving higher factuality scores, exhibit lower textual overlap with reference summaries, likely due to factual inaccuracies in the original XSum references.

The research introduces three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing, where models refine summaries based on human feedback, shows promise with both fine-tuned and zero-shot LLMs. Feedback generation proves challenging. Automatic factual error correction demonstrates competitive performance with fewer training data, and explanation generation enhances model accuracy.

Beyond the NLG tasks, DeFacto's fine-grained annotations are valuable for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub, and the paper provides further details.</sample>
    <sample id="78">Yes, the simplification process differs. DEPLAIN-apa has more reorderings and word additions, while DEPLAIN-web has more rephrasings.</sample>
    <sample id="79">Yes.</sample>
    <sample id="80">The watermark is embedded by defining a target embedding and weighting the summation of the target embedding and the original embedding proportionally to the number of triggers (words from a moderate frequency interval) in the sentence.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This video introduces ULRA (Unsupervised AES by Learning from Rank Aggregation), a novel framework for unsupervised automated essay scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to create. ULRA aims to overcome this by leveraging multiple heuristic quality signals as a form of pseudo-ground truth.

The framework consists of two main components: a Heuristic Essay Ranking (HER) module and a Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial order pairs by ranking essays based on various quality signals like unique terms and word count. The DPRA module then trains a neural AES model using these partial-order pairs, employing a Deep Pairwise Rank Aggregation loss function that assigns learnable confidence weights to each signal to handle inconsistencies.

Finally, a Scoring Strategy transforms the model's predicted scores into a predefined scoring range. Experiments in both transductive and inductive settings demonstrate that ULRA outperforms existing unsupervised AES methods and achieves competitive results compared to cross-prompt and one-shot approaches. While still lagging behind fully supervised methods due to the lack of strong supervision, ULRA represents a significant advancement in unsupervised essay scoring by effectively aggregating multiple heuristic signals.</sample>
    <sample id="83">Ja, Encoder-Decoder- oder Encoder-PTR-Modelle können durch Training in einer Mischung von verschiedenen Sprachen verbessert werden.</sample>
    <sample id="84">Shwai He presented "PAD-Net: An Efficient Framework for Dynamic Networks" at ACL 2023, addressing the limitations of fully dynamic networks. Traditional networks are static, while dynamic networks adapt their architecture or parameters based on input. While dynamic networks often outperform static ones, fully dynamic approaches suffer from excessive parameter usage, significantly increasing model size. For example, replacing BERT-Base's feed-forward layers with Mixture of Experts increases the model size by five times.

PAD-Net aims to solve this by introducing a framework that combines static and dynamic parameters. The core hypothesis is that fully dynamic networks contain partially dynamic subnetworks that can maintain or exceed the original network's representation power. PAD-Net partitions parameters into dynamic and static components, using scale factors to control their influence and constraints to accelerate training. The Iterative Mode Partition method identifies and converts redundant dynamic parameters into static ones, minimizing impact on the loss function.

Experiments demonstrate PAD-Net achieves superior performance compared to both static and fully dynamic networks, while significantly reducing parameters and computation. Ablation studies highlight the importance of dynamic ratios and scale factors. Compared to network pruning, PAD-Net maintains static parameters, leading to better results. The framework also produces more discriminating outputs, contributing to improved performance. Future work includes extending PAD-Net to other networks, hardware-friendly structures, and incorporating additional parameter modes.</sample>
    <sample id="85">"make a chocolate cake"</sample>
    <sample id="86">They validate the covertness by visualizing embeddings on PCA plots, showing it's hard to distinguish between backdoor and normal embeddings.</sample>
    <sample id="87">The work utilizes continual pre-training, building three models based on the weights and tokenization of CamemBERT and PubMedBERT, and training them on specific datasets.</sample>
    <sample id="88">Nicht-englischsprachige Länder.</sample>
    <sample id="89">"If we go on and we receive another speech chunk, and our model predicts other three words and we will look at those cross-attention weights, we will see that no word points to the last lambda speech frames. This means that these three words will be emitted."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the traditional reliance on native speakers for NLP data annotation, particularly for low-resource languages. The authors conducted a proof-of-concept study exploring the feasibility of using language learners instead.

The study involved English, Korean, and Indonesian, utilizing four tasks from the GLUE benchmark (sentiment analysis, NLI, NER, and MRC). Language learners were categorized into proficiency levels (basic, intermediate, advanced) and compared to native speakers. Participants annotated samples with varying difficulty levels, aided by additional resources like dictionaries and machine translation. Pre- and post-tests assessed language proficiency and learning effects.

The results demonstrate that language learners' annotations are nearly as accurate as those of native speakers, especially for simpler tasks. Aggregating learner annotations through majority voting achieves performance on par with native speaker annotations. Notably, language models trained on learner-annotated data achieved 95% of ground truth performance and occasionally surpassed models trained on native speaker data.

The research highlights the potential for language learners to contribute significantly to NLP data annotation, offering a novel approach for low-resource languages and broadening NLP research accessibility. Furthermore, the study observed improvements in learners' language skills through the annotation process.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">The authors compare their method with other treeless models on the COGS benchmark.</sample>
    <sample id="93">The two co-authors, Alexander Koller and Ivan Titov, are the advisors of the first author, Matthias Lindemann.</sample>
    <sample id="94">This paper introduces "Embedding Marker," a novel backdoor-based watermark method designed to protect the copyright of embedding as services (EaaS) powered by large language models. EaaS, like OpenAI's GPT embedding API, are vulnerable to model theft through embedding learning. Embedding Marker aims to embed a covert watermark within the provider's service, detectable even when another service extracts and replicates the model.

The method involves two key steps: watermark injection and copyright verification. First, a "trigger set" of moderately frequent words is selected. During injection, the provider modifies embeddings by adding a weighted contribution of a "target embedding," proportional to the number of triggers in a user's input sentence.  Copyright verification uses a backdoor and benign dataset. The provider requests embeddings from the suspected stealer's service and compares the resulting embeddings to the target embedding using cosine and L2 similarity, as well as a KS test. Significant differences indicate watermark presence.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate strong detection performance with minimal impact on embedding utility for downstream tasks. Visualization using PCA confirms the covertness of the watermark, making it difficult to distinguish between watermarked and normal embeddings. The paper addresses limitations of existing watermark methods, offering a practical solution for protecting EaaS copyright.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University, und heute präsentiere ich unsere Arbeit „NLPositionality“, die Design-Bias in Datensätzen und Modellen charakterisiert. Diese Arbeit entstand in Zusammenarbeit mit einigen Kollegen von der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Stellen wir uns vor, Sie arbeiten für eine Zeitung und sichten die Kommentare unter einem Artikel. Sie möchten toxische Inhalte entfernen. Vielleicht greifen Sie auf eine beliebte API wie die Prospective API zur Toxizitätserkennung zurück. Diese funktioniert gut, wenn Sie Carl Jones sind, aber nicht, wenn Sie Aditya Sharma sind. Die Prospective API ist dann nicht so sensibel für beleidigende Begriffe, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem es zu systematischen Leistungsunterschieden zwischen Technologien für verschiedene Bevölkerungsgruppen kommt.

Solche Design-Biases können durch die Positionierung von NLP-Forschern und Modellentwicklern entstehen. Positionierung bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Als Forscher kann die Positionierung den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann.

Eine Frage, die sich stellt, ist: Haben Datensätze und Modelle eine Positionierung? Wir behaupten nicht, dass Modelle oder Datensätze demografische Identitäten oder Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können somit bestimmte Positionierungen stärker repräsentieren als andere.

Bisherige Arbeiten haben einige anekdotische Hinweise auf Positionierung, wie z. B. kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modellpositionierung geliefert. Diese Arbeiten vergleichen jedoch nicht Endbenutzer mit Datensätzen und Modellen und untersuchen die Positionierung von Modellen und Datensätzen nicht. Dies wird jedoch zunehmend wichtig, da NLP-Aufgaben subjektiver und sozial orientierter werden. Es ist schwierig zu charakterisieren, wie diese Positionierungen verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind.

Um die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen echter Benutzer mit bestehenden Datensätzen und Modellen. Wir tun dies mit unserem Framework „NLPositionality“. Unser Framework besteht aus zwei Hauptschritten. Im ersten Schritt annotieren wir Datensätze mit vielfältigen Annotatoren. Wir tun dies, um die Demografie der ursprünglichen Datensatz-Annotatoren zu berücksichtigen, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie selten erfasst und geteilt wird. Wir annotieren dann die Daten nach Demografie und vergleichen sie mit Modellen und Datensätzen mithilfe eines Pearson-R-Korrelationskoeffizienten. Unser Framework unterscheidet sich von der Literatur über Annotatoreinigkeit, indem es Endbenutzer mit den Vorhersagen und Labels von Modellen und Datensätzen vergleicht, anstatt nur die Annotatoreinigkeit zu betrachten oder die Verteilungen der Annotatoren zu modellieren.

Unser Framework wird weitgehend durch Lab in the Wild, eine Online-Experimentplattform für Human-Computer-Interaction-Kollaborationen, ermöglicht. In Lab in the Wild können wir diverse Freiwillige rekrutieren. Im Vergleich zu Plattformen wie Mechanical Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, ermöglicht Lab in the Wild die Erhebung hochwertiger Daten. Wir führen zwei Aufgaben auf Lab in the Wild durch, eine davon ist die soziale Akzeptanz. Die Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und bewerten, wie sozial akzeptabel die Situation ist. Anschließend können sie ihre Antworten mit einer KI und anderen vergleichen, um sich in der Studie zu engagieren. Wir haben diese Annotationen mit Social Chemistry, Delphi und GPT-4 verglichen. Wir replizieren einen ähnlichen Aufbau für die Aufgabe der Erkennung von Hassreden und Toxizität, bei der die Teilnehmer eine Instanz aus Dynahate lesen und beurteilen, ob es sich um eine Hassrede handelt. Wir vergleichen diese Annotationen dann mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT-4.

Unsere Studie umfasste insgesamt über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.

Nun sind wir besser in der Lage zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es eine Positionierung in NLP gibt. Beispielsweise stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPT-4-Analyse der sozialen Akzeptanz stellen wir fest, dass sie am meisten mit konfuzianisch und englischsprachigen Ländern übereinstimmt. Wir stellen auch eine zusätzliche Übereinstimmung mit Personen mit einem Hochschulabschluss fest. Für GPT-4 in der Aufgabe der sozialen Akzeptanz stellen wir fest, dass es am meisten mit Personen mit einem Hochschul- oder Hochschulabschluss übereinstimmt, und wir stellen das Gleiche für Dynahate fest.

Wenn Modelle und Datensätze jedoch mit bestimmten Bevölkerungsgruppen übereinstimmen, werden einige unweigerlich zurückgelassen. Beispielsweise sind Datensätze und Modelle weniger mit nicht-binären Personen als mit männlichen und weiblichen Personen übereinstimmt. Wir stellen dies sowohl in der GPT-4-Aufgabe der sozialen Akzeptanz als auch in der Dynahate-Analyse fest.

Angesichts der Positionierung in NLP, was können wir dagegen tun? Wir haben einige Empfehlungen. Zuerst sollten wir alle relevanten Designentscheidungen während des gesamten Forschungsprozesses dokumentieren. Zweitens sollten wir NLP-Forschung mit dem Blickwinkel des Perspektivismus betreiben. Drittens sollten wir spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften erstellen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusives NLP nicht nur bedeutet, Technologien für alle funktionsfähig zu machen.

Damit schließt sich unsere Präsentation. Wenn Sie mehr erfahren möchten, können Sie unseren Dashboard für die aktuellsten Analyseergebnisse und unser Paper besuchen. Vielen Dank.</sample>
    <sample id="97">Drei.</sample>
    <sample id="98">Die Reduzierung sozialer und politischer Verzerrungen ist schwierig. Das Weglassen politischer Meinungen in Trainingsdaten führt zu Verzerrungen, während die "Sanierung" das Risiko von Zensur und dem Ausschluss bestimmter Perspektiven birgt. Es ist schwer zu bestimmen, was tatsächlich neutral ist und erhalten bleiben sollte.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan University. Ich möchte Ihnen unsere Arbeit "Destillation von Skriptwissen aus großen Sprachmodellen für eingeschränkte Sprachplanung" vorstellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweisen Anweisungen in Form von zielorientierten Skripten folgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie "einen Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle Ziele effektiv in Schritte zerlegen können. Bisher lag der Fokus jedoch hauptsächlich auf der Planung für die abstrakten Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie z. B. "einen Schokoladenkuchen backen", wurde bisher wenig untersucht. In dieser Arbeit definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Planungsziele ausübt. Ein abstraktes Ziel kann von verschiedenen realen, spezifischen Zielen mit vielfältigen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte schreiben, die vernünftig sind und den Einschränkungen entsprechen. In dieser Arbeit evaluieren und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da es keinen Datensatz für spezifische Ziele gibt, der unsere Studie unterstützt, müssen wir diese zuerst beschaffen. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit vielfältigen Einschränkungen für die datengestützte Erfassung durch Menschen unter Verwendung von InstructGPT. Wir wählen 100 spezifische Ziele aus und bewerten die von großen Sprachmodellen generierten Skripte. Die Tabelle zeigt die Gesamtgenauigkeit der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum die Modelle versagen. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Einhaltung der Einschränkungen jedoch nicht garantiert werden kann. Wir befassen uns mit einer detaillierteren Kategorisierung der Einschränkungen, die in wikiHow definiert sind. Die Heatmap in der Abbildung zeigt, dass die Planungsleistung von InstructGPTs je nach Kategorie der Ziele erheblich variiert. Frühere Studien haben gezeigt, dass die Qualität der Ausgaben von Sprachmodellen eine hohe Varianz aufweist, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des "übermäßigen Generierens und anschließenden Filterns", um die Generierungsqualität zu verbessern. Zunächst zeigen wir Constraint-Typen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Seed-Abstrakten Zielen. Anschließend generiert InstructGPT K Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um treue Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen den Cosinus-Ähnlichkeitswert als Ähnlichkeitswert, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielbeschränkung enthält. Wir behalten das Skript nur bei, wenn das Ziel im Zielset den höchsten Wert erzielt. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in Bezug auf die semantische Vollständigkeit als auch die Einhaltung der Einschränkungen erheblich. Da große Sprachmodelle teuer im Einsatz sind, ist es wichtig, die Sprachplanungsfähigkeiten kleinerer, spezialisierter Modelle zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt in diese Richtung. Bisherige Studien haben jedoch keine Planung für spezifische Ziele ermöglicht, und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode an, um einen Datensatz für eingeschränkte Sprachplanung zu erstellen, der als CoScript bezeichnet wird. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität des Validierungs- und Testsets sicherzustellen, bitten wir Crowdsourcing-Mitarbeiter, falsche Beispiele zu finden und zu korrigieren. Die Abbildung zeigt die Verteilung der Einschränkungen in CoScript. Wir stellen fest, dass CoScript eine hohe Vielfalt in den generierten spezifischen Zielen aufweist. Mit CoScript können wir kleinere, spezialisierte Modelle für die eingeschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt ist, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle bessere Ergebnisse erzielen können, wenn sie mit geeigneten Datensätzen trainiert werden. Zusammenfassend lässt sich sagen, dass wir das Problem der eingeschränkten Sprachplanung definieren. Wir evaluieren die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle und entwickeln eine Methode zum übermäßigen Generieren und anschließenden Filtern für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen qualitativ hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource sein kann, um die Forschung zur Sprachplanung voranzutreiben. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserem Paper.</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering that addresses the need for fewer training examples compared to existing methods. It combines unsupervised retrieval with a few-shot language model-based reranker, achieving good performance with as few as 128 examples.

The process involves retrieving candidate chains using TF-IDF and hyperlink traversal, then reranking them using a language model. The scoring function is based on the likelihood of the question given a constructed chain prompt, which includes chain documents and an instruction designed to elicit reasoning. Techniques like instruction search and sampling, and temperature scaling are explored to optimize the language model's performance.

Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers. Ablation studies confirm the importance of each component. When paired with a reader model like ELECTRA-Large, PromptRank achieves strong downstream multi-hop QA performance, only slightly underperforming MDR. The research highlights the effectiveness of language models for ranking candidate paths and emphasizes the crucial role of instructions in guiding the language model's reasoning.</sample>
    <sample id="101">PaLM's fluency is comparable to state-of-the-art systems.</sample>
    <sample id="102">First, it should be applicable to embedding as services. Second, it should not degrade the utility of the provided embeddings. Third, it should be covert enough. Finally, it needs to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">14 different languages.</sample>
    <sample id="104">Über 16.000</sample>
    <sample id="105">Cosine similarity, L2 similarity, and a KS test p-value.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to address the challenge of information seeking with complex, multi-constraint queries. The dataset is motivated by real-world scenarios like a zoologist identifying an unknown reptile species or a reader seeking their next book based on specific preferences. These situations often involve implicit set operations – intersections, complements, and unions – within queries.

QUEST comprises over 3,000 entity-seeking queries derived from Wikipedia categories across films, books, plants, and animals. The queries are generated using set operations and then paraphrased and validated by human annotators to ensure fluency and naturalness. Annotators also verify the relevance of answer entities and identify specific spans within documents that support each query constraint.

The dataset's difficulty lies in the need for systems to retrieve multi-answer sets from a large corpus, where evidence for relevance can be scattered across different parts of a document. Initial evaluations using sparse and dense retrievers, along with a T5 reranker, reveal significant room for improvement, particularly for queries involving set intersection and difference operations. The low F1 scores highlight the current limitations of systems in effectively handling these complex information needs. QUEST aims to facilitate research towards building more robust and accurate retrieval systems for users with selective information requirements.</sample>
    <sample id="107">Encoder-Decoder- und Encoder-PTR-Modelle wurden eingesetzt, wobei Encoder-Decoder die beste Leistung erbrachten. Diese Modelle konnten durch Training in einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="108">Koustav Sinha's ACL 2023 paper investigates the robustness of language model acceptability judgments when considering longer contexts. Traditional Minimal Pair Paradigm (MPP) evaluations, which compare acceptable and unacceptable sentences, often use short, isolated sentences. However, with the rise of large language models (LLMs) and their expanded context windows, this approach may be insufficient.

The research revisits the MPP pipeline to evaluate acceptability across longer sequences. They recreate sentences by combining acceptable/unacceptable queries with prefixes from the same or different datasets, creating "mismatch" scenarios. Experiments reveal that judgments are largely robust with irrelevant Wikipedia context, even up to 1024 tokens. However, when prefixes are drawn from the same dataset (BLiMP or SyntaxGym), acceptability judgments significantly shift based on whether the prefix is acceptable or unacceptable, and this effect intensifies with longer context lengths.

Analysis suggests LLMs are sensitive to latent syntactic and semantic features shared across sentences. Perturbations of sentences within acceptable or unacceptable domains consistently impacted model judgments. The key takeaway is that current MPP evaluations, with their focus on short sentences, may not fully capture the abstract linguistic knowledge embedded within LLMs and their ability to process information across extended contexts.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large dataset of instructions, inputs, and outputs entirely automatically, without human annotation. The method leverages a pre-trained GPT-3 model, initially seeded with examples from the Super-Natural Instructions dataset. The model then generates new instructions and corresponding inputs, followed by outputs based on those instructions. To enhance diversity, the system also creates paraphrases of the generated instructions.

The resulting dataset comprises 64,000 examples, expanding to 240,000 with paraphrases. Analysis reveals a correctness rate exceeding 50%, with even incorrect examples proving valuable for training. The dataset showcases remarkable creativity, encompassing tasks beyond traditional NLP benchmarks, such as evaluating scientific experiments and inventing new words.

The utility of "Unnatural Instructions" is demonstrated by fine-tuning an 11 billion-parameter T5 model, which outperforms existing instruction-tuned models (T0++ and Tk-instruct) across multiple benchmarks. Considering the cost-effectiveness of automated generation, training on "Unnatural Instructions" proves superior to training on the manually curated Super-Natural Instructions dataset. This work highlights the potential of language models to autonomously generate high-quality, diverse training data, offering a faster and cheaper alternative to human annotation.</sample>
    <sample id="111">The provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute werde ich unsere Arbeit „Funktionieren CoNLL-2003-Named-Entity-Tagger immer noch gut im Jahr 2023?“ vorstellen. Lasst uns loslegen. Unsere Arbeit untersuchte das Problem der Verallgemeinerung mithilfe der Named-Entity-Recognition-Aufgabe oder der NER-Aufgabe. Wir stellen fest, dass Modelle in CoNLL-2003 seit fast 20 Jahren zur Entwicklung von NER verwendet werden, was natürlich mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten verallgemeinern? Und was ist für eine gute Verallgemeinerung erforderlich, wenn wir neue Tagger entwickeln? Gleichzeitig, wenn wir eine schlechte Verallgemeinerung feststellen, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anleitungen annotiert haben. Anschließend haben wir über 20 Modelle auf CoNLL-2003 feinabgestimmt. Wir haben sie sowohl auf den CoNLL-03-Testdatensätzen als auch auf dem CoNLL++-Datensatz bewertet. Und zu guter Letzt haben wir den prozentualen Leistungsunterschied berechnet, um die Verallgemeinerung jedes Modells zu bewerten. Was ist also für eine gute Verallgemeinerung erforderlich? Im Laufe der Experimente haben wir festgestellt, dass drei Hauptbestandteile erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle in der Regel besser auf neue Daten verallgemeinern. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass größere Modelle in der Regel zu einer besseren Verallgemeinerung führen. Und zu guter Letzt wissen wir alle, dass die Anzahl der Feinabstimmungsexemplare die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsexemplare tatsächlich auch zu einer besseren Verallgemeinerung führen. Zu unserer nächsten Frage, was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptives Overfitting, das durch die wiederholte Verwendung desselben Testdatensatzes verursachte Overfitting ist und sich typischerweise als abnehmende Erträge auf einem neuen Testdatensatz äußert. Die zweite Hypothese ist der zeitliche Drift, der der Leistungsabfall ist, der durch die zunehmende zeitliche Lücke zwischen den Trainings- und Testdaten verursacht wird. Für Daten-Overfitting haben wir gesehen, dass die rote Best-Fit-Linie auf der rechten Grafik eine Steigung hat, die größer als eins ist. Das bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 erzielt haben, zu mehr als einer Einheit der Verbesserung auf CoNLL++ führt, was bedeutet, dass keine abnehmenden Erträge vorliegen. Dies zeigt uns, dass adaptives Overfitting in diesem Fall nicht beobachtet wird. Was ist aber mit zeitlichem Drift? Für zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten neu zu trainieren oder weiter vorzutrainieren, und wir haben festgestellt, dass die Leistung mit einer größeren zeitlichen Lücke abnimmt, was unsere Hypothese bestätigt, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Verallgemeinerung eine bessere Modellarchitektur, eine größere Modellgröße und auch mehr Feinabstimmungsexemplare benötigen. Diese gehen Hand in Hand, wir können nicht nur eine Zutat haben, sondern die anderen verwerfen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Wenn wir nun zur Frage zurückkehren, die wir in der Überschrift unseres Papiers gestellt haben: Funktionieren CoNLL-2003-Tagger immer noch im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Papier mehr Forschung darüber anregt, wie die Verallgemeinerung von Modellen verbessert werden kann. Und schließlich schauen Sie bitte unseren Artikel, unseren Datensatz an und kontaktieren Sie mich, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="114">This work from Nanyang Technological University of Singapore introduces "Grouped Head Attention" (GHT), a novel approach to compressing multi-head attention in large language models (LLMs). Addressing the heavy parameter problem of LLMs, GHT employs a divide-and-conquer strategy to group attention heads, promoting similarity within groups and separation between them during training. The method consists of two stages: group-constrained training and a Voting-to-Stay algorithm. Group-constrained training uses unsupervised learning to organize heads into groups, while Voting-to-Stay prunes redundant heads, retaining only one per group. Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 4.4% BLEU, 7% summarization improvement) alongside substantial parameter compression (up to 90%). Furthermore, the resulting "LITE" model achieves a 62% faster inference speed and 80% reduction in FLOPs. The authors suggest task-specific pruning as a future direction, leveraging the Lottery Ticket Hypothesis to further optimize LLMs for real-world applications where only a subset of capabilities are needed.</sample>
    <sample id="115">Lambda speech frames.</sample>
    <sample id="116">"Servin is a judge."</sample>
    <sample id="117">Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">The ACL 2023 submission "Improving Pretraining Techniques for Code-Switched NLP" addresses the challenge of building computational models for code-switching, a common phenomenon in linguistically diverse communities where sentences mix multiple languages. Existing multilingual models like mBERT and XLM-R struggle with code-switched tasks.

The paper introduces SwitchMLM, a novel Masked Language Modeling (MLM) technique specifically designed for code-switching. Unlike standard MLM, SwitchMLM only masks tokens at "switch-points"—transitions between languages. To overcome the need for language identification (LID) tags, a surrogate method called FrequencyMLM is proposed, using negative log likelihoods to estimate language.

Furthermore, the work proposes architectural modifications, including residual connections from intermediate layers rich in switch-point information to the final layer. An auxiliary LID-based loss is also added to encourage these intermediate layers to encode language information.

Experiments on sentiment analysis demonstrate that the combined method (Switch/FrequencyMLM + ResBERT + auxiliary loss) outperforms existing approaches across various language pairs. Probing experiments, using linear and conditional probing, confirm that the proposed methods increase the amount of switch-point information in both intermediate and final layers, validating the effectiveness of the approach. The findings suggest that incorporating switch-point awareness into pretraining significantly improves performance on code-switched NLP tasks.</sample>
    <sample id="119">GPT-4, GPT series, BART series, and RoBERTa.</sample>
    <sample id="120">Das Modell verwendet die Aufmerksamkeitswerte aus dem Cross-Attention-Mechanismus.</sample>
    <sample id="121">Saying the name of the song or its position, for example, "Easy on Me" or "the first one."</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on MultiInstruct, a novel approach to enhance multi-modal zero-shot learning through instruction tuning. Recognizing the lack of large-scale multi-modal instruction datasets compared to NLP, they created MultiInstruct, a benchmark dataset comprising 62 diverse multi-modal tasks across 10 categories, each with five expert-written instructions.

Their study utilized OFA, a unified multi-modal pre-trained model, and demonstrated that instruction tuning significantly improves performance on both seen and unseen multi-modal tasks. They explored transfer learning from natural instruction datasets, finding it beneficial for both performance and sensitivity—the model's consistency in output regardless of instruction wording.

Experiments revealed that using multiple instructions (five versus one) boosts overall performance and reduces sensitivity. Transfer learning from natural instructions also improved performance on natural instruction tasks. The research highlights the effectiveness of different fine-tuning strategies and introduces a new metric, sensitivity, to evaluate model robustness.

The team is currently expanding MultiInstruct with approximately 150 additional vision-language tasks, which will be publicly released. The dataset and model are accessible via the provided QR code. Overall, MultiInstruct advances multi-modal instruction tuning and explores techniques to improve zero-shot capabilities.</sample>
    <sample id="124">This presentation introduces "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models" by Tan Qingyu from the National University of Singapore and Alibaba. The work addresses the limitations of existing temporal reasoning studies, which often overemphasize a specific reasoning type.

The researchers break down temporal reasoning into three levels: time-to-time (e.g., year calculations), time-to-event (e.g., what team did someone play for in a specific year), and event-to-event (e.g., what did someone do after a previous event). They observed biases in models like T5 and ChatGPT, particularly a preference for the 2000-2020 timeframe.

To address this, they created the TempReason dataset, covering all three reasoning levels and a broad temporal range. They evaluated models using Closed Book QA, Open Book QA (with Wikipedia context), and a novel Reasoning QA setting (providing relevant temporal knowledge).

To improve temporal reasoning, they proposed a training strategy: Temporal span extraction pre-training and time-sensitive reinforcement learning. Their final model, TempT5, outperformed other models, including ChatGPT, on the TempReason benchmark, especially in Open Book and Reasoning QA settings. However, they noted performance fluctuations across different time periods, suggesting potential biases related to training data imbalance, which they plan to address in future work.</sample>
    <sample id="125">Yanis Labrak</sample>
    <sample id="126">Yes.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" introduces a method for transferring reasoning abilities from large language models (LLMs) to smaller, more deployable models. Chain-of-thought prompting enables LLMs to solve complex tasks, but it's computationally expensive. This work leverages large models as "teacher" models to generate step-by-step solutions, which are then used to fine-tune smaller "student" models.

A key innovation is "Diverse Reasoning," where multiple reasoning paths are generated from the teacher model using stochastic temperature sampling. This provides richer training data for the student, leading to improved performance. The researchers demonstrated that even student models with as few as 0.3 billion parameters can achieve complex reasoning capabilities through this fine-tuning process.

Experiments across 12 tasks showed significant performance gains compared to prompt-based baselines and vanilla fine-tuning, particularly in text-based tasks. Diverse Reasoning notably boosted performance on tasks like Multi Arithmatic. The authors emphasize the scalability of their approach, highlighting trade-offs between development costs (teacher model, dataset size, Diverse Reasoning) and inference costs (student model size). The code and data are publicly available, encouraging further research and exploration of this distillation technique for transferring emergent abilities to smaller models.</sample>
    <sample id="128">The KITMUS test is introduced as a diagnostic suite to evaluate knowledge integration in natural language understanding models. These models often require combining knowledge acquired during pre-training with information provided at inference time. KITMUS focuses on coreference resolution, presenting scenarios where pronoun resolution necessitates both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases"). The test features three settings: "Background-Pretrain" (background knowledge in pre-training), "Background-Both" (background and entity-specific knowledge at inference), and "Background-Inference" (only entity-specific knowledge, simulating new concepts not present in pre-training data). Evaluation with human participants and coreference resolution models reveals that models often rely on surface cues rather than true knowledge integration. While task-specific training improves performance, reliably integrating background knowledge provided solely at inference time remains a challenge, even for state-of-the-art models. The KITMUS dataset and code are publicly available.</sample>
    <sample id="129">The authors gave the example of a woman warrior.</sample>
    <sample id="130">Transformer models.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Zwei.</sample>
    <sample id="133">Mehrere Modalitäten.</sample>
    <sample id="135">ABC-Eval is a new dimensional approach to evaluating conversational AI, developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of traditional human evaluations (Likert scales, pairwise comparisons) by focusing on annotating specific behaviors exhibited by dialogue models.

ABC-Eval identifies and measures thematic errors like irrelevance, contradictions, hallucinations, common sense violations, and failures in empathy. The method was tested on four state-of-the-art models across 100 conversations, comparing it to existing evaluation methods.

The results demonstrate that ABC-Eval provides more reliable and predictive labels than Likert ratings or pairwise comparisons, as evidenced by higher inter-annotator agreement and stronger correlation with overall conversation quality. Stepwise regression analysis reveals that ABC-Eval metrics capture unique aspects of chat quality, explaining over 25% of conversation quality compared to less than 5% with Likert metrics.

The study highlights ongoing challenges, quantifying error rates such as common sense violations (20%), irrelevant information (15%), and contradictions (10%). ABC-Eval aims to provide a more precise and reliable evaluation framework, facilitating progress in conversational AI development.</sample>
    <sample id="136">Jasivan presented "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addressing the limitations of current benchmarks in evaluating numerical reasoning abilities of language models. Existing benchmarks like CommonCore and Illinois, often relying on accuracy scores, fail to pinpoint specific mathematical strengths and weaknesses.

FERMAT, introduced as a solution, is a flexible evaluation set categorized by arithmetic types (number understanding, mathematical operations, and training dependency). It utilizes math questions from Illinois and CommonCore, manipulating number representations (integers, decimals) and varying mathematical operations to assess model performance across a broad spectrum. Initial zero-shot evaluations revealed poor performance across all aspects, highlighting the benchmarks' inadequacy.

Fine-tuning with 200,000 generated examples improved performance, particularly when incorporating diverse number types. Further analysis of training dependency showed that even when models encounter identical expressions during testing, accuracy remains low, suggesting a reliance on linguistic cues rather than true mathematical understanding.

The study also investigated the impact of training templates, finding that increased language and mathematical diversity—achieved by incorporating datasets like GSM8K and AQUA—significantly boosted performance. The research concludes that current benchmarks are unrepresentative, advocating for FERMAT as a more informative alternative. It also identifies number encoding and tokenization as areas needing improvement for enhanced numerical reasoning capabilities in language models.</sample>
    <sample id="137">Tell2Design introduces a new machine-learning task: generating floor plan designs directly from natural language instructions. This addresses the need for user-friendly design tools, enabling individuals without expertise to contribute to the design process. The research presents the Tell2Design dataset, comprising 5,051 human-annotated and approximately 76,000 artificially generated language instructions paired with corresponding floor plans.

The core challenge lies in generating designs under strict constraints, understanding complex floor plan descriptions, and handling ambiguous instructions. The authors propose a sequence-to-sequence model, leveraging a transformer-based encoder-decoder framework and a pre-trained T5 language model, to represent floor plans as structured sequences of room bounding boxes.

Experiments on the T2D dataset demonstrate that this approach significantly outperforms existing text-conditional image generation models, achieving high Intersection over Union (IoU) scores. While a language distribution gap exists between artificial and human instructions, the study shows that combining both types of data during training improves performance. The research highlights the potential of language-guided design generation and provides a foundation for future work in this area.</sample>
    <sample id="138">Reliable integration of backward knowledge presented only at inference time.</sample>
    <sample id="139">Ying and Zhiyang</sample>
    <sample id="140">Yes, crowd-sourced workers were used to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">Bestehende Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie oft auf Fachwissen und menschlicher Kuratierung beruhen.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit zum Thema "Auflösung indirekter Bezugsausdrücke für die Entität Auswahl" sprechen, in der wir den AltEntities-Korpus vorstellen. Mein Name ist Javad Hosseini und dies ist eine Gemeinschaftsarbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Wahl treffen wollen. Betrachten Sie folgende alternative Frage: "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte der Nutzer zwischen einem dieser beiden Songs auswählen. Das Offensichtlichste wäre, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Songs "Easy on Me" oder seine Position "die erste" nennt. Aber manchmal ist ein indirekter Bezug angemessener, um ein natürlicheres Gespräch zu führen. Dies kann passieren, wenn der Nutzer sich den Namen nicht merken kann. Oder wenn die Aussprachen zu ähnlich sind und schwer zu disambiguieren sind. Oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "der neuere Song" oder "der Song, der nicht energiegeladen ist". Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Benchmarking von LLMs' Entitätsverständnis. Wir sind nicht fähig, einen größeren öffentlichen Datensatz für diese Aufgabe zu finden, daher erstellen wir einen mit Hilfe von Crowd-Annotationen. Unser Datensatz deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datensatz-Erfassungsmethodik betont die Informalität durch einen Cartoon-Abschlussaufbau. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Weißt du noch, welchen Song wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob einen indirekten Bezug, um eine dieser Entitäten auszuwählen, zum Beispiel "der neuere Song". Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Prompts pro Bereich ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer eine einfache Vorlage. Meinten Sie A oder B? Dabei sind A und B Stichproben aus Wikipedia. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir in der Liste höher gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen. Die erste ist uniform zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel das gleiche Genre oder der gleiche Künstler für einen Song. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie die Namen dieser Entitäten, aber sie kennen sie nicht unbedingt. Was wir also tun, ist, dass wir einige Hintergrundinformationen zu den beiden Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jedem Song und bitten die Annotatoren, mindestens einige Teile von jedem Song anzuhören und über jeden Song zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für den Song "Easy on Me". Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann baten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Zum Beispiel "der mit der Klaviermusik". Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "der ohne Worte", "nicht der mit dem 12-jährigen Jungen", oder "der fiktive", oder "stammt aus Aserbaidschan" und so weiter. Der AltEntities-Korpus hat 6.000 alternative Fragen in drei Bereichen und 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5 XL-Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf die gleichen Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95 %. Dies ist jedoch nicht realistisch. Wenn das Sprachmodell Zugriff auf einige teilweise überlappende Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugriff auf die Entitätsnamen hat, beträgt die Genauigkeit nur 60 %, so dass noch viel Verbesserungspotenzial besteht. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Wait-k strategy and Local Agreement.</sample>
    <sample id="144">Nantes University Hospital</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Dialogue summarization, a subtask of text summarization, aims to create concise summaries of dialogues. While recent advancements using large language models have improved fluency, summaries often contain factual errors, with omission being a significant issue. This paper addresses this gap by systematically analyzing omission in dialogue summarization.

Analysis reveals that approximately 70% of generated summaries exhibit omission, and omitted information is randomly distributed throughout dialogues, highlighting the difficulty in identifying key information. To facilitate further research, the authors introduce the OLDS dataset, providing high-quality omission labels for dialogue summarization across five domains. The dataset was created using diverse candidate summaries generated by various models and an automatic labeling method, validated by human evaluation.

The paper explores three baseline models for omission detection, finding the task challenging with an F1-score around 50%. However, a post-editing method that incorporates detected omissions into the summary generation process significantly improves summary quality, demonstrating the value of omission detection for enhancing dialogue summarization. The OLDS dataset and findings suggest that addressing omission is crucial for creating more accurate and reliable dialogue summaries.</sample>
    <sample id="147">Drei.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und werde kurz das Paper "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachtranslation? Simultane Sprachtranslation, oder SimulST, ist der Prozess der Übersetzung von gesprochener Sprache in Echtzeit in Text in einer anderen Sprache, der die sprachübergreifende Kommunikation ermöglicht. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wodurch zusätzliche Module entstehen, die optimiert werden müssen. Lange und komplizierte Trainingsprozeduren, beispielsweise Trainings mit unterschiedlichen Optimierungszielen. Und das Trainieren und Warten mehrerer Modelle, um unterschiedliche Latenzbereiche zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden usw. Was ist also unsere Lösung? Erstens die Verwendung bereits existierender Offline-ST-Modelle ohne erneutes Training oder die Anpassung spezifischer Architekturen für SimulST. Verwenden Sie nur ein Modell für jeden Latenzbereich und handhaben Sie die Latenz über bestimmte Parameter. Und nutzen Sie das bereits im Modell vorhandene Wissen durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Texteausgabe. Das ist der Cross-Attention-Mechanismus, und Sie können ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist der Vorschlag von EDAtt, oder Encoder-Decoder Attention, und zwar eine Strategie, bei der wir entscheiden, ob eine partielle Übersetzung ausgegeben oder nicht ausgegeben wird, basierend darauf, wohin die Aufmerksamkeit gerichtet ist. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. ihre Summe unterhalb einer bestimmten Schwelle Alpha in den letzten Lambda Sprachframes liegt, was bedeutet, dass genügend stabile Informationen empfangen wurden. Zum Beispiel, wenn wir einen Sprachabschnitt erhalten, der "Ich werde über... sprechen" enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, dann schauen wir uns die Cross-Attention-Gewichte an, und wir werden sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Sprachframes zeigt, d. h. Lambda Sprachframes. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während wir das letzte Wort nicht ausgeben und auf einen weiteren Sprachabschnitt warten, da die Summe der Cross-Attention über einer bestimmten Schwelle Alpha liegt. Wenn wir fortfahren und einen weiteren Sprachabschnitt erhalten, und unser Modell weitere drei Wörter vorhersagt, und wir uns diese Cross-Attention-Gewichte ansehen, werden wir sehen, dass kein Wort auf die letzten Lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, stellen wir die Ergebnisse der simultanen Sprachtranslation in Diagrammen dar, in denen wir auf der einen Seite BLEU haben, das die Übersetzungsqualität misst, und auf der anderen Seite die durchschnittliche Verzögerung, das Latenzmaß. Wir berücksichtigen auch die rechnerisch bewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir wollen also, dass unsere Kurven auf diesem Diagramm so hoch wie möglich sind. Aber wir wollen auch, dass sie nach links verschoben sind. Wir vergleichen mit beliebten Strategien, die auch auf Offline-Modelle angewendet werden, nämlich der Wait-k-Strategie und der Local Agreement. Wir vergleichen auch mit dem hochmodernen Architektur, die speziell für simultane Vorübersetzung entwickelt wurde. Dies sind alle Ergebnisse der simultanen Sprachtranslation-Strategie auf Deutsch. Wir sehen, dass sie alle Strategien, die auf Offline-Modelle angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechnerisch bewusste Zeit berücksichtigen, es die schnellste Strategie ist. Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unser Paper. Und wir haben den Code und die Modelle sowie die simultanen Ausgaben Open Source freigegeben, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">MeetingQA is a new dataset for extractive question answering (QA) focused on meeting transcripts, addressing a gap in NLP research that primarily focused on summarization and action item extraction. The dataset leverages the rich, domain-specific nature of meeting discussions, where questions often spark detailed responses and debates.

MeetingQA comprises 7.7K questions derived from the AMI corpus, with a significant portion (30%) being unanswerable and a notable prevalence of multi-span (40%) and multi-speaker (48%) answers. The questions themselves are often longer, open-ended, and include rhetorical questions, with a common theme of seeking opinions. A key challenge is the frequent disagreement within multi-speaker answers (70%).

The research explores various QA approaches, including context retrieval for short-context models, single-span and multi-span models, and data augmentation using silver annotations from the MediaSum dataset. Results reveal a substantial performance gap between models and human performance, with short-context models (RoBERTa) slightly outperforming long-context models (Longformer). Multi-span models showed comparable or slightly lower performance than single-span models. Silver data augmentation improved zero-shot performance, and larger instruction-tuned models (FLAN-T5) demonstrated competitive zero-shot results. Error analysis highlighted difficulties in identifying rhetorical questions and speaker attribution, indicating areas for future research.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung zu MultiInstruct vorstellen, die Multi-Modale Zero-Shot-Lernverfahren durch Instruction Tuning verbessert. Angesichts der Fortschritte bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um vortrainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf parameter- und dateneffiziente Weise wiederzuverwenden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning großen Sprachmodellen ermöglicht, auf ungesehenen Aufgaben in Zero-Shot-Manier zu arbeiten, indem sie natürlichen Anweisungen folgen. Die meisten bisherigen Arbeiten zum Instruction Tuning konzentrierten sich jedoch auf die Verbesserung der Zero-Shot-Leistung bei sprachbasierten Aufgaben, während Computer Vision und multimodale Aufgaben vernachlässigt wurden. Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction Tuning von multimodalen vortrainierten Modellen die Verallgemeinerung auf ungesehene multimodale Aufgaben tatsächlich verbessern kann. Darüber hinaus haben wir bei unserer Forschung festgestellt, dass es eine erhebliche Diskrepanz bei der Verfügbarkeit von Instructional Datensätzen zwischen NLP und multimodalen Bereichen gibt. Es gibt mehr als 1600 sprachbasierte Instruction-Aufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multimodalen Instruction-Datensatz. Dies motiviert uns zum Aufbau eines multimodalen Instruction-Tuning-Datensatzes. Hier präsentieren wir MultiInstruct, den ersten multimodalen Instruction-Tuning-Benchmark-Datensatz, der aus 62 vielfältigen multimodalen Aufgaben in 10 breiten Kategorien besteht. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen versehen. Um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein einheitliches multimodales vortrainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispiele aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabetypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence-to-Sequence-Format. Dabei werden Text, Bilder, Anweisungen und Begrenzungsrahmen im selben Token-Raum dargestellt. Nun werde ich über multimodales Instruction Tuning sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wählen 10.000 Instanzen pro Aufgabe aus. Für das Testen behalten wir die gesamte Gruppe für Common-Sense-Reasoning zurück und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous aus. Wir verwenden alle Instanzen in der Test-Split für jede Aufgabe. Darüber hinaus wählen wir zufällig 20 Aufgaben aus der Test-Split von Natural Instructions als ungesehene Aufgabe für NLP aus. Wir verwenden das vortrainierte OFA Large Model als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Instruction-Vorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment melden wir die minimale und maximale Leistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multimodale Klassifikationsaufgabe ist, melden wir die Genauigkeit. Wenn es sich um eine multimodale Generierungsaufgabe handelt, melden wir Rouge-L. Für NLP-Aufgaben melden wir ebenfalls Rouge-L. Wir führen außerdem eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, für dieselbe Aufgabe konsistent dieselben Ausgaben zu erzeugen, unabhängig von geringfügigen Variationen in der Formulierung der Anweisung. Hier sind unsere Hauptergebnisse. Wie man sieht, kann Instruction Tuning die Leistung von OFA bei gesehenen multimodalen Aufgaben deutlich verbessern. Auch das Transferlernen von Natural Instruction-Datensätzen kann dem Instruction Tuning zugute kommen. Hier kann man sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung erzielt und gleichzeitig die Sensitivität verringert. Wir haben außerdem ein Experiment durchgeführt. Wir verwenden eine Anweisung versus 5 Anweisungen. Wie man sieht, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität deutlich reduzieren. Dies zeigt den Effekt verschiedener Fine-Tuning-Strategien auf die Sensitivität des Modells. Wir können auch sehen, dass das Transferlernen von Natural Instruction-Datensätzen OFA eine viel bessere Sensitivität ermöglicht als das ursprüngliche OFA-Modell. Wir können auch sehen, dass das Transferlernen von Natural Instruction-Datensätzen OFA helfen kann, auf dem Natural Instruct-Datensatz eine viel bessere Leistung zu erzielen. Insgesamt schlagen wir den ersten großen multimodalen Instruction-Tuning-Datensatz vor, der die Zero-Shot-Fähigkeiten von OFA deutlich verbessert hat, und wir untersuchen verschiedene Transferlerntechniken und zeigen deren Vorteile. Wir haben eine neue Metrik namens Sensitivität entworfen. Einmal noch, wir sammeln einen noch größeren multimodalen Instruction-Tuning-Datensatz mit rund 150 zusätzlichen Vision-Language-Aufgaben und werden ihn veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider presented "Exploring Large Language Models for Classical Philology," detailing their work creating new language models for Ancient Greek and Latin. Existing models like Latin BERT and Ancient Greek BERT are limited as they are monolingual and their performance is not well-evaluated.

The project aimed to create comparable models, push the state-of-the-art, explore different architectures, and introduce multilingual models. They developed GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder) for Ancient Greek, and PhilBERTa and PhilTa (multilingual) for Greek, Latin, and English.

A key innovation was creating a new, high-quality Ancient Greek pre-training corpus by identifying incorrectly OCR'd Greek stop words within the Internet Archive and re-scanning texts with proper Greek OCR settings.

Benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization showed significant performance improvements over existing models for both languages, particularly in lemmatization using the encoder-decoder architecture. Analysis revealed unique behavior in the T5 encoder compared to traditional encoder-only models. While the models demonstrated strong semantic and world knowledge, the multilingual models didn't significantly outperform the monolingual ones.

The research provides powerful new tools for classical philology, including a novel dataset and insights into model architectures and multilinguality.</sample>
    <sample id="153">This presentation by Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team focuses on addressing ambiguities in text-to-image generative models. The core problem is that ambiguous prompts can lead to images that don't align with the user's intended meaning.

The research introduces a pipeline to tackle this. First, a benchmark dataset (modified LAVA corpus) is created to cover various ambiguity types. Then, a prompt disambiguation framework is employed, utilizing a language model to either generate clarifying questions for the user or propose different visual interpretations. The user selects the option aligning with their intention, resulting in a disambiguated prompt.

To evaluate faithfulness, an automatic framework is proposed. It generates images from both the original ambiguous and the disambiguated prompts. A Visual Question Answering (VQA) model then assesses whether the generated image satisfies the user's stated intention. Agreement between the automatic evaluation and human evaluation demonstrates the framework's reliability.

Key findings include disparities in resolving different ambiguity types, a positive impact of the disambiguation framework on faithful image generation, and the effectiveness of the automatic evaluation method. The work aims to improve the alignment between user intention and the images produced by text-to-image models.</sample>
    <sample id="154">University of Trento</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Shen Gao from Shandong University presented "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a joint work addressing the challenge of distilling key information from dialogues into concise summaries. Existing methods rely on pre-computed static graphs using external linguistic tools, which are prone to errors and inflexible.

The proposed SDDS model overcomes these limitations. It utilizes an Utterance Encoder to create vector representations, constructs a static graph using four heuristic methods (Discourse Parsing, Key Co-occurrence, Speaker Relationship, and Utterance Position), and then introduces a Static-Dynamic Graph module. This module combines static graphs and dynamically learns semantic relationships between utterances using a multi-head attention model.

The model fuses these structures into a unified graph and incorporates the dialogue structure information during summary generation via a dual cross-attention mechanism. The four heuristic methods for static graph construction include discourse parsing, key co-occurrence analysis, speaker relationship modeling using sliding windows, and incorporating utterance position information. A 1x1 convolutional layer fuses the static graph adjacency matrices, while a dynamic graph module learns relationships based on deep vector representations. Finally, a pre-trained language model generates the summary. The code and data are publicly available on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS introduced "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenges of coreference resolution in lengthy texts. Coreference resolution involves linking mentions of the same entity within a document, a task traditionally hampered by quadratic computational complexity. Cache-based methods offer a linear solution by using a fixed-size cache, but standard Least Recently Used (LRU) eviction policies struggle with topic shifts in long documents, leading to high cache misses.

The proposed Dual Cache tackles this by employing both a local and a global cache. The local cache, using LRU, stores entities relevant to the immediate context, while the global cache, utilizing Least Frequently Used (LFU), manages entities with broader significance across the document. The model classifies mentions, adding them to either cache based on frequency.

Evaluations on four benchmarks (LitBank, OntoNotes, WikiCoref) demonstrated that Dual Cache outperforms baselines, even those with unbounded memory, particularly in long-form content like a 30,000-word book. It significantly reduces cache misses compared to single-cache approaches and achieves the best performance-to-cost ratio, making it a more efficient and effective solution for coreference resolution in long documents.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserem ACL 2023-Paper begrüßen zu dürfen. Die Urteile von Sprachmodellen über die Akzeptanz sind nicht immer robust gegenüber dem Kontext. Dies ist eine Gemeinschaftsarbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit gehen wir auf das Minimal Pair Paradigm zurück. Das Minimal Pair Paradigm bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch Grammatikalität wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paare umfassen können. Im Minimal Pair Paradigm ist die typische Art und Weise, Sprachmodelle zu bewerten, dass man ein akzeptables oder grammatikalisches Satz und dann ein akzeptables oder ungrammatisches Satz präsentiert. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Die aktuelle MPP-Pipeline ermöglicht es uns nicht, die Akzeptanz eines Modells für längere Sätze zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern. Daher ist es entscheidend, die Akzeptanz der Modelle im gesamten Kontextfenster zu bewerten, und genau das versuchen wir hier. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell bitten, die Akzeptanz für immer längere Sequenzen zu bewerten. Das ist der Ansatz. Was wir tun, ist, dass wir diese längeren Sequenzen simulieren, indem wir die Datensätze selbst überarbeiten und Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar der Grammatikalität aus dem BLiMP-Datensatz aus dem Adjunct Island-Fall ausgewählt. Und was wir tun, ist, dass wir, um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur haben, grammatikalische Sätze aus Adjunct Island extrahieren und diese als Präfix sowohl dem akzeptablen Query-Satz als auch dem inakzeptablen Query-Satz hinzufügen. Wir können das Gleiche tun, indem wir inakzeptable Sätze aus derselben Übereinstimmung auswählen, was auch verwendet werden kann, um die Akzeptanz des Modells zu testen. Und wir können das Gleiche tun, indem wir Sätze aus einem anderen Subset oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze zwar aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem man das Modell bewertet. Und wir können das Gleiche für den Fall der Inakzeptanz tun. Schließlich können wir Sätze aus einem völlig anderen Bereich wie Wikipedia auswählen. Dies zeigt uns, ob die Akzeptanzurteile des Modells tatsächlich durch einen Kontext beeinflusst werden, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder ob er völlig irrelevant für den Satz ist, den wir betrachten. Wie schlägt sich das Modell also? Zuerst betrachten wir die Sätze aus Wikipedia, die völlig irrelevant für das aktuelle Query-Paar sind. Dort stellen wir fest, dass die MPP-Urteile meist robust für beliebige Kontextlängen sind. Wir erhöhen die Kontextlänge auf bis zu 1024, um die OPT- und GPT-2-Modelle maximal auszunutzen. Und wir sehen hier in der orangefarbenen gepunkteten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und inakzeptablen Bereichen aus demselben BLiMP- oder SyntaxGym-Datensatz. Dort stellen wir fest, dass die MPP-Urteile entweder deutlich steigen oder fallen, wenn man entweder akzeptable oder inakzeptable Präfixe hinzufügt. Aber wenn wir die Struktur abgleichen, d. h. wenn wir Sätze aus demselben Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang des MPP-Urteils für das Modell, je nachdem, ob das ausgewählte Präfix akzeptabel oder inakzeptabel ist. Dieses und dieses ist sehr groß, dieser Effekt verstärkt sich mit zunehmender Kontextlänge und würde wahrscheinlich neuere Sprachmodelle mit großen Kontextfenstern beeinflussen. Warum beeinflusst das übereinstimmende Präfix das Urteil des Sprachmodells so stark? Wir haben eine Reihe von Analysen durchgeführt, in denen wir versucht haben, den Eingangsatz zu verändern, indem wir versuchten, die relevante Struktur beizubehalten, aber dem Eingangsfeld Rauschen hinzufügten. Nachdem wir mehrere dieser Veränderungen vorgenommen haben, stellen wir fest, dass keine dieser Veränderungen das Modell dazu bringt, seine MPP-Urteilsausgabe zu ändern. Wir stellen fest, dass die Modelle auf ähnliche Weise auf die veränderten Sätze reagieren. Das heißt, wenn wir die Sätze im akzeptablen Bereich verändern, sehen wir eine ähnliche Erhöhung in allen Veränderungen, und wenn wir die Sätze im inakzeptablen Bereich verändern, sehen wir eine Abnahme der MPP-Urteile auf ähnliche Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über die Sätze hinweg gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Satz-Eingaben durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen des Sprachmodells im gesamten Kontextfenster. Bitte lesen Sie unser Paper für weitere Details zu unseren Experimenten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="160">Einem ungeordneten Multiset von Token, die im Output erscheinen werden.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign.</sample>
    <sample id="164">Schwaches überwachtes Lernen ist kostengünstiger als manuelles Labeln, da es schwache Labelquellen wie heuristische Regeln verwendet.</sample>
    <sample id="165">Wenting Zhao from Cornell University presented their paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," introducing an unsupervised learning method called LiPoR. Abductive reasoning aims to find plausible explanations bridging a context and an outcome. Current approaches often rely on supervised methods, which are hampered by noisy and subjective annotations.

LiPoR addresses this by treating explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, eliminating the need for labeled data. However, this alone doesn't prioritize plausible explanations. To solve this, LiPoR incorporates a regularizer based on the principle of mutual exclusivity – explanations in abductive reasoning are typically mutually exclusive.

The LiPoR objective function combines likelihood maximization with this mutual exclusivity regularizer, which penalizes scenarios where too many explanations receive significant probability mass. The regularizer uses entropy and the number of plausible explanations to achieve this.

Experiments on the AlphaNLI dataset demonstrate LiPoR's superior performance, outperforming zero-shot models and previous unsupervised approaches by over 4 points in accuracy. The paper and code are available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This paper introduces NDCR, a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Addressing the limitations of existing visual language models that struggle with complex descriptions, NDCR draws inspiration from Divide-and-Conquer strategies and Dual-Process Theory. The framework integrates an analogical reasoning "System 1" (Visual-Linguistic Interactor) with a logical reasoning "System 2" (Neural-Symbolic Reasoner). NDCR decomposes complex text into simple propositions using a Proposition Generator, interacts visual and propositional information, and then employs a Neural-Symbolic Reasoner with negation and conjunction operations to derive the final solution. Experimental results demonstrate that NDCR outperforms baselines and ablation studies confirm the effectiveness of each module. The system provides interpretable inference states, showcasing its operational transparency. The authors suggest that neural-symbolic computation and Divide-and-Conquer, combined with Dual-Process Theory, offer promising avenues for enhancing compositional reasoning in large language models.</sample>
    <sample id="167">750 documents were aligned manually and with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters News in 2020 and annotating it with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">David Vilar presented a review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," a joint work with Google Translate colleagues. The paper explores the use of PaLM, a 540 billion-parameter large language model, for machine translation. It's the first systematic study of prompting strategies for LLMs in this context, evaluating PaLM's translation capabilities using established MT practices and comparing it to state-of-the-art systems.

The research highlights the significant impact of prompting on translation performance, with differences in prompts leading to substantial BLEURT score variations. A 5-shot prompting strategy, where sentences are marked with their language, proved effective. The study found that example quality is more crucial than similarity to the source sentence, recommending using high-quality translations, such as those from development data, as examples.

While PaLM demonstrates fluency comparable to state-of-the-art systems, it lags in accuracy, frequently exhibiting omission errors. Despite this, PaLM's output is considered stylistically less awkward than current leading systems. The paper concludes that PaLM is approaching the performance of commercial translation systems, offering valuable insights for prompt selection and future LLM translation development.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" vorstellen. Die semantische Analyse ist eine Aufgabe, bei der semantische Repräsentationen von Benutzerabfragen wie SQL und Lambda Calculus erstellt werden. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir Abfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für Cross-Lingual Semantic Parsing werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es eine große Abdeckung bestimmter natürlicher Sprachen. Aber Chinesisch fehlt und es gibt eine fehlende Abdeckung bestimmter Bedeutungsrepräsentationen. Die Lambda-Berechnung fehlt oder sie werden nur mit bestimmten neuronalen Modellen bewertet. Zum Beispiel gibt es nur ein einzelnes Modell, um sie zu bewerten. Um dies zu erreichen, schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält 9 Datensätze in verschiedenen Bereichen, 5 semantische Analyseaufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu bewerten, betrachten wir sechs Einstellungen für das Training und die Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell für das Training und die Bewertung. Zum Beispiel trainieren wir das englische Modell mit englischen Abfragen und verwenden während der Inferenz die API, um die deutsche Abfrage ins Englische zu übersetzen, und verwenden dann das trainierte Modell, um die SQL-Ausgabe vorherzusagen. Wir testen auch Monolinguale Modelle. In dieser Einstellung sind die Quellsprache und die Zielsprache gleich, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch Monolinguale Few-Shot-Einstellungen, indem wir monolinguale Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen Multilinguale Modelle, die wir mit allen Sprachen trainieren. Zum Beispiel fügen wir deutsche, englische und chinesische Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten auch Cross-Lingual Zero-Shot- und Few-Shot-Transfer. Wir trainieren in einer Quellsprache und übertragen sie in eine andere Sprache. Beim Training trainieren wir es mit englischen Abfragen oder einer Kombination aus englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren, um die SQL-Ausgabe vorherzusagen. Und wir haben auch viele interessante Ergebnisse gefunden. Bezüglich der Analyse monolingualer Modelle bewerten wir auf zwei Gruppen von Modellen, einschließlich Encoder-PTR, was für Multilingual Pretrained Encoders mit Pointer-basierten Decodern steht, wie z. B. XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, die Multilingual Pretrained Encoder-Decoder-Modelle sind, wie z. B. mBART und mT5. Wir haben festgestellt, dass Encoder-Decoder die beste Leistung in allen neun Datensätzen erzielen. Und wir bewerten mT5 und XLM-R + PTR in der Multilingualen Einstellung. Wir haben festgestellt, dass Encoder-Decoder oder Encoder-PTR verbessert werden können, indem sie in einer Mischung aus verschiedenen Sprachen trainiert werden. Wir haben festgestellt, dass dies daran liegt, dass die meisten großen natürlichen Sprachen eine Leistungssteigerung erzielen können, außer dass die englische Leistung in sieben Datensätzen abnimmt und nur in drei Datensätzen eine Steigerung erzielt. Ich denke, dies wird als "Fluch der Multilingualität" bezeichnet. Wir vergleichen auch die Cross-Language-Performance-Lücke. In dieser Abbildung ist die blaue Linie Cross-Lingual Few-Shot-Transfer. Die orangefarbene Linie ist Cross-Lingual Zero-Shot-Transfer. Die grüne Linie ist die Monolinguale Einstellung. Wir haben festgestellt, dass beim Vergleich der grünen und orangefarbenen Linie die Zero-Shot-Einstellung, die Cross-Lingual-Transfer-Performance-Lücke erheblich ist, und beim Vergleich der blauen und orangefarbenen Linien die Transfer-Lücke mit der Few-Shot-Einstellung schnell verkürzt wird. Wir haben auch einige andere interessante Ergebnisse gefunden. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vortrainieren auf natürlicher englischer Sprache kann die Leistung von Few-Shot auf Zielsprachen erheblich steigern, und wir haben festgestellt, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für Cross-Lingual Semantic Parsing-Aufgaben sind. Zusammenfassend haben wir XSemPLR erstellt, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Arten von multilinguallen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Ergebnisse. Besuchen Sie unser Papier und unseren Code. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">Multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">ArgAnalysis35K is a novel dataset for argument quality analysis, distinguished from existing datasets by its scale, diversity, depth, and reliability. It comprises 35,000 argument-analysis pairs, making it the largest in the field. The arguments are primarily sourced from high-quality debates and expert debaters, ensuring higher quality than crowdsourced data.

Unlike datasets limited to a few pre-selected motions, ArgAnalysis35K uses 24 themes identified through expert consultation and debate circuit experience, capturing a wider range of motions. A key innovation is the introduction of "analysis," a concept encompassing claims, premises, and their connections, providing a more comprehensive explanation of an argument's reasoning.

To address annotator bias, the dataset employs instance-based annotator reliability, selectively removing biased judgments rather than entire annotators. Finally, a relevance model assigns scores (0-1) to each argument within each theme, reflecting its applicability to various topics. This allows for a more nuanced understanding of an argument's relevance beyond a single motion.

In essence, ArgAnalysis35K aims to provide a more diverse, reliable, and insightful resource for argument quality analysis, encouraging further research and feedback.</sample>
    <sample id="175">The method addresses the challenge of multiple consistent permutations by inducing the alignment as part of the training and approximating the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation.</sample>
    <sample id="176">Fairness is defined as the performance of language models varying based on the political leaning of news media categories, leading to biased detection of hate speech and misinformation depending on the model's political leaning.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">This research introduces SymbolicToM, a novel inference-time method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, the ability to reason about others' mental states, is traditionally assessed using false-belief questions. While LLMs like ChatGPT struggle with these tasks, SymbolicToM addresses this by employing explicit graphical representations of characters' beliefs, creating graphs like BBob and BBob,Alice to model different perspectives.

SymbolicToM leverages off-the-shelf Natural Language Inference and Open Information Extraction models to compute these graphs and efficiently answer ToM questions by transforming them into factual queries over the graph. Experiments demonstrate significant performance gains across various LLMs (e.g., 65 accuracy points for GPT-3) compared to supervised baselines. Furthermore, SymbolicToM exhibits strong generalization capabilities on newly designed datasets testing story structure and linguistic diversity, outperforming supervised models and enabling even advanced models like GPT-4 to achieve near-perfect scores. This plug-and-play approach offers interpretable reasoning and avoids overfitting, representing a substantial advancement in LLM ToM capabilities.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the under-studied problem of planning with specific constraints (e.g., "make a chocolate cake" vs. "make a cake"). The authors define constrained language planning as generating scripts faithful to both the overall goal and specific constraints. They first evaluate and improve large language models' constrained planning abilities, finding limitations in faithfulness to constraints despite acceptable semantic completeness. To overcome this, they propose an "over-generate-then-filter" method using InstructGPT, leveraging embeddings and keyword matching to select faithful scripts. Recognizing the cost of large language models, they then introduce CoScript, a dataset of 55,000 constrained language planning examples generated using their method and refined through crowd-sourcing.  Experiments demonstrate that smaller models (T5) fine-tuned on CoScript outperform larger models, highlighting the dataset's potential to enable constrained language planning with more efficient models. CoScript is released as a valuable resource for advancing research in this area.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Darstellung von Latina-Frauen mit Wörtern wie "vibrant" und "curvaceous", was auf eine lange Tradition der Objektivierung und Exotisierung dieser Gruppe hinweist.</sample>
    <sample id="183">The authors were inspired by a study that gave the same prompts ("Imagine you are an X. Describe yourself.") to human subjects.</sample>
    <sample id="184">CXMI (Contextual Mutual Information) und seine Erweiterung, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is trained on NACHOS, a dataset of medical data crawled from the web, while ChuBERT is trained on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Zwei.</sample>
    <sample id="188">Iterative transfer learning involves updating the model by training it on the latest set of data collected in each round of active learning.</sample>
    <sample id="189">To understand users’ language when they want to make a choice.</sample>
    <sample id="190">An attacker may steal the model through learning from the embedding.</sample>
    <sample id="191">Drei.</sample>
    <sample id="192">CAME ("Confidence-guided Adaptive Memory Efficient Optimization") is a novel optimizer designed to address the challenge of training large language models efficiently. Traditional adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from slower convergence. CAME aims to bridge this gap by achieving both fast convergence and low memory usage.

The core idea behind CAME is inspired by the inherent errors in Adafactor's non-negative matrix factorization (NMF) approach. These errors lead to unstable training. CAME tackles this by introducing a "confidence-guided" update mechanism. It calculates the residual between predicted and generated updates, treating this difference as an indicator of instability. This instability is then used as a denominator in the update step, adaptively adjusting the optimization process.

Experiments on BERT, GPT-2, and T5 models demonstrate CAME's effectiveness. It outperforms both Adam and Adafactor, achieving significant improvements in validation accuracy while using substantially less memory, especially with large batch sizes (8K to 32K). Furthermore, BERT models trained with CAME exhibit comparable performance on downstream tasks compared to baselines, but with reduced memory costs. CAME also demonstrates a smaller memory footprint compared to other memory-efficient optimizers like SM3. Ultimately, CAME offers a promising solution for training very large language models efficiently and effectively.</sample>
    <sample id="193">Around 1,000 examples of discourse unit pairs were collected.</sample>
    <sample id="194">Carnegie Mellon University und University of Washington sowie Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT, a novel framework called "Reasoning over Hierarchical Question Decomposition Tree" for explainable question answering (XQA). RoHT addresses limitations in existing XQA approaches, namely neuro-symbolic methods' reliance on incomplete knowledge bases (KBs) and decompose-based methods' struggles with the diversity of natural language. RoHT integrates knowledge from both KBs and text corpora to answer complex questions intuitively.

The framework operates in two stages. First, it constructs a Hierarchical Question Decomposition Tree (HQDT) representing the compositional structure of the question, breaking it down into atomic questions. A certainty score is assigned to each node reflecting the confidence in its generation. Second, it performs probabilistic reasoning over the HQDT, recursively traversing from root to leaves. At each node, a scheduler selects appropriate knowledge sources (KB, text, or child nodes), executors retrieve answers with probabilities, and an aggregator combines these answers to produce top key answers.

Evaluations on KQA Pro and Musique datasets demonstrate RoHT's effectiveness. On KQA Pro, RoHT outperforms existing KB QA methods and significantly improves upon results using only an incomplete KB, highlighting the benefit of integrating knowledge from different levels. On Musique, RoHT surpasses state-of-the-art methods, and supplementing text with KB knowledge further enhances performance. The results showcase RoHT's superiority over end-to-end methods and the value of explicit question decomposition.</sample>
    <sample id="196">I saw Bart and Lisa.</sample>
    <sample id="197">The common practice is to use human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="198">Because large language models are now using longer and longer context windows.</sample>
    <sample id="199">Ja, das mehrsprachige Training führte in sieben von neun Datensätzen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell.</sample>
    <sample id="200">Annotators know the names of the entities but not necessarily details about them.</sample>
    <sample id="201">State-of-the-art, neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">No.</sample>
    <sample id="203">Studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented, and it's challenging to characterize how these positionalities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="204">Die Arbeit fand heraus, dass mehrsprachige LLMs wie Codex und BLOOM für Cross-Lingual Semantic Parsing-Aufgaben immer noch unzureichend sind.</sample>
    <sample id="205">This presentation explores the propagation of political biases from pretraining data to language models and their downstream applications, highlighting potential fairness issues. The research investigates how language models acquire and exhibit political leanings, and how these biases impact performance on tasks like hate speech and fake news detection.

The study found that language models, including GPT-4, demonstrate varying political leanings across a political spectrum. Further pretraining on partisan corpora (news and social media) significantly shifts these leanings, and models trained on data after 2017 exhibit increased polarization.

Crucially, the research reveals that language models with different political leanings perform differently on downstream tasks. Left-leaning models are better at detecting hate speech against minority groups but worse at detecting it against dominant groups, and vice versa for right-leaning models. Similar biases are observed in fake news detection.

The presentation concludes by emphasizing a critical dilemma: removing political opinions from training data risks censorship, while retaining them leads to bias propagation and fairness concerns. The research calls for acknowledging and addressing these issues to prevent marginalization and ensure equitable NLP applications.</sample>
    <sample id="206">Das Modell, das für den Start des Active Learnings verwendet wird, ist das, das durch die Feinabstimmung der CE-Aufgabe (Expansion und Comparison Klassen von PDTB) gefolgt von einer weiteren Feinabstimmung auf der Debate-Aufgabe erzielt wurde.</sample>
    <sample id="207">The latest test sets were used to avoid overlap of the test data with the language model's training data.</sample>
    <sample id="208">Drei.</sample>
    <sample id="209">The method significantly improves planning ability in both semantic completeness and faithfulness to constraints, allowing T5 fine-tuned on CoScript to generate higher-quality scripts than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset can be used as a benchmark for automatic text simplification.</sample>
    <sample id="212">One T5 model.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk explores the dependency structure of coordination, highlighting differing theoretical approaches: asymmetric (Universal Dependencies, Mel'čuk's Meaning-Text Theory) where the first conjunct is the head, conjunction-headed (Prague Dependency Treebanks), and multi-headed (Hudson's Word Grammar).

The core argument presented favors symmetric coordination structures over asymmetric ones, grounded in the principle of dependency length minimization. This principle suggests shorter dependencies are preferred, similar to how direct objects ideally reside close to the verb. Przepiórkowski demonstrates how this principle can explain seemingly unusual sentence structures where elements are reordered to reduce dependency length.

Analysis of the Penn Treebank revealed a consistent tendency: the left conjunct in a coordination is typically shorter, especially when the governor (the word governing the coordination) is on the left or absent. However, this preference vanishes when the governor is on the right. This observation provides empirical support for symmetric coordination structures and challenges asymmetric models. The study measured length in characters, syllables, and words, consistently showing the left conjunct preference with left-side governors or no governor, but not with right-side governors.</sample>
    <sample id="217">"Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" introduces a novel approach to controllable dialogue generation, addressing limitations in existing methods that primarily focus on single attributes or rely on extensive labeled data. The work, by Zeng, Zhao, and He from Beijing University of Posts and Telecommunications, proposes DCG, a Disentangled Controllable Generation model, built upon DialoGPT.

DCG utilizes attribute-oriented and task-oriented prompts, combined with disentanglement learning, to enhance generation capabilities and compositional generalization. A key contribution is the introduction of MAE, a unified, reference-free evaluation framework for assessing controllability across various attribute granularities. Two benchmarks are established to validate the method's effectiveness.

Experiments on DailyDialog-CG demonstrate that DCG outperforms baselines in attribute controllability and text quality. The model effectively leverages prompts to focus on controllable information and improve text fluency. MAE exhibits strong correlation with human judgments, proving its reliability. Visualization confirms the model's ability to disentangle attribute combinations and generalize from seen to unseen attribute values. The research highlights the potential of prompt-based methods and disentanglement learning for advancing multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google</sample>
    <sample id="219">Jia-Huei Ju presented research on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports," conducted with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang. The work addresses the challenge of extracting useful information from Form 10-K reports (annual reports required by the SEC), which traditionally requires significant human effort.

The research was motivated by the observation that financial reports exhibit high text similarity year-over-year (around 80% token overlap). To tackle this, they introduced a "highlighting task" that compares and contrasts a target report with its preceding year's report (reference). The goal is to identify key words (rationale) explaining the relationship between the two.

The proposed pipeline involves document segmentation, relation recognition (classifying pairs into β, revised, and mismatched categories), and a two-stage fine-tuning process. First, out-of-domain fine-tuning is performed using the eSNLI dataset. Then, in-domain fine-tuning utilizes revised pairs with pseudo-positive labels and incorporates soft labeling techniques to address label quality issues.

Evaluation on the FINAL dataset and eSNLI demonstrated strong performance, with the model effectively identifying important words and generalizing well. The method particularly benefited from mismatched pairs, which were not used during training. Future work includes exploring improvements and incorporating information retrieval techniques.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">German to English</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates how to effectively transfer knowledge from general-purpose question answering models (trained on Wikipedia) to specialized domains like biomedicine. The core challenge arises when a model trained on one domain struggles with another due to differences in vocabulary and reasoning patterns.

The research explores two main data intervention methods: few-shot and zero-shot adaptation. Few-shot leverages a small number of target domain examples to generate additional training data using large language models. Zero-shot techniques manipulate the question, answer, and context distributions to understand their impact on model learning. Notably, cloze-style questions proved easier to curate and surprisingly effective.

The study also identifies the type of dataset shift (No shift, Concept shift, Covariate shift, Full shift) present in different target domains. Compatibility is measured by assessing the likelihood assigned to contexts (retriever) and answers (reader) by the source model. The findings reveal that few-shot adaptation generally benefits all target datasets, while zero-shot methods are particularly useful for concept and covariate shifts. Ultimately, the research demonstrates improvements in reader performance (up to 24%) by strategically applying data interventions based on the nature of the dataset shift.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART und base mBART.</sample>
    <sample id="225">53 werden für das Training und 9 für die Tests verwendet.</sample>
    <sample id="226">Two.</sample>
    <sample id="227">Current language models excel at general NLP tasks but lack grounded language understanding – the ability to map natural language to executable plans within a specific environment. This is challenging due to a lack of grounding during pre-training, creating a gap between pre-training and real-world application. Existing approaches often rely on language models to directly generate plans, which can result in invalid or ungrammatical outputs.

To address this, the authors introduce "Pangu," a novel framework that shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to propose candidate plans, while a language model (like BERT, T5, or Codex) scores and ranks these candidates. This approach alleviates the language model's burden of ensuring plan validity and grammar.

The framework was tested on knowledge-based question answering, demonstrating outstanding performance across fine-tuning and in-context learning settings, including remarkable sample efficiency. Notably, Pangu achieved over 50% accuracy with Codex using only one demonstration example. The authors observed that Pangu exhibits strong generalizability, particularly in non-i.i.d. settings, because autoregressive models tend to overfit seen structures, whereas Pangu maintains consistent probability distributions across both seen and unseen structures.

The core takeaway is that for grounded language understanding, discrimination is a more effective strategy than generation when leveraging language models. The authors welcome further discussion and collaboration.</sample>
    <sample id="228">AG News, MIND, SST2 and Enron Spam.</sample>
    <sample id="229">This presentation by Gabriella Skitalinskaya and Henning Wachsmuth focuses on detecting improvable claims in argumentative writing, a crucial aspect of effective communication. The core problem is determining when an argumentative claim is phrased optimally and doesn't require further revision. They introduce two tasks: Suboptimal-Claim detection (identifying claims needing revision) and Claim Improvement Suggestion (identifying quality issues).

The research leverages revision-based data from online debate platforms like Kialo, where revision histories reveal suboptimal (red) and optimal (green) claim versions. However, this approach presents challenges. Firstly, ensuring the representativeness and reliability of the dataset is key – is a final version truly optimal? Secondly, selecting an appropriate model architecture sensitive to subtle revisions is vital. Thirdly, considering contextual information (debate-wide conventions, parent claim relationships, domain knowledge) is complex. Finally, addressing topical and user biases within collaborative revision histories is necessary.

The paper details their analysis of these challenges and a comparison of different approaches. Their findings suggest that revision-based data can effectively be used for these tasks, modeling the distance between claim versions aids in suboptimal claim detection, and contextual information significantly impacts quality assessment depending on the task and specific issues. The full analysis and findings are available in their paper.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Simultaneous speech translation (SimulST) aims to translate spoken language into text in real-time, but current models face challenges like complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency levels. This paper introduces EDAtt (Encoder-Decoder Attention), a novel strategy that leverages existing offline speech translation models without retraining or architectural modifications. EDAtt dynamically decides when to emit partial translations based on the cross-attention mechanism between audio and text. Specifically, a word is emitted only if its attention weights are not concentrated on recent speech frames, indicating sufficient stability in the received information. Experiments on German demonstrate that EDAtt outperforms existing strategies applied to offline models, achieving higher translation quality (BLEU score) with lower latency, both in terms of average and computationally-aware lagging. The code, models, and simultaneous output are publicly available to promote reproducibility.</sample>
    <sample id="234">The prompting strategy has a big influence on the performance of LLMs for translation. A simple experiment showed differences of more than one BLEURT point, and up to 40 BLEURT points in extreme cases, depending on the prompt used. The quality of the examples is more important than their similarity to the source sentence.</sample>
    <sample id="235">Carnegie Mellon University</sample>
    <sample id="236">Five expert-written instructions.</sample>
    <sample id="237">They propose a diagnostic test suite called KITMUS, featuring a coreference resolution task that varies the availability of entity-specific and background knowledge across different settings.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset created by the University of Central Florida for developing meeting summarization technologies. It addresses the need for datasets tailored to specific reading domains, particularly the challenges of obtaining high-quality summaries and trustworthy public meeting resources.

The dataset comprises 1,366 City Council meetings from various cities (Boston, Seattle, Denver) and nearly 7,000 instances, including meeting transcripts, reference summaries, and URLs. Data collection involved using Speechmatics API for transcription, identifying meeting details, locating reference summaries, and aligning timestamps.

Analysis reveals that City Council meeting summaries often include verbatim points rather than abstractive summaries, with Seattle and Boston exhibiting the highest density scores. Evaluations using both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet) summarization systems, alongside GPT-3, showed DialogLM performing best with ROUGE-2 scores.

Human evaluation, while revealing GPT-3's strength in fluency and coherence, highlighted its weaknesses in informativeness and factuality. The study concludes that future meeting summarization solutions should prioritize capturing key discussion points and that new evaluation metrics are needed to better reflect human preferences. MeetingBank is released to the public for research and further development.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Rezension des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine Gemeinschaftsarbeit mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das im letzten Jahr, 2022, vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es Spitzenleistungen in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zum Prompting von großen Sprachmodellen für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle unter Verwendung der Best Practices der MT-Community bewertet. Dies beinhaltet die Verwendung der neuesten Testdatensätze, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir haben sie mit modernsten Systemen verglichen, also dem leistungsstärksten System, dem WMT-Evaluierungssystem. Wir verwenden modernste neuronale MT-Metriken und zeigen zusätzlich auch Ergebnisse der menschlichen Bewertung durch Experten. Abschließend geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwendeten und für jeden Satz zwei verschiedene Prompts bereitstellten. Bei den meisten Sätzen, 516 von 1.000, wurde ein Unterschied von mehr als einem BLEURT-Punkt festgestellt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten haben wir uns für eine 5-Shot-Prompting-Strategie entschieden, bei der wir jeden Satz, den wir dem System bereitstellen, einfach mit der Sprache kennzeichnen. In diesem Beispiel hier, wo wir eine Übersetzung vom Deutschen ins Englische durchführen, sind die deutschen, die Quellsätze, mit "Deutsch:" gekennzeichnet und die englischen Übersetzungen mit "Englisch:". Wir haben festgestellt, dass die tatsächliche Form des Promptings keinen großen Einfluss hat, wenn wir mehrere kurze Prompts verwenden. Es ist entscheidend für Zero- und One-Shot-Prompting. Und wenn wir wie in unserem Fall zu einem 5-Shot-Prompting übergehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Promptings. Die Beispiele tragen den größten Stellenwert. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quellsatz. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Bewertungen auf den Dev-Daten. Die Dev-Daten sind viel besser kuratiert und von höherer Qualität als die Trainingsdaten, die verrauschter sind. Ihre Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Dev-Daten. Dennoch haben spezialisierte, modernste Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. PaLM kommt aber ziemlich nahe an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Framework durchgeführt haben, zeigten, dass die Flüssigkeit von PaLM mit modernsten Systemen vergleichbar ist, aber der Hauptunterschied liegt in der Genauigkeit. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint also, dass PaLM eine besser klingende Übersetzung erstellen möchte, manchmal indem es Teile des Quellsatzes weglässt, die in der Übersetzung enthalten sind. Die Kategorie "Stil/Holprig" für PaLM ist jedoch niedriger als für die modernsten Systeme, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige Ausgaben liefert, aber dennoch mit Genauigkeitsproblemen zu kämpfen hat. Und das war's für diesen wirklich kurzen Überblick. Für weitere Details besuchen Sie bitte die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität Saarland in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine Gemeinschaftsarbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwache Supervision und schwaches überwachtes Lernen beginnen. Bei schwacher Supervision werden die Daten nicht manuell beschriftet. Stattdessen beschriften wir die Daten mit schwachen Beschriftungsquellen, wie z. B. einfachen Heuristiken, Wissensdatenbanken oder minderwertiger Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen zwar viel billiger, aber auch verrauscht, was bedeutet, dass ein gewisser Teil der Annotationen fehlerhaft ist. Wenn wir neuronale Netze direkt mit schwach beschrifteten Daten trainieren, neigen die neuronalen Netze dazu, das Beschriftungsrauschen zu merken und generalisieren nicht. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Beschriftungsrauschen zu trainieren, so dass die trainierten Modelle dennoch gut generalisieren. In den letzten Arbeiten im WSL, wobei WSL für Weakly Supervised Learning steht, wird oft behauptet, dass Menschen nur mit den schwach beschrifteten Daten trainieren und hohe Leistung auf sauberen Testdatensätzen erzielen. Diese Behauptung ist technisch gesehen nicht falsch, aber es gibt einen Haken: Man geht davon aus, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellauswahl verfügbar ist. Wir können nicht bei diesem Problemsetting aufhören, aber dies impliziert, dass zusätzliche manuelle Annotationen für das schwach überwachte Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Skepsis führt zu drei Forschungsfragen. Erstens, ist ein sauberer Validierungsdatensatz für WSL erforderlich oder können wir stattdessen einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn saubere Daten erforderlich sind, wie viele saubere Samples benötigen wir dann? Und schließlich, sollten wir die sauberen Samples nur für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit behandelt und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass aktuelle WSL-Methoden tatsächlich saubere Validierungs-Samples benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen deutlichen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle ohne saubere Validierungs-Samples nicht über die ursprünglichen schwachen Labels hinaus generalisieren, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotationskosten für die Erlangung sauberer Validierungs-Samples sollten nicht übersehen werden. Unser zweites Ergebnis ist, dass die Erhöhung der Anzahl sauberer Validierungs-Samples dazu beiträgt, dass WSL-Ansätze eine bessere Leistung erzielen, wie in der linken Abbildung gezeigt. Typischerweise benötigen wir nur 20 Samples pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir uns auf jeden Fall saubere Samples beschaffen, erzielen das direkte Training auf diesen sogar eine bessere Leistung. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Fine-Tuning-Ansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie man sieht, beginnt das Fine-Tuning mit 10 Samples pro Klasse, WSL-Ansätze zu übertreffen. Schließlich können die in früheren WSL-Ansätzen beanspruchten Leistungsverbesserungen leicht durch die Möglichkeit erreicht werden, die sauberen Validierungs-Samples kontinuierlich zu verfeinern. Wie aus den Abbildungen ersichtlich, unterschritt das Vanilla-Modell, das als FTw bezeichnet wird, anfänglich die Leistung komplexerer WSL-Methoden wie COSINE. Wenn wir jedoch die kontinuierliche Verfeinerung auf den sauberen Samples zulassen, erzielt FTw die gleiche Leistung wie andere Methoden. Daher gibt es in der Praxis keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Samples benötigen, damit sie ordnungsgemäß funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Berichten Sie zunächst über die Kriterien für die Modellauswahl. Berichten Sie beispielsweise, ob die Modellauswahl über saubere Validierungs-Samples erfolgt ist. Zweitens sollten WSL-Ansätze mit Few-Shot-Learning-Baselines verglichen werden, da beide mit sauberen Samples arbeiten. Drittens sollte ein kontinuierliches Fine-Tuning als einfacher, aber starker Baseline in zukünftiger WSL-Arbeit berücksichtigt werden. Schließlich haben wir unseren Code Open Source gestellt. Sie finden ihn über den QR-Code auf dieser Folie. Zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">This paper introduces a human-in-the-loop evaluation framework for early misinformation detection, addressing shortcomings in existing automated approaches. Current systems often rely on unrealistic datasets and exclude human input, hindering their effectiveness in real-world scenarios. The proposed framework emphasizes end-to-end systems that integrate human feedback throughout the process, rather than treating humans as a final decision-maker.

The authors present a case study focused on COVID-19 treatment misinformation, detailing a system with two components: claim detection and policy violation verification. The claim detection component uses keyword filtering and a T5 model to extract and rank potential misinformation claims based on trendiness. The policy verification component employs a BERT-based stance classification model to flag tweets supporting unapproved treatments for human review.

Evaluation demonstrates the system's ability to detect unapproved treatments *before* they are debunked in news articles, highlighting its value for early intervention. The system achieves 65% accuracy in policy violation detection and enables the confirmation of 124.2 policy violations per human hour, showcasing its efficiency. The framework provides a more realistic assessment of human-system interaction in misinformation detection and aims to inspire future research in this area.</sample>
    <sample id="242">Human evaluation, such as asking judges to select which conversation is better or rate conversations on a Likert scale.</sample>
    <sample id="243">Five.</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This work, "A Needle in a Haystack," presents a two-step pipeline for identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks, addressing issues with automatic metrics and unclear best practices for MTurk recruitment. The pipeline first assesses workers' ability to evaluate summaries across six dimensions through a "Qualification Task," categorizing them as gold, silver, bronze, or blocked. Only gold and silver workers (13% of 200) proceed. The second stage, an "Endurance Task," evaluates their capacity for handling workload, resulting in 6% of initial participants (12 workers) qualifying as gold or silver. These workers demonstrated high inter-annotator agreement (IAA), exceeding expert levels with a Krippendorff's Alpha of 0.443.

A "Reference-Based Task" further tested performance, achieving a Krippendorff's Alpha of 0.534. Comparisons with baseline MTurk workers (using MACE) and CloudResearch workers showed the pipeline achieved comparable quality (Alpha of 0.513 for CloudResearch) at a lower cost, though CloudResearch had a lower task acceptance rate. Analysis revealed a significant correlation between pipeline and CloudResearch workers, and a strong correlation between GPT models and expert judgments.

The pipeline effectively filters out low-quality workers, saving time and resources while maintaining high agreement. Future work will focus on improving worker quality in both agreement and correctness, exploring diverse applications and platforms. Limitations include the focus on English summarization on MTurk and the lack of guaranteed correctness training.</sample>
    <sample id="246">Yes, on GitHub.</sample>
    <sample id="247">FACTKG introduces a new task and dataset for Knowledge Graph-Based Fact Verification, addressing the lack of datasets utilizing knowledge graphs as evidence with natural language claims. The paper proposes FactKG, leveraging DBpedia as the knowledge graph and featuring claims in both written and colloquial styles. Claims are labeled as SUPPORTED or REFUTED, requiring evidence retrieval and verification.

The dataset incorporates five reasoning types: one-hop (single triple connection), conjunction (multiple one-hop claims), existence (entity connected to a specific relation), multi-hop (requiring inference across multiple entities), and negation (requiring additional verification). Colloquial claims are generated using a style transfer model and presupposition templates.

Experiments demonstrate that utilizing graph evidence significantly improves fact verification performance compared to claim-only baselines. The GEAR model, specifically designed for graph-based verification, outperforms all other baselines, highlighting the value of knowledge graph reasoning. The dataset and code are publicly available, encouraging further research in this area. The work has practical applications in dialogue systems and any task requiring consistency checks between knowledge graphs and natural language.</sample>
    <sample id="248">The annotators for NLPositionality came from over 1000 annotators from 87 countries, amassing over 16,000 annotations.</sample>
    <sample id="249">The input sentence was perturbed by adding noise while attempting to preserve the relevant structure.</sample>
    <sample id="250">A dimensional evaluation involves assessing multiple aspects or dimensions of chat quality to understand a model's strengths and weaknesses in more detail.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">U-CREAT is a novel unsupervised pipeline for Prior Case Retrieval (PCR), a task crucial for legal professionals facing increasing case volumes. The work introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline itself.

The IL-PCR dataset, a new benchmark for PCR tasks, comprises 7,070 Indian legal cases, significantly larger and more complex than existing datasets like COLIEE’21. U-CREAT leverages an event-based approach, extracting events (subject-verb-object triplets) from legal documents using dependency parsing. These events are then used to construct an interaction matrix, enabling efficient retrieval and ranking of relevant cases.

Experiments demonstrate that U-CREAT, particularly the Event Filtered Documents model, significantly outperforms baseline methods and even state-of-the-art supervised approaches like MTFT-BERT on the COLIEE dataset. Notably, U-CREAT achieves this without law-specific tuning and exhibits low inference times. While transformer-based models initially showed promise, tailored legal transformers (InCaseLawBERT and InLegalBERT) underperformed compared to simpler methods. The results highlight the effectiveness of event-based approaches for PCR and pave the way for future advancements in legal information retrieval.</sample>
    <sample id="253">DisorBERT is a novel double domain adaptation model developed by researchers from Mexico and Spain to detect signs of mental disorders in social media posts. The model leverages BERT, a language model, and adapts it to both social media language (Reddit) and the specific domain of mental health. This adaptation aims to improve performance when annotated data is limited, a common challenge in mental health research.

The core innovation lies in "guided masking," which encourages the model to focus on key words during training. The results, evaluated on the eRisk dataset, demonstrate a strong balance between precision and recall, outperforming existing methods like MentalBERT. Analysis of the model's predictions reveals that DisorBERT generates words with a more negative or psychological orientation compared to standard BERT, particularly when prompted with sentences from the Beck Depression Inventory (BDI).

Visualization of attention scores on user posts highlights the model's ability to identify relevant keywords and phrases associated with mental health conditions, such as "anxious" and "medication." The research concludes that the combination of double domain adaptation and guided masking effectively captures signs of mental disorders in social media, offering potential for early detection and support. Future work will explore different lexical resources and incorporate clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">This research introduces a novel framework, "Uncertainty Guided Label Denoising," to enhance document-level distant relation extraction (DocRE). Traditional DocRE methods rely on extensive human annotation, while recent approaches utilize distantly supervised (DS) data, which often contains noise. Existing methods employing pseudo-labels risk amplifying this noise.

The proposed framework addresses this by first training a DocRE model with both DS and human-annotated data to generate pseudo-labels. It then introduces uncertainty estimation, leveraging Monte Carlo dropout, to assess the reliability of model predictions. A key innovation is an instance-level uncertainty estimation method designed to handle overlapping relations, a common challenge where multiple relations exist between entities.

To further refine the process, dynamic class uncertainty thresholds are implemented, filtering pseudo-labels with high uncertainty based on class frequency. A multi-phase training strategy iteratively re-labels DS data, maximizing its utility. Experiments on public datasets demonstrate significant performance improvements compared to existing baselines, highlighting the framework's effectiveness in mitigating noise and boosting DocRE accuracy. The contributions include the uncertainty-guided framework, instance-level uncertainty estimation, dynamic thresholds, and improved performance.</sample>
    <sample id="255">It's crucial for zero- and one-shot prompting.</sample>
    <sample id="257">Four state-of-the-art chat models.</sample>
    <sample id="258">Chiang Cheng-Han introduces their work, "Can Large Language Models Be an Alternative to Human Evaluation?" which explores using LLMs to assess text quality in NLP, aiming to replace the instability and lack of reproducibility of human evaluation.

The core idea is to provide LLMs with instructions and text samples, prompting them to generate ratings. While LLM-based evaluation isn't entirely new (G-Eval exists), the authors argue it was novel at the time of their ACL submission. Their experiment involved having LLMs (T0, InstructGPT, and ChatGPT) rate stories generated by GPT-2 or written by humans across four attributes: grammar, coherence, likability, and relevance.

To validate the LLM ratings, they were compared against ground-truth ratings obtained from English teachers, who served as expert human evaluators using the same instructions and stories. The results showed that human teachers preferred human-written stories over GPT-2 generated ones. Notably, Davinci and ChatGPT demonstrated a clear preference for human-written text, mirroring the human evaluators' judgments.

The paper addresses further questions regarding agreement between LLMs and humans, the impact of instruction wording, sampling methods, and the cost-benefit analysis of LLM evaluation versus human evaluation, as well as exploring its application to other NLP tasks. The authors encourage readers to consult their paper or visit their poster at ACL for more details.</sample>
    <sample id="259">XSemPLR is a new benchmark dataset for cross-lingual semantic parsing, aiming to translate queries in multiple natural languages into various meaning representations like SQL and Lambda Calculus. Existing models often lack broad language coverage or support for specific representations. XSemPLR addresses this by providing a unified dataset with 9 datasets, 5 tasks, 8 representations, and 22 languages across 15 language families.

The benchmark evaluates models across six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.  Experiments using Encoder-Decoder and Encoder-PTR models reveal that Encoder-Decoder architectures generally perform best. Training multilingual models with a mix of languages improves performance across most languages, though English sometimes experiences a decline ("Curse of Multilinguality").

The study highlights a significant performance gap in zero-shot cross-lingual transfer, which is substantially reduced with few-shot learning.  Furthermore, pretraining on English data significantly boosts performance in few-shot settings for other languages. The research concludes that current large multilingual models like Codex and BLOOM still have limitations for cross-lingual semantic parsing. The paper and code are publicly available.</sample>
    <sample id="260">Jingwei Yi</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">Der Text nennt nur Siyu Yuan als Autor.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to label biases. Existing research highlights design choices as sources of these biases, but lacks a systematic framework for categorization and mitigation. This paper introduces a typology of label biases, identifying three key types: vanilla-label bias (model's inherent preference), context-label bias (influence of the context examples), and a novel *domain-label bias* (impact of the task corpus).

The research demonstrates that exposure to random in-domain words can significantly bias LLM predictions, even without specific examples. They propose a "domain-context calibration" method to mitigate these biases. This method uses random in-domain words as content-free text to estimate and correct label biases, improving upon previous calibration techniques that relied on single, predefined tokens.

Experiments across diverse datasets and models (including GPT-3) show that domain-context calibration significantly improves ICL performance, particularly on tasks with high domain-label bias. Analysis reveals better decision boundaries after calibration. The study concludes that using random in-domain words is superior to single tokens or random English words for effective bias mitigation, ultimately enhancing the reliability and performance of in-context learning.</sample>
    <sample id="264">Lin Wang presented "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenge of multimodal text generation where data annotation is costly and performance degrades across different domains. TAVT introduces a novel task focused on transferring knowledge between audio-visual contexts.

The core idea is to leverage a unified audio semantic space, recognizing that while visual content varies significantly (style, angle), audio content (rhythm, energy) remains relatively consistent in conveying event understanding. The framework comprises three key components: an audio-visual meta-mapper network to align visual concepts into a unified audio space using learnable visual prefixes, a transformer-based encoder-generator with modality-aware attention, and Dual Counterfactual Contrastive Learning (DCLL) for direct visual-textual alignment.

The meta-mapper network utilizes audio clusters from Flickr to create this unified space. DCLL avoids reliance on random negative samples by constructing fine-grained supervision signals from counterfactual results. The model is trained using a meta-learning approach (similar to MAML), adapting quickly to new domains with limited labeled data.

Experiments on MSVD and MSR-VTT benchmarks (cross-dataset and cross-domain settings) demonstrate TAVT's superior performance compared to existing state-of-the-art models, particularly in low-resource scenarios. Ablation studies further validate the importance of audio features in enhancing performance.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">The authors are from the University of Warsaw.</sample>
    <sample id="268">Omissions errors.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir Ihnen alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von Konversations-KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie gut es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlicher Bewertung, beispielsweise durch die Bitte von menschlichen Bewertern, zu wählen, welche von zwei Gesprächen besser ist, oder durch die Bewertung von Gesprächen anhand einer Likert-Skala. Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der Gesamtqualität des Dialogs zu liefern, aber die Qualität des Dialogs hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Ein Ansatz ist es, menschliche Bewerter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, beispielsweise die Relevanz der Modellantworten mithilfe bestehender Vergleichs- oder Likert-Skalen-Methoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem er explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, z. B. das Antworten mit irrelevanten Informationen oder das Widersprechen an sich selbst. Wir nennen diesen Ansatz die Annotation von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um Chat-Modell-Verhaltensweisen umfassend abzudecken, die in der jüngsten Literatur als wirksam für die Chat-Qualität befunden wurden. ABC-Eval kann die Raten messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Chat-Modell seinen Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder seinen Gesprächspartner widerspricht, falsche Fakten halluziniert oder gesundem Menschenverstand widerspricht und wann das Modell Empathie zeigt oder nicht. Um herauszufinden, welche Art von Bewertung am effektivsten ist, haben wir vier hochmoderne Chat-Modelle ausgewählt und sie anhand von 100 menschlich-Bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Rundenebene, Likert-Bewertungen auf Dialogebene und Dialogebenen-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu den acht am häufigsten gemessenen Aspekten des Dialogs erhoben, da dies die Standardpraxis für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse der Bewertungsergebnisse stellten wir fest, dass ABC-Eval-Verhaltensbeschriftungen insgesamt zuverlässiger sind als Beschriftungen, die mit bestehenden Methoden erhoben wurden, gemessen am Inter-Annotator-Agreement auf 100 doppelt beschrifteten Gesprächen. Darüber hinaus sind ABC-Eval-Beschriftungen prädiktiver für die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die durch bestehende Methoden erzeugt werden, wie diese einfache lineare Regression zeigt. Sie können sehen, wie die Messung des Anteils von Runden mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmessung einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression durchführten. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken einzeln entfernen, geht dabei ein beträchtlicher Teil der Informationen über die Qualität verloren. Andererseits erklärt die Kombination aller turnbasierten Likert-Metriken deutlich weniger der Qualität, und weniger dieser Metriken enthalten einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, Konversations-KI mit einer höheren Auflösung zu bewerten, als dies bisherige Methoden ermöglichten. Sie können dies an den Ergebnissen unseres Experiments sehen, dass noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20 % ihrer Antworten Verletzungen des gesunden Menschenverstands. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen und widersprechen sich selbst oder ihrem Gesprächspartner in etwa 10 % der Zeit. Mit dem rasanten Fortschritt in diesem Bereich könnten viele dieser Fehlerraten mit neuen Modellen sinken, die seit unserer Bewertung veröffentlicht wurden. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmesswerte für den Vergleich von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich genutzt werden kann, als einen sinnvollen Schritt in diese Richtung. Und wir freuen uns darauf zu sehen, wie sich die Konversations-KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Seven.</sample>
    <sample id="273">Guten Tag, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel "Wann erfordert Übersetzung Kontext? Eine datengetriebene, mehrsprachige Untersuchung" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig erstellt.

Im Wesentlichen hängt eine Menge von Übersetzungen vom Kontext ab. Nehmen wir zum Beispiel das Wort "mole". Wie würden wir es übersetzen? Wenn der vorherige Satz lautet: "Die Dinge könnten gefährlich werden, wenn die Minister es herausfinden", dann bezieht sich "mole" auf einen Spion. Wenn der vorherige Satz jedoch lautet: "Könnte es etwas Ernstes sein, Doktor?", dann bezieht sich "mole" auf einen Muttermal. Je nach Kontext ändert sich die Bedeutung des Wortes, und damit auch seine Übersetzung.

Die Bewertung, wie gut Modelle diese Fälle übersetzen können, ist jedoch schwierig. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, was es Corpus-Metriken wie BLEU erschwert, diese Übersetzungen zu erfassen. Einige haben zwar gezielte Bewertungen für kontextabhängige Übersetzungen vorgeschlagen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachgruppen, da sie in der Regel auf Fachwissen und menschliche Kuratierung angewiesen sind.

In dieser Arbeit wollen wir zwei Fragen beantworten: Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut gehen Modelle damit um?

Um die erste Frage zu beantworten, haben wir zunächst gemessen, wie stark ein Wort während der Übersetzung vom Kontext abhängt. In unserer vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht durch Messung, wie viele Informationen der Kontext C über die Zielsprache Y gibt, gegeben die Ausgangssprache X. Man kann CXMI als die Informationen betrachten, die man erhält, wenn man dem Modell Kontext gibt. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder Wortebene messen kann. Wir können Wörter mit einem hohen P-CXMI als solche betrachten, die für die Übersetzung Kontext benötigen.

Anschließend analysieren wir Wörter mit einem hohen P-CXMI, um nach Mustern zwischen diesen Wörtern zu suchen. Wir führen unsere Analyse anhand von Transkripten von TED-Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch.

Erstens betrachten wir Wortarten, die einen hohen mittleren P-CXMI aufweisen. Dies ermöglicht es uns beispielsweise, Dualpronomen im Arabischen zu finden, die einen relativ hohen P-CXMI aufweisen. Dies lässt sich erklären, da Englisch keine Dualpronomen hat, sodass man beim Übersetzen ins Arabische Kontext benötigt, um festzustellen, ob ein Pronomen dual ist. Ähnlich finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die passende Verbform auswählen wollen.

Zweitens betrachten wir Vokabeln, die über alle ihre verschiedenen Vorkommnisse hinweg einen hohen P-CXMI aufweisen. Dies hilft uns, Fälle wie den oben genannten zu identifizieren, bei dem man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Ähnlich finden wir, dass Kontext wichtig ist, um in der richtigen Formalität zu übersetzen.

Drittens betrachten wir verschiedene einzelne Token, die einen hohen P-CXMI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern eher in der Satzstruktur zum Ausdruck kommen, wie z. B. Ellipsenauflösung.

Nun nutzen wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu entwerfen. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir haben unseren Tagger Multilingual Discourse-Aware oder MuDA Tagger genannt. Wir können dann feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben.

Anschließend verwenden wir den MuDA-Tagger, indem wir den Tagger auf ein paralleles Korpus anwenden, das wir für die Bewertung verwenden möchten, und wir wenden unsere Übersetzungsmetriken der Wahl auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der maschinellen Übersetzung auf Dokumentenebene zu bewerten.

Wenn wir corpusbasierte Metriken verwenden, stellen wir fest, dass kontextunabhängige Modelle die beste Leistung bei BLEU erzielen. Wenn wir jedoch COMET verwenden, erzielen kontextbewusste Modelle die besten Ergebnisse. Wenn wir Word f-measure verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste System für die Übersetzung auf Dokumentenebene zu bestimmen, wenn wir nur corpusbasierte Metriken verwenden.

Wenn wir den MuDA-Benchmark verwenden, stellen wir fest, dass kontextbewusste Modelle für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion deutlich genauer sind als Modelle, die keinen Kontext verwenden. Diese Modelle sind jedoch bei anderen Phänomenen wie Pronomen und Verbformen nicht wesentlich besser als Modelle, die keinen Kontext verwenden. Dies deutet darauf hin, wo wir Fortschritte bei der Übersetzung auf Dokumentenebene erzielen müssen.

Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate für die Übersetzung auf Dokumentenebene.

Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden dann unsere Ergebnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentenebene zu erstellen, der uns helfen kann, zu identifizieren, welche Diskursphänomene Modelle gut oder nicht gut handhaben können, und welche Übersetzungssysteme gut für die Übersetzung auf Dokumentenebene sind.

Vielen Dank für Ihre Aufmerksamkeit. Wir sehen uns in Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">The "IndicMT Eval" dataset addresses the understudied evaluation of machine translation metrics for Indian languages. The work focuses on five languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati) and creates a dataset of 7,000 samples with 1,400 candidate translations per language generated by seven different translation models.

Bilingual expert annotators meticulously evaluated these translations, marking errors with types and severity based on the MQM framework, and providing overall scores. The dataset reveals that newer models like Indic Trans and NLLB outperform older ones. Analysis of metric correlations shows COMET-metric variants exhibit the highest overall correlations with human scores. However, many metrics demonstrate skewed score distributions, limiting their interpretability.

Interestingly, metrics correlate better with human scores when focusing on accuracy errors specifically. Leveraging the dataset, the researchers fine-tuned COMET, creating "IndicCOMET," which outperforms standard COMET baselines across multiple languages, even in zero-shot scenarios. IndicCOMET also demonstrates improved robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for more reliable evaluation of machine translation for Indian languages. The dataset is publicly available to facilitate further research.</sample>
    <sample id="277">Sie hat keinen Namen.</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness" to identify words that distinguish marked groups from unmarked ones, using weighted log-odds ratios to compare the top words for each marked group.</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">MultiEMO is a novel framework for emotion recognition in conversations (ERC) that addresses limitations in existing methods. It focuses on better utilizing multimodal information (text, audio, and visual), improving performance on minority emotions, and distinguishing between semantically similar emotions.

The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A key innovation is VisExtNet, a visual feature extractor that focuses solely on facial expressions, eliminating irrelevant scene information. The core of MultiEMO is the MultiAttn fusion network, which uses bidirectional multi-head cross-attention layers to integrate information across modalities. Textual information is fused with audio and then visual cues.

To tackle the challenges of minority and semantically similar emotions, MultiEMO introduces a Sample-Weighted Focal Contrastive Loss (SWFC). This loss function prioritizes hard-to-classify samples and maximizes inter-class distances.

Extensive experiments on the MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in recognizing minority and semantically similar emotions. However, limitations include VisExtNet's inability to differentiate speakers, the SWFC loss's need for large batch sizes, and the continued performance gap between minority and majority classes.</sample>
    <sample id="281">This work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," investigates when translation relies on context and how well current models handle these cases. The researchers address the limitations of existing evaluation methods, which often fail to capture context-dependent translations.

They extend the CXMI metric to Pointwise CXMI (P-CXMI) to measure context usage at the sentence or word level, identifying words requiring context for accurate translation. Analyzing TED talk transcripts across 14 languages, they found that context is crucial for phenomena like dual pronouns (due to differences in grammatical structures), verb form selection, proper noun consistency, and maintaining appropriate formality. They also identified sentence-level phenomena like ellipsis resolution.

To facilitate evaluation, they developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words. Using this benchmark, they evaluated models, finding that context-aware models outperform context-agnostic ones when using COMET, but corpus-level metrics like BLEU can be misleading. While context-aware models excel in formality and lexical cohesion, improvements are needed for phenomena like pronouns and ellipsis. The study also revealed that DeepL generally outperforms Google Translate in document-level translation. Ultimately, this research provides valuable insights and a benchmark for advancing document-level machine translation.</sample>
    <sample id="282">StoryTrans is a novel approach to non-parallel story-level text style transfer, addressing the limitations of existing token or sentence-level methods. It tackles the challenges of imitating complex authorial linguistic preferences at the discourse level and preserving content across different writing topics. StoryTrans learns discourse representations and combines them with style embeddings to generate text in target styles. A key innovation is a two-stage generation process: first transferring the source text with masked content, then incorporating style-specific keywords. The model utilizes an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses. Extensive experiments on new Chinese and English datasets demonstrate StoryTrans's superior performance over baselines in style control and content preservation, as confirmed by both automatic and manual evaluations. Style visualization confirms alignment with golden text, and case studies highlight StoryTrans's ability to enrich storylines and maintain source semantics while rewriting sentences in the target style. Data and code are publicly available.</sample>
    <sample id="283">Prag.</sample>
    <sample id="284">FSUIE is a novel approach for Universal Information Extraction (UIE) presented at ACL, addressing limitations in existing span-based UIE models. These models often over-rely on precise span boundaries, which can be ambiguous in annotation. FSUIE proposes a "fuzzy span" mechanism, treating span boundaries as continuous distributions rather than fixed points.

The model incorporates two key components: a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL uses Binary Cross Entropy and KL-divergence to learn from both golden and fuzzy boundaries, while FSA adaptively adjusts the model's attention span. FSA introduces an optimizable parameter (delta) to dynamically control the attention range and linearly decays attention distribution near the boundaries, focusing on relevant semantic information within a limited range.

Experiments on Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction demonstrate FSUIE's effectiveness. It achieves state-of-the-art results on several datasets, including ACE2004, 2005, and ADE for relationship extraction, and AST-V2 for aspect sentiment triplet extraction. Ablation studies confirm that both FSL and FSA contribute to improved convergence speed and information extraction capabilities. The visualization of attention distribution confirms the model's focus on preceding tokens, aligning with the intended design. Overall, FSUIE offers a more robust and generalizable approach to UIE.</sample>
    <sample id="285">Mingqi Gao from Peking University presents "Reference Matters," a study addressing factual errors in dialogue summarization. Existing solutions involve either training summarization models for factuality or using independent Factual Error Correction (FEC) models. Gao's work critiques the current FEC evaluation methods, highlighting two flaws: vague, unreliable factuality metrics and a blurring of lines between FEC and summary generation.

The core argument is that FEC models can circumvent error correction by generating entirely new, factually correct summaries without addressing the original summary's issues. To rectify this, the study proposes a new evaluation framework utilizing manually annotated reference corrections, emphasizing minimal changes (substitutions, insertions, deletions) for fluency and non-redundancy.

A novel taxonomy of factual errors is introduced, categorizing them as content-based (part-of-speech, dependencies) and form-based (addition, deletion, substitution). The framework leverages ERRANT, an error correction metric, through alignment, classification, and comparison steps.

Experiments reveal that training FEC models with reference summaries improves performance on unreliable factuality metrics, underscoring the need for evaluation reform. Combining human-annotated data with synthetic data shows promise. However, current FEC models struggle with additions and more complex errors like attribute, modality, and link errors, indicating areas for future research.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier.</sample>
    <sample id="288">BLiMP, SyntaxGym, and CrowS pairs.</sample>
    <sample id="290">FTw, COSINE</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">OSCAR 138 GB</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presented a collaborative work between the University of Turin and Amazon Alexa focusing on irony detection in natural language understanding. Traditional approaches rely on "ground truth" annotations, but this research challenges that assumption, recognizing that irony is subjective and pragmatic.

To investigate this, they developed the EPIC corpus, a dataset of approximately 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. Data was annotated by 74 crowdsourced annotators using Prolific, each annotating 200 conversations and undergoing quality control checks. Each conversation received an average of 5 annotations, asking annotators to determine if a reply was ironic.

The research revealed significant differences in annotation agreement across various demographic groups (gender, age, nationality). To address this, they developed "perspective-aware models," training separate models on data subsets based on annotator groups. While raw performance wasn't significantly improved, these models demonstrated higher confidence in their predictions compared to aggregated "gold standard" models.

Further analysis uncovered a peculiar trend: disagreement on irony perception was highest between generations close in age and between annotators from the UK and Ireland. This suggests that cultural and generational factors significantly influence the interpretation of irony, highlighting the need for nuanced, perspective-aware approaches in natural language processing.</sample>
    <sample id="297">The research project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" investigates coded language used to convey hidden meanings to specific in-groups while maintaining plausible deniability to out-groups. The study focuses on dogwhistles—terms that carry a secondary, often taboo, meaning—particularly in the context of racism, transphobia, and antisemitism.

The project developed a typology and glossary of over 340 dogwhistles, categorizing them by register (formal/informal), type (implicature vs. covert signaling), and persona (e.g., antisemitic, transphobic). A case study of historical U.S. political speeches revealed a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy.

The research also evaluated language models (specifically GPT-3) for their ability to identify and interpret dogwhistles. While GPT-3 could surface some dogwhistles, performance varied, particularly with informal and transphobic terms.  Furthermore, the study demonstrated how dogwhistles can evade content moderation systems, as sentences containing dogwhistles were often rated as less toxic than those using explicit slurs. The findings highlight the challenges in detecting and mitigating coded rhetoric online and underscore the need for improved NLP techniques to address this issue.</sample>
    <sample id="298">Das Experiment, bei dem einige Modelle mit aktuelleren Daten neu trainiert oder weiter vortrainiert wurden, zeigte, dass die Leistung mit zunehmender zeitlicher Verzögerung abnahm.</sample>
    <sample id="299">This work addresses the issue of NLI (Natural Language Inference) models relying on shortcuts—spurious correlations in datasets that lead to good in-distribution performance but poor generalization on out-of-distribution data. Existing shortcut mitigation methods often require auxiliary models with specific knowledge and assumptions about shortcut types, which limits their applicability.

The proposed solution, minimax training, avoids these limitations. It leverages the observation that NLI models struggle with "hard" examples—under-represented instances that contradict shortcuts—and that the loss on these examples decreases slower during training. Minimax training pits a learner model (the NLI model) against an auxiliary model. The learner minimizes the NLI task loss, while the auxiliary maximizes the learner's loss by generating example weights, incentivizing the learner to focus on hard examples and counteract shortcut exploitation.

This alternating optimization process doesn't require pre-defined shortcut knowledge and uses a feed-forward network for the auxiliary, minimizing computational overhead. Experiments on MNLI, FEVER, and QQP with adversarial test sets (HANS Symmetric, PAWS) demonstrate consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy compared to standard training and existing mitigation techniques. The paper further explores the impact of model size, synthetic shortcuts, and transferability, alongside a qualitative analysis of the learned example weights.</sample>
    <sample id="300">Interactive dictation is a new task that aims to create a more natural and intuitive way for users to dictate and edit documents using their voice. Unlike existing speech-to-text systems that primarily support dictation, interactive dictation allows for flexible interleaving of dictation and editing commands without the need for specific trigger words.

The work introduces and formalizes this task, designing a data collection interface and building a dataset to support it. The process involves four steps: ASR recognition, segmentation of utterances into dictation and commands, command extraction and normalization, and finally, execution of both dictation and commands to achieve the final document state.

The researchers developed a baseline system using separate models for each step, experimenting with architectures like T5 and GPT-3. Results show a trade-off between runtime and accuracy, with GPT-3 generally being more accurate but slower. For T5, predicting programs proved more efficient. The researchers released the code to encourage further research and development in this promising area.</sample>
    <sample id="302">The tokens are already present after the first step, but they are not ordered, so a permutation is needed to put them in the correct order.</sample>
    <sample id="303">The authors recommend increased transparency because it's unclear whether positive stereotypes arise from value alignment or anti-stereotyping methods, and further study is impossible without transparency.</sample>
    <sample id="304">Unacceptable minimal pair inputs are sentences created by choosing unacceptable sentences from the same matching grammatical structure or from completely unrelated domains like Wikipedia.</sample>
    <sample id="305">This presentation critiques recent advances in Weakly Supervised Learning (WSL). While WSL aims to reduce manual labeling by using cheaper, noisy labels from sources like heuristics or crowdsourcing, the authors found that many WSL methods rely on a crucial, often overlooked, component: clean validation data.

Their research addresses three key questions: the necessity of clean validation data, the required amount, and optimal utilization. The findings reveal that WSL methods demonstrably degrade without clean validation samples, failing to generalize beyond the weak labels. Surprisingly, only a small number of clean samples (around 20 per class) are needed for effective validation. However, directly fine-tuning a model on these clean samples consistently outperforms WSL approaches, even with as few as 10 samples per class.

The authors argue that the performance gains claimed by WSL methods are often inflated and achievable through simple fine-tuning. They recommend future WSL research to transparently report model selection criteria, compare against few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. The code for their experiments is publicly available. Ultimately, the presentation suggests that the practical benefits and complexity of current WSL methods are often overstated.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented research investigating entity tracking abilities in large language models (LLMs). Entity tracking, the ability to follow how entities change throughout a discourse, is crucial for understanding longer texts. Their study questioned the extent to which LLMs possess this ability, acknowledging challenges in designing an evaluation task that avoids shortcuts like exploiting common patterns in pre-training data or relying on simple word associations.

To address these challenges, they created a task involving boxes and objects, where models predict the contents of boxes after a series of state-changing operations. They implemented measures to prevent heuristic shortcuts. Experiments with Flan-T5, GPT-3, and GPT-3.5 using 2-shot in-context learning revealed that only text-davinci-003 showed non-trivial tracking.

Interestingly, GPT-3.5 models, trained on substantial amounts of code, demonstrated entity tracking, while others did not. Fine-tuning smaller models like T5-base enabled entity tracking, but randomly initialized models failed even with direct supervision, highlighting the importance of pre-training. While promising, the generalizability of these observed tracking abilities remains unclear. The researchers encourage readers to consult their paper for further results, including experiments with GPT-4.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">Jenny from Carnegie Mellon University presented "NLPositionality," a framework for characterizing design biases in NLP datasets and models, developed in collaboration with the University of Washington and Allen Institute for AI. The core issue highlighted is that NLP technologies often perform differently across populations, stemming from the positionality—the perspectives shaped by demographics and experiences—of researchers and developers.

NLPositionality addresses this by comparing annotations from diverse users (over 1,600 annotators from 87 countries) with existing datasets and models like Social Chemistry, Dynahate, Perspective API, and GPT-4. Using Lab in the Wild, an online experimentation platform, they re-annotated data for social acceptability and hate speech detection.

The findings reveal significant positionality: datasets and models largely align with English-speaking countries and individuals with higher education. Notably, they demonstrate less alignment with non-binary individuals. This highlights a crucial problem – the exclusion of certain populations when NLP technologies are developed.

To mitigate these biases, the researchers recommend meticulous documentation of design choices, adopting a perspectivist approach to NLP research, and building specialized datasets and models tailored to specific communities, citing the Masakhani initiative as an example. The project emphasizes that inclusive NLP isn't about universal functionality but about acknowledging and addressing these inherent biases.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The text does not mention the authors' university affiliation.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning benchmark dataset.</sample>
    <sample id="313">Drei.</sample>
    <sample id="314">Coordination structures are headed by the conjunction.</sample>
    <sample id="315">The prompts were inspired by those given to human subjects in a previous study.</sample>
    <sample id="316">T5, fine-tuned on CoScript, generated higher-quality scripts than most large language models.</sample>
    <sample id="317">CodeIE tackles the challenges of information extraction (IE) by reframing it as a structure-to-structure code generation task. Traditional IE models using language models like T5 and GPT-3 struggle because of a mismatch between the text-based pre-training and the linearized, structured output during inference. CodeIE utilizes code large language models (like Codex) to address this, converting text to structured formats during input and ensuring aligned structures in the output.

The approach involves designing prompts that define functions for IE tasks (e.g., named entity recognition, relation extraction) and leveraging in-context demonstrations to guide the model's code generation. Evaluations across multiple datasets demonstrate that CodeIE significantly outperforms traditional models (UIE, GPT-3) in few-shot settings.

Analysis reveals that code language models exhibit lower perplexity on code-formatted inputs and generate fewer structural errors. Furthermore, CodeIE mitigates issues like generating labels outside the predefined set and consistently outperforms GPT-3, particularly in recall. The research suggests that aligning IE with code generation and utilizing code-pre-trained models improves performance and offers valuable insights for future IE research. The paper and code are publicly available.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeit über "DrBERT: Ein robustes vortrainiertes Modell für Französisch in den Bereichen Biomedizin und Klinik" vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir stellen DrBERT vor, das erste biomedizinische Modell in Französisch, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer Daten, die aus dem Web gecrawlt wurden. Wir haben auch einen Vergleich von Modellen mit verschiedenen Vortrainingsszenarien und Datenquellen vorgestellt. Anschließend präsentieren wir unsere Ergebnisse für 11 biomedizinische und klinische Downstream-Aufgaben in Französisch. Abschließend fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 hat sich BERT zu einem der effektivsten Ansätze zur Lösung von Natural Language Processing-Aufgaben entwickelt und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder ähnlichen enormen Leistungssteigerungen. Seitdem wurde dieses Modell in andere Sprachen wie CamemBERT in Französisch und in Bereiche wie Biomedizin mit PubMedBERT und BioBERT und in den klinischen Bereich mit ClinicalBERT angepasst, hauptsächlich aber auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an In-Domain-Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für den biomedizinischen Bereich. Wir haben uns also die Frage gestellt, welche die am besten geeigneten Datenquellen für eine breite Palette von Anwendungen sind und ob gecrawlte Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager des Universitätsklinikums Nantes gewonnen wurden. Anschließend fragen wir uns, wie viele Daten wir benötigen, um ein spezialisiertes Modell in französischer Sprache zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, haben wir zunächst vier Modelle von Grund auf trainiert und verglichen: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB von NACHOS; eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen; und eine finale Version von ChuBERT mit einer Mischung aus 4 GB von NACHOS und 4 GB von klinischen Notizen. Ergänzend zu diesem Vergleich haben wir drei Modelle vorgestellt, die mit kontinuierlichem Vortraining trainiert wurden, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Ein Modell basiert auf den Gewichten von CamemBERT und wurde auf einem 4-GB-Datensatz von NACHOS trainiert. Ein weiteres Modell basiert ebenfalls auf CamemBERT, wurde aber diesmal auf den 4 GB klinischer Notizen trainiert. Schließlich ein Modell, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB von NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu evaluieren, haben wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech-Tagging und Question Answering gesammelt. Diese Modelle werden mit sechs Basislinienmodellen verglichen, nämlich CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluierung zeigt, dass die Modelle am besten auf Aufgaben funktionieren, deren Daten der Natur der Daten ähneln, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger erscheinen. Wir stellen auch fest, dass mehr Daten zu einer besseren Leistung führen. Insgesamt scheinen Modelle, die von Grund auf vortrainiert wurden, in den meisten Aufgaben eine höhere Leistung zu erzielen. Unser Experiment mit kontrolliertem Vortraining unter Verwendung der Gewichte und der Tokenisierung von CamemBERT, das auf einem 4-GB-Teilmengen-Datensatz von NACHOS trainiert wurde, zeigte vergleichbare Ergebnisse wie die von DrBERT 4 GB von Grund auf. Dies ist nicht der Fall für das Modell, das auf CamemBERT-Gewichten und -Tokenisierer basiert, das unter Stabilitätsproblemen leidet. Abschließend bieten unsere spezialisierten Systeme in neun von elf Downstream-Aufgaben eine bessere Leistung und übertreffen global das Ergebnis des generischen Modells, hier CamemBERT. Wir stellen auch fest, dass spezialisiertere Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS gewonnen wurden, sind unter der MIT-Lizenz auf Hugging Face frei verfügbar, und alle Trainingsskripte finden Sie in unserem GitHub-Repository. Vielen Dank für diese Präsentation und wir freuen uns auf den Austausch auf der Postersession in Toronto.</sample>
    <sample id="319">From-scratch pre-training and continual pre-training.</sample>
    <sample id="320">Adaptive overfitting is not observed.</sample>
    <sample id="321">The sentence pairs were analyzed for the type of simplification, with Bible texts being much stronger simplified than news or language learner texts. The corpus also showed a high variety of simplification transformations.</sample>
    <sample id="322">Enrico's ACL 23 presentation addresses how text classifiers learn about morality, moving beyond the simplistic view of morality as a single scale between "moral" and "immoral." He argues that morality is subjective and pluralistic, influenced by individual prioritization of different moral foundations—fairness, authority, care, loyalty, and sanctity—as outlined by Moral Foundation Theory.

Current NLP approaches often average moral judgments, which Enrico cautions can be misleading. His research investigates what language models actually learn about morality by applying explainable AI techniques to models trained on the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven diverse domains.

The study focuses on understanding how morality is expressed differently across these domains, such as the contrasting rhetoric surrounding #AllLivesMatter and #BlackLivesMatter.  The research demonstrates that language models can recognize these nuanced differences, for example, identifying that the concept of "subversion" is viewed negatively within the #AllLivesMatter domain (associated with words like "overthrow" and "mayhem") while being more encouraged within the #BlackLivesMatter domain.

Ultimately, Enrico's work highlights the danger of applying a single model across varied domains, emphasizing the need for a more sophisticated understanding of morality in text to avoid misinterpretations and potential harm.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach to Commonsense Question Answering (QA) that addresses limitations in existing methods combining Language Models (LMs) and Knowledge Representation Learning (KRL). Current approaches often retrieve noisy entities from knowledge bases, encode text and subgraphs separately, and neglect semantic relationships between entities.

DHLK constructs a Heterogeneous Knowledge Graph (HKG) from multiple knowledge bases, employing a two-stage pruning strategy and KRL to optimize its structure and knowledge representation. It enriches the graph by adding paraphrased entities from WordNet and Wiktionary.  The method utilizes RoBERTa and a novel Relation Mask Self-Attention (RMSA) mechanism, inspired by RGAT, to dynamically remove irrelevant entities and model the HKG. RMSA incorporates relationships into the attention mechanism, iteratively updating entity and relation embeddings.

Furthermore, DHLK enhances the QA context with HKG path information. Finally, it uses a Multi-Layer Perceptron (MLP) to predict answers based on the HKG embedding, path-enhanced context embedding, and original context embedding. Experiments on CommonsenseQA and OpenBookQA, using ConceptNet, WordNet, and Wiktionary, demonstrate DHLK's superior performance compared to existing LM and HKG methods. Key entities are extracted using KeyBERT, and knowledge paths are retrieved within ConceptNet.</sample>
    <sample id="324">Yes, language models do have varying political leanings, occupying all four quadrants on the political compass. GPT-4 is the most liberal, and GPT series are generally more socially liberal than BART series.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unser Papier über "Kompositionelle Verallgemeinerung ohne Bäume unter Verwendung von Multiset-Tagging und latenten Permutationen" geben. Dies ist eine Gemeinschaftsarbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Verallgemeinerung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und ungesehene Kompositionen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsings könnte das Testen der kompositionellen Verallgemeinerung so aussehen. Wie üblich haben wir einen Trainingsdatensatz mit Äußerungen. In diesem Fall "Das Mädchen schlief." Und "Maria wusste, dass das Mädchen schlief." Diese Äußerungen sind mit logischen Formen gepaart, die die Kernaspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Machine-Learning-Evaluierung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell ungesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Seq2Seq-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Verallgemeinerung und erzeugen oft Ausgaben, die vom Input abgekoppelt sind. Insbesondere scheitern sie oft daran, die systematischen Korrespondenzen zwischen Input und Output wiederzugeben, wie sie in dem Beispiel farblich hervorgehoben sind. Eine beliebte Methode, um dies zu beheben, ist die Integration von Bäumen in die Modelle. Die Bäume sollen den kompositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht bereitgestellt und müssen irgendwie erhalten werden. Dies kann kompliziert und manchmal rechenintensiv sein. Typischerweise beinhaltet dies eine beträchtliche vorformelspezifische Vorverarbeitung der logischen Formen, beispielsweise zur Handhabung von Variablen. Das Erhalten von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren erfordern. In diesem Papier verwenden wir keine Bäume und führen ein neuronales Seq2Seq-Modell ein, das direkt die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs modelliert. Zum ersten Mal zeigen wir eine starke Verallgemeinerung auf tiefere Rekursion, ohne auf Bäume angewiesen zu sein. Unser Ansatz sagt die Ausgabe in zwei Schritten aus. Zuerst taggen wir jedes Input-Token mit einer ungeordneten Multimenge von Tokens, die im Output erscheinen werden. Nach dem ersten Schritt haben wir also alle richtigen Tokens, aber sie sind nicht geordnet. Deshalb verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation zu berechnen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode zur Vorhersage der Permutation ein, die keine harten Einschränkungen an die möglichen Permutationen auferlegt. Dies macht unseren Ansatz recht flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell in etwa so: Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jede Position gehört. Für die erste Output-Position wählen wir einfach eines aus, wie rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Vorgang fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumfreien Modellen am COGS-Benchmark. Unser Modell übertrifft die anderen um einen großen Abstand bei der Verallgemeinerung auf tiefere Rekursion. Einige andere Arten der strukturellen Verallgemeinerung bleiben jedoch sehr herausfordernd. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Erstens wird die Ausrichtung zwischen Input und Output in den Trainingsdaten nicht angegeben. Folglich wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Darüber hinaus gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir beheben dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstwertigen Permutation NP-hart ist. Das liegt daran, dass dies mit dem "Traveling Salesman"-Problem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zu backpropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie sich bitte unser Papier an oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent.</sample>
    <sample id="327">ManagerTower is a novel vision-language (VL) architecture designed to improve representation learning by effectively utilizing multi-level unimodal semantic knowledge. Building upon BridgeTower, ManagerTower addresses its limitations by introducing "managers" in each cross-modal layer that adaptively aggregate insights from pre-trained unimodal experts (RoBERTa and CLIP-ViT base). This allows for more comprehensive cross-modal alignment and fusion compared to previous approaches that statically assign unimodal layers.

The architecture’s key innovation lies in its ability to dynamically weigh different levels of unimodal semantic knowledge, as demonstrated through visualizations showing distinct aggregation weight distributions for adaptive managers compared to static ones.  Evaluated on various downstream tasks with only 4 million images, ManagerTower achieves state-of-the-art performance, significantly outperforming BridgeTower and even surpassing larger models trained on more data. The code and models are publicly available.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, aiming to overcome the limitations of existing methods that rely on manual annotations. The core idea is to generate structured pseudo-labels to train a video sentence localization model without any human input.

Existing zero-shot methods often suffer from simplistic pseudo-queries, misalignment between queries and video segments, and a lack of robustness to label noise. To address these issues, the proposed method generates more complex, free-form pseudo-queries using a pre-trained image captioning model (BLIP). It then constructs pseudo-events by measuring frame relevance to these queries, ensuring high relevance within the event and low relevance outside. A sliding window approach identifies the optimal event duration based on this relevance difference.

To mitigate label noise, the method dynamically adjusts sample weights based on model confidence and IoU with pseudo-labels. High-confidence, high-IoU predictions are refined and used as new pseudo-labels in subsequent training rounds.

Experiments on ActivityNet Captions and Charades-STA demonstrate that the proposed method, termed SPL, significantly outperforms existing zero-shot approaches across various evaluation metrics (R@M and mIoU), showcasing its effectiveness and robustness in zero-shot video sentence localization. The code is publicly available.</sample>
    <sample id="330">Cumulative training performed equal or better than iterative training.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Transkripte von TED-Talks.</sample>
    <sample id="333">INK is a novel training framework designed to enhance Neural Machine Translation (NMT) models by injecting kNN (k-Nearest Neighbors) knowledge. NMT models often suffer from a non-smooth representation space, leading to poor generalization and performance, particularly with low-frequency tokens. kNN-MT addresses this by smoothing predictions using a datastore of representations and target tokens, but it's computationally expensive and doesn't allow for easy representation updates.

INK overcomes these limitations through an iterative training loop. It extracts kNN knowledge to guide an adapter in adjusting representations, then asynchronously updates the datastore. The adapter aligns contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token, using KL-divergence to enrich semantic meaning and address sparsity. Crucially, INK allows for the datastore to be discarded after training.

Experiments using the WMT’19 German-English translation task demonstrate that INK significantly improves performance compared to state-of-the-art kNN-MT systems, achieving a 1.99 COMET score and 1.0 BLEU score gain. INK achieves higher BLEU scores with less memory space and faster inference speed, proving that combining adapters and datastores can further refine the NMT model's representation space.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This research introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to represent out-of-vocabulary (OOV) words, a significant challenge in embedding-based models. Inspired by human learning, the method leverages word formation and association to infer OOV word meanings.

The core of the approach is a Word Relationship Graph, constructed by tokenizing OOV words into wordpieces and connecting them to relevant words, creating a two-level graph. A self-attention network assigns attributes to OOV nodes based on their characters. Two layers of Graph Attention Networks (GATs) are then applied to refine node representations, mitigating noise and capturing crucial information. A readout block layer generates a graph-level representation summarizing word formation.

To align with existing embedding models, contrastive learning is employed, using NT-XENT loss with positive samples like two-hop neighbors and synonyms. Experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of the word formation-based learning.

The model's adaptability to different languages is also explored, noting that agglutinative languages are particularly well-suited. The research concludes that the model's ability to handle complex word formations makes it promising for various languages, contingent on reasonable word decomposition.</sample>
    <sample id="338">This research investigates the quality of human-generated natural language explanations used to train AI models. While these explanations are often considered valuable for improving model performance and reasoning, the paper questions whether they are *always* helpful and proposes a method for objectively evaluating them.

The researchers highlight the limitations of existing metrics like BLEU and ROUGE, which treat human explanations as gold standards, and the Simulatability score, which doesn't account for task differences. Their work introduces a "Unified Structure" to standardize data across various tasks (commonsense QA, natural language inference, commonsense validation) and a novel metric called TREU.

TREU builds upon the Simulatability score by specifically evaluating the helpfulness of explanations during the fine-tuning process. Through experiments on five datasets and two models (T5 and BART), they found that explanations, even those considered "low quality" by humans, can still benefit model predictions. The study also revealed that fine-tuning with explanations can lead models to rely on the explanation input itself, and that the utility of explanations is highly task-dependent.

Ultimately, the research demonstrates that TREU consistently ranks dataset qualities better than the Simulatability score and provides valuable insights into how human explanations impact model performance, advocating for quality checks in future annotation efforts.</sample>
    <sample id="339">Saarland University</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a joint work with several collaborators. The core challenge addressed is the need for large, high-quality, and syntactically diverse paraphrase datasets for NLP applications like question answering and chatbots. Existing datasets either lack scale (human-annotated) or syntactic diversity (automatically generated via back-translation).

ParaAMR tackles this by leveraging Abstract Meaning Representations (AMR) – directed graphs representing sentence meaning. The method involves using a pre-trained AMR parser, randomly changing the "focus" (root node) of the AMR graph, and then generating text from the modified graph using an AMR graph-to-text generator. This process creates paraphrases that maintain semantic similarity while exhibiting syntactic variation due to the altered focus and text generation process.

The resulting ParaAMR dataset contains 15 million source sentences and approximately 6.9 paraphrases per sentence. Evaluations, both automatic and human, demonstrate that ParaAMR achieves comparable semantic similarity to other back-translation datasets while significantly improving syntactic diversity. Experiments on sentence embedding learning, syntactic control paraphrase generation, and few-shot learning showcase the benefits of using ParaAMR, leading to improved performance in these NLP tasks. The dataset is publicly available.</sample>
    <sample id="341">BLEU, average lagging, and computational-aware average lagging.</sample>
    <sample id="342">Hello, my name is Gao Jingsheng. I'm presenting our paper, "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming."

Open-domain dialogue relies on pre-trained models and large datasets, primarily text-sourced. Our work addresses the need for a large-scale, video-sourced dataset closer to real spoken conversation. Existing video datasets are limited by manual annotation. We propose LiveChat, a dataset constructed automatically using a reply-to-whom matching method.

LiveChat also tackles personalized dialogue challenges, incorporating persona information and session dialogues. It further addresses the lack of large-scale Chinese multi-party dialogue data.

We built LiveChat in three steps: scraping videos from Chinese TikTok/Douyin, transcribing audio, and constructing dialogues with our matching method. Persona information is extracted through manual labeling and rule-based/trained classifiers.

Experiments on response modeling and addressee recognition demonstrate the benefits of our persona profiles and longer sessions. BART outperforms other models, highlighting LiveChat's distinct domain. Human evaluations show LLMs perform well with rich informativeness, and in-context learning improves with more demonstrations, though excessive demonstrations introduce noise.

In conclusion, LiveChat is a valuable resource for dialogue research, and future work will focus on efficient LLM transfer learning. Thank you.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha und präsentiere heute zusammen mit meinem Co-Autor Martin unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research.

Modelle für das natürliche Sprachverständnis nutzen verschiedene Wissensquellen, wie z.B. Wissen, das in ihren Parametern gespeichert ist (meist durch Vortraining erworben), und Wissen, das zur Inferenzzeit bereitgestellt wird. Neuere Arbeiten in Bereichen wie der Fragebeantwortung zeigen, dass Modelle vortrainiertes Wissen zur Lösung von Aufgaben nutzen können. Das natürliche Sprachverständnis erfordert jedoch oft auch Wissen, das erst zur Inferenzzeit verfügbar ist. Nehmen wir zum Beispiel den Satz: "John sah den gerade gewählten Präsidenten im Fernsehen." Die vortrainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernsehen ist, aber sie wissen nicht zuverlässig, wer diese instanzspezifische Entität "John" ist oder wer der neue Präsident ist, da sich der Präsident seit dem Vortraining geändert haben könnte. Daher benötigen erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl vortrainiertes als auch zur Inferenzzeit verfügbares Wissen zu integrieren und zu nutzen.

In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir stellen eine Kerneferenzauflösungsaufgabe vor, die entwickelt wurde, um die Fähigkeit zu testen, Wissen aus verschiedenen Quellen zu nutzen. Wir evaluieren den Datensatz mit menschlichen Teilnehmern und etablierten Kerneferenzauflösungsmodellen.

Ein Beispiel aus unserem Datensatz: "Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, an dem er in einem Gericht Fälle entschied, war er froh, sich zu entspannen." Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen "er" sich bezieht, nämlich Servin. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen: erstens, wissensspezifische Informationen über eine Entität, wie z.B. "Servin ist ein Richter", und zweitens, Hintergrundwissen, wie z.B. "Richter entscheiden Fälle in Gerichten". Hintergrundwissen wird im Allgemeinen während des Vortrainings großer Sprachmodelle erlernt, während wissensspezifische Informationen typischerweise zur Inferenzzeit beobachtet werden.

Wir variieren die Verfügbarkeit dieser beiden Informationsarten, so dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen für KITMUS definiert:

1.  **Typische Einstellung ("Background-Pretrain"):** Hintergrundwissen wird als verfügbar beim Vortraining angenommen.
2.  **"Background-Both" Einstellung:** Hintergrundwissen ist sowohl beim Vortraining als auch zur Inferenzzeit verfügbar.
3.  **"Background-Inference" Einstellung:** Beide Wissensarten sind nur zur Inferenzzeit verfügbar.

Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das zur Lösung einer Aufgabe notwendige Hintergrundwissen nicht Teil der Vortrainingsdaten der Modelle ist, beispielsweise weil seit dem Vortraining neue Berufe entstanden sind.

Wir steuern die Verfügbarkeit von Fakten in den wahren Quellen wie folgt: In der "Background-Pretrain" Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker streben nach gewählten Sitzen in der Regierung" in den vortrainierten Parametern enthalten ist und im Inferenzzeit-Kontext stellen wir das wissensspezifische Wissen "Chichester ist ein Politiker" bereit. In der "Background-Both" Einstellung stellen wir zusätzlich zum wissensspezifischen Wissen auch Hintergrundwissen über Politiker im Inferenzzeit-Kontext bereit. In der "Background-Inference" Einstellung stellen wir einen fiktiven Beruf "mirituer" anstelle von Politiker bereit, da "mirituer" wahrscheinlich nicht in den vortrainierten Parametern enthalten ist.

Wir evaluieren den Datensatz sowohl mit menschlichen Teilnehmern als auch mit etablierten Kerneferenzauflösungsmodellen. Die Ergebnisse zeigen, dass die leistungsstärksten Modelle ohne aufgabenspezifisches Training auf KITMUS nicht gut abschneiden. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref deutlich besser ab als bei einer zufälligen Auswahl. Dies deutet darauf hin, dass die meisten Modelle, wenn sie auf generischen Kerneferenzauflösungsdatensätzen trainiert werden, lernen, Oberflächenhinweise auszunutzen, die bei der Prüfung auf KITMUS, wo solche Hinweise entfernt wurden, nicht nützlich sind.

Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die leistungsstärksten Modelle Schwierigkeiten haben, Hintergrundwissen zuverlässig zu integrieren, das nur zur Inferenzzeit bereitgestellt wird.

Zusammenfassend lässt sich sagen, dass viele Kerneferenzauflösungsmodelle ohne aufgabenspezifisches Training nicht in der Lage sind, über Wissen aus verschiedenen Quellen zu schlussfolgern. Mit aufgabenspezifischem Training können jedoch einige Modelle Wissen aus mehreren Quellen integrieren. Selbst die leistungsstärksten Modelle haben jedoch Schwierigkeiten, Hintergrundwissen, das nur zur Inferenzzeit präsentiert wird, zuverlässig zu integrieren.

Wenn Sie an weiteren Details interessiert sind, sehen Sie bitte unser Papier und den Datensatz sowie den Code auf GitHub. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Trees are usually not given and need to be obtained, which can be complicated and computationally expensive, often involving formalism-specific pre-processing or grammar-induction procedures.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence model for semantic parsing that achieves strong compositional generalization without relying on explicit tree structures. Compositional generalization, the ability to handle unseen combinations of phrases, is a key challenge in semantic parsing. Traditional methods often use trees to represent the compositional process, but this requires complex pre-processing and grammar induction.

The proposed model tackles this by directly modeling correspondences between input and output fragments. It operates in two stages: first, it tags each input token with a multiset of output tokens; second, it predicts a permutation to order these tokens correctly. A key innovation is a flexible permutation model that avoids hard constraints, allowing for a wider range of possible permutations.

The model's performance is demonstrated on the COGS benchmark, where it significantly outperforms other tree-less models in generalizing to deeper recursion. The paper also addresses technical challenges, including the lack of explicit alignment between input and output tokens during training and the computational complexity of finding the optimal permutation (related to the Traveling Salesman problem). These challenges are addressed through alignment induction and a GPU-friendly continuous relaxation technique that enables backpropagation. The authors encourage readers to consult the full paper or their poster for more details.</sample>
    <sample id="346">Das wird im Text nicht erwähnt.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren wurde vielfach dokumentiert, wie weit verbreitet soziale Vorurteile und Stereotypen in großen Sprachmodellen, oder LLMs, sind. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie stützen sich meist auf manuell erstellte Datensätze, die sehr zeitaufwändig zu kuratieren sind. Außerdem messen sie meist nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere demografische Gruppen oder Kontexte generalisieren oder einfach nur sehr allgemeine, breite Assoziationen erfassen, wie z. B. negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeiten in diesem Bereich keine Intersektionalität, d. h. die Vorstellung, dass facettenreiche soziale Identitäten Vorurteile verstärken und einzigartige Schadensursachen darstellen können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass diese neueren, durch Anweisungen optimierten LLMs sehr gut darin sind, Anweisungen und Prompts zu beantworten. Wir können das Modell also bitten, eine Persona zu generieren, d. h. eine Darstellung einer imaginären Person, indem wir einen Prompt wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede demografische Gruppe generalisiert werden kann, da wir einfach die gewünschte Identitätsmarkierung in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Man sieht sofort, dass die Ausgaben nicht offen negativ oder toxisch im herkömmlichen Sinne dieser Wörter sind, aber es gibt einige interessante Muster. Die asiatische Frau wird als unauffällig dargestellt, die Frau aus dem Nahen Osten wird mit Wörtern wie "exotisch" und "wie, Bezug nehmend auf eine faszinierende Region" bezeichnet. Und beide Frauen mit Farbhaut-Personas beziehen sich auf ihre Abstammung, während die Persona des weißen Mannes nichts dergleichen enthält. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste Teil ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts menschlichen Probanden gegeben wurden, was dazu führte, dass auch hier rassische Stereotypen aufgedeckt wurden. Dies ermöglicht auch einen direkten Vergleich zwischen den generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil ist "Marked Words", eine Methode zur Identifizierung der Wörter, die markierte Gruppen von nicht-markierten Gruppen unterscheiden, was ich gleich näher erläutern werde. Der Vorteil ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Marked Words-Methode greift auf das soziolinguistische Konzept der "Markierung" zurück, das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn also Menschen einen Krieger beschreiben, der eine Frau ist, werden sie normalerweise tatsächlich den Begriff "Frau-Krieger" spezifizieren und den Begriff mit "Frau" markieren. Und im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bezeichnen wir zunächst, welche Gruppen als unmarkiert und welche als markiert gelten, und vergleichen dann die Personas mithilfe der Fightin’ Words-Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas von schwarzen Frauen Fightin’ Words verwenden und die Log-Odds-Verhältnisse sowohl mit den weißen Personas als auch mit den männlichen Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter und des Lexikons betrachten, stellen wir sehr unterschiedliche Dinge fest. Während die generierten Personas eine viel höhere Rate der Wörter aus dem Lexikon aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung der Wörter. Die Stereotyp-Wörter, die in den generierten Personas enthalten sind, sind wirklich nur die Wörter "groß" und "sportlich". Es sind also wirklich nur die positiven oder zumindest nicht-negativen Wörter. Und in der Tat erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht gut. Stattdessen werden wir uns daher auf die Ergebnisse der Marked Words-Methode stützen, um zu zeigen, wie diese scheinbar positiven Wörter Stereotypen und essentialisierende Narrative erleichtern. In unserer Analyse zeigen wir auf, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst umfassen die Top-Wörter unserer Gruppen Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Diese Wörter definieren diese Gruppen nur in Bezug auf ihre Beziehung zu ihrer Identität und unterscheiden sie vom weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und Ausgrenzung dieser Gruppen bei. Darüber hinaus spiegeln sich in diesen Wörtern viele gängige Tropen wider, insbesondere für Frauen mit Farbhaut. Zum Beispiel umfassen die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebendig" und "kurvig", die sich auf einen Tropismus beziehen. Für asiatische Frauen sind die Wörter Dinge wie "zart" und "delikat" und "seidig", die sich auf eine lange Geschichte der Hypersexualisierung asiatischer Frauen beziehen, die als sehr gefügig und unterwürfig angesehen werden und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies bezieht sich auf das "Strong Black Women"-Archetyp, das von einigen als schädlich angesehen wird, da es diese demografische Gruppe unter Druck setzt, angesichts gesellschaftlicher Hindernisse widerstandsfähig und stark zu sein. Anstatt also daran zu arbeiten, diese Hindernisse zu beseitigen, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen gesundheitlichen Folgen für diese Menschen und anderen Schäden führt. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur essentialisierende Narrative widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Narrative angehen. Wir sollten auch eine intersektionale Perspektive verwenden, um Vorurteile und Schäden zu untersuchen, da viele Dinge übersehen werden könnten, wenn wir dies nicht tun. Und schließlich sollte es eine verstärkte Transparenz über Methoden zur Voreingenommenheitsminderung geben, da wir zum Beispiel nicht wissen, ob es sich um eine Art von seltsamer, übermäßig exzessiver Werteausrichtung oder vielleicht um einige andere Anti-Stereotyp-Methoden handelt, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank für Ihre Aufmerksamkeit. Ich wünsche Ihnen einen schönen Aufenthalt auf der ACL.</sample>
    <sample id="348">Myra's paper, "Marked Personas," investigates stereotypes in large language models (LLMs) using a novel approach. Traditional methods for detecting bias rely on curated datasets or broad association measures, often failing to capture nuanced stereotypes or intersectional harms. This work leverages the instruction-following capabilities of LLMs to generate personas based on prompts like "Imagine you are an [identity]. Describe yourself."

The core innovation is the "Marked Words" method, drawing on sociolinguistic principles of markedness. It identifies words that distinguish marked (marginalized) groups from unmarked (dominant) ones, revealing subtle but harmful patterns. Analysis of GPT-4 generated personas showed that while seemingly positive words like "tall" and "athletic" were prevalent, the method uncovered deeper issues.

The study found that personas often define groups solely by their identity ("culture," "tradition"), reinforcing "othering" and essentializing narratives. Specific tropes emerged: Latina women were described as "vibrant" and "curvaceous," Asian women as "petite" and "delicate," and Black women as "strong" and "resilient"—all of which perpetuate harmful stereotypes.

The paper concludes with recommendations for researchers and model owners: address positive stereotypes, adopt an intersectional lens, and increase transparency regarding bias mitigation techniques to better understand and combat these pervasive issues.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es ist mir eine Freude, Ihnen ein kurzes Werbevideo für unser Paper zu präsentieren: "Schutz des Urheberrechts großer Sprachmodelle für Embedding als Services durch Backdoor-Wasserzeichen".

Lassen Sie uns zunächst den Hintergrund von Embedding als Services vorstellen. Aktuell sind große Sprachmodelle wie GPT, LLAMA und PALM außergewöhnlich im Bereich des Verständnisses und der Generierung natürlicher Sprache. Embedding als Services ist einer der Services, die auf großen Sprachmodellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Beispielsweise bietet OpenAI eine GPT-basierte Embedding-API. Es hat sich jedoch gezeigt, dass Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Services anbieten können. Daher ist es notwendig, das Urheberrecht von Embedding als Services zu schützen.

Um das Urheberrecht von Embedding als Services zu schützen, ist eine Lösung, ein Wasserzeichen in den Provider-Service einzubetten und zu erkennen, ob ein anderer Service das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen:

1.  Die Methode sollte für Embedding als Services anwendbar sein.
2.  Das Wasserzeichen sollte die Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen.
3.  Das Wasserzeichen sollte für den Angreifer verdeckt sein oder der Angreifer kann das Wasserzeichen leicht entfernen.
4.  Das Wasserzeichen muss während des Modell-Extraktionsprozesses auf die Services des Angreifers übertragbar sein.

Bisherige Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht für Embedding als Services anwendbar oder es fehlt ihnen an Übertragbarkeit. Daher schlagen wir in diesem Paper Embedding Marker vor, eine Backdoor-basierte Wasserzeichenmethode, die für Embedding als Services anwendbar ist.

Lassen Sie mich Ihnen nun die Details von Embedding Marker vorstellen. Embedding Marker besteht aus zwei Hauptschritten: Wasserzeicheneinschub und Urheberrechtsverifizierung.

Vor diesen Hauptschritten wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Frequenzbereich. Wir gehen davon aus, dass der Provider einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann.

Beim Wasserzeicheneinschub definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Provider-Service sendet, zählt der Provider die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.

Die Urheberrechtsverifizierung dient dazu, zu erkennen, ob ein Modell hinter einem anderen Service das Wortmark enthält. Wir erstellen zunächst eine Hintertür und einen gutartigen Datensatz. Der Hintertür-Datensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Anschließend fordert der Provider die Embeddings vom Service des Angreifers mit dem Datensatz an. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem gutartigen und dem Hintertür-Datensatz, die als Delta Cosinus und Delta L2 definiert ist. Gleichzeitig wenden wir auch einen KS-Test an und verwenden seinen p-Wert als dritte Metrik.

Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Provider den Wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielen kann, während er gleichzeitig eine hervorragende Nutzbarkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf vier Datensätzen [INAUDIBLE 4:39] PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie aus den Abbildungen hervorgeht, ist es schwierig, zwischen den Hintertür-Embeddings und den normalen Embeddings zu unterscheiden.

Das war's. Vielen Dank. Wir freuen uns auf eine Diskussion mit Ihnen.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" investigates the validity of claims of superhuman performance in Natural Language Understanding (NLU). Leaderboard-based evaluation has become standard, leading to models exceeding human performance on benchmarks like SuperGLUE and SQuAD. However, the paper argues that these achievements are misleading.

The analysis reveals several issues: systems and humans are often evaluated on different subsets of the data, ground-truth answers contain errors, and human baselines are often poorly defined. Systems can exploit spurious correlations in the data, a tactic humans cannot employ. Furthermore, human performance estimations are often based on simple aggregation methods rather than comparing against the best possible human performance.

The paper also highlights concerns about annotator compensation and pool details. Low pay rates and a lack of information about annotator demographics raise questions about the quality of human annotations. Consequently, claims of superhuman performance lack scientific meaning without reliable human baselines.

The paper concludes that current claims of superhuman performance in NLU are not well-grounded and offers recommendations for constructing more reliable benchmarks.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. The study addresses whether these models still perform well on modern data and identifies factors contributing to good generalization. To assess this, the authors created CoNLL++, a new dataset annotated with CoNLL-2003 guidelines but sourced from 2020 Reuters News. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++.

Findings indicate that transformer-based architectures, larger model sizes, and more fine-tuning examples are crucial for good generalization. The research explored two hypotheses for performance drops: adaptive overfitting and temporal drift. Adaptive overfitting was not observed, while temporal drift, caused by the increasing time gap between training and testing data, was confirmed as the primary cause of performance degradation. Despite this drift, the study concludes that CoNLL-2003 taggers still function effectively in 2023, highlighting the need for further research into improving model generalization.</sample>
    <sample id="352">Annotating behaviors in chat.</sample>
    <sample id="353">This paper introduces a novel approach to Python code generation that addresses the challenge of input underspecification, a common issue where natural language descriptions (NLDs) lack crucial details. The authors propose an interactive framework where the model asks clarification questions (CQs) to gather missing specifications.

The core contribution is the CodeClarQA dataset, synthetically created with CQs focused on clarifying operation-level specifications. The dataset creation process involves identifying key operations within code, representing them using schemata, and determining if an NLD aligns with the operation documentation. If alignment is low, a CQA is generated using templates (yes/no or multiple-choice).

The proposed pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments demonstrate that the task of generating code with CQs is more challenging than existing CQA ranking tasks, and that clarifications generally improve code generation. While the interactive pipeline currently underperforms a model-only approach due to the difficulty of the CQA ranking task, analysis suggests that clarified key operations are indeed a significant factor in generating better code. The paper highlights areas for future improvement, such as better handling of taxonomy and argument information. Ultimately, the work paves the way for more robust and interactive code generation systems.</sample>
    <sample id="354">2020</sample>
    <sample id="355">Mein Name ist Vasudha, und ich bin PhD-Studentin in Informatik an der Stony Brook University. Wir haben eine Arbeit, die auf der ACL 2023 als Long Paper akzeptiert wurde, mit dem Titel "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge".

**Was ist kognitive Dissonanz und warum ist sie wichtig?**

Kognitive Dissonanz entsteht, wenn zwei Überzeugungen oder Handlungen inkonsistent sind. Ein Beispiel: Jemand sagt "Ich weiß, dass Zigaretten mich umbringen können", raucht dann aber und rechtfertigt dies mit "Ich glaube, ich könnte meinen Job ohne sie nicht behalten". Diese Inkonsistenz ist Dissonanz. Das Erkennen und Verstehen von Dissonanz in Sprache kann uns helfen, Meinungsverschiedenheiten, Wertänderungen in der Bevölkerung, psychische Gesundheitsprobleme (wie Angststörungen) und sogar Extremismus und Polarisierung zu verstehen. Es kann auch Einblicke in individuelle Denkweisen und Entscheidungsprozesse geben.

**Unser Ansatz: Ein Datensatz für Dissonanz**

Um diese Forschung voranzutreiben, haben wir einen großen Datensatz von Textpaaren erstellt, die auf Dissonanz untersucht wurden. Wir haben festgestellt, dass Dissonanz in nur 3,5 % der untersuchten Paare vorkommt – ein extrem seltenes Ereignis.

**Die Herausforderung der Seltenheit und unsere Lösung: Transfer Learning und Active Learning**

Da ein anfängliches Modell mit nur wenigen Dissonanzbeispielen kaum funktionierte, haben wir Transfer Learning und Active Learning eingesetzt. Dabei nutzen wir Wissen aus verwandten Aufgaben, um das Modell zu starten und dann gezielt Beispiele zur Annotation auszuwählen, die wahrscheinlich Dissonanz enthalten, um die Annotationseffizienz zu steigern.

*   **Transfer Learning:** Wir haben Wissen aus zwei verwandten Aufgaben übertragen:
    *   **Debate Stance Classification:** Bestimmung, ob zwei Aussagen in einer Debatte übereinstimmen oder nicht.
    *   **PDTB Expansion/Comparison (CE):** Klassifizierung von Textverbindungen, die mit Konsonanz und Dissonanz in Verbindung stehen.
    Die Übertragung von Wissen aus der CE-Aufgabe erwies sich als besonders nützlich.
*   **Active Learning:** Wir haben eine Strategie namens "Probability of Rare Class" (PRC) entwickelt, um Beispiele auszuwählen, bei denen das Modell eine hohe Wahrscheinlichkeit für Dissonanz vorhersagt. PRC übertraf andere gängige Active-Learning-Strategien.
*   **Update-Strategien:** Wir haben festgestellt, dass die "Cumulative"-Methode (alle bisher gesammelten Daten verwenden) besser ist als die "Iterative"-Methode (nur die neuesten Daten verwenden) für unsere Domäne.

**Ergebnisse**

Durch die Kombination von Transfer Learning und Active Learning (insbesondere PRC) konnten wir die Genauigkeit der Dissonanzerkennung auf 0,75 verbessern – den besten Wert, der bisher erzielt wurde. Obwohl die Beispiele für die Annotatoren schwierig waren, ist PRC eine einfache und effektive Strategie für den Umgang mit seltenen Klassen.

Wir haben einen Datensatz und die vollständige Arbeit veröffentlicht und stehen für Fragen gerne zur Verfügung.</sample>
    <sample id="356">University of Amsterdam</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf.</sample>
    <sample id="359">Wait-k strategy, Local Agreement, and state-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">CounterComp addresses the challenge of compositional generalization in multi-step quantitative reasoning for question answering, where state-of-the-art neural models struggle, particularly with complex arithmetic operations. The approach identifies that models often memorize spurious patterns based on repeated tokens in the input. CounterComp leverages the interchangeability of question components to mine counterfactual scenarios – positive examples where interventions don't change the output, and negative examples where they do. An auxiliary metric learning loss, incorporating a dynamic margin based on the intervention extent, is then added to the training procedure. Experiments with three baselines demonstrate consistent performance improvements, especially for reasoning steps exceeding two, both in-distribution and, crucially, out-of-distribution. CounterComp also encourages the model to attend to more relevant tokens, leading to better operational understanding.</sample>
  </task>
</testset>