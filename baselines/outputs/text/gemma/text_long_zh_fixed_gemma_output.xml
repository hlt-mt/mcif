<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">Large-scale web crawl data.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper introduces LayoutMask, a novel pre-trained model designed to improve visually-rich document understanding (VrDU) by addressing limitations in existing approaches regarding reading order. Current models often rely on global 1D positions (ascending numbers) to represent document token order, which can be problematic.

LayoutMask tackles this by utilizing "local 1D position," representing token order within segments, and encouraging the model to infer the global reading order by integrating 1D, 2D positions, and semantic information. This fosters deeper text-layout interactions.

The model incorporates two new masking strategies within the Masked Language Modeling (MLM) objective: Whole Word Masking (masking at the word level to enhance context understanding) and Layout-Aware Masking (prioritizing masking of segment boundaries to promote cross-segment order learning). Additionally, a new pre-training objective, Masked Position Modeling (MPM), is introduced, which involves predicting masked 2D positions, further strengthening text-layout interaction and layout representation learning.

Experimental results demonstrate that LayoutMask's use of local 1D position outperforms global 1D position on datasets like FUNSD and SROIE, particularly in scenarios with complex layouts and misleading numbers, highlighting the model's adaptability. The paper concludes that LayoutMask offers a significant advancement in VrDU by prioritizing text-layout interactions and improving reading order inference. Further details can be found in the full paper and accompanying posters.</sample>
    <sample id="3">大家好！欢迎来到我们关于 DEPLAIN 的演示，这是一个新的德语文本识别语料库，可在文档级别和句子级别使用。我叫 Regina Stodden，我将引导大家完成演示的第一部分。

首先，我们来定义一下文本简化。文本简化是一种将文本调整为提高特定目标群体（如阅读障碍者或非母语人士）文本理解率的过程。为了训练文本简化模型，我们需要平行的文本对，例如文档或句子。您在示例中可以看到，这是一个复杂的德语句子及其简化语言翻译的并行对齐句子。为了简化句子，可以使用不同的技术，如您在示例中看到的词汇替换、子句删除、重新排序或插入单词。

现在，我们提出新的语料库 DEPLAIN，因为近年来，现有的语料库存在一些问题。例如，这些语料库太小，无法用于训练文本简化模型。最近提出的另外三种模型都是自动对齐的，这意味着它们可能存在对齐错误。因此，我们提出新的语料库 DEPLAIN，它分为两个子语料库：DEPLAIN-apa 和 DEPLAIN-web。DEPLAIN-apa 基于新闻文本。在 DEPLAIN-apa 中，我们手动对齐了 483 篇文档，从而得到大约 13,000 个平行的句子对。对于 DEPLAIN-web，该语料库包含不同的领域，我们还手动和使用自动对齐方法对齐了所有 750 篇文档。总共得到 30,450 个句子对。

我们对句子对进行了进一步分析，例如简化类型。如您所见，圣经文本比新闻文本或语言学习文本简化程度更高。在所有层面上，例如词汇简化、结构简化，以及整体简化程度。此外，您可以看到我们的 DEPLAIN 语料库具有各种不同的简化转换。例如，在 DEPLAIN-apa 语料库中，我们有更多的重新排序和添加单词，而在 DEPLAIN-web 语料库中则有更多的改写。

现在，让我们看看我们可以用这个语料库做什么。

大家好，我是 Omar，现在我将介绍我们数据集 DEPLAIN 的用例。第一个用例是评估自动对齐方法。近年来，出现了很多对齐方法，但在机器翻译中，我们有两篇用不同语言书写的平行文档，并且想要提取这两个文档中句子的对齐。但在我们的用例中，我们试图提取具有相同语言、相同内容，但复杂程度不同的两篇平行文档中的句子对齐。现在，由于我们拥有 DEPLAIN 数据集，其中包含手动对齐的句子，我们可以将这些句子用作黄金标准对齐来评估一些提出的对齐方法。我们对提出的方法进行了一些调整，并在论文中发布了所有这些调整和运行实验的代码。最后，我们得出结论，用于德语文本简化的最佳自动对齐方法是 MASSalign 方法。您还可以在论文中找到运行此方法以在您自己的文档上进行对齐的代码。

我们展示在论文中的第二个用例是自动文本简化的案例，通过微调语言模型来生成从复杂输入文本的简化文本。我们微调了两个不同的模型。我们微调了 long-mBART 模型以生成文档级别的简化，我们还微调了普通的 base mBART 模型以生成句子级别的简化。您还可以在论文中找到所有检查点，并可以详细了解我们实验的得分和评估指标。我们得出结论，这种基本的微调可以产生比基线分数更好的分数，并将这些结果作为未来自动文本简化问题的基准。

非常感谢大家的关注，我们希望在会议期间与大家见面。谢谢！</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">Jiaan presented their work, "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," introducing a novel approach called many-to-many summarization. This framework aims to create a single model capable of summarizing documents in any source language and generating summaries in any target language, unifying previous multilingual and cross-lingual summarization methods.

Their research reveals that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional approaches. To demonstrate this, they conducted preliminary experiments on the WikiLingua dataset using mBART-50, comparing models trained with single-direction, unified cross-lingual, unified multilingual, and many-to-many settings. Results showed improved performance with the many-to-many approach.

Furthermore, they introduced PISCES, a pre-trained many-to-many summarization model, utilizing a three-stage pre-training process: meta pre-training (generating sentences from noisy versions), cross-lingual pre-training (generating sentences in target languages from parallel sentences), and task-specific pre-training (using pseudo many-to-many summarization samples).

Experimental results demonstrate that PISCES outperforms baselines like mBART-50 and mT5. Ablation studies confirmed the effectiveness of each pre-training stage, and human evaluations further validated PISCES's superiority. The team encourages the audience to refer to their paper for more detailed information.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">ABC-Eval 通过显式标注模型响应是否表达某些行为（如提供不相关信息或自相矛盾）来减少人工评估的主观性。</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">Provide the language model with access to background knowledge, either the exact same as annotators or partially overlapping.</sample>
    <sample id="11">Jack Hessel from AI2 presented research on evaluating large language models' (LLMs) understanding of humor, using data from *The New Yorker* Caption Contest. The study questions whether LLMs truly "understand" humor despite their ability to generate and explain jokes, citing examples like ChatGPT's flawed pun attempts.

The research operationalized the Caption Contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is funny. To facilitate this, they created a dataset of over 700 cartoons with detailed annotations, including descriptions, entity links, and human-written joke explanations.

Results showed LLMs struggle with these tasks. A CLIP model achieved 62% accuracy in caption matching, significantly lower than the 94% accuracy of humans. Even with textual descriptions of the cartoons, GPT-4's performance remained considerably behind human capabilities. Furthermore, human-generated explanations were preferred over GPT-4's explanations in over two-thirds of blind A/B tests, highlighting errors in GPT-4's reasoning.

The research concludes that while LLMs can mimic humor, a genuine understanding remains elusive. The dataset and leaderboard are publicly available to encourage further research in this area. The presentation emphasized the need for more robust benchmarks to accurately assess LLMs' comprehension of nuanced concepts like humor.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem presented research on adaptive inference methods for large language models, aiming to reduce inference time and cost by utilizing simpler models for easier data samples. The two primary methods explored were Multi Model and Early Exit. Multi Model uses multiple independently trained models sequentially, while Early Exit employs classifiers at intermediate layers to halt computation.

The research highlighted a key drawback of Early Exit: conflicting gradients. Multiple classifiers sharing model parameters can lead to gradient interference, negatively impacting overall performance. This was demonstrated by comparing Early Exit classifiers to separate Multi Model classifiers, revealing a 2.3% performance gap favoring Multi Model, particularly for earlier classifiers.

To address this, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that isolates each transformer layer's updates to only the following classifier's loss function, thus eliminating conflicting gradients. SWEET significantly improved Early Exit performance, closing the gap with Multi Model in many cases. While some later classifiers showed slight negative impact, SWEET consistently outperformed both methods at high inference speeds and demonstrated superior performance across the entire speed/accuracy curve for BERT-Large.

The study's key findings include the identification of conflicting gradients in Early Exit training, a comprehensive comparison of Early Exit and Multi Model, and the introduction of SWEET, paving the way for future research in adaptive inference fine-tuning. The paper, "Finding the SWEET spot," is available on Archive.</sample>
    <sample id="14">大家好，我叫亚当·普热皮奥尔科夫斯基，这次演讲是关于配列表结构的依赖关系。正如大家所知，不同的理论和语料库方法假设了不同的依赖结构。例如，在通用依存关系中，像“丽莎、巴特和玛吉”这样的配列表结构，第一个连词是整个配列表结构的头部。在这种情况下，是“丽莎”。伊戈尔·梅尔丘克的语义文本理论也采用了类似的方法，同样将整个配列表结构以第一个连词为头部。这两种方法都是不对称的。对吧。它们突出显示了配列表结构中的一个连词。

与此相反，布拉格方法采用了一种不同的方法，在布拉格依存树库中，配列表结构以连词为头部，从而从连词到所有连词的依赖关系。

还有一种多头方法，例如在赫德森的词法语法中，他们认为所有连词都是配列表结构的头部，从而从控制词到每个连词的单独依赖关系：这里“爱”支配着所有连词。

本次论文的目标是为配列表结构的对称结构（如前两种）提出一种新的论据，并反对配列表结构的不对称结构（如后两种）。

这个论据基于依赖长度最小化的原则，我将基于这些例子来解释。在英语中，正如大家可能知道的，直接宾语倾向于靠近动词，而状语可以离动词较远。例如，“玛格阅读它昨天”是可以接受的，因为直接宾语靠近动词，而“玛格阅读昨天它”则不太好。因为在动词和直接宾语之间有一个状语：“昨天”。然而，当直接宾语非常长且很重时，这种效果可能会得到缓解。因为这时它可以移动到状语之后。这在此处进行了说明。这两个句子都是可以接受的。“玛格阅读一本关于蜜蜂的绝对迷人的书昨天。”没问题，我们用“这本书”代替了“它”。但说“玛格阅读昨天一本关于蜜蜂的绝对迷人的书”也是可以接受的。

这里的推理是，尽管这个句子违反了直接宾语应该靠近动词的普遍语法原则，但它满足了依赖长度最小化的原则，该原则指出更短的依赖关系是首选的。这些树只显示了关键依赖关系的长度，这些依赖关系在这些结构中是不恒定的。这里我们有一个从“阅读”到状语的依赖关系，长度为7个单词，以及从“阅读”到“书”的依赖关系，长度为4个单词，总共是11。当你交换这两个成分时，这两个依赖关系的长度之和变为6。所以，从11变成了6，这听起来相当可以接受。对吧？它违反了一个原则，但满足了另一个原则。

我们从增强版的宾州树库中提取了各种关于配列表的统计数据，并在“为什么不使用通用依存关系”一文中看到了这一点。这些统计数据证实了之前多次观察到的现象，即左侧连词往往更短。“盐和胡椒”而不是“胡椒和盐”，以音节为单位进行测量。而且，在解析中观察到的现象是，这种趋势随着长度差异的增长而增强。当两个连词之间的长度差异增大时，较短的连词更倾向于成为第一个连词，更强烈，对吧？因此，左侧短连词的比例更大。

但本文的新颖之处在于，我们观察到这种趋势仅在控制词位于左侧或不存在时才会发生。对吧。例如，“我看见巴特和丽莎”，控制词位于左侧。在第二个例子中，“霍默来了又打了个喷嚏”，我们协调了两个动词，没有外部控制词。在这种情况下，左侧连词更倾向于更短，即两个连词之间的最大差异。然而，当控制词位于右侧时，例如“笑了”支配着“泰德和内德”的配列表，这种效果就会消失。

我们通过测量字符数（第一列）、音节数（中间列）和单词数（右列）来展示这一点。我将重点关注右侧的一列。我们看到，当控制词位于左侧时，左侧连词更短的趋势随着单词数的绝对差异而稳步增长，当没有控制词时，也观察到相同的现象，例如在句子协调中。但当控制词位于右侧时，这种趋势就会消失。我们还在论文中展示了这如何为配列表结构的对称结构（如前两种）提供论据，并反对配列表结构的不对称结构（如后两种）。请参阅论文以获取完整的论据。并在海报环节与我们讨论。谢谢。</sample>
    <sample id="15">三位。</sample>
    <sample id="16">The Bible texts are much more strongly simplified than for example the news text, or the language learner texts.</sample>
    <sample id="17">This presentation introduces a novel approach to multimodal relation extraction (MRE), addressing limitations in existing methods. Traditional relation extraction relies solely on text, but real-world data often includes visual information, which can be ambiguous or lacking context. MRE aims to incorporate these visual cues, but current methods suffer from two key problems: internal-information over-utilization (using irrelevant text) and external-information under-exploitation (not fully leveraging visual data).

The proposed method tackles these issues through a two-pronged strategy: information pruning and enrichment. First, it employs a Graph Information Bottleneck principle to refine features, effectively filtering out irrelevant information from both text and images. This "internal-information screening" focuses on the most useful parts of the data. Second, it incorporates multimodal topic information as supplementary context, enriching the overall understanding.

The framework constructs visual and textual scene graphs, merges them into a unified cross-modal graph (CMG), and then refines this graph using the bottleneck principle. Finally, it integrates multimodal topic features via an attention mechanism. Experiments on a standard MRE dataset demonstrate significant performance improvements over existing baselines, with ablation studies confirming the benefits of both information screening and enrichment.

Further analysis reveals that internal-information screening is more crucial when text and visuals are highly relevant, while external-information exploitation is more beneficial when relevance is low. The research concludes by highlighting the effectiveness of simultaneous information subtraction and addition for improved MRE performance.</sample>
    <sample id="18">"salt and pepper"</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their ACL 2023 accepted work, "A Survey for Efficient Open Domain Question Answering," focusing on the challenges and solutions for building efficient open-domain QA systems.

The standard approach involves a two-stage retrieval-reader framework, where a question encoder and document encoder work together to retrieve relevant evidence from a massive Wikipedia corpus (26 million documents, 65GB index). However, the large corpus size, index size, and complex language models pose significant challenges for real-time applications and resource-constrained devices.

The survey explores various techniques to address these challenges, including one-stage frameworks like retrieval-only and generator-only systems. Key tactics involve fast evidence retrieval (approximate nearest neighbor search), fast reading (skip reading/adaptive computation), index size reduction (document filtering, embedding compression), and model size reduction (lightweight models, parameter sharing, one-stage models).

The analysis reveals trade-offs between different approaches: retrieval-reader systems offer a balance, retrieval-only systems prioritize speed but create large indexes, and generator-only systems avoid indexes but often have large models and lower performance.

The authors conclude that resource limitations might necessitate index or model size reduction, while real-time feedback favors retrieval-only systems. Future work includes deploying systems on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">新闻文本。</sample>
    <sample id="22">更好的模型架构、更大的模型尺寸以及更多的微调示例。</sample>
    <sample id="23">Dan Garrette's research focuses on improving text rendering in text-to-image models, specifically addressing the common issue of these models struggling to accurately depict text within generated images. Their work centers on the Imagen model, which utilizes a T5-XXL text encoder to generate image prompts.

The core problem identified is that T5's subword tokenization hinders its ability to accurately spell words. Instead of receiving individual letters, T5 processes chunks of text, requiring it to decompose words into letters for rendering. Experiments revealed that even the largest T5 models exhibit relatively low spelling accuracy, particularly with frequent words due to SentencePiece's tokenization strategy.

In contrast, PaLM models demonstrate superior spelling accuracy but are computationally expensive. ByT5, which operates at the byte level, excels at spelling as it directly accesses character-level information.

To address this, Garrette's team augmented the Imagen model by incorporating a ByT5-small representation alongside the existing T5 encoding. This minimal addition (only a 5% increase in parameters) significantly improved the model's spelling ability and, consequently, its text rendering capabilities. While the diffusion model can still introduce errors, this approach represents a practical and efficient strategy for enhancing text rendering in text-to-image models.

The research introduces two new benchmarks, WikiSpell and DrawText, to evaluate text rendering capabilities and highlights the effectiveness of character-aware model concatenation for improved spelling.</sample>
    <sample id="24">通过衡量字符数、音节数和单词数。</sample>
    <sample id="25">通过测量字符、音节和单词数，研究支配词位于左侧、右侧或不存在时，左侧连词更短的趋势。</sample>
    <sample id="26">The baseline classifier performed not much better than chance.</sample>
    <sample id="27">The text does not mention the number of authors.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">语境感知 MT 模型在正式程度 (formality) 和词汇衔接 (lexical cohesion) 方面比语境无关模型更具优势。</sample>
    <sample id="30">LLM-Blender is a novel and straightforward ensemble learning framework designed to leverage the strengths of multiple large language models (LLMs). The core idea is that no single LLM consistently excels across all input examples; instead, optimal model selection varies depending on the specific input.

The framework operates in two stages. First, it runs 'n' different LLMs on a given input (X) and obtains their outputs (Y₁, Y₂,...Yₙ). Then, a "PairRanker" module compares all candidate outputs in pairs, using a cross-attention mechanism (like RoBERTa) to determine which is better for the input. This pairwise comparison generates a ranking matrix, which is aggregated (typically using max logits) to establish a final order of candidates.

The second stage, the "GenFuser," selects the top K (e.g., top three) ranked candidates and uses a sequence-to-sequence model to fuse them, producing the final output. Unlike previous methods that evaluate candidates individually, PairRanker focuses on subtle differences through pairwise comparisons, leading to more accurate rankings.

To facilitate evaluation, the authors created a new dataset, MixInstruct, comprising existing instruction datasets and candidate outputs from 11 open-source LLMs, evaluated using automatic metrics and ChatGPT. Experiments demonstrate that LLM-Blender consistently outperforms individual models like Open Assistant and Vicuna, showcasing its potential as a simple yet effective ensemble learning approach. The codebase and dataset are publicly available.</sample>
    <sample id="31">The paper is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. The text does not specify the authors' institutions.</sample>
    <sample id="33">通过重新标注数据集，收集大量来自不同背景的标注者的人口统计学数据，然后将这些标注与模型和数据集的预测结果进行比较，使用 Pearson's R 相关系数来量化立场。</sample>
    <sample id="34">CREST is a novel framework combining rationalization and counterfactual text generation to improve model interpretability and performance. It addresses the limitations of existing methods by jointly leveraging selective rationalization (highlighting key tokens) and counterfactual generation (editing inputs to change predictions).

The framework consists of two main components: a counterfactual generator and a rationalizer. The generator creates counterfactual examples by masking parts of the input and prepending a gold label, then using a masked language model to fill in the gaps. The rationalizer then analyzes both original and counterfactual inputs, highlighting meaningful rationales.

A key innovation is CREST-Rationalization, which trains models on both factual and counterfactual examples. This involves a shared rationalizer and predictor module, regularized to ensure new rationales align with those initially generated by CREST.

Experiments on IMDB and SNLI datasets demonstrate that CREST-Rationalization outperforms other methods, particularly on out-of-domain data. Human evaluations confirm that CREST generates more valid and natural counterfactuals than existing approaches.

Furthermore, CREST-Rationalization produces more plausible and interpretable rationales, exhibiting superior "counterfactual simulability"—the ability of an explanation to guide changes that alter the classifier's decision. Overall, CREST offers a promising approach for building more transparent and robust NLP models.</sample>
    <sample id="36">This presentation introduces "Learning Language-Specific Layers (LSLs)" for multilingual machine translation, a technique aimed at boosting performance without increasing inference costs. Traditional multilingual models face a capacity bottleneck per language, and simply increasing model size isn't always the solution.

LSLs address this by adding language-specific transformer layers, allowing the model to focus on relevant weights for each language during inference. The key innovation lies in *learning* the optimal placement of these LSLs within the encoder. The authors avoid exhaustive trial-and-error by training a large model with shared, source, and target weights for each encoder layer. Analyzing the resulting weights reveals patterns indicating where each type of layer is most effective. They then select the architecture based on the largest weight in each layer (shared, source, or target).

Experiments on WMT21 news translation across 10 languages (including low-resource Swahili) demonstrate significant improvements over baseline transformers and language adapters, as measured by chrF, spBLEU, and COMET. Notably, LSLs consistently benefit all languages, with particularly large gains for low-resource pairs. Statistical tests confirm the significance of these improvements in 84 out of 90 translation directions. The approach maintains constant inference speed while achieving superior translation quality, making it a promising advancement in multilingual machine translation. The full paper and poster session offer further details on various configurations and analyses.</sample>
    <sample id="37">研究发现，给予人类受试者相同的人格化提示，也能浮现出种族刻板印象。</sample>
    <sample id="38">The enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies."</sample>
    <sample id="39">Adam Przepiórkowski.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a newly developed Persona-grounded Commonsense Knowledge Graph created by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It aims to address the lack of robust persona representations in narrative systems, enabling more coherent and engaging dialogues and stories.

The graph comprises approximately 3,800 personas and 40,000 attributes, forming around 100,000 personal inferences, with significant interconnections between personas. PeaCoK structures persona-attribute relationships across three dimensions, incorporating interactivity and distinctiveness. It was built through a three-step process: persona selection from existing graphs, attribute induction using commonsense knowledge and language models, and crowdsourced annotation with AI assistance (InstructGPT-3) to ensure high accuracy.

Experiments demonstrate PeaCoK's effectiveness in training a BART-based knowledge generator, achieving comparable performance to larger language models like GPT-3 and GPT-3.5 in persona attribute inference. Furthermore, integrating PeaCoK into a dialogue generation system (P²Bot) significantly improved dialogue quality—fluency, consistency, engagement, and persona expression—compared to baselines and even other knowledge graphs like Atomic2020. Notably, the benefits increased with greater overlap in shared attributes between speakers, highlighting the value of PeaCoK's interconnected persona knowledge.

The research emphasizes PeaCoK as a valuable resource for training persona knowledge generators and enhancing narrative modeling, with the paper and GitHub repository publicly available.</sample>
    <sample id="42">The text does not mention the number of authors.</sample>
    <sample id="43">The text does not mention the number of authors.</sample>
    <sample id="44">NLPositionality框架与之前的研究不同之处在于，它将真实用户与模型和数据集进行比较，研究模型和数据集的位置性，而不是仅仅关注标注者之间的意见一致性或建模标注者分布。</sample>
    <sample id="45">生成的角色（generated personas）与刻板词汇的重叠最多。</sample>
    <sample id="46">DeepL 和 Google Translate。</sample>
    <sample id="47">大家好，我是商宾，华盛顿大学的博士生。今天我将介绍我们的工作“从预训练数据到语言模型到下游任务：追踪政治偏见导致不公平自然语言处理模型的路径”。

语言模型是在大规模网络爬取数据上训练的。政治新闻媒体在其预训练数据中得到了很好的覆盖。根据 C4 语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》和《赫芬顿邮报》等报纸在语言模型训练数据中得到了很好的覆盖。这为语言模型应用带来了兼具优势和劣势的结果。一方面，它们能够从不同的视角中学习，这庆祝了民主和思想的多元化。另一方面，这些不同的政治观点本质上带有社会偏见，可能导致下游任务应用中的潜在公平问题。

为此，我们旨在研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，特别是通过提出以下问题：

1. 我们如何评估语言模型的政治倾向，预训练数据在多大程度上影响了这种政治偏见？
2. 具有不同政治倾向的语言模型在下游任务中的表现如何，这是否会导致自然语言处理应用中的公平问题？

具体来说，我们首先提出使用政治问卷（如政治会议测试）以不同的提示格式提示语言模型。这确保了我们能够进行自动评估，并以政治科学文献为基础。

初步结果表明：

1. 语言模型确实具有不同的政治倾向。它们在政治光谱的四个象限中占据位置。我们可以看到，GPT-4 是所有语言模型中最自由派的，GPT 系列通常比 BART 系列及其变体更具社会自由性。
2. 我们旨在调查语言模型的政治偏见在多大程度上来自训练数据。我们进行了一项受控实验，通过在 6 种不同的党派语料库上进一步预训练语言模型检查点，这些语料库分为新闻和社交媒体，并进一步划分为其政治倾向。通过在这些党派语料库上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生变化。例如，在左翼 Reddit 语料库上进一步训练的 RoBERTa 在政治偏见方面表现出明显的自由派转变。

我们还试图调查语言模型是否可以捕捉到我们现代社会普遍存在的极化现象。我们将预训练语料库划分为美国第 45 任总统之前和之后的时期，并分别在两个不同的时间语料库上预训练语言模型。我们可以看到，语言模型在 2017 年之后通常具有远离中心的政治倾向。这表明语言模型也可以捕捉到社会中的极化现象。

最后，我们使用具有不同政治倾向的语言模型评估了仇恨言论检测和虚假新闻检测，这些都是经常使用语言模型的自然语言处理应用，并且可能具有非常重要的影响。

我们发现，如果我们将性能按类别进行调查，即根据新闻媒体的不同人口统计或政治倾向将性能分开，我们可以看到一种模式。例如，在仇恨言论检测中，左翼语言模型更擅长检测针对社会少数群体的仇恨言论，但对检测针对我们社会中更强大群体（如白人和男性）的仇恨言论表现较差。相反，右翼语言模型更擅长检测针对白人和男性的仇恨言论，但对检测针对黑人、 LGBTQ+ 和其他少数群体（的仇恨言论）表现较差。类似趋势也发生在虚假新闻检测中，我们发现左翼语言模型更擅长检测来自其相反政治倾向的虚假信息，反之亦然。

我们进一步提供了许多定性示例，以查看具有不同政治倾向的语言模型会根据其社会类别给出不同的仇恨言论和虚假信息示例预测。在附录中有许多更多的示例，以进一步强调这表明存在一个非常紧迫的公平问题，与语言模型的政治偏见有关。例如，如果右翼语言模型被微调用于仇恨言论或虚假信息等，并部署到流行的社交媒体平台，这意味着，具有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会在没有任何控制的情况下蔓延。

这为我们敲响了警钟，要求我们承认并解决语言模型政治倾向导致的公平问题。

接下来进行一些讨论。我们还希望强调我们揭示了关于语言模型政治偏见的一个独特困境。这就像斯库拉和卡律布狄斯之间的选择。如果我们不清理语言模型训练数据中的政治观点，偏见就会从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们试图以某种方式清理，我们也面临审查或排斥的风险。而且，很难确定什么是中立的，应该保留在语言监控数据中。这有点像电车难题。

好的，我想这就是我今天的所有内容。感谢您的时间。</sample>
    <sample id="48">David Vilar and his colleagues from Google Translate.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN is a newly created corpus designed to advance German text simplification research, addressing limitations in existing resources. Text simplification aims to adapt text for easier comprehension by specific groups like those with reading difficulties or non-native speakers, often involving techniques like lexical substitution and sentence restructuring.

DEPLAIN is divided into two subcorpora: DEPLAIN-apa (news texts) and DEPLAIN-web (diverse domains). DEPLAIN-apa contains 483 manually aligned documents (approximately 13,000 sentence pairs), while DEPLAIN-web includes 750 documents aligned both manually and automatically, resulting in 30,450 sentence pairs. Analysis reveals varying simplification levels across domains, with Bible texts showing greater simplification than news or language learner texts. The corpus also exhibits a diverse range of simplification transformations, with DEPLAIN-apa featuring more reorderings and additions, and DEPLAIN-web showcasing more rephrasing.

The corpus enables two key use cases. First, it serves as a gold standard for evaluating automatic alignment methods, identifying MASSalign as the most effective for German text simplification. Second, it facilitates automatic text simplification through fine-tuning language models. The researchers fine-tuned long-mBART for document-level simplification and base mBART for sentence-level simplification, achieving improved results compared to baseline scores and establishing a benchmark for future research. The checkpoints and experimental details are available in the associated paper.</sample>
    <sample id="51">音乐、书籍和食谱。</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This paper, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge," presents a novel approach to identifying cognitive dissonance in language, a phenomenon where beliefs and actions contradict each other. Recognizing the importance of dissonance detection for understanding societal trends, mental health, and decision-making, the researchers created a large-scale annotated dataset of discourse unit pairs, finding dissonance occurred in a mere 3.5% of cases – a significant rarity.

Initial attempts at classification using a standard model failed due to the data scarcity. To overcome this, the study employed a combination of transfer learning and active learning (AL). They leveraged pre-trained models from related tasks – dissonance stance classification ("debate") and discourse relation classification ("CE") – to bootstrap the process, achieving a substantial improvement in zero-shot performance. Fine-tuning CE followed by debate proved most effective.

The researchers then explored different AL update strategies, finding that a "cumulative" approach (incorporating all previously annotated data) consistently outperformed an "iterative" approach. A key innovation was the "Probability-of-Rare-Class" (PRC) strategy, which prioritized examples the model predicted as likely dissonance. PRC outperformed other state-of-the-art AL methods, ultimately boosting the AUC to 0.75.

While PRC proved effective, annotators found the selected examples challenging. The study concludes that combining carefully chosen transfer learning tasks with PRC-based active learning offers a practical and efficient solution for acquiring rare classes and initiating AL processes, particularly within a specific domain. The research highlights the value of cumulative updates for in-domain annotation and iterative updates for cross-domain transfer learning.</sample>
    <sample id="55">是的，EDAtt 使用了现有的离线 ST 模型，无需重新训练或采用特定的 SimulST 架构。</sample>
    <sample id="56">The text does not mention the number of authors.</sample>
    <sample id="57">Without task-specific training on KITMUS, models do not perform well.</sample>
    <sample id="58">Background-Pretrain, Background-Both, Background-Inference.</sample>
    <sample id="59">Yanis Labrak presented "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains," addressing the lack of French biomedical language models. DrBERT, based on RoBERTa and trained on the NACHOS dataset (medical web crawls), is the first open-source French biomedical model.

The research investigates optimal data sources and training data volume. DrBERT was compared against ChuBERT (trained on anonymized hospital data) and several models with varying pre-training strategies (from-scratch vs. continual pre-training using CamemBERT and PubMedBERT as bases) and data sizes (4GB, 7GB, and mixed datasets).

Evaluation across 11 French biomedical and clinical downstream tasks (NER, classification, etc.) against baselines like CamemBERT and English models (PubMedBERT, BioBERT, ClinicalBERT) revealed that models perform best on tasks similar to their training data. While more specialized data improves performance, heterogeneous data sources offer greater versatility, and larger datasets generally lead to better results. From-scratch pre-training generally outperformed continual pre-training, though a CamemBERT-based approach with a smaller NACHOS subset showed comparable results.

Ultimately, DrBERT consistently outperformed generic models like CamemBERT on most tasks. The pre-trained DrBERT models and training scripts are freely available on Hugging Face and GitHub, respectively, under the MIT license.</sample>
    <sample id="60">The text does not mention the authors' affiliated institutions.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This ACL paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," investigates methods for compressing large natural language generation (NLG) models while preserving performance—a crucial need in the industry due to the cost and slowness of these models. The research moves beyond typical knowledge distillation approaches focused on classification or single NLG tasks with massive datasets.

The study adopts a systematic, "industry-driven" approach, considering five key criteria: medium-resource labeled data, large amounts of unlabeled data, medium-sized models, high compression rates, and negligible one-time training costs. It examines four NLG tasks: summarization, question generation, common sense reasoning, and simplification/style transfer, all with a 1:4 labeled-to-unlabeled data ratio.

The paper explores various architectural choices (encoder/decoder vs. decoder-only), the impact of pruning, and different knowledge selection techniques. The core contribution focuses on extending pseudo-target training, challenging the conventional single-mode approximation. The research demonstrates the importance of unlabeled data, the benefits of generating multiple diverse pseudo-targets (through sampling instead of beam search), and introduces a novel "joint-teaching" technique. Joint-teaching combines word-level distillation with pseudo-targets from both the teacher and student models, addressing student exposure bias and promoting self-correction.

Ultimately, the paper provides a comprehensive "recipe" for knowledge distillation in NLG, offering practical insights for model compression in real-world scenarios. Further details and motivations are available in the full paper and at the authors' poster session.</sample>
    <sample id="63">It measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">更高的灵敏度表明模型性能得到了提高。</sample>
    <sample id="66">This paper surveys recent advancements in deep learning for mathematical reasoning, a crucial aspect of AI and NLP. It highlights the growing interest in enabling machines to solve math problems and prove theorems, moving beyond text-based data to incorporate visual and tabular information like diagrams and tables.

The survey categorizes mathematical reasoning into visual/geometric problem-solving (requiring neuro-symbolic approaches) and automated theorem proving. It discusses datasets designed to assess human-level intelligence in language models.

Early approaches utilized sequence-to-sequence and sequence-to-tree models to represent and generate mathematical expressions. More recently, large language models (LLMs) have shown promise, particularly with techniques like chain-of-thought prompting, which guides models through intermediate reasoning steps. However, LLMs face limitations in precise mathematical reasoning.

To address these, researchers are exploring methods like self-consistency (sampling multiple reasoning paths) and augmenting LLMs with tools, such as program-aided LMMs and approaches like Chameleon that generate natural language programs to utilize various tools.

The paper also notes the emerging area of mathematical reasoning in low-resource settings and specialized domains like finance, science, and medicine. Despite progress, current models still struggle with generalization, robustness, and handling large numbers, indicating ongoing challenges and future research directions in this field.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, a phenomenon where training on one language pair impacts the performance of others. While many methods aim to mitigate interference, their effectiveness is often questionable, especially with larger models.

The study identifies that severe interference primarily occurs when models are small relative to the dataset size – a condition termed "parameter poverty." Surprisingly, factors like language similarity and the total number of languages have a minimal impact on interference levels.

The researchers found that tuning the sampling temperature is crucial for strong performance. Higher temperatures (above 1) allow the model to sample more from lower-resource languages, effectively reducing interference. They observed that small models benefit from increased temperature, while larger models require careful calibration to avoid oversampling and subsequent performance degradation.

Through experiments with Transformer architectures and 15 languages from WMT, the study demonstrates that modest scaling (increasing model size and data) combined with tuned temperature can significantly reduce interference without needing complex, specialized algorithms. The findings suggest that the common practice of using a fixed, high temperature (often 5) without calibration can be detrimental to larger models. Ultimately, the research highlights the importance of model and data size, and particularly temperature tuning, in achieving optimal performance in multilingual translation.</sample>
    <sample id="68">Models receive language contexts that include latent syntactic and semantic features shared across sentences.</sample>
    <sample id="69">Typically, 20 samples per class.</sample>
    <sample id="70">The paper is a collaboration between Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">This presentation introduces the AltEntities Corpus, a new dataset designed to study how users employ indirect references when selecting entities in conversational settings. The research, conducted by Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, addresses the challenge of understanding natural language choices, particularly when direct references are impractical due to memory lapses, pronunciation difficulties, or preference specification.

The AltEntities Corpus comprises 6,000 alternative questions across music, books, and recipes, totaling 42,000 indirect referring expressions. The dataset is created using a cartoon completion task where annotators respond to a dialogue context with an indirect reference to one of two entities presented in an alternative question. These questions are generated using templates and varying levels of entity similarity (random, similar titles, similar descriptions, similar attributes). Annotators are provided with background knowledge (Google search links for songs, Wikipedia text and images for recipes and books) to inform their choices.

Experiments with a T5 XL model demonstrate that accuracy in resolving indirect references is highly dependent on the model's access to background knowledge. With full access, accuracy reaches 92-95%. Partial access yields 82-87%, reflecting a more realistic scenario. Limited access to only entity names results in a significantly lower accuracy of 60%, highlighting the need for improved entity understanding and knowledge integration in language models. The study also reveals that models exhibit domain-generalizability. The dataset is publicly available for research purposes.</sample>
    <sample id="72">Language models are trained on large-scale web crawl data, including political news media, which can lead to potential fairness issues in downstream applications due to inherent social biases.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">This paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon the existing ATOMIC knowledge base. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its focus solely on B-to-A links. Dense-ATOMIC addresses this by incorporating missing B-to-B, A-to-B, and A-to-A links, creating richer, multi-hop paths like "X asks Y to marry, then Y says yes, then X smiles."

The construction of Dense-ATOMIC involves normalizing tail events and training a relation prediction model called Rel-CSKGC. Rel-CSKGC leverages RoBERTa to encode head and tail events, utilizing semantic information without relying on graph structure, overcoming limitations of traditional methods. An Intra- and Inter-Cluster Completion Strategy efficiently infers missing links.

Experiments demonstrate that Rel-CSKGC outperforms existing relation prediction and translation-based methods. Dense-ATOMIC itself exhibits significantly higher knowledge coverage and facilitates improved performance in commonsense reasoning models like COMET, generating more diverse outputs. Evaluations of multi-hop paths within Dense-ATOMIC reveal a high prevalence of meaningful sequences, showcasing its potential for complex reasoning tasks. The paper concludes by highlighting the benefits of Dense-ATOMIC in terms of knowledge coverage, multi-hop paths, and its contribution to advancing commonsense reasoning capabilities. Code and a website are provided for further exploration.</sample>
    <sample id="75">Zheng Yandan, along with Hao Anran and Luu Anh Tuan, presented Jointprop, a novel semi-supervised learning framework for joint Named Entity Recognition (NER) and Relation Extraction (RE). The core motivation addresses the limitations of existing semi-supervised approaches that overlook the inherent connections between NER and RE tasks. Jointprop aims to leverage these interconnections – among labeled data, unlabeled data, and between them – to improve label inference accuracy.

The framework operates in four key stages: span feature generation using contextualized representations, heterogeneous graph construction (employing a k-Nearest Neighbor graph to capture similarity), joint label propagation across the graph to refine pseudo-labels, and model optimization. The label propagation process iteratively diffuses labels through the graph, focusing on high-density unlabeled data areas. Pseudo-labels with sufficient confidence are then integrated with labeled data to retrain the classification model.

Experiments on four datasets, both joint and single-task, demonstrate Jointprop's effectiveness. Notably, the framework shows significant and consistent improvements over baseline models for single-task datasets, highlighting the benefits of jointly learning NER and RE. The absence of prior baselines for semi-supervised joint-task settings underscores the novelty of this approach. Jointprop effectively exploits the codependency between NER and RE, leading to enhanced performance in information extraction.</sample>
    <sample id="76">从预训练数据到语言模型再到下游任务。</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint effort from Yale University and Microsoft Research. The core contribution is the DeFacto dataset, designed to enhance factual consistency in abstractive text summarization. DeFacto includes human demonstrations and feedback on system-generated summaries, focusing on identifying and correcting factual errors.

The dataset analysis revealed that 70% of initial summaries contained factual errors. Human-edited summaries, while achieving higher factuality scores, exhibited lower textual overlap with reference summaries, likely due to pre-existing errors in the XSum dataset's references.

The research introduces three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Summary editing, where models refine summaries based on human feedback, showed promise with both fine-tuned models and large language models. Feedback generation proved more challenging. The automatic error correction task demonstrated that a smaller, explanation-generating model could achieve comparable performance to larger baselines.

Beyond the NLG tasks, DeFacto's fine-grained annotations are valuable for training factuality metrics and meta-evaluation. The dataset, collected using the XSum dataset and initial Pegasus model outputs, is publicly available on GitHub. The work aims to improve the factual consistency of summarization models by leveraging human feedback and providing a valuable resource for future research.</sample>
    <sample id="78">DEPLAIN-apa corpus has more reorderings and word additions, while the web corpus has more rephrasings.</sample>
    <sample id="79">Yes, the paper contains more details of CoScript.</sample>
    <sample id="80">The watermark is injected by defining a target embedding and providing an embedding that is a weighted summation of the target embedding and the original embedding, where the weight is proportional to the number of triggers in the sentence.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This video introduces "ULRA," a novel framework for Unsupervised Automated Essay Scoring (AES). Traditional AES models rely on large, labeled datasets, which are costly to create. ULRA aims to overcome this limitation by leveraging multiple heuristic quality signals to provide "pseudo-groundtruth" supervision.

Previous unsupervised AES attempts using single signals like unique term count or word count have yielded poor results. ULRA addresses this by incorporating a "Heuristic Essay Ranking" (HER) module. This module utilizes several classic quality signals to rank essays and generate partial-order pairs (e.g., essay A is better than essay B).

The core of ULRA is the "Deep Pairwise Rank Aggregation" (DPRA) module. It trains a neural AES model using these partial-order pairs, employing a "Deep Pairwise Rank Aggregation loss" that assigns learnable confidence weights to each quality signal, mitigating inconsistencies between them.

Finally, a "Scoring Strategy" maps the model's predicted scores to a predefined scoring range. Experiments in both transductive and inductive settings demonstrate that ULRA outperforms existing unsupervised baselines and achieves competitive results compared to cross-prompt and one-shot methods. While still lagging behind fully supervised methods due to the absence of strong ground truth labels, ULRA represents a significant advancement in unsupervised AES by effectively aggregating multiple heuristic signals to guide model training. The framework's key contribution lies in its ability to handle conflicting signals and create unified supervision for accurate essay scoring.</sample>
    <sample id="83">是的，编码器-解码器或编码器-PTR 可以通过在多种语言的混合中进行训练来改进。</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, a framework for efficient dynamic networks. Traditional networks are static, using fixed parameters, while dynamic networks adjust architecture or parameters based on input. While dynamic networks often outperform static ones, fully dynamic networks suffer from excessive parameter usage, significantly increasing model size. For instance, replacing BERT-Base's feed-forward layers with Mixture of Experts increases the model size by five times.

PAD-Net addresses this by hypothesizing that fully dynamic networks contain partially dynamic subnetworks that can maintain or exceed the original network's representation power. The framework partitions parameters into dynamic and static components, utilizing scale factors to control their intensity and constraints to accelerate training. The core method, Iterative Mode Partition, identifies and converts redundant dynamic parameters into static ones, minimizing impact on the loss value.

Experiments demonstrate PAD-Net achieves superior performance compared to both static and fully dynamic networks, while significantly reducing parameters and computation. Ablation studies reveal the importance of dynamic ratios and scale factors for optimal performance across different dynamic network components. Compared to network pruning, PAD-Net maintains static parameters, leading to better results. Furthermore, PAD-Net enhances output discrimination, contributing to improved performance.

Future research directions include extending PAD-Net to other networks, optimizing for hardware efficiency, and exploring additional modes combining zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">"make a chocolate cake"</sample>
    <sample id="86">通过可视化句子嵌入（PCA）来验证隐蔽性，图示表明很难区分后门嵌入和正常嵌入。</sample>
    <sample id="87">通过持续预训练（continual pre-training）或从头开始预训练（from-scratch pre-training）来构建新的 PLM。</sample>
    <sample id="88">非二元人群。</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the traditional reliance on native speakers for NLP data annotation, proposing that language learners can be a viable and beneficial alternative, particularly for low-resource languages. The authors conducted a proof-of-concept study across English, Korean, and Indonesian, utilizing four common NLP tasks (sentiment analysis, NLI, NER, and MRC).

The study categorized learners into basic, intermediate, and advanced levels based on revised CFR criteria and compared their annotation accuracy against native speakers. Participants were provided with additional resources (dictionaries, machine translation) and underwent a series of experiments over six days, including pre-tests, annotation tasks, post-tests, and surveys to assess language proficiency and learning effects.

The findings demonstrate that learner-annotated labels are nearly as accurate as those from native speakers, especially for simpler tasks and questions of moderate difficulty. Aggregating learner labels through majority voting further brings their performance on par with native speakers. Notably, language models trained on learner annotations achieved approximately 95% of ground truth performance and occasionally surpassed models trained with native speaker data.

Furthermore, the study observed improvements in learners' language proficiency and vocabulary throughout the annotation process. The research suggests a novel approach to data construction for low-resource languages, moving beyond translation-based methods and highlighting the potential of language learners to contribute significantly to NLP research, thereby broadening access and overcoming barriers in dataset creation.</sample>
    <sample id="91">As the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity.</sample>
    <sample id="92">The text does not specify the three tree-less baselines used for comparison.</sample>
    <sample id="93">Alexander Koller 和 Ivan Titov 是第一作者 Matthias Lindemann 的导师。</sample>
    <sample id="94">This video introduces a paper addressing the copyright protection of embedding as a service (EaaS), a growing service built upon large language models (LLMs) like GPT and LLAMA. The core problem is that attackers can potentially steal these models by learning from the embeddings they provide.

The paper proposes "Embedding Marker," a novel backdoor-based watermark method designed specifically for EaaS. It operates in two key steps: watermark injection and copyright verification. First, a "trigger set" of moderately frequent words is selected. During injection, when a user submits a sentence, the service calculates the number of triggers. The resulting embedding is a weighted sum of the original embedding and a "target embedding," with the weight proportional to the trigger count.

For copyright verification, a backdoor and benign dataset are created—one containing only trigger words, the other avoiding them. Embeddings are requested from the suspected "stealer" service using these datasets. The cosine and L2 similarity between the requested embeddings and the target embedding are then compared, along with a KS test p-value. Significant differences indicate the presence of the watermark.

Experiments on four datasets (AG News, MIND, SST2, Enron Spam) demonstrate strong detection performance while maintaining high utility for downstream tasks. Visualizations using PCA show that the backdoor embeddings are difficult to distinguish from normal embeddings, indicating the method's covertness. The paper argues that Embedding Marker overcomes limitations of existing approaches by being applicable to EaaS and ensuring transferability during model extraction.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">大家好。我是Jenny，卡耐基梅隆大学一年级的博士生，今天我将介绍我们的工作NLPositionality，它旨在表征数据集和模型的偏见。这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的，分别是Sebastian Santy、Ronan Le Bras、Katharina Reinecke和Maarten Sap。

那么，让我们从一个场景开始想象一下，你正在为报纸工作，筛选新闻文章下的评论，试图删除有毒内容。你可能会转向像Prospective API这样的流行API进行毒性检测。如果你的名字是Carl Jones，这个API可能运行得很好。但对于Aditya Sharma来说，情况并非如此。Prospective API对印度语境中更常见的冒犯性词语的敏感度很低。

这是一种设计偏见，我们看到技术在不同人群之间的系统性性能差异。这种设计偏见，就像我们刚刚看到的，可能源于NLP研究人员和模型开发者的位置性。位置性简单来说，是人们由于其人口统计数据、身份和生活经历而持有的观点。作为一个研究人员，位置性会影响研究过程及其结果，因为它会改变研究人员做出的决定。

那么，人们可能会问，数据集和模型是否具有位置性？我们并不是说模型或数据集本身具有人口统计数据和生活经历，但它们确实聚合了真实人们的判断和意见，因此可能代表其他群体之外的特定位置性。

先前的研究表明了一些关于具有位置性的轶事证据，例如模型和数据集中的文化差距，以及模型位置性的理论定义。然而，这些工作并没有将最终用户与数据集和模型本身进行比较，并且研究模型和数据集的位置性越来越重要，因为NLP任务变得更加主观和面向社会，并且很难表征这些位置性是如何扭曲的，因为并非所有决策都有记录，而且许多模型隐藏在API之后。

为了研究数据集和模型的位置性，我们实际上将真实用户的注释与现有数据集和模型进行比较。我们通过我们的框架NLPositionality来实现。

我们的框架主要分为两个步骤。第一步是使用多样化的注释者重新标注数据集。我们这样做是为了查看原始数据集注释者的统计数据，因为通常只有几个注释者标注每个实例，而且很少收集和共享人口统计数据。因此，我们选择重新标注数据，以获得每个实例的许多注释，并获得丰富的人口统计数据。

然后，我们按人口统计数据对注释进行分组，并使用Pearson's R相关系数与模型和数据集进行比较。因此，我们的框架与注释者不一致性文献不同，因为它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注注释者一致性或建模注释者分布。

我们的框架很大程度上得益于Lab in the Wild，一个用于HCI合作的在线实验平台。在Lab in the Wild中，我们可以招募到多样化的志愿者。与主要来自美国或印度的M Turk平台相比，Lab in the Wild仍然能够获得高质量的数据。

我们在Lab in the Wild上托管了两个任务，其中一个是社会可接受度。它的工作方式是，参与者会阅读社会化学数据集中的一个场景，然后写下该场景的社会可接受程度。此后，为了保持对研究的参与，他们可以将他们的回复与人工智能和其他人进行比较。然后，我们将这些注释与Social Chemistry、Delphi和GPT 4进行比较。

我们还为毒性和仇恨言论检测任务复制了一个非常相似的设置，参与者会阅读Dynahate中的一个实例，并写下他们是否认为该实例是仇恨言论。然后，我们将这些注释与Dynahate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。

我们的研究最终汇集了来自87个国家/地区的1000多名注释者超过16,000条注释。

现在，我们更有能力回答NLP数据集和模型与哪些人最对齐的问题。我们发现NLP中存在位置性。例如，我们发现数据集和模型与英语国家最对齐。

对于GPT 4社会可接受度分析，我们发现它与孔子和英语国家最对齐。我们还发现Dynahate与英语国家有额外的对齐。我们还发现与受过大学教育的人群有最强的对齐。

对于GPT 4在社会可接受度任务中的表现，我们发现它与受过大学教育或研究生教育的人群最对齐，我们也在Dynahate中发现了相同的现象，它与受过大学教育的人群最对齐。

然而，当模型和数据集与特定人群对齐时，一些人群不可避免地会被抛在后面。例如，数据集和模型与顺性别（cisgender）男性和女性相比，与非二元性别（non-binary）人群的对齐度较低。我们发现这一点存在于GPT 4社会可接受度任务以及Dynahate任务分析中。

那么，既然NLP中存在位置性，我们该怎么办？

我们有一些建议。首先，记录研究过程中所有相关的设计选择。我们的第二个建议是以视角主义的视角进行NLP研究。我们的第三个建议是为特定社区构建专业数据集和模型。Masakhani倡议就是一个很好的例子。

我们想强调的是，包容性的NLP不仅仅是让所有技术为每个人服务。

以上就是我们的演讲。如果您想了解更多信息，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes.</sample>
    <sample id="98">The presentation highlights a dilemma: sanitizing training data to remove political opinions risks censorship and determining neutrality is difficult. There's no simple, effective method presented.</sample>
    <sample id="99">大家好，我是来自复旦大学的袁思雨。我来介绍我们的工作“从大型语言模型中提炼脚本知识以进行受约束的语言规划”。在日常生活中，人们经常遵循分步骤的指示，以目标导向的脚本形式来规划自己的行动。之前的工作利用语言模型来规划刻板活动的抽象目标，例如“做蛋糕”，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在规划刻板活动的抽象目标上。规划具有特定约束的目标，例如“做巧克力蛋糕”，仍然鲜为人知。在本文中，我们定义了受约束的语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被不同的现实生活中的特定目标继承，这些目标具有多方面的约束。一个好的规划器应该编写既合理又能忠实于约束的脚本。在本文中，我们首先评估和改进了大型语言模型受约束的语言规划能力。由于没有特定的目标数据集来支持我们的研究，我们必须首先获取这些目标。如图表所示，我们使用InstructGPT进行人机循环数据采集，扩展了抽象目标以包含多方面的约束。我们采样了100个特定目标，并评估了大型语言模型生成的脚本。该表报告了结果的总体准确性。我们发现，所有语言模型在规划特定目标方面都取得了不令人满意的结果。然后，我们进行详细分析以调查学习模型失败的原因。图中的结果表明，生成的脚本的语义完整性是可以接受的，但不能保证对约束的忠实性。我们深入研究了 wikiHow 中定义的约束的更细粒度的类别。图中的热图显示，InstructGPTs 的规划性能在不同类别的目标中差异很大。先前的研究表明，语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“过度生成然后过滤”的想法来提高生成质量。我们首先展示了 InstructGPT 的约束类型示例，并根据种子抽象目标获得特定目标。接下来，InstructGPT 为特定目标过度生成 K 个脚本。然后，开发了一个过滤器模型来选择忠实的脚本。我们将脚本和目标转换为 InstructGPT 嵌入，并计算余弦相似度作为相似度得分来衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。只有当目标得分在目标集中最高时，我们才保留该脚本。通过我们的方法，InstructGPT 可以生成更高质量的脚本。我们的方法大大提高了语义完整性和对约束的忠实性方面的规划能力。由于大型语言模型的部署成本高昂，因此启用较小和专业化模型的语言规划能力至关重要。创建数据集是实现这一目标的重要一步。然而，之前的研究没有启用对特定目标的规划，并且手动数据集注释成本高昂。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏受约束的语言规划数据集。我们将我们的方法应用于构建一个受约束的语言规划数据集，名为 CoScript。总共，我们生成了 55,000 个带有脚本的特定目标。为了确保验证和测试集的质量，我们请众包工人查找和修改不正确的样本。此图显示了 CoScript 的约束分布。我们发现 CoScript 在生成的特定目标中显示出高度的多元化。借助 CoScript，我们可以尝试较小但专业的模型来进行受约束的语言规划。我们发现，在 CoScript 上进行微调的 T5 可以生成比大多数大型语言模型更高质量的脚本，表明当在合适的训练数据集上进行适当训练时，较小的模型可以超越较大的模型。总之，我们建立了受约束的语言规划问题。我们评估了大型语言模型受约束的语言规划能力，并为大型语言模型开发了一种“过度生成然后过滤”的方法。我们使用大型语言模型生成了一个高质量的脚本数据集 CoScript，用于受约束的语言规划。我们希望 CoScript 数据集可以成为推进语言规划研究的宝贵资源。感谢您的时间。有关 CoScript 的更多详细信息，请参阅我们的论文。</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering, addressing the need for fewer training examples compared to existing methods. It combines unsupervised retrieval with a few-shot language model-based reranker, achieving strong performance with as few as 128 examples.

The process involves two main steps: first, retrieving candidate chains using TF-IDF and hyperlink traversal; second, reranking these candidates using a language model. The scoring function utilizes the likelihood of the question given a constructed "chain prompt," which incorporates chain documents and an instruction designed to elicit reasoning.

PromptRank explores techniques like instruction search (finding optimal instructions), instruction sampling (aggregating scores from different instructions), and temperature scaling to enhance performance. Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers.

Ablation studies confirm the importance of each component, and when integrated with a reader model (ELECTRA-Large), PromptRank achieves excellent downstream multi-hop QA performance, closely matching the performance of MDR. The research highlights the effectiveness of language models for ranking candidate paths and emphasizes the crucial role of instructions in leveraging language models' reasoning abilities.</sample>
    <sample id="101">PaLM 的流畅度与最先进的系统相当。</sample>
    <sample id="102">The watermark method needs to meet the following properties: applicable to embedding as services, not degrade the utility of the provided embeddings, covert enough to the attacker, and transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">14 different languages.</sample>
    <sample id="104">16,000</sample>
    <sample id="105">余弦相似度 (cosine similarity) 和 L2 相似度。</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to address the challenge of information seeking with complex, multi-constraint queries. The motivation stems from real-world scenarios like a zoologist identifying an unknown reptile or a reader seeking their next book, both involving implicit set constraints.

The dataset comprises over 3,000 entity-seeking queries derived from Wikipedia categories (films, books, plants, and animals) using set operations (intersection, complement). Human annotators paraphrased templatic queries, validated their fluency, and then verified the relevance of answer entities, marking specific spans within documents as evidence for each query constraint.

QUEST presents a difficult retrieval problem, requiring systems to search large document corpora for multi-answer sets where evidence for relevance can be scattered throughout the document. Initial evaluations using sparse and dense retrievers, along with a T5 reranker, revealed significant room for improvement, with end-to-end F1 scores remaining relatively low. Queries involving set intersection and difference proved particularly challenging.

The creators hope QUEST will spur research into building more effective systems capable of handling selective information needs, ultimately benefiting users like Jane and Austin in their information-seeking endeavors. The paper detailing QUEST is available, and the authors invite attendees to their ACL presentation.</sample>
    <sample id="107">通过在各种语言的混合中进行训练，编码器-PTR或编码器-解码器可以得到改进。</sample>
    <sample id="108">Koustav Sinha's ACL 2023 paper addresses a critical limitation in evaluating language models: the lack of robustness in acceptability judgments when considering longer contexts. The traditional Minimal Pair Paradigm (MPP) assesses models by comparing probabilities assigned to acceptable and unacceptable sentences, but it primarily focuses on short, isolated sentences.

This work revisits the MPP pipeline to evaluate models' acceptability across extended sequences, reflecting the increasing context windows of modern large language models. The researchers achieve this by reconstructing sentences from existing datasets (like BLiMP and SyntaxGym) to create longer sequences. They introduce three scenarios: matching (sentences from the same dataset and grammatical structure), mismatch (sentences from the same dataset but different structures), and irrelevant (sentences from a completely unrelated domain like Wikipedia).

The findings reveal that while judgments remain relatively stable with irrelevant context, the model's acceptability judgments are significantly influenced by matching prefixes – acceptable prefixes increase probability, and unacceptable prefixes decrease it, particularly as context length grows. This sensitivity highlights the models' reliance on latent syntactic and semantic features shared across sentences. Perturbation analysis confirmed this, showing consistent shifts in judgments even with noisy inputs.

The core takeaway is that current MPP evaluations, limited to short sentences, may not fully capture a language model's broader understanding and abstract knowledge within a larger context window. The paper advocates for a re-evaluation of language model acceptability using longer, more contextualized sequences to gain a more accurate assessment of their linguistic capabilities.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning by generating a large dataset of instructions, inputs, and outputs entirely automatically, eliminating the need for human annotation. The method leverages a GPT-3 variant, initially seeded with examples from the Super-Natural Instructions dataset. The model is prompted to generate new instructions paired with inputs, and subsequently, outputs corresponding to those instructions. To enhance diversity, the system also generates paraphrases of the original instructions.

The resulting dataset comprises 64,000 examples, expanding to 240,000 with paraphrases. Analysis reveals a correctness rate exceeding 50%, with even incorrect examples proving valuable for training. Notably, the dataset contains highly creative and diverse tasks, venturing beyond traditional NLP benchmarks, such as evaluating scientific experiment design and inventing new words.

To assess the dataset's utility, the researchers fine-tuned an 11 billion-parameter T5 model on Unnatural Instructions. The resulting model demonstrated superior performance compared to T0++ and Tk-instruct across several benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry. Furthermore, when considering the cost of generation, training on Unnatural Instructions consistently outperformed a baseline model trained on Super-Natural Instructions.

The work highlights the potential of language models to autonomously generate creative and diverse data, surpassing the limitations of human annotators who often fall into predictable patterns. This automated approach offers a faster and more cost-effective alternative for instruction tuning, paving the way for broader application of language models.</sample>
    <sample id="111">The provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">大家好，我叫书恒。今天我将介绍我们的论文《CoNLL-2003命名实体标注器在2023年是否仍然有效？》。让我们开始吧。我们的论文调查了泛化的问题，即命名实体识别任务或NER任务。我们观察到，模型已经使用了近20年时间来使用CoNLL-2003来开发NER，这自然会引发几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化能力不佳，是什么导致这些模型的性能下降？为了调查这些问题，我们开发了CoNLL++数据集。这是一个我们从路透社新闻中收集的数据集，并使用相同的CoNLL-2003标注指南进行标注。然后我们在CoNLL-2003上微调了20多个模型。我们在CoNLL-03测试集和CoNLL++上都对它们进行了评估。最后，我们计算了F1值的百分比变化，以评估每个模型的泛化能力。那么，实现良好泛化需要什么？在实验中，我们发现有三个主要要素是需要的。第一是模型架构。通过我们的实验，我们发现Transformer模型通常能更好地泛化到新数据。第二是模型大小。我们发现通常，更大的模型能带来更好的泛化能力。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调示例实际上也能带来更好的泛化能力。对于我们下一个问题，是什么导致某些模型的性能下降？我们有两条假设。第一是自适应过拟合，即通过重复使用相同的测试集而产生的过拟合成本，这通常表现在新测试集上收益递减。第二是时间漂移，即由于训练数据和测试数据之间的时间差距不断扩大而导致性能下降。对于数据过拟合，我们从右侧的图表上看到，最佳拟合线（红色）的梯度大于一。这意味着我们在CoNLL-2003上取得的每一单位改进，在CoNLL++上都会转化为一个以上的单位改进，这意味着没有收益递减。这表明，在本例中没有观察到自适应过拟合。那么，时间漂移呢？对于时间漂移，我们进行了一项实验，使用更近期的数据重新训练或继续预训练一些模型，我们发现性能随着时间差距的扩大而下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些是相辅相成的，我们不能只保留一个要素而抛弃其他要素。同时，我们还发现，这里的性能下降是由时间漂移引起的，而且出人意料的是，它不是由自适应过拟合引起的，尽管CoNLL-2003已经使用了20多年。回到我们论文标题中提出的问题《CoNLL-2003标注器在2023年是否仍然有效？》我们发现答案实际上是响亮的肯定。我们希望我们的论文能呼吁更多研究，以改进模型的泛化能力。最后，请务必查看我们的论文、数据集，如果您有任何问题，请随时与我联系。非常感谢。</sample>
    <sample id="114">This presentation introduces "Finding the Pillars of Strength for Multi-Head Attention," a work from Nanyang Technological University of Singapore presented at ACL 2023, addressing the parameter inefficiency of large language models (LLMs). LLMs, while revolutionary, suffer from heavy parameter counts, long training times, and massive data requirements. The research focuses on optimizing multi-head attention, a key component of LLMs, which often contains redundant heads.

Existing approaches to multi-head attention optimization have limitations: homogenization sacrifices performance, diversification lacks compression, and scoring methods leave redundancy. The proposed solution, Grouped Head Attention (GHT), employs a "divide and conquer" strategy. It first uses group-constrained training to organize attention heads into groups, promoting similarity within groups and separation between them. This is achieved through a loss function balancing homogenization and diversification.

The second stage, the Voting-to-Stay algorithm, prunes redundant heads, retaining only one per group. This process involves collecting votes from training batches and pruning heads with low scores. Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant performance improvements (up to 7%) and substantial parameter compression (up to 90%) compared to state-of-the-art baselines. Furthermore, the resulting "LITE" model achieves faster inference speed and reduced FLOPs.

Future work will explore task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to further optimize LLMs by removing unnecessary parameters for specific applications, drawing a parallel to uninstalling unused apps on a smartphone.</sample>
    <sample id="115">lambda speech frames</sample>
    <sample id="116">"Servin is a judge." and "Kea is a Baker."</sample>
    <sample id="117">示例质量比与源句子的相似度更重要。</sample>
    <sample id="118">This ACL 2023 submission introduces "SwitchMLM," a novel pretraining technique designed to improve performance on code-switched NLP tasks. Code-switching, the mixing of languages within a single sentence (e.g., English and Hindi), is common in linguistically diverse communities, but existing multilingual models like mBERT and XLM-R struggle with it.

The core idea of SwitchMLM is to focus the masked language modeling (MLM) objective on "switch-points"—transitions between languages within a sentence. Unlike standard MLM where all words are masked, SwitchMLM only masks tokens at these switch-points. To overcome the need for language identification (LID) tags, the authors propose FrequencyMLM, a surrogate method using monolingual corpora to estimate language probabilities.

Beyond the MLM objective, the work also incorporates architectural modifications. Recognizing that intermediate layers of BERT encode valuable switch-point information, they introduce residual connections from these layers to the final layer, alongside an auxiliary LID-based loss to further encourage language encoding.

Experiments on sentiment analysis demonstrate that the combined SwitchMLM/FrequencyMLM with ResBERT architecture outperforms existing methods across various language pairs. Probing experiments, using both linear and conditional probing, confirm that the proposed methods effectively increase the representation of switch-point information in both intermediate and final layers, validating the design choices and highlighting the benefits of focusing on these crucial linguistic transitions. The research ultimately aims to create more robust and effective models for code-switched NLP.</sample>
    <sample id="119">RoBERTa.</sample>
    <sample id="120">结合多个层的分数。</sample>
    <sample id="121">"Easy on Me" or "I Gotta Feeling", "the first one".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on "MultiInstruct," a novel approach to enhance multi-modal zero-shot learning through instruction tuning. Existing instruction tuning primarily focused on language-only tasks, leaving multi-modal applications unexplored. Recognizing a significant gap in available multi-modal instruction datasets, they created MultiInstruct, a benchmark dataset comprising 62 diverse multi-modal tasks across 10 categories, derived from 21 open-source datasets and each task featuring five expert-written instructions.

Their study utilized OFA, a unified multi-modal pre-trained model, and employed instruction tuning on 53 of the 62 tasks. They evaluated performance on both seen and unseen tasks, including a subset of natural instruction NLP tasks. Key findings demonstrated that instruction tuning significantly improved OFA's performance on multi-modal tasks and that transfer learning from natural instruction datasets further enhanced both performance and "sensitivity"—the model's consistency in generating outputs regardless of instruction wording variations.

The research also highlighted the benefits of using multiple instruction templates (five versus one), showing improved performance and reduced sensitivity with more instructions. Transfer learning from natural instructions notably improved sensitivity compared to the original OFA model. Ultimately, MultiInstruct contributes a large-scale dataset, improved zero-shot capabilities for OFA, and explores effective transfer learning techniques, introducing a new "sensitivity" metric to assess model robustness. The team is currently expanding the dataset to include 150 additional vision-language tasks, which will be publicly released.</sample>
    <sample id="124">This presentation introduces "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models," a study by Tan Qingyu from the National University of Singapore and Alibaba. The core focus is on comprehensively evaluating and enhancing LLMs' ability to reason about time.

The researchers break down temporal reasoning into three levels: time-to-time (e.g., year calculations), time-to-event (e.g., what team a person played for in a specific year), and event-to-event (e.g., what team a person played for after another). They observed that previous research overemphasized the time-to-event level.

To address this, they created the TempReason dataset, covering all three levels and a broad temporal range. They evaluated LLMs (T5-L, FLAN-T5-L, and ChatGPT) in three question-answering settings: Closed Book, Open Book, and Reasoning QA (providing relevant temporal knowledge).

Findings revealed biases in existing LLMs, particularly a preference for the 2000-2020 timeframe. ChatGPT's performance degraded significantly with month prediction and struggled with more complex reasoning. To improve temporal reasoning, the researchers proposed a training strategy: Temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model.

TempT5 demonstrated superior performance on TempReason compared to other models, especially in Open Book and Reasoning QA settings. However, performance fluctuations across different time periods were noted, suggesting potential biases related to training data imbalance, which is an area for future research. The study ultimately aims to expose temporal reasoning biases and provide a benchmark and training paradigm for better temporal reasoning in LLMs.</sample>
    <sample id="125">The text does not mention the number of authors.</sample>
    <sample id="126">Yes, Google Translate API is used to translate source queries to the target language in the Translate-Test setting.</sample>
    <sample id="127">This presentation introduces "Large Language Models Are Reasoning Teachers," a paper exploring how to transfer reasoning abilities from large language models (LLMs) to smaller, more deployable models. The core problem addressed is that chain-of-thought (CoT) reasoning, a technique enabling LLMs to solve complex tasks, currently requires massive models like GPT-3, limiting its practical application.

The proposed solution involves using large LLMs as "teacher" models to generate step-by-step solutions for complex tasks. These solutions are then used as training data to fine-tune smaller "student" models. A key innovation is "Diverse Reasoning," which generates multiple, slightly different reasoning paths from the teacher model using stochastic temperature sampling, leading to more robust student training.

Experiments across 12 tasks demonstrate that this "fine-tuned CoT" method significantly outperforms prompt-based baselines and even vanilla fine-tuning, even with student models as small as 0.3 billion parameters. The method's performance is scalable, with improvements achievable through larger datasets, better teacher models, or bigger student models, though these choices involve trade-offs between development and inference costs.

The paper emphasizes the accessibility and scalability of this distillation approach, suggesting it could be applied to transfer other emergent abilities in LLMs. The authors provide code, data, and even the OpenAI inference costs for reproducibility and encourage further research.</sample>
    <sample id="128">Akshatha and Martin introduce "KITMUS," a diagnostic test suite designed to evaluate how well natural language understanding (NLU) models integrate knowledge from various sources. NLU models rely on both knowledge embedded in their parameters during pretraining and knowledge provided during inference. KITMUS focuses on coreference resolution, specifically identifying which entity a pronoun refers to, requiring both entity-specific knowledge (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases").

KITMUS presents three settings: "Background-Pretrain" (background knowledge available during pretraining), "Background-Both" (background and entity-specific knowledge available during inference), and "Background-Inference" (both knowledge types available only during inference). The "Background-Inference" setting is crucial for simulating scenarios where necessary background knowledge wasn't present during pretraining, like new occupations.

The researchers evaluated KITMUS with human participants and existing coreference resolution models (C2F and BERT4Coref). Initial results showed that models trained on standard datasets struggled with KITMUS, relying on superficial cues. However, task-specific training on KITMUS significantly improved performance. Despite this improvement, even the best models demonstrated difficulty reliably integrating background knowledge provided solely during inference, highlighting a limitation in their ability to reason across different knowledge sources.

The paper concludes that while some models can integrate knowledge with task-specific training, a significant challenge remains in enabling them to effectively utilize inference-time background knowledge. The KITMUS dataset and code are publicly available for further research.</sample>
    <sample id="129">Asian women, Middle-Eastern women, Latina women, and Black women.</sample>
    <sample id="130">非Transformer模型。</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">两位。</sample>
    <sample id="133">多种模态。</sample>
    <sample id="135">ABC-Eval is a new, dimensional approach to evaluating conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. It addresses the limitations of traditional human evaluation methods (Likert scales, pairwise comparisons) which, while providing holistic assessments, lack granularity in identifying specific strengths and weaknesses.

ABC-Eval aims to reduce subjectivity by explicitly annotating model behaviors—such as irrelevance, contradiction, hallucination, and lack of empathy—within conversations. This allows for a precise measurement of thematic errors. The method was tested on four state-of-the-art models across 100 human-bot conversations, comparing ABC-Eval against Likert ratings (turn and dialogue levels) and pairwise comparisons.

The results demonstrate that ABC-Eval labels are more reliable (higher inter-annotator agreement) and predictive of overall conversation quality than existing methods. Specifically, measuring contradictions (self and partner) significantly explains conversation quality, surpassing the explanatory power of Likert scores. A stepwise regression analysis further confirms that ABC-Eval metrics capture unique aspects of chat quality, collectively explaining over 25% of conversation quality, a substantial improvement over turn-level Likert metrics.

The study highlights ongoing challenges, quantifying common sense violations (20%), irrelevant information (15%), and contradictions (10%) in current models. While these error rates may decrease with future advancements, ABC-Eval provides a valuable tool for reliable and precise model comparison, facilitating progress in conversational AI.</sample>
    <sample id="136">Jasivan presented "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addressing the limitations of current benchmarks in evaluating language models' numerical abilities. Existing benchmarks like CommonCore and Illinois, relying on accuracy scores, fail to pinpoint specific mathematical strengths and weaknesses, especially in models with fewer than 10 billion parameters.

FERMAT, introduced as a solution, is a flexible evaluation set categorized by arithmetic types (number understanding, operations, training dependency). It utilizes math questions from Illinois and CommonCore, varying number representations (integers, decimals) and operation complexity. Initial zero-shot evaluations revealed poor performance across all aspects, highlighting the benchmarks' inadequacy.

Fine-tuning with 200,000 generated examples improved performance, but further analysis showed models didn't simply memorize answers, suggesting linguistic nuances impact reasoning. Experiments with diverse training templates (GSM8K, AQUA) demonstrated that mathematical and linguistic diversity significantly boosted performance, surpassing results from solely base templates.

The research concludes that current benchmarks are unrepresentative and single scores are insufficient. FERMAT offers a more informative alternative, emphasizing the importance of language and mathematical diversity. The study also identifies number encoding and tokenization as areas needing improvement for enhanced numerical reasoning in language models. The QR code provides access to the paper, GitHub repo, and contact information.</sample>
    <sample id="137">This paper introduces "Tell2Design," a novel dataset and task focused on language-guided floor plan generation. Recognizing the need for design tools that respond to natural language instructions, the researchers aim to enable users without design expertise to participate in the design process.

The Tell2Design dataset comprises 5,051 human-annotated and approximately 76,000 artificially generated language instructions describing floor plan components (semantics, geometry, and topology), paired with corresponding 2D floor plans. The dataset addresses challenges including strict design constraints, understanding complex floor plan layouts from lengthy text, and dealing with ambiguous instructions.

The authors propose a sequence-to-sequence model, leveraging a transformer-based encoder-decoder architecture initialized with a pre-trained T5 language model, to generate floor plans. This approach treats floor plan generation as reconstructing room bounding boxes into a structured sequence. Experiments on the T2D dataset demonstrate that their model significantly outperforms existing text-conditional image generation baselines, achieving high IoU scores.

The study highlights a language distribution gap between artificial and human instructions, but finds that using artificial instructions for initial training improves performance when subsequently trained on human-written instructions. A case study illustrates that while image generation models can produce realistic floor plans, they struggle to accurately reflect the specific requirements outlined in the instructions. The paper concludes by positioning Tell2Design as a foundation for future research in language-guided design generation.</sample>
    <sample id="138">The ability to reliably integrate backward knowledge presented only at inference time.</sample>
    <sample id="139">Ying</sample>
    <sample id="140">是。为了确保验证和测试集的质量，研究人员请众包工人查找和修改了不正确的样本。</sample>
    <sample id="141">Existing resources supporting targeted evaluation of context-dependent translations have limited types of context-dependent translations and limited sets of languages, as they usually rely on domain knowledge and human curation.</sample>
    <sample id="142">大家好！我将介绍我们关于“解决间接指代表达以进行实体选择”的工作，其中我们推出了AltEntities语料库。我的名字是Javad Hosseini，这是与Filip Radlinski、Silvia Pareti和Annie Louis共同完成的工作。我们的目标是理解用户在想要做出选择时的语言。考虑以下替代问题：“你是说‘Easy on Me’还是‘I Gotta Feeling’？”在这里，用户想要在两首歌曲中进行选择。最明显的方法是使用直接引用，例如说歌曲的名字“Easy on Me”或它的位置，“第一首”。但有时间接引用更合适，以进行更自然的对话。这可能是因为用户记不起歌曲的名字。或者发音太相似，难以区分。或者当用户想要指定一个偏好时。以下是一些间接引用的例子，例如“较新的那首”或“不具活力的歌曲”。这是一个会话系统和基准大型语言模型实体理解中的一个重要问题。我们没有发现用于此任务的更大规模的公共数据集，因此我们使用众包注释来收集一个。我们的数据集涵盖三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，采用卡通完成设置。卡通中有三个对话框。在第一个对话框中，Bob说：“你还记得我们昨天听的那首歌吗？” 这样，Bob就设置了对话背景。在第二个对话框中，Alice说：“你是说‘Easy on Me’还是‘I Gotta Feeling’？” 这就是替代问题。在第三个对话框中，Bob使用间接引用来选择其中一个实体，例如“较新的那首”。我们自动提供第一个和第二个对话框，但第三个对话框由注释者填写。第一个对话框来自每个领域中的几个手动提示。第二个对话框，即替代问题，采用简单的模板生成：你是说A还是B？其中A和B是维基百科的样本。以下是我们在使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常更难进行区分。第一个是均匀随机采样。第二个是在实体标题相似的情况下，例如两本名为“The Return”的书。第三是在实体在维基百科上的描述相似的情况下。最后，当它们具有相似的信息框或属性时，例如歌曲的同一流派或同一艺术家。当我们将此替代问题展示给注释者时，他们知道这些实体的名称，但他们不一定了解这些实体。所以我们所做的是展示有关这两个实体的背景知识。对于歌曲，我们只需向每个歌曲显示一个Google搜索链接，然后要求注释者至少听取其中一部分并阅读每首歌曲。以下是歌曲“Easy on Me”的Google搜索结果示例。对于食谱和书籍领域，我们展示了一些来自维基百科的背景文本。对于食谱，我们还展示了它们的图像，同样来自维基百科，以便注释者了解它们的样子。然后，我们要求注释者选择其中一个实体，例如第一个实体，并用三到五个间接引用来描述它们。例如，“没有钢琴音乐的那首”。以下是一些来自我们数据集的例子。例如，“没有歌词的那首”、“不是那个12岁男孩的那首”、“虚构的那首”或“来自阿塞拜疆”等等。AltEntities语料库包含来自三个领域中的6,000个替代问题，并包含42,000个间接引用表达。以下是使用T5 XL模型的摘要。如果语言模型可以访问与注释者完全相同的背景知识，那么准确率非常高，约为92%到95%。但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率仅为60%，因此仍有很大的改进空间。我们还表明，模型具有领域泛化能力。以下是我们的数据集链接。谢谢。</sample>
    <sample id="143">Wait-k strategy, Local Agreement, and state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The text does not mention the authors' institution.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This talk introduces a paper analyzing omission errors in dialogue summarization, a significant but under-explored problem. Despite advancements using large language models, generated summaries often omit crucial information, hindering real-world usability. The analysis reveals a concerning 70% omission rate across various domains and models, with omitted information randomly distributed throughout dialogues, highlighting the difficulty in identifying key information.

To address this, the researchers defined omission detection as identifying missing content from the original dialogue within generated summaries, focusing on utterance-level omissions. Recognizing the lack of relevant datasets, they created the OLDS dataset, a publicly available resource with high-quality omission labels generated automatically and validated by human evaluation across five domains.

The paper then explores three baseline models for omission detection (pairwise classification, sequence labeling, and pointer network), finding the task challenging with an F1-score around 50% due to label imbalance. Finally, they investigated using detected omissions to refine summaries via a post-editing method, demonstrating a significant performance boost, suggesting that omission detection is a valuable task and a promising avenue for improving dialogue summarization quality. The research underscores the importance of addressing omission errors to create more complete and reliable dialogue summaries.</sample>
    <sample id="147">Esin Durmus and Dan Jurafsky.</sample>
    <sample id="148">大家好，我是来自特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮，我将简要介绍“注意力作为同时语音翻译的引导”论文，这是我和马特奥·内格里和马可·图尔奇的共同作品。什么是同时语音翻译？同时语音翻译（SimulST）是将口语实时翻译成另一种语言的文本的过程，从而实现跨语言交流。目前 SimulST 模型的难题是什么？通常会训练特定的架构，引入额外的模块进行优化。例如，涉及不同优化目标的长时间复杂训练过程。以及为了达到不同的延迟范围而训练和维护多个模型。例如，训练一个平均延迟一秒的模型，再训练一个平均延迟两秒的模型，以此类推。那么我们的解决方案是什么？首先，使用现有的离线 ST 模型，无需重新训练或采用专门的 SimulST 架构。使用一个模型来处理所有延迟范围，并通过特定参数来处理延迟。并利用模型通过音频输入和文本输出之间的注意力机制所获得的知识。这就是交叉注意力机制，您可以在右侧看到一个示例。我们的解决方案是提出 EDAtt，即编码器-解码器注意力，这是一种策略，我们根据注意力指向的位置来决定是否发出部分翻译。如果注意力不集中，即其总和低于某个阈值 alpha，指向最后 lambda 个语音帧，这意味着接收到的信息足够稳定，则会发出一个词。例如，如果接收到包含“我将要谈论……”的语音片段，并且我们的模型预测为德语翻译，我们将查看交叉注意力权重，我们会发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即 lambda 个语音帧。这意味着前两个词将被发出，因为交叉注意力的总和高于某个阈值 alpha，我们将不会发出最后一个词，而是等待另一个语音片段。如果我们继续接收另一个语音片段，并且我们的模型预测了另外三个词，我们将查看这些交叉注意力权重，我们会发现没有词指向最后 lambda 个语音帧。这意味着这三个词将被发出。如果查看 EDAtt 的主要结果，我们将在图表中绘制同时语音翻译结果，其中一边是衡量翻译质量的 BLEU，另一边是平均滞后，即延迟度量。我们还考虑了计算感知平均滞后，它考虑了模型预测输出的计算时间。因此，我们希望该图上的曲线尽可能高，并且尽可能向左移动。我们将它与应用于离线模型的流行策略进行比较，这些策略是 Wait-k 策略和局部一致性。我们还与专门为同时预翻译定制的最先进架构进行比较。这些是所有同时语音翻译策略在德语上的结果。我们看到它优于应用于离线模型的任何策略，因为曲线向左移动。如果我们考虑实际经过的时间或计算感知时间，那么它是最快的策略。如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同时输出，以促进我们工作的可重复性。感谢您的关注。</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">The ACL paper introduces MeetingQA, a new extractive question answering (QA) dataset built on meeting transcripts. Recognizing that millions of meetings generate vast amounts of transcripts ripe for NLP research, the authors address the gap left by existing summarization and action item extraction approaches, which neglect the crucial QA component of meeting discussions.

MeetingQA comprises 7.7K questions derived from the AMI corpus, with questions selected based on punctuation and filtered for brevity. Annotators labeled answer spans, achieving a high inter-annotator agreement (Krippendorff's alpha of 0.73). The dataset features unique characteristics: longer, open-ended questions, multi-speaker answers, discontinuous answer spans, and rhetorical questions, with 30% of questions being unanswerable. A significant portion (70%) of multi-speaker answers involve disagreement.

The paper explores various QA methods, including context retrieval for short-context models, single-span and multi-span models, and data augmentation using silver annotations from the MediaSum dataset. Results reveal a substantial performance gap between models and human performance (over 25 F1 points in the fine-tuned setting and nearly 50 F1 points in the zero-shot setting). Interestingly, short-context models like RoBERTa slightly outperformed long-context models like Longformer, and multi-span models showed comparable or slightly lower performance than single-span models. Error analysis highlights challenges in identifying rhetorical questions and speaker attribution, particularly in zero-shot scenarios.

MeetingQA presents a challenging and valuable resource for advancing QA research in the domain of meeting transcripts, demonstrating that current models still have significant room for improvement.</sample>
    <sample id="151">大家好，我叫 Ying，我的同事 Zhiyang 和我将为大家介绍我们的研究，关于 MultiInstruct 如何通过指令调优来改进多模态零样本学习。随着大型语言模型的进步，许多工作开始探索新的学习范式，以一种参数和数据高效的方式，将预训练的语言模型用于不同的下游任务。最近，许多研究表明，指令调优可以使大型语言模型通过遵循自然指令，在零样本方式下执行未见过的任务。然而，大多数关于指令调优的先前工作都集中于提高仅语言任务的零样本性能，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想调查指令调优多模态预训练模型是否真的可以提高对未见过的多模态任务的泛化能力。此外，在我们研究期间，我们发现 NLP 和多模态之间指令数据集的可用性存在相当大的差异。存在 1600 多项仅语言的指令任务。但是，没有大规模的公开可用的多模态指令任务。因此，这促使我们构建了一个多模态指令调优数据集。在这里，我们介绍 MultiInstruct，第一个多模态指令调优基准数据集，它由 62 种不同的多模态任务组成，涵盖 10 个广泛的类别。这些任务源自 21 个现有的开源数据集，每个任务都配备了五个专家撰写的指令。为了研究我们提出的数据集上的多模态指令调优，我们以 OFA 为基础模型，OFA 是一个统一的多模态预训练模型。OFA 使用统一的词汇表来处理语言、图像 token 和边界框的坐标。在这里，我们展示了我们 MultiInstruct 数据集中的一些示例实例，以统一各种输入和输出数据类型的处理。我们遵循 OFA 中的方法，并将所有任务都以统一的序列到序列格式进行表述。其中，文本输入、图像、指令和边界框都表示在相同的 token 空间中。好的，现在我将谈论多模态指令调优。对于训练数据集，我们使用 9 个组中的 53 个任务进行训练，并且每个任务抽取 10,000 个实例。对于测试，我们保留了整个常识推理组进行测试，并从 VQ 和 Miscellaneous 组中选择了另外 5 个任务。我们使用每个任务中的所有实例进行测试拆分。此外，我们从自然指令的测试拆分中随机抽取 20 个任务作为 NLP 的未见过的任务。我们使用预训练的 OFA 大模型作为基础模型。在训练期间，我们混合了所有任务的所有实例。每个实例都与它五个指令模板中的一个随机组合。在每个任务的测试期间，我们进行总共 5 次实验，通过使用五个指令中的一个来评估模型。在每次实验中，我们报告所有 5 次实验的最小和最大性能，以及性能的标准差。如果任务是多模态分类任务，我们报告准确率。如果它是一个多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们同样报告 Rouge-L。我们还引入了一种额外的评估指标，称为敏感性。这衡量了模型在指令措辞略有变化的情况下，始终产生相同输出的能力。这是我们的主要结果。正如我们所见，指令调优可以显著提高 OFA 在可见的多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以使指令调优受益。我们在这里可以看到，随着任务数量的增加，模型可以获得更好的性能，并且同时降低了敏感性。我们还进行了一项实验。我们使用一个指令与五个指令。正如我们所见，使用更多的指令可以提高模型的整体性能，并大大降低其敏感性。这表明不同的微调策略对模型敏感性的影响。正如我们所见，通过从自然指令数据集进行迁移学习，该模型可以获得比原始 OFA 模型更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上获得更好的性能。总而言之，我们提出了第一个大规模的多模态指令调优数据集，显著提高了 OFA 的零样本能力，并探索了不同的迁移学习技术并展示了它们的优势。我们设计了一种新的指标，称为敏感性。还有一件事，我们正在收集一个更大的多模态指令调优数据集，其中包含大约 150 个额外的视觉语言任务，并将发布它们。这是我们数据和模型的二维码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider presented work on new language models for classical philology, addressing limitations of existing models for Ancient Greek and Latin. Current models are primarily monolingual BERT variations, lacking robust evaluation and multilingual capabilities.

The project aimed to create improved models by: comparing existing models, advancing the state-of-the-art, exploring different architectures, and developing multilingual models. Two monolingual models, GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder), were created for Ancient Greek. Additionally, PhilBERTa and PhilTa, multilingual models encompassing Ancient Greek, Latin, and English, were developed.

A key innovation was a new, high-quality Ancient Greek pre-training corpus derived from the Internet Archive, overcoming OCR challenges by identifying Greek texts through mis-transcribed stop words.

Benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization demonstrated significant performance improvements over existing models for both languages. Notably, the encoder-decoder architecture (GreTa) excelled in lemmatization. Analysis revealed unique behavior of the T5 encoder compared to traditional encoder-only models.

Probing for semantic and world knowledge showed strong performance, though multilingual models didn't significantly outperform monolingual ones.

In conclusion, the research introduced powerful, newly initialized language models with native tokenizers, diverse architectures, and a novel pre-training dataset, advancing NLP applications in classical philology. Further details are available in their published paper.</sample>
    <sample id="153">Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team presented research on "Resolving Ambiguities in Text-to-Image Generative Models." The core problem addressed is that ambiguous prompts lead to inconsistent image generation, hindering user intention fulfillment.

The research introduces a pipeline to tackle this. First, a benchmark dataset (modified LAVA corpus) was created to cover various ambiguity types. Then, a prompt disambiguation framework is employed, utilizing two methods: 1) a language model generates clarifying questions for users to answer, refining the prompt, or 2) the language model generates multiple visual interpretations for the user to select from. Both approaches aim to create a disambiguated prompt reflecting the user's intended meaning.

To evaluate the effectiveness of this disambiguation, an automatic evaluation framework was developed. It generates images from both the original ambiguous prompt and the disambiguated prompt. A Visual Question Answering (VQA) model then assesses whether the generated image aligns with the user's stated intention, determining faithfulness.

Key findings include: different ambiguity types are resolved with varying success, the disambiguation framework generally improves faithful image generation, and the automatic evaluation framework correlates well with human judgment, making it a reliable evaluation tool. The research highlights the importance of addressing prompt ambiguities to improve the reliability and user satisfaction of text-to-image models.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">This work introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to dialogue summarization aiming to overcome limitations of existing methods. Current techniques often rely on pre-computed static graphs derived from external linguistic tools, which are prone to errors and inflexible.

SDDS addresses these issues by integrating both static and dynamic graph structures. The model begins by encoding dialogue utterances using an utterance encoder and constructing a static graph through four heuristic methods: Discourse Parsing Graph, Key Co-occurrence (KeyCo-occ), Speaker Relationship Modeling, and Utterance Position Graph. These methods capture discourse relations, keyword overlap, speaker interactions, and positional information, respectively.

Crucially, SDDS then employs a Dynamic Graph module, utilizing multi-head attention, to learn semantic relationships between utterances directly from their deep vector representations, without relying on pre-defined structures. A fusion method combines the static and dynamic graph representations into a unified graph. Finally, a pre-trained language model, enhanced with a graph attention layer, generates the summary, effectively fusing the structural information into the final output.

The SDDS model demonstrates a more robust and adaptable approach to dialogue summarization by dynamically learning relationships alongside static structural information, leading to improved summary quality. The code and data are publicly available on GitHub.</sample>
    <sample id="158">This presentation introduces "Dual Cache," a novel approach to improve neural coreference resolution, particularly for long documents. Coreference resolution aims to link mentions of the same entity within a text, a task traditionally hampered by quadratic computational complexity. Cache-based methods offer a linear solution by storing entities in a fixed-size cache, but standard Least Recently Used (LRU) eviction policies struggle with long documents due to topic shifts and scattered entity mentions.

The Dual Cache addresses this by employing two caches: a local cache using LRU for local entities and a global cache using Least Frequently Used (LFU) for globally important entities. The model classifies mentions as new or existing, and based on frequency, adds them to either the local or global cache. When caches are full, the respective eviction policies are triggered.

Experiments on four benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate Dual Cache's superior performance compared to baselines, even those with unbounded memory, especially in long-form content like a 30,000-word book.  It significantly reduces cache misses compared to single-cache methods.

The key takeaway is that Dual Cache achieves a better balance between efficiency and performance, offering the highest performance/cost ratio among cache-based coreference resolution models. By strategically separating local and global entities, Dual Cache overcomes the limitations of single-cache approaches and provides a more effective solution for handling long documents.</sample>
    <sample id="159">大家好，我是Koustav Sinha，很高兴欢迎大家参加我们ACL 2023论文的讨论。语言模型对可接受性判断并非总是对上下文稳健。这是一项我和John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams共同完成的工作。在这个工作中，我们重新审视了最小对模型范式（Minimal Pair Paradigm，MPP）。最小对模型范式基本上是用来评估语言模型在可接受性判断上的表现，这也可以包括语法性，例如BLiMP、SyntaxGym，或者在刻板印象方面的可接受性，例如CrowS对。在最小对模型范式中，评估语言模型的一种典型方式是展示一个可接受的句子或一个语法正确的句子，然后展示一个可接受的句子或一个语法错误的句子。希望的是，模型会给可接受的句子赋予更高的概率。当前的MPP流程基本上不允许我们评估模型对更长句子的可接受性。如今，大型语言模型正在生成越来越长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要，而我们正在努力做到这一点。我们试图通过要求模型在越来越长的序列上评估可接受性，从而重新审视MPP流程。这就是我们的方法。

我们所做的是，为了模拟这些更长的序列，我们重新审视数据集本身，然后通过选择这些数据集中可接受或不可接受的句子来重建句子。例如，这里我们选择了一个来自BLiMP数据集的典型语法性对，来自附例岛（Adjunct Island）的情况。我们所做的是，为了重建可接受且具有相同语法结构的更长序列，我们从附例岛中提取语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。我们也可以用同样的方式选择不可接受的句子来测试模型的接受性。我们还可以用同样的方式选择来自不同子集或不同数据集的句子。这就是我们所说的“不匹配”场景。在这种情况下，句子仍然来自相关的语料库，但不是用于评估的同一个语料库。我们也可以对不可接受的情况做同样的事情。最后，我们可以选择来自完全无关领域（例如维基百科）的句子。这将告诉我们，模型的接受性判断是否受到任何上下文的影响，例如，上下文是否来自数据集的不同子集，或者它是否与我们正在查看的句子完全无关。

模型表现如何？首先，我们查看了来自维基百科的句子，这些句子与当前的查询对完全无关。在那里，我们发现MPP判断在任意上下文长度下大多是稳健的。我们增加了上下文长度，最多达到1024，以最大化OPT和GPT 2模型的上下文窗口。我们在这里看到，在橙色虚线中，MPP判断相对稳定。

现在，当选择来自同一数据集的句子时会发生什么？在这里，我们选择或创建来自同一BLiMP或SyntaxGym数据集的句子。在那里，我们看到，当添加可接受的前缀或不可接受的前缀时，MPP判断会显著增加或减少。但是，当匹配结构时，也就是说，当我们从BLiMP或SyntaxGym中选择句子时，我们看到MPP判断会大幅增加或减少，这取决于所选的前缀是否可接受。这种效应非常大，并且随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新型语言模型。

为什么匹配的前缀会如此影响语言模型的判断？我们进行了一系列分析，试图通过尝试保留相关结构并向输入添加噪声来扰动输入句子。经过多次扰动后，我们发现这些噪声中的任何一种都无法使模型改变其MPP判断打印的方式。基本上，我们发现模型以相似的方式对扰动后的句子敏感。也就是说，当我们扰动可接受领域的句子时，我们看到所有扰动中MPP判断都相似地增加，而当我们扰动不可接受领域的句子时，我们看到MPP判断在相似的方式中减少。

我们工作的关键要点是，语言模型对跨句子共享的潜在句法和语义特征敏感。我们目前以短句和单句输入的方式进行的MPP评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢大家的聆听。</sample>
    <sample id="160">一个无序的多集词元。</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">弱监督学习使用弱标注源（如简单的启发式规则、知识库或低质量众包）来标注数据，比手动标注数据更便宜。</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive reasoning, addressing the limitations of existing supervised approaches that rely on subjective and noisy human annotations of plausible explanations. Abductive reasoning aims to find explanations that bridge the gap between a given context and an outcome.

LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, avoiding the need for labeled data. However, this alone doesn't guarantee preference for plausible explanations. To address this, LiPoR incorporates a regularizer based on the principle of mutual exclusivity – the idea that explanations in abductive reasoning are often mutually exclusive.

The LiPoR objective function combines maximizing the likelihood of the outcome and enforcing mutual exclusivity among explanations. The regularizer penalizes scenarios where the probability mass is distributed across too many explanations, encouraging the model to select a smaller, more plausible subset.

Experiments on the AlphaNLI dataset demonstrate that LiPoR significantly outperforms existing zero-shot models and previous unsupervised approaches, achieving a 4-point accuracy improvement over a strong GPT-3 baseline. This highlights the effectiveness of LiPoR's unsupervised learning framework and its ability to leverage the inherent structure of abductive reasoning to achieve state-of-the-art performance. The paper and code are available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This work introduces "NDCR," a novel Neural Divide-and-Conquer Reasoning framework for image retrieval from linguistically complex text. Existing visual language models struggle with this task due to the complexity of the text and the similarity of images. NDCR addresses this by drawing inspiration from the Divide-and-Conquer strategy and Dual-Process Theory, mimicking human cognitive processes.

The framework integrates two systems: System 1 (Visual-Linguistic Interactor) and System 2 (Neural-Symbolic Reasoner). System 1, akin to analogical reasoning, interacts visual and proposition information to generate matching scores and reasoning states. System 2, representing logical reasoning, integrates these states and results through a negation executor and conjunction operation to derive a final solution. A Proposition Generator initially decomposes complex text into simpler propositions.

Experimental results demonstrate that NDCR outperforms existing baselines, and ablation studies confirm the effectiveness of each module. The system's ability to present inference states and results highlights its interoperable processing. The authors suggest that neural-symbolic calculation, combined with Divide-and-Conquer and Dual-Process Theory, offers a promising avenue for enhancing compositional reasoning and planning in large language models, similar to chain-of-thought prompting. Ultimately, NDCR leverages the strengths of both analogical and logical reasoning to tackle the challenges of complex image-text retrieval.</sample>
    <sample id="167">DEPLAIN-web includes 750 documents aligned both manually and with automatic alignment methods, resulting in 30,450 sentence pairs.</sample>
    <sample id="168">It was collected from Reuters News from 2020, and then annotated with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper, a collaboration with Google Translate, presents the first systematic study of prompting large language models (LLMs) for machine translation, specifically using PaLM (a 540 billion-parameter model). The research evaluates PaLM's translation capabilities using established MT best practices, including current test sets and comparisons against state-of-the-art systems and WMT evaluations.

A key finding is the significant impact of prompting strategy on translation performance. Even slight prompt variations can lead to substantial BLEURT score differences (up to 40 points). The study found that a 5-shot prompting strategy, where sentences are marked with their language (e.g., "German: [sentence] English: [translation]"), proved effective. Crucially, example quality outweighs similarity to the source sentence; using high-quality translations from development data (dev data) yields better results than using training data.

While PaLM demonstrates impressive fluency comparable to state-of-the-art systems, it lags in accuracy. The most frequent errors involve omissions – PaLM sometimes sacrifices accuracy for a more fluent-sounding translation by dropping parts of the source sentence. However, PaLM’s output is perceived as less stylistically awkward than that of current state-of-the-art systems. Overall, PaLM’s translation performance is close to that of a commercial system like Google Translate, highlighting the potential of LLMs in machine translation despite accuracy limitations.</sample>
    <sample id="170">大家好，我叫张宇森，来自宾夕法尼亚州立大学。今天我将介绍我们的工作“XSemPLR：跨语言语义解析在多种自然语言和含义表示中的应用”。语义解析的任务是构建用户查询的语义表示，例如SQL和Lambda演算。跨语言语义解析的任务是将多种自然语言的查询翻译成多种含义表示。如图所示，我们需要使用神经网络模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是单独提出的，并在有限的任务和应用的数据集上进行评估。例如，对某些自然语言的覆盖率很高，但中文缺失，并且缺乏对某些含义表示的覆盖。Lambda演算缺失，或者它们只在某些神经网络模型上进行评估。例如，只有单个模型来评估它们。为此，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于在多种自然语言和含义表示中进行跨语言语义解析。它包含9个数据集，涵盖各种领域，5个语义解析任务，8个含义表示，以及22种自然语言，属于15个语系。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译-测试。我们使用谷歌翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英文数据上训练英文模型，在推理时将德语查询使用API翻译成英文，然后使用训练好的模型进行预测。我们还将测试单语模型。在这种设置中，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试单语少样本设置，仅使用10%的训练数据训练单语模型。我们还测试多语言模型，我们为所有语言训练一个多语言模型。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理时，我们可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上训练，然后迁移到另一种语言。因此，在训练时，我们使用英语查询或英语和德语少样本查询的组合来训练一个多语言模型来预测SQL输出。我们还发现了很多有趣的发现。关于单语模型的分析，我们评估了包括编码器-指针解码器（Encoder-PTR）在内的两组模型，例如XLM-R + PTR和mBERT + PTR。我们还评估了编码器-解码器模型，例如mBART和mT5。我们发现编码器-解码器在所有9个数据集上都获得了最佳性能。我们在多语言设置下评估了mT5和XLM-R + PTR。我们发现编码器-解码器或编码器-PTR可以通过在各种语言的混合中进行训练来得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集中的性能下降，只有在三个数据集中的性能提升。我认为这被称为“多语言诅咒”。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现零样本设置中的跨语言迁移性能差距很大，然后比较蓝线和橙线，我们发现少样本设置下迁移差距迅速缩短。我们还发现了一些其他的有趣的发现。例如，编码器-解码器优于先前的工作，或取得了可比的结果。在英语自然语言上进行预训练可以显著提高目标自然语言的少样本性能，并且我们发现像Codex和BLOOM这样的多语言语言模型仍然不足以用于跨语言语义解析任务。总而言之，我们构建了XSemPLR，这是一个统一的基准，用于跨语言语义解析，涵盖多种自然语言和含义表示。我们对三种具有代表性的多语言语言模型进行了全面的基准研究。我们的结果表明了很多有趣的发现。欢迎访问我们的论文和代码。感谢聆听。</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">Inadequate.</sample>
    <sample id="174">Thea, a co-author of "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis," highlights the dataset's unique features compared to existing argument analysis datasets. Current datasets often suffer from low quality due to crowdsourcing, limited diversity stemming from a small number of pre-selected motions, a lack of depth in explaining argument reasoning, and a rigid one-to-one argument-motion association.

ArgAnalysis35K addresses these issues in several key ways. Firstly, it boasts a massive scale – 35,000 argument-analysis pairs – with a significantly higher proportion of arguments sourced from high-quality debaters. Secondly, it offers greater diversity by selecting 24 themes based on expert input and capturing numerous motions within each, moving away from pre-defined motion sets.

A crucial innovation is the introduction of "analysis," a broader concept encompassing claims, premises, and their connections, providing a more comprehensive explanation of an argument's reasoning. The dataset also employs "instance-based annotator reliability," mitigating bias by selectively removing judgments on topics where annotators demonstrate potential bias, maximizing annotation utility.

Finally, ArgAnalysis35K incorporates a "relevance model," assigning scores to arguments based on their applicability across various themes, recognizing that arguments often have broader relevance than a single motion. This allows for a more nuanced understanding of argument utility. The dataset aims to provide a more diverse, reliable, and comprehensive resource for argument quality analysis.</sample>
    <sample id="175">通过诱导对齐作为训练的一部分来解决这个问题，并使用 GPU 友好的连续松弛来近似找到最高分排列。</sample>
    <sample id="176">Downstream NLP models exhibit fairness issues when their performance varies significantly based on the political leaning of the news media or the demographic/political categories of the content they are evaluating (e.g., hate speech or fake news). Left-leaning models perform better on detecting hate speech targeting minority groups but worse on detecting hate speech targeting powerful groups, and vice versa for right-leaning models.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presented "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, the ability to understand others' mental states, is traditionally tested with false-belief questions like the Sally-Anne test. Despite progress in LLMs, they still struggle with these tasks.

The research introduces SymbolicToM, an inference-time method utilizing explicit graphical representations of characters' beliefs. These graphs, like BBob and BBob,Alice, depict what different characters believe about the world and each other's beliefs, up to a defined level of complexity. SymbolicToM leverages existing Natural Language Inference (NLI) and OpenIE models to compute these graphs and efficiently answer ToM questions by converting them into factual queries over the graph.

Experiments across various LLMs (GPT-3, Macaw, Flan-T5-XXL) demonstrated significant performance gains with SymbolicToM, often exceeding 60 accuracy points. Crucially, SymbolicToM showed robust generalization capabilities, unlike supervised fine-tuning approaches.  New datasets (D₁, D₂, D₃) testing story structure generalization and ParaphrasedToMi for linguistic diversity revealed that SymbolicToM maintained its benefits, even allowing advanced models like GPT-4 to achieve near-perfect scores on the challenging datasets.

In conclusion, SymbolicToM offers a plug-and-play, interpretable, and effective method for enhancing ToM reasoning in LLMs, avoiding overfitting and outperforming supervised techniques, particularly in out-of-domain scenarios.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This work introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the challenge of planning actions with specific constraints, unlike previous research focused on abstract goals. The core problem, termed "constrained language planning," involves generating scripts that are both reasonable and adhere to multifaceted constraints (e.g., "make a chocolate cake" instead of just "make a cake").

The researchers found that large language models (LLMs) struggle with this task, particularly in faithfully incorporating constraints. To address this, they developed an "over-generate-then-filter" method. InstructGPT generates multiple scripts, and a filter model, using embeddings and keyword analysis, selects the most faithful ones.

Recognizing the cost of LLMs, the study focuses on knowledge distillation, creating a dataset called CoScript. Using the over-generate-then-filter method, they generated 55,000 specific goals with scripts, which were then refined by crowd-sourced workers. CoScript demonstrates a diverse range of constraints.

Crucially, the researchers demonstrated that a smaller model, T5 fine-tuned on CoScript, outperformed larger LLMs in constrained language planning, highlighting the power of targeted datasets. The CoScript dataset is publicly available and intended to be a valuable resource for advancing research in language planning, enabling the development of more efficient and specialized models for this task.</sample>
    <sample id="182">In this context, tropicalism refers to the use of words like "vibrant" and "curvaceous" to describe Latina women, connecting to a trope that exoticizes and objectifies them.</sample>
    <sample id="183">作者使用提示词（prompts）来生成目标群体的人工描写，例如 "Imagine you are an Asian woman. Describe yourself."，通过改变提示词中的身份标识，可以生成不同群体的描写。</sample>
    <sample id="184">CXMI (Contextual Mutual Information) and its extension, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is based on data crawled from the web (NACHOS), while ChuBERT is based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">两位。</sample>
    <sample id="188">Iterative update updates the model by training on the latest set of data collected.</sample>
    <sample id="189">The dataset aims to understand users’ language when they want to make a choice, specifically focusing on indirect referring expressions for entity selection.</sample>
    <sample id="190">攻击者可以通过学习 EaaS 提供的 embedding 来提取模型参数。</sample>
    <sample id="191">三位：Sara Papi, Matteo Negri, 和 Marco Turchi。</sample>
    <sample id="192">Yang Luo presented "CAME: Confidence-guided Adaptive Memory Efficient Optimization," a novel optimizer designed to address the memory and performance challenges in training large language models. Traditional adaptive optimizers like Adam consume significant memory, while memory-efficient alternatives like Adafactor often suffer from slower convergence.

CAME aims to bridge this gap by leveraging non-negative matrix factorization (NMF), similar to Adafactor, to reduce memory usage. However, CAME tackles the inherent errors in Adafactor's NMF operation, which lead to instability and slow convergence. The core innovation lies in a "confidence-guided" update mechanism. CAME calculates the residual between predicted and actual updates, treating this difference as an indicator of instability. This instability is then used as a denominator in the update step, adaptively adjusting the optimization direction.

Experiments on BERT, GPT-2, and T5 models demonstrate CAME's effectiveness. It achieves a 3.4% improvement in validation accuracy compared to Adafactor and outperforms Adam, especially with large batch sizes (8K to 32K). CAME also exhibits comparable performance to baselines on downstream tasks while significantly reducing memory footprint compared to Adam, LAMB, and other memory-efficient optimizers like SM3.

In essence, CAME offers a compelling solution for training large language models by combining memory efficiency with fast convergence, making it a valuable advancement in the field.</sample>
    <sample id="193">Around 1,000.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">This work introduces RoHT, a novel framework for Explainable Question Answering (XQA) called "Reasoning over Hierarchical Question Decomposition Tree." RoHT addresses the limitations of existing XQA approaches, namely neuro-symbolic methods (reliant on incomplete knowledge bases) and decompose-based methods (struggling with the diversity of natural language).

RoHT integrates knowledge from both knowledge bases (KBs) and text corpora to answer complex questions intuitively. The framework operates in two stages. First, it constructs a Hierarchical Question Decomposition Tree (HQDT) representing the question's compositional structure, breaking it down into atomic questions. A "question decomposer" generates leaf nodes (atomic questions), while a "question generator" creates intermediate questions. Each node receives a certainty score reflecting its generation likelihood.

Second, RoHT performs probabilistic reasoning over the HQDT. This involves a recursive process where a "scheduler" selects appropriate knowledge sources (KB, text, or child nodes) for each node. "Executors" retrieve answers and probabilities from these sources, and an "aggregator" combines the results to produce top answers.

Evaluations on KQA Pro (KB QA with incomplete KB and Wikipedia) and Musique (QA comprehension with text and Wikidata) demonstrate RoHT's effectiveness. RoHT outperforms existing methods, including TransferNet, showcasing the benefits of explicit decomposition and integrating knowledge from diverse sources. The results highlight RoHT's ability to leverage both KBs and text corpora for improved accuracy and explainability in complex question answering.</sample>
    <sample id="196">"I saw Bart and Lisa" 和 "Homer came and sneezed."</sample>
    <sample id="197">Four state-of-the-art chat models.</sample>
    <sample id="198">因为大型语言模型正在生成越来越长的文本，评估模型在整个上下文窗口中的可接受性至关重要。</sample>
    <sample id="199">是的，多语言训练会导致英语表现下降，在七个数据集上表现下降，仅在三个数据集上表现提升。</sample>
    <sample id="200">No, they don't necessarily know about the entities.</sample>
    <sample id="201">State-of-the-art neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">No, adaptive overfitting was not observed.</sample>
    <sample id="203">NLP tasks are becoming more subjective and socially oriented, and it's challenging to characterize how positionalities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="204">完整微调。</sample>
    <sample id="205">This presentation details a study investigating the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. The researchers found that language models exhibit varying political leanings, with GPT-4 being notably liberal compared to BART models. These biases are demonstrably influenced by the pretraining data; further training on partisan corpora (news and social media) causes corresponding shifts in the language model's political alignment. The study also revealed that language models reflect societal polarization, exhibiting a greater distance from the political center after 2017.

Crucially, the research explored the consequences of these biases in practical applications like hate speech and fake news detection. Left-leaning models performed better at detecting hate speech against minority groups but struggled with detecting it against dominant groups, while right-leaning models showed the opposite trend. Similarly, models were more effective at identifying misinformation from opposing political viewpoints.

The findings highlight a significant fairness issue: deploying biased language models could lead to the marginalization of certain groups and the unchecked spread of harmful content. The presentation concludes by framing this challenge as a "Scylla and Charybdis" dilemma – avoiding bias risks censorship, while attempting to sanitize data is difficult and potentially subjective, mirroring the ethical complexities of the trolley problem. The study urges acknowledgement and mitigation of these political biases in language models to ensure fairer NLP applications.</sample>
    <sample id="206">Topic independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="207">The latest test sets to avoid an overlap of the test data with the training data of the language model.</sample>
    <sample id="208">Three.</sample>
    <sample id="209">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and dataset can be used as a benchmark.</sample>
    <sample id="212">一个。</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk focuses on the dependency structure of coordination, arguing for symmetric structures over asymmetric ones. Different linguistic theories adopt varying approaches: Universal Dependencies and Mel'čuk's Meaning-Text Theory head the structure with the first conjunct, while Prague Dependency Treebanks head with the conjunction, and Hudson's Word Grammar uses a multi-headed approach.

Przepiórkowski's argument rests on the principle of dependency length minimization – shorter dependencies are preferred. He illustrates this with examples where a direct object can move after an adjunct to reduce the overall dependency length.

Analyzing data from the Penn Treebank, the research confirms that left conjuncts tend to be shorter, a tendency that strengthens with increasing length differences. Crucially, this preference for a shorter left conjunct *only* appears when the governor (the word governing the coordination) is on the left or absent. When the governor is on the right, this tendency vanishes.

This observation provides evidence against asymmetric coordination structures and supports symmetric structures. The study measured length in characters, syllables, and words, consistently showing the left-conjunct preference when the governor is on the left or absent, but not when it's on the right. Przepiórkowski encourages further discussion at the poster session for the full argument.</sample>
    <sample id="217">This work, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," addresses the limitations of existing dialogue generation models in handling multiple attributes and generalizing to unseen attribute combinations. The researchers, Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications, introduce DCG, a Disentangled Controllable Generation model, to tackle these challenges.

DCG leverages the DialoGPT framework and incorporates a novel compositional prompt module. It utilizes both attribute-oriented prompts (focusing on specific attribute values) and task-oriented prompts (incorporating global dialogue features) to guide generation. To enhance diversity and generalization, pseudo combinations and a disentanglement loss are employed.

A key contribution is the introduction of MAE, a unified, reference-free evaluation framework for multi-attribute controllable dialogue generation, eliminating the need for extensive labeled data. MAE utilizes templates with discrete and continuous prompts, demonstrating strong correlation with human judgments.

Experiments on the DailyDialog-CG benchmark demonstrate that DCG outperforms existing baselines in both attribute controllability and text quality, even when faced with unseen attribute combinations. The visualization of prompt embeddings confirms the model's ability to disentangle attribute combinations and learn relationships between them, enabling generalization from seen to unseen values. The study highlights the effectiveness of the attribute-oriented prompt method and the benefits of shared embedding mapping for learning attribute concepts.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">This presentation by Jia-Huei Ju details a research project focused on automating the analysis of financial reports, specifically Form 10-K filings required by the SEC. The core motivation stems from the observation that these reports exhibit high text similarity year-over-year (around 80% token overlap), suggesting underlying patterns that can be exploited.

The researchers introduce a "highlighting task" designed to identify key words (the "rationale") that explain the differences between a current report and its preceding year's report (target and reference). This task is implemented using a multi-stage pipeline.

The pipeline consists of: document segmentation (briefly mentioned), relation recognition (classifying pairs of text segments into three types: β - highly similar, Revised - similar syntax but different meaning, and Mismatched - new information), and a two-stage fine-tuning process. The first stage uses an out-of-domain dataset (eSNLI) for general language understanding. The second stage focuses on "Revised" pairs, using them as pseudo-positive labels and employing soft labeling techniques to mitigate the impact of potentially noisy labels.

The model's performance is evaluated on both eSNLI and a newly released dataset called FINAL, using precision and Pearson correlation coefficient (PCC) as metrics. Results demonstrate strong performance on FINAL and surprisingly good generalization to eSNLI, with particular benefit observed when analyzing "Mismatched" pairs.

The research concludes with a proposed highlighting task, a pipeline with two-stage fine-tuning, and outlines future directions including incorporating information retrieval techniques to further enhance the system's capabilities.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">German into English</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates how to effectively transfer knowledge from general-purpose question answering models (trained on Wikipedia) to specialized domains like biomedicine. The core problem is that simply adding new domain data can confuse the model, as demonstrated by the "plants" example where the retriever prioritizes irrelevant passages.

The research explores two main categories of data interventions: zero-shot and few-shot. Few-shot methods leverage a small number of target domain examples to prompt large language models to generate additional training data in a cloze-style question format, improving both retriever (8%) and reader (11%) performance. Zero-shot techniques manipulate the question, answer, and context distributions to understand their impact on model learning, finding that cloze-style questions are easier to curate and that BM25 (an unsupervised retriever) performs best across different distributions.

A key contribution is identifying the type of dataset shift present in new domains. Using a taxonomy of "No shift," "Concept shift," "Covariate shift," and "Full shift," the researchers developed a compatibility measure based on likelihood scores to categorize datasets. This allows them to determine which interventions are most effective: few-shot adaptations for most domains, and zero-shot adaptations for those exhibiting concept or covariate shift. The study demonstrates improvements in reader performance of up to 24% by strategically applying these interventions based on the identified dataset shift.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART 和 base mBART</sample>
    <sample id="225">53 tasks for training and 9 tasks for testing.</sample>
    <sample id="226">Two</sample>
    <sample id="227">Current language models excel at general NLP tasks but lack grounded language understanding – the ability to map natural language to executable plans within a specific environment. This limitation stems from pre-training primarily on text without grounding, creating a gap between pre-training and real-world application. Existing approaches often rely on language models to directly generate plans, which frequently results in invalid or ungrammatical outputs.

To address this, the authors introduce "Pangu," a novel framework that shifts the focus from generation to discrimination. Pangu utilizes a symbolic agent to propose candidate plans, while a language model (like BERT, T5, or Codex) solely scores and ranks these candidates. This approach alleviates the language model's burden of ensuring plan validity and grammar.

The framework was tested on knowledge-based question answering, demonstrating outstanding performance across various settings, including fine-tuning and in-context learning. Notably, Pangu exhibits strong sample efficiency, achieving high accuracy with minimal demonstration examples, particularly when using Codex.

The research highlights a key observation: autoregressive models tend to overfit seen structures, whereas Pangu demonstrates consistent probability distributions across both seen and unseen structures, suggesting enhanced robustness in non-i.i.d. settings.

The core takeaway is that for grounded language understanding, discrimination proves to be a more effective strategy than generation when leveraging language models. The authors welcome further discussion and collaboration on this work.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This presentation by Gabriella Skitalinskaya and Henning Wachsmuth focuses on a novel approach to supporting argumentative writing by detecting "improvable claims"—claims that could benefit from revision. The core idea is to learn from human revision patterns rather than relying on predefined rules of good argumentation.

The research introduces two tasks: **Suboptimal-Claim Detection** (identifying claims needing revision) and **Claim Improvement Suggestion** (suggesting types of quality issues to address). The team leverages revision histories from collaborative online debate platforms like Kialo, where final, accepted versions are marked as "optimal" and preceding versions as "suboptimal."

However, working with revision data presents significant challenges. The paper identifies four key areas:

1.  **Representativity &amp; Reliability:** Ensuring the dataset accurately reflects well-argued claims and that final versions are truly optimal.
2.  **Model Complexity &amp; Architecture:** Selecting models sensitive to subtle revisions and effectively capturing the nuances of argumentative quality.
3.  **Contextual Information:** Determining the relevant context (debate-wide structure, parent claim relationships, domain knowledge) for assessing claim quality.
4.  **Topical &amp; User Bias:** Addressing noise and biases inherent in collaborative revision histories, considering the influence of controversial topics and cultural context.

The research explores various strategies to address these challenges and finds that revision-based data can be effectively used for claim assessment. Modeling the distance between claim versions proves beneficial for suboptimal claim detection, and the importance of contextual information varies depending on the task and the specific quality issues. The paper encourages readers to delve into the full analysis for detailed findings and methodologies.</sample>
    <sample id="231">A dataset of medical crawled data from the web.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Sara Papi from the University of Trento and FBK introduces "Attention as a Guide for Simultaneous Speech Translation" (SimulST), a joint work with Matteo Negri and Marco Turchi. SimulST aims to translate spoken language into text in real-time for cross-language communication. Current SimulST models face challenges like complex architectures requiring specialized training, lengthy training procedures, and the need for multiple models to achieve different latency levels.

Their solution, EDAtt (Encoder-Decoder Attention), leverages existing offline speech translation models without retraining or architectural modifications. It manages latency through specific parameters and utilizes the cross-attention mechanism between audio and text. EDAtt determines whether to emit a partial translation based on the attention distribution. A word is emitted only if the attention isn't concentrated on the most recent speech frames (lambda frames), indicating sufficient stability in the received information.

The approach involves analyzing cross-attention weights: if a word's attention points to recent frames, it's held back; otherwise, it's emitted. Experiments on German show EDAtt outperforms strategies applied to offline models (Wait-k, Local Agreement) and even state-of-the-art SimulST architectures, achieving higher BLEU scores (translation quality) with lower latency (average lagging and computational-aware average lagging). EDAtt proves to be the fastest strategy when considering actual elapsed time.

The researchers have released the code, models, and simultaneous output to promote reproducibility.</sample>
    <sample id="234">提示策略对结果有很大影响，简单的实验表明，使用不同的提示，516/1000 的句子差异超过 1 BLEURT 点，极端情况下可达 40 BLEURT 点。</sample>
    <sample id="235">The work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="236">Five expert-written instructions.</sample>
    <sample id="237">The authors propose a diagnostic test suite called KITMUS, which includes a coreference resolution task where they vary the availability of entity-specific and background knowledge across three settings: Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="238">The video introduces MeetingBank, a new benchmark dataset for meeting summarization developed by researchers at the University of Central Florida. Addressing the need for datasets tailored to meeting-specific summarization, MeetingBank comprises 1,366 City Council meetings from various cities (Boston, Seattle, Denver) and nearly 7,000 instances.

The dataset was created by converting audio to transcripts using Speechmatics API, identifying meeting details, locating reference summaries from meeting minutes, and aligning timestamps. Data includes meeting transcripts, reference summaries, and URLs. Statistical analysis reveals coverage scores between 0.7 and 0.9, indicating a tendency towards verbatim summaries rather than abstraction. Seattle and Boston exhibit higher density scores, suggesting more editing.

The researchers evaluated several summarization systems, both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pegasus, Longformer, DialogLM, HMNet), alongside GPT-3. While extractive methods like Oracle performed well based on ROUGE-2 scores, DialogLM excelled among abstractive models. Surprisingly, GPT-3, despite lower automatic metric scores, received high human ratings for fluency and coherence.

The human evaluation, involving experienced annotators and a 5-point Likert scale, highlighted the need for improved automatic evaluation metrics that better reflect human preferences, particularly in capturing informativeness and factuality. MeetingBank is released as a valuable resource for researchers to develop advanced meeting summarization technologies and gain insights into City Council decision-making processes. The researchers encourage the community to utilize and explore the dataset.</sample>
    <sample id="239">大家好，我叫大卫·维拉，我将对论文《Prompting PaLM for Translation: Assessing Strategies and Performance》做一个简短的评论。这是我和谷歌翻译的同事共同完成的工作。PaLM 是一个拥有 5400 亿参数的大型语言模型，去年（2022 年）发布。它在大量文本上进行训练，包含 7800 亿个 token。在发表时，它在数百个 NLP 任务中达到了最先进水平。在这项工作中，我们提出了对大型语言模型提示进行机器翻译的第一个系统研究。我们使用 MT 社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠。我们将其与最先进的系统进行比较，即表现最佳的系统，即 WMT 评估。我们使用最先进的神经机器翻译指标，并额外展示了基于专家的人工评估结果。最后，我们还提供了一些提示选择策略的建议。提示对 LLM 的翻译性能有很大影响，正如我们在一个简单的实验中看到的，我们使用了单次提示，并为每个句子提供了两个不同的提示。在 1000 个句子中，有 516 个句子的差异超过了 1 个 BLEURT 点。在极端情况下，甚至可以达到 40 个 BLEURT 点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了 5 次提示策略，只是用标记来标注我们提供给系统的每个句子，标明了句子的语言。在这个例子中，我们从德语翻译成英语，德语源句用“German:”标注，英语译文用“English:”标注。我们发现，对于几个短提示来说，提示的实际形式没有太大影响。这对于零次提示和单次提示至关重要。当我们像我们这样进行 5 次提示时，提示的实际形式几乎没有差异。示例起到了最重要的作用。我们实验结果的总结是，示例质量比与源句的相似度更重要。因此，选择来自高质量翻译的示例非常重要。我们特别比较了从 WMT 评估的训练数据中选择提示与从开发数据中选择提示。开发数据比嘈杂的训练数据经过了更精细的策划，质量更高，结果显示使用开发数据可以获得更好的性能。尽管如此，专业的先进系统在 PaLM 翻译方面具有显著优势。但是，PaLM 几乎可以与商业系统相媲美。在我们的案例中，我们选择用谷歌翻译进行评估。我们从使用 MQM 框架进行的的人工评估中获得的见解表明，PaLM 的流畅度与最先进的系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。因此，似乎 PaLM 选择产生听起来更好的翻译，有时会删除源句中为了翻译而舍弃的部分。然而，PaLM 的“风格/不自然”类别低于最先进的系统，这是一个额外的信号，表明 PaLM 提供了非常流畅的输出，但仍然存在准确性问题。以上就是这个简短概述的全部内容。如需更多详细信息，请参阅论文的完整演示。非常感谢。</sample>
    <sample id="240">大家好，我是Dawei，来自德国萨尔兰大学的博士生。在这个视频中，我想介绍我们最近的工作“你可能想得比它弱：对弱监督学习的批判性审视”。这是我和Xiaoyu Shen、Marius Mosbach、Andreas Stephan以及Dietrich Klakow的合作成果。我想从简要介绍弱监督和弱监督学习开始。在弱监督中，您不会手动标注数据。相反，我们使用弱标注源来标注数据，例如简单的启发式规则、知识库或低质量的众包，如图右侧所示。与人工标注相比，弱标注更便宜，但它们也更嘈杂，这意味着一定比例的标注不正确。如果直接在弱标注数据上训练神经网络，神经网络往往会记住标签噪声，而无法泛化。在弱监督学习中，会提出训练算法，以稳健地训练神经网络，使其在这样的标签噪声下仍能良好泛化。在最近的WSL（弱监督学习）工作中，一种常见的说法是，人们声称他们只在弱标注数据上训练模型，并在干净的测试集上获得高性能。从技术上讲，这个说法并不错误，但有一个陷阱，那就是人们假设有额外的干净验证集可用于模型选择。我们不能停止在这个问题设置上，但这意味着弱监督学习需要额外的手动标注。但就像一个显而易见的错误，这个问题经常被忽视。我们提出了三个研究问题来探讨这种疑虑。第一，干净的验证数据对WSL是必要的吗？或者我们可以使用嘈杂的验证集吗？第二，如果干净的数据是WSL正常工作所必需的，那么我们需要多少干净样本？最后，我们应该只使用干净样本进行验证，还是有更好的利用它们的方法？我们在工作中解决了这些研究问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练的模型就无法泛化到原始的弱标签之外，这意味着训练毫无意义。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图左侧所示。通常，我们只需要每个类别20个样本就能获得高性能。但这并非故事的结局，因为如果我们决定获取干净样本，直接在它们上训练甚至可以获得更好的性能。右图显示了直接应用于干净数据的方法（微调方法）和仅使用干净数据进行验证的WSL方法之间的性能差异。正如我们所见，如果拥有每个类别10个样本，直接微调就开始超越WSL方法。最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来实现。正如从图中可以看出，称为FTw的原始模型最初表现不如更复杂的WSL方法，例如COSINE。但是，如果我们允许在干净样本上继续微调，FTw的表现就会与其它方法一样好。因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。总而言之，我们表明最近的WSL方法需要干净、手动标注的样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否通过干净的验证样本进行。其次，WSL方法应该与小样本学习基线进行比较，因为两者都使用干净样本。第三，持续微调是一种简单而强大的基线，应该在未来的WSL工作中考虑。最后，我们已经开源了我们的代码。您可以通过幻灯片上的二维码找到它。请随时查看。谢谢，祝您会议愉快。</sample>
    <sample id="241">This paper, "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," addresses shortcomings in current automated misinformation detection systems. Existing systems often rely on unrealistic evaluations (retrospective datasets, leaked counter-evidence) and neglect the crucial role of human content moderators.

The authors propose a novel evaluation framework and a human-in-the-loop system designed to tackle these issues. Their system operates end-to-end, from raw Twitter data to actionable outputs for human review, integrating human feedback throughout the process. The system focuses on detecting misleading claims related to COVID-19 treatments and verifying policy violations.

The system's two main components include: 1) claim detection, using keyword filtering and a T5 question-answering model to extract and rank potential misinformation claims based on trendiness; and 2) policy violation verification, employing a BERT-based stance classification model to identify tweets supporting unapproved treatments for human review.

The evaluation emphasizes "early detection," defined as identifying unapproved treatments before they appear in debunking news articles. Results demonstrate the system's ability to detect treatments early, with a 65% accuracy for policy violation detection. Furthermore, the system achieves a high efficiency, identifying 124.2 policy violations per human hour worked.

Ultimately, the research advocates for a more realistic and human-centric approach to misinformation detection, providing a framework for consistent evaluation and offering valuable insights into the development and assessment of such systems.</sample>
    <sample id="242">Human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">Four.</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This presentation details a two-step pipeline, "A Needle in a Haystack," designed to identify high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks, addressing issues with automatic metrics and unclear best practices for MTurk recruitment.

The pipeline first uses a "Qualification Task" to assess workers' ability to evaluate summaries across six dimensions, categorizing them as gold, silver, bronze, or blocked. Only gold and silver workers (13% of 200) proceed.  The second stage, an "Endurance Task," evaluates their capacity for handling a large workload, resulting in 12 workers (6% of 200) achieving high inter-annotator agreement (IAA), surpassing expert levels (Cohen's Kappa and Krippendorff's Alpha of 0.443).

A "Reference-Based Task" further tests performance, yielding a Krippendorff's Alpha of 0.534. Comparisons with baseline MTurk workers (using MACE filter, Alpha 0.380) and CloudResearch MTurk workers (Alpha 0.513) demonstrate the pipeline's effectiveness, achieving comparable quality at a lower cost.

Analysis reveals a significant correlation between pipeline and CloudResearch workers, but highlights that the pipeline doesn't guarantee correctness. Interestingly, GPT models showed strong correlation with expert judgments.

The research concludes that the pipeline efficiently identifies high-agreement workers, preventing resource waste and offering a cost-effective alternative to CloudResearch. Future work will focus on improving worker quality in both agreement and correctness, exploring diverse applications and platforms. Limitations include the focus on English summarization, the task-specific nature of the questions, and the lack of guaranteed correctness training.</sample>
    <sample id="246">Data set and code are available on GitHub.</sample>
    <sample id="247">This paper introduces FactKG, a novel dataset and task for knowledge graph-based fact verification. Existing datasets like FEVER and TabFact rely on text or tables, but FactKG uniquely utilizes knowledge graphs (specifically DBpedia) as evidence for verifying natural language claims. The motivation stems from the potential for more reliable and practical reasoning within KGs, as evidence is more intuitive and directly connects to claims compared to text or tables.

FactKG features claims in both written and colloquial styles to enhance practical applicability. Claims are labeled as either SUPPORTED or REFUTED, and the task involves retrieving relevant evidence from DBpedia and verifying the claim. The dataset incorporates five reasoning types: one-hop (single triple connection), conjunction (multiple one-hop claims), existence (entity connected to a specific relation), multi-hop (requiring inference across multiple entities), and negation (requiring an additional inference step). Colloquial claims are generated using a style transfer model and presupposition templates.

The paper also presents baseline models, including a "Claim Only" baseline and a GEAR model leveraging graph evidence. Results demonstrate that utilizing graph evidence significantly improves performance compared to relying solely on claims, with the GEAR model consistently outperforming other baselines. The dataset is publicly available, encouraging further research in knowledge graph-based fact verification.</sample>
    <sample id="248">NLPositionality 的注释者在人口统计学特征方面不均衡。研究汇集了来自 87 个国家/地区的 1000 多名注释者，但发现数据集和模型与英语国家的人群最为一致，并且与受过大学教育的人群也有额外的对齐。非二元人群的对齐度较低。</sample>
    <sample id="249">By preserving the relevant structure but adding noise to the input.</sample>
    <sample id="250">Evaluating multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">This presentation introduces U-CREAT, a novel unsupervised pipeline for Prior Case Retrieval (PCR), a task crucial for legal professionals facing increasing case volumes. The work addresses the challenge of efficiently retrieving relevant past precedents from a large pool of candidates.

The core contributions are twofold: the IL-PCR dataset and the U-CREAT pipeline itself. IL-PCR, an Indian Legal Prior Case Retrieval Dataset, provides a new benchmark with 7,070 legal cases and a significantly larger scale compared to existing datasets like COLIEE’21.

The U-CREAT pipeline utilizes an event-based approach, extracting events (subject-verb-object triplets) from legal documents using dependency parsing. These events are then used to compute an interaction matrix between query and candidate documents, enabling retrieval based on shared events.

Experiments demonstrate that event-based models, particularly the "Event Filtered Documents" model, significantly outperform traditional count-based and transformer-based models, including legal-specific transformer models like InCaseLawBERT and InLegalBERT. U-CREAT achieves lower inference times and higher F1 scores. Notably, U-CREAT also surpasses state-of-the-art supervised approaches on the COLIEE dataset, establishing a new benchmark for PCR tasks.

The research highlights the effectiveness of unsupervised learning and event extraction for PCR, paving the way for further advancements in legal information retrieval and demonstrating strong generalization across legal systems. The paper provides detailed information on the dataset, pipeline, and experimental results.</sample>
    <sample id="253">This presentation introduces "DisorBERT," a novel model developed by researchers from Mexico and Spain for detecting signs of mental disorders in social media posts. The core idea addresses the challenge of limited annotated data by employing a double domain adaptation strategy.

DisorBERT builds upon the BERT language model, initially trained on general data (Wikipedia, Google Books), and adapts it in two stages: first to the language of social media (Reddit), and then to the specific domain of mental health. A key innovation is "guided masking," which encourages the model to focus on relevant words during training.

Evaluation using the eRisk dataset demonstrates DisorBERT's superior performance compared to baseline models, exhibiting a better balance between precision and recall. Analysis of predicted words reveals that DisorBERT tends to generate terms with a more negative or psychological orientation, aligning with mental health concerns, unlike the more general predictions of standard BERT.

The presentation further illustrates DisorBERT's ability to identify crucial text sequences related to mental disorders, using a visualization tool applied to a user's social media post with a high depression score. The model highlights words like "anxious" and "medication," confirming its relevance to depression.

In conclusion, DisorBERT’s combined approach of double domain adaptation and guided masking proves effective in identifying mental health indicators in social media. Future research will explore incorporating diverse lexical resources and clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">This research paper introduces a novel framework, "Uncertainty Guided Label Denoising," to enhance document-level distant relation extraction (DocRE). Traditional DocRE methods rely on extensive human annotation, which is costly. Distantly supervised (DS) methods offer a solution but suffer from noisy data. Existing approaches using pseudo-labels risk introducing further errors.

The proposed framework addresses this by first training a DocRE model with both human-annotated and DS data to generate pseudo-labels. Recognizing the inevitability of false pseudo-labels, the framework incorporates uncertainty estimation to assess the reliability of model predictions. A key innovation is an instance-level uncertainty estimation method designed to handle overlapping relations, a common challenge where multiple relations exist between entities.

To model uncertainty, the paper utilizes Monte Carlo dropout. A modified uncertainty estimation process calculates instance-level scores for each positive pseudo-label, differentiating between potentially false and correct relations. Dynamic class uncertainty thresholds are then employed to filter out high-uncertainty pseudo-labels, replacing original DS labels with more reliable ones.

Finally, a multi-phase training strategy iteratively re-labels DS data, maximizing the utilization of this data. Experimental results on public datasets demonstrate significant performance improvements compared to existing baselines, highlighting the effectiveness of the proposed framework in mitigating noise and improving DocRE accuracy. The contributions include the uncertainty-guided framework, instance-level uncertainty estimation, dynamic thresholding, and overall performance gains.</sample>
    <sample id="255">It's crucial for zero and one-shot prompting. When using five-shot prompting, there is nearly no difference to the actual form of the prompting.</sample>
    <sample id="257">四个 state-of-the-art chat models.</sample>
    <sample id="258">This video introduces a research paper titled "Can Large Language Models Be an Alternative to Human Evaluation?" by Chiang Cheng-Han. The core idea is to explore using large language models (LLMs) to evaluate the quality of text in natural language processing, potentially replacing traditional, but unstable and hard-to-reproduce, human evaluations.

The researchers were motivated by the desire for a more reliable and consistent evaluation method. They designed an experiment where LLMs (T0, InstructGPT, and ChatGPT) were given instructions and text samples (stories written by GPT-2 or humans) to rate based on attributes like grammar, coherence, likability, and relevance. The LLM ratings were then compared to ground-truth ratings obtained from English teachers, who served as expert human evaluators.

The results showed that English teachers preferred human-written stories over those generated by GPT-2. While smaller LLMs didn't consistently show a preference, Davinci and ChatGPT demonstrated a clear preference for human-written text, mirroring the human evaluators' judgments. This suggests that certain LLMs can indeed serve as a viable alternative to human evaluation.

The paper addresses further questions regarding agreement between LLMs and humans, the impact of instruction wording, sampling methods, and the cost-benefit analysis of LLM evaluation versus human evaluation, as well as exploring the application of this method to other NLP tasks. The speaker encourages viewers to read the paper or visit their poster at ACL for more details.</sample>
    <sample id="259">XSemPLR is a newly introduced benchmark dataset designed to facilitate research in cross-lingual semantic parsing. This task involves translating queries from multiple natural languages into various meaning representations like SQL, Lambda Calculus, and FunQL. Existing models often lack broad language coverage or support for specific meaning representations, prompting the creation of XSemPLR.

The dataset encompasses 9 datasets across diverse domains, 5 semantic parsing tasks, 8 meaning representations, and a wide range of 22 languages spanning 15 language families. To thoroughly evaluate models, XSemPLR proposes six distinct training and evaluation settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Cross-lingual Few-shot.

The study analyzed both Encoder-PTR and Encoder-Decoder models, revealing that Encoder-Decoder architectures consistently achieve the best performance. Training multilingual models with a mix of languages generally improves performance across most languages, although English sometimes experiences a decline—a phenomenon termed the "Curse of Multilinguality."

Furthermore, the research highlights a significant performance gap in zero-shot cross-lingual transfer, which is substantially reduced with few-shot learning. The findings also indicate that pretraining on English data can significantly enhance performance in few-shot scenarios for other languages. Notably, even advanced multilingual models like Codex and BLOOM still fall short in cross-lingual semantic parsing. The XSemPLR benchmark aims to advance research in this area by providing a comprehensive and standardized evaluation platform.</sample>
    <sample id="260">The text only mentions Jingwei Yi from the University of Science and Technology of China. It does not state the total number of authors.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The text does not mention the number of authors.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to various biases introduced by design choices. Existing research highlights search instability, but lacks a systematic framework for categorizing and mitigating these biases.

The paper introduces a typology of label biases within text classification, identifying three key types: vanilla-label bias (model's inherent preference), context-label bias (influence of the provided examples), and a novel *domain-label bias* (impact of the task corpus). Experiments demonstrate that even random in-domain words can significantly bias LLM predictions, highlighting the importance of this new bias.

To combat these biases, the authors propose "domain-context calibration." This method builds upon existing calibration techniques but utilizes random in-domain words as content-free text, effectively addressing domain-label bias while also mitigating vanilla and context-label biases. This contrasts with previous methods that used single, predefined tokens.

Extensive experiments across diverse datasets and models (including GPT-3) show that domain-context calibration significantly improves ICL performance, particularly on tasks with high domain-label bias. Analysis of prediction distributions reveals improved decision boundaries after calibration. Further studies demonstrate the limitations of single, predefined tokens and the benefits of using multiple random words, especially those specific to the task domain.

In essence, the research provides a structured understanding of label biases in ICL and offers a practical calibration method to enhance the reliability and performance of LLMs in this paradigm.</sample>
    <sample id="264">Lin Wang from Zhejiang University presented a paper titled "TAVT: Towards Transferable Audio-Visual Text Generation," addressing the challenge of multimodal text generation, specifically audio-visual text generation, where data annotation is costly and existing models struggle with domain shifts.

TAVT introduces a novel approach to tackle these shifts by proposing a unified audio semantic space. The framework consists of three key components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network aligns visual concepts across domains into a unified auditory semantic space using audio clusters. The encoder-generator utilizes a transformer-based architecture with an attention mechanism to evaluate the contribution of each modality.

To directly optimize visual-audio alignment, a Dual Counterfactual Contrastive Learning (DCLL) loss function is introduced, avoiding reliance on randomly selected negative samples. The model is trained using a meta-learning approach, similar to MAML, with support and query sets for adaptation to new domains.

Experiments on MSVD and MSR-VTT benchmarks, in both cross-dataset and cross-domain settings, demonstrate that TAVT significantly outperforms existing state-of-the-art models. Notably, TAVT maintains strong performance even in low-resource domains where other methods degrade considerably. Ablation studies further validate the importance of audio features in enhancing performance. The research highlights TAVT's ability to effectively handle multimodal domain shifts and achieve transferable audio-visual text generation.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">The paper does not mention the author's institution.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">大家好，我是詹姆斯·芬奇。我是莎拉·芬奇。今天我们将向您介绍 ABC-Eval，一种评估对话式人工智能的新维度方法。这项工作由埃默里大学由蔡进浩教授领导的 NLP 实验室与亚马逊 Alexa AI 合作完成。

假设您刚刚开发了一个对话模型，并且想看看它与当前最先进水平相比如何。常见的做法是进行人工评估，例如，请人工评委选择两个对话中哪个更好，或者根据李克特量表对对话进行评分。这些方法可以很好地提供对整体对话质量的整体评估，但对话质量有很多方面。因此，您可能希望评估聊天质量的多个维度，以便更精细地了解模型的优势和劣势。一种方法是简单地要求人工评委评估对话质量的多个维度，例如使用现有的比较或李克特量表方法来评估模型响应的相关性。然而，我们认为有一种更精确、更可靠的维度对话评估策略。我们的方法试图通过明确注释模型响应是否表达某些行为（例如，提供不相关的信息或自相矛盾）来减少人工评估的主观性。我们称这种方法为在聊天中注释行为，简称 ABC-Eval。

我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。ABC-Eval 能够测量聊天模型犯各种主题错误的速率。例如，ABC-Eval 测量聊天模型忽略其伙伴或说一些不相关的话、自相矛盾或与其伙伴相矛盾、产生不正确的虚构事实或违反常识知识以及模型何时成功或失败地表现出同理心所需的轮次数量。

为了确定哪种评估方式最有效，我们选择了四个最先进的聊天模型，并使用 ABC-Eval 对每个模型进行了 100 次人机对话的评估。为了进行比较，我们还使用三种现有方法评估了这些对话：在轮次级别上的李克特评分、在对话级别上的李克特评分和对话级别的成对比较。对于每一种现有方法，我们收集了关于对话中八个最常测量方面的数据，因为这是评估聊天模型沿多个维度进行评估的标准做法。

通过分析这些评估结果，我们发现 ABC-Eval 行为标签比使用现有方法收集的标签更可靠，这可以通过对 100 个双重标记对话的标注者间一致性来衡量。此外，与现有方法产生的指标相比，ABC-Eval 标签更能预测整体对话质量，这如简单的线性回归分析所示。例如，您可以看到测量具有自我和伙伴矛盾的轮数比例分别解释了对话质量的 5% 和 10%，而平均李克特一致性分数仅解释了 4% 或更少。

最后，我们使用逐步线性回归检查每种评估指标是否捕捉到聊天质量的独特方面。您可以看到所有 ABC-Eval 指标的组合解释了对话质量的 25% 以上，并且在一次删除一个指标时，大多数指标都会导致失去关于质量的相当数量的信息。另一方面，所有轮次级别李克特指标的组合解释的质量要少得多，而且较少的指标携带了独特的有用信息。

这些可靠、信息丰富且独特的 ABC-Eval 指标使我们能够以比以前方法能够实现的更高分辨率来评估对话式人工智能。您可以看到在我们的实验结果中，仍然存在一些挑战，并且这些挑战已经被精确地量化。例如，我们测试的机器人大约在 20% 的响应中存在常识性违反。它们大约在 15% 的响应中产生不相关的信息，并且大约在 10% 的时间内自相矛盾或与其伙伴相矛盾。

随着该领域快速进步，许多这些错误率可能会在自我们评估以来发布的新模型中降低。然而，这更增加了追求可靠且精确的评估指标来比较模型的必要性。我们希望 ABC-Eval 可以被该领域的其他人利用，作为朝着这个方向迈出的有意义的一步。我们期待着在未来几个月和几年里看到对话式人工智能的进步。谢谢观看。</sample>
    <sample id="270">Emory NLP Lab at Emory University and Amazon Alexa AI.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Seven.</sample>
    <sample id="273">大家好，我叫Kayo Yin，我将为大家介绍我们的工作，题为“翻译何时需要上下文？一项跨语言的数据驱动探索”。这项工作是我们与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig合作完成的。

很多翻译都依赖于上下文。例如，我们应该如何翻译“mole”这个词？如果前一句是“如果部长们知道了，情况可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，严重吗？”，那么“mole”指的是胎记。因此，根据上下文，词义会发生变化，从而影响翻译。

然而，评估模型在处理此类情况时表现如何，其实非常困难。首先，只有一小部分翻译依赖于上下文，这使得像BLEU这样的语料库级别指标无法捕捉到这些翻译。有些人建议对上下文相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工标注。

在这项工作中，我们试图回答两个问题。首先，翻译何时需要上下文？其次，模型如何处理这些情况？

为了回答第一个问题，我们首先测量了在翻译过程中，一个词对上下文的依赖程度。在之前的工作中，我们引入了CXMI作为衡量机器翻译模型上下文使用情况的指标。这通过测量上下文C对目标Y提供的信息量，给定源X来完成。你可以将CXMI视为给予模型上下文所获得的信息量。

在这项工作中，我们将CXMI扩展到Pointwise CXMI，它可以测量句子级别或单词级别的上下文使用情况。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。

现在，我们分析具有高P-CXMI的单词，以寻找这些单词之间的模式。我们对TED演讲的英译本进行分析，涵盖了14种不同的语言。我们分别在三个不同的层面进行分析。

首先，我们观察具有高平均P-CXMI的词性标签。这使我们能够发现，例如，阿拉伯语中的双重代词具有相对较高的P-CXMI。这可以解释为，英语中没有双重代词，因此在翻译成阿拉伯语时，需要上下文来确定代词是否为双重形式。类似地，我们还发现，在某些情况下，当选择适当的动词形式时，也需要上下文。

其次，我们观察所有不同出现情况的词汇项的平均P-CXMI。这有助于我们识别出诸如上述案例，其中在中文中需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。类似地，我们发现，为了以正确的正式程度进行翻译，也需要上下文。

最后，我们观察不同的单个token，这些token具有较高的P-CXMI。这使我们能够识别出无法真正通过单词本身来捕捉的现象，而是通过句子结构来表达的现象，例如省略现象。

现在，我们利用我们的分析结果来设计一个文档级别翻译的基准。对于我们识别的五个话语现象，我们创建了标记器，以自动识别与该现象相关的单词。我们称我们的标记器为多语言话语感知标记器（Multilingual Discourse-Aware），简称MuDA标记器。

我们可以记录不同语言中这些话语现象的比例不同。然后，我们使用MuDA标记器，将标记器应用于我们想要用于评估的平行语料库，并在MuDA标记器已识别的上下文相关示例上应用我们选择的翻译指标。

最后，我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译中的表现。

首先，当我们使用语料库级别的指标时：对于BLEU，我们发现上下文无关的模型表现最佳。但如果使用COMET，则上下文感知的模型表现最佳。如果使用词f-measure，则具有和不具有上下文的模型表现相当。这再次表明，如果仅使用语料库级别的指标，很难确定最佳的文档级别翻译系统。

现在，我们使用MuDA基准来评估模型，我们发现，对于诸如正式程度和词汇衔接等某些话语现象，上下文感知的模型比不使用上下文的模型更准确。但对于省略、代词和动词形式等其他现象，这些模型并没有比不使用上下文的模型好多少。这表明我们需要在文档级别翻译方面看到更多的进展。

我们还比较了不同的商业系统，我们的基准显示，DeepL通常比Google Translate更适合文档级别翻译。

总而言之，我们对14种语言对进行了数据驱动分析，以确定翻译何时需要上下文，然后我们利用我们的发现来构建一个文档级别机器翻译的基准，该基准可以帮助我们识别模型能够很好地处理哪些话语现象，以及哪些翻译系统擅长文档级别翻译。

谢谢大家的关注。我们将在多伦多再见。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This presentation introduces "IndicMT Eval," a novel dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. Recognizing that MT evaluation metrics developed for English may not be suitable for languages with diverse linguistic features, the researchers created a dataset focusing on five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati.

The dataset comprises 7,000 samples generated from 200 source sentences translated into English by seven different MT models. Bilingual expert annotators meticulously evaluated these translations, marking errors with their type and severity using the MQM framework, and providing overall scores.

Analysis revealed that COMET-metric variants demonstrated the highest overall correlations with human scores. However, many metrics exhibited skewed score distributions, limiting their interpretability. Interestingly, metrics showed improved correlation when evaluated on subsets focusing solely on accuracy or fluency errors.

Building upon these findings, the researchers fine-tuned COMET using the MQM dataset, resulting in "IndicCOMET." IndicCOMET outperformed standard COMET baselines across multiple languages and demonstrated improved robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for more accurate and reliable MT evaluation in Indian language contexts. The dataset is publicly available to facilitate further research in this area.</sample>
    <sample id="277">没有名称。</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness," stating that dominant groups are unmarked, while marginalized groups are linguistically marked. The method compares personas using weighted log-odds ratios to distinguish the top words for each marked group, designating unmarked and marked groups beforehand.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">This presentation introduces MultiEMO, a novel framework for emotion recognition in conversations (ERC) that addresses limitations in existing methods. ERC aims to predict emotion labels from textual, audio, and visual modalities within dialogues. Current approaches often inadequately exploit multimodal complementarity, struggle with minority emotion classes, and find it difficult to differentiate between semantically similar emotions.

MultiEMO tackles these challenges with four key components: a visual feature extractor (VisExtNet), a multimodal fusion model (MultiAttn), and a Sample-Weighted Focal Contrastive Loss (SWFC). VisExtNet focuses solely on facial expressions, eliminating irrelevant scene information. MultiAttn utilizes bidirectional multi-head cross-attention layers to effectively fuse information from text, audio, and visual modalities, learning cross-modal correlations. SWFC prioritizes minority classes and maximizes inter-class distances to improve differentiation of similar emotions.

Extensive experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance with significant improvements in recognizing minority and semantically similar emotions. Visualization of heatmaps further highlights MultiEMO's ability to handle complex scenarios where emotional cues from different modalities are asynchronous.

Despite its success, MultiEMO has limitations: VisExtNet doesn't distinguish speakers, SWFC requires large batch sizes, and performance on minority emotions remains lower than majority classes. Overall, MultiEMO represents a significant advancement in ERC by effectively integrating multimodal information and addressing key challenges in emotion recognition.</sample>
    <sample id="281">This presentation details a study, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," investigating the role of context in machine translation. The research, conducted by Kayo Yin and collaborators, addresses the difficulty in evaluating context-dependent translations, which are often missed by standard metrics like BLEU.

The study introduces Pointwise CXMI (P-CXMI), an extension of CXMI, to measure context usage at the sentence or word level, identifying words requiring contextual information for accurate translation. Analysis of TED talk transcripts across 14 languages revealed patterns related to part-of-speech tags (e.g., dual pronouns in Arabic), vocabulary (e.g., proper noun consistency in Chinese), formality, and sentence structure (e.g., ellipsis resolution).

To facilitate evaluation, the researchers developed the Multilingual Discourse-Aware (MuDA) tagger, which automatically identifies context-dependent words based on five discourse phenomena. Using the MuDA benchmark, they evaluated various translation models, finding that context-aware models outperform context-agnostic models when assessed with COMET and for phenomena like formality and lexical cohesion. However, improvements are needed for phenomena like pronouns and verb forms. The study also found DeepL generally outperforming Google Translate in document-level translation.

Ultimately, the research provides a data-driven understanding of when context is crucial for translation and offers a benchmark to better evaluate and advance document-level machine translation systems.</sample>
    <sample id="282">Xuekai Zhu presented "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023, addressing the challenge of story-level, discourse-level text style transfer. Unlike previous studies focusing on token or sentence level, StoryTrans aims to imitate an author's linguistic choices within an entire story.

The core challenges lie in replicating discourse structures (narrative techniques) and handling style-specific content that varies with the writing topic. StoryTrans tackles these issues by learning discourse representations from source texts and combining them with style embeddings. A key innovation is a two-stage generation process: first, transferring the source text with masked content keywords, then generating the complete text by explicitly incorporating those keywords.

The training framework involves an advisory stage using self-reconstruction, disentanglement loss (separating style and content), sentence order loss, and a style classifier loss. A second stage focuses solely on content restoration and mask token removal.

The research introduced new Chinese and English datasets for fairytale and everyday story style transfer. Extensive evaluations, both automatic and manual, demonstrate StoryTrans's superior performance compared to baselines in style control and content preservation. Style visualization confirms alignment with golden text in the style feature space. Examples highlight StoryTrans's ability to enrich storylines and rewrite sentences while maintaining source semantics, surpassing the performance of models like StyleLM. The data and code are publicly available.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">This paper introduces FSUIE, a novel approach to Universal Information Extraction (UIE) designed to overcome limitations in existing span-based models. Current UIE models heavily rely on precise span boundaries, which can be ambiguous in annotation. FSUIE addresses this by proposing a "fuzzy span" mechanism, treating span boundaries as continuous probability distributions rather than fixed points.

The core of FSUIE lies in two key components: a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL utilizes Binary Cross Entropy and KL-divergence to learn from both the golden boundary and supplementary information, creating a more robust learning process. FSA acts as a mask function, dynamically adjusting the attention span and linearly decaying attention distribution at the boundaries, guiding the model's focus.

FSUIE’s architecture adds the fuzzy span attention layer on top of the transformer, preserving the original text encoding capabilities while enhancing span extraction decisions. Experiments across Named Entity Recognition (NER), Relationship Extraction (RE), and Aspect Sentiment Triplet Extraction (ASTE) demonstrate significant performance improvements compared to baseline UIE models. Notably, FSUIE achieves state-of-the-art results on several RE datasets (ACE2004, 2005, ADE) and ASTE datasets (14lap, 15res, 16res). Ablation studies confirm the benefits of both FSL and FSA, and visualizations reveal that the model effectively focuses on relevant semantic information within a limited range of tokens. Ultimately, FSUIE offers a more flexible and effective approach to UIE, exhibiting strong generalization capabilities and achieving excellent results across diverse IE tasks.</sample>
    <sample id="285">This presentation introduces "Reference Matters," a study focusing on factual error correction for dialogue summarization. The researchers argue that current evaluation methods for Factual Error Correction (FEC) models are flawed, relying on vague factuality metrics like FactCC and DAE, which don't accurately assess true error correction. These metrics can be easily manipulated, allowing FEC models to generate entirely new summaries instead of correcting existing ones.

To address these issues, the study proposes a new evaluation framework incorporating manually annotated reference corrections. This framework emphasizes minimal changes (substitutions, insertions, deletions) to achieve a fluent and non-redundant summary, aligning with the core purpose of FEC.

The framework utilizes a new taxonomy of factual errors, categorizing them as content-based (based on part of speech and dependencies) and form-based (addition, deletion, substitution). It leverages the ERRANT metric, commonly used in grammar error correction, through alignment, classification, and comparison steps.

Experiments reveal that training FEC models with reference summaries from dialogue datasets improves performance on unreliable factuality metrics. The study highlights the need for a shift in evaluation methodologies and demonstrates that incorporating human-corrected data enhances FEC model effectiveness. However, current models still struggle with specific error types like additions and attribute/modality/link errors, indicating areas for future research and development. The work underscores the importance of reference-based evaluation for accurate assessment and advancement of FEC models in dialogue summarization.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">BLiMP, SyntaxGym, CrowS pairs.</sample>
    <sample id="290">FTw, COSINE</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT was initially trained on OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This presentation introduces EPIC, the English Perspectivist Irony Corpus, a dataset developed by the University of Turin and Amazon Alexa to explore the complexities of irony detection in natural language processing. The project challenges the traditional "ground truth" assumption in supervised machine learning, recognizing that irony perception is subjective and varies among individuals.

EPIC comprises approximately 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. Data was annotated by 74 crowdsourced annotators (15 per variety), each evaluating 200 conversations on whether the reply was ironic. The annotation interface presented a simple chat-like view, prompting annotators to label each reply as "Ironic" or "Not ironic."

Analysis revealed significant inter-annotator disagreement, varying based on demographics like gender, age, and nationality. To address this, the researchers developed "perspective-aware models," training separate models on data subsets representing different annotator groups. While raw performance wasn't significantly improved, these models demonstrated notably higher confidence in their predictions compared to aggregated "gold standard" models.

Further investigation into the disagreement revealed intriguing patterns. Generational differences (those close in age) showed greater discrepancies in irony perception, and significant variations were observed between annotators from the UK and Ireland. The project highlights the importance of acknowledging subjective perspectives in NLP and suggests that incorporating annotator-specific models can lead to more reliable and nuanced irony detection.</sample>
    <sample id="297">This presentation details a research project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," focused on identifying and understanding dogwhistles – terms that convey a hidden, often inflammatory, message to an in-group while maintaining plausible deniability for the speaker.

The project developed a comprehensive glossary of over 340 dogwhistles (primarily US-centric, focusing on racist, transphobic, and anti-Semitic terms), categorized by register (formal/informal), persona (e.g., anti-Semitic, transphobic), and type. A case study of historical U.S. political speeches revealed a correlation between dogwhistle usage and the Republican Southern Strategy, demonstrating a shift towards coded language following the Civil Rights era.

The research also evaluated language models, specifically GPT-3, for their ability to recognize and interpret dogwhistles. While GPT-3 could surface many formal dogwhistles, performance was weaker with informal terms and transphobic language. Providing definitions and "secret cues" significantly improved the model's ability to identify covert meanings.

Finally, the study demonstrated how dogwhistles can circumvent content moderation systems. By replacing explicit slurs with dogwhistles, automated toxicity detection scores decreased, indicating that coded language can effectively evade detection. The project highlights the importance of understanding dogwhistles for NLP, linguistics, political influence, and combating online hate speech.</sample>
    <sample id="298">Retraining or continuing to pre-train some models with more recent data showed that performance degrades with a larger temporal gap.</sample>
    <sample id="299">This presentation introduces "Improving the robustness of NLI models with minimax training," a novel approach to address the issue of NLI models relying on spurious correlations (shortcuts) within datasets. These shortcuts lead to strong in-distribution performance but brittle behavior when faced with out-of-distribution adversarial examples.

Existing shortcut mitigation methods often require auxiliary models with specific designs or pre-trained language models, demanding domain knowledge and incurring computational costs. This new method bypasses these limitations by employing a minimax training objective between a learner and an auxiliary model.

The core idea is that NLI models struggle with "hard" examples – under-represented instances that contradict common shortcuts. The learner aims to minimize the NLI task loss, while the auxiliary attempts to maximize the learner's loss by generating example weights, forcing the learner to focus on these challenging, hard examples. This alternating optimization process encourages the learner to prioritize learning from data that counters shortcut exploitation.

Crucially, the method doesn't assume knowledge of specific shortcut types and doesn't require the auxiliary to mirror the learner's behavior. A feed-forward network is used to model the auxiliary, keeping computational overhead manageable.

Evaluations on MNLI, FEVER, and QQP datasets, using adversarial test sets like HANS Symmetric and PAWS, demonstrate consistent improvements in out-of-distribution performance compared to standard training and existing shortcut mitigation techniques, all while maintaining high in-distribution accuracy. The paper further explores the impact of pre-training, auxiliary size, and provides a qualitative analysis of the learned example weight distribution.</sample>
    <sample id="300">This presentation introduces "interactive dictation," a novel task aiming to create a more natural and intuitive voice-based document editing experience. Unlike existing speech-to-text systems that primarily focus on dictation, interactive dictation allows users to seamlessly interleave dictation and editing commands using natural language, without needing to memorize specific commands.

The work, conducted at Semantic Machines, formalizes interactive dictation as a four-step process: 1) ASR recognition, 2) segmentation of utterances into dictation and commands, 3) command extraction and normalization, and 4) sequential execution of dictations and commands to achieve the final document state.

To address the lack of existing data, the researchers designed a specialized annotation interface allowing users to dictate and issue commands, which were then recorded to create a dataset. A baseline system was developed, employing separate models for each step, including segmentation, ASR repair, and interpretation. The interpretation model was tested with T5 and GPT-3 architectures, exploring both program prediction and direct state prediction.

Results showed a trade-off between runtime and accuracy, with GPT-3 generally being more accurate but slower. For T5, program prediction proved more efficient with minimal accuracy loss. The researchers acknowledge significant room for improvement and have released the code to encourage further research in this promising area. The ultimate goal is to move towards a more human-like interaction where the system can understand and respond to voice commands naturally during the dictation process.</sample>
    <sample id="302">因为在第一步中，每个输入词元都被标记为一个无序的多重集，所以需要排列这些词元才能得到正确的输出顺序。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为他们不确定这些“积极”刻板印象是由于过度价值对齐还是其他反刻板印象方法造成的，而没有透明度，无法进一步研究这些问题。</sample>
    <sample id="304">Minimal pair paradigms (MPP) evaluate language models on acceptability judgments, typically by comparing the probabilities assigned to an acceptable and an unacceptable sentence.</sample>
    <sample id="305">This presentation critiques recent advances in Weakly Supervised Learning (WSL), highlighting a significant, often overlooked, dependency on clean validation data. Dawei and his team investigated whether clean validation data is truly necessary for WSL to be effective, how much is needed, and how best to utilize it.

Their research revealed that many current WSL methods critically rely on clean validation samples to generalize beyond the noisy weak labels. Without them, performance drops considerably, rendering the training process ineffective. They found that as few as 20 clean samples per class can improve performance, but surprisingly, directly fine-tuning a model on these clean samples consistently outperforms WSL approaches, especially with just 10 samples per class.

The study demonstrates that the performance gains often attributed to complex WSL methods can be readily achieved through simple fine-tuning on clean validation data. This suggests that the complexity and computational cost of advanced WSL techniques are often unnecessary.

The authors recommend future WSL research should transparently report model selection criteria (specifically, the use of clean validation data), compare WSL methods against few-shot learning baselines, and consider continuous fine-tuning as a strong and straightforward alternative. They emphasize that the need for clean annotations in WSL is frequently underestimated, and the practical benefits of current methods are often overstated. The code for their experiments is publicly available.</sample>
    <sample id="306">This paper investigates the ability of large language models (LLMs) to track entities and their states across a discourse, a crucial skill for understanding longer texts. The researchers found that while LLMs often appear to perform well, they frequently rely on shortcuts like memorizing common patterns or simple word associations rather than genuine entity tracking.

To address this, they designed a novel evaluation task using boxes and objects, incorporating state-changing operations. This task was specifically crafted to minimize the use of heuristics and pre-training biases. Experiments with Flan-T5, GPT-3, and GPT-3.5 using 2-shot in-context learning revealed a significant difference: only text-davinci-003 showed non-trivial entity tracking. Other models largely repeated initial states.

Interestingly, the study found a correlation between code pre-training and entity tracking ability. GPT-3.5 models, trained on substantial code data, demonstrated tracking capabilities, while those without code-heavy pre-training did not. Fine-tuning smaller models like T5-base enabled entity tracking, but randomly initialized models failed even with direct supervision, highlighting the importance of pre-training.

The researchers acknowledge that the generalizability of these findings remains unclear and encourage readers to consult their paper for more detailed results, including experiments with GPT-4. The work underscores the challenges in evaluating LLMs' true understanding and suggests that pre-training on code may unlock latent entity tracking abilities.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">Jenny presented "NLPositionality," a framework investigating biases in NLP datasets and models. The core idea is that these datasets and models reflect the positionality—the perspectives shaped by demographics and experiences—of the people who create them, leading to skewed performance across different populations.

The research highlights that existing NLP tools, like Perspective API, perform well for some groups (e.g., Carl Jones) but poorly for others (e.g., Aditya Sharma), demonstrating this bias. To address this, NLPositionality re-annotates datasets with diverse annotators from 87 countries using the "Lab in the Wild" platform, collecting demographic data alongside annotations. This allows comparison of model and dataset predictions with real user judgments.

The study, involving over 16,000 annotations, reveals that datasets and models generally align with English-speaking countries and individuals with higher education. However, this alignment leaves certain groups behind, such as non-binary individuals.

The presentation concludes with recommendations for mitigating these biases: meticulously documenting design choices, adopting a perspectivist approach to NLP research, and developing specialized datasets and models tailored to specific communities, citing the Masakhani initiative as an example. The goal isn't universal applicability but rather acknowledging and addressing the inherent positionality within NLP technologies. A dashboard and paper are available for further exploration.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The text does not mention the authors' affiliated institutions.</sample>
    <sample id="312">MultiInstruct 是第一个大型多模态指令调整数据集，包含 62 个多模态任务，涵盖 10 个类别，每个任务配备 5 个专家撰写的指令。</sample>
    <sample id="313">James Finch and Sarah Finch</sample>
    <sample id="314">Coordination structures headed by the conjunction.</sample>
    <sample id="315">The prompt used to generate personas was inspired by a study that gave prompts to human subjects. The text does not specify the length of the prompts.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="317">Peng Li from Fudan University presented "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing the challenges of information extraction (IE) using traditional language models like T5 and GPT-3. These models struggle because the structured output of IE doesn't align with the plain text format used during pre-training, requiring extensive training data and complex decoding.

CodeIE tackles this by reframing IE as a structure-to-structure code generation task, leveraging code-specialized large language models like Codex. The approach involves designing prompts that define functions for IE, incorporating comments to guide the model in extracting entities and relationships and outputting them in a structured code format.

Experiments across named entity recognition and relation extraction datasets demonstrated that CodeIE significantly outperformed traditional models (UIE, GPT-3) in few-shot settings. Analysis revealed that code language models exhibit lower perplexity on code-formatted inputs and produce fewer structural errors. Furthermore, Codex consistently outperformed GPT-3, and code-style prompts generally yielded better recall.

The research suggests that aligning pre-training and output formats, along with utilizing code-optimized models, improves IE performance. The paper and code are publicly available, aiming to inspire further exploration in this area.</sample>
    <sample id="318">大家好，我是Yanis Labrak，我将向大家介绍我们的工作“DrBERT：一个在生物医学和临床领域中强大的法语预训练模型”。在本次演讲中，我们将首先讨论医疗保健领域的语言建模。然后我们将介绍我们文章的主要贡献。我们推出了第一个法语生物医学模型DrBERT，它基于RoBERTa，并在NACHOS数据集上进行训练，该数据集是从网络抓取的医学数据。我们还介绍了对多种预训练设置和数据源的模型的比较。然后，我们将在法语的11个生物医学和临床下游任务上展示我们的结果。最后，我们将总结实验，并提供有关如何访问这些模型的更多详细信息。

自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，与历史静态和上下文方法（如Word2vec、fastText）相比，它提供了巨大的性能提升。此后，该模型已被适配到许多其他语言，例如法语的CamemBERT，以及生物医学领域的PubMedBERT和BioBERT，以及临床领域的ClinicalBERT，但主要是在英语领域。针对其他语言的专业模型很少，并且通常基于持续预训练，因为缺乏领域内数据。然而，在法语中，直到现在还没有任何开源的生物医学模型。

因此，我们自问：最合适的的数据来源是什么，以支持广泛的使用？抓取的数据是否是临床数据的良好替代品？为了回答这个问题，我们将DrBERT与我们的ChuBERT模型进行比较，后者基于从南特大学医院数据仓库获得的数据进行匿名化处理。

此后，我们自问：训练法语专业模型需要多少数据？是4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型：DrBERT的第一个版本，使用7GB的NACHOS；NACHOS的4GB数据集的第二个版本；ChuBERT的第一个版本，这是一个临床模型，使用4GB从临床记录中提取的句子；以及ChuBERT的最终版本，使用4GB的NACHOS数据集和4GB的临床记录的混合。

除了这种比较之外，我们还引入了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于CamemBERT的权重并在4GB的NACHOS数据集上进行训练；另一个也基于CamemBERT，但这次在4GB的临床记录上进行训练；最后，一个基于英语生物医学模型PubMedBERT，并在4GB的NACHOS数据集上进行训练。

总共，我们有七个模型。为了评估我们这七个模型，我们收集了公共和私有下游任务的数据，例如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行比较，包括CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT和ClinicalBERT。

评估结果表明，模型在与模型训练数据性质相同的数据任务上表现最佳。然而，我们可以观察到来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总的来说，从头开始的预训练似乎在大多数任务上获得了更高的性能。然而，我们对使用CamemBERT权重和分词器在NACHOS的4GB子集中进行控制预训练的实验表明，结果与DrBERT 4GB从头开始的预训练结果相当。这与基于CamemBERT权重和分词器的模型不同，后者存在稳定性问题。

最后，作为结论，我们的专用系统在11个下游任务中的9个任务上提供了更好的性能，并且总体上超过了通用模型（即CamemBERT）的结果。我们还观察到，更专业的数据更好，但可扩展性较差。所有从NACHOS获得并公开的预训练模型都可以在Hugging Face上免费获取，并采用MIT许可证，所有训练脚本都在我们的GitHub存储库中。

感谢本次演讲，我们期待在多伦多的海报环节与大家交流。</sample>
    <sample id="319">From-scratch pre-training, continual pre-training, and control pre-training.</sample>
    <sample id="320">Adaptive overfitting is not observed.</sample>
    <sample id="321">The paper proposes using manually aligned sentences in DEPLAIN as a gold standard to evaluate automatic alignment methods. They also fine-tuned language models (long-mBART and mBART) and evaluated their performance using scores and evaluation metrics.</sample>
    <sample id="322">Enrico's ACL 23 presentation explores how text classifiers learn about morality, challenging the common NLP approach of treating morality as a single scale between "moral" and "immoral." He argues this simplification is dangerous because morality is subjective and varies significantly between individuals and groups.

Drawing on Moral Foundation Theory, which posits five distinct moral foundations (fairness, authority, care, loyalty, and sanctity), Enrico's research investigates whether language models can grasp these nuanced differences. The presentation highlights the limitations of training models on aggregated moral judgments, as it obscures the pluralistic nature of moral interpretation.

His work utilizes the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven diverse domains (e.g., #AllLivesMatter and #BlackLivesMatter), to examine how morality is expressed differently in various contexts. Through explainable AI techniques, Enrico's team analyzes how language models process morality within these domains.

A key finding demonstrated is the model's ability to recognize differing rhetorical approaches to "subversion" (rebellion to authority) between the ALM and BLM domains. ALM associates subversion with negative terms like "overthrow" and "mayhem," while BLM subtly encourages it.

Ultimately, Enrico's research cautions against using a single model across diverse domains, as it can lead to misinterpretations of morality. He emphasizes the need for models to understand the context-dependent expression of moral values, a crucial step towards responsible and accurate language understanding. He invites attendees to learn more at his ACL presentation in Toronto.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach to tackle Commonsense Question Answering (QA) by dynamically reasoning over heterogeneous knowledge graphs (HKGs) and integrating language models. Existing methods often suffer from noisy entities in subgraphs and limited interaction between text and knowledge.

DHLK addresses these issues by first constructing an HKG from multiple knowledge bases (ConceptNet, WordNet, Wiktionary) using a two-stage pruning strategy and Knowledge Representation Learning (KRL) to optimize its structure. The HKG is enriched by adding paraphrased entities from WordNet and Wiktionary.

The core of DHLK lies in its dynamic entity pruning and multimodal fusion. It utilizes RoBERTa and a novel Relation Mask Self-Attention (RMSA) mechanism, inspired by RGAT, to encode and fuse the question-answer context with the HKG entities. RMSA incorporates relationship information into the attention mechanism, allowing for more nuanced understanding of the graph structure.  Entities with low relevance, determined by RoBERTa's attention weights, are dynamically removed.

Furthermore, DHLK enhances the QA context with HKG path information before final answer prediction.  The method employs a multi-layer perceptron (MLP) to combine the HKG embedding, path-enhanced context embedding, and original context embedding to generate answer probabilities. Experimental results on CommonsenseQA and OpenBookQA demonstrate DHLK's effectiveness compared to existing language model and HKG-based approaches. Key entities are extracted using KeyBERT, and knowledge paths are retrieved within ConceptNet to facilitate the reasoning process.</sample>
    <sample id="324">Yes, language models do have varying political leanings. They occupy all four quadrants on the political compass, with GPT-4 being the most liberal.</sample>
    <sample id="325">大家好！我叫马蒂亚斯·林德曼，今天我将向大家简要介绍我们的论文“使用多集标记和潜在排列，无需树木的组合泛化”。这篇论文是与我的导师亚历山大·科勒和伊万·蒂托共同完成的。组合泛化可以理解为学习器处理更深层递归和训练期间单独看到的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。通常，我们都有一个训练集，其中包含例句，例如“那个女孩睡了”和“玛丽知道那个女孩睡了”。这些例句与表示其核心含义的逻辑形式配对。与标准的机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练期间看到了浅层递归，但测试的是一个具有更深层递归的例子。朴素的 seq2seq 模型难以处理这种分布外泛化，并且通常会产生与输入分离的输出。特别是，它们通常无法重现输入和输出之间的系统对应关系，例如在示例中用颜色编码的对应关系。一种流行的解决此问题的方法是在模型中集成树。这些树旨在捕获与逻辑形式相关的例句的组合过程。这效果很好，但通常没有提供树，需要以某种方式获得它们。这可能很复杂且计算成本高昂。通常，这涉及对逻辑形式进行大量的特定形式的预处理，例如处理变量符号。获得树可能还涉及专门的语法归纳程序。在本文中，我们不使用树，而引入了一个直接对输入和输出片段之间的对应关系进行建模的神经 seq2seq 模型。我们首次展示了在不依赖树的情况下对更深层递归的强大泛化能力。我们的方法分两个步骤从输入预测输出。首先，我们使用一个无序的多集标记标记每个输入标记，其中包含将在输出中出现的标记。在第一步之后，我们拥有了所有正确的标记，但它们的顺序不正确。这就是为什么在第二步中，我们使用另一个模型来预测排列，以便将它们排列成正确的顺序。我们引入了一种新方法来预测排列，该方法对可能的排列没有任何硬性约束。这使得我们的方法非常灵活和富有表现力。从概念上讲，我们的排列模型的工作方式大致如下。我们从左到右遍历输出，确定将哪个多集标记放入每个位置。对于第一个输出位置，我们只需选择一个，如图中红色突出显示的那样。然后我们跳转到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续此过程，直到第一阶段中的每个标记都被访问一次。为了让大家先睹为快，以下是在 COGS 基准测试中，我们将我们的方法与其他无树模型进行比较的结果。我们的模型在泛化到更深层递归方面明显优于其他模型。尽管如此，其他类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的实际挑战。首先，训练数据中没有给出输入和输出之间的对齐方式。因此，对于给定的标记，我们不知道它来自哪个多集标记，这给训练带来了挑战。此外，有时存在多个与数据一致的排列，但语言学上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。我们的排列方法非常灵活，但它带来了寻找最高分排列的挑战，因为这与“旅行商问题”相关，是 NP 难问题。我们使用 GPU 友好的连续松弛来近似此问题，这还允许我们反向传播解决方案并学习更符合语言学规律的排列。如果您想了解更多关于我们实验以及我们如何应对这些挑战的信息，请阅读我们的论文或访问我们的海报。</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent, such as stating "I know cigarettes could kill me" and then saying "I grabbed a couple of smokes after the meeting."</sample>
    <sample id="327">This presentation introduces ManagerTower, a novel vision-language (VL) architecture designed to improve representation learning. Building upon previous models like METER and BridgeTower, ManagerTower addresses their limitations by enabling more effective utilization of semantic knowledge from different layers within unimodal encoders.

Traditional two-tower architectures often overlook valuable semantic information at various depths within visual and textual encoders. BridgeTower attempts to address this by connecting unimodal layers to cross-modal layers, but it suffers from inefficient layer utilization and limited scalability.

ManagerTower overcomes these issues by introducing "managers" within each cross-modal layer. These managers adaptively aggregate insights from multiple unimodal representations, essentially acting as experts at different levels. This allows for a more comprehensive cross-modal alignment and fusion process. The architecture utilizes RoBERTa and CLIP-ViT base as unimodal encoders and is flexible, allowing for different encoder choices.

Experiments demonstrate that ManagerTower achieves superior performance on various downstream tasks, including Visual Question Answering (VQA), even with a relatively small pre-training dataset (4 million images). Notably, it outperforms models with larger datasets or more parameters. Visualization of manager weights reveals that adaptive managers dynamically adjust their aggregation strategies based on the cross-modal layer and the type of unimodal information (textual or visual), showcasing their ability to effectively leverage diverse semantic knowledge. The paper, code, and models are publicly available.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">This paper introduces a novel approach to zero-shot video sentence localization, aiming to overcome the limitations of existing methods that rely on manual annotations. The core idea is to generate structured pseudo-labels to train a video sentence localization model without any human intervention.

Existing zero-shot methods often suffer from simplistic pseudo-queries, misalignment between pseudo-queries and video segments, and the risk of label noise. To address these issues, the proposed method generates more complex, free-form pseudo-queries using a pre-trained image captioning model (BLIP). It then constructs pseudo-events by analyzing frame relevance to the queries, ensuring high relevance within the event and low relevance outside. A sliding window approach is used to identify the optimal event boundaries based on similarity scores.

To mitigate label noise, the method employs a two-pronged strategy: sample re-weighting based on prediction confidence and IoU, and label refinement by incorporating high-confidence predictions as new pseudo-labels. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate that the proposed method, termed SPL, significantly outperforms existing zero-shot approaches across various evaluation metrics (R@M and mIoU). The code is publicly available.</sample>
    <sample id="330">Cumulative performed equal or better than Iterative across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">TED talks transcripts.</sample>
    <sample id="333">This paper introduces INK, a novel framework for enhancing Neural Machine Translation (NMT) models by injecting kNN (k-Nearest Neighbors) knowledge. Traditional NMT models often suffer from a non-smooth representation space, leading to poor generalization and performance, particularly with low-frequency tokens. kNN-MT addresses this by smoothing predictions using a large datastore of representations and target tokens, but it's computationally expensive and struggles with updating representations.

INK overcomes these limitations by iteratively refining the NMT model's representation space. The training loop involves two steps: first, extracting kNN knowledge to guide an adapter in adjusting representations, and second, updating the datastore asynchronously. This process aligns contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token, addressing sparsity and enriching semantic meaning. Crucially, INK allows for the datastore to be discarded after training, reducing memory footprint and accelerating inference.

Experiments using the WMT’19 German-English translation task demonstrate that INK significantly outperforms state-of-the-art kNN-MT systems, achieving substantial gains in BLEU and COMET scores. The framework’s efficiency is further highlighted by its ability to achieve high performance with smaller adapters, requiring less memory space. The research also suggests that combining adapters and datastores can further refine the representation space, indicating potential for even greater improvements with future framework designs. Overall, INK offers a practical and effective solution for improving NMT performance by leveraging kNN knowledge without the drawbacks of traditional kNN-MT approaches.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This research introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to effectively represent out-of-vocabulary (OOV) words, a significant challenge in embedding-based models. Inspired by human learning, the method leverages word formation and association to infer OOV word meanings.

The core of the approach is a Word Relationship Graph, constructed by tokenizing OOV words into wordpieces and connecting them to relevant words, creating a two-level graph. A self-attention network assigns attributes to OOV nodes based on their characters. Two concatenated Graph Attention Networks (GATs) then process the graph, focusing on important information and mitigating noise. A readout block layer captures overall graph information, summarizing word formation.

To align with existing embedding models, contrastive learning is employed, using NT-XENT loss with positive samples like two-hop neighbors and synonyms. This encourages proximity between the OOV word's graph-level embedding and its background embedding while distancing it from other samples.

Extensive experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, proving the effectiveness of the word formation-based learning. The model benefits both static and contextual models in downstream applications. Future work explores expanding the model to other languages, noting that agglutinative languages are particularly well-suited, while fusional languages present greater challenges, with success dependent on reasonable word decomposition. The model's adaptability to complex word formations is highlighted as a key strength.</sample>
    <sample id="338">This research paper, "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," investigates the quality of human-annotated explanations used to train AI models. The core problem addressed is the lack of a reliable method to evaluate these explanations, which are often subjective and task-dependent, unlike labels. Traditional metrics like BLEU and ROUGE are inadequate, and existing "simulatability" scores fail to account for task differences and varying utility during fine-tuning and inference.

The researchers propose a novel evaluation metric, TREU, which builds upon the simulatability score by specifically assessing the helpfulness of explanations during the fine-tuning process. They introduce a unified data structure, converting diverse tasks into a multiple-choice format with and without explanations. Through experiments on five datasets (CoS-E, ECQA, e-SNLI, ComVE), they found that fine-tuning with explanations can improve model performance, even when explanations are considered "low quality."

Key findings include that fine-tuning doesn't necessarily impart new knowledge but can lead models to rely on explanations for prediction. The study demonstrates that TREU consistently ranks dataset qualities better than the simulatability score, and that explanation helpfulness is highly task-dependent, influenced by factors like negation and writing style. Ultimately, the research advocates for rigorous quality checks of human annotations and lays the groundwork for more effective human-AI collaboration in explanation generation.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a joint work aiming to address the limitations of existing paraphrase datasets. While human-annotated datasets are high-quality, they are small, and automatically generated datasets (like back-translation) often lack syntactic diversity.

ParaAMR tackles this by leveraging Abstract Meaning Representations (AMR) – directed graphs representing sentence meaning. The core idea is AMR back-translation: parsing a sentence into an AMR graph, randomly selecting a new "focus" node to become the root, modifying edges accordingly, and then generating text from the altered graph. This process ensures semantic similarity while introducing syntactic variation because the text generator emphasizes the new focus.

The resulting ParaAMR dataset contains 15 million source sentences with approximately 6.9 paraphrases each. It demonstrates significantly higher syntactic diversity compared to other back-translation datasets, while maintaining comparable semantic similarity.

The research team evaluated ParaAMR's effectiveness across several NLP applications. Training sentence embeddings with ParaAMR yielded better performance on the STS benchmark. Furthermore, it improved syntactic control in paraphrase generation and enhanced few-shot learning through data augmentation due to its syntactic diversity.

In conclusion, ParaAMR offers a valuable resource for NLP research, providing a large-scale, syntactically diverse paraphrase dataset that benefits various downstream tasks. The dataset is publicly available.</sample>
    <sample id="341">BLEU, average lagging, and computational-aware average lagging.</sample>
    <sample id="342">This presentation introduces LiveChat, a novel large-scale personalized dialogue dataset automatically constructed from live streaming videos on Chinese TikTok and Douyin. The research team from Shanghai Jiao Tong University and Xiaobing.AI aimed to address limitations in existing open-domain dialogue datasets, particularly the lack of video-sourced data, personalized dialogue, and large-scale multi-party conversations in Chinese.

LiveChat's construction involves three key steps: extracting audio from videos and transcribing it, matching audience comments to form dialogues using a reply-to-whom method, and collecting persona information. Persona extraction is achieved through both manual labeling and automated methods using rules and classifiers.

Experiments on response modeling and addressee recognition demonstrate the benefits of LiveChat's persona profiles and longer average sessions. Notably, the dataset's distinct domain leads to superior performance from BART compared to other pre-trained dialogue models. Human evaluations further highlight LiveChat's richness in informativeness. In-context learning experiments reveal that increasing the number of demonstrations improves performance up to 8 shots, after which noise from random selection leads to a slight decrease.

The research concludes that LiveChat offers a valuable resource for advancing dialogue research, particularly in personalized and multi-party conversation scenarios. Future work will focus on efficient transfer learning of large language models (LLMs) to better leverage the dataset's unique characteristics.</sample>
    <sample id="343">大家好，我是Akshatha，今天我和我的合著者Martin将为大家介绍我们的工作“KITMUS测试：评估来自多个来源的知识整合”。这项工作是McGill大学、Mila和微软研究院之间的合作。自然语言理解模型依赖于各种知识来源，例如包含在其参数中的知识，通常通过预训练获得，以及在推理时提供的输入中的知识。最近在问答等任务中的研究表明，模型可以使用预训练时期的知识来解决任务。但自然语言理解通常需要也提供在推理时期的知识。例如，在句子“John在电视上看到了新当选的总统”中，预训练参数可以包含关于总统做什么以及电视是什么的信息，但它无法可靠地知道这个特定实体“John”是谁，或者新总统是谁，因为总统可能在预训练之后发生了变化。因此，对于知识密集型NLU任务，成功的模型需要能够整合和使用预训练时期的和推理时期的知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一个指代消解任务，旨在探测从不同来源获取知识的能力。我们使用人类研究参与者和已建立的指代消解模型评估数据集。这里有一个我们数据集中的例子。Servin是一名法官。Kea是一名面包师。Servin和Kea在公园里见面。经过在法庭上审理案件的漫长一天后，他很高兴放松一下。这里的任务是识别代词“他”所指代的正确实体，在本例中是Servin。解决给定的代词需要两种类型的信息。首先，特定于实体的知识，例如“Servin是一名法官”。其次，背景知识，例如“法官在法庭上审理案件”。通常，背景知识是在大型语言模型的预训练期间学习的，而特定于实体的知识通常在推理时观察到。我们改变了这两种信息可用性的方式，使得它可能存在于单个来源或多个来源中。我们定义了KITMUS的三种设置。首先，我们有典型的设置：“背景-预训练”，其中假设背景知识在预训练时可用。其次，有“背景-两者”设置，其中背景知识在预训练时和推理时都可用。最后，是“背景-推理”设置，其中只有在推理时才提供两种知识。这种设置尤其有趣，因为它模拟了解决任务所需的背景知识不是模型预训练数据的一部分的情况。例如，因为自预训练以来出现了新的职业。以下是我们在真实来源中控制事实可用性的一个例子。在“背景-预训练”设置中，我们假设背景知识“政客竞选政府中的选举席位”包含在预训练参数和推理时期的上下文中，我们提供特定于实体的知识“Chichester是一名政客”。在“背景-两者”设置中，我们不仅提供特定于实体的知识，还提供政客在推理时期的背景知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”，因为“mirituer”不太可能包含在预训练参数中。我们使用人类研究参与者和已建立的指代消解模型评估数据集。在这个图中，我们展示了在“背景-预训练”设置中最困难的变体上表现最佳的模型的结果。在没有在KITMUS上进行特定任务训练的情况下，这两个模型表现不佳。然而，当在KITMUS上进行训练时，C2F和BERT4Coref的表现明显优于随机选择。这表明，当在通用的指代消解数据集上进行训练时，大多数模型学会利用表面线索，而这些线索在测试KITMUS时没有用，因为这些线索已被删除。与虚构知识的额外实验表明，即使是表现最佳的模型，也无法可靠地整合仅在推理时提供的反向知识。总之，我们论文的主要结论是，许多指代消解模型在没有特定任务训练的情况下似乎无法推理来自不同来源的知识。然而，通过特定任务训练，一些模型可以成功地整合来自多个来源的知识。尽管如此，即使是表现最佳的模型似乎也难以可靠地整合仅在推理时呈现的反向知识。如果您有兴趣了解更多细节，请参阅我们的论文，并在GitHub上查看数据集和代码。感谢大家的聆听。</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow, which can be complicated and computationally expensive. This may involve formalism-specific pre-processing of the logical forms or specialized grammar-induction procedures.</sample>
    <sample id="345">Matthias Lindemann introduces a paper on "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations," a joint work with Alexander Koller and Ivan Titov. The paper addresses the challenge of compositional generalization in semantic parsing, where models struggle to handle unseen combinations of phrases despite having seen individual phrases during training.

Traditional approaches often rely on tree structures to capture compositional relationships, but these trees require complex pre-processing or grammar induction. This research presents a novel neural seq2seq model that achieves strong generalization to deeper recursion *without* using trees.

The model operates in two stages. First, it tags each input token with a multiset of output tokens. Second, it predicts a permutation to order these tokens correctly. A key innovation is a flexible permutation model that doesn't impose strict constraints, allowing for expressive representation. The permutation process is visualized as a sequential selection of tokens from the multiset.

Experiments on the COGS benchmark demonstrate significant outperformance compared to other treeless models, particularly in generalizing to deeper recursion. The paper also tackles technical challenges: inferring input-output alignment during training and addressing the NP-hard problem of finding optimal permutations. A GPU-friendly continuous relaxation is used to approximate the permutation search and enable backpropagation for learning. The authors encourage readers to consult the paper or their poster for more details on the experiments and solutions to these challenges.</sample>
    <sample id="346">The provided text does not mention the authors' affiliated institution.</sample>
    <sample id="347">大家好，我是Myra，今天我将介绍我们的论文“标记人格：使用自然语言提示来衡量语言模型中的刻板印象”。这篇工作是与Esin Durmus和Dan Jurafsky合作完成的。近年来，许多人记录了大型语言模型（LLM）中社会偏见和刻板印象的普遍存在。然而，这些衡量方法存在各种局限性。它们通常依赖于耗时且难以构建的手工数据集，或者仅仅捕捉非常笼统的关联，例如对特定群体的负面联想。此外，大多数研究都没有考虑到交叉性，即多重社会身份可以加剧偏见并成为伤害的独特根源。

为了克服这些局限性，我们利用了这些新型指令调优LLM的一个特性，即它们非常擅长响应指令和提示。因此，我们可以要求模型生成一个“人格”，即使用提示“想象你是一名亚裔女性。描述一下你自己”来描绘一个虚构的个体。我们可以立即看到，这非常容易推广到任何人口统计群体，因为我们只需将所需的身份标记指定到提示中即可。

以下是GPT-4的一些示例生成结果。我们可以立即看到，虽然这些输出在传统意义上不是公开负面或有毒的，但存在一些有趣的模式。亚裔女性被描绘成不起眼；中东女性被用诸如“异国情调”之类的词语来描述，并提及“迷人的地区”。而且，有色人种女性的人格都提到了祖先，而白人男性人格则没有。

为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人格。我们生成人格的提示灵感来自于一项研究，该研究向人类受试者提供这些提示，发现通过向人类受试者提供提示，他们也能浮现出种族刻板印象。这使得我们能够直接比较生成的个性与人类书写的回复。第二部分是“标记词语”，这是一种识别区分标记群体和未标记群体的词语的方法，我稍后会详细说明。

这种方法的优点是，我们可以在不依赖任何特定词典的情况下，获得非常具体的刻板印象和模式。

“标记词语”方法借鉴了社会语言学中的“标记性”概念，该概念指出存在一个未标记的默认值，而任何与该默认值不同的群体在语言上都会被标记。例如，“战士”这个词通常与男性相关。因此，当人们描述一位女性战士时，他们通常会明确指出“女性战士”，并在术语中标记“女性”。更广泛地说，社会中的优势群体在语言上和社会上都是未标记的，而边缘群体通常会被标记。

在我们的方法中，我们首先指定未标记和标记群体是什么，然后使用“Fightin’ Words”方法（基本上是使用加权对数比率来区分每个标记群体的最常用词语）来比较人格。

现在，我们来看一些结果。首先，我们使用一个刻板印象词典，发现生成的个性比人类书写的个性包含更多的刻板印象。然而，当我们实际查看单词的分布和词典时，发现情况非常不同。虽然生成的个性具有更高的词典词汇使用率，但人类书写的个性具有更广泛的词汇分布。而生成的个性中的刻板印象词语，实际上只是“高”和“运动型”这两个词。

因此，这个词典并不能很好地捕捉到我们在前面幻灯片中看到的许多有害模式。为了做到这一点，我们将转向“标记词语”方法的结果，以展示这些看似积极的词语如何促进刻板印象和本质化叙事。

在我们的分析中，我们揭示了这些看似积极的描述如何反映有害模式。首先，从我们的群体中，最常用的词语包括“文化”、“传统”、“自豪”和“异国情调”。这些词语仅通过它们与身份的关系来定义这些群体，并将它们与白人规范区分开来。这为这些群体带来了长期的歧视和其他化历史。

此外，还有许多常见的刻板印象反映在这些词语中，尤其是在有色人种女性中。例如，描述拉丁裔女性的词语包括“充满活力”和“曲线优美”，这与热带主义的刻板印象有关。对于亚裔女性，词语包括“娇小”、“精致”和“丝滑”，这与亚裔女性被过度性化、被视为非常顺从和温顺的历史有关。最后，对于黑人女性，我们看到一些最常用的词语是“坚强”和“有韧性”。这与人们称为“坚强的黑人女性”的典型形象有关。虽然乍一看听起来很积极，但研究表明，这种类型的典型形象实际上非常有害，因为它给这些人群带来了巨大的压力，要求他们克服社会障碍，而不是真正努力改变这些障碍，从而导致这些人群出现负面的健康状况，以及其他危害。

更广泛地说，我们发现每个标记群体的词语几乎只是反映了非常本质化的叙事。

基于这些模式，我们得出三个建议，供模型所有者参考。首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们还应该使用交叉视角来研究偏见和危害，因为如果不这样做，可能会忽略很多问题。最后，应该增加关于偏见缓解方法的透明度，因为例如，这些积极的刻板印象，我们不知道这是因为存在某种奇怪的过度价值对齐，还是因为一些其他的反刻板印象方法导致了这些有害的模式。除非我们有更多的透明度，否则我们无法做出任何假设或进一步研究。

谢谢大家聆听。ACL见！</sample>
    <sample id="348">Myra's paper, "Marked Personas," investigates stereotypes in large language models (LLMs) using a novel approach. Traditional methods for detecting bias rely on curated datasets or broad association measures, often failing to capture nuanced stereotypes or intersectional harms. This work addresses these limitations by leveraging the instruction-following capabilities of LLMs.

The core idea is to prompt the model to generate personas ("Imagine you are an Asian woman. Describe yourself.") for various demographic groups. These generated personas are then analyzed using a "Marked Words" method, drawing on sociolinguistic principles. Markedness theory posits that dominant groups are unmarked, while marginalized groups are linguistically marked. The method identifies words that distinguish marked groups from unmarked ones, revealing specific stereotypes without relying on pre-defined lexicons.

The study found that LLMs generate personas containing more stereotypes than human-written ones, but the stereotypes are often subtle and positive (e.g., "tall," "athletic"). However, the Marked Words analysis reveals that even these seemingly positive portrayals perpetuate harmful essentializing narratives. For example, Latina women are described with "vibrant" and "curvaceous" (tropicalism), Asian women with "petite" and "delicate" (hyper-sexualization), and Black women with "strong" and "resilient" (the "Strong Black Women" archetype, which can be detrimental).

The paper concludes with recommendations: researchers should address positive stereotypes and intersectional biases, and model owners should increase transparency regarding bias mitigation techniques to better understand the origins of these patterns.</sample>
    <sample id="349">大家好，我叫易景伟，来自中国科学技术大学。很高兴能为大家带来我们论文的短视频广告。我们的论文题目是：通过后门水印保护嵌入式服务的大型语言模型版权。让我们首先介绍一下嵌入式服务的背景。目前，像GPT、LLAMA、PALM这样的LLM在自然语言理解和生成方面表现出色。嵌入式服务是建立在LLM之上的服务之一，可以辅助各种NLP任务。例如，OpenAI提供基于GPT的嵌入式API。然而，最近的研究表明，攻击者可以通过学习嵌入式信息来窃取模型，并提供类似的服务。因此，保护嵌入式服务的版权是必要的。

为了保护嵌入式服务的版权，一种解决方案是在服务提供商的服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下几个特性。首先，该方法应该适用于嵌入式服务。其次，水印不应降低提供的嵌入式的效用。第三，水印应该足够隐蔽，或者攻击者可以轻松地删除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作可以大致分为四类。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。

因此，在本文中，我们提出了Embedding Marker，这是一种基于后门的水印方法，适用于嵌入式服务。接下来，我将介绍Embedding Marker的细节。Embedding Marker包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在适中的频率区间内的单词。我们假设提供商可以收集一个通用文本语料库并统计单词频率。

在水印注入中，我们首先定义一个目标嵌入。当用户将句子发送到提供商服务时，提供商会统计句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，提供的嵌入将完全等于目标嵌入。

版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门和良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。然后，提供商使用数据集向窃贼的服务请求嵌入。计算请求的嵌入和目标嵌入之间的余弦相似度和L2相似度。我们计算良性和后门数据集之间的相似度差异，定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三指标。

我们在四个数据集（AG News、MIND、SST2和Enron Spam）上进行了实验。我们假设提供商使用Wiki Text数据集来计算单词频率。在四个数据集上的结果表明，我们的Embedding Marker可以在保持下游任务的良好效用时，实现出色的检测性能。我们还通过PCA可视化句子嵌入来验证提供的嵌入的隐蔽性[INAUDIBLE 4:39]。图例中的图代表每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。

以上就是全部内容。谢谢大家。欢迎与我们讨论。</sample>
    <sample id="350">This presentation critiques the prevalent practice of declaring "superhuman" performance in Natural Language Understanding (NLU) based on leaderboard scores in benchmarks like SuperGLUE and SQuAD. While models frequently surpass human scores on these benchmarks, the paper argues that such claims are premature and lack scientific rigor.

The core issue lies in flawed comparisons between systems and humans. Firstly, systems are evaluated on the full test set, while humans are often assessed on significantly smaller subsets. Secondly, the datasets themselves contain errors in ground-truth answers, creating an unfair playing field. Systems can exploit spurious correlations in the data, a tactic unavailable to humans.

Furthermore, the paper highlights problems with how human baselines are established. Simple aggregation methods are used instead of comparing against the best possible human performance. Crucially, annotator compensation varies widely, with some tasks offering extremely low pay, potentially impacting the quality of human annotations. Details about the annotator pool—size, hiring process, cultural background—are frequently omitted, further undermining the validity of comparisons.

The presentation concludes that current claims of superhuman performance in NLU are not scientifically meaningful due to these methodological shortcomings. The paper advocates for more reliable benchmark construction, emphasizing the need for fair comparisons, accurate ground truth, and well-compensated, diverse annotator pools. Ultimately, the paper urges caution in interpreting leaderboard scores and calls for a more nuanced understanding of what it truly means for an AI to outperform humans in tasks involving knowledge, reasoning, and inference.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. The core question is whether models developed for CoNLL-2003 still perform well on modern data and what factors contribute to good generalization.

To address this, the researchers created CoNLL++, a new dataset of Reuters News articles from 2020 annotated using the CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-03 and CoNLL++, measuring performance changes using F1 scores.

The study identified three key ingredients for good generalization: transformer-based model architectures, larger model sizes, and a greater number of fine-tuning examples. They explored two hypotheses for performance drops: adaptive overfitting (due to repeated use of the CoNLL-2003 test set) and temporal drift (caused by the time gap between training and testing data).

Interestingly, adaptive overfitting was not observed, as improvements on CoNLL-2003 consistently translated to greater improvements on CoNLL++. However, temporal drift was confirmed as the primary cause of performance degradation, with performance declining as the temporal gap increased.

The conclusion is that while CoNLL-2003 taggers still work well in 2023, achieving good generalization requires a combination of advanced model architectures, larger models, and ample fine-tuning data. The paper highlights the importance of addressing temporal drift in NER model development and encourages further research into improving generalization.</sample>
    <sample id="352">Annotating Behaviors in Chat or ABC-Eval</sample>
    <sample id="353">This paper introduces a novel approach to Python code generation that addresses the common problem of input underspecification – when natural language descriptions (NLDs) lack crucial details needed to generate accurate code. The core idea is to incorporate interactivity by asking clarification questions (CQAs) during the code generation process.

The authors introduce "CodeClarQA," a synthetic dataset designed to facilitate this interactive approach. It focuses on identifying and clarifying missing operation-level specifications within the code. A key component is a method to determine if an NLD adequately describes a key operation, using schema element similarity scores. If a key operation is deemed "missing," the system generates CQAs (either yes/no or multiple-choice) to elicit the necessary information.

The proposed pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments demonstrate that this interactive task is more challenging than existing clarification question ranking tasks, and that clarifications generally improve code generation. While the pipeline currently underperforms models trained solely on NLDs and code (due to the difficulty of the CQ ranking task), analysis suggests that clarified key operations are indeed a significant factor in generating better code.

The paper acknowledges limitations, such as the need to distinguish operations with similar names and the reliance on operation documentation rather than argument values. Future work aims to address these challenges and further refine the interactive code generation pipeline. The authors encourage feedback and provide access to their paper and code.</sample>
    <sample id="354">The provided text does not specify a year. It states that the red best fit line has a gradient greater than one, indicating that every unit of improvement on CoNLL-2003 translates to more than one unit of improvement on CoNLL++.</sample>
    <sample id="355">大家好，我叫瓦苏达，是斯托尼布鲁克大学计算机科学专业的博士候选人。我想介绍一下我们被ACL 2023接受的论文，题为“认知失调检测的迁移学习：解决罕见类别挑战”。

我们首先定义了认知失调，并解释了为什么这是一个重要的研究课题。简单来说，认知失调是指两个信念或行为不一致，例如，一个人说“我知道香烟可能会杀死我”，然后又说“我在会议结束后抽了几根烟”。这两个信念和行为是不一致的，处于失调状态。进一步解释说“没有它们，我恐怕无法保住我的工作”则为第二次发生提供了理由，从而形成了和谐关系。虽然认知失调是一种在日常决策中经常经历的常见现象，但在其他类型的论证关系中，语言中表达的失调却非常罕见。

那么，这有什么意义呢？研究语言中表达的认知失调有助于我们理解人们之间的分歧影响，跟踪信念和价值观的趋势，以及人口的态度变化。高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。研究语言中表达的认知失调还可以有助于理解极端主义和弱势群体两极分化。最后，认知失调对于理解个人的认知风格和决策过程也至关重要。

为了创建认知失调资源，我们进行了一项大规模的失调关系标注工作。我们采用了“以失调为先”的方法，流程图如下。推文通过PDTB解析器传递，根据我们在论文中描述的指南对论证单元对进行标注。正如我们所看到的，在标注的对中，只有3.5%发现了失调。在收集了大约1000个论证单元对后，我们对一个最初仅使用43个失调示例训练的分类器进行了训练。不出所料，该分类器的表现并没有比随机猜测好多少。

鉴于失调的低发生率和缺乏任何先前的此类数据集，我们面临着绝对稀有问题的挑战。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，以进行标注，从而在较少的标注轮次中收集更多的失调样本，降低整体标注成本，同时提高失调检测的准确性。由于初始模型完全无法捕捉到失调类别，因此我们在主动学习过程开始时，从相关的任务中迁移权重。我们从两个不同的任务中迁移：与主题无关的失调立场分类（确定来自不同人员的辩论陈述是否一致或不一致，无论主题如何，我们称之为“辩论”）以及PDTB的扩展和比较类别的二元分类（这两个类别与和谐和失调的概念密切相关，我们称之为“CE”）。

我们发现，在迁移后的零样本性能在标注的数据集中已经明显优于随机猜测，其中最佳AUC为0.62。此外，通过迭代地在两个任务上进行微调，我们发现先对CE任务进行微调，然后对“辩论”任务进行进一步微调，可以获得更好的零样本性能。因此，这就是我们用来启动主动学习的模型。

接下来，我们确定了在每轮稀有情况下，使用新数据更新模型的最佳方法。“累积”策略会累积主动标注迄今为止收集的所有数据，而“迭代”策略则通过在最新收集的数据集上进行训练来更新模型。在不同的策略中，我们发现“累积”策略在各个方面都优于或等于“迭代”策略。

接下来，为了增加失调示例的数量，我们使用“概率罕见类策略”（PRC）来选择当前模型最有可能为失调的示例。我们将此策略与其他常用的最先进的AL策略进行比较。我们发现，提出的PRC策略优于其他最先进的策略，尽管差异很小。需要注意的是，随机策略的性能明显较低。在对两个最佳策略进行进一步的AL轮次后，我们将失调分类AUC提高到0.75，这是迄今为止在该任务上取得的最佳性能。

我们还检查了每种策略在标注质量和标注员成本方面的可行性。我们发现，PRC策略具有最高的失调百分比，并且最适合罕见类别。然而，标注员也认为这些示例很难。

总而言之，我们发现PRC是一种简单的AL策略，用于罕见类别的获取和启动AL，并且与适当设计的迁移学习任务相结合，可以显著提高性能。我们还发现，迭代更新对于来自不同领域的迁移学习很有用，而领域内的主动标注则受益于累积更新。

以下是我们的核心数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。</sample>
    <sample id="356">The authors are Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">专门为同时预翻译定制的架构。</sample>
    <sample id="361">Armineh Nourbakhsh's research, "CounterComp," addresses the challenge of compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. Current state-of-the-art neural models struggle with these tasks, particularly when requiring more than two arithmetic operations, due to memorizing spurious patterns within the input data.

The core idea of CounterComp is to leverage counterfactual scenarios to mitigate this issue. The approach identifies that certain components of questions are interchangeable, and altering them can lead to changes in the output. The method mines "positive" and "negative" examples from the training data. Positive examples involve question interventions that *don't* change the output, while negative examples involve interventions that *do*.

These examples are then used to create triplets, which are fed into an auxiliary metric learning loss during training. A key innovation is a dynamic margin within this loss, which adapts based on the degree of change introduced by the question intervention.

Experiments demonstrate that incorporating the CounterComp loss consistently improves performance across three state-of-the-art baselines, especially as the number of reasoning steps increases. Crucially, the method enhances performance on both in-distribution and, more significantly, out-of-distribution samples, showcasing improved compositional generalization. Qualitative analysis reveals that CounterComp encourages the model to focus on more relevant tokens within the input, aligning them with the necessary operations in the output.</sample>
  </task>
</testset>