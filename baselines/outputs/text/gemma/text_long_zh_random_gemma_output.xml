<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">Large-scale web crawl data.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="2">This paper by Ant Group introduces LayoutMask, a novel pre-trained model designed to improve Visually-rich Document Understanding (VrDU) by addressing limitations in existing approaches related to reading order. Current models often rely on global 1D positions (ascending numbers) to represent document token order, which proves problematic for complex layouts.

LayoutMask departs from this by utilizing "local 1D position," representing token order within segments. This forces the model to infer the global reading order by integrating 1D position, 2D position (spatial layout), and semantic information, fostering deeper text-layout interactions.

The model incorporates several key innovations:

*   **Position Encoding:** Employs local 1D positions instead of global ones.
*   **Masking Strategies:** Introduces Whole Word Masking (masking at word-level to enhance context understanding) and Layout-Aware Masking (prioritizing masking of segment boundaries to encourage cross-segment analysis).
*   **Masked Position Modeling (MPM):** A new pre-training objective that recovers randomly masked 2D positions, promoting spatial reasoning and text-layout interaction.

Experiments demonstrate that LayoutMask's use of local 1D positions outperforms global 1D positions on datasets like FUNSD and SROIE, particularly in scenarios with complex layouts and ambiguous entities like "Total." The paper concludes that local 1D position offers greater adaptability to these challenging cases, highlighting the model's effectiveness in improving VrDU performance through enhanced text-layout interaction and layout representation learning. Further details are available in the full paper and accompanying posters.</sample>
    <sample id="3">DEPLAIN是一个新的德语文本识别语料库，分为文档级别和句子级别。它用于文本简化，即调整文本以提高特定目标群体的理解度，例如阅读障碍者或非母语人士。

该语料库包含复杂德语句子及其简化版本的平行对，简化技术包括词汇替换、从句删除、重新排序和插入词语。

DEPLAIN分为两个子语料库：DEPLAIN-apa（基于新闻文本，包含483篇手动对齐的文档，约13000个平行句子对）和DEPLAIN-web（包含不同领域的750篇文档，手动和自动对齐，总共30450个平行句子对）。

分析显示，圣经文本比新闻文本或语言学习文本简化程度更高，且DEPLAIN语料库包含多种简化转换类型。

DEPLAIN语料库有两个主要用途：

1.  评估自动对齐方法，研究表明MASSalign方法最适合德语文本简化。
2.  通过微调语言模型实现自动文本简化，使用long-mBART进行文档级别简化，使用base mBART进行句子级别简化，并将其作为未来自动文本简化的基准。</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL model</sample>
    <sample id="6">Jiaan presented "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a joint work introducing many-to-many summarization, a novel approach that combines multilingual and cross-lingual summarization into a single, more general framework. This framework allows a model to summarize documents from any source language into any target language.

Preliminary analyses revealed that many-to-many summarization facilitates better knowledge transfer across languages compared to traditional multilingual and cross-lingual methods. To realize this, they developed PISCES, a pre-trained many-to-many summarization model utilizing a three-stage pre-training process.

The first stage, meta pre-training, focuses on generating original sentences from noisy versions. The second, cross-lingual pre-training, involves generating sentences in a target language from noisy parallel sentences in different source languages. Finally, task-specific pre-training uses pseudo many-to-many summarization samples.

Experiments on the WikiLingua dataset, using models like mBART-50, demonstrated that PISCES outperforms baselines like mBART-50 and mT5. Ablation studies confirmed the effectiveness of each pre-training stage, and human evaluations further validated PISCES's superiority. The research highlights the potential of many-to-many summarization for improved cross-lingual summarization capabilities.</sample>
    <sample id="7">Yes.</sample>
    <sample id="8">ABC-Eval 通过明确标注模型响应是否表达某些行为（例如，提供不相关信息或自相矛盾）来减少人工评估的主观性。</sample>
    <sample id="9">Clean, manually annotated samples.</sample>
    <sample id="10">Provide the language model with access to background knowledge, either the exact same as the annotators or partially overlapping.</sample>
    <sample id="11">Jack Hessel from AI2 presented research exploring whether large language models (LLMs) truly "understand" humor, using data from *The New Yorker* Caption Contest. While LLMs can generate and even attempt to explain jokes, their understanding remains questionable.

The research operationalized the Caption Contest into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why a caption is funny. A new dataset was created, annotating cartoons with details like locations, descriptions, and entity links, alongside a corpus of human-written joke explanations.

Results showed LLMs struggle. A CLIP model fine-tuned on the dataset achieved 62% accuracy in caption matching, significantly lower than the 94% accuracy of humans. Even with human-provided descriptions of the cartoons, models like GPT-4 still lagged behind human performance.

In the explanation generation task, GPT-4 produced explanations containing errors, such as misattributing dialogue in a cartoon. Human-written explanations were preferred over GPT-4's in over two-thirds of blind A/B tests.

The study highlights a gap between LLMs' ability to mimic humor and genuine comprehension. The researchers released the dataset and leaderboard to encourage further research in this area, aiming to improve LLMs' understanding of humor and its nuances. The work suggests that while LLMs can generate outputs that *appear* humorous, they often lack a deeper understanding of the underlying mechanisms of humor.</sample>
    <sample id="12">Five.</sample>
    <sample id="13">Daniel Rotem presented research on adaptive inference methods for large language models, aiming to reduce inference time and cost. The two primary approaches are Multi Model and Early Exit, both utilizing multiple classifiers to halt computation on simpler inputs.

Multi Model stores separate models, while Early Exit uses classifiers at intermediate layers of a single model. Multi Model offers versatility but suffers from overhead, as it runs all previous models even when halting. Early Exit is faster and memory-efficient but faces a key challenge: conflicting gradients.

The research hypothesizes that multiple classifiers sharing model parameters in Early Exit lead to conflicting gradient updates, degrading overall performance. Experiments comparing Early Exit and Multi Model classifiers confirmed this, with Multi Model classifiers outperforming Early Exit classifiers by an average of 2.3%, particularly for earlier classifiers.

To address this, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that isolates gradient updates to only the following classifier, eliminating conflicting gradients. SWEET significantly closes the performance gap between Early Exit and Multi Model. While some later classifiers may be negatively impacted, SWEET consistently outperforms both methods at high inference speeds and demonstrates superior performance across the entire speed/accuracy curve for BERT-Large.

The key takeaways are the identification of conflicting gradients in Early Exit training, a comprehensive comparison of adaptive inference methods, and the introduction of SWEET as a promising solution for improving Early Exit performance. The paper, "Finding the SWEET spot," is available on Archive for further details.</sample>
    <sample id="14">大家好，我叫亚当·普热皮奥尔科夫斯基，这次演讲是关于配列表结构的依存关系。正如大家所知，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存关系中，像“丽莎、巴特和玛吉”这样的配列表结构，第一个连词是整个配列表结构的头部，也就是丽莎。伊戈尔·梅尔丘克的语义文本理论也采用了类似的方法，同样将整个配列表结构以第一个连词为头部。这两种方法都是不对称的，它们突出显示了配列表结构中的一个连词。

相反，布拉格方法采用了一种不同的方法，在布拉格依存树库中，配列表结构以连词为头部，从而从连词指向所有连词。还有一种多头方法，例如在赫德森的词法语法中，他们认为所有连词都是配列表结构的首部，从而从支配词指向所有连词。

本次研究的目的是为配列表结构的对称结构（如前两种）提出一种新的论据，并反对配列表结构的不对称结构（如后两种）。这个论据基于依存关系长度最小化的原则，我将通过这些例子来解释。

在英语中，大家可能知道，直接宾语更倾向于靠近动词，而状语可以离动词更远。“玛格阅读它昨天”是可以接受的，因为直接宾语靠近动词，“玛格阅读昨天它”则不太好，因为状语“昨天”位于动词和直接宾语之间。然而，当直接宾语非常长时，这种效果可能会得到缓解。

例如，这两个句子都可以接受：“玛格阅读一本关于蜜蜂的绝对迷人的书昨天。” 也可以说：“玛格阅读昨天一本关于蜜蜂的绝对迷人的书。” 原因是，虽然后一个句子违反了直接宾语应该靠近动词的通用语法原则，但它满足了依存关系长度最小化的原则，即更短的依存关系更受欢迎。

这些树图只显示了关键依存关系的长度，即在这些结构中不恒定的那些。在这里，我们有一个从“阅读”到状语的依存关系，长度为7个单词，以及从“阅读”到“书”的依存关系，长度为4个单词，总共是11。当交换这两个成分时，这两个依存关系的长度之和变为6。因此，从11变为6，这使得它听起来还可以接受。

我们从增强版的宾夕法尼亚树库中提取了各种关于配列表的统计数据，并在论文“为什么不使用通用依存关系”中进行了分析。这些统计数据证实了之前多次观察到的现象，即左侧连词往往更短，例如“盐和胡椒”，而不是“胡椒和盐”，以音节为单位进行测量。此外，在解析中也观察到，这种趋势随着长度差异的增加而增强，当两个连词的长度差异增大时，更短的连词更倾向于放在前面，比例更大。

但本论文的新颖之处在于，我们观察到这种趋势仅在支配词位于左侧或不存在时才会发生。支配词位于左侧，例如“我看见巴特和丽莎”。当支配词不存在时，例如“荷马来了又打了个喷嚏”，这里是两个动词的配列，没有外部支配词。在这些情况下，左侧连词更倾向于更短，并且连词长度差异最大。然而，当支配词位于右侧时，例如“笑了”支配“泰德和内德”，这种趋势就会消失。

我们通过测量字符数、音节数和单词数来证明这一点。我们看到，当支配词位于左侧时，左侧连词更短的趋势随着单词数的绝对差异而稳步增长，当没有支配词时，也观察到相同的现象。但当支配词位于右侧时，这种趋势就会消失。我们在论文中展示了这如何为配列表结构的不对称结构（如前两种）提供论据，并为对称结构（如后两种）提供论据。

请参阅论文以获取完整的论据，并在海报环节与我们交流。谢谢。</sample>
    <sample id="15">3</sample>
    <sample id="16">Bible texts are much more strongly simplified than news texts or language learner texts.</sample>
    <sample id="17">This presentation introduces a novel approach to multimodal relation extraction (MRE), addressing limitations in existing methods. Traditional relation extraction relies solely on text, but real-world data often includes visual information, which can be ambiguous or insufficient on its own. MRE aims to leverage both text and visuals, but current approaches suffer from two key problems: internal-information over-utilization (using irrelevant text) and external-information under-exploitation (not fully utilizing visual data).

The proposed method tackles these issues through a two-pronged strategy: fine-grained information pruning and external information enrichment. First, it constructs a unified cross-modal graph (CMG) from text and image scene graphs. Then, it employs a Graph Information Bottleneck principle to filter nodes and edges within the CMG, effectively removing irrelevant information.  Finally, it enriches the CMG features with multimodal topic information, retrieved as keywords from both text and images, using an attention mechanism to enhance context.

Experiments on a standard MRE dataset demonstrate significant performance improvements compared to existing methods, with ablation studies confirming the contributions of both information screening and external enrichment. Further analysis reveals that internal-information screening (GENE) is more beneficial when text and visuals are highly relevant, while external-information exploitation (LAMO) is more effective when relevance is low.

In essence, the research proposes a system that simultaneously subtracts irrelevant information and adds valuable context, leading to a more robust and accurate multimodal relation extraction framework. The QR code at the end provides access to the full research paper for those interested in further details.</sample>
    <sample id="18">"salt and pepper"</sample>
    <sample id="19">Zhang Qin from Shenzhen University presented their ACL 2023 accepted work, "A Survey for Efficient Open Domain Question Answering," focusing on the challenges and solutions for building efficient open-domain QA systems.

The standard approach involves a two-stage retrieval-reader framework, where a question encoder and document encoder work together to retrieve relevant evidence from a massive Wikipedia corpus (26 million documents, 65GB index). However, the large corpus size, index size, and complex language models pose significant challenges for real-time applications and resource-constrained devices.

The survey explores various techniques to address these challenges, including one-stage frameworks like retrieval-only and generator-only systems. Key tactics involve fast evidence retrieval (approximate nearest neighbor search), fast reading (skip reading/adaptive computation), index size reduction (document filtering, embedding compression), and model size reduction (lightweight models, parameter sharing, one-stage models).

The analysis reveals trade-offs between different approaches: retrieval-reader systems offer a balance, retrieval-only systems prioritize speed but create large indexes, and generator-only systems avoid indexes but often have large models and lower performance.

The authors conclude that resource limitations might necessitate index or model size reduction, while real-time feedback favors retrieval-only systems. Future work includes deploying systems on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Yes, the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license.</sample>
    <sample id="21">新闻文本。</sample>
    <sample id="22">Better model architecture, larger model size, and more fine-tuning examples.</sample>
    <sample id="23">Dan Garrette's research focuses on improving text rendering in text-to-image models, specifically addressing the common issue of these models struggling to accurately depict text within generated images. Their work centers on the Imagen model, which utilizes a T5-XXL text encoder to generate image prompts.

The core problem identified is that T5's subword tokenization hinders its ability to accurately spell words. Instead of receiving individual letters, T5 processes chunks of text, requiring it to decompose words into letters for rendering. Experiments revealed that even the largest T5 models exhibit relatively low spelling accuracy, particularly with frequent words due to SentencePiece's tokenization strategy.

In contrast, PaLM models demonstrate superior spelling accuracy but are computationally expensive. ByT5, which operates at the byte level, excels at spelling as it directly accesses character-level information.

To address this, Garrette's team augmented the Imagen model by incorporating a ByT5-small representation alongside the existing T5 encoding. This minimal addition (only a 5% increase in parameters) significantly improved the model's spelling ability and, consequently, its text rendering capabilities. While the diffusion model can still introduce errors, this approach represents a practical and efficient strategy for enhancing text rendering in text-to-image models.

The research introduces two new benchmarks, WikiSpell and DrawText, to evaluate text rendering capabilities and highlights the effectiveness of character-aware model concatenation for improved spelling.</sample>
    <sample id="24">By measuring length in characters, syllables, and words.</sample>
    <sample id="25">通过测量字符、音节和单词数，观察支配词在左侧、右侧或缺失时，左侧连词是否倾向于更短。</sample>
    <sample id="26">The baseline classifier performed not much better than chance.</sample>
    <sample id="27">The text does not mention the number of authors.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Context-aware MT models demonstrate significantly higher accuracy than context-agnostic models specifically for formality and lexical cohesion.</sample>
    <sample id="30">LLM-Blender is a novel and straightforward ensemble learning framework designed to leverage the strengths of multiple large language models (LLMs). The core idea stems from the observation that no single LLM consistently excels across all input examples, despite leaderboard rankings.

The framework operates in two stages. First, it runs 'n' different LLMs on a given input (X), generating multiple candidate outputs (Y₁ to Yₙ). Then, a crucial component called PairRanker compares these candidates in pairs. PairRanker utilizes a cross-attention module (like RoBERTa) to analyze subtle differences between candidate pairs alongside the original input, providing a more nuanced comparison than methods that evaluate candidates individually. This pairwise comparison generates a comparison matrix, which is then aggregated (using max logits or bubble sort) to determine a ranking of the candidates.

The second stage, GenFuser, selects the top K (e.g., top three) ranked candidates and feeds them into a sequence-to-sequence model for fused generation, producing the final output. Experiments demonstrate that LLM-Blender significantly outperforms individual LLMs like Open Assistant and Vicuna across various metrics, including those evaluated by ChatGPT.

To facilitate research and evaluation, the authors also introduced MixInstruct, a new dataset comprising existing instruction datasets and candidate outputs from 11 open-source LLMs. The codebase and dataset are publicly available, encouraging further exploration and development in LLM ensemble learning. LLM-Blender offers a simple yet effective approach to harnessing the collective power of multiple LLMs for improved performance.</sample>
    <sample id="31">The paper is a joint work with John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams. The text does not specify the authors' institutions.</sample>
    <sample id="33">The NLPositionality framework quantifies positionality by re-annotating datasets with diverse annotators, collecting demographic data, and then comparing annotator judgments with existing datasets and models using Pearson's R correlation scores.</sample>
    <sample id="34">CREST is a novel framework that combines rationalization and counterfactual text generation to improve model interpretability and performance. The core idea is to leverage the strengths of both selective rationalization (highlighting key tokens) and counterfactual generation (editing inputs to change predictions).

The framework consists of two main components: a counterfactual generator and a rationalizer. The generator creates counterfactual examples by masking parts of the input and prepending a gold label, then using a masked language model to fill in the gaps. The rationalizer then analyzes both the original and counterfactual inputs, highlighting meaningful rationales.

A key innovation is CREST-Rationalization, which trains a model on both factual and counterfactual examples. This involves two processing flows – one for the original input and one for the counterfactual – both feeding into a shared rationalizer and predictor. A regularization term encourages the rationales for both inputs to be similar, focusing the model on factual and counterfactual reasoning.

Experiments on IMDB and SNLI datasets demonstrate that CREST-Rationalization achieves state-of-the-art results, particularly on out-of-domain data. Human evaluations confirm that CREST generates more valid and natural counterfactuals compared to existing methods like MiCE.

Furthermore, the study assesses the interpretability of CREST-generated rationales across three dimensions: plausibility, forward simulability, and a novel "counterfactual simulability" metric. Results show that CREST produces more plausible and counterfactually simulable rationales, indicating that the explanations effectively guide changes in the classifier's decision when presented with corresponding edits. Overall, CREST offers a promising approach for building more interpretable and robust NLP models.</sample>
    <sample id="36">This presentation introduces "Learning Language-Specific Layers (LSLs) for Multilingual Machine Translation," a novel approach to enhance multilingual translation models. The core problem addressed is the limited capacity per language in standard multilingual models, despite the benefits of scalability, speed, and improved low-resource translation.

The solution, LSLs, involves adding a dedicated transformer layer for each language. A language selector, based on either the source or target language, activates only the relevant sublayer during inference, maintaining constant inference costs. The key innovation lies in *learning* the optimal placement of these LSLs within the encoder.

Instead of manual placement or trial-and-error, the authors train a large model with shared, source, and target weights for each encoder layer. By analyzing the trained weights, they identify patterns indicating the relative importance of each component across different layers. They observed that source weights are consistently important, while target weights show a reversed importance pattern from bottom to top of the encoder.

The placement selection process involves choosing the component (shared, source, or target) with the largest weight for each layer. This results in a customized architecture with strategically placed LSLs.

Experiments on WMT21 news translation across 10 languages (including Swahili as a low-resource language) demonstrate significant improvements over baseline transformer models and language adapter approaches, as measured by chrF, spBLEU, and COMET. Notably, LSLs provide substantial gains for low-resource languages, with statistically significant improvements observed in 84 out of 90 translation directions. The approach also maintains faster inference speeds compared to larger baseline models. The presentation concludes by encouraging the audience to consult the full paper and poster session for further details and results.</sample>
    <sample id="37">研究发现，给予人类受试者相同的人格化提示，也能浮现出种族刻板印象。</sample>
    <sample id="38">The enhanced version of the Penn Treebank.</sample>
    <sample id="39">One.</sample>
    <sample id="40">Topic-independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="41">PeaCoK is a newly developed Persona-grounded Commonsense Knowledge Graph created by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. It addresses the need for narrative systems to better understand personas and their grounding in real-world knowledge. PeaCoK contains approximately 3,800 personas and 40,000 attributes, forming around 100,000 personal inferences, with significant interconnections between personas.

The graph's construction involved three key steps: selecting personas from existing commonsense graphs, inducing attributes using commonsense knowledge graphs and large language models, and crowdsourcing relation annotations with a human-AI majority voting scheme (achieving 87% accuracy). This AI-assisted annotation process, utilizing InstructGPT-3, efficiently resolves disagreements among human annotators.

Experiments demonstrate PeaCoK's effectiveness in improving language models' persona knowledge generation. A BART-based model trained on PeaCoK outperformed larger language models (GPT-3 and GPT-3.5) in attribute inference tasks, showcasing its ability to enable lightweight models to achieve comparable knowledge generation capabilities.

Furthermore, PeaCoK enhances downstream narrative modeling, specifically in dialogue generation. By augmenting speaker profiles with facts retrieved from PeaCoK, the system (P²Bot) generated more fluent, consistent, engaging, and persona-expressive dialogues compared to baselines. Notably, PeaCoK’s persona-centric knowledge proved more beneficial than general social commonsense knowledge (as demonstrated by comparison with Atomic2020). Human evaluations revealed that dialogue quality improved with greater overlap in shared attributes between speakers, emphasizing the importance of interconnected persona knowledge for narrative coherence and engagement.

The paper and associated GitHub repository are publicly available, offering a valuable resource for researchers and developers working on narrative understanding and generation.</sample>
    <sample id="42">The text does not mention the number of authors.</sample>
    <sample id="43">The text does not mention the number of authors.</sample>
    <sample id="44">NLPositionality框架与先前研究的不同之处在于，它将真实用户（end users）的标注与数据集和模型进行比较，而非仅仅关注标注者之间的意见一致性或建模标注者分布。</sample>
    <sample id="45">生成的persona。</sample>
    <sample id="46">DeepL and Google Translate.</sample>
    <sample id="47">今天Shangbin博士的报告主要探讨了政治偏见如何在预训练数据、语言模型和下游任务之间传播，从而导致不公平的自然语言处理（NLP）模型。

**主要内容：**

1.  **语言模型的政治倾向评估：** 研究人员使用政治问卷（如政治会议测试）来自动评估语言模型的政治倾向。初步结果表明，语言模型确实存在不同的政治倾向，它们分布在政治光谱的四个象限中。GPT-4是最自由派的模型，GPT系列通常比BART系列更倾向于社会自由主义。
2.  **预训练数据对政治偏见的影响：** 通过在不同政治倾向的新闻和社交媒体语料库上进一步预训练语言模型，研究人员发现模型的政治倾向会随之改变。例如，在左倾Reddit语料库上训练的RoBERTa模型，政治倾向明显左移。
3.  **社会极化：** 研究人员还发现，语言模型能够捕捉到社会中日益严重的极化现象。在2017年之前和之后分别训练的语言模型，后者通常更远离政治中心。
4.  **下游任务中的公平性问题：** 研究人员评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测等任务中的表现。结果显示，左倾模型在检测针对弱势群体的仇恨言论方面表现更好，但在检测针对强势群体的仇恨言论方面表现较差，反之亦然。在虚假新闻检测中，左倾模型更容易识别出与自己政治立场相反的信息，反之亦然。
5.  **面临的困境：** 研究人员指出，处理语言模型政治偏见存在两难境地：不进行政治观点过滤可能导致偏见传播和公平性问题，而试图进行过滤则可能导致审查或排除。如何确定什么是中立且应保留在语言模型训练数据中的内容，是一个极具挑战性的问题。

**总结：**

该研究强调了语言模型政治偏见带来的公平性问题，并呼吁人们关注和解决这些问题。</sample>
    <sample id="48">David Vilar and his colleagues from Google Translate.</sample>
    <sample id="49">1024</sample>
    <sample id="50">DEPLAIN是一个新的德语文本简化语料库，分为DEPLAIN-apa（新闻文本）和DEPLAIN-web（不同领域文本）两个子语料库。DEPLAIN-apa包含483篇手动对齐的文档，约13000个句子对；DEPLAIN-web包含750篇文档，约30450个句子对，采用手动和自动对齐相结合的方法。

语料库分析显示，圣经文本的简化程度高于新闻文本和语言学习者文本。DEPLAIN语料库在词汇简化、结构简化和整体简化方面具有多样性，不同子语料库的简化方式也不同。

DEPLAIN语料库有两个主要应用场景：首先，可以作为黄金标准来评估自动对齐方法，实验结果表明MASSalign方法效果最佳；其次，可以通过微调语言模型实现自动文本简化，实验中使用了long-mBART和mBART模型，并取得了优于基线的成绩，为未来自动文本简化任务提供了基准。</sample>
    <sample id="51">音乐、书籍和食谱。</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">This work addresses the challenge of detecting cognitive dissonance in language, a rare phenomenon with implications for understanding disagreement, mental health, and decision-making. Cognitive dissonance occurs when beliefs or actions are inconsistent, as illustrated by someone acknowledging the health risks of smoking while continuing to smoke and justifying it.

Researchers created a dataset of approximately 1,000 discourse unit pairs, finding dissonance in only 3.5% of them. Initial attempts at classification using a model trained on a tiny dataset (43 examples) performed poorly due to the rarity of dissonance.

To overcome this, the study explored transfer learning and active learning (AL) to efficiently annotate more dissonant examples. They transferred knowledge from related tasks: "debate" (stance classification in debates) and "CE" (expansion/comparison relations from the Penn Discourse Treebank). Fine-tuning CE followed by debate proved most effective for initial performance.

The research then investigated different AL update strategies ("Cumulative" vs. "Iterative") and a novel Probability-of-Rare-Class (PRC) selection strategy to prioritize dissonant examples. PRC outperformed other state-of-the-art AL methods, though the improvement was modest. Combining PRC with the best AL strategy resulted in an AUC of 0.75, representing the best performance achieved on this task.

While PRC proved effective for rare class acquisition, annotators found the selected examples challenging. The study concludes that PRC, combined with appropriate transfer learning, is a valuable strategy for rare class acquisition and cold-starting active learning, with cumulative updates being beneficial for in-domain annotation.</sample>
    <sample id="55">Yes, EDAtt uses existing offline ST models without retraining or adopting specific architectures for SimulST.</sample>
    <sample id="56">The text does not mention the number of authors.</sample>
    <sample id="57">Without task-specific training on KITMUS, most models do not perform well.</sample>
    <sample id="58">Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="59">This presentation introduces DrBERT, the first open-source biomedical model in French. Addressing the scarcity of French language models in specialized domains, the research explores optimal data sources and training strategies for biomedical NLP.

The core contribution is DrBERT, built upon RoBERTa and pre-trained on NACHOS, a large dataset of medical web crawls. The study compares DrBERT against ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital, to assess the value of crawled versus clinical data. Furthermore, it investigates the impact of training data size, comparing models trained on 4GB, 7GB, and 8GB datasets.

A total of seven models were evaluated: four from-scratch models (DrBERT variations and ChuBERT variations), and three models utilizing continual pre-training based on CamemBERT and PubMedBERT. These were benchmarked against six baseline models, including CamemBERT and English biomedical models.

The results demonstrate that model performance is best when aligned with the training data's nature, but heterogeneous data sources offer greater versatility. More training data generally leads to improved performance, and from-scratch pre-training tends to outperform continual pre-training. Notably, a DrBERT model trained on a smaller 4GB subset of NACHOS achieved comparable results to the larger from-scratch DrBERT, highlighting the potential of efficient pre-training.

Ultimately, DrBERT consistently outperformed the generic CamemBERT model across most downstream tasks, confirming the benefits of specialized pre-training. The pre-trained DrBERT models and training scripts are publicly available on Hugging Face and GitHub, respectively, under the MIT license, encouraging further research and application in French biomedical NLP.</sample>
    <sample id="60">The author's affiliation is not explicitly stated in the provided text.</sample>
    <sample id="61">Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training," investigates methods for compressing large natural language generation (NLG) models while preserving performance—a crucial need in the industry due to the cost and slowness of these models.

The research departs from existing knowledge distillation work, which often focuses on classification, NLU, or pre-training, and from NLG studies that concentrate on single tasks with massive datasets. Instead, it conducts a systematic study of task-specific knowledge distillation across diverse NLG tasks (summarization, question generation, common sense reasoning, simplification, and style transfer) in realistic, "industry-driven" settings.

These settings are defined by five key criteria: medium-resource labeled datasets (due to annotation costs), large amounts of unlabeled data, medium-sized off-the-shelf models, a focus on inference time efficiency (high compression rates), and negligible one-time training resources.

The study explores eight stages, including architectural choices (encoder/decoder vs. decoder-only), the impact of pruning, and comparisons of knowledge selection approaches. The core contribution focuses on extending the use of pseudo-targets—outputs generated by the teacher model to augment training data.

The paper challenges the traditional single-mode pseudo-target generation using beam search, demonstrating that:

*   Unlabeled data is crucial for boosting distillation.
*   Generating multiple pseudo-targets improves student performance.
*   Sampling pseudo-targets (even with high temperature for diversity) exposes the student to a wider range of knowledge.

Finally, the authors introduce "joint-teaching," a novel technique combining word-level distillation on both teacher- and student-generated pseudo-targets to address student exposure bias and improve learning. The paper provides a recipe for knowledge distillation in NLG, aiming to provide practical guidance for model compression.</sample>
    <sample id="63">It measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Higher sensitivity indicates improved model performance.</sample>
    <sample id="66">This paper surveys recent advancements in "Deep Learning for Mathematical Reasoning," a crucial area of AI and NLP focused on enabling machines to solve mathematical problems and prove theorems. The field has seen a surge of interest, driven by the need for machines to understand and utilize numerical data and language effectively.

Mathematical reasoning extends beyond text, encompassing visual (diagrams, geometric problems) and tabular data. The paper highlights neuro-symbolic approaches for geometric problem-solving, combining neural networks with symbolic reasoning over diagrams, theorems, and solvers. Automated theorem proving, aiming to formally demonstrate mathematical truths, is also discussed.

The rise of pre-trained language models (LLMs) has significantly impacted the field. Sequence-to-sequence and sequence-to-tree models have been employed to represent and generate mathematical expressions. LLMs, particularly when prompted with "chain-of-thought" reasoning steps, show promise in solving complex word problems. However, LLMs face limitations in precise mathematical reasoning.

To address these limitations, researchers are exploring techniques like self-consistency (sampling multiple reasoning paths) and augmenting LLMs with tools. Program-aided LLMs, like Chameleon, which generate natural language programs to utilize various tools, are proving effective.

The survey also notes the growing interest in mathematical reasoning in low-resource settings (non-English datasets) and specialized domains (finance, science, medicine). Despite progress, current models still struggle with generalization and robustness, particularly with large numbers and inconsistent reasoning. The paper concludes by acknowledging the ongoing challenges and future directions in this rapidly evolving field.</sample>
    <sample id="67">This work investigates interference and synergy in multilingual translation models, challenging the assumption that specialized algorithms are always needed to mitigate negative effects. The study identifies key factors influencing these phenomena, finding that severe interference primarily occurs when models are small relative to the dataset size – a condition termed "parameter poverty."

Contrary to common assumptions, language similarity (e.g., Romance vs. Slavic languages) has a surprisingly limited impact on interference levels, especially when sufficient data is available for the low-resource language pair. Similarly, the total number of languages in the model doesn't significantly affect interference.

The research highlights the crucial role of temperature sampling in controlling the trade-offs. A temperature greater than 1 allows the model to sample more from lower-resource languages. The study demonstrates that using a tuned temperature is key to strong performance, with uncalibrated, high temperatures (often a default of 5) proving detrimental in larger models. A baseline for battling interference is weak due to size in small models, and weak due to uncalibrated temperature for larger ones.

The findings reveal scaling laws applicable to bilingual scenarios also hold true for multilingual models, but with the added complexity of data size for other languages. Ultimately, the research suggests that modest scaling (increasing model size and data) combined with carefully tuned temperature can substantially reduce interference without requiring complex, specialized algorithms. The paper encourages readers to explore further experiments on the effect of the number of languages and results with more languages, which share similar trends.</sample>
    <sample id="68">Models are sensitive to latent syntactic and semantic features which are shared across sentences.</sample>
    <sample id="69">Typically, 20 samples per class.</sample>
    <sample id="70">The paper is a collaboration between Esin Durmus and Dan Jurafsky.</sample>
    <sample id="71">The work introduces the AltEntities Corpus, a new dataset designed to study how users employ indirect referring expressions when selecting entities in conversational settings. The research, a joint effort by Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, addresses the challenge of understanding natural language choices, particularly when direct references like names or positions are unsuitable.

The corpus comprises 6,000 alternative questions across three domains: music, books, and recipes, yielding 42,000 indirect referring expressions. The dataset's creation utilizes a cartoon completion setup where annotators respond to a dialogue context ("Remember that song we were listening to yesterday?") with an alternative question ("Do you mean A or B?") and then provide an indirect reference to select one of the entities.

A key aspect of the data collection process is the generation of alternative questions. These questions are created using a template ("Do you mean A or B?") with entities sampled from Wikipedia using increasingly specific similarity criteria: uniform random selection, similar titles, similar descriptions, and similar attributes (e.g., genre, artist). Annotators are provided with background knowledge—Google search links for songs, Wikipedia text and images for recipes and books—to inform their choices and descriptions.

Experiments with a T5 XL model reveal that accuracy in resolving indirect references is highly dependent on the model's access to background knowledge. With identical background knowledge as the annotators, accuracy reaches 92-95%. Partial overlap yields 82-87%, a more realistic scenario. However, with only entity names, accuracy drops to 60%, highlighting significant room for improvement. The study also demonstrates the models' ability to generalize across domains. The AltEntities Corpus is publicly available, offering a valuable resource for benchmarking and advancing entity understanding in conversational AI and LLMs.</sample>
    <sample id="72">Because existing methods for evaluating political leaning in language models are not well-grounded in political science literature.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">This paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph built upon the existing ATOMIC knowledge base. ATOMIC, while high-quality, suffers from limited knowledge coverage and a lack of multi-hop paths due to its focus solely on B-to-A links. Dense-ATOMIC addresses this by incorporating missing B-to-B, A-to-B, and A-to-A links, creating richer, multi-hop paths like "X asks Y to marry, then Y says yes, then X smiles."

The construction of Dense-ATOMIC involves three key steps: normalizing tail events to standardize their representation, training a relation prediction model, and then using this model to construct the expanded knowledge graph. To overcome limitations of traditional ATOMIC completion methods—sparse graph structure and insufficient semantic utilization—the authors propose Rel-CSKGC. This method predicts relations given head and tail events, leveraging RoBERTa for semantic encoding and a novel Intra- and Inter-Cluster Completion Strategy to manage computational expense.

Rel-CSKGC outperforms existing relation prediction and translation-based methods, both automatically and through human evaluation. Evaluations on Dense-ATOMIC demonstrate its improved knowledge coverage, with significantly more 1-, 2-, and 3-hop paths. Furthermore, Dense-ATOMIC enhances the performance of COMET, enabling more diverse result generation. Analysis of randomly sampled multi-hop paths from Dense-ATOMIC reveals a high prevalence of meaningful sequences, showcasing its potential for commonsense reasoning. The paper concludes by highlighting the contributions of Dense-ATOMIC and Rel-CSKGC, providing code and a website for further exploration.</sample>
    <sample id="75">Jointprop is a novel semi-supervised learning framework developed by Zheng Yandan, Hao Anran, and Luu Anh Tuan, designed to improve both Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The core motivation stems from the observation that existing semi-supervised approaches often overlook the inherent connections between NER and RE, potentially hindering label alignment and information integration. Jointprop addresses this by modeling these tasks jointly and leveraging heterogeneous graphs to propagate labels effectively.

The framework operates in four key stages. First, it generates span and span pair representations using contextualized token embeddings and a trained classifier to predict labels for unlabeled data. Second, it constructs a k-Nearest Neighbor heterogeneous graph, connecting entity and relation nodes based on their similarity, facilitating label smoothing across neighboring unlabeled data. Third, it performs joint label propagation through the graph, iteratively refining pseudo-labels until convergence, allowing labels to diffuse across high-density areas of unlabeled data. Finally, it optimizes the model by filtering low-confidence pseudo-labels, combining the remaining high-confidence labels with existing labeled data, and retraining the classification model.

Experiments conducted on four datasets, including both joint and single-task datasets, demonstrate the effectiveness of Jointprop. Notably, on joint datasets, the framework benefits from the codependency between NER and RE. Even on single-task datasets, Jointprop consistently outperforms baseline models for both NER and relation extraction, showcasing its ability to leverage unlabeled data and inter-task connections to achieve significant improvements. The absence of prior baselines for semi-supervised joint-task settings highlights the novelty of this approach.</sample>
    <sample id="76">The political bias propagation pipeline goes from pretraining data to language models to downstream tasks.</sample>
    <sample id="77">This video presents the work "On Improving Summarization Factual Consistency from Natural Language Feedback," a joint effort from Yale University and Microsoft Research. The core contribution is the introduction of DeFacto, a new dataset designed to improve factual consistency in abstractive text summarization. DeFacto includes human demonstrations and feedback on system-generated summaries, collected using the XSum dataset and initial outputs from the Pegasus model.

The dataset analysis reveals that 70% of initial summaries contain factual errors. Human-edited summaries, while achieving higher factuality scores, exhibit lower textual overlap with reference summaries, likely due to pre-existing factual inaccuracies in the XSum dataset.

The research proposes three new natural language generation (NLG) tasks leveraging DeFacto: summary editing, feedback generation, and automatic factual error correction. Summary editing, where models refine summaries based on human feedback, shows promise with both fine-tuned models and large language models. Feedback generation, requiring models to produce helpful feedback, remains challenging. Automatic factual error correction demonstrates comparable performance to baselines with less training data, and generating explanations alongside corrections improves performance.

Beyond the NLG tasks, DeFacto’s fine-grained annotations are valuable for training factuality metrics and conducting meta-evaluation. The dataset is publicly available on GitHub, and the paper provides further details. The overall goal is to advance the factual consistency of summarization models by utilizing human feedback and creating targeted training tasks.</sample>
    <sample id="78">DEPLAIN-apa corpus has more reorderings and word additions, while the web corpus has more rephrasings.</sample>
    <sample id="79">Yes, the CoScript dataset is available.</sample>
    <sample id="80">The watermark is injected by defining a target embedding and creating a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers (words from a trigger set) in the sentence.</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This video presents a new framework called ULRA (Unsupervised AES by Learning from Rank Aggregation) for Automated Essay Scoring (AES) without requiring labeled essay data. Current unsupervised AES methods using single heuristic signals (like unique term count or word count) have limitations. ULRA addresses this by leveraging multiple heuristic quality signals to create a "pseudo-groundtruth" for training.

The framework consists of two main modules:

1.  **HER (Heuristic Essay Ranking):** This module generates partial order pairs by ranking essays based on various quality signals (e.g., readability, lexical diversity). It transforms these rankings into pairs indicating which essay is considered "better" according to each signal.

2.  **DPRA (Deep Pairwise Rank Aggregation):** This module trains a neural AES model using the partial order pairs. It employs a novel "Deep Pairwise Rank Aggregation loss" that assigns learnable confidence weights to each quality signal, allowing the model to prioritize more reliable signals and resolve inconsistencies between them.

Finally, a "Scoring Strategy" maps the model's predicted scores to a predefined scoring range.

Experiments in both transductive and inductive settings show that ULRA significantly outperforms existing unsupervised AES methods. While it doesn't yet match the performance of supervised methods (due to the lack of strong ground truth labels), it achieves competitive results compared to cross-prompt and one-shot approaches.

In essence, ULRA's key innovation is its ability to aggregate information from multiple heuristic signals to provide more robust and effective supervision for unsupervised AES, leading to improved essay scoring accuracy.</sample>
    <sample id="83">Yes, Encoder-Decoder models like mT5 can be improved by training in a mixture of various languages.</sample>
    <sample id="84">Shwai He's ACL 2023 paper introduces PAD-Net, a framework for efficient dynamic networks. Traditional networks are static, using fixed parameters, while dynamic networks adjust architecture or parameters based on input, often leading to improved performance. However, fully dynamic networks, where all parameters are dynamic, suffer from excessive parameter usage, significantly increasing model size.

PAD-Net addresses this by hypothesizing that fully dynamic networks contain partially dynamic subnetworks that can maintain or exceed the original network's representation power. The framework partitions parameters into dynamic and static components, using scale factors to control their influence and constraints to accelerate training.

The core method, Iterative Mode Partition, identifies and converts redundant dynamic parameters into static ones, minimizing impact on the loss function. Experiments demonstrate PAD-Net outperforms both static and fully dynamic networks, achieving better performance with significantly fewer parameters and reduced computation.

Ablation studies reveal the importance of dynamic ratios for components like Dynamic Convolution and Mixture of Experts, as well as the scale factors governing dynamic and static parameters. PAD-Net also surpasses network pruning techniques due to its preservation of static parameters and produces more discriminating outputs.

Future research directions include extending PAD-Net to other networks, optimizing for hardware efficiency, and exploring additional parameter modes (e.g., incorporating zero elements).</sample>
    <sample id="85">"make a chocolate cake"</sample>
    <sample id="86">By visualizing the embeddings of sentences on four datasets using PCA, it's hard to distinguish between the backdoor embeddings and normal embeddings.</sample>
    <sample id="87">通过持续预训练（continual pre-training）或从头开始预训练（from-scratch pre-training）来构建新的 PLM。</sample>
    <sample id="88">非二元人群。</sample>
    <sample id="89">"I'm going to talk about..."</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the traditional reliance on native speakers for NLP data annotation, proposing that language learners can be a viable and beneficial alternative, particularly for low-resource languages. The authors conducted a proof-of-concept study across English, Korean, and Indonesian, utilizing four common NLP tasks (sentiment analysis, NLI, NER, and MRC) from the GLUE benchmark.

The study meticulously controlled for language proficiency, categorizing learners into basic, intermediate, and advanced levels based on revised CFR criteria. Both language learners and native speakers participated, annotating a randomly sampled set of 120 questions categorized by difficulty. Learners were further divided into groups receiving different types of additional resources (dictionaries, machine translation) to assess their impact on annotation accuracy and learning.

The experimental design involved a pre-test to gauge language proficiency, annotation of 10 questions, a post-test to measure learning effects, and a final survey.  The results demonstrate that learner-annotated labels are surprisingly accurate, especially for simpler tasks and questions of moderate difficulty.  Crucially, aggregating learner labels through majority voting yields results comparable to those of native speakers.

Furthermore, training language models on learner-generated data achieved approximately 95% of the performance of models trained on native speaker data, and occasionally even surpassed them. The study also observed improvements in learner vocabulary and grammar through pre- and post-test comparisons.

Ultimately, the paper advocates for a novel approach to data construction in low-resource settings, moving beyond translation-based methods. It highlights the potential of leveraging language learners to broaden NLP research and overcome barriers in building benchmark datasets for languages where native speaker recruitment is challenging.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="92">The author compares their method with other treeless models on the COGS benchmark.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are Matthias Lindemann's advisors.</sample>
    <sample id="94">This paper introduces "Embedding Marker," a novel backdoor-based watermark method designed to protect the copyright of embedding as services (EaaS) powered by large language models (LLMs). EaaS, like OpenAI's GPT embedding API, are vulnerable to model theft via embedding learning. Existing watermark solutions either lack applicability to EaaS or fail to ensure transferability during model extraction.

Embedding Marker addresses these shortcomings through two key steps: watermark injection and copyright verification. First, a "trigger set" of moderately frequent words is selected. During injection, when a user submits a sentence, the service counts trigger words. The resulting embedding is a weighted sum of the original embedding and a "target embedding," with the weight proportional to the trigger count. If the trigger count exceeds a threshold (m), the embedding becomes solely the target embedding.

Copyright verification involves constructing backdoor and benign datasets—one containing only trigger words, the other avoiding them. The provider then requests embeddings from the suspected "stealer" service using these datasets. Cosine and L2 similarities between the requested embeddings and the target embedding are calculated, along with a KS test p-value. The difference in these similarities (delta cosine and delta L2) indicates potential copyright infringement.

Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate Embedding Marker's strong detection performance while maintaining high utility for downstream tasks. Visualization using PCA confirms the covertness of the watermark, making it difficult to distinguish between backdoor and normal embeddings. The method is applicable to EaaS, preserves utility, is covert, and ensures transferability.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">大家好。我是Jenny，卡内基梅隆大学一年级的博士生，今天我将为大家介绍我们的工作NLPositionality，它旨在刻画数据集和模型的偏见。这项工作是与华盛顿大学和人工智能研究所的一些同事合作完成的，分别是Sebastian Santy、Ronan Le Bras、Katharina Reinecke和Maarten Sap。

那么，我们先想象一下，你正在为报纸工作，正在筛选新闻文章下的评论，试图删除有毒内容。你可能会转向像Prospective API这样的流行API进行毒性检测。如果你的名字是Carl Jones，这个API就能很好地检测到有毒内容。但如果你的名字是Aditya Sharma，情况就不一样了。Prospective API对在印度语境中更常见的冒犯性词语的敏感度很低。

这就是一个设计偏见的例子，我们看到技术在不同人群之间的系统性性能差异。像我们刚刚看到的这种设计偏见，可能源于NLP研究人员和模型开发者的位置性。位置性简单来说，是人们由于他们的种族、身份和生活经历而持有的观点。作为一名研究人员，位置性会影响研究过程及其结果，因为它会改变研究人员做出的决定。

那么，人们可能会问，数据集和模型是否有位置性？我们并不是说模型或数据集本身具有人口统计学身份和生活经历，但它们确实聚合了真实人们的判断和意见，因此可能代表某些位置性而忽略其他位置性。

先前的研究表明了一些关于存在位置性的轶事证据，例如模型和数据集中的文化差距，以及对模型位置性的理论定义。然而，这些工作并没有将最终用户与数据集和模型本身进行比较，并且研究模型和数据集的位置性越来越重要，因为NLP任务变得更加主观和面向社会，而很难表征这些位置性是如何扭曲的，因为并非所有决策都有记录，而且许多模型隐藏在API之后。

为了研究数据集和模型的位置性，我们实际上将真实用户的注释与现有数据集和模型进行比较。我们通过我们的框架NLPositionality来实现。

我们的框架主要分为两个步骤。第一步是使用多样化的注释者重新标注数据集。我们这样做是为了查看原始数据集注释者的种族统计信息，因为通常只有几个注释者标注每个实例，而且很少收集和共享种族统计信息。因此，我们选择重新标注数据，以获得每个实例的许多注释，并获得丰富的种族数据。

然后，我们按种族对注释进行分组，并使用Pearson's R相关系数与模型和数据集进行比较。因此，我们的框架与注释者不一致性文献不同，因为它将最终用户与模型和数据集的预测和标签进行比较，而不是仅仅关注注释者一致性或建模注释者分布。

我们的框架很大程度上得益于Lab in the Wild，一个用于HCI合作的在线实验平台。在Lab in the Wild中，我们可以招募到多样化的志愿者。与主要来自美国或印度的M Turk平台相比，Lab in the Wild仍然能够获得高质量的数据。

我们在Lab in the Wild上托管了两个任务，其中一个是社会可接受度。它的工作方式是，参与者会阅读社会化学数据集中的一个情境，然后写下这个情境的社会可接受程度。之后，为了保持对研究的参与，他们可以将他们的回答与人工智能和其他人进行比较。

然后，我们将这些注释与Social Chemistry、Delphi和GPT 4进行比较。我们还为毒性和仇恨言论检测任务复制了类似的设置，参与者会阅读Dynahate中的一个实例，并写下他们是否认为这是一个仇恨言论的实例。然后，我们将这些注释与Dynahate、Perspective API、Rewire API、Hate Roberta和GPT 4进行比较。

我们的研究最终汇集了来自87个国家/地区的1000多名注释者超过16,000条注释。

现在，我们更有能力回答“NLP数据集和模型与哪些人群最对齐？”

我们发现NLP中存在位置性。例如，我们发现数据集和模型与英语国家最对齐。对于GPT 4社会可接受度分析，我们发现它与儒家和英语国家的人群最对齐。我们还发现Dynahate也与英语国家有额外的对齐。我们还发现与受过大学教育的人群有最强的对齐。

对于GPT 4在社会可接受度任务中的表现，我们发现它与受过大学教育或研究生教育的人群最对齐，我们也在Dynahate中发现了同样的现象，它与受过大学教育的人群最对齐。

然而，当模型和数据集与特定人群对齐时，一些人群不可避免地会被抛在后面。例如，数据集和模型与非二元性别的人群相比，与男性和女性人群的对齐度较低。我们也在GPT 4社会可接受度任务以及Dynahate任务分析中发现了这一点。

那么，既然NLP中存在位置性，我们该怎么办？

我们有一些建议。首先，记录研究过程中所有相关的设计选择。我们的第二个建议是以视角主义的视角进行NLP研究。我们的第三个建议是为特定社区构建专业数据集和模型。Masakhani倡议就是一个很好的例子。

我们想强调的是，包容性的NLP不仅仅是让所有技术为每个人服务。

以上就是我们的演讲内容。如果您想了解更多信息，请随时查看我们的仪表板，以获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives. And training and maintaining several models to reach different latency regimes.</sample>
    <sample id="98">The presentation highlights a dilemma: sanitizing training data to reduce bias risks censorship and determining neutrality is difficult. There's no easy solution presented, but the core challenge is balancing the need to mitigate bias-driven fairness issues with the risk of censorship and exclusion.</sample>
    <sample id="99">大家好，我是来自复旦大学的袁思雨。我来介绍我们的工作“从大型语言模型中提炼脚本知识以进行受约束的语言规划”。在日常生活中，人们经常遵循分步骤的指示，以形式为目标导向脚本的方式来规划自己的行动。之前的工作利用语言模型来规划刻板活动的抽象目标，例如“做蛋糕”，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在规划刻板活动的抽象目标上。规划具有特定约束的目标，例如“做巧克力蛋糕”，仍然鲜为人知。在本文中，我们定义了受约束的语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被不同的现实生活中的特定目标继承，这些目标具有多方面的约束。一个好的规划器应该编写既合理又能忠实于约束的脚本。在本文中，我们首先评估和改进了大型语言模型受约束的语言规划能力。由于没有特定的目标数据集来支持我们的研究，我们必须首先获取这些目标。如图表所示，我们使用InstructGPT进行人机循环数据采集，扩展了抽象目标以包含多方面的约束。我们采样了100个特定目标，并评估了大型语言模型生成的脚本。该表报告了结果的总体准确性。我们发现，所有语言模型在规划特定目标方面都取得了不令人满意的结果。然后，我们进行详细分析以调查学习模型失败的原因。图中的结果表明，生成的脚本的语义完整性是可以接受的，但不能保证对约束的忠实性。我们深入研究了 wikiHow 中定义的约束的更细粒度的类别。图中的热图显示，InstructGPTs 的规划性能在不同类别的目标中差异很大。先前的研究表明，语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“过度生成然后过滤”的想法来提高生成质量。我们首先展示了 InstructGPT 的约束类型示例，并根据种子抽象目标获得特定目标。接下来，InstructGPT 为特定目标过度生成 K 个脚本。然后，开发了一个过滤器模型来选择忠实的脚本。我们将脚本和目标转换为 InstructGPT 嵌入，并计算余弦相似度作为相似度得分来衡量语义相似度。此外，我们奖励包含目标约束关键词的脚本。只有当目标得分在目标集中最高时，我们才保留该脚本。通过我们的方法，InstructGPT 可以生成更高质量的脚本。我们的方法大大提高了语义完整性和对约束的忠实性方面的规划能力。由于大型语言模型的部署成本高昂，因此启用较小和专业化模型的语言规划能力至关重要。创建数据集是实现这一目标的重要一步。然而，之前的研究没有启用对特定目标的规划，并且手动数据集注释成本高昂。因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏受约束的语言规划数据集。我们将我们的方法应用于构建一个受约束的语言规划数据集，名为 CoScript。总共，我们生成了 55,000 个带有脚本的特定目标。为了确保验证和测试集的质量，我们请众包工人查找和修改不正确的样本。此图显示了 CoScript 的约束分布。我们发现 CoScript 在生成的特定目标中显示出高度的多元化。借助 CoScript，我们可以尝试较小但专业的模型来进行受约束的语言规划。我们发现，在 CoScript 上进行微调的 T5 可以生成比大多数大型语言模型更高质量的脚本，表明当在合适的训练集上进行适当训练时，较小的模型可以超越较大的模型。总之，我们建立了受约束的语言规划问题。我们评估了大型语言模型受约束的语言规划能力，并为大型语言模型开发了一种“过度生成然后过滤”的方法。我们使用大型语言模型生成了一个高质量的脚本数据集 CoScript，用于受约束的语言规划。我们希望 CoScript 数据集可以成为推进语言规划研究的宝贵资源。感谢您的时间。有关 CoScript 的更多详细信息，请参阅我们的论文。</sample>
    <sample id="100">PromptRank is a data-efficient approach to multi-hop question answering, addressing the need for fewer training examples compared to existing methods. It combines unsupervised retrieval with a few-shot language model-based reranker, achieving strong performance with as few as 128 examples.

The process involves two main steps: first, retrieving candidate chains using TF-IDF and hyperlink traversal; second, reranking these candidates using a few-shot language model. The scoring function utilizes the likelihood of the question given a constructed chain prompt, which includes chain documents marked with indicator tokens and an instruction designed to elicit reasoning.

PromptRank explores techniques like instruction search (finding optimal instructions), instruction sampling (aggregating scores from different instructions), and temperature scaling to enhance performance. Experiments using GPT2-XL and T5-XL on the HotpotQA dataset demonstrate that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art dense retrievers.

Ablation studies confirm the importance of each component, and downstream QA performance using a reader model (ELECTRA-Large) shows promising results, underperforming only slightly compared to MDR. The research highlights the effectiveness of language models for ranking candidate paths, the significance of the instruction in eliciting reasoning, and the superior performance of scoring based on the likelihood of the question given the chain.</sample>
    <sample id="101">PaLM 的流畅度与最先进的系统相当。</sample>
    <sample id="102">The watermark method needs to meet the following properties: applicable to embedding as services, not degrade the utility of the provided embeddings, covert enough to the attacker, and transferable to the attacker's services during the model extraction process.</sample>
    <sample id="103">14 different languages.</sample>
    <sample id="104">Over 16,000 annotations were amassed from over 1000 annotators.</sample>
    <sample id="105">Cosine similarity and L2 similarity.</sample>
    <sample id="106">QUEST is a new retrieval dataset designed to address the challenge of information seeking with complex, multi-constraint queries. The dataset is motivated by real-world scenarios like a zoologist identifying an unknown reptile species and a reader seeking their next book based on specific preferences. These situations highlight the need for systems to handle queries that implicitly involve set operations (intersection, complement, etc.).

QUEST comprises over 3,000 entity-seeking queries across four domains (films, books, plants, and animals). The queries are constructed using Wikipedia categories and refined through multiple rounds of human annotation to ensure fluency, relevance, and accurate attribution of evidence within documents. Annotators identify spans of text within documents that support different query constraints, marking which parts of the document provide evidence for each aspect of the query.

The dataset's difficulty stems from the need to retrieve multi-answer sets from a large document corpus, where relevance evidence can be scattered across different parts of a document. Initial evaluations using sparse and dense retrievers, along with a T5-based reranker, reveal significant room for improvement. Current systems struggle to achieve high F1 scores, particularly with queries involving set intersection and difference operations.

The creators of QUEST hope it will spur research into developing more effective information retrieval systems capable of understanding and satisfying users' selective information needs, ultimately benefiting scenarios similar to those faced by Jane and Austin. The paper detailing QUEST is available, and the authors invite attendees to a presentation at ACL.</sample>
    <sample id="107">Encoder-Decoder models obtain the best performance on all nine datasets, and Encoder-PTR can be improved by training in a mixture of various languages.</sample>
    <sample id="108">This talk presents a research paper exploring the limitations of current methods for evaluating language model acceptability judgments, particularly concerning their robustness when faced with longer contexts. The researchers, led by Koustav Sinha, argue that the standard Minimal Pair Paradigm (MPP) pipeline, which assesses models on short, paired sentences (e.g., grammatical vs. ungrammatical), doesn't adequately reflect how models handle acceptability across extended sequences, especially with the rise of large language models (LLMs) boasting expansive context windows.

The core of their work involves revisiting and expanding the MPP pipeline to evaluate acceptability judgments within longer sequences. They achieve this by reconstructing sentences from existing datasets like BLiMP and SyntaxGym, creating "matched" and "mismatched" contexts. "Matched" contexts use sentences from the same dataset and grammatical phenomenon, while "mismatched" contexts draw from different subsets or entirely unrelated domains like Wikipedia.

Their findings reveal a significant sensitivity of LLMs to the syntactic and semantic features shared across sentences. While judgments remained relatively stable when presented with irrelevant Wikipedia context, the model's acceptability scores dramatically shifted when presented with "matched" prefixes (acceptable or unacceptable sentences from the same grammatical phenomenon). This effect intensified with increasing context length, suggesting a potential issue for evaluating newer LLMs.

Further analysis, involving perturbing input sentences, indicated that models are consistently sensitive to these underlying features, regardless of minor noise introduced. The researchers conclude that current MPP evaluations, focused on short, single sentences, may not fully capture the abstract linguistic knowledge embedded within LLMs and that a more context-aware evaluation approach is needed. The paper advocates for a re-evaluation of how we assess language model understanding of acceptability, particularly as models continue to grow in size and context window capacity.</sample>
    <sample id="109">"Unnatural Instructions" introduces a novel approach to instruction tuning for language models, bypassing the need for human annotation. The core idea is to leverage large language models (LLMs) themselves to generate a vast dataset of instructions, inputs, and outputs entirely automatically.

The process begins with a small "seed" of manually created examples from the Super-Natural Instructions dataset. A GPT-3 variant is then prompted to generate new instruction-input pairs, followed by prompting the model to generate corresponding outputs for those instructions. To further enhance diversity, the system automatically generates paraphrases of each instruction. This results in a dataset of 64,000 examples, expanding to 240,000 with paraphrases.

The generated "Unnatural Instructions" dataset demonstrates surprising creativity and diversity, encompassing tasks beyond typical NLP benchmarks, such as evaluating scientific experiment design and inventing new words. While correctness isn't perfect (over 50% are correct), even incorrect examples offer valuable training data.

Crucially, the utility of this automatically generated dataset is demonstrated by fine-tuning an 11 billion-parameter T5 model on it. This model significantly outperforms existing instruction-tuned models (T0++ and Tk-instruct) across several benchmarks (Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry).  Furthermore, when considering the cost of generation, training on "Unnatural Instructions" proves more efficient than training on the manually annotated Super-Natural Instructions.

The work highlights the potential of LLMs to autonomously create high-quality training data, offering a faster, cheaper, and potentially more diverse alternative to human annotation, which can often lead to predictable patterns and annotation artifacts.</sample>
    <sample id="111">The provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">大家好，我叫舒恒。今天我将为大家介绍我们的论文《CoNLL-2003命名实体标注器在2023年是否仍然有效？》。

我们的论文研究了命名实体识别任务（NER任务）中的泛化问题。我们观察到，模型已经使用了近20年时间来基于CoNLL-2003开发NER，这自然会引发几个问题。首先，这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？同时，如果观察到泛化能力不佳，是什么导致这些模型的性能下降？

为了调查这些问题，我们开发了CoNLL++数据集。这是一个我们从路透社新闻中收集的数据集，并使用相同的CoNLL-2003标注指南进行标注。然后，我们在CoNLL-2003上微调了20多个模型，并在CoNLL-03测试集和CoNLL++上评估了它们。最后，我们计算了F1值的百分比变化，以评估每个模型的泛化能力。

那么，实现良好泛化需要什么？通过实验，我们发现有三个主要要素。第一是模型架构。我们的实验发现，Transformer模型通常能更好地泛化到新数据。第二是模型大小。我们发现，通常情况下，更大的模型能带来更好的泛化能力。最后，我们都知道，微调示例的数量直接影响下游任务的性能。在这里，我们同样发现，更多的微调示例也能带来更好的泛化能力。

对于我们下一个问题，是什么导致一些模型的性能下降？我们提出了两个假设。第一个是自适应过拟合，即通过重复使用相同的测试集而产生的过拟合成本，这通常表现在新测试集上的边际效应递减。第二个假设是时间漂移，即由于训练数据和测试数据之间的时间差距不断扩大而导致性能下降。

对于数据过拟合，我们从右侧的图表上看到，最佳拟合线（红色）的梯度大于1。这意味着我们在CoNLL-2003上取得的每一点改进，在CoNLL++上都能带来超过一点的改进，这意味着没有边际效应递减。这表明，在这种情况下没有观察到自适应过拟合。

那么，时间漂移呢？对于时间漂移，我们进行了一个实验，使用更近期的数据重新训练或继续预训练了一些模型，我们发现性能随着时间差距的扩大而下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。

我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些要素是相互关联的，我们不能仅仅拥有其中一个要素而放弃其他要素。同时，我们还发现，这里的性能下降是由时间漂移引起的，而且出人意料的是，它并非由自适应过拟合引起，尽管CoNLL-2003已经使用了20多年。

回到我们论文标题中提出的问题：CoNLL-2003标注器在2023年是否仍然有效？我们发现答案是绝对肯定的。我们希望我们的论文能促使人们对如何改进模型的泛化能力进行更多的研究。最后，请务必查阅我们的论文和数据集，如果您有任何问题，请随时与我联系。谢谢大家。</sample>
    <sample id="114">This work from Nanyang Technological University of Singapore, presented at ACL 2023, addresses the heavy parameter problem in large language models (LLMs) by optimizing multi-head attention. Current LLMs, while revolutionary, suffer from massive size, long training times, and huge data requirements.

The research focuses on the redundancy within multi-head attention, where some heads can be pruned without performance loss. Existing approaches (homogenization, diversification, and scoring) have limitations – sacrificing performance, lacking parameter efficiency, or leaving significant redundancy.

The proposed solution, Grouped Head Attention (GHT), employs a "divide and conquer" strategy. It involves two stages:

1.  **Group-Constrained Training:** Divides attention heads into groups, encouraging similarity within groups and separation between them using a novel loss function combining homogenization and diversification terms.
2.  **Voting-to-Stay Algorithm:** Prunes redundant heads, retaining only one representative head per group based on a voting mechanism across the training dataset.

Experiments on machine translation, language modeling, and abstractive summarization demonstrate significant improvements over state-of-the-art baselines (up to 4.4% BLEU improvement in machine translation) alongside substantial parameter compression (up to 90%). A "LITE" model achieved 62% faster inference speed and 80% reduction in FLOPs while maintaining comparable performance.

Future work will explore task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to further reduce redundancy and improve efficiency, mirroring the concept of uninstalling unused apps to optimize device performance. The authors believe LLMs are often over-parameterized for specific real-world applications, and targeted pruning can unlock significant benefits without sacrificing accuracy.</sample>
    <sample id="115">lambda speech frames</sample>
    <sample id="116">"Servin is a judge." and "Kea is a Baker."</sample>
    <sample id="117">示例质量比与源句子的相似度更重要。</sample>
    <sample id="118">This ACL 2023 submission introduces "SwitchMLM," a novel pretraining technique designed to improve performance on code-switched NLP tasks. Code-switching, the mixing of languages within a single sentence (e.g., English and Hindi), is common in linguistically diverse communities, but existing multilingual models like mBERT and XLM-R struggle with it.

The core idea of SwitchMLM is to focus the masked language modeling (MLM) objective on "switch-points"—transitions between languages within a sentence. Unlike standard MLM where all words are masked, SwitchMLM only masks tokens at these switch-points. To overcome the need for language identification (LID) tags, the authors propose FrequencyMLM, a surrogate method using monolingual corpora to estimate language probabilities.

Beyond the MLM objective, the work also incorporates architectural modifications. Recognizing that intermediate layers of BERT encode valuable switch-point information, they introduce residual connections from these layers to the final layer, alongside an auxiliary LID-based loss to further encourage language encoding.

Experiments on sentiment analysis demonstrate that the combined SwitchMLM/FrequencyMLM with ResBERT architecture outperforms existing methods across various language pairs. Probing experiments, using both linear and conditional probing, confirm that the proposed methods effectively increase the representation of switch-point information in both intermediate and final layers, validating the design choices and highlighting the benefits of focusing on these crucial linguistic transitions. The research ultimately aims to create more robust and effective models for code-switched NLP.</sample>
    <sample id="119">RoBERTa, GPT-4, and GPT series.</sample>
    <sample id="120">The cross-attention mechanism is leveraged, and the sum of the cross-attention weights is considered.</sample>
    <sample id="121">"Easy on Me" or "I Gotta Feeling", "the first one".</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">This research introduces MultiInstruct, a novel benchmark dataset designed to advance multi-modal zero-shot learning through instruction tuning. The core problem addressed is the lack of large-scale, publicly available multi-modal instruction datasets, which hinders the application of instruction tuning techniques—proven effective in language-only tasks—to computer vision and multi-modal scenarios.

MultiInstruct comprises 62 diverse multi-modal tasks spanning 10 categories, derived from 21 existing datasets. Each task is enriched with five expert-written instructions, aiming to improve model generalization. The research utilizes OFA, a unified multi-modal pre-trained model, as a base and formulates all tasks into a unified sequence-to-sequence format.

The study investigates multi-modal instruction tuning on MultiInstruct, employing a training split of 53 tasks and a testing split including a common sense reasoning group and additional tasks from VQ and Miscellaneous categories. A subset of natural instruction tasks is also used as an unseen NLP task.

Key findings include:

*   **Significant Performance Improvement:** Instruction tuning substantially enhances OFA's performance on seen multi-modal tasks.
*   **Benefit of Transfer Learning:** Transfer learning from natural instruction datasets positively impacts instruction tuning, improving both performance and sensitivity.
*   **Impact of Instruction Quantity:** Utilizing multiple instructions (5 vs. 1) improves overall performance and reduces sensitivity.
*   **Sensitivity Metric:** A new "sensitivity" metric is introduced to measure a model's consistency in producing outputs regardless of instruction wording variations.

The research demonstrates the effectiveness of instruction tuning for multi-modal tasks and highlights the importance of diverse instruction sets. The authors are also expanding MultiInstruct to include approximately 150 additional vision-language tasks, which will be publicly released.</sample>
    <sample id="124">This presentation introduces "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models," a study by Tan Qingyu from the National University of Singapore and Alibaba. The core focus is on comprehensively evaluating and enhancing how LLMs handle temporal reasoning – understanding and reasoning about time.

The researchers break down temporal reasoning into three levels: time-to-time (e.g., "What year is after 2010?"), time-to-event (e.g., "What team did Lionel Messi play for in 2010?"), and event-to-event (e.g., "What team did Lionel Messi play for after FC Barcelona?"). They observed that previous research often overemphasized the time-to-event level.

To address this, they created the TempReason dataset, covering all three levels and a broad temporal range. They then evaluated LLMs (T5-L, FLAN-T5-L, and ChatGPT) in three question-answering settings: Closed Book, Open Book (with Wikipedia context), and Reasoning QA (providing relevant temporal knowledge).

Findings revealed biases in existing LLMs, particularly a preference for the 2000-2020 timeframe. ChatGPT, while initially promising, showed significant performance drops in month prediction and struggled with more complex temporal reasoning.

To improve temporal reasoning, the researchers proposed a training strategy: Temporal span extraction pre-training and time-sensitive reinforcement learning. This resulted in the TempT5 model, which outperformed other models, including ChatGPT and fine-tuned versions of T5, especially in Open Book and Reasoning QA settings. However, even TempT5 exhibited some performance fluctuations across different time periods, suggesting potential biases related to training data imbalance. Future work aims to mitigate these biases and further refine temporal reasoning capabilities in LLMs.</sample>
    <sample id="125">The text does not mention the number of authors.</sample>
    <sample id="126">Yes, Google Translate API is used to translate source queries to the target language in the Translate-Test setting.</sample>
    <sample id="127">This work, "Large Language Models Are Reasoning Teachers," addresses the limitation of chain-of-thought (CoT) reasoning, which typically requires extremely large language models (LLMs) like GPT-3. These large models are costly to deploy, hindering broader application.

The proposed solution leverages large LLMs as "reasoning teachers" to transfer their reasoning abilities to significantly smaller models. The core method involves using large models to generate step-by-step solutions for complex tasks, then using these solutions as training data to fine-tune smaller models.

A key innovation is "Diverse Reasoning," which generates multiple, slightly different reasoning paths from the teacher model using stochastic temperature sampling. This provides richer training data, leading to improved student performance.

Experiments across 12 tasks demonstrate that fine-tuned CoT models, even with as few as 0.3 billion parameters, can achieve notable performance on complex reasoning tasks, outperforming prompt-based baselines and vanilla fine-tuning. Diverse Reasoning significantly boosts performance, particularly on tasks like Multi Arithmatic.

The research highlights the scalability of the approach, with performance improvements possible through larger datasets, better teacher models, or bigger student models. However, it acknowledges the trade-offs between development costs (teacher model, dataset size, Diverse Reasoning) and inference costs (student model size).

The paper provides extensive details, code, and data, including OpenAI inference costs, encouraging further research and exploration of this distillation technique for transferring emergent abilities to smaller, more accessible models.</sample>
    <sample id="128">The KITMUS test is a newly developed diagnostic tool designed to evaluate how well natural language understanding (NLU) models integrate knowledge from various sources. These sources include knowledge embedded within the model's parameters during pre-training and knowledge provided as input during inference. The core challenge addressed is that NLU often requires combining both types of knowledge, as pretrained knowledge alone is insufficient for instance-specific information.

KITMUS utilizes a coreference resolution task to probe this ability. The task involves identifying the correct entity a pronoun refers to, requiring both entity-specific facts (e.g., "Servin is a judge") and background knowledge (e.g., "Judges decide cases in law courts"). The test suite defines three settings: "Background-Pretrain" (background knowledge available at pretrain time), "Background-Both" (background knowledge available at both pretrain and inference time), and "Background-Inference" (background knowledge available only at inference time). The "Background-Inference" setting is particularly valuable as it simulates scenarios where necessary background knowledge wasn't present during pretraining, such as new occupations.

The researchers evaluated KITMUS using human participants and existing coreference resolution models (C2F and BERT4Coref). Results showed that models initially perform poorly without task-specific training, relying on superficial cues. Training on KITMUS significantly improved performance, indicating the potential for models to learn knowledge integration. However, even the best-performing models struggled to reliably integrate background knowledge provided solely at inference time, particularly when dealing with fictional or novel concepts.

In essence, KITMUS highlights a current limitation in NLU models: their difficulty in reasoning across different knowledge sources without specialized training. While progress is being made, reliably integrating inference-time background knowledge remains a significant challenge. The paper and associated dataset and code are publicly available for further exploration.</sample>
    <sample id="129">Asian women, Middle-Eastern women, Latina women, and Black women.</sample>
    <sample id="130">Non-transformer models.</sample>
    <sample id="131">Clean test sets.</sample>
    <sample id="132">Two.</sample>
    <sample id="133">The authors used multi-modal data (language, image tokens, and bounding box coordinates).</sample>
    <sample id="135">ABC-Eval is a new, dimensional approach to evaluating conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. It aims to provide a more precise and reliable evaluation than traditional methods like Likert ratings and pairwise comparisons.

The core idea of ABC-Eval is to annotate model responses based on specific behaviors, rather than relying on subjective overall quality ratings. These behaviors include issues like providing irrelevant information, contradicting oneself or the partner, hallucinating facts, violating common sense, and failing to show empathy. By measuring the rates of these behaviors, ABC-Eval offers a fine-grained understanding of a model's strengths and weaknesses.

The researchers tested ABC-Eval on four state-of-the-art chat models, comparing it to Likert ratings (turn-level and dialogue-level) and pairwise comparisons. The results showed that ABC-Eval labels demonstrated higher inter-annotator agreement and were more predictive of overall conversation quality. Furthermore, a stepwise linear regression analysis revealed that ABC-Eval metrics captured unique aspects of chat quality, explaining over 25% of conversation quality compared to less than 4% explained by turn-level Likert metrics.

The study quantified several persistent challenges in conversational AI, such as common sense violations (around 20% of responses), irrelevant information (15%), and contradictions (10%). While these error rates may decrease with future model improvements, the researchers emphasize the need for reliable and precise evaluation metrics like ABC-Eval to effectively compare and track progress in the field. Ultimately, ABC-Eval offers a higher-resolution evaluation of conversational AI, enabling a more detailed understanding of model performance.</sample>
    <sample id="136">Jasivan presented "FERMAT: An Alternative to Accuracy for Numerical Reasoning," addressing the limitations of current benchmarks in evaluating numerical reasoning abilities of language models, particularly those with fewer than 10 billion parameters. Existing benchmarks like CommonCore and Illinois, while showing slightly better performance, aren't representative of real-world numerical reasoning needs. Accuracy and F1 scores alone don't provide sufficient insight into a model's mathematical strengths and weaknesses.

FERMAT is introduced as a flexible evaluation set based on arithmetic types, focusing on number understanding, mathematical operations, and training dependency. It utilizes math questions extracted from Illinois and CommonCore, manipulating number representations (integers, decimals) and varying mathematical operations (single vs. combined). Initial zero-shot evaluations revealed poor performance across all aspects, highlighting the need for improved evaluation methods.

Fine-tuning with 200,000 generated examples, using templates created by math teachers, significantly improved performance on both original benchmarks and FERMAT's specific aspects. Further analysis of training dependency showed that even when models encounter expressions seen during training, accuracy remains below 50%, suggesting a lack of memorization and highlighting the importance of linguistic nuances in question phrasing.

The study also investigated the impact of training templates, finding that increased language and mathematical diversity—achieved by incorporating templates from GSM8K and AQUA—led to the most promising performance improvements. The research concludes that current benchmarks are unrepresentative, single scores are insufficient, and FERMAT offers a more informative alternative. Finally, the study points to number encoding and tokenization as areas ripe for further improvement in language models' numerical reasoning capabilities.</sample>
    <sample id="137">This paper introduces "Tell2Design," a novel dataset and task focused on language-guided floor plan generation. Recognizing the need for design tools that can translate natural language instructions into functional layouts, the researchers address a gap in current generative AI, which primarily focuses on artistic image generation.

The core task involves generating 2D floor plans from sets of natural language instructions detailing semantics (room type), geometry (shape and dimensions), and topology (room relationships). The Tell2Design dataset comprises 5,051 human-annotated and approximately 76,000 artificially generated instructions, resulting in floor plans with an average of over 200 words and more than 10 sentences per plan.

The researchers highlight three key challenges: strict design constraints, understanding the overall floor plan from complex text, and dealing with ambiguous instructions. To tackle these, they propose a sequence-to-sequence model using a transformer-based encoder-decoder architecture, treating floor plan generation as a structured target sequence problem. This allows the model to handle varying numbers of rooms and effectively process lengthy instructions. The model leverages a pre-trained T5 language model for enhanced language understanding.

Experimental results on the T2D dataset demonstrate that their sequence-to-sequence model significantly outperforms existing text-conditional image generation baselines, achieving high Intersection over Union (IoU) scores. However, a performance drop is observed when testing on human-written instructions after training solely on artificial ones, indicating a language distribution gap. Interestingly, using artificial instructions for initial training ("warming up") improves performance when subsequently trained on human instructions, suggesting a beneficial interplay between the two data types. A case study illustrates that while image generation models produce realistic visuals, they struggle to accurately reflect the specified instructions.

Ultimately, the paper establishes a foundation for language-guided design generation and provides a valuable resource (Tell2Design) for future research in this area.</sample>
    <sample id="138">Reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="139">Ying</sample>
    <sample id="140">Yes, crowd-sourced workers were asked to find and revise incorrect samples in the validation and test set of CoScript.</sample>
    <sample id="141">Existing resources support limited types of context-dependent translations and limited sets of languages, as they usually rely on domain knowledge and human curation.</sample>
    <sample id="142">大家好！我将介绍我们关于“解决间接指代表达以进行实体选择”的工作，其中我们推出了AltEntities语料库。我的名字是Javad Hosseini，这是与Filip Radlinski、Silvia Pareti和Annie Louis共同完成的工作。我们的目标是理解用户在想要做出选择时的语言。考虑以下替代问题：“你是说‘Easy on Me’还是‘I Gotta Feeling’？”在这里，用户想要在其中一首歌曲之间进行选择。最明显的事情是使用直接引用，例如说歌曲的名字“Easy on Me”或它的位置，“第一个”。但有时间接引用更合适，以进行更自然的对话。这可能是因为用户记不起歌曲的名字。或者发音太相似，难以区分。或者当用户想要指定一个偏好时。以下是一些间接引用的例子，例如“较新的那个”或“不具活力的歌曲”。这是一个会话系统和基准大型语言模型实体理解中的一个重要问题。我们不知道是否存在一个用于此任务的更大规模的公共数据集，因此我们使用众包注释来收集一个。我们的数据集涵盖三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，采用卡通完成设置。卡通片有三个对话框。在第一个对话框中，Bob说：“你还记得我们昨天听的那首歌吗？” 这样，Bob就设置了对话背景。在第二个对话框中，Alice说：“你是说‘Easy on Me’还是‘I Gotta Feeling’？” 这就是替代问题。在第三个对话框中，Bob使用间接引用来选择其中一个实体，例如“较新的那个”。我们自动提供第一个和第二个对话框，但第三个对话框由注释者填写。第一个对话框来自每个领域中的几个手动提示。第二个对话框，即替代问题，是按照以下方式生成的。我们总是使用一个简单的模板。你是说A还是B？其中A和B是维基百科的样本。以下是我们在使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常更难进行区分。第一个是均匀随机采样。第二个是在实体具有相似标题时，例如两个名为“The Return”的书籍。第三是在它们在维基百科上有相似描述时。最后，当它们具有相似的信息框或属性时，例如一首歌曲的相同流派或相同的艺术家。当我们将这个替代问题展示给注释者时，他们知道这些实体的名称，但他们不一定知道关于这些实体的知识。所以我们所做的是展示关于这两个实体的背景知识。对于歌曲，我们只需向每个歌曲显示一个Google搜索链接，然后要求注释者至少听一下每首歌曲的一部分，并阅读关于每首歌曲的信息。以下是歌曲“Easy on Me”的Google搜索结果。对于食谱和书籍领域，我们显示一些来自维基百科的背景文本。对于食谱，我们还显示它们的图像，同样来自维基百科，以便注释者知道它们是什么样子的。然后，我们要求注释者选择其中一个实体，例如，第一个实体，并用三到五个间接引用来描述它们。例如，“带有钢琴音乐的那个”。以下是我们在数据集中的一些例子。例如，“没有歌词的那个”、“不是那个12岁男孩的那个”、“虚构的那个”或“来自亚美尼亚的那个”，等等。AltEntities语料库有6,000个替代问题，涵盖了三个领域，并且有42,000个间接引用表达。以下是使用T5 XL模型的实验结果总结。如果语言模型可以访问与注释者完全相同的背景知识，那么准确率非常高，大约在92%到95%之间。但这并不现实。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率仅为60%，因此还有很大的改进空间。我们还表明，模型具有领域泛化能力。以下是我们的数据集链接。谢谢。</sample>
    <sample id="143">Wait-k strategy, Local Agreement, and state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="144">The presentation does not explicitly state the authors' affiliated institutions.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng from Fudan University presented a paper analyzing omission in dialogue summarization. Despite recent advancements using large language models, generated summaries often contain factual errors, with omission being a significant contributor to poor quality. Their analysis revealed that approximately 70% of summaries across five domains and six models suffer from omission, and this information is randomly distributed throughout the dialogue.

To address this, they defined omission detection as identifying missing content from the original dialogue within a generated summary, focusing on utterance-level omissions. Recognizing the lack of relevant datasets, they created the OLDS dataset, built on five existing benchmarks, using diverse candidate summaries and an automatic method with human evaluation for omission labeling. The dataset is publicly available.

They then explored three baseline models (pairwise classification, sequence labeling, and pointer network) for omission detection, evaluating them using Precision, Recall, F1-score, and a word-level omission recall (WR) score. Results showed a challenging task with an F1-score around 50% due to label imbalance.

Finally, they investigated using detected omissions to refine summaries through a post-editing method. Concatenating the candidate summary with the omitted content significantly improved summary quality, demonstrating the value of omission detection and suggesting refinement based on detected omissions as a promising avenue for improving dialogue summarization.</sample>
    <sample id="147">Three. Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="148">大家好，我是来自特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮，我将简要介绍“注意力引导的同步语音翻译”论文，这是我和马特奥·内格里和马可·图尔奇的联合作品。

什么是同步语音翻译？同步语音翻译，或 SimulST，是将口语实时翻译成另一种语言文本的过程，从而实现跨语言交流。

而目前 SimulST 模型存在什么问题？通常会训练特定的架构，引入额外的模块进行优化。例如，涉及不同优化目标的长时间复杂训练过程。以及为了达到不同的延迟范围而训练和维护多个模型。例如，训练一个平均延迟一秒的模型，再训练一个平均延迟两秒的模型，以此类推。

那么，我们的解决方案是什么？首先，使用现有的离线语音翻译模型，无需重新训练或采用专门针对 SimulST 的架构。使用单一模型来处理每种延迟范围，并通过特定参数来控制延迟。并且利用模型通过音频输入和文本输出之间的注意力机制所获得的知识。这就是交叉注意力机制，您可以在右侧看到一个示例。

我们的解决方案是提出 EDAtt，或编码器-解码器注意力机制，这是一种策略，我们根据注意力指向的位置来决定是否发出部分翻译。如果注意力不集中，即其总和低于某个阈值 alpha，并且指向最后 lambda 个语音帧，这意味着接收到的信息已经足够稳定，则会发出一个词。例如，如果接收到包含“我将要谈论……”的语音片段，并且我们的模型预测为德语翻译，我们将查看交叉注意力权重，我们会发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即 lambda 个语音帧。这意味着前两个词将被发出，因为交叉注意力的总和高于某个阈值 alpha，我们将不会发出最后一个词，而是等待另一个语音片段。如果继续接收另一个语音片段，并且我们的模型预测另外三个词，我们将查看这些交叉注意力权重，我们会发现没有词指向最后 lambda 个语音帧。这意味着这三个词将被发出。

如果查看 EDAtt 的主要结果，我们将在图表中绘制同步语音翻译结果，其中 BLEU 值在一侧，用于衡量翻译质量，而平均滞后值是延迟度量，我们还考虑了计算感知平均滞后值，该值考虑了模型预测输出的计算时间。因此，我们希望该图表上的曲线尽可能高，但也要尽可能地向左偏移。我们将其与应用于离线模型的流行策略进行比较，这些策略是 Wait-k 策略和局部一致性。我们还与专门为同步预翻译定制的最先进架构进行比较。这些是所有同步语音翻译策略在德语上的结果。我们看到它优于应用于离线模型的各种策略，因为曲线向左偏移。并且，如果我们考虑实际经过的时间或计算感知时间，那么它是最快的策略。

如果您想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同步输出，以促进我们工作的可重复性。感谢您的关注。</sample>
    <sample id="149">Yes.</sample>
    <sample id="150">The ACL paper introduces MeetingQA, a new extractive question answering (QA) dataset built on meeting transcripts. Recognizing the untapped potential of meeting discussions for NLP research, the authors address the gap left by existing summarization and action item extraction approaches that neglect the crucial QA component.

MeetingQA comprises 7.7K questions derived from the AMI corpus, a collection of 100 hours of multi-party meeting transcripts. The dataset features unique characteristics: longer, open-ended questions designed to spark discussion, answers contributed by multiple speakers, discontinuous answer spans, and rhetorical questions. Annotators achieved a high inter-annotator agreement (Krippendorff's alpha of 0.73) in labeling answer spans. The dataset is challenging, with 30% of questions being unanswerable, 40% having multi-span answers, and 48% involving multiple speakers. A significant portion (70%) of multi-speaker answers contain disagreement.

The paper explores various QA approaches, including context retrieval for short-context models, single-span and multi-span models, and data augmentation using silver annotations from the MediaSum dataset. Results reveal a substantial performance gap between models and human baselines (over 25 F1 points in the fine-tuned setting and nearly 50 F1 points in the zero-shot setting). Interestingly, short-context models like RoBERTa slightly outperformed long-context models like Longformer, and single-span models generally performed comparably or slightly better than multi-span models. Silver data augmentation improved zero-shot performance, and larger instruction-tuned models like FLAN-T5 showed promising zero-shot results.

Error analysis highlighted challenges in identifying rhetorical questions and accurately pinpointing the relevant sentences within answers. Models also struggled to determine which speaker provided the answer, particularly in the zero-shot setting. The authors conclude that MeetingQA presents a valuable and challenging dataset for advancing QA research in the meeting domain, emphasizing the need for further development to overcome existing limitations.</sample>
    <sample id="151">大家好，我叫 Ying，我的同事 Zhiyang 和我将为大家介绍我们的研究，关于 MultiInstruct 如何通过指令调优来提升多模态零样本学习。

随着大型语言模型的进步，许多研究开始探索一种参数和数据效率高的学习范式，即利用预训练的语言模型来执行不同的下游任务。最近，许多研究表明，指令调优能够使大型语言模型通过遵循自然指令，在零样本方式下执行未见过的任务。然而，大多数之前的指令调优工作都集中于提升语言任务的零样本性能，而计算机视觉和多模态任务则被忽略了。因此，在这项工作中，我们想研究指令调优多模态预训练模型是否真的能够提升对未见过的多模态任务的泛化能力。

此外，在我们的研究过程中，我们发现 NLP 和多模态领域在指令数据集的可用性方面存在相当大的差异。NLP 领域存在 1600 多项语言任务的指令，但没有大规模的公开可用的多模态指令数据集。这促使我们构建了一个多模态指令调优数据集。

我们在此介绍 MultiInstruct，这是第一个多模态指令调优基准数据集，包含 62 种多样的多模态任务，涵盖 10 个大的类别。这些任务源自 21 个现有的开源数据集，并且每个任务都配备了五个专家编写的指令。

为了研究我们在提出的数据集上的多模态指令调优，我们以 OFA，一种统一的多模态预训练模型，作为我们的基础模型。OFA 使用统一的词汇表来处理语言、图像 token 和边界框的坐标。

这里展示了我们 MultiInstruct 数据集的一些示例，以统一各种输入和输出数据类型的处理方式。我们遵循 OFA 的方法，将所有任务都以统一的序列到序列格式进行表述。其中，文本输入、图像、指令和边界框都表示在相同的 token 空间中。

现在，我将介绍多模态指令调优。对于训练数据集，我们使用 9 个组中的 53 个任务进行训练，并且每个任务抽取 10,000 个样本。对于测试，我们保留了常识推理组用于测试，并从 VQ 和 Miscellaneous 组中额外选择了 5 个任务。我们使用每个任务的所有实例进行测试。此外，我们还从自然指令的测试集随机抽取 20 个任务作为 NLP 的未见过的任务。

我们使用预训练的 OFA 大模型作为基础模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例都会被随机地与它五个指令模板中的一个组合。在测试时，对于每个任务，我们进行总共 5 次实验，通过使用五个指令中的一个来评估模型。在每次实验中，我们报告所有 5 次实验的最小和最大性能，以及性能的标准差。如果任务是多模态分类任务，我们报告准确率。如果任务是多模态生成任务，我们报告 Rouge-L。对于 NLP 任务，我们同样报告 Rouge-L。

我们还引入了一个额外的评估指标，称为“敏感性”。这个指标衡量了模型在指令措辞略有变化的情况下，始终产生相同输出的能力。

这是我们的主要结果。正如我们所见，指令调优可以显著提升 OFA 在可见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以促进指令调优。

我们还可以看到，随着任务数量的增加，模型可以实现更好的性能，并且同时降低其敏感性。

我们还进行了一项实验，比较了使用一个指令与使用五个指令的效果。正如我们所见，使用更多的指令可以提高模型的整体性能，并显著降低其敏感性。这表明不同的微调策略对模型敏感性的影响。

我们可以看到，通过从自然指令数据集进行迁移学习，模型可以实现比原始 OFA 模型更高的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助 OFA 在自然指令数据集上获得更好的性能。

总而言之，我们提出了第一个大规模的多模态指令调优数据集，显著提升了 OFA 的零样本能力，并探索了不同的迁移学习技术，展示了它们的优势。我们设计了一个新的指标，称为“敏感性”。

最后，我们正在收集一个更大的多模态指令调优数据集，包含大约 150 个额外的视觉语言任务，并将发布它们。

这是我们数据集和模型的二维码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider presented work on developing new language models specifically for classical philology, focusing on Ancient Greek and Latin. The project aimed to address limitations of existing models, which were primarily monolingual BERT models with largely unexplored performance.

The core goals were to enable model comparison, advance the state-of-the-art, explore different architectures, and introduce multilingual models. The team created GreBERTa (RoBERTa) and GreTa (T5 encoder-decoder) for Ancient Greek, and PhilBERTa and PhilTa (multilingual) encompassing Ancient Greek, Latin, and English.

A key innovation was developing a new, high-quality Ancient Greek pre-training corpus by leveraging the Internet Archive. They identified incorrectly transcribed Greek stop words to locate Greek texts within the archive and then re-OCR'd them with Greek character support.

Benchmarking on part-of-speech tagging, dependency parsing, and lemmatization revealed significant performance improvements over existing models for both languages. Notably, the encoder-decoder architecture (GreTa) demonstrated exceptional lemmatization performance, exceeding the state-of-the-art by 5 percentage points for Ancient Greek. Analysis of GreTa's encoder revealed a different behavior compared to native encoder-only models, requiring more training to reach comparable performance.

The research also explored semantic and world knowledge capabilities, finding that the new models outperformed previous ones. Interestingly, the multilingual models did not show a significant performance advantage over the monolingual models in these areas.

In conclusion, the project delivered powerful new language models, a novel pre-training dataset, and valuable insights into model architectures and the impact of multilinguality for classical philology applications.</sample>
    <sample id="153">This presentation by Ninareh Mehrabi from Amazon Alexa AI's Responsible AI team focuses on addressing ambiguities in text-to-image generative models. The core problem is that ambiguous prompts can lead to images that don't accurately reflect the user's intended meaning.

The research introduces a pipeline to tackle this issue, starting with a benchmark dataset based on the LAVA corpus, designed to cover various ambiguity types. The pipeline then employs a prompt disambiguation framework, utilizing two approaches:

1.  **Clarifying Questions:** A language model generates questions to clarify the user's intention, and the user's answers are appended to the original prompt.
2.  **Visual Interpretations:** The language model generates multiple visual interpretations, and the user selects the one aligning with their intention.

Following disambiguation, the research evaluates the faithfulness of the generated images. This is achieved through an automatic evaluation framework using a Visual Question Answering (VQA) model. The VQA model assesses whether the generated image satisfies the user's stated intention, effectively determining if the image is faithful to the prompt.

Key findings from the study include:

*   Different ambiguity types are resolved with varying degrees of success.
*   The proposed disambiguation framework generally improves the faithfulness of image generation.
*   The automatic evaluation framework demonstrates strong agreement with human evaluations, suggesting its reliability for assessing text-to-image models.

Ultimately, the work aims to improve the accuracy and user satisfaction of text-to-image models by addressing prompt ambiguities and providing a robust evaluation method. The paper details further findings and discussions for those interested in a deeper dive.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">This presentation introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to dialogue summarization developed by a team from Shandong University. The core challenge addressed is distilling key information from complex, multi-participant dialogues into concise summaries.

Existing methods often rely on pre-computed static graph structures derived from external linguistic tools like discourse parsing and dialogue state tracking. However, these methods suffer from two key drawbacks: dependence on the accuracy of external tools (leading to error propagation) and a rigid, static graph structure that doesn't adapt to the summarization task.

SDDS overcomes these limitations by introducing a Static-Dynamic Graph module. The model comprises four main components: an Utterance Encoder, a Static Graph construction phase, the core Static-Dynamic Graph module, and a Summary Generator (a pre-trained language model).

The Static Graph construction utilizes four heuristic methods: Discourse Parsing Graph (based on discourse parsing), Key Co-occurrence (identifying utterances with shared keywords), Speaker Relationship Modeling (analyzing speaker interaction frequency), and Utterance Position Graph (using relative distances between utterances). These methods generate adjacency matrices, which are then fused using 1x1 convolutional layers.

The Dynamic Graph module, leveraging a multi-head attention model, dynamically captures semantic relationships between utterances based on their deep vector representations, without relying on pre-computed structures.  A fusion method combines the dynamic and static graph representations into a unified graph.

Finally, a dual cross-attention mechanism, incorporating a graph attention layer, integrates the structural information into the summary generation process. This allows the model to effectively leverage both static and dynamic dialogue structures for improved summarization. The code and data are publicly available on GitHub.</sample>
    <sample id="158">This presentation introduces "Dual Cache," a novel approach to improve neural coreference resolution, particularly for long documents. Coreference resolution aims to link mentions of the same entity within a text, a task traditionally hampered by quadratic computational complexity. Cache-based methods offer a linear complexity solution by storing entities in a fixed-size cache, but standard Least Recently Used (LRU) eviction policies struggle with long documents due to topic shifts and scattered entity mentions.

The core problem addressed is the high cache miss rate caused by LRU when encountering new mentions in long documents, especially for frequently occurring "global" entities. Dual Cache tackles this by employing two caches: a local cache using LRU for local entities and a global cache using Least Frequently Used (LFU) for global entities.

The model processes the document sequentially, classifying mentions as new or belonging to existing entities. High-frequency entities are added to the global cache, while others go to the local cache. When caches are full, the respective eviction policies are triggered.

Experiments on four benchmarks (LitBank, OntoNotes, WikiCoref) demonstrate Dual Cache's superior performance compared to baselines, even those with unbounded memory, especially on long documents like a 30,000-word book.  It significantly reduces cache misses and achieves the best performance/cost ratio among cache-based methods. In essence, Dual Cache optimizes coreference resolution in long documents by strategically separating and managing local and global entities within a dual caching system, leading to improved efficiency and accuracy.</sample>
    <sample id="159">大家好，我是Koustav Sinha，很高兴欢迎大家参加我们ACL 2023论文的讨论。语言模型对可接受性的判断并不总是对上下文稳健的。这是一项我和John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams共同完成的工作。在这个工作中，我们重新审视了最小对（Minimal Pair, MPP）范式。最小对范式基本上是用来评估语言模型在可接受性判断上的表现，这也可以包括语法性，比如BLiMP、SyntaxGym，或者在刻板印象方面的可接受性，比如CrowS对。在最小对范式中，评估语言模型的一种典型方式是展示一个可接受的句子或一个语法正确的句子，然后展示一个可接受的句子或一个语法错误的句子。希望模型能够将更高的概率赋予可接受的句子。当前的MPP流程基本上不允许我们评估模型对更长句子的可接受性。如今，大型语言模型正在生成越来越长的上下文窗口。因此，评估模型在整个上下文窗口中的可接受性至关重要，而我们正在努力做到这一点。我们试图通过要求模型在越来越长的序列上评估可接受性，从而重新审视MPP流程。这就是我们的方法。

我们所做的是，为了模拟这些更长的序列，我们重新审视数据集本身，然后通过选择这些数据集中可接受或不可接受的句子来重建句子。例如，这里我们选择了一个典型的来自BLiMP数据集的语法性对，来自附例岛（Adjunct Island）的情况。我们所做的是，为了重建更长的序列，并且这些序列是可接受的，并且具有相同的语法结构，我们从附例岛中提取语法正确的句子，然后将其作为前缀添加到可接受的查询和不可接受的查询中。我们也可以用同样的方式选择不可接受的句子，这也可以用来测试模型的接受度。我们还可以用同样的方式选择来自不同子集或不同数据集的句子。这就是我们所说的“不匹配”场景。在这种情况下，句子仍然来自相关的语料库，但不是用于评估的同一个语料库。我们也可以对不可接受性的情况做同样的事情。最后，我们可以选择来自完全无关领域（例如维基百科）的句子。这将告诉我们，模型的接受度判断是否真的受到任何上下文的影响，例如，上下文是否来自数据集的不同子集，或者它是否与我们正在查看的句子完全无关。

模型表现如何？首先，我们查看了来自维基百科的句子，这些句子与当前的查询对完全无关。在那里，我们发现MPP判断在很大程度上对任意上下文长度保持稳健。我们增加了上下文长度，最多达到1024，以最大化OPT和GPT 2模型的上下文窗口。我们在这里看到，在橙色虚线中，MPP判断相对稳定。

现在，当选择来自同一数据集的句子时会发生什么？在这里，我们选择或创建来自同一BLiMP或SyntaxGym数据集的句子。在那里，我们看到，当添加可接受的前缀或不可接受的前缀时，MPP判断会显著增加或减少。但是，当匹配结构时，也就是说，当我们从BLiMP或SyntaxGym中选择句子时，我们看到MPP判断会大幅增加或大幅减少，这取决于所选的前缀是否可接受。这种效应非常大，并且随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新型语言模型。

为什么匹配的前缀会如此影响语言模型的判断？我们进行了一系列分析，试图通过尝试保留相关结构并向输入添加噪声来扰动输入句子。经过多次扰动后，我们发现这些噪声中的任何一种都没有真正导致模型改变其MPP判断打印的方式。基本上，我们发现模型以相似的方式对扰动后的句子敏感。也就是说，当我们扰动可接受领域的句子时，我们会看到所有扰动中MPP判断的相似增加，而当我们扰动不可接受领域的句子时，我们会看到MPP判断在相似的方式中减少。

我们工作的关键要点是，语言模型对跨句子共享的潜在句法和语义特征敏感。我们目前以短句和单句输入的方式进行的MPP评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢大家的聆听。</sample>
    <sample id="160">An unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="163">MASSalign</sample>
    <sample id="164">Weak supervision is cheaper than manual labeling because it uses weak labeling sources like heuristic rules, knowledge bases, or low-quality crowdsourcing.</sample>
    <sample id="165">This paper introduces LiPoR, a novel unsupervised learning method for abductive commonsense reasoning. Traditional abductive reasoning aims to find plausible explanations that bridge the gap between a given context and an outcome. Existing approaches often rely on supervised learning, which suffers from the problem of noisy and subjective annotation of plausible explanations – a recent study showed disagreement among crowd workers in 60% of cases.

LiPoR addresses this limitation by formulating abductive reasoning as an unsupervised problem. It treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, effectively avoiding the need for labeled data on explanation plausibility. However, simply maximizing likelihood doesn't guarantee the selection of plausible explanations.

To overcome this, LiPoR incorporates a novel regularizer based on the principle of mutual exclusivity among explanations. The core idea is that explanations in abductive reasoning are typically mutually exclusive – if one explanation is true, others are likely false. The regularizer penalizes models that assign probability mass to more than a predefined number (M) of explanations, encouraging the selection of a smaller, more plausible subset.

The LiPoR objective function combines the likelihood maximization with this mutual exclusivity regularizer. The regularizer utilizes entropy of the probability distribution over explanations to enforce this exclusivity.

Experiments on the AlphaNLI dataset, a standard benchmark for abductive reasoning, demonstrate that LiPoR significantly outperforms existing zero-shot models and previous unsupervised approaches, achieving an accuracy improvement of over 4 absolute points compared to a strong GPT-3 baseline. This highlights the effectiveness of LiPoR's unsupervised learning framework and its ability to leverage the inherent structure of abductive reasoning – specifically, the mutual exclusivity of explanations – to achieve state-of-the-art performance. The paper and code are available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">This work introduces "NDCR," a novel Neural Divide-and-Conquer Reasoning framework designed to address the challenges of image retrieval from linguistically complex text. Existing visual language models struggle with this task due to the complexity of the descriptions and the similarity of images. NDCR draws inspiration from the Divide-and-Conquer strategy and Dual-Process Theory in human cognition.

The framework mimics human thinking by integrating two systems: System 1 (analogical reasoning) and System 2 (logical reasoning). System 1, embodied by the Visual-Linguistic Interactor, performs initial visual-proposition interaction, generating matching scores and reasoning states. System 2, the Neural-Symbolic Reasoner, then integrates these states and results to solve the complex proposition. This system includes a negation executor and a conjunction operation to handle both positive and negative reasoning.

The process begins with a Proposition Generator, which decomposes the complex text into simpler propositions, leveraging BART's decoder for explanation. The final solution is obtained by combining the outputs of both System 1 and System 2. Experimental results demonstrate that NDCR outperforms existing baselines, and ablation studies confirm the effectiveness of each module. Case studies showcase the framework's ability to present intermediate inference states and results, highlighting its interoperable processing.

The authors suggest that neural-symbolic calculation, combined with Divide-and-Conquer and Dual-Process Theory, offers a promising avenue for enhancing compositional reasoning and planning in large language models. The Divide-and-Conquer approach shares similarities with chain-of-thought prompting, effectively breaking down complex problems into manageable steps. Ultimately, NDCR represents a significant step towards more robust and interpretable image retrieval from complex textual descriptions.</sample>
    <sample id="167">DEPLAIN-web includes 750 documents, aligned both manually and with automatic alignment methods, resulting in 30,450 sentence pairs.</sample>
    <sample id="168">It was collected from Reuters News from 2020 and annotated with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">This paper presents a systematic study of using large language models (LLMs), specifically PaLM (a 540 billion-parameter model), for machine translation. It's the first comprehensive evaluation of LLM prompting strategies within the machine translation community's best practices, utilizing current test sets and comparing performance against state-of-the-art neural machine translation (NMT) systems and human evaluations.

The research highlights the significant impact of prompting on translation quality. Even slight variations in prompts can lead to substantial BLEURT score differences (up to 40 points). The study found that a 5-shot prompting strategy, where each sentence is marked with its language (e.g., "German: [sentence] English: [translation]"), proved effective. Crucially, the quality of the examples provided in the prompt is more important than their similarity to the source sentence. Using high-quality translations from development data (dev data), which is more curated than training data, yielded better results.

While PaLM demonstrates impressive fluency comparable to state-of-the-art systems, it lags in accuracy. The most frequent errors involve omissions – PaLM sometimes drops parts of the source sentence to produce a more fluent-sounding translation. However, PaLM's output is generally less stylistically awkward than that of current state-of-the-art systems.

Overall, the paper concludes that PaLM's translation capabilities are approaching those of commercial systems like Google Translate, offering valuable insights into prompt selection strategies for LLM-based machine translation. Further details can be found in the full presentation.</sample>
    <sample id="170">大家好，我叫张宇森，来自宾夕法尼亚州立大学。今天我将介绍我们的工作“XSemPLR：跨语言语义解析在多种自然语言和含义表示中的应用”。语义解析的任务是构建用户查询的语义表示，例如SQL和Lambda演算。跨语言语义解析的任务是将多种自然语言的查询翻译成多种含义表示。如图所示，我们需要使用神经网络模型将多种自然语言的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是单独提出的，并在有限的任务和应用的数据集上进行评估。例如，对某些自然语言的覆盖率很高，但中文缺失，并且缺乏对某些含义表示的覆盖。Lambda演算缺失，或者它们只在某些神经网络模型上进行评估。例如，只有单个模型来评估它们。为此，我们提出了XSemPLR。我们提供了一个统一的数据集XSemPLR，用于在多种自然语言和含义表示中进行跨语言语义解析。它包含9个数据集，涵盖各种领域，5个语义解析任务，8个含义表示，以及22种自然语言，属于15个语系。为了更好地评估我们的基准，我们考虑了六种训练和评估设置。第一种是翻译-测试。我们使用谷歌翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英文数据上训练英文模型，在推理时将德语查询使用API翻译成英文，然后使用训练好的模型进行预测。我们还将测试单语模型。在这种设置中，源语言和目标语言相同，例如德语到德语或英语到英语。我们还测试单语少样本设置，仅使用10%的训练数据训练单语模型。我们还测试多语言模型，我们为所有语言训练一个多语言模型。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理时，我们可以使用该模型翻译德语查询或中文查询等。我们还考虑了跨语言零样本和少样本迁移。我们在一种源语言上训练，然后迁移到另一种语言。因此，在训练时，我们使用英语查询或英语和德语少样本查询的组合来训练一个多语言模型来预测SQL输出。我们还发现了很多有趣的发现。关于单语模型的分析，我们评估了包括编码器-指针解码器（Encoder-PTR）在内的两组模型，例如XLM-R + PTR和mBERT + PTR。我们还评估了编码器-解码器模型，例如mBART和mT5。我们发现编码器-解码器在所有9个数据集上都获得了最佳性能。我们在多语言设置下评估了mT5和XLM-R + PTR。我们发现编码器-解码器或编码器-PTR可以通过在各种语言的混合中进行训练来得到改进。我们发现这是因为大多数主要自然语言都可以获得性能提升，除了英语在七个数据集中的性能下降，只有在三个数据集中的性能提升。我认为这被称为“多语言诅咒”。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现零样本设置中的跨语言迁移性能差距很大，然后比较蓝线和橙线，我们发现少样本设置下迁移差距迅速缩短。我们还发现了一些其他的有趣的发现。例如，编码器-解码器优于先前的工作，或取得了可比的结果。在英语自然语言上进行预训练可以显著提高目标自然语言的少样本性能，并且我们发现像Codex和BLOOM这样的多语言语言模型仍然不足以用于跨语言语义解析任务。总而言之，我们构建了XSemPLR，这是一个统一的基准，用于跨语言语义解析，涵盖多种自然语言和含义表示。我们对三种具有代表性的多语言语言模型进行了全面的基准研究。我们的结果表明了很多有趣的发现。欢迎访问我们的论文和代码。感谢聆听。</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No.</sample>
    <sample id="174">Thea, a co-author of "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis," presents the dataset's unique features compared to existing argument analysis datasets. The core of the dataset is its focus on argument quality assessment on a scale of 0 to 1, reflecting the persuasiveness and coherence of an argument.

ArgAnalysis35K addresses several shortcomings of current datasets: lack of quality, limited diversity, shallow explanations, and rigid motion association. It distinguishes itself through:

*   **Scale and Quality:** It boasts 35,000 argument-analysis pairs, significantly larger than existing datasets, with a majority (85%) sourced from high-quality debate tournaments and expert debaters.
*   **Diversity:** Instead of focusing on a limited set of motions, it utilizes 24 themes derived from debate experience and expert input, capturing a wider range of motions within each theme.
*   **Analysis Component:** It introduces the concept of "analysis," which goes beyond simple claims or premises. Analysis represents a coherent explanation of an argument, potentially combining claims, premises, and reasoning to provide a deeper understanding.
*   **Instance-Based Annotator Reliability:** Recognizing annotator biases, the dataset employs instance-based reliability, filtering out biased judgments on a per-argument basis rather than dismissing entire annotators.
*   **Relevance Model:** Unlike datasets that rigidly link arguments to single motions, ArgAnalysis35K incorporates a relevance model. This assigns scores (0-1) indicating how relevant an argument is to each theme, acknowledging that arguments can be applicable across various topics.

Ultimately, ArgAnalysis35K aims to provide a more diverse, reliable, and comprehensive resource for argument quality analysis, offering higher-quality arguments, relevance scores, and more robust scoring due to instance-based reliability. Thea encourages viewers to explore the full paper and provide feedback.</sample>
    <sample id="175">It addresses the challenge of multiple consistent permutations by inducing the alignment as part of the training and uses a GPU-friendly continuous relaxation to approximate finding the highest-scoring permutation, allowing for backpropagation and learning of more plausible permutations.</sample>
    <sample id="176">Downstream NLP models exhibit fairness issues when their performance varies significantly based on the political leaning of the news media or the demographic/political categories of the content they are evaluating (e.g., hate speech or fake news). Left-leaning models may perform better on detecting hate speech against minority groups but worse on detecting it against powerful groups, and vice versa for right-leaning models.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presented "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," focusing on improving Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, the ability to understand others' mental states, is traditionally assessed using false-belief questions, like the Sally-Anne test, which probes understanding of situations where beliefs don't align with reality. Despite progress in LLMs like ChatGPT and GPT-3, they still struggle with these tasks.

The core of the presentation introduced SymbolicToM, a novel inference-time method designed to enhance ToM reasoning. SymbolicToM utilizes explicit graphical representations of mental states—multiple graphs representing different characters' beliefs about each other (e.g., Bob's belief and Bob's belief about Alice's belief). These graphs are computed using readily available Natural Language Inference (NLI) and OpenIE models.

Once these graphs are generated for a story, answering questions becomes efficient. The system identifies entities, retrieves the relevant belief graph, reformulates the question as a factual query on the graph, and then uses a language model to generate the final answer.

Experiments demonstrated significant performance gains across various LLMs (GPT-3, Macaw, Flan-T5-XXL) when using SymbolicToM, with accuracy improvements ranging from 51 to 67 points. Crucially, the research also tested generalization capabilities. Two new datasets (D₁ and D₂) were created to assess story structure generalization (combining stories) and a third (ParaphrasedToMi) tested linguistic generalization. While supervised models (fine-tuned GPT-3) suffered performance drops on these datasets, SymbolicToM consistently showed improvements, enabling even powerful models like GPT-4 to achieve near-perfect scores on D₁.

In conclusion, SymbolicToM offers a plug-and-play, inference-time solution that avoids overfitting, provides interpretable reasoning, and significantly boosts ToM performance in LLMs, particularly in out-of-domain scenarios and with increased linguistic diversity.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">This work introduces "CoScript," a novel dataset and methodology for constrained language planning, addressing a gap in existing research that primarily focuses on abstract goal planning with large language models (LLMs). The core problem tackled is planning actions based on step-by-step instructions, but with specific constraints (e.g., "make a chocolate cake" instead of just "make a cake").

The researchers found that LLMs like InstructGPT struggle with this constrained planning, demonstrating acceptable semantic completeness but lacking faithfulness to the specified constraints. To understand this, they analyzed performance across different constraint categories, revealing significant variance.

To improve LLM performance, they developed an "over-generate-then-filter" approach. InstructGPT generates multiple scripts for a given constrained goal, and a filter model, using embeddings and keyword matching, selects the most faithful script. This significantly enhances both semantic completeness and constraint adherence.

Recognizing the cost of deploying large LLMs, the team explored knowledge distillation. They leveraged the improved LLM planning capabilities to automatically generate a large dataset (CoScript) of 55,000 specific goals and scripts. Human annotators then validated and refined the dataset to ensure quality.

Crucially, they demonstrated that a smaller, specialized model (T5) fine-tuned on CoScript can outperform larger LLMs in constrained language planning, highlighting the power of targeted datasets. CoScript exhibits a diverse range of constraints, making it a valuable resource for advancing research in language planning and enabling the development of more efficient and specialized planning models. The paper concludes by emphasizing CoScript's potential to facilitate further progress in this area.</sample>
    <sample id="182">In this paper, tropicalism, as reflected in the generated personas, is exemplified by words like "vibrant" and "curvaceous" when describing Latina women, connecting to a trope that portrays them in a tropicalized manner.</sample>
    <sample id="183">The authors create persona descriptions by prompting the language model to generate a depiction of an imagined individual using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="184">CXMI (Contextual Mutual Information) and its extension, Pointwise CXMI (P-CXMI).</sample>
    <sample id="185">DrBERT is based on data crawled from the web (NACHOS), while ChuBERT is based on anonymized data from the Nantes University Hospital data warehouse.</sample>
    <sample id="187">Two. Ying and Zhiyang.</sample>
    <sample id="188">Iterative update updates the model by training on the latest set of data collected.</sample>
    <sample id="189">数据集的目标是理解用户在想做选择时使用的语言。</sample>
    <sample id="190">Attackers may steal the model by learning from the embedding and providing similar services.</sample>
    <sample id="191">Three.</sample>
    <sample id="192">This presentation introduces CAME (Confidence-guided Adaptive Memory Efficient Optimization), a novel optimizer designed to address the memory and performance challenges in training large language models (LLMs). Current adaptive optimizers like Adam consume significant memory (tripling requirements), while memory-efficient alternatives like Adafactor often suffer from slower convergence. CAME aims to bridge this gap by achieving both fast convergence and low memory usage.

The core idea behind CAME is inspired by the inherent errors in Adafactor's Non-negative Matrix Factorization (NMF) approach. Adafactor's NMF, while reducing memory from O(mn) to O(m+n), introduces inaccuracies that slow down training. CAME tackles this by analyzing the discrepancy between predicted updates and actual updates (the "instability").

Specifically, CAME calculates an "instability matrix" based on the residual between momentum and current updates. This instability matrix is then used as a denominator when updating the momentum, effectively adaptively adjusting the step size based on the confidence in the update. This confidence-guided approach mitigates the errors inherent in Adafactor's NMF.

Experimental results on BERT, GPT-2, and T5 models demonstrate CAME's effectiveness. It achieves a 3.4% improvement in validation accuracy compared to Adafactor and outperforms Adam, especially with large batch sizes (8K to 32K). Furthermore, BERT models trained with CAME exhibit comparable performance to baselines on downstream tasks while using less memory. Memory usage comparisons show CAME significantly reduces memory footprint compared to Adam, LAMB, and even other memory-efficient optimizers like SM3.

In essence, CAME offers a compelling solution for training LLMs by leveraging the benefits of memory-efficient optimization while maintaining competitive performance through a novel confidence-guided updating mechanism. Its ability to handle large batch sizes further enhances its practicality for training very large models.</sample>
    <sample id="193">Around 1,000.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces RoHT, a novel framework called "Reasoning over Hierarchical Question Decomposition Tree" for explainable question answering (XQA). RoHT addresses limitations in existing XQA approaches, namely neuro-symbolic methods (reliant on incomplete knowledge bases) and decompose-based methods (struggling with the diversity of natural language). The core idea is to integrate knowledge from heterogeneous sources—knowledge bases (KBs) and text corpora—through question decomposition.

RoHT operates in two stages. First, it constructs a Hierarchical Question Decomposition Tree (HQDT) representing the compositional structure of a complex question. The HQDT breaks down the question into sub-questions, with leaf nodes representing atomic questions. A "question decomposer" generates leaf questions, and a "question generator" creates intermediate questions based on grouped leaf questions. A "certainty score" is assigned to each node to reflect the confidence in its generation.

Second, RoHT performs probabilistic reasoning over the HQDT. This involves a recursive process from root to leaves, where a "scheduler" selects appropriate knowledge sources (KB, text corpus, or recursive solving of children), "executors" retrieve answers with probabilities, and an "aggregator" combines candidate answers to produce top results.

The framework was evaluated on two challenging datasets: KQA Pro (an incomplete KB QA dataset with Wikipedia as a supplementary text corpus) and Musique (a QA comprehension dataset with Wikidata as a supplementary KB). Results demonstrate RoHT's superiority over existing methods. On KQA Pro, RoHT outperforms KB QA methods and TransferNet, highlighting the benefits of integrating sub-question answers and explicit decomposition. On Musique, RoHT improves upon state-of-the-art methods and surpasses TransferNet, showing the value of combining text and KB knowledge. The study also reveals that supplementing text information with KB knowledge can further enhance performance.</sample>
    <sample id="196">"I saw Bart and Lisa" and "Homer came and sneezed."</sample>
    <sample id="197">Four state-of-the-art chat models.</sample>
    <sample id="198">因为大型语言模型现在具有更长的上下文窗口，评估模型在整个上下文窗口中的可接受性至关重要。</sample>
    <sample id="199">Yes, English performance drops in seven datasets and only gains in three datasets, which is known as the "Curse of Multilinguality."</sample>
    <sample id="200">No, they don't necessarily know about the entities.</sample>
    <sample id="201">State-of-the-art, neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">No, the paper does not mention whether generalization regression affects specific NER types.</sample>
    <sample id="203">NLP中的立场很重要，因为数据集和模型会聚合真实人们的判断和意见，从而代表某些立场而非其他立场。随着NLP任务变得更加主观和面向社会，了解和解决这些偏差变得至关重要，因为它们可能导致技术对不同人群的性能差异。</sample>
    <sample id="204">完整微调。</sample>
    <sample id="205">This presentation details a study investigating the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. The research highlights a critical dilemma: language models, trained on vast web-crawled datasets including politically-charged news sources, inevitably absorb and reflect societal biases.

The study first developed a method using political questionnaires (Political Compass Test) to automatically evaluate the political leaning of language models. Results revealed that models exhibit varying political leanings, with GPT-4 demonstrating the most liberal bias compared to BART series. Further experiments involving fine-tuning models on partisan corpora (news and social media) demonstrated a direct correlation between training data and model political leaning – models shifted ideologically based on the data they were trained on. The research also found that models trained on data post-2017 exhibited a greater polarization compared to those trained on earlier data, reflecting societal shifts.

Crucially, the study evaluated the performance of these politically-biased models on hate speech and fake news detection. The findings revealed significant fairness issues: left-leaning models were better at detecting hate speech targeting minority groups but worse at detecting it against dominant groups, while right-leaning models showed the opposite trend. Similarly, models were more effective at detecting misinformation from opposing political viewpoints.

The presentation concludes by emphasizing the "Scylla and Charybdis" dilemma – the challenge of mitigating bias without risking censorship or defining a universally accepted notion of neutrality. The research serves as a warning, highlighting the potential for biased language models to marginalize certain groups and exacerbate societal inequalities, urging for careful consideration and proactive solutions to address these fairness concerns in NLP applications.</sample>
    <sample id="206">Topic independent dissonance stance classification (debate) and binary classification of expansion and comparison classes of PDTB (CE).</sample>
    <sample id="207">The latest test sets were used to avoid overlap with the language model's training data.</sample>
    <sample id="208">Three.</sample>
    <sample id="209">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">是的，论文中的结果和数据集可以用作基准。</sample>
    <sample id="212">One.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">Adam Przepiórkowski's talk focuses on the dependency structure of coordination, arguing for symmetric structures over asymmetric ones. Different linguistic theories adopt varying approaches: Universal Dependencies and Mel'čuk's Meaning-Text Theory head the structure with the first conjunct, while Prague Dependency Treebanks head with the conjunction, and Hudson's Word Grammar uses a multi-headed approach.

Przepiórkowski's argument rests on the principle of dependency length minimization – shorter dependencies are preferred. He illustrates this with examples where a direct object can move after an adjunct to reduce the overall dependency length.

Analyzing data from the Penn Treebank, the research confirms that left conjuncts tend to be shorter, a tendency that strengthens with increasing length differences. Crucially, this preference for a shorter left conjunct *only* appears when the governor (the word governing the coordination) is on the left or absent. When the governor is on the right, this tendency vanishes.

This observation provides evidence against asymmetric coordination structures and supports symmetric structures. The study measured length in characters, syllables, and words, consistently showing the left-conjunct preference when the governor is on the left or absent, but not when it's on the right. Przepiórkowski encourages further discussion at the poster session for the full argument.</sample>
    <sample id="217">This work, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," addresses the limitations of existing controllable dialogue generation (CDG) methods, which often focus on single attributes or rely on labeled data. The core problem tackled is enabling models to generate dialogues with multiple attributes and generalize to unseen combinations of those attributes.

The researchers introduce DCG (Disentangled Controllable Generation), a novel model built upon DialoGPT. DCG utilizes a compositional prompt module with two key prompt types: attribute-oriented prompts (focusing on specific attribute values) and task-oriented prompts (incorporating global dialogue features). To enhance diversity and generalization, pseudo combinations are added to the prompts, and a disentanglement loss is employed to separate attribute representations.

A significant contribution is the introduction of MAE (Multi-Attribute Evaluation), a unified, reference-free evaluation framework for assessing controllability across different attribute granularities. MAE utilizes templates with discrete prompts and a trainable continuous dialogue-oriented prompt to minimize bias.

Experiments on the DailyDialog-CG benchmark demonstrate that DCG outperforms existing baselines in both attribute controllability and text quality. The results highlight the effectiveness of attribute-oriented prompts for controllability, task-oriented prompts for text quality, and disentanglement learning for compositional generalization. Notably, DCG exhibits strong performance on unseen attribute combinations, effectively transferring knowledge from seen attributes.

Furthermore, MAE's correlation with human judgments is superior to traditional metrics, proving its reliability. The framework's generality is confirmed by its successful implementation with BART. Visualization of prompt embeddings via PCA confirms the model's ability to disentangle attribute combinations and learn relationships between them, enabling generalization to unseen scenarios. The study concludes that DCG effectively addresses compositional generalization challenges in multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Google Translate</sample>
    <sample id="219">This research, presented by Jia-Huei Ju and colleagues, addresses the challenge of extracting valuable information from lengthy financial reports (specifically Form 10-K filings). The core observation is that these reports exhibit high text similarity year-over-year, suggesting that changes between reports hold key insights.

To tackle this, the team introduces a "highlighting task" and a multi-stage pipeline. The task involves comparing a target report with its preceding year's report (reference) and identifying the words that explain the differences. The model predicts the importance of each word, effectively highlighting the rationale behind changes.

The proposed pipeline consists of:

*   **Document Segmentation:** (Details omitted due to time)
*   **Relation Recognition (Stage 1):** Classifies pairs of text segments into three categories: highly similar (β), syntactically similar but semantically different (Revised), and entirely new information (Mismatched).
*   **Fine-tuning (Stage 2 &amp; 2+):** Initially uses an out-of-domain dataset (eSNLI) for general language understanding. Then, it fine-tunes on the "Revised" pairs, treating revised words as pseudo-positive labels and employing soft labeling techniques to mitigate the impact of potentially noisy labels.

The model's performance is evaluated on both eSNLI and a newly released dataset called FINAL, using precision and Pearson correlation coefficient (PCC). Results demonstrate strong performance on FINAL and maintain generalization ability on eSNLI. The method particularly excels at identifying changes in "Mismatched" pairs, which were not directly used during training.

In conclusion, the research proposes a novel highlighting task, a pipeline with two-stage fine-tuning, and a new dataset (FINAL) to facilitate financial signal discovery within annual reports. Future work includes exploring additional features and information retrieval techniques.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">German into English.</sample>
    <sample id="222">This work, "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," investigates how to improve question answering (QA) systems when applied to new domains (like biomedicine) after being trained on a general domain (like Wikipedia).

The core problem is that models trained on Wikipedia struggle when faced with domain-specific data, leading to incorrect answers due to confusion caused by unfamiliar terminology. Simply adding the new domain's data isn't enough.

The research explores two main approaches to address this: **few-shot** and **zero-shot** data interventions. Few-shot involves using a small number of examples from the target domain to prompt large language models to generate more training data. Zero-shot techniques manipulate the question, answer, and context distributions without any target domain examples.

Key findings include:

*   **Few-shot adaptation** consistently improves both retriever and reader performance (8% and 11% average improvements, respectively).
*   **Zero-shot techniques** involving cloze-style questions proved effective and easier to curate than standard question formats. Varying answer distributions using named entity recognition also showed promise.
*   The research introduces a method to **classify dataset shifts** into four categories: No shift, Concept shift, Covariate shift, and Full shift, based on the compatibility of the retriever and reader with the target domain. Compatibility is measured by likelihood scores.
*   Different data interventions are most effective depending on the type of shift. Few-shot methods work well across all domains, while zero-shot methods are particularly useful for concept and covariate shifts.

Overall, the study demonstrates that targeted data interventions can significantly improve QA performance in new domains, and that understanding the nature of the dataset shift is crucial for selecting the most effective intervention strategy. The work achieves up to 24% improvement in reader performance.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">long-mBART 和 base mBART</sample>
    <sample id="225">53 tasks for training and 9 tasks for testing.</sample>
    <sample id="226">Two.</sample>
    <sample id="227">Current language models, despite their success, lack grounded language understanding – the ability to map natural language to executable plans or programs within a specific environment. This limitation stems from pre-training primarily on textual data without grounding, creating a gap between pre-training and real-world application. Existing approaches often rely on language models to directly generate plans, which frequently results in invalid or ungrammatical outputs.

To address this, the authors propose a novel framework called "Pangu," inspired by Chinese mythology. Pangu separates the "heaven" (symbolic world of plans) from the "earth" (language model). Instead of generating plans, the language model acts as a discriminator, scoring and ranking candidate plans proposed by a symbolic agent interacting with the environment. This approach alleviates the burden on the language model to ensure plan validity and grammar.

The framework was tested on knowledge-based question answering, demonstrating outstanding performance across various language models (BERT, T5, Codex) and learning paradigms (fine-tuning, in-context learning). Notably, Pangu exhibits strong sample efficiency, achieving high accuracy with minimal training data, particularly when using Codex with in-context learning.

A key observation highlights Pangu's robustness: autoregressive models tend to overfit seen structures, while Pangu maintains consistent probability distributions across both seen and unseen structures, suggesting improved generalizability in non-i.i.d. settings.

The core takeaway is that for grounded language understanding, **discrimination is a more effective strategy than generation** when utilizing language models. The authors welcome further discussion and collaboration.</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">This presentation by Gabriella Skitalinskaya and Henning Wachsmuth focuses on a novel approach to supporting argumentative writing by automatically detecting and suggesting improvements for claims. The core problem they address is how to determine when an argumentative claim is phrased effectively and doesn't require further revision—a challenge faced by many novice writers.

To tackle this, they introduce two tasks: **Suboptimal-Claim Detection** (identifying claims needing revision) and **Claim Improvement Suggestion** (suggesting types of quality issues to address). Instead of defining "good" claims explicitly, their work leverages revision patterns observed in collaborative online debate platforms like Kialo. They treat final, accepted versions as "optimal" (green) and preceding revisions as "suboptimal" (red).

However, using revision data presents several challenges. The paper identifies four key areas:

1.  **Representativity and Reliability:** Ensuring the dataset accurately reflects well-argued claims and that final versions are truly optimal.
2.  **Model Complexity and Architecture:** Selecting a model sensitive to subtle revisions and effectively capturing the nuances of argumentative quality.
3.  **Contextual Information:** Determining the relevant context (debate-wide structure, parent claim relationships, domain knowledge) influencing claim quality assessment.
4.  **Topical and User Bias:** Addressing noise and biases inherent in collaborative revision histories, considering the influence of controversial topics and cultural context.

The research explores various strategies to address these challenges and systematically compares different approaches for the two tasks. Their findings suggest that revision-based data *can* be effectively used, modeling the distance between claim versions is helpful for suboptimal claim detection, and contextual information's impact varies depending on the task and the specific quality issues. The paper provides a detailed analysis of their findings and encourages readers to consult it for further information.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web used to train DrBERT.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Sara Papi from the University of Trento and FBK introduces "Attention as a Guide for Simultaneous Speech Translation" (SimulST), a joint work with Matteo Negri and Marco Turchi. SimulST aims to translate spoken language into text in real-time for cross-language communication. Current SimulST models face challenges like complex architectures requiring specialized training, lengthy training procedures, and the need for multiple models to achieve different latency levels.

Their solution, EDAtt (Encoder-Decoder Attention), leverages existing offline speech translation models without retraining or architectural modifications. It manages latency through specific parameters and utilizes the cross-attention mechanism between audio and text. EDAtt determines whether to emit a partial translation based on the attention distribution. A word is emitted only if the attention isn't concentrated on the most recent speech frames (lambda frames), indicating sufficient stability in the received information.

The approach involves analyzing cross-attention weights: if a word's attention points to recent frames, it's held back; otherwise, it's emitted. Experiments on German show EDAtt outperforms strategies applied to offline models (Wait-k, Local Agreement) and even state-of-the-art SimulST architectures, achieving higher BLEU scores (translation quality) with lower latency (average lagging and computational-aware average lagging). EDAtt proves to be the fastest strategy when considering actual elapsed time.

The researchers have released the code, models, and simultaneous output to promote reproducibility.</sample>
    <sample id="234">Prompting has a big influence on the performance of the LLMs for translation; in a simple experiment, the difference observed was of more than one BLEURT point, and in extreme cases, up to 40 BLEURT points.</sample>
    <sample id="235">The paper does not explicitly state the authors' affiliated institutions.</sample>
    <sample id="236">Five expert-written instructions.</sample>
    <sample id="237">通过定义三种KITMUS设置：Background-Pretrain、Background-Both 和 Background-Inference，来控制知识的可用性。</sample>
    <sample id="238">This video introduces MeetingBank, a new benchmark dataset for meeting summarization developed by researchers at the University of Central Florida. The dataset addresses the need for specialized summarization technologies for meetings, a common occurrence in various professional settings.

MeetingBank is built from City Council meetings, comprising 1,366 meetings and nearly 7,000 instances. The data collection process involves using Speechmatics API for transcript generation, identifying meeting details, locating reference summaries from meeting minutes, and aligning timestamps to pair transcripts with summaries.

The dataset provides detailed statistics on meeting duration, speaker count, token count, and summary characteristics across different cities (Boston, Seattle, Denver). Analysis reveals that the summaries tend to include verbatim points rather than abstractive summaries, with Seattle and Boston exhibiting higher density scores (indicating more editing).

The video presents an evaluation of various summarization systems, including extractive methods (Oracle, LEAD, LexRank, TextRank) and abstractive models (BART-Large, Pegasus, Longformer, DialogLM, HMNet), as well as GPT-3. While traditional metrics favored DialogLM, human evaluation revealed that GPT-3 excelled in fluency and coherence, despite lower scores in informativeness and factuality.

The researchers emphasize the need for improved automatic evaluation metrics that better reflect human preferences in meeting summarization. MeetingBank is released as a valuable resource for researchers to develop advanced meeting summarizers and gain insights into the decision-making processes within City Council meetings. The dataset is available for download and further exploration.</sample>
    <sample id="239">大家好，我叫大卫·维拉，我将对论文《Prompting PaLM for Translation: Assessing Strategies and Performance》做一个简短的评论。这是我和谷歌翻译的同事共同完成的工作。PaLM 是一个拥有 5400 亿参数的大型语言模型，去年（2022 年）发布。它在大量文本上进行训练，包含 7800 亿个 token。在发表时，它在数百个 NLP 任务中达到了最先进水平。在这项工作中，我们提出了对大型语言模型提示进行机器翻译的第一个系统研究。我们使用 MT 社区的最佳实践来评估这些模型的翻译能力。这包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠。我们将其与最先进的系统进行比较，即表现最佳的系统，即 WMT 评估。我们使用最先进的神经机器翻译指标，并额外展示了基于专家的人工评估结果。最后，我们还提供了一些提示选择策略的建议。提示对 LLM 的翻译性能有很大影响，正如我们在一个简单的实验中看到的，我们使用了单次提示，并为每个句子提供了两个不同的提示。在 1000 个句子中，有 516 个句子的差异超过了 1 个 BLEURT 点。在极端情况下，甚至可以达到 40 个 BLEURT 点。因此，选择一个好的提示策略非常重要。在我们的实验中，我们选择了 5 次提示策略，只是用标记来标注我们提供给系统的每个句子，标明了句子的语言。在这个例子中，我们从德语翻译成英语，德语源句用“German:”标注，英语译文用“English:”标注。我们发现，对于几个短提示来说，提示的实际形式没有太大影响。这对于零次提示和单次提示至关重要。当我们像我们这样进行 5 次提示时，提示的实际形式几乎没有差异。示例起到了最重要的作用。我们实验结果的总结是，示例质量比与源句的相似度更重要。因此，选择来自高质量翻译的示例非常重要。我们特别比较了从 WMT 评估的训练数据中选择提示与从开发数据中选择提示。开发数据比嘈杂的训练数据经过了更精细的策划，质量更高，结果显示使用开发数据可以获得更好的性能。尽管如此，专业的先进系统在 PaLM 翻译方面具有显著优势。但是，PaLM 几乎可以与商业系统相媲美。在我们的案例中，我们选择用谷歌翻译进行评估。我们从使用 MQM 框架进行的的人工评估中获得的见解表明，PaLM 的流畅度与最先进的系统相当，但主要区别在于准确性。特别是，最常见的错误是遗漏错误。因此，似乎 PaLM 选择产生听起来更好的翻译，有时会删除源句中为了翻译而舍弃的部分。然而，PaLM 的“风格/不自然”类别低于最先进的系统，这是一个额外的信号，表明 PaLM 提供了非常流畅的输出，但仍然存在准确性问题。以上就是这个简短概述的全部内容。如需更多详细信息，请参阅论文的完整演示。非常感谢。</sample>
    <sample id="240">大家好，我是Dawei，来自德国萨尔兰大学的博士生。在这个视频中，我想介绍我们最近的工作“你可能想得比它弱：对弱监督学习的批判性审视”。这是我和Xiaoyu Shen、Marius Mosbach、Andreas Stephan以及Dietrich Klakow的合作成果。我想从简要介绍弱监督和弱监督学习开始。在弱监督中，您不会手动标注数据。相反，我们使用弱标注来源来标注数据，例如简单的启发式规则、知识库或低质量的众包，如图右侧所示。与人工标注相比，弱标注成本更低，但它们也存在噪声，这意味着一定比例的标注不正确。如果直接在弱标注数据上训练神经网络，神经网络往往会记住标签噪声，而无法泛化。在弱监督学习中，会提出训练算法，以稳健地训练神经网络，使其在这样的标签噪声下仍能良好泛化。在最近的WSL（弱监督学习）工作中，一种常见的说法是，人们声称他们只在弱标注数据上训练模型，并在干净的测试集上获得高性能。从技术上讲，这个说法并不错误，但有一个陷阱，那就是人们假设有额外的干净验证集可用于模型选择。我们不能停止在这个问题设置上，但这意味着弱监督学习需要额外的手动标注。但就像一个显而易见的错误，这个问题经常被忽视。我们提出了三个研究问题来探讨这种疑虑。第一，干净的验证数据对WSL是必要的吗？或者我们可以使用一个嘈杂的验证集吗？第二，如果干净的数据是WSL起作用的必要条件，那么我们需要多少干净样本？最后，我们应该只使用干净样本进行验证，还是有更好的利用它们的方法？我们在工作中解决了这些研究问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练的模型就无法泛化到原始的弱标签之外，这意味着训练毫无意义。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图左侧所示。通常，我们只需要每个类别20个样本就能获得高性能。但这并非故事的结局，因为如果我们决定获取干净样本，直接在它们上训练甚至可以获得更好的性能。右图显示了直接应用于干净数据的方法（微调方法）和仅使用干净数据进行验证的WSL方法之间的性能差异。正如我们所见，如果拥有每个类别10个样本，直接微调就开始超越WSL方法。最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来实现。正如从图中可以看出，称为FTw的原始模型最初表现不如更复杂的WSL方法，例如COSINE。但是，如果我们允许在干净样本上继续微调，FTw的表现就会与其它方法一样好。因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。总而言之，我们表明最近的WSL方法需要干净、手动标注的样本才能正常工作。它们的性能提升和实用性被严重高估了。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否通过干净的验证样本进行。其次，WSL方法应该与小样本学习基线进行比较，因为两者都使用干净样本。第三，持续微调是一种简单而强大的基线，应该在未来的WSL工作中考虑。最后，我们已经开源了我们的代码。您可以通过幻灯片上的二维码找到它。请随时查看。谢谢，祝您会议愉快。</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" addresses shortcomings in current automated misinformation detection systems, focusing on realistic evaluation and human integration. Existing systems often rely on retrospectively constructed datasets and struggle to detect misinformation early, before widespread debunking. They also frequently exclude or marginalize human input, failing to reflect the noisy, real-world nature of social media platforms.

The authors propose a novel evaluation framework for end-to-end systems that incorporate human feedback throughout the process, rather than solely at the final determination stage. Their system, exemplified by a case study on COVID-19 treatment misinformation, comprises two main components: claim detection and policy violation verification.

The claim detection component uses keyword filtering and a T5 question-answering model to extract potential misinformation claims from raw tweets, ranking them by trendiness. These claims are then presented to human verifiers. The policy violation verification component employs a BERT-based stance classification model to identify tweets supporting unapproved treatments, flagging them for human review.

The evaluation emphasizes "early detection," defined as identifying unapproved treatments *before* their appearance in debunking news articles, highlighting the system's practical utility. Results demonstrate the system's ability to detect treatments early and achieve a 65% precision rate for policy violation detection. Crucially, the system significantly reduces human workload, enabling the verification of 124.2 policy violations per human hour.

The paper advocates for a more realistic and human-centric approach to misinformation detection, providing a framework for consistent evaluation and offering valuable insights into the development and assessment of such systems from an external perspective.</sample>
    <sample id="242">Common practices include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">Four.</sample>
    <sample id="244">"Judges decide cases in law courts."</sample>
    <sample id="245">This work, "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization," presents a two-step pipeline for identifying high-quality Amazon Mechanical Turk (MTurk) workers for summarization tasks, addressing issues with automatic metrics and unclear best practices for MTurk recruitment.

The pipeline begins with a "Qualification Task" designed to assess workers' ability to evaluate summaries across six dimensions. This task, involving document evaluation and an attention check, categorizes workers into gold, silver, bronze, and block tiers, with only gold and silver workers progressing. This initial stage yielded 26 qualified workers (13% of 200 participants).

The second stage, an "Endurance Task," evaluates workers' capacity for handling a large workload, consisting of 10 HITs focused on saliency assessment. This resulted in 12 workers (6% of 200) demonstrating high inter-annotator agreement (IAA), surpassing expert levels, with a Krippendorff's Alpha of 0.443.

A "Reference-Based Task" further tested performance on the actual summarization task, achieving a Krippendorff's Alpha of 0.534. Comparisons were made against baseline MTurk workers using statistical filters like MACE (Alpha 0.380) and CloudResearch MTurk workers (Alpha 0.513), demonstrating comparable quality with the pipeline approach while avoiding resource waste.

Analysis revealed a significant Spearman's correlation between Pipeline and CloudResearch workers, and a strong correlation between GPT models and expert judgments. The pipeline effectively filters out low-quality workers, achieving high agreement at a lower cost.

The study concludes that the pipeline, resulting in 4 gold and 8 silver workers (6% of 200), provides a best practice for large-scale, high-agreement annotation. Future work will focus on improving worker quality in both agreement and correctness, exploring diverse applications, languages, and platforms. Limitations include the focus on English summarization, the task-specific nature of the questions, and the lack of guaranteed correctness training.</sample>
    <sample id="246">Yes, the dataset and code are available on GitHub.</sample>
    <sample id="247">This presentation introduces "FACTKG," a new dataset and task for Knowledge Graph-Based Fact Verification. The core problem addressed is the lack of datasets utilizing knowledge graphs (KGs) as evidence for verifying natural language claims, despite KGs' potential for reliable and practical fact-checking.

Existing datasets like FEVER and TabFact rely on text or tables, requiring additional interpretation for reasoning. FACTKG leverages DBpedia as its KG, offering more intuitive evidence and facilitating direct connections to claims. The dataset features claims in both written and colloquial styles to enhance practical applicability.

The task involves retrieving evidence from DBpedia and verifying claims, categorized into five reasoning types:

*   **One-hop:** Verifying a single triple connection between entities.
*   **Conjunction:** Checking multiple one-hop claims simultaneously.
*   **Existence:** Confirming an entity's connection to a specific relation.
*   **Multi-hop:** Requiring inference across multiple KG hops to establish a connection.
*   **Negation:** Involving an additional inference step after finding graph evidence.

To generate colloquial claims, the researchers employed a style transfer model and presupposition templates.

The dataset's performance was evaluated against baselines, including a "Claim Only" approach and the GEAR model (which utilizes graph evidence). Results demonstrate that leveraging graph evidence significantly improves verification accuracy, with the GEAR model consistently outperforming other baselines.

In essence, FACTKG provides a valuable resource for advancing research in fact verification by incorporating the strengths of knowledge graphs and addressing the limitations of existing datasets. The dataset and associated code are publicly available for further exploration and development.</sample>
    <sample id="248">No, the annotators were diverse, coming from over 1000 annotators from 87 countries.</sample>
    <sample id="249">By adding noise to the input while trying to preserve the relevant structure.</sample>
    <sample id="250">进行维度评估意味着评估对话质量的多个方面，以了解模型的优势和劣势，而不仅仅是整体质量。</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">This presentation introduces U-CREAT, a novel unsupervised pipeline for Prior Case Retrieval (PCR), a task crucial for legal professionals to find relevant precedents. The core challenge is efficiently retrieving relevant cases from a large pool, given a legal query.

The work makes two key contributions:

1.  **IL-PCR Dataset:** A new benchmark dataset for PCR tasks focused on Indian legal cases. It's significantly larger and more complex than existing datasets like COLIEE’21, featuring more cases, longer documents, a larger vocabulary, and a higher number of citations. This dataset is publicly available.

2.  **U-CREAT Pipeline:** An unsupervised approach leveraging event extraction for PCR. It avoids the need for law-specific tuning and demonstrates strong performance across both Indian and Canadian legal systems.

The U-CREAT pipeline's key innovation is its event-based approach. It extracts events (subject-verb-object triplets) from legal documents using dependency parsing. These events are then used to create an interaction matrix between the query and candidate documents, identifying common events. Different retrieval models utilize this matrix to rank candidates.

Experiments showed that event-based models, particularly the "Event Filtered Documents" model, significantly outperform traditional count-based and transformer-based models (including legal-specific BERT variants) on both the IL-PCR and COLIEE datasets. Event-based models also exhibit lower inference times and higher F1 scores. U-CREAT, specifically the Event Filtered Documents approach, currently represents the state-of-the-art for the COLIEE’21 task. The research highlights the potential of unsupervised event extraction for improving PCR performance and opens avenues for future research in this area.</sample>
    <sample id="253">This presentation introduces "DisorBERT," a novel model for detecting signs of mental disorders in social media posts, developed by a collaborative team from Mexico and Spain. The core problem addressed is the potential of social media data to identify individuals experiencing mental health difficulties, aiming to create technology that can provide early warnings and support.

The key challenge tackled is the limited availability of annotated data for mental health-specific tasks. DisorBERT employs a "double domain adaptation" strategy, building upon the pre-trained BERT language model. It first adapts BERT to the general language of social media (specifically Reddit) and then further specializes it for the mental health domain. A "guided masking" technique is incorporated to focus the model's attention on relevant words during training.

Experimental results using the eRisk dataset demonstrate that DisorBERT achieves a better balance between precision and recall compared to baseline models. Analysis of the model's predictions reveals that DisorBERT tends to generate words with a more negative or psychologically oriented meaning compared to standard BERT, particularly when prompted with sentences from the Beck Depression Inventory (BDI).

Visualization of attention scores on a user's social media post highlights the model's ability to identify key phrases related to mental health, such as "anxious" and "medication." The model outperforms MentalBERT, a model trained on a larger dataset.

The presentation concludes that DisorBERT's combined approach of double domain adaptation and guided masking effectively captures signs of mental disorders. Future work will explore different lexical resources and the integration of clinical data to further enhance the model's performance.</sample>
    <sample id="254">This research introduces a novel framework, "Uncertainty Guided Label Denoising," to enhance document-level distant relation extraction (DocRE). DocRE aims to identify relationships between entities within documents, often relying on distantly supervised (DS) data which is prone to noise. Existing methods using pseudo-labels risk introducing false relations and overlooking correct ones.

The proposed framework addresses this by first training a DocRE model with both human-annotated and DS data to generate pseudo-labels. Recognizing the inevitability of false pseudo-labels, the framework incorporates uncertainty estimation to assess the reliability of model predictions. A key innovation is an instance-level uncertainty estimation method designed to handle overlapping relations, where an entity pair can have multiple relationships.

To model uncertainty, the framework utilizes Monte Carlo dropout, modified to calculate instance-level uncertainty scores for each positive pseudo-label, overcoming limitations of previous approaches. Dynamic class uncertainty thresholds are then employed to filter out pseudo-labels with high uncertainty, replacing them with more reliable ones.

A multi-phase training strategy iteratively re-labels DS data, maximizing its utility. Experimental results on public datasets demonstrate significant performance improvements compared to existing baselines.

The core contributions are: (1) a framework for uncertainty-guided label denoising, (2) an instance-level uncertainty estimation method for overlapping relations, (3) an iterative re-labeling strategy with dynamic thresholds to address the long-tail problem, and (4) substantial performance gains in DocRE.</sample>
    <sample id="255">It's crucial for zero and one-shot prompting. When using five-shot prompting, there is nearly no difference to the actual form of the prompting.</sample>
    <sample id="257">Four state-of-the-art chat models.</sample>
    <sample id="258">This video summarizes the research paper "Can Large Language Models Be an Alternative to Human Evaluation?" by Chiang Cheng-Han. The core idea is to explore using large language models (LLMs) to automatically evaluate the quality of text in natural language processing, potentially replacing or supplementing traditional human evaluation.

The motivation stems from the instability and difficulty in reproducing human evaluations, which are often used to assess the quality of generated text. The researchers questioned if LLMs, known for their ability to follow natural language instructions, could perform this evaluation task.

The experiment involved using LLMs (T0, InstructGPT - curie and davinci, and ChatGPT) to rate stories generated by GPT-2 or written by humans, based on four attributes: grammar, coherence, likability, and relevance. The LLM ratings were then compared against "ground truth" ratings obtained from English teachers, who served as expert human evaluators using the same instructions and stories.

The results showed that English teachers preferred human-written stories over those generated by GPT-2. While smaller LLMs didn't consistently show a preference, Davinci and ChatGPT demonstrated a clear preference for human-written text, mirroring the human evaluators' judgments. This suggests that certain LLMs can indeed serve as a viable alternative to human evaluation.

The video concludes by mentioning that the paper addresses further questions, such as the agreement between LLM and human ratings, the impact of instruction wording, sampling methods, the cost-benefit analysis of LLM evaluation versus human evaluation, and the applicability of this approach to other NLP tasks. Readers are encouraged to consult the paper or visit the poster stand at ACL for more details.</sample>
    <sample id="259">The presentation introduces XSemPLR, a new benchmark dataset for cross-lingual semantic parsing, aiming to address limitations in existing approaches. Current models often focus on limited languages, meaning representations (like Lambda Calculus), or are evaluated using only a single model architecture.

XSemPLR comprises 9 datasets across 5 semantic parsing tasks, 8 meaning representations, and 22 languages spanning 15 language families. It provides six distinct training and evaluation settings: Translate-Test (using Google Translate), Monolingual, Monolingual Few-shot (10% training data), Multilingual (training a single model on multiple languages), Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer.

The study evaluated two model types: Encoder-PTR (e.g., XLM-R + PTR, mBERT + PTR) and Encoder-Decoder (e.g., mBART, mT5). Encoder-Decoder models consistently outperformed Encoder-PTR across all datasets. Training multilingual models with a mix of languages generally improved performance across most major languages, although English performance occasionally decreased—a phenomenon termed the "Curse of Multilinguality."

Analysis of cross-lingual transfer revealed a significant performance gap in the Zero-shot setting, which rapidly diminishes with Few-shot learning. The research also highlighted that pretraining on English significantly boosts Few-shot performance in other languages. Notably, the study found that large multilingual language models like Codex and BLOOM are currently inadequate for cross-lingual semantic parsing.

In conclusion, XSemPLR offers a comprehensive benchmark for evaluating cross-lingual semantic parsing models, revealing valuable insights into model performance and transfer learning capabilities. The findings emphasize the importance of diverse training data, appropriate model architectures (Encoder-Decoder), and the potential benefits of English pretraining for improving performance in other languages.</sample>
    <sample id="260">The content does not mention the number of authors.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper does not mention the number of authors.</sample>
    <sample id="263">This work addresses the instability of in-context learning (ICL) in large language models (LLMs) due to various biases introduced by design choices like example selection and order. Existing research has highlighted search instability, but a systematic categorization of these biases and effective mitigation strategies have been lacking.

The paper introduces a typology of label biases within the context of text classification, identifying three key types: vanilla-label bias (model's inherent preference for certain labels), context-label bias (influence of the provided examples), and a novel *domain-label bias* (impact of the task corpus on predictions). The researchers demonstrate that simply exposing the model to random in-domain words can significantly bias its predictions, highlighting the importance of this new bias type.

They propose a novel calibration method called *domain-context calibration* to mitigate these biases. Unlike previous methods that use predefined "content-free" tokens, domain-context calibration utilizes random in-domain words as content-free text. This approach acknowledges and addresses the domain-label bias by incorporating elements from the task corpus while maintaining near content-free randomness.

Experiments across diverse datasets and models (including GPT-3) show that domain-context calibration significantly improves ICL performance, particularly on tasks with high domain-label bias. Analysis of prediction distributions reveals improved decision boundaries after calibration. Further studies demonstrate that using random English words instead of a single predefined token is beneficial, and employing multiple random words yields even better results. Ultimately, the research provides a systematic investigation of label biases in ICL and a practical calibration method to enhance the reliability and performance of LLMs in this paradigm.</sample>
    <sample id="264">大家好，我是浙江大学的林王，今天分享我的论文“TAVT：面向可迁移的视听文本生成”。

目前，机器翻译和图像描述等单模态文本生成任务已经取得了显著进展。但视听文本生成任务由于数据标注困难且昂贵，现有方法在不同领域容易出现性能下降。

为了突破这一限制，我们提出了“可迁移视听文本生成”任务，主要挑战在于视听模态的领域迁移，如视觉风格、音频能量等。我们观察到，对于同一事件，视觉内容会因图像风格和拍摄角度而显著变化，但音频内容（如节奏和能量）变化对事件理解影响较小。因此，我们认为可以使用统一的视听语义空间来对齐不同领域的视觉概念。

我们的框架由三个部分组成：视听元映射网络、视听编码器和语言模型生成器、以及反事实对比学习。视听元映射网络将不同领域的视觉概念映射到统一的视听语义空间。我们利用Flickr数据集的音频片段，通过K-means聚类构建统一的视听语义空间，并引入可学习的视觉前缀。

我们采用基于Transformer的编码器和生成器，并引入alpha值评估不同模态对每个词的贡献。为了直接优化视听对齐，我们提出了双反事实对比学习（DCLL）。

实验结果表明，我们的方法在跨数据集和跨领域设置下均优于现有方法，尤其是在低资源领域表现出色。通过消融实验，我们分析了音频特征对性能提升的影响。</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">The paper does not mention the author's institution.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">大家好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将向您介绍 ABC-Eval，这是一种评估对话式人工智能的新维度方法。这项工作由埃默里大学 NLP 实验室在 Jinho Choi 教授的领导下完成，并与亚马逊 Alexa AI 合作。

假设您刚刚开发了一个对话模型，并想了解它与当前最先进水平的比较情况。常见的做法是进行人工评估，例如请人工评委选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法可以很好地提供对整体对话质量的整体评估，但对话质量有很多方面。因此，您可能希望评估聊天质量的多个维度，以便更精细地了解模型的优势和劣势。

一种方法是简单地要求人工评委评估对话质量的多个维度，例如使用现有的比较或李克特量表方法来评估模型响应的相关性。然而，我们认为有一种更精确、更可靠的维度对话评估策略。我们的方法试图通过明确标注模型响应是否表达了某些行为（例如，提供不相关的信息或自相矛盾）来减少人工评估的主观性。我们称这种方法为“标注聊天行为”或简称 ABC-Eval。

我们开发这种方法是为了全面涵盖最近文献中被认为会影响聊天质量的聊天模型行为。ABC-Eval 能够测量聊天模型犯各种主题错误的速率。例如，ABC-Eval 测量聊天模型忽略其对话伙伴或说出不相关内容、自相矛盾或与对话伙伴矛盾、产生不正确的虚构事实或违反常识知识以及模型是否成功或未能表现出同理心等情况的次数。

为了确定哪种评估方式最有效，我们选择了四个最先进的聊天模型，并使用 ABC-Eval 对每个模型进行了 100 次人机对话的评估。为了进行比较，我们还使用三种现有方法评估了这些对话：在回合级别上的李克特评分、在对话级别上的李克特评分和对话级别的成对比较。对于每一种现有方法，我们收集了关于对话中八个最常测量方面的数据，因为这是评估聊天模型沿多个维度进行评估的标准做法。

通过分析这些评估结果，我们发现 ABC-Eval 行为标签的整体可靠性高于现有方法收集的标签，这可以通过对 100 个双重标注对话的评委间一致性来衡量。此外，与现有方法产生的指标相比，ABC-Eval 标签更能预测整体对话质量，这如简单的线性回归分析所示。例如，您可以看到测量包含自我和对话伙伴矛盾的回合比例分别可以解释对话质量的 5% 和 10%，而平均李克特一致性分数只能解释 4% 或更少。

最后，我们使用逐步线性回归检查每种评估指标是否捕捉到聊天质量的独特方面。您可以看到所有 ABC-Eval 指标的组合解释了超过 25% 的对话质量，并且在一次删除一个指标时，大多数指标都会导致失去大量关于质量的信息。另一方面，所有回合级别李克特指标的组合解释的质量要少得多，而且较少的指标携带独特的有用信息。

这些可靠、信息丰富且独特的 ABC-Eval 指标使我们能够以比以前方法更高的分辨率评估对话式人工智能。您可以看到我们实验的结果表明，仍然存在一些挑战，并且这些挑战已经被精确地量化。例如，我们测试的聊天机器人在大约 20% 的响应中存在常识性错误，大约 15% 的响应中产生不相关的信息，并且大约 10% 的时间会自相矛盾或与对话伙伴矛盾。

随着该领域快速进步，许多错误率可能会在自我们评估以来发布的新模型中降低。然而，这更增加了追求可靠且精确的评估指标来比较模型的必要性。我们希望 ABC-Eval 能够被该领域的其他研究人员利用，作为朝着这个方向迈出的有意义的一步。我们期待着在未来几个月和几年里看到对话式人工智能的进步。谢谢观看。</sample>
    <sample id="270">Emory NLP Lab at Emory University and Amazon Alexa AI.</sample>
    <sample id="271">FTw</sample>
    <sample id="272">Seven.</sample>
    <sample id="273">大家好，我叫Kayo Yin，我将为大家介绍我们的工作，题目是“翻译何时需要上下文？一项跨语言的数据驱动探索”。这项工作是我们与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig合作完成的。

很多翻译都依赖于上下文。例如，我们应该如何翻译“mole”这个词？如果前一句是“如果部长们知道了，情况可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，严重吗？”，那么“mole”指的是胎记。因此，根据上下文，词义会发生变化，从而影响翻译。

然而，评估模型在处理此类情况时表现如何，其实非常困难。首先，只有一小部分翻译依赖于上下文，这使得像BLEU这样的语料库级别指标无法捕捉到这些翻译。有些人建议对上下文相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工标注。

在这项工作中，我们试图回答两个问题。首先，翻译何时需要上下文？其次，模型如何处理这些情况？

为了回答第一个问题，我们首先测量了在翻译过程中，一个词对上下文的依赖程度。在之前的工作中，我们引入了CXMI作为衡量机器翻译模型上下文使用情况的指标。这通过测量上下文C对目标Y提供的信息量，给定源X来完成。你可以将CXMI视为给予模型上下文所获得的信息量。

在这项工作中，我们将CXMI扩展到Pointwise CXMI，它可以测量在句子级别或单词级别上的上下文使用情况。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。

现在，我们分析具有高P-CXMI的单词，以寻找这些单词之间的模式。我们对TED演讲的英译本进行分析，涵盖了14种不同的语言。我们分别在三个不同的层面进行分析。首先，我们观察具有高平均P-CXMI的词性标签。这使我们能够发现，例如，阿拉伯语中的双重代词具有相对较高的P-CXMI。这可以解释为，英语没有双重代词，因此在翻译成阿拉伯语时，需要上下文来确定代词是否为双重形式。类似地，我们还发现，在某些情况下，当选择适当的动词形式时，也需要上下文。

然后，我们观察所有不同出现情况的词汇项目平均的P-CXMI。这有助于我们识别出诸如上面这种情况，在中文中，需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。类似地，我们发现，为了以正确的正式程度进行翻译，也需要上下文。

最后，我们观察不同的单个token，这些token具有较高的P-CXMI。这使我们能够识别出无法真正通过单词本身来捕捉的现象，而是通过句子结构来表达的现象，例如省略现象。

现在，我们利用我们的分析结果来设计一个文档级别翻译的基准。对于我们识别的五个话语现象，我们创建了标记器，以自动识别与该现象相关的单词。我们称我们的标记器为Multilingual Discourse-Aware，或MuDA标记器。

然后，我们还可以注意到，不同的语言具有不同比例的话语现象。然后，我们使用MuDA标记器，将标记器应用于我们想要用于评估的平行语料库，并在MuDA标记器已识别的上下文相关示例上应用我们选择的翻译指标。

最后，我们使用我们的基准以及其他指标来评估不同的模型在文档级别机器翻译中的表现。

首先，当我们使用语料库级别的指标时：对于BLEU，我们发现上下文无关的模型表现最好。但如果使用COMET，则上下文感知的模型表现最好。如果使用词f-measure，则具有和不具有上下文的模型表现相当。这再次表明，如果仅使用语料库级别的指标，很难确定最佳的文档级别翻译系统。

现在，我们使用MuDA基准来评估模型，我们发现，对于诸如正式程度和词汇衔接等某些话语现象，上下文感知的模型比不使用上下文的模型更准确。但对于省略、代词和动词形式等其他现象，这些模型并没有比不使用上下文的模型好多少。这表明我们需要在文档级别翻译方面看到更多的进展。

我们还比较了不同的商业系统，我们的基准表明，DeepL通常比Google Translate更适合文档级别翻译。

总而言之，我们对14种语言对进行了数据驱动分析，以确定翻译何时需要上下文，然后我们利用我们的发现来构建一个文档级别机器翻译的基准，该基准可以帮助我们识别模型能够很好地处理哪些话语现象，以及哪些翻译系统擅长文档级别翻译。

谢谢大家的关注。我们将在多伦多再见。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This work introduces "IndicMT Eval," a novel dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. Recognizing the limitations of applying English-centric MT evaluation metrics to languages with diverse linguistic features (grammar, vocabulary, sentence structure, resources), the researchers focused on five Indian languages: Tamil, Malayalam (Dravidian), and Hindi, Marathi, and Gujarati (Indo-Aryan).

The dataset comprises 7,000 samples generated by translating 200 source sentences into English using seven different MT models/APIs. Crucially, these translations were meticulously annotated by bilingual expert annotators using the MQM framework, capturing error types (accuracy, fluency, special categories), severity, and an overall score.

Analysis of the dataset revealed performance variations among MT models, with newer models like Indic Trans and NLLB outperforming older ones.  The study examined the correlation between various MT metrics (overlap-based, embedding-based, COMET) and human scores. While chrF showed the highest correlation among overlap-based metrics, COMET-based metrics demonstrated the strongest overall correlations. A key observation was the skewed scoring range of many metrics, limiting their interpretability compared to the full range of human scores.

Further analysis, splitting the dataset by error type (fluency vs. accuracy), showed that metrics generally correlated better with accuracy errors. Leveraging the annotated data, the researchers fine-tuned COMET, creating "IndicCOMET," which outperformed standard COMET baselines on several languages and demonstrated improved zero-shot performance on unseen languages. Finally, IndicCOMET exhibited greater robustness on the ACES Translation Accuracy Challenge Sets, highlighting its potential for more reliable MT evaluation in Indian language contexts. The dataset is publicly available to facilitate further research in this area.</sample>
    <sample id="277">没有名称。</sample>
    <sample id="278">The Marked Words method draws upon the sociolinguistic concept of "markedness," where dominant groups are unmarked, and marginalized groups are linguistically marked. The method compares personas using weighted log-odds ratios to distinguish the top words for each marked group, designating unmarked and marked groups beforehand.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">This presentation introduces MultiEMO, a novel framework for emotion recognition in conversations (ERC) that addresses limitations in existing approaches. ERC aims to predict emotions from textual, audio, and visual cues within dialogues.

MultiEMO tackles three key challenges: inadequate multimodal information utilization, poor performance on minority emotions, and difficulty distinguishing between semantically similar emotions. The framework comprises four components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification.

Key contributions include:

*   **VisExtNet:** A visual feature extractor focusing solely on facial expressions, eliminating irrelevant scene information that can confuse emotion recognition. It uses MTCNN and VGGFace2 pre-trained ResNet-101.
*   **MultiAttn:** A multimodal fusion network employing bidirectional multi-head cross-attention layers to integrate textual, audio, and visual information, allowing each modality to learn from the others.
*   **Sample-Weighted Focal Contrastive Loss (SWFC):** A loss function that prioritizes minority emotion classes and maximizes the distance between different emotion pairs, improving distinction between similar emotions.

Experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in recognizing minority and semantically similar emotions. Visualization of heatmaps further highlights MultiEMO's ability to handle complex scenarios where modalities provide asynchronous emotional cues.

Limitations include VisExtNet's inability to differentiate speakers, the SWFC loss's need for large batch sizes, and the continued performance gap between minority and majority emotion classes.</sample>
    <sample id="281">This work, "When Does Translation Require Context? A Data-driven, Multilingual Exploration," investigates the role of context in machine translation across 14 languages. The researchers address the difficulty of evaluating context-dependent translations, which are often missed by standard metrics like BLEU.

They extend the CXMI metric to Pointwise CXMI (P-CXMI) to measure context usage at the sentence and word level, identifying words requiring contextual information for accurate translation. Analysis of TED talk transcripts revealed patterns: dual pronouns in Arabic needing context due to English's lack of dual pronouns, verb form selection, consistent translation of proper nouns in Chinese, and maintaining formality levels. They also identified phenomena like ellipsis resolution, which rely on sentence structure rather than individual words.

To facilitate evaluation, they developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words related to five discourse phenomena. Using the MuDA benchmark, they evaluated models, finding that context-aware models outperformed context-agnostic models in formality and lexical cohesion but showed little improvement in areas like pronouns and verb forms. Corpus-level metrics like BLEU favored context-agnostic models, while COMET favored context-aware ones.

The study also compared commercial systems, demonstrating DeepL's superior performance over Google Translate in document-level translation. Ultimately, the research provides a data-driven understanding of when context is crucial for translation and offers a benchmark to guide future development of document-level machine translation systems, highlighting areas needing further progress.</sample>
    <sample id="282">Xuekai Zhu presented "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023, addressing story-level, discourse-level style transfer—a significant advancement over previous token or sentence-level approaches. The core challenge lies in replicating an author's complex linguistic preferences, particularly discourse structures, and managing style's strong association with specific content.

StoryTrans tackles these issues by learning discourse representations from source texts and combining them with style embeddings. A key innovation is a training objective that minimizes style influence in discourse representations and enhances content preservation through a two-stage generation process. First, the model transfers the source text with masked content keywords, then generates the complete text incorporating these keywords.

The training framework utilizes an advisory approach, including self-reconstruction, disentanglement, sentence order, and style classifier losses for the first stage. The second stage focuses solely on content restoration. Experiments on newly created Chinese and English datasets involving fairytale and everyday story transfers demonstrate StoryTrans's superior performance over baselines in style control and content preservation, as confirmed by both automatic and manual evaluations. Style visualization confirms alignment with golden text. StoryTrans effectively enriches storylines and maintains source semantics, unlike StyleLM which inserts unrelated sentences. The data and code are publicly available.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">FSUIE is a novel universal information extraction (UIE) model designed to address limitations in existing span-based UIE models that overly rely on precise span boundaries and struggle with the mismatch between transformer feature extraction and IE tasks. The core idea is to introduce a "fuzzy span" mechanism, recognizing the ambiguity in span annotation and the need for adaptive attention.

FSUIE incorporates two key components: a Fuzzy Span Loss (FSL) and a Fuzzy Span Attention (FSA). FSL addresses the span boundary ambiguity by predicting a continuous boundary distribution instead of a precise one. It combines Binary Cross Entropy (BCE) loss with KL-divergence to incorporate supplementary information. FSA, implemented as a mask function, dynamically adjusts the attention span and linearly decays attention distribution near the boundaries, guiding the model towards more relevant semantic information. Critically, the FSA layer is added only at the top level, preserving the text encoding capabilities of the underlying transformer.

Experiments across named entity recognition (NER), relationship extraction (RE), and aspect sentiment triplet extraction (ASTE) demonstrate FSUIE's effectiveness. In NER, FSUIE-base significantly outperforms UIE-base, particularly on smaller datasets. On RE, FSUIE achieves state-of-the-art (SOTA) results on ACE2004, 2005, and ADE datasets, showcasing its ability to extract relationship elements with a unified structure and strong generalization capabilities.  FSUIE also achieves SOTA results on several AST-V2 datasets. Ablation studies confirm that both FSL and FSA contribute to improved convergence speed and information extraction capability. Visualization of the attention distribution reveals that the module focuses on semantic information within a limited range of preceding tokens, validating the design.

In conclusion, FSUIE’s fuzzy span loss and attention mechanism offer a significant advancement in UIE, achieving excellent results across diverse IE tasks and demonstrating improved generalization.</sample>
    <sample id="285">This presentation introduces "Reference Matters," a study focusing on factual error correction for dialogue summarization. The core argument is that current evaluation methods for Factual Error Correction (FEC) models are flawed and hinder progress in the field.

Existing FEC evaluations rely on factuality metrics like FactCC and DAE, which provide overall scores without pinpointing specific errors. This approach blurs the distinction between genuine error correction and the FEC model simply generating a different, factually correct summary, potentially bypassing the original errors.

To address these issues, the researchers propose a new evaluation framework grounded in manually annotated reference corrections. This framework emphasizes minimal changes (substitutions, insertions, deletions) to achieve a fluent and non-redundant summary, reflecting the core requirements of factual error correction.  Reference corrections offer superior training data compared to pseudo data and enable more accurate performance evaluation.

The study also introduces a new taxonomy of factual errors, categorizing them as content-based (based on part of speech and dependencies) and form-based (addition, deletion, substitution). This detailed classification informs the evaluation process, which leverages the ERRANT metric used in grammar error correction.

Experiments using the new framework revealed key findings: training FEC models with reference summaries from dialogue datasets yields the best results according to unreliable factuality metrics, highlighting the need for evaluation method reform. Incorporating human-corrected summaries improves FEC model performance, suggesting a promising avenue for future research. However, current FEC models struggle with specific error types, particularly additions and attribute/modality/link errors, indicating areas for further development. The study ultimately advocates for a shift towards more rigorous, reference-based evaluation and training strategies for factual error correction in dialogue summarization.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">Four.</sample>
    <sample id="288">BLiMP, SyntaxGym, and CrowS pairs.</sample>
    <sample id="290">FTw, COSINE</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT was initially trained on OSCAR 138 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This presentation introduces EPIC, a new dataset for studying irony detection in natural language processing, developed through a collaboration between the University of Turin and Amazon Alexa. The core issue addressed is the limitation of traditional NLP models that rely on a single "ground truth" for annotation, particularly when dealing with nuanced phenomena like irony.

EPIC consists of approximately 300 short conversations collected from Reddit and Twitter over 1½ years, spanning five English varieties. The dataset was annotated by 74 crowdsourced annotators using Prolific, each providing 5 annotations per conversation, with quality control measures in place. Annotators were asked a simple question: "Is the reply ironic?"

The research explored inter-annotator agreement, revealing significant variations based on annotator demographics like gender, age, and nationality. To account for these differences, the team developed "perspective-aware models," training separate models on data subsets based on annotator groups.

While raw performance wasn't significantly improved by these perspective-aware models, they demonstrated notably higher confidence in their predictions compared to models trained on aggregated "gold standard" labels. Further analysis of the data revealed that disagreement on irony perception is most pronounced between generations close in age and between annotators from the UK and Ireland.

The EPIC dataset and perspective-aware modeling approach offer a novel way to tackle the complexities of irony detection, acknowledging the subjective nature of interpretation and moving beyond the assumption of a single, definitive truth. The research highlights the importance of considering annotator perspectives in NLP model development.</sample>
    <sample id="297">This presentation details a research project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," focused on identifying and understanding dogwhistles – terms that convey a hidden, often inflammatory, message to an in-group while maintaining plausible deniability for the speaker. The project addresses the challenge of studying these subtle forms of communication, which are crucial for understanding political influence and online abuse.

The core of the project involves a comprehensive glossary of over 340 dogwhistles, primarily focused on racist, transphobic, and anti-Semitic terms, compiled from various sources. These terms are categorized using a typology based on register (formal/informal), persona (e.g., anti-Semitic, transphobic), and type, which describes how the dogwhistle functions within a sentence.

A historical case study of U.S. political speeches revealed a correlation between the frequency of racial dogwhistles in the Congressional Record and the Republican Southern Strategy, demonstrating a shift towards coded language following the Civil Rights era.

The research also evaluated the capabilities of language models, specifically GPT-3, in recognizing and interpreting dogwhistles. While GPT-3 could surface many dogwhistles, particularly those in formal language, performance was weaker with informal terms and transphobic language. Providing definitions and "secret cues" significantly improved the model's ability to identify covert meanings.

Finally, the project demonstrated how dogwhistles can circumvent content moderation systems. Toxicity detection APIs, like Prospective API, consistently rated sentences containing dogwhistles as less toxic compared to those using direct slurs or group labels, highlighting a significant vulnerability in automated content moderation. The project ultimately aims to improve our understanding of coded rhetoric and its implications for online discourse and political manipulation.</sample>
    <sample id="298">Retraining or continuing to pre-train some models with more recent data showed that performance degrades with a larger temporal gap.</sample>
    <sample id="299">This work addresses the issue of NLI (Natural Language Inference) models relying on spurious correlations, or "shortcuts," within datasets, which leads to poor performance on out-of-distribution data. Existing shortcut mitigation methods often require domain-specific knowledge, assume the learner and auxiliary models exploit the same shortcuts, and incur significant computational overhead due to the use of pre-trained language models as auxiliaries.

The proposed solution is a minimax training method that doesn't require prior knowledge of shortcuts. The core idea is that NLI models struggle with "hard" examples – under-represented instances that contradict common shortcuts – and that focusing on these examples is crucial for generalization.

The minimax training involves two models: a learner and an auxiliary. The learner aims to minimize the NLI task loss, while the auxiliary attempts to *maximize* the learner's loss by generating example weights. This incentivizes the learner to concentrate on areas of the input space where it performs poorly (i.e., hard examples). The two models are trained iteratively. Crucially, the learner operates independently at test time, without needing the auxiliary.

The method was evaluated on MNLI, FEVER, and QQP datasets, using adversarial test sets HANS Symmetric and PAWS. Results demonstrate consistent improvements in out-of-distribution performance compared to standard training and existing shortcut mitigation techniques, while maintaining high in-distribution accuracy. The paper also explores the impact of model size, the size of the auxiliary model, and provides a qualitative analysis of the learned example weight distribution.</sample>
    <sample id="300">This presentation introduces "interactive dictation," a novel task aiming to create a more natural and intuitive voice-based document editing experience. Unlike existing speech-to-text systems that primarily focus on dictation, interactive dictation allows users to seamlessly interleave dictation and editing commands using natural language.

The core idea is to move beyond fixed command templates (like those in Dragon NaturallySpeaking or Microsoft Word Dictate) and enable users to issue edits conversationally, without needing specific trigger words. For example, a user could dictate a sentence, then correct a mistake mid-utterance ("on Friday the 23rd"), and subsequently issue a command to replace a phrase ("Replace 'the event’ with 'it'").

The research team formalized interactive dictation as a four-step process: 1) Automatic Speech Recognition (ASR) to transcribe audio, 2) Segmentation of the transcript into dictation and command utterances, 3) Extraction and normalization of commands, and 4) Execution of dictation and commands sequentially to reach the final document state.

To facilitate research, they designed a new data collection interface and built a dataset. A baseline system was developed, employing separate models for each step, including an interpretation model tested with both T5 and GPT-3 architectures. The models were evaluated on their ability to accurately predict the final document state. Results showed a trade-off between runtime and accuracy, with GPT-3 generally being more accurate but slower. For T5, predicting intermediate programs proved more efficient without sacrificing accuracy.

The researchers emphasize that this is an early stage of development and that significant room for improvement exists. They have released the code to encourage further research and development in this promising area.</sample>
    <sample id="302">因为在第一步中，每个输入词元都被标记为一个无序的多重集，所以需要排列输出序列中的词元，将它们放到正确的顺序中。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为他们无法确定这些方法是否导致了积极刻板印象和本质化叙事，例如，无法确定这些现象是由于过度价值对齐还是其他反刻板印象方法造成的。</sample>
    <sample id="304">Minimal pair paradigms (MPP) evaluate language models on acceptability judgments, typically by comparing an acceptable sentence to an unacceptable one and expecting the model to assign higher probability to the acceptable sentence.</sample>
    <sample id="305">This presentation critiques recent advancements in Weakly Supervised Learning (WSL), highlighting a significant, often overlooked, dependency on clean validation data. Dawei and his team's research, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," challenges the common claim that WSL methods can achieve high performance solely on weakly labeled data.

The core argument is that many WSL approaches rely on a clean validation set for model selection, effectively negating the claim of complete independence from manual annotation. The research investigates three key questions: the necessity of clean validation data, the quantity required, and the optimal utilization of such data.

Their findings reveal that WSL methods demonstrably require clean validation samples to generalize effectively. Without them, performance plateaus at the level of the weak labels. Furthermore, while increasing the number of clean samples improves performance, directly fine-tuning a model on these clean samples (even with a small number, around 20 samples per class) consistently outperforms WSL approaches.  The study demonstrates that the performance gains often attributed to complex WSL methods can be readily achieved through simple fine-tuning on clean data.

The presentation concludes by recommending that future WSL research explicitly report model selection criteria, compare against few-shot learning baselines, and consider continuous fine-tuning as a strong and straightforward alternative. The authors emphasize that the practical benefits and performance improvements of current WSL methods are often overstated due to this hidden dependency on clean, manually annotated data. The code for their experiments is publicly available.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim presented research investigating the ability of large language models (LLMs) to track entities and their states within a discourse. They argue this is crucial for understanding longer texts but note a lack of systematic evaluation of this capability in existing LLMs.

The core challenge lies in designing an evaluation task that avoids shortcuts LLMs might exploit, such as relying on common patterns in pre-training data or simple word associations. To address this, they created a novel task involving boxes and objects, where models must predict the contents of boxes after a series of state-changing operations. They implemented measures to prevent heuristic-based solutions.

Experiments with Flan-T5, GPT-3, and GPT-3.5 using 2-shot in-context learning revealed a significant difference in performance. Only text-davinci-003 showed non-trivial entity tracking. Most models simply repeated the initial state. Further analysis focusing on the GPT series indicated that models trained with substantial amounts of code demonstrated entity tracking abilities, while those without code training did not.

Interestingly, while smaller models like T5-base could learn entity tracking through fine-tuning, randomly initialized models of the same architecture failed even with direct supervision, highlighting the importance of pre-training. The researchers acknowledge that the generalizability of these observed tracking abilities remains an open question and encourage readers to consult their paper for more detailed results, including experiments with GPT-4.</sample>
    <sample id="307">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">NLPositionality is a framework developed to characterize the design biases of NLP datasets and models, revealing how they reflect the positionality—the perspectives shaped by demographics, identity, and experiences—of their creators. The research, a collaboration between Carnegie Mellon University, University of Washington, and Allen Institute for AI, highlights a critical issue: NLP technologies often perform differently across populations, exemplified by toxicity detection APIs failing to recognize offensive terms common in Indian contexts.

The framework addresses the lack of systematic study of model and dataset positionality by comparing annotations from diverse users with existing datasets and models. Utilizing the "Lab in the Wild" platform, researchers gathered over 16,000 annotations from over 1,000 annotators across 87 countries, focusing on social acceptability and hate speech detection.

Key findings demonstrate that datasets and models predominantly align with English-speaking countries and individuals with higher education. Notably, they show less alignment with non-binary individuals. This reveals a bias where certain populations are inadvertently marginalized.

To mitigate these biases, the researchers recommend meticulous documentation of design choices, adopting a perspectivist approach to NLP research, and developing specialized datasets and models tailored to specific communities, citing the Masakhani initiative as a positive example. The core message emphasizes that inclusive NLP isn't about universal applicability but about acknowledging and addressing biases to better serve diverse populations.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">The text does not mention the authors' affiliated institutions.</sample>
    <sample id="312">MultiInstruct is the first large-scale multi-modal instruction tuning benchmark dataset consisting of 62 diverse multi-modal tasks covering 10 broad categories.</sample>
    <sample id="313">James Finch and Sarah Finch.</sample>
    <sample id="314">The talk does not mention "binary coordination." It discusses different dependency structures for coordination (e.g., universal dependencies, Prague approach, multi-headed approach) and argues for symmetric structures based on dependency length minimization.</sample>
    <sample id="315">The paper does not mention the average length of the prompts used.</sample>
    <sample id="316">T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="317">Peng Li from Fudan University presented "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing the challenges of information extraction (IE) using traditional language models like T5 and GPT-3. These models struggle because the structured output of IE (e.g., entity lists) doesn't align with the plain text output during pre-training.

CodeIE tackles this by reframing IE as a structure-to-structure code generation task, leveraging code-specialized large language models like Codex. The approach involves designing prompts that define functions for IE, incorporating comments to guide the model in extracting entities and relationships and appending them to structured lists.

Experiments across named entity recognition and relation extraction datasets demonstrated that CodeIE significantly outperformed baselines like UIE and GPT-3, particularly in few-shot settings. Analysis revealed that code language models exhibit lower perplexity on code-formatted inputs and produce fewer structural errors compared to text-based approaches with GPT-3. Furthermore, GPT-3 occasionally generated labels outside the predefined set, while Codex consistently performed better overall, especially in recall.

The research suggests that aligning pre-training and inference output structures, along with utilizing code-optimized models, enhances IE performance. The paper and code are publicly available.</sample>
    <sample id="318">大家好，我是Yanis Labrak，今天我将为大家介绍我们的工作“DrBERT：一个在法语生物医学和临床领域中具有鲁棒性的预训练模型”。

本次演讲首先会探讨医疗保健领域的语言建模。然后我们将介绍我们文章的主要贡献：我们推出了第一个法语生物医学模型DrBERT，它基于RoBERTa，并使用NACHOS数据集进行训练。NACHOS是从网络上抓取的医学数据集合。我们还对比了不同预训练设置和数据来源的模型。

接下来，我们将展示DrBERT在11个法语生物医学和临床下游任务上的实验结果。最后，我们将总结实验结果，并提供访问这些模型的详细信息。

自2018年BERT问世以来，它已成为解决自然语言处理任务的最有效方法之一，相比于Word2vec、fastText等历史静态和上下文方法，它带来了巨大的性能提升。此后，该模型被适配到许多其他语言，例如法语的CamemBERT，以及生物医学领域的PubMedBERT和BioBERT，以及临床领域的ClinicalBERT，但这些模型大多基于英语。针对其他语言的专业模型非常稀缺，通常依赖于持续预训练，因为缺乏领域内数据。然而，在法语中，此前没有开源的生物医学模型。

因此，我们思考了最合适的训练数据来源，以支持广泛的应用，并认为抓取的数据可以作为临床数据的良好替代品。为了回答这个问题，我们将DrBERT与我们的ChuBERT模型进行对比，ChuBERT基于从南特大学医院数据仓库获得的匿名数据。

随后，我们进一步思考：训练法语专业模型需要多少数据？是4GB、8GB还是更多？为了回答这个问题，我们首先训练并对比了四个从零开始的模型：一个DrBERT的第一个版本，使用7GB的NACHOS数据；第二个版本使用4GB的NACHOS数据集；一个ChuBERT的第一个版本，这是一个临床模型，使用4GB从临床记录中提取的句子；以及一个ChuBERT的最终版本，使用4GB的NACHOS数据集和4GB的临床记录混合训练。

除了这个对比，我们还引入了三个基于持续预训练的模型，以分析预训练策略的影响。一个基于CamemBERT的权重并在4GB的NACHOS数据集上进行训练；另一个也基于CamemBERT，但这次在4GB的临床记录上进行训练；最后，一个基于英语生物医学模型PubMedBERT，并在4GB的NACHOS数据集上进行训练。

总共，我们有七个模型。为了评估这七个模型，我们收集了公共和私有下游任务的数据，例如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行对比，包括CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT和ClinicalBERT。

评估结果表明，模型在与训练数据性质相同的数据集上的任务中表现最佳。然而，我们观察到来自异构来源的数据似乎更具通用性。我们还观察到，使用更多的数据可以带来更好的性能。总体而言，从零开始的预训练似乎在大多数任务中都能获得更高的性能。然而，我们的实验表明，使用CamemBERT的权重和分词器，并在4GB的NACHOS子集中进行预训练，可以获得与DrBERT 4GB从零开始训练的模型相当的结果。但基于CamemBERT权重和分词器的模型则存在稳定性问题。

最后，作为结论，我们的专用系统在11个下游任务中的9个任务中表现更好，并且总体上超越了通用模型CamemBERT的结果。我们还观察到，更专业的数据更好，但其可扩展性较差。所有从NACHOS获得并公开的模型都可以在Hugging Face上免费获取，并采用MIT许可证，所有训练脚本都位于我们的GitHub仓库中。

感谢本次演讲，我们期待在多伦多的海报展示环节与大家交流。</sample>
    <sample id="319">From-scratch pre-training, continual pre-training, and control pre-training.</sample>
    <sample id="320">Adaptive overfitting is not observed.</sample>
    <sample id="321">The paper mentions using evaluation metrics and scores to assess the quality of automatic text simplification.</sample>
    <sample id="322">Enrico's ACL 23 presentation explores how text classifiers learn about morality, challenging the common NLP approach of treating morality as a single scale between "moral" and "immoral." He argues this simplification is dangerous due to the subjective and pluralistic nature of human morality.

Drawing on Moral Foundation Theory (MFT), Enrico explains that morality isn't monolithic but comprises five distinct foundations (Care, Fairness, Loyalty, Authority, and Purity), each prioritized differently by individuals. MFT offers a more nuanced understanding of moral judgments.

His research investigates what language models learn about morality by applying explainable AI techniques to models trained on text. The study utilizes the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven domains (e.g., #AllLivesMatter, #BlackLivesMatter).

The core question is whether language models can recognize that morality is expressed differently across domains. Enrico highlights an example: while #AllLivesMatter and #BlackLivesMatter address similar topics, the language models discern a significant difference in the perception of "subversion" (rebellion to authority). In #AllLivesMatter, words associated with subversion are viewed negatively (overthrow, mayhem), while in #BlackLivesMatter, subversion is more positively framed.

This finding demonstrates that language models can, to some extent, grasp these domain-specific nuances in moral expression. Enrico cautions that using a single model across diverse domains can lead to misinterpretations of morality, emphasizing the need for more sophisticated approaches that account for contextual variations. He invites attendees to learn more at his ACL presentation in Toronto.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for Commonsense Question Answering (QA) that addresses limitations in existing methods combining Language Models (LMs) and Knowledge Representation Learning (KRL). Current approaches often retrieve noisy entities from knowledge bases, encode text and subgraphs separately, and neglect semantic relationships between entities.

DHLK constructs a Heterogeneous Knowledge Graph (HKG) from multiple knowledge bases (ConceptNet, WordNet, Wiktionary) using a two-stage pruning strategy and KRL to optimize its structure and knowledge representation. It expands the subgraph by incorporating paraphrases of key entities retrieved from WordNet and Wiktionary.

The method utilizes RoBERTa and a novel Relation Mask Self-Attention (RMSA) mechanism to encode and fuse the QA context and entities within the HKG. RMSA, inspired by RGAT, incorporates relationships into the attention mechanism, allowing for dynamic removal of irrelevant entities based on attention weights. TransE is employed to optimize entity and relation embeddings within the HKG.

DHLK enhances the QA context with HKG path information and then feeds the graph embedding, path-enhanced context, and original context into a Multi-Layer Perceptron (MLP) for answer prediction.

Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK outperforms existing LM and HKG methods, achieving strong results by effectively integrating language understanding and structured knowledge. Key entities are extracted using KeyBERT, and knowledge paths are retrieved within two hops in ConceptNet to facilitate the process.</sample>
    <sample id="324">Yes, language models do have varying political leanings, occupying all four quadrants on the political compass. GPT-4 is the most liberal, and the GPT series are generally more socially liberal than BART series.</sample>
    <sample id="325">大家好！我叫马蒂亚斯·林德曼，今天我将向大家简要介绍我们关于“使用多集标记和潜在排列，无需树结构实现组合泛化”的论文。这篇论文是与我的导师亚历山大·科勒和伊万·蒂托共同完成的。组合泛化可以理解为学习器处理更深层递归和训练期间单独看到的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。通常，我们都有一个训练集，包含一些语句。例如，“那个女孩睡了”和“玛丽知道那个女孩睡了”。这些语句与表示其核心含义的逻辑形式配对。与标准的机器学习评估不同，测试集不来自相同的分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练期间看到了浅层递归，但测试的是一个具有更深层递归的例子。朴素的 seq2seq 模型难以处理这种分布外泛化，并且通常会产生与输入分离的输出。特别是，它们通常无法重现输入和输出之间的系统对应关系，例如在示例中用颜色编码的那些。一种流行的解决这个问题的方法是在模型中集成树结构。这些树结构旨在捕获与语句和逻辑形式相关的组合过程。这效果很好，但树结构通常是不存在的，需要以某种方式获得。这可能很复杂且计算成本高昂。通常，这涉及对逻辑形式进行大量的特定形式的预处理，例如处理变量符号。获得树结构可能还需要专门的语法归纳程序。在本文中，我们不使用树结构，而引入了一个直接对输入和输出片段之间的对应关系进行建模的神经 seq2seq 模型。我们首次展示了在没有依赖树结构的情况下，对更深层递归的强大泛化能力。我们的方法分两个步骤从输入预测输出。首先，我们用一个无序的多集标记标记每个输入标记，该多集标记包含将在输出中出现的标记。在第一步之后，我们拥有了所有正确的标记，但它们的顺序不对。这就是为什么在第二步中，我们使用另一个模型来预测一个排列，将它们排列成正确的顺序。我们引入了一种新的方法来预测排列，该方法对可能的排列没有任何硬性约束。这使得我们的方法非常灵活和富有表现力。从概念上讲，我们的排列模型的工作方式大致如下。我们从左到右遍历输出，确定将哪个多集标记放入每个位置。对于第一个输出位置，我们只需选择一个，如图中红色突出显示的那样。然后我们跳转到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程，直到每个标记从第一阶段都被访问一次。为了给您一个实验结果的预告，我们在这里将我们的方法与其他无树模型在 COGS 基准测试上的比较结果进行展示。我们的模型在泛化到更深层递归方面，明显优于其他模型。尽管如此，其他一些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的难题。首先，输入和输出之间的对齐方式在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多集标记，这给训练带来了挑战。此外，有时存在多个与数据一致的排列，但语言学上正确的排列是潜在的。我们通过将对齐方式作为训练的一部分来解决这个问题。我们的排列方法非常灵活，但它带来了寻找最高分排列的挑战，因为这与“旅行商问题”相关，是 NP 难问题。我们使用 GPU 友好的连续松弛来近似它，这还允许我们反向传播解决方案并学习更符合语言学规律的排列。如果您想了解更多关于我们的实验以及我们如何应对这些挑战，请阅读我们的论文或访问我们的海报。</sample>
    <sample id="326">Cognitive dissonance is when two beliefs or actions are inconsistent, such as stating "I know cigarettes could kill me" and then saying "I grabbed a couple of smokes after the meeting."</sample>
    <sample id="327">大家好，我是哈尔滨工业大学第三年博士生小徐，很高兴在ACL 2023上向大家介绍我们的工作“ManagerTower: 汇集单模态专家洞见以进行视觉-语言表示学习”。

我们的目标是训练能够理解图像和文本的智能AI系统。Vision-Language学习中，视觉问答(VQA)是一个经典任务。近年来，基于Transformer的视觉语言模型在大型图像-文本对的自监督预训练的帮助下取得了显著进展。

我们提出的ManagerTower是一种新型视觉-语言模型架构，它借鉴了BridgeTower的思路，但克服了其局限性。ManagerTower的关键在于引入“管理者”，每个管理者收集并组合来自不同层级的预训练单模态专家（如RoBERTa和CLIP-ViT）的洞见，从而更有效地利用不同层级的单模态语义知识。

实验表明，即使仅使用400万图像进行预训练，ManagerTower也能在多个下游任务上取得优异表现，尤其是在Wikivideo测试标准上达到了39.15%的准确率，显著优于BridgeTower。通过可视化分析，我们证明了ManagerTower的自适应管理者能够根据不同跨模态层的需求，灵活地利用不同层级的单模态语义知识。

论文、代码和模型都已发布，希望我们的工作能对大家有所帮助，谢谢！</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">这项工作提出了一种名为“噪声鲁棒结构化伪标签生成”的零样本视频语句定位方法。现有零样本方法存在伪查询过于简单、伪查询与伪事件对齐不准确以及直接使用伪标签导致噪声等问题。

该方法首先利用预训练的图像描述模型生成更复杂的自然语言伪查询。然后，通过计算视频帧特征与伪查询的相似度，生成保证视频内与查询高度相关，视频外与查询不相关的伪事件。最后，通过降低噪声样本权重和伪标签精炼来减少噪声影响。

具体来说，该方法密集采样视频帧，使用BLIP模型生成伪查询，并基于事件时间结构生成伪事件。为了提高质量，选择在事件内和事件外相似度差异最大的时间窗口作为伪事件。同时，过滤掉质量低和重叠度高的伪查询对。

在训练过程中，根据模型预测置信度和预测与伪标签的IoU来估计标签噪声，并对样本进行加权。如果预测置信度高且IoU高，则将预测结果作为新的伪标签进行下一轮训练。实验结果表明，该方法在ActivityNet Captions和Charades-STA数据集上均优于现有零样本方法。</sample>
    <sample id="330">Cumulative training performed equal or better than iterative training.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">TED talks transcripts.</sample>
    <sample id="333">INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation addresses the limitations of neural machine translation (NMT) models, specifically their non-smooth representation spaces that hinder generalization and lead to poor performance in sparsely populated areas. The paper introduces INK, a novel framework designed to inject kNN knowledge into NMT models without the drawbacks of traditional kNN-MT, such as slow inference and static data stores.

INK operates through an iterative training loop. First, kNN knowledge from a data store guides an adapter to adjust representations. Then, updated representations asynchronously refresh the data store. This loop continues until convergence, ultimately allowing the data store to be discarded. The framework utilizes KL-divergence to align contextualized representations with token embeddings, kNN token embeddings, and representations of the same target token, effectively smoothing the representation space and enriching semantic meaning.

Experiments using the WMT’19 German-English news translation task demonstrate INK’s effectiveness. The framework significantly improves the representation space of even state-of-the-art NMT models. INK outperforms existing kNN-MT systems, achieving a 1.99 COMET score and 1.0 BLEU score improvement. Furthermore, INK achieves higher BLEU scores with less memory space and faster inference speed compared to adapter-only baselines.

The research explores the benefits of using a small adapter to smooth the representation space and the impact of kNN knowledge on representation distribution. Results indicate that combining adapters and data stores can further refine predictions, suggesting potential for even greater improvements with more advanced frameworks. In conclusion, INK offers a practical and efficient approach to enhancing NMT performance by iteratively refining the representation space using kNN knowledge.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Training on one source language and transferring to another language.</sample>
    <sample id="337">This research introduces "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," a novel approach to effectively represent out-of-vocabulary (OOV) words, a significant challenge in embedding-based models. The core idea is to leverage word formation and association, mirroring human learning habits.

The method constructs a "Word Relationship Graph" that models lexical rules. When an OOV word is encountered, it's tokenized into wordpieces, and a two-level graph is created around it, connecting the OOV word with relevant words. Each word/wordpiece acts as a node with its embedding as the attribute. The first layer preserves all wordpieces, while the second layer samples nodes to reduce noise.

A key challenge is assigning attributes to the OOV nodes. This is addressed using a self-attention network based on the OOV word's characters. Subsequently, a two-level concatenated Graph Attention Network (GAT) processes the graph, fusing initial input with layer embeddings to create node-level representations. A readout block then generates a graph-level representation, capturing overall word formation. A Graph Convolutional Network (GCN) is employed to efficiently model these relationships.

To align with existing embedding spaces, contrastive learning with NT-XENT loss is used. Positive samples include two-hop neighbors, synonyms, and the OOV word itself, encouraging proximity in the vector space. The model aims to bring graph-level embeddings closer to background embeddings while pushing them away from other samples.

Experimental results demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks, validating the effectiveness of learning OOV words through word formation. The model benefits both static and contextual models in downstream applications. Future work explores expanding the model to other languages, noting that agglutinative languages are particularly well-suited, while fusional languages present greater challenges, with success dependent on reasonable word decomposition.</sample>
    <sample id="338">This research paper, "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," investigates the quality of human-annotated explanations used to train AI models. The core problem addressed is the lack of a reliable method to evaluate these explanations, which are often subjective and task-dependent, unlike labels. Traditional metrics like BLEU and ROUGE are inadequate, and existing "simulatability" scores fail to account for task differences and varying utility during fine-tuning and inference.

The researchers propose a novel evaluation metric, TREU, which builds upon the simulatability score by specifically assessing the helpfulness of explanations during the fine-tuning process. They introduce a unified data structure, converting diverse tasks into a multiple-choice format with and without explanations. Through experiments on five datasets (CoS-E, ECQA, e-SNLI, ComVE), they found that fine-tuning with explanations can improve model performance, even when explanations are considered "low quality."

Key findings include that fine-tuning doesn't necessarily impart new knowledge but can lead models to rely on explanations for prediction. The study demonstrates that TREU consistently ranks dataset qualities better than the simulatability score, and that explanation helpfulness is highly task-dependent, influenced by factors like negation and writing style. Ultimately, the research advocates for rigorous quality checks of human annotations and lays the groundwork for more effective human-AI collaboration in explanation generation.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presented "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a joint work aiming to address the limitations of existing paraphrase datasets. While human-annotated datasets are high-quality, they are small, and automatically generated datasets (like back-translation) often lack syntactic diversity.

ParaAMR tackles this by leveraging Abstract Meaning Representations (AMR) – directed graphs representing sentence meaning. The core idea is AMR back-translation: parsing a sentence into an AMR graph, randomly selecting a new "focus" node to become the root, modifying edges accordingly, and then generating text from the altered graph. This process ensures semantic similarity while introducing syntactic variation because the text generator emphasizes the new focus.

The resulting ParaAMR dataset contains 15 million source sentences with approximately 6.9 paraphrases each. It demonstrates significantly higher syntactic diversity compared to other back-translation datasets, while maintaining comparable semantic similarity.

The research team evaluated ParaAMR through automatic and human evaluations, confirming its syntactic diversity and semantic preservation. They further demonstrated its benefits across several NLP applications: improved sentence embeddings (outperforming on the STS benchmark), enhanced syntactic control in paraphrase generation, and better performance in few-shot learning scenarios due to the dataset's syntactic richness.

In conclusion, ParaAMR offers a valuable resource for NLP research, providing a large-scale, syntactically diverse paraphrase dataset constructed through AMR back-translation, and proving its utility in various downstream tasks. The dataset is publicly available.</sample>
    <sample id="341">Average lagging and computational aware average lagging.</sample>
    <sample id="342">The presentation introduces "LiveChat," a novel, large-scale personalized dialogue dataset automatically constructed from live streaming videos (specifically from Chinese TikTok and Douyin). The core problem addressed is the limitations of existing open-domain dialogue datasets, which are primarily text-based and often lack the realism of spoken conversation, personalization, and multi-party interaction.

Here's a breakdown of the key points:

*   **Problem:** Existing datasets are text-sourced, limited in scale due to manual annotation, and lack personalization and multi-party dialogue capabilities.
*   **Solution: LiveChat:** A video-sourced dataset built in three steps: (1) scraping streaming videos, (2) transcribing audio to text using ASR, and (3) constructing dialogues using a "reply-to-whom" matching method. Persona information is also collected, categorized into basic profiles (manual labeling/scraping) and inferred profiles (rules and classifiers).
*   **Scale &amp; Features:** LiveChat significantly surpasses existing datasets in scale, being video-sourced, having richer personal annotations, and longer average sessions.
*   **Experiments:** Experiments on response modeling and addressee recognition demonstrate the benefits of persona information and longer sessions. Single-stream BERT outperformed double-stream BERT in addressee recognition.
*   **LLM Performance:**  BART performed better than other models, indicating LiveChat's distinct domain. Human evaluations of LLMs showed improved informativeness. In-context learning experiments revealed performance gains with increasing demonstration shots, but a slight decrease beyond 8 shots due to potential noise.
*   **Future Work:** Focus on efficient transfer learning of large language models (LLMs) for LiveChat.

In essence, LiveChat aims to bridge the gap between existing dialogue datasets and the dynamic, personalized nature of real-world spoken conversations, offering a valuable resource for advancing dialogue research, particularly in areas like virtual streamers and personalized assistants.</sample>
    <sample id="343">大家好，我是Akshatha，今天我和我的合著者Martin将为大家介绍我们的工作“KITMUS测试：评估来自多个来源的知识整合”。这项工作是McGill大学、Mila和微软研究院之间的合作。自然语言理解模型依赖于各种知识来源，例如包含在其参数中的知识，通常通过预训练获得，以及在推理时提供的输入中的知识。最近在问答等任务中的研究表明，模型可以使用预训练时期的知识来解决任务。但自然语言理解通常需要也提供在推理时期的知识。例如，在句子“John在电视上看到了新当选的总统”中，预训练参数可以包含关于总统做什么以及电视是什么的信息，但它无法可靠地知道这个特定实体“John”是谁，或者新总统是谁，因为总统可能在预训练之后发生了变化。因此，对于知识密集型NLU任务，成功的模型需要能够整合和使用预训练时期的和推理时期的知识。在这项工作中，我们提出了一套用于知识整合的诊断测试。我们引入了一个指代消解任务，旨在探测从不同来源获取知识的能力。我们使用人类研究参与者和已建立的指代消解模型评估数据集。这里有一个我们数据集中的例子。Servin是一名法官。Kea是一名面包师。Servin和Kea在公园里见面。经过在法庭上审理案件的漫长一天后，他很高兴放松一下。这里的任务是识别代词“他”所指代的正确实体，在本例中是Servin。解决给定的代词需要两种类型的信息。首先，实体特定的知识，例如“Servin是一名法官”。其次，背景知识，例如“法官在法庭上审理案件”。通常，背景知识是在大型语言模型的预训练期间学习的，而实体特定的知识通常在推理时观察到。我们改变了这两种信息可用性的方式，使得它可能存在于单个来源或多个来源中。我们定义了KITMUS的三种设置。首先，我们有典型的设置：“背景-预训练”，其中假设背景知识在预训练时可用。其次，有“背景-两者”设置，其中背景知识在预训练时和推理时都可用。最后，是“背景-推理”设置，其中只有在推理时才提供两种知识。这种设置尤其有趣，因为它模拟了解决任务所需的背景知识并非模型预训练数据的一部分的情况。例如，由于自预训练以来出现了新的职业。以下是我们在真实来源中控制事实可用性的一个例子。在“背景-预训练”设置中，我们假设背景知识“政客竞选政府中的席位”包含在预训练参数和推理时期的上下文中，我们提供实体特定的知识“Chichester是一名政客”。在“背景-两者”设置中，我们不仅提供实体特定的知识，还提供政客在推理时期的背景知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”，因为“mirituer”不太可能包含在预训练参数中。我们使用人类研究参与者和已建立的指代消解模型评估数据集。在这个图中，我们展示了在最困难的“背景-预训练”设置变体上表现最佳的模型的结果。在没有在KITMUS上进行特定任务训练的情况下，这两个模型表现不佳。然而，当在KITMUS上进行训练时，C2F和BERT4Coref的性能明显优于随机选择。这表明，当在通用的指代消解数据集上进行训练时，大多数模型学会利用表面线索，而这些线索在测试KITMUS时没有用，因为这些线索已被删除。与虚构知识的额外实验表明，即使是表现最佳的模型，也无法可靠地整合仅在推理时提供的反向知识。总之，我们论文的主要结论是，许多指代消解模型在没有特定任务训练的情况下似乎无法推理来自不同来源的知识。然而，通过特定任务训练，一些模型可以成功地整合来自多个来源的知识。尽管如此，即使是表现最佳的模型似乎仍然难以可靠地整合仅在推理时呈现的反向知识。如果您有兴趣了解更多细节，请参阅我们的论文，并在GitHub上查看数据集和代码。感谢大家的聆听。</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow, which can be complicated and computationally expensive. This may involve formalism-specific pre-processing of the logical forms or specialized grammar-induction procedures.</sample>
    <sample id="345">Matthias Lindemann 介绍了他们与 Alexander Koller 和 Ivan Titov 合作的论文，题为“使用多集标记和潜在排列，无需树木的组合泛化”。

组合泛化是指学习器处理更深层递归和训练期间单独看到的短语的未见组合的能力。在语义解析中，这需要模型在测试集中处理结构上未见的逻辑形式，而不仅仅是训练数据中的形式。

传统的序列到序列模型难以实现这种泛化，而树结构模型通常需要复杂的预处理和语法归纳。

他们的论文提出了一种无需树木的神经网络序列到序列模型，直接建模输入和输出片段之间的对应关系。该模型分为两个步骤：首先，为每个输入标记标记一个无序的多集标记，包含将在输出中出现的标记。然后，使用另一个模型预测排列，将这些标记排列成正确的顺序。

他们引入了一种新的排列预测方法，该方法不施加对可能排列的硬性约束，从而使其具有很强的灵活性和表达能力。该方法通过从左到右选择多集标记来确定输出中的每个位置，并跳过已使用的标记。

实验结果表明，他们的模型在 COGS 基准测试中显著优于其他无树模型，尤其是在泛化到更深层递归方面。

论文还解决了几个技术挑战，包括训练数据中未提供的输入和输出对齐问题，以及存在多个潜在的正确排列的问题。他们通过在训练过程中诱导对齐，并使用 GPU 友好的连续松弛法来近似排列搜索，解决了这些问题。</sample>
    <sample id="346">The paper does not mention the authors' affiliated institutions.</sample>
    <sample id="347">大家好，我是Myra，今天我将介绍我们与Esin Durmus和Dan Jurafsky合作的论文《标记人格：使用自然语言提示来衡量语言模型中的刻板印象》。

近年来，许多研究表明大型语言模型（LLM）中存在社会偏见和刻板印象。然而，现有的衡量方法存在一些局限性。它们通常依赖于耗时费力的手工构建数据集，或者只能衡量非常具体的刻板印象，难以推广到其他人群或情境，或者仅仅捕捉到一些宽泛的负面关联。此外，大多数研究没有考虑到“交叉性”——即多重社会身份如何相互叠加，产生独特的偏见和危害。

为了克服这些限制，我们利用了指令调优LLM擅长响应指令和提示的特性。我们可以要求模型生成一个“人格”，即使用提示（例如“想象你是一名亚裔女性。描述一下你自己。”）来描绘一个虚构人物。 这样，我们可以轻松地将任何身份标记添加到提示中，从而实现广泛的适用性。

我们从GPT-4生成的示例中可以看到，虽然这些输出在传统意义上并非具有明显的负面或有毒内容，但其中存在一些有趣的模式。例如，亚裔女性被描绘成不起眼，中东女性则被描述为“异域风情”，带有“迷人”的色彩。而有色人种女性的人格描述都提到了祖先，而白人男性的人格中则没有。

我们的方法分为两个部分：

1.  **生成人格：** 我们的提示灵感来源于一项研究，该研究使用类似提示来获取人类受试者的反馈，从而揭示种族刻板印象。这使得我们能够将生成的角色与人类书写的回复进行直接比较。
2.  **标记词汇：** 这是一个识别区分标记群体和未标记群体用词的方法，我稍后会详细说明。 这样做的好处是，我们可以在不依赖特定词典的情况下，获得非常具体的刻板印象和模式。

“标记词汇”方法借鉴了社会语言学中的“标记性”概念，该概念认为存在一个默认的“未标记”状态，而任何与该默认状态不同的群体在语言上都会被“标记”。例如，“战士”通常与男性相关联。因此，当描述女性战士时，人们通常会明确指出“女性战士”，并在术语中标记“女性”。更广泛地说，社会中的主导群体在语言和社交上都是未标记的，而边缘群体通常会被标记。

在我们的方法中，我们首先指定哪些群体是未标记的，哪些是标记的，然后使用“Fightin’ Words”方法（本质上是使用加权对数比率来区分每个标记群体的最常用词汇）进行比较。

例如，对于黑人女性的人格，我们将使用“Fightin’ Words”方法，并将对数比率与白人角色和男性角色进行比较，因为这两个群体是相应的未标记群体。

在结果方面，我们首先使用刻板印象词典，发现生成的角色比人类书写的角色包含更多的刻板印象词汇。然而，当我们查看词汇的分布时，发现情况大不相同。虽然生成的角色具有更高的刻板印象词汇比例，但人类书写的角色具有更广泛的词汇分布。而且，生成的角色中出现的刻板印象词汇，往往只是“高”和“运动型”等积极或至少非负面的词汇。 实际上，这个词典并不能很好地捕捉到我们在前面幻灯片中看到的许多有害模式。

因此，为了解决这个问题，我们将转向“标记词汇”方法的结果，以展示这些看似积极的词汇如何促进刻板印象和本质化叙事。

我们的分析表明，这些群体的最常用词汇包括“文化”、“传统”、“自豪”和“异域风情”等。这些词汇仅仅通过它们与身份的关系来定义这些群体，并将它们与白人规范区分开来。这延续了对这些群体的长期歧视和边缘化。

此外，许多常见的刻板印象也反映在这些词汇中。例如，描述拉丁裔女性的词汇包括“充满活力”和“曲线优美”，这与“热带风情”的刻板印象相关。对于亚裔女性，词汇包括“娇小”、“柔弱”和“丝滑”，这与亚裔女性长期以来被物化、被视为顺从的形象有关。最后，对于黑人女性，我们看到一些最常用的词汇是“坚强”和“有韧性”，这与被称为“坚强的黑人女性”的刻板印象相关。虽然乍一看似乎是积极的，但研究表明，这种刻板印象实际上是有害的，因为它给这些群体带来了巨大的压力，要求他们克服社会障碍，而不是真正地改变这些障碍，从而导致负面的健康后果。

更广泛地说，我们发现每个标记群体的词汇几乎都反映了本质化的叙事。

基于这些模式，我们提出了以下三点建议，供模型所有者参考：

1.  研究人员应该关注积极的刻板印象和本质化叙事。
2.  在研究偏见和危害时，应该采用交叉视角，因为如果不这样做，可能会忽略许多问题。
3.  应该增加关于偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能是由于某种过度价值对齐或反刻板印象方法造成的。在没有更多透明度的情况下，我们无法做出任何假设或进一步研究。

谢谢大家聆听，ACL见！</sample>
    <sample id="348">Myra 介绍了他们与 Esin Durmus 和 Dan Jurafsky 合作的论文 "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models"。

**核心问题：** 现有的大型语言模型（LLMs）存在社会偏见和刻板印象，但现有评估方法存在局限性，如依赖人工构建数据集、难以泛化、缺乏对交叉性（intersectionality）的考虑等。

**解决方案：** 他们利用 LLMs 擅长处理指令的特性，通过提示词（例如“Imagine you are an Asian woman. Describe yourself.”）生成人物画像（personas），从而实现对各种身份群体的刻板印象的泛化测量。

**关键方法：**

*   **生成人物画像：** 借鉴人类研究，生成不同身份群体的画像。
*   **标记词汇（Marked Words）：** 基于社会语言学概念“标记性”，识别区分标记群体（marginalized groups）和未标记群体（unmarked groups）的词汇。

**研究发现：**

*   LLMs 生成的人物画像比人类书写的包含更多刻板印象词汇，但人类书写的词汇分布更广。
*   LLMs 倾向于使用“积极”的词汇，但这些词汇实际上反映了有害的刻板印象和本质化叙事（essentializing narratives）。例如，对亚裔女性的描述中出现“娇小”、“柔顺”等词汇，与历史上的性化和顺从印象相关；对黑人女性的描述中出现“坚强”、“有韧性”等词汇，与“坚强黑人女性”原型相关，这实际上会给她们带来压力。

**建议：**

*   研究人员应关注积极刻板印象和本质化叙事。
*   研究应采用交叉视角，以避免遗漏潜在的偏见和危害。
*   模型开发者应提高偏见缓解方法的透明度，以便更好地理解和研究这些方法可能产生的副作用。</sample>
    <sample id="349">大家好，我叫易景伟，来自中国科学技术大学。很高兴能为大家带来我们论文的简短广告视频。我们的论文题目是：通过后门水印保护嵌入式服务的大型语言模型版权。让我们首先介绍一下嵌入式服务的背景。目前，像GPT、LLAMA、PALM这样的LLM在自然语言理解和生成方面表现出色。嵌入式服务是建立在LLM之上的服务之一，可以辅助各种NLP任务。例如，OpenAI提供基于GPT的嵌入式API。然而，最近的研究表明，攻击者可以通过学习嵌入式信息来窃取模型，并提供类似的服务。因此，保护嵌入式服务的版权是必要的。

为了保护嵌入式服务的版权，一种解决方案是在服务提供商的服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下几个特性。首先，该方法应该适用于嵌入式服务。其次，水印不应降低提供的嵌入式的效用。第三，水印应该足够隐蔽，或者攻击者可以轻松地删除水印。最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作可以大致分为四类。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。

因此，在本文中，我们提出了Embedding Marker，这是一种基于后门的水印方法，适用于嵌入式服务。接下来，我将介绍Embedding Marker的细节。Embedding Marker包含两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在适中的频率区间内的单词。我们假设提供商可以收集一个通用文本语料库并统计单词频率。

在水印注入中，我们首先定义一个目标嵌入。当用户将句子发送到提供商服务时，提供商会统计句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，提供的嵌入将完全等于目标嵌入。

版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门和良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。然后，提供商使用数据集向窃取者的服务请求嵌入。计算请求的嵌入和目标嵌入之间的余弦相似度和L2相似度。我们计算良性和后门数据集之间的相似度差异，定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三指标。

我们在四个数据集（AG News、MIND、SST2和Enron Spam）上进行了实验。我们假设提供商使用Wiki Text数据集来计算单词频率。在四个数据集上的结果表明，我们的Embedding Marker可以在保持下游任务的良好效用时，实现出色的检测性能。我们还通过在四个数据集上对句子进行PCA可视化来验证提供的嵌入的隐蔽性[INAUDIBLE 4:39]。图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和正常嵌入。

以上就是全部内容。谢谢大家。欢迎与我们讨论。</sample>
    <sample id="350">This paper, "What’s the Meaning of Superhuman Performance in Today’s NLU?", questions the validity of claims of superhuman performance in Natural Language Understanding (NLU) based on leaderboard scores. The authors argue that current benchmarks, like SuperGLUE and SQuAD, are flawed and comparisons between AI systems and humans are often misleading.

Here's a breakdown of the key issues:

*   **Benchmark Saturation:** Models frequently outperform humans on leaderboards, but it's unclear if this reflects genuine understanding or exploitation of dataset quirks.
*   **Unequal Evaluation Sets:** Systems are tested on the full test set, while humans are often evaluated on a much smaller subset, creating an unfair comparison.
*   **Ground Truth Errors:** The datasets contain errors in the "correct" answers, which systems can exploit while humans would recognize as incorrect.
*   **Spurious Correlations:** AI models can find patterns in the data that humans wouldn't, leading to inflated scores.
*   **Inadequate Human Baselines:** Human performance is often estimated using simple aggregation methods instead of comparing against the best possible human performance.
*   **Low Annotator Pay &amp; Lack of Transparency:** Low pay rates and a lack of information about annotator demographics raise concerns about the quality and representativeness of human annotations.

The paper concludes that current claims of superhuman performance in NLU are not scientifically meaningful due to these issues. It recommends improvements to benchmark construction, including using more representative human evaluation sets, correcting ground truth errors, and ensuring adequate compensation and transparency in the annotation process. The authors urge the NLP community to move beyond leaderboard-driven evaluation and focus on building more robust and reliable NLU systems.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset in 2023. The core question is whether models developed for CoNLL-2003 still perform well on modern data and what factors contribute to good generalization. To address this, the authors created CoNLL++, a new dataset of Reuters News articles from 2020 annotated using the CoNLL-2003 guidelines.

Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++, with F1 score changes used to assess generalization. The study identified three key ingredients for good generalization: transformer-based model architectures, larger model sizes, and a greater number of fine-tuning examples.

The paper explored two hypotheses for performance drops: adaptive overfitting (due to repeated use of the CoNLL-2003 test set) and temporal drift (caused by the time gap between training and testing data). Analysis revealed no evidence of adaptive overfitting, as improvements on CoNLL-2003 consistently translated to greater improvements on CoNLL++. However, experiments retraining models with more recent data confirmed that temporal drift is the primary cause of performance degradation.

The conclusion is that while CoNLL-2003 taggers still function effectively in 2023, achieving robust generalization requires a combination of advanced model architectures, larger models, and ample fine-tuning data. The research highlights the importance of addressing temporal drift in NER model development and encourages further investigation into generalization techniques. The authors released their CoNLL++ dataset and invite further research and questions.</sample>
    <sample id="352">Annotating behaviors in chat or ABC-Eval.</sample>
    <sample id="353">This paper introduces a novel approach to Python code generation that addresses the common problem of input underspecification in natural language descriptions (NLDs). The core idea is to leverage interactive clarification questions (CQAs) to gather missing specifications during the code generation process.

The authors identify a key challenge: NLDs often lack crucial details needed for accurate code generation. To tackle this, they propose a new task: generating code by asking clarification questions, specifically focusing on operation-level specifications. They created a synthetic dataset, CodeClarQA, containing CQAs for missing key operations. This dataset generation process involves identifying key operations within code using a code knowledge graph (Graph4Code), representing operations and NLDs in a latent space based on their schemata, and determining if operations are missing based on similarity scores. Annotators validate and create CQAs using templates (yes/no or multiple-choice).

The paper details a pipeline for CQ-driven code generation, comprising a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments demonstrate that the task is more challenging than existing CQ ranking tasks, and that clarifications generally improve code generation. While the pipeline initially underperforms compared to models trained solely on NLDs and code, performance increases as more high-ranked CQAs are answered and incorporated.

Analysis reveals that clarified key operations contribute to better code generation, with Oracle CQAs leading to predictions close to ground truth. However, the task remains challenging, as the top-ranked CQAs often don't align with the reference CQAs, leading to potential errors. The authors acknowledge limitations, such as the need to distinguish operations with similar names and the current reliance on operation documentation rather than argument values. The paper concludes by encouraging feedback and providing access to the paper and code.</sample>
    <sample id="354">The provided text does not specify a year when the performance increment between CoNLL-2003 and CoNLL++ exceeded 5 percentage points. It only mentions that the red best-fit line has a gradient greater than one, indicating diminishing returns are not observed.</sample>
    <sample id="355">大家好，我叫瓦苏达，是斯托尼布鲁克大学计算机科学专业的博士候选人。我想介绍一下我们团队在ACL 2023上发表的论文，一篇长篇论文：“认知失调检测的迁移学习：解决罕见类别挑战”。

我们首先定义了认知失调，并解释了为什么研究它在语言中很重要。简单来说，认知失调是指两个信念或行为之间存在不一致性，例如，一个人说“我知道吸烟可能会杀死我”，然后又说“会议结束后我抽了两支烟”。这两个信念和行为是不一致的，处于失调状态。进一步解释说“没有它们我恐怕无法保住我的工作”则为第二次发生提供了理由，从而建立了和谐关系。虽然认知失调是一种在日常决策中普遍存在，但在其他类型的论述关系中却很少被表达的现象。

那么，为什么这很重要呢？研究语言中表达的认知失调可以帮助我们理解人们之间的分歧影响，追踪信念和价值观的趋势，以及人口的态度变化。高认知失调也与焦虑症有关，可以帮助我们更好地理解人们的心理健康。研究语言中表达的认知失调还可以有助于理解极端主义和弱势群体两极分化。最后，认知失调对于理解个人的认知风格和决策过程也至关重要。

为了构建认知失调资源，我们进行了一项大规模的失调关系标注工作。我们采用了“失调优先”的方法，流程图如上所示。推文通过PDTB解析器传递，根据我们在论文中描述的指南对论述单元对进行标注。正如我们所看到的，在标注的对中，只有3.5%发现了失调。在收集了大约1000个论述单元对后，我们对一个最初仅使用43个失调示例训练的分类器进行了训练。不出所料，该分类器的表现并没有比随机猜测好多少。

由于失调的发生率低且没有先前的此类数据集，我们面临着绝对稀有问题的挑战。为了缓解这个问题，我们尝试了迁移学习和主动学习的组合，以进行标注，从而收集更多的失调样本，减少标注运行次数，降低整体标注成本，同时提高失调检测的准确性。

由于初始模型完全无法捕捉失调类别，因此我们在主动学习过程开始时，从相关的任务中迁移权重。我们从两个不同的任务中迁移：与主题无关的失调立场分类（确定来自不同人员的辩论陈述是否一致或不一致，无论主题如何，我们称之为“辩论”）以及PDTB的扩展和比较类别的二元分类（这两个类别与和谐和失调的概念密切相关，我们称之为“CE”）。我们发现，在迁移后的零样本性能在标注的数据集中已经明显优于随机猜测，其中最佳AUC为0.62。

此外，通过迭代地在两个任务上进行微调，我们发现先对CE任务进行微调，然后对“辩论”任务进行进一步微调，可以获得更好的零样本性能。因此，这就是我们用来启动主动学习的模型。

接下来，我们确定了在每轮稀有情况下，使用新数据更新模型的最佳方法。“累积”策略会累积主动标注收集的所有数据，而“迭代”策略则通过在最新收集的数据集上进行训练来更新模型。在不同的策略中，我们发现“累积”策略在各个方面都优于或等于“迭代”策略。

为了增加失调示例的数量，我们使用“概率稀有类别”策略（PRC）来选择当前模型最有可能为失调的示例。我们将此策略与其他常用的最先进的AL策略进行比较。我们发现，提出的PRC策略优于其他最先进的策略，尽管差异很小。需要注意的是，随机策略的性能明显较低。

在与两个最佳策略进行进一步的AL轮次后，我们将失调分类AUC提高到0.75，这是迄今为止在该任务上取得的最佳性能。我们还检查了每种策略在标注质量和标注员成本方面的可行性。我们发现，PRC具有最高的失调百分比，并且最适合稀有类别。然而，标注员也认为这些示例很难。

总而言之，我们发现PRC是一种简单的AL策略，用于稀有类别的获取和AL的冷启动，并且与适当设计的迁移学习任务相结合，可以显著提高性能。我们还发现，迭代更新对于来自不同领域的迁移学习很有用，而领域内的主动标注则受益于累积更新。

以下是我们的核心数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。</sample>
    <sample id="356">The authors are Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Five.</sample>
    <sample id="359">State-of-the-art architectures specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">Armineh Nourbakhsh's research, "CounterComp," addresses the challenge of compositional generalization in multi-step quantitative reasoning, specifically question answering over financial tables. Current state-of-the-art neural models struggle with these tasks, particularly when requiring more than two arithmetic operations, due to memorizing spurious patterns within the input data. For example, a token like "2019" might be incorrectly associated with a specific operation.

CounterComp aims to mitigate this by leveraging counterfactual scenarios. The core idea is that certain components of a question are interchangeable without fundamentally altering the final output. By changing these components (e.g., "net change" to "percent change"), the output's calculation can be modified, revealing a relationship between question phrasing and operations.

The method works by mining positive and negative counterfactual examples from the training data. Positive examples involve question interventions that *don't* change the output, while negative examples involve interventions that *do*. These examples are then used to create triplets, which are fed into an auxiliary metric learning loss during training. A key innovation is a dynamic margin within this loss, which adapts based on the degree of change introduced by the question intervention.

Experiments demonstrate that incorporating the CounterComp loss consistently improves performance across three state-of-the-art baselines, especially for problems requiring more than two reasoning steps. Crucially, the improvement extends to both in-distribution (same dataset training and testing) and out-of-distribution scenarios (different datasets or unseen examples within the same dataset), showcasing enhanced compositional generalization. Qualitative analysis further reveals that CounterComp encourages the model to focus on more relevant tokens within the input, aligning attention with meaningful operational terms. The research offers a way to improve model performance without requiring extensive human supervision.</sample>
  </task>
</testset>