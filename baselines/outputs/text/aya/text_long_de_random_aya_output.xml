<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind **große Web-Crawl-Datensätze**, insbesondere solche, die **politische Nachrichtenmedien** umfassen. Zu diesen Quellen gehören unter anderem der C4 Corpus, der New York Times, Los Angeles Times, The Guardian und die Huffington Post. Diese Daten ermöglichen es den Modellen, aus verschiedenen Perspektiven zu lernen, können aber auch zu sozialen Vorurteilen und potenziellen Fairnessproblemen in Downstream-Aufgaben führen, wenn die politischen Neigungen in den Modellen nicht sorgfältig überwacht werden.</sample>
    <sample id="1">Die Autoren, Akshatha und Martin, sind mit der folgenden Universität verbunden:

- **McGill University**
- **Mila** (ein Forschungszentrum für künstliche Intelligenz)
- **Microsoft Research**</sample>
    <sample id="2">Die Präsentation behandelt ein neuartiges Pre-Training-Modell namens **LayoutMask** für die Verarbeitung visuell reicher Dokumente (VrDU), wie Formulare, Quittungen und Plakate.

Aktuelle Modelle für die Dokumentenvorverarbeitung kämpfen mit der Reihenfolge, in der Text gelesen wird, da sie globale 1D-Positionen verwenden, die nicht die komplexen Layouts und Beziehungen innerhalb von Dokumenten erfassen. LayoutMask adressiert dieses Problem, indem es sich auf Text- und Layoutinformationen konzentriert und "lokale 1D-Positionen" verwendet, die in-Segment-Token-Reihenfolgen darstellen.

Das Modell integriert zwei innovative Maskierungsstrategien in die gängige Aufgabe des Maskierten Sprachmodellierens:

1. **Whole Word Masking:** Maskiert Wörter statt Token, zwingt das Modell, mehr Kontext zu nutzen und fördert so Text-Layout-Interaktionen.
2. **Layout-Aware Masking:** Erhöht die Wahrscheinlichkeit, dass die ersten und letzten Wörter eines Segments maskiert werden, was das Modell dazu zwingt, über Segmentgrenzen hinweg zu denken.

Darüber hinaus führt LayoutMask ein neues Pre-Training-Objekt namens **Masked Position Modeling** ein, das die Modellierung von 2D-Positionen und semantischen Beziehungen kombiniert.

Die Experimente zeigen, dass LayoutMask, insbesondere mit lokaler 1D-Positionierung, in Benchmarks wie FUNSD und SROIE bessere Leistungen erbringt als Modelle mit globaler 1D-Positionierung. Die Ergebnisse belegen die Wirksamkeit der vorgeschlagenen Ansätze zur Verbesserung der Text-Layout-Interaktionen und der Layout-Darstellungen in Dokumentenmodellen.</sample>
    <sample id="3">## Präsentation von DEPLAIN: Ein neues Korpus für die Textidentifikation auf Dokument- und Satzebene

**Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textvereinfachung auf Dokument- und Satzebene in deutscher Sprache.**

Mein Name ist Regina Stodden und ich führe Sie heute durch den ersten Teil der Präsentation.

**Was ist Textvereinfachung?**

Textvereinfachung ist ein Prozess, bei dem ein Text angepasst wird, um seine Verständlichkeit für eine spezifische Zielgruppe zu verbessern, z.B. für Menschen mit Leseproblemen oder Nichtmuttersprachler. Um ein Modell für die Textvereinfachung zu trainieren, benötigen wir parallele Paare von Texten, z.B. Dokumente oder Sätze. Ein Beispiel zeigt einen komplexen deutschen Satz und seine Übersetzung in eine einfache Sprache, die beide parallel ausgerichtet sind. Zur Vereinfachung eines Satzes stehen verschiedene Techniken zur Verfügung, wie lexikalische Substitution, Satzteilentfernung, Umordnung oder Einfügung von Wörtern.

**Probleme mit bestehenden Korpora**

In den letzten Jahren gab es einige Herausforderungen bei bestehenden Korpora für die Textvereinfachung:

* **Größe:** Viele Korpora sind zu klein, um ein effektives Modell für die Textvereinfachung zu trainieren.
* **Automatische Ausrichtung:** Drei kürzlich vorgeschlagene Korpora basieren auf automatischen Ausrichtungen, was zu Fehlern führen kann.

**Das DEPLAIN-Korpus**

Daher präsentieren wir DEPLAIN, ein neues Korpus, das in zwei Unterkorpora aufgeteilt ist:

* **DEPLAIN-apa:** Basierend auf Nachrichtenartikeln. 483 Dokumente wurden manuell ausgerichtet, was etwa 13.000 parallele Satzpaare ergibt.
* **DEPLAIN-web:** Enthält Texte aus verschiedenen Bereichen. 750 Dokumente wurden sowohl manuell als auch automatisch ausgerichtet. Insgesamt 30.450 Satzpaare.

Eine detailliertere Analyse unserer Satzpaare zeigt Unterschiede in der Art der Vereinfachung:

* Biblische Texte sind deutlich stärker vereinfacht als Nachrichtenartikel oder Texte für Sprachlerner.
* Auf allen Ebenen, einschließlich lexikalischer Vereinfachung, Strukturvereinfachung und dem Gesamtniveau der Vereinfachung, ist eine große Vielfalt an Vereinfachungsumwandlungen zu beobachten.

**Anwendungsfälle für DEPLAIN**

DEPLAIN bietet vielfältige Möglichkeiten:

* **Bewertung von automatischen Ausrichtungsmethoden:** Da wir über manuell ausgerichtete Satzpaare verfügen, können wir diese als "Goldstandard" verwenden, um die Leistung verschiedener automatischer Ausrichtungsmethoden zu bewerten. Wir haben Anpassungen an bestehende Methoden vorgenommen und alle Codes und Anpassungen in unserer Veröffentlichung zur Verfügung gestellt. 
Unsere Ergebnisse zeigen, dass die Methode MASSalign die beste Leistung für die Textvereinfachung in deutscher Sprache erzielt.

* **Automatische Textvereinfachung:** Wir haben zwei Sprachmodelle feinabgestimmt, um komplexe Texte in vereinfachte Versionen umzuwandeln:
    * **long-mBART:** Für die Dokumentebene.
    * **mBART:** Für die Satzebene.

Die feinabgestimmten Modelle erzielten bessere Ergebnisse als die Basismodelle, und wir schlagen diese als neue Benchmark für die automatische Textvereinfachung vor.</sample>
    <sample id="4">Der/die Referent*in heißt Kayo Yin.</sample>
    <sample id="5">Das Modell, das eine Genauigkeit von 82–87 % erreichte, ist der T5 XL (Text-to-Text Transfer Transformer) in Kombination mit teilweise überlappendem Hintergrundwissen, das das Modell selbst abgerufen hat.</sample>
    <sample id="6">In ihrer Präsentation "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" stellen die Forscher um Jiaan eine neue Herangehensweise an die mehrsprachige und kreuzsprachige Zusammenfassung von Texten vor. Sie bezeichnen diese Methode als "viele-zu-viele Zusammenfassung" (many-to-many summarization), die darauf abzielt, ein einziges Modell zu entwickeln, das Dokumente in jeder Quelle Sprache zusammenfassen und die Zusammenfassung in jeder gewünschten Zielsprache generieren kann.

Die Studie vergleicht drei Ansätze: mehrsprachige Zusammenfassung, kreuzsprachige Zusammenfassung und die neue viele-zu-viele Methode. Durch Experimente mit dem WikiLingua-Datensatz auf mehreren Sprachen wurde festgestellt, dass die viele-zu-viele Zusammenfassung die effektivste Transferleistung von Aufgabenkenntnissen zwischen Sprachen bietet.

Als Beitrag präsentieren die Autoren PISCES, ein vorab trainiertes viele-zu-viele Zusammenfassungsmodell. PISCES durchläuft ein dreistufiges Training: Meta-Vorabtraining, kreuzsprachiges Vorabtraining und aufgabenbezogenes Vorabtraining. Diese Methode übertrifft laut den Ergebnissen andere Basismodelle wie mBART-50 und mT5.

Die Präsentation betont die Bedeutung der vielen-zu-viele Zusammenfassung als allgemeine Lösung für mehrsprachige Zusammenfassungsprobleme und hebt die Wirksamkeit des vorgeschlagenen PISCES-Modells hervor, das vielversprechende Ergebnisse in verschiedenen Tests gezeigt hat. Die Forscher laden dazu ein, ihre Arbeit zu lesen, um mehr über die Feinheiten der Implementierung und die erzielten Ergebnisse zu erfahren.</sample>
    <sample id="7">Ja, laut der in der Präsentation vorgestellten Forschung funktionieren CoNLL-2003-Tagger (Named Entity Recognition Modelle) immer noch gut im Jahr 2023. Die Studie zeigt, dass diese Modelle, obwohl sie seit fast 20 Jahren verwendet werden, durch die Kombination einer besseren Modellarchitektur, größerer Modellgrößen und mehr Feinabstimmungsdaten eine gute Generalisierung auf moderne Daten erreichen können.</sample>
    <sample id="8">Die vorgeschlagene Methode, ABC-Eval, ist neu, weil sie die Bewertung von Chat-Modellen durch die explizite Annotation von Verhaltensweisen in den Gesprächen reduziert. Anstatt nur allgemeine Bewertungen oder Likert-Skalen zu verwenden, identifiziert ABC-Eval spezifische Verhaltensmuster wie Irrelevanz, Widersprüche, Halluzinationen und Empathiemangel, um eine präzisere und zuverlässigere Bewertung von Chat-Qualität zu ermöglichen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Lernansatzes (Weakly Supervised Learning, WSL) hängt von der Verfügbarkeit **sauberer, manuell annotierter Validierungsdaten** ab. 

Unsere Forschung zeigt, dass WSL-Methoden ohne diese sauberen Daten eine erhebliche Leistungseinbuße erfahren und dass die bisher behauptete hohe Leistung oft nur durch den Zugriff auf saubere Daten erreicht wird.</sample>
    <sample id="10">Das Ergebnis kann durch mehrere Maßnahmen verbessert werden:

1. **Erweiterung und Vielfalt des Trainingsdatensatzes**: Der aktuelle Datensatz umfasst 6.000 alternative Fragen und 42.000 indirekte Referenzen. Eine Erweiterung und Diversifizierung des Datensatzes könnte die Genauigkeit erhöhen, indem mehr Variationen und komplexere Szenarien abgedeckt werden.

2. **Verbesserung der Hintergrundinformationen**: Die Qualität und Tiefe der bereitgestellten Hintergrundinformationen spielen eine entscheidende Rolle. Eine genauere und umfassendere Bereitstellung von Kontext könnte die Leistung der Sprachmodelle weiter steigern.

3. **Feinabstimmung der Sprachmodelle**: Durch spezifische Feinabstimmung der Sprachmodelle (z.B. T5 XL) auf die Aufgabenstellung und den Datensatz kann die Genauigkeit erhöht werden. Dies könnte durch zusätzliche Trainingsdaten oder spezifische Hyperparameter-Optimierung erreicht werden.

4. **Integration von Kontextualisierungstechniken**: Die Implementierung fortschrittlicher Kontextualisierungstechniken, wie z.B. das Einbeziehen von Dialogkontext oder die Nutzung von Transformer-basierten Modellen, die auf Kontext angewiesen sind, könnte die Leistung verbessern.

5. **Evaluierung und Optimierung der Bewertungsmetriken**: Eine sorgfältige Überprüfung und Optimierung der verwendeten Bewertungsmetriken kann helfen, die tatsächliche Leistungsfähigkeit der Modelle besser zu erfassen und zu verbessern.</sample>
    <sample id="11">Jack Hessel, Forschungswissenschaftler bei AI2, präsentierte auf ACL seine Arbeit über die Bewertung des Humorverständnisses großer Sprachmodelle, basierend auf Daten aus dem New Yorker Caption Contest.

Obwohl große Modelle wie ChatGPT und Googles PaLM in der Lage sind, Witze zu generieren und sogar zu erklären, zeigt Hessel, dass sie Humor nicht wirklich verstehen. Als Beispiel führt er ein ChatGPT-Output für einen "Pineapple"-Knock-Knock-Witz an, bei dem das Modell zwar ein Wort einfügt, aber keinen echten Sinn ergibt und sogar behauptet, ein Wortspiel (Pun) zu sein.

Um dies strukturierter zu untersuchen, nutzt Hessel den New Yorker Caption Contest, bei dem Leser witzige Beschreibungen für Cartoons einreichen. Er definiert drei Aufgaben: Matching (richtige Beschreibung auswählen), Qualitätsklassifizierung (beste Beschreibung auswählen) und Erklärungsgenerierung (Warum ist der Witz lustig erklären).

Die Ergebnisse zeigen, dass selbst hochentwickelte Modelle wie CLIP und GPT-4 mit großen Abständen zu menschlichen Leistungen auf diesen Aufgaben zurückbleiben. Hessel hebt die Herausforderungen bei der Verarbeitung visueller Informationen und dem Verständnis subtiler humorvoller Nuancen hervor.

Er stellt eine neue Datensammlung mit über 700 Cartoons und 650 Erklärungen zur Verfügung, um zukünftige Forschung zu fördern. Hessel betont die Bedeutung dieser Arbeit für das Verständnis der Grenzen und Möglichkeiten von KI in Bezug auf Humor.</sample>
    <sample id="12">An der Arbeit sind **fünf** Autoren beteiligt: Dawei (der Präsentierende), Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">**Titel: Finding the SWEET Spot: Verbesserung adaptiver Inferenz in Low-Resource-Umgebungen**

Mein Name ist Daniel Rotem, und ich präsentiere meine Arbeit, die im Labor von Professor Roy Schwartz an der Hebräischen Universität in Jerusalem durchgeführt wurde. Adaptiver Inferenz ist eine Methode, um die Verarbeitungszeit großer Sprachmodelle zu reduzieren. Wir nutzen die Variationen der Realwelt-Daten und setzen für einfachere Fälle kleinere Modelle ein, um die durchschnittlichen Inferenzkosten zu senken.

Es gibt zwei gängige adaptive Inferenzmethoden: Multi-Modell und Early Exit. Beim Multi-Modell werden mehrere Modelle trainiert und sequenziell für die Inferenz verwendet. Early Exit hingegen integriert mehrere Klassifikatoren in das Modell, die zusammen trainiert werden und die Berechnung bei einem bestimmten Punkt beenden.

Unsere Analyse zeigt, dass Multi-Modell flexibler und kosteneffizienter ist, aber mit Overhead und höheren Speicherkosten verbunden ist. Early Exit bietet schnellere Inferenz, ist aber anfällig für "konfliktierende Gradienten", da geteilte Modellparameter die Leistung beeinträchtigen können.

Unsere Forschung bestätigt, dass Multi-Modell-Klassifikatoren im Vergleich zu Early Exit bessere Ergebnisse liefern. Wir stellen das SWEET-Verfahren (Separating Weights in Early Exit Transformers) vor, das dieses Problem löst, indem es die Modellschichten nur mit Gradienten aus der nachfolgenden Klassifikatorin aktualisiert.

SWEET verbessert die Leistung bei schneller Inferenz und übertrifft beide Methoden bei BERT-Large über den gesamten Geschwindigkeits-Genauigkeits-Kurven. Unsere Arbeit beleuchtet die Herausforderungen adaptiver Inferenz und eröffnet neue Wege für die Optimierung von Early Exit-Architekturen.</sample>
    <sample id="14">## Die Abhängigkeitsstruktur der Koordination: Ein Plädoyer für Symmetrie

Mein Name ist Adam Przepiórkowski und in meinem Vortrag geht es um die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gehen verschiedene Theorien und Korpusansätze von unterschiedlichen Strukturen der Koordination aus.

**Beispielsweise:**

* **Universal Dependencies:** Hier wird die gesamte Koordination von dem ersten Konjunkt (z.B. "Lisa" in "Lisa, Bart und Maggie") dominiert.
* **Mel'čuks Theorie der Bedeutungstextes:** Ähnlich wie bei den Universal Dependencies wird auch hier die gesamte Koordination vom ersten Konjunkt geleitet.

Diese **asymmetrischen** Ansätze setzen ein Konjunkt als Hauptteil der Struktur fest.

Im Gegensatz dazu gibt es **symmetrische** Ansätze:

* **Prague-Ansatz:** Hier wird die Koordination von **jeder** Konjunktion ausgingen, d.h. wir erhalten Abhängigkeiten vom Gouverneur zu allen Konjunktionen.
* **Hudson's Word Grammar:** Hier gilt ebenfalls, dass alle Konjunktionen Köpfe der Koordination sind, und wir erhalten Abhängigkeiten vom Gouverneur zu jeder Konjunktion einzeln.

Mein Vortrag zielt darauf ab, **neue Argumente für symmetrische Koordinationenstrukturen** wie diese beiden zu liefern und **asymmetrische Strukturen** wie die obigen zu widerlegen.

**Mein Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge:**

Wir wissen, dass direkte Objekte in der englischen Sprache nahe am Verb stehen sollten, während Adjunkte weiter entfernt sein können. "Marge las es gestern" ist grammatikalisch in Ordnung, während "Marge las gestern es" unangenehm klingt.

Dieses Prinzip kann jedoch umgangen werden, wenn das direkte Objekt sehr lang ist und an Position nach dem Adjektiv gestellt werden kann.

**Beispiel:**

"Marge las dieses absolut faszinierende Buch über Bienen gestern."

Hier ist "es" (das direkte Objekt) durch "dieses absolut faszinierende Buch über Bienen" ersetzt worden, was länger ist als "es", aber die Satzstruktur verbessert.

Die **Länge der kritischen Abhängigkeiten** (die nicht konstant zwischen den beiden Strukturen sind) wurde in meiner Arbeit analysiert.

**Ergebnisse:**

* Links stehende Konjunkte neigen dazu kürzer zu sein (gemessen in Silben und Wörtern).
* Je größer der Längenunterschied zwischen den Konjunkten, desto wahrscheinlicher ist es, dass das kürzere Konjunkt links steht.

**Neu an meiner Arbeit:**

Wir haben festgestellt, dass diese Tendenz **nur auftritt, wenn der Gouverneur links steht oder fehlt**.

Wenn der Gouverneur links steht, wie in "Ich sah Bart und Lisa", wächst die Präferenz für ein kürzeres linkes Konjunkt mit steigendem Unterschied in der Wortlänge.

Fehlt der Gouverneur, wie in "Homer kam und nieste", verschwindet dieser Effekt.

Unsere Ergebnisse zeigen, dass **die Abhängigkeitslänge bei symmetrischen Koordinationen minimiert wird**, während asymmetrische Strukturen diese Tendenz **verletzen**.

Weitere Details und eine detailliertere Argumentation finden Sie in meiner Arbeit. Kommen Sie uns gerne an unserem Posterstand fragen!</sample>
    <sample id="15">An der Arbeit sind drei Autoren beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Basierend auf dem englischen Inhalt des Vortrags, scheint es, dass **Bibeltexte** im Vergleich zu anderen Textdomänen stärker vereinfacht werden. Die Analyse der Satzpaare zeigt, dass Bibeltexte auf allen Ebenen (lexikalische Vereinfachung, Strukturvereinfachung, Gesamtniveau der Vereinfachung) deutlicher vereinfacht sind.</sample>
    <sample id="17"># Multimodale Relation Extraktion: Überwindung von Herausforderungen durch Informationsverfeinerung und -ergänzung

Diese Arbeit präsentiert einen neuartigen Ansatz für die multimodale Relation Extraktion (MRE), der Text und visuelle Daten kombiniert, um komplexe Beziehungen in realen Szenarien besser zu verstehen. Die Herausforderungen bestehen darin, relevante Informationen effektiv zu filtern und externe Wissensquellen zu nutzen.

Der vorgeschlagene Ansatz umfasst zwei Hauptkomponenten:

1. **Graph Information Bottleneck (GIB) für feinere Informationsverfeinerung:** GIB optimiert die Fusion von Text- und Bilddarstellungen in einem gemeinsamen Graphen (CMG), indem es redundante Informationen entfernt und die nützlichsten Knoten und Kanten beibehält.

2. **Multimodale Themeninformation als Kontextverstärkung:** Ein Themenmodell extrahiert verborgene Themen aus Text und Bildern, bereichert den CMG mit diesen Themen und verbessert so den Kontext.

Durch diese Methode wird eine bessere Leistung bei der MRE erzielt, wie Experimente auf einem Standard-Datensatz zeigen. Die Ergebnisse deuten darauf hin, dass die Informationsverfeinerung und -ergänzung in verschiedenen Text-Bild-Relevanz-Kontexten unterschiedlich wirksam sind.

Zusammenfassend bietet diese Arbeit einen innovativen Rahmen für MRE, der interne und externe Informationsquellen effizient nutzt und so die Genauigkeit in der Relation Extraktion verbessert.</sample>
    <sample id="18">Das Beispiel für die Präferenz für kürzere linke Konjunktionen ist die Phrase "I saw Bart and Lisa" im Vergleich zu "Homer came and sneezed". In dem ersten Beispiel, wo "Bart und Lisa" koordiniert werden und der Governor (das Verb "sah") links steht, ist die linke Konjunktion ("Bart") kürzer. Diese Beobachtung bestätigt die Tendenz, dass linke Konjunktionen tendenziell kürzer sind, besonders wenn der Governor links oder abwesend ist.</sample>
    <sample id="19">In ihrer Präsentation für ACL 2023 diskutiert Zhang Qin, Master-Studentin an der Shenzhen University, ihre Forschung zum Thema "A Survey for Efficient Open Domain Question Answering". Ihr Fokus liegt auf der Optimierung von Systemen für offene Domänen-Fragebeantwortung (ODQA), die in Echtzeit und auf ressourcenbeschränkten Geräten funktionieren.

Der aktuelle Standardansatz für ODQA besteht aus einem zweistufigen Modell, das eine Retrieval- und eine Reader-Komponente umfasst. Die Herausforderungen dabei sind die große Größe von Wikipedia-Korpora (26 Millionen Dokumente), die schnelle Indexsuche (65 GB Indexdatei) und die Komplexität großer Sprachmodelle.

Qin und ihr Team untersuchen verschiedene Ansätze zur Verbesserung der Effizienz. Dazu gehören:

* **Schnellere Evidenzsuche:** Durch den Einsatz approximativer Nearest-Neighbor-Suchmethoden anstelle von Brute-Force-Suche.
* **Schnellere Lektüre:** Techniken wie adaptive Berechnung, die das Auslassen unwahrscheinlicher Kontextinformationen ermöglicht.
* **Indexverkleinerung:** Methoden wie Dokumentenfilterung und Dimensionsreduktion.
* **Modellverkleinerung:** Verwendung leichterer Modelle, Parameter-Sharing oder die Zusammenführung von Retrieval- und Reader-Funktionen in einem Modell.

Die Analyse verschiedener ODQA-Modelle zeigt, dass Retrieval- und Reader-Systeme eine gute Balance zwischen Geschwindigkeit, Speicherverbrauch und Leistung bieten. Die Studie schlägt verschiedene Strategien vor, um ODQA-Systeme für ressourcenbeschränkte Umgebungen zu optimieren.

Zukünftige Forschungsrichtungen umfassen die Implementierung auf Low-Power-Geräten und die Entwicklung zusätzlicher Bewertungsmetriken.</sample>
    <sample id="20">Ja, die von Ihnen erwähnten Modelle, einschließlich DrBERT und ChuBERT, sind frei verfügbar und können für Ihre Forschung verwendet werden. Sie sind auf der Plattform Hugging Face und unter der MIT-Lizenz veröffentlicht, und die Trainingsskripte sind auf dem GitHub-Repository der Autoren verfügbar. Dies ermöglicht es Ihnen, die Modelle herunterzuladen, zu nutzen und in Ihren eigenen Projekten zu erweitern.</sample>
    <sample id="21">DEPLAIN-apa basiert auf Nachrichtentexten. Das bedeutet, dass die 483 Dokumente, die manuell ausgerichtet wurden, aus Nachrichtenquellen stammen. Im Gegensatz zu DEPLAIN-web, das verschiedene Domänen abdeckt und sowohl manuell als auch automatisch ausgerichtete Dokumente aus dem Internet enthält.</sample>
    <sample id="22">Basierend auf der Präsentation führen **drei Faktoren** zu einer guten Generalisierung:

1. **Modellarchitektur**: Transformer-Modelle generalisieren oft besser zu neuen Daten.
2. **Modellgröße**: Größere Modelle führen in der Regel zu besserer Generalisierung.
3. **Anzahl der Feinabstimmungsexemplare**: Mehr Feinabstimmungsexemplare verbessern die Generalisierung.</sample>
    <sample id="23">Dan Garrette diskutiert die Herausforderungen bei der Text-in-Bild-Generierung durch Modelle wie Imagen, die trotz ihrer hohen Bildqualität oft Schwierigkeiten bei der korrekten Darstellung von Text haben. Er untersucht die Ursachen und präsentiert Forschungsergebnisse zu den Textencodern, insbesondere T5 und PaLM.

T5, basierend auf SentencePiece-Tokenisierung, kämpft mit der Rechtschreibung, insbesondere bei häufigen Wörtern, da diese oft durch wenige Subwort-IDs dargestellt werden. Größere T5-Modelle erreichen zwar bessere, aber immer noch unbefriedigende Genauigkeiten. Im Gegensatz dazu übertrifft PaLM bei der Rechtschreibung, erfordert aber mehr Ressourcen.

ByT5, ein Modell, das Bytes statt Subwörter verarbeitet, zeigt eine viel bessere Leistung bei der Rechtschreibung. Die Forscher entdeckten, dass häufige Wörter für T5 schwieriger sind, da sie oft durch wenige Token repräsentiert werden. ByT5 hingegen hat direkten Zugriff auf die Zeichendaten und übertrifft dabei T5 unabhängig von der Worthäufigkeit.

Um Text-Bild-Modelle zu verbessern, integrierten die Forscher einen kleinen ByT5-Encoder in das Imagen-Modell, was zu einer besseren Textdarstellung und insgesamt verbesserter Bildgenerierung führte. Obwohl die Diffusion-Modelle noch Fehler einführen können, zeigt diese Strategie ein effizientes Mittel, um die Rechtschreibfähigkeiten von Text-in-Bild-Modellen zu steigern.

Die Arbeit führt zwei Benchmarks ein: WikiSpell für Textmodelle und DrawText für Text-zu-Bild-Modelle, und schlägt vor, dass die Kombination von Modellen, die Zeichendaten verarbeiten, ein vielversprechender Ansatz für die Verbesserung der Textdarstellung in der KI-Bildgenerierung ist.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem verschiedene statistische Analysen der Koordinationsstrukturen aus dem erweiterten Penn Treebank vorgenommen wurden. Die Messungen berücksichtigten die Länge der Abhängigkeiten in Charakteren, Silben und Wörtern. Insbesondere wurde festgestellt, dass bei Koordinationsstrukturen, bei denen der Regent auf der linken Seite oder gar nicht vorhanden ist, die linke Konjunktion tendenziell kürzer ist, je größer der Unterschied in der Länge zur rechten Konjunktion. Bei Koordinationsstrukturen mit einem Regenten auf der rechten Seite verschwindet dieser Effekt.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem verschiedene statistische Analysen und Vergleichsebenen hinsichtlich der Koordinationsstrukturen aus dem erweiterten Penn Treebank extrahiert wurden. Die Forscher untersuchten insbesondere:

1. **Länge der Konjunkte:** Sie verglichen die Längen der linken und rechten Konjunkte in Koordinationsstrukturen, gemessen in Wörtern, Silben und Zeichen.
2. **Position des Gouverneurs:** Sie unterschieden zwischen Szenarien, in denen ein Gouverneur (z.B. ein Verb) auf der linken oder rechten Seite der Koordinationsstruktur steht, und Fällen, in denen kein Gouverneur vorhanden ist (z.B. zwei unabhängige Sätze).

Durch diese Kombination von Messgrößen und Kontexten konnten sie feststellen, dass die Position des Gouverneurs einen signifikanten Einfluss auf die Präferenz für kürzere linke Konjunkte hat. Wenn der Gouverneur auf der linken Seite steht, nimmt die Tendenz zu, dass das linke Konjunkt kürzer ist, je größer der Unterschied in der Länge zur rechten Konjunkt ist. Diese Beobachtung unterstützt die Argumentation für symmetrische Koordinationsstrukturen und spricht gegen asymmetrische Ansätze.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten (wie im Fall von seltenen Klassen wie Dissonanz in Sprache) trainiert wird, leistet in der Regel nicht viel besser als zufällig. Dies liegt daran, dass die Daten nicht ausreichen, um das Modell effektiv zu trainieren, insbesondere wenn die Zielklasse nur eine kleine Minderheit der Daten darstellt.</sample>
    <sample id="27">Basierend auf dem präsentierten Inhalt, ist Shangbin der einzige Autor, der die Arbeit "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" erwähnt. Es wird nicht explizit auf weitere Mitautoren hingewiesen. Daher ist die Antwort **1 Autor**.</sample>
    <sample id="28">Im Beispielgespräch werden die Personen als **Bob** und **Alice** bezeichnet.</sample>
    <sample id="29">Kontextsensitive Modelle schneiden besser ab bei den Diskursphänomenen **Formality** (Formalität) und **Lexical Cohesion** (lexikalische Kohärenz).

Im Gegensatz dazu zeigen kontextagnostische Modelle eine vergleichbare Leistung oder nur geringfügig schlechtere Ergebnisse bei anderen Phänomenen wie **Ellipsen**, **Pronomen** und **Verb Form** (Verbform).</sample>
    <sample id="30"># **LLM-Blender: Ein effektives Ensemble-Lernen für Large Language Models**

Die Präsentation stellt LLM-Blender vor, ein einfaches und effektives Framework für Ensemble-Lernen, das auf Large Language Models (LLMs) angewendet wird. Das Ziel ist es, die Leistung einzelner Modelle zu verbessern, indem man mehrere LLMs kombiniert.

Der Schlüsselansatz besteht aus zwei Stufen. Zunächst werden mehrere LLMs (n) mit einem bestimmten Eingabebeispiel ausgeführt, um verschiedene Ausgaben (Y₁ bis Yₙ) zu erhalten. Anschließend verwendet das System ein Modul namens PairRanker, das ein Paarweise Vergleichsverfahren anwendet, um die Kandidaten zu bewerten. Es lernt, welche Ausgabe für ein bestimmtes Eingabebeispiel besser ist, indem es die Eingabe mit jedem Kandidatenpaar kombiniert und ein Cross-Attention-Modell (z.B. RoBERTa) verwendet.

Die Ergebnisse dieser Vergleiche werden aggregiert, um eine Rangfolge der Kandidaten zu erhalten. In der zweiten Stufe werden die besten Kandidaten (z.B. die Top 3) als Eingabe für ein Sequenz-zu-Sequenz-Modell verwendet, genannt GenFuser, das die endgültige Ausgabe generiert.

Die Forscher heben die Vorteile von PairRanker hervor, das im Gegensatz zu früheren Methoden Kandidatenpaare zusammen betrachtet, anstatt sie einzeln zu bewerten. Dies ermöglicht eine genauere und differenziertere Bewertung. Experimentelle Ergebnisse auf einem neuen Datensatz, MixInstruct, zeigen, dass LLM-Blender in 68-76% der Fälle bessere Ergebnisse liefert als führende Einzelmodelle wie Open Assistant und Vicuna.

Zusammenfassend bietet LLM-Blender eine vielversprechende, einfache Lösung für Ensemble-Lernen in der LLMs-Domäne, mit einem robusten Framework und einem bereitgestellten Datensatz für zukünftige Forschung.</sample>
    <sample id="31">Basierend auf dem vorgestellten Inhalt und den genannten Autoren gehört die Forschungsgruppe den folgenden Universitäten an:

1. **Carnegie Mellon University** (John Gauthier, Aaron Mueller, Kanishka Misra, Roger Levy, Adina Williams)
2. **University of Washington** (Koustav Sinha, Karen Fences)

Diese Universitäten sind bekannt für ihre starken Forschungsprogramme im Bereich der Sprachmodelle und natürlichen Sprachverarbeitung.</sample>
    <sample id="33">Das vorgestellte Framework **NLPositionality** quantifiziert die Positionalität, indem es die Annotationen von Datensätzen mit realen Benutzern vergleicht. Es erreicht dies in zwei Schritten:

1. **Re-Annotation von Datensätzen**: Durch die Einbeziehung einer vielfältigen Gruppe von Annotatoren, die über demografische Daten verfügen, werden viele Annotationen für jede Instanz gesammelt.
2. **Korrelationsanalyse**: Die Annotationen werden dann mit den Vorhersagen von Modellen und den Labels der Datensätze verglichen, indem ein Pearson's R-Korrelationskoeffizient berechnet wird. Dies ermöglicht es, die Übereinstimmung zwischen den Benutzeransichten und den Modellen/Datensätzen zu messen.</sample>
    <sample id="34"># **CREST: Ein Rahmenwerk für rationale Erklärungen und kontrastive Textgenerierung**

Marcos Treviso präsentiert seine Forschung zum "CREST" (Joint Framework for Rationalization and Counterfactual Text Generation), einem Kollaborationsergebnis mit Kollegen. CREST vereint Methoden zur Interpretation von Entscheidungen von Klassifizierern, indem es rationale Erklärungen und kontrastive Textgenerierung kombiniert.

Der Ansatz besteht aus zwei Hauptkomponenten. Erstens generiert ein Modell kontrastive Beispiele (Counterfactuals) durch Maskierung und Ersetzung von Wörtern im Originaltext. Zweitens führt ein rationalisierendes Modell, das trainiert wurde, um sinnvolle Erklärungen (Z) zu erzeugen, eine Entscheidung aus. Diese Erklärungen werden dann für die Weiterverarbeitung verwendet.

Die Qualität der generierten Counterfactuals wurde durch automatische und menschliche Bewertungen überprüft, wobei CREST positiv abschnitt. Die Forscher untersuchten auch die Anwendungsmöglichkeiten, einschließlich Datenverstärkung und eine neue Methode, die sowohl faktische als auch kontrastive Beispiele rationalisiert. Experimente zeigten, dass CREST-Rationalisierung die Leistung auf verschiedenen Datensätzen verbessert, insbesondere bei Out-of-Domain-Daten.

Eine Analyse der Interpretierbarkeit der von CREST generierten Erklärungen ergab, dass sie plausibler und simulierbarer sind als Erklärungen anderer Methoden. Die Einführung eines neuen Metrikkonzepts, "Counterfactual Simulability", unterstreicht die Stärke von CREST bei der Erzeugung aussagekräftiger und wirkungsvoller Erklärungen.

Zusammenfassend bietet CREST einen innovativen Ansatz für rationale Erklärungen und verbessert die Leistung von Downstream-Modellen durch die Nutzung von Counterfactuals. Die Arbeit betont die Bedeutung interpretierbarer Modelle und ihre Auswirkungen auf die künstliche Intelligenz.</sample>
    <sample id="36">Die Präsentation stellt eine neue Methode vor, die als "Language-Specific Layers" (LSLs) bezeichnet wird, um die Effizienz und Leistung bei der mehrsprachigen maschinellen Übersetzung zu verbessern. Ziel ist es, die Kapazität pro Sprache zu erhöhen, ohne die Inferenzkosten zu steigern.

Die Forscher schlagen vor, in einem mehrsprachigen Übersetzungssystem für jede Sprache eine spezifische LSL-Schicht zu haben. Während der Inferenzzeit wählt das System die relevante LSL aus, wodurch nur diese Schicht aktiviert wird. Diese Methode ermöglicht eine konstante Inferenzgeschwindigkeit.

Der Vortrag erläutert den Prozess der LSL-Platzierung. Anstatt durch Trial-and-Error, was exponentiell teuer wäre, lernt das Modell die optimale Platzierung. Jede Encoder-Schicht verfügt über drei Gewichte: ein gemeinsames, ein Quell- und ein Zielgewicht. Das Modell wird mit diesen Gewichten trainiert, um die Bedeutung zu bestimmen. Die größte Gewichtsgröße bestimmt die Art der LSL-Schicht.

Die Experimente umfassten 10 Sprachen, einschließlich europäischer und asiatischer Sprachen sowie Swahili als Low-Resource-Sprache. Die Ergebnisse zeigen, dass die gelernten Architekturen signifikante Verbesserungen gegenüber Sprachadapter- und Basismodellen aufweisen, insbesondere bei Low-Resource-Sprachen. Die Präsentation betont die Effizienz und die statistisch signifikanten Verbesserungen, die durch die vorgeschlagene Methode erzielt werden.</sample>
    <sample id="37">Die vorherige Studie, bei der menschliche Teilnehmende die gleichen Persona-Prompts erhielten, ergab, dass die generierten Persona-Beschreibungen **Rassenstereotype aufwiesen**. Die ForscherInnen stellten fest, dass die menschlichen Antworten, ähnlich wie die von GPT-4 generierten, bestimmte Muster aufwiesen, die auf Stereotype hinwiesen, insbesondere in Bezug auf die Darstellung von Frauen verschiedener ethnischer Hintergründe.

Konkret wurden in den menschlichen Antworten Begriffe wie "unassuming" (unauffällig) für eine asiatische Frau, "exotic" (exotisch) für eine Frau aus dem Nahen Osten und Bezüge auf "Ancestry" (Ahnen) für Frauen mit farbiger Hautfarbe beobachtet, während die Beschreibung eines weißen Mannes keine solchen spezifischen Merkmale enthielt.</sample>
    <sample id="38">In der Studie wurden verschiedene Statistiken über Koordination aus der erweiterten Version des Penn Treebanks extrahiert, wie im Papier "Why Wouldn't You Use Universal Dependencies" beschrieben. Darüber hinaus wurde die Analyse auf Koordinationen in Sätzen aus der Praxis angewendet, um Beobachtungen über die Länge und Position der Konjunkte zu machen.</sample>
    <sample id="39">Basierend auf dem vorgelegten Inhalt spricht Adam Przepiórkowski über seine eigene Arbeit. Es sind somit **keine weiteren Autoren** an der Arbeit beteiligt.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz im Kontext der Arbeit von Vasudha et al. sind:

1. **Debate Stance Classification**: Bestimmt, ob zwei Debaten-Aussagen von verschiedenen Personen übereinstimmen oder widersprechen, unabhängig vom Thema.
2. **Binary Klassifikation von Expansion und Comparison Klassen des PDTB**: Diese Klassen sind eng mit der Konzeption von Konsonanz und Dissonanz verbunden.</sample>
    <sample id="41">**Kurzfassung:**

Das Projekt "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" (Persona-getriebene Allgemeine Wissensgrafik für kohärente und ansprechende Erzählungen) ist eine Zusammenarbeit zwischen der NLP-Forschungsgruppe der EPFL Universität und Sony Group Corporation. Ziel ist es, natürliche Sprachverarbeitungssysteme zu entwickeln, die die Persönlichkeit und Interessen von Sprechern, Zuhörern oder Charakteren in Dialogen und Geschichten verstehen.

PeaCoK ist eine umfassende Wissensgrafik, die etwa 3.800 Personenprofile und 40.000 Attribute umfasst, die komplexe Verbindungen zwischen ihnen herstellen. Die Beziehungen zwischen Personen und Attributen werden in drei Dimensionen unterteilt, einschließlich vier Hauptbeziehungen und Interaktivität/Unterschiedlichkeit. Die Grafik wurde in drei Schritten aufgebaut: Auswahl von Personen aus bestehenden Wissensgrafiken, Induzierung von Attributen aus verschiedenen Quellen und Crowdsourcing der Beziehungsannotationen mit Hilfe eines AI-gestützten Mehrheitsvotings.

Die Studie zeigt, dass PeaCoK als Trainingsdaten für Sprachmodelle effektiv ist. Ein auf PeaCoK trainiertes BART-Modell übertrifft bei der Vorhersage von Persönlichkeitsattributen große Sprachmodelle wie GPT-3 und GPT-3.5.

Darüber hinaus verbessert PeaCoK die Generierung kohärenter und ansprechender Dialoge. Im Vergleich zum Baseline-Modell P²Bot führt die Integration von PeaCoK-Wissen zu besseren Bewertungen in Bezug auf Fließfähigkeit, Konsistenz, Engagement und Persönlichkeitsausdruck. Die Ergebnisse unterstreichen die Bedeutung von PeaCoK's verknüpftem Weltwissen für die Erstellung überzeugenderer Erzählungen.</sample>
    <sample id="42">Basierend auf dem von Ihnen bereitgestellten Text, ist nur ein Autor genannt: Shuheng. Der Text deutet nicht darauf hin, dass es weitere Mitautoren gibt.</sample>
    <sample id="43">Basierend auf dem vorgelegten Text ist nur eine Person, Vasudha, als Autorin des Papiers "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" genannt, das für ACL 2023 akzeptiert wurde. Es wird also von einem einzelnen Autor verfasst.</sample>
    <sample id="44">Das vorgestellte Framework **NLPositionality** unterscheidet sich von bisherigen Arbeiten in mehreren wesentlichen Punkten:

1. **Vergleich mit Endbenutzern:** Während vorherige Studien oft nur auf Annotator-Übereinstimmung oder Modell-Annotator-Verteilungen schauten, vergleicht NLPositionality direkt die Vorhersagen und Labels von Modellen und Datensätzen mit den Annahmen realer Benutzer.

2. **Diversität der Annotatoren:** NLPositionality fokussiert auf die Re-Annotation von Datensätzen mit einer Vielzahl von Annotatoren aus unterschiedlichen Demografien. Dies ermöglicht eine umfassendere Untersuchung von Positionalitäten, da die Original-Datensätze oft nur wenige Annotatoren pro Instanz haben und Demografiedaten selten erfasst werden.

3. **Korrelationsanalyse:** Anstatt nur Annotator-Übereinstimmung zu messen, verwendet NLPositionality eine Pearson's R-Korrelationsscore, um die Übereinstimmung zwischen Benutzer-Annahmen und Modellen/Datensätzen zu quantifizieren.

4. **Online-Crowdsourcing-Plattformen:** Das Framework nutzt Plattformen wie *Lab in the Wild*, die es ermöglichen, eine große Anzahl von diversen Teilnehmern aus verschiedenen Ländern zu rekrutieren, um qualitativ hochwertige Daten zu sammeln.

5. **Fokus auf soziale Ausrichtung:** Im Gegensatz zu vielen früheren Studien, die sich auf objektive Kriterien konzentrierten, untersucht NLPositionality, wie NLP-Modelle und -Datensätze soziale Normen und Werte widerspiegeln, indem sie Aufgaben wie soziale Akzeptanz und Hassrede-Detektion analysieren.</sample>
    <sample id="45">Basierend auf der Präsentation von Myra, wird das Setup, das **die meisten Überschneidungen mit dem Lexikon der Stereotypen aufweist**, als das **generierte Persona-Setup** beschrieben. Die Studie vergleicht die von einem Sprachmodell generierten Personas mit menschlichen geschriebenen Personas und stellt fest, dass die generierten Personas viel häufiger Stereotypen-bezogene Wörter enthalten.

Während die menschlichen Personas eine breitere Palette von Wörtern verwenden, konzentrieren sich die generierten Personas stark auf die im Lexikon der Stereotypen aufgeführten Begriffe, wobei nur wenige zusätzliche oder abweichende negative Stereotypen auftauchen.</sample>
    <sample id="46">In der Präsentation wurden die kommerziellen Systeme **DeepL** und **Google Translate** verglichen. Die Analyse zeigt, dass DeepL in der Regel genauer ist als Google Translate bei der Dokumentenübersetzung auf Dokumentenebene.</sample>
    <sample id="47">##  Von der Vorabdaten bis zu den Sprachmodellen: Aufdeckung der Spuren politischer Voreingenommenheit in NLP-Modellen

Hallo, ich bin Shangbin, Doktorand an der Universität Washington. Heute präsentiere ich unsere Arbeit mit dem Titel: **"Von Vorabdaten bis zu Sprachmodellen bis hin zu Downstream-Aufgaben: Verfolgen der Spuren politischer Voreingenommenheit, die zu unfairen NLP-Modellen führen"**.

Sprachmodelle werden auf riesigen Datenmengen aus dem Webcrawl trainiert. Politische Nachrichtenmedien sind in diesen Daten gut vertreten. Eine Analyse des C4-Korpus zeigt, dass Zeitungen wie die New York Times, Los Angeles Times, The Guardian und die Huffington Post häufig vorkommen. Dies bringt sowohl Vorteile als auch Herausforderungen mit sich. Einerseits können Sprachmodelle aus einer Vielzahl von Perspektiven lernen, was die Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu Ungerechtigkeiten in Downstream-Aufgaben führen.

Um diesen Problemen auf den Grund zu gehen, untersuchen wir die Pipeline der politischen Voreingenommenheit vom Trainingsdaten bis hin zu den Sprachmodellen und ihren Auswirkungen auf Downstream-Aufgaben.

**Unsere Forschungsfragen lauten:**

1. **Wie können wir die politische Ausrichtung von Sprachmodellen bewerten und welche Rolle spielen dabei die Vorabdaten?**
2. **Wie beeinflussen die politischen Neigungen der Sprachmodelle ihre Leistung in Downstream-Aufgaben und führen sie zu Ungerechtigkeiten in NLP-Anwendungen?**

Zu diesem Zweck haben wir folgende Schritte unternommen:

* **Automatisierte Bewertung:** Wir verwenden politische Testfragen, wie den Political Conference Test, um die Neigung von Sprachmodellen zu messen und unsere Ergebnisse mit der politischen Wissenschaft zu untermauern.
* **Untersuchung der politischen Neigung:** Unsere vorläufigen Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen aufweisen. Sie verteilen sich über alle vier politischen Lager. GPT-4 ist dabei das am stärksten linke Modell. GPT-Modelle neigen insgesamt zu einer sozialeren Liberalität als BART-Modelle und ihre Varianten.
* **Einfluss der Vorabdaten:** Um zu untersuchen, ob die politischen Voreingenommenheiten der Modelle aus den Trainingsdaten stammen, haben wir ein kontrolliertes Experiment durchgeführt. Wir haben Sprachmodelle zusätzlich auf sechs parteiische Korpora trainiert, die aus Nachrichten und Social Media stammen und nach ihrer politischen Ausrichtung aufgeteilt sind. Die Ideologie der Modelle verschob sich entsprechend. Zum Beispiel zeigte RoBERTa nach zusätzlichem Training auf einem linke-lastigen Reddit-Korpor ein deutliches linksliberales Einlagen.
* **Politische Polarisierung:** Wir haben auch untersucht, ob Sprachmodelle die in unserer Gesellschaft vorherrschende Polarisierung aufgreifen. Indem wir Trainingsdaten vor und nach der Präsidentschaft von Trump (45. Präsident der USA) trennten, konnten wir feststellen, dass die Modelle nach 2017 tendenziell weiter von der Mitte entfernt waren, was auf eine Übernahme der gesellschaftlichen Polarisierung hindeutet.

**Bewertung der politischen Modelle in Downstream-Aufgaben:**

Wir haben Sprachmodelle mit unterschiedlichen politischen Neigungen auf Aufgaben wie Hassrede- und Falschinformationenerkennung getestet, die häufig NLP-Modelle einsetzen und potenziell weitreichende Auswirkungen haben.

Unsere Ergebnisse zeigen, dass die Leistung der Modelle je nach Zielgruppe und politischer Ausrichtung variiert:

* **Hassrede-Erkennung:** Linke Modelle sind besser bei der Erkennung von Hassrede gegen Minderheiten, während rechte Modelle besser bei der Erkennung von Hassrede gegen weiße Männer sind.
* **Falschinformationenerkennung:** Linke Modelle erkennen Falschinformationen besser aus rechtslastigen Quellen und umgekehrt.

**Diskussion:**

Unsere Arbeit zeigt ein Dilemma bezüglich der politischen Voreingenommenheit von Sprachmodellen auf. Die Entfernung politischer Meinungen aus den Trainingsdaten birgt das Risiko von Zensur oder Ausgrenzung. Es ist schwierig, eine neutrale Auswahl an Trainingsdaten zu treffen. Dies erinnert an das Dilemma des elektrischen Trolley-Problems.

Zusammenfassend lässt sich sagen, dass unsere Forschung die dringende Notwendigkeit aufzeigt, die Auswirkungen politischer Voreingenommenheit in NLP-Modellen ernst zu nehmen und nach Lösungen zu suchen.</sample>
    <sample id="48">Basierend auf dem von Ihnen bereitgestellten Text, sind **zwei** Autoren an der Arbeit beteiligt: David Vilar und seine Kollegen von Google Translate.</sample>
    <sample id="49">Die MPP-Auswertungen (Minimal Pair Paradigmen) wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50"># **DEPLAIN: Ein neues Korpus für die Textvereinfachung in Deutsch**

Die Präsentation stellt DEPLAIN vor, ein neues Korpus für die Textidentifikation und -vereinfachung auf Dokument- und Satzebene in Deutsch. DEPLAIN wurde entwickelt, um die Herausforderungen bestehender Korpora zu überwinden, die oft zu klein sind oder automatisch ausgerichtete Paare enthalten, was zu Fehlern führen kann.

Der Vortrag teilt zwei Subkorpora auf: DEPLAIN-apa, basierend auf Nachrichtenartikeln, mit 483 manuell ausgerichteten Dokumenten und 13.000 Satzpaaren, und DEPLAIN-web, das verschiedene Domänen abdeckt und sowohl manuelle als auch automatische Ausrichtungsmethoden verwendet, was zu 30.450 Satzpaaren führt. Die Analyse zeigt, dass das Bibeltext-Material stark vereinfacht wurde, während Nachrichten- und Sprachlerntexte weniger vereinfacht sind. Das Korpus zeichnet sich durch eine große Vielfalt an Vereinfachungstransformationen aus.

Ein Hauptanwendungsfall ist die Bewertung automatischer Ausrichtungsmethoden. DEPLAIN ermöglicht es Forschern, Methoden zu testen, die Ausrichtungen zwischen Sätzen in Dokumenten mit unterschiedlicher Komplexität vornehmen. Die Studie kam zu dem Schluss, dass MASSalign die beste Methode für die deutsche Textvereinfachung ist. Ein weiterer Anwendungsfall ist die automatische Textvereinfachung durch Feinabstimmung von Sprachmodellen. Die Forscher haben zwei Modelle, long-mBART und base mBART, feinabgestimmt, um Dokument- und Satzebene-Vereinfachungen zu erzeugen, und bessere Ergebnisse als Basismodelle erzielt.

Zusammenfassend bietet DEPLAIN eine wertvolle Ressource für die Forschung zur Textvereinfachung und kann als Benchmark für zukünftige Arbeiten dienen.</sample>
    <sample id="51">Sie haben Datensätze für drei Domains aufgenommen: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven und Vorurteile, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Im Kontext von NLP (Natural Language Processing) und KI-Modellen bedeutet dies, dass die Daten und Algorithmen, die entwickelt werden, unbewusst die Sichtweisen und Erfahrungen ihrer Schöpfer und der dominanten Gruppen in der Gesellschaft widerspiegeln können. Diese Positionalität kann zu systematischen Leistungsunterschieden führen, wie sie beispielsweise zwischen verschiedenen kulturellen oder sprachlichen Gruppen auftreten.

Zusammengefasst: Positionalität ist die Art und Weise, wie die Perspektiven und Einflüsse der Entwickler und Schöpfer von KI-Systemen und Daten in den Algorithmen und Datensätzen zum Ausdruck kommen.</sample>
    <sample id="53">Der/die Referent*in ist Dawei.</sample>
    <sample id="54">## Transfer Learning for Dissonance Detection: A Summary

Cognitive dissonance, the conflict between beliefs and actions, is a common yet rarely expressed phenomenon in language. Understanding it is crucial for gauging public opinion, mental health, and societal polarization.  This work presents a novel approach to detect dissonance in text using transfer learning and active learning techniques.

We address the "rare-class challenge" posed by dissonance's low occurrence (3.5% in our dataset). Our method leverages pre-trained models for topic-independent stance classification and PDTB class classification, fine-tuning them for dissonance detection. We then employ active learning strategies, comparing "cumulative" and "iterative" model updates, and a novel "Probability-of-Rare-Class" (PRC) selection method for acquiring more dissonance examples.

Our experiments demonstrate that PRC outperforms traditional active learning strategies, reaching a dissonance classification AUC of 0.75. We also find that iterative fine-tuning benefits from transfer learning tasks from related domains, while cumulative updates are more effective for in-domain annotations.

This work contributes a valuable resource for dissonance analysis in language, paving the way for further research on its societal implications and enabling more nuanced understanding of human cognition and decision-making.</sample>
    <sample id="55">Ja, EDAtt (Encoder-Decoder Attention) passt zu einem bestehenden Offline-ST-Modell, da es vorschlägt, das bereits vorhandene Modell ohne erneutes Training oder spezifische Architekturänderungen für SimulST (Simultaneous Speech Translation) zu nutzen. Stattdessen wird die Latenz durch spezifische Parameter gehandhabt und die bereits erworbene Kenntnis des Modells durch die Cross-Attention-Mechanik genutzt.</sample>
    <sample id="56">Basierend auf dem präsentierten Inhalt ist Yusen Zhang der einzige Autor, der explizit erwähnt wird. Es wird jedoch angedeutet, dass die Arbeit in Zusammenarbeit mit anderen Forschern von Penn State University durchgeführt wurde, was impliziert, dass es mehrere Autoren gibt, aber keine spezifische Anzahl wird genannt.</sample>
    <sample id="57">Ja, das Modell, das in der Testsuite "KITMUS" bewertet wurde, funktioniert, aber es zeigt Schwächen bei der Integration von Wissen, das erst bei der Inferenzzeit verfügbar ist. Mit spezifischer Training auf der KITMUS-Testsuite verbessern sich die Leistungen der Modelle, was darauf hinweist, dass sie lernen, Wissen aus verschiedenen Quellen zu kombinieren. Dennoch haben selbst die besten Modelle Schwierigkeiten, neues oder "rückwärts" verfügbares Wissen zu integrieren, das nicht im Voraus trainiert wurde.</sample>
    <sample id="58">Die drei Varianten von KITMUS (Knowledge Integration from Multiple Sources Test) sind:

1. **Background-Pretrain**: Hintergrundwissen wird angenommen, dass es im Pretraining verfügbar ist.
2. **Background-Both**: Hintergrundwissen ist sowohl im Pretraining als auch bei der Inferenz verfügbar.
3. **Background-Inference**: Nur fiktives oder nicht im Pretraining enthaltenes Hintergrundwissen ist bei der Inferenz verfügbar.</sample>
    <sample id="59"># DrBERT: A Robust Pre-trained Model for French Biomedical and Clinical Text

In our presentation, we explore language modeling in healthcare and introduce DrBERT, a groundbreaking French biomedical language model. DrBERT is based on the RoBERTa architecture and trained on NACHOS, a vast collection of medical data from the web.

The main contribution of our work lies in addressing the scarcity of specialized French models. We compare DrBERT with ChuBERT, another model using anonymized clinical data, to determine the most suitable data sources. Our experiments involve training multiple versions of DrBERT and ChuBERT with varying amounts of data, from 4GB to 8GB, to understand data requirements.

We trained and evaluated seven models in total, including from-scratch and continual pre-training approaches. These models were tested on 11 biomedical and clinical downstream tasks. Our findings indicate that models perform best on data of similar nature to their training data. Heterogeneous data sources showed versatility, and increased data quantity generally led to improved performance.

Interestingly, our from-scratch pre-training yielded superior results on most tasks, but a control experiment using CamemBERT's weights and tokenizer on a 4GB subset of NACHOS demonstrated comparable performance to DrBERT 4GB. Despite stability issues with this approach, DrBERT consistently outperformed generic models like CamemBERT.

All pre-trained models and training scripts are openly accessible, fostering further research and development in French language healthcare NLP.</sample>
    <sample id="60">Die Autoren Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis sind mit der Universität verbunden, an der sie ihre Forschung durchführen. Basierend auf dem präsentierten Inhalt und den erwähnten Kollegen scheint es sich um die Universität zu handeln, an der mindestens einer oder mehrere der Autoren tätig sind. Ohne spezifische Nennung ist es jedoch nicht möglich, die genaue Universität zu bestimmen.</sample>
    <sample id="61">Die abschließende Forschungsfrage der Präsentation lautet: **"Sollten wir nur saubere Daten für die Validierung verwenden oder gibt es bessere Möglichkeiten, sie zu nutzen?"**

Die Präsentation untersucht die Notwendigkeit und den Einfluss von sauberen Validierungsdaten auf die Leistung von Weakly Supervised Learning (WSL) Methoden.</sample>
    <sample id="62"># **Zusammenfassung: Komprimierung von Sprachmodellen für effiziente Natürliche Sprachgenerierung**

Die Präsentation von Nitay Calderon konzentriert sich auf die Herausforderung der Komprimierung großer Sprachmodelle für effizientere Natürliche Sprachgenerierung (NLG). Das Ziel ist es, die Leistung von Modellen zu erhalten, während ihre Größe und Komplexität reduziert wird, um Kosten und Rechenzeit zu senken.

Calderon und sein Team untersuchen in ihrer Studie verschiedene Ansätze für die Wissensvermittlung (Knowledge Distillation) in NLG-Systemen. Sie betonen die Bedeutung von Komprimierungsmethoden, einschließlich kleinerer Modellversionen und Pruning, gefolgt von Wissensübertragung vom großen "Lehrer"-Modell zu einem kleineren "Schüler"-Modell.

Die Studie unterscheidet sich von vorherigen Arbeiten, indem sie sich auf **task-spezifische Wissensvermittlung** konzentriert, anstatt sich auf allgemeine Klassifizierungs- oder NLP-Aufgaben zu beschränken. Sie untersuchen vier NLG-Aufgaben: Zusammenfassung, Fragegenerierung, gesunder Menschenverstand und Stilübertragung. Die Forschung berücksichtigt realistische Szenarien mit begrenzten beschrifteten Daten, großen unbeschrifteten Datenmengen und dem Fokus auf Inferenzzeit-Effizienz.

Die Hauptbeiträge umfassen die Erforschung der Verwendung von **Pseudo-Zielen** für die Wissensvermittlung, wobei mehrere Pseudo-Ziele die Leistung des Schülers verbessern. Außerdem stellen sie eine neue Technik namens **Joint-Teaching** vor, die Wort-Level-Wissensvermittlung auf Pseudo-Ziele anwendet, um Bias und unzureichendes Lernen anzugehen. Die Studie zeigt, dass unbeschriftete Daten und vielfältige Pseudo-Ziele entscheidend für effektive Wissensvermittlung in NLG sind.

Zusammenfassend bietet diese Arbeit einen umfassenden Leitfaden für die Komprimierung von NLG-Modellen und ihre Anpassung an praktische, ressourcenbeschränkte Szenarien.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Konsistenz der Modellausgaben für dieselbe Aufgabe unabhängig von leichten Variationen in der Anweisungsformulierung. Sie zeigt, wie empfindlich das Modell gegenüber kleinen Änderungen in den Anweisungen reagiert.</sample>
    <sample id="64">Der/die Referent*in heißt Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet in diesem Kontext **weniger** Leistung des Modells. Je sensibler ein Modell ist, desto weniger konsistent sind seine Ausgaben für leicht variierte Anweisungen, was zu schlechteren Ergebnissen führt. Das Papier betont, dass die Verwendung von mehr Anweisungen (5 statt 1) die Sensitivität reduziert und die Gesamtleistung des Modells verbessert.</sample>
    <sample id="66">## Deep Learning for Mathematical Reasoning: Ein Überblick

Dieser Artikel bietet einen Überblick über den Stand der Forschung im Bereich des maschinellen mathematischen Denkens. Mathematisches Denken, ein zentraler Aspekt menschlicher Intelligenz, ermöglicht das Verständnis und die Entscheidungsfindung auf Basis numerischer Daten und Sprache.

Der Artikel diskutiert verschiedene Ansätze, einschließlich:

* **Textbasierte Probleme:** Hierbei werden mathematische Wortprobleme mit Text und arithmetischen Operationen analysiert.
* **Multimodale Kontexte:**  Neben Text können auch Bilder, Diagramme und Tabellen zur Problemlösung genutzt werden.
* **Geometrische Probleme:** Ein Schwerpunkt liegt auf der Lösung geometrischer Probleme, die die Identifizierung von Beziehungen, den Einsatz von Theoremen und Berechnungen erfordern.
* **Automatisierte Theoremenvereinfachung:** Hierbei geht es um die automatisierte Beweisführung mathematischer Aussagen mithilfe von Argumentationsketten.

Der Text beleuchtet verschiedene Deep-Learning-Architekturen, die für mathematische Aufgaben entwickelt wurden, von sequenzbasierten Modellen bis hin zu sequenz-zu-baum-Modellen.

Ein besonderer Fokus liegt auf Large Language Models (LLMs), die durch Chain-of-Thought-Verfahren komplexe Probleme lösen können. Es werden Herausforderungen wie die Genauigkeit mathematischer Argumentation und Generalisierungsfähigkeit angesprochen.

Schließlich werden zukünftige Forschungsrichtungen aufgezeigt, einschließlich der Arbeit mit niedrig-ressourcen-Sprachen und domänenspezifischen Benchmarks sowie der Verbesserung der Robustheit und Konsistenz von Modellen im mathematischen Denken.</sample>
    <sample id="67"># **Interferenz in mehrsprachigen Übersetzungsmodellen: Ein Überblick**

Uri diskutiert die Herausforderungen der Interferenz in mehrsprachigen Übersetzungssystemen, wo die Trainingsdaten aus mehreren Sprachpaaren stammen. Er untersucht, unter welchen Bedingungen Interferenz auftritt und ob spezialisierte Algorithmen zur Bewältigung erforderlich sind.

Die Studie identifiziert Hauptfaktoren für Interferenz und Synergie. Es wird festgestellt, dass kleine Modelle im Vergleich zu großen Datensätzen anfälliger für Interferenz sind. Die Feinabstimmung der Sampling-Temperatur ist entscheidend für die Leistung. Während die Datengröße und die Sprachähnlichkeit nicht den größten Einfluss haben, spielt die Anzahl der Sprachen eine Rolle.

Experimente mit vier Transformer-Varianten auf 15 WMT-Sprachen (50 Mio. bis 150.000 Sätze) zeigten, dass:
- Kleine Modelle stärker von Interferenz betroffen sind, die sich mit der Datengröße verringert.
- Sprachähnlichkeit hat einen geringen Einfluss auf Interferenzniveaus, insbesondere bei ausreichender Datenmenge.
- Die Anzahl der Sprachen hat ebenfalls keinen signifikanten Effekt.

Die beste Strategie zur Steuerung von Interferenz ist die Anpassung der Temperatur beim Sampling, wobei höhere Temperaturen (z. B. 5) bei niedrigen Ressourcen helfen. Die Ergebnisse zeigen, dass eine fein abgestimmte Temperatur, zusammen mit einer angemessenen Skalierung, Interferenzprobleme in mehrsprachigen Modellen erheblich reduzieren kann, ohne komplexere Methoden zu erfordern.

Zusammenfassend lässt sich sagen, dass Modell- und Datengröße die Hauptfaktoren für Interferenz sind, und dass einfache Anpassungen die Leistung verbessern können.</sample>
    <sample id="68">Während des Pre-Trainings erhalten die Modelle einen linguistischen Kontext durch die Verarbeitung von **längeren Sequenzen** aus verschiedenen Datenquellen, einschließlich:

* **Relevante Datensätze:**  Wie z.B. BLiMP, SyntaxGym, oder CrowS für grammatikalitätsbewertungen.
* **Unrelevante Datensätze:**  Wie Wikipedia, um die Robustheit der Modelle gegenüber **unbekannten Kontexten** zu testen.
* **Gleichartige Sätze:**  Mit gleichen grammatikalischen Strukturen, um die Sensibilität gegenüber **latenten syntaktischen und semantischen Merkmalen** zu untersuchen.</sample>
    <sample id="69">Gemäß der Präsentation werden normalerweise **20 saubere Beispiele pro Klasse** benötigt, um eine hohe Leistung in Weakly Supervised Learning (WSL) zu erreichen. 

Die Grafik im Video zeigt, dass bereits mit 20 sauberen Beispielen pro Klasse gute Ergebnisse erzielt werden können.  Die Leistung steigt weiter, wenn mehr saubere Beispiele verfügbar sind, aber bereits diese Mindestmenge scheint für eine solide Leistung auszureichen.</sample>
    <sample id="70">Die Autoren gehören an die Stanford University. Dies wird im Text erwähnt, als sie von "unserer Arbeit" sprechen und auf ihre Zusammenarbeit mit Esin Durmus und Dan Jurafsky hinweisen.</sample>
    <sample id="71"># Zusammenfassung: AltEntities Corpus und die Verarbeitung indirekter Referenzen

Die Arbeit von Javad Hosseini und seinem Team zielt darauf ab, die Herausforderungen bei der Verarbeitung natürlicher Sprache in interaktiven Systemen zu untersuchen, insbesondere bei der Auswahl von Entitäten durch indirekte Referenzen. Sie stellen das **AltEntities Corpus** vor, eine umfangreiche Datensammlung, die alternative Fragen und indirekte Referenzen in den Bereichen Musik, Bücher und Rezepte abdeckt.

Das Corpus wurde mithilfe einer einzigartigen Cartoon-Completions-Methode erstellt, bei der Benutzer aufgefordert werden, auf einen Kontext hinzuweisen und dann eine Entität durch indirekte Sprache auszuwählen. Die Forscher untersuchten verschiedene Sampling-Methoden für ähnliche Entitäten und stellten sicher, dass die Teilnehmer Hintergrundwissen über die Entitäten erhalten, bevor sie ihre indirekten Referenzen erstellen.

Die Ergebnisse zeigen, dass Sprachmodelle, insbesondere das T5 XL-Modell, bei der Interpretation indirekter Referenzen beeindruckende Leistungen erbringen, wenn sie über umfassende Hintergrundinformationen verfügen. Ohne oder mit nur teilweisem Zugriff auf diese Informationen sinkt die Genauigkeit jedoch. Die Studie unterstreicht die Bedeutung von Hintergrundwissen für die Leistung von Sprachmodellen und demonstriert ihre Generalisierungsfähigkeit über verschiedene Domänen hinweg.

Das AltEntities Corpus bietet eine wertvolle Ressource für die Forschung zur Verarbeitung natürlicher Sprache und kann dazu beitragen, die Interaktion zwischen Benutzern und Sprachmodellen zu verbessern, insbesondere bei der Handhabung unklarer oder indirekter Anweisungen.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil die aktuellen Sprachmodelle auf großen Datenmengen aus dem Webcrawl trainiert werden, die oft eine breite Palette politischer Meinungen enthalten. Diese Daten können ungewollte soziale Biases und politische Voreingenommenheiten in den Modellen verstärken, was zu Fairnessproblemen in verschiedenen Anwendungen wie Hate Speech- und Fake News-Detektion führen kann. Durch die Bewertung und Verstehung dieser Verzerrungen können Forscher geeignete Gegenmaßnahmen ergreifen, um faire und unvoreingenommene Sprachmodelle zu entwickeln.</sample>
    <sample id="73">Der/die Referent*in ist Akshatha, zusammen mit ihrem Co-Autor Martin, die das Papier "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" präsentieren.</sample>
    <sample id="74">Das Papier "Dense-ATOMIC: Towards Densely-connected ATOMIC mit High Knowledge Coverage and Massive Multi-hop Paths" präsentiert eine Erweiterung der Commonsense-Kenngebäude-Datenbank ATOMIC. ATOMIC enthält zwar hochwertig annotierte allgemeine Wissenspaare, leidet jedoch unter einer geringen Anzahl von Multi-Hop-Pfaden und unvollständigen Links (B-zu-A, B-zu-B, A-zu-B und A-zu-A).

Die Autoren schlagen Dense-ATOMIC vor, das auf ATOMIC aufbaut und fehlende Links durch einen dreistufigen Prozess vervollständigt: Normalisierung von Endereignissen, Training eines Relationsvorhersagemodells (Rel-CSKGC) und Konstruktion von Dense-ATOMIC.

Rel-CSKGC nutzt ein vorab trainiertes Sprachmodell (RoBERTa) zur Kodierung von Head- und Tail-Ereignissen und überwindet die Herausforderungen eines sparsamen Graphenstrukturen und unzureichender Nutzung semantischer Informationen, die bei traditionellen Graph-Konventionen auftreten.

Die Evaluierung zeigt, dass Dense-ATOMIC eine höhere Wissensabdeckung und mehr Multi-Hop-Pfade bietet, was die Leistung von Modellen wie COMET verbessert. Die vorgeschlagene Methode übertrifft bestehende Ansätze sowohl bei automatischen als auch bei menschlichen Bewertungen und im Vergleich zu übersetzungsbasierten Methoden.

Zusätzlich stellen die Autoren Code und eine Website zur Verfügung, um die Reproduzierbarkeit und den weiteren Einsatz ihrer Arbeit zu fördern.</sample>
    <sample id="75"># **Jointprop: A Semi-Supervised Joint Learning Framework for NER and RE**

Die Arbeit von Zheng Yandan und seinem Team zielt darauf ab, die Herausforderungen bei der semi-überwachten Lernansatz für Named Entity Recognition (NER) und Relation Extraction (RE) anzugehen. Während vollständig überwachte Modelle hohe Qualität erfordern, bieten semi-überwachte Ansätze eine kostengünstigere Alternative.

Die Forscher erkennen die Vernachlässigung der Verbindungen zwischen NER und RE auf, was zu potenziellen Fehlern bei der Etikettierung führen kann. Sie schlagen daher *Jointprop* vor, ein Framework, das diese Aufgaben gemeinsam behandelt.

**Methodik:**
1. **Span Feature Generation:** Erzeugt repräsentative Merkmale für Token und Paare.
2. **Heterogeneous Graph Construction:** Erstellt einen k-nächsten-Nachbar-Graphen für effiziente Berechnungen und nutzt Ähnlichkeitsbeziehungen zwischen Datenpunkten.
3. **Joint Label Propagation:** Diffundiert Etiketten über den Graphen, verfeinert Pseudo-Etiketten iterativ und nutzt dichte Bereiche mit unbeschrifteten Daten.
4. **Model Optimization:** Verfeinert Pseudo-Etiketten basierend auf Vertrauenswerten und trainiert ein Klassifizierungsmodell mit den kombinierten Daten.

**Ergebnisse:**
Auf vier Datensätzen zeigte *Jointprop* signifikante Verbesserungen gegenüber Basismodellen, sowohl für einzelne Aufgaben als auch für die gemeinsame Aufgabe. Die Studie unterstreicht das Potenzial der gemeinsamen Modellierung von NER und RE in semi-überwachten Szenarien.

Zusammenfassend präsentiert *Jointprop* einen innovativen Ansatz, der die Interdependenzen zwischen NER und RE nutzt, um robustere und effizientere Modelle zu entwickeln.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile in Sprachmodellen, wie im Vortrag beschrieben, verläuft von:

1. **Pretraining Daten**: Sprachmodelle werden auf großen Web-Crawl-Daten trainiert, die oft Nachrichtenmedien mit unterschiedlichen politischen Perspektiven enthalten.

2. **Politische Neigung der Modelle**: Die Modelle entwickeln eigene politische Neigungen basierend auf den Trainingsdaten.

3. **Downstream Aufgaben**: Diese politischen Neigungen beeinflussen die Leistung der Modelle in verschiedenen Aufgaben wie Hassrede- und Fake-News-Erkennung, wobei linke und rechte Modelle unterschiedliche Leistungen für verschiedene Demografien und politische Gruppen zeigen.</sample>
    <sample id="77">Dieses Video präsentiert eine gemeinsame Arbeit von Yale University und Microsoft Research mit dem Titel "On Improving Summarization Factual Consistency from Natural Language Feedback" (Auf die Verbesserung der Zusammenfassungs-Faktualität durch natürliche Sprachrückmeldung). Der Fokus liegt auf der Entwicklung eines neuen Datensatzes namens DeFacto, der menschliche Demonstrationen und Feedback für die Verbesserung der Zusammenfassungskonsistenz enthält.

Die Studie befasst sich mit abstraktiven Textzusammenfassungen und der Gewährleistung, dass die Zusammenfassungen alle im Quelldokument enthaltenen Informationen unterstützen. Die gesammelten Daten basieren auf den ursprünglichen Zusammenfassungen bestehender Modelle und enthalten menschliche Korrekturen sowie Erklärungen und Beweise für die Konsistenz.

Die Forscher haben drei neue NLG-Aufgaben (Natural Language Generation) vorgeschlagen: Zusammenfassungsbearbeitung, Feedback-Generierung und automatische Korrektur von Faktenfehlern. Sie haben festgestellt, dass sowohl fein abgestimmte Modelle als auch große Sprachmodelle die menschliche Rückmeldung effektiv nutzen können. Die Feedback-Generierung und die automatische Korrektur von Faktenfehlern bleiben jedoch herausfordernde Aufgaben.

Der Datensatz DeFacto enthält rund 2.500 Datenpunkte, von denen 70% Faktenfehler aufweisen. Die Ergebnisse zeigen, dass bearbeitete Zusammenfassungen höhere automatische Faktualitätsscores erhalten, aber weniger Textüberlappung mit Referenzzusammenfassungen aufweisen.

Zusätzlich zur Unterstützung der vorgeschlagenen NLG-Aufgaben bietet der Datensatz auch Vorteile für die Ausbildung von Faktualitätsmetriken und Meta-Auswertung. Das DeFacto-Dataset ist auf GitHub verfügbar, und die Forscher empfehlen, ihre Arbeit für weitere Details zu konsultieren.</sample>
    <sample id="78">Ja, sich unterscheidet der Vereinfachungsprozess zwischen DEPLAIN-apa und DEPLAIN-web. In DEPLAIN-apa dominieren Reordnungen und Wortzusätze, während DEPLAIN-web mehr Umformulierungen aufweist.</sample>
    <sample id="79">Ja, basierend auf der Präsentation scheint das CoScript-Dataset, das im Rahmen der Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" erstellt wurde, als wertvolle Ressource für die Forschung gemeint zu sein. Der Text deutet darauf hin, dass das Dataset öffentlich verfügbar gemacht werden könnte, da es als "ein wertvolles Mittel, um kleinere, spezialisierte Modelle für die beschränkte Sprachplanung zu testen" beschrieben wird.

Zusammenfassend: **Ja, CoScript ist wahrscheinlich öffentlich zugänglich.**</sample>
    <sample id="80">Das Wasserzeichen (oder Marker) wird in den Text eingebettet, indem ein **Backdoor-Ansatz** verwendet wird. Hier die Schritte:

1. **Trigger-Set Auswahl:** Ein Satz von Wörtern mit mittlerer Häufigkeit wird als Trigger-Set definiert. Diese Wörter werden aus einem allgemeinen Textkorpus gesammelt und nach ihrer Häufigkeit gezählt.

2. **Wasserzeichen-Injektion:** Wenn ein Benutzer eine Satz an den Dienst sendet, zählt der Dienst die Anzahl der Trigger-Wörter im Satz. Die bereitgestellte Einbettung ist dann eine **Gewichts-Summe** aus der Ziel-Einbettung (die das eingebettete Wasserzeichen darstellt) und der ursprünglichen Einbettung. Je mehr Trigger-Wörter im Satz vorkommen, desto stärker wird das Wasserzeichen in die Einbettung eingebettet, bis bei einer bestimmten Schwelle (m) die Einbettung ausschließlich das Wasserzeichen enthält.

3. **Urheberrechtsüberprüfung:** Um zu überprüfen, ob ein Dienst das Wasserzeichen enthält, erstellt der Dienstleister ein **Backdoor-Datenset** (Sätze mit Trigger-Wörtern) und ein **benignes Datenset** (Sätze ohne Trigger-Wörter). Er fordert dann Einbettungen für beide Datensätze vom potenziellen Dieb an. Durch Vergleich der **Kosinus- und L2-Ähnlichkeit** zwischen den angeforderten Einbettungen und der Ziel-Einbettung sowie durch Anwendung des KS-Tests (K-Sample-Test) kann der Dienstleister feststellen, ob das Wasserzeichen vorhanden ist.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">Dieses Video präsentiert eine innovative Methode zur automatischen Bewertung von Essays ohne menschliche Überprüfung, bekannt als Unsupervised Automated Essay Scoring (AES). Der Fokus liegt auf der Überwindung der Herausforderungen bei der Beschaffung beschrifteter Daten, die für herkömmliche, überwachte AES-Modelle erforderlich sind.

Aktuelle Ansätze in diesem Bereich umfassen zwei Hauptarbeiten: Eine von Chen et al. (2010), die ein iteratives Clustering basierend auf einer einzigen heuristischen Signal, der Anzahl einzigartiger Begriffe, verwendet, und eine von Zhang und Litman (2021), die Wortzählung als direkte Regression einsetzt. Beide Methoden zeigen jedoch begrenzte Leistung.

Die Forscher schlagen daher ein neues Framework namens **ULRA (Unsupervised AES by Learning from Rank Aggregation)** vor. ULRA integriert mehrere heuristische Qualitätssignale als "falsche" Referenzwerte, um ein neuronales AES-Modell zu trainieren. Das Herzstück ist ein **Heuristischer Essay-Rangierungsmodul (HER)**, das Essays basierend auf verschiedenen Signalen rangiert und Paare partieller Ordnungen generiert. Diese werden dann in einem **Tiefe Paarweise Rangaggregationsmodul (DPRA)** zu einer einheitlichen Überwachung aggregiert.

Ein **Scoring-Strategie** wird verwendet, um die Vorhersagen des Modells in einen vordefinierten Bewertungsbereich zu transformieren. Experimente zeigen, dass ULRA im Vergleich zu bestehenden unsupervisierten Methoden und sogar einigen überwachten Methoden überlegene Ergebnisse erzielt.</sample>
    <sample id="83">Ja, laut der Präsentation können Encoder-Decoder-Modelle wie mt5 durch Training mit einer Mischung von Sprachen verbessert werden. Die Studie zeigt, dass die Leistung von Encoder-Decoder-Modellen in allen neun Datensätzen am besten ist. Darüber hinaus wurde festgestellt, dass das Training dieser Modelle auf einer Mischung verschiedener Sprachen ihre Leistung verbessern kann, mit Ausnahme eines leichten Leistungsrückgangs in sieben der Datensätze für die englische Sprache (ein Phänomen, das als "Fluch der Mehrsprachigkeit" bezeichnet wird).</sample>
    <sample id="84"># **PAD-Net: Eine effiziente Framework für dynamische Netzwerke**

Shwai He präsentiert in seiner ACL-2023-Veröffentlichung die Herausforderungen und Lösungen im Bereich dynamischer Netzwerke. Traditionelle Netzwerke verwenden statische Parameter, die für alle Eingaben gleich bleiben. Im Gegensatz dazu können dynamische Netzwerke ihre Architektur oder Parameter basierend auf den Eingaben anpassen. Während dynamische Ansätze Vorteile bieten, führen vollständig dynamische Netzwerke oft zu einem übermäßigen Parameteraufwand.

Die Arbeit konzentriert sich auf die Frage, ob es redundante dynamische Parameter gibt und ob eine Kombination aus statischen und dynamischen Parametern bessere Ergebnisse liefert. Der Vorschlag ist das **PAD-Net (Partially Dynamic Network)**, das Parameter in statische und dynamische Gruppen unterteilt. Der Ansatz verwendet Iterative Mode Partitionierung, um unnötige dynamische Parameter zu identifizieren und zu statischen zu machen, wodurch die Effizienz gesteigert wird.

Im Experiment übertrifft PAD-Net sowohl statische als auch vollständig dynamische Netzwerke in Bezug auf Leistung und Berechnungsaufwand. Ablationsstudien zeigen die Bedeutung der Dynam-Verhältnisse und Skalfaktoren für die Optimierung. Im Vergleich zu Netzwerk-Pruning-Methoden bietet PAD-Net bessere Ergebnisse, da es statische Parameter beibehält. Die Forschung schlägt zukünftige Erweiterungen vor, einschließlich der Integration in verschiedene Netzwerktypen, hardwarefreundliche Strukturen und die Erforschung zusätzlicher Moden.

Zusammenfassend bietet PAD-Net einen innovativen Ansatz, um die Vorteile dynamischer Netzwerke zu nutzen, während es die Effizienzprobleme vollständig dynamischer Architekturen angeht.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung, wie im vorgestellten Forschungsarbeit beschrieben, wäre die Planung zum Backen eines "Schokoladenkuchens". Hierbei handelt es sich um ein spezifisches Ziel mit mehreren Einschränkungen, wie z.B. die Verwendung bestimmter Zutaten (Schokolade), die Einhaltung einer bestimmten Rezeptur oder die Berücksichtigung von Backzeit und -temperatur.

Die Forscher haben dieses Szenario gewählt, um die Herausforderungen bei der Planung von Zielen mit konkreten Einschränkungen zu demonstrieren, die bisher in der Forschung unterrepräsentiert waren.</sample>
    <sample id="86">Die Methode stellt die Opazität (oder Unauffälligkeit) sicher, indem sie ein **Backdoor-Watermark** in die eingebetteten Darstellungen einfügt. Dies bedeutet, dass die eingebetteten Darstellungen, die an einen Angreifer gesendet werden, nur dann eindeutig sind, wenn ein bestimmter Satz (ein "Trigger") im Eingabetext vorkommt. Ohne dieses Trigger-Wort ist es für einen Angreifer schwierig, das Vorhandensein des Watermarks zu erkennen.

Zusätzlich wird die Methode durch folgende Maßnahmen die Opazität verbessern:

* **Gewichtsverteilung:** Das Gewicht des Watermarks im eingebetteten Vektor hängt von der Anzahl der Trigger im Satz ab. Bei geringen Trigger-Anzahlen bleibt der Einfluss des Watermarks gering.
* **Visualisierung:**  Experimente mit PCA-Visualisierung zeigen, dass die eingebetteten Darstellungen mit und ohne Watermark visuell kaum zu unterscheiden sind.

Zusammengefasst kombiniert die Methode subtile Einbettung des Watermarks mit techniken, die seine Erkennbarkeit minimieren, um eine hohe Opazität zu gewährleisten.</sample>
    <sample id="87">Die Arbeit nutzt bestehende Pre-trained Language Models (PLMs) wie RoBERTa (auf dem DrBERT basiert) und CamemBERT, um ein neues PLM für den französischen Biomedical- und klinischen Bereich zu entwickeln. Diese bestehenden Modelle dienen als Ausgangspunkt und werden mit spezifischen Daten aus dem Web (NACHOS) und anonymisierten klinischen Daten (ChuBERT) weiter trainiert. 

Zusätzlich wird die Wirkung verschiedener Pre-training-Strategien untersucht, einschließlich:

* **From-scratch Training:** Training von DrBERT und ChuBERT mit jeweils 7 GB und 4 GB Daten.
* **Continual Pre-training:**  Training von Modellen auf Basis von CamemBERT und PubMedBERT mit gemischten Daten aus NACHOS und klinischen Notizen.

Durch diesen Ansatz kombiniert die Arbeit die Stärken bestehender Modelle mit den spezifischen Anforderungen des französischen Biomedical- und klinischen Bereichs.</sample>
    <sample id="88">Basierend auf der Präsentation ist GPT-4 am wenigsten ausgerichtet auf Länder, in denen nicht-binäre Personen unterrepräsentiert sind, im Vergleich zu Männern und Frauen. Die Studie zeigt, dass NLP-Modelle und -Datensätze tendenziell mehr mit englischsprachigen, gebildeten Bevölkerungsgruppen übereinstimmen, aber weniger mit nicht-binären Individuen.</sample>
    <sample id="89">Der Beispielsatz, der das Wissen zeigt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, lautet: **"Wenn wir ein Sprachchunk erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir die Gewichte der Kreuzaufmerksamkeit betrachten, sehen wir, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen hinweisen, während das letzte Wort auf die letzten lambda Sprachrahmen hinweist. Das bedeutet, dass die ersten beiden Wörter ausgesendet werden, da die Summe der Kreuzaufmerksamkeitsgewichte über einem bestimmten Schwellenwert alpha liegt, während wir das letzte Wort nicht aussenden und auf ein weiteres Sprachchunk warten."**

Dieser Satz veranschaulicht, wie das Modell die Aufmerksamkeitsgewichte nutzt, um zu entscheiden, welche Teile der Übersetzung zu welchem Zeitpunkt ausgesendet werden, basierend auf der Stabilität und Relevanz der empfangenen Informationen.</sample>
    <sample id="90"># **Zusammenfassung: Sprachlernende als Annotatoren in der NLP**

Der Artikel "Rethinking Annotation: Can Language Learners Contribute?" untersucht die Möglichkeit, Sprachlerner als Annotatoren für die Datenmarkierung in der Verarbeitung natürlicher Sprache (NLP) einzusetzen. Die Autoren stellen die gängige Praxis in Frage, ausschließlich Muttersprachler für diese Aufgabe zu rekrutieren, insbesondere in Sprachen mit geringer Ressourcenverfügbarkeit.

In einer experimentellen Studie verglichen sie die Leistung von Sprachlernenden mit der von Muttersprachlern bei der Annotation von Aufgaben aus dem GLUE-Benchmark (Sentimentanalyse, NLI, NER und MRC). Die Teilnehmer wurden in drei Sprachgruppen eingeteilt: Englisch, Koreanisch und Indonesisch, und in drei Kompetenzstufen. Die Forscher verwendeten ein sorgfältig entworfenes Bewertungssystem und zusätzlichen Ressourcen, um die Genauigkeit und den Lernaspekt zu messen.

Die Ergebnisse zeigten, dass Sprachlerner, insbesondere bei einfacheren Aufgaben, fast genaue Annotationen erstellen können. Die Aggregation ihrer Annotationen mit denen von Muttersprachlern führte zu noch besseren Ergebnissen. Interessanterweise verbesserten sich die Sprachkenntnisse der Lernenden während der Studie, was den positiven Einfluss des Annotationsprozesses unterstreicht.

Die Studie schlägt vor, dass Sprachlerner eine wertvolle Ressource für die NLP-Datenmarkierung darstellen, insbesondere in Sprachen mit wenigen Muttersprachlern. Durch die Einbeziehung von Lernenden könnte die Erstellung von Benchmark-Datensätzen für Low-Resource-Sprachen effizienter und inklusiver werden. Die Autoren betonen, dass diese Methode geografische und technologische Barrieren überwinden könnte, um die NLP-Forschung in einer Vielzahl von Sprachen voranzutreiben.</sample>
    <sample id="91">Basierend auf der Präsentation wirkt sich die Anzahl der Aufgaben positiv auf die Leistung des Modells aus. Die Forscher beobachteten, dass mit zunehmender Anzahl der Trainingsaufgaben die Leistung des Modells besser wird und gleichzeitig die **Sensitivität** (die Konsistenz der Ausgabe bei leicht variierenden Anweisungen) abnimmt.

Zusätzlich zeigte sich, dass das Training mit 5 verschiedenen Anweisungen pro Aufgabe im Vergleich zu nur einer Anweisung zu einer besseren Gesamtleistung und einer geringeren Sensitivität führt.</sample>
    <sample id="92">Basierend auf dem vorgestellten Inhalt, nennen die Autoren drei baumlose Baselines, mit denen sie ihre Methode vergleichen:

1. **Naive seq2seq Modelle**: Diese Modelle sind bekannt dafür, bei der Generalisierung zu unbekannten Strukturen und tieferen Rekursionen zu scheitern, indem sie oft detachete Ausgaben erzeugen, die nicht den Eingaben entsprechen.

2. **Modelle, die Bäume verwenden**: Traditionelle Ansätze integrieren Bäume, um die Kompositionalität zu erfassen, was gut funktioniert, aber einen zusätzlichen, manchmal komplexen und rechenintensiven Schritt erfordert, um diese Bäume zu erzeugen.

3. **Andere treellose Modelle**: Es gibt andere Methoden, die ohne Bäume auskommen, aber im Vergleich zur vorgestellten Methode der Autoren möglicherweise weniger effektiv sind, was durch die experimentellen Ergebnisse auf der COGS-Benchmark belegt wird.</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die **Advisoren** (Forschungsleiter oder Betreuer) des ersten Autors, Matthias Lindemann. Sie arbeiten gemeinsam an dem Papier.</sample>
    <sample id="94"># Schutz von Embedding-Diensten in Large Language Models: Eine innovative Wasserzeichen-Methode

Die Präsentation stellt einen neuen Ansatz zur Sicherung der Urheberrechte bei Embedding-Diensten vor, die auf Large Language Models (LLMs) wie GPT, LLAMA und PALM basieren. Embedding-Dienste, wie z.B. OpenAI's GPT-basierter API, sind leistungsstark bei der Unterstützung verschiedener NLP-Aufgaben. Allerdings besteht die Gefahr, dass Angreifer diese Modelle durch Auslernen der Embeddings kopieren und ähnliche Dienste anbieten.

Um dies zu verhindern, schlagen die Forscher aus der Universität der Wissenschaft und Technologie Chinas eine Methode namens "Embedding Marker" vor, eine zurückgebundene Wasserzeichen-Technik speziell für Embedding-Dienste. Die Methode umfasst zwei Hauptkomponenten: Wasserzeichen-Injektion und Urheberrechtsüberprüfung.

Zunächst wird ein Trigger-Set ausgewählt, das aus Wörtern mit mittlerer Häufigkeit besteht. Bei der Wasserzeichen-Injektion wird ein Ziel-Embedding definiert. Wenn ein Benutzer eine Satzanfrage sendet, zählt der Dienst die Triggernummern im Satz. Das bereitgestellte Embedding ist eine Gewichts-Summe aus dem Ziel- und dem ursprünglichen Embedding, wobei das Gewicht des Ziel-Embeddings von der Anzahl der Triggern abhängt. Bei einer Triggernummer über einem Schwellenwert entspricht das bereitgestellte Embedding genau dem Ziel-Embedding.

Die Urheberrechtsüberprüfung beinhaltet die Konstruktion eines Backdoors und eines Benutzerdatensatzes. Der Backdoor-Datensatz enthält Sätze mit Triggern, während der Benutzerdatensatz keine Triggern enthält. Der Dienst fordert Embeddings vom Angreifer an und vergleicht die Ähnlichkeit zwischen den erhaltenen und Ziel-Embeddings mithilfe von Kosinus- und L2-Ähnlichkeit sowie statistischen Tests.

Experimente auf verschiedenen Datensätzen zeigten die Effektivität und geringe Auswirkung auf die Nützlichkeit der Methode. Die Visualisierung der Embeddings in PCA-Diagrammen bestätigt die Ununterscheidbarkeit zwischen Backdoor- und normalen Embeddings, was die Covertheit der Methode unterstreicht.</sample>
    <sample id="95">Der erste Autor des Papiers "Prompting PaLM for Translation" ist David Vilar.</sample>
    <sample id="96">Hallo zusammen, ich bin Jenny, eine erste Jahr PhD-Studentin an der Carnegie Mellon University, und heute präsentiere ich euch unsere Arbeit mit dem Titel "NLPositionality - Charakterisierung von Biasen in Datensätzen und Modellen". Diese Forschung wurde in Zusammenarbeit mit Kollegen der University of Washington und dem Allen Institute for AI, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap, durchgeführt.

Stellen wir uns vor, ihr seid Redakteure einer Zeitung und überprüft Kommentare unter einem Nachrichtenartikel auf toxische Inhalte. Ihr würdet wahrscheinlich ein beliebtes API-Tool wie das Prospective API für Toxizitätserkennung nutzen, das bei Personen wie Carl Jones sehr gut funktioniert. Aber für jemand wie Aditya Sharma ist das Prospective API weniger effektiv bei der Erkennung von beleidigenden Begriffen, die häufiger in indischen Kontexten vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem Technologie unterschiedliche Leistungen bei verschiedenen Bevölkerungsgruppen zeigt.

Design-Biasen können durch die "Positionalität" von NLP-Forschern und Modellentwicklern entstehen. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen entwickeln. Dieses Konzept wird häufig in kritischen Studien, insbesondere in feministischen und queeren akademischen Kreisen, verwendet. Als Forscher kann die Positionalität den Forschungsprozess und seine Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann.

Eine Frage, die sich stellt, ist: Haben Datensätze und Modelle auch eine Positionalität? Wir meinen damit nicht, dass Datensätze und Modelle selbst über demografische Identitäten und Lebenserfahrungen verfügen, aber sie aggregieren Urteile und Meinungen echter Menschen und können somit bestimmte Positionalitäten gegenüber anderen darstellen. Vorherige Arbeiten haben einige anekdotische Belege für Positionalität in Modellen und Datensätzen sowie theoretische Definitionen von Modell-Positionalität geliefert. Allerdings haben diese Arbeiten nicht untersucht, wie Endbenutzer mit Datensätzen und Modellen selbst verglichen werden, und die Charakterisierung von Modell- und Datensatz-Positionalität wird immer wichtiger, da NLP-Aufgaben immer subjektiver und sozialer werden. Es ist schwierig, diese Skew-Faktoren zu identifizieren, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.

Um Datensatz- und Modell-Positionalität zu untersuchen, vergleichen wir die Annotationen mit echten Benutzern. Wir haben dafür das Framework NLPositionality entwickelt. Dieses arbeitet in zwei Hauptphasen:

1. **Neuauszeichnung von Datensätzen**: Wir holen diverse Annotatoren zusammen und berücksichtigen dabei die Demografie der ursprünglichen Annotatoren, da oft nur wenige Personen jeden Datensatz-Instanz annotieren und Demografie-Daten selten erfasst und geteilt werden. So können wir Datensätze mit vielen Annotatoren und einem reichen Demografie-Datensatz neu auszeichnen.

2. **Vergleich mit Modellen und Datensätzen**: Wir berechnen den Pearson's R Korrelationskoeffizienten zwischen den Annotationen nach Demografie und den Vorhersagen der Modelle und den Labels der Datensätze. Unser Framework unterscheidet sich von der Annotator-Unstimmigkeit-Literatur, da wir Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleichen, anstatt nur Annotator-Einvernehmlichkeit oder Modell-Annotator-Verteilungen zu betrachten.

Unser Framework wird durch Lab in the Wild und die Online-Crowdsourcing-Plattform unterstützt. Lab in the Wild ist eine Online-Experiment-Plattform, auf der wir eine Vielzahl von Freiwilligen rekrutieren können. Im Vergleich zu Plattformen wie MTurk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, bietet Lab in the Wild hochwertige Daten. Wir haben zwei Aufgaben auf Lab in the Wild eingerichtet: eine für soziale Akzeptanz und eine für Toxizität und Hassrede.

Unsere Studie sammelte über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern. Jetzt sind wir besser in der Lage zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir haben herausgefunden, dass es in NLP Positionalität gibt. Zum Beispiel stimmen Datensätze und Modelle am meisten mit englischsprachigen Ländern überein. Für die soziale Akzeptanzanalyse von GPT 4 stimmen sie am meisten mit konfuzianischen und englischsprachigen Ländern überein. Auch Dynahate stimmt am meisten mit englischsprachigen Ländern überein.

Zusätzlich finden wir, dass Datensätze und Modelle am meisten mit Personen mit einem College- oder Hochschulabschluss übereinstimmen. Allerdings sind einige Gruppen weniger gut abgedeckt. So sind Datensätze und Modelle weniger mit nicht-binären Personen übereinstimmend als mit Männern und Frauen.

Angesichts der Positionalität in NLP geben wir folgende Empfehlungen:

1. **Dokumentation aller relevanten Design-Entscheidungen** während des Forschungsprozesses.
2. **Durchführung von NLP-Forschung mit einem perspektivischen Ansatz**.
3. **Erstellung spezialisierter Datensätze und Modelle für vier spezifische Gemeinschaften**. Ein gutes Beispiel ist die Masakhani-Initiative.

Inklusive NLP bedeutet nicht nur, dass alle Technologien für jeden funktionieren. Das war's. Vielen Dank für eure Aufmerksamkeit. Für weitere Informationen könnt ihr unseren Dashboard mit den neuesten Analyseergebnissen und unserem Papier besuchen.</sample>
    <sample id="97">Die Referentin geht auf zwei Hauptprobleme von SimulST ein:

1. **Spezifische Architekturen und Trainingsverfahren**: Aktuelle Modelle erfordern oft spezielle Architekturen und lange, komplexe Trainingsverfahren mit mehreren Optimierungsobjektiven.

2. **Mehrere Modelle für verschiedene Latenzregime**: Um verschiedene Latenzzeiten (z.B. 1 Sekunde vs. 2 Sekunden) zu erreichen, müssen mehrere Modelle trainiert und gewartet werden.</sample>
    <sample id="98">Soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen können effektiv reduziert werden durch mehrere Strategien:

1. **Diverse und ausgewogene Datensätze**: Sicherstellen, dass die Trainingsdaten eine breite Palette von Perspektiven und Meinungen abdecken, um Überrepräsentation bestimmter politischer Standpunkte zu vermeiden.

2. **Datensanierung und -filterung**: Implementieren von Mechanismen zur Identifizierung und Entfernung von verzerrten oder extremen Inhalten. Dies kann durch maschinelles Lernen, manuelle Überprüfung oder eine Kombination aus beidem erfolgen.

3. **Transparenz und Auditierung**: Offenlegen der Quellen und Inhalte der Trainingsdaten sowie regelmäßige Auditierungen, um sicherzustellen, dass keine unerwünschten Verzerrungen eingeschleust werden.

4. **Kontrollierte Experimente**: Durchführen von kontrollierten Experimenten, bei denen Modelle auf verschiedenen, parteiischeren Datensätzen weiter trainiert werden, um die Auswirkungen politischer Verzerrungen zu messen und zu verstehen.

5. **Ethikrichtlinien und -komitees**: Einrichten von Ethikkomitees, die die Datensätze und Trainingsmethoden überwachen und Empfehlungen zur Reduzierung von Verzerrungen geben.

6. **Förderung der Vielfalt im Team**: Einbeziehung von Experten mit unterschiedlichen Hintergründen und Perspektiven in den Datensammlungsprozess und die Modellentwicklung, um eine breitere Palette von Einwänden zu berücksichtigen.

Diese Ansätze können dazu beitragen, die politischen und sozialen Verzerrungen in NLP-Modellen zu minimieren und fairere und objektivere Ergebnisse zu erzielen.</sample>
    <sample id="99">## Einführung unserer Arbeit: "Wissensvermittlung aus großen Sprachmodellen für eingeschränkte Sprachplanung"

In unserem Alltag planen Menschen ihre Handlungen oft anhand von Schritt-für-Schritt-Anleitungen in Form von zielorientierten Skripten. Vorherige Arbeiten haben Sprachmodelle eingesetzt, um die abstrakten Ziele typischer Aktivitäten wie "einen Kuchen backen" zu planen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Diese Forschung konzentrierte sich jedoch hauptsächlich auf die Planung abstrakter Ziele von Standardaktivitäten. Die Planung von Zielen mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", bleibt ein weniger erforschtes Gebiet.

In dieser Arbeit definieren wir das Problem der **eingeschränkten Sprachplanung**, das verschiedene Einschränkungen auf die Ziele der Planung auferlegt. Ein abstraktes Ziel kann durch verschiedene spezifische Ziele mit vielschichtigen Einschränkungen vererbt werden. Ein guter Planer sollte Skripte erstellen, die sowohl sinnvoll als auch treu gegenüber den Einschränkungen sind.

Zunächst bewerten und verbessern wir die Fähigkeit großer Sprachmodelle, eingeschränkte Sprachplanung durchzuführen. Da es keine vorhandene Datensammlung mit spezifischen Zielen gibt, sammeln wir diese selbst. Wie in der Tabelle gezeigt, erweitern wir abstrakte Ziele mit vielschichtigen Einschränkungen mithilfe von InstructGPT für eine datengetriebene Mensch-im-Loop-Interaktion. Wir haben 100 spezifische Ziele gesammelt und die von großen Sprachmodellen generierten Skripte bewertet. Die Tabelle gibt die Gesamtgenauigkeit der Ergebnisse wieder. Wir stellen fest, dass alle Sprachmodelle bei der Planung spezifischer Ziele unbefriedigende Ergebnisse erzielen.

In einer detaillierten Analyse untersuchen wir, warum Sprachmodelle scheitern. Die Ergebnisse in der Grafik zeigen, dass die semantische Vollständigkeit der generierten Skripte akzeptabel ist, die Treue zu den Einschränkungen jedoch nicht garantiert werden kann. Wir untersuchen genauer die feineren Themenkategorien der in WikiHow definierten Einschränkungen. Die Wärmebildkarte in der Grafik zeigt, dass die Planungsleistung von InstructGPT für Ziele verschiedener Kategorien stark variiert. Vorherige Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen stark schwankt, was zu schlechten Leistungen führt. Daher greifen wir auf die Idee des "Übergenerierens und Filtern" zurück, um die Generierungsqualität zu verbessern.

Zunächst identifizieren und präsentieren wir den Typ der Einschränkungen mit Beispielen für InstructGPT und generieren spezifische Ziele basierend auf den Samen abstrakten Zielen. Dann generiert InstructGPT K Skripte für jedes spezifische Ziel und ein Filtermodell wird entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in Embeddings und berechnen die Kosinus-Ähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Ein Skript wird nur beibehalten, wenn das Ziel das höchste Punktedurchschnitt in der Zielmenge erreicht. Mit dieser Methode kann InstructGPT Skripte höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in Bezug auf semantische Vollständigkeit als auch auf Treue zu den Einschränkungen.

Da große Sprachmodelle kostspielig in der Implementierung sind, ist es entscheidend, Sprachplanungsfähigkeiten in kleineren und spezialisierten Modellen zu ermöglichen. Die Erstellung einer Datensammlung ist ein wichtiger Schritt in diese Richtung. Vorherige Studien haben jedoch keine Planung spezifischer Ziele ermöglicht und manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensvermittlung, um Datensätze für die eingeschränkte Sprachplanung aus großen Sprachmodellen zu destillieren.

Wir wenden unsere Methode an, um einen Datensatz für die eingeschränkte Sprachplanung namens **CoScript** zu erstellen. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testsets zu gewährleisten, bitten wir Crowdsourcer über das Internet, fehlerhafte Proben zu finden und zu korrigieren. Die Grafik zeigt die Verteilung der Einschränkungen in CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen aufweist. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt wurde, Skripte höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hinweist, dass kleinere Modelle bei angemessener Schulung auf geeigneten Datensätzen größere Modelle übertreffen können.

Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung etabliert, die Fähigkeit großer Sprachmodelle bewertet und eine Methode "Übergenerieren und Filtern" für große Sprachmodelle entwickelt. Wir haben große Sprachmodelle genutzt, um einen hochwertigen Skriptendatensatz **CoScript** für die eingeschränkte Sprachplanung zu erstellen. Wir hoffen, dass CoScript eine wertvolle Ressource für die weitere Forschung auf dem Gebiet der Sprachplanung darstellt. Weitere Details zu CoScript finden Sie in unserer Arbeit.</sample>
    <sample id="100">Die Präsentation behandelt PromptRank, eine innovative Methode für Multi-Hop Frage-Antwort-Systeme (QA), die komplexe Fragen erfordern mehrere Informationssprünge in einem Dokumentenkorpus. Im Gegensatz zu traditionellen Systemen, die Tausende von Trainingsbeispielen benötigen, ist PromptRank dateneffizient und erreicht gute Ergebnisse mit nur 128 Beispielen.

Die Methode kombiniert eine unsupervisierte Retrieval-Methode mit einem wenigen Schuss-Sprachmodell-Reranker. Zuerst werden mögliche Antwortketten mithilfe von TF-IDF-Abruf und Hyperlink-Traverse identifiziert. Dann werden diese Ketten mit einem Sprachmodell bewertet, das die Wahrscheinlichkeit berechnet, die Frage angesichts der Kette zu beantworten.

Der Schlüssel zu PromptRank liegt in der Konstruktion der "Chain-Prompt", die die Kette von Dokumenten in eine für das Sprachmodell verständliche Form bringt. Ein Anweisungs-Token und eine Anweisung, wie z.B. "Lesen Sie die vorherigen Dokumente und beantworten Sie die Frage", helfen dem Modell, über die Kette hinweg zu raten.

Die Präsentation zeigt, dass PromptRank im Vergleich zu vollständig überwachten Systemen und modernen dichten Retriever-Systemen wettbewerbsfähig abschneidet. Abhängig von der Qualität der Anweisungen und der Temperaturanpassung des Sprachmodells kann die Leistung weiter verbessert werden.

Zusammenfassend lässt sich sagen, dass PromptRank ein vielversprechender Ansatz ist, um die Anforderungen an Trainingsdaten für Multi-Hop QA zu reduzieren und leistungsstarke Fragebeantwortungssysteme zu ermöglichen.</sample>
    <sample id="101">Basierend auf der Präsentation von David Vilar und der durchgeführten Studien, ist die Sprachgewandtheit (Fluency) von PaLM **vergleichbar mit der von State-of-the-Art-Systemen**.

Die menschliche Bewertung ergab, dass PaLM flüssige Übersetzungen liefert, die in Bezug auf Klang und Stil mit den besten Systemen vergleichbar sind. Der Hauptunterschied liegt in der **Genauigkeit**, wobei PaLM häufig Teile des Quelltextes weglässt, um eine bessere klangliche Übersetzung zu erzielen, was zu Omissionen führt.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens, wie im Kontext des Papiers "Protecting the Copyright of Large Language Models" beschrieben, sind:

1. **Anwendbarkeit auf Embedding-as-a-Service (EaaS):** Das Verfahren sollte speziell für die Integration in EaaS-Dienste geeignet sein.
2. **Nicht-Degradation der Nutzbarkeit:** Das eingebettete Wasserzeichen darf die Qualität und Nutzbarkeit der bereitgestellten Embeddings nicht beeinträchtigen.
3. **Covertheit:** Es sollte für Angreifer schwer erkennbar sein, dass ein Wasserzeichen vorhanden ist, oder Angreifer sollten es leicht entfernen können.
4. **Transferierbarkeit:** Das Wasserzeichen muss während des Modell-Extraktionsprozesses an Angreifer übertragen werden können, um die Urheberschaft nachzuweisen.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, darunter Arabisch, Chinesisch und 12 weitere Sprachen.</sample>
    <sample id="104">Basierend auf dem präsentierten Inhalt extrahiert das Team **viele Instanzen** (mehr als 16.000) aus den ursprünglichen Datensätzen für die erneute Annotierung.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind:

1. **Cosine Similarity**
2. **L2 (Euclidean) Distance**
3. **KS (Kolmogorov-Smirnov) Test p-value**</sample>
    <sample id="106"># **QUEST: Ein Dataset für die Handhabung impliziter Set-Constraints in Informationssuche**

Die Arbeit präsentiert QUEST, ein innovatives Dataset, das sich mit der Herausforderung befasst, Informationssuchanfragen mit impliziten Set-Constraints zu verarbeiten. Die Autoren, in Zusammenarbeit mit Google DeepMind, führen zwei Szenarien ein: die Zoologen-Jane, die eine unbekannte Reptilart identifizieren möchte, und der Buchliebhaber Austin, der nach historischen Romanen aus Frankreich sucht. Diese Fälle veranschaulichen, wie Menschen oft komplexe Informationsbedürfnisse mit mehreren Einschränkungen oder Präferenzen ausdrücken.

Das QUEST-Dataset enthält 3.000+ Suchanfragen, die von Menschen verfasst und sorgfältig annotiert wurden. Jede Anfrage beinhaltet implizite Set-Operationen, und die Antwort-Entitäten wurden auf ihre Relevanz überprüft. Die Annotatoren markierten auch die Textabschnitte in den Dokumenten, die zu den verschiedenen Einschränkungen der Anfragen passen.

Der Aufbau von QUEST umfasst die Verwendung von Wikipedia-Kategorien aus vier Domänen und die Durchführung von Set-Operationen, um komplexe Anfragen zu generieren. Die Validierung der Anfragen und der Beweis der Relevanz der Antworten wurden von mehreren Annotatoren durchgeführt.

Die Bewertung des Systems erfordert die Fähigkeit, mehrdeutige Antworten aus einem großen Dokumentenkorpus zu extrahieren, wobei die Relevanz-Beweise aus verschiedenen Textabschnitten stammen können. Die Ergebnisse zeigen, dass die Leistung der Retriever verbessert werden kann, während End-zu-End-Systeme, einschließlich eines T5-Rerankers, mit niedrigen F1-Werten kämpfen, insbesondere bei Anfragen mit Set-Intersection und Set-Difference.

Das QUEST-Dataset zielt darauf ab, die Forschung im Bereich der Informationssuche mit selektiven Bedürfnissen voranzutreiben und die Entwicklung verbesserter Systeme zu fördern.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren (wie Encoder-PTR oder Encoder-Decoder Modelle wie mBART und mT5), wurden in dieser Aufgabe eingesetzt, um **quersprachliche semantische Parsingsaufgaben zu lösen**. Diese Modelle werden mit einem gemischten Trainingssatz verschiedener Sprachen trainiert, um ihre Leistung in verschiedenen natürlichen Sprachen und Bedeutungsdarstellungen zu verbessern. Sie erzielten die besten Ergebnisse auf allen neun Datensätzen im Vergleich zu Modellen, die nur auf einer Sprache trainiert wurden.</sample>
    <sample id="108">In ihrer ACL 2023-Veröffentlichung untersuchen Koustav Sinha und ihr Team die Robustheit von Sprachmodellen bei Akzeptanzurteilen im Kontext längerer Sequenzen. Die aktuelle Methode zur Bewertung, bekannt als Minimal Pair Paradigma (MPP), konzentriert sich auf kurze, einzelne Sätze. Die Forscher argumentieren, dass dies in der heutigen Ära großer Sprachmodelle mit großen Kontextfenstern nicht mehr ausreicht.

Um dies zu untersuchen, erweitern sie die MPP-Pipeline, indem sie Modelle dazu bringen, Akzeptanzurteile über längere Sequenzen hinweg zu treffen. Sie simulieren diese langen Sequenzen, indem sie akzeptable und inakzeptable Sätze aus bestehenden Datensätzen wie BLiMP und SyntaxGym kombinieren. Dabei stellen sie fest, dass die Urteile des Modells stark von der Herkunft der Sätze beeinflusst werden.

Die Ergebnisse zeigen, dass MPP-Urteile gegenüber irrelevanten Kontexten wie Wikipedia-Sätzen robust sind. Doch wenn Sätze aus demselben Datensatz verwendet werden, variieren die Urteile erheblich, je nachdem, ob der vorangestellte Satz akzeptabel oder inakzeptabel ist. Dieser Effekt verstärkt sich mit zunehmender Kontextlänge.

Die Analyse enthüllt, dass Sprachmodelle auf latente syntaktische und semantische Merkmale reagieren, die über Sätze hinweg geteilt werden. Die aktuelle MPP-Bewertung mit kurzen Eingaben erfasse möglicherweise nicht die gesamte abstrakte Wissensbasis der Modelle innerhalb ihres Kontextfensters.</sample>
    <sample id="109">**Zusammenfassung: Unnatural Instructions: Automatisierte Erstellung von Anweisungen für Sprachmodelle**

Die Präsentation behandelt die Herausforderung der Anweisungs-Tuning für Sprachmodelle, um deren Fähigkeit zu verbessern, Aufgaben ohne direkte menschliche Anleitung zu bewältigen. Der Vorschlag besteht darin, ein umfangreiches Dataset namens *Unnatural Instructions* zu erstellen, das natürliche Sprachanweisungen, Eingaben und Ausgaben umfasst.

Die Datenerstellung erfolgt vollständig automatisch, indem ein GPT-3-Varianten-Modell mit einem kleinen Satz von Beispielen aus einem bestehenden Dataset, *Super-Natural Instructions*, trainiert wird. Das Modell generiert dann weitere Anweisungen und Eingaben und erzeugt sogar Paraphrasen, um die Vielfalt zu erhöhen. Dieser Prozess führt zu einem Dataset mit 64.000 und bei Berücksichtigung der Paraphrasen zu 240.000 Beispielen.

Die Analyse zeigt, dass die generierten Anweisungen kreativ und vielfältig sind, einschließlich ungewöhnlicher Aufgaben wie der Überprüfung der Experimentenqualität oder der Erfindung neuer Wörter. Trotzdem sind über 50 % der Beispiele korrekt, was ihre Nützlichkeit für das Training unterstreicht.

Die Forscher haben ein 11-Milliarden-Parameter-T5-Modell mit *Unnatural Instructions* feinabgestimmt und seine Leistung im Vergleich zu anderen Methoden bewiesen. Die Ergebnisse zeigen, dass das automatisch generierte Dataset effektiv ist und langfristig Kosten und Aufwand im Vergleich zu manueller Annotation einspart. Diese Arbeit demonstriert das Potenzial von Sprachmodellen, qualitativ hochwertige Trainingsdaten zu erzeugen.</sample>
    <sample id="111">Die Autoren entscheiden, welche Wörter mittlere Häufigkeit haben, indem sie einen Textkorpus (eine allgemeine Textsammlung) analysieren und die Wortfrequenzen darin zählen. Sie wählen eine Gruppe von Wörtern aus, die sich in einem moderaten Frequenzintervall befinden – also nicht zu häufig und nicht zu selten vorkommen. Diese Wörter werden als "Trigger-Set" bezeichnet und später für die Einbettung von Watermarks verwendet.</sample>
    <sample id="112">## Präsentation: Funktionieren CoNLL-2003 Named Entity Tagger noch im Jahr 2023?

Guten Tag an alle, ich heiße Shuheng. Heute möchte ich euch unsere Arbeit vorstellen: „Funktionieren CoNLL-2003 Named Entity Tagger noch im Jahr 2023?“

In unserer Arbeit untersuchten wir das Problem der Generalisierung im Rahmen der Aufgabe Named Entity Recognition (NER). Die Verwendung von Modellen aus dem CoNLL-2003-Datensatz zur Entwicklung von NER-Systemen besteht seit fast 20 Jahren. Dies wirft natürlich einige Fragen auf:

* **Können diese Modelle auf moderne Daten übertragen werden?** Und was ist notwendig, um gute Generalisierungsfähigkeiten bei neuen Taggern zu erreichen?
* **Wenn wir eine Verschlechterung der Leistung beobachten, was sind die Ursachen dafür?**

Um diese Fragen zu beantworten, entwickelten wir das CoNLL++-Datenset. Es handelt sich dabei um einen Datensatz, den wir aus Reuters-Nachrichten aus dem Jahr 2020 zusammengestellt und mit den gleichen CoNLL-2003-Annotationsrichtlinien annotiert haben. Wir feinabstimmten über 20 Modelle auf CoNLL-2003 und bewerteten ihre Leistung sowohl auf den CoNLL-03-Testsets als auch auf dem CoNLL++. Um die Generalisierungsfähigkeit zu quantifizieren, berechneten wir den prozentualen F1-Anstieg für jedes Modell.

**Was ist notwendig für eine gute Generalisierung?**

Unsere Experimente zeigten drei Hauptfaktoren, die für eine gute Generalisierung entscheidend sind:

1. **Modellarchitektur:** Transformer-Modelle generalisieren in der Regel besser auf neue Daten.
2. **Modellgröße:** Größere Modelle führen in der Regel zu besserer Generalisierung.
3. **Anzahl der Beispiele für die Feinabstimmung:** Mehr Beispiele für die Feinabstimmung führen ebenfalls zu besserer Generalisierung.

**Was verursacht die Leistungsschwankung einiger Modelle?**

Wir hatten zwei Hypothesen:

1. **Adaptives Overfitting:** Dies tritt auf, wenn ein Modell durch wiederholtes Verwenden desselben Testdatensatzes übermäßig an diesen angepasst wird, was zu einer Abnahme der Leistung auf neuen Testdatensätzen führt.
2. **Zeitlicher Drift:** Dies ist die Leistungsabnahme, die durch den zunehmenden zeitlichen Abstand zwischen Trainings- und Testdaten verursacht wird.

Für adaptives Overfitting konnten wir in unseren Daten keine Hinweise auf eine abnehmende Leistung (d.h. eine Steigung der Best-Fit-Geraden größer als 1) feststellen.

Für zeitlichen Drift zeigten unsere Experimente, dass die Leistung mit zunehmendem zeitlichen Abstand zwischen Training und Testdaten abnimmt, was unsere Hypothese bestätigt.

**Fazit:**

Für eine gute Generalisierung benötigen wir eine verbesserte Modellarchitektur, größere Modelle und mehr Beispiele für die Feinabstimmung. Diese Faktoren sind miteinander verbunden – wir können nicht nur einen Faktor optimieren und die anderen vernachlässigen.

Überraschenderweise haben wir herausgefunden, dass die Leistungsschwankungen nicht durch adaptives Overfitting, sondern hauptsächlich durch zeitlichen Drift verursacht werden, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird.

**Zusammenfassend:**

Die Antwort auf unsere Frage in der Titel unserer Arbeit lautet: **Ja, CoNLL-2003 Tagger funktionieren noch im Jahr 2023.**

Wir hoffen, dass unsere Arbeit weitere Forschung zur Verbesserung der Generalisierungsfähigkeit von Modellen anregt.

Schaut euch unsere Arbeit und unseren Datensatz an und zögert nicht, mir Fragen zu stellen. Vielen Dank!</sample>
    <sample id="114">**Abstract: "Finding the Pillars of Strength for Multi-Head Attention"**

Unsere Arbeit, präsentiert auf ACL 2023, zielt darauf ab, die Herausforderungen großer Sprachmodelle (LLMs) hinsichtlich ihrer Parametergröße und Trainingszeit anzugehen. Wir schlagen eine neue Methode namens "Gruppen-gesteuerte Aufmerksamkeit" (GHT) vor, um Redundanz in Multi-Head-Aufmerksamkeitsmechanismen zu reduzieren.

GHT verwendet eine Divide-and-Conquer-Strategie, die in zwei Phasen aufgeteilt ist. Zunächst wird durch "Gruppen-beschränkte Ausbildung" die Aufmerksamkeit in Gruppen unterteilt, wodurch ähnliche intra-gruppige und unterschiedliche inter-gruppige Köpfe entstehen. Anschließend wendet die "Voting-to-Stay"-Algorithmus prunende Schritte an, um nur einen Kopf pro Gruppe beizubehalten, was zu erheblicher Parameterkompression führt.

Unsere Experimente auf drei Aufgaben – maschinelle Übersetzung, abstrakte Zusammenfassung und Sprachmodellierung – zeigen die Effektivität von GHT und seiner verbesserten Version GHT-PS. GHT-PS erreicht bis zu 4,4% BLEU-Verbesserung bei maschineller Übersetzung und 7% Verbesserung bei abstrakten Zusammenfassungen mit 32,1% Parameterkompression.

Zusätzlich zeigen wir, dass unsere LITE-Modelle 90% der Parameter prunen, die Inferenzgeschwindigkeit um 62% erhöhen und die FLOPs um 80% reduzieren, während sie vergleichbare Leistung wie ihre voll ausgestatteten Gegenstücke erbringen. Wir schlagen vor, dass zukünftige Forschung auf automatisierter, aufgabenbezogener Pruning-Techniken für LLMs fokussiert sein sollte.</sample>
    <sample id="115">Der Ansatz verwendet ein Sprachsegment (Lambda) von **letzten 100 bis 200 Millisekunden** des empfangenen Audios. Dies wird im Text als "lambda speech frames" bezeichnet.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass "Servin ein Richter ist" und "Kea ein Bäcker ist". Dieses Wissen hilft dabei zu bestimmen, auf welche Person sich das Pronomen "er" in dem Satz "Nach einem langen Arbeitstag entschied er Fälle in einem Gericht" bezieht.</sample>
    <sample id="117">Der wichtigste Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist **die Qualität des Beispiels**. Laut der Präsentation ist die Qualität der Beispiele entscheidender für die Leistung großer Sprachmodelle (LLMs) bei der Übersetzung als die Ähnlichkeit mit dem Ausgangssatz. Hochwertige Beispiele führen zu besseren Ergebnissen.</sample>
    <sample id="118">In ihrer ACL 2023-Präsentation stellen die Autoren ihre Arbeit "Verbesserung von Vorab-Trainingstechniken für code-geschaltete NLP" vor. Code-Switching, die Verwendung mehrerer Sprachen in einem Satz, ist in sprachlich vielfältigen Gemeinschaften wie Indien weit verbreitet. Aktuelle mehrsprachige Modelle wie mBERT und XLM-R schneiden bei code-geschalteten Aufgaben wie Fragebeantwortung und Sentimentanalyse schlecht ab.

Um dies zu verbessern, schlagen die Autoren **SwitchMLM** vor, ein neuartiges Vorab-Training (MLM) für Code-Switching. SwitchMLM konzentriert sich auf **Switch-Punkte**, wo die Sprachen wechseln, und maskiert nur diese Wörter. Eine Alternative, **FrequencyMLM**, nutzt Worthäufigkeiten in Monolingual-Korpora zur Schätzung von Switch-Punkten.

Zusätzlich zu diesem MLM-Ansatz schlagen die Autoren **architektonische Modifikationen** vor:

* **Residuale Verbindungen:** Verbindungen zwischen tieferen und höheren Schichten von BERT erhöhen die Switch-Punkt-Information in den höheren Schichten.
* **Auxiliärer LID-Verlust:** Dieser Verlust zwingt eine bestimmte Schicht, mehr Informationen über Sprachwechsel zu lernen.

Durch die Kombination dieser Techniken erzielten die Autoren in Sentimentanalyse-Aufgaben die besten Ergebnisse.

**Probing-Experimente** bestätigten, dass die vorgeschlagenen Methoden tatsächlich mehr Switch-Punkt-Information in den Schichten von BERT einbringen.

Zusammenfassend bieten die Autoren einen neuartigen Ansatz für das Vorab-Training, der Code-Switching-Phänomene effektiver berücksichtigt und die Leistung in code-geschalteten NLP-Aufgaben verbessert.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf **Sprachmodelle, die auf großen Web-Crawl-Daten trainiert wurden**, insbesondere solche, die **politische Nachrichtenmedien** enthalten. 

Konkret werden Modelle wie **GPT-4, GPT-Serie, BART-Serie** und **RoBERTa** untersucht.  Die Forschung analysiert, wie sich die **politischen Vorurteile** in diesen Modellen manifestieren und wie diese Vorurteile die Leistung der Modelle in **Downstream-Aufgaben** wie **Hassrede- und Fake-News-Detektion** beeinflussen.</sample>
    <sample id="120">Das Modell kombiniert Aufmerksamkeitswerte aus mehreren Ebenen. Es nutzt die Kreuzaufmerksamkeit zwischen Audio-Eingabe und Text-Ausgabe, wobei die Aufmerksamkeit auf verschiedene Teile der Audio-Daten hinweist, um zu entscheiden, wann und welche Wörter übersetzt und ausgegeben werden sollen.</sample>
    <sample id="121">Beispiele für direkte Inferenz (oder direkte Referenzen) sind:

- "Sagen Sie 'Easy on Me' oder 'I Gotta Feeling'." (direkt auf die Songnamen verweisend)
- "Die erste Songauswahl." (verweist auf die Position in einer Liste)

Direkte Referenzen sind klar und spezifisch auf eine einzelne Entität ausgerichtet.</sample>
    <sample id="122">Die Autoren gehören der Fudan University an.</sample>
    <sample id="123">In ihrer Präsentation untersuchen Ying und Zhiyang die Möglichkeiten von MultiInstruct, einem neuen Ansatz zur Verbesserung des mehrmodalen Zero-Shot Lernens über Anweisungs-Tuning. Mit dem Fortschritt großer Sprachmodelle haben Forscher begonnen, neue Lernparadigmen zu erkunden, um vorab trainierte Modelle effizient für verschiedene Aufgaben wiederzuverwenden. Während Anweisungs-Tuning bereits in der Sprachverarbeitung erfolgreich eingesetzt wurde, blieb die Anwendung auf Computer Vision und mehrmodale Aufgaben ununtersucht.

Die Autoren stellen MultiInstruct vor, ein erstes Benchmark-Dataset für mehrmodales Anweisungs-Tuning, das 62 vielfältige mehrmodale Aufgaben aus 21 offenen Datensätzen umfasst. Jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Sie verwenden OFA, ein einheitliches mehrmodales vorab trainiertes Modell, und formulieren alle Aufgaben in einer sequenziellen Format, um unterschiedliche Eingabe- und Ausgabedatenarten zu vereinen.

Ihre Experimente zeigen, dass Anweisungs-Tuning die Leistung von OFA auf gesehene mehrmodale Aufgaben erheblich verbessert. Transferlernen aus natürlichen Anweisungs-Datensätzen verbessert diese Leistung weiter. Ein neu eingeführter Metrik, die Sensitivität, misst die Konsistenz der Modell-Ausgaben bei leicht variierenden Anweisungen. Die Ergebnisse deuten darauf hin, dass die Verwendung mehrerer Anweisungen die Modellleistung und -stabilität verbessert.

Zukünftig planen die Forscher die Veröffentlichung eines noch größeren Datensatzes mit 150 zusätzlichen mehrmodalen Anweisungs-Aufgaben.</sample>
    <sample id="124">In ihrer Präsentation diskutiert Tan Qingyu von der National University of Singapore und Alibaba ihre Arbeit zur Verbesserung der zeitlichen Begriffsfähigkeit großer Sprachmodelle (LLMs). Sie unterteilen zeitliche Begriffsfähigkeit in drei Ebenen: Zeit-zu-Zeit-Begriff (z.B. "Was ist das Jahr nach 2010?"), Zeit-zu-Ereignis-Begriff (z.B. "Für welches Team spielte Lionel Messi 2010?") und Ereignis-zu-Ereignis-Begriff (z.B. "Für welches Team spielte Lionel Messi nach FC Barcelona?").

Vorherige Studien konzentrierten sich hauptsächlich auf L2-Begriff (Zeit-zu-Ereignis), während die Forscher eine umfassendere Analyse anstreben. Sie führten Experimente durch, um Biases bei der Vorhersage von Jahren zu identifizieren, wobei Modelle wie T5-L, FLAN-T5-L und ChatGPT getestet wurden. ChatGPT zeigte zwar fast eine Lösung für Jahresvorhersagen, aber seine Leistung verschlechterte sich stark bei Monatsvorhersagen.

Als Reaktion darauf schufen sie das TempReason-Dataset, das alle drei Begriffsstufen und einen langen zeitlichen Rahmen abdeckt. Sie schlugen drei QA-Einstellungen vor: Closed Book, Open Book und Reasoning QA (mit bereitgestellten zeitlichen Informationen).

Um die zeitliche Begriffsfähigkeit zu verbessern, schlugen sie eine Trainingsstrategie mit zwei Komponenten vor: Temporal Span-Extraktionstraining und zeitabhängiges Verstärkungslernen. Ihr Modell, TempT5, übertraf andere Modelle in verschiedenen Testsetzungen, insbesondere in komplexeren Aufgaben wie Reasoning QA.

Die Studie beleuchtet Schwächen bei der zeitlichen Begriffsfähigkeit von LLMs und bietet einen neuen Benchmark und Trainingsansatz für zukünftige Verbesserungen.</sample>
    <sample id="125">Basierend auf dem von dir bereitgestellten Text, sind **mindestens** zwei Autoren an der Arbeit beteiligt. Der Text erwähnt Yanis Labrak und die Entwicklung des DrBERT-Modells. Außerdem wird auf mehrere Modelle und Experimente eingegangen, die von einem Team von Forschern durchgeführt wurden, was auf mehrere Autoren hindeutet.

Eine präzise Anzahl der Autoren wird jedoch nicht explizit genannt.</sample>
    <sample id="126">Ja, die Präsentation erwähnt explizit, dass die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe des Google Translate API als Teil der Baseline-Methode betrachtet wurde. Dies geschah im Rahmen der "Translate-Test"-Einstellung, wo die Anfrage in die Zielsprache übersetzt und dann ein monolinguales Modell für das semantische Parsing verwendet wurde.</sample>
    <sample id="127">In ihrer Präsentation stellen Namgyu Ho, Laura Schmid und Se-Young Yun ihre Forschung vor, die darauf abzielt, die Fähigkeiten großer Sprachmodelle (LLMs) in kleinere Modelle zu übertragen. Sie präsentieren die Methode "Large Language Models Are Reasoning Teachers", die darauf abzielt, die Einschränkungen von Chain-of-Thought-Prompting (CoT) zu überwinden.

Traditionell erfordert CoT, bei dem Modelle komplexe Aufgaben Schritt für Schritt lösen, sehr große Modelle wie GPT-3 oder PALM. Die Forscher umgehen dieses Problem, indem sie diese großen Modelle als Lehrer nutzen, um kleinere Modelle zu trainieren. Sie generieren Schritt-für-Schritt-Lösungen für komplexe Aufgaben als Trainingsdaten für kleinere Modelle.

Ein zentraler Beitrag ist die Einführung von "Diverse Reasoning", einer Technik, die mehrere unterschiedliche Lösungswege für eine Aufgabe erzeugt. Diese Vielfalt verbessert das Training der Schülermodelle erheblich. Experimente zeigen, dass die so trainierten Modelle, genannt "fine-tuned CoT", komplexe Aufgaben, einschließlich Textverständnis und mathematischer Berechnungen, erfolgreich lösen können.

Die Studie vergleicht verschiedene Methoden und hebt die Skalierbarkeit und Effektivität der vorgeschlagenen Methode hervor. Sie betont auch die Herausforderungen und Trade-offs, wie Entwicklungskosten und Inferenzkosten, die bei der praktischen Anwendung zu berücksichtigen sind. Die Forscher stellen ihren Code und Daten zur Verfügung, um die Reproduzierbarkeit zu fördern und zukünftige Arbeiten zu unterstützen.</sample>
    <sample id="128"># **The KITMUS Test: Evaluating Multi-Source Knowledge Integration in NLU**

Dieses Papier präsentiert den KITMUS-Test (Knowledge Integration from Multiple Sources), ein Diagnosetool zur Bewertung der Fähigkeit von Natural Language Understanding (NLU)-Modellen, Wissen aus verschiedenen Quellen zu integrieren. Die Autoren betonen, dass moderne NLU-Modelle sowohl vorab erlerntes Wissen (aus Parameter-Trainings) als auch kontextabhängiges Wissen bei der Inferenz nutzen.

Der KITMUS-Test konzentriert sich auf Coreference-Auflösung, um die Fähigkeit der Modelle zu untersuchen, auf verschiedene Wissensarten zuzugreifen. Es werden drei Szenarien definiert: "Background-Pretrain" (nur vorab erlerntes Wissen), "Background-Both" (beide Wissensarten verfügbar) und "Background-Inference" (nur kontextabhängiges Wissen). Die Studie zeigt, dass Modelle ohne spezifisches Training auf KITMUS schlecht abschneiden, aber mit Training ihre Leistung verbessern können.

Die Ergebnisse deuten darauf hin, dass einige Modelle lernen, Wissen aus verschiedenen Quellen zu integrieren, während selbst die besten Modelle Schwierigkeiten haben, rückwärts gerichtetes Wissen (nur bei der Inferenz verfügbar) zuverlässig zu nutzen. Die Arbeit unterstreicht die Notwendigkeit von task-spezifischem Training für komplexe NLU-Aufgaben, bei denen die Integration mehrerer Wissensquellen erforderlich ist.</sample>
    <sample id="129">Die Autoren haben mehrere markierte Gruppen als Beispiele gegeben, darunter:

* **Asiatische Frau:** Beschrieben als "klein" ("petite"), "zart" ("delicate"), "seidenweich" ("silky") - reflektiert historische Stereotype und Sexualisierung.
* **Mittelöstliche Frau:** Beschrieben mit Wörtern wie "exotisch" und "verführerisch" - verstärkt das "Andere" und "Mysteriöse" Stereotyp.
* **Schwarze Frau:** Beschrieben mit "stark" ("strong") und "resilient" - verfestigt das "Starke Schwarze Frau"-Stereotyp, das trotz positiver Darstellung schädlich sein kann.

Diese Beispiele verdeutlichen, wie positive oder neutrale Beschreibungen oft tief verwurzelte, aber schädliche Stereotype und Narrative verstärken können.</sample>
    <sample id="130">Basierend auf der Präsentation generalisieren **CoNLL-2003-basierte Modellarchitekturen** nicht gut auf moderne Daten. Die Studie zeigt, dass Transformer-Modelle in der Regel besser generalisieren, während traditionelle Modelle möglicherweise an Leistung einbüßen.</sample>
    <sample id="131">Im englischen Inhalt wird nicht explizit der Name der Testdatensätze erwähnt. Es wird jedoch diskutiert, dass die Methoden in der Weakly Supervised Learning (WSL) traditionell davon ausgehen, dass ein zusätzlicher **sauberer Validierungssatz** verfügbar ist. Dieser Punkt hebt die Notwendigkeit von manuellen Annotationen in WSL hervor und stellt die Effektivität dieser Methoden in Frage.

Zusammengefasst werden die Testdatensätze im Kontext der Forschung nicht namentlich genannt, sondern vielmehr die Notwendigkeit und Auswirkungen von sauberen Validierungsdaten auf die Leistung der Modelle betont.</sample>
    <sample id="132">An der Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" sind zwei Autoren beteiligt: Akshatha und Martin. Sie präsentieren ihre gemeinsame Forschung, die eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research ist.</sample>
    <sample id="133">Die Autoren arbeiten mit **mehreren Modalitäten**. Der Titel der Präsentation ("MultiInstruct") und der Inhalt verweisen klar auf die Untersuchung von Multi-Modal-Lernansätzen, insbesondere auf die Verbesserung der Zero-Shot-Leistung durch Instruction Tuning für Aufgaben, die Text, Bilder und ggf. Bounding Boxes umfassen.</sample>
    <sample id="135">In ihrer Präsentation stellen James und Sarah Finch das innovative Evaluierungssystem ABC-Eval (annotating behaviors in chat) vor, das von der Emory NLP Lab unter der Leitung von Professor Jinho Choi entwickelt wurde. ABC-Eval bietet eine neue, dimensionale Herangehensweise an die Bewertung von Konversations-KI.

Aktuell werden Konversationsmodelle meist durch menschliche Bewertungen, wie z.B. die Auswahl der besseren Konversation oder Likert-Skalen, beurteilt. Während diese Methoden insgesamt die Dialogqualität bewerten, gibt es viele feinere Aspekte. Daher schlagen die Finches eine präzisere Strategie vor: die Annotation von Verhaltensweisen in Dialogen.

ABC-Eval misst die Häufigkeit verschiedener thematischer Fehler, wie irrelevante Aussagen, Widersprüche, Halluzinationen oder Verstöße gegen Alltagswissen. Durch die Analyse von vier State-of-the-Art-Modellen auf 100 menschlichen-Bot-Konversationen pro Modell, verglichen mit traditionellen Methoden, zeigte ABC-Eval überlegene Zuverlässigkeit und Vorhersagekraft für die Gesamtqualität.

Die Studie ergab, dass ABC-Eval-Labels präziser und informierender sind als bestehende Methoden. Jedes ABC-Eval-Metrik erfasst einzigartige Aspekte der Konversationsqualität, während herkömmliche Likert-Skalen weniger aussagekräftig sind.

Die Finches betonen, dass ABC-Eval, trotz verbleibender Herausforderungen, einen Fortschritt in der präzisen Bewertung von Konversations-KI darstellt und hoffen, dass es von der Forschungsgemeinschaft angenommen wird.</sample>
    <sample id="136"># **Zusammenfassung: FERMAT - Eine Alternative zur Genauigkeit für numerisches Denken**

Jasivan präsentiert seine Forschung an der Universität Sheffield, die sich mit den Herausforderungen beim numerischen Denken in Sprachmodellen befasst. Die Arbeit zielt darauf ab, die Schwächen der aktuellen Bewertungsmetriken und Benchmarks zu beheben, die oft nur auf Genauigkeit basieren.

Der Vortrag beginnt mit einem praktischen Beispiel aus der Infotab-Plattform, wo die Klassifizierung von Aussagen erfordert, ob sie eine Entnahme, einen Widerspruch oder neutral sind, basierend auf einer Tabelle. Hier zeigt sich, dass Sprachmodelle, insbesondere größere Modelle, bei numerischen Inferenzen variierende Leistungen erbringen.

Die Forscher stellen FERMAT (Flexible Evaluation Set based on Arithmetic Types) vor, einen neuen Bewertungsansatz, der mathematische Fähigkeiten in Sprachmodellen testet. FERMAT umfasst Fragen aus Illinois und CommonCore, die verschiedene numerische Konzepte abdecken, einschließlich Zahlverständnis, mathematischer Operationen und Trainingsabhängigkeit. Die Fragen variieren in ihrer Darstellung und Komplexität.

Die Ergebnisse zeigen, dass die meisten Modelle, einschließlich des ursprünglichen Benchmark-Datensatzes, in verschiedenen Aspekten schlecht abschneiden. Durch die Generierung von 200.000 Trainingsbeispielen und die Feinabstimmung von Modellen mit Hilfe von Mathematiklehrern verbessert sich die Leistung. Die Studie betont die Bedeutung von Sprach- und Mathematikdiversität und schlägt vor, dass die Wortwahl und die Art der Präsentation von Zahlen während des Trainings und Testens entscheidend sein können.

Abschließend argumentieren die Autoren, dass FERMAT eine umfassendere Bewertungsmethode bietet, die die Schwächen der aktuellen Benchmarks überwindet und Bereiche wie Tokenisierung und Zahlencodierung identifiziert, die für Verbesserungen im numerischen Denken von Sprachmodellen relevant sind.</sample>
    <sample id="137"># **Tell2Design: Ermöglichen von Sprachgesteuerter Bodenplanerstellung**

Die Arbeit von Sicong et al. aus der Singapore University of Technology and Design präsentiert *Tell2Design*, ein Dataset und ein Modell für die Erstellung von Bodenplänen basierend auf natürlicher Sprache. Ziel ist es, den Designprozess demokratischer zu gestalten, indem Menschen ohne Design-Expertise ihre eigenen Raumkonzepte durch Sprachanweisungen erstellen können.

Das Projekt definiert eine neue Herausforderung im maschinellen Lernen: die Generierung von 2D-Bodenplänen, die spezifischen Sprachanweisungen entsprechen. Die Eingabe besteht aus Textanweisungen, die Semantik, Geometrie und Topologie des Raums beschreiben, während die Ausgabe eine strukturierte Innenraumgestaltung ist.

Die Forscher haben ein Dataset mit 5.051 menschlich annotierten und 76.000 synthetisch erzeugten Anweisungen erstellt. Sie verwenden ein Encoder-Decoder-Framework mit einem Transformer-basierten Modell, um die Bodenplanerstellung als Sequenz-zu-Sequenz-Problem zu behandeln. Dies ermöglicht die Verarbeitung verschiedener Anweisungslängen und Komplexitäten.

Die Ergebnisse zeigen, dass das vorgeschlagene Modell, *T2D*, im Vergleich zu Text-zu-Bild-Generierungsmodellen überlegen ist, was durch hohe Intersection-over-Union (IoU) Scores belegt wird. Die Studie hebt die Herausforderungen bei der Verarbeitung unstrukturierter Textanweisungen und der Ambiguität hervor, aber die Verwendung von synthetischen Anweisungen zur Vorabschulung verbessert die Leistung.

*Tell2Design* bietet ein wertvolles Dataset und einen Ansatz für die Sprachgesteuerte Gestaltung, mit dem Ziel, den Designprozess durch die Integration von Benutzerpräferenzen in natürlicher Sprache zu verbessern.</sample>
    <sample id="138">Nach Ansicht der Autoren ist die Integration von **Kenntnissen aus verschiedenen Quellen** bei natürlicher Sprachverarbeitung (NLU) ein zu wenig erforschtes Gebiet. Ihr Papier, "The KITMUS Test", zielt darauf ab, diese Lücke zu schließen, indem es einen Diagnosetest für die Fähigkeit von Modellen entwickelt, sowohl vorab erlerntes als auch bei der Inferenz bereitgestelltes Wissen zu integrieren.</sample>
    <sample id="139">Die Referenten sind Ying und Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Nach der Generierung wurde das Dataset von crowd-sourced Arbeitern überprüft und korrigierte Fehlproben entfernt, um sicherzustellen, dass die Qualität der spezifischen Ziele und Skripte gewährleistet ist.</sample>
    <sample id="141">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in mehreren Bereichen:

1. **Begrenzte Abdeckung von Kontextabhängigkeiten**: Nur ein kleiner Teil der Übersetzungen hängt vom Kontext ab, was es schwierig macht, corpus-basierte Metriken wie BLEU zu verwenden, um diese Fälle zu erfassen.

2. **Domänen- und Sprachabhängigkeit**: Ressourcen, die auf domänenspezifisches Wissen und menschliche Kuratierung angewiesen sind, sind oft auf begrenzte Sprachpaare und Domänen beschränkt.

3. **Mangel an Automatisierung**: Die Erstellung und Pflege solcher Ressourcen erfordert erhebliche manuelle Arbeit, was ihre Skalierbarkeit einschränkt.

Diese Ressourcen sind oft unzureichend, um die gesamte Bandbreite kontextabhängiger Übersetzungen in verschiedenen Sprachen und Kontexten abzudecken.</sample>
    <sample id="142">##  Entscheiden zwischen Entitäten: Die AltEntities Corpus-Datenmenge

Hallo! Ich möchte Ihnen unsere Arbeit vorstellen: **"Resolving Indirect Referring Expressions for Entity Selection"**, in der wir das **AltEntities Corpus** vorstellen. Gemeinsam mit Filip Radlinski, Silvia Pareti und Annie Louis untersuchen wir, wie Nutzer ihre Sprache verwenden, wenn sie eine Wahl treffen möchten.

Stellen Sie sich die Situation vor: Ein Nutzer möchte zwischen zwei Liedern wählen, zum Beispiel "Easy on Me" oder "I Gotta Feeling". Ein direkter Verweis wäre hier offensichtlich: Der Nutzer könnte den Liednamen nennen oder die Position in einer Liste angeben. Manchmal ist jedoch ein indirekter Verweis angemessener für einen natürlicheren Dialog. Dies kann der Fall sein, wenn:

* Der Nutzer den Liednamen nicht mehr weiß.
* Die Aussprache ähnlich ist und eine Unterscheidung schwierig ist.
* Der Nutzer eine Präferenz ausdrücken möchte.

Unser Fokus liegt auf diesem wichtigen Problem in konversationsbasierten Systemen und für die Bewertung von Sprachmodellen (LLMs). Da es keine umfassende öffentliche Datensammlung für diese Aufgabe gibt, haben wir selbst eine erstellt.

**Das AltEntities Corpus:**

Unser Corpus umfasst **3 Domänen:** Musik, Bücher und Rezepte. Die Erstellung der Datensätze erfolgte mit einem Fokus auf Informalität mithilfe eines Cartoon-Completions-Verfahrens.

* **Cartoon:** Das Cartoon besteht aus drei Sprechblasen. In der ersten Sprechblase fragt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?"

* **Alternative Frage:** In der zweiten Sprechblase stellt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?"

* **Indirekter Verweis:** In der dritten Sprechblase wählt Bob eine der beiden Entitäten über einen indirekten Verweis aus, z.B. "das neuere".

Die ersten beiden Sprechblasen werden automatisch generiert. Die dritte Sprechblase (der indirekte Verweis) wird von einem Annotator ausgefüllt.

**Datenerstellung:**

Für die zweite Sprechblase (die alternative Frage) verwenden wir ein einfaches Template: "Meinst du A oder B?". A und B sind zufällig aus Wikipedia ausgewählte Entitäten.

Wir haben verschiedene Sampling-Methoden verwendet, um Entitäten mit zunehmender Ähnlichkeit zu generieren:

* **Zufällige Auswahl:** Entitäten werden willkürlich aus Wikipedia ausgewählt.
* **Ähnliche Titel:** Entitäten haben ähnliche Namen, z.B. zwei Bücher mit dem Titel "Die Rückkehr".
* **Ähnliche Beschreibungen:** Entitäten haben ähnliche Beschreibungen auf Wikipedia.
* **Ähnliche Attribute:** Entitäten haben gemeinsame Merkmale, z.B. gleiche Genre oder gleiche Künstler.

**Annotatoren:**

Die Annotatoren wussten die Namen der Entitäten, nicht aber Details zu ihnen. Daher erhielten sie Hintergrundinformationen:

* **Musik:** Google-Suchlinks zu den Liedern und die Aufforderung, mindestens einen Teil jedes Liedes zu hören und zu lesen.
* **Bücher und Rezepte:** Wikipedia-Textausschnitte und für Rezepte auch Bilder aus Wikipedia.

Die Annotatoren wurden dann gebeten, eine der Entitäten auszuwählen und diese mithilfe von 3-5 indirekten Verweisen zu beschreiben. Beispiele: "das Lied ohne Worte", "nicht das mit dem 12-jährigen Jungen" oder "das fiktive Lied" usw.

**Ergebnisse:**

Das **AltEntities Corpus** enthält 6.000 alternative Fragen und 42.000 indirekte Verweise über die drei Domänen hinweg.

Wir haben die Leistung von Sprachmodellen wie dem T5 XL getestet.

* Mit vollem Zugriff auf die gleichen Hintergrundinformationen wie die Annotatoren erreichen die Modelle eine Genauigkeit von 92-95%.
* Bei teilweise übereinstimmenden Hintergrundinformationen liegt die Genauigkeit bei 82-87%.
* Bei nur Zugriff auf die Entitätsnamen sinkt die Genauigkeit auf 60%.

Unsere Ergebnisse zeigen auch die Generalisierungsfähigkeit der Modelle über die Domänen hinweg.

**Verfügbarkeit:**

Der Link zu unserem **AltEntities Corpus** finden Sie hier: [Link zur Datensammlung]</sample>
    <sample id="143">Der Ansatz "Attention as a Guide for Simultaneous Speech Translation" (EDAtt) wird verglichen mit folgenden bestehenden SimulST-Richtlinien:

1. **Wait-k Strategie**: Eine populäre Methode, die darauf abzielt, die Ausgabe zu verzögern, um sicherzustellen, dass die Übersetzung stabiler und genauer ist.
2. **Local Agreement**: Eine weitere gängige Strategie, die die Konsistenz innerhalb der übersetzten Sätze verbessert.
3. **State-of-the-art Architektur speziell für SimulST**: Ein speziell für gleichzeitige Sprachübersetzung entwickeltes Modell, das als Benchmark dient.

Der EDAtt-Ansatz übertrifft diese Methoden in Bezug auf sowohl die Übersetzungsqualität (gemessen durch BLEU) als auch die Latenz (durchschnittliches Verzögern und rechnerisch bewertetes Verzögern).</sample>
    <sample id="144">Basierend auf dem präsentierten Inhalt gehören die Autoren der **Nantes University** an. Der Text erwähnt explizit "Nantes University Hospital data warehouse" und die Entwicklung eines Modells namens **ChuBERT**, was auf eine Verbindung zu dieser Universität hinweist.</sample>
    <sample id="145">Die Referentin heißt Jenny.</sample>
    <sample id="146">Yicheng, ein PhD-Student von Fudan University, präsentiert seine Forschung zur Analyse von Omissionen in der Dialogzusammenfassung. Dialogzusammenfassung, ein Unterbereich der Textzusammenfassung, zielt darauf ab, die wichtigsten Informationen aus Dialogen in prägnanten Zusammenfassungen zu extrahieren.

Trotz Fortschritten mit großen, vorab trainierten Sprachmodellen, bleiben Zusammenfassungen oft fehlerhaft, einschließlich faktischer Inkonsistenzen und Omissionen, die kritische Informationen weglassen. Yichengs Forschung konzentriert sich auf dieses Problem, das in etwa 70% der generierten Zusammenfassungen auftritt, unabhängig von Domain oder Modell.

Um Omissionen systematisch zu analysieren, haben die Autoren das OLDS-Dataset erstellt, das auf fünf bestehenden Benchmarks basiert und diverse Zusammenfassungen für jede Dialogeinstellung bietet. Sie untersuchen drei verschiedene Modellarchitekturen für die Omissionenerkennung und erreichen eine F1-Score von etwa 50%.

Darüber hinaus zeigen sie, dass die Integration erkannten Omissionen in den Zusammenfassungsprozess die Qualität erheblich verbessern kann. Die Präsentation schließt mit der Betonung der Wichtigkeit der Omission-Analyse und der vielversprechenden Perspektive für zukünftige Verbesserungen in der Dialogzusammenfassung.</sample>
    <sample id="147">An der Arbeit "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sind drei Autoren beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler, und ich möchte Ihnen kurz die Arbeit "Attention als Leitfaden für die gleichzeitige Sprachübersetzung" vorstellen, die in Zusammenarbeit mit Matteo Negri und Marco Turchi entstanden ist. Was ist gleichzeitige Sprachübersetzung (SimulST)? SimulST ist der Prozess, gesprochene Sprache in Echtzeit in Text in einer anderen Sprache zu übersetzen, was die Kommunikation zwischen verschiedenen Sprachen ermöglicht.

Welche Probleme haben die aktuellen SimulST-Modelle? Üblicherweise werden spezifische Architekturen trainiert, was zusätzliche Module und eine komplexe Trainingsroutine erfordert. Die Trainingsverfahren sind oft lang und umfassen verschiedene Optimierungsziele. Außerdem müssen mehrere Modelle für unterschiedliche Latenzzeiten trainiert und gewartet werden.

Unsere Lösung: Wir schlagen vor, bereits vorhandene Offline-ST-Modelle (Sprachübersetzung) ohne Retraining oder Anpassung der Architektur für SimulST zu nutzen. Anstatt mehrere Modelle für verschiedene Latenzzeiten zu trainieren, verwenden wir für jede Latenzregime nur ein Modell und steuern die Latenz durch spezifische Parameter. Wir nutzen dabei das Wissen, das das Modell durch den Aufmerksamkeitsmechanismus zwischen Audioeingabe und Textausgabe bereits erworben hat, also die Kreuzaufmerksamkeit.

Unsere Lösung heißt EDAtt (Encoder-Decoder Attention) und ist eine Strategie, bei der wir entscheiden, ob ein Wort übersetzt oder nicht, basierend auf der Aufmerksamkeit, die es erhält. Ein Wort wird übersetzt, wenn die Aufmerksamkeit nicht auf die letzten Lambda-Sprachrahmen konzentriert ist, d.h. wenn die Summe der Aufmerksamkeitsgewichte unter einem bestimmten Schwellenwert alpha liegt.

Beispiel: Wenn wir ein Sprachsegment mit "Ich werde über..." erhalten und das Modell die Übersetzung ins Deutsche vorhersagt, können wir die Kreuzaufmerksamkeitsgewichte überprüfen. Die ersten beiden Wörter zeigen auf die frühesten empfangenen Sprachrahmen, während das letzte Wort auf die letzten Rahmen zeigt. Daher werden die ersten beiden Wörter übersetzt, während das letzte Wort nicht übersetzt wird und wir auf ein weiteres Sprachsegment warten.

Die Hauptresultate von EDAtt zeigen, dass unsere Strategie sowohl die Übersetzungsqualität (gemessen durch BLEU) als auch die Latenz (durch durchschnittliche Verzögerung und rechenzeitbewusste durchschnittliche Verzögerung) verbessert. Im Vergleich zu anderen Strategien, die auf Offline-Modellen basieren, wie Wait-k und Local Agreement, und sogar im Vergleich zum State-of-the-Art-Architektur für gleichzeitige Vorübersetzung, übertrifft unsere Methode alle anderen.

Zusammenfassend lässt sich sagen, dass unsere Strategie sowohl in Bezug auf Qualität als auch Geschwindigkeit die beste Leistung erbringt. Weitere Details und Ergebnisse finden Sie in unserer Arbeit, und wir haben den Code, die Modelle und die gleichzeitige Ausgabe offen zugänglich veröffentlicht, um die Reproduzierbarkeit unserer Forschung zu gewährleisten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz (CoNLL++) ist öffentlich zugänglich, wie im Paper erwähnt.</sample>
    <sample id="150">**Zusammenfassung: "MEETINGQA: Extractive Question-Answering auf Meeting-Transkripten"**

Die Präsentation behandelt das ACL-Papier "MEETINGQA", das sich mit der Aufgabe des Frage-Antwortens (QA) in Meeting-Transkripten befasst. Die Autoren betonen die Potenzialität von Meeting-Transkripten als neue Domäne für NLP-Forschung, insbesondere aufgrund ihrer Länge, Domänenspezifität und Informationsdichte.

Bisherige Arbeiten konzentrierten sich hauptsächlich auf Zusammenfassung und Extraktion von Aktionspunkten, während die QA-Komponente vernachlässigt wurde. Um dies zu beheben, wurde das MeetingQA-Dataset eingeführt, das aus Fragen von Meeting-Teilnehmern und den entsprechenden Antworten besteht. Das Dataset umfasst 7.700 Fragen und zeigt vielfältige Antworten, einschließlich mehrsprachiger, mehrteiliger und rhetorischer Fragen.

Die Datenerfassung beinhaltete die Auswahl von Fragen basierend auf Satzzeichen und die Annotation von Antworten durch Annotatoren. Die Ergebnisse zeigen, dass Modelle, insbesondere kurze Kontextmodelle wie RoBERTa, in feinabgestimmten Szenarien überraschend gut abschneiden. Allerdings besteht immer noch ein erheblicher Leistungsunterschied zum menschlichen Niveau (25 F1-Punkte).

Zusätzlich präsentiert die Arbeit Zero-Shot-Ergebnisse, die die Herausforderungen bei der Verarbeitung ungesichteter Fragen und der Zuordnung von Antworten zu Sprechern aufzeigen. Die Studie schlägt vor, dass Data Augmentation und größere, auf Anweisungen trainierte Modelle wie FLAN-T5 dazu beitragen können, die Leistung in Zero-Shot-Szenarien zu verbessern.

Insgesamt unterstreicht die Arbeit die Komplexität der QA-Aufgabe in Meeting-Transkripten und präsentiert wertvolle Einblicke und ein nützliches Dataset für zukünftige Forschung in diesem Bereich.</sample>
    <sample id="151">## Präsentation: MultiInstruct – Verbesserung des mehrmodalen Zero-Shot Lernens durch Instruktions-Tuning

**Einführung:**

In unserer Arbeit untersuchen wir, ob das Instruktions-Tuning mehrmodaler, vorab trainierter Modelle die Generalisierung auf unbekannte mehrmodale Aufgaben verbessern kann. Bisher konzentrierten sich die meisten Arbeiten zum Instruktions-Tuning auf Sprachaufgaben, während Computer Vision und mehrmodale Aufgaben vernachlässigt wurden.

**Problemstellung:**

Während es für Sprachaufgaben über 1600 öffentlich verfügbare Instruktions-Datensätze gibt, fehlt es an einem großen, öffentlichen Datensatz für mehrmodale Instruktionsaufgaben.

**Lösung: MultiInstruct**

Daher haben wir **MultiInstruct** entwickelt, den ersten mehrmodalen Instruktions-Tuning-Datensatz. Er umfasst 62 vielfältige mehrmodale Aufgaben in 10 Kategorien, die aus 21 bestehenden Open-Source-Datensätzen stammen. Jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.

**Datensatzaufbau:**

* **Aufgabenformulierung:** Alle Aufgaben werden in einem einheitlichen Sequenz-zu-Sequenz-Format formuliert, in dem Text, Bilder, Anweisungen und Bounding Boxes in einem gemeinsamen Token-Raum dargestellt werden.
* **Datensatzgröße:** Für das Training verwenden wir 53 Aufgaben aus 9 Gruppen mit jeweils 10.000 Instanzen pro Aufgabe. Für die Tests reservieren wir die gesamte Gruppe "Allgemeines Verständnis" und wählen zusätzlich 5 Aufgaben aus den Gruppen "VQ" und "Diverse".

**Modell und Training:**

Als Basismodell verwenden wir den vorab trainierten OFA-Modell (ein mehrmodales, vereintes Modell). Während des Trainings mischen wir alle Instanzen aller Aufgaben und kombinieren jede Instanz zufällig mit einer der fünf Anweisungs-Vorlagen.

**Auswertung:**

* Für mehrmodale Klassifizierungsaufgaben berechnen wir die Genauigkeit.
* Für mehrmodale Generierungsaufgaben verwenden wir ROUGE-L.
* Für Sprachaufgaben (als Teil des NLP-Teils) ebenfalls ROUGE-L.
* Zusätzlich einführen wir die Metrik **Sensitivität**, um die Konsistenz der Modell-Ausgabe bei leicht variierenden Anweisungen zu messen.

**Ergebnisse:**

Unsere Ergebnisse zeigen, dass Instruktions-Tuning die Leistung von OFA auf gesehene mehrmodale Aufgaben deutlich verbessert. Transferlernen von natürlichen Instruktionsdatensätzen verbessert die Leistung und reduziert die Sensitivität.

**Zukünftige Arbeiten:**

* Wir erweitern **MultiInstruct** um weitere 150 mehrmodale Instruktionsaufgaben.
* Wir untersuchen verschiedene Fine-Tuning-Strategien und deren Einfluss auf die Sensitivität.

**Nächste Schritte:**

* **QR-Code:** Der QR-Code führt zu unserem Datensatz und Modell.

Vielen Dank!</sample>
    <sample id="152"># **Zusammenfassung: Sprachmodelle für die klassische Philologie**

In seiner Präsentation stellt Frederick Riemenschneider innovative Ansätze für die Anwendung von Sprachmodellen in der klassischen Philologie vor, insbesondere für Altgriechisch und Latein. Er betont die Notwendigkeit fortgeschrittener Tools, da die aktuelle Landschaft der Sprachmodelle noch Verbesserungspotenzial aufweist, insbesondere im Hinblick auf die Multilingualität.

Riemenschneider und sein Team haben zwei monolinguale Modelle entwickelt: GreBERTa (basierend auf RoBERTa) und GreTa (ein Encoder-Decoder-Modell nach T5-Architektur), die beide Altgriechisch verarbeiten. Darüber hinaus wurden PhilBERTa und PhilTa als multilinguales Äquivalent für Altgriechisch, Latein und Englisch erstellt.

Der Prozess begann mit der Datensammlung, wobei das Team Open Greek &amp; Latin und neue Ressourcen aus dem Internet Archive nutzte, das OCR-Transkripte griechischer Texte enthielt. Sie entwickelten einen neuen Datensatz durch Identifizierung griechischer Stoppwörter. Für die mehrsprachigen Modelle wurden zusätzliche Datenquellen wie das Corpus Corporum (Latein) und englische Texte antiker Bezüge herangezogen.

Die Modelle wurden auf verschiedenen Aufgaben getestet, darunter Teil-des-Spruchs-Tagging, Abhängigkeitsanalyse und Lemmatisierung. Die Ergebnisse zeigten, dass die entwickelten Modelle die aktuelle Bestleistung für Altgriechisch und Latein übertreffen. Besonders hervorzuheben ist die verbesserte Lemmatisierung um 5 Prozentpunkte im Vergleich zum vorherigen Staat der Kunst.

Die Untersuchung der Encoder- und Decoder-Komponenten von T5-Modellen ergab interessante Erkenntnisse. Während der Encoder anfänglich schlecht abschneidet, nähert er sich nach mehr Training der Leistung eines reinen Encoder-Modells an. Die Studie kommt zu dem Schluss, dass die mehrsprachigen Modelle nicht zwangsläufig bessere Leistungen erbringen, aber die Möglichkeit bieten, Text in mehreren Sprachen zu verarbeiten.</sample>
    <sample id="153">In ihrer Präsentation behandelt Ninareh Mehrabi, Postdoktorandin am Amazon Alexa AI's Responsible AI Team, die Herausforderungen durch Ambiguitäten in Text-zu-Bild-Generativmodellen. Ihr Forschungsprojekt konzentriert sich auf die Untersuchung und Lösung von Unklarheiten in Benutzeranweisungen, die zu ungenauen Ergebnissen führen können.

Mehrabi und ihr Team haben ein Benchmark-Dataset erstellt, das verschiedene Arten von Ambiguitäten abdeckt, basierend auf dem bestehenden LAVA-Corpus. Sie entwickelten zwei Methoden zur Klärung dieser Ambiguitäten: eine, bei der ein Sprachmodell Fragen generiert und der Benutzer die Absicht klarstellt, und eine andere, bei der das Modell alternative visuelle Szenarien vorschlägt. Diese Schritte führen zu präziseren Eingaben für die Text-zu-Bild-Modelle.

Nach der Disambiguierung der Eingaben wird ein automatisches Bewertungssystem eingesetzt, um zu überprüfen, ob die generierten Bilder der Benutzerabsicht entsprechen. Dies geschieht durch die Kombination der Bilder und der Benutzerabsicht als Eingabe für ein VQA-Modell (Visual Question Answering). Das VQA-Modell gibt an, ob die Absicht im Bild erfüllt ist oder nicht.

Die Ergebnisse zeigen, dass verschiedene Ambiguitätsarten unterschiedlich gut aufgelöst werden können und dass die vorgeschlagenen Methoden die Bildgenauigkeit verbessern. Die automatische Bewertung korreliert gut mit menschlichen Bewertungen, was die Zuverlässigkeit des Systems unterstreicht. Die Arbeit zielt darauf ab, die Leistung und Genauigkeit von Text-zu-Bild-Modellen zu verbessern, indem sie eine klare Kommunikation zwischen Benutzern und Modellen fördert.</sample>
    <sample id="154">Die Autoren gehören der University of Trento und Fondazione Bruno Kessler an.</sample>
    <sample id="155">Der/die Referent*in in dem beschriebenen Kontext ist **Javad Hosseini**. Er ist einer der Autoren der Arbeit und erwähnt explizit seinen Namen sowie die Mitautoren: Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="157"># **Dialogue Summarization mit statisch-dynamischer Strukturfusion**

Shen Gao von der Shandong University präsentiert eine neue Methode für die Dialogzusammenfassung, die als "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS) bezeichnet wird. Das Ziel ist es, die wichtigsten Informationen aus Dialogen zu extrahieren und prägnante Zusammenfassungen zu erstellen.

Bisherige Ansätze konzentrierten sich auf die Modellierung von Dialogen mit statischen Graphen, die mit externen Sprachwerkzeugen wie Diskursanalyse und Dialogzustandsverfolgung erstellt wurden. Diese Methoden haben jedoch zwei Herausforderungen: Abhängigkeit von potenziell fehlerhaften externen Tools und die Trennung zwischen Graphenkonstruktion und Graphenrepräsentation, was eine Anpassung an spezifische Aufgaben erschwert.

Das SDDS-Modell überwindet diese Probleme mit vier Hauptkomponenten:

1. **Utterance Encoder**: Wandelt Dialogbeiträge in Vektoren um.
2. **Statisch-Dynamisches Graphenmodul**: Kombiniert statische Graphen und erfasst semantische Beziehungen zwischen Sätzen mithilfe einer dynamischen Graphenkomponente.
3. **Pre-trainiertes Sprachmodell als Summary Generator**: Fügt statische und dynamische Strukturen für die endgültige Zusammenfassung zusammen.

Das Modell verwendet heuristische Methoden, um statische Dialogstrukturen zu modellieren, wie z. B. Diskursanalyse und die Erfassung von Schlüsselwörtern in Sätzen. Es berücksichtigt auch Sprecherbeziehungen und Positionsinformationen. Die dynamische Graphenkomponente nutzt ein Multi-Head-Aufmerksamkeitsmodell, um semantische Beziehungen basierend auf Vektoren zu erfassen.

Die Integration statischer und dynamischer Informationen führt zu einer effektiven Dialogzusammenfassung, wie die Präsentation demonstriert. Das Team hat den Code und die Daten auf GitHub veröffentlicht.</sample>
    <sample id="158"># **Dual Cache für die verbesserte Kernreferenzauflösung in langen Dokumenten**

Qipeng Guo von AWS präsentierte eine innovative Methode namens "Dual Cache für Long Document Neural Coreference Resolution", die die Herausforderungen bei der Kernreferenzauflösung in langen Texten angeht.

Die Kernreferenzauflösung zielt darauf ab, Mentions in einem Dokument zu identifizieren und zu klustern, die auf dieselbe Entität verweisen. Traditionelle Ansätze haben eine quadratische Komplexität in Bezug auf Rechenleistung und Speicherverbrauch, während neuere cache-basierte Methoden die Effizienz auf eine lineare Ebene bringen.

Das Problem bei diesen Methoden ist, dass in langen Dokumenten mit wechselnden Themen Mentions von Entitäten weit verstreut sein können, was zu einem hohen Cache-Miss führt, insbesondere bei der LRU (Least Recently Used)-Eviktion. Die Studie von Guo und seinem Team konzentrierte sich auf die Optimierung dieser Cache-Mechanismen.

Als Lösung schlagen sie einen Dual-Cache vor, der aus einem lokalen und einem globalen Cache besteht. Der lokale Cache verwendet LRU für kurzlebige Entitäten, während der globale Cache Entitäten mit LFU (Least Frequently Used) eviziert, wenn er voll ist. Dieser Ansatz ermöglicht es dem Modell, Entitäten effizienter zu verwalten, insbesondere bei häufig erwähnten Entitäten.

Die Evaluierung auf vier öffentlichen Datensätzen zeigte, dass der Dual-Cache signifikant bessere Ergebnisse liefert als Single-Cache-Methoden, insbesondere bei längeren Dokumenten. Er reduziert Cache-Misses effektiv und bietet ein optimales Verhältnis zwischen Leistung und Kosten. Die Präsentation betont die Effektivität des Dual-Cache-Ansatzes bei der Verbesserung der Kernreferenzauflösung in langen Dokumenten.</sample>
    <sample id="159">##  Sprachmodelle und ihre Akzeptanz: Eine tiefere Analyse mit längeren Sequenzen

Hallo zusammen, ich bin Koustav Sinha und ich freue mich, euch heute unsere Arbeit zur ACL 2023 vorzustellen. Sprachmodelle basieren oft auf Akzeptanzurteilen, die grammatikalische Korrektheit oder Stereotypen einschließen können. Bisherige Methoden zur Bewertung dieser Modelle (Minimal Pair Paradigmen) konzentrieren sich auf kurze, einzelne Sätze. Da moderne Sprachmodelle jedoch immer größere Kontextfenster verarbeiten, ist es essentiell, ihre Akzeptanzurteile über längere Sequenzen hinweg zu untersuchen.

In unserer Arbeit revisieren wir diese Standard-Pipeline und entwickeln einen Ansatz, der Sprachmodelle dazu bringt, Akzeptanzurteile für längere Sätze zu treffen. Um dies zu erreichen, erstellen wir simulierte lange Sequenzen, indem wir akzeptable und inakzeptable Sätze aus bestehenden Datensätzen auswählen und kombinieren.

**Unser Vorgehen:**

1. **Relevante Datensätze:** Wir beginnen mit Datensätzen wie BLiMP oder SyntaxGym, die Akzeptanz- und Inakzeptanzpaare enthalten.
2. **Längere Sequenzen erstellen:** Aus akzeptablen und inakzeptablen Sätzen aus diesen Datensätzen bilden wir Paare, indem wir sie als Präfixe zu ein und demselben Satz hinzufügen.
3. **Kontextvariationen:** Wir testen die Modelle mit verschiedenen Kontextlängen:
    * **Relevanter Kontext:** Sätze aus dem gleichen Datensatz wie das Testpaar.
    * **Unrelevanter Kontext:** Sätze aus Wikipedia, völlig unabhängig vom Testpaar.
4. **Ergebnisanalyse:** Wir stellen fest, dass:
    * **Stabile Urteile:**  Urteile aus Minimal Pair Paradigmen (MPP) sind gegenüber längeren Kontextlängen robust, auch bei 1024 Token.
    * **Einfluss relevanter Präfixe:**  Wenn wir Sätze aus dem gleichen Datensatz als Präfixe verwenden, ändern sich die MPP-Urteile stark, je nachdem, ob das Präfix akzeptabel oder inakzeptabel ist. Dieser Effekt wird mit steigender Kontextlänge stärker.

**Warum sind Präfixe so einflussreich?**

Unsere Analyse zeigt, dass Sprachmodelle auf gemeinsame syntaktische und semantische Merkmale in den Sätzen reagieren, unabhängig von der spezifischen Wortfolge.

**Zusammenfassende Erkenntnisse:**

Unsere Arbeit zeigt, dass Sprachmodelle subtile, latente syntaktische und semantische Merkmale in Sätzen erkennen und darauf reagieren. Die gängigen Methoden zur Bewertung von Akzeptanzurteilen mit kurzen Sätzen erfassen möglicherweise nicht das vollständige Ausmaß des Wissens, das Sprachmodelle in ihren großen Kontextfenstern besitzen.

Wir hoffen, dass unsere Forschung einen Beitrag zur Entwicklung robusterer und präziserer Sprachmodelle leistet. 
Bitte lest unseren Artikel für detailliertere Informationen zu unseren Experimenten. Vielen Dank für eure Aufmerksamkeit!</sample>
    <sample id="160">Im ersten Schritt der Methode werden den Input-Token **ungeordnete Multisets** von Tokens zugeordnet, die im Output vorkommen werden.</sample>
    <sample id="161">In CoScript sind insgesamt 55.000 spezifische Skripte vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN, basierend auf den Ergebnissen der Studie, ist **MASSalign**.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen (WSL) besteht darin, dass es kostengünstiger ist als traditionelles, manuell annotiertes Lernen, da es schwache Label-Quellen wie einfache Regeln oder niedrigqualitative Crowdsourcing nutzt. Trotz der geringeren Genauigkeit dieser schwachen Labels können WSL-Methoden durch spezielle Trainingsalgorithmen robuste neuronale Netze erzeugen, die besser generalisieren und nicht an die Label-Rauschen scheitern.</sample>
    <sample id="165">In ihrer Präsentation stellt Wenting Zhao, PhD-Studentin an der Cornell University, ihre Forschung zum Thema abduktives Denken vor, insbesondere ihren Ansatz "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". 

Abduktives Denken sucht nach plausiblen Erklärungen für ein beobachtetes Phänomen (Ausgang Y) basierend auf einem gegebenen Kontext (X). In einem Beispiel wird gezeigt, wie die Erklärung "Ihr Flug wurde verzögert" die Lücke zwischen Kontext und Ausgang schließt.

Aktuelle Methoden verlassen sich auf überwachte Lernansätze, die explizit plausible Erklärungen erfordern, was subjektiv und fehleranfällig sein kann. Zhao et al. stellen die Frage, ob abduktives Denken ohne Überwachung erlernt werden kann.

Ihr Vorschlag ist LiPoR (Likelihood Learning with Posterior Regularization), ein unsupervises Modell. LiPoR behandelt Erklärungen als latente Variablen und maximiert die Wahrscheinlichkeit des Ausgangs unter Berücksichtigung aller möglichen Erklärungen. Um plausible Erklärungen zu bevorzugen, integriert LiPoR einen Regularisierer, der auf der gegenseitigen Ausschlussfähigkeit der Erklärungen basiert.

Die Forschung zeigt, dass LiPoR im Vergleich zu anderen Methoden, einschließlich einem GPT-3-Zero-Shot-Baseline, auf dem AlphaNLI-Datensatz überlegen ist, mit einer Verbesserung der Genauigkeit um über 4 Punkte.

Zhaos Arbeit demonstriert einen vielversprechenden Weg, abduktives Denken ohne manuelle Beschriftung von Erklärungen zu lernen.</sample>
    <sample id="166"># **A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Complex Text**

Diese Arbeit präsentiert einen innovativen Ansatz für die Herausforderung der Bildrückgewinnung aus komplexen Textbeschreibungen, bei der Bilder hochgradig ähnlich und die Texte lang sind. Der vorgeschlagene Rahmen, NDCR (Neural Divide-and-Conquer Reasoner), kombiniert Divide-and-Conquer-Strategien mit Dual-Process-Theorie, um komplexe Bild-Text-Abgleichaufgaben zu bewältigen.

Der Ansatz besteht aus drei Hauptkomponenten:

1. **Proposition Generator**: Zerlegt komplexe Textpropositionen in einfache Darstellungen.
2. **Visual-Linguistic Interactor (System 1)**: Führt eine anfängliche Interaktion zwischen visuellen Inhalten und einfachen Propositionen durch, generiert Matching-Scores und Reasoning-States.
3. **Neural-Symbolic Reasoner (System 2)**: Integriert die Reasoning-States und einfachen Propositionen, um die endgültige Lösung zu bestimmen, mithilfe von Negationsausführung und Konjunktionsoperation.

Experimente zeigen, dass NDCR im Vergleich zu Basismethoden überlegen ist. Die Methodik ermöglicht die Darstellung von Inferenzschritten und verbessert die Komplexitätsbewältigung in Bildrückgewinnungstasks. Die Studie schlägt vor, dass neuronale symbolische Berechnungen und Divide-and-Conquer-Strategien zusammenwirken können, um die Leistungsfähigkeit großer Sprachmodelle in komplexen Aufgaben zu steigern.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden zu gleichen Teilen sowohl manuell als auch automatisch ausgerichtet. Genauer gesagt:

- **Manuell**: 750 Dokumente wurden vollständig manuell ausgerichtet.
- **Automatisch**: 750 Dokumente wurden zusätzlich mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem Nachrichtenartikel aus Reuters News im Jahr 2020 gesammelt und anschließend mit den gleichen Annotation-Richtlinien wie beim CoNLL-2003-Datensatz annotiert wurden.</sample>
    <sample id="169"># **Prompting PaLM für die Maschinelle Übersetzung: Strategien und Leistung**

Diese Studie untersucht die Effektivität von Prompting-Techniken bei der Verwendung großer Sprachmodelle (LLMs) für maschinelle Übersetzung (MT). Die Forscher, einschließlich David Vilar und sein Team von Google Translate, konzentrierten sich auf PaLM, ein beeindruckendes 540-Milliarden-Parameter-Modell, das bei seiner Einführung im Jahr 2022 fortschrittliche Leistungen in verschiedenen NLP-Aufgaben erbrachte.

Die Studie führt eine umfassende Bewertung der Prompting-Strategien für PaLM durch, indem sie die neuesten Testdatensätze der MT-Community verwendet, um eine faire Vergleichbarkeit zu gewährleisten. Die Ergebnisse zeigen, dass das Prompting einen erheblichen Einfluss auf die Leistung von LLMs hat. Einfache Experimente mit einem- und mehrfachen Prompting-Schritten ergaben Unterschiede von bis zu 40 BLEURT-Punkten.

Die Autoren empfehlen eine 5-Shot-Prompting-Strategie, bei der Beispiele mit klaren Sprachmarkierungen bereitgestellt werden. Ihre Experimente ergaben, dass die Qualität der Beispiele entscheidender ist als die Ähnlichkeit mit der Quellensprache. Hochwertige, kuratierte Daten aus dem WMT-Datensatz (Dev-Daten) verbesserten die Leistung im Vergleich zu Trainingsdaten.

Trotzdem zeigte PaLM bei der menschlichen Bewertung, die die Fluenz und Genauigkeit verglich, eine Leistung, die kommerziellen Systemen wie Google Translate nahekommt. Die häufigsten Fehler bei PaLM waren Omissionen, was darauf hindeutet, dass das Modell manchmal Teile der Quelltexte weglässt, um eine bessere Klangqualität zu erreichen. Insgesamt bietet PaLM flüssige Übersetzungen, weist aber noch immer Genauigkeitsprobleme auf.</sample>
    <sample id="170">## XSemPLR: Cross-Lingual Semantic Parsing in Mehreren natürlichen Sprachen und Bedeutungsrepräsentationen

**Einführung**

Hallo zusammen, ich bin Yusen Zhang von der Penn State University. Heute möchte ich unsere Arbeit mit dem Titel "XSemPLR: Cross-Lingual Semantic Parsing in Mehreren natürlichen Sprachen und Bedeutungsrepräsentationen" vorstellen.

Semantic Parsing ist die Aufgabe, semantische Darstellungen von Benutzerabfragen wie SQL und Lambda-Kalkül zu erstellen. Cross-Lingual Semantic Parsing (CLSP) geht einen Schritt weiter und übersetzt Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen.

Wie in der Abbildung zu sehen ist, müssen wir mithilfe neuronaler Modelle Abfragen in verschiedenen Sprachen in SQL, Lambda oder FunQL (und andere) übersetzen. Bisherige CLSP-Modelle wurden separat entwickelt und auf Datensätzen mit begrenzten Aufgaben und Anwendungen getestet. Während viele Modelle auf bestimmte natürliche Sprachen abzielen, fehlt es an Abdeckung für Chinesisch und bestimmte Bedeutungsrepräsentationen wie Lambda-Kalkül.

Um diese Lücke zu schließen, stellen wir **XSemPLR** vor: Ein einheitlicher Datensatz für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. XSemPLR umfasst:

* 9 Datensätze aus verschiedenen Domänen
* 5 Semantic-Parsing-Aufgaben
* 8 Bedeutungsrepräsentationen
* 22 natürliche Sprachen in 15 Sprachfamilien

Um unseren Benchmark effektiv zu testen, definieren wir sechs Szenarien:

1. **Translate-Test:** Verwendung des Google Translate API zur Übersetzung der Quellsprache in die Zielsprache, gefolgt von der Verwendung eines monolingualen Modells für Training und Evaluation.

2. **Monolingual (selbe Sprache):** Die Quell- und Zielsprache sind identisch, z.B. Deutsch zu Deutsch oder Englisch zu Englisch.

3. **Monolingual Few-Shot:** Training monolingualer Modelle mit nur 10% des Trainingsdatensatzes.

4. **Multilingual:** Training eines multilingualen Modells für alle Sprachen, z.B. eine Kombination aus Deutsch, Englisch und Chinesisch.

5. **Cross-lingual Zero-Shot und Few-Shot Transfer:** Training auf einer Quellsprache und Transfer zu einer anderen Sprache. Während des Trainings können wir entweder Englisch oder eine Kombination aus Englisch und Deutsch (Few-Shot) verwenden, um ein multilinguales Modell zu erstellen.

**Ergebnisse und Analyse:**

* **Monolinguale Modelle:** Wir untersuchten zwei Modellgruppen: Encoder-PTR (Multilingual vorab trainierte Encoder mit Pointer-basierten Decodern, z.B. XLM-R + PTR und mBERT + PTR) und Encoder-Decoder (Multilingual vorab trainierte Encoder-Decoder-Modelle, z.B. mBART und mT5). Encoder-Decoder-Modelle erzielten in allen neun Datensätzen die besten Ergebnisse.

* **Multilinguales Training:** Wir verglichen mT5 und XLM-R + PTR in einem multilingualen Szenario. Encoder-Decoder oder Encoder-PTR konnten durch Training mit einer Mischung verschiedener Sprachen verbessert werden. Dies liegt daran, dass die meisten wichtigen natürlichen Sprachen von diesem Training profitieren, mit Ausnahme eines Leistungsrückgangs für Englisch in sieben Datensätzen und Verbesserungen in drei.

* **Cross-Language-Performance-Gap:** Wir beobachteten einen signifikanten Unterschied zwischen Zero-Shot und monolingualen Szenarien. Der Few-Shot-Transfer reduzierte den Unterschied deutlich.

* **Weitere Erkenntnisse:**

    * Encoder-Decoder-Modelle übertreffen frühere Arbeiten oder erreichen vergleichbare Ergebnisse.
    * Vorabtraining auf Englisch kann die Leistung im Few-Shot-Modus auf Zielsprachen erheblich steigern.
    * Aktuelle multilingualen Sprachmodelle wie Codex und BLOOM sind für CLSP-Aufgaben immer noch nicht ausreichend.

**Fazit:**

Wir haben XSemPLR als einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen entwickelt. Durch eine umfassende Evaluierung auf drei repräsentativen Typen multilingualer Sprachmodelle konnten wir interessante Erkenntnisse gewinnen.</sample>
    <sample id="171">Basierend auf der Präsentation wurden bereits mehrere Arbeiten im Bereich der Schutzmechanismen für Embedding-as-a-Service-Dienste durchgeführt:

- **Bestehende Methoden:** Es wurden vier Kategorien bestehender Methoden identifiziert, die jedoch entweder nicht für Embedding-as-a-Service geeignet sind oder an Transferabilität mangeln.
- **Backdoor-basierte Watermarking-Methode:** Die vorgestellte Arbeit, "Embedding Marker", schlägt eine Backdoor-basierte Watermarking-Methode vor, die speziell für Embedding-as-a-Service-Dienste entwickelt wurde.
- **Watermark-Injektion und Copyright-Verifizierung:** Die Methode umfasst zwei Hauptkomponenten: Watermark-Injektion, bei der ein Ziel-Embedding in die bereitgestellten Embeddings gemischt wird, basierend auf der Anzahl von Triggern in einem Satz, und Copyright-Verifizierung, um zu überprüfen, ob ein stehlendes Service das eingebettete Watermark enthält.
- **Experimentelle Validierung:** Die Effektivität der Methode wurde auf vier Datensätzen (AG News, MIND, SST2, Enron Spam) getestet und durch Visualisierung der Embeddings mit PCA hinsichtlich ihrer Covertness überprüft.</sample>
    <sample id="172">Nein, laut der Präsentation sind mehrsprachige Sprachmodelle wie Codex und BLOOM **nicht ausreichend** für Cross-Lingual Semantic Parsing (CLSP) Aufgaben. Die Studie zeigt, dass diese Modelle trotz ihrer Multilingualität in CLSP-Aufgaben, insbesondere im Zero-Shot und Few-Shot Transfer, noch Verbesserungspotenzial haben.</sample>
    <sample id="174">In ihrem Video erläutert Thea, Co-Autorin der Studie "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis", die Einzigartigkeit ihres Datensatzes im Vergleich zu anderen Argumentationsanalysedatenbanken.

Das Problem mit vielen bestehenden Datensätzen ist ihre geringe Qualität, begrenzte Vielfalt und mangelnde Tiefe bei der Argumentationsbewertung. ArgAnalysis35K adressiert diese Probleme auf mehrere Weisen:

* **Größe und Qualität:** Mit 35.000 Argument-Analyse-Paaren ist es der größte Datensatz in diesem Bereich. Die Argumente stammen zu 85% aus hochwertigen Quellen wie Debattierwettbewerben, Experten und fortgeschrittenen Debattierern.

* **Vielfalt:** Anstatt vorgegebene Argumente aus einer begrenzten Anzahl von Motiven auszuwählen, deckt der Datensatz 24 Themen ab, die in Debatten relevant sind.

* **Analyse als neues Element:**  ArgAnalysis35K geht über einfache Argumente hinaus und integriert Analysen, die aus Kombinationen von Behauptungen, Prämissen und Statistiken bestehen können.

* **Instance-basierte Annotator-Zuverlässigkeit:**  Der Datensatz berücksichtigt individuelle Vorurteile von Annotatoren und eliminiert nur deren Urteile, die stark voreingenommen sind, anstatt ganze Annotatoren auszuschließen.

* **Relevanzmodell:** Jedes Argument wird nach seiner Relevanz für ein bestimmtes Thema bewertet, wodurch die Bandbreite der Anwendbarkeit von Argumenten verdeutlicht wird.

Zusammengefasst bietet ArgAnalysis35K einen umfassenden, qualitativ hochwertigen und vielfältigen Datensatz für die Argumentationsanalyse, der durch innovative Elemente wie Analysen und ein Relevanzmodell heraussticht.</sample>
    <sample id="175">Die Methode geht mit der Mehrdeutigkeit der Permutationen um, indem sie eine **kontinuierliche Entspannung** (continuous relaxation) verwendet. Diese Entspannung ist GPU-freundlich und ermöglicht es, die wahrscheinlichsten linguistisch sinnvollen Permutationen zu approximieren. Anstatt nach der optimalen Permutation zu suchen, die NP-hard ist, findet das Modell eine Annäherung, die durch Backpropagation optimiert werden kann.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells (Natural Language Processing) wird definiert durch die Fähigkeit, ohne Voreingenommenheit oder Diskriminierung gegenüber verschiedenen sozialen Gruppen, politischen Meinungen oder Demografien zu funktionieren. Ein faires Modell sollte gleichermaßen effektiv sein, unabhängig von der Zielgruppe oder dem Kontext, und keine ungerechten oder ungleichen Ergebnisse liefern.

In der Präsentation wird dies durch die Untersuchung der Leistung von Sprachmodellen mit unterschiedlichen politischen Neigungen bei Aufgaben wie der Hassrede- und Fake-News-Erkennung veranschaulicht. Die Ergebnisse zeigen, dass Modelle mit verschiedenen politischen Standpunkten unterschiedliche Vorhersagen für Inhalte aus verschiedenen sozialen Kategorien treffen, was zu potenziellen Fairnessproblemen führt.</sample>
    <sample id="177">Der/die Referent*in heißt Yanis Labrak.</sample>
    <sample id="178">Der/die Referent*in heißt Koustav Sinha.</sample>
    <sample id="179">**Abstract:**

Melanie Sclar präsentiert "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker". Theorie des Geistes (Theory of Mind, ToM) bezieht sich auf die Fähigkeit, die mentalen Zustände anderer zu verstehen. Traditionell wird sie in Menschen und Sprachmodellen durch Lesekomprehensionsaufgaben mit mehreren Charakteren gemessen, insbesondere durch False-Belief-Fragen. Diese testen das Verständnis, wenn die Realität nicht mit den Überzeugungen der Charaktere übereinstimmt.

Die Studie konzentriert sich auf die Verbesserung der ToM-Fähigkeiten bei Large Language Models (LLMs), insbesondere bei der Beantwortung von False-Belief-Fragen. Das vorgeschlagene System, SymbolicToM, nutzt explizite grafische Darstellungen, um mentale Zustände zu modellieren. Diese Graphen repräsentieren die Überzeugungen der Charaktere und ihre gegenseitigen Annahmen.

Experimente zeigen signifikante Leistungssteigerungen für verschiedene LLMs, einschließlich GPT-3 und Macaw, bei Second-Order-False-Belief-Fragen. SymbolicToM übertrifft Supervised-Baselines und ermöglicht es fortgeschrittenen Modellen wie GPT-4, komplexe Story-Strukturen zu meistern, selbst bei linguistischer Diversität (ParaphrasedToMi).

Zusammenfassend bietet SymbolicToM eine effiziente und interpretierbare Methode zur Verbesserung der ToM-Fähigkeiten von LLMs, indem es grafische Darstellungen nutzt und Overfitting vermeidet.</sample>
    <sample id="180">Der/die Referent*in heißt Myra.</sample>
    <sample id="181">**Abstract**

In unserem Papier "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" untersuchen wir, wie große Sprachmodelle dabei unterstützt werden können, Aufgaben mit spezifischen Einschränkungen zu planen. Während vorherige Arbeiten die Verwendung von Sprachmodellen für die Planung abstrakter Ziele, wie "einen Kuchen backen", gezeigt haben, bleibt die Planung für Ziele mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", unterforscht.

Wir definieren das Problem des *beschränkten Sprachplanens* und zeigen, dass bestehende Modelle bei der Erstellung schrittweiser Anleitungen, die spezifischen Anforderungen entsprechen, oft scheitern. Durch eine detaillierte Analyse erkennen wir, dass die erzeugten Skripte zwar semantisch vollständig, aber nicht immer den Einschränkungen treu sind.

Um dies zu verbessern, entwickeln wir eine Methode namens *über-generieren-dann-filtern*. Diese Technik beinhaltet die Übergenerierung mehrerer Skripte und die anschließende Auswahl der treuesten Skripte mithilfe eines Filtermodells. Darüber hinaus erstellen wir *CoScript*, ein Dataset mit 55.000 spezifischen Zielen und Skripten, das durch Übermodelle generiert und von Crowdsourcing-Arbeitern validiert wurde.

Unsere Ergebnisse zeigen, dass diese Ansätze die Qualität der Skripte verbessern und kleinere, spezialisierte Modelle in der Lage machen, effektivere Sprachplanung für Ziele mit spezifischen Einschränkungen durchzuführen. *CoScript* stellt ein wertvolles Ressource für zukünftige Forschung auf diesem Gebiet dar.</sample>
    <sample id="182">Im Zusammenhang mit dieser Arbeit bezieht sich Tropikalismus auf die Stereotypisierung und Darstellung von Frauen aus Lateinamerika mit Begriffen wie "vibrant" und "curvaceous", die an eine exotische und tropische Assoziation erinnern. Dies ist ein wiederkehrendes Motiv, das oft mit dieser ethnischen Gruppe verbunden wird und zu einer Essenzialisierung und "Othering" dieser Frauen beiträgt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen (z.B. "Asiatische Frau", "Mittelöstliche Frau" usw.) durch die Generierung von Personas mit natürlichen Sprachbefehlen (Prompts) erstellt. Die Prompts waren inspiriert von einem früheren Studienansatz, bei dem Menschen solche Beschreibungen verfassten. Durch diesen Ansatz konnten die Autoren die generierten Personas direkt mit den menschlichen Antworten vergleichen.</sample>
    <sample id="184">In dieser Arbeit wurde **CXMI (Contextual Word Usage Measure for Machine Translation)** verwendet, um die Kontextnutzung durch maschinelle Übersetzungsmodelle zu messen. CXMI misst, wie viel Information das Kontext **C** über das Ziel **Y** liefert, gegeben die Quelle **X**.

Die Arbeit erweitert CXMI zu **Pointwise CXMI**, das die Kontextnutzung auf Satz- oder Wortniveau messen kann.</sample>
    <sample id="185">Der Hauptunterschied zwischen DrBERT und ChuBERT liegt in ihren Trainingsdaten und ihrem Anwendungsbereich:

- **DrBERT**: Basiert auf dem RoBERTa-Modell und wurde mit einem großen Datensatz namens NACHOS (einer Sammlung medizinischer Web-Crawl-Daten) trainiert. Es ist für eine breite Palette von biomedicalen und klinischen Aufgaben in französischer Sprache konzipiert.

- **ChuBERT**: Basiert ebenfalls auf RoBERTa, aber trainiert mit anonymisierten Daten aus dem Datenwarehouse des Nantes University Hospital (einem klinischen Datensatz). Es ist spezifisch für klinische Anwendungen und in französischer Sprache.

Zusammenfassend lässt sich sagen, dass DrBERT aufgrund seiner Trainingsdaten und seines breiteren Anwendungsbereichs (biomedizinische und klinische Domänen) im Vergleich zu ChuBERT, das sich auf klinische Aufgaben konzentriert, einen anderen Fokus hat.</sample>
    <sample id="187">Zwei Autoren: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen ist eine Methode, bei der ein Modell zunächst auf einer verwandten Aufgabe (z.B. *Debate* oder *CE*-Klassifikation) **vorab** trainiert wird, um Kenntnisse zu erwerben, die dann auf die Hauptaufgabe der **Dissonanzdetektion** übertragen werden.

Im Kontext deiner Arbeit bedeutet dies:

1. **Vorabtraining:** Ein Modell wird auf einer Aufgabe trainiert, die thematisch oder konzeptionell mit Dissonanz zu tun hat (z.B. *Debate* für die Bestimmung von Einvernehmung oder *CE* für Vergleich und Expansion).
2. **Feinabstimmung:** Das vorab trainierte Modell wird dann iterativ auf der Hauptaufgabe (Dissonanzdetektion) feinabgestimmt, wobei es von den Erkenntnissen aus den vorherigen Iterationen profitiert.

Diese Vorgehensweise hilft dabei, ein Modell zu entwickeln, das besser in der Lage ist, seltene Dissonanzbeispiele zu erkennen, da es auf bereits erworbenen Kenntnissen aufbaut.</sample>
    <sample id="189">Das Ziel des AltEntities Corpus (AltEntities Datensatz) ist es, ein umfassendes und öffentlich zugängliches Datenset für die Aufgabe der Resolving Indirect Referring Expressions for Entity Selection (Auflösung indirekter Bezugsausdrücke zur Entitätsauswahl) zu schaffen. Der Datensatz soll Sprachmodellen helfen, natürliche und indirekte Referenzen von Benutzern zu verstehen, wenn diese zwischen verschiedenen Entitäten (z.B. Songs, Bücher, Rezepte) wählen möchten, insbesondere in Fällen, in denen direkte Referenzen unangemessen oder schwierig sind.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen Embedding-as-a-Service (EaaS) extrahieren, indem er die folgenden Schritte ausführt:

1. **Lernen des Embeddings:** Der Angreifer nutzt den EaaS-Dienst, um viele verschiedene Eingaben zu senden und die entsprechenden Embeddings zu sammeln. Durch maschinelles Lernen kann er dann ein Modell erstellen, das in der Lage ist, die Embeddings zu reproduzieren.

2. **Modell-Extrahierung:** Mit dem trainierten Modell kann der Angreifer die internen Modellparameter extrahieren, die für die Erzeugung der Embeddings verantwortlich sind. Dies kann durch verschiedene Methoden erfolgen, einschließlich der Analyse der Gewichte und Aktivierungen des Modells.

3. **Erstellung eines ähnlichen Dienstes:** Nach dem Besitz der Modellparameter kann der Angreifer einen eigenen Dienst erstellen, der ähnliche Embeddings erzeugt. Dies ermöglicht es ihm, einen konkurrierenden oder identischen Dienst anzubieten, der auf den gestohlenen Modellparametern basiert.

Um dies zu verhindern, wie in Ihrem Papier erwähnt, sind Methoden wie Watermarking (z.B. Embedding Marker) notwendig, um die Urheberschaft und Integrität von EaaS-Diensten zu schützen.</sample>
    <sample id="191">An der Arbeit sind drei Autoren beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192"># **CAME: Ein effizienter, memory-effizienter Optimierer für große Sprachmodelle**

Die Präsentation von Yang Luo konzentriert sich auf die Herausforderung der Optimierung großer Sprachmodelle mit begrenzter Speicherkapazität. Traditionelle adaptive Optimiermethoden wie Adam sind zwar schnell, erfordern aber erheblichen Speicher für Momentangaben. Memory-effiziente Alternativen wie Adafactor reduzieren den Speicherbedarf, leiden aber an Leistungseinbußen.

Die Lösung, CAME (Confidence-guided Adaptive Memory Efficient Optimization), zielt darauf ab, sowohl schnelle Konvergenz als auch niedrigen Speicherverbrauch zu erreichen. Es baut auf der Technik der nichtnegativen Matrixfaktorisierung (NMF) auf, die Speicheranforderungen reduziert. CAME verbessert Adafactor, indem es fehlerhafte Updates minimiert, die zu langsamer Konvergenz führen.

Die Schlüsselidee besteht darin, die Unsicherheit bei der Aktualisierung zu berücksichtigen. CAME berechnet die Instabilität zwischen dem Momentan- und dem vorherigen Update und verwendet dies, um den Aktualisierungsschritt anzupassen. Diese adaptive, vertrauensbasierte Aktualisierung verbessert die Stabilität des Trainings.

Experimente auf Datensätzen wie BookCorpus und Wikipedia zeigen, dass CAME im Vergleich zu Adam und Adafactor bessere Validierungsgenauigkeit erreicht, insbesondere bei großen Batch-Größen. Es reduziert den Speicherbedarf erheblich, ohne die Leistung zu beeinträchtigen, und übertrifft sogar bestehende Memory-Effizienz-Optimierer.

Zusammenfassend präsentiert CAME einen innovativen Ansatz, um die Konvergenzgeschwindigkeit und Speichereffizienz in der Optimierung großer Sprachmodelle zu vereinen, was es zu einem vielversprechenden Werkzeug für die Trainingsaufgaben von Sprachmodellen macht.</sample>
    <sample id="193">Der Text gibt keine spezifische Anzahl an Annotatoren an, die zum Erstellen des ursprünglichen Datensatzes verwendet wurden. Es wird lediglich erwähnt, dass etwa 1.000 Beispiele von Discourse-Unit-Paaren annotiert wurden.</sample>
    <sample id="194">Die Autoren der Präsentation gehören an die **Carnegie Mellon University** (CMU) und die **University of Washington**, sowie dem **Allen Institute for AI**. Dies wird im Text mehrfach erwähnt, insbesondere am Anfang, wo die Zusammenarbeit mit diesen Institutionen hervorgehoben wird.</sample>
    <sample id="195"># **Zusammenfassung: Reasoning over Hierarchical Question Decomposition Tree (RoHT) für erklärbare Fragebeantwortung (XQA)**

Die Arbeit konzentriert sich auf die Verbesserung der erklärbaren Fragebeantwortung, indem sie eine innovative Methode namens RoHT (Reasoning over Hierarchical Question Decomposition Tree) einführt. Das Ziel ist es, komplexe Fragen effektiv zu beantworten und Erklärungen für die Antworten zu liefern.

Die Autoren identifizieren Herausforderungen in bestehenden Ansätzen: Neuro-symbolische Methoden sind auf strukturierte Wissensbasen (KB) beschränkt, während dekompositionsbasierte Methoden mit der Vielfalt natürlicher Sprache zu kämpfen haben. Sie schlagen RoHT vor, ein zweistufiges Framework, das Fragezerlegung und Wissensintegration verbessert.

RoHT erstellt zunächst ein Hierarchisches Fragezerlegungsbaum (HQDT), das die Struktur komplexer Fragen darstellt. Der Baum zerlegt Fragen in Unterfragen und identifiziert atomare Fragen als Blätter. Dann führt das System probabilistische Inferenz über den HQDT durch, um Antworten aus Wissensbasen und Textkorpora zu integrieren. Der Prozess wählt Wissensquellen für jede Frage aus und aggregiert Antworten mit Wahrscheinlichkeiten.

Die Evaluierung auf Datensätzen wie KQA Pro und Musique zeigt die Effektivität von RoHT. Es übertrifft bestehende Methoden bei der Beantwortung komplexer Fragen, insbesondere wenn sowohl Wissensbasen als auch Textkorpora genutzt werden. RoHT demonstriert die Vorteile einer expliziten Zerlegung und der Integration heterogener Wissensquellen für verbesserte XQA-Leistung.

Zusammenfassend bietet RoHT eine vielversprechende Lösung für XQA, indem es die Granularität der Fragezerlegung und die Integration verschiedener Wissensquellen berücksichtigt, was zu präziseren und erklärbaren Antworten führt.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer (Governor) auf der linken Seite ist: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme (oder Chatbots) umfasst derzeit hauptsächlich drei Methoden zur Bewertung ihrer Leistung:

1. **Likert-Bewertungen auf Turn-Ebene**: Bewertungen, die auf einer Skala gegeben werden, um die Qualität jeder einzelnen Antwort oder jedes einzelnen Dialogturns zu messen.
2. **Likert-Bewertungen auf Dialog-Ebene**: Allgemeine Bewertungen der gesamten Konversation.
3. **Dialog-Level Pairwise Comparisons**: Menschen vergleichen direkt zwei Konversationen und wählen die bessere aus.

Diese Methoden haben jedoch Einschränkungen und bieten oft nur eine grobe Einschätzung der Dialogqualität.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil moderne Sprachmodelle immer größere Kontextfenster verwenden. Um die Robustheit ihrer Akzeptanzurteile zu gewährleisten, ist es entscheidend, wie sie auf längere Sequenzen reagieren und ob sie zwischen akzeptablen und inakzeptablen Sätzen innerhalb eines größeren Kontextes unterscheiden können.</sample>
    <sample id="199">Ja, das mehrsprachige Training (Multilingual Setting) führte in einigen Fällen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell. Dieses Phänomen wird als "Fluch der Mehrsprachigkeit" (Curse of Multilinguality) bezeichnet. Die Leistung des englischen Modells sank in sieben von neun Datensätzen, während sie in drei Datensätzen verbesserte.

Die Studie zeigt jedoch auch, dass ein gemischtes Training mit verschiedenen Sprachen die Leistung der Encoder-Decoder-Modelle verbessern kann, obwohl es auch Herausforderungen gibt.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entitäten im Voraus nicht. Sie erhalten Hintergrundinformationen über die beiden Optionen, indem sie Links zu Google-Suchergebnissen oder Wikipedia-Texten und -Bildern erhalten, bevor sie indirekte Referenzen zur Auswahl und Beschreibung einer Entität angeben.</sample>
    <sample id="201">Für die Bewertung der maschinellen Übersetzung (MT) in der Studie wurden mehrere Metriken verwendet:

1. **BLEURT (Bilingual Evaluation Understudy)**: Eine weit verbreitete Metrik zur Bewertung der Qualität von maschinellen Übersetzungen, die die Ähnlichkeit zwischen der generierten Übersetzung und einer Referenzübersetzung misst.

2. **Neural MT Metrics**: Moderne, auf neuronalen Netzen basierende Metriken, die spezifisch für die Bewertung von NMT-Systemen (Neural Machine Translation) entwickelt wurden.

3. **Expert-Based Human Evaluation**: Zusätzlich zu den automatisierten Metriken wurden auch Bewertungen durch menschliche Experten durchgeführt, die die Qualität der Übersetzungen anhand des MQM-Frameworks (Multidimensional Quality Metrics) beurteilten.

Diese Kombination aus automatisierten und menschlichen Bewertungsmetriken ermöglichte eine umfassende Analyse der Übersetzungskapazitäten des PaLM-Modells.</sample>
    <sample id="202">Basierend auf der Präsentation, beeinflusst die **Regression (d.h. das Fehlen von "diminishing returns" beim Feinabstimmen auf einem neuen Datensatz, wie CoNLL++)** die Generalisierungsfähigkeit von NER-Modellen unabhängig vom Typ der NER-Aufgabe.

Die Studie zeigt, dass adaptive Overfitting (übermäßiges Anpassen an das Trainingsdatenset) nicht der Hauptfaktor für die Leistungseinbußen ist. Stattdessen ist es hauptsächlich auf **temporale Drift** zurückzuführen, also den zeitlichen Abstand zwischen Trainings- und Testdaten.

Zusammenfassend lässt sich sagen, dass die Generalisierung für alle NER-Typen relevant ist und die Studie Faktoren identifiziert hat, die die Fähigkeit von Modellen beeinflussen, auf neue Daten zu verallgemeinern.</sample>
    <sample id="203">Positionalität ist für NLP (Natural Language Processing) wichtig, weil sie aufzeigt, wie Vorurteile und Unterschiede in den Perspektiven der Entwickler und der verwendeten Daten die Leistung und Genauigkeit von Modellen und Algorithmen beeinflussen können. Durch die Untersuchung der "Positionen" (demografischen Einflüsse und Lebenserfahrungen) der Beteiligten können Forscher wie Jenny und ihr Team erkennen, ob Modelle und Datensätze bestimmte Bevölkerungsgruppen systematisch bevorzugen oder benachteiligen. Dies ist entscheidend, um sicherzustellen, dass NLP-Technologien fair und inklusiv sind, insbesondere bei zunehmend subjektiven und sozial ausgerichteten Aufgaben.</sample>
    <sample id="204">Basierend auf der Präsentation wurden mehrsprachige LLMs wie BLOOM **nicht durch Adapter, sondern durch eine vollständige Feinabstimmung** angepasst für die Aufgaben des cross-lingualen semantischen Parsings. Der Text erwähnt, dass die Forscher festgestellt haben, dass mehrsprachige Sprachmodelle wie Codex und BLOOM "noch unzureichend" für diese Aufgaben sind, was darauf hindeutet, dass eine einfache Anpassung mit Adaptern nicht ausreicht. Stattdessen wurde eine vollständige Feinabstimmung durchgeführt.</sample>
    <sample id="205"># Von der Voreingenommenheit in der Datenverarbeitung zu ungerechten NLP-Modellen: Eine Untersuchung politischer Voreingenommenheit in Sprachmodellen

Die Arbeit untersucht die Auswirkungen politischer Voreingenommenheit in großen Sprachmodellen, die auf Web-Crawl-Daten trainiert sind, die häufig Nachrichtenmedien umfassen. Während die Vielfalt der Perspektiven ein Vorteil ist, können die inhärenten sozialen Voreingenommenheiten in politischen Meinungen zu Fairness-Problemen in NLP-Anwendungen führen.

Die Studie beantwortet zwei Hauptfragen: (1) Wie kann die politische Ausrichtung von Sprachmodellen bewertet und wie beeinflusst das Trainingsdaten? (2) Wie wirken sich Sprachmodelle mit unterschiedlichen politischen Einstellungen auf Downstream-Aufgaben aus und führen sie zu Fairness-Problemen?

Die Forscher verwenden politische Fragebögen und Testmethoden, um die politischen Neigungen verschiedener Sprachmodelle, einschließlich GPT-4 und GPT-Serien im Vergleich zu BART-Varianten, zu bewerten. Sie zeigen, dass diese Modelle unterschiedliche politische Ausrichtungen aufweisen. Durch ein kontrolliertes Experiment mit zusätzlichem Training auf parteiischen Korpora wurde festgestellt, dass die Trainingsdaten die politischen Voreingenommenheiten der Modelle beeinflussen.

Darüber hinaus untersuchen sie die Polarisierung in den Modellen, indem sie Daten vor und nach der Präsidentschaft von Donald Trump verwenden. Die Ergebnisse deuten darauf hin, dass die Modelle die gesellschaftliche Polarisierung aufgreifen. Bei der Bewertung von Sprachmodellen mit verschiedenen politischen Neigungen in Aufgaben wie der Hassrede- und Falschinformationenerkennung wurden signifikante Unterschiede in der Leistung je nach dem Zielpublikum festgestellt.

Die Studie hebt die Dringlichkeit von Fairness-Problemen im Zusammenhang mit politischen Voreingenommenheiten in Sprachmodellen hervor und präsentiert ein Dilemma zwischen der Vermeidung von Voreingenommenheit und Zensur bei der Datenaufbereitung.</sample>
    <sample id="206">Im Rahmen Ihrer Arbeit verwenden Sie ein Modell, das durch **Transferlernen** und **aktives Lernen** verbessert wird. Konkret beginnen Sie mit dem Transfer von Gewichten aus zwei verwandten Aufgaben:

1. **Debate-Stance-Klassifikation**: Bestimmt, ob zwei Debattenbeiträge von verschiedenen Personen übereinstimmen oder widersprechen, unabhängig vom Thema.
2. **Klassifikation von Expansion und Vergleich** aus dem PDTB (Pragmatic Development Treebank), da diese Klassen eng mit der Konzeption von Konsonanz und Dissonanz verbunden sind.

Diese anfänglichen Schritte ermöglichen es Ihnen, ein Modell zu erstellen, das bereits eine bessere Leistung als Zufallserwartung zeigt. Anschließend nutzen Sie aktive Lernstrategien, um das Modell iterativ zu verfeinern und mehr Daten für seltene Dissonanzbeispiele zu sammeln.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die neuesten Testsets der MT-Community (Maschinelle Übersetzung), um eine Überlappung mit den Trainingsdaten zu vermeiden. Speziell wurde der WMT (Workshop on Machine Translation) Evaluation-Datensatz genutzt.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen:

1. Adressieren von positiven Stereotypen und essentiellen Narrativen.
2. Verwendung eines intersectionellen Ansatzes zur Untersuchung von Vorurteilen und Schäden.
3. Erhöhte Transparenz bei Methoden zur Bias-Minderung.</sample>
    <sample id="209">Der Text beschreibt eine Methode zur Verbesserung der Fähigkeit von großen Sprachmodellen, **beschränkte Sprachplanung** durchzuführen. Die vorgeschlagene Methode, die **über-generieren-dann-filtern** (over-generate-then-filter) Technik beinhaltet, führt zu einer **erheblichen Verbesserung** sowohl in der **semantischen Vollständigkeit** als auch in der **Treue zu den Einschränkungen** im Vergleich zu früheren Ansätzen.

Obwohl der Text keine direkte Vergleichszahl für den Gewinn gegenüber der stärksten Baseline angibt, lässt sich aus den Ergebnissen schließen, dass die **Qualität der generierten Skripte deutlich steigt**. Die Visualisierungen zeigen, dass das über-generieren-dann-filtern die Leistung von InstructGPT erheblich verbessert, was darauf hindeutet, dass die vorgeschlagene Methode einen **bedeutenden Vorteil** gegenüber den bestehenden Baselines bietet.</sample>
    <sample id="210">Der/die Referent*in heißt Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie, DEPLAIN, können als Benchmark für die Aufgaben der automatischen Textvereinfachung in deutscher Sprache verwendet werden. Die Autoren haben in ihrer Arbeit gezeigt, wie man automatische Ausrichtungsmethoden bewertet und wie man Sprachmodelle feinabstimmt, um vereinfachte Texte zu generieren. Die erzielten Ergebnisse und der bereitgestellte Code bieten eine solide Grundlage für zukünftige Forschung und Vergleiche in diesem Bereich.</sample>
    <sample id="212">Die Arbeit experimentiert mit **einem kleineren Modell**, genauer gesagt T5, das nach der Feinabstimmung auf dem CoScript-Datensatz bessere Ergebnisse bei der Generierung von Skripten erzielt als die meisten großen Sprachmodelle.</sample>
    <sample id="213">Das Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung ist **OFA (Unified Multi-Modal Pre-trained Model)**.</sample>
    <sample id="215">Adam Przepiórkowski präsentiert seine Forschung zur Abhängigkeitsstruktur bei Koordinationen in der Linguistik. Er vergleicht verschiedene theoretische Ansätze, die unterschiedliche Strukturen für Koordinationsverbindungen vorschlagen. Dazu gehören asymmetrische Ansätze, bei denen ein Konjunkt als Hauptteil der Struktur herausgehebt wird, und symmetrische Ansätze, die alle Konjunkte gleichberechtigt behandeln.

Przepiórkowski stützt seine Argumentation für symmetrische Strukturen auf das Prinzip der Minimierung der Abhängigkeitslänge. Er veranschaulicht, dass direkte Objekte in der englischen Sprache nahe am Verb platziert werden sollten, während Adjunkte weiter entfernt sein können. Durch die Minimierung der Abhängigkeitslänge können lange direkte Objekte näher an das Verb bewegt werden, was die grammatikalische Akzeptanz erhöht.

Durch die Analyse des Penn Treebank-Korpora und anderer Daten bestätigte er die Beobachtung, dass linke Konjunkte tendenziell kürzer sind, besonders wenn der Regent (das Wort, das die Koordination überwacht) links oder abwesend ist. Diese Tendenz verschwindet, wenn der Regent rechts steht. Diese Beobachtung dient als Argument gegen asymmetrische Koordinationen und unterstützt symmetrische Modelle.

Zusammenfassend zeigt Przepiórkowskis Arbeit, dass die Abhängigkeitslänge und die Position des Regenten entscheidende Faktoren bei der Bestimmung der Struktur von Koordinationen sind, und bietet einen neuen Einblick in die Debatte über die Abhängigkeitsstruktur in der Linguistik.</sample>
    <sample id="217">**Zusammenfassung: "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation"**

Diese Arbeit konzentriert sich auf die Herausforderung der generativen Modellierung von Dialogen mit mehreren steuerbaren Attributen. Bisherige Methoden behandeln entweder einzelne Attribute oder kombinieren steuerbare Controller mit spezifischen Labels, aber nicht kontinuierliche Attribute. Die Forscher, Weihao Zeng, Lulu Zhao und Keqing He, schlagen DCG (Disentangled Controllable Generation) vor, ein Modell, das Attribute durch Entfesselung verschiedener Kombinationen lernt.

DCG verwendet ein Referenz-freies Bewertungsrahmenwerk namens MAE, das für unterschiedliche Attributgranularitäten geeignet ist. Es wurden zwei Benchmarks erstellt, um die Effektivität des Ansatzes zu demonstrieren. Das Modell, basierend auf DialoGPT, nutzt zwei Arten von Prompt-Modulen: attribute-orientierte und aufgabenorientierte Prompts. Attribute-orientierte Prompts fokussieren auf spezifische Attribute, während aufgabenorientierte Prompts globale Merkmale einbeziehen.

Ein wesentlicher Beitrag ist die Einführung von Pseudo-Kombinationen und einer Entfesselungsverlustfunktion, um die Vielfalt der Prompts zu erhöhen und die Unterscheidung zwischen Attributkombinationen zu verbessern. Die Ergebnisse zeigen, dass DCG im Vergleich zu Basismodellen und anderen Bewertungsmetriken überlegene Kontrollierbarkeit und Textqualität bei neuen Attributkombinationen erreicht.

Zusätzlich wurde ein automatisierter Bewertungsmetrik, MAE, entwickelt, der sowohl für diskrete als auch kontinuierliche Attribute korrelierte Koeffizienten übertrifft. Die Studie unterstreicht die Bedeutung von Prompt-Design und Bewertungsmetriken für die Generierung von Dialogen mit mehreren steuerbaren Attributen.</sample>
    <sample id="218">Die Autoren gehören zu Google Translate, was impliziert, dass sie mit der Universität oder Forschungseinrichtung verbunden sind, die Google unterstützt und fördert. Eine spezifische Universitätsangabe wird im Text nicht erwähnt.</sample>
    <sample id="219"># **A Compare-and-Contrast Multistage Pipeline for Financial Signal Extraction**

Die Präsentation beschreibt eine Forschung, die sich mit der Analyse von Finanzberichten und der Extraktion relevanter Signale befasst. Die Forscher, Jia-Huei Ju und sein Team, entwickeln einen Ansatz, um die Herausforderungen bei der Auswertung der Form 10-K-Berichte zu bewältigen, die jährliche Berichte von Unternehmen sind, die an die SEC (Securities and Exchange Commission) abgegeben werden.

Die Hauptmotivation für diese Arbeit stammt aus zwei Beobachtungen: Erstens ist der Text in den Berichten sehr konsistent, mit etwa 80% übereinstimmenden Token, und die Inhalte variieren jährlich. Die Präsentation veranschaulicht diese Textähnlichkeit über aufeinanderfolgende Jahre.

Das Team schlägt einen zweistufigen Pipeline-Ansatz vor:

1. **Highlighting-Aufgabe**: Hierbei geht es darum, die Wichtigkeit von Wörtern zwischen einem Zielbericht und seinem vorherigen Bericht zu vergleichen und zu kontrastieren. Das Ziel ist es, die Beziehungen zwischen den beiden zu identifizieren.

2. **Pipeline-Struktur**:
   - **Dokumentsegmentierung (Stage 0)**: Nicht im Detail behandelt, aber für die Vorbereitung der Daten wichtig.
   - **Relationen-Erkennung (Stage 1)**: Klassifizierung von Wortpaaren in drei Typen: β (höchste Ähnlichkeit), revidiert und nicht übereinstimmend.
   - **Feinabstimmung**: Außerhalb und innerhalb des Domänenkontexts, wobei ein externes Dataset (eSNLI) und ein selbst erstelltes Dataset (FINAL) verwendet werden. Sie nutzen Soft-Labeling-Techniken und mischen Ziele, um die Qualität der Pseudolabel zu verbessern.

Die Bewertung zeigt, dass ihr Ansatz eine hohe Genauigkeit bei der Vorhersage wichtiger Wörter erreicht, sowohl im eigenen Dataset als auch im eSNLI-Dataset. Die Forscher betonen die Vorteile ihrer Methode für die Simulation nicht übereinstimmender Paare und sehen vielversprechende Möglichkeiten für zukünftige Verbesserungen und Anwendungen.</sample>
    <sample id="220">Die Autoren gehören zur Stony Brook University.</sample>
    <sample id="221">In der Arbeit wurden hauptsächlich Sprachpaare untersucht, die Deutsch und Englisch umfassen, insbesondere die Übersetzung von Deutsch nach Englisch. Es wird jedoch erwähnt, dass die Methoden und Erkenntnisse auch auf andere Sprachpaare anwendbar sein könnten.</sample>
    <sample id="222">Dieser Forschungsbeitrag befasst sich mit der Herausforderung der Domänenanpassung in der offenen Domänen-Fragebeantwortung (Open-Domain Question Answering, ODQA). Das Ziel ist es, Modelle zu entwickeln, die auf einem allgemeinen Wissenscorpus, wie Wikipedia, trainiert sind, und dennoch präzise Antworten auf Fragen aus spezifischen Domänen wie Biomedizin liefern können.

Die Autoren untersuchen zwei Ansätze für Dateninterventionen: **Few-Shot** und **Zero-Shot** Methoden. Few-Shot-Methoden beinhalten das Training von Modellen mit wenigen Beispielen aus der Zieldomäne, während Zero-Shot-Methoden keine Beispiele aus der Zieldomäne verwenden. Sie untersuchen, wie diese Interventionen die Leistung von *Retriever* und *Reader* Modellen beeinflussen.

Die Studie identifiziert sechs Zieldomänen und analysiert den **Datenschichtwechsel** (Dataset Shift) zwischen der Quelldomäne (Wikipedia) und den Zieldomänen. Sie klassifizieren den Schichtwechsel in vier Typen: **Kein Schichtwechsel**, **Konzeptschichtwechsel**, **Covariatschichtwechsel** und **Vollschichtwechsel**.

Durch Experimente mit verschiedenen Dateninterventionen und der Analyse der Kompatibilität zwischen Quell- und Zielmodellen, zeigen die Autoren, dass **Few-Shot-Interventionen** für alle Zieldomänen effektiv sind, während **Zero-Shot-Interventionen** bei **Konzept-** und **Covariatschichtwechsel** nützlich sind. Die Ergebnisse zeigen eine signifikante Verbesserung der Leserleistung von bis zu 24%.

Zusammenfassend präsentiert die Arbeit einen umfassenden Ansatz zur Domänenanpassung in ODQA, der die Art des Schichtwechsels berücksichtigt und effektive Interventionen für verschiedene Szenarien vorschlägt.</sample>
    <sample id="223">Der/die Referent*in heißt Shangbin.</sample>
    <sample id="224">Während der Experimente wurden zwei Modelle untersucht:

1. **long-mBART**: Feinabgestimmt für Dokument-Level-Textvereinfachung.
2. **base mBART**: Feinabgestimmt für Satz-Level-Textvereinfachung.</sample>
    <sample id="225">Für das Training werden 53 Aufgaben aus 9 Gruppen verwendet, und für die Tests werden insgesamt 10 Aufgaben (die gesamte Gruppe "Common Sense Reasoning" sowie 5 zusätzliche Aufgaben aus den Gruppen "VQ" und "Miscellaneous") verwendet.</sample>
    <sample id="226">Basierend auf dem englischen Inhalt sind zwei Autoren an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Der Text diskutiert die Herausforderungen und den Potenzialen von Sprachmodellen im Bereich der "grounded language understanding" (gegrundetes Sprachverständnis), bei dem natürliche Sprachausdrücke in ausführbare Pläne oder Programme umgewandelt werden müssen. Der Mangel an grounding während der Prätrainingsphase wird als Hauptproblem identifiziert, was zu Diskrepanzen zwischen dem Training und den tatsächlichen Anwendungen führt.

Die Autoren schlagen ein neues Framework namens "Pangu" vor, das die Rolle von Sprachmodellen von der Generierung auf die Diskriminierung konzentriert. In diesem Ansatz interagiert ein symbolischer Agent mit der Umgebung und schlägt Kandidatenpläne vor, während das Sprachmodell diese Kandidaten bewertet und rangiert. Dies entlastet das Sprachmodell von der Aufgabe, grammatikalisch korrekte und gültige Pläne selbst zu generieren.

Pangu wurde erfolgreich auf der Aufgabe des wissensbasierten Frage-Antwortens getestet, zeigt aber seine Generellität für andere Szenarien. Die Experimente umfassen verschiedene Sprachmodelle (BERT, T5, Codex) und Lernmethoden (Feinabstimmung, In-Context-Lernen). Die Ergebnisse belegen die überlegene Leistung von Pangu in Bezug auf Genauigkeit und Sample-Effizienz, insbesondere bei der Verwendung von Codex mit In-Context-Lernen.

Ein wichtiger Erkenntnisgewinn ist, dass autoregressive Modelle wie ArcaneQA während des Trainings tendenziell überanpassen, während Pangu eine robustere Leistung sowohl bei bekannten als auch bei unbekannten Strukturen zeigt. Die Schlussfolgerung lautet, dass Diskriminierung eine effektivere Strategie für Sprachmodelle in der grounded language understanding darstellt, anstatt sich auf die Generierung zu verlassen. Das Team lädt zu Diskussionen und Kollaborationen ein und freut sich auf Feedback zu ihrer Arbeit.</sample>
    <sample id="228">Die Autoren haben an vier Datensätzen experimentiert: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">**Zusammenfassung: Detektion improvierbarer Behauptungen in argumentativem Schreiben**

Dieser Vortrag präsentiert eine gemeinsame Arbeit von Gabriella Skitalinskaya und Henning Wachsmuth über die Erkennung von Behauptungen, die in argumentativem Schreiben verbessert werden können.

Textüberarbeitung ist ein kritischer Aspekt professionellen Schreibens, insbesondere in der Argumentation, wo die Wortwahl die Wirkung auf das Publikum stark beeinflussen kann. Die Forscher untersuchen, wie man erkennt, ob eine Argumentationsbehauptung optimal formuliert ist oder ob Überarbeitungen erforderlich sind.

Ihre Arbeit konzentriert sich auf zwei Aufgaben: 1. **Suboptimale Behauptungserkennung**: Bestimmen, ob eine Behauptung Überarbeitungen benötigt oder optimale Formulierung erreicht hat. 2. **Vorschlag zur Verbesserung der Behauptung**: Identifizierung von Qualitätsproblemen, die bei der Überarbeitung behoben werden sollten.

Anstatt direkt aus menschlichen Überarbeitungsmustern zu lernen, untersuchen sie die Herausforderungen bei der Verwendung von Überarbeitungs-basierten Daten. Argumentative Texte aus Online-Debattenplattformen wie Kialo bieten wertvolle Einblicke.

In ihrer Arbeit identifizieren sie vier Hauptprobleme:

1. **Darstellung und Zuverlässigkeit**: Erstellen eines zuverlässigen Datensatzes, der die Qualität von Argumenten effektiv darstellt.
2. **Modellkomplexität und -architektur**: Auswahl geeigneter Modelle, die Überarbeitungsaspekte erfassen.
3. **Kontextabhängigkeit**: Bestimmung relevanter Kontextfaktoren für die Argumentqualität.
4. **Topische und Benutzerverzerrung**: Berücksichtigung von Benutzerfehlern und -vorurteilen in Debatten.

Ihre Ergebnisse zeigen, dass Überarbeitungs-Daten nützlich sind, und sie schlagen Methoden vor, um diese Herausforderungen anzugehen, mit dem Schluss, dass Kontextinformationen je nach Aufgabe und Textqualitätsproblemen relevant sein können.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer Inhalte, die aus dem Web gekrauselt (oder "crawled") wurden. Dies steht für "NAtural CHoice of Open Source Data". Dieser Datensatz wurde verwendet, um DrBERT, ein von RoBERTa abgeleitetes prä-trainiertes Sprachmodell für biomedizinische und klinische Domänen in französischer Sprache, zu trainieren.</sample>
    <sample id="232">Der/die Referent*in heißt David Vilar.</sample>
    <sample id="233">**Abstract:**

Die Arbeit "Attention as a Guide for Simultaneous Speech Translation" (SimulST) von Sara Papi, Matteo Negri und Marco Turchi zielt darauf ab, die Echtzeit-Übersetzung gesprochener Sprache zu verbessern. Aktuelle SimulST-Modelle erfordern oft spezifische Architekturen und komplexe Trainingsverfahren, was die Entwicklung und Wartung erschwert.

Die Autoren schlagen EDAtt (Encoder-Decoder Attention) vor, eine Strategie, die bestehende Offline-Übersetzungsmodelle nutzt, ohne sie neu zu trainieren. EDAtt entscheidet, ob ein Wort übersetzt werden soll, basierend auf der Aufmerksamkeitsverteilung zwischen Audio- und Textausgabe. Wörter werden nur dann ausgegeben, wenn die Aufmerksamkeit nicht auf die letzten Teile des Audios gerichtet ist, was eine stabile Information garantiert.

Die Ergebnisse zeigen, dass EDAtt im Vergleich zu etablierten Strategien und sogar zu spezialisierten Architekturen bessere Übersetzungsqualität (gemessen durch BLEU) und geringere Latenz (durchschnittliches Verzögern) erreicht, während es gleichzeitig rechenintensivere Modelle übertrifft. Die Studie betont die Effizienz und Effektivität von EDAtt bei der SimulST, wobei der Code und die Modelle offen zugänglich sind, um die Reproduzierbarkeit zu fördern.</sample>
    <sample id="234">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Ergebnisse der Übersetzung mit großen Sprachmodellen (LLMs) wie PaLM. Die Studie zeigt, dass die Auswahl und Qualität der Beispiele (Prompts) entscheidend für die Leistung sind.

- **Einfluss auf die Leistung:** Einfache Prompt-Formen (z.B. Markierung der Sprache) haben bei mehreren kurzen Promptings nicht viel Auswirkung. Stattdessen ist die Qualität der Beispiele, die dem Modell präsentiert werden, entscheidend.
- **Empfohlene Strategie:** Ein 5-Shot-Prompting-Ansatz, bei dem mehrere Beispiele (in diesem Fall 5) verwendet werden, liefert bessere Ergebnisse als einflussreichere, aber weniger informative Prompts.
- **Schlüsselfaktor:** Die Qualität der Beispiele, insbesondere aus hochwertigen Übersetzungen, ist wichtiger als die Ähnlichkeit mit der Quellensentenz.
- **Fazit:** Eine sorgfältige Auswahl der Prompt-Beispiele kann die Leistung von LLMs für die maschinelle Übersetzung erheblich verbessern und sie annähernd an die Leistung spezialisierter kommerzieller Systeme heranbringen.</sample>
    <sample id="235">Basierend auf dem präsentierten Inhalt und den erwähnten Autoren gehört das Forschungsteam der **University of Pennsylvania** an. Dies lässt sich aus der Erwähnung von "Patrick Fernandes", "Emmy Liu", "André F. T. Martins" und "Graham Neubig" schließen, die alle mit dieser Universität in Verbindung gebracht werden können.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen, wie im Kontext von MultiInstruct erwähnt, sind spezifische Textbeschreibungen, die für jede der 62 multi-modalen Aufgaben in der Dataset-Sammlung verfasst wurden. Diese Anweisungen dienen dazu, dem Modell zu helfen, die gewünschten Aufgaben zu verstehen und auszuführen. Jede Aufgabe hat fünf solcher Anweisungen, was dem Modell verschiedene Formulierungen und Perspektiven bietet, um die gleiche Aufgabe zu erfüllen.

Beispielsweise könnte eine Anweisung für eine Bildklassifizierungsaufgabe lauten: "Klassifiziere das Bild nach der Art des Tieres, das darauf zu sehen ist" oder "Identifiziere die Hauptfarbe des Objekts im Mittelpunkt des Bildes". Für eine Text-zu-Bild-Generierungsaufgabe könnte eine Anweisung sein: "Erstelle ein Bild eines historischen Gebäudes in der Stadt, mit einem blauen Himmel im Hintergrund".</sample>
    <sample id="237">Die Autoren schlagen vor, ein diagnostisches Testsuite namens **KITMUS (Knowledge Integration from Multiple Sources)** zu verwenden. Diese Testsuite umfasst eine **Corefenz-Aufgabe**, die darauf abzielt, die Fähigkeit von Modellen zu untersuchen, Wissen aus verschiedenen Quellen zu integrieren. 

KITMUS variiert die Verfügbarkeit von **entitätsbezogenem Wissen** (spezifisch für eine Situation) und **Hintergrundwissen** (allgemein und oft im Modell während des Pretrainings gelernt), um verschiedene Szenarien zu simulieren. So können sie untersuchen, wie Modelle mit Informationen umgehen, die entweder während des Trainings oder erst bei der Inferenz verfügbar sind.</sample>
    <sample id="238">In diesem Video präsentiert Yebowen Hu von der University of Central Florida das neu entwickelte Benchmark-Dataset **MeetingBank**. Das Dataset zielt darauf ab, die Notwendigkeit von Zusammenfassungstechnologien für verschiedene Lesedomänen zu decken, insbesondere für Meetings, die in unserem schnellen Tempo alltäglich stattfinden.

Hu beschreibt den Prozess der Datensammlung, der Audio-zu-Text-Konvertierung mithilfe von Speechmatics API, die Identifizierung von Meeting-Typen und -Daten aus Websites, die Extraktion von Zusammenfassungen und Meeting-Segmenten sowie die Zeitstempelausrichtung beinhaltet.

MeetingBank umfasst 1.366 Stadtverordnetenversammlungen und fast 7.000 Zusammenfassungseinheiten. Die Statistiken des Datasets zeigen Meetingdauer, Tokenanzahl, Sprecheranzahl und das Jahr der Sammlung.

Die Qualität der Zusammenfassungen wird durch Abdeckung (Prozentsatz der Zusammenfassungswörter im Transkript) und Dichte (Grad der Extraktion aus Referenztexten) gemessen.

Hu bewertet verschiedene Summarisierungssysteme, sowohl extraktive (Oracle, LEAD, LexRank, TextRank) als auch abstractive (BART-Large, Pagasus, Longformer, DialogLM, HMNet), einschließlich eines GPT-3-Modells. Die Ergebnisse zeigen, dass DialogLM die höchste ROUGE-2-Punktzahl erzielt, während GPT-3, trotz niedrigerer automatischer Metriken, in der menschlichen Bewertung hervorragend abschneidet.

Die Hauptbeiträge sind die Erstellung eines umfassenden MeetingBank-Datasets und die Erkenntnisse über die Leistungsfähigkeit verschiedener Summarisierungsmethoden. Hu betont die Wichtigkeit, sich auf die Hauptinhalte von Meetings zu konzentrieren und neue automatische Bewertungsmetriken zu entwickeln, die den menschlichen Präferenzen besser entsprechen.</sample>
    <sample id="239">**Kurze Rezension des Papiers: "Prompting PaLM für die Übersetzung: Bewertung von Strategien und Leistung"**

Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Zusammenfassung der gemeinsamen Arbeit mit meinen Kollegen von Google Translate vorstellen: "Prompting PaLM für die Übersetzung: Bewertung von Strategien und Leistung".

In dieser Arbeit präsentieren wir die erste systematische Untersuchung der Verwendung von Prompting bei großen Sprachmodellen (LLMs) für maschinelle Übersetzung. Wir untersuchten die Übersetzungsfähigkeiten eines 540 Milliarden Parameter umfassenden Modells namens PaLM, das im letzten Jahr 2022 vorgestellt wurde. PaLM wurde auf einer riesigen Textsammlung mit 780 Milliarden Token trainiert und erreichte zum Zeitpunkt der Veröffentlichung Staat-der-Kunst-Ergebnisse in Hunderten von NLP-Aufgaben.

Unser Ansatz beinhaltete die Anwendung der besten Praktiken der Übersetzungscommunity, indem wir die neuesten Testsets verwendeten, um eine Überschneidung mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen die Leistung von PaLM mit der von State-of-the-Art-Systemen, wie es im WMT-Bewertungssystem (World Machine Translation) der Fall ist. Wir nutzten fortschrittliche neuronale Übersetzungsmetriken und ergänzten diese um Ergebnisse aus menschlicher Bewertung durch Experten.

Unsere Experimente zeigten, dass das Prompting einen erheblichen Einfluss auf die Leistung von LLMs bei der Übersetzung hat. Ein einfaches Experiment mit einem-Schuss-Prompting (one-shot prompting) und zwei verschiedenen Prompts pro Satz ergab einen Unterschied von mehr als einem BLEURT-Punkt (ein Maß für die Qualität von Übersetzungen) bei der Mehrheit der Sätze (516 von 1000). In extremen Fällen konnte der Unterschied sogar bis zu 40 BLEURT-Punkte betragen.

Wir entschieden uns für eine 5-Schuss-Prompting-Strategie, bei der wir jedes Satzpaar, das dem System präsentiert wurde, mit der entsprechenden Sprache markierten. Unsere Ergebnisse zeigten, dass die Form des Promptings bei mehreren kurzen Promptings weniger wichtig ist, während sie bei ein- und null-Schuss-Promptings entscheidend ist. Bei fünf-Schuss-Prompting spielt die tatsächliche Form keine große Rolle mehr, vielmehr sind die Beispiele entscheidend.

Zusammenfassend lässt sich sagen, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Daher ist es entscheidend, Beispiele aus hochwertigen Übersetzungen auszuwählen. Wir verglichen beispielsweise Prompts aus dem Trainingsdatensatz mit denen aus dem WMT-Dev-Datensatz, der kuratierter und qualitativ hochwertiger ist als der Trainingsdatensatz. Die Ergebnisse zeigten, dass die Verwendung des Dev-Datensatzes zu einer besseren Leistung führt.

Trotzdem haben spezialisierte State-of-the-Art-Systeme immer noch einen erheblichen Vorteil gegenüber den Übersetzungen von PaLM. PaLM selbst kommt jedoch sehr nahe an die Leistung kommerzieller Systeme heran. In unserer Bewertung verwendeten wir Google Translate als Referenz.

Die menschliche Bewertung mit dem MQM-Framework (Machine Quality Metric) ergab, dass PaLM in Bezug auf die Fließfähigkeit der Übersetzungen mit State-of-the-Art-Systemen mithalten kann, aber die Hauptunterschiede liegen in der Genauigkeit. Häufigste Fehler bei PaLM waren Omissionen, was darauf hindeutet, dass das Modell manchmal Teile des Quelltextes weglässt, um eine bessere klangliche Übersetzung zu erzeugen. Die Kategorie "Stil/Unnatürlich" für PaLM war jedoch niedriger als bei den State-of-the-Art-Systemen, was darauf hinweist, dass PaLM flüssige, aber dennoch präzise Übersetzungen liefert.

Das war eine kurze Übersicht. Für detailliertere Informationen laden wir Sie zur vollständigen Präsentation des Papiers ein. Vielen Dank!</sample>
    <sample id="240">##  "Weaker Than You Think: Eine kritische Betrachtung des schwach überwachten Lernens"

Hallo, ich bin Dawei, Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere aktuelle Arbeit vorstellen: "Weaker Than You Think: Eine kritische Betrachtung des schwach überwachten Lernens". Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.

Ich möchte mit einem kurzen Einblick in schwach überwachte Lernmethoden beginnen. Bei schwach überwachten Lernansätzen werden Daten nicht manuell beschriftet. Stattdessen nutzen wir schwache Beschriftungsquellen wie einfache heuristische Regeln, Wissensdatenbanken oder niedrigqualitative Crowdsourcing-Daten, wie im Bild rechts dargestellt. Im Vergleich zu manuellen Beschriftungen sind schwache Beschriftungen kostengünstiger, aber auch lauter, d.h. ein Teil der Beschriftungen ist falsch. Wenn wir neuronale Netze direkt auf schwach beschrifteten Daten trainieren, neigen sie dazu, die Beschriftungsfehler zu memorisieren und nicht zu generalisieren.

In schwach überwachten Lernansätzen (WSL) werden Algorithmen entwickelt, die neuronale Netze robust trainieren sollen, um trotz der Beschriftungsrauschen eine gute Generalisierung zu erreichen. In jüngster Zeit behaupten viele WSL-Arbeiten, mit nur schwach beschrifteten Daten hohe Leistungen auf sauberen Testdatensätzen zu erzielen. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken: Die Menschen gehen dabei oft davon aus, dass zusätzlich saubere Validierungsdaten verfügbar sind, um das Modell auszuwählen. Dieses Problem können wir nicht ignorieren, es impliziert jedoch, dass manuelle Beschriftungen erforderlich sind.

Diese Unsicherheit führt zu drei Forschungsfragen:

1. Ist saubere Validierungsdaten für WSL notwendig oder können wir stattdessen einen lauten Validierungssatz verwenden?
2. Wenn saubere Daten erforderlich sind, wie viele saubere Proben brauchen wir?
3. Sollen wir saubere Proben nur für die Validierung verwenden oder gibt es bessere Möglichkeiten?

In unserer Arbeit haben wir diese Fragen untersucht und unsere Erkenntnisse sind wie folgt:

Erstens haben wir festgestellt, dass viele aktuelle WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um ordnungsgemäß zu funktionieren. Ohne saubere Validierungsdaten kommt es zu einem starken Leistungsabfall, wie die Abbildung zeigt. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen und die Kosten für die Beschaffung sauberer Validierungsproben nicht übersehen werden sollten.

Zweitens hilft die Erhöhung der Anzahl sauberer Validierungsproben, WSL-Ansätzen zu besseren Leistungen zu verhelfen, wie die Abbildung auf der linken Seite zeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um hohe Leistungen zu erzielen. Aber das ist noch nicht alles: Wenn wir uns entscheiden, saubere Proben zu verwenden, führt das Training direkt darauf sogar zu besseren Leistungen. Die Abbildung auf der rechten Seite zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Ansätzen, die saubere Daten nur für die Validierung nutzen. Wie wir sehen können, übertreffen direkte Feinabstimmungen WSL-Ansätze bereits bei 10 Proben pro Klasse.

Schließlich lässt sich der in früheren WSL-Arbeiten beanspruchte Leistungsgewinn leicht durch die Fortsetzung des Feinabstimmungsverfahrens auf saubere Validierungsproben erreichen. Wie die Abbildungen zeigen, unterliegt das einfache Modell FTw zu Beginn komplexeren WSL-Methoden wie COSINE. Doch wenn wir die Möglichkeit haben, auf saubere Proben weiter zu trainieren, erreicht FTw eine gleich gute Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz benötigen.

Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Methoden saubere, manuell beschriftete Daten benötigen, um effektiv zu sein. Ihre Leistungssteigerung und Praktikabilität sind übertrieben. Für zukünftige Arbeiten empfehlen wir Folgendes:

1. Berichten Sie über die Modellauswahlkriterien. Geben Sie beispielsweise an, ob die Modellauswahl über saubere Validierungsproben erfolgt.
2. Vergleichen Sie WSL-Methoden mit wenigen Schuss-Lernbasen, da beide mit sauberen Daten arbeiten.
3. Die kontinuierliche Feinabstimmung ist ein einfacher, aber starker Basismethode, die in zukünftigen WSL-Arbeiten berücksichtigt werden sollte.
4. Wir haben unseren Code open-source veröffentlicht. Finden Sie ihn im QR-Code auf diesem Slide.

Vielen Dank und viel Spaß auf der Konferenz!</sample>
    <sample id="241">In ihrer Arbeit "Human-in-the-loop Evaluation for Early Misinformation Detection" untersuchen Ethan und sein Team die Herausforderungen bei der automatischen Erkennung von Falschinformationen in sozialen Medien. Sie kritisieren, dass bestehende Systeme oft unrealistisch bewertet werden, indem sie retrospektive Datensätze verwenden, und dass sie anfällig für Gegenbeweise sind, die erst nach der öffentlichen Entlarvung der Falschinformationen auftauchen.

Um diese Probleme anzugehen, schlagen sie ein **human-zentriertes, end-to-end-System** vor, das menschliche Moderatoren in den gesamten Prozess integriert. Das System besteht aus zwei Hauptkomponenten:

1. **Misleidende Behauptungen erkennen:** Hier werden Tweets nach relevanten Schlüsselwörtern gefiltert und ein T5-Modell für die Behauptungsentnahme eingesetzt. Behauptungen zu unbewiesenen COVID-19-Behandlungen werden nach ihrer Popularität bewertet und dann Menschen zur Überprüfung vorgelegt.
2. **Politische Verstöße überprüfen:** Ein BERT-basiertes Stance-Klassifizierungsmodell identifiziert Tweets, die unzugelassene Behandlungen befürworten, und markiert diese für die menschliche Überprüfung.

Die Studie bewertet die Effektivität des Systems anhand der frühen Erkennung unbewiesener Behandlungen und der Identifizierung von Twitter-Policy-Verstößen. Die Ergebnisse zeigen, dass das System 65 % der Policy-Verstöße korrekt identifiziert und 124,2 Verstöße pro menschlicher Arbeitsstunde aufdeckt.

Zusammenfassend präsentiert die Arbeit einen vielversprechenden Ansatz für die Entwicklung und Bewertung menschlicher In-the-loop-Systeme zur Falschinformationenerkennung in Echtzeit.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen:

1. **Human-Bewertung**: Menschen werden gebeten, zu bewerten, welche Konversation besser ist oder Konversationen auf einer Likert-Skala zu bewerten.
2. **Komparative Bewertung**: Mehrere Dimensionen der Dialogqualität werden explizit bewertet, wie z.B. Relevanz der Antworten.
3. **Likert-Skalen auf Turn- oder Dialogebene**: Bewertungen auf individuellen Turnen oder gesamten Dialogen.
4. **Paarevergleiche auf Dialogebene**: Menschen vergleichen direkt zwei Dialoge und wählen den besseren aus.

Diese Methoden haben jedoch Nachteile in Bezug auf Subjektivität und Feinheit der Bewertung.</sample>
    <sample id="243">An der Arbeit sind insgesamt 5 Autoren beteiligt: Jenny (als Erstautorin), Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea ist das benötigte Hintergrundwissen:

1. **Berufliches Wissen**: "Servin ist ein Richter" und "Kea ist ein Bäcker".
2. **Gerichtliche Abläufe**: "Richter entscheiden Fälle in Gerichtssälen".

Dieses Hintergrundwissen ist notwendig, um zu verstehen, dass der Pronomen "er" in "nach einem langen Arbeitstag, der damit verbracht wurde, Fälle in einem Gerichtssaal zu entscheiden, war er glücklich, sich zu entspannen" auf Servin bezieht, da Servin der Richter ist, der Fälle entscheidet.</sample>
    <sample id="245"># **A Needle in a Haystack: Eine Analyse hochübereinstimmender MTurk-Arbeitskräfte für die Zusammenfassung**

Diese Arbeit präsentiert eine Methode zur Identifizierung hochübereinstimmender Amazon Mechanical Turk (MTurk) Arbeitskräfte für Textzusammenfassungstasks. Das Ziel ist es, die Herausforderungen bei der Bewertung der Qualität von MTurk-Arbeitskräften und der Automatisierung von Metriken anzugehen.

Die Forscher entwickelten einen zweistufigen Pipeline-Ansatz. Zunächst werden *Qualifikationsaufgaben* verwendet, um Arbeitskräfte basierend auf ihrer Fähigkeit, verschiedene Bewertungsaspekte korrekt zu beurteilen, in vier Kategorien einzuteilen. Dies führt zu einer Auswahl von 26 hochqualifizierten Arbeitskräften (8 Gold, 18 Silber). Die zweite Stufe, *Ausdaueraufgaben*, testet die Kapazität der Arbeitskräfte, hohe Arbeitslasten zu bewältigen, und ergibt 12 qualifizierte Arbeitskräfte (4 Gold, 8 Silber).

Ein *Referenz-basiertes Task* wurde eingeführt, um die allgemeine Leistung zu bewerten. Die Ergebnisse zeigen, dass die Pipeline-Arbeitskräfte eine hohe Inter-Annotator-Übereinstimmung (IAA) erreichen, mit einem besten Krippendorff's Alpha von 0,443. Im Vergleich zu Baseline-MTurk-Arbeitskräften und CloudResearch-Arbeitskräften bietet die Pipeline eine ähnliche Qualität bei reduzierten Kosten.

Die Studie schlägt vor, dass die vorgeschlagene Pipeline eine effiziente Methode ist, um hochübereinstimmende Arbeitskräfte zu finden, und kann als Best Practice für große, kostengünstige Zusammenfassungsprojekte dienen. Zukünftige Arbeiten konzentrieren sich auf die Verbesserung der Arbeitskräfteauswahl und die Erweiterung auf verschiedene Sprachen und Plattformen. Einschränkungen umfassen den Fokus auf englische Zusammenfassungen und die Notwendigkeit, die Fragegestaltung weiter zu optimieren.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Er kann auf GitHub eingesehen und heruntergeladen werden, wie im Vortrag erwähnt.</sample>
    <sample id="247"># **FACTKG: Fact Verification via Knowledge Graph Reasoning**

Die Präsentation von Jiho Kim von KAIST AI stellt einen neuen Ansatz für die Faktenüberprüfung vor, der auf Wissensgraphen (Knowledge Graphs, KG) basiert. Bisherige Datensätze für die Faktenüberprüfung, wie FEVER, VitaminC, TabFact und InfoTabs, nutzten entweder Text oder Tabellen als Beweise. Kim und sein Team schlagen jedoch vor, Wissensgraphen als neue Beweismittel zu nutzen, da sie eine direktere und verlässlichere Verbindung zu den Behauptungen herstellen können.

Der vorgeschlagene Datensatz, **FactKG**, enthält Behauptungen in natürlicher Sprache, die entweder schriftlich oder kolloial formuliert sind, und zielt darauf ab, die Konsistenz zwischen diesen Behauptungen und dem Wissensgraphen DBpedia zu überprüfen. Die Behauptungen sind mit zwei Labels versehen: **Unterstützt** oder **Rückgängig gemacht**. Die Aufgabe besteht darin, relevante Informationen aus dem KG zu extrahieren und die Behauptungen mithilfe dieser Beweise zu überprüfen.

Die Forscher haben fünf Arten des Denkens definiert, um die Behauptungen zu verifizieren: ein-Hop, Konjunktion, Existenz, Multi-Hop und Negation. Sie demonstrieren, wie diese Methoden auf verschiedene Behauptungsarten angewendet werden können. Die Datensammlung umfasst statistische Daten und zeigt, dass die vorgeschlagenen Basismodelle, einschließlich des GEAR-Modells, die Leistung im Vergleich zu Baselines, die nur Text verwenden, verbessern.

Diese Arbeit eröffnet neue Möglichkeiten für die Faktenüberprüfung in natürlicher Sprache, indem sie die Stärken von Wissensgraphen nutzt, und bietet einen wertvollen Datensatz für die Forschung in diesem Bereich.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind **nicht** in Bezug auf jede demographische Gruppe ausgewogen.

Im Vortrag wird erwähnt, dass die ursprünglichen Datensätze oft nur wenige Annotatoren pro Instanz haben und dass Demografiedaten selten erfasst und geteilt werden. Um dies zu umgehen, haben die Forscher einen **re-Annotationsprozess** durchgeführt, bei dem **diverse Annotatoren** hinzugezogen wurden.

Zwar wurden über 16.000 Annotationen von über 1000 Annotatoren aus 87 Ländern gesammelt, aber die Verteilung ist nicht gleichmäßig. Bestimmte Gruppen, wie Personen mit College- oder Hochschulabschluss und Englischsprachige, sind **überrepräsentiert**. Im Gegensatz dazu sind **nicht-binäre Personen** und andere Minderheitengruppen **unterrepräsentiert**.</sample>
    <sample id="249">Innerhalb der akzeptablen Domain wurden Sätze durcheinander gebracht, indem **relevante, aber strukturverschiedene Sätze aus derselben Datensätze** ausgewählt und als Präfixe zu den akzeptablen und inakzeptablen Eingabepaaren hinzugefügt wurden. 

Der Fokus lag darauf, zu untersuchen, wie das Modell auf Sätze reagiert, die zwar semantisch relevant, aber syntaktisch unterschiedlich sind.</sample>
    <sample id="250">Eine dimensionale Bewertung, im Kontext von ABC-Eval, bezieht sich auf die Methode, die Dialogmodelle anhand mehrerer spezifischer und klar definierter Aspekte oder Dimensionen von Dialogqualität zu beurteilen, anstatt nur eine allgemeine Bewertung abzugeben. Anstatt nur Holistische Bewertungen (z.B. mit Likert-Skalen) zu verwenden, identifiziert ABC-Eval Verhaltensweisen wie Relevanz, Selbst- oder Partnerkontradiktion, Halluzinationen oder Verstöße gegen allgemeine Kenntnisse. Dies ermöglicht eine feinere und präzisere Analyse der Stärken und Schwächen eines Dialogmodells.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China an.</sample>
    <sample id="252">Die Präsentation von Sai Kiran Tanikella und seinem Team stellt "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion" vor, eine innovative Methode zur effizienten Suche relevanter Rechtspraxisfälle.

Die Herausforderung bei der Suche in einer wachsenden Menge an Rechtsfällen besteht darin, relevante frühere Präzedenzfälle (zitierte Dokumente) für eine gegebene Abfrage zu finden. Die Arbeit führt zwei wichtige Beiträge ein: den Indian Legal Prior Case Retrieval (IL-PCR) Datensatz und die U-CREAT-Pipeline.

Der IL-PCR-Datensatz enthält 7.070 indische Rechtsfälle mit durchschnittlich 6.775 Zitaten pro Dokument, was einen umfassenden Testboden für Algorithmen bietet.

U-CREAT nutzt eine unsupervised Lernansatz und eine Ereignis-basierte Methode. Durch die Extraktion von Ereignissen aus Falldokumenten, dargestellt als Subjekt-Verb-Objekt-Tripletts, wird die Relevanz zwischen Abfrage und Kandidatendokumenten bestimmt.

Die Experimente verglichen verschiedene Modelle, darunter count-basierte, Transformer-basierte und event-basierte Ansätze. Event-basierte Modelle, wie das Event Filtered Documents Modell, zeigten signifikant bessere Ergebnisse in Bezug auf F1-Score und Inferenzzeit im Vergleich zu anderen Methoden, einschließlich staat-des-Kunst-Ansätzen.

Zusammenfassend bietet U-CREAT eine vielversprechende Lösung für die Prior Case Retrieval Aufgabe und eröffnet neue Perspektiven für die Rechtsrecherche.</sample>
    <sample id="253">**Kurzfassung:**

In ihrer Präsentation "DisorBERT: Ein Doppeltes Domänenanpassungsmodell zur Erkennung von Anzeichen psychischer Störungen in sozialen Medien" stellen die Forscher Mario Ezra Aragón und sein Team ein neuartiges Modell vor, das darauf abzielt, psychische Gesundheitsprobleme durch die Analyse von Social-Media-Posts automatisch zu erkennen.

Psychische Störungen wie Depression, PTBS, Bulimie und Anorexie betreffen das Denken, Fühlen, die Stimmung und das Verhalten. Heutzutage bieten soziale Medien eine große Menge an Inhalten, die wertvolle Einblicke in die Herausforderungen der Menschen bieten können. Einige Nutzer teilen offen ihre Erfahrungen, während andere anonym über psychische Gesundheitsprobleme sprechen und Hilfe suchen.

Das vorgestellte Modell, DisorBERT, basiert auf BERT, einem Sprachmodell, das auf Wikipedia und Google Books trainiert wurde. Durch eine Doppelte Domänenanpassung wird BERT an die spezifische Sprache von Reddit und den Bereich der psychischen Gesundheit angepasst. Dies verbessert die semantische Verständnisfähigkeit des Modells und ermöglicht es, sich auf die Erkennung von Anzeichen psychischer Störungen zu spezialisieren.

Die Ergebnisse auf dem eRisk-Datensatz zeigen, dass DisorBERT eine gute Balance zwischen Präzision und Rückruf aufweist, während andere Methoden in einer Dimension hervorstechen, aber in der anderen nachlassen. Durch eine Analyse der vorhergesagten Wörter und Textsegmente demonstriert DisorBERT eine Tendenz, Wörter mit negativer oder psychologischer Relevanz zu fokussieren, was auf seine Fähigkeit hinweist, Anzeichen von psychischen Störungen zu erkennen.

Zukünftige Arbeiten beinhalten die Erforschung verschiedener Lexika und die Integration klinischer Daten.</sample>
    <sample id="254">**Zusammenfassung:**

Die Arbeit "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" von Sun Qi und seinem Team aus der Nanjing University of Science and Technology adressiert die Herausforderungen bei der Extraktion von Beziehungen in Dokumenten (Document-level Relation Extraction, DocRE) unter Verwendung von distanziert überwachtem Daten (DS-Daten).

Bisherige Methoden stützen sich auf aufwendig annotierte Datensätze, was zeit- und arbeitsintensiv ist. DS-Daten bieten eine Alternative, enthalten aber oft verschiedene Noise-Level. Vorherige Ansätze bekämpfen diesen Noise mit Pseudo-Labels, laufen jedoch immer noch Gefahr, durch falsch-positive Pseudo-Labels verunreinigt zu werden.

Die Forscher schlagen ein neues Framework vor, das Unsicherheitsschätzungen integriert, um die Qualität von DS-Labels zu verbessern. Der Ansatz umfasst:

1. **Vorab-Denoising DocRE Modell:** Trainiert mit DS- und manuell annotierten Daten, um Pseudo-Labels zu generieren.
2. **Instanz-Level Unsicherheitsschätzung:** Für überlappende Beziehungen, um Unsicherheiten für jedes Pseudo-Label zu erfassen.
3. **Iterative Re-Labeling Strategie:** Mit dynamischen Klassenerunsicherheits-Schwellenwerten, um Long-Tail-Probleme anzugehen.
4. **Multi-Phase Training:** Iterative Neubewertung der DS-Daten für bessere Modellleistung.

Durch die Implementierung dieser Techniken konnte das Framework signifikante Verbesserungen in der DocRE-Leistung auf öffentlichen Datensätzen erzielen, wie die Ergebnisse zeigen. Die Hauptbeiträge liegen in der Verbesserung der Labelqualität, der Behandlung überlappender Beziehungen und der effektiven Nutzung von DS-Daten.</sample>
    <sample id="255">Die Form des Prompts ist **besonders bei sehr kurzen Promptings (z.B. Zero-Shot oder One-Shot) wichtig**. Bei längeren Promptings (wie 5-Shot) hat die Form weniger Einfluss, während die **Qualität und Relevanz der Beispiele** entscheidender sind.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art (aktuell beste) Dialogmodelle evaluiert.</sample>
    <sample id="258">In diesem Video präsentiert Chiang Cheng-Han seine Forschung zum Thema: "Können große Sprachmodelle eine Alternative zur menschlichen Bewertung darstellen?"

Die Studie schlägt vor, große Sprachmodelle (LSM) zur Bewertung der Qualität von Texten in der natürlichen Sprachverarbeitung (NLP) einzusetzen. Anstatt menschliche Evaluatoren zu beauftragen, werden LSM mit Anweisungen trainiert und mit Beispielen versorgt, die bewertet werden sollen. Das Ziel ist, ob LSM, die natürliche Sprachanweisungen verstehen können, zuverlässige Bewertungen liefern.

Bisher wurden menschliche Bewertungen als Standard verwendet, aber die Studie betont, dass es beim Einreichen keine vorherigen Arbeiten gab, die die Idee der LSM-Bewertung untersuchten. Die Forschung führte Experimente durch, bei denen LSM Geschichten, die entweder von GPT-2 generiert oder von Menschen geschrieben wurden, anhand von Grammatik, Kohärenz, Beliebtheit und Relevanz bewerteten.

Die Ergebnisse zeigten, dass menschliche Evaluatoren (in diesem Fall Englischlehrer) menschlich geschriebene Texte bevorzugten, während einige LSM keine klare Vorliebe zeigten. Allerdings demonstrierten Davinci und ChatGPT eine deutliche Präferenz für menschliche Texte, was darauf hindeutet, dass bestimmte LSM als Alternative zur menschlichen Bewertung geeignet sein können.

Die Präsentation hebt auch weitere Forschungsfragen hervor, wie die Konsistenz der Bewertungen zwischen LSM und menschlichen Evaluatoren, die Auswirkungen von Anweisungsänderungen und die Vorteile und Nachteile gegenüber menschlicher Bewertung. Die vollständigen Ergebnisse sind in der Arbeit verfügbar, die auf ACL vorgestellt wurde.</sample>
    <sample id="259">**Zusammenfassung der Präsentation: "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations"**

Yusen Zhang von der Penn State University stellte ein neues Modell namens XSemPLR vor, das die Übersetzung und semantische Analyse von Benutzerabfragen in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen (wie SQL oder Lambda-Kalkül) vereint.

Bisherige Modelle konzentrierten sich oft auf einzelne Sprachen oder Bedeutungsrepräsentationen. XSemPLR überwindet diese Einschränkungen, indem es einen einheitlichen Datensatz für 22 natürliche Sprachen in 15 Sprachfamilien bereitstellt, die auf 9 Datensätzen in verschiedenen Bereichen basieren.

Die Studie untersucht sechs Trainings- und Evaluierungsszenarien:

1. **Translate-Test:** Verwendung von Google Translate für die Sprachübersetzung und dann monolingualen Modellen.
2. **Monolingual:** Training und Evaluierung auf der gleichen Sprache (z.B. Deutsch zu Deutsch).
3. **Monolingual Few-shot:** Training mit nur 10% der Trainingsdaten.
4. **Multilingual:** Training eines Modells auf mehreren Sprachen gleichzeitig.
5. **Cross-lingual Zero-shot und Few-shot Transfer:** Training auf einer Sprache und Transfer zu einer anderen.

Die Ergebnisse zeigen, dass Encoder-Decoder-Modelle die beste Leistung auf allen Datensätzen erzielen. Multilinguales Training verbessert die Leistung, führt aber zum "Fluch der Multilingualität" (Leistungsrückgang in einigen Sprachen).

Die Studie vergleicht auch die Leistung zwischen Zero-shot und Few-shot Transfer und stellt fest, dass Few-shot Transfer die Lücke schnell schließt. Sie hebt ferner die Grenzen aktueller multilingualer Modelle wie Codex und BLOOM für Cross-Lingual Semantic Parsing hervor.

XSemPLR bietet eine umfassende Plattform für die Forschung und Bewertung im Bereich der mehrsprachigen semantischen Analyse.</sample>
    <sample id="260">Basierend auf dem bereitgestellten Text sind **zwei** Autoren an der Arbeit beteiligt:

1. Jingwei Yi (University of Science and Technology of China)
2. (Unnamenterwähnt, implizit der erste Autor, der die Präsentation hält)</sample>
    <sample id="261">Ein guter Planer sollte zwei Hauptkriterien erfüllen:

1. **Semantische Vollständigkeit**: Die generierten Skripte sollten sinnvoll und logisch sein.
2. **Treue zu den Einschränkungen**: Die Skripte sollten die spezifischen, oft vielfältigen Einschränkungen der geplanten Ziele genau befolgen.</sample>
    <sample id="262">Basierend auf dem vorgestellten Inhalt scheint die Arbeit von einem Team um Siyu Yuan von der Fudan University durchgeführt worden zu sein. Da keine spezifische Anzahl an Autoren genannt wird, kann man annehmen, dass es mehrere Autoren gibt, typischerweise im Bereich eines akademischen Papiers. Eine genaue Anzahl wird nicht angegeben.</sample>
    <sample id="263">Die Präsentation behandelt das Thema "Minderung von Label-Voreingenommenheit für In-Context Lernen" im Kontext großer Sprachmodelle. In-Context Lernen ist ein beliebter Ansatz, der jedoch aufgrund verschiedener Designentscheidungen, wie der Auswahl und Reihenfolge von Beispielen, instabil sein kann. Vorherige Studien zeigten, dass diese Instabilität auf Label-Voreingenommenheit zurückzuführen ist. Bisher gab es jedoch keine umfassende Diskussion über bestehende Erkenntnisse zu Bias-Problemen im In-Context Lernen oder Methoden zur Erkennung und Minderung neuer Bias-Typen.

Die Autoren schlagen eine systematische Untersuchung von Label-Bias in In-Context Lernen vor und identifizieren einen neuen, wichtigen Bias-Typ: Domain-Label-Bias. Sie entwickeln ein neuartiges Kalibrierungsverfahren, das alle Bias-Typen berücksichtigt. Durch Experimente wird bestätigt, dass das Training mit Texten aus der Aufgabendomäne die Vorlieben des Modells für Label-Namen stark beeinflussen kann.

Die Lösung besteht in der "Domain-Context-Kalibrierung", die zufällige Wörter aus der Aufgabendomäne als contentfreie Texte verwendet, um Modell-Voreingenommenheiten zu schätzen und zu korrigieren. Im Vergleich zu früheren Methoden zeigt diese Kalibrierung eine signifikante Leistungssteigerung, insbesondere bei Aufgaben mit hohem Domain-Label-Bias. 

Zusammenfassend bieten die Autoren einen umfassenden Ansatz zur Bewältigung von Bias in In-Context Lernen und demonstrieren die Effektivität ihrer Methode anhand verschiedener Modelle und Datensätze.</sample>
    <sample id="264"># **TAVT: Transferable Audio-Visual Text Generation**

Lin Wang präsentiert seine Forschung zur Verbesserung der Generierung von Audio-Text in verschiedenen Domänen, eine Herausforderung im Bereich der Multimodalität. Der Vortrag fokussiert sich auf die Entwicklung einer neuen Aufgabe namens *Transferable Audio-Visual Text Generation (TAVT)*, um die Einschränkungen bei der Datenerfassung zu überwinden.

Die Hauptidee besteht darin, ein Modell zu schaffen, das sich an neue Multimodal-Domänen mit begrenztem beschriftetem Datenanfang anpassen kann. Die Herausforderung liegt in den Domänenschichten, die sich in visueller Stil, Audio-Energie und mehr unterscheiden. Die Forscher beobachten, dass der Inhalt des visuellen Inhalts bei Änderungen des Stils und der Aufnahmemethode stark variieren kann, während der Audio-Inhalt stabiler bleibt.

Das vorgeschlagene Framework besteht aus drei Komponenten:

1. **Audio-Visual Meta-Mapper**: Dieser Teil kartiert visuelle Konzepte in einen einheitlichen auditiven Semantikraum, unabhängig von Domänenunterschieden.

2. **Audio-Visual Encoder und Language Model Generator**: Ein Transformer-basiertes System, das Alpha-Werte verwendet, um die Beiträge verschiedener Modalitäten zu jedem Wort zu gewichten.

3. **Dual Counterfactual Contrastive Learning (DCLL)**: Ein innovativer Ansatz, der feinere Überwachungssignale aus Gegenfaktischen Ergebnissen generiert, um die visuelle-textuelle Ausrichtung zu optimieren.

Die Experimente auf den Benchmarks MSVD und MSR-VTT zeigen, dass TAVT im Vergleich zu bestehenden Ansätzen überlegen ist, insbesondere in niedrig-ressourcenreichen Domänen. Die Arbeit zielt darauf ab, Fortschritte in der Multimodal-Textgenerierung zu erzielen und die Anpassungsfähigkeit an verschiedene Audio-Visuelle Szenarien zu verbessern.</sample>
    <sample id="265">Der/die Referentin heißt Vasudha.</sample>
    <sample id="266">Basierend auf dem vorgestellten Inhalt gehört der Autor, Adam Przepiórkowski, an die Universität, die mit dem "enhanced version of the Penn Treebank" und der Veröffentlichung "Why wouldn't you use universal dependencies" in Verbindung gebracht wird. Ohne spezifischere Nennung ist es jedoch nicht genau bestimmbar, um welche Universität es sich handelt.</sample>
    <sample id="268">Basierend auf der Präsentation sind die häufigsten Fehler von PaLM **Omissionsfehler**, bei denen Teile der Quelltexte aus der Übersetzung weggelassen werden, um eine bessere Klangqualität zu erreichen.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und hier ist meine Frau Sarah Finch. Heute erzählen wir euch alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationsbasierter künstlicher Intelligenz. Diese Arbeit wurde vom Emory NLP-Labor unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt.

Stellen wir uns vor, du hast gerade ein Dialogmodell entwickelt und möchtest wissen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis besteht darin, menschliche Bewertungen durchzuführen, wie zum Beispiel indem man menschliche Richter bittet, zu entscheiden, welche der beiden Konversationen besser ist, oder indem man Konversationen auf einer Likert-Skala bewertet. Diese Methoden eignen sich gut, um eine holistische Bewertung der Gesamtqualität von Dialogen zu erhalten. Allerdings hat die Dialogqualität viele Facetten. Daher könntest du mehrere Dimensionen der Chat-Qualität bewerten wollen, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.

Eine Möglichkeit besteht darin, menschliche Richter einfach bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie beispielsweise die Relevanz der Antworten des Modells mithilfe bestehender vergleichender Methoden oder Likert-Skalen. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz zielt darauf ab, die Subjektivität menschlicher Bewertungen zu verringern, indem explizit annotiert wird, ob jede Antwort des Modells bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Geben irrelevanter Informationen oder das Kontradieren von sich selbst oder seinem Partner. Wir nennen diesen Ansatz die Annotation von Verhaltensweisen im Chat oder kurz ABC-Eval.

Wir haben diese Methode entwickelt, um umfassend die Chat-Modellverhaltensweisen abzudecken, die in jüngster Literatur als einflussreich auf die Dialogqualität angesehen werden. ABC-Eval ist in der Lage, die Raten verschiedener thematischer Fehler zu messen, die Chat-Modelle begehen können. Beispielsweise misst ABC-Eval die Anzahl der Runden, in denen ein Chat-Modell seinen Partner ignoriert oder irrelevante Aussagen macht, sich selbst oder seinen Partner widerspricht, falsche Fakten erfindet oder gegen allgemeine Wissensregeln verstößt, und wann das Modell erfolgreich oder fehlschlägt, Empathie zu zeigen.

Um zu bestimmen, welche Bewertungsmethode am effektivsten ist, haben wir vier State-of-the-Art-Chat-Modelle ausgewählt und sie auf 100 menschlich-Bot-Konversationen pro Modell mit ABC-Eval bewertet. Zusätzlich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialogebene und dialogebene Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Dialogaspekte gesammelt, da dies der Standard in der Bewertung von Chat-Modellen über mehrere Dimensionen ist.

Aus unserer Analyse der Bewertungsergebnisse haben wir festgestellt, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die mit bestehenden Methoden gesammelten Labels, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Konversationen. Darüber hinaus sind die ABC-Eval-Labels prädictiver für die Gesamtqualität der Konversation, wie durch eine einfache lineare Regressionsanalyse gezeigt. Beispielsweise erklärt die Messung des Anteils der Runden mit Selbst- und Partnerwidersprüchen 5% bzw. 10% der Konversationsqualität, während die durchschnittlichen Likert-Konsistenzbewertungen nur 4% oder weniger erklären.

Schließlich haben wir überprüft, ob jede Bewertungsmetrik ein einzigartiges Merkmal der Chat-Qualität erfasst, indem wir eine Schrittweise lineare Regressionsanalyse durchgeführt haben. Siehst du, die Kombination aller ABC-Eval-Metriken erklärt über 25% der Konversationsqualität, und wenn du die Metriken nacheinander entfernst, verliert die meisten davon eine beträchtliche Menge an Information über die Qualität. Im Gegensatz dazu erklärt die Kombination aller Turn-Level-Likert-Metriken viel weniger Qualität, und nur wenige dieser Metriken tragen einzigartige Informationen bei.

Diese zuverlässigen, informativen und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationsbasierte künstliche Intelligenz mit einer höheren Auflösung als es frühere Methoden schaffen können, zu bewerten. Wie du aus den Ergebnissen unseres Experiments sehen kannst, bleiben noch einige Herausforderungen, die präzise quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20% ihrer Antworten Regelverstöße, sie geben in etwa 15% der Antworten irrelevante Informationen und widersprechen sich oder ihren Partner in etwa 10% der Fälle. Mit dem schnellen Fortschritt im Bereich könnte sich der Anteil dieser Fehlerraten bei neuen Modellen, die nach unserer Bewertung veröffentlicht werden, verringern. Genau deswegen ist es umso wichtiger, zuverlässige und präzise Bewertungsmetriken zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in der Branche als ein bedeutender Schritt in diese Richtung genutzt wird, und wir freuen uns darauf, wie sich konversationsbasierte künstliche Intelligenz in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren, James und Sarah Finch, sowie Professor Jinho Choi, sind mit der **Emory University** verbunden. Das Emory NLP Lab, das diese Arbeit geleitet hat, ist ebenfalls an dieser Universität angesiedelt.</sample>
    <sample id="271">In der vorgestellten Arbeit steht "CFT" für "Fine-tuning" (auf Deutsch: Feinabstimmung). Die Forscher untersuchen, wie effektiv es ist, Modelle direkt auf sauberen (manuell annotierten) Trainingsdaten zu feinabstimmen (Fine-tuning), im Vergleich zu Methoden der schwachen Überwachung (Weakly Supervised Learning, WSL), die schwache Label-Quellen verwenden.</sample>
    <sample id="272">An der Arbeit sind insgesamt 7 Autoren beteiligt: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.</sample>
    <sample id="273">##  Wenn Übersetzung Kontext erfordert: Eine datengestützte, mehrsprachige Untersuchung

Mein Name ist Kayo Yin und ich präsentiere unsere Arbeit mit dem Titel *"Wenn Übersetzung Kontext erfordert: Eine datengestützte, mehrsprachige Untersuchung"*. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig erstellt.

Wie in diesem Beispiel mit dem Wort "Mole": Je nach Kontext kann sein Bedeutung variieren. Als "Spion" in einem Satz über Minister, oder als "Fleck" in einem medizinischen Kontext.  Die Übersetzung hängt also stark vom Kontext ab. Die Bewertung, wie gut Modelle mit solchen Fällen umgehen, ist jedoch schwierig. Standardmetriken wie BLEU sind hier unzureichend, da sie nur globale Übereinstimmungen messen. Während es Vorschläge für gezielte Evaluierung von Kontextabhängigkeiten gibt, sind diese oft auf spezifische Domänen und Sprachpaare beschränkt.

In unserer Arbeit stellen wir zwei Fragen:

1. **Wann erfordert Übersetzung Kontext?**
2. **Wie gut meistern Modelle diese Fälle?**

Um die erste Frage zu beantworten, haben wir eine erweiterte Version der Metrik **CXMI (Context eXpedientness in Machine Translation)** entwickelt, die wir als **Pointwise CXMI** bezeichnen. Sie ermöglicht die Messung der Kontextabhängigkeit auf Satz- oder Wortniveau. Wörter mit hohem **P-CXMI** benötigen demnach Kontext für eine korrekte Übersetzung.

Wir untersuchten Wörter mit hohem **P-CXMI** und identifizierten Muster. Unsere Analyse basierte auf Transkripten von TED-Vorträgen, die in 14 verschiedene Sprachen übersetzt wurden. Wir analysierten:

* **Part-of-Speech-Tags:** Mit hohem Mittelwert **P-CXMI**. So fanden wir beispielsweise Dualpronomen im Arabischen, die aufgrund des Fehlens dieser Form im Englischen Kontext benötigen.

* **Vokabular:** Mit hohem **P-CXMI** im Durchschnitt über alle Vorkommen. Dies zeigte sich bei Eigennamen im Chinesischen, wo Kontext für die Konsistenz innerhalb eines Dokuments notwendig ist.

* **Individuelle Token:**  Um Phänomene zu identifizieren, die über das Wort hinausgehen, wie das Auflösen von Ellipsen.

Aus diesen Erkenntnissen entwickelten wir den **Multilingual Discourse-Aware (MuDA) Tagger**, um automatisch Wörter zu markieren, die an fünf verschiedenen Diskursphänomenen beteiligt sind.

Wir nutzten den MuDA-Tagger und andere Metriken, um verschiedene Übersetzungssysteme auf Dokumentenebene zu bewerten.

* **Corpus-basierte Metriken:** BLEU zeigte, dass kontextunabhängige Modelle die besten Ergebnisse lieferten. COMET bevorzugte kontextbewusste Modelle. Die Wort-F-Maßstabe ergab vergleichbare Leistungen für kontextbasierte und -unabhängige Modelle.

* **MuDA-Benchmark:** Kontextbewusste Modelle überzeugten bei Diskursphänomenen wie Formalität und Lexikalische Kohäsion. Bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen waren die Unterschiede geringer.

Unser Vergleich verschiedener kommerzieller Systeme zeigte, dass DeepL in der Regel präzisere Dokumentübersetzungen liefert als Google Translate.

**Fazit:**

Unsere datengestützte Analyse in 14 Sprachpaaren hilft uns zu verstehen, wann Übersetzungen Kontext erfordern und ermöglicht die Entwicklung eines Benchmarks für Dokumentenübersetzung. Dieser Benchmark kann aufzeigen, in welchen Bereichen Modelle stark sind und in welchen sie verbessert werden müssen.</sample>
    <sample id="274">Der/die Referent*in heißt Yusen Zhang.</sample>
    <sample id="276"># **IndicMT Eval: Eine Meta-Evaluierung von Maschinenübersetzungsmetriken für indische Sprachen**

Dieses Forschungsprojekt zielt darauf ab, die Bewertung von Übersetzungen in indische Sprachen zu verbessern, indem es die Lücke zwischen der englischen und anderen Sprachrichtungen schließt. Die Autoren sammelten eine Dataset mit 7.000 Beispielen aus fünf indischen Sprachen: Tamil, Malayalam, Hindi, Marathi und Gujarati.

Sie generierten 1.400 Übersetzungsoptionen für jedes Beispiel mithilfe von sieben verschiedenen Übersetzungssystemen. Anschließend wurden diese Übersetzungen von bilingualen Experten annotiert, die Fehler typisierten und bewerteten. Die Studie konzentriert sich auf verschiedene Metriken zur Bewertung der Übersetzungen und ihre Korrelation mit menschlichen Bewertungen.

Die Ergebnisse zeigen, dass overlap-basierte Metriken wie chrF zwar die höchste Korrelation für einige Sprachen aufweisen, aber insgesamt schlecht abschneiden. Embedding-basierte Metriken wie LabSE und BERTscore mit multilingualen Modellen zeigen bessere Korrelationen. Die COMET-Metrik-Varianten erreichen die höchsten Korrelationen in allen Sprachen.

Ein interessantes Ergebnis ist die Skewierung der Metrik-Scores, wobei viele Metriken einen engen Bereich abdecken, während menschliche Bewertungen einen breiteren Bereich aufweisen. Die Feinabstimmung der COMET-Metrik mit dem MQM-Dataset (einem fein abgestuften Fehlerkatalog) verbessert die Leistung, insbesondere bei der Bewertung von Genauigkeitsfehlern.

Schließlich demonstriert die Studie die Wirksamkeit des feinabgestimmten "IndicCOMET" bei der Bewertung von Übersetzungen in indischen Sprachen und übertrifft die Basismetriken in mehreren Fällen. Das Dataset ist öffentlich verfügbar und bietet einen wertvollen Beitrag zur Forschung im Bereich der Maschinenübersetzung für indische Sprachen.</sample>
    <sample id="277">Die neue Methode, die im Paper beschrieben wird, hat keinen spezifischen Namen. Sie wird als "Multiset Tagging und Latent Permutationen" bezeichnet, ohne einen eigenständigen Namen zu tragen.</sample>
    <sample id="278">Die Methode der „markierten Wörter“ (Marked Words) nutzt das sociolinguistische Konzept der „Markierung“, um Gruppenunterschiede zu identifizieren. Sie vergleicht die generierten Personas mit einem Satz von Schlüsselwörtern, die Gruppen als „markiert“ oder „unmarkiert“ kennzeichnen. Top-Wörter für markierte Gruppen helfen dabei, stereotype und schädliche Narrative zu erkennen.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280"># **MultiEMO: Ein multimodales Fusionsframework für die Emotionserkennung in Gesprächen**

Shi Tao präsentiert seine Forschung zum Thema Emotionserkennung in Gesprächen (ERC), insbesondere ein neues Modell namens MultiEMO. Das Ziel ist die Vorhersage der Emotionen in jeder Äußerung eines Dialogs, die über Text, Audio und visuelle Modalitäten verfügen.

Die Herausforderungen in diesem Bereich umfassen die effektive Nutzung multimodaler Informationen, die Verbesserung der Leistung bei Minderheitsklassen und die Unterscheidung ähnlicher Emotionen. MultiEMO löst diese Probleme mit vier Hauptkomponenten:

1. **VisExtNet**: Ein innovativer visueller Feature-Extraktor, der Gesichtsausdrücke integriert, ohne redundante Szeneninformationen zu erfassen.
2. **MultiAttn**: Ein multimodales Fusionsmodell, das bidirektionale Multi-Head-Cross-Attention-Schichten verwendet, um Modalitäten zu integrieren.
3. **Sample-Weighted Focal Contrastive Loss**: Diese Verlustfunktion priorisiert schwierige Klassen und maximiert Abstände zwischen ähnlichen Emotionen.
4. **Experimente**: MultiEMO erreicht Spitzenleistungen auf den ERC-Datensätzen MELD und IEMOCAP, insbesondere bei Minderheitsklassen und ähnlichen Emotionen.

Die Präsentation betont die Verbesserungen, die MultiEMO gegenüber bestehenden Methoden bietet, und hebt die Herausforderungen hervor, wie z. B. die Verarbeitung asynchroner emotionaler Hinweise aus verschiedenen Modalitäten. Trotz einiger Einschränkungen, wie der Unfähigkeit, Sprecher von irrelevanten Personen zu unterscheiden, zeigt MultiEMO vielversprechende Ergebnisse.</sample>
    <sample id="281">In ihrer Präsentation "When Does Translation Require Context? A Data-driven, Multilingual Exploration" untersuchen Kayo Yin und ihr Team, wann Übersetzungen Kontext benötigen. Sie stellen fest, dass viele Übersetzungen von der vorherigen Kontextinformation abhängen, wie im Beispiel des Wortes "mole". Um diese Abhängigkeit besser zu verstehen, haben sie eine erweiterte Version der CXMI (Context Usage Measure for Machine Translation) entwickelt, die Punktuelle CXMI (P-CXMI), um die Kontextabhängigkeit auf Satz- oder Wortniveau zu messen.

Durch die Analyse von TED-Talk-Übersetzungen in 14 Sprachen identifizierten sie Phänomene wie duale Pronomen in Arabisch und die Bedeutung des Kontextes bei der Wahl der passenden Verbformen. Sie stellten fest, dass bestimmte Sprachen und Phänomene, wie die korrekte Formalität und die Übersetzung von Eigennamen, häufig Kontext erfordern.

Als Nächstes entwickelten sie einen Multilingualen Diskurs-Bewussten Tagger (MuDA), um kontextabhängige Beispiele in Parallelkorpora zu identifizieren. Sie verglichen verschiedene Übersetzungsmodelle und -systeme, einschließlich DeepL und Google Translate, anhand von P-CXMI-basierten Metriken und kontextabhängigen und -unabhängigen Modellen.

Ihre Ergebnisse zeigen, dass kontextbewusste Modelle bei bestimmten Diskursphänomenen, wie Formalität und lexikalische Kohäsion, überlegen sind, während kontextunabhängige Modelle bei anderen Phänomenen bessere Ergebnisse liefern. Diese Erkenntnisse bieten wertvolle Einblicke in die Herausforderungen und Fortschritte bei der Dokumentenübersetzung auf maschineller Basis.</sample>
    <sample id="282">**Abstract: StoryTrans: Nicht-parallele Autor-Stil-Übertragung bei Geschichten**

Diese Arbeit präsentiert StoryTrans, ein innovatives Modell für nicht-parallele Textstilübertragung auf der Ebene von Geschichten und Diskursen. Bisher konzentrierten sich die meisten Studien auf Token- oder Satzebene, wie z.B. Sentiment- oder Formalitätstransfer. StoryTrans geht einen bedeutenden Schritt weiter, indem es die Autorensprache auf der Diskurs- und Storyebene imitiert, was entscheidend für den Nachbau des Autorenstils ist.

Die Hauptaufgabe besteht darin, komplexe sprachliche Präferenzen und Diskursstrukturen in langen Texten zu imitieren, insbesondere die stark mit bestimmten Themen verknüpften Stile. StoryTrans löst dies durch zwei Hauptkomponenten:

1. **Diskursdarstellung und Stil-Embedding:** Das Modell lernt Diskursdarstellungen aus Quelltexten und kombiniert diese mit lernbaren Stil-Embeddings, um Texte in Zielstilen zu generieren.
2. **Zwei-Stufen-Generierung:** Zuerst wird der Quelltext mit stil-spezifischen Schlüsselwörtern maskiert übertragen, gefolgt von einer expliziten Generierung der gesamten Geschichte, die diese Schlüsselwörter einbezieht.

Das Training umfasst ein disentanglement-Verfahren, um Stil und Inhalt zu trennen, sowie eine sorgfältige Optimierung für Inhaltserhaltung und Stilkontrolle. Experimentelle Ergebnisse auf chinesischen und englischen Datensätzen zeigen, dass StoryTrans im Vergleich zu Basismodellen bessere Ergebnisse bei Stilkontrolle und Inhaltserhaltung erzielt.</sample>
    <sample id="283">Die zuerst erwähnte symmetrische Abhängigkeitsstruktur ist das **Prague-Modell**.</sample>
    <sample id="284">In seiner Präsentation auf der ACL Main Conference 2022 stellt Peng Tianshuo von der Wuhan University sein Forschungspapier "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" vor. Das Papier konzentriert sich auf die Verbesserung der Genauigkeit und Effizienz von Modellen für die universelle Informationsextraktion (UIE).

Aktuelle spanbasierte UIE-Modelle identifizieren und markieren Textabschnitte, was stark von den annotierten Grenzpositionen abhängt. Um diese Abhängigkeit zu verringern, schlägt das Papier einen "fuzzy" Ansatz für die Span-Grenzen vor, anstatt präzise Positionen zu verwenden. Es erkennt die Ambiguität bei der Markierung von Gold-Span-Grenzen an.

Das FSUIE-Modell integriert einen neuartigen "Fuzzy Span Attention"-Mechanismus, der die Aufmerksamkeitsspanne dynamisch anpasst und ein kontinuierliches Verteilungsmodell der korrekten Grenzwahrscheinlichkeiten verwendet. Dieser Mechanismus reduziert die Abhängigkeit von festen Span-Grenzen und verbessert die Modellleistung.

Experimente auf verschiedenen UIE-Aufgaben, einschließlich Named Entity Recognition, Relationship Extraction und Aspect Sentiment Triplet Extraction, zeigen die Effektivität von FSUIE. Das Modell erzielt herausragende Ergebnisse und übertrifft die Basismodelle ohne Fuzzy-Span-Mechanismus, insbesondere bei kleineren Datensätzen.

Zusammenfassend führt die Arbeit einen innovativen Ansatz für UIE ein, der die Genauigkeit verbessert und die Abhängigkeit von manuellen Annotationen verringert, was zu einem vielversprechenden Werkzeug für natürliche Sprachverarbeitung macht.</sample>
    <sample id="285">In ihrer Präsentation "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization" erläutert Mingqi Gao von der Peking Universität ihre Forschung zu fehlerhaften Zusammenfassungen und der Korrektur dieser Fehler.

Zwei Hauptansätze zur Lösung von Factualitätsproblemen in der Zusammenfassung existieren:

1. **Integration von Factualität in das Training:** Ziel ist es, Modelle zu entwickeln, die präzisere Zusammenfassungen generieren.
2. **Factual Error Correction (FEC) Modelle:** Diese Modelle korrigieren Fehler in bereits generierten Zusammenfassungen unabhängig vom Zusammenfassungsmodell.

Gao und ihr Team konzentrieren sich auf die zweite Methode und kritisieren die aktuelle Bewertung von FEC-Modellen. Aktuelle Methoden verwenden Metriken wie FactCC oder DAE, die nur eine allgemeine Bewertung der Factualität liefern und keine spezifischen Fehlerarten identifizieren.

Um diese Probleme zu adressieren, schlagen sie ein neues Bewertungsrahmenwerk vor, das auf ERRANT basiert. Es führt eine Klassifizierung von Fehlern in inhaltliche und formbezogene Kategorien ein und umfasst drei Schritte: Ausrichtung, Klassifizierung und Vergleich.

Ihre Ergebnisse zeigen, dass das Training von FEC-Modellen mit korrektierten Zusammenfassungen aus Dialog-Summenkapsel-Datensätzen die besten Ergebnisse liefert. Sie betonen die Notwendigkeit neuer Bewertungsansätze und die Verbesserung der Fähigkeit von FEC-Modellen, komplexe Fehler wie Zusätze und Attributfehler zu korrigieren.</sample>
    <sample id="286">Die Referenten sind James Finch und Sarah Finch.</sample>
    <sample id="287">An der Arbeit sind vier Autoren beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Die im Vortrag erwähnten Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind **BLiMP (Benchmark for Language Model Predictability)** und **SyntaxGym**. Diese Datensätze bieten Sätze, die für ihre syntaktische Struktur bekannt sind, und ermöglichen es, die Akzeptanz- und Grammatikalitätsurteile von Sprachmodellen zu bewerten.

Zusätzlich wird auch **Wikipedia** als Datensatz erwähnt, um die Robustheit der Modelle gegenüber völlig irrelevanten Kontexten zu testen.</sample>
    <sample id="290">In der Präsentation werden fünf Methoden für die erste Forschungsfrage erwähnt, die sich mit der Notwendigkeit eines sauberen Validierungssatzes in der Schwachen Überwachungslernforschung (WSL) befasst. Die Abkürzungen dieser Methoden sind:

1. **FTw (Fine-tuning on Weak data)**
2. **COSINE (eine komplexere WSL-Methode)**

Die Präsentation argumentiert, dass diese Methoden tatsächlich saubere, manuell annotierte Daten benötigen, um effektiv zu sein, und dass ihre Leistungssteigerungen und Praktikabilität oft übertrieben dargestellt werden.</sample>
    <sample id="291">Das Modell wird anhand von 11 verschiedenen **biomedizinischen und klinischen Downstream-Aufgaben** in französischer Sprache evaluiert. Dazu gehören Aufgaben wie **Named Entity Recognition, Klassifikation, Part-of-Speech-Tagging und Fragebeantwortung**.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit einem 4 GB großen Datensatz aus NACHOS (einem Web-Krawldaten-Datensatz in Französisch) trainiert.</sample>
    <sample id="295">Der/die Referent*in heißt Adam Przepiórkowski.</sample>
    <sample id="296"># **Ironieerkennung in der natürlichen Sprache: Eine Zusammenarbeit zwischen der Universität Turin und Amazon Alexa**

Valerio Basile präsentiert in diesem Video eine Forschung, die die Grenzen der aktuellen Ansätze in der Verarbeitung natürlicher Sprache (NLP) erforscht, insbesondere im Hinblick auf Ironieerkennung. Die Studie ist das Ergebnis einer Zusammenarbeit zwischen der Universität Turin und Amazon Alexa und konzentriert sich auf die Herausforderungen der Annotation und Modellierung von Ironie in Textdaten.

Die Forscher haben ein Korpus namens EPIC (English Perspectivist Irony Corpus) erstellt, das 300 kurze Konversationen aus sozialen Medien, Reddit und Twitter umfasst. Diese Daten wurden von 74 Annotatoren in 5 verschiedenen englischen Varianten bewertet, wobei jeder Annotator 200 Texte erhielt. Die Annotatoren wurden gebeten, zu beurteilen, ob eine Antwort ironisch ist oder nicht, was zu einer Vielzahl von Antworten führte.

Die Analyse ergab, dass die Inter-Annotator-Übereinstimmung je nach dem, wie die Daten aufgeteilt wurden (z. B. nach Geschlecht, Alter oder Nationalität), variierte. Die Forscher entwickelten dann *perspektivenbewusste Modelle*, indem sie ein vorab trainiertes Sprachmodell auf verschiedene Teilmengen der Daten anpassten. Diese Modelle zeigten im Vergleich zu aggregierten Goldstandard-Modellen eine höhere Zuversicht bei ihren Vorhersagen.

Interessanterweise entdeckten sie, dass die Unterschiede in den Annotationen mit dem Alter und der geografischen Herkunft zusammenhingen. Generationen, die nahe beieinander liegen, und Annotatoren aus dem Vereinigten Königreich und Irland zeigten die größten Variationen in ihren Ironieperzeptionen. Die Studie unterstreicht die Komplexität der Ironieerkennung und die Notwendigkeit, die Perspektive der Annotatoren bei der Modellentwicklung zu berücksichtigen.</sample>
    <sample id="297">In ihrer Präsentation diskutiert die Autorin das Projekt "Von Dogwhistles zu Bullhorns: Die Entschleierung kodierter Rhetorik mit Sprachmodellen", das sich mit der Identifizierung und dem Verständnis von Dogwhistles in politischen Reden befasst. Dogwhistles sind codierte Begriffe, die eine doppelte Botschaft vermitteln: eine offenkundig harmlose für die breite Öffentlichkeit und eine versteckte, oft beleidigende oder diskriminierende Botschaft für eine bestimmte Gruppe. Ein bekanntes Beispiel ist die Verwendung von "Cosmopolitan" als Codewort für Juden.

Das Projekt entwickelt eine Typologie und ein Glossar mit 340 Dogwhistle-Termen und -Symbolen, die rassistisch, transphob oder antisemitisch sind. Es untersucht historische US-Politische Reden und findet eine enge Korrelation zwischen der Häufigkeit rassistischer Dogwhistles und der sogenannten Republican Southern Strategy. Die Analyse zeigt, dass Dogwhistles häufiger in konservativen Reden auftauchen.

Zusätzlich wird die Leistung von Sprachmodellen, insbesondere GPT-3, bei der Erkennung von Dogwhistles getestet. Die Ergebnisse zeigen, dass GPT-3 formelle Dogwhistles gut identifizieren kann, aber Schwierigkeiten mit informellen und transphobischen Begriffen hat.

Schließlich demonstriert die Studie, wie Dogwhistles Online-Inhaltsmoderation umgehen können. Durch den Austausch von Standard-Gruppen-Labels und Slurs durch Dogwhistles werden hasserfüllte Sätze als weniger toxisch eingestuft, was die Wirksamkeit von Automatisierungssystemen zur Inhaltsmoderation untergräbt.

Zusammenfassend bietet das Projekt wertvolle Einblicke in die komplexe Welt der kodierten Rhetorik und ihre Auswirkungen auf politische Kommunikation und Online-Interaktionen.</sample>
    <sample id="298">Die Schlussfolgerung, dass die zeitliche Verzögerung (temporal drift) die Hauptursache für den Leistungsverlust ist, basierte auf einem Experiment, bei dem einige Modelle mit jüngeren Daten neu trainiert oder weiter vorab trainiert wurden. Es wurde festgestellt, dass die Leistung mit zunehmendem zeitlichen Abstand zwischen Trainings- und Testdaten abnahm. Dies bestätigte die Hypothese der Forscher über die Ursache des Leistungsverlusts.</sample>
    <sample id="299">**Verbessern der Robustheit von NLI-Modellen mit Minimax-Training**

In dieser Arbeit untersuchen wir die Schwachstellen von Natural Language Inference (NLI)-Modellen, die trotz beeindruckender Leistung in Benchmarks anfällig für Ausnutzen durch Kurzweglösungen sind. Diese Kurzweglösungen sind spurige Korrelationen zwischen Eingabemerkmalen und Labels, die während der Datenerstellung entstehen.

Unsere Lösung: Ein Minimax-Training-Ansatz, der ein Learner- und ein Auxiliary-Modell kombiniert. Während das Learner-Modell NLI-Aufgaben minimiert, maximiert das Auxiliary-Modell die Verluste des Learners, indem es Beispielgewichte generiert, die den Learner zu schwierigeren, unterrepräsentierten Beispielen lenken. Diese "harten" Beispiele sind entscheidend für eine robuste Generalisierung.

Unser Ansatz:

1. **Dynamik-basiert:** Er generiert Gewichte ohne Vorwissen über Kurzwegmuster.
2. **Effizient:** Verwendet ein einfaches Feed-Forward-Netzwerk als Auxiliary.
3. **Flexibel:** Anwendbar auf verschiedene Modelle, Datensätze und Testumgebungen.

In Experimenten auf MNLI, FEVER, QQP und ihren Ausnutzbarkeits-Testsets (HANS, PAWS) zeigt unser Ansatz eine verbesserte Auslandsleistung im Vergleich zu Standard- und Shortcut-Minderungs-Methoden, während die Inlandsleistung erhalten bleibt.

Zusätzlich untersuchen wir die Auswirkungen von Modellgröße, Auxiliary-Größe und qualitativen Aspekten der erlernten Gewichte.

Wir freuen uns auf Diskussionen während unserer Posterpräsentation!</sample>
    <sample id="300">**Zusammenfassung: Interaktive Diktation und ein neuer Ansatz für Sprachbearbeitung**

Belinda präsentiert eine innovative Arbeit über interaktive Diktation, ein System, das es Benutzern ermöglicht, Dokumente sowohl mündlich zu diktieren als auch per Sprachbefehl zu bearbeiten. Im Gegensatz zu bestehenden Lösungen, die oft auf feste Befehlsstrukturen angewiesen sind, zielt dieses Projekt auf eine natürlichere und intuitivere Interaktion ab.

Die Forscher definieren interaktive Diktation als Prozess, bei dem Benutzer diktieren und bearbeiten können, ohne zwischen diesen Aktionen eine Trennworte oder Befehle zu verwenden. Ein Beispiel ist die Korrektur eines Datums von "on Friday the 23rd" auf "Just wanted to ask about the event on the 23rd" während der Diktation.

Um dieses Konzept zu realisieren, entwickelten sie einen vierstufigen Ansatz:

1. **ASR-Erkennung:** Umwandlung von Audio in Text.
2. **Segmentierung:** Trennung von Diktat- und Befehlsuttern.
3. **Befehlsextraktion:** Verarbeitung und Normalisierung von Befehlen.
4. **Ausführung:** Umsetzung von Diktat- und Befehlssätzen in der Dokumentenbearbeitung.

Sie entwickelten eine benutzerfreundliche Schnittstelle für die Datenerfassung und schufen ein Dataset für Trainingszwecke. Ihr Basissystem verwendet GPT-3 und T5 Modelle für verschiedene Aufgaben, wobei GPT-3 in Genauigkeit überlegen ist, aber langsamer als T5, das Programme vorhersagt, um Effizienz zu steigern.

Die Arbeit schließt mit der Veröffentlichung von Code und einem detaillierten Bericht in einem Papier, das zukünftige Entwicklungen in diesem spannenden Bereich der Sprachinteraktion fördern soll.</sample>
    <sample id="302">Die Token der Ausgabesequenz müssen permutiert werden, weil die ursprüngliche Reihenfolge der Tokens in der Ausgabe nicht vorhersehbar ist, nachdem die entsprechenden Eingabetoken verarbeitet wurden. Das Modell muss daher die beste Reihenfolge der Tokens bestimmen, um die logische Struktur der Ausgabe zu reproduzieren.</sample>
    <sample id="303">Die Autoren empfehlen eine transparentere Berichterstattung über Methoden zur Bias-Reduktion, weil sie besorgt sind, dass positive Stereotypen und essenzialistische Darstellungen, die in LLMs auftreten, möglicherweise auf unerwünschte "Wertausrichtungen" oder andere, nicht vollständig verstandene Methoden zurückzuführen sind. Ohne Transparenz kann man diese Muster nicht angemessen untersuchen und potenzielle Schadwirkungen nicht vollständig erkennen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Sätze, die als ungrammatisch oder inakzeptabel eingestuft werden, im Gegensatz zu akzeptablen Sätzen. Diese Eingaben werden verwendet, um die Fähigkeit von Sprachmodellen zu bewerten, zwischen akzeptablen und inakzeptablen Sätzen zu unterscheiden. Das Papier, über das gesprochen wird, schlägt vor, diese Methode auf längere Sequenzen auszuweiten, um die Robustheit von Sprachmodellen in größeren Kontextfenstern zu untersuchen.</sample>
    <sample id="305">In seiner Präsentation "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" diskutiert Dawei, ein PhD-Student an der Saarland Universität, die Herausforderungen und Missverständnisse rund um das schwache Überwachen (Weak Supervision) und das schwache überwachtes Lernen (Weakly Supervised Learning, WSL).

WSL nutzt kostengünstige, aber fehleranfällige Quellen wie Heuristiken oder Crowdsourcing, um Daten zu labeln, anstatt auf teure manuelle Annotationen zu setzen. Bisherige Ansätze behaupten, mit WSL auf sauberen Testdaten hohe Leistungen zu erzielen, wobei ein oft übersehener Aspekt die Notwendigkeit eines sauberen Validierungssets für die Modellauswahl ist.

Dawei und sein Team untersuchten, ob saubere Validierungsdaten für WSL wirklich zwingend notwendig sind. Ihre Forschungsergebnisse zeigen, dass WSL-Methoden ohne saubere Validierungsdaten stark an Leistung einbüßen. Sie fanden heraus, dass nur 20 saubere Proben pro Klasse oft ausreichen, um hohe Leistungen zu erzielen, und dass direkte Feinabstimmung auf sauberen Daten sogar bessere Ergebnisse liefern kann.

Die Studie kritisiert die übertrieben optimistischen Leistungsansprüche vieler WSL-Ansätze und empfiehlt, saubere Validierungsdaten offenzulegen, WSL gegenüber Few-Shot-Lernmethoden zu vergleichen und kontinuierliche Feinabstimmung als einfache und effektive Alternative zu betrachten. Das Forschungsteam hat seinen Code open-source veröffentlicht.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren ihre Forschung zur Entitätsverfolgung in Sprachmodellen. Sie argumentieren, dass die Fähigkeit, Entitäten und ihren Zustand in einem Dialog zu verfolgen, für das Verständnis längerer Texte entscheidend ist, aber bisher wenig untersucht wurde.

Ihr Ansatz besteht darin, eine Aufgabe zu entwickeln, um die Fähigkeit von Sprachmodellen zu bewerten, den Zustand von Entitäten in einem Kontext zu verfolgen. Sie entwerfen ein Szenario mit Boxen und Objekten, in dem das Modell die Inhalte jeder Box basierend auf einer anfänglichen Beschreibung und nachfolgenden Aktionen vorhersagen muss.

Die Forscher untersuchten verschiedene Modelle, einschließlich Flan-T5 und GPT-3, und stellten fest, dass nur GPT-3.5-Modelle, die auf Code trainiert wurden, eine nicht-triviale Entitätsverfolgung zeigten. Sie schlagen vor, dass die Code-Komponente in der Vorabschulung für diese Fähigkeit entscheidend ist.

Die Experimente ergaben auch, dass kleinere Modelle wie T5-Base mit direkter Feinabstimmung lernen können, Entitätsverfolgung zu betreiben, während zufällig initialisierte Modelle es nicht können, was die Bedeutung der Vorabschulung unterstreicht.

Die Präsentation betont die Herausforderungen bei der Bewertung von Entitätsverfolgungsfähigkeiten und die Notwendigkeit, diese Aufgaben sorgfältig zu gestalten, um Heuristiken und Memorisierung zu vermeiden. Die Ergebnisse werden in einem Artikel auf ArXiv veröffentlicht, und die Autoren sind für Rückmeldungen und Fragen offen.</sample>
    <sample id="307">Die Autoren verwendeten verschiedene Bewertungsmetriken, um die Leistung ihrer Modelle zu messen, darunter:

1. **Genauigkeit (Accuracy)** für Klassifizierungsaufgaben.
2. **F1-Score** für Aufgaben wie Named Entity Recognition (NER) und Fragebeantwortung.
3. **Part-of-Speech (POS) Tagging** Bewertung basierend auf Genauigkeit und Recall.

Zusätzlich wurden die Modelle mit **Basismodellen** wie CamemBERT, PubMedBERT, BioBERT und ClinicalBERT verglichen, um ihre relative Leistung zu bewerten.</sample>
    <sample id="308"># **NLPositionality: Charakterisierung der Positionen in NLP-Datensätzen und -Modellen**

In ihrer Präsentation untersuchen Jenny und ihr Team die **Designbiasse** in NLP-Systemen und -Datensätzen. Sie stellen die These auf, dass **Positionalität**, beeinflusst durch Demografie und Lebenserfahrung, die Leistung und Ausrichtung von Modellen und Datensätzen prägen kann.

Die Forscherinnen und Forscher argumentieren, dass NLP-Modelle und -Datensätze implizit Vorurteile und Perspektiven ihrer Entwickler und Annotatoren übernehmen können. Um dies zu demonstrieren, entwickelten sie das Framework **NLPositionality**, das die Annahmen von Benutzern mit denen von Modellen und Datensätzen vergleicht.

Durch die **Re-Annotation** von Datensätzen mit einer vielfältigen Gruppe von Annotatoren und den Vergleich mit verschiedenen NLP-Modellen wie GPT-4 und Perspective API, konnten sie **systematische Unterschiede** in der Leistung je nach Region und Bildungshintergrund aufdecken. Sie fanden heraus, dass Modelle und Datensätze tendenziell mit englischsprachigen Ländern und Personen mit Hochschulbildung übereinstimmen.

Ein bemerkenswerter Bias betrifft **nicht-binäre Personen**, die in Datensätzen und Modellen weniger vertreten sind als Männer und Frauen. Die Studie betont die Wichtigkeit, diese Positionalität in der NLP-Forschung zu berücksichtigen und empfiehlt transparente Dokumentation, eine perspektivische Forschungspraxis und die Entwicklung von Datensätzen und Modellen, die spezifischen Gemeinschaften gerecht werden.

Die Präsentation unterstreicht die Notwendigkeit, inklusivere NLP-Praktiken zu fördern, um sicherzustellen, dass Technologie für alle zugänglich und fair ist.</sample>
    <sample id="309">Die Metrik, die verwendet wurde, um die Übereinstimmung zwischen den Kommentatoren zu messen, ist die **inter-annotator agreement** (Inter-Bewertungsübereinstimmung). Konkret wurde dies auf 100 doubly-gelabelten Konversationen (Konversationen mit zwei verschiedenen Bewertungen) gemessen.</sample>
    <sample id="310">Die Domain, die gewählt wurde, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen, ist **Wikipedia**.</sample>
    <sample id="311">Basierend auf dem präsentierten Inhalt und den Referenzen in der Präsentation gehören die Autoren an die **Ruhr-Universität Bochum**. Dies wird durch die Erwähnung der Universität in der Präsentation und die Verweise auf die Veröffentlichung in einem wissenschaftlichen Papier impliziert.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks dadurch, dass es **das erste große, öffentlich verfügbare Dataset für Multi-Modal-Anweisungs-Tuning ist**. 

Im Gegensatz zu bestehenden NLP-Benchmarks mit Tausenden von Sprachaufgaben, gibt es bisher keine vergleichbare Ressource für Multi-Modale Aufgaben. MultiInstruct umfasst 62 vielfältige Multi-Modale-Aufgaben in 10 Kategorien, die aus 21 offenen Datensätzen stammen und jeweils mit fünf Experten-Anweisungen versehen sind.</sample>
    <sample id="313">An der Arbeit sind zwei Autoren beteiligt: James Finch und Sarah Finch.</sample>
    <sample id="314">Die binäre Koordination, wie in der Präsentation beschrieben, bezieht sich auf die Struktur, in der zwei Elemente (in diesem Fall Konjunktionen oder Phrasen) gleichberechtigt nebeneinander stehen und von einem gemeinsamen Kopf oder Gouverneur abhängig sind. Im Gegensatz zu asymmetrischen Ansätzen, bei denen ein Element als Hauptteil hervorgehoben wird, werden bei symmetrischen Strukturen alle Konjunktionen als gleichwertig betrachtet.

In der englischen Sprache wird dies oft durch die Positionierung von direkten Objekten und Nebensätzen veranschaulicht. Direkte Objekte neigen dazu, nahe am Verb zu sein, während Nebensätze weiter entfernt sein können. Die Präsentation zeigt, dass diese Regel gebrochen werden kann, wenn das direkte Objekt lang und schwerfällig ist, um die Gesamtlänge der Abhängigkeiten zu minimieren.

Die Definition kann also wie folgt zusammengefasst werden: Binäre Koordination ist eine Struktur, in der zwei Elemente gleichberechtigt koordiniert sind und gemeinsam von einem Gouverneur abhängig sind, was zu symmetrischen Abhängigkeitsbeziehungen führt.</sample>
    <sample id="315">Die Länge der in der Studie verwendeten Prompts wird nicht explizit in Sekunden oder Minuten angegeben. Der Text deutet jedoch darauf hin, dass die Prompts kurz und prägnant sind, da sie spezifische Identitätsmarker enthalten, um Personas zu generieren, wie z.B. "Imagine you are an Asian woman. Describe yourself." Ohne weitere Kontextinformationen lässt sich eine genaue durchschnittliche Dauer nicht bestimmen.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, das auf dem CoScript-Datensatz (der aus großen Sprachmodellen destillierten Datensammlung) feinabgestimmt wurde, in der Lage ist, qualitativ hochwertigere Skripte zu generieren als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere Modelle, wenn sie auf geeigneten Datensätzen trainiert werden, die Leistung größerer Modelle übertreffen können.</sample>
    <sample id="317"># **CodeIE: Verbesserte Few-Shot-Fähigkeiten bei der Informationsextraktion**

Die Arbeit von Peng Li und seinem Team von Fudan University konzentriert sich auf die Verbesserung der Effizienz von Informationsextraktionsmodellen, insbesondere bei wenigen Trainingsbeispielen. Informationsextraktion ist eine grundlegende Aufgabe in der Verarbeitung natürlicher Sprache (NLP), die das Extrahieren strukturierter Daten aus unstrukturierten Texten beinhaltet.

Die Forscher schlagen vor, die traditionelle textbasierte Informationsextraktion durch eine Code-Generierungstechnik zu ersetzen. Sie präsentieren CodeIE, ein System, das Text-zu-Struktur-Codegenerierung nutzt, um die Herausforderungen bei der Ausrichtung von Eingabe- und Ausgabestilen zu überwinden. Durch die Verwendung von Code-Large-Language-Modellen wie Codex können sie Text nahtlos in strukturierte Formate umwandeln.

In ihren Experimenten untersuchten sie verschiedene Modelle, einschließlich T5, GPT-3 und Codex, und verglichen traditionelle Text-Prompts mit Code-Prompts. Die Ergebnisse zeigten, dass CodeIE, insbesondere mit Codex und Code-Prompts, in wenigen Schritten signifikant bessere Ergebnisse bei der Namenserkennung und Beziehungsauswahl lieferte. Die Analyse enthüllte, dass Code-Prompts zu einer besseren Ausrichtung an der Aufgabe, geringeren strukturellen Fehlern und einer höheren Genauigkeit führten.

Die Studie betont die Vorteile der Code-Generierung für Informationsextraktionstasks und bietet einen neuen Ansatz, der die Leistung in wenigen Trainingsbeispielen verbessern könnte.</sample>
    <sample id="318">## DrBERT: Ein robustes prätrainiertes Modell auf Französisch für die Biomedical- und Klinische Domäne

**Einführung**

In dieser Präsentation diskutieren wir zunächst die Bedeutung von Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir unsere Hauptbeiträge aus unserer Arbeit vor: **DrBERT**, das erste biomedicale Modell auf Französisch, basierend auf RoBERTa und trainiert auf **NACHOS**, einer Sammlung medizinischer Web-Crawl-Daten. Wir vergleichen DrBERT mit verschiedenen Modellvarianten und Trainingsmethoden und präsentieren unsere Ergebnisse auf 11 biomedicalen und klinischen Downstream-Aufgaben in Französisch.

**Hintergrund: BERT und seine Varianten**

Seit seiner Einführung im Jahr 2018 hat BERT einen enormen Fortschritt in der Verarbeitung natürlicher Sprache (NLP) dargestellt und im Vergleich zu früheren statischen und kontextuellen Methoden wie Word2vec, fastText oder ähnlichen eine enorme Leistungssteigerung erzielt. Seitdem wurden Varianten dieses Modells in zahlreichen Sprachen angepasst, darunter Französisch mit **CamemBERT**, sowie für spezifische Domänen wie Biomedizin (**PubMedBERT**, **BioBERT**) und Klinische Texte (**ClinicalBERT**), meist jedoch in englischer Sprache. Für Französisch gab es bisher keine offene Quell-Lösung für biomedizinische Modelle.

**Fragen und Herausforderungen**

Unsere Frage lautete daher: Welche Datenquellen eignen sich am besten für eine breite Palette an Anwendungen? Wir vermuten, dass Web-Crawl-Daten eine gute Alternative zu klinischen Daten darstellen könnten.

Um dies zu untersuchen, vergleichen wir **DrBERT** mit **ChuBERT**, einem Modell basierend auf anonymisierten Daten aus der Daten-Warehouse des Universitätsklinikums Nantes. Ein weiterer Aspekt unserer Forschung betrifft die Frage, wie viel Trainingsdaten benötigt man für ein spezialisiertes Modell auf Französisch?

**Datensätze und Modelle**

Um diese Fragen zu beantworten, haben wir vier verschiedene DrBERT-Varianten trainiert und verglichen:

* **DrBERT 7 GB:**  Trainiert auf 7 GB NACHOS
* **DrBERT 4 GB:** Trainiert auf 4 GB NACHOS
* **ChuBERT (Klinisch):** Trainiert auf 4 GB klinischen Notizen
* **ChuBERT (Mix):** Trainiert auf einer Kombination aus 4 GB NACHOS und 4 GB klinischen Notizen

Zusätzlich haben wir drei Modelle mit kontinuierlichem Training untersucht:

* **CamemBERT (NACHOS):**  Trainiert auf 4 GB NACHOS
* **CamemBERT (Klinisch):** Trainiert auf 4 GB klinischen Notizen
* **PubMedBERT (NACHOS):** Trainiert auf 4 GB NACHOS (basierend auf einem englischen biomedicalen Modell)

Insgesamt haben wir **sieben Modelle** für verschiedene Downstream-Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech-Tagging und Fragebeantwortung erstellt.

**Ergebnisse und Schlussfolgerungen**

Die Bewertung unserer Modelle zeigte, dass die Leistung am besten ist, wenn die Trainingsdaten der Art der Downstream-Aufgabe entsprechen. Allerdings scheinen Daten aus heterogenen Quellen flexibler einsetzbar zu sein. Wir konnten außerdem feststellen, dass mehr Daten zu besseren Ergebnissen führen.

Insgesamt erzielten bei den meisten Aufgaben Modelle, die von Grund auf neu trainiert wurden, bessere Ergebnisse als Modelle mit kontinuierlichem Training. Eine Ausnahme bildet das Modell, das auf den CamemBERT-Gewichten und -Tokenisierung mit 4 GB NACHOS trainiert wurde, welches vergleichbare Ergebnisse wie DrBERT 4 GB erreichte, aber mit Stabilitätsproblemen verbunden war.

**Fazit:**

Unser **DrBERT**-System übertraf in neun von elf Downstream-Aufgaben die Leistung des generischen CamemBERT-Modells. Wir stellen fest, dass spezialisiertere Daten besser sind, aber nicht skalierbar.

Alle **NACHOS**-basierten prätrainierten Modelle sind unter der MIT-Lizenz auf **Hugging Face** verfügbar, ebenso wie alle Trainingsskripte auf unserem **GitHub-Repository**.

Vielen Dank für Ihre Aufmerksamkeit. Wir freuen uns auf den Austausch während der Poster-Session in Toronto.</sample>
    <sample id="319">In der Arbeit werden zwei Haupt-Lernstrategien untersucht:

1. **From-Scratch Pre-training**: Dies beinhaltet das Training von Modellen von Grund auf (from scratch) mit einem bestimmten Datensatz, in diesem Fall NACHOS oder klinische Daten aus dem Nantes University Hospital.

2. **Continual Pre-training**: Hierbei werden vorab trainierte Modelle (wie CamemBERT) mit spezifischen Datensätzen (NACHOS oder klinische Notizen) weiter trainiert, um sie für spezifische Aufgaben oder Domänen anzupassen.

Zusätzlich wird die Wirkung verschiedener Datenquellen (NACHOS vs. klinische Notizen) und Datenmengen auf die Modellleistung verglichen.</sample>
    <sample id="320">Basierend auf der Präsentation ist der Faktor der Überanpassung (adaptive Overfitting), der speziell auf die Wiederverwendung von Tests zurückzuführen ist, **nicht signifikant** in diesem Szenario. Die Autoren haben festgestellt, dass die Verbesserungen auf dem CoNLL-2003-Datensatz (dargestellt durch die Steigung der roten Best-Fit-Linie) nicht exponentiell ansteigen, wenn die Verbesserungen auf dem CoNLL++-Datensatz gemessen werden. Dies deutet darauf hin, dass die Überanpassung nicht durch die wiederholte Verwendung desselben Testdatensatzes verursacht wird.</sample>
    <sample id="321">Die Qualität der Textvereinfachung wurde anhand verschiedener Metriken beurteilt, einschließlich:

1. **Lexikalische Vereinfachung**: Analyse der verwendeten Wörter und deren Komplexität.
2. **Strukturvereinfachung**: Überprüfung der Satzstruktur und der logischen Organisation des Textes.
3. **Gesamtniveau der Vereinfachung**: Bewertung der Gesamtverständlichkeit und Lesbarkeit des vereinfachten Textes.

Zusätzlich wurden die verschiedenen Transformationen der Vereinfachung in den Korpora analysiert, wie Reordnungen, Wortzusätze und Umformulierungen, um die Vielfalt und Effektivität der Vereinfachungsmethoden zu verstehen.

Für die automatische Ausrichtung von Sätzen wurde der DEPLAIN-Korpus als Goldstandard verwendet, um die Leistung verschiedener Methoden zu bewerten. Es wurde festgestellt, dass die Methode MASSalign die besten Ergebnisse lieferte.

Bei der automatischen Textvereinfachung wurden zwei Sprachmodelle (long-mBART und base mBART) feinabgestimmt, um die Qualität der generierten vereinfachten Texte zu verbessern. Die erzielten Ergebnisse wurden als neue Benchmark für das Problem der automatischen Textvereinfachung vorgeschlagen.</sample>
    <sample id="322">In seiner Präsentation auf ACL 23 untersucht Enrico die Frage: "Was lernt ein Textklassifikator über Moral?" Er betont die Bedeutung von Moral als Leitfaden für das Unterscheiden von richtig und falsch in der menschlichen Gesellschaft. Während die aktuelle NLP-Community Moral oft auf einer einfachen Skala zwischen moralisch und unmoralisch behandelt, argumentiert Enrico, dass Moral subjektiv ist und durch soziale Theorien wie die Moral-Grundlagen-Theorie besser verstanden werden kann.

Die Moral-Grundlagen-Theorie identifiziert fünf verschiedene moralische Aspekte, die Menschen unterschiedlich gewichten. Enrico und sein Team untersuchten, wie Sprachmodelle diese Feinheiten in Texten erfassen können. Sie verwendeten den Moral-Foundation-Twitter-Korpus, der 35.000 Tweets aus sieben Domänen umfasst, um zu sehen, ob Modelle erkennen, dass Moral in verschiedenen Kontexten unterschiedlich ausgedrückt wird.

Ihre Experimente zeigten, dass Sprachmodelle tatsächlich Unterschiede in der Rhetorik zwischen ähnlichen Domänen wie #AllLivesMatter und #BlackLivesMatter erkennen können. Während ALM Subversion ablehnt, wird sie in BLM als rebellisch und ermutigend dargestellt. Diese Ergebnisse deuten darauf hin, dass Modelle die Nuancen moralischer Ausdrucksweisen erfassen können.

Die Studie warnt jedoch davor, dass die Verwendung eines einzigen Modells für verschiedene Domänen zu Missverständnissen führen kann. Enrico betont die Notwendigkeit, die spezifischen moralischen Ausdrucksweisen in verschiedenen Kontexten zu berücksichtigen, um eine sichere und verantwortungsvolle Verarbeitung von Texten mit moralischen Implikationen zu gewährleisten.</sample>
    <sample id="323">**Zusammenfassung:**

Die Arbeit von Yujie Wang aus der Shanxi University in China präsentiert eine neue Methode namens DHLK (Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning) für die Beantwortung von Fragen mit gesundem Menschenverstand (Commonsense QA). Commonsense QA stellt eine Herausforderung für Maschinen dar, da sie ein tiefes Verständnis der Sprache und des allgemeinen Wissens erfordert.

Bisherige Ansätze kombinieren Sprachmodelle und Wissensgraphen, um relevante Informationen aus Wissensbasen zu extrahieren. Allerdings leiden diese Methoden unter Rauschen durch irrelevante Entitäten und vernachlässigen die Interaktion zwischen Text und Graphen. DHLK adressiert diese Probleme durch:

1. **Erstellung eines optimierten HKG (Heterogeneous Knowledge Graph):** Durch eine zweistufige Prünkung und KRL (Knowledge Representation Learning) wird ein strukturierter Graph aus mehreren Wissensbasen aufgebaut.

2. **Fusionsstrategie:** Entitäten werden mit Textkontexten mithilfe von RoBERTa und Mask Self-Attention verschmolzen. Relevante Entitäten werden dynamisch ausgewählt und weniger relevante entfernt.

3. **Modellierung von Subgraphen:** Anstelle von GNNs (Graph Neural Networks) wird Relation Mask Self-Attention (RMSA) verwendet, um Subgraphen zu modellieren und Entitäten- und Beziehungseembdingings zu optimieren.

Schließlich wird ein MLP (Multi-Layer Perceptron) für die Antwortenvorhersage eingesetzt. Die Methode wird auf Datensätzen wie CommonsenseQA und OpenBookQA getestet und zeigt im Vergleich zu anderen Ansätzen vielversprechende Ergebnisse.</sample>
    <sample id="324">Ja, Sprachmodelle zeigen unterschiedliche politische Vorurteile. Die Forschungsergebnisse deuten darauf hin, dass diese Vorurteile von den Trainingsdaten beeinflusst werden und sich in den Modellen manifestieren. Die Modelle belegen in politischen Tests verschiedene Positionen auf einem Spektrum, wobei einige (wie GPT-4) als eher liberal eingestuft werden, während andere (wie BART-Serie) tendenziell konservativer sind.

Zusätzlich zeigt die Studie, dass:

* Sprachmodelle Vorurteile aufgreifen, die in ihren Trainingsdaten vorhanden sind, einschließlich politischer Polarisierung.
* Die Ausrichtung eines Sprachmodells auf politische Themen hängt stark von den spezifischen Trainingsdaten ab.
* Sprachmodelle mit unterschiedlichen politischen Vorurteilen führen auf Aufgaben wie Hassrede- und Falschinformationenerkennung zu unterschiedlichen Ergebnissen, abhängig von der Zielgruppe oder dem politischen Hintergrund der zu klassifizierenden Inhalte.

Zusammenfassend lässt sich sagen, dass Sprachmodelle tatsächlich unterschiedliche politische Vorurteile aufweisen und diese Vorurteile potenziell zu Ungerechtigkeiten in NLP-Anwendungen führen können.</sample>
    <sample id="325">Hallo! Ich heiße Matthias Lindemann, und heute möchte ich Ihnen einen kurzen Überblick über unsere Arbeit mit dem Titel "Kompositorische Generalisierung ohne Bäume mittels Mehrset-Markierung und latenten Permutationen" geben. Diese Arbeit ist eine gemeinsame Anstrengung mit meinen Betreuern Alexander Koller und Ivan Titov.

Kompositorische Generalisierung bezieht sich auf die Fähigkeit eines Lernens, tiefere Rekursionen und bisher nicht gesehene Kompositionen von Phrasen zu bewältigen, die während des Trainings individuell präsent waren. Im Kontext der semantischen Analyse könnte ein Test für kompositorische Generalisierung wie folgt aussehen:

Wie gewohnt verfügen wir über einen Trainingsdatensatz mit Äußerungen. Nehmen wir zum Beispiel "Das Mädchen schlief." und "Maria wusste, dass das Mädchen schlief.". Diese Äußerungen sind mit logischen Formen gepaart, die die wesentlichen Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur Standard-Maschinellem-Lernen-Auswertung stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell unbekannte logische Formen. Im Beispiel wird das Modell auf eine tiefere Rekursion getestet, die es während des Trainings nicht gesehen hat. Naive Sequenz-zu-Sequenz-Modelle (seq2seq) kämpfen bei dieser Art von Aus-der-Verteilung-Generalisierung und erzeugen oft Ausgaben, die vom Eingabestatus entkoppelt sind. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in diesem Beispiel farbcodiert sind.

Ein beliebter Ansatz, um dies zu beheben, besteht darin, Bäume in die Modelle zu integrieren. Diese Bäume sollen den kompositorischen Prozess darstellen, der Äußerungen mit logischen Formen verbindet. Dieses Vorgehen funktioniert gut, erfordert aber oft die Extraktion von Bäumen, was komplex und rechenintensiv sein kann. Typischerweise beinhaltet dies eine erhebliche Vorverarbeitung der logischen Formen, um variable Symbole zu behandeln. Die Extraktion von Bäumen kann auch spezielle Grammatikinduktionsverfahren erfordern.

In dieser Arbeit verwenden wir keine Bäume und stellen ein neuronales seq2seq-Modell vor, das direkt die Korrespondenzen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe modelliert. Wir demonstrieren zum ersten Mal eine starke Generalisierung zu tieferer Rekursion ohne die Verwendung von Bäumen. Unser Ansatz prognostiziert die Ausgabe basierend auf der Eingabe in zwei Schritten. Zuerst markieren wir jedes Eingabetoken mit einem ungeordneten Mehrset der Token, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt verfügen wir über alle richtigen Token, aber sie sind nicht geordnet. Daher verwenden wir im zweiten Schritt ein weiteres Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen.

Wir führen eine neue Methode zur Permutationsvorhersage ein, die keine harten Einschränkungen für die möglichen Permutationen vornimmt. Dies macht unseren Ansatz flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr wie folgt: Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Mehrset-Token an jeder Position platziert werden soll. Für die erste Ausgabeposition wählen wir einfach eines aus, wie hier rot markiert. Dann springen wir zum nächsten Mehrset-Token, um die zweite Token in der Ausgabe zu bestimmen. Wir bestimmen die dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Mehrset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde.

Um Ihnen einen Vorgeschmack auf unsere experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treellosen Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen deutlich bei der Generalisierung zu tieferer Rekursion. Dennoch bleiben einige andere Arten struktureller Generalisierung sehr herausfordernd.

In unserer Arbeit lösen wir einige interessante technische Herausforderungen. Erstens ist die Zuordnung zwischen Eingabe und Ausgabe nicht im Trainingsdatensatz gegeben. Daher wissen wir für ein bestimmtes Token nicht, aus welchem Mehrset es stammt, was eine Herausforderung für das Training darstellt. Außerdem gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir beheben dies, indem wir die Zuordnung als Teil des Trainings induzieren.

Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass die Suche nach der besten Permutation NP-hart ist. Dies hängt mit dem "Reisenden-Handelskompass"-Problem zusammen. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Entspannung, die auch das Backpropagieren durch die Lösung ermöglicht und so linguistisch plausiblere Permutationen lernt.

Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie sich unseren Artikel an oder kommen Sie an unserem Poster vorbei.</sample>
    <sample id="326">Kognitive Dissonanz beschreibt zwei oder mehr widersprüchliche Überzeugungen oder Handlungen einer Person. Ein Beispiel wäre jemand, der sagt, dass Rauchen schädlich ist, aber dennoch raucht. Diese widersprüchlichen Aussagen und Handlungen erzeugen ein Gefühl der Unruhe oder Unvereinbarkeit, was als kognitive Dissonanz bezeichnet wird.</sample>
    <sample id="327">**Abstract**

Unsere Arbeit, "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning", zielt darauf ab, die Leistung von Vision-Language-Modellen zu verbessern. Wir schlagen eine neue Architektur namens ManagerTower vor, die auf der BridgeTower-Konzeption aufbaut. Im Gegensatz zu BridgeTower nutzt ManagerTower die semantischen Kenntnisse aus verschiedenen Schichten der unimodalen Encoder effektiver.

ManagerTower integriert "Manager" in jeder Schicht der cross-modalen Architektur, die adaptive Agregationen von Repräsentationen aus mehreren unimodalen Quellen ermöglichen. Wir verwenden RoBERTa und CLIP-ViT als Encoder und zeigen, dass ManagerTower mit nur vier Millionen Trainingsbildern herausragende Leistungen auf verschiedenen Aufgaben erzielt.

Unsere Experimente belegen, dass ManagerTower im Vergleich zu BridgeTower und anderen Basismodellen signifikant bessere Genauigkeiten erreicht, insbesondere auf dem Wikivideo-Teststandard (39,15%). Durch die Visualisierung der Aggregationsgewichte der Manager zeigen wir, dass adaptive Manager die semantischen Kenntnisse aus verschiedenen Schichten selektiv und anpassungsfähig nutzen, was zu einer umfassenderen cross-modalen Vertiefung führt.

Der Code und das Modell sind öffentlich zugänglich, und wir hoffen, dass unsere Arbeit einen wertvollen Beitrag zur Entwicklung effizienter Vision-Language-Systeme leistet.</sample>
    <sample id="328">Basierend auf den vorgestellten Ergebnissen ist **GPT-4** das Sprachmodell, das am meisten links steht. Es belegte den liberalsten Platz in der politischen Skala der untersuchten Modelle.</sample>
    <sample id="329">**Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization**

Dieses Forschungsprojekt, durchgeführt an der Peking Universität in Zusammenarbeit mit Shaogang Gong, Hailin Jin, Yuxin Peng und Yang Liu, zielt darauf ab, die Herausforderungen bei der Trainierung von Modellen für die zero-shot Video-Satz-Lokalisierung zu überwinden.

Die Aufgabe besteht darin, in langen Videos die relevantesten Segmente zu finden, die einer gegebenen natürlichen Sprachabfrage entsprechen. Traditionelle Methoden erfordern umfangreiche manuelle Annotationen, was zeitaufwändig und teuer ist.

Die Forscher schlagen eine **noise-resistente Methode zur Generierung strukturierter Pseudo-Labels** vor. Diese Methode umfasst die folgenden Schritte:

1. **Erstellung komplexer Pseudo-Abfragen:** Mit einem vorab trainierten Bild-Text-Modell (z.B. BLIP) werden aus Video-Frames komplexe, freie Formulierungen generiert.
2. **Generierung von Pseudo-Ereignissen:** Ein weiteres vorab trainiertes Modell misst die Relevanz zwischen einzelnen Frames und den Pseudo-Abfragen, um Pseudo-Ereignisse zu generieren, die eine hohe Relevanz innerhalb der Ereignisse und eine niedrige Relevanz außerhalb gewährleisten.
3. **Reduzierung von Label-Noise:**  Durch Gewichtung von Proben und die Erstellung von "noisy labels" wird der Einfluss von fehlerhaften Labels minimiert.

Die Ergebnisse zeigen, dass die vorgeschlagene Methode in Vergleich zu anderen zero-shot Ansätzen auf zwei Datensätzen (ActivityNet Captions und Charades-STA) bessere Ergebnisse erzielt.

Die Präsentation schließt mit der Verfügbarkeit des Codes über einen QR-Code.</sample>
    <sample id="330">Basierend auf der Präsentation scheint das kumulativen Training (**"Cumulative"**) in den meisten Fällen besser oder zumindest gleichwertig zum iterativen Training (**"Iterative"**) für aktives Lernen zu sein. Die Studie fand heraus, dass kumulatives Training, das alle bisher gesammelten Daten berücksichtigt, in der Regel gleichwertig oder besser abschneidet als iteratives Training, bei dem das Modell nur mit den neuesten Daten aktualisiert wird.</sample>
    <sample id="331">Die Referentin heißt Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus einem parallelen Korpus, der für die Evaluierung verwendet wird. Dieser Korpus wurde spezifisch ausgewählt, um verschiedene Discourse-Phänomene zu enthalten, die für die Bewertung von Kontextabhängigkeiten bei der maschinellen Übersetzung relevant sind.</sample>
    <sample id="333">In ihrer Arbeit "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation" untersuchen Forscher aus Nanjing University, Shanghai AI Lab, Nanjing University und der University of Hong Kong die Herausforderungen bei der Generalisierung und Leistung von neuronalen Maschinendüsenübersetzungen (NMT).

Sie identifizieren ein Problem in der Darstellung des NMT-Modells: niedrigfrequente Token sind sparsam verteilt, was zu "Löchern" mit schlecht definierter Semantik führt. Um dies zu beheben, schlagen sie INK (Inject kNN Knowledge) vor, ein Framework, das kNN-MT (k-Nearest Neighbor Machine Translation) verbessert.

INK besteht aus zwei Schritten: Erstens extrahiert es kNN-Wissen aus einem Datenspeicher, um einen Adapter zu trainieren, der die Darstellung anpasst. Zweitens aktualisiert es die Darstellungen asynchron und erneuert den Datenspeicher.

Im Experiment wird INK mit dem Gewinnermodell des WMT'19 German-English News-Translation-Tasks getestet. Die Ergebnisse zeigen, dass INK die Leistung des Modells erheblich verbessert, insbesondere bei der Glättung der Darstellung und der Reduzierung des Speicherbedarfs während der Inferenz.

INK übertrifft den aktuellen Stand der Technik und erzielt eine durchschnittliche Verbesserung von 1.99 COMET-Score und 1.0 BLEU-Score. Die Forscher betonen die Effektivität von INK bei der Optimierung der NMT-Modelle und ihrer schnellen Inferenzgeschwindigkeit.</sample>
    <sample id="335">Der/die Referent*in heißt Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer (Cross-lingual transfer) bezieht sich in diesem Kontext auf die Fähigkeit eines Modells, Wissen oder Fähigkeiten, die es in einer Sprache erworben hat, auf eine andere Sprache zu übertragen. Dies geschieht, indem ein Modell, das in einer Sprache trainiert wurde, auf eine ähnliche Aufgabe in einer anderen Sprache angewendet wird, ohne oder mit minimaler zusätzlicher Schulung in dieser neuen Sprache.</sample>
    <sample id="337">Die Präsentation stellt eine neue Methode zur Lösung des Problems vor, wie man Aus-des-Wortschatzes (OOV) Wörter in Wort-Einbettungsmodellen effektiv verarbeitet. OOV-Wörter sind für die Leistung von Modellen, die auf Einbettungen basieren, kritisch, aber schwer darzustellen.

Die Forscher entwickeln einen Ansatz, der von menschlichen Lerngewohnheiten inspiriert ist. Sie führen ein **Word Relationship Graph (WRG)** ein, der Wortbildung und Assoziationen imitiert. OOV-Wörter werden in Wortstücke zerlegt und mit relevanten Wörtern verknüpft, was zu einem zweistufigen Graphen führt.

Der WRG verwendet ein Graph-Neural-Netzwerk (GNN), um die Beziehungen zwischen den Wörtern zu erfassen. Ein Self-Attention-Netzwerk hilft dabei, Attributen für OOV-Knoten zu bestimmen. Zwei Graph Attention Network-Ebenen extrahieren wichtige Informationen und reduzieren Rauschen.

Ein **Readout-Block** fasst die gesamte Grapheninformation zusammen. Ein Graph Convolutional Network (GCN) analysiert die Wortbildung. Der Verlustfunktion wird Kontrastives Lernen hinzugefügt, um Einbettungen zu optimieren.

Die Experimente zeigen, dass das Modell OOV-Wörter effizient lernt und in verschiedenen Downstream-Aufgaben überlegen ist. Die Methode kann sowohl für statische als auch für kontextbezogene Modelle angepasst werden.

Zukünftige Arbeiten konzentrieren sich auf die Erweiterung des Modells auf andere Sprachen, wobei die Herausforderung bei fusionalen Sprachen liegt. Der Erfolg hängt von der Effektivität der Wortzerlegung ab.</sample>
    <sample id="338">**Zusammenfassung:**

In der Präsentation wird ein Forschungsprojekt mit dem Titel "Sind menschliche Erklärungen immer hilfreich? Objektive Bewertung menschlicher natürlicher Spracherklärungen" vorgestellt. Das Projekt ist eine Zusammenarbeit zwischen Forschern von Rensselaer Polytechnic Institute, Northeastern University und IBM Research.

Die Studie zielt darauf ab, die Qualität menschlicher Erklärungen in verschiedenen Aufgaben zu bewerten, da diese oft zur Schulung von Modellen verwendet werden, um verständliche Erklärungen zu generieren. Bisherige Metriken behandeln menschliche Annotationen als Goldstandard, was bei subjektiven und aufgabenabhängigen Erklärungen problematisch ist.

Die Forscher schlagen einen einheitlichen Datenformatvorlage vor, der verschiedene Aufgaben in ein einheitliches mehrstufiges Format umwandelt. Sie untersuchten die Nützlichkeit von Erklärungen in fünf großen Datensätzen für verschiedene Aufgaben, einschließlich Commonsense QA, Natural Language Inference und Commonsense Validierung.

Die Experimente zeigten, dass die Feinabstimmung mit Erklärungen nicht unbedingt neues Wissen vermittelt und dass die Nützlichkeit von Erklärungen stark von der Aufgabe und dem Format abhängt. Als Ergebnis wurde eine neue Metrik namens TREU eingeführt, die die Nützlichkeit von Erklärungen während der Feinabstimmung bewertet und bessere Ergebnisse liefert als die etablierte Metrik simulatability.

Die Studie betont die Wichtigkeit sorgfältiger Qualitätskontrollen bei der Zusammenarbeit mit Menschen in der Annotation und stellt einen wertvollen Beitrag zur Bewertung und Verbesserung der Qualität menschlicher Erklärungen dar.</sample>
    <sample id="339">Die Autoren gehören der Saarland University in Deutschland an.</sample>
    <sample id="340">**Zusammenfassung: ParaAMR: Ein syntaktisch vielfältiges Paraphrase-Dataset für NLP-Anwendungen**

In ihrer Arbeit präsentieren Kuan-Hao Huang und ihr Team ein neues Dataset namens ParaAMR, das für die Aufgabe der Paraphrasegenerierung in der NLP-Forschung entwickelt wurde. Das Ziel war es, ein groß angelegtes Dataset mit syntaktischer Vielfalt zu schaffen, das die bestehenden Einschränkungen menschlich annotierter Datensätze überwindet.

Der Ansatz besteht darin, Abstract Meaning Representations (AMR) Graphen zu nutzen, die die semantische Struktur von Sätzen erfassen. Durch die Modifikation dieser Graphen, insbesondere durch das Ändern des Fokusknotens und der zugehörigen Kanten, wird eine Vielzahl syntaktisch unterschiedlicher, aber semantisch ähnlicher Sätze generiert. Dieser Prozess, genannt AMR Back-Translation, erzeugt ein Dataset mit 15 Millionen Sätzen und etwa 6,9 Paraphrase-Beispielen pro Satz.

Die Forscher untersuchten die Qualität von ParaAMR im Vergleich zu anderen automatisch generierten Datensätzen. Die Ergebnisse zeigen, dass ParaAMR syntaktisch vielfältigere Paraphrase-Beispiele liefert. Quantitative Analysen, einschließlich automatischer und menschlicher Bewertungen, bestätigen die hohe Qualität und Vielfalt des Datasets.

Die Vorteile von ParaAMR werden in verschiedenen NLP-Anwendungen demonstriert. Es verbessert die Leistung bei der Lernung von Satzembeddings und ermöglicht eine bessere syntaktische Kontrolle bei der Paraphrasegenerierung. Darüber hinaus ist es für Datenverstärkungstechniken in Few-Shot-Lernaufgaben nützlich.

Zusammenfassend bietet ParaAMR eine wertvolle Ressource für die NLP-Forschung, die die Entwicklung leistungsfähigerer und vielfältigerer Paraphrase-Generatoren ermöglicht. Das Dataset ist öffentlich zugänglich und wird die Weiterentwicklung von Anwendungen in den Bereichen Fragebeantwortung, Chatbots und Robustheit fördern.</sample>
    <sample id="341">Die Autoren verwenden zwei Arten von Latenzmessungen:

1. **Durchschnittliche Verzögerung (Average Lagging)**: Eine direkte Messung der Zeitspanne zwischen der Eintreffung des Audios und der Ausgabe der Übersetzung.
2. **Computational Aware Average Lagging**: Diese Messung berücksichtigt sowohl die tatsächliche verstrichene Zeit als auch die Rechenzeit des Modells zur Vorhersage der Ausgabe.</sample>
    <sample id="342"># LiveChat: Ein großes, personalisiertes Dialog-Dataset aus Live-Streaming

## Einführung

Open-Domain-Dialoge sind konversative Austausche zwischen Mensch und künstlicher Intelligenz ohne spezifisches Ziel, die ein breites Themenspektrum abdecken. Während bestehende große Datensätze hauptsächlich aus Textquellen (Online-Chats) bestehen, ist die Erstellung von Video-basierten Dialog-Datensätzen für realistischere Sprachmodelle von entscheidender Bedeutung.

Unsere Forschung zielt darauf ab, diese Lücke zu schließen, indem wir *LiveChat* vorstellen, ein großes, personalisiertes Dialog-Dataset, das automatisch aus Live-Streaming-Videos konstruiert wurde. Es überwindet Herausforderungen bei der Skalierung und Personalisierung, die bei manuell annotierten Datensätzen auftreten.

## LiveChat-Datensatzerstellung

LiveChat wurde in drei Schritten erstellt:

1. **Datenerfassung:** Ursprüngliche Streaming-Videos wurden aus chinesischen Plattformen wie TikTok und Douyin extrahiert.
2. **Audio-Transkription:** Audio wurde aus Videos extrahiert und mithilfe von ASR in Text umgewandelt.
3. **Dialoggenerierung:** Kommentare der Zuschauer wurden gesammelt und mit einer innovativen "Antwort-zu-Wer-Methode" zu Dialogen verarbeitet.

## Experimentelle Ergebnisse

Wir führten Experimente auf zwei Benchmark-Aufgaben durch:

- **Antwortmodellierung:** Zeigte, dass Personaprofile und längere Durchschnitts-Sitzungen die Leistung verbessern.
- **Adressatenerkennung:** Doppelte BERT-Ströme waren weniger effektiv als einzelne.

Außerdem untersuchten wir die Leistung vorab trainierter Dialogmodelle (BART, LLMs) auf LiveChat. Die Ergebnisse zeigten, dass LiveChat ein einzigartiges und herausforderndes Dataset darstellt, das zu besseren Sprachmodellen führt.

Zukünftige Arbeiten konzentrieren sich auf effiziente Transfer-Lernmethoden für LiveChat.</sample>
    <sample id="343">**Präsentation: "The KITMUS Test: Evaluierung der Wissensintegration aus mehreren Quellen"**

Hallo zusammen, ich bin Akshatha und heute präsentiere ich zusammen mit meinem Co-Autor Martin unsere Arbeit mit dem Titel "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Diese Forschung ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research.

Modelle für das Verständnis natürlicher Sprache (Natural Language Understanding, NLU) greifen auf verschiedene Wissensquellen zurück. Dazu gehören das Wissen, das in ihren Parametern während der Vorabtrainingsphase enthalten ist, sowie das Wissen, das während der Inferenzphase in den Eingaben bereitgestellt wird. Aktuelle Arbeiten in Aufgaben wie Frage-Antwort-Systemen zeigen, dass Modelle in der Lage sind, Wissen aus der Vorabtrainingsphase für die Lösung von Aufgaben zu nutzen. Allerdings erfordern komplexe NLU-Aufgaben oft Wissen, das auch während der Inferenzphase bereitgestellt wird. Nehmen wir zum Beispiel den Satz: "John sah den neu gewählten Präsidenten im Fernsehen." Die Parameter des vorab trainierten Modells könnten Informationen über Präsidenten und Fernseher enthalten, aber sie können nicht zuverlässig bestimmen, wer diese spezifischen Entitäten "John" oder "der neu gewählte Präsident" sind, da sich diese Personen seit der Vorabtrainingsphase geändert haben könnten.

Daher benötigen erfolgreiche Modelle für anspruchsvolle NLU-Aufgaben die Fähigkeit, sowohl Wissen aus der Vorabtrainingsphase als auch Wissen aus der Inferenzphase zu integrieren. In dieser Arbeit stellen wir ein Diagnosetest-Paket für die Wissensintegration vor. Wir führen eine Coreferenz-Aufgabe ein, die darauf abzielt, die Fähigkeit zu untersuchen, Wissen aus verschiedenen Quellen zu nutzen.

**Das KITMUS-Datenset**

Unser Datenset besteht aus Beispielen wie diesem: "Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich im Park. Nach einem langen Arbeitstag im Gericht, freute er sich, sich zu entspannen." Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die das Pronomen "er" verweist, in diesem Fall "Servin". Die Auflösung einer Coreferenz erfordert zwei Arten von Informationen:

- **Entitäts-spezifisches Wissen:** Zum Beispiel "Servin ist Richter."
- **Hintergrundwissen:** Zum Beispiel "Richter entscheiden über Fälle in Gerichten."

Im Allgemeinen wird Hintergrundwissen während der Vorabtrainingsphase großer Sprachmodelle gelernt, während entitätsbezogenes Wissen typischerweise während der Inferenzphase beobachtet wird.

Wir haben drei KITMUS-Einstellungen (KITMUS = Knowledge Integration from Multiple Sources) definiert:

1. **Background-Pretrain:** Hier wird angenommen, dass Hintergrundwissen während der Vorabtrainingsphase verfügbar ist.
2. **Background-Both:** In dieser Einstellung ist sowohl das Hintergrundwissen als auch das entitätsbezogene Wissen während der Vorabtrainingsphase und der Inferenzphase verfügbar.
3. **Background-Inference:** Hier ist nur das entitätsbezogene Wissen während der Inferenzphase verfügbar, während das Hintergrundwissen nicht Teil der Vorabtrainingsdaten der Modelle ist.

**Experimentelle Ergebnisse**

Wir haben das Datenset sowohl mit menschlichen Teilnehmern als auch mit etablierten Coreferenz-Modellen bewertet. In der folgenden Grafik sehen Sie die Ergebnisse der besten Modelle im schwierigsten Varianten der "Background-Pretrain"-Einstellung. Ohne spezifisches Training auf KITMUS erreichen beide Modelle keine guten Ergebnisse. Nach dem Training auf KITMUS verbessern sich jedoch sowohl C2F als auch BERT4Coref deutlich im Vergleich zur zufälligen Wahl. Dies deutet darauf hin, dass die meisten Modelle ohne spezifisches Training dazu neigen, sich auf oberflächliche Hinweise zu verlassen, die auf KITMUS nicht hilfreich sind.

Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die besten Modelle Schwierigkeiten haben, rückwärts gerichtetes Wissen zu integrieren, das nur während der Inferenzphase bereitgestellt wird.

**Zusammenfassung**

Viele Coreferenz-Modelle scheinen Schwierigkeiten zu haben, Wissen aus verschiedenen Quellen zu nutzen, ohne spezifisches Training. Mit dem richtigen Training gelingt es jedoch einigen Modellen, Wissen aus mehreren Quellen zu integrieren. Trotzdem scheinen selbst die leistungsstärksten Modelle Probleme mit der zuverlässigen Integration von Wissen, das nur während der Inferenzphase bereitgestellt wird, zu haben.

Für weitere Details laden wir Sie ein, unseren Artikel zu lesen und das Datenset und den Code auf GitHub zu erkunden. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">Die Nachteile baumbasierter Methoden, wie sie in der Einführung beschrieben werden, sind:

1. **Komplexität und Kosten**: Die Erstellung von Bäumen kann komplex und rechenintensiv sein, insbesondere bei der Verarbeitung formaler Symbole und der Anwendung spezialisierter Grammatikinduktionsverfahren.
2. **Abhängigkeit von der Verfügbarkeit von Bäumen**: Diese Methoden sind an die Vorhandensein und Korrektheit von Bäumen gebunden, was zusätzliche Schritte und potenzielle Fehlerquellen einführt.
3. **Begrenzte Flexibilität**: Bäume bieten oft harte Einschränkungen für die Modellierung von Korrespondenzen zwischen Eingabe und Ausgabe, was die Flexibilität und Ausdrucksstärke der Modelle einschränken kann.</sample>
    <sample id="345">**Zusammenfassung: Kompositionelle Generalisierung ohne Bäume**

Die Arbeit von Matthias Lindemann und seinen Kollegen Alexander Koller und Ivan Titov konzentriert sich auf die Verbesserung der Fähigkeit von Sprachmodellen, komplexe Sätze und logische Formen zu verstehen und zu generieren, ein Konzept als *kompositionelle Generalisierung* bezeichnet.

Traditionelle Ansätze verwenden Sequenz-zu-Sequenz (seq2seq) Modelle, die oft Schwierigkeiten haben, sich an neue, strukturell komplexere Sätze anzupassen. Um dies zu lösen, werden häufig Bäume in die Modelle integriert, um die rekursive Natur der Sprache zu erfassen. Allerdings ist die Erstellung dieser Bäume rechenintensiv und erfordert oft eine aufwendige Vorverarbeitung.

Die Forscher stellen ein neuartiges seq2seq-Modell vor, das Bäume umgeht und direkt die Korrespondenzen zwischen Eingabe- und Ausgabefragmente modelliert. Der Ansatz besteht aus zwei Schritten: Erstens wird jeder Eingabetoken mit einem Multiset von Ausgabetokens markiert, und zweitens wird ein Permutationsmodell eingesetzt, um die Tokens in der richtigen Reihenfolge anzuordnen.

Ein Hauptbeitrag ist die Entwicklung eines Permutationsmodells, das flexibel und ausdrucksstark ist. Es lernt, die richtigen Token-Reihenfolgen zu bestimmen, indem es von links nach rechts über die Ausgabe navigiert und Multisets verwendet, um die Korrespondenzen zu erfassen. Dieser Ansatz übertrifft andere treellose Modelle in Experimenten auf dem COGS-Benchmark bei der Generalisierung tiefer Rekursionen.

Die Studie behandelt auch Herausforderungen wie die fehlende Eingabe-Ausgabe-Ausrichtung und die Bestimmung der optimalen Permutationen. Sie lösen diese Probleme durch die Induktion von Ausrichtungen während des Trainings und durch die Verwendung einer kontinuierlichen Entspannungsmethode für die Permutationsvorhersage.</sample>
    <sample id="346">Basierend auf dem präsentierten Inhalt und ohne spezifische Nennung, können wir schließen, dass die Autoren mit einer Universität assoziiert sind, die das Papier und die Forschung im Bereich der Sprachverarbeitung und maschinellen Lernens unterstützt, insbesondere im Kontext von Named Entity Recognition (NER) und Transformer-Modellen. Ohne konkrete Nennung könnte dies eine führende Universität in diesen Bereichen sein, wie z.B. eine renommierte Institution in den USA, Europa oder Asien, die forschungsstark in der Informatik ist.</sample>
    <sample id="347">Hallo, ich bin Myra und heute spreche ich über unsere Arbeit mit dem Titel „Marked Personas: Verwendung von natürlichen Sprachaufforderungen, um Stereotype in Sprachmodellen zu messen“. Diese Studie wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele Forscher die weit verbreiteten sozialen Vorurteile und Stereotype in großen Sprachmodellen (LLMs) dokumentiert. Diese Methoden haben jedoch erhebliche Einschränkungen. Sie basieren oft auf handkuratierten Datensätzen, die sehr zeitaufwändig zu erstellen sind, und sie messen meist nur sehr spezifische Stereotype, die nicht gut auf andere Demografien oder Kontexte übertragbar sind, oder sie erfassen lediglich allgemeine, breite Assoziationen wie negative Verbindungen zu bestimmten Gruppen. Außerdem berücksichtigt die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, also die Vorstellung, dass mehrschichtige soziale Identitäten Vorurteile verstärken und einzigartige Schadensorte darstellen können.

Um diese Probleme zu überwinden, nutzen wir die Tatsache, dass neuere, auf Anweisungen trainierte Sprachmodelle sehr gut auf Anweisungen und Aufforderungen reagieren. Wir können das Modell also bitten, eine Persona zu generieren, zum Beispiel: „Stellen Sie sich eine asiatische Frau vor. Beschreiben Sie sich selbst.“ Sofort sehen wir, dass diese Ausgabe sehr generalisierbar ist, da wir einfach jede gewünschte Identitätsmarke in die Aufforderung einfügen können. Hier sind einige Beispiele von GPT-4: Während die Ausgaben nicht offensichtlich negativ oder toxisch sind, zeigen sie interessante Muster. Die asiatische Frau wird als unauffällig dargestellt, während die Frau aus dem Nahen Osten als „exotisch“ und „verzaubernd“ beschrieben wird, was auf ein faszinierendes Region hinweist. Beide Personas von Menschen mit Farbe erwähnen ihre Abstammung, während die Persona des weißen Mannes nichts dergleichen enthält.

Unser Ansatz besteht aus zwei Teilen. Erstens generieren wir diese Personas. Unsere Aufforderungen zur Generierung basieren auf einer Studie, in der diese Aufforderungen an menschliche Probanden gegeben wurden, was auch stereotype menschliche Antworten aufdeckte. Dies ermöglicht einen direkten Vergleich zwischen den von uns generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil ist die „Marked Words“-Methode, mit der wir Wörter identifizieren, die Gruppen, die als markiert gelten, von unmarkierten Gruppen unterscheiden. Der Vorteil dieser Methode ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne auf ein spezifisches Vokabular angewiesen zu sein.

Die „Marked Words“-Methode stützt sich auf das sociolinguistische Konzept der „Markierung“, das besagt, dass es eine unmarkierte Standardgruppe gibt und jede Gruppe, die sich von dieser abhebt, linguistisch markiert ist. Nehmen wir zum Beispiel das Wort „Kriegerin“. Normalerweise ist es mit Männern assoziiert. Wenn Menschen eine Kriegerin beschreiben, die eine Frau ist, fügen sie oft das Wort „Frau“ hinzu und markieren es so. Im Allgemeinen sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen oft markiert sind. In unserer Methode bestimmen wir zunächst die unmarkierten und markierten Gruppen und vergleichen dann die Personas mit der „Fightin’ Words“-Methode, die auf gewichteten Log-Wahrscheinlichkeiten basiert, um die Top-Wörter für jede markierte Gruppe zu ermitteln.

Zu den Ergebnissen: Wir verwendeten zunächst ein Stereotypen-Lexikon und fanden heraus, dass die generierten Personas mehr Stereotype enthalten als die von Menschen geschriebenen. Beim genauen Hinsehen auf die Verteilung der Wörter und des Lexikons zeigten sich jedoch unterschiedliche Muster. Während die generierten Personas einen höheren Anteil an den Lexicon-Wörtern aufweisen, haben die von Menschen geschriebenen Texte eine viel breitere Wortverteilung, und die Stereotypen-Wörter in den generierten Personas beschränken sich auf „groß“ und „athletisch“, also eher positive oder zumindest nicht-negative Begriffe. Dieses Lexicon erfasst die schädlichen Muster, die wir zuvor beobachtet haben, gar nicht gut.

Um diese schädlichen Muster aufzudecken, wenden wir uns an die Ergebnisse unserer „Marked Words“-Methode. Unsere Analyse zeigt, wie diese scheinbar positiven Beschreibungen tatsächlich schädliche Narrative und Stereotypen widerspiegeln. Die Top-Wörter für jede Gruppe umfassen Begriffe wie „Kultur“, „Tradition“, „stolz“ und „exotisch“, die diese Gruppen nur durch ihre Beziehung zu ihrer Identität definieren und sie von der weißen Norm abgrenzen. Dies trägt zu einer langen Geschichte von Diskriminierung und „Anderen“-Machung bei. Außerdem werden in diesen Wörtern, besonders bei Frauen mit Farbe, häufige Tropen wiedergegeben. Bei lateinamerikanischen Frauen sind Begriffe wie „lebhaft“ und „kurvig“ mit einem Tropus der „Tropik“ verbunden. Bei asiatischen Frauen sind Begriffe wie „klein“, „zart“ und „seidenweich“ mit einer langen Geschichte der Sexualisierung und Passivität von asiatischen Frauen verbunden. Und für schwarze Frauen sind Begriffe wie „stark“ und „resilient“ mit dem „Starken schwarzen Frauen“-Stereotyp verbunden, das, wie andere Studien gezeigt haben, tatsächlich schädlich ist, da es diesen Gruppen unnötigen Druck auferlegt, Hindernisse zu überwinden, anstatt sie zu beseitigen, was zu negativen gesundheitlichen Folgen und anderen Schäden führt.

Insgesamt zeigen die Wörter für jede Gruppe, dass sie essenzielle Narrative wiedergeben. Basierend auf diesen Mustern ziehen wir drei Empfehlungen für Modellbesitzer: Erstens sollten Forscher positive Stereotype und essenzielle Narrative angehen. Zweitens sollten wir eine intersektionale Linse bei der Untersuchung von Vorurteilen und Schaden anwenden, da viele Aspekte übersehen werden könnten, wenn wir das nicht tun. Und drittens sollte es mehr Transparenz bei der Offenlegung von Methoden zur Bias-Minderung geben, da positive Stereotype oder andere unerwünschte Effekte dieser Methoden möglicherweise zu diesen schädlichen Mustern beitragen. Ohne weitere Untersuchungen können wir diese Annahmen nicht treffen. Vielen Dank für Ihre Aufmerksamkeit. Ich wünsche Ihnen eine angenehme ACL-Konferenz.</sample>
    <sample id="348">In ihrer Arbeit "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" untersuchen Myra, Esin Durmus und Dan Jurafsky, wie große Sprachmodelle (LLMs) Stereotype und soziale Vorurteile reproduzieren.

Sie kritisieren herkömmliche Messmethoden, die auf handkuratierten Datensätzen basieren, als zeitaufwendig und spezifisch. Stattdessen nutzen sie die Fähigkeit moderner, anweisungsbasierter LLMs, Personas zu generieren. Durch Prompts wie "Stelle dir eine asiatische Frau vor. Beschreibe sie." können sie Stereotype in den Antworten der Modelle sichtbar machen.

Ihre Analyse zeigt, dass die generierten Personas zwar keine offensichtlichen negativen Assoziationen enthalten, aber subtile, schädliche Muster aufweisen. Mit der "Marked Words"-Methode identifizieren sie Wörter, die bestimmte Gruppen markieren und von einer unmarkierten (meist weißen) Norm abweichen.

Die Forschung enthüllt, wie scheinbar positive Beschreibungen (z.B. "stark" für schwarze Frauen) tatsächlich essenzielle Narrative und schädliche Stereotypen verstärken. Sie plädieren für mehr Transparenz bei Bias-Milderungsmaßnahmen und eine intersektionale Analyse, um soziale Vorurteile in LLMs umfassend zu verstehen und zu bekämpfen.

Zusammenfassend zeigen die Autoren, dass LLMs Stereotype internalisieren und diese durch natürliche Sprachanweisungen aktivieren können, und fordern zu mehr Verantwortung und Forschung in diesem Bereich auf.</sample>
    <sample id="349">## Schutz von Embeddings als Dienstleistungen: Ein neuer Ansatz mit "Embedding Marker"

**Einführung:**

Große Sprachmodelle wie GPT, LLAMA und PALM haben in der Verarbeitung natürlicher Sprache (NLP) bemerkenswerte Leistungen erbracht. "Embedding als Dienstleistung" baut auf diesen Modellen auf und bietet Unterstützung für verschiedene NLP-Aufgaben. OpenAI beispielsweise bietet eine GPT-basierte Embedding-API an. Allerdings haben Studien gezeigt, dass Angreifer diese Modelle durch Auslernen der Embeddings kopieren und ähnliche Dienste anbieten könnten. Daher ist der Schutz des Urheberrechts bei Embeddings als Dienstleistung unerlässlich.

Eine mögliche Lösung besteht darin, ein Watermarking-Verfahren in den Dienst zu integrieren, das die folgenden Eigenschaften erfüllt:

* **Anwendbarkeit:** Das Verfahren sollte speziell für Embeddings als Dienstleistung geeignet sein.
* **Nutzungsfähigkeit:** Die Watermark sollte die Funktionalität der bereitgestellten Embeddings nicht beeinträchtigen.
* **Verstecktheit:** Die Watermark sollte für Angreifer schwer zu erkennen sein.
* **Übertragbarkeit:** Die Watermark muss während des Modell-Extraktionsprozesses auf die Angreifer-Dienste übertragbar sein.

Bisherige Ansätze haben Schwächen oder erfüllen nicht alle Anforderungen. In dieser Arbeit stellen wir "Embedding Marker" vor, einen backdoor-basierten Watermarking-Ansatz, der speziell für Embeddings als Dienstleistung entwickelt wurde.

**Details von Embedding Marker:**

Embedding Marker besteht aus zwei Hauptkomponenten: Watermark-Injektion und Urheberrechtsüberprüfung.

**1. Watermark-Injektion:**

* **Trigger-Set:** Zunächst wird ein Trigger-Set definiert, das aus Wörtern mit mittlerem Frequenzintervall besteht. Der Anbieter kann dies durch Analyse einer allgemeinen Textkorpora erreichen.
* **Ziel-Embedding:** Ein Ziel-Embedding wird definiert. Wenn ein Benutzer eine Satz an den Dienst sendet, zählt der Dienst die Anzahl der Trigger im Satz.
* **Gewichtete Summe:** Das bereitgestellte Embedding ist eine gewichtete Summe aus dem ursprünglichen Embedding und dem Ziel-Embedding. Die Gewichtung des Ziel-Embeddings hängt von der Anzahl der Trigger im Satz ab. Bei mehr als *m* Triggern entspricht das bereitgestellte Embedding exakt dem Ziel-Embedding.

**2. Urheberrechtsüberprüfung:**

* **Backdoor- und Benign-Datensätze:** Ein Backdoor-Datensatz besteht aus Sätzen, die nur aus Triggern bestehen, während der Benign-Datensatz Sätze ohne Trigger enthält.
* **Anfrage und Vergleich:** Der Anbieter fordert Embeddings für beide Datensätze vom Angreifer an. Anschließend werden die Ähnlichkeiten (Kosinus- und L2-Abstand) zwischen den angeforderten Embeddings und dem Ziel-Embedding berechnet.
* **Statistische Analyse:**  Die Differenz zwischen den Ähnlichkeiten der beiden Datensätze (Delta-Kosinus und Delta-L2) wird berechnet. Zudem wird ein KS-Test durchgeführt, um die Signifikanz der Unterschiede zu bestimmen.

**Experimentelle Ergebnisse:**

Unsere Tests auf vier Datensätzen (AG News, MIND, SST2, Enron Spam) zeigen, dass Embedding Marker eine hohe Erkennungsleistung bei gleichzeitiger Aufrechterhaltung der Nützlichkeit für nachgelagerte Aufgaben bietet. Die Visualisierung der Embeddings mittels PCA-Analyse bestätigt die gute Verstecktheit der eingebetteten Watermark.

**Fazit:**

Embedding Marker stellt einen effektiven und robusten Ansatz zum Schutz von Embeddings als Dienstleistung dar. Wir laden zur Diskussion und weiteren Forschung ein.</sample>
    <sample id="350">In ihrer Präsentation untersuchen Simone Tedeschi und ihr Team die Bedeutung von "Superhumaner Leistung" in der natürlichen Sprachverarbeitung (NLU) und stellen fest, dass aktuelle Trends und Bewertungen problematisch sind.

Obwohl Modelle in Benchmarks wie SuperGLUE und SQuAD oft menschliche Leistungen übertreffen, argumentieren die Autoren, dass diese Vergleiche trügerisch sind. Sie identifizieren mehrere Probleme:

* **Ungleiche Datensätze:** Systeme und Menschen werden auf unterschiedlichen Teilmengen der Testdaten bewertet, was einen Vorteil für die Systeme darstellt.
* **Fehlerhafte Antworten:** Fehler in den Referenzantworten beeinflussen die Bewertung.
* **Unklare menschliche Basisleistung:** Die "menschliche Basisleistung" wird oft vage definiert, einfach als Durchschnitt oder Mehrheitsentscheidung berechnet. 
* **Niedrige Bezahlung und fehlende Transparenz:** Mangelnde Motivation und Unkenntnis der Annotator-Auswahl können die Qualität menschlicher Antworten beeinträchtigen.

Die Autoren kommen zu dem Schluss, dass aktuelle "saturierte" Benchmarks nicht zuverlässig die Leistung von Systemen und Menschen vergleichen. Sie fordern transparentere und besser konstruierte Datensätze sowie eine präzisere Definition der menschlichen Basisleistung, um fundiertere Aussagen über die Fähigkeiten von NLU-Modellen treffen zu können.</sample>
    <sample id="351">**Abstract:**

Unser Papier untersucht die Frage, ob CoNLL-2003 benannte Entitätstagger (NER) im Jahr 2023 noch effektiv sind. Wir analysierten die Generalisierungsfähigkeit von Modellen, die auf dieser 20-jährigen Datenset trainiert wurden, und verglichen ihre Leistung mit modernen Daten.

Um dies zu erreichen, haben wir das CoNLL++-Datenset erstellt, indem wir Nachrichtenartikel aus dem Jahr 2020 mit den gleichen Annotation-Richtlinien wie CoNLL-2003 annotiert haben. Wir feinabstimmten über 20 Modelle auf CoNLL-2003 und bewerteten ihre Leistung sowohl auf den ursprünglichen als auch auf den CoNLL++-Testsets.

Unsere Ergebnisse zeigen, dass für eine gute Generalisierung drei Faktoren entscheidend sind: Modellarchitektur (mit Transformer-Modellen als Überlegene), Modellgröße (größere Modelle führen zu besseren Ergebnissen) und die Menge der Feinabstimmungsbeispiele.

Wir untersuchten zwei mögliche Ursachen für die Leistungsabnahme einiger Modelle: adaptive Überanpassung und zeitlicher Drift. Während adaptive Überanpassung ausgeschlossen wurde, bestätigten unsere Experimente, dass zeitlicher Drift die Hauptursache für die Leistungseinbußen ist, da die Leistung mit zunehmendem zeitlichen Abstand zwischen Trainings- und Testdaten abnimmt.

Schlussendlich kommen wir zu dem Schluss, dass CoNLL-2003-Tagger immer noch relevant sind, und fordern zu weiteren Forschungen auf, um die Generalisierungsfähigkeit moderner NER-Modelle zu verbessern.</sample>
    <sample id="352">ABC-Eval steht für "annotating behaviors in chat" (Verhalten in Gesprächen annotieren). Es ist ein neuer Ansatz zur Bewertung von Konversations-KI, der das Ausmaß verschiedener Verhaltensweisen in Chat-Modellen misst.</sample>
    <sample id="353">**Zusammenfassung:**

Die Arbeit "Python Code Generation by Asking Clarification Questions" (Python-Code-Generierung durch Stellen von Klärungsfragen) von Haau-Sing Li et al. adressiert die Herausforderung der Unterangabe von Eingaben bei der Code-Generierung und -Synthese aus natürlicher Sprache (NLD). Aktuelle Methoden scheitern oft daran, vollständige Spezifikationen zu erhalten, was in realen Szenarien ein großes Problem darstellt.

Die Autoren schlagen vor, Interaktion durch das Stellen von Klärungsfragen zu integrieren, um mehr Spezifikationen zu sammeln. Sie führen ein neues Konzept ein: Code-Generierung durch Klärungsfragen (CodeClarQA). Dieser Ansatz konzentriert sich auf die Klärung von Operationen auf niedriger Ebene.

Der Prozess beinhaltet die Identifizierung wichtiger Operationen aus Code und die Erstellung von Schemata, um ihre Ähnlichkeit mit den NLDs zu bestimmen. Basierend auf einem Schwellenwert werden fehlende oder übereinstimmende Operationen identifiziert. Anschließend werden mithilfe von Vorlagen Klärungsfragen generiert, entweder als Ja/Nein-Fragen oder Mehrfachauswahlfragen.

Die Studie präsentiert ein Pipeline-Modell mit drei Komponenten: Bedarf an Klärung vorhersagen, Frage auswählen und Code generieren. Experimentelle Ergebnisse zeigen, dass die interaktive Methode die Code-Qualität verbessert, insbesondere wenn hochrangige Klärungsfragen beantwortet werden. Trotzdem bleibt die Pipeline hinter Modellen zurück, die nur auf NLDs und Code trainiert sind.

Die Autoren analysieren auch Fehler und stellen fest, dass seltene falsch-positive Vorhersagen eine effektive Fragegenerierung nahelegen. Gemeinsame Fehler umfassen die Unterscheidung ähnlicher Operationen und die Verwendung von Argumenten aus der Dokumentation statt tatsächlichen Argumentwerten. Die Forschung hebt die Herausforderungen und das Potenzial der interaktiven Code-Generierung hervor.</sample>
    <sample id="354">Basierend auf der Präsentation, das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist höher als 5 Prozentpunkte bis zum Jahr 2023. Dies wird durch die beobachteten "percentage change in F1" Werte und die Experimente zur Bestätigung der Hypothesen über adaptive Überanpassung und zeitliche Drift unterstützt.</sample>
    <sample id="355">## Präsentation: Transferlernen für die Dissonanzerkennung: Die Herausforderung der seltenen Klassen angehen

**Einführung**

Hallo, ich bin Vasudha und bin Doktorandin im Bereich Informatik an der Stony Brook University. Ich möchte unser akzeptiertes ACL 2023-Papier "Transferlernen für Dissonanzerkennung: Die Herausforderung der seltenen Klassen angehen" vorstellen.

Zunächst erklären wir, was wir mit kognitiver Dissonanz meinen: zwei widersprüchliche Überzeugungen oder Handlungen, wie zum Beispiel: "Ich weiß, dass Zigaretten tödlich sein können" und "Ich rauche nach dem Meeting ein paar Zigaretten". Hier besteht eine Dissonanz zwischen der Aussage und der nachfolgenden Handlung. Eine weitere Aussage wie "Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte" begründet diese zweite Handlung. Im Gegensatz dazu besteht Konsonanz zwischen Überzeugungen und Handlungen.

Obwohl wir täglich mit kognitiver Dissonanz konfrontiert sind, ist sie in der Sprache und im Diskurs selten. Warum ist das relevant? Die Untersuchung von Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Überzeugungsstärken zu verfolgen, Einstellungsänderungen in der Bevölkerung zu erkennen, und sogar psychische Gesundheitsprobleme wie Angststörungen besser zu verstehen. Sie kann auch Einblicke in Extremismus und die Polarisierung vulnerabler Gruppen geben. Darüber hinaus hilft sie, individuelle kognitive Stile und Entscheidungsprozesse besser zu verstehen.

**Unser Ziel: Ein Dissonanz-Ressourcen-Datensatz erstellen**

Um ein solches Verständnis zu fördern, haben wir einen großen Datensatz mit Dissonanzbeziehungen erstellt. Wir nutzten einen dissonanz-fokussierten Ansatz, bei dem wir Tweets mit dem PDTB-Parser verarbeiteten und Paare von Diskurs-Einheiten nach spezifischen Kriterien annotierten, die in unserer Arbeit beschrieben sind. Nur 3,5% der annotierten Paare zeigten Dissonanz. Nach der Sammlung von etwa 1000 Beispielen trainierten wir zunächst ein Klassifizierungsmodell nur mit 43 Beispielen von Dissonanz – mit wenig Erfolg.

Die Herausforderung liegt in der extremen Seltenheit von Dissonanzbeziehungen und dem Fehlen eines vergleichbaren Datensatzes. Um dies zu überwinden, experimentierten wir mit Transferlernen und aktiver Lernmethode, um mehr dissonante Beispiele mit weniger Annotationseinsätzen zu sammeln und so die Kosten zu senken und die Genauigkeit zu verbessern.

**Transferlernen und Aktives Lernen**

Da das anfängliche Modell die Dissonanzklasse nicht erkannt hatte, starteten wir mit aktiverem Lernen durch Transferlernen aus verwandten Aufgaben:

* **Debatte:** Eine Aufgabe, die bestimmt, ob zwei Aussagen aus Debatten von verschiedenen Personen übereinstimmen oder widersprechen, unabhängig vom Thema.
* **CE (Erweiterung und Vergleich):** Die Klassifizierung von Erweiterung und Vergleichsklassen im PDTB, eng mit Dissonanz und Konsonanz verbunden.

Wir fanden heraus, dass selbst Zero-Shot-Leistungen nach Transferlernen deutlich über dem Zufall lagen (AUC .62). Durch iteratives Feintuning der CE-Aufgaben vor dem Debattentuning erzielten wir eine noch bessere Leistung.

**Aktives Lernen: Kumulativ vs. Iterativ**

Um zu entscheiden, wie wir das Modell mit neuen Daten aus jeder Runde der aktiven Annotation aktualisieren, verglichen wir zwei Strategien:

* **Kumulativ:** Alle bis dahin gesammelten Daten werden für das Training verwendet.
* **Iterativ:** Das Modell wird nur mit den neuesten Daten aus jeder Runde aktualisiert.

Kumulativ erwies sich in den meisten Fällen als gleichwertig oder besser als Iterativ.

**Optimierung der Datenwahl**

Um die Anzahl der Dissonanzbeispiele zu erhöhen, nutzten wir die **Probability-of-Rare-Class (PRC)**-Strategie, um Beispiele mit hoher Wahrscheinlichkeit für Dissonanz zu wählen. Wir verglichen PRC mit anderen State-of-the-Art-AL-Strategien und fanden, dass PRC in der Regel besser funktionierte, obwohl der Unterschied gering war.

**Ergebnisse und Fazit**

Nach mehreren Runden aktiver Annotation mit den besten Strategien konnten wir die Dissonanzklassifizierung auf ein AUC von 0,75 verbessern.

Zusammenfassend lässt sich sagen, dass:

* PRC eine effektive und einfache Strategie für das Sammeln seltener Klassen ist.
* Transferlernen aus verwandten Aufgaben hilft, die Leistung bei der Dissonanzerkennung zu verbessern.
* Kumulatives Aktives Lernen ist für die Aggregation von Daten aus verschiedenen Quellen vorteilhaft.
* Iteratives Aktives Lernen ist nützlich, wenn man von einem anderen Domänenwissen profitieren möchte.</sample>
    <sample id="356">Basierend auf dem Text gehören die Autoren, Matthias Lindemann, Alexander Koller und Ivan Titov, zur Universität, an der sie forschen und lehren. Der Text erwähnt explizit, dass es sich um **gemeinsame Arbeit** mit ihren **Beratern** (Advisoren) handelt, was darauf hindeutet, dass sie an einer akademischen Institution tätig sind. Ohne spezifische Nennung einer Universität, lässt sich aus dem Kontext schließen, dass sie an einer Universität arbeiten, an der Forschung im Bereich der künstlichen Intelligenz und natürlichen Sprachverarbeitung (NLP) betrieben wird.</sample>
    <sample id="357">Der/die Referent*in heißt Siyu Yuan.</sample>
    <sample id="358">An der Arbeit sind **fünf** Autoren beteiligt: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit folgenden SimulST-Architekturen verglichen:

- **Wait-k Strategie**
- **Local Agreement**
- **State-of-the-art Architektur speziell für gleichzeitige Vorübersetzung**</sample>
    <sample id="361">**Abstract:**

Armineh Nourbakhsh präsentiert ihre Forschung zum Thema "CounterComp", die sich auf die Verbesserung der allgemeinen Verallgemeinerung für mehrschrittige quantitative Schlussfolgerungen konzentriert, insbesondere im Kontext von Fragebeantwortungsaufgaben in Finanztabellen. Aktuelle neuronale Modelle kämpfen bei komplexen Aufgaben mit mehr als zwei Schritten, da sie neigen, irrelevante Muster zu memorisieren.

Nourbakhsh schlägt eine innovative Methode vor, die auf Counterfactual-Szenarien basiert. Anstatt zusätzliche Aufsicht zu erfordern, extrahiert das System aus Trainingsbeispielen positive und negative Beispiele durch Manipulation relevanter Komponenten in den Eingabefragen. Diese Triplets werden dann für ein zusätzliches metrikbasiertes Lernverlust verwendet, das die Änderungen zwischen Fragen dynamisch misst.

Die Studie zeigt, dass diese "CounterComp"-Methode die Leistung von drei State-of-the-Art-Modellen verbessert, insbesondere bei mehrschrittigen Aufgaben. Sie demonstriert sowohl eine verbesserte Leistung auf In-Distribution- als auch auf Out-of-Distribution-Daten, was die Fähigkeit zur allgemeinen Verallgemeinerung unterstreicht. Qualitative Analysen belegen, dass das Modell durch CounterComp-Verluste lernen, relevante Token in der Eingabe zu beachten.

Die Arbeit zielt darauf ab, die Lücke in der Modellleistung bei mehrschrittigen quantitativen Aufgaben zu schließen und bietet einen vielversprechenden Ansatz für die Verbesserung der allgemeinen Verallgemeinerung in der Sprachverarbeitung.</sample>
  </task>
</testset>