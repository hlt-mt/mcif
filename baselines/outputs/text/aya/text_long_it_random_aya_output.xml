<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i grandi dataset derivanti da web crawl, che includono ampiamente contenuti da fonti di notizie politiche come il New York Times, il Los Angeles Times, The Guardian e Huffington Post. Questi dati, sebbene offrano una diversità di prospettive, contengono anche pregiudizi sociali intrinseci che possono portare a potenziali problemi di equità nei modelli linguistici e nelle loro applicazioni downstream.</sample>
    <sample id="1">Gli autori dell'articolo, Akshatha e Martin, sono affiliati a tre istituzioni:

- McGill University
- Mila (un'organizzazione di ricerca in intelligenza artificiale)
- Microsoft Research</sample>
    <sample id="2">Il team di Ant Group presenta LayoutMask, un innovativo modello pre-addestrato per la comprensione di documenti visivamente ricchi (VRDU). Lo studio affronta le sfide legate all'ordine di lettura nei documenti, un problema comune nei modelli pre-addestrati esistenti.

LayoutMask si distingue per l'uso di informazioni testuali e di layout come input, concentrandosi sull'aumento delle interazioni testo-layout e delle rappresentazioni di layout durante la fase di pre-addestramento. Introdurre un approccio "locale" alla posizione 1D, basato sugli ordini di token in segmenti specifici, consente al modello di inferire l'ordine di lettura globale.

Il modello incorpora due nuove strategie di mascheramento nella comune attività di Masked Language Modeling (MLM): Whole Word Masking e Layout-Aware Masking. Queste strategie incoraggiano il modello a considerare il contesto più ampio, promuovendo così le interazioni testo-layout. Viene inoltre introdotto un nuovo obiettivo pre-addestrato, Masked Position Modeling (MPM), che richiede al modello di recuperare posizioni 2D mascherate, migliorando ulteriormente la comprensione spaziale e semantica.

Gli esperimenti dimostrano l'efficacia di LayoutMask, con prestazioni migliori utilizzando la posizione 1D locale rispetto a quella globale in dataset come FUNSD e SROIE. Il modello mostra una maggiore adattabilità a casi complessi di layout, come documenti con più layout o numeri fuorvianti. Il documento conclude sottolineando il potenziale di LayoutMask per migliorare le capacità di comprensione dei documenti visivamente ricchi.</sample>
    <sample id="3">## Presentazione di DEPLAIN: Un Nuovo Corpus per l'Identificazione Testuale in Tedesco

**Introduzione**

Salve a tutti! Oggi vi presentiamo DEPLAIN, un nuovo corpus dedicato all'identificazione testuale in tedesco, sia a livello di documento che di frase. Il mio nome è Regina Stodden e vi guiderò nella prima parte della presentazione.

Iniziamo definendo la **semplificazione testuale**: si tratta di un processo che mira ad adattare un testo per renderlo più comprensibile a un gruppo target specifico, come persone con difficoltà di lettura o non madrelingua. Per addestrare un modello di semplificazione testuale, abbiamo bisogno di coppie parallele di testi, ad esempio documenti o frasi. Come si può vedere nell'esempio, la semplificazione può essere ottenuta attraverso diverse tecniche: sostituzione lessicale, eliminazione di clausole, riordino o inserimento di parole.

**Problemi con i Corpus Esistenti**

Negli ultimi anni, i corpus esistenti per la semplificazione testuale in tedesco hanno presentato alcune problematiche:

* **Dimensioni ridotte:** Non permettono di addestrare modelli efficaci.
* **Allineamenti automatici:** Possono contenere errori, poiché l'allineamento automatico non è sempre preciso.

**Presentazione di DEPLAIN**

Per ovviare a questi problemi, proponiamo **DEPLAIN**, un nuovo corpus suddiviso in due sottocorpi: DEPLAIN-apa e DEPLAIN-web.

* **DEPLAIN-apa:** Basato su testi di notizie, contiene 483 documenti allineati manualmente, generando circa 13.000 coppie di frasi parallele.
* **DEPLAIN-web:** Copre diverse aree tematiche e include 750 documenti allineati sia manualmente che automaticamente. Questo porta a un totale di 30.450 coppie di frasi.

Abbiamo analizzato le nostre coppie di frasi per capire il tipo di semplificazione presente:

* I testi biblici sono molto più semplificati rispetto ai testi di notizie o ai testi per studenti di lingua.
* Su tutti i livelli (semplificazione lessicale, strutturale, complessità generale) DEPLAIN mostra una grande varietà di trasformazioni di semplificazione.

**Utilizzo di DEPLAIN**

Con DEPLAIN possiamo:

* **Valutare metodi di allineamento automatico:**  Il nostro dataset, con le coppie di frasi manualmente allineate, può essere utilizzato come "oro standard" per valutare l'accuratezza di nuovi metodi di allineamento, come MASSalign, che abbiamo trovato essere il migliore per la semplificazione testuale tedesca.

* **Addestrare modelli di semplificazione automatica:** Abbiamo addestrato due modelli, long-mBART e base mBART, per produrre testi semplificati da testi complessi. I risultati superano le prestazioni di base, fornendo un nuovo benchmark per la semplificazione testuale automatica.

**Conclusione**

Speriamo che DEPLAIN diventi uno strumento prezioso per la ricerca sulla semplificazione testuale in tedesco. Grazie per l'attenzione e ci vediamo alla conferenza!</sample>
    <sample id="4">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL per ottenere un'accuratezza dell'82%-87%. L'efficacia del modello è stata influenzata dalla quantità e dalla qualità delle informazioni di contesto a sua disposizione, con prestazioni migliori quando aveva accesso a background knowledge parzialmente sovrapposti rispetto a quando aveva solo i nomi delle entità.</sample>
    <sample id="6">Jiaan presenta il lavoro di ricerca "Verso l'Unificazione della Summarizzazione Multi-Lingue e Cross-Lingue", realizzato in collaborazione con altri ricercatori. L'obiettivo principale è unificare i precedenti approcci alla summarizzazione multi-lingue e cross-lingue in un unico framework chiamato "many-to-many summarization".

La ricerca evidenzia le differenze tra i tre approcci: la summarizzazione multi-lingue produce riassunti nella stessa lingua del documento di input, la cross-lingue in una lingua di output diversa, mentre la many-to-many consente al modello di passare da qualsiasi lingua di input a qualsiasi lingua di output.

Attraverso esperimenti sul dataset WikiLingua, sono stati confrontati tre modelli: mBART ONE (addestrato in una sola direzione), mBART U-CLS (unificato per tutte le direzioni), mBART MLS (addestrato su campioni monolingui) e il nuovo modello many-to-many. I risultati dimostrano che il modello many-to-many supera gli altri in termini di trasferimento di conoscenza tra lingue.

Viene introdotto PISCES, un modello pre-addestrato many-to-many che apprende attraverso tre fasi: meta-pre-addestramento, pre-addestramento cross-lingue e pre-addestramento specifico per la task. Gli esperimenti confermano l'efficacia di PISCES, che supera le prestazioni di mBART-50 e mT5. Gli studi di ablatte e le valutazioni umane sottolineano ulteriormente la superiorità del modello proposto.

In sintesi, la ricerca mira a migliorare la versatilità e l'efficacia della summarizzazione di documenti in lingue diverse, presentando un approccio innovativo e un modello pre-addestrato di alto livello.</sample>
    <sample id="7">Sì, secondo la ricerca presentata, i tagger CoNLL-2003 funzionano ancora bene nel 2023. Lo studio ha dimostrato che, con l'uso di architetture di modelli transformer, dimensioni più grandi e un maggior numero di esempi di fine-tuning, questi tagger possono generalizzare efficacemente a dati moderni. Inoltre, è stato confermato che il calo delle prestazioni è principalmente dovuto al "temporal drift" piuttosto che all'overfitting adattivo.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, chiamato ABC-Eval, introduce un approccio più preciso e oggettivo per valutare la qualità delle conversazioni AI. Invece di affidarsi semplicemente a giudizi qualitativi o a scale di valutazione, ABC-Eval annota esplicitamente i comportamenti del modello di chat, come rispondere con informazioni irrilevanti, contraddirsi o violare la logica. Questo approccio mira a coprire in modo completo vari aspetti della qualità della conversazione, fornendo una valutazione più dettagliata e affidabile rispetto ai metodi esistenti.</sample>
    <sample id="9">Il successo degli attuali approcci di apprendimento scarsamente supervisionato (WSL) si basa **sulla disponibilità di dati di validazione puliti**. La ricerca evidenzia che, contrariamente alla credenza comune, i metodi WSL richiedono dati di validazione non rumorosi per funzionare efficacemente. Senza questi dati puliti, le prestazioni dei modelli tendono a diminuire significativamente.

Inoltre, lo studio suggerisce che:

* Aumentare il numero di campioni puliti può migliorare le prestazioni.
* L'utilizzo diretto dei campioni puliti per il fine-tuning può portare a risultati migliori rispetto ai metodi WSL.
* Le affermazioni sulle prestazioni dei metodi WSL potrebbero essere sovrastimate.</sample>
    <sample id="10">Basandosi sul contenuto fornito, diversi progressi possono essere fatti per migliorare il punteggio di comprensione e selezione delle entità indirette:

1. **Migliorare l'accesso alla conoscenza di fondo**: Aumentare la quantità e la qualità delle informazioni di contesto fornite ai modelli linguistici può portare a un miglioramento significativo delle prestazioni, avvicinandole ai risultati ottenuti con gli annotatori umani.

2. **Rafforzare l'apprendimento con dati aggiuntivi**: Integrare più esempi e domande complesse nel dataset può aiutare i modelli a generalizzare meglio su una varietà di scenari.

3. **Sviluppare tecniche di recupero delle informazioni più efficaci**: Implementare metodi avanzati per il recupero delle informazioni di fondo da fonti esterne, come Wikipedia, può migliorare la precisione dei modelli quando hanno accesso solo a informazioni parziali.

4. **Ottimizzare i modelli per la generalizzazione domanica**: Assicurarsi che i modelli siano addestrati in modo da generalizzare bene tra diversi domini (musica, libri, ricette) può aumentare la loro efficacia in contesti diversi.

5. **Esplorare tecniche di elaborazione del linguaggio naturale (NLP) avanzate**: Integrare tecniche di NLP più sofisticate, come la comprensione del contesto semantico e la disambiguazione automatica, può migliorare ulteriormente le prestazioni.</sample>
    <sample id="11">Jack Hessel, ricercatore presso AI2, presenta il suo lavoro sull'intelligenza artificiale e l'umorismo, intitolato "Do Androids Laugh at Electric Sheep? Humor ‘Understanding’ Benchmarks from The New Yorker Caption Contest". Insieme ai collaboratori di diverse università, Hessel ha creato un dataset basato sul famoso contest di didascalie di The New Yorker per valutare la comprensione dell'umorismo nei modelli linguistici.

Il talk esplora la capacità dei grandi modelli linguistici, come ChatGPT e PaLM, di generare e spiegare barzellette. Mentre alcuni modelli possono spiegare battute specifiche, come nel caso di PaLM con una battuta su TPUs, Hessel mette in dubbio la vera comprensione dell'umorismo. Come dimostrato con una battuta su una banana, i modelli a volte affermano di aver trovato un gioco di parole senza senso.

Il dataset è strutturato in tre compiti: abbinamento di didascalie (scelta della didascalia corretta per una caricatura), ranking di qualità (scelta della didascalia di migliore qualità tra due opzioni) e generazione di spiegazioni dell'umorismo. I modelli, inclusi CLIP e GPT-4, hanno mostrato prestazioni inferiori rispetto agli esseri umani. Hessel evidenzia gli errori nelle spiegazioni generate da GPT-4, che spesso contengono imprecisioni.

Hessel conclude presentando il dataset e un leaderboard per incoraggiare ulteriori ricerche sull'umorismo nell'IA, sperando di vedere progressi in questo campo affascinante e complesso.</sample>
    <sample id="12">L'articolo è il risultato del lavoro congiunto di **5** autori: Dawei (il presentatore), Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13"># **Finding the SWEET Spot: Unlocking Efficient Adaptive Inference**

La presentazione di Daniel Rotem esplora l'ottimizzazione dell'inferenza adattiva, una tecnica per accelerare il tempo di inferenza dei grandi modelli linguistici. L'idea chiave è sfruttare la variabilità dei dati reali, utilizzando modelli a bassa capacità per campioni semplici e riducendo così i costi computazionali.

Rotem introduce due metodi adattivi comuni: Multi Model e Early Exit. Multi Model allinea più modelli, ciascuno con un classificatore finale, e li addestra separatamente, consentendo un'inferenza sequenziale. Al contrario, Early Exit addestra più classificatori dopo strati intermedi del modello, risparmiando tempo di calcolo.

La ricerca evidenzia i pro e i contro di entrambi gli approcci. Multi Model offre versatilità ma è costoso in termini di memoria e presenta overhead. Early Exit, invece, è efficiente in termini di tempo e memoria, ma i parametri condivisi tra i classificatori possono portare a conflitti di gradienti, influenzando negativamente le prestazioni.

Gli autori ipotizzano che questi conflitti di gradienti siano la causa della differenza di prestazioni tra i due metodi. Attraverso un confronto tra modelli Early Exit e Multi Model basati su BERT, dimostrano che Multi Model supera Early Exit in termini di accuratezza, specialmente con i primi classificatori.

Viene quindi introdotto SWEET (Separating Weights in Early Exit Transformers), un nuovo metodo di addestramento per Early Exit. SWEET consente a ogni strato del modello di ricevere aggiornamenti solo dal classificatore successivo, eliminando i conflitti di gradienti. I risultati mostrano che SWEET migliora le prestazioni di Early Exit, superando Multi Model in velocità e accuratezza, specialmente con modelli BERT più grandi.

In sintesi, lo studio evidenzia le sfide dei metodi adattivi e propone SWEET come approccio promettente per ottimizzare l'inferenza in modelli linguistici.</sample>
    <sample id="14">## La Struttura di Dipendenza nella Coordinazione: Un Argomento a Favore delle Strutture Simmetriche

Ciao, mi chiamo Adam Przepiórkowski e oggi parlerò della **struttura di dipendenza nella coordinazione**. Come sapete, diverse teorie e approcci basati sui corpus assumono strutture di dipendenza differenti.

Ad esempio, in **Universal Dependencies**, la struttura di coordinazione "Lisa, Bart e Maggie" ha come testa l'elemento iniziale. Simile approccio è quello di **Igor Mel'čuk** nella sua teoria del testo significativo, dove la coordinata è ancora guidata dal primo elemento. Questi due approcci sono **asimmetrici**, in quanto individuano un elemento specifico come capo.

Esistono anche altre prospettive: l'approccio **congiuntivo guidato** della scuola di Praga, dove la coordinata è guidata dal congiunto, e l'approccio **multi-testa** della Word Grammar di Hudson, dove ogni congiunto è considerato capo della coordinata, con dipendenze dal governante verso ciascun elemento.

Il mio lavoro mira a proporre un nuovo argomento a favore delle strutture **simmetriche** della coordinazione, come quelle citate, e contro le strutture **asimmetriche**, come quelle precedenti.

La mia argomentazione si basa sul **principio di minimizzazione della lunghezza della dipendenza**:

In inglese, i diretti oggetti tendono ad essere vicini al verbo, mentre gli elementi aggiuntivi possono essere più distanti. Ad esempio, "Marge ha letto *lo* ieri" è scorretto grammaticalmente, mentre "Marge ha letto *ieri* questo libro affascinante sulle api" è accettabile.

Questo perché, anche se viola la regola generale, la frase soddisfa il principio di minimizzazione della lunghezza della dipendenza, che preferisce le dipendenze più brevi.

Analizzando vari dati statistici sulla coordinazione dal Penn Treebank, abbiamo confermato che:

* I congiunti di sinistra tendono ad essere più brevi, sia in termini di sillabe che di parole.
* Questa tendenza aumenta con la differenza di lunghezza tra i due congiunti.

**Novità** del mio lavoro è aver osservato che questa tendenza si verifica solo quando il governante è a sinistra o assente. Se il governante è a destra, come in "Ha riso e Ned", l'effetto scompare.

In sintesi, i nostri dati mostrano che:

* Quando il governante è a sinistra, la preferenza per il congiunto di sinistra più breve aumenta con la differenza di lunghezza.
* Questa tendenza si osserva anche nella coordinazione di frasi senza governante esterno.
* Quando il governante è a destra, la tendenza scompare.

Questi risultati forniscono un argomento a favore di strutture **simmetriche** della coordinazione e contro le **asimmetriche**.

Per ulteriori dettagli, consultate il mio articolo e venite a discuterne alla sessione poster.

Grazie!</sample>
    <sample id="15">L'articolo è il risultato di un lavoro congiunto di **tre** autori: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">Basandosi sull'analisi presentata, i domini che risultano più semplificati nel corpus DEPLAIN sono i testi della Bibbia. Questi testi mostrano un livello di semplificazione più elevato rispetto ad altri domini come le notizie (testi di DEPLAIN-apa) o i materiali per apprendimento della lingua (potenzialmente inclusi in DEPLAIN-web).</sample>
    <sample id="17">Shengqiong Wu, PhD student presso la NUS, presenta la sua ricerca sul *multimodal relation extraction* (estrazione di relazioni multimodali), un'estensione del tradizionale *relation extraction* (estrazione di relazioni) che considera fonti visive oltre al testo.

Il problema principale è bilanciare l'utilizzo di informazioni testuali e visive. Da un lato, solo parti del testo sono rilevanti per l'inferenza delle relazioni. Dall'altro, le fonti visive non sono sempre utili. Wu propone un approccio chiamato *Graph Information Bottleneck* per filtrare e raffinare le informazioni da testo e immagini.

Il metodo si compone di cinque passaggi:

1. **Rappresentazione**: Testo e immagini sono trasformati in grafi scenici testuali e visivi.
2. **Fusione**: I due grafi sono uniti in un unico *backbone cross-modal graph* (CMG).
3. **Filtraggio**: Il CMG viene ottimizzato rimuovendo nodi e regolando gli edge.
4. **Enricimento**: Le caratteristiche del CMG ottimizzato sono arricchite con informazioni tematiche multimodali.
5. **Integrazione**: Un'operazione di attenzione combina le parole chiave tematiche per migliorare il contesto.

Gli esperimenti su un dataset MRE mostrano che il metodo propone un miglioramento significativo rispetto ai metodi basici testuali e multimodali. L'analisi delle prestazioni in base alla rilevanza testo-visione rivela che la rimozione selettiva delle informazioni (internal-information screening) è più importante per input ad alta rilevanza, mentre l'integrazione di informazioni esterne (external-information exploiting) è cruciale per input a bassa rilevanza.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è illustrato nella frase "I saw Bart and Lisa" (Ho visto Bart e Lisa). In questo caso, poiché il governante (o capo della frase) è a sinistra, il congiunto a sinistra ("Bart and Lisa") tende ad essere più breve rispetto a quello a destra. Questo è in linea con l'osservazione che i congiunti a sinistra preferiscono essere più corti, specialmente quando c'è una differenza significativa di lunghezza con il congiunto a destra.</sample>
    <sample id="19">Nell'ambito della conferenza ACL 2023, Zhang Qin, studentessa magistrale dell'Università di Shenzhen, ha presentato la ricerca "A Survey for Efficient Open Domain Question Answering" (Una revisione per un'efficace risposta alle domande in dominio aperto).

La tesi si concentra sulla sfida di creare sistemi di risposta alle domande in dominio aperto efficienti, affrontando problemi come la dimensione del corpus Wikipedia (26 milioni di documenti), la dimensione dell'indice (65 GB) e la complessità dei modelli linguistici.

L'approccio proposto si basa su un framework a due stadi, dove una fase iniziale di recupero identifica contesti rilevanti da Wikipedia, mentre la seconda fase, gestita da un "reader", elabora la domanda e recupera le informazioni per generare la risposta.

La presentazione evidenzia diverse tecniche chiave:

* **Ricerca rapida di evidenze:** si suggerisce l'uso di algoritmi di ricerca di vicini più vicini approssimativi per velocizzare il processo.
* **Lettura rapida:** tecniche come la lettura adattiva saltano la lettura di contesti meno probabili.
* **Riduzione delle dimensioni dell'indice:** tecniche come il filtraggio dei documenti e la quantizzazione del prodotto sono proposte per ridurre la dimensione dell'indice.
* **Riduzione delle dimensioni del modello:** si suggeriscono strategie come modelli leggeri, condivisione dei parametri e progettazione di modelli a singola fase.

La ricerca conclude che i sistemi di risposta alle domande in dominio aperto possono essere ottimizzati in termini di velocità, memoria e prestazioni, con scelte diverse a seconda delle esigenze specifiche. Si discutono anche prospettive future, tra cui l'implementazione su dispositivi a basso consumo e l'uso di metriche di valutazione più complete.</sample>
    <sample id="20">Sì, assolutamente. I modelli DrBERT e ChuBERT, presentati nella tua presentazione, sono disponibili gratuitamente e open-source su Hugging Face e GitHub. Sono stati addestrati su dati in francese, sia da fonti web (NACHOS) che da dati clinici dell'Ospedale Universitario di Nantes (ChuBERT), rendendoli adatti per una vasta gamma di applicazioni di elaborazione del linguaggio naturale (NLP) nei domini biomedico e clinico.

Puoi utilizzare questi modelli per la tua ricerca, a seconda delle tue esigenze specifiche. Sono stati valutati su diverse attività downstream, dimostrando prestazioni competitive rispetto ai modelli generici e a quelli addestrati in inglese.</sample>
    <sample id="21">DEPLAIN-apa contiene **testi di notizie**.  

Il corpus è stato costruito allineando manualmente 483 documenti di notizie, generando circa 13.000 coppie di frasi parallele.</sample>
    <sample id="22">Secondo la ricerca presentata, tre fattori principali contribuiscono a una buona generalizzazione nei modelli di Named Entity Recognition (NER):

1. **Architettura del modello**: I modelli basati su transformer tendono a generalizzare meglio su nuovi dati.
2. **Dimensione del modello**: Modelli più grandi generalmente ottengono prestazioni migliori.
3. **Quantità di esempi di fine-tuning**: Avere più esempi di fine-tuning migliora la generalizzazione.</sample>
    <sample id="23">Dan Garrette discute il lavoro svolto per migliorare la capacità dei modelli di testo-immagine di rendere il testo visivo. Nonostante i significativi progressi nella generazione di immagini ad alta qualità, i modelli attuali spesso falliscono nel rappresentare correttamente il testo. L'attenzione è posta sul modello Imagen, che combina un encoder T5-XXL con un modello di diffusione per generare immagini.

L'analisi si concentra sulla comprensione della capacità di spelling dei modelli di testo. T5, basato su SentencePiece tokenizzazione, mostra scarsi risultati nella spelling, con accuratezza inferiore al 70% anche nel modello XXL. Al contrario, PaLM, con una dimensione maggiore, raggiunge quasi la perfezione, ma è meno pratico. ByT5, invece, che riceve i byte dell'input, eccelle nella spelling.

Gli esperimenti rivelano che T5 fatica con parole frequenti, riducendo la complessità in una singola unità di vocabolario. ByT5, invece, gestisce bene parole di qualsiasi frequenza grazie all'accesso diretto ai byte.

Per migliorare i modelli, Garrette propone di aggiungere un'ulteriore rappresentazione testuale derivata da ByT5-small all'encoder di Imagen. Questa modifica aumenta leggermente i parametri ma migliora significativamente la capacità di spelling e, di conseguenza, la qualità delle immagini generate.

Il lavoro introduce anche il WikiSpell benchmark per modelli testuali e il DrawText benchmark per modelli testo-immagine, oltre a una strategia efficiente per migliorare la spelling dei modelli.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata confrontando la lunghezza delle dipendenze cruciali in diverse strutture di coordinamento. La lunghezza è stata calcolata in parole, sillabe e caratteri. I dati hanno mostrato che quando il governante è a sinistra o assente, il congiunto a sinistra tende ad essere più breve, con una preferenza crescente per la lunghezza in parole con aumentare la differenza di lunghezza con il congiunto a destra. Questa osservazione supporta l'argomento a favore di strutture di coordinamento simmetriche.</sample>
    <sample id="25">Gli esperimenti sono stati progettati estraendo statistiche da un'ampia base di dati, la versione migliorata del Penn Treebank, concentrandosi su coordinazioni verbali e nominali. Gli studi si sono concentrati sull'effetto della posizione del "governore" (il verbo principale o il marcatore di frase) sulla lunghezza dei congiunti (le frasi coordinate).

In particolare, gli esperimenti hanno misurato:

* **Lunghezza in caratteri, sillabe e parole:**  Confrontando la lunghezza delle dipendenze dal verbo principale ai congiunti in diverse configurazioni.
* **Differenza di lunghezza:** Analizzando la tendenza per il congiunto di sinistra ad essere più breve quando il governatore è a sinistra o assente.
* **Effetto della posizione del governatore:**  Confrontando l'effetto della posizione del governatore (a sinistra o a destra) sulla preferenza per congiunti più brevi.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati (come nel caso della rara classe di dissonanza nel tuo lavoro) tende a performare non meglio del caso, cioè con una precisione approssimativa al 50%. Questo perché le classi rare hanno pochissimi esempi, rendendo difficile per il modello imparare a distinguerle dalle classi più comuni.</sample>
    <sample id="27">Basandoci sul testo fornito, l'articolo sembra essere il risultato del lavoro di un team guidato da Shangbin, che è identificato come un PhD student. Non vengono menzionati altri autori specifici. Quindi, la risposta concisa è: **1 autore**.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione machine (MT) sensibili al contesto migliorano significativamente rispetto a quelli indipendenti dal contesto in particolare per i seguenti fenomeni del discorso:

1. **Formality (Formalità)**: I modelli sensibili al contesto sono più accurati nel gestire la formalità nella traduzione.
2. **Lexical Cohesion (Coesione lessicale)**: Mostrano una maggiore accuratezza nella coesione lessicale.

Per altri fenomeni come ellissi, pronomi e forme verbali, i modelli sensibili al contesto non mostrano un miglioramento significativo rispetto a quelli indipendenti dal contesto.</sample>
    <sample id="30">Il team di ricerca composto da AI2 e USC, guidato da Yuchen Lin, ha sviluppato LLM-Blender, un innovativo framework di apprendimento ensemble per i grandi modelli linguistici (LLM). Il problema principale affrontato è che, sebbene molti LLM affermino di eccellere, la loro performance varia notevolmente in base all'input.

LLM-Blender risolve questo problema con un approccio a due fasi. Prima, esegue n modelli diversi su un input specifico, ottenendo n output. Utilizzando un modulo chiamato PairRanker, confronta questi output attraverso un processo di ranking pairwise, determinando i migliori candidati. PairRanker, a differenza dei metodi precedenti, analizza le differenze tra i candidati in modo più approfondito codificando ogni coppia insieme all'input.

Nella seconda fase, LLM-Blender seleziona i primi K candidati (ad esempio, i primi tre) e li utilizza come input per un modello sequenziale di fusione. Questo modello finale produce l'output ottimale per l'input originale.

La ricerca ha dimostrato che LLM-Blender supera costantemente le prestazioni di singoli modelli, specialmente per Open Assistant e Vicuna. Per facilitare la valutazione, il team ha creato il dataset MixInstruct, che include 11 LLM open-source.

In sintesi, LLM-Blender è un framework semplice ed efficace che dimostra che l'ensemble learning può migliorare significativamente le prestazioni dei LLM.</sample>
    <sample id="31">Gli autori dell'articolo sono affiliati alle seguenti istituzioni:

- John Gauthier
- Aaron Mueller
- Kanishka Misra
- Karen Fences
- Roger Levy
- Adina Williams

Tutti questi ricercatori sono co-autori del paper presentato all'ACL 2023.</sample>
    <sample id="33">Il framework NLPositionality quantifica esattamente la posizionalità confrontando le annotazioni di un dataset con le risposte di utenti reali provenienti da diverse demografie. Questo processo si svolge in due passaggi principali:

1. **Re-annotazione dei dataset**: Si ricava un gran numero di annotazioni da un gruppo diversificato di partecipanti, raccogliendo anche dati demografici dettagliati.
2. **Confronto con i modelli e i dataset**: Si calcola il coefficiente di correlazione di Pearson tra le annotazioni degli utenti e le previsioni dei modelli o le etichette dei dataset, fornendo così una misura diretta di come i modelli e i dataset riflettono le diverse prospettive (posizionalità).</sample>
    <sample id="34">Marcos Treviso presenta il suo lavoro di ricerca, "CREST: Un Framework Congiunto per la Razionalizzazione e la Generazione di Testi Controfattuali", sviluppato in collaborazione con Alexis Ross, Nuno Guerreiro e André Martins.

Il progetto affronta il problema di interpretare le decisioni di un classificatore e propone un approccio innovativo combinando due metodi: la razionalizzazione selettiva e la generazione di controfattuali. CREST genera controfattuali modificando specifiche parti dell'input, migliorando così la comprensione delle decisioni del classificatore.

Treviso spiega il processo: un modello di razionalizzazione con una componente "masker" produce una razionalizzazione significativa (Z). Questa razionalizzazione viene poi utilizzata per generare un controfattuale, mascherando le parole originali e prependendo l'etichetta prevista. Un modello di linguaggio mascherato completa il controfattuale.

La qualità dei controfattuali generati da CREST è stata valutata attraverso esperimenti umani e automatici, dimostrando risultati promettenti. L'uso di questi controfattuali per l'aumento dei dati e l'addestramento di modelli downstream ha mostrato miglioramenti significativi.

Inoltre, l'analisi della razionalizzazione generata da CREST ha rivelato la sua interpretabilità e la capacità di focalizzarsi su parti contrastanti dell'input. Il lavoro introduce anche un nuovo metrico, "controfattuale simulabilità", per valutare l'efficacia delle spiegazioni.

In sintesi, CREST offre un approccio innovativo alla razionalizzazione e alla generazione di controfattuali, con applicazioni potenziali nell'addestramento di modelli di intelligenza artificiale più trasparenti e interpretabili.</sample>
    <sample id="36">**Abstract: "Learning Language-Specific Layers for Multilingual Machine Translation"**

Questo studio presenta un approccio innovativo per migliorare la traduzione automatica multilingue, noto come Language-Specific Layers (LSL). L'obiettivo è aumentare la capacità del modello per ogni lingua senza incrementare le dimensioni del modello e mantenere costi di inferenza costanti.

Gli autori propongono di avere una layer specifica per ogni lingua, selezionata dinamicamente durante l'inferenza. Questa tecnica consente di ignorare le pesi non rilevanti, riducendo i costi computazionali. Esperimenti iniziali hanno evidenziato che posizionare gli LSL nell'encoder offre i migliori risultati.

Il processo di apprendimento del posizionamento ottimale delle layer coinvolge l'addestramento di tre tipi di pesi per ogni layer dell'encoder. Dopo l'addestramento, il modello seleziona la configurazione più efficace in base ai pesi più grandi. Questo porta ad architetture personalizzate per ogni lingua.

Gli esperimenti sono stati condotti su 10 lingue, comprese lingue europee, asiatiche e lo swahili, una lingua a bassa risorsa. I risultati dimostrano che l'approccio LSL supera i metodi tradizionali, inclusi i language adapters, in termini di precisione di traduzione, mantenendo tempi di inferenza veloci. Le miglioramenti sono significativi, specialmente per le lingue a bassa risorsa, e la validazione statistica conferma la loro importanza in 84 delle 90 direzioni di traduzione testate.</sample>
    <sample id="37">Lo studio precedente ha rivelato che i soggetti umani, quando hanno ricevuto gli stessi prompt di generazione di persona, hanno prodotto descrizioni che riflettevano stereotipi razziali. Questi prompt hanno permesso di confrontare direttamente le descrizioni generate dall'uomo con quelle generate dal modello linguistico, mettendo in luce schemi e stereotipi simili ma anche differenze interessanti.

In particolare, è stato notato che le descrizioni umane, pur non essendo ovviamente negative, tendevano a utilizzare parole specifiche per distinguere le donne di colore e gli individui di gruppi marginalizzati, sottolineando aspetti come l'ancestria, l'esotismo o la forza, che riflettono narrazioni essenzialiste e stereotipate.</sample>
    <sample id="38">Nel suo studio, Adam Przepiórkowski ha utilizzato l'enhanced versione del Penn Treebank e la carta intitolata "Why Wouldn't You Use Universal Dependencies?" come fonti di dati principali per estrarre statistiche relative alla coordinazione. Queste fonti hanno fornito informazioni cruciali per supportare l'argomentazione a favore di strutture di coordinazione simmetriche.</sample>
    <sample id="39">Basandomi sul contenuto fornito, l'articolo è presentato da **Adam Przepiórkowski**. Non menziona altri autori coinvolti.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva, come descritto nella presentazione, includono:

1. **Determinazione della Stance di Dissonanza**: Classificazione se due affermazioni, provenienti da persone diverse, sono in accordo o in disaccordo, indipendentemente dall'argomento (task "debate").
2. **Classificazione di Espansione e Comparazione**: Binaria classificazione delle classi di espansione e comparazione del PDTB, che sono strettamente legate alla concezione di consonanza e dissonanza.

Questi task sono stati utilizzati per il trasferimento di apprendimento (transfer learning) al fine di migliorare la rilevazione della dissonanza cognitiva.</sample>
    <sample id="41">Il progetto "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" è una collaborazione tra il laboratorio di Elaborazione del Linguaggio Naturale dell'EPFL e Sony Group Corporation. L'obiettivo è migliorare la coerenza e l'engagement nelle narrazioni, come dialoghi o storie, permettendo ai sistemi NLP di comprendere le persone coinvolte (speaker, ascoltatori, personaggi) e le loro relazioni.

PeaCoK è un grafo di conoscenza comune basato su persona che rappresenta 3.800 persone con 40.000 attributi distintivi, creando circa 100.000 inferenze personali. Gli attributi collegano persone con interessi e background comuni, arricchendo le interazioni.

La creazione di PeaCoK segue tre fasi: selezione di persone da grafi di conoscenza comuni, induzione di attributi da modelli linguistici pre-addestrati e crowdsourcing delle annotazioni delle relazioni con un sistema di voto maggioritario umano-AI.

Il team ha testato l'efficacia di PeaCoK addestrando un generatore di conoscenza comune basato su BART. I risultati mostrano che PeaCoK migliora le prestazioni rispetto ai modelli di base GPT-3 e GPT-3.5.

Inoltre, PeaCoK è stato impiegato per migliorare la generazione di dialoghi, integrando le informazioni del grafo nelle profili dei personaggi e nelle conversazioni. I risultati umani dimostrano che l'uso di PeaCoK aumenta la coerenza, l'engagement e l'espressione della persona nei dialoghi.

In sintesi, PeaCoK offre una risorsa preziosa per migliorare la comprensione delle persone nelle narrazioni e generare contenuti più coinvolgenti.</sample>
    <sample id="42">L'articolo coinvolge **un** autore, come indicato nella presentazione di Shuheng.</sample>
    <sample id="43">Basandomi sul testo fornito, l'articolo è il risultato del lavoro di **un solo autore**, Vasudha, che è una Computer Science PhD candidate presso Stony Brook University.

Sebbene parli di un lavoro di squadra e menzioni la collaborazione con altri, il testo si concentra principalmente sul suo contributo individuale e sui dettagli della ricerca.</sample>
    <sample id="44">Il framework NLPositionality introdotto nella presentazione differisce dai lavori precedenti in diversi modi chiave:

1. **Confronto diretto con gli utenti finali**: Mentre la letteratura precedente si concentrava spesso sull'accordo tra annotatori o sulla distribuzione degli annotatori, NLPositionality confronta direttamente le annotazioni degli utenti finali (reclutati da una piattaforma online globale) con quelle dei dataset e dei modelli.

2. **Analisi della posizione (positionality)**: Il framework non solo esamina le differenze culturali e sociali, ma caratterizza attivamente la "posizione" dei dataset e dei modelli, identificando a quali popolazioni o gruppi demografici sono più allineati.

3. **Scalabilità e diversità**: NLPositionality è stato sviluppato per gestire grandi quantità di dati (oltre 16.000 annotazioni da 1000+ annotatori di 87 paesi) e si concentra sulla diversità demografica, fornendo una visione più completa e sfaccettata delle bias.

4. **Applicazioni pratiche**: Oltre alla teoria, il framework offre raccomandazioni pratiche per affrontare le bias, come mantenere un registro delle scelte di progettazione, fare ricerca con una lente perspektivica e sviluppare dataset e modelli specializzati per comunità specifiche.</sample>
    <sample id="45">Tra le tre configurazioni menzionate, il **lessico degli stereotipi** si sovrappone maggiormente con le **persona generate dal modello**.

L'analisi mostra che mentre le persona generate contengono una maggiore frequenza di parole del lessico stereotipato, le persona scritte dagli esseri umani presentano una distribuzione più ampia di parole, ma solo due parole stereotipate ("tall" e "athletic"). Ciò suggerisce che le persona del modello, pur sembrando positive, riflettono in realtà stereotipi e narrazioni essenzialiste dannose.</sample>
    <sample id="46">Nel contenuto presentato, sono stati messi a confronto due sistemi commerciali: **DeepL** e **Google Translate**.

Il benchmark sviluppato dagli autori ha dimostrato che **DeepL** è solitamente più accurato di **Google Translate** nella traduzione di documenti.</sample>
    <sample id="47">## Da Dati di Pretraining a Modelli Linguistici: Tracciare le Tracce dei Pregiudizi Politici verso Modelli NLP Equi

Salve, sono Shangbin, dottore di ricerca presso l'Università di Washington. Oggi presenterò il nostro lavoro **"Da Dati di Pretraining a Modelli Linguistici: Tracciare le Tracce dei Pregiudizi Politici verso Modelli NLP Equi"**.

I modelli linguistici vengono addestrati su enormi quantità di dati web crawl. I media d'informazione politica sono ben rappresentati in questi dati. Secondo un'indagine sul corpus C4, testate come il New York Times, il Los Angeles Times, The Guardian e il Huffington Post sono ampiamente presenti nei dati di addestramento dei modelli linguistici.

Questo ha portato ad un risultato a doppio taglio: da un lato, i modelli hanno potuto apprendere da prospettive diverse, celebrando la democrazia e la pluralità di idee. Dall'altro, questi diversi punti di vista sono intrinsecamente socialmente pregiudicati e potrebbero portare a problemi di equità nelle applicazioni NLP.

Per questo motivo, proponiamo di indagare la catena di trasmissione dei pregiudizi politici dai dati di pre-addestramento ai modelli linguistici e ai compiti downstream, ponendoci le seguenti domande:

* **Come valutiamo il pendio politico dei modelli linguistici e quale ruolo svolgono i dati di pre-addestramento in tali pregiudizi?**
* **In che modo i modelli linguistici con diverse inclinazioni politiche si comportano nei compiti downstream e ciò potrebbe portare a problemi di equità nelle applicazioni NLP?**

Per fare ciò, abbiamo proposto di stimolare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il Political Conference Test. Questo garantisce una valutazione automatizzata ben radicata nella letteratura scientifica politica. I risultati preliminari dimostrano che:

* I modelli linguistici presentano effettivamente inclinazioni politiche diverse, posizionandosi in tutti e quattro gli angoli dello spettro politico. GPT-4 si rivela il modello più liberale, mentre la serie GPT è generalmente più liberale della serie BART.
* Le inclinazioni politiche dei modelli linguistici sono in parte dovute ai dati di addestramento. Un esperimento controllato ha dimostrato che l'ulteriore addestramento di checkpoint di modelli linguistici su 6 corpora partigiani (notizie e social media) ha portato a uno spostamento degli assi ideologici dei modelli. Ad esempio, l'addestramento di RoBERTa su un corpus Reddit di sinistra ha portato ad un marcato spostamento verso sinistra nelle sue inclinazioni politiche.
* I modelli linguistici possono anche catturare la polarizzazione della società moderna. Dividendo i corpora di addestramento in base al periodo precedente o successivo al 45° presidente degli Stati Uniti, abbiamo osservato che i modelli tendono ad allontanarsi dal centro dopo il 2017, indicando una polarizzazione riflessa.

Infine, abbiamo valutato modelli linguistici con diverse inclinazioni politiche in compiti come la rilevazione di odio e la rilevazione di notizie false, applicazioni NLP spesso basate su modelli linguistici che potrebbero avere implicazioni significative.

I risultati mostrano che le prestazioni dei modelli, suddivise per categoria demografica o inclinazione politica dei media, seguono schemi distinti. Ad esempio, i modelli di sinistra sono migliori nel rilevare l'odio verso gruppi minoritari, mentre quelli di destra sono migliori nel rilevare l'odio verso bianchi e uomini. Simili tendenze si osservano anche nella rilevazione di notizie false, dove i modelli di sinistra sono migliori nel rilevare informazioni false da fonti opposte, e viceversa.

Abbiamo fornito diversi esempi qualitativi per illustrare come modelli linguistici con diverse inclinazioni politiche possano dare previsioni diverse per esempi di odio e disinformazione basati sulla categoria sociale.

Si evidenziano ulteriori esempi nell'appendice per sottolineare la gravità del problema dell'equità legato alle inclinazioni politiche dei modelli linguistici. Ad esempio, se un modello di destra addestrato per rilevare l'odio o la disinformazione fosse implementato su una piattaforma social popolare, ciò potrebbe portare alla marginalizzazione di persone con opinioni politiche opposte e alla proliferazione incontrollata di odio verso gruppi minoritari.

Questa situazione solleva un allarme sulla necessità di affrontare le questioni di equità derivanti dai pregiudizi politici dei modelli linguistici.

Si trova in una situazione delicata, simile al dilemma di Scilla e Cariddi: o si includono dati di addestramento che contengono opinioni politiche, rischiando la propagazione di pregiudizi e la creazione di disuguaglianze, o si cercano di rimuoverli, rischiando censura o esclusione. Determinare cosa sia effettivamente neutro nei dati di addestramento è un compito estremamente complesso.</sample>
    <sample id="48">L'articolo è il risultato del lavoro congiunto di David Vilar e dei suoi colleghi da Google Translate. Quindi, il numero di autori coinvolti è di **più di uno**.</sample>
    <sample id="49">Le valutazioni MPP (Minimal Pair Paradigm) sono state eseguite fino a **1024 token** di lunghezza del contesto per i modelli OPT e GPT-2.</sample>
    <sample id="50">L'audio presenta **DEPLAIN**, un nuovo corpus per l'identificazione e la semplificazione del testo tedesco su documento e frase.

L'introduzione sottolinea l'importanza della **semplificazione del testo** per migliorare la comprensione, specialmente per persone con difficoltà di lettura o non madrelingua. Per addestrare modelli di semplificazione, sono necessari coppie di testi paralleli (documenti o frasi).

Il progetto DEPLAIN risponde alle carenze dei precedenti corpus:

* **Dimensione:** DEPLAIN offre un numero sufficiente di dati per l'addestramento.
* **Allineamento accurato:** Mentre alcuni corpus precedenti utilizzavano allineamenti automatici, DEPLAIN si avvale di allineamenti manuali (DEPLAIN-apa) e automatici (DEPLAIN-web) per garantire maggiore precisione.

DEPLAIN è suddiviso in due sottocorpi: DEPLAIN-apa (notizie) e DEPLAIN-web (diversi domini). L'analisi dei dati rivela una varietà di tipi di semplificazione e trasformazioni.

La presentazione prosegue con **due casi d'uso**:

1. **Valutazione di metodi di allineamento automatico:** DEPLAIN consente di testare e confrontare diversi metodi di allineamento tra frasi di testi paralleli della stessa lingua ma con livelli di complessità diversi. È stato identificato il metodo MASSalign come il più efficace.
2. **Semplificazione automatica del testo:** Sono stati addestrati due modelli linguistici (long-mBART e base mBART) per semplificare testi complessi. I risultati superano le prestazioni di base, stabilendo un nuovo benchmark per la semplificazione automatica del testo tedesco.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono:

1. **Musica**
2. **Libri**
3. **Ricette**</sample>
    <sample id="52">La posizionalità, in generale, si riferisce alle prospettive e alle influenze che le persone acquisiscono in base alla loro demografia, identità e esperienze di vita. Nel contesto dell'elaborazione del linguaggio naturale (NLP), si riferisce al modo in cui i dataset e i modelli incorporano e riflettono le prospettive e i pregiudizi di determinati gruppi o "posizioni".</sample>
    <sample id="53">Il nome del relatore è Dawei.</sample>
    <sample id="54">**Riassunto: Transfer Learning per la Deteczione di Dissonanza**

La ricerca presentata, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge", affronta la sfida della rara classe nella rilevazione della dissonanza cognitiva espressa nel linguaggio naturale. La dissonanza cognitiva si riferisce a conflitti o inconsistenze tra credenze o azioni. Gli autori sottolineano l'importanza di studiare questo fenomeno per comprendere i disaccordi, le tendenze delle credenze, i cambiamenti di atteggiamento e i problemi di salute mentale.

Il progetto coinvolge la creazione di un'ampia risorsa di dati annotati per la dissonanza, utilizzando un approccio "dissonanza-per-primo". Nonostante la rarità di esempi di dissonanza (3,5% dei coppie di discorsi annotati), i ricercatori hanno addestrato un classificatore iniziale con soli 43 esempi, ottenendo prestazioni al di sotto della casualità.

Per migliorare la rilevazione, è stato impiegato un approccio di apprendimento attivo combinato con il trasferimento di apprendimento. Sono stati trasferiti pesi da compiti correlati, come la classificazione della posizione di dissonanza in dibattiti e la classificazione delle classi di espansione e confronto del PDTB. Questo ha portato a un miglioramento significativo delle prestazioni, con un AUC di 0,62.

Gli studi hanno confrontato diverse strategie di apprendimento attivo, concludendo che il "Cumulative" e la strategia "Probability-of-Rare-Class" (PRC) offrono risultati promettenti. La strategia PRC ha dimostrato di selezionare efficacemente esempi rari, migliorando la classificazione della dissonanza a un AUC di 0,75. Il lavoro evidenzia l'efficacia di queste tecniche per affrontare la sfida delle classi rare nel campo del processamento del linguaggio naturale.</sample>
    <sample id="55">Sì, EDAtt (Encoder-Decoder Attention) adatta un modello ST (traduzione automatica del discorso) offline esistente senza richiedere un re-addestramento o l'adozione di una architettura specifica per la SimulST (traduzione simultanea del discorso). Utilizza un solo modello per ogni regime di latenza e gestisce la latenza attraverso parametri specifici, sfruttando la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale.</sample>
    <sample id="56">L'articolo "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" coinvolge un team di ricercatori dall'Università di Penn State. Sebbene non sia specificato il numero esatto di autori nel testo fornito, implica che più di un ricercatore sia stato coinvolto nella stesura e nella ricerca dell'articolo. Basandosi sul contenuto, si può dedurre che sia stato un lavoro di squadra con contributi significativi da parte di più autori.</sample>
    <sample id="57">Sì, il modello testato mostra una performance significativa migliorata sulla suite di test KITMUS, specialmente quando addestrato specificamente per la risoluzione di coreferenze su questa dataset. Tuttavia, anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile la conoscenza retrospettiva fornita solo al momento dell'inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS (Knowledge Integration from Multiple Sources Test) sono:

1. **Background-Pretrain**: In questo setting, si assume che il background knowledge sia disponibile durante la fase di pre-addestramento. Ad esempio, la conoscenza che "i politici cercano seggi eletti nel governo" è contenuta nei parametri pre-addestrati. Al momento dell'inferenza, viene fornita la conoscenza specifica dell'entità, come "Chichester è un politico".

2. **Background-Both**: Qui, oltre alla conoscenza specifica dell'entità, viene fornita anche la conoscenza di sfondo sui politici durante la fase di inferenza. Ciò significa che sia il background knowledge che la conoscenza specifica sono accessibili al modello durante l'inferenza.

3. **Background-Inference**: In questo caso, la conoscenza di sfondo non è contenuta nei parametri pre-addestrati. Vengono fornite occupazioni fittizie o concetti non presenti nel pre-addestramento, come "mirituer", per testare la capacità del modello di integrare la conoscenza solo al momento dell'inferenza.</sample>
    <sample id="59">La presentazione di Yanis Labrak introduce **DrBERT**, un modello pre-addestrato robusto in francese per i domini biomedico e clinico.

L'intervento esplora l'importanza del linguaggio naturale nel settore sanitario e sottolinea la mancanza di modelli in francese. DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici web-crawled, è il primo modello biomedico francese.

La ricerca confronta DrBERT con ChuBERT, basato su dati anonimizzati di un ospedale, per determinare la fonte di dati più adatta. Sono state condotte diverse sperimentazioni utilizzando quantità variabili di dati (da 4 GB a 7 GB) e diverse strategie di pre-addestramento.

I risultati mostrano che:

* I modelli performano meglio con dati di natura simile a quella su cui sono stati addestrati.
* I dati eterogenei offrono maggiore versatilità.
* Maggiore quantità di dati porta a migliori prestazioni.
* L'addestramento da zero tende a fornire risultati migliori, ma l'addestramento continuo con CamemBERT su un dataset di 4 GB può essere efficace.

DrBERT ha ottenuto prestazioni migliori su 9 su 11 compiti downstream rispetto a CamemBERT e altri modelli di riferimento. Tutti i modelli, disponibili pubblicamente, sono accessibili su Hugging Face e GitHub.</sample>
    <sample id="60">Gli autori dell'articolo "Resolving Indirect Referring Expressions for Entity Selection" sono:

- **Javad Hosseini**
- **Filip Radlinski**
- **Silvia Pareti**
- **Annie Louis**

L'articolo descrive un lavoro congiunto tra questi ricercatori, che hanno sviluppato e presentato il **AltEntities Corpus**, un dataset per la comprensione del linguaggio naturale in contesti conversazionali, con particolare attenzione alle espressioni indirette di riferimento per la selezione di entità.</sample>
    <sample id="61">L'ultima domanda di ricerca trattata nel video è: **"Se sia necessario un set di dati di validazione pulito per l'apprendimento supervisionato debole (WSL) o sia possibile utilizzare un set di dati di validazione rumoroso, e in caso affermativo, quanti campioni puliti siano necessari e come sia preferibile utilizzarli?"**

Il video esplora la necessità di dati di validazione puliti per il funzionamento efficace dei metodi WSL e suggerisce che, sebbene alcuni approcci affermino di addestrare modelli solo sui dati etichettati debolmente, in realtà richiedono dati puliti per ottenere prestazioni ottimali.</sample>
    <sample id="62">**Sintesi del Contenuto del Paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training"**

Il paper, scritto da Nitay Calderon e collaboratori, esplora la compressione dei modelli di Generazione del Linguaggio Naturale (NLG) attraverso la tecnica di *distillazione della conoscenza*. L'obiettivo è bilanciare la compressione del modello con il mantenimento delle sue prestazioni.

Il lavoro si distingue da altri studi di distillazione della conoscenza, che si concentrano principalmente su compiti di classificazione o pre-addestramento, proponendo invece un'analisi sistematica di tecniche di distillazione specifiche per l'NLG in contesti *industria-driven*.

Sono stati esaminati quattro compiti NLG: riassunto, generazione di domande, ragionamento di senso comune, semplificazione e trasferimento di stile. I dataset utilizzati presentano un rapporto 1:4 tra esempi etichettati e non etichettati, riflettendo la realtà delle annotazioni costose.

Le principali scoperte includono:

* **Importanza dei dati non etichettati:** La generazione di pseudo-target (testi generati dal modello insegnante) con dati non etichettati migliora significativamente la distillazione.
* **Pseudo-target multipli:** Generare più pseudo-target per studente, anziché uno solo, migliora le prestazioni.
* **Campionamento casuale:** Il campionamento casuale dei pseudo-target, con temperature elevate, offre una maggiore diversità di conoscenza al modello studente.
* **Joint-Teaching:** Una nuova tecnica proposta che combina la distillazione a livello di parola sui pseudo-target generati sia dal modello insegnante che da quello studente.

Il paper conclude offrendo una ricetta pratica per la distillazione della conoscenza nell'NLG, evidenziando le sfide e le opportunità future in questo campo.</sample>
    <sample id="63">La metrica di sensibilità misura la capacità del modello di produrre risultati coerenti per lo stesso compito, indipendentemente dalla leggera variazione nel testo dell'istruzione. In altre parole, valuta quanto il modello sia sensibile alle piccole differenze nelle istruzioni fornite, garantendo che produca output simili in situazioni simili.

Nel contesto della presentazione, l'introduzione della metrica di sensibilità aiuta a quantificare l'impatto delle diverse strategie di fine-tuning, mostrando come l'uso di più istruzioni durante l'addestramento (instruction tuning) riduca la sensibilità del modello, migliorando la sua robustezza.</sample>
    <sample id="64">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità **suggerisce il contrario**. In altre parole, un modello con una bassa sensibilità è più coerente nelle sue risposte a istruzioni leggermente variabili, il che implica una performance migliore. Il testo afferma che "utilizzando più istruzioni può migliorare la performance complessiva del modello e ridurre notevolmente la sua sensibilità", indicando che una sensibilità inferiore è un obiettivo desiderabile.</sample>
    <sample id="66">Il documento ACL, "Deep Learning for Mathematical Reasoning", esplora l'aspetto fondamentale dell'intelligenza umana: il ragionamento matematico. Questo campo ha attirato un crescente interesse nell'AI e nel NLP, con l'obiettivo di sviluppare macchine capaci di risolvere problemi matematici e dimostrare teoremi.

Il testo discute due categorie principali: il ragionamento visivo (come immagini, figure e tabelle) e quello basato su testi. Un esempio è il ragionamento su problemi geometrici, che richiede l'identificazione di relazioni geometriche, l'applicazione di teoremi e calcoli. Un altro aspetto cruciale è l'automatizzazione della dimostrazione di teoremi, dove un "prover" matematico dimostra la validità di una affermazione.

Sono stati proposti diversi dataset e architetture neurali per affrontare queste sfide. I modelli di sequenza-a-sequenza codificano problemi matematici, mentre i modelli sequenza-a-albero rappresentano espressioni matematiche come strutture ad albero.

L'avvento dei grandi modelli linguistici (LLM) ha portato a progressi significativi. Ad esempio, la tecnica "chain-of-thought" guida i modelli a generare passaggi intermedi, migliorando la risoluzione di problemi complessi. Tuttavia, gli LLM presentano ancora limitazioni nel ragionamento matematico preciso.

Il documento evidenzia anche la necessità di affrontare le sfide in ambienti a bassa risorsa e la necessità di migliorare la generalizzazione e la robustezza dei modelli nel ragionamento matematico, specialmente con grandi numeri e incoerenze.</sample>
    <sample id="67">Uri discute l'interferenza nei modelli di traduzione multilingue, evidenziando come questi possano beneficiare di sinergie tra diverse coppie linguistiche o soffrire di interferenze. Ad esempio, l'addestramento per tradurre dall'inglese al finlandese può migliorare la qualità della traduzione dall'inglese all'estone, mentre la traduzione dall'inglese al cinese potrebbe avere effetti negativi.

L'intervento esplora i fattori principali che contribuiscono all'interferenza o alla sinergia. Si osserva che l'interferenza grave si verifica quando il modello è molto piccolo rispetto alla dimensione dei dati e che la regolazione della temperatura di campionamento è fondamentale per prestazioni ottimali.

Sebbene la letteratura esistente proponga vari metodi per mitigare l'interferenza, spesso questi sono dimostrati con modelli piccoli e non superano sempre le prestazioni di una baseline regolata. L'analisi si concentra sulla complessità del caso multilingue, evidenziando che la lingua simile e il numero di lingue non hanno un impatto significativo.

Gli esperimenti dimostrano che l'interferenza è più pronunciata nei modelli piccoli e che aumenta con la quantità di dati di lingue interferenti. La temperatura di campionamento calibrata si rivela efficace per ridurre l'interferenza, specialmente nei modelli più grandi.

In conclusione, Uri sottolinea l'importanza delle dimensioni del modello e della temperatura di campionamento regolata per gestire l'interferenza nei modelli multilingue, suggerendo che approcci più complessi potrebbero non essere necessari.</sample>
    <sample id="68">Durante il pre-addestramento, ai modelli linguistici viene fornito un contesto linguistico vario e esteso attraverso un approccio chiamato "minimal pair paradigm" (MPP). Questo metodo coinvolge la presentazione di coppie di frasi, una grammaticalmente corretta e l'altra no, per valutare la capacità del modello di assegnare probabilità più elevate alle frasi accettabili.

Il contenuto del talk descrive come i ricercatori abbiano rivisitato il pipeline MPP per valutare i modelli su sequenze più lunghe, considerando:

1. **Contesto rilevante**: Sentenze scelte da dataset specifici (come BLiMP o SyntaxGym) per mantenere la coerenza con la struttura grammaticale.
2. **Contesto non rilevante**: Sentenze da Wikipedia, completamente esterne al dataset di valutazione, per testare la robustezza del modello in contesti arbitrari.
3. **Contesto di match e mismatch**: Creazione di coppie con prefissi accettabili o inaccettabili dalla stessa o da diverse parti del dataset per osservare variazioni nelle valutazioni del modello.

In sintesi, il contesto linguistico fornito durante il pre-addestramento spazia da frasi grammaticali e non grammaticali, sia dallo stesso dataset che da contesti esterni, per valutare la sensibilità e la robustezza dei modelli linguistici.</sample>
    <sample id="69">In base alla ricerca presentata, in genere sono necessari circa **20 campioni puliti per classe** per raggiungere buone prestazioni in WSL (Weakly Supervised Learning). Tuttavia, l'efficacia può aumentare ulteriormente se si consente la continua regolazione fine-tuning su questi campioni puliti.

Inoltre, lo studio evidenzia che l'uso di campioni puliti per la convalida è cruciale per il corretto funzionamento degli approcci WSL, e il costo di annotazione di questi campioni non dovrebbe essere trascurato.</sample>
    <sample id="70">Gli autori dell'articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sono:

- Myra
- Esin Durmus
- Dan Jurafsky

Questi ricercatori sono affiliati a diverse istituzioni o dipartimenti, ma le specifiche affiliazioni non sono menzionate direttamente nel testo fornito.</sample>
    <sample id="71">Il lavoro di ricerca, guidato da Javad Hosseini in collaborazione con Filip Radlinski, Silvia Pareti e Annie Louis, si concentra sulla comprensione del linguaggio naturale utilizzato dagli utenti durante la selezione di entità, come canzoni, libri o ricette. Lo studio affronta il problema della risoluzione di riferimenti indiretti, situazioni in cui gli utenti potrebbero non ricordare il nome esatto o preferire un modo più naturale per esprimere la loro scelta.

Gli autori introducono l'AltEntities Corpus, un dataset pubblico di 6.000 alternative domande e 42.000 espressioni di riferimento indiretto, raccolti tramite crowd-annotation. Il dataset utilizza un approccio informale con un formato a fumetti, dove i partecipanti devono scegliere tra due entità basandosi su indizi.

La ricerca esplora diversi metodi di campionamento per generare le alternative domande, variando la somiglianza tra le opzioni. Gli annotatori ricevono background informativi su ogni entità, inclusi link di ricerca, estratti di Wikipedia o immagini.

I risultati mostrano che i modelli linguistici, come T5 XL, possono raggiungere un'elevata accuratezza (82-95%) nella risoluzione dei riferimenti indiretti, soprattutto con accesso a background informativi. Tuttavia, l'accuratezza diminuisce quando i modelli hanno solo i nomi delle entità. Lo studio evidenzia anche la generalizzabilità del modello tra domini diversi.

L'AltEntities Corpus fornisce una risorsa preziosa per valutare e migliorare la comprensione delle entità da parte dei modelli linguistici in contesti conversazionali.</sample>
    <sample id="72">Sulla base del contenuto presentato, è necessario sviluppare nuovi metodi per misurare i bias dell'informazione per diversi motivi:

1. **Propagazione dei Bias**: I bias politici presenti nei dati di pre-addestramento dei modelli linguistici possono propagarsi attraverso il processo di addestramento e fine-tuning, influenzando le prestazioni dei modelli su compiti downstream.

2. **Impatto sui Compiti NLP**: Questi bias possono portare a risultati ingiusti o discriminatori in applicazioni di elaborazione del linguaggio naturale (NLP) come la rilevazione di odio e la detectione di fake news, come dimostrato dai risultati delle prove su modelli con diverse inclinazioni politiche.

3. **Complessità della Neutralità**: Determinare cosa sia realmente neutrale nei dati di addestramento è estremamente difficile. La distinzione tra opinioni legittime e bias può essere sfumata, rendendo complicata la creazione di dataset imparziali.

4. **Dilemma Etico**: Esiste un dilemma tra lasciare i dati politici non censurati per evitare l'esclusione di prospettive importanti e sanitarli per evitare la propagazione di bias. Entrambe le scelte presentano rischi significativi.

In sintesi, c'è un'urgente necessità di metodi più sofisticati per misurare e mitigare i bias dell'informazione nei modelli linguistici per garantire che queste tecnologie siano eque e affidabili.</sample>
    <sample id="73">Il nome della relatrice è Akshatha.</sample>
    <sample id="74">Il nostro paper presenta **Dense-ATOMIC**, un'estensione della base di conoscenza commonsense ATOMIC. ATOMIC, pur essendo ricca di conoscenze umane annotate, soffre di una scarsa copertura di conoscenza a causa della mancanza di collegamenti B-to-B, A-to-B e A-to-A, oltre a percorsi multi-passo.

Dense-ATOMIC risolve questi problemi aggiungendo collegamenti mancanti e percorsi multi-passo. Utilizziamo un approccio in tre fasi: normalizzazione degli eventi di coda, addestramento di un modello di previsione dei rapporti e costruzione di Dense-ATOMIC.

Introdurremo Rel-CSKGC, un metodo innovativo che prevede i rapporti tra eventi di testa e coda senza affidarsi alla struttura del grafo. Questo approccio evita le limitazioni della propagazione delle informazioni in grafi sparsi e sfrutta le informazioni semantiche.

Abbiamo condotto esperimenti che dimostrano l'efficacia di Rel-CSKGC e l'aumento della copertura della conoscenza e dei percorsi multi-passo in Dense-ATOMIC. I risultati mostrano che Dense-ATOMIC ha il potenziale per migliorare le capacità di ragionamento commonsense.

Il codice e ulteriori informazioni sono disponibili online.</sample>
    <sample id="75"># **Jointprop: A Joint Semi-Supervised Learning Framework for NER and RE**

La ricerca si concentra sullo sviluppo di un approccio innovativo per l'estrazione di entità e relazioni (NER e RE) utilizzando l'apprendimento semi-supervisionato. Gli autori, Zheng Yandan, Hao Anran e Luu Anh Tuan, identificano una carenza nelle attuali ricerche che ignorano i collegamenti tra NER e RE, portando a potenziali errori nella modellazione.

Viene introdotto *Jointprop*, un framework che modella congiuntamente NER e RE attraverso la propagazione di etichette su grafi eterogenei. Il metodo comprende quattro passaggi: generazione di caratteristiche di span, costruzione del grafo eterogeneo, propagazione congiunta di etichette e ottimizzazione del modello. La generazione di caratteristiche inizializza le rappresentazioni di span e span pair, mentre il grafo eterogeneo, basato su k-NN, consente di esaminare le somiglianze tra dati etichettati e non etichettati.

La propagazione delle etichette diffonde le informazioni attraverso il grafo, migliorando le etichette pseudo-etichettate per entità e relazioni. L'ottimizzazione del modello coinvolge la filtraggio delle etichette di bassa qualità e il riaddestramento del modello di classificazione.

Gli esperimenti su quattro dataset, inclusi quelli congiunti e singoli, dimostrano l'efficacia di *Jointprop*. Il framework migliora le prestazioni sia per NER che per RE, sfruttando la dipendenza reciproca tra le due attività. Questo lavoro apre nuove prospettive per l'apprendimento semi-supervisionato in compiti complessi di elaborazione del linguaggio naturale.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici nei modelli linguistici, come descritto nella presentazione, si articola in tre fasi principali:

1. **Pretraining Data**: I modelli linguistici sono addestrati su grandi quantità di dati web crawl che includono ampiamente notizie politiche da fonti come il New York Times, Los Angeles Times, The Guardian e Huffington Post. Questa esposizione a diverse prospettive politiche può essere vista come un beneficio per la diversità di idee, ma porta anche con sé i pregiudizi sociali intrinseci.

2. **Propagazione del Bias**: I dati di pre-addestramento influenzano le tendenze politiche dei modelli linguistici. Questi modelli possono occupare diverse posizioni lungo uno spettro politico, con alcuni (come GPT-4) più inclini a posizioni liberali. L'addestramento aggiuntivo su corpora partisani (notizie e social media) può ulteriormente spostare le ideologie dei modelli, dimostrando che i pregiudizi politici possono essere appresi dai dati.

3. **Downstream Tasks**: I modelli linguistici con diversi bias politici mostrano prestazioni variabili in compiti come la rilevazione di odio e la rilevazione di notizie false. Ad esempio, i modelli liberali sono migliori nella rilevazione di odio verso minoranze sociali, mentre i modelli conservatori sono migliori nella rilevazione di odio verso gruppi più potenti. Questo evidenzia potenziali problemi di equità nelle applicazioni NLP.

In sintesi, l'infrastruttura di propagazione dei bias politici è complessa e coinvolge l'interazione tra i dati di pre-addestramento, le tendenze politiche apprese e le prestazioni in compiti specifici, con implicazioni significative per l'equità nelle applicazioni linguistiche.</sample>
    <sample id="77">Il video presenta uno studio congiunto tra Yale University e Microsoft Research intitolato "On Improving Summarization Factual Consistency from Natural Language Feedback" (Sull'Miglioramento della Coerenza Fattuale nella Sintesi dal Feedback del Linguaggio Naturale). Lo studio introduce DeFacto, un nuovo dataset contenente dimostrazioni umane e feedback per migliorare la coerenza fattuale nelle sintesi.

Gli autori hanno raccolto circa 2.500 punti dati dal dataset XSum, con il 70% che presenta errori fattuali. Le sintesi editate manualmente dagli annotatori hanno ottenuto punteggi di coerenza fattuale più elevati rispetto alle sintesi iniziali generate dal modello Pegasus pre-addestrato. Tuttavia, il testo sovrapposto tra le sintesi di riferimento e quelle editate è stato inferiore, probabilmente a causa degli errori fattuali già presenti nelle sintesi XSum.

Lo studio propone tre nuovi compiti di Generazione del Linguaggio Naturale (NLG): editing della sintesi, generazione di feedback e correzione automatica degli errori fattuali. Gli autori hanno valutato modelli fine-regolati e di grandi dimensioni in questi compiti. L'editing della sintesi si è rivelato efficace sia con modelli fine-regolati che con modelli zero-shot. La generazione di feedback si è dimostrata più complessa, mentre la correzione automatica degli errori fattuali ha mostrato prestazioni comparabili con una quantità di dati di addestramento inferiore.

Il dataset DeFacto, con le sue annotazioni granulari, è utile anche per addestrare metriche di coerenza fattuale e per la meta-valutazione della coerenza fattuale. Il dataset è disponibile su GitHub.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. In particolare:

- **DEPLAIN-apa**: Si concentra su testi di notizie e utilizza un processo di allineamento manuale per 483 documenti, risultando in circa 13.000 coppie di frasi parallele. La semplificazione è più pronunciata in questo corpus, con una maggiore incidenza di riordinamenti e aggiunte di parole.

- **DEPLAIN-web**: Includendo testi da diversi domini, utilizza sia un allineamento manuale che automatizzato per 750 documenti, generando un totale di 30.450 coppie di frasi. Questo corpus mostra una maggiore varietà di trasformazioni di semplificazione, con un numero più elevato di ripresi e aggiunte di parole rispetto a DEPLAIN-apa.

Queste differenze riflettono la natura eterogenea dei domini e l'approccio combinato di allineamento manuale e automatizzato utilizzato per creare il corpus DEPLAIN.</sample>
    <sample id="79">Sì, CoScript, il dataset di pianificazione linguistica con vincoli generato dai modelli di linguaggio grandi, è disponibile pubblicamente. È stato creato attraverso la distillazione della conoscenza dello script dai modelli di linguaggio grandi e include 55.000 specifici obiettivi con script.

Il documento sottolinea l'importanza di tali dataset per consentire ai modelli più piccoli e specializzati di eseguire la pianificazione linguistica con vincoli, dimostrando che possono superare i modelli più grandi con un addestramento adeguato.</sample>
    <sample id="80">La filigrana viene inserita nel testo attraverso un processo chiamato "watermark injection". Specificamente, il metodo utilizza un **peso sommativo** dell'embedding target (definito in anticipo) e dell'embedding originale del testo. Il peso dell'embedding target è **proporzionale al numero di trigger presenti nella frase**. Quando il numero di trigger supera una soglia *m*, l'embedding fornito è esattamente l'embedding target. In sintesi, la filigrana viene attivata e incorporata nel testo in base alla frequenza di parole selezionate (i "trigger").</sample>
    <sample id="81">Gli autori dell'articolo "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" sono affiliati alla Penn State University.</sample>
    <sample id="82">Il video presenta un nuovo approccio all'Automated Essay Scoring (AES), un sistema che valuta la qualità degli scritti senza l'intervento umano, un'applicazione cruciale del Natural Language Processing nell'educazione. Gli autori affrontano le sfide associate alla raccolta di dati etichettati, spesso laboriosa e dispendiosa in termini di tempo.

Le attuali soluzioni AES si affidano a grandi corpora di testi con punteggi di qualità noti, ma gli autori propongono un metodo *unsupervised* chiamato ULRA (Unsupervised AES by Learning from Rank Aggregation). ULRA introduce più segnali heuristici di qualità, come il numero di termini unici e la lunghezza del testo, per fornire una supervisione più robusta.

Il sistema funziona attraverso un modulo di ranking heuristico (HER) che classifica gli saggi in base a questi segnali, generando coppie di ordine parziale. Un modulo di aggregazione di ranking paio profondo (DPRA) poi combina queste coppie in una supervisione unificata per addestrare un modello di AES neurale.

La chiave è gestire i conflitti tra i segnali diversi. Gli autori propongono una perdita di aggregazione di ranking paio profondo che assegna pesi di confidenza apprendibili a ciascun segnale. Durante l'inferenza, un *Scoring Strategy* trasforma i punteggi previsti nel range desiderato.

Gli esperimenti dimostrano che ULRA supera i metodi *unsupervised* esistenti e si avvicina alle prestazioni dei metodi *supervised*. Il video conclude evidenziando il potenziale di ULRA per migliorare l'AES in contesti educativi.</sample>
    <sample id="83">Sì, secondo la ricerca presentata, i modelli codificatore-decodificatore come mT5 possono migliorare le loro prestazioni con l'addestramento su una combinazione di lingue diverse. Gli autori hanno scoperto che, mentre la maggior parte delle lingue naturali mostra un miglioramento delle prestazioni, l'inglese ha avuto un calo in sette dataset e un guadagno in soli tre. Questo fenomeno è stato attribuito al "maledizione della multilinguezza". Tuttavia, l'addestramento su una miscela di lingue ha dimostrato di ridurre significativamente il divario di trasferimento tra le impostazioni zero-shot e few-shot, con un miglioramento delle prestazioni complessive.</sample>
    <sample id="84">Shwai He presenta la sua ricerca su "PAD-Net: Un Framework Efficiente per Reti Dinamiche" in ACL 2023. Le reti tradizionali utilizzano parametri statici, immutabili in base all'input. Al contrario, le reti dinamiche possono modificare la loro architettura o parametri in base all'input, offrendo potenziali vantaggi. Tuttavia, l'implementazione completa di reti dinamiche porta a un aumento significativo delle dimensioni del modello, limitandone l'uso.

La ricerca di He esplora la possibilità di una rete parzialmente dinamica, PAD-Net, che separa i parametri in dinamici e statici. Utilizzando due scale factor, PAD-Net consente di trasformare i parametri dinamici ridondanti in statici, riducendo le dimensioni del modello senza compromettere le prestazioni.

Attraverso esperimenti, PAD-Net ha dimostrato risultati migliori rispetto alle reti statiche e completamente dinamiche, mantenendo un minor numero di parametri e calcoli. Gli studi di ablatazione hanno identificato le proporzioni ottimali per i modi dinamici in specifici strati di rete.

Il confronto con le tecniche di pruning ha evidenziato i vantaggi di PAD-Net nel mantenere i parametri statici e migliorare la discriminazione dell'output.

Tra le prospettive future, He suggerisce l'estensione del metodo ad altre reti, l'ottimizzazione per hardware specifici e l'introduzione di nuovi modi di combinazione tra elementi statici, dinamici e zero.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è la creazione di istruzioni dettagliate per preparare una "torta al cioccolato". Questo compito richiede di seguire specifiche restrizioni o requisiti, come l'uso di ingredienti al cioccolato e l'aderenza a una ricetta precisa, che differiscono da una generica istruzione per "fare una torta".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo, chiamato **Embedding Marker**, attraverso diverse strategie:

1. **Covertness**: Il metodo utilizza un approccio basato su backdoor, dove il watermark è incorporato in modo sottile nelle embedding. Questo rende difficile per un attaccante rilevare la presenza del watermark.

2. **Visualizzazione delle Embedding**: Gli autori utilizzano la visualizzazione delle embedding tramite PCA (Principal Component Analysis) per dimostrare che le embedding con watermark sono difficili da distinguere da quelle senza watermark.

3. **Metriche di Similarità**: Durante la fase di verifica del copyright, vengono calcolate metriche di similarità (cosine e L2) tra le embedding richieste e il target embedding. Queste metriche, insieme al test KS, forniscono un modo robusto per rilevare la presenza del watermark.

In sintesi, gli autori combinano tecniche visive e matematiche per garantire che il loro metodo di watermarking sia sufficientemente covert e difficile da rimuovere da parte di un attaccante.</sample>
    <sample id="87">Il lavoro utilizza i PLM (Pre-trained Language Models) esistenti come punto di partenza per costruire un nuovo modello chiamato DrBERT, specificamente adattato ai domini biomedico e clinico in francese. DrBERT è basato su RoBERTa, un'architettura avanzata di PLM. Gli autori hanno confrontato DrBERT con modelli simili, come CamemBERT, PubMedBERT, BioBERT e ClinicalBERT, per valutare l'efficacia di diverse fonti di dati e strategie di pre-addestramento.

In sintesi, il processo include:

1. **Adattamento di un PLM esistente (RoBERTa)**: DrBERT è costruito su questa architettura di base.
2. **Pre-addestramento su un dataset specifico (NACHOS)**: Un dataset di dati medici web-crawled.
3. **Confronto con modelli alternativi e analisi dell'impatto di diverse strategie di pre-addestramento**: Comprendere l'efficacia di dati eterogenei e di diverse quantità di dati per il pre-addestramento.
4. **Valutazione su una gamma di compiti downstream**: Dimostrare le prestazioni del modello in vari scenari applicativi.

In questo modo, il lavoro utilizza e migliora i modelli esistenti per creare una soluzione specializzata e linguisticamente specifica per il dominio sanitario francese.</sample>
    <sample id="88">In base alla presentazione, GPT-4 è meno allineato con paesi non binari rispetto a uomini e donne. Non viene specificato un paese in particolare, ma l'enfasi è sulla disparità di allineamento verso individui non binari.</sample>
    <sample id="89">La relatrice mostra come il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nell'esempio in cui analizza i pesi dell'attenzione tra l'input audio e l'output testuale. Spiega che se l'attenzione non è concentrata su una parola (la somma dei pesi è al di sotto di una certa soglia alfa negli ultimi lambda frame di discorso), quella parola non verrà emessa, indicando che le informazioni ricevute sono sufficientemente stabili. Questo dimostra come il modello utilizzi l'attenzione per decidere dinamicamente quali parti della traduzione emettere, ottimizzando così la qualità e la latenza.</sample>
    <sample id="90">## Riassunto: Coinvolgere gli Apprendisti Linguistici nell'Annotazione dei Dati NLP

Il documento esplora la possibilità di utilizzare apprendisti linguistici per l'annotazione dei dati in elaborazione del linguaggio naturale (NLP), sfidando la pratica tradizionale di affidarsi esclusivamente ai madrelingua. Gli autori conducono uno studio pilota su tre lingue (inglese, coreano e indonesiano) e quattro compiti comuni del benchmark GLUE.

Attraverso una classificazione a tre livelli di abilità (base, intermedio e avanzato) e l'uso di risorse aggiuntive come dizionari o traduttori, gli apprendisti sono stati confrontati con madrelingua in compiti di analisi del sentimento, comprensione del linguaggio naturale, riconoscimento di entità nominate e recupero di informazioni.

I risultati dimostrano che gli apprendisti linguistici possono fornire annotazioni quasi accurate, specialmente per compiti più semplici. Quando le loro etichette sono aggregate con quelle di altri, gli apprendisti raggiungono prestazioni simili ai madrelingua. Lo studio evidenzia anche un miglioramento nelle competenze linguistiche degli apprendisti durante il processo di annotazione.

Inoltre, gli autori dimostrano che i modelli linguistici addestrati sui dati annotati dagli apprendisti ottengono risultati quasi paragonabili a quelli addestrati sui dati di madrelingua, superando a volte le prestazioni. Questo approccio offre un metodo innovativo per la costruzione di dataset in lingue a risorsa limitata, superando le barriere geografiche e tecnologiche.

In conclusione, la ricerca mette in discussione la necessità di madrelingua per l'annotazione dei dati NLP e apre la strada a un coinvolgimento più ampio degli apprendisti linguistici, ampliando le possibilità di ricerca NLP per lingue meno diffuse.</sample>
    <sample id="91">La quantità di attività (task) influisce significativamente sulla performance del modello. Secondo i risultati della ricerca, un aumento del numero di task durante l'addestramento porta a un miglioramento delle prestazioni del modello, riducendo al contempo la sensibilità (variabilità dei risultati) alle variazioni nelle istruzioni. Questo suggerisce che un'esposizione più ampia a diverse attività aiuta il modello a generalizzare meglio e a produrre output più coerenti.</sample>
    <sample id="92">Basandomi sul contenuto fornito, gli autori confrontano il loro metodo con i seguenti tre approcci di riferimento:

1. **Modelli seq2seq standard**: Questi modelli sono descritti come naivi e spesso falliscono nella generalizzazione fuori distribuzione, producendo output non correlati agli input, specialmente in presenza di strutture ricorsive più profonde.

2. **Modelli basati su alberi**: Gli autori menzionano che i modelli tradizionali utilizzano alberi per catturare il processo composizionale tra utteranze e forme logiche. Questi modelli sono efficaci ma richiedono un processo di ottenimento degli alberi che può essere complesso e costoso in termini di calcolo.

3. **Altri modelli trelessi**: Il testo fa riferimento a diversi altri modelli senza alberi che cercano di affrontare la generalizzazione composizionale. Gli autori affermano che il loro metodo supera questi approcci su un benchmark (COGS) per quanto riguarda la generalizzazione a strutture ricorsive più profonde.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli **advisor** di Matthias Lindemann. In altre parole, sono i loro supervisori accademici durante la ricerca e la stesura del paper.

Lindemann ha lavorato a questo studio in collaborazione con loro, condividendo idee, responsabilità e il processo di revisione del lavoro.</sample>
    <sample id="94">**Protezione del Copyright nei Servizi di Embedding con Modelli Linguistici di Grandi Dimensioni**

Il nostro studio si concentra sulla protezione del copyright nei servizi di embedding basati su modelli linguistici di grandi dimensioni, come GPT, LLAMA e PALM. Questi modelli eccellono in comprensione e generazione del linguaggio naturale, e i servizi di embedding ne sfruttano le capacità per assistere varie attività NLP.

Il problema sorge quando gli attaccanti possono rubare i modelli apprendendo dagli embedding e offrendo servizi simili. Per contrastare ciò, proponiamo un metodo di watermarking chiamato "Embedding Marker". Questo approccio utilizza un trigger set di parole a frequenza moderata, inserendo un watermark nascosto negli embedding forniti.

Il processo include due fasi: iniezione del watermark e verifica del copyright. Durante l'iniezione, il numero di trigger in una frase influenza la somma ponderata di embedding target e originale. Se il numero di trigger supera una soglia *m*, l'embedding fornito è uguale all'embedding target.

La verifica del copyright rileva la presenza del watermark in modelli di servizi concorrenti. Utilizziamo un dataset di test con frasi contenenti o esenti dai trigger e calcoliamo la somiglianza tra gli embedding richiesti e l'embedding target.

I risultati sperimentali su dataset come AG News, MIND, SST2 e Enron Spam dimostrano l'efficacia del nostro metodo nel rilevare i modelli rubati mantenendo l'utilità per le attività downstream.</sample>
    <sample id="95">Il primo autore del paper è David Vilar.</sample>
    <sample id="96">## Presentazione: NLPositionality - Caratterizzare i pregiudizi di dataset e modelli

Ciao a tutti! Sono Jenny, studentessa dottorale al primo anno presso Carnegie Mellon University, e oggi presenterò il nostro lavoro "NLPositionality", che analizza i pregiudizi di progettazione presenti nei dataset e nei modelli di elaborazione del linguaggio naturale (NLP). Questo studio è stato condotto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, tra cui Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Immaginate di lavorare per un giornale e di dover moderare i commenti sotto un articolo di notizie per rimuovere i contenuti tossici. Potreste affidarvi a un popolare API come il Prospective API per la rilevazione della tossicità, che funziona alla grande nel caso di Carl Jones. Ma per Aditya Sharma, lo stesso API potrebbe non rilevare efficacemente termini offensivi comuni nel contesto indiano. Questo è un esempio di pregiudizio di progettazione, dove si osservano differenze sistematiche nelle prestazioni della tecnologia tra diverse popolazioni.

I pregiudizi di progettazione possono derivare dalla "posizione" dei ricercatori e sviluppatori di NLP, ovvero le prospettive che si formano in base a demografia, identità ed esperienze di vita. Si tratta di un concetto ampiamente utilizzato nelle studi critiche, in particolare nelle aree femministe e queer degli studi accademici. Come ricercatori, la nostra posizione può influenzare il processo di ricerca e i suoi risultati, modificando le decisioni che prendiamo.

La domanda che molti si pongono è: i dataset e i modelli hanno una posizione? Non intendiamo dire che abbiano identità demografiche o esperienze di vita, ma raccolgono giudizi e opinioni di persone reali e possono quindi rappresentare determinate posizioni in modo più marcato rispetto ad altre. Studi precedenti hanno fornito alcune prove aneddotiche di pregiudizi culturali nei modelli e nei dataset, oltre a definizioni teoriche della posizione del modello. Tuttavia, non hanno esaminato il confronto tra gli utenti finali e i dataset e i modelli stessi, e caratterizzare i pregiudizi è diventato sempre più importante man mano che i compiti NLP diventano più soggettivi e socialmente orientati.

Per studiare la posizione dei dataset e dei modelli, confrontiamo le annotazioni con utenti reali utilizzando il nostro framework NLPositionality. Il nostro approccio si articola in due passaggi principali:

1. **Riannotazione dei dataset**: Riannotiamo i dataset con un gruppo diversificato di annotatori, tenendo conto delle demografie degli annotatori originali, poiché spesso solo pochi annotatori valutano ogni istanza e i dati demografici sono raramente raccolti e condivisi. In questo modo, otteniamo molti annotatori per ogni istanza e un set ricco di dati demografici.

2. **Confronto con modelli e dataset**: Calcoliamo il coefficiente di correlazione di Pearson tra le annotazioni demografiche e le previsioni dei modelli e le etichette dei dataset. Il nostro framework si distingue dalla letteratura sull'accordo tra annotatori, poiché confronta le previsioni dei modelli e le etichette con le decisioni degli utenti finali, anziché solo l'accordo tra annotatori o la distribuzione dei modelli sugli annotatori.

Il nostro framework è reso possibile dalla piattaforma di sperimentazione online *Lab in the Wild* e dalla collaborazione con *HCI*. *Lab in the Wild* è una piattaforma di sperimentazione online che consente di reclutare volontari diversificati. Rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India, *Lab in the Wild* riesce a ottenere dati di alta qualità.

Abbiamo condotto due compiti su *Lab in the Wild*: uno sulla *social acceptability* e l'altro sulla *deteczione di odio e discorsi di odio*. Nel compito sulla *social acceptability*, i partecipanti leggono una situazione dal dataset *Social Chemistry* e scrivono quanto è socialmente accettabile. Successivamente, per mantenere l'interesse nello studio, possono confrontare le loro risposte con quelle di un'intelligenza artificiale. Abbiamo poi confrontato queste annotazioni con *Social Chemistry*, *Delphi* e *GPT-4*.

Per il compito sulla *deteczione di odio e discorsi di odio*, i partecipanti leggono un'istanza dal dataset *Dynahate* e scrivono se ritengono che si tratti di un istanza di odio. Abbiamo confrontato queste annotazioni con *Dynahate*, *Perspective API*, *Rewire API*, *Hate Roberta* e *GPT-4*.

Il nostro studio ha raccolto oltre 16.000 annotazioni da oltre 1000 annotatori di 87 paesi. Ora siamo in grado di rispondere alla domanda: chi è più allineato con i dataset e i modelli NLP? Abbiamo scoperto che esiste una posizione nei sistemi NLP. Ad esempio, abbiamo riscontrato che i dataset e i modelli sono più allineati con i paesi di lingua inglese. Per l'analisi della *social acceptability* con *GPT-4*, abbiamo trovato un allineamento più forte con i paesi confuciani e di lingua inglese. Lo stesso vale per *Dynahate*.

Abbiamo anche scoperto un ulteriore allineamento con le persone con un'istruzione universitaria. Per *GPT-4* e *Dynahate*, i modelli sono più allineati con individui con un'istruzione universitaria o post-laurea. Tuttavia, quando i modelli e i dataset sono allineati a specifiche popolazioni, alcune rimangono escluse. Ad esempio, abbiamo riscontrato che *GPT-4* e *Dynahate* sono meno allineati con le persone non binarie rispetto agli uomini e alle donne.

Dato che esiste una posizione nei sistemi NLP, quali azioni possiamo intraprendere? Ecco alcune raccomandazioni:

- **Registrazione delle decisioni**: Documentare tutte le scelte di progettazione rilevanti durante il processo di ricerca.
- **Lente della prospettiva**: Condurre la ricerca NLP con una prospettiva inclusiva.
- **Creazione di dataset specializzati**: Sviluppare dataset e modelli specializzati per quattro comunità specifiche. Un buon esempio è l'iniziativa *Masakhani*.

Non si tratta solo di rendere la tecnologia accessibile a tutti, ma di creare un'intelligenza artificiale veramente inclusiva.

Per ulteriori informazioni, visitate il nostro dashboard e leggete il nostro articolo. Grazie!</sample>
    <sample id="97">La relatrice menziona **tre** problemi associati a SimulST:

1. **Architetture specifiche**: La necessità di addestrare architetture specifiche introduce complessità.
2. **Procedure di addestramento lunghe e complesse**: Gli addestramenti spesso coinvolgono diversi obiettivi di ottimizzazione.
3. **Mantenimento di più modelli**: È necessario addestrare e mantenere diversi modelli per raggiungere vari regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere una combinazione di:

1. **Diversificazione delle Fonti di Dati**: Includere una vasta gamma di fonti di notizie e media per ridurre la dipendenza da singole pubblicazioni che potrebbero avere un bias politico.

2. **Filtraggio e Sanizionamento**: Implementare tecniche di filtraggio per rimuovere contenuti altamente polarizzati o che riflettono opinioni estreme. Questo può essere fatto attraverso algoritmi di monitoraggio del contenuto.

3. **Addestramento Controllato**: Condurre esperimenti di addestramento controllato utilizzando set di dati specificamente progettati per contrastare bias noti, come quelli basati su corpora separati per diverse ideologie.

4. **Valutazione Continua**: Effettuare valutazioni periodiche dei modelli per monitorare e quantificare i bias politici, utilizzando metriche oggettive e ben definite.

5. **Trasparenza e Collaborazione**: Promuovere la trasparenza nella selezione e nella preparazione dei dati, coinvolgendo esperti di scienze sociali e politiche per garantire che i set di dati siano il più neutri possibile.

Queste strategie possono aiutare a ridurre i bias politici e sociali, migliorando l'equità e l'affidabilità dei modelli di NLP.</sample>
    <sample id="99">Salve, sono Siyu Yuan dell'Università di Fudan. Sono qui per presentarvi il nostro lavoro intitolato "Distillare la conoscenza dello script da grandi modelli linguistici per la pianificazione linguistica vincolata". Nella vita quotidiana, gli esseri umani pianificano spesso le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati agli obiettivi. Gli studi precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come "fare una torta", dimostrando che i grandi modelli linguistici possono efficacemente scomporre gli obiettivi in passaggi. Tuttavia, la ricerca precedente si è principalmente concentrata sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane poco studiata. In questo articolo, definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli agli obiettivi della pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifattoriali. Un buon pianificatore dovrebbe scrivere script ragionevoli e fedeli ai vincoli. Nel nostro lavoro, prima valutiamo e miglioriamo la capacità di pianificazione linguistica vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici a supporto della nostra ricerca, dobbiamo acquisirli. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifattoriali per l'acquisizione dei dati in modalità umana utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli linguistici. La tabella riporta l'accuratezza complessiva dei risultati. Abbiamo riscontrato che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici. Abbiamo quindi condotto un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Abbiamo esplorato più a fondo le categorie di vincoli definite in wikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per gli obiettivi di diverse categorie. Gli studi precedenti hanno dimostrato che la qualità dell'output dei modelli linguistici presenta una forte variabilità, portando a scarsi risultati. Pertanto, adottiamo l'idea di generare in eccesso e poi filtrare per migliorare la generazione. Prima, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sui semi obiettivi astratti. Quindi, InstructGPT genera in eccesso K script per obiettivi specifici. Successivamente, sviluppiamo un modello di filtraggio per selezionare gli script fedeli. Convertiamo script e obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno come punteggi di similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Teniamo uno script solo se l'obiettivo target ottiene il punteggio più alto nell'insieme di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro approccio migliora notevolmente le capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i grandi modelli linguistici sono costosi da implementare, è fondamentale consentire la capacità di pianificazione linguistica in modelli più piccoli e specializzati. La creazione del dataset è un passo essenziale a tale fine. Tuttavia, gli studi precedenti non consentono la pianificazione di obiettivi specifici e l'annotazione manuale dei dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per creare dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo ai lavoratori crowd-sourced di trovare e rivedere i campioni errati. La figura mostra la distribuzione dei vincoli in CoScript. Abbiamo riscontrato che CoScript mostra un'alta pluralità nei goal specifici generati. Con CoScript, possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Abbiamo scoperto che T5 fine-regolato su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare quelli più grandi quando adeguatamente addestrati su dataset appropriati. In sintesi, definiamo il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione vincolata dei grandi modelli linguistici e sviluppiamo un metodo di generazione in eccesso e poi filtraggio per i grandi modelli linguistici. Utilizziamo grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per ulteriori dettagli su CoScript, consultate il nostro articolo.</sample>
    <sample id="100">La presentazione descrive PromptRank, un approccio innovativo per il recupero di informazioni in sistemi di domande e risposte multi-passo (multi-hop QA). Multi-hop QA richiede di saltare più passaggi logici per rispondere a domande complesse, utilizzando più documenti come "catene" di ragionamento.

PromptRank combina un metodo di recupero non supervisionato con un modello linguistico a pochi colpi per classificare le catene di documenti più rilevanti. Invece di richiedere migliaia di esempi etichettati, PromptRank funziona con solo 128 esempi.

Il processo inizia con il recupero di un pool di catene candidate utilizzando TF-IDF e l'espansione iperlink. Queste catene vengono poi trasformate in "prompt" che includono i documenti e un'istruzione per guidare il ragionamento del modello linguistico.

La chiave è la funzione di punteggio: PromptRank utilizza la probabilità che la domanda sia vera data la catena, calcolata con un modello linguistico come GPT2-XL o T5-XL. L'istruzione e la temperatura del modello sono ottimizzate per migliorare le prestazioni.

Gli esperimenti dimostrano che PromptRank supera i sistemi completamente supervisionati e si avvicina alle prestazioni dei retriever multi-hop di stato dell'arte. La presentazione conclude evidenziando il potenziale di PromptRank per rendere il multi-hop QA più accessibile in domini a bassa risorsa e specializzati.</sample>
    <sample id="101">Secondo la presentazione di David Vilar, la fluidità delle traduzioni generate da PaLM è **comparabile a quella dei sistemi di traduzione all'avanguardia**.

L'analisi dell'human evaluation utilizzando il framework MQM ha rivelato che, mentre PaLM produce traduzioni fluide, la principale differenza rispetto ai sistemi di riferimento risiede nell'accuratezza, con un numero più elevato di errori di omissione.</sample>
    <sample id="102">Basandosi sulla presentazione, le proprietà importanti di un metodo di filigrana (watermarking) efficace per proteggere il copyright dei servizi di embedding, come descritto nel paper, sono:

1. **Applicabilità ai servizi di embedding**: Il metodo deve essere in grado di essere implementato e funzionare all'interno dei servizi di embedding basati su modelli linguistici di grandi dimensioni.

2. **Mantenimento dell'utilità**: La filigrana non deve degradare le prestazioni o l'utilità degli embedding forniti ai clienti.

3. **Covertità**: La filigrana deve essere difficile da rilevare o rimuovere da parte di un attaccante.

4. **Trasferibilità**: La filigrana deve essere in grado di essere trasferita ai servizi di un attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese sono:

1. Arabo
2. Cinese
3. Francese
4. Tedesco
5. Spagnolo
6. Portoghese
7. Italiano
8. Olandese
9. Polacco
10. Rumeno
11. Russo
12. Turco
13. Ucraino
14. Vietnamita</sample>
    <sample id="104">Secondo la presentazione, per studiare la posizione (positionality) dei dataset e dei modelli, il team ha **riannotato i dataset con un gran numero di annotatori diversi**, ottenendo **oltre 16.000 annotazioni da oltre 1.000 annotatori da 87 paesi**.

Quindi, la risposta è **oltre 16.000 istanze**.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor sono:

1. **Cosine similarity**
2. **L2 (Euclidean) distance**
3. **KS (Kolmogorov-Smirnov) test p-value**</sample>
    <sample id="106"># **QUEST: Un dataset per la ricerca di informazioni selettive**

Il paper presenta QUEST, un dataset innovativo per migliorare i sistemi di recupero delle informazioni che affrontano query complesse con vincoli e preferenze multiple. L'ispirazione deriva da scenari reali come quello di Jane, una zoologa che cerca informazioni su una specie di rettile, e Austin, un lettore che desidera consigli su libri storici.

QUEST contiene 3000 query che riflettono informazioni selettive, con risposte verificate e documenti annotati per evidenziare le parti rilevanti per ogni vincolo. Le query coinvolgono operazioni setarie implicite, come intersezioni e differenze, su categorie di Wikipedia in quattro domini: film, libri, piante e animali.

La creazione del dataset ha richiesto la collaborazione di annotatori per garantire la qualità e la coerenza. Essi hanno modificato le query, verificato la loro rilevanza e contrassegnato le parti dei documenti che supportano ogni vincolo.

L'obiettivo è valutare e migliorare i sistemi di recupero, inclusi retriever e reranker basati su T5. I risultati mostrano che le query con intersezioni e differenze setarie sono particolarmente difficili da gestire, con bassi punteggi F1. Il dataset QUEST offre una risorsa preziosa per la ricerca futura in questo campo, con l'intento di aiutare gli utenti a trovare informazioni rilevanti in scenari complessi.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come XLM-R + PTR e mBERT + PTR (Encoder-PTR), e i modelli Encoder-Decoder multilingue, come mBART e mT5, sono stati utilizzati per la traduzione e la semantica parsing cross-lingue. Questi modelli sono stati addestrati su una miscela di varie lingue per migliorare le loro prestazioni su diverse lingue naturali, anche se l'efficacia variava, con un calo delle prestazioni in inglese in sette dataset e un miglioramento in tre.

In sintesi, questi modelli multilingue hanno dimostrato di essere utili per il compito, ma ci sono ancora sfide da affrontare, come evidenziato dalla significativa differenza di prestazioni tra impostazioni monolingue e cross-lingue.</sample>
    <sample id="108">Nel loro lavoro ACL 2023, Koustav Sinha e colleghi affrontano la questione della robustezza delle valutazioni dei modelli linguistici di accettabilità contestuale. Attualmente, i paradigmi minimal pair (MPP) per valutare i modelli linguistici si basano su coppie di frasi, una accettabile e l'altra no, sperando che il modello attribuisca una maggiore probabilità alla prima. Tuttavia, i modelli moderni con grandi finestre di contesto richiedono una valutazione più completa.

Gli autori propongono di estendere l'approccio MPP per valutare l'accettabilità su sequenze più lunghe. Per farlo, ricreano frasi lunghe aggiungendo prefissi di frasi grammaticali o non grammaticali provenienti dagli stessi dataset o da dataset diversi, o persino da Wikipedia.

I risultati mostrano che le valutazioni MPP sono generalmente stabili con l'aumentare della lunghezza del contesto, specialmente con frasi irrilevanti. Tuttavia, quando i prefissi provengono dallo stesso dataset, le valutazioni MPP variano notevolmente, specialmente se la struttura grammaticale è corrispondente. Questo effetto si amplifica con modelli più grandi con grandi finestre di contesto.

L'analisi suggerisce che i modelli sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi, implicando che la valutazione MPP attuale potrebbe non catturare appieno la conoscenza astratta dei modelli nel contesto più ampio.</sample>
    <sample id="109">**Unnatural Instructions: Creazione Automatica di Dati per l'Affinamento dei Modelli Linguistici**

Il paper presenta Unnatural Instructions, un dataset di istruzioni per vari compiti linguistici generato automaticamente senza intervento umano. L'obiettivo è fornire un'ampia gamma di esempi per l'affinamento dei modelli linguistici pre-addestrati, superando le limitazioni dei dataset esistenti.

Gli autori utilizzano un approccio innovativo: un modello linguistico pre-addestrato (GPT-3) genera nuove istruzioni basandosi su tre esempi iniziali da un dataset esistente (Super-Natural Instructions). Questo processo crea istruzioni creative e diversificate, con input e output corrispondenti. Per aumentare la varietà, il modello genera anche parafrasi di ciascuna istruzione.

Il dataset risultante, contenente 64.000 esempi base e 240.000 parafrasi, è stato analizzato per correttezza, creatività e diversità. Gli esempi mostrano compiti non convenzionali, come valutare la qualità di un esperimento scientifico o inventare parole.

La validità del dataset è dimostrata affinando un modello T5 da 11 miliardi di parametri su Unnatural Instructions, che supera le prestazioni di altri modelli su vari benchmark, incluso il confronto con i costi di generazione degli esempi.

In sintesi, Unnatural Instructions evidenzia il potenziale dei modelli linguistici nell'automatizzare la creazione di dati diversificati per l'affinamento, superando le limitazioni e i costi associati all'annotazione umana.</sample>
    <sample id="111">Gli autori selezionano le parole a frequenza moderata utilizzando un corpus di testo generale. Contano la frequenza di ogni parola in questo corpus e identificano un intervallo di frequenza moderato. Questo approccio consente loro di definire un set di "trigger" parole che saranno utilizzate per l'inserimento del watermark nel processo di embedding.</sample>
    <sample id="112">##  Funzionano ancora i tagger di entità nominative di CoNLL-2003 nel 2023?

Ciao a tutti, mi chiamo Shuheng. Oggi presenterò la nostra ricerca intitolata "Funzionano ancora i tagger di entità nominative di CoNLL-2003 nel 2023?". In questo studio abbiamo esplorato la capacità di generalizzazione dei modelli di Named Entity Recognition (NER), una componente fondamentale del trattamento del linguaggio naturale.

Osserviamo che i modelli basati su CoNLL-2003, un dataset di riferimento per il NER da quasi 20 anni, sollevano alcune domande:

* **Generalizzano ai dati moderni?**
* **Quali sono i fattori che contribuiscono a una buona generalizzazione?**
* **Perché si verifica un calo delle prestazioni in alcuni casi?**

Per rispondere a queste domande, abbiamo creato il dataset CoNLL++. Si tratta di un insieme di dati estratti da articoli di Reuters del 2020, annotati seguendo le stesse linee guida di CoNLL-2003. Abbiamo quindi addestrato oltre 20 modelli su CoNLL-2003 e valutato le loro prestazioni sia sui set di test originali di CoNLL-2003 che su CoNLL++. Infine, abbiamo calcolato la percentuale di variazione dell'F1 per misurare la capacità di generalizzazione di ogni modello.

**Quali sono i fattori per una buona generalizzazione?**

Le nostre sperimentazioni hanno evidenziato tre elementi chiave:

1. **Architettura del modello:** I modelli Transformer si sono dimostrati generalmente più efficaci nella generalizzazione su nuovi dati.
2. **Dimensione del modello:** Modelli più grandi tendono a generalizzare meglio.
3. **Quantità di esempi di addestramento:** Un numero maggiore di esempi di addestramento si traduce in una migliore performance.

**Cosa causa il calo delle prestazioni?**

Abbiamo formulato due ipotesi:

* **Overfitting adattivo:** L'utilizzo ripetuto dello stesso set di test porta a un miglioramento che non si traduce in guadagni significativi su nuovi set di dati.
* **Drift temporale:** La performance diminuisce a causa del divario temporale crescente tra i dati di addestramento e quelli di test.

Per l'overfitting adattivo, abbiamo osservato che la pendenza della migliore linea di regressione (rappresentata nella figura) è superiore a 1. Ciò indica che ogni unità di miglioramento su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, dimostrando l'assenza di overfitting adattivo in questo caso.

Per il drift temporale, abbiamo sperimentato il riaddestramento o il pre-addestramento di alcuni modelli con dati più recenti. Abbiamo riscontrato che la performance diminuisce con l'aumentare del divario temporale, confermando la nostra ipotesi.

**Conclusione:**

Per ottenere una buona generalizzazione, è necessario un'architettura di modello efficace, una dimensione di modello adeguata e un numero sufficiente di esempi di addestramento. Questi fattori sono interconnessi e non si può puntare su uno solo.

Sorprendentemente, abbiamo scoperto che il calo delle prestazioni non è dovuto all'overfitting adattivo, ma al drift temporale, anche se CoNLL-2003 è in uso da oltre 20 anni.

**Rispondendo alla domanda del titolo:** Sì, i tagger di CoNLL-2003 funzionano ancora nel 2023. La nostra ricerca incoraggia ulteriori studi per migliorare la capacità di generalizzazione dei modelli.

Vi invitiamo a consultare il nostro articolo, il nostro dataset e a contattarci per qualsiasi domanda.

Grazie!</sample>
    <sample id="114">Nel nostro lavoro presentato all'ACL 2023, "Finding the Pillars of Strength for Multi-Head Attention", esploriamo come ridurre la complessità dei grandi modelli linguistici (LLM) senza compromettere le prestazioni. Gli LLM attuali, pur essendo rivoluzionari, soffrono di ingenti requisiti di parametri, tempo di addestramento e risorse.

Proponiamo un approccio innovativo chiamato *Grouped Head Attention* che utilizza una strategia di divisione e conquista. In primo luogo, addestriamo i modelli dividendo i head di attenzione in gruppi, rendendo quelli all'interno di un gruppo più simili e quelli tra gruppi più diversi. Successivamente, applichiamo un algoritmo di *Voting-to-Stay* per eliminare i head ridondanti all'interno di ogni gruppo, ottenendo una significativa compressione dei parametri.

I nostri modelli, GHT e GHT-PS, hanno dimostrato miglioramenti notevoli in compiti come traduzione automatica, riassunto testuale e modellazione del linguaggio. GHT-PS, in particolare, raggiunge il 32,1% di compressione dei parametri mantenendo prestazioni comparabili.

Guardando al futuro, crediamo che la direzione promettente sia il *pruning* automatico e task-specifico, ispirato all'ipotesi della "Lotteria dei Biglietti". Proprio come le app inutilizzate su un iPhone possono essere disinstallate, riteniamo che i parametri ridondanti nei LLM possano essere eliminati senza impattare le prestazioni per le applicazioni specifiche.</sample>
    <sample id="115">L'approccio EDAtt (Encoder-Decoder Attention) proposto nel paper utilizza un segmento parlato di dimensione definita, chiamato "lambda speech frames". La scelta di quando emettere o meno una traduzione parziale si basa sull'attenzione tra l'input audio e l'output testuale, con parole emesse solo se l'attenzione non è concentrata su informazioni recenti (sotto un certo soglia alfa) all'interno dei lambda frame finali.

In sintesi, l'approccio lavora con segmenti di discorso di una lunghezza specifica, determinata dai "lambda speech frames", per gestire la traduzione simultanea in tempo reale.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie includono:

1. **Informazione di background**: La conoscenza che "i giudici decidono casi nei tribunali di legge" (o, in termini più generali, le occupazioni e le loro attività). Questa informazione è tipicamente acquisita durante il pre-addestramento dei modelli linguistici.

2. **Entità-specifiche**: La conoscenza che "Servin è un giudice" e "Kea è un panettiere". Questa informazione è solitamente osservata al momento dell'inferenza.

Queste due tipologie di conoscenza sono essenziali per risolvere correttamente il pronome "he" nel contesto dato, identificando che si riferisce a Servin.</sample>
    <sample id="117">Secondo la ricerca presentata da David Vilar, il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente è **la qualità dell'esempio**. I risultati sperimentali hanno dimostrato che la qualità e la cura degli esempi utilizzati per il prompting hanno un impatto significativo sulle prestazioni del modello di traduzione, più che la somiglianza con la frase sorgente.</sample>
    <sample id="118">Lo studio presentato, "Improving Pretraining Techniques for Code-Switched NLP" (Migliorare le tecniche di pre-addestramento per l'NLP code-switching), affronta la sfida del processamento del linguaggio naturale (NLP) in comunità linguisticamente diverse, come quelle dell'India, dove il code-switching, ovvero l'alternanza tra lingue in una stessa frase, è comune.

I ricercatori propongono SwitchMLM, una nuova tecnica di pre-addestramento del linguaggio (MLM) progettata specificamente per gestire le frasi code-switching. SwitchMLM si concentra sui *switch-point*, ovvero le transizioni tra lingue all'interno di una frase. A differenza dei metodi standard MLM, che mascherano le parole con uniformità, SwitchMLM mascherare solo i *switch-point*, richiedendo l'accesso a dati etichettati con l'identificazione della lingua (LID). Come alternativa, introducono FrequencyMLM, che assegna etichette LID basate sulla frequenza delle parole nei corpus monolingui.

Inoltre, gli autori suggeriscono modifiche architettoniche, tra cui l'aggiunta di connessioni residuate e una perdita LID ausiliaria. Queste modifiche mirano ad aumentare la quantità di informazioni sui *switch-point* nelle rappresentazioni intermedie e finali del modello. Gli esperimenti di probing confermano che i metodi proposti migliorano la capacità del modello di catturare le informazioni sui *switch-point*.

In sintesi, questo lavoro offre un approccio innovativo per migliorare le prestazioni dei modelli NLP nel gestire il code-switching, con implicazioni significative per le applicazioni in contesti linguisticamente diversi.</sample>
    <sample id="119">L'articolo si concentra principalmente sui modelli linguistici **GPT-4, GPT serie, BART serie e RoBERTa**.

Durante gli esperimenti, gli autori hanno valutato e confrontato le tendenze politiche di questi modelli dopo averli addestrati su diverse fonti di dati, inclusi:

* **Corpora di notizie**: come il C4 Corpus e corpora di notizie separati in base all'orientamento politico (prima e dopo l'elezione del 45° presidente degli Stati Uniti).
* **Corpora di social media**: ulteriormente suddivisi in base all'orientamento politico.

Gli esperimenti hanno esaminato come queste diverse fonti di dati influenzano le tendenze politiche dei modelli e come queste tendenze si riflettono nelle loro prestazioni in compiti come la rilevazione di odio e la rilevazione di notizie false.</sample>
    <sample id="120">Il modello utilizza **i punteggi di attenzione tra l'input audio e l'output testuale (cross-attention) di un singolo livello**.

L'approccio EDAtt (Encoder-Decoder Attention) si basa sulla decisione di emettere o meno una parte della traduzione in base a dove punta l'attenzione.  Questo avviene analizzando la stabilità delle informazioni ricevute, misurata dalla somma dei pesi di attenzione verso gli ultimi lambda frame di discorso.

In altre parole, il modello non combina i punteggi di attenzione di più livelli, ma si affida all'attenzione tra un solo livello encoder-decoder per prendere decisioni in tempo reale sulla traduzione.</sample>
    <sample id="121">Gli esempi di inferenza diretta menzionati nel testo includono l'uso diretto del nome di un'entità o della sua posizione per fare una scelta, come nel caso di "Easy on Me" o "la prima canzone". Un altro esempio è quando un utente specifica chiaramente la sua preferenza, come "la canzone più nuova" o "quella meno energica".

Queste inferenze dirette sono più semplici e immediate, ma il testo esplora principalmente le inferenze indirette, dove l'utente potrebbe usare un linguaggio più vago o ambiguo per fare riferimento a un'entità.</sample>
    <sample id="122">Gli autori dell'articolo "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" sono affiliati a Fudan University.</sample>
    <sample id="123">Lo studio presentato da Ying e Zhiyang si concentra sull'aumento dell'apprendimento multi-modale zero-shot tramite l'ottimizzazione di istruzioni per modelli pre-addestrati. Con l'ascesa dei grandi modelli linguistici, la ricerca si è rivolta all'utilizzo efficiente di questi modelli per vari compiti downstream. Mentre l'ottimizzazione di istruzioni ha dimostrato di migliorare le prestazioni zero-shot nei compiti linguistici, è stata meno esplorata nelle aree della visione artificiale e dei compiti multi-modali.

Gli autori hanno creato MultiInstruct, il primo dataset di ottimizzazione di istruzioni multi-modali, contenente 62 compiti diversificati che coprono 10 categorie. Questo dataset, derivato da 21 dataset open-source, fornisce cinque istruzioni per ogni compito. Utilizzando il modello OFA, che gestisce testo, immagini e coordinate di bounding box in uno spazio token unificato, hanno dimostrato l'efficacia dell'ottimizzazione di istruzioni in un formato sequenza-a-sequenza.

L'esperimento ha coinvolto 53 compiti di addestramento e il gruppo di ragionamento comune per il test. Sono stati condotti cinque esperimenti per compito, utilizzando una delle cinque istruzioni per ogni istanza. Le metriche di valutazione includono accuratezza per i compiti di classificazione multi-modale e ROUGE-L per i compiti di generazione.

I risultati mostrano un notevole miglioramento delle prestazioni di OFA sui compiti visti multi-modali e benefici derivanti dall'apprendimento da un dataset di istruzioni naturali. L'introduzione della metrica "sensibilità" misura la coerenza delle uscite del modello per istruzioni leggermente variabili. Lo studio propone anche un'estensione del dataset MultiInstruct con 150 compiti aggiuntivi.</sample>
    <sample id="124">**Riassunto della Presentazione: "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models"**

Tan Qingyu, dell'Università Nazionale di Singapore e di Alibaba, presenta una ricerca incentrata sul miglioramento delle capacità di ragionamento temporale nei Large Language Models (LLM).

La ricerca suddivide il ragionamento temporale in tre livelli:
1. **Tempo-a-tempo**: Risposte basate su una semplice comprensione dell'asse temporale, come "Qual è l'anno successivo al 2010?".
2. **Tempo-a-evento**: Richiede la correlazione tra tempo ed eventi, ad esempio, "Quale squadra ha giocato Lionel Messi nel 2010?".
3. **Evento-a-evento**: Coinvolge il ragionamento su più eventi temporali, come "Quale squadra ha giocato Lionel Messi dopo il FC Barcelona?".

Gli autori notano che le ricerche precedenti si sono concentrate troppo sul secondo livello (L2). Per un'analisi più completa, hanno creato il dataset **TempReason**, che copre tutti e tre i livelli e un ampio intervallo temporale.

Hanno testato diversi LLM, tra cui ChatGPT, trovando pregiudizi nelle previsioni degli anni tra il 2000 e il 2020. ChatGPT mostra un calo significativo nelle previsioni mensili.

Per migliorare il ragionamento temporale, propongono:
- **TempT5**: Un modello basato su T5 con pre-addestramento su estrazioni di span temporali e apprendimento rinforzato tempo-sensibile.
- **Reasoning QA**: Un nuovo setting di test che fornisce al modello tutte le conoscenze temporali rilevanti.

I risultati mostrano che TempT5 supera significativamente le prestazioni di altri LLM, ma mostra fluttuazioni in base ai periodi temporali. La ricerca suggerisce che sono necessari ulteriori miglioramenti per superare i pregiudizi del ragionamento temporale.</sample>
    <sample id="125">L'articolo menzionato coinvolge almeno due autori: Yanis Labrak e un secondo autore non specificato nel testo fornito.</sample>
    <sample id="126">Sì, secondo la presentazione, l'uso di modelli di traduzione automatica (come Google Translate API) per tradurre le query da un linguaggio naturale al linguaggio di destinazione prima di applicare il parsing semantico è considerato un approccio standard. Questo metodo è una delle sei impostazioni proposte per valutare il modello XSemPLR, insieme ad altre opzioni che includono il training e l'inferenza monolingua, multilingua, e le tecniche di trasferimento zero-shot e few-shot.</sample>
    <sample id="127"># **Large Language Models as Reasoning Teachers**

La ricerca presentata, condotta da Namgyu Ho, Laura Schmid e Se-Young Yun, mira a superare le limitazioni della tecnica di chain-of-thought reasoning nei modelli linguistici di grandi dimensioni.

La tecnica tradizionale richiede modelli enormi per gestire compiti complessi a più passaggi, come le domande matematiche. Gli autori propongono un approccio innovativo: utilizzare questi grandi modelli come insegnanti per trasferire le loro capacità di ragionamento a modelli più piccoli.

Il metodo implica la generazione di soluzioni passo-passo per compiti complessi utilizzando modelli di grandi dimensioni e l'utilizzo di queste soluzioni come dati di addestramento per modelli più piccoli. Questo processo, chiamato "fine-tuning CoT", consente ai modelli più piccoli di eseguire ragionamenti complessi.

La novità principale è l'introduzione del "Diverse Reasoning", una tecnica che genera più soluzioni passo-passo per ogni domanda, migliorando l'addestramento del modello studente. I risultati dimostrano che questo metodo può raggiungere prestazioni notevoli in vari compiti, specialmente in quelli testuali, superando persino l'addestramento tradizionale.

Gli autori evidenziano la scalabilità del loro approccio e le relative sfide, come i costi di sviluppo e inferenza. Il lavoro suggerisce che la distillazione delle conoscenze da modelli di grandi dimensioni a modelli più piccoli potrebbe essere applicabile anche ad altre capacità emergenti, offrendo un metodo accessibile ed efficace per migliorare le prestazioni dei modelli linguistici.</sample>
    <sample id="128"># **The KITMUS Test: Integrando Conoscenza da Multipli Fonti**

Akshatha e Martin presentano il loro studio, "The KITMUS Test", che esplora la capacità dei modelli di comprensione del linguaggio naturale (NLU) di integrare conoscenza da fonti multiple. I modelli NLU moderni si affidano a conoscenze acquisite durante la pre-addestramento e a quelle fornite negli input durante l'inferenza. Lo studio si concentra sulla sfida di combinare queste due forme di conoscenza per compiti complessi.

I ricercatori introducono KITMUS (Knowledge Integration from Multiple Sources), una suite di test diagnostici per valutare questa capacità. Il test principale è una task di risoluzione di coreferenze, progettata per verificare l'uso di conoscenze diverse. L'esempio fornito descrive una situazione in cui il pronome "he" deve essere associato alla persona corretta, richiedendo sia conoscenze specifiche sull'entità (ad esempio, Servin è un giudice) che conoscenze di sfondo (ad esempio, i giudici decidono casi nei tribunali).

KITMUS offre tre scenari: "Background-Pretrain" (conoscenze di sfondo disponibili durante la pre-addestramento), "Background-Both" (disponibili sia durante la pre-addestramento che all'inferenza) e "Background-Inference" (conoscenze di sfondo fornite solo all'inferenza). Gli esperimenti mostrano che i modelli, anche quelli avanzati, faticano a integrare conoscenze senza un addestramento specifico su KITMUS. Con l'addestramento, alcuni modelli mostrano miglioramenti significativi, ma anche i migliori hanno difficoltà con conoscenze di sfondo presentate solo all'inferenza.

In sintesi, lo studio evidenzia la complessità dell'integrazione di conoscenze multiple e suggerisce che i modelli NLU necessitano di un'addestramento mirato per eccellere in compiti che richiedono tale integrazione.</sample>
    <sample id="129">Gli autori hanno utilizzato come esempio un gruppo contrassegnato una "donna asiatica" e un "uomo bianco". Attraverso i prompt, il modello ha generato descrizioni di queste figure, rivelando stereotipi e narrazioni essenzialiste.

Per la donna asiatica, le parole chiave includevano "piccola", "delicata" e "setosa", che riflettono stereotipi storici e la sessualizzazione delle donne asiatiche. Per l'uomo bianco, non c'erano parole distinte, sottolineando la sua posizione come gruppo non contrassegnato.

Questi esempi dimostrano come il modello, anche quando genera descrizioni apparentemente positive, possa perpetuare stereotipi e pregiudizi di lunga data.</sample>
    <sample id="130">Basandoci sulla presentazione, le architetture dei modelli che non generalizzano adeguatamente sono quelle **non basate su transformer**. Gli esperimenti condotti mostrano che i modelli transformer tendono a generalizzare meglio su dati nuovi rispetto ad altre architetture.</sample>
    <sample id="131">Nel contesto della presentazione "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", i set di dati di test menzionati sono:

1. **Set di dati "clean" o "pulito****: Utilizzato per valutare la capacità dei modelli di generalizzare al di là delle etichette deboli.
2. **Set di dati di validazione (potenzialmente rumoroso)**: Necessario per il processo di selezione del modello, anche se l'ideale sarebbe un set di dati pulito.

Questi set di dati sono cruciali per valutare le prestazioni dei modelli di apprendimento debolmente supervisionato (WSL).</sample>
    <sample id="132">L'articolo è stato scritto da **due** autori: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con **più modalità**. Il testo è menzionato come una delle modalità, ma la presentazione include anche immagini, istruzioni e bounding box, indicando un approccio multi-modale. Il lavoro si concentra specificamente sull'uso dell'instruction tuning per migliorare l'apprendimento zero-shot in compiti multi-modali.</sample>
    <sample id="135">**Sintesi di ABC-Eval: Una Nuova Metodologia per Valutare l'AI Conversazionale**

James e Sarah Finch presentano ABC-Eval, un approccio innovativo per valutare le prestazioni dei modelli di intelligenza artificiale conversazionale (AI). Sviluppato dal laboratorio NLP di Emory University in collaborazione con Amazon Alexa AI, ABC-Eval mira a superare le limitazioni delle valutazioni umane tradizionali.

L'approccio comune è quello di affidarsi a giudici umani per confrontare o valutare le conversazioni. Tuttavia, i Finch propongono un metodo più oggettivo annotando specifici comportamenti dei modelli di dialogo. ABC-Eval classifica le risposte dei modelli in base a vari comportamenti, come irrilevanza, contraddizioni, violazioni della logica o mancanza di empatia.

Attraverso un'analisi di quattro modelli di chat di punta, i ricercatori hanno scoperto che ABC-Eval fornisce valutazioni più affidabili rispetto ai metodi esistenti. Le metriche di ABC-Eval prevedono con maggiore precisione la qualità complessiva delle conversazioni e catturano aspetti unici del dialogo.

I risultati mostrano che i modelli hanno ancora difficoltà con violazioni della logica comune (20%), informazioni irrilevanti (15%) e contraddizioni (10%). I Finch sottolineano l'importanza di metriche di valutazione precise man mano che il campo dell'AI conversazionale continua a evolversi. ABC-Eval, sperano, possa diventare uno strumento prezioso per confrontare i modelli e guidare i futuri progressi nel settore.</sample>
    <sample id="136">Jasivan, insieme alla sua supervisora Nafise, ha condotto uno studio presso l'Università di Sheffield intitolato "FERMAT: Un'alternativa alla precisione per il ragionamento numerico". Il lavoro esplora le sfide nel valutare le capacità di ragionamento numerico dei modelli linguistici, in particolare quelli di dimensioni più accessibili.

Il problema principale è che i benchmark esistenti, come le metriche di accuratezza e F1, non forniscono informazioni dettagliate sulle debolezze e sui punti di forza dei modelli in compiti matematici. Lo studio introduce FERMAT, un set di dati flessibile basato su tipi aritmetici, che valuta la comprensione numerica, le operazioni matematiche e la dipendenza dall'addestramento.

FERMAT include domande basate su problemi matematici reali, con numeri rappresentati in vari formati (decimali, interi grandi e piccoli) e operazioni combinate. I risultati mostrano che i modelli tendono a performare male in modo generale, ma un fine-tuning con un set di dati generato da insegnanti di matematica migliora le prestazioni.

L'analisi della dipendenza dall'addestramento rivela che, anche quando l'espressione esatta è presente durante l'addestramento, l'accuratezza rimane bassa, suggerendo che i modelli non memorizzano semplicemente le espressioni. Lo studio conclude che la diversità linguistica e matematica è cruciale per migliorare le prestazioni, e identifica la codifica numerica e la tokenizzazione come aree di miglioramento.

Il lavoro evidenzia la necessità di benchmark più informativi come FERMAT per una valutazione più completa delle capacità di ragionamento numerico dei modelli linguistici.</sample>
    <sample id="137">Il documento presenta "Tell2Design: A Dataset for Language-Guided Floor Plan Generation", un lavoro di ricerca pubblicato all'ACL 2023 da Sicong e colleghi della Singapore University of Technology and Design.

L'obiettivo è consentire alle persone senza competenze di progettazione di creare piani di pavimento specificando le loro esigenze in linguaggio naturale, come se "raccontassero" le loro istruzioni.

Il lavoro introduce un nuovo compito di apprendimento automatico: generare piani di pavimento 2D ragionevoli basati su istruzioni testuali che descrivono componenti semantici, geometrici e topologici.

La ricerca crea il dataset Tell2Design, composto da 5.051 istruzioni umane raccolte da lavoratori Amazon Mechanical Turk e 76.000 istruzioni generate artificialmente.

Il team propone un modello basato su un framework encoder-decoder, che tratta la generazione di piani di pavimento come un problema sequenza-a-sequenza. Questo consente di gestire istruzioni di lunghezza variabile.

I risultati dimostrano che il modello Tell2Design supera i metodi di generazione di immagini condizionate al testo in termini di precisione nella corrispondenza con le istruzioni, grazie alla sua capacità di estrarre informazioni chiave dalle istruzioni testuali.

Il lavoro apre la strada a future ricerche sulla generazione di design guidati dal linguaggio, con un focus iniziale sui piani di pavimento.</sample>
    <sample id="138">Secondo gli autori del lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources", una area poco studiata della Natural Language Understanding (NLU) è la capacità dei modelli di integrare e utilizzare sia la conoscenza acquisita durante il pre-addestramento che quella fornita al momento dell'inferenza. Il loro studio si concentra sulla valutazione di questa capacità attraverso un test diagnostico chiamato KITMUS, che introduce una task di risoluzione di coreferenze per esaminare l'uso di conoscenze da diverse fonti.</sample>
    <sample id="139">I relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript ha subito controlli di qualità. Dopo la generazione iniziale, i lavoratori esterni hanno esaminato e rivisto i campioni per garantire l'accuratezza e la qualità del dataset. Questo processo ha contribuito a rimuovere eventuali errori o inesattezze, assicurando così un dataset affidabile per la ricerca sul pianificazione linguistica vincolata.</sample>
    <sample id="141">I limiti delle risorse esistenti per la traduzione dipendente dal contesto includono:

1. **Copertura Limitata**: Le risorse attuali supportano solo un numero limitato di tipi di traduzioni dipendenti dal contesto e di coppie di lingue, spesso basate su conoscenze di dominio e curazione umana.

2. **Incapacità di Captare la Varietà**: Corpus-level metrics come BLEU non sono in grado di catturare efficacemente le traduzioni dipendenti dal contesto a causa della loro natura generale.

3. **Mancanza di Automazione**: Le valutazioni mirate sulle traduzioni dipendenti dal contesto spesso richiedono interventi manuali, limitando la loro scalabilità.</sample>
    <sample id="142">## Risolvere le espressioni di riferimento indiretto per la selezione delle entità: Presentazione del corpus AltEntities

Ciao! Oggi parlerò del nostro lavoro intitolato "Risolvere le espressioni di riferimento indiretto per la selezione delle entità", in cui introduciamo il **corpus AltEntities**. Insieme a Filip Radlinski, Silvia Pareti e Annie Louis, abbiamo esplorato come comprendere il linguaggio degli utenti quando desiderano fare una scelta.

Immaginate la situazione: un utente vuole selezionare una tra due canzoni, ad esempio "Easy on Me" o "I Gotta Feeling". 
La soluzione più ovvia sarebbe usare un riferimento diretto, dicendo ad esempio il nome della canzone o la sua posizione nella lista. Ma a volte un riferimento indiretto è più appropriato per una conversazione più naturale. Questo può accadere quando l'utente non ricorda il nome della canzone, le pronunce sono troppo simili o l'utente vuole esprimere una preferenza.

Ecco alcuni esempi di riferimenti indiretti: "la più nuova" o "la canzone meno energica". Questo problema è cruciale per i sistemi conversazionali e per valutare la comprensione delle entità da parte dei modelli linguistici (LLM). Finora, non esisteva un dataset pubblico di grandi dimensioni per questo compito. Per questo motivo, abbiamo creato uno noi stessi utilizzando l'annotazione crowdsourcing.

Il nostro dataset copre tre domini diversi: **musica, libri e ricette**. La metodologia di raccolta dati enfatizza l'informalità tramite un setup di completamento con cartoni animati. Il cartone presenta tre bolle del dialogo:

* **Bolla 1 (Bob):** "Ricordi quella canzone che stavamo ascoltando ieri?"
* **Bolla 2 (Alice):** "Intendi 'Easy on Me' o 'I Gotta Feeling'?" (questa è la domanda alternativa)
* **Bolla 3 (Bob):**  Bob usa un riferimento indiretto per scegliere una delle due entità, ad esempio "la più nuova".

La prima e la seconda bolla sono generate automaticamente, mentre la terza è completata dall'annotatore.

La bolla 2, la domanda alternativa, viene generata utilizzando un semplice template: "Intendi A o B?" dove A e B sono esempi tratti da Wikipedia. Abbiamo utilizzato diversi metodi di campionamento per creare coppie di entità sempre più simili tra loro, rendendo la disambiguazione più difficile.

* **Caso 1 (Caso casuale):**  Selezione casuale di due entità da Wikipedia.
* **Caso 2 (Titoli simili):** Entità con titoli simili, ad esempio due libri con lo stesso nome come "Il Ritorno".
* **Caso 3 (Descrizioni simili):** Entità con descrizioni simili su Wikipedia.
* **Caso 4 (Attributi simili):** Entità con informazioni simili, come lo stesso genere musicale o lo stesso artista.

Gli annotatori conoscono i nomi delle entità, ma non necessariamente i dettagli specifici. Per questo, forniamo loro un contesto informativo, come un link di ricerca Google per ogni canzone, o estratti di Wikipedia per libri e ricette. Gli annotatori ascoltano/leggono le informazioni e poi scelgono un'entità e la descrivono utilizzando 3-5 espressioni di riferimento indiretto.

Ecco alcuni esempi: "quella senza parole", "non quella con il ragazzo di 12 anni", "quella fittizia", "viene dall'Azerbaigian".

Il **corpus AltEntities** contiene 6.000 domande alternative e 42.000 espressioni di riferimento indiretto, distribuite sui tre domini.

I risultati con il modello T5 XL mostrano che:

* Con lo stesso contesto informativo degli annotatori, il modello raggiunge un'accuratezza del 92-95%.
* Con un contesto parzialmente sovrapponibile, l'accuratezza scende al 82-87%.
* Con solo i nomi delle entità, l'accuratezza è del 60%.

Abbiamo dimostrato che i modelli sono in grado di generalizzare tra domini diversi.

Potete trovare il **corpus AltEntities** qui: [Link al dataset]</sample>
    <sample id="143">L'approccio descritto nel paper "Attention as a Guide for Simultaneous Speech Translation" confronta la sua strategia EDAtt (Encoder-Decoder Attention) con diverse politiche SimulST esistenti:

1. **Wait-k Strategy**: Una strategia comune che ritarda l'emissione di una traduzione fino a quando non sono stati ricevuti k frame di discorso successivi.
2. **Local Agreement**: Un'altra strategia che cerca di concordare localmente le traduzioni basandosi su frame precedenti.
3. **State-of-the-art Architecture**: Architetture specifiche progettate appositamente per la traduzione simultanea pre-elaborata.

Il paper valuta queste politiche in termini di qualità della traduzione (misurata con BLEU) e latenza (media e computazionalmente consapevole), dimostrando che EDAtt supera tutte le strategie applicate ai modelli offline, offrendo sia alta qualità che tempi di elaborazione più rapidi.</sample>
    <sample id="144">Gli autori dell'articolo "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" sono affiliati con l'Università di Nantes, in Francia.</sample>
    <sample id="145">La relatrice è Jenny.</sample>
    <sample id="146"># Analisi dell'Omissione nella Sintesi del Dialogo: Un Approccio per Migliorare la Qualità

Questa presentazione si concentra sull'analisi e la risoluzione del problema delle omissioni nella sintesi del dialogo, un aspetto critico ma spesso trascurato nella generazione di riepiloghi di conversazioni. Lo studio, condotto da Yicheng e colleghi di Fudan University, evidenzia la prevalenza delle omissioni nelle sintesi generate da modelli linguistici pre-addestrati di grandi dimensioni.

La ricerca rivela che circa il 70% dei riepiloghi presenta omissioni, indicando una sfida significativa nella creazione di sintesi accurate. L'analisi della distribuzione delle informazioni omise mostra che non seguono un modello specifico all'interno del dialogo, rendendo difficile per i modelli attuali identificare i punti chiave.

Per affrontare questo problema, gli autori introducono il dataset OLDS, che etichetta in modo accurato le omissioni in cinque domini diversi. Il dataset include varie sintesi candidate generate da diversi modelli, con etichette automatiche e valutazioni umane per garantire la qualità.

Vengono testati tre framework di modelli di base con diverse architetture per il compito di rilevamento delle omissioni. I risultati mostrano una sfida intrinseca nel compito, con un F1-score del 50%. Tuttavia, l'integrazione delle informazioni omise per il raffinamento della sintesi dimostra un miglioramento significativo della qualità, suggerendo che il rilevamento delle omissioni è una strategia promettente.

In sintesi, questo lavoro fornisce una comprensione approfondita delle omissioni nella sintesi del dialogo e propone un approccio pratico per migliorare la qualità delle sintesi.</sample>
    <sample id="147">L'articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" è stato scritto in collaborazione da **tre** autori: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Salve, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler, e sto per presentarvi brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", un lavoro congiunto con Matteo Negri e Marco Turchi.

**Cosa è la traduzione simultanea della parola?**

La traduzione simultanea della parola, o SimulST, è il processo di traduzione di un discorso parlato in un testo in un'altra lingua in tempo reale, consentendo la comunicazione tra lingue diverse.

**Quali sono i problemi dei modelli SimulST attuali?**

I modelli SimulST attuali presentano alcune sfide: l'uso di architetture specifiche, che richiedono una formazione aggiuntiva e complessa, con diversi obiettivi di ottimizzazione. Inoltre, è necessario addestrare e mantenere più modelli per raggiungere diversi regimi di latenza, ad esempio un modello con una latenza media di un secondo e un altro con due secondi.

**La nostra soluzione:**

La nostra soluzione è quella di utilizzare modelli di traduzione automatica offline esistenti senza riaddestrarli o adottare architetture specifiche per SimulST. Utilizziamo un solo modello per ogni regime di latenza e gestiamo la latenza attraverso parametri specifici. Sfruttando la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, noto come attenzione incrociata (cross-attention).

Abbiamo proposto EDAtt (Encoder-Decoder Attention), una strategia che decide se emettere o meno una traduzione parziale in base a dove punta l'attenzione. Una parola viene emessa se l'attenzione non è concentrata, ovvero se la somma dei pesi di attenzione verso gli ultimi lambda frame di discorso è al di sotto di una certa soglia alfa. Ad esempio, se riceviamo un chunk di discorso contenente "Sto per parlare di...", e il nostro modello prevede la traduzione in tedesco, osservando i pesi di attenzione incrociata, vedremo che le prime due parole puntano ai primi frame di discorso ricevuti, mentre l'ultima parola punta agli ultimi frame. Questo significa che le prime due parole verranno emesse, mentre non emetteremo l'ultima parola in attesa di un altro chunk di discorso.

**Risultati di EDAtt:**

I risultati di EDAtt mostrano curve di traduzione simultanea con un alto valore di BLEU (misura di qualità della traduzione) e un basso valore di latenza media, sia computazionale che effettiva. Confronto con altre strategie popolari applicate a modelli offline (Wait-k e Local Agreement) e con l'architettura di stato dell'arte specificamente progettata per la pre-traduzione simultanea.

In sintesi, EDAtt supera tutte le strategie applicate ai modelli offline, con curve spostate verso sinistra, e si rivela la strategia più veloce in termini di tempo effettivo e computazionale.

Per ulteriori dettagli, consultate il nostro paper. Abbiamo rilasciato open source il codice, i modelli e gli output simultanei per facilitare la riproducibilità del nostro lavoro.

Grazie per l'attenzione.</sample>
    <sample id="149">Sì, il dataset CoNLL++ sviluppato nello studio è disponibile pubblicamente. Gli autori hanno reso accessibile il dataset per facilitare la riproducibilità e ulteriori ricerche.</sample>
    <sample id="150">Il paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" presenta un nuovo dataset, MeetingQA, dedicato alla risposta alle domande in trascrizioni di riunioni. Gli autori, provenienti da Adobe Research e UNC Chapel Hill, riconoscono il potenziale informativo delle trascrizioni di riunioni, spesso lunghe e specifiche di dominio, ma sottolinea la mancanza di attenzione alla componente QA (Question Answering).

MeetingQA raccoglie domande poste durante le riunioni e le relative risposte, con scenari unici come risposte multi-parlante, risposte composte da più frasi disgiunte e domande retoriche. Il dataset comprende 7.700 domande suddivise in set di addestramento, sviluppo e test, con una varietà di tipi di domande e risposte.

Gli autori sperimentano diversi approcci, inclusi modelli a contesto breve e lungo, modelli single-span e multi-span, e l'uso di dati di augmentazione. I risultati mostrano una sfida significativa per i modelli QA, sia in setting fine-tuned che zero-shot, con un divario significativo rispetto alle prestazioni umane.

L'analisi degli errori evidenzia difficoltà nell'identificare domande retoriche, risposte irrilevanti e attribuzione delle risposte ai parlanti corretti, specialmente in assenza di addestramento. MeetingQA rappresenta una risorsa preziosa per la ricerca NLP, offrendo un nuovo fronte per lo sviluppo di modelli QA robusti in contesti reali.</sample>
    <sample id="151">## Presentazione di MultiInstruct: Migliorare l'Apprendimento Zero-Shot Multi-Modale tramite Tuning delle Istruzioni

Salve a tutti, io sono Ying e insieme al mio collega Zhiyang, presenterò la nostra ricerca su **MultiInstruct**, un metodo per migliorare l'apprendimento zero-shot multi-modale tramite il tuning delle istruzioni.

Con l'avvento dei grandi modelli linguistici, molte ricerche hanno esplorato nuovi paradigmi di apprendimento che permettono di riutilizzare i modelli pre-addestrati per diverse attività downstream in modo efficiente in termini di parametri e dati. Studi recenti hanno dimostrato che il tuning delle istruzioni consente ai grandi modelli linguistici di eseguire attività mai viste prima seguendo semplici istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti si è concentrata sull'aumento delle prestazioni zero-shot su attività linguistiche, mentre le attività di visione e multi-modali sono state trascurate.

Nel nostro lavoro, vogliamo indagare se il tuning delle istruzioni su modelli multi-modali pre-addestrati può effettivamente migliorare la generalizzazione ad attività multi-modali mai viste prima.

Durante la nostra ricerca, abbiamo notato una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modali. Esistono oltre 1600 attività di istruzioni linguistiche. Tuttavia, non esiste un dataset multi-modale di istruzioni di grandi dimensioni pubblicamente disponibile. Questo ci ha motivato a creare un dataset di tuning delle istruzioni multi-modali.

Presentiamo **MultiInstruct**, il primo dataset di benchmark per il tuning delle istruzioni multi-modali che comprende 62 attività diverse, suddivise in 10 categorie ampie. Queste attività sono derivate da 21 dataset open-source esistenti e ogni attività è correda da cinque istruzioni scritte da esperti.

Per indagare il tuning delle istruzioni multi-modali sul nostro dataset proposto, utilizziamo OFA, un modello multi-modale pre-addestrato unificato. OFA utilizza un vocabolario unificato per testo, token di immagini e coordinate di una casella di delimitazione.

Ecco alcuni esempi di istanze dal nostro dataset MultiInstruct, che illustrano come unifichiamo il trattamento di diversi tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutte le attività in un formato sequenza-a-sequenza unificato. In questo formato, testo, immagini, istruzioni e caselle di delimitazione sono rappresentati nello stesso spazio token.

Per quanto riguarda il dataset di addestramento, utilizziamo 53 attività da 9 gruppi per l'addestramento, campionando 10.000 istanze per attività. Per il test, riserviamo l'intero gruppo di ragionamento di senso comune per il test e selezioniamo ulteriori 5 attività dai gruppi VQ e Miscellanea. Utilizziamo tutte le istanze nella suddivisione di test per ogni attività.

Inoltre, per ogni attività non linguistica, campioniamo casualmente 20 attività dalla suddivisione di test del dataset di istruzioni naturali come attività non vista per l'NLP. Utilizziamo il modello pre-addestrato OFA large come modello base.

Durante l'addestramento, mescoliamo tutte le istanze per tutte le attività. Ogni istanza viene combinata casualmente con una delle cinque template di istruzione. Durante il test, per ogni attività, conduciamo 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo la prestazione minima e massima e la deviazione standard della prestazione su tutti e 5 gli esperimenti.

Se l'attività è una classificazione multi-modale, riportiamo l'accuratezza. Se è un'attività di generazione multi-modale, utilizziamo ROUGE-L. Per le attività NLP, utilizziamo ROUGE-L anch'esso.

Introdotti anche un nuovo metrico chiamato **sensibilità**, che misura la capacità del modello di produrre gli stessi output per la stessa attività indipendentemente dalla leggera variazione del testo dell'istruzione.

I nostri risultati principali mostrano che il tuning delle istruzioni può migliorare significativamente le prestazioni di OFA sulle attività multi-modali viste. Il trasferimento di apprendimento dai dataset di istruzioni naturali beneficia il tuning delle istruzioni.

Come si può notare, aumentando il numero di attività, il modello ottiene prestazioni migliori con una sensibilità inferiore. Abbiamo anche condotto un esperimento confrontando l'uso di una singola istruzione rispetto a 5 istruzioni, dimostrando che l'uso di più istruzioni migliora le prestazioni complessive e riduce la sensibilità del modello.

In conclusione, proponiamo il primo dataset di tuning delle istruzioni multi-modale di grandi dimensioni, migliorando significativamente le capacità di OFA, esploriamo diverse tecniche di trasferimento di apprendimento e ne dimostriamo i vantaggi. Abbiamo inoltre introdotto un nuovo metrico, la **sensibilità**.

Stiamo attualmente raccogliendo un dataset ancora più grande per il tuning delle istruzioni multi-modali, con circa 150 attività aggiuntive di visione e linguaggio, che pubblicheremo.

Ecco il codice QR per accedere ai dati e al modello.

Grazie.</sample>
    <sample id="152">**Sintesi del Presentazione: "Esplorazione dei Modelli di Linguaggio di Grandi Dimensioni per la Filologia Classica"**

Frederick Riemenschneider ha presentato il lavoro innovativo che interseca l'NLP (Elaborazione del Linguaggio Naturale) e la filologia classica. Sono stati sviluppati modelli linguistici avanzati per il greco antico e il latino, affrontando le sfide della multilingualità.

La ricerca ha creato due modelli monolingui (GreBERTa e GreTa) per il greco antico, migliorando le capacità di comprensione e generazione del testo. Inoltre, PhilBERTa e PhilTa, modelli multilinguali, sono stati pre-addestrati su greco, latino ed inglese.

Il processo ha incluso la raccolta di dati di addestramento da fonti come Open Greek &amp; Latin e il nuovo corpus dall'Internet Archive, superando le limitazioni dell'OCR per il greco.

I modelli sono stati testati su compiti come l'etichettatura delle parti del discorso, l'analisi della dipendenza e la lemmatizzazione, superando le prestazioni precedenti per greco e latino.

L'analisi ha rivelato che i modelli encoder-decoder, come GreTa, richiedono più addestramento per raggiungere prestazioni ottimali rispetto ai modelli encoder-only.

Nonostante le prestazioni equivalenti dei modelli multilinguali e monolingui, la ricerca ha evidenziato il potenziale di elaborare testi in più lingue con un unico modello.

In conclusione, questo lavoro offre nuovi strumenti potenti per la filologia classica, migliorando l'analisi testuale e la comprensione del contesto storico.</sample>
    <sample id="153"># Risolvere le Ambiguità nei Modelli Generativi Testo-Immagine

La presentazione di Ninareh Mehrabi si concentra sull'affrontare le ambiguità presenti nei prompt utilizzati dai modelli generativi testo-immagine, un aspetto cruciale per migliorare la qualità e la precisione delle immagini generate.

Mehrabi e il suo team hanno identificato diverse tipologie di ambiguità nei prompt, come ad esempio la mancanza di chiarezza su dove dovrebbe trovarsi un oggetto (come i fiori in un'immagine di una ragazza che entra in una stanza). Per risolvere questo problema, hanno sviluppato due approcci.

**Disambiguazione dei Prompt:** Il primo metodo coinvolge un modello linguistico che genera domande di chiarimento, le quali vengono poi risposte dagli utenti per fornire un'interpretazione più precisa. In alternativa, il modello genera diverse configurazioni visive, permettendo all'utente di scegliere quella più adatta alla propria intenzione.

**Valutazione delle Immagini Generate:** Una volta disambiguati i prompt, il team valuta la fedeltà delle immagini generate all'intenzione originale dell'utente. Utilizzano un modello VQA (Visual Question Answering) per determinare se l'immagine soddisfa la domanda o l'intenzione dell'utente.

I risultati dimostrano che la disambiguazione migliora la qualità delle immagini generate e che il metodo di valutazione automatizzato è affidabile. La ricerca evidenzia la necessità di affrontare le ambiguità per migliorare le prestazioni dei modelli generativi testo-immagine.</sample>
    <sample id="154">Gli autori dell'articolo "Attention as a Guide for Simultaneous Speech Translation" sono Sara Papi, Matteo Negri e Marco Turchi. Le loro affiliazioni sono:

- Sara Papi: University of Trento e Foundazione Bruno Kessler
- Matteo Negri: (non specificato nell'estratto, ma presumibilmente associato a una delle istituzioni menzionate)
- Marco Turchi: (simile al caso precedente)

L'articolo presenta una ricerca congiunta tra queste istituzioni.</sample>
    <sample id="155">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="157"># **Dialogo Riassunto con Fusione Struttura Statica-Dinamica (SDDS)**

La ricerca si concentra sullo sviluppo di un modello innovativo per la sintesi del dialogo, chiamato SDDS, che mira a estrarre informazioni essenziali da un contesto di dialogo multi-partecipante e semi-strutturato.

Il problema della sintesi del dialogo è complesso ma affascinante, offrendo un modo rapido per comprendere i punti salienti di una conversazione complessa. Gli approcci esistenti si affidano a strutture grafiche statiche pre-calcolate, spesso con l'aiuto di strumenti linguistici esterni, che possono essere inaffidabili.

Il modello SDDS propone quattro componenti chiave:

1. **Codifica dell'Utenza**: Converte ogni affermazione in una rappresentazione vettoriale.
2. **Modellazione della Struttura Statica**: Utilizza metodi esistenti per creare grafici statici basati su discorsi e relazioni tra affermazioni.
3. **Modulo Grafico Statico-Dinamico**: Combina grafici statici e apprende dinamicamente relazioni semantiche tra affermazioni utilizzando un modulo di grafico dinamico.
4. **Generatore di Sintesi**: Un modello linguistico pre-addestrato che fonde la struttura statica e dinamica per generare un riassunto finale.

Il modello introduce metodi per modellare la struttura del dialogo, inclusi grafici basati su discorsi, relazioni tra oratori e posizione delle affermazioni. Un modulo di attenzione multi-testa calcola le relazioni semantiche. SDDS integra efficacemente informazioni strutturali nel processo di generazione, offrendo un riassunto accurato e contestualizzato.

Il codice e i dati sono disponibili su GitHub, facilitando la riproducibilità e l'esplorazione di questo approccio innovativo alla sintesi del dialogo.</sample>
    <sample id="158"># **Dual Cache per la Risoluzione della Coreferenza in Documenti Lunghi**

Qipeng Guo di AWS presenta il loro lavoro su un sistema innovativo per la risoluzione della coreferenza in documenti lunghi.

La coreferenza risolve le ambiguità nelle menzioni di entità all'interno di un testo, collegando le menzioni a un'entità unica. Il problema tradizionale è la complessità computazionale e di memoria quadratica per confrontare tutte le coppie di menzioni. I metodi cache-based recenti migliorano l'efficienza, ma le politiche di evizione, come LRU (Least Recently Used), possono causare elevate mancate corrispondenze in documenti lunghi con argomenti che cambiano frequentemente.

La soluzione proposta è un sistema a doppio cache, con una cache locale e una globale. La cache locale utilizza LRU per le entità locali, mentre la cache globale impiega una politica LFU (Least Frequently Used) per le entità ad alta frequenza. Il sistema scansiona il documento, classificando le nuove menzioni e decidendo se aggiungerle alla cache locale o globale.

I test su quattro set di dati pubblici mostrano che il doppio cache supera i metodi tradizionali, anche senza dati di addestramento. In un test su un libro di 30.000 parole, il doppio cache riduce significativamente le mancate corrispondenze. La ricerca evidenzia i vantaggi del doppio cache in termini di efficienza e riduzione delle mancate corrispondenze, offrendo il miglior rapporto prestazioni/costo rispetto ai metodi a singola cache.</sample>
    <sample id="159">## Presentazione della ricerca: Valutazione della coerenza linguistica in modelli di grandi dimensioni con un approccio basato su coppie minime estese

**Salve a tutti,**

Mi chiamo Koustav Sinha e sono entusiasta di presentarvi la nostra ricerca, pubblicata ad ACL 2023, che esplora la valutazione della coerenza linguistica nei modelli linguistici di grandi dimensioni.

La valutazione della coerenza linguistica nei modelli, spesso basata su giudizi di accettabilità o grammaticalità (come BLiMP, SyntaxGym o coppie CrowS), si concentra tipicamente su coppie di frasi brevi, una accettabile e l'altra inaccettabile o grammaticalmente errata. L'obiettivo è che il modello attribuisca una probabilità maggiore alla frase accettabile.

Tuttavia, **i tradizionali pipeline di valutazione basate su coppie minime (MPP) non consentono di valutare la coerenza del modello su sequenze più lunghe**, una necessità cruciale per i modelli linguistici moderni con finestre di contesto sempre più ampie.

La nostra ricerca mira a **rivisitare l'approccio MPP** permettendo al modello di valutare la coerenza su sequenze più estese. Per farlo, **simuliamo sequenze più lunghe ricreando frasi da dataset esistenti**, scegliendo tra frasi accettabili e inaccettabili con strutture grammaticali corrispondenti.

Ad esempio, da un dataset come BLiMP, prendiamo una coppia di frasi grammaticalmente corrette o errate, aggiungendole come prefisso a una frase accettabile e a una inaccettabile. Possiamo anche scegliere frasi da un dataset diverso (scenario di "mismatch") o da un dominio completamente estraneo come Wikipedia.

**I nostri risultati mostrano:**

* **Robustezza in contesti arbitrari:** Le valutazioni MPP sono relativamente stabili anche con contesti di lunghezza fino a 1024 token per modelli come OPT e GPT-2.
* **Sensibilità al contesto:** Quando le frasi provengono dallo stesso dataset, le valutazioni MPP variano significativamente in base al prefisso (accettabile o inaccettabile) aggiunto. Questo effetto aumenta con la lunghezza del contesto, un fattore importante per i modelli moderni con ampie finestre di contesto.
* **Sensibilità a strutture condivise:**  Anche piccole perturbazioni nelle frasi, pur mantenendo la struttura grammaticale, influenzano le valutazioni MPP in modo simile, suggerendo che i modelli siano sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi.

**Conclusione:** I modelli linguistici di grandi dimensioni sono sensibili a caratteristiche sintattiche e semantiche condivise tra frasi, e le tradizionali valutazioni MPP basate su brevi sequenze potrebbero non catturare appieno la conoscenza astratta dei modelli all'interno della loro finestra di contesto.

Vi invitiamo a leggere il nostro articolo per ulteriori dettagli sui nostri esperimenti.

Grazie per l'attenzione!</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un **multiset di token** che appariranno nell'output.</sample>
    <sample id="161">In base al contenuto fornito, **CoScript contiene 55.000 script specifici**.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN, secondo la presentazione, è **MASSalign**. Questo è stato concluso dopo aver confrontato diversi metodi di allineamento automatico e utilizzare il dataset DEPLAIN, con le sue coppie di frasi manualmente allineate, come standard d'oro per la valutazione.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato (WSL) è la capacità di addestrare modelli di intelligenza artificiale in modo efficiente utilizzando dati etichettati in modo meno rigoroso e più economico rispetto alle annotazioni manuali tradizionali. Invece di affidarsi esclusivamente a etichette umane costose e dispendiose in termini di tempo, il WSL impiega fonti di etichettatura deboli come regole heuristiche, basi di conoscenza o crowdsourcing di bassa qualità.

Il video presenta una ricerca che esamina criticamente il WSL e solleva domande importanti sulla necessità di dati di convalida puliti e sul numero di campioni puliti richiesti per un addestramento efficace. Gli autori scoprono che, contrariamente alle affermazioni precedenti, molti metodi WSL attuali dipendono dai dati di convalida puliti per ottenere prestazioni elevate. Inoltre, dimostrano che l'aumento del numero di campioni puliti può migliorare le prestazioni e che l'addestramento diretto su questi campioni può portare a risultati migliori rispetto ai metodi WSL.

In sintesi, il vantaggio principale del WSL è la possibilità di sfruttare fonti di dati più accessibili e meno costose per l'addestramento, pur mantenendo la capacità di creare modelli che generalizzano bene su dati puliti.</sample>
    <sample id="165">**Sintesi della Presentazione: "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations"**

Wenting Zhao, PhD student presso Cornell University, ha presentato il suo recente studio sull'abduzione del ragionamento logico comune, proponendo un approccio innovativo chiamato LiPoR (Likelihood Learning with Posterior Regularization).

L'abduzione del ragionamento, partendo da un contesto (ad esempio, "Emily è rimasta bloccata nel traffico"), mira a trovare un' spiegazione plausibile (come "Il suo volo era in ritardo") che colleghi le informazioni fornite al risultato (Emily ha comunque preso il volo).

Gli approcci tradizionali si affidano a metodi supervisionati, che richiedono l'annotazione manuale di spiegazioni plausibili, spesso soggettiva e imprecisa. Zhao solleva la questione se sia possibile apprendere il ragionamento abductivo senza supervisione.

La soluzione proposta è LiPoR, un metodo di apprendimento non supervisionato. Tratta le spiegazioni come variabili latenti e formula un obiettivo che massimizza la verosimiglianza dell'esito dato il contesto, marginalizzando le altre spiegazioni. Un regularizzatore aggiuntivo, basato sulla mutua esclusività delle spiegazioni (come nel caso di "Il volo era in ritardo" e "Il volo è partito in orario"), preferisce spiegazioni plausibili.

I risultati dimostrano che LiPoR supera i modelli zero-shot esistenti, incluso GPT-3, di oltre 4 punti percentuali di accuratezza su AlphaNLI, il dataset di ragionamento abductivo più diffuso.

In sintesi, lo studio introduce un approccio promettente per l'abduzione del ragionamento comune senza supervisione.</sample>
    <sample id="166"># **A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Complex Text**

Questo studio propone un approccio innovativo per il recupero di immagini da testi complessi, affrontando le sfide poste da descrizioni lunghe e immagini altamente simili. L'approccio si ispira alla strategia Divide-and-Conquer e alla Dual-Process Theory cognitiva.

La ricerca introduce un framework che integra due sistemi di ragionamento: System 1, basato sull'analogia e simile al ragionamento intuitivo umano, e System 2, responsabile del ragionamento logico e astratto. Il modello propone due componenti principali: un *Proposition Generator* che semplifica le complesse affermazioni testuali e un *Visual-Linguistic Interactor* (System 1) che interagisce tra le proposte visive e linguistiche.

Il *Neural-Symbolic Reasoner* (System 2) combina i risultati dei ragionamenti di System 1. Questo modulo esegue la negazione e opera la congiunzione su stati di ragionamento positivi e negativi per derivare conclusioni. L'integrazione dei risultati di System 1 e System 2 fornisce una soluzione completa.

Gli esperimenti dimostrano l'efficacia del metodo proposto, NDCR, che supera i baseline esistenti. Gli studi di abolizione confermano il contributo di ogni modulo. L'abstract evidenzia il potenziale di questo approccio per migliorare il ragionamento composizionale dei modelli linguistici, suggerendo che la combinazione di Divide-and-Conquer e Dual-Process Theory può portare a progressi nel campo del recupero di immagini da testi complessi.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati utilizzando una combinazione di metodi manuali e automatici. In particolare:

1. **Allineamento manuale**: 750 documenti sono stati allineati manualmente, fornendo un set di dati di alta qualità con allineamenti precisi.
2. **Allineamento automatico**: Oltre all'allineamento manuale, 750 documenti sono stati allineati anche con metodi automatici. Questo ha permesso di aumentare la quantità di dati disponibili e di valutare l'efficacia degli strumenti automatici.

In sintesi, DEPLAIN-web sfrutta sia il lavoro manuale di esperti per un allineamento preciso, sia le tecniche automatiche per aumentare la scala e la varietà del corpus.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo articoli di Reuters News dal 2020 e annotandoli utilizzando le stesse linee guida di annotazione del CoNLL-2003. Questo dataset è stato utilizzato per valutare la generalizzazione di oltre 20 modelli di Named Entity Recognition (NER) fine-tunnati su CoNLL-2003.</sample>
    <sample id="169">David Vilar presenta una revisione del documento "Prompting PaLM per la Traduzione: Valutazione di Strategie e Prestazioni", un lavoro congiunto con i colleghi di Google Translate. PaLM, un modello linguistico di grandi dimensioni con 540 miliardi di parametri, è stato addestrato su 780 miliardi di token e ha raggiunto risultati di punta in numerosi compiti NLP al momento della pubblicazione.

L'articolo introduce la prima indagine sistematica sull'uso dei prompt per i modelli linguistici di grandi dimensioni (LLM) nella traduzione automatica. Gli autori hanno valutato la capacità di traduzione di PaLM utilizzando le migliori pratiche della comunità MT, confrontando i risultati con sistemi di riferimento WMT. Sono state impiegate metriche di traduzione neurale e valutazioni umane esperte.

I risultati evidenziano l'impatto significativo della scelta del prompt sulla prestazione degli LLM. Un semplice esperimento con prompt one-shot ha dimostrato differenze di oltre 40 punti BLEU. Gli autori hanno optato per una strategia di 5-shot prompting, etichettando le frasi di origine e di destinazione. Le loro scoperte suggeriscono che la qualità degli esempi è più cruciale della somiglianza con la frase di origine, soprattutto per prompt brevi.

L'analisi ha rivelato che le traduzioni PaLM, pur mostrando una fluidità paragonabile ai sistemi di punta, soffrono di errori di omissione, con una valutazione più bassa nella categoria "Stile/Imbarazzante" rispetto ai sistemi di riferimento. In sintesi, PaLM si avvicina alle prestazioni di un sistema commerciale, ma con alcune problematiche di accuratezza.</sample>
    <sample id="170">## Presentazione: XSemPLR: Cross-Lingua Semantic Parsing in Diverse Lingue e Rappresentazioni del Significato

**Introduzione**

Salve a tutti, sono Yusen Zhang dell'Università della Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingua Semantic Parsing in Diverse Lingue Naturali e Rappresentazioni del Significato".

La **semantic parsing** è il processo di creazione di rappresentazioni semantiche per query utente, come SQL o Lambda Calculus. Il **cross-lingual semantic parsing** si concentra sulla traduzione di query in diverse lingue naturali in rappresentazioni del significato corrispondenti. Come illustrato nella figura, l'obiettivo è tradurre le query in diverse lingue utilizzando modelli neurali per ottenere risultati in SQL, Lambda o FunQL, tra gli altri.

**Sfide attuali**

I modelli esistenti per il cross-lingual semantic parsing sono stati sviluppati separatamente e valutati su set di dati limitati a specifici compiti e applicazioni. Alcune lingue, come il cinese, sono sottorappresentate, e alcune rappresentazioni del significato, come la calcolo Lambda, sono assenti o valutate solo con un singolo modello.

**Proposta: XSemPLR**

Per affrontare queste sfide, proponiamo **XSemPLR**, un benchmark unificato per il cross-lingual semantic parsing che include:

* **9 set di dati** in vari domini
* **5 compiti di parsing semantico**
* **8 rappresentazioni del significato**
* **22 lingue naturali** appartenenti a 15 famiglie linguistiche

Per una valutazione più completa, consideriamo 6 scenari:

1. **Traduzione-Test:** Utilizziamo Google Translate API per tradurre le query dalla lingua sorgente alla lingua di destinazione, quindi utilizziamo un modello monolingue per l'addestramento e la valutazione.

2. **Modelli Monolingue:** La lingua sorgente è la stessa della lingua di destinazione (es. tedesco-tedesco o inglese-inglese). Testiamo anche un'impostazione di *few-shot learning* con solo il 10% dei dati di addestramento.

3. **Modelli Multilingue:** Addestriamo un unico modello multilingue per tutte le lingue combinando query da diverse lingue (es. tedesco, inglese, cinese) e durante l'inferenza utilizziamo questo modello per tradurre e prevedere query in qualsiasi lingua.

4. **Trasferimento Cross-Lingua Zero-shot e Few-shot:** Addestriamo su una lingua sorgente e trasferiamo le conoscenze ad un'altra lingua. Durante l'addestramento, utilizziamo combinazioni di query inglesi o *few-shot* inglesi e tedesche.

**Risultati**

* **Analisi dei modelli monolingue:** Abbiamo valutato due gruppi di modelli:
    * **Encoder-PTR:** Combina encoder multilingue pre-addestrati con decodificatori basati su pointer (es. XLM-R + PTR e mBERT + PTR)
    * **Encoder-Decoder:** Modelli multilingue pre-addestrati con architetture encoder-decoder (es. mBART e mT5)

I risultati mostrano che i modelli **Encoder-Decoder** ottengono le migliori prestazioni su tutti i 9 set di dati.

* **Confronto Cross-Lingua:** Abbiamo osservato un divario significativo nelle prestazioni tra impostazioni monolingue (verde) e zero-shot cross-lingua (arancione). Il *curse of multilinguality* si manifesta con un leggero calo delle prestazioni inglesi in 7 set di dati e un miglioramento in 3. Con l'impostazione *few-shot*, il divario di trasferimento si riduce rapidamente (blu vs arancione).

**Conclusioni**

Abbiamo sviluppato XSemPLR, un benchmark completo per il cross-lingual semantic parsing. I nostri risultati evidenziano:

* L'efficacia dei modelli **Encoder-Decoder**
* L'importanza del pre-addestramento su inglese per migliorare le prestazioni *few-shot*
* Le limitazioni dei modelli multilingue esistenti come Codex e BLOOM in questo ambito.

Speriamo che XSemPLR diventi uno strumento prezioso per la ricerca sul cross-lingual semantic parsing!</sample>
    <sample id="171">I lavori connessi trattano la protezione dei diritti d'autore nei servizi di embedding basati su modelli linguistici di grandi dimensioni. Esistono quattro categorie principali di metodi proposti nella letteratura:

1. **Metodi basati su watermark tradizionali**: Questi metodi non sono specificamente applicabili ai servizi di embedding o mancano di transferabilità.
2. **Metodi di watermarking non applicabili ai servizi di embedding**: Alcuni approcci non tengono conto delle specifiche esigenze dei servizi di embedding.
3. **Metodi di watermarking con transferabilità limitata**: Alcuni lavori mostrano limitazioni nella capacità di trasferire il watermark ai servizi di un attaccante.
4. **Embedding Marker (il metodo proposto nel paper)**: Un approccio backdoor-based che è specifico per i servizi di embedding, mantiene l'utilità delle embedding fornite e offre un buon livello di covertness e transferabilità.

Il paper introduce Embedding Marker, che si distingue per la sua applicabilità, efficacia e covertness nel proteggere i diritti d'autore dei servizi di embedding.</sample>
    <sample id="172">In base alla presentazione, gli LLM multilingue come Codex e BLOOM **non sono ancora adeguati** per le attività di Cross-Lingual Semantic Parsing (CLSP). Lo studio ha dimostrato che, sebbene questi modelli possano ottenere risultati promettenti, il divario di prestazione tra impostazioni monolingue e cross-lingue è significativo, suggerendo che c'è ancora spazio per miglioramenti.</sample>
    <sample id="174">Nel suo intervento, Thea, co-autrice del paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis", presenta le caratteristiche uniche del dataset creato.

Il problema con i dataset esistenti, spesso raccolti su piattaforme di crowdsourcing, è la loro qualità limitata, la mancanza di diversità (limitati a un numero ristretto di argomenti), la superficialità nell'analisi e l'associazione di ogni argomento a una singola affermazione.

ArgAnalysis35K si distingue per:

* **Dimensioni e qualità:** Con 35.000 coppie di analisi argomentativa, è il dataset più grande nel suo genere, composto per l'85% da argomenti provenienti da dibattiti di alto livello, esperti e debatteri intermedi, garantendo una qualità superiore.

* **Diversità:** Invece di limitarsi a pochi argomenti, il dataset copre 24 temi diversi, catturando il maggior numero possibile di argomenti per ogni tema da fonti affidabili.

* **Analisi approfondita:** Introdotto il concetto di "analisi" che va oltre le semplici affermazioni o premesse, combinandole per fornire una comprensione più completa dell'argomento.

* **Affidabilità annotatori:** Introduce un metodo di affidabilità annotatore istantanea che elimina le valutazioni soggettive basate su pregiudizi specifici, sfruttando al meglio le competenze di tutti gli annotatori.

* **Modello di rilevanza:** Assegna un punteggio di rilevanza a ogni argomento per ogni tema, mostrando come un singolo argomento può essere applicato a contesti diversi.

In conclusione, ArgAnalysis35K offre un dataset più ricco, affidabile e versatile per lo studio dell'analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un approccio di rilassamento continuo GPU-friendly, che approssima il problema NP-hard del "Viaggiatore di Commercio" permettendo il backpropagation e l'apprendimento delle permutazioni linguisticamente più plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a valle (downstream) viene definita in base alla sua capacità di eseguire compiti specifici (come la rilevazione di odio o la rilevazione di notizie false) in modo imparziale e senza discriminare tra diverse categorie sociali o gruppi politici. Questo include la valutazione delle prestazioni del modello per diverse demografie e orientamenti politici, mostrando che modelli con inclinazioni politiche diverse possono avere punti di forza e debolezze differenti in base al tipo di input. In sintesi, un modello NLP è equo quando produce risultati coerenti, accurati e non discriminatori per tutti gli utenti, indipendentemente dalle loro affiliazioni politiche o sociali.</sample>
    <sample id="177">Il nome della relatrice è Yanis Labrak.</sample>
    <sample id="178">Il nome della relatrice è Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presenta la sua ricerca incentrata sul miglioramento delle capacità di "teoria della mente" nei modelli linguistici di grandi dimensioni (LLM). La teoria della mente si riferisce alla capacità di comprendere e ragionare sulle credenze, le intenzioni e le prospettive mentali degli altri. Tradizionalmente, questa abilità viene valutata attraverso compiti di comprensione del testo che coinvolgono più personaggi.

Sclar introduce SymbolicToM, un metodo innovativo per migliorare la teoria della mente nei LLM. Questo approccio utilizza rappresentazioni grafiche esplicite per modellare gli stati mentali dei personaggi. I grafici rappresentano le credenze di un personaggio sulla realtà e sulle credenze degli altri personaggi. L'algoritmo calcola questi grafici per diversi livelli di complessità, consentendo di rispondere a domande relative alla teoria della mente.

Gli esperimenti dimostrano l'efficacia di SymbolicToM su vari LLM, inclusi GPT-3 e GPT-4. I risultati mostrano miglioramenti significativi nelle prestazioni, specialmente nelle domande di secondo ordine che richiedono la comprensione delle credenze altrui. Il metodo è robusto e generalizzabile, come dimostrato da due nuovi dataset progettati per testare la generalizzazione strutturale e linguistica.

In sintesi, SymbolicToM offre un modo plug-and-play per potenziare la comprensione della teoria della mente nei LLM, migliorando la loro capacità di ragionare su situazioni complesse che coinvolgono più personaggi e credenze. La ricerca evidenzia l'importanza di approcci simbolici e grafici per affrontare le carenze dei LLM nella comprensione della mente umana.</sample>
    <sample id="180">Il nome della relatrice è Myra.</sample>
    <sample id="181"># **Distilling Script Knowledge for Constrained Language Planning**

Questo studio si concentra sulla pianificazione linguistica vincolata, un'area critica nell'intelligenza artificiale del linguaggio naturale. Gli autori affrontano la sfida di insegnare ai modelli linguistici di grandi dimensioni (LLM) a pianificare azioni con specifici vincoli, come la preparazione di un "torta al cioccolato", che richiede un'attenzione diversa rispetto a un generico "fare una torta".

La ricerca introduce il problema della pianificazione linguistica vincolata, sottolineando la necessità di script realistici e coerenti con i vincoli. Poiché i dati specifici mancano, gli autori utilizzano un approccio innovativo per acquisire questi obiettivi. Estendono gli obiettivi astratti con vincoli multi-facettati e coinvolgono gli esseri umani nella creazione di 100 obiettivi specifici. L'analisi dei risultati rivela che i modelli linguistici tradizionali lottano con la fedeltà ai vincoli.

Per migliorare le prestazioni, viene implementata una tecnica di "over-generate-then-filter". Questo processo coinvolge la generazione di più script e la selezione di quelli più coerenti con i vincoli utilizzando la somiglianza semantica e le ricompense basate su parole chiave. Il risultato è un miglioramento significativo nella qualità degli script generati.

Inoltre, gli autori creano un dataset, CoScript, che contiene 55.000 obiettivi specifici e script associati. CoScript dimostra una diversità elevata, consentendo la formazione di modelli più piccoli ma specializzati. Gli esperimenti dimostrano che i modelli più piccoli, come T5, possono generare script di alta qualità dopo la formazione su CoScript, sfidando l'idea che i modelli più grandi siano sempre superiori.

In sintesi, questo lavoro contribuisce a una migliore comprensione della pianificazione linguistica vincolata e fornisce risorse preziose per la ricerca futura.</sample>
    <sample id="182">Nel contesto dell'articolo, il tropicalismo si riferisce a un insieme di stereotipi e tratti associati a donne di origine latina, come "vibrant" (vibrante) e "curvaceous" (curvosa), che evocano immagini esotiche e tropicali, rafforzando una rappresentazione stereotipata.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target attraverso due metodi principali:

1. **Generazione di Personas**: Hanno utilizzato prompt specifici per generare descrizioni di persone appartenenti a diversi gruppi demografici (ad esempio, "Immagina di essere una donna asiatica. Descrivi te stessa"). Questo ha permesso di confrontare le generazioni del modello con risposte umane, rivelando stereotipi e modelli interessanti.

2. **Metodo Marked Words**: Questo metodo identifica le parole che distinguono i gruppi "marcati" (marginalizzati) da quelli "non marcati" (dominanti). Utilizzando log-odds ratios, il metodo evidenzia le parole chiave che caratterizzano ciascun gruppo marcato, rivelando stereotipi e narrazioni essenzializzanti nascosti dietro descrizioni apparentemente positive.</sample>
    <sample id="184">Nel lavoro presentato, è stato utilizzato **CXMI (Context eXpedient Measure for Information)** per misurare l'utilizzo del contesto durante la traduzione automatica. CXMI valuta quanto informazione un contesto (C) fornisce sul target (Y) dato la fonte (X). Una versione estesa chiamata **Pointwise CXMI** misura l'utilizzo del contesto a livello di frase o parola.</sample>
    <sample id="185">La differenza principale tra DrBERT e ChuBERT risiede nelle **fonti di dati** su cui sono stati addestrati:

* **DrBERT** è stato addestrato su **NACHOS**, un dataset di testo medico web-crawled in francese.
* **ChuBERT** è stato addestrato su **dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes** (un contesto clinico).

Inoltre, DrBERT è stato addestrato **da zero** con set di dati più grandi (7GB vs 4GB), mentre ChuBERT utilizza una versione ridotta del dataset NACHOS (4GB) combinata con note cliniche.</sample>
    <sample id="187">Basandomi sul contenuto fornito, ci sono due autori coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento (Iterative Transfer Learning) è una strategia che combina il trasferimento del peso da compiti correlati con l'apprendimento attivo. In questo approccio, il modello viene inizialmente addestrato su compiti simili (come la classificazione di stanze di dibattito o la classificazione di espansioni e confronti) per acquisire conoscenze di base. Successivamente, il modello viene raffinato iterativamente, prima su un compito di base e poi sul compito specifico della dissonanza, migliorando così la sua capacità di rilevare dissonanze nel linguaggio.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities Corpus è fornire una risorsa per addestrare e valutare i modelli linguistici (LLM) nella comprensione delle espressioni indirette di riferimento per la selezione delle entità. Il set di dati copre tre domini diversi (musica, libri e ricette) e raccoglie espressioni indirette utilizzate dagli utenti per scegliere tra alternative, come "Easy on Me" e "I Gotta Feeling". Il set di dati aiuta a migliorare la capacità dei modelli di gestire ambiguità e preferenze nell'interazione conversazionale.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un servizio di embedding come EaaS (Embedding as a Service) sfruttando tecniche di apprendimento da embedding. Recentemente, è stato dimostrato che gli attaccanti possono rubare modelli apprendendo dagli embedding forniti e replicando servizi simili. Il modo in cui ciò può avvenire è il seguente:

1. **Apprendimento dagli Embedding**: L'attaccante raccoglie e analizza gli embedding forniti dal servizio EaaS per comprendere la struttura e i parametri del modello sottostante.
2. **Replica del Modello**: Utilizzando gli embedding appresi, l'attaccante può ricostruire o quantificare il modello, ottenendo così i suoi parametri.
3. **Fornire Servizi Simili**: Una volta ottenuti i parametri, l'attaccante può offrire un servizio simile, potenzialmente con performance degradate ma comunque capaci di ingannare gli utenti.

Per contrastare questa minaccia, il documento presentato introduce un metodo di watermarking chiamato "Embedding Marker", che utilizza backdoor per inserire e verificare watermark nei servizi EaaS, migliorando la protezione del copyright.</sample>
    <sample id="191">L'articolo coinvolge tre autori: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Lo studio presentato, intitolato "CAME: Confidence-guided Adaptive Memory Efficient Optimization", affronta la sfida di sviluppare un ottimizzatore per la formazione di grandi modelli linguistici che offra sia una rapida convergenza che un basso consumo di memoria. Gli ottimizzatori adattivi tradizionali, come Adam, richiedono una grande quantità di memoria per conservare le stime dei momenti primi e secondi dei gradienti per ogni parametro. Gli ottimizzatori memoria-efficienti esistenti, come Adafactor, riducono il consumo di memoria ma a scapito delle prestazioni.

Gli autori propongono CAME, che si basa sulla non-negativa fattorizzazione di matrici (NMF) per ridurre la memoria necessaria. Risolvendo un problema di divergenza minima, Adafactor presenta errori che rallentano la convergenza. CAME affronta questi errori introducendo un fattore di confidenza basato sul divario tra il momento previsto e quello generato.

Gli esperimenti su dataset come BookCorpus e English Wikipedia dimostrano che CAME supera Adafactor e, in alcuni casi, anche Adam, migliorando la precisione di validazione del 3,4% con lo stesso numero di passi di formazione. CAME mostra anche una riduzione significativa del consumo di memoria, specialmente con batch di dimensioni più grandi.

Inoltre, CAME si è dimostrato efficace nella formazione di modelli BERT-based, mantenendo prestazioni comparabili al baseline con una memoria inferiore. La sua capacità di gestire grandi batch lo rende un'estensione promettente per gli ottimizzatori memoria-efficienti esistenti.</sample>
    <sample id="193">Nel contenuto fornito, non è specificato il numero esatto di annotatori impiegati per creare il set di dati iniziale. Tuttavia, si menziona che è stato utilizzato un "dissonanza-first approach" e che sono stati raccolti circa 1000 esempi di coppie di unità discorsive, suggerendo un processo di annotazione collaborativo con più individui.</sample>
    <sample id="194">Gli autori dell'articolo sono affiliati a diverse istituzioni:

- Carnegie Mellon University (Jenny, la presentatrice)
- University of Washington (Sebastian Santy, Ronan Le Bras)
- Allen Institute for AI (Katharina Reinecke, Maarten Sap)</sample>
    <sample id="195">Il nostro lavoro, "Reasoning over Hierarchical Question Decomposition Tree per Explainable Question Answering" (RoHT), affronta le sfide dell'Explainable Question Answering (XQA) integrando conoscenze da fonti eterogenee.

Esistono due approcci principali nell'XQA: neuro-simbolici (che utilizzano rappresentazioni formali come SPARQL) e basati sulla decomposizione (che generano passaggi intermedi in linguaggio naturale). Tuttavia, entrambi hanno limitazioni. I metodi neuro-simbolici dipendono da KB strutturati, limitati nella completezza, mentre quelli basati sulla decomposizione faticano con la diversità del linguaggio naturale.

RoHT propone un nuovo framework a due fasi:

1. **Costruzione dell'HQDT**: Crea una struttura gerarchica della domanda complessa, suddividendola in sotto-domande e identificando le domande atomiche.
2. **Ragionamento probabilistico**: Fusione di conoscenze da KB e testi corporei a diversi livelli dell'HQDT. Considera la probabilità di generazione stringa e di risposta, determinando le fonti più appropriate per ogni sotto-domanda e aggregando le risposte con le probabilità più alte.

RoHT è stato testato su dataset complessi come KQA Pro e Musique, dimostrando prestazioni superiori a metodi esistenti, sia con solo KB che con integrazione di testi. Il nostro approccio evidenzia i vantaggi della decomposizione esplicita e dell'utilizzo congiunto di conoscenze strutturate e non strutturate.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa". In questo caso, "Bart and Lisa" è una coordinazione di due nomi (o conjuncts) con "I saw" come governatore posizionato a sinistra. Il governatore, in questo contesto, è la parola che collega le due parti della coordinazione.</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo, come evidenziato nello studio, includono quattro modelli di chat di stato dell'arte selezionati per l'analisi. Questi modelli sono stati valutati utilizzando diverse metodologie, tra cui ABC-Eval (annotazione comportamentale nel dialogo), Likert ratings a livello di turno e dialogo, e confronti dialogici a coppie.

Lo studio ha concluso che ABC-Eval fornisce valutazioni più affidabili e predittive della qualità del dialogo rispetto ai metodi esistenti, identificando con precisione vari comportamenti e errori comuni nei modelli di chat.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici moderni hanno finestre di contesto sempre più ampie. La valutazione tradizionale basata su coppie minime (MPP) non consente di esaminare come i modelli gestiscono l'accettabilità in sequenze più lunghe, che è cruciale per comprendere appieno le loro capacità e potenziali bias.

Il paper propone un approccio innovativo per simulare sequenze più lunghe, utilizzando dati set esistenti e ricreando frasi con strutture grammaticali corrispondenti. Questo permette di testare come i modelli rispondono a contesti diversi, inclusi quelli non correlati, rivelando la loro sensibilità a latenti caratteristiche sintattiche e semantiche.</sample>
    <sample id="199">Sì, secondo la presentazione, la formazione di modelli multilingue ha causato un calo delle prestazioni in sette dei nove dataset, con solo un miglioramento in tre. Questo fenomeno è noto come il "Curse of Multilinguality" (Maledizione della multilingue), dove l'aggiunta di lingue diverse dall'inglese può ridurre le prestazioni del modello su tale lingua.</sample>
    <sample id="200">Sì, gli annotatori conoscono i nomi delle entità in anticipo, ma non necessariamente i dettagli specifici delle stesse. Ricevono una breve descrizione o informazioni di sfondo per aiutarli a fare una scelta informata.</sample>
    <sample id="201">Le metriche di valutazione utilizzate per la traduzione automatica (MT) nello studio includono:

1. **BLEURT**: Una metrica di valutazione automatica che misura la qualità della traduzione in base alla somiglianza con una traduzione di riferimento.
2. **Metriche neurali di MT**: Metriche avanzate basate su reti neurali che valutano la qualità della traduzione.
3. **Valutazione umana con MQM (Multidimensional Quality Metrics)**: Un framework di valutazione umana che fornisce giudizi qualitativi sulla fluidità e l'accuratezza delle traduzioni.</sample>
    <sample id="202">Sì, il nostro studio suggerisce che la generalizzazione è un problema rilevante per i modelli NER addestrati con dati più vecchi, come quelli del dataset CoNLL-2003. Abbiamo osservato che le prestazioni di questi modelli tendono a diminuire quando applicati a dati più recenti (come il dataset CoNLL++), il che è attribuito principalmente al *temporal drift* (deriva temporale) causato dalla disparità tra i dati di addestramento e quelli di test.

Quindi, la risposta concisa è: **Sì, il regresso nella generalizzazione influisce specificamente sui modelli NER addestrati con dati datati.**</sample>
    <sample id="203">La posizionalità nella NLP (Natural Language Processing) è importante perché evidenzia le differenze sistematiche nelle prestazioni delle tecnologie tra diverse popolazioni. I modelli e i dataset riflettono le prospettive e le esperienze dei creatori e degli annotatori, che possono essere influenzate da demografia, identità e vita personale. Studiare questa posizionalità è cruciale man mano che i compiti NLP diventano più soggettivi e socialmente orientati, permettendo di identificare e mitigare i pregiudizi che possono lasciare alcuni gruppi indietro.</sample>
    <sample id="204">Secondo la presentazione, gli LLM multilingue come BLOOM **non sono stati adeguatamente affinati** per le attività di cross-lingual semantic parsing. Il testo afferma chiaramente che "abbiamo trovato che modelli multilingue linguistici come Codex e BLOOM sono ancora inadeguati per i compiti di cross-lingual semantic parsing".

Quindi, la risposta è: **nessuna messa a punto, solo valutazione**.</sample>
    <sample id="205">Lo studio presentato da Shangbin, PhD student all'Università di Washington, esplora il problema del pregiudizio politico nei modelli linguistici addestrati su grandi quantità di dati web, inclusi contenuti di notizie politiche. La ricerca evidenzia come le diverse prospettive politiche presenti nei dati di pre-addestramento possano portare a potenziali questioni di equità nelle applicazioni NLP.

L'indagine si concentra su tre aspetti principali:

1. **Valutazione del pregiudizio politico**: Gli autori propongono l'uso di questionari politici, come il Political Conference Test, per valutare le inclinazioni politiche dei modelli linguistici. I risultati mostrano che questi modelli presentano vari gradi di pregiudizio politico, con GPT-4 che emerge come il più liberale.

2. **Influenza dei dati di addestramento**: Esperimenti controllati dimostrano che l'addestramento aggiuntivo su corpora partitici, sia di notizie che di social media, modifica le coordinate ideologiche dei modelli. Ad esempio, l'addestramento di RoBERTa su un corpus di sinistra porta a un'inclinazione liberale più pronunciata.

3. **Impatto sui compiti downstream**: L'analisi delle prestazioni dei modelli con diversi pregiudizi politici in compiti come la rilevazione di odio e la detectione di fake news rivela disparità. Modelli di sinistra eccellono nella rilevazione di odio verso minoranze, mentre quelli di destra sono più efficaci nel rilevare odio verso gruppi dominanti. Simili tendenze si osservano nella detectione di fake news.

Lo studio evidenzia un dilemma: rimuovere le opinioni politiche dai dati di addestramento potrebbe portare a censura, mentre mantenerle potrebbe causare pregiudizi. Gli autori sottolineano l'urgente necessità di affrontare le questioni di equità legate ai pregiudizi politici nei modelli linguistici.</sample>
    <sample id="206">Nel loro lavoro, gli autori fanno ricorso a **modelli di trasferimento dell'apprendimento** per migliorare la rilevazione della dissonanza, affrontando la sfida della rarità della classe.

In particolare, utilizzano:

* **Trasferimento da compiti correlati:** come la classificazione della posizione sulla dissonanza nei dibattiti (debate) e la classificazione delle espansioni e delle comparazioni nel PDTB.
* **Active Learning:** per raccogliere più dati etichettati su dissonanza con un numero ridotto di iterazioni di annotazione.

Combinano queste tecniche per ottenere un modello efficace che può rilevare la dissonanza anche con una bassa frequenza di occorrenza.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM in questo studio includono:

1. **Test set più recenti**: Per evitare che i dati di test si sovrappongano ai dati di addestramento del modello, sono stati utilizzati i test set più recenti disponibili.
2. **WMT (Workshop on Machine Translation) evaluation**: Il sistema è stato confrontato con i migliori sistemi di traduzione automatica secondo gli standard WMT.

Inoltre, sono state utilizzate metriche di valutazione neurale di stato dell'arte e una valutazione umana basata sull'esperienza per ottenere una visione completa delle prestazioni del modello.</sample>
    <sample id="208">Gli autori hanno proposto **tre** suggerimenti alla fine del loro lavoro:

1. Affrontare attivamente gli stereotipi positivi e le narrazioni essenzialiste.
2. Utilizzare un approccio intersezionale per studiare i pregiudizi e i danni per evitare di trascurare aspetti importanti.
3. Aumentare la trasparenza sui metodi di mitigazione dei pregiudizi, come ad esempio il valore di allineamento eccessivo o altri metodi anti-stereotipazione.</sample>
    <sample id="209">Il metodo proposto offre diversi vantaggi rispetto al metodo di riferimento:

1. **Miglioramento della Qualità delle Script**: Implementando una strategia di "over-generate-then-filter", il metodo migliora la fedeltà alle restrizioni e la completezza semantica delle script generate dai modelli di linguaggio.

2. **Riduzione dei Costi**: Permette l'uso di modelli più piccoli e specializzati per il pianificazione linguistica, riducendo i costi associati all'implementazione di modelli di grandi dimensioni.

3. **Creazione di un Dataset di Alta Qualità**: Genera un dataset, CoScript, di 55.000 specifici obiettivi con script, fornendo una risorsa preziosa per la ricerca sulla pianificazione linguistica.

4. **Varietà e Affidabilità**: CoScript mostra una vasta gamma di obiettivi specifici, aumentando la diversità e l'affidabilità del dataset.

5. **Superamento dei Modelli Più Grandi**: Dimostra che modelli più piccoli, fine-tunati su CoScript, possono generare script di qualità superiore rispetto alla maggior parte dei modelli di grandi dimensioni.</sample>
    <sample id="210">Il nome della relatrice è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati presentati nell'articolo possono essere utilizzati come parametri di riferimento. Gli autori hanno creato un nuovo corpus, DEPLAIN, con paralleli coppie di frasi tedesche complesse e semplificate manualmente e automaticamente allineate. Questo consente di:

1. **Valutare metodi di allineamento automatico** tra frasi di documenti paralleli di uguale lingua ma diversi livelli di complessità.
2. **Sviluppare e valutare modelli di semplificazione automatica del testo** attraverso il fine-tuning di modelli linguistici come long-mBART e base mBART.

I risultati ottenuti, inclusi i punteggi e le metriche di valutazione, possono servire come benchmark per futuri lavori nel campo della semplificazione automatica del testo in tedesco.</sample>
    <sample id="212">Nell'articolo, viene menzionato l'utilizzo di **un modello più piccolo**, specificamente T5 fine-tuned su CoScript, per generare script di alta qualità.

Quindi, la risposta è **1**.</sample>
    <sample id="213">Il modello di base utilizzato per analizzare l'ottimizzazione delle istruzioni multimodali è OFA (Unified Multi-Modal Pre-trained Model).</sample>
    <sample id="215">Adam Przepiórkowski presenta la sua ricerca sul *Dependency Structure of Coordination*, esplorando diverse teorie e approcci corpus-based che descrivono la struttura della coordinazione nelle frasi. Si concentrano su due tipi di approcci: asimmetrici e simmetrici.

Gli approcci asimmetrici, come quelli proposti da Universal Dependencies e dalla teoria di Igor Mel'čuk, identificano un congiunto come capo della struttura coordinata, spesso il primo. Al contrario, l'approccio *Prague* e la *Word Grammar* di Hudson suggeriscono strutture simmetriche, dove tutti i congiunti hanno lo stesso ruolo di capo.

Przepiórkowski propone un argomento a favore delle strutture simmetriche basandosi sul *principio di minimizzazione della lunghezza della dipendenza*. Esempia la preferenza grammaticale per la vicinanza del diretto oggetto al verbo e come questa possa essere bilanciata dalla lunghezza del nome proprio. Dimostra che, in alcune strutture, la violazione di questa regola è accettabile se si soddisfa il principio di minimizzazione della lunghezza.

Analizzando i dati del Penn Treebank, scoprono che i congiunti di sinistra tendono ad essere più corti, specialmente quando il governante è a sinistra o assente. Questa osservazione supporta l'idea che le strutture simmetriche siano preferibili alle asimmetriche, poiché soddisfano il principio di lunghezza minima delle dipendenze.

In sintesi, la ricerca di Przepiórkowski offre una nuova prospettiva sulla struttura della coordinazione, suggerendo che la simmetria sia una caratteristica più naturale rispetto agli approcci asimmetrici.</sample>
    <sample id="217">Il lavoro "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" si concentra sulla generazione di dialoghi controllabili che considerano più attributi contemporaneamente, affrontando le limitazioni dei metodi esistenti.

L'approccio attuale ignora la complessità della generazione di dialoghi con più attributi, mentre i metodi precedenti si concentrano su singoli attributi. Gli autori propongono **DCG (Disentangled Controllable Generation)**, un modello che apprende concetti di attributi da valori osservati e utilizza una perdita di disaccoppiamento per gestire combinazioni di attributi diverse.

Viene introdotto **MAE (Multi-Attribute Evaluation framework)**, un metodo di valutazione unificato che non richiede dati etichettati aggiuntivi. Questo framework valuta la qualità e il controllo dei dialoghi generati a diversi livelli di granularità degli attributi.

Il lavoro introduce due benchmark e dimostra l'efficacia di DCG attraverso esperimenti. Il modello si basa su DialoGPT con un modulo di prompt composizionale. Sono stati progettati due tipi di prompt: **attributo-orientato** per controllare informazioni specifiche e **task-orientato** per migliorare la coerenza testuale.

I risultati dimostrano che DCG supera altri modelli in termini di controllo degli attributi e qualità del testo, sia per combinazioni viste che non viste. Il metodo automatizzato MAE si è dimostrato più efficace di metriche tradizionali sia per attributi discreti che continui.

In sintesi, lo studio propone una soluzione innovativa per la generazione di dialoghi controllabili multi-attributo, aprendo la strada a dialoghi più flessibili e realistici.</sample>
    <sample id="218">Gli autori dell'articolo "Prompting PaLM for Translation: Assessing Strategies and Performance" sono affiliati a Google Translate, come menzionato da David Vilar nella sua presentazione.</sample>
    <sample id="219">Jia-Huei Ju, ricercatore dell'Academia Sinica, presenta il lavoro di ricerca "Un pipeline multistadio per l'identificazione di segnali finanziari nei report finanziari". Lo studio, condotto con Yu-Shiang Huang, Cheng-Wei Lin e i professori Che Lin e Chuan-Ju Wang, si concentra sull'analisi dei report finanziari annuali (Form 10-K) richiesti dalla SEC.

L'osservazione chiave è la forte somiglianza testuale (80% di token identici) e la dipendenza annuale dei contenuti. Da questa osservazione, gli autori propongono un'attività di evidenziazione e un pipeline in più fasi. Il compito prevede il confronto e il contrasto del contesto tra il report dell'anno in esame e quello dell'anno precedente.

Il pipeline comprende:
1. Segmentazione del documento.
2. Riconoscimento delle relazioni.
3. Fine-tuning fuori e dentro il dominio.

Durante il fine-tuning, i modelli vengono addestrati su un dataset esterno (eSNLI) e su coppie di revisione e mismatch. L'uso di tecniche di soft labeling migliora la qualità delle etichette pseudo.

Gli autori valutano le prestazioni utilizzando metriche come precisione e correlazione tra previsioni e annotazioni. I risultati dimostrano l'efficacia del loro approccio, che mostra una buona generalizzazione anche su dati non utilizzati durante l'addestramento.

Il lavoro introduce un'attività di evidenziazione e un pipeline semplice, lasciando spazio a futuri miglioramenti e integrazioni con tecniche di recupero delle informazioni.</sample>
    <sample id="220">Gli autori dell'articolo "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" accettato per ACL 2023 sono affiliati a Stony Brook University, dove uno dei co-autori, Vasudha, è una PhD candidate in Computer Science.</sample>
    <sample id="221">Nell'articolo "Prompting PaLM for Translation: Assessing Strategies and Performance", sono state analizzate principalmente coppie linguistiche formate da tedesco verso inglese (German-to-English) e altre lingue, sebbene l'analisi si concentri principalmente sulla valutazione delle strategie di prompting per il modello PaLM. L'uso dei dati di valutazione WMT (Workshop on Machine Translation) implica che altre coppie linguistiche, come francese-inglese, spagnolo-inglese, ecc., siano state considerate indirettamente attraverso il confronto con sistemi di traduzione di riferimento.</sample>
    <sample id="222"># **Adattamento o Annotazione: Affrontare le Sfide del Trasferimento di Dominio nella Risposta alle Domande Aperte**

Questo lavoro esplora strategie per migliorare la generalizzazione di modelli di risposta alle domande aperte (Open-Domain QA) in nuovi domini, affrontando le sfide poste dai cambiamenti nel dominio. L'obiettivo è consentire ai modelli di adattarsi a domini specifici senza una formazione estesa.

La ricerca si concentra su un'impostazione in cui un modello di base, addestrato su Wikipedia, deve rispondere a domande in un nuovo dominio, come la biomedicina. Viene evidenziato un problema di discriminazione quando il modello si trova di fronte a passaggi di testo specifici del nuovo dominio, portando a risposte errate.

Vengono proposti tre contributi principali:

1. **Interventi sui dati**: Esplorano tecniche few-shot e zero-shot per generare dati di addestramento aggiuntivi. I metodi few-shot coinvolgono grandi modelli linguistici per creare esempi mirati, mentre i metodi zero-shot controllano le interazioni tra domande, risposte e contesti.

2. **Identificazione del cambiamento di dominio**: Classificano il tipo di cambiamento di dominio (nessun cambiamento, concetto, covariata o completo) utilizzando una misura di compatibilità calcolata su un sottoinsieme di domande/risposte/contesti.

3. **Interventi mirati**: Determinano che gli interventi few-shot sono efficaci per la maggior parte dei domini, mentre i metodi zero-shot sono utili per i domini con cambiamenti concettuali o di covariata.

I risultati dimostrano miglioramenti significativi nelle prestazioni del lettore, fino al 24%, con interventi mirati al tipo di cambiamento di dominio. Questo lavoro fornisce una comprensione approfondita di come i modelli Open-Domain QA possano adattarsi a nuovi domini con strategie di intervento efficaci.</sample>
    <sample id="223">Il nome della relatrice è Shangbin. È una PhD student presso l'Università di Washington e sta presentando la sua ricerca intitolata "Da dati di pre-addestramento a modelli linguistici verso compiti downstream: Tracciare i sentieri dei pregiudizi politici che portano a modelli NLP ingiusti".</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati due modelli principali:

1. **long-mBART**: Fine-tunato per produrre simplificazioni a livello di documento.
2. **mBART (base)**: Fine-tunato per produrre simplificazioni a livello di frase.

Entrambi i modelli hanno dimostrato risultati promettenti, superando i punteggi di base e stabilendo un nuovo benchmark per il problema dell'automatica semplificazione del testo.</sample>
    <sample id="225">Delle 62 attività diverse in MultiInstruct, **53 attività vengono utilizzate per l'addestramento** e il resto **riservato per il test**.

Più specificamente, per l'addestramento vengono utilizzati 53 attività da 9 gruppi, mentre per il test viene riservata l'intera categoria "Common Sense Reasoning" e ulteriori 5 attività da "VQ" e "Miscellaneous".</sample>
    <sample id="226">L'articolo menziona due autori coinvolti nella presentazione: Regina Stodden e Omar.</sample>
    <sample id="227">Il testo discute la sfida della comprensione linguistica radicata (Grounded Language Understanding, GLU), ovvero la capacità di tradurre espressioni naturali in azioni eseguibili in un ambiente specifico. Gli autori sottolineano che la mancanza di grounding durante la pre-formazione rende difficile per i modelli linguistici attuali eseguire compiti GLU, come la generazione di query SQL per database o istruzioni per robot.

Viene introdotto un nuovo approccio, chiamato Pangu, che separa il processo di discriminazione da quello di generazione. Invece di far generare direttamente ai modelli linguistici un piano o un programma, Pangu utilizza un agente simbolico per proporre candidati piani e un modello linguistico per valutare e classificare questi piani. Questo approccio riduce la necessità per il modello linguistico di gestire la grammatica e la validità del piano.

Gli autori hanno testato Pangu in vari scenari, tra cui la risposta a domande basate su conoscenze, utilizzando modelli linguistici come BERT, T5 e Codex. I risultati dimostrano l'eccellente performance di Pangu, inclusa una forte efficienza campionaria. L'analisi rivela che i modelli autoregressivi tendono all'overfitting, mentre Pangu mostra una maggiore robustezza.

In conclusione, il messaggio chiave è che la generazione potrebbe non essere la strategia ottimale per la GLU, e che la discriminazione offre un approccio più efficace per sfruttare i modelli linguistici in questo ambito.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam. Inoltre, hanno utilizzato il dataset Wiki Text per contare la frequenza delle parole.</sample>
    <sample id="229">Il discorso di Gabriella Skitalinskaya si concentra sull'importanza della revisione del testo nell'ambito della scrittura argomentativa e sulla creazione di strumenti per aiutare i scrittori novizi a migliorare le loro affermazioni. Insieme a Henning Wachsmuth, ha sviluppato un lavoro che mira a identificare le affermazioni che necessitano di revisione e a suggerire miglioramenti.

L'intervento introduce due compiti: il primo, "Rilevamento di Affermazioni Subottimali", consiste nel determinare se una dichiarazione richiede revisione o può essere considerata ben formulata. Il secondo compito è "Suggerimento di Miglioramento delle Affermazioni", dove l'obiettivo è identificare i tipi di problemi di qualità da migliorare.

La ricerca esplora le sfide nell'utilizzo dei dati basati sulla revisione, considerando le differenze tra domini e le varie percezioni della qualità. Si concentra sull'argomentazione testuale, analizzando come modellare la qualità dei testi argomentativi utilizzando piattaforme di dibattito online collaborative come Kialo.

Skitalinskaya evidenzia quattro sfide principali: rappresentatività e affidabilità dei dati, complessità e architettura del modello, dipendenza dal contesto e pregiudizi topici e degli utenti. La ricerca propone soluzioni e confronta approcci per affrontare queste sfide, dimostrando l'efficacia dei dati basati sulla revisione per i compiti proposti. Lo studio conclude che la modellazione della distanza tra versioni di affermazioni è utile per rilevare le affermazioni subottimali e che l'impatto del contesto varia a seconda del compito e dei problemi di qualità del testo.</sample>
    <sample id="231">NACHOS è un dataset di dati medici crawlat dal web, utilizzato come fonte di addestramento per il modello DrBERT, il primo modello biomedico in francese.</sample>
    <sample id="232">Il nome della relatrice è David Vilar.</sample>
    <sample id="233">Sara Papi, dell'Università di Trento e della Fondazione Bruno Kessler, presenta il suo studio "Attention as a Guide for Simultaneous Speech Translation".

Il *Simultaneous Speech Translation* (SimulST) consente la traduzione in tempo reale del parlato in un testo in una lingua diversa, favorendo la comunicazione interculturale. I modelli SimulST attuali presentano però svantaggi: richiedono architetture specifiche, processi di addestramento lunghi e complessi, e la gestione di diversi modelli per vari ritardi.

La soluzione proposta da Papi e colleghi è *EDAtt* (Encoder-Decoder Attention), una strategia che sfrutta l'attenzione tra input audio e output testuale dei modelli di traduzione automatica pre-addestrati. EDAtt decide se emettere o meno una parte della traduzione in base alla concentrazione dell'attenzione. Se l'attenzione non è focalizzata (somma sotto una soglia α negli ultimi λ frame audio), la parola non viene emessa, in attesa di ulteriori informazioni.

I risultati mostrano che EDAtt supera le strategie basate su modelli offline, sia in termini di qualità della traduzione (misurata con BLEU) che di latenza (con e senza considerazione dei tempi computazionali). Inoltre, EDAtt è il metodo più veloce.

Lo studio, disponibile in open source con codice e modelli, evidenzia l'efficacia di EDAtt nel migliorare le prestazioni e l'efficienza del SimulST.</sample>
    <sample id="234">La strategia del prompting influisce significativamente sui risultati della traduzione con grandi modelli linguistici (LLM) come PaLM. Lo studio ha dimostrato che:

1. **Prompts di alta qualità sono cruciali**: La qualità degli esempi utilizzati per il prompting è più importante della somiglianza con la frase di origine.
2. **Numero di prompts**: Una strategia di 5-shot prompting, dove vengono forniti diversi esempi, ha un impatto minore sulla forma del prompt rispetto ai prompts di una o zero shot. Gli esempi di alta qualità sono determinanti.
3. **Scelta dei dati**: Utilizzare dati di valutazione curati (come il dev data) rispetto ai dati di addestramento più rumorosi può migliorare le prestazioni.

Nonostante ciò, i sistemi specializzati di stato dell'arte ancora superano PaLM, ma PaLM si avvicina molto alle prestazioni di un sistema commerciale come Google Translate, specialmente in termini di fluidità, con errori principali dovuti all'omissione di parti della frase di origine.</sample>
    <sample id="235">Gli autori dell'articolo "When Does Translation Require Context?" sono affiliati con le seguenti istituzioni:

- Patrick Fernandes
- Emmy Liu
- André F. T. Martins
- Graham Neubig

Tutti collaborano o hanno collaborato con Kayo Yin nella ricerca presentata.</sample>
    <sample id="236">Secondo la presentazione, per ogni task nel dataset MultiInstruct, sono disponibili **cinque istruzioni scritte da esperti**. Queste istruzioni sono progettate per guidare il modello multi-modale OFA nell'esecuzione di compiti specifici.

L'obiettivo della ricerca è valutare l'efficacia dell'instruction tuning per migliorare le prestazioni del modello su compiti multi-modali mai visti prima.</sample>
    <sample id="237">Gli autori propongono il KITMUS Test (Knowledge Integration from Multiple Sources Test), una suite di test diagnostici progettata per valutare la capacità dei modelli di linguaggio naturale di integrare e utilizzare informazioni provenienti da più fonti: conoscenza contenuta nei parametri pre-addestrati e conoscenza fornita negli input durante l'inferenza.

Il test include un'attività di risoluzione dei coreferenti, che mette alla prova la capacità del modello di sfruttare conoscenze specifiche dell'entità e conoscenze di sfondo. Gli autori variano la disponibilità di queste due tipologie di informazioni in tre diversi scenari: "Background-Pretrain", "Background-Both" e "Background-Inference", quest'ultimo simulando casi in cui le conoscenze di sfondo necessarie non sono presenti nei dati di addestramento pregresso.</sample>
    <sample id="238">Nel suo discorso, Yebowen Hu della University of Central Florida presenta **MeetingBank**, un nuovo dataset di benchmark per la sintesi di riunioni.

Il dataset è stato creato per soddisfare la crescente necessità di tecnologie di sintesi per domini di lettura diversi, in particolare per le riunioni. MeetingBank contiene trascrizioni, riassunti di riferimento e risorse aggiuntive di 1.366 riunioni del consiglio comunale, per un totale di quasi 7.000 istanze.

Hu illustra il processo di raccolta dati, che include la conversione audio in testo tramite Speechmatics API e l'estrazione di riassunti e segmenti di riunione da siti web dedicati.

L'analisi delle statistiche del dataset rivela che i riassunti delle riunioni variano in termini di livello di astrazione, con alcuni che includono molte parole verbatim e altri più sintetici.

Vengono valutati diversi sistemi di sintesi, sia estrattivi che abstrattivi, tra cui BART-Large, Pagasus e GPT-3. I risultati mostrano che, mentre i sistemi estrattivi come Extr-Oracle ottengono buoni punteggi ROUGE-2, i modelli abstrattivi come DialogLM superano in termini di qualità percepita da valutatori umani, con GPT-3 che si distingue per fluidità e coerenza, ma con punteggi inferiori in termini di informatività e fattualità.

MeetingBank offre una risorsa preziosa per la ricerca sulla sintesi di riunioni e fornisce approfondimenti sul processo decisionale dei consigli comunali.</sample>
    <sample id="239">## Recensione del Paper: "Prompting PaLM per la Traduzione: Valutazione di Strategie e Prestazioni"

Ciao a tutti, mi chiamo David Vilar e oggi vi presento una breve recensione del paper "Prompting PaLM per la Traduzione: Valutazione di Strategie e Prestazioni", un lavoro congiunto con i colleghi di Google Translate.

PaLM è un modello linguistico di grandi dimensioni, con 540 miliardi di parametri, presentato lo scorso anno nel 2022. È stato addestrato su una vasta raccolta di testi, comprendente 780 miliardi di token, e al momento della pubblicazione ha raggiunto risultati di stato dell'arte in centinaia di compiti NLP.

In questo studio, presentiamo la prima analisi sistematica dell'utilizzo dei prompt per i modelli linguistici di grandi dimensioni (LLM) nella traduzione automatica. Abbiamo valutato la capacità di traduzione di questi modelli seguendo le migliori pratiche della comunità di MT. Ciò include l'utilizzo degli ultimi set di dati di test per evitare sovrapposizioni con i dati di addestramento del modello linguistico. Abbiamo confrontato le prestazioni con i sistemi di stato dell'arte, come quelli presenti nella competizione WMT.

Abbiamo impiegato metriche di traduzione automatica neurale all'avanguardia e, in aggiunta, abbiamo ottenuto risultati da valutazioni umane basate sull'esperienza. Infine, abbiamo fornito alcune raccomandazioni per le strategie di selezione dei prompt.

Come dimostrato da un semplice esperimento, la scelta del prompt ha un impatto significativo sulle prestazioni degli LLM per la traduzione. Utilizzando un approccio di prompting a "un-shot" e fornendo due diversi prompt per ogni frase, abbiamo osservato una differenza di oltre un punto BLEU su 1000 frasi. In casi estremi, la differenza può raggiungere i 40 punti BLEU.

Nei nostri esperimenti, abbiamo optato per una strategia di prompting a "5-shot", dove ogni frase fornita al sistema è contrassegnata con la lingua di appartenenza. Ad esempio, per la traduzione da tedesco a inglese, le frasi di origine in tedesco sono contrassegnate con ":tedesco" e le traduzioni in inglese con ":inglese". Abbiamo riscontrato che la forma del prompt ha un'influenza minore con prompt brevi, mentre è cruciale per i prompt a "zero" e "uno-shot". Con prompt più lunghi, come nel nostro caso a "cinque-shot", la forma del prompt non influenza significativamente i risultati, sono gli esempi a determinare la performance.

In sintesi, i nostri risultati sperimentali dimostrano che la qualità degli esempi è più importante della somiglianza con la frase di origine. È quindi fondamentale selezionare esempi di alta qualità, preferibilmente da traduzioni di riferimento di alta qualità.

Abbiamo confrontato la selezione di prompt dal dataset di addestramento con prompt dal set di dati di sviluppo WMT, che è più curato e di qualità superiore. I risultati hanno mostrato prestazioni migliori utilizzando il set di dati di sviluppo.

Tuttavia, i sistemi specializzati di stato dell'arte hanno ancora un vantaggio significativo rispetto alle traduzioni di PaLM. PaLM si avvicina comunque alle prestazioni di un sistema commerciale. Nella nostra valutazione, abbiamo scelto di confrontare con Google Translate.

Le valutazioni umane, condotte utilizzando il framework MQM, hanno rivelato che la fluidità di PaLM è paragonabile a quella dei sistemi di stato dell'arte, ma la differenza principale risiede nella precisione. In particolare, gli errori più comuni sono quelli di omissione, PaLM a volte omette parti della frase di origine per produrre una traduzione più fluida. La categoria "Stile/Imbarazzante" per PaLM è inferiore rispetto ai sistemi di riferimento, confermando la fluidità delle sue traduzioni ma con problemi di accuratezza.

In conclusione, questa breve panoramica offre un assaggio dei risultati presentati nel paper. Per maggiori dettagli, vi invito a consultare la presentazione completa del lavoro. Grazie!</sample>
    <sample id="240">## "Weaker Than You Think: Un'Analisi Critica dell'Apprendimento con Supervisione Debole"

Salve, sono Dawei, dottorando presso l'Università del Saarland in Germania. In questo video, vorrei presentarvi il nostro recente lavoro intitolato "Weaker Than You Think: Un'Analisi Critica dell'Apprendimento con Supervisione Debole". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.

Iniziamo con una breve introduzione alla supervisione debole e all'apprendimento con supervisione debole. Nella supervisione debole, non si etichettano manualmente i dati. Al contrario, etichettiamo i dati utilizzando fonti di etichettatura deboli, come semplici regole heuristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, le annotazioni deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa percentuale delle annotazioni è errata. Se addestriamo direttamente le reti neurali sui dati etichettati debolmente, tendono a memorizzare il rumore delle etichette e non generalizzano.

Nell'apprendimento con supervisione debole, sono stati proposti algoritmi di addestramento per addestrare in modo robusto le reti neurali in presenza di tale rumore di etichette, in modo che i modelli addestrati generalizzino ancora bene.

Recentemente, in alcuni lavori sull'apprendimento con supervisione debole (WSL), si afferma che è possibile addestrare modelli solo sui dati etichettati debolmente e ottenere ottime prestazioni su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un aspetto da considerare: si presuppone che sia disponibile un set di validazione pulito aggiuntivo per la selezione del modello. Non possiamo fermarci a questo problema, ma implica che siano necessarie ulteriori annotazioni manuali nell'apprendimento con supervisione debole. Tuttavia, questa necessità è spesso ignorata, come un elefante nella stanza.

Questa domanda solleva tre domande di ricerca:

1. È necessario un set di validazione pulito per il WSL o possiamo utilizzare un set di validazione rumoroso?
2. Se un set di dati pulito è richiesto o essenziale per il WSL, quanti campioni puliti sono necessari?
3. Dovremmo utilizzare solo i campioni puliti per la validazione o esistono modi migliori per utilizzarli?

Nel nostro lavoro, abbiamo affrontato queste domande di ricerca e i nostri risultati sono i seguenti:

In primo luogo, abbiamo scoperto che, contrariamente a quanto si potrebbe pensare, i metodi WSL recenti richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Senza campioni di validazione puliti, le reti neurali addestrate non possono generalizzare oltre le etichette deboli originali, il che rende l'addestramento inutile. Questo indica che gli approcci WSL dipendono effettivamente dai dati etichettati puliti e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.

In secondo luogo, aumentare il numero di campioni di validazione puliti aiuta gli approcci WSL a raggiungere prestazioni migliori, come mostrato nella figura a sinistra. Solitamente, sono necessari solo 20 campioni per classe per ottenere prestazioni elevate. Tuttavia, la storia non finisce qui, perché se decidiamo di accedere ai campioni puliti, l'addestramento diretto su di essi produrrà prestazioni ancora migliori. La figura a destra mostra la differenza di prestazioni tra l'addestramento diretto (fine-tuning) su campioni puliti e gli approcci WSL che utilizzano i campioni puliti solo per la validazione. Come possiamo vedere, anche con solo 10 campioni per classe, il fine-tuning diretto supera gli approcci WSL.

Infine, il miglioramento delle prestazioni rivendicato nei precedenti approcci WSL può essere facilmente ottenuto consentendo la continuazione del fine-tuning sui campioni di validazione puliti. Come si può osservare dalle figure, il modello base, chiamato FTw, inizialmente ha prestazioni inferiori rispetto ai metodi WSL più complessi, come COSINE. Tuttavia, se consentiamo il fine-tuning continuo sui campioni puliti, FTw raggiunge prestazioni equivalenti agli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.

In sintesi, abbiamo dimostrato che gli approcci WSL recenti richiedono campioni puliti e manualmente annotati per funzionare correttamente. I loro vantaggi di prestazioni e praticità sono eccessivamente esaltati. Le nostre raccomandazioni concrete per il futuro lavoro includono:

1. **Raporare i criteri di selezione del modello**. Ad esempio, indicare se la selezione del modello è effettuata tramite campioni di validazione puliti.
2. **Confrontare gli approcci WSL con i baseline di apprendimento a poche riprese**, poiché entrambi lavorano su campioni puliti.
3. **Considerare il fine-tuning continuo** come un semplice ma potente baseline per il futuro lavoro nell'apprendimento con supervisione debole.
4. **Abbiamo reso open-source il nostro codice**. Potete trovarlo tramite il codice QR su questa diapositiva. Sentitevi liberi di provarlo.

Grazie e godetevi la conferenza.</sample>
    <sample id="241">**Abstract:**

Il nostro studio, "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments", esplora un approccio innovativo per il rilevamento precoce della disinformazione sui social media, concentrandoci sulla pandemia di COVID-19. Gli attuali sistemi automatizzati di rilevamento della disinformazione presentano due principali carenze: una valutazione irrealistica e la mancanza di coinvolgimento umano.

Proponiamo un framework di valutazione che integra gli esseri umani nel processo di rilevamento della disinformazione, creando un sistema end-to-end che va dai tweet grezzi alle azioni umane. Il sistema ha due componenti principali. Il primo rileva affermazioni fuorvianti, utilizzando un modello T5 per estrarre domande e risposte da tweet rilevanti, identificando trattamenti potenzialmente inefficaci o non approvati. Le affermazioni sono poi classificate in base alla loro tendenza e presentate a un moderatore umano per la verifica.

Il secondo componente si concentra sulla verifica delle violazioni delle politiche, utilizzando un modello BERT per classificare il tono dei tweet verso trattamenti non approvati. I tweet con un tono di supporto sono segnalati per la revisione umana.

Abbiamo valutato il nostro sistema in termini di rilevamento precoce e verifica delle violazioni delle politiche. I risultati mostrano che il sistema rileva con successo trattamenti non approvati prima che appaiano nelle notizie di smentita, e raggiunge una probabilità del 65% nella rilevazione delle violazioni delle politiche, con 124,2 violazioni confermate per ora di lavoro umana.

Il nostro lavoro fornisce una guida per lo sviluppo e la valutazione di futuri sistemi di rilevamento della disinformazione con il coinvolgimento umano, offrendo una prospettiva preziosa per chi è esterno al settore.</sample>
    <sample id="242">I metodi comuni di valutazione per i sistemi di dialogo includono:

1. **Valutazione umana con scale di Likert**: Chiedere a giudici umani di valutare la qualità delle conversazioni utilizzando scale di valutazione (ad esempio, da 1 a 5).
2. **Confronti diretti**: Chiedere ai giudici di selezionare quale conversazione (tra due opzioni) è migliore.
3. **Valutazione di diverse dimensioni**: Chiedere ai giudici di valutare specifiche dimensioni della qualità del dialogo, come rilevanza delle risposte.

Tuttavia, questi metodi possono essere soggetti a soggettività. ABC-Eval, come descritto, mira a superare queste limitazioni con un approccio più strutturato e oggettivo, annotando comportamenti specifici nei dialoghi.</sample>
    <sample id="243">L'articolo menziona la collaborazione di cinque autori: Jenny (la presentatrice), Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Quindi, la risposta concisa è **cinque**.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie includono:

1. **Entità specifiche**: Informazioni come "Servin è un giudice" e "Kea è un panettiere".
2. **Conoscenza di sfondo**: Informazioni come "I giudici decidono casi nei tribunali di legge".

Queste due tipologie di conoscenza sono cruciali per risolvere correttamente il coreferenza, con la prima fornita tipicamente all'inferenza e la seconda appresa durante il pre-addestramento dei modelli linguistici.</sample>
    <sample id="245">Lining Zhang presenta la ricerca "A Needle in a Haystack: Un'analisi dei lavoratori ad alto accordo su MTurk per la sintesi". Lo studio propone un pipeline a due fasi per identificare lavoratori di alta qualità su Amazon Mechanical Turk (MTurk).

La prima fase, la *Qualificazione Task*, valuta le capacità degli annotatori in sei dimensioni attraverso un test con tre documenti e relative sintesi. I lavoratori sono classificati in quattro categorie: oro, argento, bronzo e bloccati. Solo i lavoratori oro e argento superano questa fase, identificando il 13% dei 200 partecipanti.

La seconda fase, l'*Endurance Task*, testa la capacità di gestione di un carico di lavoro elevato. I lavoratori devono completare 10 HIT (Human Intelligence Tasks) con 4 sintesi per ogni documento. Il 6% dei partecipanti supera questa prova, ottenendo un alto livello di accordo inter-annotatore (IAA).

La *Task di riferimento* valuta le prestazioni generali nella vera attività di annotazione. I risultati mostrano che 8 lavoratori su 12 completano tutte le HIT, con un alto coefficiente di accordo.

Gli autori confrontano diversi approcci per i *Baseline MTurk Workers*, trovando che un filtro statistico chiamato MACE offre il miglior risultato. Esplorano anche i *CloudResearch MTurk Workers*, reclutati per qualità superiore, ma con minore tasso di accettazione dei compiti.

La ricerca conclude che il pipeline proposto, pur con alcune limitazioni, offre un metodo efficace per ottenere annotatori ad alto accordo a basso costo, con prestazioni comparabili a quelle di CloudResearch. In futuro, gli autori pianificano di espandere la ricerca a diverse lingue, applicazioni e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Jiho Kim, ricercatore di KAIST AI, ha presentato il loro lavoro innovativo intitolato "FACTKG: Fact Verification via Reasoning on Knowledge Graphs". Il loro obiettivo è affrontare la mancanza di dataset per la verifica dei fatti che utilizzano grafici di conoscenza come evidenza per le affermazioni in linguaggio naturale.

Il team ha introdotto un nuovo compito, la Verifica dei Fatti basata su Grafici di Conoscenza (Knowledge Graph-Based Fact Verification), che sfrutta i vantaggi dei grafici di conoscenza per una verifica più affidabile e pratica dei fatti. Il dataset creato, chiamato FactKG, utilizza il grafico di conoscenza DBpedia e include due stili di affermazioni: scritte e colloquiali. Le affermazioni sono etichettate come "SUPPORTATE" o "REFUTATE" e il compito richiede di recuperare le evidenze dal DBpedia e verificare le affermazioni utilizzandole.

FactKG introduce cinque tipi di ragionamento: uno-hop, congiunzione, esistenza, multi-hop e negazione. Ad esempio, le affermazioni uno-hop possono essere verificate controllando una singola tripla nel grafico, mentre quelle multi-hop richiedono percorsi più complessi. Il dataset include anche affermazioni colloquiali, gestite tramite un modello di trasferimento di stile colloquiale o template di presupposizioni.

I ricercatori hanno sviluppato baseline per confrontare le prestazioni, dimostrando che l'uso di grafici di conoscenza migliora significativamente la verifica dei fatti rispetto ai metodi basati solo sulle affermazioni. Il dataset FactKG è disponibile per il download e gli autori invitano la comunità di ricerca a contribuire e contattarli.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono necessariamente bilanciati rispetto a ciascun gruppo demografico. Lo studio ha cercato di ottenere un ampio spettro di annotatori provenienti da 87 paesi diversi, ma i partecipanti principali provenivano principalmente da paesi di lingua inglese e con un'istruzione universitaria. Sono stati riscontrati squilibri, ad esempio, nella rappresentazione di persone non binarie rispetto a uomini e donne.

L'obiettivo era di confrontare le annotazioni di questi partecipanti con vari dataset e modelli per analizzare la posizione (positionality) di questi strumenti NLP rispetto a diverse popolazioni.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate preservando la struttura rilevante ma aggiungendo rumore o modificando leggermente il contenuto. Questo processo ha coinvolto vari tipi di perturbazioni per analizzare come il modello rispondeva a cambiamenti minimi nella sintassi e nel significato. L'obiettivo era determinare se il modello fosse sensibile a queste variazioni e se tali cambiamenti influenzassero le sue valutazioni di accettabilità.</sample>
    <sample id="250">Avere una valutazione dimensionale significa analizzare e valutare le prestazioni di un modello di conversazione AI (come un chatbot) in più aspetti o "dimensioni" specifici, piuttosto che solo in termini di qualità complessiva. Invece di un giudizio generale, si mirano a identificare e misurare comportamenti specifici del modello, come la pertinenza delle risposte, la coerenza, la violazione del senso comune, ecc. Questo approccio più dettagliato permette di comprendere meglio i punti di forza e di debolezza del modello.

Nel contesto di ABC-Eval, questo significa valutare conversazioni attraverso l'annotazione esplicita di comportamenti specifici del modello, come rispondere con informazioni irrilevanti, contraddirsi o violare la conoscenza comune. Questo metodo mira a fornire una valutazione più precisa e oggettiva rispetto ai metodi tradizionali basati su giudizi umani soggettivi.</sample>
    <sample id="251">Gli autori dell'articolo provengono dall'Università di Scienza e Tecnologia della Cina (University of Science and Technology of China).</sample>
    <sample id="252">La presentazione, condotta da Sai Kiran Tanikella, un studente magistrale dell'IIT Kanpur, introduce "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion", un progetto di ricerca congiunto con Abhinav Joshi, Akshat Sharma e Ashutosh Modi.

Il discorso si concentra sulla sfida della ricerca di casi precedenti in un volume crescente di documentazione legale. La "Prior Case Retrieval" (PCR) mira a recuperare documenti rilevanti citati all'interno di un nuovo documento legale.

I ricercatori hanno contribuito con due innovazioni: il dataset IL-PCR (Indian Legal Prior Case Retrieval) e la pipeline U-CREAT. IL-PCR, un insieme di 7070 casi legali indiani con un numero medio di 6775 citazioni per documento, fornisce un nuovo standard per la valutazione degli algoritmi PCR.

U-CREAT utilizza un approccio basato su eventi, senza bisogno di regolazioni specifiche per legge o demografia. Estrae eventi da documenti legali tramite analisi sintattica, formando triplette soggetto-verbo-oggetto. Questi eventi sono poi utilizzati per creare una matrice di interazione tra query e candidati, che viene utilizzata per ordinare i candidati.

Gli esperimenti hanno confrontato diversi modelli, mostrando che le tecniche basate su eventi superano significativamente i metodi tradizionali. In particolare, il modello "Event Filtered Documents" ha ottenuto i migliori risultati, con tempi di inferenza più brevi e un punteggio F1 più alto.

La presentazione conclude sottolineando l'importanza e il potenziale di U-CREAT per il campo della ricerca di casi precedenti.</sample>
    <sample id="253">Mario Ezra Aragón ha presentato "DisorBERT", un modello di adattamento di dominio doppio per rilevare segni di disturbi mentali sui social media, frutto di una collaborazione tra ricercatori messicani e spagnoli.

L'obiettivo è identificare automaticamente i disturbi mentali analizzando i post sui social media, fornendo un supporto per tecnologie in grado di segnalare l'insorgenza di tali condizioni.

Il lavoro si basa sull'idea di adattare un modello pre-addestrato (BERT) al dominio specifico dei post sui social media e della salute mentale. Questo processo, chiamato adattamento di dominio, consente al modello di acquisire un vocabolario e una comprensione semantica più pertinenti al nuovo dominio.

DisorBERT utilizza un processo di "masking guidato" per concentrarsi su parole chiave durante l'addestramento. I risultati, ottenuti sul dataset eRisk, dimostrano l'efficacia del modello nel bilanciare precisione e richiamo, superando modelli di base come BERT.

L'analisi delle risposte di DisorBERT e BERT a frasi mascherate da un questionario di depressione (Beck's Depression Inventory) evidenzia come DisorBERT sia in grado di identificare termini più specifici e correlati ai disturbi mentali.

Infine, la presentazione include una visualizzazione interattiva dei post di un utente depresso, mostrando come DisorBERT concentri l'attenzione su parole chiave rilevanti come "ansia" e "medicazione".

In conclusione, DisorBERT dimostra un approccio promettente per il rilevamento dei disturbi mentali sui social media, con potenziali applicazioni future nell'uso di risorse lessicali diverse e dati clinici.</sample>
    <sample id="254">Il lavoro di ricerca presentato, "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction", si concentra sull'affinamento dei modelli di estrazione delle relazioni a livello di documento utilizzando dati distanti supervisionati. L'estrazione delle relazioni a livello di documento mira a identificare i legami tra entità all'interno di un documento, un processo che richiede grandi quantità di dati annotati manualmente, un'attività costosa e dispendiosa in termini di tempo.

Gli autori affrontano la sfida della rumorosità nei dati distanti supervisionati, che contengono vari livelli di rumore. Mentre i metodi precedenti utilizzano pseudo-etichette per pre-addestrare i modelli, questi possono indurre rumore a causa di false etichette positive. Per mitigare questo problema, propongono un framework innovativo che integra l'incertezza guidata nella denoising delle etichette.

Il framework include un pre-addestramento di un modello DocRE con dati distanti supervisionati e annotazioni umane per generare pseudo-etichette. Introdurre l'estima dell'incertezza consente di valutare la affidabilità delle previsioni del modello. Per gestire le relazioni sovrapposte, viene implementata una metodologia di stima dell'incertezza a livello di istanza. Inoltre, viene adottata una strategia di rilabeling dinamica e una strategia di addestramento in fasi per migliorare le prestazioni.

La ricerca dimostra l'efficacia del loro approccio su set di dati pubblici, superando i benchmark esistenti. I contributi principali includono un framework di denoising etichette guidato dall'incertezza, un metodo di stima incertezza istantanea per relazioni sovrapposte, una strategia di rilabeling dinamica per le classi a coda lunga e miglioramenti significativi nelle prestazioni del modello.</sample>
    <sample id="255">La forma del prompting si rivela importante principalmente nei casi di **zero-shot e one-shot prompting**. In questi scenari, la specificità e la qualità del prompt possono influenzare significativamente la performance del modello. Per prompt più lunghi, come il cinque-shot prompting utilizzato nello studio, la forma del prompt ha un impatto minore rispetto alla qualità e alla pertinenza degli esempi forniti.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di stato dell'arte.

Nel loro studio, hanno esaminato le prestazioni di questi modelli in 100 conversazioni umane-bot ciascuno, utilizzando il metodo ABC-Eval (annotazione comportamentale del dialogo).

Hanno anche confrontato i risultati di ABC-Eval con tre metodi di valutazione esistenti: valutazioni su scala Likert a livello di turno e di dialogo, e confronti di dialogo a coppie.</sample>
    <sample id="258">Chiang Cheng-Han presenta il suo lavoro di ricerca intitolato "Can Large Language Models Be an Alternative to Human Evaluation?" (Possono i grandi modelli linguistici sostituire la valutazione umana?). Lo studio esplora l'idea di utilizzare modelli linguistici di grandi dimensioni per valutare la qualità del testo nel campo del Natural Language Processing (NLP).

L'approccio proposto è di fornire istruzioni ai modelli e farli valutare campioni di testo. L'autore sottolinea che, sebbene esista un precedente con opere come G-Eval, al momento della presentazione non esistevano ricerche che esplorassero specificamente l'uso di modelli linguistici di grandi dimensioni per la valutazione.

La motivazione principale è affrontare le limitazioni della valutazione umana, spesso instabile e difficile da replicare. Cheng-Han si chiede se sia possibile trovare un'alternativa affidabile. I modelli linguistici di grandi dimensioni, con la loro capacità di seguire istruzioni in linguaggio naturale, sembrano una soluzione promettente.

Nell'esperimento, i modelli sono stati addestrati a valutare storie generate da GPT-2 o scritte da umani, basandosi su quattro attributi: grammatica, coerenza, piacevolezza e rilevanza. I risultati indicano che alcuni modelli, come Davinci e ChatGPT, preferiscono le storie scritte da umani, suggerendo che i modelli linguistici di grandi dimensioni possono fornire valutazioni significative in alcune circostanze.

Il video conclude invitando gli spettatori a leggere il documento completo per approfondire i dettagli e le implicazioni di questa ricerca innovativa.</sample>
    <sample id="259">**Sintesi della Presentazione: XSemPLR - Un Benchmark Unificato per la Parsimonia Semantica Multilingue**

Yusen Zhang, dell'Università della Pennsylvania, ha presentato *XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations*, un progetto che mira a colmare le lacune nell'attuale ricerca sulla parsimonia semantica multilingue.

Il problema consiste nel tradurre query in diverse lingue naturali in rappresentazioni semantiche, come SQL o Lambda Calculus. Gli attuali modelli si concentrano su lingue specifiche o rappresentazioni limitate. XSemPLR risolve questo problema offrendo un dataset unificato che include 9 dataset in vari domini, 5 compiti di parsimonia semantica, 8 rappresentazioni semantiche e 22 lingue naturali in 15 famiglie linguistiche.

La ricerca esplora sei scenari di valutazione:

1. **Traduzione-Test:** Utilizza Google Translate per tradurre le query e modelli monolingue per l'addestramento e la valutazione.
2. **Monolingua:** Lo stesso linguaggio di origine e di destinazione, ad esempio tedesco-tedesco.
3. **Monolingua Few-shot:** Addestramento con solo il 10% dei dati.
4. **Multilingue:** Addestramento di un modello per tutte le lingue.
5. **Cross-lingue Zero-shot e Few-shot:** Trasferimento dell'addestramento da una lingua a un'altra.

I risultati mostrano che i modelli Encoder-Decoder ottengono le migliori prestazioni su tutti i dataset. La valutazione del modello multilingue rivela un "maledizione della multilingue" con miglioramenti per la maggior parte delle lingue, ma un calo delle prestazioni in inglese in alcuni casi.

La ricerca evidenzia il divario di prestazioni tra impostazioni zero-shot e few-shot, dimostrando l'efficacia del trasferimento cross-lingue. I modelli XSemPLR superano i precedenti lavori e evidenziano le carenze dei modelli multilingue esistenti.</sample>
    <sample id="260">Basandomi sul testo fornito, l'articolo è stato scritto da **un solo autore**, Jingwei Yi, che rappresenta l'Università di Scienza e Tecnologia della Cina.  

Sebbene parli di un lavoro di ricerca collaborativo, il testo è strutturato come una presentazione personale.</sample>
    <sample id="261">Un buon pianificatore ideale dovrebbe possedere le seguenti qualità:

- **Semantica Completa:** Generare script con un significato completo e coerente.
- **Fede ai Vincoli:** Assicurarsi che gli script generati rispettino fedelmente i vincoli specifici imposti sui obiettivi.
- **Varietà e Adattabilità:** Gestire efficacemente obiettivi con vincoli diversi e categorie tematiche varie.
- **Qualità Consistente:** Ridurre la varianza nelle uscite del modello per migliorare le prestazioni.</sample>
    <sample id="262">L'articolo coinvolge un team di ricercatori, con un focus principale su Siyu Yuan da Fudan University. Tuttavia, non viene specificato il numero esatto di autori. Il testo suggerisce che diversi individui abbiano contribuito alla ricerca, considerando la varietà di contributi descritti, come la definizione del problema, la raccolta dei dati, l'analisi, lo sviluppo del metodo e la creazione del dataset.</sample>
    <sample id="263">Il nostro lavoro, "Mitigando i Bias dei Label per l'Apprendimento in Contesto", affronta le sfide legate alla stabilità e ai pregiudizi nell'apprendimento in contesto, una tecnica popolare per i grandi modelli linguistici. Gli studi precedenti hanno dimostrato che l'instabilità di queste scelte, come l'ordine degli esempi, introduce bias nelle previsioni dei modelli.

Abbiamo identificato e classificato diversi tipi di bias, tra cui il bias "vanilla-label", "context-label" e, novità assoluta, il "domain-label bias", che riflette l'influenza del corpus di testo del compito sulla predizione del modello. Attraverso esperimenti, abbiamo confermato che il corpus di testo può influenzare le scelte del modello.

Per affrontare questi bias, proponiamo un metodo di "domain-context calibration". Questo approccio utilizza testi privi di contenuto, ma generati casualmente dal corpus del compito, per stimare e mitigare i bias su ogni label. Rispetto ai metodi precedenti che usano token fissi, il nostro metodo offre miglioramenti significativi, specialmente nei compiti con alto domain-label bias.

Le nostre ricerche forniscono una comprensione approfondita dei bias nell'apprendimento in contesto e un efficace strumento per migliorare le prestazioni dei modelli linguistici.</sample>
    <sample id="264">Lin Wang, studente laureato presso l'Università di Zhejiang, Cina, ha presentato la sua ricerca intitolata "TAVT: Towards Transferable Audio-Visual Text Generation". L'obiettivo è affrontare le sfide nella generazione di testo multimodale, dove l'annotazione dei dati è costosa.

Wang ha evidenziato le difficoltà nella generazione di testo audio-visivo a causa di vari tipi di shift multimodali, come lo stile visivo e l'energia audio. La sua soluzione proposta è un nuovo approccio chiamato Transferable Audio-Visual Text Generation (TAVT).

Il framework TAVT è composto da tre parti: un meta-mapper audio-visivo, un encoder audio-visivo e un generatore di modelli linguistici, insieme a un apprendimento contrastivo controfattuale. Il meta-mapper allinea i concetti visivi attraverso domini diversi in uno spazio semantico audio unificato. Utilizzando cluster di audio e token appresi, il modello può generare audio coerenti con le immagini.

L'encoder e il generatore linguistici utilizzano un approccio di apprendimento basato su prompt, assegnando pesi alle modalità per ogni parola. La perdita di apprendimento e i dettagli di addestramento sono introdotti per ottimizzare l'allineamento visivo-testuale.

Wang ha condotto esperimenti su benchmark MSVD e MSR-VTT, dimostrando l'efficacia di TAVT rispetto ad altri approcci di stato dell'arte, specialmente in domini a bassa risorsa. Gli esperimenti di ablatte hanno evidenziato l'importanza delle caratteristiche audio.

In sintesi, TAVT mira a semplificare la generazione di testo audio-visivo attraverso l'apprendimento di uno spazio semantico audio trasferibile, dimostrando risultati promettenti.</sample>
    <sample id="265">Il nome della relatrice è Vasudha.</sample>
    <sample id="266">Sulla base del contenuto presentato, l'autore del talk, Adam Przepiórkowski, sembra essere affiliato con:

1. **Università o istituto non specificato**: Non viene menzionata una specifica affiliazione accademica, ma il contesto suggerisce una ricerca accademica avanzata.
2. **Ricerca linguistica**: Il talk si concentra su strutture di coordinazione e teorie del dipendenze grammaticali.
3. **Contributi alla teoria del linguaggio**: L'autore presenta argomenti a favore di strutture di coordinazione simmetriche e contro quelle asimmetriche.

Inoltre, fa riferimento a lavori specifici come "Why Wouldn't You Use Universal Dependencies" e alla versione migliorata del Penn Treebank, indicando una familiarità con la letteratura e le risorse linguistiche esistenti.</sample>
    <sample id="268">Secondo la presentazione di David Vilar, gli errori più comuni di PaLM, un modello linguistico di grandi dimensioni, sono **omissioni**. PaLM tende a omettere parti del testo di origine durante la traduzione, producendo un output che suona migliore ma che può essere meno accurato.</sample>
    <sample id="269">Salve, io sono James Finch. E questa è Sarah Finch. Oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'Intelligenza Artificiale Conversazionale. Questo lavoro è stato condotto dal Laboratorio NLP di Emory, guidato dal Professor Jinho Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI. Immaginate di aver sviluppato un modello di dialogo e di voler confrontare le sue prestazioni con quelle degli altri sistemi all'avanguardia. La pratica comune è quella di utilizzare la valutazione umana, come chiedere a giudici umani di scegliere quale conversazione sia migliore o di valutare le conversazioni su una scala di Likert. Questi metodi funzionano bene per fornire una valutazione olistica della qualità del dialogo, ma la qualità del dialogo ha molte sfaccettature. Pertanto, potreste voler valutare diverse dimensioni della qualità del dialogo per comprendere i punti di forza e di debolezza del modello a un livello più dettagliato.

Un approccio potrebbe essere quello di chiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, utilizzando metodi comparativi esistenti o scale di Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio mira a ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio "annotazione dei comportamenti nel dialogo" o, più brevemente, ABC-Eval. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di dialogo che, secondo la letteratura recente, influenzano la qualità del dialogo.

ABC-Eval è in grado di misurare i tassi di vari errori tematici commessi dai modelli di dialogo. Ad esempio, misura il numero di turni in cui un modello di dialogo ignora il suo partner o fornisce informazioni irrilevanti, si contraddice o viola la conoscenza comune e quando il modello mostra o non mostra empatia. Per determinare quale metodo di valutazione sia il più efficace, abbiamo selezionato quattro modelli di dialogo all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo valutato anche queste conversazioni utilizzando tre metodi esistenti: valutazioni di Likert a livello di turno, valutazioni di Likert a livello di dialogo e confronti di dialogo a coppie.

Per ogni metodo esistente, abbiamo raccolto valutazioni su otto aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di dialogo su più dimensioni. Dalla nostra analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento ABC-Eval sono complessivamente più affidabili rispetto alle etichette raccolte dai metodi esistenti, come dimostrato dall'accordo inter-annotatore su 100 conversazioni doppiamente etichettate. Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione, come mostrato da questo semplice analisi di regressione lineare. Ad esempio, potete vedere come la percentuale di turni con contraddizioni tra sé e il partner spiega il 5% e il 10% della qualità della conversazione, mentre le medie dei punteggi di coerenza di Likert spiegano solo il 4% o meno.

Infine, abbiamo verificato se ogni metrica di valutazione catturasse un aspetto unico della qualità del dialogo utilizzando una regressione lineare passo-passo. Potete vedere come la combinazione di tutte le metriche ABC-Eval spiega oltre il 25% della qualità della conversazione, e mentre rimuoviamo una metrica alla volta, la maggior parte di esse perde una quantità significativa di informazioni sulla qualità. Al contrario, la combinazione di tutte le metriche di Likert a livello di turno spiega molto meno della qualità e solo poche di queste metriche portano informazioni uniche.

Queste metriche ABC-Eval affidabili, informative e distinte consentono di valutare l'Intelligenza Artificiale Conversazionale con una risoluzione superiore rispetto ai metodi precedenti. Come potete vedere dai risultati del nostro esperimento, rimangono ancora diverse sfide da affrontare e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni della conoscenza comune nel 20% delle loro risposte, forniscono informazioni irrilevanti nel 15% delle risposte e si contraddicono o contraddicono il loro partner nel 10% delle volte. Con il rapido progresso del campo, molte di queste percentuali di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, è proprio per questo motivo che è importante perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione, e non vediamo l'ora di vedere come l'Intelligenza Artificiale Conversazionale evolverà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo, James Finch e Sarah Finch, descrivono il loro lavoro sviluppato presso l'Emory NLP Lab, guidato dal Professor Jinho Choi presso Emory University, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">Nel contesto dell'articolo "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", CFT sta per "Fine-tuning (Continua)". Il termine si riferisce alla pratica di continuare ad addestrare un modello su dati di validazione puliti dopo la fase iniziale di addestramento con dati debolmente etichettati.</sample>
    <sample id="272">L'articolo è il risultato di un lavoro congiunto di **7** autori: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams.</sample>
    <sample id="273">## Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati

**Introduzione**

Salve, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'esplorazione multilingue basata sui dati", realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.

Molte traduzioni dipendono dal contesto. Pensate alla parola "mole": nel contesto "Le cose potrebbero diventare pericolose se i ministri lo scoprono", "mole" si riferisce a uno spionaggio. Invece, in "Potrebbe essere qualcosa di serio, dottore?", "mole" indica una macchia di nascita. Il significato della parola, e quindi la sua traduzione, cambia a seconda del contesto.

Tuttavia, valutare quanto bene i modelli gestiscono queste sfumature è complesso. Innanzitutto, solo una piccola percentuale delle traduzioni dipende dal contesto, rendendo inefficaci metriche corpus-based come BLEU per catturarle. Alcuni hanno proposto valutazioni mirate sulle traduzioni che dipendono dal contesto, ma queste risorse spesso supportano solo tipi limitati di traduzioni e un numero ristretto di lingue, poiché si basano su conoscenze di dominio e curazione umana.

**Le nostre domande**

Il nostro lavoro si propone di rispondere a due domande:

1. **Quando la traduzione richiede contesto?**
2. **Quanto bene i modelli gestiscono queste situazioni?**

Per rispondere alla prima, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione. Nel lavoro precedente, abbiamo introdotto CXMI (Context eXpression Measurement for Machine Translation) come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo metodo valuta quanto informazione il contesto C fornisce sulla traduzione target Y, data la fonte X. Potete pensare a CXMI come all'informazione acquisita fornendo contesto al modello.

In questo studio, estendiamo CXMI a Pointwise CXMI, in grado di misurare l'uso del contesto a livello di frase o parola. Le parole con alto P-CXMI richiedono, in genere, contesto per la traduzione. Analizziamo quindi queste parole per individuare schemi e scopriamo che:

* **Pronomi duali in arabo:** hanno un alto P-CXMI perché l'inglese non ha pronomi duali, quindi il contesto è essenziale per la traduzione in arabo.
* **Scelta della forma verbale:** alcune lingue richiedono contesto per la scelta della forma verbale appropriata.
* **Nome propri e formalità:** in cinese, per esempio, i nomi propri richiedono contesto per essere tradotti correttamente all'interno di un documento, e lo stesso vale per la formalità.
* **Risoluzione delle ellissi:** alcuni fenomeni, come la risoluzione delle ellissi, sono espressi nella struttura della frase e non possono essere catturati dalla sola parola.

**Creazione di un benchmark**

Utilizzando i nostri risultati, abbiamo sviluppato un benchmark per la traduzione documentale. Abbiamo creato tagger automatizzati (chiamati MuDA tagger) per identificare parole relative a cinque fenomeni discorsivi diversi.

Abbiamo poi valutato diversi modelli di traduzione utilizzando sia metriche corpus-level (BLEU, COMET, word f-measure) sia il nostro benchmark MuDA.

* **BLEU:** i modelli senza contesto ottengono le prestazioni migliori.
* **COMET:** i modelli con contesto superano quelli senza.
* **word f-measure:** le prestazioni dei modelli con e senza contesto sono simili.

Questo evidenzia la complessità nella scelta del miglior sistema di traduzione documentale basandosi esclusivamente sulle metriche corpus-level.

Utilizzando il benchmark MuDA, abbiamo scoperto che i modelli che utilizzano il contesto sono significativamente più accurati per fenomeni come formalità e coesione lessicale. Tuttavia, per altri fenomeni come ellissi, pronomi e forma verbale, la differenza di prestazioni tra modelli con e senza contesto è minima.

Infine, abbiamo confrontato diverse piattaforme di traduzione commerciali e il nostro benchmark indica che DeepL è generalmente più accurato di Google Translate nella traduzione documentale.

**Conclusione**

In sintesi, abbiamo condotto un'analisi basata sui dati su 14 coppie di lingue per identificare quando la traduzione richiede contesto e sviluppato un benchmark per la traduzione documentale che aiuta a capire quali fenomeni discorsivi i modelli gestiscono meglio e quali sistemi di traduzione eccellono nella traduzione di documenti.

Grazie per l'attenzione. A Toronto ci vediamo presto!</sample>
    <sample id="274">Il nome della relatrice è Yusen Zhang.</sample>
    <sample id="276">Il lavoro presentato, "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages", si concentra sulla valutazione delle metriche di traduzione automatica (MT) per le lingue indiane, un'area sottostimata nella ricerca attuale. Gli autori, Ananya e Vignesh, hanno creato un dataset per meta-valutare le metriche di traduzione, raccogliendo 7.000 campioni di traduzione da cinque lingue indiane: Tamil, Malayalam, Hindi, Marathi e Gujarati.

Il processo inizia con la selezione casuale di 200 frasi dal dataset Flores e la generazione di traduzioni multiple utilizzando sette diversi sistemi di traduzione. Gli annotatori bilingui esperti valutano quindi le traduzioni, identificando errori di accuratezza, significato, fluidità e altri tipi specifici. Questo approccio dettagliato fornisce un'annotazione ricca per ogni traduzione.

La ricerca confronta diverse metriche di traduzione, osservando che le metriche basate su overlap come chrF hanno una correlazione elevata ma sono meno performanti complessivamente. Le metriche basate su embedding, come LabSE, e quelle che utilizzano BERTscore con modelli multilingue mostrano una correlazione migliore. Tuttavia, molti metrici producono punteggi in un intervallo ristretto, rendendo difficile l'interpretazione.

Gli autori propongono un fine-tuning della metrica COMET utilizzando il loro dataset MQM, ottenendo risultati promettenti. Il fine-tuning migliora la correlazione con i punteggi umani, soprattutto per gli errori di accuratezza. Inoltre, IndicCOMET MQM supera le metriche di base COMET in tre lingue su cinque, dimostrando la sua efficacia nella valutazione della traduzione automatica per le lingue indiane. Il dataset è reso pubblico per favorire ulteriori ricerche.</sample>
    <sample id="277">Il nuovo metodo descritto nel paper non ha un nome specifico menzionato nel testo fornito. Tuttavia, è caratterizzato da due fasi principali:

1. **Taggatura Multiset**: Ogni token dell'input viene etichettato con un multiset di token che appariranno nell'output.
2. **Predizione di Permutazione**: Un modello aggiuntivo prevede l'ordine (permutazione) dei token nell'output, basandosi sui multiset generati nella prima fase.

Questo approccio innovativo consente di ottenere una forte generalizzazione a ricorsioni più profonde senza l'uso di alberi, superando le limitazioni dei metodi tradizionali che si affidano a strutture arboree.</sample>
    <sample id="278">L'autrice descrive il metodo "Parole Contrassegnate" come un approccio che identifica le parole che distinguono i gruppi marcati da quelli non marcati, basandosi sul concetto sociolinguistico di "markedness". Questo metodo consente di evidenziare stereotipi specifici e modelli senza affidarsi a un lessico predefinito. Le "parole contrassegnate" sono quelle che definiscono un gruppo rispetto a un default sociale, spesso attribuendo caratteristiche uniche o distinte.</sample>
    <sample id="279">Gli autori dell'articolo "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models" sono affiliati all'Università di Washington, in particolare come PhD studenti.</sample>
    <sample id="280">**Abstract:**

Il lavoro "MultiEMO: Un Framework di Fusione Multimodale Correlazione-Consapevole Basato sull'Attenzione per il Riconoscimento delle Emozioni nelle Conversazioni" di Shi Tao affronta le sfide nel riconoscimento delle emozioni nelle conversazioni (ERC), proponendo un approccio innovativo chiamato MultiEMO.

L'obiettivo è prevedere l'etichetta emotiva di ogni enunciato in una conversazione, considerando informazioni testuali, audio e visive. Gli autori identificano tre problemi principali: insufficiente sfruttamento delle informazioni multimodali, prestazioni insoddisfacenti per le classi di emozioni minoritarie e difficoltà nel distinguere tra emozioni semanticamente simili.

MultiEMO si compone di quattro elementi chiave: estrazione delle caratteristiche unimodali, modellazione del contesto, fusione multimodale e classificazione delle emozioni. Introdurre un estraente visivo, VisExtNet, evita l'inutile codifica di informazioni ambientali ridondanti. MultiAttn, un modello di fusione multimodale, integra le informazioni testuali, audio e visive tramite attenzione bidirezionale multi-testa.

Un'innovazione cruciale è la Sample-Weighted Focal Contrastive Loss (SWFC), che attribuisce maggiore importanza alle classi minoritarie e alle coppie di campioni con etichette diverse, migliorando la distinzione tra emozioni simili.

Gli esperimenti sui dataset MELD e IEMOCAP dimostrano che MultiEMO raggiunge prestazioni di stato dell'arte, con miglioramenti significativi nelle classi minoritarie e nelle emozioni simili. Nonostante alcune limitazioni, come la mancanza di distinzione tra parlanti e persone irrilevanti, MultiEMO rappresenta un passo avanti nel campo del riconoscimento delle emozioni nelle conversazioni.</sample>
    <sample id="281">Il lavoro presentato, "When Does Translation Require Context?", esplora come il contesto influenzi la traduzione automatica, un aspetto spesso trascurato nelle valutazioni tradizionali. L'obiettivo è determinare quando la traduzione richiede contesto e come valutare la capacità dei modelli di gestire tali casi.

Gli autori hanno sviluppato un nuovo approccio, chiamato Pointwise CXMI, per misurare il grado di dipendenza da contesto di ogni parola durante la traduzione. Analizzando i transcripti di TED Talks tradotti in 14 lingue, hanno identificato pattern interessanti. Ad esempio, alcune lingue richiedono contesto per la scelta della forma verbale corretta o per la traduzione di nomi propri all'interno di un documento.

Il team ha creato un benchmark multilingue, MuDA, per valutare i sistemi di traduzione a livello di documento. Questo benchmark identifica e classifica cinque fenomeni discorsivi chiave. I risultati mostrano che i modelli basati sul contesto ottengono prestazioni migliori per alcuni fenomeni, mentre i modelli agnostici del contesto eccellono in altri.

La ricerca evidenzia le sfide nella valutazione della traduzione a livello di documento e suggerisce aree di miglioramento. Inoltre, confronta diversi sistemi commerciali, dimostrando che DeepL spesso supera Google Translate in accuratezza nella traduzione di documenti. Questo studio fornisce una comprensione approfondita del ruolo del contesto nella traduzione automatica multilingue.</sample>
    <sample id="282">Xuekai Zhu presenta il suo nuovo lavoro, "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing", presentato ad ACL 2023. Lo studio affronta la sfida del trasferimento di stile testuale non parallelo, un'area cruciale nella generazione del linguaggio naturale.

La ricerca si concentra sul trasferimento di stile a livello di storia e discorso, imitando le scelte linguistiche degli autori. Gli autori identificano due sfide principali: la complessità delle preferenze linguistiche degli autori in testi lunghi e la forte associazione tra stile e argomenti specifici di scrittura.

Per superare queste sfide, viene introdotto StoryTrans, un modello di generazione che apprende rappresentazioni discorsive dai testi di origine e le combina con embedding di stile appresi per generare testi in stili target. StoryTrans utilizza un nuovo obiettivo di addestramento per ridurre le caratteristiche stilistiche nelle rappresentazioni discorsive e migliorare la conservazione del contenuto.

Il processo di addestramento è diviso in due fasi. La prima fase si concentra sul trasferimento del testo di origine con parole chiave specifiche dello stile mascherate, mentre la seconda fase riempie i contenuti mancanti. L'addestramento utilizza diverse perdite, tra cui la ricostruzione dell'input, la disaccoppiamento stile-contenuto, l'ordine delle frasi e la classificazione dello stile.

Gli esperimenti dimostrano l'efficacia di StoryTrans nel trasferimento di stili di autori diversi su storie fiabesche ed everyday, con valutazioni automatiche e manuali che superano i baseline. Il modello mantiene anche il contenuto originale mentre adotta lo stile desiderato.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella **dell'approccio di Praga**, che si riferisce a **Praga**. In questa struttura, le coordinate sono testate dal congiunto (conjunct) più a sinistra.</sample>
    <sample id="284">Peng Tianshuo, dell'Università di Wuhan, ha presentato il suo studio intitolato "FSUIE: Un Meccanismo di Spanna Fuzzy per il Potenziamento dell'Estrazione Universale di Informazioni" al conferimento ACL.

L'intervento affronta le sfide degli attuali modelli di estrazione di informazioni (UIE) basati su span, che dipendono fortemente dalle posizioni precise degli span. L'oratore introduce l'idea di utilizzare confini di span "fuzzy" invece che precisi, affrontando l'ambiguità nella delimitazione degli span.

Il modello FSUIE propone due innovazioni: un meccanismo di perdita di span fuzzy che modella i confini come distribuzioni di probabilità continue, e un'attenzione di span adattiva che regola dinamicamente la portata dell'attenzione. Questo meccanismo, chiamato Fuzzy Span Attention (FSA), consente al modello di concentrarsi su informazioni semantiche in un intervallo limitato di token precedenti.

FSUIE è stato testato con successo in tre compiti di estrazione di informazioni: riconoscimento di entità nominate (NER), estrazione di relazioni (RE) e estrazione di triplette di sentimenti di aspetti (ASTE). I risultati dimostrano un notevole miglioramento nelle prestazioni, specialmente con set di dati di piccole dimensioni, grazie all'efficace utilizzo delle informazioni di annotazione e alla modellazione adattiva degli span.

In sintesi, FSUIE offre un approccio innovativo per migliorare l'estrazione di informazioni, riducendo la dipendenza dai confini degli span e adattando dinamicamente l'attenzione del modello.</sample>
    <sample id="285">Nel loro lavoro, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework", Mingqi Gao e il suo team da Peking University affrontano la sfida dei factual error nei riassunti di dialoghi generati da modelli.

Esistono due approcci principali per risolvere questo problema: integrare obiettivi di factualità nella formazione o creare modelli separati di correzione degli errori fattuali (FEC). Nonostante l'importanza del tema, la valutazione dei modelli FEC è carente.

Gli autori criticano le metriche attuali, come FactCC e DAE, per la loro valutazione vaga e non affidabile. Propongono invece un nuovo framework di valutazione basato su ERRANT per la correzione degli errori grammaticali.

Attraverso esperimenti con diversi modelli FEC e modalità di formazione, i risultati dimostrano che:

* La formazione con riassunti di dialoghi corretti manualmente da fonti esterne offre i migliori risultati.
* È necessario un cambiamento nelle metriche di valutazione dei modelli FEC.
* Integrare dati corretti manualmente con dati sintetici può migliorare le prestazioni.
* I modelli FEC attuali faticano con errori di aggiunta e non gestiscono errori più complessi come attributi, modalità o collegamenti.

In sintesi, lo studio evidenzia le sfide nella valutazione e nel miglioramento dei modelli FEC per la sintesi di dialoghi, aprendo la strada a future ricerche più efficaci.</sample>
    <sample id="286">Il nome della relatrice è **Sarah Finch** e il relatore è **James Finch**.</sample>
    <sample id="287">L'articolo è il risultato di un lavoro congiunto di **quattro** autori:

* Javad Hosseini
* Filip Radlinski
* Silvia Pareti
* Annie Louis</sample>
    <sample id="288">Secondo la presentazione, diversi insiemi di dati possono essere utilizzati per testare fenomeni sintattici:

1. **BLiMP (Base Language and Inference for Machine Perception)**: Un insieme di dati che include coppie di frasi grammaticali e non grammaticali.
2. **SyntaxGym**: Un altro insieme di dati utilizzato per valutare la sintassi e la grammatica.
3. **CrowS (Computer-Assisted Crowdsourcing of Syntax)**: Un insieme di dati che utilizza la crowdsourcing per creare coppie di frasi con valutazioni di accettabilità.
4. **Wikipedia**: Utilizzato per testare la robustezza delle valutazioni in contesti completamente irrilevanti rispetto ai dati di addestramento.

Questi insiemi di dati permettono di simulare e valutare la capacità dei modelli linguistici di riconoscere l'accettabilità e la grammaticalità delle frasi in vari contesti.</sample>
    <sample id="290">Le abbreviazioni dei cinque metodi per la prima domanda di ricerca, come menzionato nel video, sono:

1. **FTw (Fine-tuning on clean validation samples)**
2. **COSMINE (un metodo specifico di Weakly Supervised Learning menzionato come esempio)**

Questi metodi sono stati utilizzati per esplorare l'efficacia di diversi approcci nella Weakly Supervised Learning (WSL) in presenza o assenza di dati di validazione puliti.</sample>
    <sample id="291">Il modello DrBERT viene valutato su 11 attività downstream nel dominio biomedico e clinico in francese, tra cui:

- Riconoscimento di entità nominate
- Classificazione
- Etichettatura delle parti del discorso
- Domande e risposte</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un set di 4 GB di NACHOS (un dataset di dati medici web-crawled in francese).</sample>
    <sample id="295">Il nome del relatore è Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presenta un progetto di ricerca collaborativo tra l'Università di Torino e Amazon Alexa incentrato sulla comprensione del linguaggio naturale (NLU) e il processamento del linguaggio naturale (NLP). Lo studio esplora i limiti dell'approccio basato sui dati, che si affida a grandi set di dati annotati manualmente per addestrare modelli di machine learning.

In particolare, il team si è concentrato sull'ironia, un fenomeno pragmatico complesso nel linguaggio naturale, difficile da rilevare per i modelli NLP. Invece di etichettare le frasi come "ironica" o "non ironica", gli autori hanno sviluppato un corpus chiamato EPIC (English Perspectivist Irony Corpus) con 300 brevi conversazioni raccolte da social media e forum online in un periodo di 1,5 anni.

Utilizzando una piattaforma di crowdsourcing, 74 annotatori hanno valutato l'ironia in queste conversazioni, fornendo in media 5 annotazioni per frase. Gli studi hanno rivelato variazioni nelle valutazioni tra diversi gruppi demografici, con modelli "perspettivi-consapevoli" che mostrano una maggiore sicurezza nelle previsioni rispetto ai modelli standard.

L'analisi delle cause di queste differenze ha evidenziato che le generazioni vicine e i partecipanti provenienti da diverse regioni geografiche tendono ad avere percezioni più divergenti dell'ironia. Questo lavoro apre nuove prospettive per migliorare la comprensione dell'ironia da parte dei modelli NLP.</sample>
    <sample id="297">Il discorso si concentra sull'analisi dei "dogwhistles" (sottintesi razziali o ideologici) nella retorica politica, in particolare nel contesto delle discorsi di Senator Josh Hawley. Un dogwhistle è un termine o simbolo che trasmette un messaggio nascosto a un gruppo interno mentre sembra indirizzarsi a un gruppo esterno, spesso con connotazioni negative. Nel caso di Hawley, il termine "cosmopolita" potrebbe essere interpretato come un dogwhistle anti-semita.

Il progetto presentato sviluppa una tipologia e un glossario di oltre 340 termini dogwhistle, inclusi quelli razzisti, transofobici e anti-semiti. Analizza discorsi politici storici negli Stati Uniti, evidenziando una correlazione tra l'uso di dogwhistles e la strategia politica repubblicana del Sud, che ha intensificato l'uso di tali tattiche dopo il movimento per i diritti civili.

Sono stati condotti esperimenti con modelli linguistici, come GPT-3, per valutare la loro capacità di identificare i dogwhistles. I risultati mostrano che i modelli funzionano meglio con dogwhistles formali, ma faticano con quelli informali e transofobici.

Infine, lo studio dimostra come i dogwhistles possano eludere i sistemi di moderazione online. Sostituendo i termini offensivi con dogwhistles, le frasi hateful ricevono valutazioni meno negative, evidenziando la necessità di strategie più sofisticate per la rilevazione di contenuti dannosi.</sample>
    <sample id="298">La conclusione che la deriva temporale è la causa principale della perdita di prestazioni è stata raggiunta attraverso due osservazioni chiave:

1. **Analisi della curva di miglioramento**: La curva di miglioramento sul dataset CoNLL++ mostrava una pendenza maggiore di 1, indicando che ogni unità di miglioramento su CoNLL-2003 si traduceva in più di un'unità di miglioramento su CoNLL++, il che esclude l'adattamento all'overfitting.

2. **Esperimento di ritrainamento**: Il ritrainamento o il pre-addestramento di alcuni modelli con dati più recenti ha dimostrato che le prestazioni degradano con l'aumentare del gap temporale, confermando l'ipotesi della deriva temporale come causa principale della perdita di prestazioni.</sample>
    <sample id="299">Michalis Korakakis presenta il suo lavoro di ricerca incentrato sull'aumento della robustezza dei modelli NLI (Natural Language Inference) attraverso un approccio di addestramento minimax. I modelli NLI hanno raggiunto risultati eccellenti su vari benchmark, ma recenti studi hanno rivelato che il loro successo è spesso dovuto all'apprendimento di "scorciatoie" spurious presenti nei dataset. Queste scorciatoie, come ad esempio un'alta sovrapposizione di parole tra premessa e ipotesi nel dataset MNLI, possono far sì che i modelli NLI si affidino troppo a queste correlazioni, rendendoli vulnerabili a test out-of-distribution.

Il metodo proposto da Korakakis mira a ridurre la dipendenza dai shortcut. L'idea chiave è che i modelli NLI tendono a trascurare esempi "difficili" che contrastano con le scorciatoie presenti negli esempi più comuni. L'approccio minimax addestra due modelli: un "learner" che minimizza l'errore NLI e un "auxiliary" che massimizza la perdita del learner generando pesi di esempio. Questo processo incoraggia il learner a concentrarsi sugli esempi difficili.

La ricerca dimostra l'efficacia di questa tecnica in diversi dataset, migliorando le prestazioni out-of-distribution senza compromettere l'accuratezza in-distribution. Korakakis discute anche l'impatto della pre-formazione del learner, le dimensioni dell'auxiliary e presenta un'analisi qualitativa dei pesi di esempio appresi. Il lavoro si conclude invitando gli ascoltatori a discutere ulteriormente durante la sessione poster.</sample>
    <sample id="300">Belinda presenta il suo lavoro sull'interattiva dettatura, un processo che consente agli utenti di dettare e modificare documenti con la voce in modo naturale. L'obiettivo è creare un sistema che comprenda correzioni vocali e comandi durante la dettatura, senza la necessità di trigger predefiniti.

La presentazione introduce quattro passaggi chiave per il processo:

1. **Riconoscimento vocale (ASR):** La prima fase trasforma l'audio in testo.
2. **Segmentazione:** Il testo viene suddiviso in segmenti di dettatura e comandi.
3. **Estrazione e normalizzazione dei comandi:** I comandi vocali vengono analizzati e preparati per l'esecuzione.
4. **Esecuzione:** I segmenti di dettatura e comando vengono elaborati in sequenza per creare il documento finale.

Per addestrare il sistema, i ricercatori hanno sviluppato un'interfaccia di raccolta dati che consente agli utenti di dettare e inserire testo, ma anche di emettere comandi vocali per modificare il contenuto. Il dataset raccolto include varie sequenze di dettatura e comandi.

Il team ha costruito un sistema di base che utilizza modelli pre-addestrati T5 e GPT-3 per ogni fase. I risultati mostrano un compromesso tra velocità e accuratezza, con GPT-3 più preciso ma più lento. La previsione diretta dello stato finale ha ottenuto migliori risultati con GPT-3, mentre la previsione di programmi intermedi ha migliorato l'efficienza con T5.

Il codice e il dataset sono stati resi pubblici per incoraggiare ulteriori ricerche su questo nuovo e promettente campo.</sample>
    <sample id="302">La permutazione dei token è necessaria per la sequenza di output perché, dopo aver identificato i token corretti, non sono ordinati. Il modello, quindi, deve prevedere l'ordine corretto in cui inserirli nella sequenza di output. Questo processo è cruciale per garantire che l'output rifletta accuratamente le corrispondenze sistematiche tra l'input e l'output, che sono state apprese durante l'addestramento.

In sintesi, la permutazione consente al modello di:

1. **Organizzare i token corretti**: Trasformare un insieme di token identificati in una sequenza ordinata.
2. **Rispettare le relazioni semantiche**: Assicurarsi che l'ordine dei token nell'output rispecchi le relazioni e la struttura del significato del testo di input.</sample>
    <sample id="303">Gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias perché ritengono che, senza una maggiore trasparenza, sia difficile capire se i risultati positivi o apparentemente non negativi generati dai modelli derivano da un valore di allineamento eccessivo o da altri metodi di mitigazione che, pur intenzionalmente, producono pattern dannosi. Questa mancanza di chiarezza può portare a sottovalutare o trascurare i pregiudizi nascosti nei modelli.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima (Minimal Pair Acceptability Judgments o MPP) sono una metodologia utilizzata per valutare i modelli linguistici, confrontando la probabilità assegnata da questi modelli a frasi grammaticali e non grammaticali. Questa tecnica implica la presentazione al modello di una frase accettabile e una non accettabile, sperando che attribuisca una maggiore probabilità alla prima.

Tradizionalmente, il pipeline MPP valuta i modelli su frasi brevi, ma con l'aumento delle dimensioni dei contesti nei modelli linguistici di grandi dimensioni, è diventato cruciale valutare la loro accettabilità su sequenze più lunghe.

Il lavoro descritto nell'intervento rivede questo approccio creando input più lunghi aggiungendo o rimuovendo frasi grammaticali o non grammaticali da dataset esistenti, testando così la sensibilità del modello a diversi contesti.</sample>
    <sample id="305">Il video presenta la ricerca "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", condotta da Dawei e colleghi. La ricerca si concentra sul campo dell'apprendimento supervisionato debole, dove i dati non sono etichettati manualmente, ma tramite fonti di etichettatura deboli come regole heuristiche o crowdsourcing di bassa qualità.

Gli autori mettono in luce un'affermazione comune nel campo: l'idea che sia possibile addestrare modelli di intelligenza artificiale con dati etichettati debolmente e ottenere ottime prestazioni su set di test puliti. Tuttavia, sottolineano un aspetto cruciale: questa affermazione presuppone l'accesso a un set di validazione pulito per la selezione del modello. Senza questo, i modelli tendono a memorizzare il rumore etichettato, compromettendo la generalizzazione.

La ricerca risponde a tre domande fondamentali: 1) è necessario un set di validazione pulito per l'apprendimento supervisionato debole? 2) Quanti campioni puliti sono necessari? 3) È preferibile utilizzare solo campioni puliti per la validazione? I risultati dimostrano che i metodi WSL richiedono campioni puliti per funzionare efficacemente, e che un numero relativamente piccolo (20 campioni per classe) può portare a prestazioni elevate. L'addestramento diretto sui campioni puliti supera spesso le prestazioni dei metodi WSL.

Gli autori concludono che le prestazioni migliorate dichiarate nei metodi WSL sono spesso raggiungibili con un semplice addestramento continuo sui campioni di validazione puliti, senza la necessità di complesse architetture. Consigliano inoltre di riportare chiaramente i criteri di selezione del modello e di confrontare WSL con approcci di apprendimento a poche riprese.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano il loro lavoro sull'Entity Tracking in Language Models, con un focus su come gli agenti possano comprendere e tracciare gli stati delle entità all'interno di un discorso.

L'obiettivo è valutare la capacità dei modelli linguistici pre-addestrati, come GPT-3 e GPT-3.5, di monitorare gli stati delle entità mentre un discorso procede. Questo è cruciale per la comprensione di testi complessi.

Hanno progettato un compito che coinvolge scatole e oggetti, dove il modello deve prevedere il contenuto delle scatole dopo una serie di operazioni. Hanno preso precauzioni per evitare che i modelli usino semplici associazioni o memorizzino sequenze di stato.

I risultati mostrano che solo il modello GPT-3.5, addestrato su una grande quantità di codice, mostra una capacità non triviale di tracciamento delle entità. Questo suggerisce che l'addestramento su codice è fondamentale per questa abilità.

Hanno anche scoperto che i modelli più piccoli, come T5-base, possono imparare il tracciamento delle entità con il fine-tuning, ma i modelli inizializzati casualmente non ci riescono.

In sintesi, la ricerca evidenzia il ruolo cruciale dell'addestramento, specialmente su dati strutturati come il codice, per sviluppare la capacità di tracciamento delle entità nei modelli linguistici. I dettagli completi sono disponibili nel loro articolo su arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per confrontare i loro modelli, tra cui:

1. **Precisione (Accuracy)** per compiti di classificazione.
2. **F1-score** per compiti di riconoscimento delle entità nominate e altre attività di elaborazione del linguaggio naturale (NLP).
3. **BLEU score** per compiti di generazione del testo o traduzione.
4. **ROUGE score** per compiti di riassunto.
5. **Perplexity** per valutare la qualità della previsione del modello.

Queste metriche sono state applicate a una serie di compiti biomedicali e clinici per valutare le prestazioni dei modelli.</sample>
    <sample id="308">Jenny, una dottoranda di Carnegie Mellon University, ha presentato il suo studio NLPositionality, che esplora le bias di progettazione nei dataset e nei modelli di elaborazione del linguaggio naturale (NLP). L'esempio iniziale riguarda la moderazione dei commenti online, dove un API come Prospective API funziona bene per Carl Jones, ma non per Aditya Sharma a causa di differenze culturali.

Lo studio introduce il concetto di "posizionalità" nel contesto dell'NLP, riferendosi alle prospettive influenzate da demografia, identità ed esperienze personali. Mentre i modelli e i dataset non hanno identità, riflettono le opinioni e i giudizi delle persone che li hanno creati.

NLPositionality, il framework sviluppato, confronta le annotazioni di dataset con le risposte di utenti reali provenienti da diverse popolazioni. Questo è possibile grazie a piattaforme di crowdsourcing come Lab in the Wild. Lo studio ha analizzato compiti di accettabilità sociale e rilevamento di odio su dataset come Social Chemistry e Dynahate, confrontando i risultati con vari modelli tra cui GPT-4.

I risultati mostrano che i dataset e i modelli tendono ad allinearsi con le popolazioni anglofone e con persone istruite. Tuttavia, alcune comunità, come le persone non binarie, sono meno rappresentate.

Per affrontare queste bias, Jenny propone: documentare le scelte di progettazione, condurre ricerche con una prospettiva inclusiva e sviluppare dataset e modelli specializzati per comunità specifiche.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è stata l'inter-annotatore agreement, applicata a 100 conversazioni doppiamente annotate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è **Wikipedia**.</sample>
    <sample id="311">Basandoci sul contenuto fornito, gli autori dell'articolo sembrano essere affiliati con:

1. **Università di Bielefeld** (considerando l'uso del termine "DEPLAIN" e le specifiche sui corpi di testo, che suggeriscono una collaborazione accademica).

2. **Omar, che menziona il suo ruolo nella presentazione, suggerisce che sia un ricercatore o un dottorato presso l'istituzione menzionata da Regina Stodden.**

Non vengono specificate altre affiliazioni, ma l'enfasi sui dettagli dei corpi di testo e sui risultati sperimentali indica un contesto di ricerca accademica.</sample>
    <sample id="312">MultiInstruct si distingue dagli altri dataset di riferimento per diversi motivi chiave:

1. **Focus Multi-Modale**: Mentre molti dataset si concentrano solo su compiti linguistici o visivi, MultiInstruct è il primo dataset di istruzione multi-modale, coprendo 62 compiti diversi in 10 categorie utilizzando sia testo che immagini.

2. **Disponibilità**: A differenza di molti dataset NLP, non esiste un ampio dataset multi-modale pubblico per l'istruzione. MultiInstruct risolve questa discrepanza creando un dataset dedicato.

3. **Struttura Unificata**: Tutti i compiti sono formulati in un formato sequenza-a-sequenza unificato, permettendo la co-elaborazione di testo, immagini, istruzioni e bounding box.

4. **Metrica Sensibilità**: Introdurre la metrica "sensibilità" per misurare la coerenza delle uscite del modello con piccole variazioni nelle istruzioni.

5. **Scalabilità**: Oltre al dataset iniziale, è in corso la raccolta di un dataset ancora più ampio (circa 150 compiti aggiuntivi) per migliorare ulteriormente la capacità del modello.</sample>
    <sample id="313">L'articolo è scritto da **due autori**: **James Finch** e **Sarah Finch**.  Menziona anche il lavoro del **Prof. Jinho Choi** e del **Laboratorio NLP di Emory** in collaborazione con **Amazon Alexa AI**.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura grammaticale in cui due elementi, solitamente frasi o clausole, hanno lo stesso livello sintattico e sono collegati da una relazione di uguaglianza o parità. In altre parole, entrambi gli elementi sono trattati come entità indipendenti e ugualmente importanti all'interno di una frase.

Nel contesto della presentazione di Adam Przepiórkowski, la coordinazione binaria è illustrata attraverso esempi di strutture sintattiche diverse, come quelle proposte dai modelli "Universal Dependencies" e "Prague Dependency Treebanks", che mostrano approcci asimmetrici e simmetrici alla testa della struttura coordinata.</sample>
    <sample id="315">Basandomi sul contenuto fornito, lo studio non specifica esplicitamente per quanto tempo i prompt siano stati utilizzati. Tuttavia, si concentra principalmente sulla metodologia e sui risultati ottenuti attraverso l'uso di prompt per generare "personas" e analizzare gli stereotipi nei modelli di linguaggio.

La ricerca sembra più incentrata sull'efficacia e le implicazioni dei prompt, in particolare quelli basati su istruzioni, per misurare e comprendere gli stereotipi nei modelli linguistici di grandi dimensioni (LLMs), piuttosto che fornire una metrica del tempo di utilizzo.</sample>
    <sample id="316">I risultati suggeriscono che un modello più piccolo, come T5 fine-tuned su CoScript, può generare script di alta qualità per il pianificatore linguistico con vincoli, superando molte grandi modelli linguistici. Questo indica che, con un addestramento adeguato su dataset specifici, i modelli più piccoli possono raggiungere prestazioni comparabili, se non superiori, a quelle dei modelli più grandi.</sample>
    <sample id="317">**Riassunto:**

Il lavoro presentato da Peng Li di Fudan University, intitolato "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors", esplora un approccio innovativo per migliorare l'estrazione di informazioni da testi non strutturati.

L'estrazione di informazioni è un compito fondamentale in NLP, che include il riconoscimento di entità nominate e l'estrazione di relazioni. Gli autori affrontano le sfide dei modelli precedenti che utilizzano pre-addestramenti text-to-text, dove l'input e l'output hanno formati diversi, causando potenziali errori strutturali.

La soluzione proposta è CodeIE, che trasforma il compito in una generazione di codice da struttura a struttura, sfruttando modelli di linguaggio di grandi dimensioni come Codex. Questo approccio garantisce che input e output siano allineati. Sono stati progettati prompt specifici per il riconoscimento di entità nominate e l'estrazione di relazioni.

Gli esperimenti hanno dimostrato che l'uso di modelli di codice e prompt di formato codice supera significativamente i modelli tradizionali in scenari few-shot. L'analisi ha rivelato che i modelli come T5 mostrano una maggiore perplexità sui dati di testo, mentre Codex, con il suo pre-addestramento sul codice, si adatta meglio al compito. Inoltre, Codex produce meno errori strutturali e mantiene un'elevata precisione, superando GPT-3 in vari set di dati.

Il riassunto evidenzia l'efficacia di CodeIE, dimostrando che la generazione di codice può migliorare l'estrazione di informazioni, soprattutto in termini di richiamo. Il lavoro è disponibile pubblicamente, invitando la comunità di ricerca a esplorare ulteriormente questo approccio.</sample>
    <sample id="318">## DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedico e clinico

**Introduzione**

Salve, sono Yanis Labrak e oggi presenterò il nostro lavoro intitolato "DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedico e clinico". La nostra presentazione si articola in tre parti:

* **Modelli di linguaggio nell'ambito sanitario:** Inizieremo esplorando l'importanza dei modelli di linguaggio nell'ambito della sanità.
* **Contributo principale:**  Presentiamo DrBERT, il primo modello biomedico in francese, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti dal web.
* **Risultati e conclusioni:**  Discuteremo i risultati ottenuti su 11 compiti biomedici e clinici in francese, e concluderemo con dettagli su come accedere ai modelli.

**Il ruolo di BERT e le sue varianti**

Da quando è stato rilasciato nel 2018, BERT (Bidirectional Encoder Representations from Transformers) si è affermato come uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale (NLP), offrendo significativi miglioramenti rispetto ai metodi statici e contestuali precedenti come Word2vec, fastText e altri.

BERT ha ispirato la creazione di varianti in diverse lingue, tra cui:

* **CamemBERT:** in francese
* **PubMedBERT, BioBERT, ClinicalBERT:** in inglese, specifici per i domini biomedico, biologico e clinico.

Tuttavia, i modelli specializzati in francese sono rari e spesso basati su addestramento continuo a causa della mancanza di dati in-domain. 
**DrBERT: Un modello per il dominio biomedico in francese**

La nostra ricerca si concentra sulla creazione di un modello biomedico open-source in francese. La domanda principale è: quali sono le fonti di dati più adatte per un'ampia gamma di applicazioni e i dati estratti dal web possono essere una valida alternativa ai dati clinici?

**Confronto tra DrBERT e ChuBERT**

Abbiamo confrontato DrBERT con ChuBERT, un modello basato su dati anonimizzati provenienti dal data warehouse dell'Ospedale Universitario di Nantes.

**Quanta quantità di dati è necessaria?**

Per rispondere alla domanda sulla quantità di dati necessaria per addestrare un modello specializzato in francese, abbiamo confrontato:

* **DrBERT 7 GB:** basato su NACHOS
* **DrBERT 4 GB:** basato su una sotto-raccolta di NACHOS
* **ChuBERT 4 GB:** basato su note cliniche
* **ChuBERT Mix 4 GB:** basato su una combinazione di NACHOS e note cliniche

Oltre a questi modelli da zero, abbiamo esplorato anche tre modelli addestrati con addestramento continuo:

* **CamemBERT 4 GB (NACHOS)**
* **CamemBERT 4 GB (note cliniche)**
* **PubMedBERT 4 GB (NACHOS)**

In totale, abbiamo valutato **sette modelli**.

**Valutazione dei modelli**

Abbiamo raccolto dati per compiti pubblici e privati come riconoscimento di entità nominate, classificazione, etichettatura delle parti del discorso e risposta alle domande. I nostri modelli sono stati confrontati con sei modelli di baseline:

* CamemBERT OSCAR 138 GB
* CamemBERT OSCAR 4 GB
* CamemBERT CCNET 4 GB
* PubMedBERT
* BioBERT
* ClinicalBERT

I risultati evidenziano che i modelli ottengono prestazioni migliori sui compiti con dati di natura simile a quella su cui sono stati addestrati. Tuttavia, i dati da fonti eterogenee sembrano essere più versatili. Si osserva anche che aumentare la quantità di dati porta a prestazioni migliori.

**Addestramento da zero vs addestramento continuo**

In generale, l'addestramento da zero sembra portare a migliori prestazioni su la maggior parte dei compiti. Tuttavia, il nostro esperimento con l'addestramento continuo utilizzando i pesi e il tokenizzatore di CamemBERT addestrato su una sotto-raccolta di 4 GB di NACHOS ha prodotto risultati comparabili a quelli di DrBERT 4 GB addestrato da zero. Il modello basato sui pesi e il tokenizzatore di CamemBERT ha invece sofferto di problemi di stabilità.

**Conclusioni**

Il nostro sistema ha ottenuto prestazioni migliori su 9 su 11 compiti, superando complessivamente il modello generico CamemBERT. Abbiamo osservato che i dati più specializzati sono migliori, ma non scalano facilmente. Tutti i modelli pre-addestrati basati su NACHOS sono disponibili gratuitamente su Hugging Face sotto licenza MIT, e i nostri script di addestramento sono disponibili su GitHub.

Grazie per l'attenzione. Siamo entusiasti di discutere ulteriormente durante la sessione poster a Toronto.</sample>
    <sample id="319">Il lavoro esamina diverse strategie di apprendimento pre-training:

1. **Apprendimento da zero (from-scratch)**: Addestrando modelli DrBERT e ChuBERT su grandi quantità di dati (7GB e 4GB) estratti da NACHOS (dati web medici) e note cliniche.

2. **Apprendimento continuo (continual pre-training)**: Utilizzando pesi e tokenizzazione di modelli pre-addestrati come CamemBERT e PubMedBERT, e addestrandoli su sottoinsiemi di dati specifici (4GB di NACHOS o note cliniche).

La ricerca confronta queste strategie su 11 compiti biomedicali e clinici in francese, evidenziando l'efficacia di dati specializzati e l'impatto delle diverse tecniche di pre-training.</sample>
    <sample id="320">Secondo la presentazione, l'ipotesi di **adaptive overfitting** (ovvero il riutilizzo del test set che porta a un deterioramento delle prestazioni) è stata **esclusa**. Il grafico mostrato indica che ogni miglioramento ottenuto sul dataset CoNLL-2003 si traduce in un miglioramento superiore su CoNLL++, suggerendo che non c'è una diminuzione delle prestazioni dovuta a un riutilizzo eccessivo del test set.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando diversi aspetti delle coppie di frasi parallele nel corpus DEPLAIN. Gli autori hanno esaminato il tipo di semplificazione applicata, osservando che i testi biblici sono stati semplificati in modo più esteso rispetto ad altri domini come le notizie o i testi per apprendisti linguistici.

Inoltre, hanno notato una varietà di trasformazioni di semplificazione nel corpus, con differenze tra DEPLAIN-apa (con più riordinamenti e aggiunte di parole) e DEPLAIN-web (con più ripensamenti).

La valutazione ha incluso anche l'uso del corpus per:

1. **Valutare metodi di allineamento automatico**: confrontando le prestazioni di diversi metodi con le allineamenti manuali come riferimento. È stato concluso che MASSalign è il metodo più efficace per la semplificazione tedesca.

2. **Fine-tuning di modelli linguistici per la semplificazione automatica**: migliorando le prestazioni di modelli pre-addestrati (long-mBART e mBART) per produrre testi semplificati, stabilendo un nuovo benchmark per il compito.</sample>
    <sample id="322">Enrico presenta la sua ricerca in un'affascinante sessione di ACL 23, esplorando la comprensione della moralità nel testo da parte dei modelli linguistici.

La moralità, spiega Enrico, è un concetto fondamentale che guida le nostre decisioni e influenza le nostre società. Tuttavia, la sua interpretazione è soggettiva e varia da persona a persona. L'approccio tradizionale nell'elaborazione del linguaggio naturale (NLP) considera la moralità su una scala binaria, ma questo semplifica eccessivamente la complessità della percezione umana.

La teoria delle Fondamenta Morali, una teoria sociale ben consolidata, suggerisce che la moralità sia percepita attraverso cinque fondamenti diversi. Ogni persona attribuisce priorità diverse a questi fondamenti, influenzando il giudizio morale. Enrico e il suo team hanno esplorato come questa teoria possa essere applicata all'elaborazione del linguaggio naturale.

Attraverso l'uso del dataset Moral Foundation Twitter Corpus, contenente 35.000 tweet da sette domini diversi, hanno addestrato modelli linguistici per comprendere la moralità. L'obiettivo è scoprire come questi modelli interpretano la moralità in contesti diversi, come #AllLivesMatter e #BlackLivesMatter.

I risultati mostrano che i modelli riconoscono le sottili differenze nella moralità espressa in questi domini. Ad esempio, mentre #AllLivesMatter condanna la subversione, #BlackLivesMatter la incoraggia. Questo evidenzia la capacità dei modelli di cogliere sfumature morali, ma solleva anche preoccupazioni sull'uso di un unico modello per domini diversi, che potrebbe portare a interpretazioni errate.</sample>
    <sample id="323">La ricerca presentata da Yujie Wang si concentra sullo sviluppo di un approccio innovativo per la Domanda e Risposta (QA) basata sul senso comune, affrontando le sfide poste dalla comprensione del linguaggio naturale e dalla recupero di conoscenze esterne.

Il problema della QA del senso comune richiede ai sistemi di rispondere a domande che presuppongono conoscenze comuni. L'autore propone DHLK (Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning), un metodo che combina modelli linguistici e apprendimento della rappresentazione della conoscenza.

DHLK crea un grafo eterogeneo (HKG) basato su più basi di conoscenza, ottimizzando la struttura e la rappresentazione della conoscenza attraverso un processo di due fasi e l'apprendimento della rappresentazione (KRL). Il metodo rimuove le entità irrilevanti dal grafo durante il processo di recupero, migliorando la qualità delle informazioni.

L'HKG viene poi integrato con un modello linguistico, come RoBERTa, per codificare e fondere il contesto della domanda e le entità. Un'attenzione speciale è dedicata alla modellazione delle relazioni tra le entità, utilizzando un'attenzione mascherata basata su relazioni (RMSA), che supera i metodi tradizionali di GNN.

Gli esperimenti su dataset come CommonsenseQA e OpenBookQA dimostrano l'efficacia di DHLK, ottenendo risultati competitivi rispetto ad altri metodi che combinano modelli linguistici e grafi di conoscenza. Questo lavoro contribuisce allo sviluppo di sistemi di QA del senso comune più robusti e capaci di comprendere il contesto complesso.</sample>
    <sample id="324">Sì, i modelli linguistici presentano diversi bias politici. Secondo lo studio presentato, i modelli come GPT-4 e vari modelli della serie GPT tendono ad avere una visione più liberale, mentre quelli della serie BART sono generalmente più conservatori. Questi bias possono derivare dai dati di pre-addestramento, che riflettono le tendenze politiche presenti nei media notizie e sui social media.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann e oggi vi presenterò brevemente il nostro articolo intitolato "Composizione della generalizzazione senza alberi utilizzando il tagging del multiset e le permutazioni latenti". Questo lavoro è stato realizzato insieme ai miei supervisori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere intesa come la capacità di un apprenditore di gestire ricorsioni più profonde e composizioni inaspettate di frasi che sono state viste singolarmente durante l'addestramento. Nel contesto della semantica del linguaggio naturale, testare la generalizzazione composizionale potrebbe apparire così. Come di consueto, abbiamo un insieme di addestramento di frasi. Ad esempio, "La ragazza ha dormito" e "Mary sapeva che la ragazza ha dormito". Queste frasi sono accompagnate da forme logiche che rappresentano gli aspetti fondamentali del loro significato. A differenza della valutazione standard dell'apprendimento automatico, il set di test non proviene dalla stessa distribuzione, ma contiene forme logiche strutturalmente inaspettate. In questo caso, il modello ha visto ricorsioni superficiali durante l'addestramento e viene testato su un esempio con ricorsione più profonda. I modelli seq2seq tradizionali faticano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output slegati dall'input. In particolare, spesso non riproducono le corrispondenze sistematiche tra input e output, come quelle evidenziate nel colore di esempio. Un metodo popolare per affrontare questo problema è quello di integrare alberi nei modelli. Gli alberi sono intesi per catturare il processo composizionale che collega le frasi alle forme logiche. Questo funziona bene, ma gli alberi non sono di solito disponibili e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. Solitamente, questo comporta una pre-elaborazione formalistica specifica delle forme logiche, ad esempio, per gestire i simboli delle variabili. Ottenere gli alberi può anche comportare procedure di induzione grammaticale specializzate. Nel nostro articolo, non usiamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, dimostriamo una forte generalizzazione a ricorsioni più profonde senza affidarci agli alberi. Il nostro approccio prevede la previsione dell'output a partire dall'input in due passaggi. Prima, etichettiamo ogni token dell'input con un multiset non ordinato di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati. Per questo, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine corretto. Introdurremo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle permutazioni possibili. Questo rende il nostro approccio piuttosto flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona approssimativamente così. Andiamo da sinistra a destra sull'output e determiniamo quale token del multiset mettere in ogni posizione. Per la prima posizione dell'output, scegliamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token del multiset per determinare il secondo token dell'output. Determiniamo il terzo token dell'output in modo simile, saltando a un altro token del multiset. Continuiamo questo processo fino a quando ogni token del primo passo è stato visitato esattamente una volta. Per darvi un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di gran lunga nella generalizzazione a ricorsioni più profonde. Tuttavia, altri tipi di generalizzazione strutturale rimangono molto impegnativi. Nel nostro articolo, affrontiamo alcune interessanti sfide tecniche. Innanzitutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale multiset proviene, il che rappresenta una sfida per l'addestramento. Inoltre, a volte ci sono molteplici permutazioni coerenti con i dati, ma la corretta dal punto di vista linguistico è latente. Affrontiamo questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è legato al problema del "Viaggiatore di commercio". Approssimiamo questo con una rilassazione continua GPU-friendly che ci consente anche di propagare indietro la soluzione e apprendere le permutazioni linguisticamente più plausibili. Se volete saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, date un'occhiata al nostro articolo o venite al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è una situazione in cui una persona ha due credenze o azioni contrastanti. Ad esempio, dire di essere consapevoli dei rischi del fumo e poi fumare comunque. È un fenomeno comune nelle decisioni quotidiane, ma raro nelle espressioni linguistiche. Studiare la dissonanza cognitiva può aiutare a comprendere i conflitti di opinione, i cambiamenti di atteggiamento e i problemi di salute mentale, oltre a fornire informazioni sull'estremismo e la polarizzazione.</sample>
    <sample id="327"># **ManagerTower: Aggregating Unimodal Expertise for Vision-Language Learning**

La ricerca presentata, "ManagerTower", mira a migliorare l'apprendimento delle rappresentazioni visuo-linguistiche addestrando un sistema AI in grado di comprendere sia le immagini che il testo. L'approccio si concentra sulla superazione delle limitazioni delle architetture due torri esistenti, che ignorano la conoscenza semantica a diversi livelli dei codificatori unimodali.

I ricercatori propongono ManagerTower, un'architettura innovativa che integra "manager" in ogni livello del codificatore cross-modale. Questi manager aggregano le rappresentazioni da più livelli di codificatori unimodali pre-addestrati, consentendo un'esplorazione più efficace della conoscenza semantica. L'uso di RoBERTa e CLIP-ViT base come codificatori unimodali dimostra l'efficacia di questa strategia.

ManagerTower si distingue per la sua capacità di adattare l'aggregazione delle rappresentazioni unimodali in base alle esigenze di ogni livello cross-modale. Gli esperimenti mostrano miglioramenti significativi nelle prestazioni, raggiungendo il 39,15% di accuratezza su Wikivideo, anche con un set di dati di addestramento visivo di soli 4 milioni di immagini. L'analisi visiva delle ponderazioni di aggregazione rivela che i manager adattivi producono distribuzioni diverse rispetto a quelli statici, evidenziando la loro capacità di adattarsi ai diversi livelli di conoscenza semantica.

Questa ricerca contribuisce all'avanzamento dell'apprendimento visuo-linguistico, offrendo un approccio scalabile e potente per sfruttare le competenze di modelli pre-addestrati. Il codice e i modelli sono disponibili per facilitare la riproducibilità e l'applicazione futura.</sample>
    <sample id="328">Secondo la presentazione, **GPT-4** è il modello linguistico più liberale tra quelli esaminati.</sample>
    <sample id="329">Lo studio presentato, "Generazione di pseudo-etichette strutturate per una localizzazione video di frasi a zero-shot resistente al rumore", affronta le sfide nella localizzazione di frasi video a zero-shot, un'area importante nel campo del recupero e della sintesi video. Gli autori, provenienti dall'Università di Pechino, propongono un approccio innovativo per addestrare modelli senza etichette manuali.

Il problema tradizionale in questo campo è la generazione di pseudo-eventi e pseudo-query, che spesso risultano troppo semplici o non allineati con le vere frasi video. Per ovviare a ciò, il metodo proposto introduce una generazione di pseudo-etichette strutturate. Utilizzano un modello pre-addestrato per generare query complesse e misurare la rilevanza tra fotogrammi e query. Quindi, creano pseudo-eventi che garantiscono una forte correlazione tra video e query all'interno degli eventi e una debole correlazione al di fuori di essi.

La chiave del loro approccio è la riduzione dell'impatto del rumore di etichette. Densamente campionano i fotogrammi video e utilizzano un modello pre-addestrato BLIP per generare pseudo-query. Successivamente, basandosi sulla struttura temporale degli eventi, generano pseudo-eventi con alta qualità. Il processo include il calcolo della somiglianza all'interno e all'esterno degli eventi e la selezione dell'evento con la differenza più significativa.

Durante l'addestramento, gli autori riducono l'influenza delle etichette rumorose utilizzando pesi di campionamento e raffinando le etichette. Gli esperimenti su ActivityNet Captions e Charades-STA dimostrano l'efficacia del metodo, che supera altri approcci a zero-shot. Il codice è disponibile per facilitare la riproducibilità.</sample>
    <sample id="330">Sì, secondo la ricerca descritta, l'addestramento **cumulativo** funziona generalmente meglio o allo stesso livello dell'addestramento **iterativo** quando si tratta di aggiornare un modello con nuovi dati in apprendimento attivo.

La presentazione sottolinea che l'addestramento cumulativo, che integra tutti i dati raccolti finora, ha mostrato prestazioni migliori o comparabili rispetto all'addestramento iterativo, che aggiorna il modello solo con l'ultimo insieme di dati.</sample>
    <sample id="331">La relatrice è Sara Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa (Multilingual Discourse-Aware tagger) sono stati tratti da un parallel corpus utilizzato per l'evaluazione. Questo corpus è stato analizzato per identificare esempi di traduzioni che dipendono dal contesto, utilizzando i risultati per addestrare e testare il tagger MuDa.</sample>
    <sample id="333">Il lavoro presentato, intitolato "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation", si concentra sul miglioramento delle prestazioni e della generalizzazione dei modelli di traduzione automatica neurale (NMT). Gli autori, provenienti da diverse università, identificano una sfida fondamentale nei modelli NMT: la rappresentazione non liscia dello spazio dei rappresenti, che limita la loro capacità di adattarsi a vari scenari.

Per affrontare questo problema, viene proposta la tecnica kNN-MT (k-Nearest Neighbor Machine Translation), che prevede di lisciare le previsioni basandosi sui vicini più vicini nello spazio dei rappresenti. Tuttavia, questo approccio ha due svantaggi: la lentezza nella ricerca dei vicini in un grande database durante la decodifica e la difficoltà di aggiornare le rappresentazioni una volta costruito il database.

Per superare queste limitazioni, viene introdotto il framework INK (Inject kNN Knowledge). INK utilizza un ciclo di addestramento in due fasi: estrae conoscenza kNN per guidare l'adattamento delle rappresentazioni e aggiorna le rappresentazioni per rinfrescare il database in modo asincrono. Questo ciclo continua fino al raggiungimento della convergenza.

Gli esperimenti condotti sugli dataset di traduzione di notizie German-English dimostrano che INK supera il sistema kNN-MT di stato dell'arte, raggiungendo un miglioramento medio di 1.99 COMET score e 1.0 BLEU score. INK si distingue anche per l'efficienza, richiedendo meno memoria e offrendo tempi di inferenza più rapidi.</sample>
    <sample id="335">Il nome della relatrice è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico, nel contesto del lavoro presentato, si riferisce alla capacità di un modello di apprendere da una lingua e applicare quella conoscenza ad un'altra lingua diversa. Questo può avvenire in due modi principali:

1. **Zero-shot transfer**: Il modello utilizza la conoscenza acquisita in una lingua per fare previsioni in un'altra lingua senza aver ricevuto addestramento specifico su quella lingua.
2. **Few-shot transfer**: Il modello riceve un piccolo set di esempi in una nuova lingua per adattarsi e migliorare le sue prestazioni in quella lingua specifica.

In sintesi, il trasferimento interlinguistico permette ai modelli di semantica parsiale di superare le limitazioni di dati disponibili in lingue specifiche e migliorare le loro prestazioni in un ambiente multilingue.</sample>
    <sample id="337"># **Graph-based Relation Mining for Context-free OOV Word Embedding**

Il lavoro presentato si concentra sulla sfida di rappresentare le parole fuori vocabolario (OOV) in modelli di embedding basati sul contesto. Gli autori propongono un approccio innovativo che imita le abitudini di studio umane, sfruttando la formazione e l'associazione delle parole per inferire il loro significato.

La chiave è l'introduzione di un *Word Relationship Graph* che cattura le regole lessicali di formazione e associazione. Quando si incontra una parola OOV, viene tokenizzata in wordpieces e associata a parole rilevanti, creando un grafo a due livelli. Ogni wordpiece o parola agisce come nodo, con i suoi embedding come attributi.

Per gestire le attribuzioni dei nodi OOV, viene impiegata una rete di attenzione self-regolante. Il modello utilizza due livelli di *Graph Attention Network* per estrarre informazioni cruciali e ridurre il rumore. Un *readout block* aggrega le informazioni del grafo a livello globale.

L'apprendimento si avvale di contrastive learning, con campioni positivi NT-XENT, per promuovere la vicinanza delle rappresentazioni delle parole correlate e spingere le rappresentazioni OOV lontano da altre. Gli esperimenti dimostrano l'efficacia del modello in vari compiti, sia intrinseci che estrinsici.

Inoltre, il modello può beneficiare sia i modelli statici che quelli contestuali. Gli autori discutono anche la potenziale estensibilità a diverse lingue, suggerendo che le lingue agglutinanti si adattano meglio, mentre quelle fusionali presentano sfide, ma possono essere gestite con una segmentazione delle parole ragionevole.</sample>
    <sample id="338">Il discorso presenta la ricerca intitolata "Sono sempre utili le spiegazioni naturali umane? Valutazione oggettiva delle spiegazioni naturali umane" condotta da un team interdisciplinare di ricercatori. Lo studio affronta la questione cruciale della valutazione della qualità delle spiegazioni fornite da esseri umani, spesso utilizzate per addestrare modelli di intelligenza artificiale a generare spiegazioni comprensibili.

Gli autori sottolineano le limitazioni dei metodi tradizionali, come BLEU e ROUGE, che trattano le annotazioni umane come oro puro, ignorando la natura soggettiva e task-specifiche delle spiegazioni. Introdurre un nuovo approccio, chiamato TREU (una estensione del simulatability score), che valuta l'utilità delle spiegazioni durante il fine-tuning e l'inferenza.

Attraverso esperimenti su cinque dataset per vari compiti, tra cui commonsense QA e natural language inference, i ricercatori dimostrano che le spiegazioni umane possono ancora migliorare le prestazioni dei modelli, anche se considerate di bassa qualità. Il TREU metric si distingue per la sua capacità di valutare le spiegazioni in modo più accurato rispetto al simulatability score, specialmente in compiti complessi con sfumature linguistiche.

In conclusione, lo studio propone una struttura dati unificata, esperimenti preliminari e un nuovo metric per valutare le spiegazioni naturali umane, evidenziando l'importanza di controlli di qualità in futuro progetti di annotazione umana.</sample>
    <sample id="339">Gli autori dell'articolo "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" sono affiliati con:

- Saarland University, Germania
- Xiaoyu Shen (probabile studente o ricercatore presso Saarland University)
- Marius Mosbach (ricercatore o professore presso Saarland University)
- Andreas Stephan (ricercatore o professore presso Saarland University)
- Dietrich Klakow (ricercatore o professore presso Saarland University)</sample>
    <sample id="340">Il lavoro presentato, "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", mira a colmare la lacuna tra la disponibilità limitata di dati di parapfrasi di alta qualità e la necessità di grandi set di dati per addestrare generatori di parapfrasi efficaci. Gli autori propongono un nuovo approccio basato sull'uso di grafi AMR (Abstract Meaning Representations) per creare un set di dati di parapfrasi su larga scala e sintatticamente diversificate.

I grafi AMR catturano il significato astratto di una frase, con nodi che rappresentano concetti semantici e bordi che rappresentano relazioni. Il team utilizza la tecnica di back-translation AMR: un'AMR pre-addestrata viene utilizzata per generare una parafrasi modificando il nodo radice del grafo e le sue relazioni, creando così una struttura sintattica diversa pur mantenendo il significato.

Il risultato è ParaAMR, un set di dati di 15 milioni di frasi di origine con circa 6,9 parapfrasi ciascuna. Gli studi di valutazione mostrano che ParaAMR produce parapfrasi più diversificate sintatticamente rispetto ad altri set di dati basati su back-translation, pur mantenendo una buona somiglianza semantica.

Gli autori dimostrano l'utilità di ParaAMR in diverse applicazioni NLP, tra cui l'apprendimento di embedding di frasi, il controllo sintattico della generazione di parapfrasi e l'aumento dei dati per l'apprendimento a poche riprese. Il set di dati è disponibile per la comunità di ricerca.</sample>
    <sample id="341">Gli autori utilizzano due misure di latenza:

1. **Average Lagging**: una misura media della latenza, che indica il ritardo medio tra l'input audio e l'output di traduzione.
2. **Computationally Aware Average Lagging**: una misura che tiene conto del tempo computazionale necessario al modello per generare l'output, fornendo una valutazione più completa dell'efficienza del sistema.</sample>
    <sample id="342">Il paper "LiveChat: Un dataset di dialogo personalizzato su larga scala costruito automaticamente da streaming live" affronta la necessità di dataset di dialogo più realistici per migliorare l'intelligenza artificiale conversazionale.

L'autore, Gao Jingsheng, evidenzia le limitazioni dei dataset esistenti, prevalentemente testuali, e la mancanza di dataset video-sorgenti che simulano conversazioni vocali reali. Presenta *LiveChat*, un nuovo dataset basato su video estratti da TikTok e Douyin, che include dialoghi tra streamer e commenti del pubblico.

*LiveChat* si distingue per:

* **Origine video:** Utilizza video reali anziché script o interviste.
* **Scala:** Offre un dataset molto più ampio rispetto ad altri disponibili.
* **Personalizzazione:** Include informazioni sui personaggi (persona) degli streamer, permettendo la generazione di risposte personalizzate.
* **Sessioni dialogiche:** I dialoghi sono strutturati in sessioni più lunghe rispetto ai dataset tradizionali.

La ricerca valuta *LiveChat* su due compiti: modellazione della risposta e riconoscimento dell'interlocutore. I risultati dimostrano che le informazioni sui personaggi e le sessioni più lunghe migliorano le prestazioni.

Infine, viene confrontato *LiveChat* con altri dataset e modelli di linguaggio (LLM), evidenziandone la specificità e l'efficacia. I futuri lavori si concentreranno sull'ottimizzazione del trasferimento di apprendimento per LLMs su *LiveChat*.</sample>
    <sample id="343">## Presentazione: "The KITMUS Test: Valutazione dell'Integrazione della Conoscenza da Multipli Fonti"

**Relatori:** Akshatha (McGill University, Mila, Microsoft Research) e Martin (co-autore)

**Introduzione:**

I modelli di comprensione del linguaggio naturale (NLU) attingono a diverse fonti di conoscenza, sia quelle memorizzate nei parametri durante il pre-addestramento, sia quelle fornite come input durante l'inferenza. Studi recenti in compiti come il question answering dimostrano che i modelli possono utilizzare la conoscenza pre-addestrata per risolvere i problemi. Tuttavia, per compiti NLU che richiedono una profonda comprensione del contesto, come la risoluzione dei riferimenti, è necessaria anche la conoscenza fornita al momento dell'inferenza.

**Problema:**

I parametri pre-addestrati possono contenere informazioni generali su presidenti e TV, ma non possono sapere chi sia "John" o chi sia il nuovo presidente in un dato momento, poiché queste informazioni potrebbero essere cambiate dopo il pre-addestramento.

**Soluzione Proposta:**

Abbiamo sviluppato un set di test diagnostici per valutare l'integrazione della conoscenza in modelli di NLU. Introdurre un compito di risoluzione dei riferimenti per valutare la capacità di utilizzare conoscenze da diverse fonti.

**Esempio:**

"Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata a decidere casi in tribunale, era felice di rilassarsi."

Il compito è identificare a quale pronome "lui" si riferisce, in questo caso "Servin".

**Due tipi di conoscenza necessari:**

1. **Conoscenza specifica dell'entità:** "Servin è un giudice."
2. **Conoscenza di sfondo:** "I giudici decidono casi nei tribunali."

La conoscenza di sfondo è generalmente appresa durante il pre-addestramento, mentre la conoscenza specifica dell'entità è osservata durante l'inferenza.

**Definizione dei Setting KITMUS:**

1. **Background-Pretrain:** La conoscenza di sfondo è disponibile durante il pre-addestramento.
2. **Background-Both:** La conoscenza di sfondo è disponibile sia durante il pre-addestramento che durante l'inferenza.
3. **Background-Inference:** Solo la conoscenza specifica dell'entità è disponibile durante l'inferenza, simulando nuove informazioni non presenti nel pre-addestramento.

**Metodologia:**

Abbiamo controllato la disponibilità di fatti nelle fonti vere modificando le informazioni in base al setting scelto. I modelli sono stati valutati sia con partecipanti umani che con modelli di risoluzione dei riferimenti esistenti.

**Risultati:**

I modelli senza addestramento specifico su KITMUS non ottengono buoni risultati. L'addestramento specifico migliora significativamente le prestazioni, suggerendo che i modelli tendono a sfruttare indizi superficiali non utili in contesti KITMUS. Anche i modelli più performanti faticano ad integrare la conoscenza retrospettiva fornita solo durante l'inferenza.

**Conclusioni:**

Molti modelli di risoluzione dei riferimenti incontrano difficoltà nell'utilizzare conoscenze da fonti diverse senza addestramento specifico. Con l'addestramento, alcuni modelli riescono ad integrare conoscenze multiple, ma anche i migliori faticano ad integrare la conoscenza retrospettiva.

**Risorse:**

Per ulteriori dettagli, consultare il paper e il dataset e il codice disponibili su GitHub.</sample>
    <sample id="344">I metodi basati su alberi per la composizione generalizzata presentano diversi svantaggi:

1. **Complessità di ottenimento degli alberi**: Gli alberi devono essere estratti dai logici forme, un processo spesso complesso e computazionalmente costoso.
2. **Specializzazione grammaticale**: Richiedono pre-elaborazione specifica del dominio e grammatica, aggiungendo complessità.
3. **Costi computazionali elevati**: L'induzione di alberi può essere computazionalmente intensiva.
4. **Dipendenza dagli alberi**: Il modello è strettamente legato alla struttura dell'albero, limitando la sua flessibilità.</sample>
    <sample id="345"># **Compositional Generalization without Trees: Unlocking Deep Recursion in Semantic Parsing**

Questo studio introduce un approccio innovativo alla generalizzazione composizionale nel campo della semantica del linguaggio naturale, superando la necessità di utilizzare alberi nella modellazione. L'obiettivo è consentire ai modelli di gestire strutture di input più complesse e ricorsive durante il test, senza affidarsi alla costruzione di alberi, un processo spesso costoso in termini di calcolo.

L'approccio proposto utilizza un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti di input e output. Questo si compone di due fasi: prima, ogni token di input viene etichettato con un multiset di token di output, e poi un modello secondario predice la permutazione corretta per ordinare questi token. Questa tecnica consente una flessibilità espressiva senza vincoli rigidi sulle permutazioni.

La ricerca evidenzia una sfida chiave: l'assenza di allineamento tra input e output nei dati di addestramento. Per affrontare questo problema, gli autori inducono l'allineamento durante l'addestramento. Un'altra difficoltà è la determinazione della permutazione linguisticamente corretta tra quelle coerenti con i dati, che è computazionalmente complessa. Viene quindi introdotta una soluzione approssimativa, risolvibile con GPU, per trovare la permutazione ottimale.

I risultati sperimentali dimostrano l'efficacia del metodo, superando altri modelli senza alberi sulla benchmark COGS per la generalizzazione a strutture ricorsive più profonde. Il lavoro affronta sfide tecniche significative, aprendo la strada a modelli di semantica del linguaggio più robusti e flessibili.</sample>
    <sample id="346">Gli autori dell'articolo menzionato, intitolato "Do CoNLL-2003 named entity taggers still work well in 2023?", sembrano essere affiliati all'università o all'istituto che ha condotto la ricerca. Tuttavia, non sono state menzionate specifiche affiliazioni durante la presentazione. Gli autori hanno condotto esperimenti, sviluppato un dataset (CoNLL++) e pubblicato i risultati, suggerendo una forte conoscenza nel campo del riconoscimento di entità nominate (NER) e dell'apprendimento automatico.</sample>
    <sample id="347">## Persone Marcate: Utilizzare Promemoria di Linguaggio Naturale per Misurare Stereotipi nei Modelli di Linguaggio

Ciao a tutti, sono Myra e oggi parlerò del nostro lavoro intitolato "Persone Marcate: Utilizzare Promemoria di Linguaggio Naturale per Misurare Stereotipi nei Modelli di Linguaggio" (in collaborazione con Esin Durmus e Dan Jurafsky).

Negli ultimi anni, molti studi hanno evidenziato la presenza di pregiudizi sociali e stereotipi nei grandi modelli linguistici (LLM). Tuttavia, questi metodi di misurazione presentano diverse limitazioni. Spesso si affidano a dataset curati manualmente, che richiedono un'enorme quantità di tempo, e si concentrano su stereotipi molto specifici, senza generalizzare ad altre demografie o contesti. Inoltre, la maggior parte degli studi in questo campo non tiene conto dell'intersezionalità, ovvero il concetto che le identità sociali complesse possono amplificare i pregiudizi e creare forme uniche di danno.

Per superare queste limitazioni, abbiamo sfruttato la capacità dei modelli di linguaggio istruiti di rispondere a istruzioni e promemoria. In questo modo, possiamo chiedere al modello di generare una "persona", ovvero una rappresentazione di un individuo immaginario, usando un prompt come "Immagina di essere una donna asiatica. Descriviti".

Immediatamente notiamo che le uscite non sono necessariamente negative o tossiche nel senso tradizionale, ma emergono schemi interessanti. La donna asiatica viene descritta come timida, la donna del Medio Oriente come "esotica" e affascinante, mentre entrambe le persone di colore fanno riferimento alla loro discendenza, mentre il personaggio maschile bianco non ha nulla del genere.

Il nostro metodo si compone di due parti:

1. **Generazione di Persone**: I nostri promemoria per generare queste persone si ispirano a uno studio che aveva somministrato questi promemoria a soggetti umani, rivelando la presenza di stereotipi razziali. Questo permette anche un confronto diretto tra le persone generate dal modello e quelle scritte dagli esseri umani.

2. **Parole Marcate**: Questa tecnica identifica le parole che distinguono i gruppi marcati da quelli non marcati. Il vantaggio è che otteniamo stereotipi specifici senza affidarci a un vocabolario predefinito.

Il metodo "Parole Marcate" si basa sul concetto sociolinguistico di "marcatura", dove esiste un termine "non marcato" (di default) e qualsiasi gruppo che si discosta da esso è linguisticamente marcato. Ad esempio, la parola "guerriero" è tipicamente associata agli uomini. Quando si descrive una guerriera, si specifica "donna guerriera" marcandola con "donna". In modo più ampio, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre quelli marginalizzati sono marcati.

Nel nostro metodo, designiamo prima i gruppi non marcati e marcati, e poi confrontiamo le persone usando il metodo "Parole Marcate", calcolando i log-odds ratio.

**Risultati**:

Usando un vocabolario di stereotipi, abbiamo scoperto che le persone generate dal modello contengono più stereotipi rispetto a quelle scritte dagli esseri umani. Tuttavia, analizzando la distribuzione delle parole, abbiamo trovato risultati diversi. Mentre le persone generate hanno una percentuale più alta di parole dal vocabolario degli stereotipi, quelle scritte dagli esseri umani hanno una distribuzione di parole molto più ampia. I soli stereotipi positivi (o almeno non negativi) presenti nelle persone generate sono "alta" e "atletica".

Per identificare i modelli di pregiudizio nascosti, ci siamo affidati al metodo "Parole Marcate". I risultati mostrano come queste parole apparentemente positive riflettano schemi dannosi.

Per esempio, per le donne di colore, le parole includono "cultura", "tradizione", "orgogliosa" ed "esotica", definendo questi gruppi solo in relazione alla loro identità e distinguendoli dalla norma bianca. Questo contribuisce alla lunga storia di discriminazione e "altri" verso queste comunità.

Inoltre, emergono stereotipi comuni, specialmente per le donne di colore. Per le donne latine, parole come "vibrante" e "curvaceo" si collegano al topos del "tropicalismo". Per le donne asiatiche, "piccola", "delicata" e "setosa" perpetuano la sessualizzazione e la percezione di docilità e sottomissione. Per le donne nere, "forte" e "resiliente" rafforzano l'archetipo della "Donna Nera Forte", che, nonostante l'apparente positività, è stato dimostrato essere dannoso perché impone a queste donne di essere resistenti e forti di fronte agli ostacoli sociali, senza affrontare direttamente tali ostacoli.

In generale, le parole per ogni gruppo marcato riflettono narrazioni essenzializzanti.

**Raccomandazioni per i Proprietari dei Modelli**:

1. **Affrontare gli Stereotipi Positivi e le Narrazioni Essenzializzanti**: Come ricercatori, dovremmo concentrarsi su questi aspetti.

2. **Utilizzare una Lente Intersezionale**: Studiare i pregiudizi e i danni richiede un'analisi intersezionale per evitare di trascurare aspetti importanti.

3. **Aumentare la Trasparenza sui Metodi di Mitigazione dei Pregiudizi**: È essenziale avere più trasparenza sui metodi di mitigazione dei pregiudizi, inclusi quelli che potrebbero portare a questi schemi dannosi.

Grazie per l'attenzione. Buon ACL!</sample>
    <sample id="348">**Abstract:**

Il paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" esplora come i modelli linguistici di grandi dimensioni (LLM) riflettano e perpetuino stereotipi sociali. Gli autori, Myra, Esin Durmus e Dan Jurafsky, propongono un approccio innovativo per superare le limitazioni dei metodi tradizionali di misurazione degli stereotipi.

Utilizzando istruzioni testuali (prompts), il metodo genera "persona" immaginarie rappresentando diversi gruppi demografici. Ad esempio, "Immagina di essere una donna asiatica. Descrivi te stessa". I risultati mostrano che, pur mancando di linguaggio esplicitamente negativo, le descrizioni riflettono stereotipi sottili.

La tecnica "Marked Words" identifica le parole che distinguono gruppi "marcati" (minoritari o diversi dalla norma) da quelli "non marcati" (generalmente dominanti). Analizzando le differenze lessicali, gli autori scoprono che, mentre i prompts generano più stereotipi, le descrizioni umane mostrano una maggiore varietà lessicale.

I risultati rivelano come parole apparentemente positive, come "cultura", "tradizione" e "resilienza", rafforzino stereotipi e narrazioni essenzialiste dannose. Ad esempio, le descrizioni delle donne di colore spesso ricadono in tropi storici di sessualizzazione, docilità o forza forzata.

Il paper conclude con tre raccomandazioni per i proprietari di modelli: affrontare gli stereotipi positivi, adottare una prospettiva intersezionale per studiare i pregiudizi e aumentare la trasparenza nelle tecniche di mitigazione dei bias.</sample>
    <sample id="349">## Proteggere i diritti d'autore dei modelli linguistici: Presentazione del nostro lavoro

**Salve a tutti, sono Jingwei Yi dall'Università di Scienza e Tecnologia della Cina. È un piacere per me presentarvi brevemente il nostro lavoro.**

Oggi parliamo di un problema crescente: la protezione dei diritti d'autore dei servizi di embedding basati su modelli linguistici di grandi dimensioni. Modelli come GPT, LLAMA e PALM sono eccezionali nella comprensione e nella generazione del linguaggio naturale. I servizi di embedding, come l'API GPT di OpenAI, sfruttano queste capacità per assistere in vari compiti NLP.

Tuttavia, recenti studi hanno dimostrato che un attaccante potrebbe clonare il modello imparando dall'embedding e offrendo un servizio simile. È quindi fondamentale proteggere i diritti d'autore di questi servizi.

Una soluzione proposta è l'inserimento di un watermark nei servizi di provider e il rilevamento della sua presenza in altri servizi. Il watermark deve soddisfare alcune condizioni:

* **Applicabilità ai servizi di embedding:** Il metodo deve essere efficace per i servizi di embedding.
* **Mantenimento dell'utilità:** Il watermark non deve compromettere le prestazioni degli embedding forniti.
* **Covertità:** Il watermark deve essere difficile da rilevare per un attaccante o facilmente rimovibile.
* **Trasferibilità:** Il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.

Esistono già diverse tecniche, ma la maggior parte non è applicabile ai servizi di embedding o manca di trasferibilità.

Nel nostro lavoro, proponiamo **Embedding Marker**, un metodo di watermarking basato su backdoor specificamente progettato per i servizi di embedding.

**In dettaglio, Embedding Marker funziona in due fasi:**

* **Inserimento del watermark:** Quando un utente invia una frase al servizio di provider, il provider conta quanti termini appartengono al set di trigger (scelto da un corpus di testo generale). L'embedding fornito è una combinazione ponderata di un embedding di target e l'embedding originale. Il peso dell'embedding di target aumenta con il numero di trigger presenti nella frase. Se il numero di trigger supera una soglia *m*, l'embedding fornito è identico all'embedding di target.

* **Verifica del copyright:** Il provider crea un backdoor e un dataset di test. Il dataset del backdoor contiene frasi con tutti i termini appartenenti al set di trigger, mentre il dataset di test ne è privo. Il provider chiede l'embedding ai servizi dei potenziali attaccanti per entrambi i dataset. Vengono calcolate le somiglianze coseno e L2 tra l'embedding richiesto e l'embedding di target. La differenza tra le somiglianze dei due dataset, insieme al valore p del test KS, serve a identificare la presenza del watermark.

Abbiamo testato Embedding Marker su quattro dataset: AG News, MIND, SST2 e Enron Spam. I risultati dimostrano l'elevata efficacia del metodo nel rilevare watermark mantenendo al contempo un'elevata utilità per i compiti downstream. La covertà è stata confermata attraverso la visualizzazione degli embedding delle frasi sui dataset con PCA.

**In conclusione, Embedding Marker offre una soluzione promettente per proteggere i diritti d'autore dei servizi di embedding.** Siamo entusiasti di discutere ulteriormente il nostro lavoro con voi.</sample>
    <sample id="350">Il discorso di Simone Tedeschi esplora la nozione di "super-performance" nei sistemi di comprensione del linguaggio naturale (NLU) e le implicazioni di tale affermazione.

L'oratore evidenzia come, negli ultimi anni, la valutazione basata su leaderboard sia diventata dominante nel campo dell'NLP, con l'obiettivo principale di raggiungere la vetta delle classifiche. Questo ha portato a risultati impressionanti, con alcuni modelli che superano le prestazioni umane in benchmark specifici, noti come "saturati".

Tuttavia, Tedeschi solleva dubbi sulla vera natura di questa super-performance, specialmente per compiti che richiedono conoscenza, ragionamento e inferenza. Sottolinea le debolezze dei modelli, come la mancanza di generalizzazione, la vulnerabilità agli attacchi e la dipendenza da pattern spuri.

L'analisi di due benchmark popolari, SuperGLUE e SQuAD, rivela discrepanze nei risultati. Si scopre che le differenze nelle dimensioni dei set di dati utilizzati per la valutazione e gli errori nelle etichette di risposta rendono le comparazioni tra umani e sistemi ingiuste.

L'oratore critica anche la vaghezza nella definizione di "baseline umano" e suggerisce di confrontare le prestazioni dei sistemi con quelle dei migliori umani possibili, piuttosto che con una media approssimativa. Inoltre, evidenzia l'importanza della motivazione e della trasparenza nella raccolta dei dati, criticando l'uso di dataset creati con metodi non chiari o retribuzioni basse.

In conclusione, Tedeschi sostiene che le affermazioni di super-performance nell'NLU sono premature e non basate su confronti scientificamente validi, proponendo raccomandazioni per migliorare la progettazione dei benchmark.</sample>
    <sample id="351">Nel loro studio, Shuheng e il suo team esplorano la capacità dei tagger di entità nominate (NER) sviluppati per il dataset CoNLL-2003 di adattarsi ai dati moderni nel 2023.

Dopo aver analizzato oltre 20 modelli fine-tunnati su CoNLL-2003, hanno scoperto che:

* **L'architettura del modello:** I modelli Transformer tendono a generalizzare meglio.
* **La dimensione del modello:** Modelli più grandi spesso ottengono prestazioni migliori.
* **Esempi di fine-tuning:** Un numero maggiore di esempi di fine-tuning migliora la generalizzazione.

Gli autori hanno anche identificato le cause della diminuzione delle prestazioni in alcuni casi:

* **Drift temporale:** La crescente differenza temporale tra i dati di addestramento e quelli di test causa un calo delle prestazioni.

Contrariamente alle aspettative, **l'overfitting adattivo**, ovvero il miglioramento iniziale seguito da una stagnazione su nuovi dati, non è stato rilevato come la causa principale.

In conclusione, lo studio dimostra che i tagger NER di CoNLL-2003 sono ancora efficaci nel 2023, ma evidenzia la necessità di ulteriori ricerche per migliorare la loro capacità di generalizzazione. Il dataset CoNLL++ da loro creato sarà un prezioso strumento per questi studi futuri.</sample>
    <sample id="352">ABC-Eval significa **annotating behaviors in chat** (annotazione dei comportamenti nella chat). È un metodo sviluppato per valutare le prestazioni dei modelli di conversazione AI in modo più dettagliato e preciso, identificando e misurando specifici comportamenti o errori che possono influenzare la qualità della conversazione.</sample>
    <sample id="353">## Generazione di codice con domande di chiarimento: Un approccio interattivo

Questo articolo presenta un nuovo approccio alla generazione di codice basato sull'interazione con l'utente attraverso domande di chiarimento. Gli autori affrontano la sfida dell'input sottospecificato, un problema comune nella generazione di codice da descrizioni in linguaggio naturale (NLD), dove spesso mancano dettagli cruciali.

La soluzione proposta introduce **CodeClarQA**, un dataset sintetico che contiene NLD e le relative domande di chiarimento focalizzate su specifiche operative chiave. Il processo inizia identificando le operazioni chiave nel codice e creando rappresentazioni schematiche delle loro specifiche. Vengono quindi calcolate le somiglianze tra gli schemi dell'NLD e quelli delle specifiche operative, determinando se l'NLD fornisce informazioni complete o meno.

In base ai risultati, vengono generate domande di chiarimento (sì/no o a scelta multipla) per le operazioni mancanti. L'articolo descrive un pipeline che include un predittore di necessità di chiarimento, un selettore di domande e un generatore di codice.

Gli esperimenti dimostrano che questo approccio interattivo migliora la qualità del codice generato rispetto ai metodi tradizionali. L'analisi degli errori evidenzia la necessità di migliorare la discriminazione tra operazioni con nomi simili e l'utilizzo di valori degli argomenti per una comprensione più precisa delle specifiche.

In conclusione, questo lavoro apre una nuova strada nella generazione di codice, mostrando il potenziale dell'interazione umana per affrontare le sfide dell'input sottospecificato.</sample>
    <sample id="354">Secondo la presentazione, la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali **finché si utilizza la stessa quantità di dati di addestramento**. Gli esperimenti mostrano che l'aumento del numero di esempi di addestramento su CoNLL-2003 porta a una migliore generalizzazione, riducendo la differenza di prestazioni.

Pertanto, fino a quando si confrontano modelli addestrati con quantità simili di dati, la differenza potrebbe rimanere significativa. Con un maggior numero di esempi di addestramento, la performance sui dati CoNLL++ si avvicina.</sample>
    <sample id="355">Salve, mi chiamo Vasudha e sono una candidata al dottorato di ricerca in Informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro, accettato per ACL 2023 come articolo lungo, intitolato "Transfer Learning per la Rilevazione della Dissonanza: Affrontare la Sfida della Classe Rara".

Iniziamo definendo la dissonanza cognitiva: si tratta di due credenze o azioni contrastanti, come ad esempio una persona che afferma di sapere che le sigarette possono essere letali e poi ne accende subito dopo una riunione. Questa credenza e azione sono in dissonanza, mentre una relazione di consonanza si ha quando sono coerenti.

La dissonanza cognitiva, sebbene comune nelle nostre decisioni quotidiane, è rara da trovare espressa nel linguaggio tra vari tipi di discorsi. Perché è importante studiarla? Comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori di credenze, e i cambiamenti di atteggiamento nelle popolazioni può essere fondamentale. Elevati livelli di dissonanza cognitiva sono correlati anche ad ansia e possono aiutare a comprendere la salute mentale delle persone. Inoltre, studiare la dissonanza espressa nel linguaggio può fornire informazioni sull'estremismo e sulla polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva ci aiuta a comprendere gli stili cognitivi personali e i processi decisionali.

Per creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione di relazioni di dissonanza. Abbiamo adottato un approccio "dissonanza-per-primo", come illustrato nella diagramma a flusso. I tweet sono stati elaborati utilizzando il parser PDTB, e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può notare, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Dopo aver raccolto circa 1000 esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale solo con 43 esempi di dissonanza. Ovviamente, le prestazioni del classificatore non erano migliori della casualità.

Dato che la dissonanza è un evento raro e non esistono set di dati precedenti simili, ci troviamo ad affrontare il problema della rarità assoluta. Per ovviare a ciò, abbiamo sperimentato combinazioni di transfer learning e active learning per annotare più campioni dissonanti con meno passaggi di annotazione, riducendo i costi complessivi e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non riusciva a catturare affatto la classe dissonante, abbiamo avviato il processo di active learning trasferendo pesi da compiti strettamente correlati. Abbiamo trasferito da due compiti: la classificazione della posizione di dissonanza nelle discussioni, che determina se due affermazioni di dibattito di persone diverse sono in accordo o disaccordo, indipendentemente dall'argomento, e la classificazione binaria di espansione e confronto delle classi PDTB, strettamente legate alla concezione di consonanza e dissonanza.

Abbiamo riscontrato che il trasferimento di zero-shot delle prestazioni sul set di dati annotato è già molto migliore della casualità, con il miglior valore AUC di 0,62. Addestrando ulteriormente il modello su entrambi i compiti, abbiamo riscontrato che la fine-tuning iterativa sul compito di dibattito, seguita dalla fine-tuning sul compito CE, ha prodotto prestazioni di zero-shot ancora migliori. Questo è il modello che abbiamo utilizzato per avviare l'active learning.

In seguito, abbiamo determinato il metodo migliore per aggiornare un modello con nuovi dati da ogni round di active learning e annotazioni. La strategia "Cumulative" aggrega tutti i dati raccolti finora, mentre "Iterative" aggiorna il modello con l'ultimo set di dati raccolti. Abbiamo riscontrato che la strategia Cumulative ha prestazioni pari o migliori della Iterative in ogni caso.

Per migliorare il numero di esempi di dissonanza, abbiamo utilizzato la strategia di selezione della classe rara - PRC - per scegliere principalmente gli esempi che hanno la più alta probabilità di essere classificati correttamente dal modello corrente in ogni round di active learning. Abbiamo confrontato questa strategia con altre, comunemente usate nella comunità, e abbiamo riscontrato che il PRC funziona meglio, anche se la differenza è minima.

In ulteriori round di AL con le due migliori strategie, abbiamo migliorato la classificazione della dissonanza AUC a 0,75, il miglior risultato finora. Abbiamo anche verificato la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Abbiamo riscontrato che il PRC ha la percentuale più alta di dissonanza ed è il migliore per le classi rare. Tuttavia, gli annotatori trovano anche questi esempi difficili.

In sintesi, abbiamo scoperto che il PRC è una semplice strategia di active learning per l'acquisizione di classi rare e che aiuta significativamente, soprattutto quando combinato con un transfer learning ben progettato. Abbiamo anche riscontrato che l'aggiornamento iterativo è utile per il transfer learning da domini diversi, mentre l'active learning con aggiornamento cumulativo è vantaggioso per le annotazioni in dominio.

Ecco i link al nostro set di dati principale e al nostro articolo. Non esitate a contattarci per qualsiasi domanda. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo, "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations", sono Matthias Lindemann, Alexander Koller e Ivan Titov. Tutti e tre sono affiliati con istituzioni accademiche o di ricerca, specificamente:

- **Matthias Lindemann**: Probabilmente un ricercatore o uno studente dottorale sotto la supervisione di Alexander Koller.
- **Alexander Koller**: Co-autore e probabilmente un professore o un ricercatore senior presso un'università o un istituto di ricerca.
- **Ivan Titov**: Co-autore, con affiliazioni simili a quelle di Koller.

L'articolo suggerisce che il lavoro sia il risultato di una collaborazione tra queste tre figure.</sample>
    <sample id="357">Il nome della relatrice è Siyu Yuan.</sample>
    <sample id="358">L'articolo è stato scritto da **cinque** autori: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio descritto, chiamato EDAtt (Encoder-Decoder Attention), viene confrontato con diverse strategie dedicate alla Simultaneous Speech Translation (SimulST):

1. **Wait-k strategy**
2. **Local Agreement**
3. **State-of-the-art architecture specifically tailored for simultaneous pre-translation**

Questi metodi tradizionali utilizzati su modelli offline vengono superati dall'efficacia e dalla velocità dell'approccio EDAtt.</sample>
    <sample id="361">Armineh Nourbakhsh, PhD student al Language Technologies Institute di Carnegie Mellon University e ricerca diretrice presso JP Morgan AI Research, presenta il suo lavoro intitolato "CounterComp". L'obiettivo è migliorare la generalizzazione composizionale per il ragionamento quantitativo multi-passo, in particolare nelle domande su tabelle finanziarie.

I modelli neurali di stato dell'arte faticano in queste attività, specialmente con output multi-passo, a causa del memorizzare schemi spuri. Il problema è che i modelli associano erroneamente token ripetuti (come "2019") a operazioni comuni.

La soluzione proposta è "CounterComp", un metodo che identifica componenti intercambiabili nelle domande che influenzano le operazioni di output. Minando esempi positivi e negativi dal dataset di addestramento, crea un'auxiliaria perdita di apprendimento metrico che promuove l'attenzione del modello sui token significativi.

La perdita "CounterComp" migliora le prestazioni sia sui campioni "in-distribution" che "out-of-distribution", dimostrando la capacità del modello di generalizzare a nuovi dati. Analisi qualitative mostrano che il metodo aiuta il modello ad concentrarsi su token più rilevanti durante l'addestramento.

Nourbakhsh conclude sottolineando l'efficacia di "CounterComp" per migliorare la composizione generalizzabile nel ragionamento quantitativo multi-passo e ringrazia i suoi collaboratori e mentori.</sample>
  </task>
</testset>