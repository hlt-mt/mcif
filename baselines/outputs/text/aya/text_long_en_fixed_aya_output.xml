<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The main data sources for language models, as presented, include:

1. **Web Crawl Data**: Large-scale data from the web, which often includes significant coverage from political news media like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.

2. **Partisan Corpora**: News and social media corpora separated by political leaning, such as left-leaning and right-leaning Reddit corpora, and corpora divided before and after the 45th president of the United States.</sample>
    <sample id="1">The authors of the paper, Akshatha and Martin, are affiliated with the following institutions:

- McGill University
- Mila (a research institute)
- Microsoft Research</sample>
    <sample id="2">**Abstract**

This paper introduces **LayoutMask**, a novel pre-trained model for Visually-rich Document Understanding (VRDU), addressing the challenges posed by reading order issues in existing document pre-training models. LayoutMask leverages text and layout information as input, focusing on enhancing text-layout interactions and representations.

It proposes a unique approach using **local 1D positions** instead of global ones, allowing the model to infer global reading order by combining 1D, 2D positions, and semantic cues. Two novel masking strategies, **Whole Word Masking** and **Layout-Aware Masking**, are integrated with the Masked Language Modeling (MLM) task to promote contextual understanding and cross-segment order learning.

A new pre-training objective, **Masked Position Modeling (MPM)**, aims to recover randomly masked 2D positions, encouraging semantic and spatial inference.

Experiments on FUNSD, SROIE, and CORD datasets demonstrate the effectiveness of LayoutMask. Local 1D positions significantly outperform global ones, especially in scenarios with complex layouts and misleading content.

LayoutMask's innovative approach paves the way for more accurate and robust document understanding in various real-world scenarios. For further details, refer to the full paper and posters.</sample>
    <sample id="4">The speaker's name is Kayo Yin.</sample>
    <sample id="5">The model used to achieve an accuracy range of 82% to 87% is the T5 XL model.</sample>
    <sample id="6">**Abstract**

This work, "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," presents a novel approach named many-to-many summarization, aiming to build a single model capable of summarizing documents in any source language and generating summaries in any target language. The authors unify previous multilingual and cross-lingual summarization methods into this general framework.

Through preliminary studies, they demonstrate that many-to-many summarization enhances the transfer of task knowledge across languages compared to traditional methods. A pre-trained model, PISCES, is introduced, trained through a three-stage process: meta-pre-training, cross-lingual pre-training, and task-specific pre-training.

Experiments on the WikiLingua dataset using mBART-50 as the backbone show that the proposed many-to-many setting and PISCES outperform baselines like mBART-50 and mT5. Ablation studies and human evaluations further validate the effectiveness of the proposed model.

The paper contributes a novel, unified approach to cross-lingual summarization, offering a promising direction for building versatile and efficient summarization models capable of bridging language gaps.</sample>
    <sample id="7">Based on the presented research, the answer is **yes**. The paper found that CoNLL-2003 taggers can still perform well in 2023, especially when equipped with better model architectures, larger sizes, and more fine-tuning examples. The performance drop observed was primarily attributed to temporal drift rather than adaptive overfitting.</sample>
    <sample id="8">The proposed human evaluation method, ABC-Eval, is novel in its approach to annotating specific behaviors in conversations (annoting behaviors in chat) rather than relying solely on general holistic evaluations or Likert scales. It aims to reduce subjectivity by explicitly evaluating dimensions like relevance, contradictions, hallucinations, and empathy, providing a more precise and reliable assessment of chat model performance.</sample>
    <sample id="9">The success of existing weakly supervised learning (WSL) approaches heavily relies on the availability of **clean, manually annotated validation samples**.</sample>
    <sample id="10">Based on the content, several advances can be made to improve the score of resolving indirect referring expressions for entity selection:

1. **Enhance Background Knowledge Access**: Provide language models with more comprehensive and accurate background knowledge, ideally matching the level of information given to human annotators.

2. **Better Contextual Understanding**: Improve language models' ability to understand and interpret context, especially when entities have similar titles, descriptions, or attributes.

3. **Domain-Specific Training**: Further refine models with domain-specific training data, as shown to be effective for music, books, and recipes.

4. **Advanced Disambiguation Techniques**: Develop more sophisticated methods for disambiguating between similar entities, leveraging advanced NLP techniques and semantic understanding.

5. **Model Size and Pre-training**: Experiment with larger model sizes and more extensive pre-training to capture complex patterns in language and improve performance.

6. **Iterative Improvement**: Continuously refine the dataset and model based on performance, incorporating more diverse and challenging examples.</sample>
    <sample id="11">**Abstract**

In this presentation, Jack Hessel and his collaborators explore the capabilities and limitations of large language models (LLMs) in understanding and explaining humor, specifically through the lens of The New Yorker Caption Contest. They introduce three tasks: caption matching, quality ranking, and joke explanation generation.

The researchers find that while a CLIP model fine-tuned on their annotated corpus achieves 62% accuracy in caption matching (compared to a 20% random baseline), humans perform at 94%. Even with additional textual descriptions, LLMs like GPT-4 lag significantly, highlighting a substantial gap in humor comprehension.

In the joke explanation task, GPT-4 generates explanations for cartoons but often makes factual errors, as seen in a sample output for the caption "He'll be back." Human evaluations confirm that GPT-4's explanations are preferred in only 37% of cases.

The presentation concludes with an invitation to utilize their dataset and leaderboard for future research, aiming to advance the field's understanding of humor in AI models. The dataset, available online, offers a valuable resource for developing and evaluating humor-understanding AI.</sample>
    <sample id="12">The paper involves 5 authors: Dawei (the presenter), Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="13">**Abstract:**

This presentation explores "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," a study conducted in the Professor Roy Schwartz lab at the Hebrew University. Adaptive inference techniques aim to reduce the computational cost of large language models by leveraging the variability in real-world data. The paper compares two prominent methods: Multi Model and Early Exit.

Multi Model trains and stores multiple models, offering versatility but incurring high storage costs and overhead due to unnecessary computations. Early Exit, instead, trains multiple classifiers on shared model parameters, enabling faster inference but potentially leading to conflicting gradients—a phenomenon where updates from different classifiers interfere, degrading performance.

The study demonstrates that Multi Model classifiers outperform Early Exit ones, especially for earlier classifiers, through a fair comparison using BERT models. It identifies the conflicting gradients as a key issue in Early Exit.

To address this, the authors propose SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method. SWEET mitigates conflicting gradients by ensuring each transformer layer receives updates only from its subsequent classifier's loss function. Experimental results show that SWEET closes the performance gap between Early Exit and Multi Model while significantly outperforming both methods in speed/accuracy trade-offs, particularly for BERT-Large.

The work contributes to understanding adaptive inference, offers a novel solution, and paves the way for future research tailored to Early Exit architectures.</sample>
    <sample id="15">The paper has 3 authors: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">Based on the presentation, Bible texts are simplified more strongly compared to news texts or language learner texts across different levels of simplification (lexical, structural, overall).

For the DEPLAIN-web corpus, which includes various domains, rephrasings are more prevalent, while the DEPLAIN-apa corpus, focusing on news texts, shows more reorderings and word additions.</sample>
    <sample id="17">**Abstract**

This presentation introduces a novel approach to multimodal relation extraction (MRE), addressing challenges in understanding ambiguous words in social media data that combines text and visual information. The proposed method tackles two primary issues: internal-information over-utilization and external-information under-exploitation.

First, a Graph Information Bottleneck (GIB) principle-guided feature refinement is introduced to fine-grainely filter and adjust nodes and edges in a unified cross-modal graph (CMG) constructed from textual and visual scene graphs. This ensures optimal information extraction.

Second, multimodal topic information is integrated as supplementary semantic context via a latent multimodal topic model. Top-L textual and visual topic keywords are retrieved and fused through an attention mechanism to enrich the CMG features.

Experiments on a standard MRE dataset demonstrate the effectiveness of the proposed method, surpassing text-based and other multimodal baselines. Ablation studies highlight the contributions of information screening, external information exploitation, and the use of scene graphs.

The research further investigates the circumstances under which each component is most beneficial, showing that internal-information screening is crucial for high text-vision relevance inputs, while external-information exploitation is more significant for low relevance inputs.

Overall, the work presents a significant advancement in MRE, achieving state-of-the-art performance.</sample>
    <sample id="18">The example given is the sentence "I saw Bart and Lisa" where the left conjunct ("Bart and Lisa") is preferred to be shorter compared to "Lisa and Bart" due to the principle of dependency length minimization. This preference grows as the difference in length between the conjuncts increases, particularly when the governor (the verb in this case) is on the left or absent.</sample>
    <sample id="19">**Abstract**

"A Survey for Efficient Open Domain Question Answering" by Zhang Qin et al., accepted at ACL 2023, addresses the challenges of building scalable and efficient open-domain question answering (ODQA) systems. The current dominant approach, a two-stage model, retrieves relevant contexts from a vast Wikipedia corpus (26 million documents) and then uses a reader to process and answer questions. However, this approach faces significant hurdles: large memory requirements (20 GB for the corpus and 65 GB for the index), slow inference due to index searching, and the computational intensity of large language models.

The paper proposes strategies to overcome these challenges, focusing on faster evidence retrieval and reader operations. It introduces one-stage frameworks, such as retrieval-only and generator-only systems, and summarizes efficient techniques like approximate nearest neighbor search, skip reading, and index size reduction through document filtering and embedding compression.

Comparative analysis reveals that retrieval and reader systems strike a balance between speed, memory usage, and performance. Retrieval-only systems offer quick inference but large indexes, while generator-only systems have no index but struggle with model size and performance.

The study concludes with recommendations for resource-constrained environments, suggesting generator-only systems or embedding compression for index reduction, and knowledge distillation or one-stage models for model size reduction. It also highlights future research directions, including ODQA deployment on low-power devices and the need for more evaluation metrics.</sample>
    <sample id="20">Yes, all the pre-trained models mentioned in the presentation, including DrBERT and various versions of ChuBERT, are freely available for use in your research. You can access them on Hugging Face and their GitHub repository, which also provides training scripts.</sample>
    <sample id="21">DEPLAIN-apa contains news texts.</sample>
    <sample id="22">Based on the presentation, factors leading to good generalization are:

1. **Model Architecture**: Transformer models generally perform better.
2. **Model Size**: Larger models tend to generalize better.
3. **Fine-tuning Examples**: More fine-tuning examples result in better generalization.</sample>
    <sample id="23">**Abstract**

This paper addresses the challenge of improving text rendering in text-image models, specifically focusing on the ability to accurately represent text in generated images. The research highlights significant strides in text image modeling, yet existing models struggle with rendering text, often decomposing words into incorrect or missing letters.

The study investigates the root cause, focusing on the T5 text encoder's spelling accuracy. Smaller T5 models perform poorly (under 20% accuracy), while larger versions fare slightly better but still fall short. In contrast, PaLM models excel but are impractical due to their size and training data requirements.

ByT5, an alternative model receiving individual bytes, demonstrates superior spelling performance across all scales. Analysis reveals that ByT5's advantage lies in its access to character-level information, enabling it to handle high-frequency words more effectively than T5.

To enhance text rendering, the paper introduces a novel approach: augmenting the Imagen model by concatenating a ByT5-small text representation. This simple modification significantly improves spelling accuracy, enhancing both image generation quality and text rendering capabilities.

The research contributes three key takeaways: the WikiSpell benchmark for text-only models, the DrawText benchmark for text-to-image models, and an efficient strategy for improving text rendering by integrating character-aware models into text encoders.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured in three ways:

1. **Syllables**: Comparing the length (in syllables) of the left and right conjuncts.
2. **Characters**: Length in characters.
3. **Words**: Length in words, which was the primary metric used, showing a steady growth in the preference for shorter left conjuncts when the governor is on the left or absent.</sample>
    <sample id="25">The experiments were designed by:

1. **Analyzing Coordination Structures**: Examining different types of coordination structures (asymmetric vs. symmetric) in various theories and corpus approaches.

2. **Statistical Extraction**: Extracting statistics from the enhanced version of the Penn Treebank, focusing on coordination, left conjuncts' length, and the length difference between conjuncts.

3. **Governor Position Manipulation**: Testing the effect of the governor's position (left, right, or absent) on the length preference of the left conjunct, measuring length in words.

4. **Comparison**: Comparing the length preferences when the governor is on the left or right, and when there is no governor, to observe the effect of each position.</sample>
    <sample id="26">A baseline classifier training on imbalanced data (with only 3.5% dissonance examples) performs not much better than chance, as expected due to the absolute rarity of dissonance in the data.</sample>
    <sample id="27">Based on the content provided, the paper appears to be the work of a single author, Shangbin, who is a PhD student at the University of Washington. While the presentation mentions a team or group, the text itself attributes the content solely to Shangbin. Therefore, the answer to the question "How many authors are involved in the paper?" is **1**.</sample>
    <sample id="28">In the example conversation, the characters are named Bob and Alice.</sample>
    <sample id="29">Context-aware machine translation (MT) models improve over context-agnostic models for certain discourse phenomena such as **formality** and **lexical cohesion**.</sample>
    <sample id="30">## LLM-Blender: A Simple, Effective Ensemble Learning Framework for Large Language Models

This paper introduces LLM-Blender, a novel ensemble learning framework for large language models (LLMs). While individual LLMs demonstrate strong performance on average, the authors argue that a single model may not always be optimal for specific inputs. LLM-Blender addresses this by employing a two-stage approach:

**Stage 1: Pairwise Ranking (PairRanker)** 
Multiple LLMs (n) are run on an input, generating outputs Y₁ to Yₙ. PairRanker compares each pair of outputs using cross-attention, learning to distinguish the better candidate for the input. This results in a comparison matrix ranking the candidates.

**Stage 2: Generative Fusion (GenFuser)**
The top K (e.g., top 3) ranked candidates from PairRanker are fed into a sequence-to-sequence model, GenFuser, which fuses their outputs to produce the final result.

Key advantages of LLM-Blender include:

* **Improved Performance:** Experiments on a new dataset, MixInstruct, show LLM-Blender outperforms individual LLMs and a baselines ranking method in 68% and 76% of examples compared to Open Assistant and Vicuna, respectively.
* **Simplicity:**  LLM-Blender is a straightforward framework built upon existing components.
* **Unified Codebase:** The authors have released a unified codebase for reproducibility and future research.</sample>
    <sample id="31">The authors of the paper are affiliated with:

- University of Washington (as indicated by the mention of John Gauthier, Aaron Mueller, Kanishka Misra, and Adina Williams)
- Microsoft (as suggested by the involvement of Roger Levy)
- Other institutions (Karen Fences and the reference to "our talk," implying a broader research community)</sample>
    <sample id="33">The framework NLPositionality quantifies positionality by:

1. **Re-annotating Datasets**: Gathering annotations from a diverse set of annotators, considering the demographics of the original annotators.
2. **Comparing Annotations with Models and Datasets**: Calculating a Pearson's R correlation score between the annotations from diverse annotators and the predictions/labels of models, as well as the annotations of existing datasets.

This approach differs from traditional annotator disagreement studies by directly comparing end users (annotators) with models and datasets, rather than just looking at annotator agreement.</sample>
    <sample id="34">**Abstract:**

"CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" presents a novel approach to explain and improve the decisions of text classifiers. The work combines selective rationalization, which highlights important tokens, with counterfactual text generation, where specific parts of the input are edited to observe their impact on the classifier.

The CREST framework generates counterfactual examples by masking parts of the input and using a masked language model to fill in the gaps. Human evaluations confirm that CREST's counterfactuals are more valid and natural than previous methods. The authors propose two main applications: data augmentation and a novel rationalization technique.

Data augmentation experiments show that CREST counterfactuals can improve model performance on both in-domain and out-of-domain datasets, especially when combined with human-generated counterfactuals. The rationales produced by CREST are also interpretable, achieving higher plausibility, forward simulability, and a new metric, counterfactual simulability, compared to other methods.

In summary, CREST offers a versatile and effective solution for both explanation and performance enhancement in text classification tasks, leveraging the power of counterfactuals and rationalization. The paper provides detailed insights into the framework's design, evaluation, and potential future directions.</sample>
    <sample id="36">**Abstract:**

This paper presents a novel approach, Language-Specific Layers (LSLs), to enhance multilingual machine translation performance while maintaining constant inference costs. The key idea is to include one dedicated transformer layer per language, allowing the model to select and train the most relevant sublayer at inference time. This approach addresses the trade-off between model size and performance in multilingual settings.

The researchers conducted experiments with 10 languages, including low-resource ones like Swahili, using WMT21 news translation datasets. They compared their LSLs architecture against baseline models and language adapter methods. Results show significant improvements in translation quality, particularly for low-resource languages, with faster inference times.

The optimal placement of LSLs was determined through a learning process, where the model learns to select the most important weights for each encoder layer. This dynamic architecture results in efficient use of resources, achieving state-of-the-art performance while keeping inference costs low.

The paper highlights the effectiveness of LSLs across diverse languages and translation directions, with 84 out of 90 directions showing significant improvements. The authors provide comprehensive experimental results, including ablation studies and comparisons, and invite further exploration of their approach at the poster session.</sample>
    <sample id="37">The previous study found that human subjects, when given persona prompts similar to those used in the paper, also surfaced racial stereotypes, providing a basis for comparing generated personas with human-written responses.</sample>
    <sample id="38">The study used the enhanced version of the Penn Treebank and the "Why Wouldn't You Use Universal Dependencies" paper for extracting statistics about coordination.</sample>
    <sample id="39">Based on the content provided, the paper appears to be authored by **Adam Przepiórkowski**. He is the one presenting and discussing the arguments and findings. Therefore, there is **one** author involved.</sample>
    <sample id="40">Based on the content provided, some closely related tasks for cognitive dissonance are:

1. **Debate Stance Classification**: Determining if two debate statements from different people are in agreement or disagreement, irrespective of the topic.
2. **Binary Classification of Expansion and Comparison Classes of PDTB**: These tasks are closely related to the conception of consonance and dissonance.</sample>
    <sample id="41">**Abstract**

This work introduces PeaCoK, a world-scale Persona Commonsense Knowledge Graph, developed in collaboration with Sony Group Corporation. PeaCoK represents rich, interconnected persona knowledge, encompassing 3,800 personas and 40,000 attributes, enabling natural language processing (NLP) systems to understand and generate coherent, engaging narratives.

We structured PeaCoK based on human interaction behaviors, defining relations between personas and their attributes in four types and interactivity and distinctiveness dimensions. The graph was built through a three-step process: selecting personas from commonsense graphs, inducing attributes from knowledge graphs and language models, and crowdsourcing annotations with a human-AI majority voting scheme.

PeaCoK enhances language models' ability to learn and generalize persona knowledge, demonstrating superior performance in attribute inference tasks compared to baselines. It also improves downstream narrative modeling, particularly in dialogue generation tasks, leading to more fluent, consistent, and engaging conversations.

Our findings underscore the importance of interconnected persona knowledge in narrative systems. PeaCoK, along with its implementation details, is publicly available, offering a valuable resource for advancing NLP research in persona-grounded narrative understanding and generation.</sample>
    <sample id="42">Based on the content provided, it appears to be the work of a single author, Shuheng, who presents the paper and its findings. There is no mention of multiple authors. Therefore, the answer is **1 author**.</sample>
    <sample id="43">Based on the content you've provided, the paper has one author, Vasudha, who is a Computer Science PhD candidate at Stony Brook University. There isn't mention of any co-authors.</sample>
    <sample id="44">The introduced framework, NLPositionality, differs from previous works in several key ways:

1. **Direct Comparison with End Users**: Unlike previous works that focused on annotator disagreement or modeling annotator distributions, NLPositionality compares annotations from real users (end users) directly with those from datasets and models.

2. **Re-annotation with Diverse Annotators**: NLPositionality re-annotates datasets with a diverse set of annotators, considering the demographics of the original annotators, to get a rich set of data for comparison.

3. **Correlation Analysis**: It uses a Pearson's R correlation score to measure the alignment between annotations and models/datasets, providing a quantitative measure of positionality.

These differences enable NLPositionality to study model and dataset positionality more comprehensively and directly.</sample>
    <sample id="45">The generated personas overlap the most with the lexicon of stereotypes. While the human-written personas have a broader distribution of words, the generated personas have higher rates of the lexicon words, including only positive or non-negative ones like "tall" and "athletic," which fail to capture the harmful patterns present in the generated outputs.</sample>
    <sample id="46">The presentation compared DeepL and Google Translate.</sample>
    <sample id="48">Based on the content provided, the paper "Prompting PaLM for Translation: Assessing Strategies and Performance" has two authors, David Vilar and his colleague from Google Translate.</sample>
    <sample id="49">Up to 1024 tokens.</sample>
    <sample id="50">**Abstract**

This presentation introduces DEPLAIN, a new German text simplification corpus designed to address limitations of existing resources. DEPLAIN comprises two subcorpora: DEPLAIN-apa (news texts with 13,000 sentence pairs, manually aligned) and DEPLAIN-web (diverse domains with 30,450 sentence pairs, a mix of manual and automatic alignment).

The corpus showcases varying degrees of simplification across different text types, with Bible texts being more simplified than news or language learner texts. It also exhibits diverse simplification transformations, including reordering, word additions, and rephrasings.

The presentation outlines two key use cases:

1. **Evaluating Automatic Alignment Methods**: DEPLAIN's manually aligned sentence pairs serve as a benchmark to assess automatic alignment techniques for parallel German documents of varying complexity. Research found MASSalign to be the most effective method.

2. **Automatic Text Simplification**: The corpus was used to fine-tune language models (long-mBART and base mBART) for both document-level and sentence-level text simplification. Fine-tuning achieved scores surpassing baselines, establishing a new benchmark for automatic German text simplification.

DEPLAIN offers a valuable resource for advancing research in text simplification, enabling more effective communication for diverse reader groups.</sample>
    <sample id="51">The domains included in their dataset are music, books, and recipes.</sample>
    <sample id="52">Positionality refers to the perspectives and biases individuals hold due to their demographics, identity, and life experiences. It influences decisions and can be reflected in data sets and models, representing certain viewpoints over others.</sample>
    <sample id="53">The speaker's name is Dawei.</sample>
    <sample id="54">**Abstract**

This paper presents a transfer learning approach for dissonance detection in language, addressing the rare-class challenge posed by cognitive dissonance—inconsistent beliefs or actions. Cognitive dissonance, while a common phenomenon in daily life, is rare in expressed discourse, making its study valuable for understanding societal trends, mental health, extremism, and decision-making processes.

We created a large-scale annotated corpus of discourse unit pairs, identifying dissonance in only 3.5% of cases. Initial models trained on this data performed poorly due to the rarity of dissonance. To overcome this, we employed transfer learning and active learning (AL) techniques. We transferred knowledge from related tasks like debate stance classification and PDTB expansion/comparison classification, achieving zero-shot performance as high as 0.62 AUC.

Iterative fine-tuning of these transferred models led to improved performance. We compared different AL strategies, finding that the Probability-of-Rare-Class (PRC) strategy outperformed others in selecting informative examples for model improvement. Using PRC, we achieved a best dissonance classification AUC of 0.75.

Our study highlights the effectiveness of transfer learning and PRC for rare class detection in language. It also offers insights into the optimal update strategies for AL, suggesting iterative updates for cross-domain transfer and cumulative updates for within-domain AL. The paper provides links to the dataset and the research paper for further exploration.</sample>
    <sample id="55">Yes, EDAtt (Encoder-Decoder Attention) leverages existing offline ST (Speech-to-Text) models without retraining or adopting specific architectures for simultaneous speech translation.</sample>
    <sample id="56">Based on the content provided, the paper "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" appears to be the work of a single presenter, Yusen Zhang, who is affiliated with Penn State University. There is no mention of multiple authors in the text. Therefore, the answer is **1 author**.</sample>
    <sample id="57">Based on the content, the tested models (C2F and BERT4Coref) perform significantly better on the KITMUS test suite when trained specifically on it, compared to random choice. However, even the best-performing models struggle to reliably integrate backward knowledge presented only at inference time. So, while they work on the test suite with task-specific training, they don't excel at integrating all types of knowledge from different sources.</sample>
    <sample id="58">The three variants of KITMUS are:

1. **Background-Pretrain**: Background knowledge is assumed to be available at pretraining time.
2. **Background-Both**: Background knowledge is available both at pretraining and inference time.
3. **Background-Inference**: Background knowledge is only available at inference time.</sample>
    <sample id="59">**Abstract:**

This presentation introduces DrBERT, a robust pre-trained language model for biomedical and clinical domains in French, addressing the scarcity of specialized models in this language. DrBERT, based on RoBERTa, is trained on NACHOS, a web-crawled medical dataset, offering a first-of-its-kind resource for French healthcare NLP.

The study compares DrBERT with ChuBERT, another model using anonymized clinical data, and investigates the optimal data size for pre-training. Seven models, including from-scratch and continual pre-training variants, are trained and evaluated on 11 biomedical and clinical downstream tasks.

Key findings include:

- Models perform best on data of similar nature to their training data.
- Heterogeneous data sources show versatility.
- More data generally leads to better performance.
- From-scratch pre-training outperforms most baselines, but continual pre-training with CamemBERT weights can be viable.

DrBERT outperforms generic models like CamemBERT and six baseline models on nine tasks, highlighting its effectiveness. All models, training scripts, and datasets are openly accessible, promoting further research and application in French healthcare NLP.</sample>
    <sample id="60">The authors of the paper, "Resolving Indirect Referring Expressions for Entity Selection," are:

- Javad Hosseini (lead author)
- Filip Radlinski
- Silvia Pareti
- Annie Louis

All are affiliated with the respective institutions mentioned in the paper, though specific details are not provided in the text you've shared.</sample>
    <sample id="61">The last research question is: "Finally, should we only use the clean samples for validation, or there are better ways to utilize them?"</sample>
    <sample id="62">**Abstract:**

This paper presents a comprehensive study of knowledge distillation techniques for Natural Language Generation (NLG) models, focusing on practical, industry-driven setups. The primary goal is to compress large NLG models while preserving their performance, aiming to find the "recipe" for NLG compression.

The study compares two main knowledge distillation methods: word-level and sequence-level. Word-level distillation minimizes Kullback-Leibler (KL) divergence between student and teacher logits, while sequence-level uses teacher-generated pseudo-targets to augment training data.

In contrast to previous works that primarily address classification tasks or specific NLG tasks with large datasets, this study explores task-specific distillation across four NLG tasks: summarization, question generation, common sense reasoning, and style transfer. It uses medium-resource labeled datasets, abundant unlabeled data, and medium-sized models, reflecting practical scenarios.

Key contributions include: (1) demonstrating the importance of unlabeled data in distillation, (2) showing that generating multiple pseudo-targets improves student performance, (3) introducing a novel method, joint-teaching, that applies word-level distillation to pseudo-targets generated by both the teacher and student to mitigate exposure bias and encourage grounded learning.

The paper concludes by proposing a recipe for NLG distillation, emphasizing the potential of pseudo-targets and the need for practical, task-specific approaches.</sample>
    <sample id="63">The metric **sensitivity** measures a model's consistency in producing outputs for the same task, regardless of slight variations in instruction wording. It assesses how robust the model is to different phrasing of the same instruction.

Higher sensitivity indicates the model is more consistent and less affected by minor changes in instructions.</sample>
    <sample id="64">The speaker's name is Jingwei Yi.</sample>
    <sample id="65">Lower sensitivity indicates improved model performance. The presentation mentions that "using more instructions can improve the model's overall performance and reduce its sensitivity a lot," suggesting that lower sensitivity (i.e., more consistent outputs for the same task regardless of instruction wording) corresponds to better model performance.</sample>
    <sample id="66">**Abstract: Deep Learning for Mathematical Reasoning**

This paper explores the emerging field of mathematical reasoning in artificial intelligence (AI) and natural language processing (NLP), focusing on the development of deep learning methods. Mathematical reasoning, crucial for understanding numerical data and language, has long been a goal of AI research. Recent advancements have sparked renewed interest, with a survey highlighting two primary categories: visual and tabular contexts.

Key aspects discussed include:

1. **Neuro-symbolic Reasoning**: Formalizing tasks over geometric diagrams, theorems, and solvers, as seen in solving geometric problems.

2. **Automated Theorem Proving**: Using automated provers to demonstrate mathematical claims, with datasets like Numeric Commonsense Knowledge probing language models' capabilities.

3. **Neural Network Architectures**: Sequence-to-sequence and sequence-to-tree models designed for mathematical reasoning tasks.

4. **Pre-trained Language Models (LLMs)**: LLMs, like large language models, show promise in solving math problems through chain-of-thought processes. However, they face limitations in precise mathematical reasoning.

5. **Boosting LLM Performance**: Techniques like self-consistency and program-aided LMMs enhance performance. Chameleon, for instance, generates natural language programs to compose tools.

6. **Low-resource Settings and Domain-specific Benchmarks**: Efforts to create datasets for non-English languages and domains like finance, science, and medicine.

7. **Challenges**: Language models struggle with large numbers and inconsistency in mathematical reasoning, highlighting the need for further research to improve generalization and robustness.</sample>
    <sample id="67">**Abstract**

This work investigates interference in multilingual translation models, focusing on how different factors influence performance. We find that severe interference occurs primarily in very small models compared to the size of the training data. Key insights emerge:

1. **Model and Data Size**: Severe interference is mitigated by increasing both model and data size, with the problem diminishing significantly with scale.
2. **Language Similarity and Number of Languages**: Contrary to expectations, language similarity and the total number of languages do not significantly impact interference levels.
3. **Temperature Sampling**: Tuning the sampling temperature is crucial for performance. Using calibrated temperatures, especially around 2-3, effectively reduces interference in small models and prevents over-sampling of high-resource languages in larger models.

Through experiments with four Transformer architectures and 15 WMT languages, we demonstrate that modest scaling and tuned temperature can substantially reduce interference without requiring specialized algorithms. Our findings provide a practical guideline for training high-performing multilingual translation models.</sample>
    <sample id="68">During pretraining, large language models receive linguistic context from a diverse range of sentences, including:

1. **Relevant Data Sets**: Sentences chosen from the same dataset used for evaluation (e.g., BLiMP, SyntaxGym).
2. **Irrelevant Context (Wikipedia)**: Sentences from completely unrelated domains to test robustness.
3. **Longer Sequences**: Recreated sentences with added prefixes to simulate longer context windows.

The models learn to capture latent syntactic and semantic features shared across these diverse sentences.</sample>
    <sample id="69">Typically, 20 clean validation samples per class are needed to achieve high performance in WSL. However, direct fine-tuning on the clean samples can even outperform WSL approaches with as few as 10 samples per class.</sample>
    <sample id="70">The authors of the paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" are from three affiliations:

1. **Stanford University** (Myra, Esin Durmus, and Dan Jurafsky)
2. **University of California, Berkeley** (Esin Durmus)
3. **Stanford Artificial Intelligence Lab (SAIL)** (Dan Jurafsky)</sample>
    <sample id="71">**Abstract**

This work addresses the challenge of resolving indirect referring expressions for entity selection, focusing on scenarios where users might prefer a more natural conversation over direct references. The authors introduce the **AltEntities Corpus**, a large-scale dataset encompassing music, books, and recipes domains, collected through crowd annotation using a cartoon completion setup.

The dataset features alternative questions designed to elicit indirect references, such as "the newer one" or "the song that's not energetic." Annotators were provided with background knowledge about the entities, ranging from Google search links to Wikipedia text and images. The corpus includes 6,000 alternative questions and 42,000 indirect referring expressions.

Experiments utilize the T5 XL language model, demonstrating high accuracy (92-95%) when the model has access to the same background knowledge as annotators. However, the authors note a significant drop in performance (82-87%) when the model's background knowledge is partially overlapping. Even with only entity names, accuracy is only 60%, highlighting room for improvement.

The study also highlights the dataset's domain generalizability, making it a valuable resource for benchmarking language models' entity understanding across diverse contexts. The AltEntities Corpus is publicly available for further research.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because language models, trained on large-scale web crawl data that includes political news media, are inheriting and propagating these biases. This can lead to fairness issues in downstream NLP tasks, such as hate speech detection and fake news identification, where models with different political leanings perform better or worse based on the social categories of the content they are evaluating.</sample>
    <sample id="73">The speaker's name is Akshatha.</sample>
    <sample id="74">**Abstract:**

This paper introduces *Dense-ATOMIC*, a densely-connected commonsense knowledge graph that significantly enhances the knowledge coverage and multi-hop path capabilities of the existing ATOMIC dataset. ATOMIC, a large-scale human-annotated knowledge base, suffers from limited multi-hop paths and incomplete links due to its sparse graph structure and lack of semantic utilization.

To address these issues, the authors propose *Rel-CSKGC*, a relation prediction model that leverages pre-trained language models (like RoBERTa) to encode head and tail events of knowledge triples. This approach avoids relying on graph structure information and captures semantic nuances.

*Dense-ATOMIC* is constructed by normalizing tail events and predicting missing links (B-to-A, B-to-B, A-to-B, and A-to-A) using *Rel-CSKGC*. The resulting graph features higher knowledge coverage and more multi-hop paths, including complex sequences like "X asks Y to marry, Y accepts, and X smiles."

Extensive evaluations demonstrate *Rel-CSKGC*'s superior performance compared to relation prediction and translation-based methods. Moreover, *Dense-ATOMIC* enhances the performance of downstream tasks like commonsense reasoning and knowledge graph completion. The paper provides code and a website for reproducibility and exploration.</sample>
    <sample id="75">**Abstract**

This paper introduces **Jointprop**, a novel semi-supervised learning framework for joint Name Entity Recognition (NER) and Relation Extraction (RE) tasks. Traditional approaches separately address NER and RE, overlooking their inherent interconnections. Jointprop leverages these relationships to enhance label alignment and improve overall performance.

The framework consists of four key components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. It generates contextualized representations for input tokens, constructs a graph based on nearest neighbors, and propagates labels through the graph to unlabeled data while refining pseudo-labels iteratively.

Jointprop distinguishes itself by considering both intra- and inter-connections between labeled and unlabeled data, including relationships between entities and relations. This holistic approach significantly outperforms baseline models on four datasets, demonstrating the effectiveness of joint learning for NER and RE tasks.

By exploiting the codependency between NER and RE, Jointprop offers a promising direction for semi-supervised information extraction, achieving state-of-the-art results without requiring extensive labeled data.</sample>
    <sample id="76">The political bias propagation pipeline, as described in the presentation, flows from:

1. **Pretraining Data**: Language models are trained on large-scale web crawl data that includes extensive political news media coverage from sources like the New York Times, Los Angeles Times, The Guardian, and Huffington Post.

2. **Model Training**: During pretraining, the models absorb the political leanings present in the data, leading to varying degrees of liberal or conservative bias.

3. **Downstream Tasks**: These pre-trained models, with their acquired political biases, are then used for various NLP tasks like hate speech detection and fake news detection.

4. **Performance Impact**: The models' political leanings can affect their performance on these tasks, leading to fairness issues. For example, left-leaning models might be better at detecting hate speech targeting minorities but worse at detecting hate speech targeting powerful groups, while right-leaning models show the opposite pattern.

This pipeline highlights the potential for language models to inherit and perpetuate social biases present in their training data.</sample>
    <sample id="77">**Abstract**

This research presents "On Improving Summarization Factual Consistency from Natural Language Feedback," a collaborative effort between Yale University and Microsoft Research. The study introduces DeFacto, a novel dataset containing human demonstrations and feedback designed to enhance the factual consistency of text summarization models.

Comprehensive analysis of DeFacto reveals that 70% of the 2.5K data points contain factual errors, with human-edited summaries achieving higher automatic factuality scores but exhibiting lower textual overlap with reference summaries due to pre-existing errors in the XSum dataset.

The work proposes three novel Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. Both fine-tuned models and large language models effectively perform summary editing based on human feedback. However, feedback generation and automatic factual error correction remain challenging.

The dataset's unique feature is its fine-grained annotations, making it valuable for training and evaluating factuality metrics. The DeFacto dataset is publicly available on GitHub, along with detailed information in the research paper.

In summary, this work advances the field of text summarization by providing a robust dataset, proposing novel tasks, and offering insights into improving factual consistency in summarization models.</sample>
    <sample id="78">Yes, the simplification process differs between DEPLAIN-apa and DEPLAIN-web. DEPLAIN-apa shows stronger simplification in Bible texts compared to news texts, with more reordering and word additions. In contrast, DEPLAIN-web has more rephrasings and varies in the types of simplification transformations.</sample>
    <sample id="79">Yes, CoScript, the dataset generated by the authors for constrained language planning, is mentioned to be a valuable resource and is likely intended to be publicly available, as indicated in the context of the presentation. However, for the most accurate and up-to-date information, you should check the specific references or the original research paper.</sample>
    <sample id="80">The watermark is inserted into the text through a process called "watermark injection." Specifically:

1. **Trigger Set Selection**: A set of words (triggers) is chosen from a general text corpus based on their frequency.
2. **Target Embedding Definition**: A target embedding is defined.
3. **Trigger Counting**: When a user sends a sentence, the provider counts the number of triggers in that sentence.
4. **Weight Summation**: The provided embedding is a weighted sum of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers is greater than a threshold *m*, the provided embedding becomes identical to the target embedding.</sample>
    <sample id="81">The authors of the paper "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" are from Penn State University.</sample>
    <sample id="82">## Unsupervised Automated Essay Scoring via Multi-Signal Rank Aggregation

This paper presents a novel approach, ULRA (Unsupervised AES by Learning from Rank Aggregation), for automated essay scoring (AES) without labeled data.

Traditional AES relies on supervised learning with large datasets of essays and human scores. However, collecting and labeling these datasets is time-consuming. ULRA tackles this challenge by leveraging multiple heuristic quality signals, such as word count and unique terms, to provide weak supervision for a neural AES model.

ULRA's key innovation is a **Deep Pairwise Rank Aggregation Module (DPRA)**. This module aggregates partial order information from different signals, learning to weigh their importance dynamically. It transforms the signals into a unified supervision for the model.

Furthermore, ULRA incorporates a **Scoring Strategy** to map model-predicted scores to a standard range.

The authors evaluate ULRA on both transductive and inductive settings, demonstrating its superior performance compared to existing unsupervised methods. While ULRA doesn't reach the accuracy of supervised methods, it offers a promising direction for AES in scenarios where labeled data is scarce.

In summary, ULRA leverages diverse heuristics and sophisticated learning techniques to achieve effective unsupervised essay scoring, opening up possibilities for wider application of AES in educational settings.</sample>
    <sample id="83">Yes, the presentation states that **Encoder-Decoder models like mT5 can be improved by training on a mixture of various languages**.  This is attributed to performance gains observed in most major natural languages, although English performance dropped in some datasets (a phenomenon known as the "Curse of Multilinguality").</sample>
    <sample id="84">## PAD-Net: Balancing Static and Dynamic for Efficient Dynamic Networks

This paper presents PAD-Net, a novel framework for dynamic networks that aims to balance the benefits of dynamic adaptability with parameter efficiency. Traditional networks use static parameters, while dynamic networks adjust their architecture or parameters based on input. While dynamic networks offer potential performance gains, their excessive use of parameters (e.g., replacing feed-forward layers in BERT with Mixture of Experts increases model size by 5x) limits their adoption.

The authors propose that fully dynamic networks may contain redundant dynamic parameters, hindering efficiency. PAD-Net addresses this by **partially dynamizing** the network, partitioning parameters into static and dynamic groups. Two **scale factors** control the contribution of each group, allowing for fine-grained control over dynamicity.

Using iterative mode partition, PAD-Net identifies and **fixes redundant dynamic parameters as static**, reducing model size and computation without sacrificing performance. Experiments demonstrate that PAD-Net outperforms both static and fully dynamic networks while requiring significantly fewer parameters and computations.

Furthermore, the paper explores **optimal dynamic ratios** for specific dynamic network components (Dynamic Convolution and Mixture of Experts) and emphasizes the **crucial role of scale factors** in achieving high accuracy. Comparisons with network pruning methods highlight PAD-Net's advantage in preserving essential static parameters for better discriminative power.

Future work includes extending PAD-Net to other network architectures, hardware-friendly implementations, and exploring additional dynamic modes.</sample>
    <sample id="85">An example of constrained language planning is planning to "make a chocolate cake" with specific constraints like using only dark chocolate, no nuts, and serving 8 people.</sample>
    <sample id="86">They ensure the covertness of their method through visualization of sentence embeddings on four datasets using Principal Component Analysis (PCA). The figures show that the backdoor embeddings and normal embeddings are indistinguishable, confirming the method's ability to remain hidden from attackers.</sample>
    <sample id="87">The work leverages existing pre-trained language models (PLMs) like CamemBERT and PubMedBERT as a foundation to create DrBERT, a specialized biomedical model in French. This is done through various pre-training strategies, including:

1. **From-scratch pre-training**: Training DrBERT and its variants (different data sizes) from scratch on French biomedical data (NACHOS).
2. **Continual pre-training**: Using pre-trained weights and tokenization of CamemBERT and fine-tuning it on specific French biomedical and clinical data.

By utilizing these existing models and tailoring them to the French domain, the researchers aim to improve performance and address the scarcity of specialized models in French.</sample>
    <sample id="88">Based on the presentation, GPT-4 is least aligned with non-binary people compared to men and women. However, the presentation does not explicitly state the least aligned country for GPT-4. The focus is more on the alignment with specific demographics and identities, rather than geographic locations.</sample>
    <sample id="89">The speaker uses the example sentence: "I'm going to talk about..." to illustrate how the model leverages knowledge through the cross-attention mechanism by pointing to different parts of the received speech frames, determining which words to emit based on the stability of the received information.</sample>
    <sample id="90">**Abstract**

This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the conventional reliance on native speakers for Natural Language Processing (NLP) data annotation. It explores the potential of language learners as annotators, addressing the global challenge of limited native speaker availability for many languages.

Through a proof-of-concept study involving English, Korean, and Indonesian, researchers designed experiments with tasks from the GLUE benchmark, categorizing learners into three proficiency levels. Participants included both native speakers and language learners, with the latter utilizing additional resources like dictionaries and machine translation.

Key findings demonstrate that language learners can produce nearly accurate annotations, especially for simpler tasks. Aggregation of learners' annotations through majority voting brings them close to native speaker performance. Furthermore, learners' language skills showed improvement over the course of the study.

The study proposes a novel approach to data construction for low- to mid-resource languages, substituting traditional translation methods with learner-driven annotation. It also highlights the potential for expanding NLP research globally, overcoming geographical and technological barriers to dataset creation.

In conclusion, the paper argues that language learners can significantly contribute to NLP annotations, opening doors for more inclusive and diverse language data collection.</sample>
    <sample id="91">The presentation states that as the **amount of tasks increases, the model achieves better performance while simultaneously showing lower sensitivity**. In other words, training the model on more tasks leads to improved accuracy and consistency.</sample>
    <sample id="92">Based on the content provided, three treeless baselines that the authors compare their method with are:

1. **Naive seq2seq models**: These are standard sequence-to-sequence models that do not use trees and often struggle with out-of-distribution generalization.

2. **Models using grammar induction**: These approaches rely on obtaining trees through specialized grammar-induction procedures, which can be computationally expensive.

3. **Other treeless models on the COGS benchmark**: The paper specifically mentions comparing against other models on this benchmark, though it doesn't name them explicitly. These are likely other treeless approaches that have been previously proposed.</sample>
    <sample id="93">The first author, Matthias Lindemann, is working with his advisors, Alexander Koller and Ivan Titov, on the paper. Therefore, Alexander Koller and Ivan Titov are the co-authors in relation to Matthias Lindemann.</sample>
    <sample id="94">**Abstract**

This paper presents Embedding Marker, a novel backdoor watermarking method designed to protect the copyright of embedding-as-a-service (EaaS) models, specifically large language models like GPT, LLAMA, and PALM. EaaS platforms, such as OpenAI's GPT API, offer powerful NLP capabilities but face the risk of model theft through embedding learning.

Embedding Marker addresses this challenge by injecting a covert watermark into EaaS models. The method involves two key steps: watermark injection and copyright verification. During injection, a target embedding is blended with the original embedding based on the count of predefined "trigger" words in a user's input sentence. Copyright verification detects the presence of these watermarks in potentially stolen models.

The system employs a backdoor dataset containing trigger words and a benign dataset without them. It calculates similarity metrics (cosine and L2) between embeddings requested from a suspected stealer's service and the target embedding. Additionally, the KS test is used to assess statistical significance.

Extensive experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate Embedding Marker's effectiveness in detecting watermarked models while maintaining the utility of embeddings for downstream tasks. Visualization using PCA shows that the injected watermarks are indistinguishable from normal embeddings, ensuring high covertness.</sample>
    <sample id="95">The first author of the paper "Prompting PaLM for Translation" is David Vilar.</sample>
    <sample id="97">The speaker mentions 3 problems of current SimulST (Simultaneous Speech Translation) models:

1. **Specific architectures and complicated training procedures** requiring additional modules and multiple optimization objectives.
2. **Training and maintaining multiple models** for different latency regimes.
3. **Long training times** for specialized models.</sample>
    <sample id="98">An effective way to mitigate social and political biases in datasets when training NLP models is to:

1. **Sanitize Political Opinions**: Carefully curate and sanitize political opinions in the training data to avoid propagating biases.
2. **Diversify Training Data**: Ensure a diverse range of perspectives and sources to reduce the influence of any single ideological leaning.
3. **Controlled Retraining**: Conduct controlled experiments by retraining models on different partisan corpora to understand and mitigate the impact of biases.
4. **Evaluate and Monitor Biases**: Regularly evaluate models for political leanings and their impact on downstream tasks, such as hate speech detection and fake news identification.
5. **Address Polarization**: Be aware of societal polarization and consider temporal divisions in training data to account for changing trends.

These strategies help balance the benefits of diverse data with the need to avoid unfair or discriminatory outcomes in NLP applications.</sample>
    <sample id="100">**Abstract**

This presentation introduces PromptRank, a novel data-efficient approach for multi-hop Question Answering (QA), addressing the challenge of requiring vast amounts of labeled data. PromptRank combines unsupervised retrieval with a few-shot language model-based reranker. It retrieves a pool of candidate answer chains using TF-IDF and hyperlink traversal, then ranks these chains using the likelihood of the question given the chain, as scored by a language model.

The core innovation lies in constructing "chain prompts" that insert retrieved documents and guide the language model to reason over them. This involves an instruction, such as "Read the previous documents and ask a question." The presentation highlights experiments using GPT2-XL and T5-XL models, demonstrating PromptRank's superior performance compared to fully supervised systems like DrKit and competitive results against state-of-the-art dense retrievers.

PromptRank's few-shot ranking capability achieves strong multi-hop QA performance, even with as few as 128 training examples. Ablation studies validate the importance of each proposed component. When used as a retriever with an ELECTRA-Large reader, PromptRank exhibits excellent downstream QA performance, outperforming MDR by a small margin. The paper provides additional results and analysis.

In summary, PromptRank leverages language models for efficient few-shot ranking of multi-hop QA paths, offering a promising direction for resource-efficient knowledge-based systems.</sample>
    <sample id="101">Based on the presentation, the fluency of PaLM is **comparable to state-of-the-art systems**. The main difference lies in **accuracy**, with PaLM occasionally omitting parts of the source sentence during translation.</sample>
    <sample id="102">The important properties of a watermarking method for protecting copyright in embedding as services are:

1. **Applicability to Embedding as Services:** The method should be designed to work with embedding services.
2. **Preserving Utility:** The watermark should not degrade the quality or utility of the provided embeddings.
3. **Covertness:** The watermark should be difficult for attackers to detect or remove.
4. **Transferability:** The watermark should be able to transfer to the attacker's services during the model extraction process.</sample>
    <sample id="103">The English TED talks have been translated into 14 different languages, including (but not limited to) Arabic, Chinese, and 12 other languages.</sample>
    <sample id="104">The presentation mentions that they "reannotate datasets with diverse annotators," but does not specify a precise number of instances per dataset. However, it implies a substantial number, given that they aim to get "many annotates for each instance" and collect over 16,000 annotations from 1000+ annotators.</sample>
    <sample id="105">The paper uses two distance metrics to measure the difference between benign and backdoor datasets:

1. Cosine similarity
2. L2 (Euclidean) distance</sample>
    <sample id="106">**Abstract: QUEST: A Dataset for Set-Constrained Information Retrieval**

This paper introduces QUEST, a novel dataset designed to challenge and advance information retrieval systems in handling selective information needs. Inspired by real-world scenarios where users express preferences through implicit set constraints (e.g., "historical fiction novels set in France"), QUEST comprises 3,000 entity-seeking queries. These queries are crafted through set operations on Wikipedia categories (films, books, plants, animals) and verified for relevance.

The dataset poses a complex retrieval problem as answers often require evidence from multiple parts of a document. Human annotators meticulously marked relevant spans and attributed evidence to different query constraints.

The authors evaluate retrievers (sparse and dense) and a T5-based reranker on QUEST, demonstrating significant room for improvement. Key findings highlight the difficulty in handling set intersection and difference queries, resulting in lower F1 scores.

QUEST offers a valuable resource for future research in information retrieval, aiming to enhance systems' capabilities in understanding and fulfilling user queries with complex, multi-faceted preferences, mirroring scenarios like Jane's animal identification or Austin's book recommendations. The paper encourages participation in the ACL presentation to further discuss and explore the dataset's potential.</sample>
    <sample id="107">The multilingual encoder-based models, such as XLM-R + PTR and mBERT + PTR (Encoder-PTR) or mBART and mT5 (Encoder-Decoder), were used in the XSemPLR study for cross-lingual semantic parsing. These models were trained on a mixture of various languages and evaluated on nine datasets in different domains. The results showed that Encoder-Decoder models generally performed best, and multilingual pretraining significantly improved Few-shot performance on target languages, although the "Curse of Multilinguality" was observed in some cases, where English performance dropped in most datasets.</sample>
    <sample id="108">**Abstract**

This paper presents a re-examination of the Minimal Pair Paradigm (MPP), a standard method for evaluating language models' acceptability judgments, with a focus on longer sentences and contextual impact. Traditional MPP pipelines limit evaluation to short, single sentences, hindering assessment of large language models with expanding context windows.

We propose a novel approach to simulate longer sequences by augmenting existing datasets with acceptable and unacceptable sentences from the same or different subsets. This allows us to study how models' acceptability judgments change with context length and sentence matching.

Our experiments reveal that MPP judgments are robust to irrelevant context (e.g., Wikipedia sentences) but significantly influenced by context matching, especially when sentences originate from the same dataset. As context length increases, this effect intensifies, suggesting models rely heavily on latent syntactic and semantic features shared across sentences.

Through perturbation analysis, we confirm that models are sensitive to structural changes while maintaining consistent responses to similar perturbations. These findings underscore the need for more comprehensive MPP evaluation methods that account for the abstract knowledge models acquire within larger context windows.</sample>
    <sample id="109">**Abstract:**

"Unnatural Instructions" presents a novel approach to collect and utilize diverse language task instructions without manual human labor. The paper addresses the challenge of expanding instruction tuning datasets beyond academic benchmarks to encompass a broader range of textual tasks.

The authors propose a fully automatic data generation process using a pre-trained GPT-3 variant. This process involves prompting the model with three examples from a baseline dataset (Super-Natural Instructions) and then asking it to generate a fourth example, followed by creating paraphrases. This method yields a large dataset of 240,000 instructions with corresponding inputs and outputs.

The generated instructions exhibit creativity, diversity, and correctness. Over 50% are accurate, and even incorrect examples offer valuable insights. The dataset includes unique tasks like evaluating scientific experiments and inventing new words, expanding the scope of instruction tuning.

The paper demonstrates the effectiveness of this approach by fine-tuning a 11B-parameter T5 model on Unnatural Instructions, which outperforms baseline models on various benchmarks. This method reduces the need for costly human annotation, leveraging language models' speed and cost-efficiency to create rich, diverse datasets for instruction tuning.

In summary, "Unnatural Instructions" offers a scalable, automatic solution to expand instruction tuning datasets, enabling language models to generalize better to unseen tasks.</sample>
    <sample id="111">The authors select a trigger set, which consists of words in a moderate frequency interval. They assume the provider can collect a general text corpus and count the word frequency to determine these moderate-frequency words.</sample>
    <sample id="114">**Abstract**

Our paper, "Finding the Pillars of Strength for Multi-Head Attention" presented at ACL 2023, tackles the limitations of large language models (LLMs) regarding parameter density and computational requirements. LLMs, while powerful, are heavy, requiring vast amounts of parameters, training time, and data.

We propose a novel approach called **Grouped Head Attention (GHT)** that compresses multi-head attention through a two-stage process. **Stage 1: Group-Constrained Training** divides attention heads into groups, encouraging intra-group similarity and inter-group diversity. **Stage 2: Voting-to-Stay Algorithm** prunes heads within each group, keeping only the most relevant one.

This method achieves significant parameter compression (up to 90%) while maintaining performance across various NLP tasks, including machine translation, language modeling, and abstractive summarization.

Our experiments demonstrate:

* **Improved performance:** GHT and its refined version, GHT-PS, outperform state-of-the-art baselines by 3.8% to 7% BLEU in machine translation, 2.8% to 2.9% in language modeling, and 6.7% to 7% in abstractive summarization.
* **Substantial compression:** GHT-PS compresses 32.1% of parameters while maintaining comparable performance.
* **Enhanced efficiency:** Our LITE model achieves 90% parameter pruning, 62% faster inference speed, and 80% reduction in FLOPs compared to a model with equivalent performance.

We believe our work paves the way for more efficient and task-specific LLMs, mirroring the principle of "uninstalling unused apps" on devices.</sample>
    <sample id="115">The approach uses a speech segment size defined by "lambda speech frames." The specific threshold for emitting words based on attention weights is determined by a parameter "alpha."</sample>
    <sample id="116">The entity-specific knowledge needed in the example is: "Servin is a judge."</sample>
    <sample id="117">Based on the content provided, the most important factor between the example quality and the similarity to the source sentence is **example quality**. The paper states that "it's the examples that carry most of the weight" and that selecting examples from high-quality translations is crucial.</sample>
    <sample id="118">**Abstract:**

This paper presents a novel approach to improve multilingual pre-training for code-switched Natural Language Processing (NLP). Code-switching, common in linguistically diverse regions like India, poses challenges for existing models like mBERT and XLM-R. We introduce SwitchMLM, a Multi-Language Masking (MLM) technique tailored for code-switching. SwitchMLM identifies and masks *switch-points*, or language transitions, in code-switched sentences, enhancing language awareness.

To overcome the need for labeled data, we propose FrequencyMLM, a surrogate method using word frequency to assign switch-point tags. We also introduce architectural modifications to BERT, focusing on residual connections and an auxiliary Language Identification (LID)-based loss. This encourages intermediate layers to encode more switch-point information, which is then amplified in the final layer.

Our experiments on sentiment analysis demonstrate superior performance with the combined approach of Switch/FrequencyMLM, ResBERT, and the auxiliary loss. Probing experiments confirm that our methods effectively increase switch-point information in both intermediate and final layers. By addressing the unique challenges of code-switching, this work advances multilingual NLP for diverse linguistic contexts.</sample>
    <sample id="119">The paper focuses on several language models in its extended experiments, including:

1. GPT-4
2. GPT series (specific models within the GPT family)
3. BART series and its variants
4. RoBERTa

These models are evaluated for their political leanings and the impact of different training data on their biases.</sample>
    <sample id="120">The model leverages the **cross-attention mechanism between the audio input and textual output**, which combines scores from both the encoder and decoder layers. This combined attention is key to its strategy, EDAtt.</sample>
    <sample id="121">Based on the content provided, direct inference examples are explicit references to the entity, such as:

- Saying the name of the song directly, e.g., "Easy on Me."
- Providing the position, e.g., "the first one."

These are straightforward ways to select an entity without using indirect language.</sample>
    <sample id="122">The authors of the paper are from Fudan University.</sample>
    <sample id="123">**Abstract**

This presentation introduces **MultiInstruct**, the first large-scale multi-modal instruction tuning dataset, designed to improve zero-shot learning capabilities of pre-trained models across diverse computer vision and language tasks. Existing works primarily focus on language-only instruction tuning, leaving multi-modal tasks underserved.

The authors address this gap by:

1. **Creating MultiInstruct**: Compiling 62 multi-modal tasks from 21 open-source datasets, each with five expert-written instructions, covering 10 broad categories.

2. **Utilizing OFA**: Employing OFA, a unified multi-modal pre-trained model, as the base architecture, handling text, images, instructions, and bounding boxes in a unified token space.

3. **Training and Evaluation**: Training on 53 tasks from 9 groups, testing on reserved and selected tasks, and evaluating performance using accuracy (for classification) and ROUGE-L (for generation). A new metric, **sensitivity**, measures consistency in outputs despite instruction wording variations.

Key findings demonstrate that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks and reduces sensitivity to instruction wording. Transfer learning from natural language instruction datasets further improves performance and sensitivity.

The presentation concludes with plans to expand MultiInstruct to 150 additional vision-language tasks and encourages further exploration of instruction tuning for multi-modal learning.</sample>
    <sample id="124">**Abstract**

This work, "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models" by Tan Qingyu from the National University of Singapore and Alibaba, addresses the under-researched aspect of temporal reasoning in large language models (LLMs). It categorizes temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. The study identifies a bias in prior works that predominantly focus on Level 2 (time-to-event) reasoning.

To comprehensively evaluate and improve LLMs' temporal understanding, the authors introduce TempReason, a dataset covering all three levels with extensive temporal coverage. They propose three QA settings: Closed Book, Open Book, and a novel Reasoning QA, where temporal knowledge is provided.

The study introduces TempT5, a model enhanced through two components: Temporal Span Extraction pre-training and time-sensitive reinforcement learning. These techniques aim to improve LLM performance in temporal span reconstruction and penalize temporally incorrect predictions.

Experiments on TempReason show significant improvements over baseline models like ChatGPT and FLAN-T5-L, particularly in Level 1 (time-to-time) and Reasoning QA. However, even TempT5 exhibits fluctuations in performance across different time periods, suggesting potential data imbalance issues.

In conclusion, the paper highlights biases in LLM temporal reasoning, introduces a comprehensive benchmark, and proposes a training paradigm to advance the field's understanding and capabilities in this critical aspect of language modeling.</sample>
    <sample id="125">Based on the content provided, it appears that the presentation is given by Yanis Labrak, but there is no explicit mention of the number of authors involved in the paper. However, considering the detailed description of the research and the various models compared, it is likely that the paper has multiple authors. A safe estimate would be at least 3-5 authors, given the complexity of the work.</sample>
    <sample id="126">Yes, using a machine translation (MT) API (like Google Translate) to translate the source natural language query to a target language before applying a semantic parsing model is considered a baseline approach in the presented work. This is part of the "Translate-Test" setting described in the presentation.</sample>
    <sample id="127">**Abstract**

"Large Language Models Are Reasoning Teachers" presents a novel approach to enable smaller language models to perform complex, multi-step reasoning tasks, traditionally requiring massive models like GPT-3 or PALM. The study introduces a method that leverages chain-of-thought prompting on large models to generate step-by-step solutions, which are then used to fine-tune smaller models.

Key contributions include:

1. **Distillation Technique**: Using large models as teachers to transfer reasoning abilities to much smaller models, below 1 billion parameters.
2. **Diverse Reasoning**: A novel technique that generates multiple diverse reasoning samples from the teacher model, enhancing student training.
3. **Performance**: The method significantly outperforms baselines, achieving notable results on 12 tasks, including text-based data understanding and arithmetic.
4. **Scalability and Trade-offs**: Demonstrating the scalability of the approach while highlighting trade-offs between development and inference costs, model size, and dataset size.

The paper provides extensive details, code, and data, encouraging future work and discussions. It concludes that this method offers an accessible, effective, and scalable way to transfer reasoning abilities from large to small models, potentially applicable to other emergent abilities.</sample>
    <sample id="128">**Abstract:**

This paper presents "The KITMUS Test," a diagnostic suite designed to evaluate natural language understanding (NLU) models' ability to integrate knowledge from multiple sources—both pretraining and inference time. Coreference resolution, specifically, is used as a benchmark to probe this integration. The authors introduce three settings: "Background-Pretrain" (background knowledge at pretraining), "Background-Both" (available at both times), and "Background-Inference" (only at inference, simulating new knowledge).

The KITMUS dataset includes scenarios where pronouns need to be resolved using entity-specific and background knowledge. Experiments show that without task-specific training, established coreference resolution models struggle, even in the "Background-Pretrain" setting. Models tend to rely on surface cues, which are less effective in the more challenging settings. Even with fictional knowledge, the best models struggle to integrate backward knowledge presented at inference time.

The study highlights the limitations of current NLU models in reasoning with dynamic, context-dependent knowledge. It underscores the need for task-specific training to enhance models' ability to integrate knowledge from diverse sources, especially in real-world NLU tasks where background knowledge might not be captured during pretraining. The paper offers the KITMUS dataset and code for further research and evaluation.</sample>
    <sample id="129">The authors used the example of "black women" as a marked group in their study. They analyzed the generated personas and found that words like "strong," "resilient," and others reflect essentializing narratives and stereotypes associated with this demographic.</sample>
    <sample id="130">Based on the presentation, the text suggests that **models based on older architectures, specifically those used in CoNLL-2003**, may not generalize well to modern data. The presentation highlights that **transformer models** generally perform better in terms of generalization.</sample>
    <sample id="131">The text does not mention any specific names of testing datasets. It only refers to them as "clean test sets" and "clean validation samples."</sample>
    <sample id="132">The paper involves two authors: Akshatha and Martin.</sample>
    <sample id="133">The author works with **multiple modalities**. The research specifically focuses on **multi-modal** pre-trained models and tasks involving **language, images, instructions, and bounding boxes**.</sample>
    <sample id="135">**Abstract**

This paper introduces ABC-Eval, a novel dimensional approach to evaluate conversational AI models, developed by the Emory NLP Lab in collaboration with Amazon Alexa AI. Traditional human evaluations, while effective, are subjective and overlook the multifaceted nature of dialogue quality. ABC-Eval aims to enhance precision and reliability by annotating specific behaviors in conversations, such as irrelevance, contradictions, and hallucinations.

The method was tested on four state-of-the-art chat models, evaluating 100 human-bot conversations each. Comparisons were made with three existing evaluation methods: turn-level and dialogue-level Likert ratings, and pairwise comparisons. ABC-Eval's behavior labels demonstrated higher inter-annotator agreement and better prediction of overall conversation quality.

Analysis revealed that ABC-Eval metrics capture unique aspects of chat quality, explaining over 25% of conversation quality in a stepwise linear regression. Common issues like common sense violations, irrelevant information, and contradictions remain prevalent (around 20%, 15%, and 10% respectively), highlighting the need for precise evaluation.

ABC-Eval offers a more granular and reliable method for comparing conversational AI models. The authors hope it will advance the field, enabling more accurate progress tracking as AI continues to evolve.</sample>
    <sample id="136">**Abstract:**

This presentation explores the challenges and limitations of numerical reasoning in large language models, focusing on the lack of representative benchmarks and the need for more informative evaluation metrics. The research, titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning," was conducted by Jasivan and supervisor Nafise at the University of Sheffield.

FERMAT, a flexible evaluation set, is introduced to address these issues. It comprises math worded questions from Illinois and CommonCore, with varying number representations (integers, decimals) and mathematical operations (single vs. combined). The baseline evaluation reveals poor performance across models, highlighting the unrepresentativeness of existing benchmarks.

Fine-tuning models with 200,000 generated questions improves performance, especially with diverse language and mathematical content from sources like GSM8K and AQUA. The study emphasizes the importance of language and mathematical diversity in enhancing model capabilities.

Key findings include:

- Existing benchmarks are insufficient for assessing numerical reasoning.
- Single accuracy scores are not informative about models' strengths and weaknesses.
- Language and mathematical diversity, along with number encoding and tokenization, are areas for improvement.

FERMAT aims to fill these gaps, providing a more comprehensive framework for evaluating numerical reasoning in language models.</sample>
    <sample id="137">## Tell2Design: Enabling Language-Guided Floor Plan Generation

This paper introduces **Tell2Design**, a novel dataset and framework for generating floor plans from natural language instructions. Traditional generative AI focuses on creating realistic images from text descriptions, often for artistic purposes. However, real-world design applications, like architectural floor plan creation, require adhering to specific user requirements and constraints.

Tell2Design tackles this challenge by defining a new task: generating structured 2D floor plans directly from text instructions. These instructions detail room semantics (type and function), geometry (shape and size), and topology (room connections). The dataset comprises 5,051 human-annotated and 76,000 artificially generated instructions paired with corresponding floor plans.

The authors propose a sequence-to-sequence model, leveraging the transformer architecture and pre-trained language model T5. This model excels in understanding complex, unstructured text and generating target bounding box sequences for rooms, regardless of instruction length.

Experiments demonstrate the model's superior performance compared to text-conditional image generation baselines, achieving high Intersection over Union (IoU) scores. Even when trained on artificial instructions, the model benefits from subsequent fine-tuning on human-written instructions, highlighting the value of diverse training data.

Tell2Design paves the way for future research in language-guided design generation, offering a valuable resource for developing AI tools that can translate user vision into tangible designs through natural language interaction.</sample>
    <sample id="138">The authors claim that the integration of knowledge from **both pretraining and inference-time sources** in Natural Language Understanding (NLU) models is an understudied area. They highlight that while models can use pretrained knowledge, successfully integrating instance-specific knowledge supplied at inference time remains challenging.</sample>
    <sample id="139">The speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks by crowd-sourced workers to find and revise incorrect samples.</sample>
    <sample id="141">The limits of existing resources for context-dependent translation include:

1. **Limited Scope**: Most resources rely on domain knowledge and human curation, supporting only limited types of context-dependent translations and a narrow set of languages.
2. **Inability to Capture All Context Cases**: Corpus-level metrics like BLEU cannot capture subtle context-dependent translations, as only a small portion of translations depend on context.
3. **Insufficient Evaluation Methods**: While some targeted evaluations exist, they are not comprehensive and may not represent the full range of context-dependent scenarios.</sample>
    <sample id="143">The approach, EDAtt, is compared to:

1. **Wait-k strategy**
2. **Local Agreement**
3. **State-of-the-art architecture specifically tailored for simultaneous pre-translation**</sample>
    <sample id="144">The authors of the paper "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" are affiliated with the following institutions:

- Unspecified French research/academic institutions (based on the use of NACHOS data and the mention of Nantes University Hospital)
- Hugging Face (mentioned as where all pre-trained models are available)
- GitHub (mentioned as hosting training scripts)

While specific affiliations are not explicitly stated in the text provided, these institutions are key contributors to the research and dissemination of the DrBERT model.</sample>
    <sample id="145">The speaker's name is Jenny.</sample>
    <sample id="146">**Title: Analyzing and Addressing Omission in Dialogue Summarization**

This presentation explores the critical issue of omission in dialogue summarization, a subfield of text summarization focusing on concise representations of dialogue content. While recent advances using large-scale pretrained language models have improved dialogue summarization, errors, particularly factual inaccuracies and omissions, remain prevalent.

The study reveals a significant omission problem, with approximately 70% of generated summaries lacking critical information. Omission patterns are found to be random across dialogue positions, highlighting the challenge of identifying key information. To tackle this, the authors introduce the OLDS dataset, meticulously constructed from five existing benchmarks, offering high-quality omission labels.

The work proposes an omission detection task, defining it as identifying missing utterances in candidate summaries compared to gold references. Three baseline frameworks—pair-wise classification, sequence labeling, and pointer networks—are evaluated, achieving around 50% F1-score, indicating the task's complexity.

Furthermore, the presentation demonstrates the potential of using detected omissions for summary refinement, showing substantial performance improvements. These findings underscore the importance of addressing omission for enhancing the quality and reliability of dialogue summarization systems.</sample>
    <sample id="147">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" has 3 authors: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes, the dataset mentioned in the presentation, CoNLL++, is a collected and annotated version of Reuters News data from 2020, making it publicly available for research purposes.</sample>
    <sample id="150">**Abstract:**

"MEETINGQA: Extractive Question-Answering on Meeting Transcripts" presents a novel dataset and a comprehensive study of question-answering (QA) in real-life meeting scenarios. Meeting transcripts, often long and domain-specific, pose a unique challenge in Natural Language Processing (NLP), with prior works focusing primarily on summarization and action item extraction.

The paper introduces MeetingQA, a dataset containing 7,700 questions and their corresponding answer spans from nearly 100 hours of manually transcribed multi-party meetings. Questions are characterized by their length, open-ended nature, and intent to elicit detailed discussions. The dataset includes various answer scenarios, such as multi-speaker contributions and discontinuous answer spans.

The authors demonstrate the challenges posed by MeetingQA, achieving a human performance F1 score of 84.6 on the test set. They employ diverse models, including short-context (e.g., RoBERTa) and long-context (e.g., Longformer) models, as well as single-span and multi-span variants. Results show a significant gap between fine-tuned models and human performance, with short-context models outperforming long-context ones.

MeetingQA also explores zero-shot performance, highlighting the dataset's difficulty. Silver data augmentation improves zero-shot results, with larger models like FLAN-T5 achieving performance comparable to fine-tuned models. Error analysis reveals weaknesses in identifying rhetorical questions, irrelevant sentences, and speaker attribution, particularly in the zero-shot setting.

Overall, MeetingQA offers a valuable resource for advancing QA in meeting transcripts, presenting both opportunities and challenges for future NLP research.</sample>
    <sample id="152">**Abstract**

This presentation introduces innovative language models tailored for classical philology, focusing on Ancient Greek and Latin texts. The project aims to advance the field by addressing the current limitations of existing models, which are predominantly monolingual BERT models with uncharted performance.

Two monolingual models, GreBERTa (based on RoBERTa) and GreTa (based on T5 architecture), were developed for Ancient Greek. To expand capabilities, multilingual models PhilBERTa and PhilTa were created, pre-trained on Ancient Greek, Latin, and English.

Data gathering involved leveraging Open Greek &amp; Latin, along with a novel corpus from the Internet Archive, manually identifying Greek texts through OCR errors. Additional resources for Latin and English were utilized for multilingual models.

Rigorous benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization demonstrated superior performance by the new models over the state-of-the-art. Interestingly, the T5 encoder in GreTa initially underperformed but converged to match native encoder-only models with further training.

Lemmatization, a key strength of encoder-decoder models, showed a 5% improvement over existing methods for Ancient Greek. Multilingual models did not significantly outperform monolinguals in semantic and world knowledge tasks.

In conclusion, these models represent significant progress, offering native tokenization, diverse architectures, and a high-quality pre-training dataset for Ancient Greek. The paper provides comprehensive details.</sample>
    <sample id="153">**Abstract:**

This presentation explores the challenges posed by ambiguities in text-to-image generative models, where prompts can be interpreted in multiple ways, hindering the creation of accurate images. The authors propose a comprehensive approach to address this issue, focusing on prompt disambiguation and image evaluation.

They introduce a benchmark dataset, derived from LAVA, that categorizes various ambiguity types. Their framework employs a language model to generate clarifying questions or alternative visual interpretations, which are then refined through user interaction. This process leads to disambiguated prompts.

The study then evaluates the effectiveness of this disambiguation by generating images using text-to-image models and assessing their fidelity to user intent using a Visual Question Answering (VQA) model. The findings reveal significant improvements in faithful image generation, highlighting the success of the proposed methods.

Key contributions include: (1) identifying and classifying ambiguities in text-to-image prompts, (2) developing a prompt disambiguation framework, (3) creating an automatic evaluation system to assess image-user intent alignment, and (4) demonstrating the reliability of the proposed evaluation framework through human evaluation.

By addressing these ambiguities, the work aims to enhance the performance and reliability of text-to-image models, ensuring generated images better reflect user expectations.</sample>
    <sample id="154">The authors of the "Attention as a Guide for Simultaneous Speech Translation" paper are from two affiliations: the University of Trento and Foundazione Bruno Kessler, both located in Italy.</sample>
    <sample id="155">The name of the speaker mentioned in the text is **Javad Hosseini**.</sample>
    <sample id="157">**Abstract**

This presentation introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS), a novel approach to distilling key information from multi-participant dialogues. Traditional methods rely on pre-computed static graph structures, often prone to errors from external linguistic tools. SDDS addresses these limitations by proposing a four-component model:

1. **Utterance Encoding**: Converts dialogue utterances into vector representations.
2. **Static-Dynamic Graph Module**: Combines multiple static graphs with a dynamic graph capturing semantic relationships between utterances based on their vector representations.
3. **Summary Generator**: Uses a pre-trained language model to fuse static and dynamic dialogue structures into a concise summary.

SDDS innovates with four heuristic dialogue structure modeling methods, including Discourse Parsing Graph and Speaker Interaction Frequency Matrix, to build static graphs. A Dynamic Graph module employs multi-head attention for semantic relationship discovery. Cross-graph fusion and a dual cross-attention mechanism integrate static and dynamic information for accurate summarization.

The model's code and data are publicly available on GitHub, encouraging further exploration and development in dialogue summarization.</sample>
    <sample id="158">**Abstract:**

The talk introduces "Dual Cache for Long Document Neural Coreference Resolution," addressing the challenge of identifying entity references in lengthy texts. Coreference resolution aims to link multiple mentions of an entity within a document. Traditional methods face quadratic complexity issues, while recent cache-based approaches, though efficient, struggle with cache misses in long documents due to scattered mentions.

The proposed dual cache system comprises a local cache and a global cache. The local cache, employing LRU (Least Recently Used) eviction, stores frequently mentioned entities. In contrast, the global cache uses LFU (Least Frequently Used) to manage less frequent entities. The model scans the document sequentially, classifying mentions and deciding between local or global cache storage. Qualified entities are added to the appropriate cache.

Evaluations on public benchmarks and a 30,000-word book demonstrate dual cache's superior performance over single-cache methods, significantly reducing cache misses. Even without training data, dual cache outperforms unbounded memory baselines. The approach balances efficiency and performance, offering the highest cost-effectiveness among cache-based models.

In summary, the dual cache strategy enhances coreference resolution in long documents by differentiating local and global entity mentions, leading to improved efficiency and reduced cache-related overhead.</sample>
    <sample id="160">In the first step, the method maps the input tokens to unordered multisets of tokens that will appear in the output.</sample>
    <sample id="161">55,000 scripts are represented in CoScript.</sample>
    <sample id="163">Based on the presentation, the best automatic alignment method for DEPLAIN (a German text simplification corpus) is MASSalign, as concluded in the authors' experiments.</sample>
    <sample id="164">The benefit of weakly supervised learning (WSL) is that it allows training neural networks using cheaper and noisier data (weakly labeled data) compared to manually annotated data, potentially leading to cost savings. However, it has been found that WSL methods still require clean validation data to achieve high performance and that the performance gains claimed in previous studies may be overestimated.</sample>
    <sample id="165">**Abstract**

This paper introduces **LiPoR (Likelihood Learning with Posterior Regularization)**, a novel unsupervised learning method for abductive commonsense reasoning. Abductive reasoning aims to identify the most plausible explanation for an outcome given a context, bridging information gaps. Traditional approaches rely on supervised learning, requiring manually annotated plausible explanations, which are subjective and noisy.

LiPoR tackles this challenge by framing explanations as latent variables and maximizing the marginal likelihood of the outcome given the context, marginalizing over all possible explanations. To prefer plausible explanations, LiPoR incorporates a regularizer that leverages mutual exclusivity—a key characteristic of explanations. This regularizer encourages explanations to be mutually exclusive, ensuring that only a subset aligns with the context.

The paper demonstrates LiPoR's effectiveness on AlphaNLI, the leading abductive reasoning dataset. LiPoR outperforms previous unsupervised methods and even strong zero-shot baselines like GPT-3 by over 4 accuracy points, showcasing the power of unsupervised learning in abductive reasoning.

By eliminating the need for labeled explanations, LiPoR opens new possibilities for training models on unannotated data, advancing the field of commonsense reasoning. The paper is available at tinyurl.com/zhao-lipor.</sample>
    <sample id="166">**Abstract**

This paper introduces a novel neural framework, NDCR (Neural Divide-and-Conquer Reasoning), for image retrieval from linguistically complex text. The challenge lies in retrieving similar images from lengthy, intricate textual descriptions. Existing visual language models, while powerful for simple tasks, struggle with complex reasoning.

NDCR draws inspiration from Divide-and-Conquer strategies and Dual-Process Theory. It comprises three key components:

1. **Proposition Generator**: Decomposes complex text into simple proposition representations.
2. **Visual-Linguistic Interactor (System 1)**: Performs visual-proposition interaction, generating matching scores and reasoning states.
3. **Neural-Symbolic Reasoner (System 2)**: Integrates reasoning states and simple proposition results to arrive at final solutions through negation execution and conjunction operations.

Experiments demonstrate NDCR's superior performance compared to baselines. Ablation studies validate the effectiveness of each module. The framework showcases interoperable processing, presenting inference states and results at each step.

The paper concludes by suggesting potential benefits of neural-symbolic calculation for enhancing large language models' compositional reasoning and planning. Integrating Divide-and-Conquer with Dual-Process Theory offers a promising path for tackling complex reasoning problems in AI.</sample>
    <sample id="167">The allocation was 50% manual and 50% automatic alignment methods. This is specified by the statement: "In DEPLAIN-web, this corpus includes different domains and we also align all of these 750 documents, on the one hand manually and on the other hand with automatic alignment methods."</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting and annotating Reuters News articles from 2020 using the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="169">**Abstract:**

This paper, "Prompting PaLM for Translation: Assessing Strategies and Performance," investigates the effectiveness of prompting large language models (LLMs) for machine translation. The study focuses on PaLM, a 540-billion parameter model trained on 780 billion tokens, which has shown state-of-the-art performance in numerous NLP tasks.

The authors conducted a systematic evaluation using best practices from the MT community, including the latest test sets and state-of-the-art neural MT metrics, complemented by human expert assessment. They explored various prompting strategies, concluding that 5-shot prompting, where each sentence is marked with its language, offers the best balance.

Key findings include:

- Prompting significantly impacts LLM performance, with even simple changes affecting BLEURT scores by up to 40 points.
- Example quality is paramount, outweighing the form of the prompt for short promptings.
- PaLM's fluency rivals commercial systems, but accuracy suffers, with common errors involving omission.
- Specialized MT systems still outperform PaLM, but PaLM's translations are close to commercial quality.

The study recommends selecting high-quality translation examples for prompting, emphasizing the importance of content over formatting.</sample>
    <sample id="171">The existing works on protecting the copyright of large language models embedded as services include:

1. **Watermarking Methods**: These methods embed a watermark in the provider service to detect if another service contains the watermark. However, many struggle with applicability to embedding as services, lack of transferability, or impact on embedding utility.

2. **Classification**: Existing approaches can be broadly categorized into four types, but each has limitations as mentioned above.

The paper being discussed, "Embedding Marker," addresses these issues by proposing a backdoor-based watermark method specifically applicable to embedding as services.</sample>
    <sample id="172">Based on the presented content, multilingual LLMs like Codex and Bloom are **inadequate** for cross-lingual semantic parsing (CLSP) tasks. The research found that while they outperform previous work or achieve comparable results, they still do not meet the requirements of this complex task.</sample>
    <sample id="174">**Abstract:**

Thea, co-author of "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis," highlights the dataset's uniqueness and advantages over existing resources. The paper introduces ArgAnalysis35K, containing 35,000 argument-analysis pairs, sourced from high-quality tournaments, expert and intermediate debaters, and novice participants.

Key distinctions include:

- **Dataset Size and Quality:** Far larger than previous datasets, with arguments sourced from diverse, credible sources, ensuring high quality.

- **Thematic Diversity:** Instead of focusing on specific motions, 24 themes are selected based on expert advice and debate circuit experience, capturing a broader range of arguments relevant to parliamentary debates.

- **Analysis Incorporation:** ArgAnalysis35K introduces the concept of 'analysis'—a combination of claims, premises, or statistics—to provide deeper context and explanation for arguments, enhancing understanding.

- **Instance-Based Annotator Reliability:** Recognizing human biases, the dataset employs an instance-based approach, eliminating only biased judgments while utilizing the rest, thereby improving overall annotation reliability.

- **Relevance Modeling:** Arguments are scored for relevance to specific themes, allowing users to filter and select arguments based on their intended topic, enhancing the dataset's utility for various analysis purposes.

The authors emphasize the dataset's potential for advancing argument quality analysis research, encouraging readers to explore the paper and provide feedback.</sample>
    <sample id="175">The method deals with ambiguity of permutations through a continuous relaxation approach, which approximates the NP-hard "Traveling Salesman" problem and allows backpropagation to learn linguistically more plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP (Natural Language Processing) model is defined by its ability to perform equally well across different social categories, regardless of their political affiliations or demographics. This means the model should not exhibit biases that result in better performance for certain groups (e.g., minority groups or specific political viewpoints) at the expense of others, particularly in tasks like hate speech detection and fake news identification.</sample>
    <sample id="177">The speaker's name is Yanis Labrak.</sample>
    <sample id="178">The speaker's name is Koustav Sinha.</sample>
    <sample id="179">**Abstract:**

This paper presents **SymbolicToM**, a novel method designed to enhance Theory of Mind (ToM) reasoning in large language models (LLMs). ToM refers to the ability to understand and predict the mental states of others, crucial for complex communication and comprehension.

Traditional methods measure ToM through false-belief tasks, where models are quizzed about characters' beliefs in relation to an object's location. Existing LLMs, including ChatGPT and GPT-3, struggle with these tasks.

SymbolicToM addresses this by employing explicit graphical representations of mental states. It generates belief graphs for each character in a story, capturing their knowledge about the world state. These graphs are computed at inference time using off-the-shelf Natural Language Inference (NLI) and OpenIE models.

The method demonstrates significant performance improvements on ToM tasks for various LLMs, including GPT-3 and GPT-4, compared to supervised baselines. It achieves this through:

* **Efficient querying:**  SymbolicToM retrieves relevant belief graphs and recursively answers questions based on these graphs.
* **Generalization:**  It successfully generalizes to new story structures and linguistic variations, showcasing robust understanding beyond the training data.

Overall, SymbolicToM offers a promising plug-and-play solution for enhancing ToM in LLMs, leading to more nuanced and accurate language understanding.</sample>
    <sample id="180">The speaker's name is Myra.</sample>
    <sample id="181">**Abstract: Distilling Script Knowledge for Constrained Language Planning**

This paper tackles the challenge of constrained language planning, where models must generate step-by-step instructions tailored to specific goals with diverse constraints, such as "make a chocolate cake." Existing approaches focus on abstract goals of stereotypical activities, leaving specific goal planning under-researched.

We introduce the *Constrained Language Planning* (CLP) problem and demonstrate that current large language models (LLMs) struggle with it. Through human-in-the-loop data acquisition, we create a dataset of 100 specific goals with corresponding scripts. Analysis reveals that LLMs fail due to a lack of faithfulness to constraints.

To address this, we propose an *over-generate-then-filter* method. This involves InstructGPT generating multiple scripts per goal, followed by a filter model selecting the most faithful script based on semantic similarity and constraint keywords. This improves script quality.

Recognizing the cost and scarcity of large models, we further develop *CoScript*, a dataset of 55,000 specific goals and scripts distilled from large models. CoScript enables training smaller, specialized models for CLP, demonstrating their potential to outperform larger models with suitable training data.

Our work establishes the CLP problem, offers a solution for improving LLM performance, and provides CoScript as a valuable resource for advancing research in language planning with specific constraints.</sample>
    <sample id="182">In the context of the paper, tropicalism refers to a trope that associates Latina women with vibrant, curvaceous features, often reinforcing stereotypes and "othering" based on their relationship to a perceived exotic or tropical identity.</sample>
    <sample id="183">The authors relied on a study where prompts were given to human subjects to write portrayals of different target groups, enabling direct comparison with the AI-generated personas.</sample>
    <sample id="184">The work used **CXMI (Context Usage Measure for Machine Translation)** and its extension **Pointwise CXMI** to measure context usage by machine translation models. Specifically, they analyzed words with high P-CXMI (Pointwise CXMI) to identify patterns and determine when translation requires context.</sample>
    <sample id="185">The main difference between DrBERT and ChuBERT is their data sources and training methods:

- **DrBERT** is based on **NACHOS**, a large dataset of crawled medical web data in French. It is trained from scratch using a 7GB or 4GB subset of NACHOS.

- **ChuBERT** is based on **anonymized clinical data** from the Nantes University Hospital data warehouse (4GB of sentences from clinical notes). It is also trained from scratch and compared against a version that includes a mix of NACHOS and clinical data.

In summary, DrBERT leverages a broader range of medical web data, while ChuBERT focuses on anonymized clinical data.</sample>
    <sample id="187">Two authors are involved in the paper: Ying and Zhiyang.</sample>
    <sample id="188">Iterative transfer learning involves sequentially fine-tuning a model on multiple related tasks. Starting with weights transferred from a closely related task (like debate or CE classification), the model is further refined by additional fine-tuning on the specific dissonance detection task after each round of active learning, leading to improved performance.</sample>
    <sample id="189">The goal of the AltEntities Corpus dataset is to help language models understand and resolve indirect referring expressions when users want to make a choice between entities, such as songs, books, or recipes. It aims to benchmark Large Language Models' (LLMs) entity understanding in conversational settings.</sample>
    <sample id="190">An attacker can extract model parameters through an Embedding as a Service (EaaS) by learning from the embeddings provided by the service. They can then replicate similar services using these extracted parameters, posing a copyright infringement risk to the original model developers.</sample>
    <sample id="191">The paper has 3 authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">## **CAME: Efficient Large Language Model Training with Adaptive Memory Optimization**

This presentation introduces CAME, a novel optimizer designed to balance fast convergence and low memory usage in large language model training. Existing methods like Adam struggle with high memory consumption due to storing moment estimates for every parameter. While memory-efficient optimizers like Adafactor reduce memory usage, they often sacrifice training speed due to errors in their update calculations.

CAME leverages non-negative matrix factorization (NMF) to significantly reduce memory requirements, achieving a complexity of O(m+n) instead of O(mn). Building on Adafactor's analytic solution for NMF, CAME addresses its convergence issues by introducing a **confidence-guided** approach. This approach dynamically adjusts update steps based on the residual between predicted and generated updates, mitigating errors and preserving stability.

Extensive experiments on BookCorpus, English Wikipedia, BERT, GPT-2, and T5 demonstrate CAME's superior performance compared to Adam and Adafactor. CAME achieves higher validation accuracy while using significantly less memory, even with large batch sizes (up to 32K). Additionally, CAME maintains competitive end-task performance on downstream tasks for BERT-based models.

In summary, CAME offers a promising solution for training large language models with limited memory resources, paving the way for more efficient and accessible language model development.</sample>
    <sample id="193">The presentation does not specify the exact number of annotators used to create the initial dataset. However, it mentions that around 1,000 examples of discourse unit pairs were collected through annotation.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">**Abstract:**

This work introduces *Reasoning over Hierarchical Question Decomposition Tree (RoHT)*, a novel framework for Explainable Question Answering (XQA) that addresses limitations in existing neuro-symbolic and decompose-based methods. RoHT tackles the challenges of granular question decomposition and finding optimal solutions in complex XQA tasks.

The framework consists of two stages. First, it builds a Hierarchical Question Decomposition Tree (HQDT) to represent the hierarchical structure of complex questions, breaking them down into sub-questions and atomic queries. Second, RoHT performs probabilistic reasoning over the HQDT, fusing knowledge from knowledge bases (KBs) and text corpora at different levels. This process involves generating leaf questions, creating intermediate questions, computing certainty scores, and recursively solving the tree from the root to the leaves.

RoHT is evaluated on two complex QA datasets, KQA Pro and Musique. The results demonstrate its effectiveness in improving answer recall and accuracy compared to existing methods. Even with incomplete KBs, RoHT outperforms traditional KB-based methods by leveraging answers from sub-questions. Integrating text corpora further enhances performance, showing the value of combining KB and text knowledge. The study highlights the promise of RoHT in advancing the state-of-the-art in explainable and complex question answering.</sample>
    <sample id="196">The example where the governor (or subject) is on the left is "I saw Bart and Lisa."</sample>
    <sample id="197">The state-of-the-art models in dialogue systems, as mentioned in the text, are four advanced chat models that were evaluated using ABC-Eval and other methods. These models are at the forefront of conversational AI technology.</sample>
    <sample id="198">We need to evaluate the models' acceptability throughout the context window because modern large language models have increasingly larger context windows, making it crucial to assess their acceptability judgments across longer sequences. The current minimal pair paradigm (MPP) pipeline is limited to short, single-sentence inputs, which may not fully capture the model's abstract knowledge and sensitivity to context.</sample>
    <sample id="199">Based on the presented content, training multilingual models (with a mix of various languages) did not universally cause a performance drop compared to monolingual English models. In fact, in most cases (eight out of nine datasets), multilingual training led to improved performance. However, there was a notable exception where English performance dropped in seven out of nine datasets. This phenomenon is referred to as the "Curse of Multilinguality."</sample>
    <sample id="200">No, the annotators **do not** necessarily know about the entities in advance. They are provided with background knowledge about the two entities, such as Google search links for songs, Wikipedia text for books and recipes, and images for recipes, to help them make a selection and generate indirect referring expressions.</sample>
    <sample id="201">The evaluation used both:

1. **Neural MT metrics** (such as BLEURT)
2. **Expert-based human evaluation** using the MQM framework.</sample>
    <sample id="202">Based on the presentation, the regression in generalization impacts all NER types uniformly. The study found that the main cause of poor generalization (performance drop) is temporal drift, not adaptive overfitting, and this affects all models regardless of the specific NER types they are designed for. The key factors for better generalization are model architecture, model size, and more fine-tuning examples, not specific to particular named entity types.</sample>
    <sample id="203">Positionality in NLP matters because it highlights how datasets and models can exhibit biases, reflecting the perspectives and experiences of their creators and the populations they are trained on. This can lead to systematic performance differences, disadvantaging certain groups. Understanding and addressing this positionality is crucial for creating fairer and more inclusive NLP technologies.</sample>
    <sample id="204">Based on the content provided, the multilingual LLMs like BLOOM mentioned in the presentation were **not** specifically mentioned as being fine-tuned with adapters or through full fine-tuning. The text focuses on the general inadequacy of these models for cross-lingual semantic parsing tasks without specifying the fine-tuning approach used.</sample>
    <sample id="205">**Abstract**

This presentation investigates the propagation of political biases from pretraining data to language models and their downstream applications, focusing on fairness issues. The study leverages large-scale web crawl data, which extensively includes political news media, to analyze the political leanings of prominent language models like GPT-4, GPT series, and BART series.

Preliminary results show that these models exhibit varying political biases, with GPT models leaning more to the left. Further experiments demonstrate that language models can absorb ideological shifts from partisan corpora, with notable changes observed after 2017, reflecting societal polarization.

Evaluating models with different political leanings on tasks like hate speech and fake news detection reveals performance disparities based on social categories. Left-leaning models perform better at detecting hate speech against minority groups, while right-leaning models excel at identifying hate speech targeting white men. Similar trends are observed in fake news detection.

The research highlights a pressing fairness issue, suggesting that deploying models with political biases could exacerbate existing societal divisions. It presents a dilemma: omitting political opinions from training data risks bias, while sanitization may lead to censorship or exclusion. The study calls for careful consideration and innovative solutions to mitigate these challenges.</sample>
    <sample id="206">They use a combination of two models for transfer learning:

1. **Topic Independent Dissonance Stance Classification (Debate task)**
2. **Binary Classification of Expansion and Comparison classes from PDTB (CE tasks)**

They find that fine-tuning the CE tasks followed by further fine-tuning on the debate task yields the best performance.</sample>
    <sample id="207">The recent test sets used to assess PaLM's capabilities in this study are the latest test sets from the machine translation (MT) community, ensuring no overlap with the model's training data. Specifically, they used the WMT evaluation data.</sample>
    <sample id="208">The authors proposed three recommendations for model owners.</sample>
    <sample id="209">The proposed method, specifically the "over-generate-then-filter" approach, significantly improves the planning ability of large language models. While the baseline models achieved an overall accuracy of around 50% (as shown in the table), the proposed method increases the quality of generated scripts, leading to a notable gain in both semantic completeness and faithfulness to constraints.

Quantitatively, the method allows InstructGPT to outperform the baseline models, achieving higher accuracy in planning for specific goals with constraints. However, the exact gain in accuracy is not explicitly stated in the summary, but the figure and the overall improvement suggest a substantial increase.</sample>
    <sample id="210">The speaker's name is Shuheng.</sample>
    <sample id="211">Yes, the results and dataset presented in the paper can be used as a benchmark for the problems of automatic text alignment and automatic text simplification in German, as the authors have proposed them as such.</sample>
    <sample id="212">The paper experiments with at least one smaller model, specifically T5 fine-tuned on CoScript, which outperforms most large language models. However, the exact number of smaller models tested is not explicitly stated in the summary.</sample>
    <sample id="213">The base model used for investigating multi-model instruction tuning is OFA (Unified Multi-Modal Pre-trained Model).</sample>
    <sample id="215">**Abstract**

This talk presents an argument for symmetric structures in coordination, contrasting with existing asymmetric theories. The core argument is based on the principle of dependency length minimization, which suggests that shorter dependencies are grammatically preferred.

The author examines English coordination structures, focusing on the position of conjuncts relative to the governing verb. They find that left conjuncts tend to be shorter, a phenomenon amplified when the governing verb is on the left or absent. This trend is evident in both sentence and syntactic structures.

By measuring dependency lengths in characters, syllables, and words, the study reveals a steady increase in the preference for shorter left conjuncts when the governor is on the left. This preference disappears when the governor is on the right. These findings support symmetric coordination structures, such as those proposed in the Prague approach and Hudson's Word Grammar, over asymmetric ones like Universal Dependencies and Mel'čuk's theory.

The paper draws on statistical analyses of the enhanced Penn Treebank and parsing data, confirming the observed trends. The conclusion advocates for a re-evaluation of asymmetric coordination theories in light of dependency length minimization principles. The author encourages further discussion at the poster session.</sample>
    <sample id="217">**Abstract**

This work, "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," tackles the challenge of generating diverse and controllable dialogues with multiple attributes. Existing methods either focus on single attributes or struggle with continuous attributes and limited controllability.

We introduce **DCG** (Disentangled Controllable Generation), a novel framework that leverages compositional prompts and disentanglement learning. DCG learns attribute concepts from seen values and disentangles different attribute combinations, enabling it to generate unseen attribute values.

Two key contributions are highlighted:

1. **Unified Evaluation Framework (MAE):** A reference-free metric for evaluating multi-attribute dialogue generation at various granularities, eliminating the need for large-scale labeled data.

2. **Prompt-based Approach:** DCG uses both attribute-oriented (instance-specific) and task-oriented (global) prompts, enhancing controllability and text quality.

Through experiments on two benchmarks, we demonstrate that DCG outperforms baselines in attribute controllability and text equality, even for unseen attribute combinations. Our evaluation framework and prompt design significantly advance the field of multi-attribute controllable dialogue generation.</sample>
    <sample id="218">The authors of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance" are affiliated with Google Translate, as stated by David Vilar during his review.</sample>
    <sample id="219">**Abstract:**

This work presents a compare-and-contrast multistage pipeline for uncovering financial signals in annual reports, specifically focusing on the U.S. Form 10-K filings. Motivated by the high textual similarity and yearly dependencies within these reports, the study introduces a highlighting task to identify and compare key words and phrases between consecutive years.

The pipeline comprises four stages. **Stage 0** (not discussed here) segments the documents. **Stage 1** classifies report pairs into three types: highly similar (β), syntactically similar but semantically different (revised), and contrasting (mismatched). **Stage 2 and 2+** involve out-of-domain and in-domain fine-tuning using an external dataset (eSNLI) and a custom dataset (FINAL). The fine-tuning incorporates soft labeling techniques and pseudo-positive labels to improve model performance.

The evaluation, using both FINAL and eSNLI datasets, measures performance with precision and correlation (PCC). The proposed model demonstrates superior performance on FINAL and maintains generalization to eSNLI. Notably, it benefits from mismatched pairs during simulation, even though they were not used during training.

The study contributes a highlighting task, the FINAL dataset, and a two-stage fine-tuning pipeline. Future work aims to enhance effectiveness and explore additional features and techniques from information retrieval to broaden the pipeline's applications in financial report analysis.</sample>
    <sample id="220">The authors of the paper are from Stony Brook University.</sample>
    <sample id="221">The paper analyzed translation between German into English.</sample>
    <sample id="222">**Abstract:**

This work, titled "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering," addresses the issue of transferring knowledge from a general-purpose domain (like Wikipedia) to specific domains, such as biomedical, for open-domain question answering (QA). The authors highlight challenges arising from domain mismatch, where a model trained on general content struggles to accurately answer questions in a new domain due to differences in language and knowledge.

To overcome this, they propose a systematic investigation of data interventions, categorized as zero-shot and few-shot methods. Few-shot techniques involve prompting large language models to generate domain-specific examples, improving retriever performance by 8% and reader performance by 11% on average. Zero-shot methods, lacking domain examples, control question, answer, and context distributions to understand their impact on model learning.

The authors also introduce a framework to classify dataset shift types (no shift, concept shift, covariate shift, full shift) based on the compatibility of retriever and reader models with target domains. They find that few-shot adaptations are effective for datasets with concept and covariate shifts, while zero-shot adaptations assist in no-shift scenarios.

Through extensive experimentation, the study demonstrates successful domain adaptation, achieving up to a 24% improvement in reader performance. Key insights include the effectiveness of specific data interventions depending on the type of dataset shift, and the importance of fine-tuning models for better generalization across domains.</sample>
    <sample id="223">The speaker's name is Shangbin.</sample>
    <sample id="224">The experiments investigated two models:

1. **long-mBART**: Fine-tuned for document-level text simplification.
2. **base mBART**: Fine-tuned for sentence-level text simplification.</sample>
    <sample id="225">For training, 53 tasks from 9 groups are used (53 tasks x 10,000 instances per task = 530,000 instances).

For testing, the entire common sense reasoning group is reserved, along with an additional 5 tasks from the VQ and Miscellaneous groups (totaling 6 tasks). All instances in the test split are used for each task.</sample>
    <sample id="226">Based on the content provided, two authors are mentioned: Regina Stodden and Omar. They are the ones presenting and discussing the DEPLAIN corpus and its use cases.</sample>
    <sample id="227">**Abstract**

This work addresses the challenge of grounded language understanding, where natural language expressions need to be mapped onto executable plans or programs in specific environments, such as smart assistants, semantic search, and robotic control. Current models struggle due to a lack of grounding during pre-training, leading to invalid or non-grammatical plans.

We propose **Pangu**, a novel framework that separates the generation of plans from their evaluation. Pangu leverages a symbolic agent to propose candidate plans and a language model to score and rank them, offloading the need for grammaticality and validity checks from the language model.

Our framework is demonstrated on knowledge-based question answering, achieving outstanding performance with models like BERT, T5, and Codex, through both fine-tuning and in-context learning. Pangu exhibits strong sample efficiency, requiring as little as one demo example to achieve over 50% accuracy on GRAIL queries.

Key findings include: (1) Autoregressive models overfit seen structures, while Pangu maintains consistent probability distributions for seen and unseen structures, indicating robustness; (2) Generation may not be optimal for grounded understanding; and (3) Discrimination, not generation, is a more effective strategy for language models in this domain.

Pangu offers a promising direction for future research in grounded language understanding.</sample>
    <sample id="228">The authors experimented with the following datasets:

1. AG News
2. MIND
3. SST2
4. Enron Spam</sample>
    <sample id="229">**Abstract**

This paper presents a collaborative study with Henning Wachsmuth on detecting improvable claims for argumentative writing support. Text revision, crucial for professional writing, significantly impacts the effectiveness of argumentative claims. The authors introduce two tasks: Suboptimal-Claim detection (identifying claims needing revision) and Claim Improvement Suggestion (suggesting types of quality issues to improve).

The work explores challenges in leveraging revision-based data from platforms like Kialo, where arguments and their revisions exhibit domain-specific goals and quality notions. Four key challenges are addressed:

1. **Representativity and Reliability**: Ensuring datasets accurately represent argument quality.
2. **Model Complexity and Architecture**: Selecting models sensitive to subtle revisions.
3. **Contextual Dependence**: Determining relevant contextual information for argument quality.
4. **Topical and User Bias**: Mitigating noise from controversial topics and user biases.

The study employs various models and architectures, analyzing their impact on claim assessment performance. Key findings include the effectiveness of revision-based data, the utility of modeling claim distance, and the context-dependent nature of argument quality. The paper offers a detailed analysis of strategies tackling each challenge, highlighting both strengths and weaknesses.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for pre-training the DrBERT biomedical model in French.</sample>
    <sample id="232">The speaker's name is David Vilar.</sample>
    <sample id="233">**Abstract:**

This paper presents EDAtt, a novel approach to simultaneous speech translation (SimulST) that leverages existing offline speech-to-text models without retraining. Unlike traditional SimulST models that require specialized architectures and lengthy training processes, EDAtt uses a single model for each latency regime, managing latency through attention-based parameters.

The core innovation is a strategy that decides whether to emit or withhold partial translations based on the distribution of attention weights between audio input and text output. Words are emitted if the attention is spread widely across recent speech frames, indicating stable information. Conversely, emission is withheld if attention is concentrated in the past, suggesting incomplete or uncertain information.

EDAtt outperforms popular strategies applied to offline models, including Wait-k and Local Agreement, and even state-of-the-art architectures specifically designed for SimulST. It achieves high translation quality while maintaining low latency, both in terms of raw time and computational resources.

The paper includes experimental results in German, demonstrating EDAtt's effectiveness, and provides open-source code, models, and simultaneous outputs to facilitate reproducibility. By focusing on efficient and adaptable model usage, EDAtt offers a promising direction for real-time, cross-language communication.</sample>
    <sample id="234">The prompting strategy significantly impacts the performance of large language models (LLMs) for translation. The study found that:

- Prompting can alter translation quality by more than one BLEURT point (up to 40 points in extreme cases).
- A 5-shot prompting strategy, marking sentences with their language, performs well and is less sensitive to the form of prompting.
- Example quality is more critical than the similarity to the source sentence, especially for zero and one-shot prompting.
- Selecting prompts from high-quality training data improves performance.
- PaLM's fluency is comparable to state-of-the-art systems, but it tends to omit parts of the source sentence, leading to accuracy issues.</sample>
    <sample id="235">The authors of the paper "When Does Translation Require Context? A Data-driven, Multilingual Exploration" are affiliated with the following institutions:

1. **University of Toronto** (Kayo Yin, Patrick Fernandes, André F. T. Martins)
2. **Stanford University** (Emmy Liu)
3. **University of Edinburgh** (Graham Neubig)</sample>
    <sample id="236">According to the presentation, each task in the MultiInstruct dataset is equipped with **five expert-written instructions**.</sample>
    <sample id="237">The authors propose the **KITMUS Test**, a diagnostic test suite that includes a coreference resolution task designed to probe models' ability to integrate knowledge from both pretraining (background knowledge) and inference-time inputs (entity-specific knowledge).</sample>
    <sample id="238">**Abstract**

This video introduces MeetingBank, a new benchmark dataset designed to address the urgent need for summarization technologies in diverse reading domains, particularly for meetings. Developed by Yebowen Hu from the University of Central Florida, MeetingBank comprises 1,366 City Council meetings, totaling nearly 7,000 instances.

The dataset was meticulously created through a multi-step process involving Speechmatics API for audio-to-transcript conversion, web scraping for meeting data, and alignment of timestamps. It includes meeting transcripts, reference summaries, and other relevant resources.

Statistical analysis reveals varying levels of abstraction in meeting summaries, with coverage and density scores indicating a mix of verbatim points and extracted references. Top-tier summarization systems, including extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pagasus, Longformer, DialogLM, HMNet) models, were evaluated using ROUGE-2, BERTScore, MoverScore, and human assessment.

The findings highlight the promise of extractive methods like Extr-Oracle and the superior human-assessed performance of GPT-3, despite its lower automatic metric scores. The dataset offers a valuable resource for researchers and developers working on meeting summarization, providing insights into the decision-making processes of City Councils. Users are encouraged to download and explore MeetingBank for further research and development.</sample>
    <sample id="241">**Abstract:**

This paper presents a human-in-the-loop evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment claims on Twitter. The authors address two key shortcomings of existing automated approaches: unrealistic evaluation methods and lack of human-centric design.

The proposed system comprises two main components. The first detects misleading claims through keyword filtering and a T5 question-answering model, ranking them by trendiness for human verification. The second component verifies policy violations using a BERT-based stance classification model to identify authors' stances towards unapproved treatments.

The evaluation focuses on early detection, defined as identifying unapproved treatments before they appear in debunking news articles. The system successfully detected several such claims. Additionally, a Likert scale assessment showed 65% accuracy in policy violation detection, with 124.2 violations confirmed per human hour.

The framework offers a realistic, end-to-end approach integrating human feedback throughout the process. It provides a consistent evaluation method for future human-in-the-loop misinformation systems and offers industry insights for outsiders. The work aims to advance the development of effective, human-centric misinformation detection tools.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include:

1. **Human Evaluation**: Asking human judges to select better conversations or rate them on a Likert scale.
2. **Comparative Methods**: Evaluating multiple dimensions of dialogue quality by asking judges to compare specific aspects of responses.
3. **Likert Scale Ratings**: Providing scales for evaluating dialogue quality at both turn-level and dialogue-level.
4. **Pairwise Comparisons**: Comparing conversations at the dialogue level to determine which is better.</sample>
    <sample id="243">The paper involves 4 authors: Jenny (the presenter), Sebastian Santy, Ronan Le Bras, and Katharina Reinecke. A further contributor, Maarten Sap, is also mentioned.</sample>
    <sample id="244">The background knowledge needed in the example is:

- "Judges decide cases in law courts" (from pretraining)
- "Servin is a judge" (entity-specific knowledge provided at inference time)

This knowledge is required to correctly resolve the pronoun "he" to refer to Servin.</sample>
    <sample id="245">**Abstract**

This work presents a two-step pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for text summarization tasks, addressing the challenges posed by automatic metrics and poor recruitment practices. The pipeline comprises a *Qualification Task* and an *Endurance Task*, designed to filter workers based on their ability to accurately evaluate multiple dimensions and handle heavy workloads, respectively.

In the Qualification Task, 26 MTurk workers (8 gold, 18 silver) passed based on their performance across three documents and one attention check. The Endurance Task tested 200 workers, resulting in 12 high-agreement workers (4 gold, 8 silver). These workers achieved Inter-Annotator Agreement (IAA) comparable to expert judgments.

A reference-based task evaluated general performance, with 12 MTurk workers completing all 30 HITs. The pipeline's effectiveness was compared to Baseline MTurk workers (using MACE filtering) and CloudResearch workers. Results showed that the pipeline achieved similar quality to CloudResearch while maintaining a lower task acceptance rate.

Analysis of correctness across annotation sources revealed significant correlations between pipeline and CloudResearch workers. However, the pipeline's training of correctness was not guaranteed. Future work aims to expand to different languages, tasks, and platforms, while addressing limitations such as task-specific question design and platform-specific testing (currently limited to English summarization on MTurk).</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">**Abstract**

This paper introduces **FactKG**, a novel dataset and task for knowledge graph-based fact verification. Existing datasets, such as FEVER, TabFact, and their variants, primarily rely on text or tables as evidence. FactKG, however, leverages DBpedia, a comprehensive knowledge graph, to offer intuitive and direct connections between claims and evidence.

The dataset contains natural language claims in both written and colloquial styles, labeled as either *SUPPORTED* or *REFUTED*. It challenges models with five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. One-hop claims rely on single triples, while conjunction and existence claims involve multiple triples. Multi-hop reasoning is required for claims needing indirect connections, and negation necessitates additional verification.

FactKG's practical utility stems from its integration with modern dialogue systems that utilize knowledge graphs. It enables consistency checks between user statements and knowledge graphs.

The paper presents baselines that demonstrate the effectiveness of utilizing knowledge graph evidence for fact verification. The **GEAR** model, which incorporates graph evidence, outperforms baselines that rely solely on textual claims, achieving a significant improvement over the majority class baseline of 51%.

FactKG is publicly available, encouraging further research and development in knowledge graph-based fact verification.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic. The presentation mentions that only a few annotators usually annotate each instance in the original datasets, and while re-annotating, they aimed to get a rich set of demographic data by recruiting a large number of diverse annotators from 87 countries. However, the focus was on gathering many annotations rather than achieving perfect balance across all demographics.</sample>
    <sample id="249">Sentences in the acceptable domain were perturbed by adding noise while preserving their relevant structural elements. This involved various perturbations to the input sentences without altering their core grammaticality or meaning.</sample>
    <sample id="250">Dimensional evaluation means assessing the quality of a conversational AI model by examining multiple specific aspects or behaviors of its responses, rather than just a single overall score. It provides a more nuanced understanding of the model's strengths and weaknesses by measuring various dimensions like relevance, contradictions, irrelevant information, and empathy.</sample>
    <sample id="251">The authors of the paper are from the University of Science and Technology of China.</sample>
    <sample id="252">**Abstract:**

This presentation introduces U-CREAT, an unsupervised case retrieval system for legal professionals, addressing the challenge of prior case retrieval in the face of burgeoning legal document volumes. The work, a collaboration between Sai Kiran Tanikella (IIT Kanpur) and colleagues, makes two key contributions: the Indian Legal Prior Case Retrieval (IL-PCR) dataset and the U-CREAT pipeline.

The IL-PCR dataset comprises 7,070 Indian legal cases with extensive citations, serving as a robust benchmark for prior case retrieval (PCR) algorithms. It surpasses existing datasets like COLIEE’21 in size, document length, vocabulary, and citations.

U-CREAT leverages unsupervised learning and event-based approaches, demonstrating high efficiency, swift inference, and generalization across Indian and Canadian legal systems without requiring domain-specific tuning. Event extraction, using dependency parsing, identifies subject-verb-object triplets (events) from case documents. These events are then used to compute interaction matrices, ranking candidates based on similarity.

Experiments validate U-CREAT's superior performance compared to baseline methods and existing models, including transformer-based approaches tailored for legal text. Event-based models, particularly the Event Filtered Documents approach, achieve significant improvements. U-CREAT's state-of-the-art performance on both IL-PCR and COLIEE’21 datasets opens new avenues for PCR research and development.</sample>
    <sample id="253">**Abstract:**

"DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media" presents a novel approach to automatically identify mental health issues through social media analysis. The research focuses on leveraging domain adaptation techniques to fine-tune the BERT language model for the specific domain of mental health discourse on Reddit.

The study addresses the challenge of insufficient annotated data for mental health detection by utilizing knowledge from a related domain. The proposed DisorBERT model first learns the language of social media and then specializes in recognizing mental disorders. Guided masking techniques are employed to encourage the model to focus on critical words during training.

Evaluation using the eRisk dataset demonstrates DisorBERT's superior performance over baselines, achieving a good balance between precision and recall. The model's effectiveness is illustrated through case studies, showing its ability to identify negative psychological indicators in sentences from Beck's Depression Inventory.

DisorBERT's predictions are characterized by a bias towards words associated with mental disorders. Interactive visualizations highlight the model's attention to relevant keywords and sentences, such as "anxious" and "medication," in a depression-related post.

Future work includes exploring diverse lexical resources and clinical data. The study concludes that double domain adaptation and guided masking effectively capture mental health signs on social media, offering a promising direction for automated mental health monitoring.</sample>
    <sample id="254">**Abstract**

This paper presents a novel framework for document-level distant relation extraction (DocRE) called "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The primary challenge addressed is the noise inherent in distantly supervised data (DS), which is crucial for pretraining DocRE models. Existing methods mitigate noise using pseudo labels, but these labels can introduce false positives, leading to incorrect relations.

Our framework introduces uncertainty-guided label denoising to enhance DS data quality. We train a pre-denoising DocRE model using both DS and human-annotated data, generating pseudo labels. To account for inevitable false pseudo labels, we employ instance-level uncertainty estimation, capturing uncertainty scores for overlapping relations. This is achieved through a modified Monte Carlo dropout technique, allowing for dynamic class uncertainty thresholds.

We propose a re-labeling strategy that iteratively replaces high-uncertainty pseudo labels with more reliable ones. This dynamic thresholding addresses the long-tail problem, where frequent classes have lower uncertainty than long-tail classes. Extensive experiments on public datasets demonstrate our framework's superior performance compared to baselines, highlighting its effectiveness in improving DocRE accuracy.

In summary, our contribution lies in a robust framework that leverages uncertainty estimation to enhance DS data, improve overlapping relation handling, and mitigate the long-tail problem, ultimately boosting DocRE performance.</sample>
    <sample id="255">Based on the content provided, the form of the prompting is crucial in **zero-shot and one-shot prompting scenarios**. In these cases, the specific phrasing of the prompt can significantly impact performance. However, with a 5-shot prompting strategy (or more), the actual form of the prompt becomes less important, and the quality of the examples provided carries the most weight.</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">**Abstract:**

This work explores the potential of large language models (LLMs) as alternatives to human evaluation in natural language processing (NLP). The study proposes using LLMs to rate the quality of text samples based on instructions, aiming to replicate the stability and reproducibility of human evaluation.

Previous to this research, human evaluation was the standard for assessing NLP tasks. However, this study argues for the novelty of using LLMs, as no prior work had explored this concept at the time of submission. The experiment involves evaluating stories generated by GPT-2 or written by humans across four attributes: grammar, coherence, likability, and relevance.

Four LLMs, including T0, InstructGPT (Curie and Davinci), and ChatGPT, were employed. Results show that while human raters (English teachers) consistently preferred human-written stories, smaller LLMs did not exhibit a clear preference. However, Davinci and ChatGPT demonstrated a strong preference for human-written text, suggesting their viability as LLM evaluators.

The study delves into the agreement between LLM and human raters, the impact of instruction wording and LLM response sampling, and the benefits and drawbacks of LLM evaluations compared to human evaluations. It also addresses the broader applicability of LLM evaluations to other NLP tasks, leaving several questions open for further exploration.</sample>
    <sample id="259">**Abstract:**

"XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" presents a novel unified benchmark for cross-lingual semantic parsing (CLSP) across diverse languages and representations. The study addresses existing gaps in CLSP literature, particularly the lack of comprehensive datasets and models for languages like Chinese, and the underrepresentation of meaning representations like Lambda calculus.

XSemPLR offers a diverse dataset encompassing 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages from 15 language families. It introduces six evaluation settings, including Translate-Test, Monolingual (Few-shot), Multilingual, Cross-lingual Zero-shot, and Few-shot transfer.

The research compares performance across Encoder-PTR (multilingual pretrained encoders with pointer-based decoders) and Encoder-Decoder (multilingual pretrained encoder-decoder models) architectures. Key findings include:

- Encoder-Decoder models outperform others across all nine datasets.
- Multilingual training improves performance for most languages, except English, highlighting the "Curse of Multilinguality."
- Cross-lingual transfer shows significant performance gaps compared to monolingual settings, but these gaps narrow with Few-shot transfer.
- Pretraining on English enhances Few-shot performance on target languages.
- Current multilingual models like Codex and BLOOM remain inadequate for CLSP tasks.

XSemPLR provides a foundational resource for advancing CLSP research and facilitates future comparisons and progress in this field.</sample>
    <sample id="260">Based on the content provided, it appears the paper has one author, Jingwei Yi, from the University of Science and Technology of China. While the presentation style suggests a group effort, the specific contribution and authorship details point to a single author.</sample>
    <sample id="261">A good planner should:

1. **Write scripts that are reasonable:** The plans should make sense and be coherent.
2. **Faithful to constraints:** The generated scripts should accurately reflect and adhere to the specific goals' constraints.</sample>
    <sample id="262">Based on the content provided, the paper involves the authors from Fudan University, specifically Siyu Yuan, and potentially other contributors mentioned in the context of the research (e.g., crowd-sourced workers for dataset revision). Since the question asks for a concise answer regarding the number of authors, the direct answer would be **at least 1 (Siyu Yuan)**. The full extent of the author list is not explicitly stated in the text.</sample>
    <sample id="263">**Abstract**

This work addresses the instability and bias inherent in in-context learning, a popular method for utilizing large language models. We categorize label biases in text classification tasks, identifying three primary types: vanilla-label bias (model preference for label names), context-label bias (influence of context), and domain-label bias (effect of the task corpus).

Through experiments, we confirm that domain-label bias significantly impacts model predictions, often leading to poor performance. We propose **domain-context calibration**, a novel method that enhances in-context learning. This approach uses random in-domain words as content-free text to estimate and mitigate all types of label biases, including the newly recognized domain-label bias.

Our experiments across various models and datasets demonstrate domain-context calibration's effectiveness. It substantially improves in-context learning performance, especially on tasks with high domain-label bias. We attribute this success to the method's ability to consider domain-specific biases and use multiple random words for calibration, outperforming previous methods that relied on single, predefined tokens.

In summary, our work provides a comprehensive analysis of label biases in in-context learning and introduces a robust calibration technique to address these biases, enhancing the stability and accuracy of large language models.</sample>
    <sample id="264">**Abstract: "TAVT: Towards Transferable Audio-Visual Text Generation"**

This paper introduces TAVT, a novel multimodal text generation framework addressing the challenges of data annotation and domain shifts in audio-visual text generation. Existing methods struggle with significant performance drops due to varying construction conditions across domains.

TAVT aims to train a model capable of learning and quickly adapting to new multimodal domains with limited labeled data. The proposed framework is modular, consisting of an audio-visual meta-mapper network, audio-visual encoder, and language model generator, along with counterfactual contrastive learning.

The audio-visual meta-mapper network aligns visual concepts across domains into a unified auditory semantic space, utilizing audio clusters from the Flickr dataset. Inspired by prompt learning, learnable tokens, called visual prefixes, are introduced to improve audio reconstruction and align visual content with the audio space.

The transformer-based encoder and generator incorporate modality contribution weights, with a dual counterfactual contrastive learning (DCLL) loss for direct optimization of visual-audio alignment.

Experimental results on MSVD and MSR-VTT benchmarks demonstrate TAVT's superior performance compared to state-of-the-art (SOTA) approaches, even under low-resource domain conditions. Ablation studies highlight the effectiveness of the proposed audio features.

To the best of the authors' knowledge, TAVT is the first work exploring transferable audio-visual text generation.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">Based on the content provided, the author of the paper, Adam Przepiórkowski, appears to be affiliated with an academic or research institution, but the specific name or affiliations are not explicitly mentioned in the text. The content focuses on the theoretical argument and linguistic analysis rather than providing direct details about the author's affiliations.</sample>
    <sample id="268">Based on the content provided, the most common errors of PaLM in machine translation are **omission errors**, where it sometimes drops parts of the source sentence while aiming to produce a better-sounding translation.</sample>
    <sample id="270">The authors of the paper, James Finch and Sarah Finch, are affiliated with Emory University, specifically the Emory NLP Lab led by Professor Jinho Choi, in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for Continuous Fine-tuning.</sample>
    <sample id="272">The paper involves 7 authors: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, and Adina Williams.</sample>
    <sample id="274">The speaker's name is Yusen Zhang.</sample>
    <sample id="276">**Abstract**

This paper introduces "IndicMT Eval," a dataset designed to meta-evaluate machine translation (MT) metrics specifically for Indian languages. While extensive work exists to evaluate English-to-other-language MT, metrics for the reverse direction, particularly for Indian languages, are scarce. Recognizing the unique linguistic features of these languages, we selected 200 sentences from the Flores dataset, generating 1,400 candidate translations using seven MT models. Human annotations, employing bilingual experts, captured detailed error types and severity, leading to a rich dataset of 7,000 samples.

Our analysis compared seven MT metrics across five Indian languages, including Dravidian (Tamil, Malayalam) and Indo-Aryan (Hindi, Marathi, Gujarati) languages. We found that newer models like NLLB and Indic Trans outperformed older ones. Correlation analysis revealed that overlap-based metrics like chrF had high correlations but low overall performance, while embedding-based metrics like LabSE and BERTscore showed better correlations. COMET-metric variants consistently demonstrated the highest correlations.

Furthermore, we fine-tuned the top-performing COMET metric using our dataset, creating IndicCOMET MQM. This fine-tuned version outperformed COMET baselines on three languages and showed higher correlations across all five. IndicCOMET MQM's robustness was validated on ACES Translation Accuracy Challenge Sets, achieving a higher correlation score (0.36) than its counterpart (0.272).

The dataset and findings are publicly available, aiming to advance the study of MT evaluation metrics for Indian languages.</sample>
    <sample id="277">The new method does not have a specific named approach mentioned in the text. It is referred to as "predicting the permutation" or "our permutation model."</sample>
    <sample id="278">The "Marked Words" method identifies words that distinguish marked (marginalized) groups from unmarked (dominant) ones, using sociolinguistic concepts of "markedness" and weighted log-odds ratios to highlight harmful stereotypes and essentializing narratives within generated personas.</sample>
    <sample id="279">The authors of the paper, "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models," are affiliated with the University of Washington. Specifically, the lead author is Shangbin, a PhD student at the university.</sample>
    <sample id="280">**Abstract:**

This paper introduces MultiEMO, a novel attention-based multimodal fusion framework for Emotion Recognition in Conversations (ERC). Addressing key challenges in ERC, MultiEMO leverages the complementarity of multimodal information—textual, audio, and visual—more effectively than existing methods.

The proposed framework consists of four main components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. A novel VisExtNet visual feature extractor integrates facial expressions from multiple frames without redundant scene information. MultiAttn, the multimodal fusion model, employs bidirectional multi-head cross-attention layers to integrate each modality with complementary information from the others.

MultiEMO introduces a Sample-Weighted Focal Contrastive Loss (SWFC) to handle minority and semantically similar emotion classes. SWFC assigns higher importance to hard-to-classify examples and ensures mutually exclusive sample pairs.

Extensive experiments on MELD and IEMOCAP datasets demonstrate MultiEMO's state-of-the-art performance, achieving significant improvements in minority and similar emotions. Visualization shows MultiEMO's effectiveness in challenging scenarios with asynchronized modal emotional tendencies.

While MultiEMO has limitations, such as speaker-irrelevant people in visual features and batch size requirements for SWFC, it represents a substantial advancement in ERC, pushing the boundaries of emotion recognition accuracy and robustness.</sample>
    <sample id="281">**Abstract:**

This study, "When Does Translation Require Context?", investigates the role of context in machine translation, addressing a critical yet under-evaluated aspect. The research, led by Kayo Yin and colleagues, explores how context influences translations and aims to develop a better understanding of when and how models should consider context.

The authors introduce and extend the CXMI (Context Usage Measure for Machine Interpretation) to Pointwise CXMI, enabling context-level analysis at sentence or word scales. They analyze TED talk transcripts translated into 14 languages, identifying patterns in words with high context dependency. Key findings include the importance of context for dual pronouns and verb forms in certain languages, and the role of context in choosing proper nouns and maintaining formality.

Building upon these insights, the team designed a Multilingual Discourse-Aware (MuDA) tagger to automatically identify discourse phenomena in parallel corpora. They then evaluated various translation models using both corpus-level metrics (BLEU, COMET, word f-measure) and the MuDA benchmark. Results show that context-aware models outperform others for specific phenomena like formality, while models without context perform better for others.

The study highlights the need for further progress in document-level translation, particularly for handling ellipsis, pronouns, and verb forms. It also compares commercial translation systems, indicating DeepL's superior accuracy over Google Translate for document-level tasks. This work contributes a data-driven approach to understanding and evaluating context-dependent translations, aiming to enhance machine translation capabilities.</sample>
    <sample id="282">## StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations

This paper introduces **StoryTrans**, a novel model for non-parallel text style transfer at the **story level**, addressing a crucial gap in natural language generation (NLG). Existing methods primarily focus on token-level or sentence-level transfers, like sentiment or formality shifts. StoryTrans goes further by leveraging **discourse representations** to capture complex authorial preferences, including narrative techniques and topic-specific styles.

The main challenges lie in imitating these nuanced choices while preserving the original content. To overcome this, StoryTrans combines:

* **Discourse representations:** Learned from source texts to capture authorial style.
* **Style embeddings:** Learnable representations specific to target styles.
* **Two-stage generation:** First, transfers style-specific keywords while masking others. Second, generates the complete story, incorporating the keywords explicitly.

The model is trained using a novel objective that reduces stylistic features in discourse representations and enhances content preservation. Extensive experiments on Chinese and English datasets demonstrate StoryTrans outperforms baselines in style control and content fidelity, both automatically and through manual evaluation.

StoryTrans effectively enriches storylines, maintains source semantics, and produces coherent text in target styles, showcasing its potential for diverse NLG applications. The paper includes datasets, code, and further details.</sample>
    <sample id="283">The first mentioned symmetrical dependency structure is the **Prague approach**, specifically assuming **conjunction-headed** structures for coordination.</sample>
    <sample id="284">**Abstract**

This paper introduces **FSUIE**, a novel approach for Universal Information Extraction (UIE) that leverages a **fuzzy span mechanism** to enhance performance. Traditional UIE models heavily rely on precise span boundaries, which are prone to ambiguity. FSUIE addresses this by modeling span boundaries as continuous distributions, representing the likelihood of correct positions within a specific range.

The core contributions are:

1. **Fuzzy Span Loss:** This loss function calculates a probability distribution for span boundaries, mitigating the impact of annotation ambiguity. It combines Binary Cross Entropy with KL-divergence to incorporate supplementary information.

2. **Fuzzy Span Attention:** A novel attention mechanism that dynamically adjusts the attention span and smoothly decays attention weights at boundaries. This allows the model to focus on relevant semantic information within a limited context.

FSUIE is evaluated on three key UIE tasks: Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction. The results demonstrate significant performance improvements, achieving state-of-the-art results on several datasets, especially under limited training data.

Through ablation studies, the effectiveness of both fuzzy span loss and attention is confirmed, highlighting their individual and combined contributions to the model's success. Visualizations further illustrate the module's focused attention within relevant token ranges.</sample>
    <sample id="285">**Abstract**

This work, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization," from Mingqi Gao of Peking University, addresses the persistent factual errors in dialogue summaries generated by models. It distinguishes two primary solutions: integrating factuality objectives into summarization models or employing independent Factual Error Correction (FEC) models.

The study identifies flaws in current FEC evaluation methods, which rely on vague overall factuality scores from metrics like FactCC and DAE. These methods blur the distinction between summarization and error correction, potentially leading to models that generate factually correct but content-disconnected summaries.

To address these issues, the authors propose a fine-grained evaluation framework based on ERRANT, focusing on minimal substitution, insertion, and deletion operations for error correction. They introduce a taxonomy of factual errors categorized by content and form.

Key findings include:

- Training FEC models with human-annotated reference summaries from dialogue datasets yields superior performance to unreliable factuality metrics alone.
- Human-corrected summaries significantly enhance FEC model performance during training.
- Combining human-annotated and synthetic data is a promising approach.
- Current FEC models struggle with addition and attribute, modality, and link errors.

The study advocates for more rigorous evaluation methods and highlights the importance of human-corrected data in advancing factual error correction for dialogue summarization.</sample>
    <sample id="286">The speakers are James Finch and Sarah Finch.</sample>
    <sample id="287">The paper is a joint work involving 4 authors:

1. Javad Hosseini
2. Filip Radlinski
3. Silvia Pareti
4. Annie Louis</sample>
    <sample id="288">Based on the content, the datasets used to test syntactic phenomena include:

1. **BLiMP (Bilingual Language Model Perturbations)**: A dataset that focuses on evaluating language models' sensitivity to grammaticality.
2. **SyntaxGym**: Another dataset used for testing language models' syntactic understanding.
3. **CrowS (Computationally-generated Crowdsourced Sentences)**: Used for evaluating acceptability judgments based on stereotypes.
4. **Wikipedia**: Used as an external, unrelated domain to test the robustness of language models' acceptability judgments against arbitrary context.</sample>
    <sample id="290">The abbreviations for the methods mentioned in the first research question (whether clean validation data is necessary for Weakly Supervised Learning - WSL) are not explicitly stated in the text. However, based on the context and the methods discussed in the presentation, they could be:

1. **WSL**: Weakly Supervised Learning (the general approach)
2. **FTw**: Vanilla model (presumably a simple baseline)
3. **COSINE**: A specific complex WSL method (as mentioned as a comparison to the vanilla model)

The other two methods mentioned in the context of the research question (clean vs. noisy validation sets) are not abbreviated, but they are the core concepts being compared.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT is initially trained on a 4 GB set of NACHOS (a dataset of medical crawled data from the web) in the French language.</sample>
    <sample id="295">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="296">**Abstract**

This study, a collaboration between the University of Turin and Amazon Alexa, investigates the limitations of supervised machine learning in natural language understanding, specifically focusing on irony detection. Traditional approaches assume a single "ground truth" annotation, but this research challenges this assumption, highlighting the complexity of irony as a pragmatic phenomenon.

A corpus named EPIC (English Perspectivist Irony Corpus) was developed, containing 300 short conversations from social media, Reddit, and Twitter, spanning 1.5 years and five varieties of English. The data was annotated by 74 annotators using a crowdsourcing platform, with each annotator evaluating 200 texts.

Analysis revealed significant inter-annotator agreement variations across different groups, including gender, age, and nationality. These differences were leveraged to create "perspective-aware models" by fine-tuning a pre-trained language model on subsets of the data annotated by specific groups.

Results showed that perspective-aware models demonstrated higher confidence in their predictions compared to traditional aggregated models. Further investigation uncovered intriguing patterns: annotations from annotators of similar ages and geographical origins often diverged, suggesting generational and cultural differences in irony perception.

The study concludes with a call for further discussions and questions at the poster session, emphasizing the importance of considering contextual and demographic factors in NLP models.</sample>
    <sample id="297">**Abstract**

"From Dogwhistles to Bullhorns" investigates the use of coded rhetoric, particularly dogwhistles, in political discourse, focusing on their anti-Semitic dimensions. The paper introduces a glossary of 340+ terms and symbols, categorized by register (formal/informal), persona (e.g., anti-Semitic, transphobic), and type (implicature-adding vs. signaling).

Through a historical case study of U.S. political speeches, the authors demonstrate a correlation between dogwhistle frequency and the Republican Southern Strategy, revealing an increase since the Civil Rights era. They establish a conservative association with dogwhistles over time.

Language models, specifically GPT-3, are evaluated for their ability to identify dogwhistles. While GPT-3 excels with formal dogwhistles, performance drops significantly for informal and transphobic ones. Prompting strategies, including definitions and secret cues, improve recognition.

The study also highlights dogwhistles' role in evading content moderation. By replacing slurs with dogwhistles, hateful sentences are rated less toxic, demonstrating their effectiveness in masking harmful rhetoric.

In summary, the project offers a typology, glossary, and case studies to advance understanding of dogwhistles, their context, and their impact on political communication and online moderation.</sample>
    <sample id="298">The conclusion that temporal drift is the main cause of performance loss was led by the following findings:

1. **Gradient Analysis**: The graph showed that the improvement on CoNLL++ did not follow the diminishing returns pattern expected from adaptive overfitting, suggesting temporal drift was more likely.

2. **Retraining Experiment**: Models retrained or pre-trained with more recent data showed performance degradation with increasing temporal gaps, confirming the hypothesis of temporal drift.</sample>
    <sample id="299">**Abstract:**

This work addresses the vulnerability of Natural Language Inference (NLI) models to shortcuts—spurious correlations in training data that lead to overfitting and poor out-of-distribution performance. Existing shortcut mitigation methods rely on auxiliary models and specific knowledge of dataset biases, limiting their applicability. We propose a novel training method that leverages minimax optimization between a learner and an auxiliary. The auxiliary aims to maximize the learner's loss by assigning weights to training instances, emphasizing hard examples that contradict shortcuts. This approach encourages the learner to focus on these under-represented instances, improving its generalization.

Our method, dubbed "Minimax Training for Robust NLI" (MTR-NLI), does not require domain-specific knowledge and can be applied to any NLI dataset. We evaluate MTR-NLI on MNLI, FEVER, and QQP, as well as their adversarial test sets, demonstrating consistent improvements in out-of-distribution performance compared to baseline and state-of-the-art shortcut mitigation methods. We also explore the impact of pre-training the learner, the size of the auxiliary, and transfer to larger models and synthetic shortcuts. Additionally, we qualitatively analyze the learned example weight distributions.

In summary, MTR-NLI offers a robust and adaptable solution to shortcut mitigation, enhancing the reliability of NLI models across diverse scenarios.</sample>
    <sample id="300">**Abstract**

This work introduces **interactive dictation**, a task enabling users to dictate and edit documents using voice in a natural, intuitive manner. Unlike existing speech-to-text systems that lack edit capabilities, interactive dictation seamlessly interweaves dictation and command phases without requiring trigger words. Users can correct mistakes, replace text, and issue complex commands using everyday language.

We propose a four-step framework for interactive dictation: speech recognition, segmentation, command extraction, and interpretation. To facilitate this task, we designed a novel data collection interface and built a dataset consisting of user dictations and commands.

We developed a baseline system using separate models for each step, experimenting with T5 and GPT-3 architectures. Our findings highlight a trade-off between runtime and accuracy, with GPT-3 models achieving higher accuracy but at the cost of speed. Predicting the next state directly outperformed predicting intermediate programs for GPT-3, while T5 showed a less pronounced distinction.

The paper provides detailed insights into the system's performance and offers code for future research on interactive dictation, a promising direction for enhancing human-computer interaction with voice-based text editing.</sample>
    <sample id="302">The tokens from the input are not ordered in the output, so a permutation model is used to correctly arrange them according to the expected output structure.</sample>
    <sample id="303">The authors recommended increased transparency about bias mitigation methods because they identified pernicious patterns resulting from positive stereotypes generated by LLMs, and they want to understand if these patterns stem from weird value alignments or other anti-stereotyping methods. Without transparency, it's impossible to fully study and address these issues.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that share the same grammatical structure but differ in their acceptability or sterotypical features. These inputs are used to test a language model's acceptability judgments by presenting it with pairs of sentences where one is acceptable and the other is not.</sample>
    <sample id="305">**Abstract:**

This presentation discusses the "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" research, focusing on the challenges and limitations of Weakly Supervised Learning (WSL) methods. The study questions the common assumption that WSL models can achieve high performance solely on weakly labeled data, without the need for clean validation sets.

The research reveals that recent WSL techniques heavily rely on clean validation data for proper functioning, leading to significant performance drops when such data is absent. Furthermore, increasing the number of clean validation samples improves WSL performance, with as little as 20 samples per class proving sufficient. Direct fine-tuning on these clean samples, however, surpasses WSL methods, indicating overstated performance gains.

The study highlights that the computational benefits of WSL are offset by the hidden cost of manual annotation for validation data. It recommends that future WSL work should: (1) disclose model selection criteria, (2) compare against few-shot learning baselines, (3) consider continuous fine-tuning as a baseline, and (4) share code for transparency. The findings challenge the current understanding of WSL's practicality and performance, emphasizing the importance of clean data in its success.</sample>
    <sample id="306">**Abstract**

This paper investigates the ability of large language models to track entity states within discourses, crucial for understanding complex narratives. The authors introduce a task involving boxes and objects, where models must predict box contents based on initial descriptions and subsequent state-changing operations. This design aims to prevent models from relying on simple heuristics or memorization.

Experiments using Flan-T5 and GPT-3/-3.5 models with 2-shot in-context learning reveal varying performance. While most models simply repeat initial states, GPT-3.5 models trained on substantial code exhibit non-trivial tracking abilities. This suggests that pre-training on code is key to developing entity state tracking capabilities.

Smaller models like T5-base can learn the task through direct fine-tuning, but randomly initialized models cannot, emphasizing the importance of pre-training. The study provides valuable insights into the limitations and potential of current language models in tracking entity states, highlighting the need for more sophisticated evaluation tasks.

The authors conclude by discussing their findings in detail, including GPT-4 experiments, and invite further exploration and discussion at ACL or via email/Twitter.</sample>
    <sample id="307">The authors used various evaluation metrics for their models, including:

- Named Entity Recognition
- Classification
- Part-of-Speech Tagging
- Question Answering

They compared these models against baseline models like CamemBERT, PubMedBERT, BioBERT, and ClinicalBERT, and evaluated performance on both public and private downstream tasks.</sample>
    <sample id="308">**Abstract**

This presentation, "NLPositionality: Characterizing Design Biases of Datasets and Models," explores the concept of positionality in Natural Language Processing (NLP), highlighting how researchers' demographics and life experiences can influence technology development. The work, a collaboration between Carnegie Mellon University, the University of Washington, and the Allen Institute for AI, investigates whether NLP datasets and models exhibit biases by comparing their performance across diverse populations.

Using a framework called NLPositionality, the study re-annotates datasets with a diverse pool of annotators to uncover hidden biases. This approach, facilitated by online crowdsourcing platforms like Lab in the Wild, allows for rich demographic data collection. The analysis reveals that datasets and models tend to align most closely with English-speaking, college-educated individuals, neglecting others, such as non-binary people.

The presentation underscores the importance of documenting design choices, conducting NLP research with a perspective-aware lens, and developing specialized datasets and models tailored to specific communities. The study's findings emphasize the need for inclusivity in NLP, advocating for more equitable technology that serves a broader range of users. The authors provide a dashboard and a paper for further exploration of these critical issues.</sample>
    <sample id="309">The metric used for measuring inter-annotator agreement was the inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">The completely unrelated domain chosen was Wikipedia.</sample>
    <sample id="311">Based on the content provided, the authors of the paper introducing DEPLAIN (a new corpus for German text identification) are affiliated with the following:

1. Regina Stodden (specific role not explicitly stated, but she is mentioned as a guide through the presentation)
2. Omar (mentioned as the speaker for the use cases section, suggesting a research or academic role)

The presentation also mentions collaborations or references to work from other institutions or researchers, but the specific affiliations for those are not detailed in the text.</sample>
    <sample id="312">MultiInstruct differs from other benchmarks primarily in these aspects:

1. **Multi-Modal Focus**: While most previous works focus on language-only zero-shot tasks, MultiInstruct is the first large-scale benchmark specifically designed for multi-modal instruction tuning.

2. **Instruction Tuning Dataset**: It provides a dataset with 62 diverse multi-modal tasks covering 10 broad categories, derived from 21 existing open-source datasets, and includes five expert-written instructions per task.

3. **Unified Representation**: MultiInstruct unifies the processing of various input and output data types, such as text, images, instructions, and bounding boxes, into a single token space.

4. **Sensitivity Metric**: It introduces a new evaluation metric called "sensitivity" to measure the model's consistency in producing outputs for the same task despite slight variations in instruction wording.

5. **Large Scale and Diverse Tasks**: MultiInstruct is the first to offer a large-scale multi-modal instruction tuning dataset with significantly improved capabilities for models like OFA.</sample>
    <sample id="313">The paper involves two authors: James Finch and Sarah Finch. However, the work was led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="314">Binary coordination refers to the structural arrangement of two elements (usually clauses or phrases) connected by a coordinating conjunction (e.g., "and," "but," "or"), where one element is typically considered the head or governing element, and the other is its complement.</sample>
    <sample id="315">The information provided does not specify the average length of the prompts used in the study. The text focuses on the methodology, findings, and recommendations rather than the exact duration of the prompts.</sample>
    <sample id="316">The findings imply that smaller models like T5, fine-tuned on the generated CoScript dataset, can outperform many large language models in terms of script generation quality for constrained language planning tasks. This suggests that specialized, smaller models can be effective alternatives to large models for specific language planning applications.</sample>
    <sample id="317">**Abstract**

This paper introduces CodeIE, a novel approach to improve few-shot performance in information extraction tasks, specifically named entity recognition (NER) and relation extraction (RE). Traditional methods use pre-trained language models like T5 and GPT-3, which operate in text-to-text fashion during pre-training but struggle with mismatched outputs during inference due to the linearization of structured outputs.

CodeIE addresses this issue by framing information extraction as a structure-to-structure code generation task, leveraging large code language models like Codex. This approach ensures aligned structures between input and output, facilitating better learning.

Through experiments on multiple datasets, CodeIE demonstrates significant and consistent improvements over baseline models, including UIE and GPT-3, in both one-shot and few-shot settings. Analysis reveals several key advantages:

- Lower perplexity on code format inputs compared to text format inputs.
- Fewer structural errors during decoding.
- Better adherence to predefined label sets.
- Superior overall performance, especially in recall, using code format prompts.

These findings highlight the effectiveness of CodeIE in enhancing few-shot information extraction capabilities, offering a promising direction for future research in natural language processing.</sample>
    <sample id="319">The work investigates the following learning strategies:

1. **From-scratch pre-training** using different amounts of data (7GB, 4GB, and 4GB mixed with clinical notes).
2. **Continual pre-training** using CamemBERT weights and tokenization on NACHOS (4GB subset).
3. **Comparison** between models trained on diverse data sources (NACHOS vs. clinical notes).
4. **Evaluation** of models on various biomedical and clinical downstream tasks.</sample>
    <sample id="320">Based on the presentation, the factor of overfitting due to test reuse (adaptive overfitting) is significant enough that every unit of improvement on CoNLL-2003 translates to **more than one unit improvement** on CoNLL++. This is indicated by the best-fit line's gradient greater than one in the graph shown.</sample>
    <sample id="321">The quality of the simplification was evaluated using manually aligned sentence pairs as a gold standard, comparing them with automatically aligned sentences. The performance of different automatic alignment methods was assessed, concluding that MASSalign was the best for German text simplification. Additionally, the effectiveness of fine-tuning language models (long-mBART and base mBART) for automatic text simplification was evaluated using various metrics.</sample>
    <sample id="322">**Abstract**

This paper investigates what text classifiers, specifically language models, learn about morality. While previous approaches treat morality as a singular scale, we acknowledge its subjective nature and the existence of diverse moral interpretations. Drawing from Moral Foundation Theory, we explore how different moral foundations—like fairness, authority, etc.—shape our judgments.

We focus on understanding how language models express morality across distinct domains, using a dataset of 35,000 tweets categorized by hashtags like #AllLivesMatter and #BlackLivesMatter. Our key finding is that models recognize varying moral expressions within seemingly similar domains. For instance, they associate "overthrow" and "mayhem" with subversion in #AllLivesMatter, but interpret subversion as encouragement in #BlackLivesMatter.

Through explainable AI techniques, we demonstrate that language models can discern subtle moral differences. However, using a single model for diverse domains can lead to misunderstandings. Our work highlights the importance of domain-specific models to avoid dangerous moral misinterpretations, underscoring the need for nuanced approaches in NLP.

In conclusion, this study offers insights into the moral understanding of language models and emphasizes the significance of considering moral pluralism in natural language processing.</sample>
    <sample id="323">**Abstract:**

This paper presents DHLK, a novel approach for Commonsense Question Answering (QA) that leverages both language models and knowledge representation learning. Commonsense QA demands machines to access and integrate external knowledge, a challenge addressed by integrating language models and knowledge bases.

Existing methods retrieve relevant knowledge from knowledge bases through entity matching and subgraph construction, but often introduce irrelevant entities and lack interaction between text and knowledge. DHLK overcomes these by constructing a hybrid knowledge graph (HKG) using a two-stage pruning strategy and knowledge representation learning (KRL). The HKG is formed by combining entity embeddings from multiple knowledge bases, including WordNet and Wiktionary, and enhancing them with paraphrase information.

DHLK employs RoBERTa and Mask Self-Attention to encode and fuse QA contexts and entities, dynamically removing less relevant entities based on attention weights. It utilizes TransE for entity and relation optimization within the HKG and introduces Relation Mask Self-Attention (RMSA) for subgraph modeling. After obtaining graph embeddings from the HKG, DHLK integrates path information into the QA context and predicts answers using a Multi-Layer Perceptron (MLP).

Experiments on CommonsenseQA and OpenBookQA demonstrate the effectiveness of DHLK, achieving competitive performance compared to other language model and HKG-based methods.</sample>
    <sample id="324">Yes, language models do have different political biases, as demonstrated in the study. They exhibit varying political leanings, ranging from liberal to conservative, which are influenced by the political perspectives present in their pretraining data. These biases can significantly impact their performance on downstream tasks, such as hate speech and fake news detection, leading to fairness issues.</sample>
    <sample id="326">Cognitive dissonance is the state of having conflicting beliefs or actions, such as stating one thing while doing another, leading to internal inconsistency.</sample>
    <sample id="327">**Abstract**

This paper introduces ManagerTower, a novel Vision-Language (VL) model architecture that enhances cross-modal alignment and fusion by leveraging insights from pre-trained unimodal experts at different semantic levels. Building on the BridgeTower architecture, ManagerTower addresses its limitations by adaptively aggregating multiple unimodal representations into each cross-modal layer using well-designed "managers."

ManagerTower utilizes RoBERTa and CLIP-ViT base as unimodal encoders and incorporates managers in each cross-modal layer. These managers adaptively combine insights from various levels of unimodal expertise, enabling more comprehensive semantic knowledge exploitation.

Despite pre-training on only four million images, ManagerTower outperforms models trained on larger datasets or with more parameters. Visualization of aggregation weights reveals that adaptive managers effectively tailor the contribution of different unimodal layers across cross-modal layers, demonstrating the model's ability to learn from diverse semantic levels.

The paper provides a detailed architecture description, code, and pre-trained models, highlighting ManagerTower's potential to advance VL research.</sample>
    <sample id="328">Based on the presentation content, GPT-4 is identified as the most liberal language model among the ones tested.</sample>
    <sample id="329">**Abstract:**

This work presents a novel zero-shot video sentence localization method, termed SPL, that leverages structured pseudo-label generation to overcome the challenges of manual annotation and label noise. Video sentence localization aims to identify relevant video segments based on natural language queries, with broad applications in video retrieval and summarization.

Existing zero-shot methods generate pseudo-events and pseudo-queries, but suffer from simple queries, unalignment between pseudo-queries and events, and label noise. SPL addresses these issues by:

1. **Complex Pseudo-Queries:** Using a pre-trained image caption model to generate free-form pseudo-queries, enhancing query complexity.
2. **Structured Pseudo-Events:** Modeling temporal event structures to ensure high relevance within events and low relevance outside events for each pseudo-query.
3. **Noise-Resistant Training:** Reducing label noise through sample re-weighting and label refinement, based on model confidence and prediction IoU.

Experiments on ActivityNet Captions and Charades-STA datasets demonstrate SPL's superior performance compared to existing zero-shot methods, achieving state-of-the-art results. The code is available upon request.</sample>
    <sample id="330">Yes, according to the presentation, the "Cumulative" strategy for updating a model with new data from each round of active learning performed equal to or better than the "Iterative" strategy across the board.</sample>
    <sample id="331">The speaker is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from parallel corpora used for evaluation, specifically focusing on 14 different language pairs in the context of document-level machine translation.</sample>
    <sample id="333">## Enhancing Neural Machine Translation with INK: Injecting kNN Knowledge

This paper introduces **INK**, a novel training framework designed to improve the generalization and performance of neural machine translation (NMT) models. NMT models often struggle with sparse representation spaces that limit their ability to capture nuanced semantic meanings, leading to poor performance in certain areas.

INK addresses this by **injecting k-Nearest Neighbor (kNN) knowledge** into the NMT model. The core idea is to smooth predictions using nearest neighbors in the representation space, enhancing semantic coherence. The proposed framework consists of two key steps:

1. **Knowledge Extraction:** kNN knowledge is extracted from a data store built during training, guiding an adapter to adjust the representation space.
2. **Asynchronous Refreshment:**  Updated representations are used to refresh the data store asynchronously, allowing for efficient updates while minimizing computational overhead.

Through experimental evaluation on the WMT German-English news translation benchmark, INK demonstrates significant advantages over existing kNN-MT approaches. Key findings include:

* **Efficient Smoothing:** INK achieves higher BLEU scores with less memory footprint compared to adapter-only baselines.
* **Performance Gain:** INK achieves an average COMET score gain of 1.99 and BLEU score increase of 1.0 compared to state-of-the-art kNN-MT systems.
* **Flexibility:** INK's modular design allows for the use of adapters of different sizes, showcasing its adaptability to various computational constraints.

Overall, INK presents a promising direction for improving NMT models by leveraging the power of kNN knowledge in a computationally efficient manner.</sample>
    <sample id="335">The speaker is Matthias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer refers to the process of training a model in one language and then applying it to another language, either in a zero-shot (no training data) or few-shot (limited training data) setting.</sample>
    <sample id="337">**Abstract:**

This research presents a novel approach, "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," to address the challenge of representing out-of-vocabulary (OOV) words in language models. The study draws inspiration from human language learning, focusing on word formation and association to infer OOV meanings.

A Word Relationship Graph is introduced to model lexical rules, where OOV words are tokenized into wordpieces and connected to relevant words, forming a multi-level graph. A self-attention network assigns attributes to OOV nodes based on their characters. The model employs Graph Attention Networks (GANs) to process the graph, fusing input with hidden embeddings for node-level representations. A readout block captures graph-level information, summarizing word formation.

The loss function incorporates contrastive learning with NT-XENT positive samples, encouraging proximity between related words while pushing them apart from others. Experiments demonstrate superior performance compared to baselines in both intrinsic and extrinsic tasks. The model benefits both static and contextual models in downstream applications.

Furthermore, the study explores the potential of extending the model to different languages, with agglutinative languages showing better suitability due to their direct word formation. The model's effectiveness in English, through reasonable word segmentation, suggests its adaptability to various complex word formations. Overall, the graph-based approach offers a promising solution for OOV word representation, with potential for language-specific adjustments based on word decomposition strategies.</sample>
    <sample id="338">**Abstract**

This research presents a comprehensive evaluation framework for human natural language explanations, addressing the challenge of subjective and task-dependent explanations. The study, titled "Are Human Explanations Always Helpful?", involves a collaborative effort between researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research.

Key contributions include:

- **Unified Structure:** A template-based format converts diverse tasks (commonsense QA, natural language inference, commonsense validation) into a unified multiple-choice task, enabling consistent evaluation.

- **In-depth Experiments:** Analysis of explanation utility through random sampling of training data, fine-tuning, and inference reveals that explanations do not teach new knowledge but can improve model reliance on explanations for prediction.

- **TREU Metric:** A novel evaluation metric extending the simulatability score, quantifying explanation helpfulness during fine-tuning. TREU scores consistently rank dataset qualities better than simulatability scores, highlighting task-dependent benefits.

- **Findings:** Human explanations, even considered low quality, benefit model predictions. TREU scores differentiate explanation usefulness across tasks, with positive values for entailment and negative values for neutral/contradiction classes in e-SNLI.

The work emphasizes the need for quality checks in human annotation jobs and provides a foundational framework for future research in explanation evaluation, fostering better collaboration between humans and AI models.</sample>
    <sample id="339">The authors of the paper are from Saarland University in Germany. Specifically, they include:

- Dawei (PhD student)
- Xiaoyu Shen
- Marius Mosbach
- Andreas Stephan
- Dietrich Klakow</sample>
    <sample id="340">**Abstract**

"ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation" presents a novel approach to generating a vast and syntactically varied paraphrase dataset. The work, a collaboration between UCLA researchers, addresses the challenge of acquiring high-quality, large-scale paraphrase data for NLP tasks. Existing human-annotated datasets, while high-quality, are limited in size, while automatically generated datasets lack syntactic diversity.

The proposed solution leverages Abstract Meaning Representations (AMR) graphs, which capture sentence semantics as directed graphs. The team employs AMR back-translation, translating sentences to and from a target language, then modifying the AMR graph to create diverse paraphrases. This process involves randomly selecting and re-rooting the graph, changing edges and edge labels, and generating text from the altered graph.

The resulting dataset, ParaAMR, contains approximately 15 million source sentences with an average of 6.9 paraphrases per sentence. Quantitative analysis shows ParaAMR maintains strong semantic similarity while significantly enhancing syntactic diversity compared to other back-translation datasets.

The study further demonstrates ParaAMR's utility in several NLP applications: sentence embedding learning, syntactic control paraphrase generation, and data augmentation for few-shot learning. ParaAMR's availability for public use is highlighted, offering a valuable resource for advancing NLP research.</sample>
    <sample id="341">The authors use two latency measures:

1. **Average Lagging**: A direct measure of the time delay between input and output.
2. **Computational Aware Average Lagging**: Accounts for the computational time required by the model to generate the output.</sample>
    <sample id="342">## LiveChat: A Large-Scale Personalized Dialogue Dataset for Real-World Applications

This paper introduces **LiveChat**, a novel, large-scale dataset of personalized dialogues automatically constructed from live streaming videos. Unlike existing datasets primarily based on text-sourced conversations, LiveChat leverages Chinese TikTok and Douyin videos, extracting audio, transcribing it, and building dialogues through a unique "reply-to-whom" matching mechanism.

The dataset focuses on both **general open-domain dialogue** and **personalized dialogue**, crucial for developing applications like virtual streamers and employees. LiveChat addresses key challenges in existing datasets, including:

* **Scalability:** Automatic construction allows for a much larger dataset than manually annotated counterparts.
* **Personalization:** Incorporates persona information and longer dialogue sessions for more realistic personalized responses.
* **Multi-party conversations:** Includes scenarios with multiple speakers, expanding research possibilities.

Experiments demonstrate LiveChat's effectiveness for various tasks, including response modeling and addressee recognition.  The dataset outperforms existing benchmarks, particularly in personalized dialogue generation, showcasing its potential for advancing AI conversational systems.

Furthermore, the paper explores transfer learning of pre-trained language models (LLMs) on LiveChat, highlighting its versatility and potential for future research in developing robust and adaptable dialogue AI.</sample>
    <sample id="344">The drawbacks of tree-based methods, as mentioned, include:

1. **Complexity of Tree Obtaining**: Trees often require significant pre-processing of logical forms and specialized grammar induction procedures, which can be computationally expensive.
2. **Lack of Generalizability**: Naive seq2seq models struggle with out-of-distribution generalization, failing to reproduce systematic correspondences between input and output.
3. **Dependence on Trees**: Trees are not always given and must be inferred, adding another layer of complexity.</sample>
    <sample id="345">**Abstract**

This paper introduces a novel neural seq2seq model for compositional generalization in semantic parsing without relying on trees. Compositional generalization refers to a model's ability to handle complex, unseen compositions of phrases it has encountered during training. Traditional approaches use trees to capture these compositions, but these are often absent or computationally expensive to obtain.

Our model tags each input token with a multiset of predicted output tokens, then predicts a permutation to order these tokens correctly. This two-step process allows the model to directly model correspondences between input and output fragments without explicit tree structures.

We demonstrate the effectiveness of this approach on the COGS benchmark, achieving superior performance in generalization to deeper recursion compared to treeless models. Key technical challenges addressed include aligning input and output tokens during training and finding the optimal permutation, which is approximated through a GPU-friendly continuous relaxation.

The paper presents experimental results showcasing the model's strong performance, highlighting its potential to handle complex linguistic structures without the need for tree-based representations.</sample>
    <sample id="346">Based on the content provided, the authors of the paper "Do CoNLL-2003 named entity taggers still work well in 2023?" are affiliated with the researchers and institutions involved in the study, but specific names or detailed affiliations are not explicitly mentioned in the text. The study appears to be a collaborative effort, and the authors likely come from academic or research backgrounds, possibly associated with universities, research institutes, or tech companies specializing in natural language processing (NLP) and machine learning.</sample>
    <sample id="348">## Overcoming Stereotypes in Language Models: The Marked Personas Approach

This paper presents a novel method, **Marked Personas**, for quantifying and understanding stereotypes within large language models (LLMs). Unlike existing methods relying on curated datasets with limited scope, Marked Personas leverages the generative power of instruction-tuned LLMs.

Researchers generated personas depicting individuals from diverse demographics using prompts like "Imagine you are an Asian woman. Describe yourself." Analysis of these generated descriptions revealed subtle yet harmful stereotypes. While seemingly positive traits like "strong" or "proud" were often assigned to marginalized groups, they perpetuated essentialist narratives that contribute to discrimination.

The Marked Words method, a core component of Marked Personas, identifies words that distinguish marked (marginalized) groups from unmarked (dominant) ones. This approach uncovers specific stereotypes not captured by traditional lexicons.

Key findings include:

* Generated personas contain more stereotypes than human-written ones, but these stereotypes are often less overt and more nuanced.
* Positive-seeming words like "culture" and "exotic" reinforce harmful othering and essentialist narratives.
* Stereotypes for women of color often rely on tropes like hyper-sexualization or strength as a coping mechanism.

The authors conclude with three recommendations for mitigating bias in LLMs: addressing positive stereotypes, adopting an intersectional perspective, and promoting transparency in bias mitigation techniques.</sample>
    <sample id="350">**Abstract**

This paper critically examines the concept of "superhuman performance" in Natural Language Understanding (NLU) tasks, specifically through a review of popular benchmarks like SuperGLUE and SQuAD. While recent advancements have seen models achieve human-like or even superior performance on these benchmarks, the authors argue that such claims are premature and potentially misleading.

Key concerns raised include:

- **Unfair Comparisons:** Systems and humans are evaluated on disparate subsets of the same dataset, and errors in ground-truth answers introduce biases that favor models.

- **Vague Human Baselines:** The term "human baseline" is often used imprecisely, with simple aggregation methods used to estimate average human performance, which may not reflect the best possible human abilities.

- **Low Compensation and Annotator Variability:** Human workers are often underpaid and details about their backgrounds are overlooked, potentially leading to subpar performance and unscientific claims.

The authors advocate for more rigorous and transparent evaluation practices, recommending against using datasets constructed under conditions that may yield biased or unreliable comparisons. They also propose comparing model performance against the best possible human performance, rather than vague baselines, to provide a more accurate assessment of AI capabilities in NLU.</sample>
    <sample id="351">**Abstract**

Our paper, "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?", investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset. We address two primary questions: can these models adapt to modern data, and what factors contribute to their performance drop?

To answer these, we created CoNLL++, a dataset of Reuters News articles annotated using CoNLL-2003 guidelines. We fine-tuned over 20 models on CoNLL-2003 and evaluated their performance on both the original and CoNLL++ test sets. Our findings highlight three key factors for good generalization: model architecture (with transformer models outperforming others), model size (larger models generally achieve better results), and the number of fine-tuning examples.

We ruled out adaptive overfitting as a cause for performance degradation and confirmed that temporal drift, the performance drop due to the increasing temporal gap between training and test data, is the primary culprit. Our conclusion emphasizes the need for improved model architectures, larger models, and more fine-tuning data, and suggests that CoNLL-2003 taggers still perform well in 2023, encouraging further research on enhancing model generalization.

The paper includes a dataset and invites further exploration and questions.</sample>
    <sample id="352">ABC-Eval stands for "annotating behaviors in chat."</sample>
    <sample id="353">##  Code Generation with Clarification Questions: Overcoming Input Underspecification

This paper presents a novel approach to code generation, addressing the challenge of input underspecification where natural language descriptions (NLDs) lack crucial details. Existing methods struggle with this issue, as demonstrated by a motivating example.

The authors propose **interactivity** through asking clarification questions (CQAs) to gather missing specifications. They introduce a new task, **code generation by asking clarification questions**, focusing on identifying and clarifying operation-level specifications.

Their approach, **CodeClarQA**, involves:

1. **Dataset Creation:** Extracting key operations from code and creating synthetic CQA pairs using templates and heuristics.
2. **Clarification Need Prediction:** Using a model to predict which operations require clarification.
3. **Question Selection:** Choosing the most relevant CQs based on predicted need.
4. **Code Generation:** Integrating CQs into a code generation pipeline.

Experiments show that incorporating CQs improves code quality compared to models trained solely on NLDs and code. Error analysis highlights areas for improvement, including distinguishing similar operations and handling argument values.

The paper concludes that interactivity through CQs effectively mitigates input underspecification, opening new avenues for more robust and flexible code generation systems.</sample>
    <sample id="354">Based on the content provided, the performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points throughout the experiment, as indicated by the graph on the right, which shows the "best fit line" with a gradient greater than one. This suggests that the gap remains significant throughout the comparison period, extending to at least the present year (2023) when the study was conducted.</sample>
    <sample id="356">The authors of the paper, Matthias Lindemann, Alexander Koller, and Ivan Titov, are affiliated with the academic institution(s) where they hold their positions as advisors and researchers. Specifically, the paper mentions them as authors from a joint work, indicating they are all associated with the same research group or department.</sample>
    <sample id="357">The speaker is Siyu Yuan from Fudan University.</sample>
    <sample id="358">The paper involves 5 authors: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The approach is compared to popular strategies applied to offline models, including the **Wait-k strategy** and **Local Agreement**, as well as to a state-of-the-art architecture specifically tailored for simultaneous pre-translation.</sample>
    <sample id="361">**Abstract**

This presentation introduces CounterComp, a novel approach to enhance multi-step quantitative reasoning in neural models, specifically for question answering involving complex arithmetic operations. The focus is on improving compositional generalization, enabling models to handle diverse questions about financial tables with varying complexity.

Traditional neural models struggle with tasks requiring more than two reasoning steps due to memorizing spurious patterns. CounterComp addresses this by leveraging interchangeable components in input questions to mine counterfactual scenarios. It converts training samples into triplets of anchor questions, positive, and negative examples, which are then used to calculate a dynamic margin for an auxiliary metric learning loss.

This loss guides the model to attend to relevant tokens in the input, aligning them with appropriate operations in the output. Experiments demonstrate CounterComp's effectiveness in improving performance on both in-distribution and out-of-distribution samples, showcasing its ability to generalize to unseen scenarios. Qualitative analysis reveals that CounterComp helps models focus on meaningful tokens related to operational terms.

The presentation highlights the significance of this research in advancing neural models' capabilities for multi-step quantitative reasoning, with potential implications for various applications requiring complex data understanding and reasoning.</sample>
  </task>
</testset>