<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬虫数据，其中包括政治新闻媒体，如《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等。</sample>
    <sample id="1">这篇论文的作者所属机构是麦吉尔大学、Mila和微软研究。</sample>
    <sample id="2">Tu Yi from Ant Group presents a paper on Visually-rich Document Understanding (VrDU), focusing on understanding various document types like forms, receipts, and posters. The paper introduces a novel pre-trained model, LayoutMask, addressing issues in existing document pre-training models related to reading order. Traditional models use global 1D positions to represent token order, which can be problematic. LayoutMask proposes using local 1D positions, which do not provide cross-segment orders, encouraging the model to infer global reading order by integrating 1D position, 2D position, and semantic information, enhancing text-layout interactions.

LayoutMask introduces two novel masking strategies for the Masked Language Modeling task: Whole Word Masking and Layout-Aware Masking. Whole Word Masking masks at the word level, challenging the model to use more context for prediction, thus promoting text-layout interactions. Layout-Aware Masking increases the probability of masking the first and last words of each segment, encouraging the model to learn cross-segment orders by finding contexts in adjacent segments.

Additionally, LayoutMask features a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions. This task is akin to a cloze test, requiring the model to use semantic relations and 2D position clues to infer correct word placements, promoting text-layout interactions and better layout representation learning.

Experiments show that using Local-1D positions outperforms Global-1D on FUNSD and SROIE datasets, with a slight underperformance on CORD. The performance gap is mainly due to the entity "Total," which is challenging to recognize using ordinary reading order implied by Global-1D. Local-1D is more adaptive to such cases, as demonstrated in the SROIE dataset examples. For further details, the paper and posters are recommended.</sample>
    <sample id="3">欢迎参加我们关于 DEPLAIN 的新语料库的介绍，这是一种用于德语文本识别的新工具，适用于文档级和句子级。我是 Regina Stodden，我将引导您了解本次演示的第一部分。首先，我们定义一下文本简化。文本简化是一种改进特定目标群体（如阅读困难者或非母语人士）对文本理解的过程。为了训练文本简化模型，我们需要平行的文本对，例如文档或句子。这里的例子展示了一个复杂的德语句子及其简化语言的翻译。为了简化句子，可以采用多种技术，如词汇替换、从句删除、重排序或插入词语。

我们现在提出了我们的新语料库 DEPLAIN，因为近年来现有语料库存在一些问题。例如，这些语料库太小，无法用于训练文本简化模型。另外，近年来提出的三个模型都是自动对齐的，这意味着它们的对齐可能存在错误。因此，我们提出了新的语料库 DEPLAIN，分为两个子语料库：DEPLAIN-apa 和 DEPLAIN-web。DEPLAIN-apa 基于新闻文本，我们手动对齐了483个文档，得到大约13,000个平行句子对。DEPLAIN-web 包括不同领域的文本，我们对这750个文档进行了手动和自动对齐，总共得到30,450个句子对。

我们对这些句子对进行了更深入的分析，例如简化类型。例如，圣经文本的简化程度比新闻文本或语言学习文本更强。在各个层面上，如词汇简化、结构简化和整体简化水平，我们的 DEPLAIN 语料库展示了多种不同的简化转换。例如，在 DEPLAIN-apa 语料库中，重排序和词语添加更多，而在 DEPLAIN-web 语料库中，重新表达更多。

现在，我是 Omar，我将讨论我们的数据集 DEPLAIN 的用例。首先，我们可以评估自动对齐方法。近年来，有许多对齐方法，通常用于机器翻译，即从两个不同语言的平行文档中提取句子对齐。但在我们的用例中，我们试图从两个平行文档中提取句子对齐，这些文档使用相同的语言，内容相同，但复杂程度不同。现在，由于我们有手动对齐的 DEPLAIN 语料库，我们可以将这些句子作为金标准对齐来评估一些提出的对齐方法。我们对这些方法进行了一些调整，并在论文中发布了这些调整和运行实验的代码。最终，我们得出结论，德语文本简化中最佳的自动对齐方法是 MASSalign 方法。您也可以在论文中找到在自己文档上运行此方法的代码。

我们论文中的第二个用例是自动文本简化，通过微调语言模型从复杂的输入文本生成简化文本。我们微调了两个不同的模型：微调 long-mBART 以生成文档级简化，以及微调基础 mBART 以生成句子级简化。您也可以在论文中找到所有检查点，并查看我们实验的分数和评估指标的更多细节。我们得出结论，基本微调可以获得比基线分数更好的分数，并将这些结果作为未来自动文本简化问题的基准。谢谢大家的关注，希望在会议上见到大家。谢谢。</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan introduces the work "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a collaborative effort with Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. The paper presents a novel approach called many-to-many summarization, which integrates multilingual and cross-lingual summarization into a unified model capable of processing documents in any source language and generating summaries in any target language. This approach aims to enhance the model's ability to transfer task knowledge across different languages more effectively than previous methods.

The team conducted preliminary studies comparing multilingual summarization, cross-lingual summarization, and many-to-many summarization. They found that many-to-many summarization offers superior knowledge transfer across languages. To illustrate the differences, they used the WikiLingua dataset, which includes English, French, Hindi, Chinese, Thai, and Turkish samples. Four models were trained using the mBART-50 backbone: mBART ONE (separate models for each language direction), mBART U-CLS (a unified model for all cross-lingual samples), mBART MLS (a unified model for all monolingual samples), and mBART Many-to-Many Summarization (trained and evaluated in all directions).

The results showed that the many-to-many model outperformed the others in transferring task knowledge across languages. Additionally, the team proposed PISCES, a pre-trained many-to-many summarization model, which underwent a three-stage pre-training process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. PISCES demonstrated superior performance over baselines like mBART-50 and mT5, as confirmed by experimental results and ablation studies. Human studies further validated PISCES's effectiveness. The paper encourages readers to explore these findings in detail.</sample>
    <sample id="7">根据所给的英文内容，CoNLL-2003 标注器在 2023 年仍然有效。</sample>
    <sample id="8">提出的人工评估方法ABC-Eval的新颖之处在于，它通过明确注释模型回复中是否表现出某些行为（如回应无关信息、自相矛盾等），来减少人工评估的主观性。这种方法专注于评估对话质量的多个维度，通过测量模型在不同主题错误上的表现，提供了更可靠和预测性更强的评估结果。与传统的Likert评分和对话级别的比较方法相比，ABC-Eval能够更好地捕捉对话质量的独特方面，并且其标签在可靠性和信息量上都表现出色。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于额外的干净验证集。这些方法通常需要干净的验证样本来进行模型选择，以确保模型能够在干净的测试集上表现良好。如果没有干净的验证样本，模型的性能会显著下降，无法有效泛化。</sample>
    <sample id="10">为了提高分数，可以采取以下措施：

1. **增强背景知识获取**：改进模型获取和处理背景知识的能力，使其能够更好地理解和利用相关信息。

2. **改进模型架构**：使用更先进的模型架构或优化现有模型，以提高其理解和生成间接指代表达的能力。

3. **数据增强**：扩展和多样化训练数据集，包括更多样化的间接指代表达和背景知识。

4. **跨域训练**：通过跨域训练提高模型的泛化能力，使其能够在不同领域中更好地应用。

5. **上下文理解**：改进模型对上下文的理解，使其能够更准确地捕捉和解释对话中的细微差别。

6. **多模态学习**：结合文本和其他模态（如音频、图像）的信息，提高模型对实体的理解能力。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented a study on humor understanding benchmarks using The New Yorker Caption Contest. The study, a collaborative effort with several universities and organizations, explores whether large language models like ChatGPT and Google's PaLM can genuinely understand humor. While these models can generate and explain jokes, their understanding is questionable. For instance, when asked to create a knock-knock joke involving a pineapple, ChatGPT's attempt lacked a coherent pun, highlighting potential gaps in its humor comprehension.

To systematically evaluate humor understanding, the study utilized The New Yorker Caption Contest, where readers submit captions for cartoons. The contest involves tasks like matching captions to cartoons, ranking caption quality, and generating explanations for why jokes are funny. The research team annotated over 700 cartoons and collected explanations for more than 650 jokes.

In the matching task, a CLIP model fine-tuned on the annotated corpus achieved 62% accuracy, significantly lower than human performance at 94%. When language models like GPT-4 were tested with human-authored image descriptions, they still lagged behind humans in matching and quality ranking tasks. In explanation generation, GPT-4's explanations were often less accurate than human ones, as shown in blind A/B studies where human explanations were preferred in over two-thirds of cases.

The study highlights the challenges large language models face in truly understanding humor, despite their ability to generate and explain jokes. The researchers are excited to share their dataset and encourage further exploration in this area.</sample>
    <sample id="12">这篇论文有五位作者：Dawei、Xiaoyu Shen、Marius Mosbach、Andreas Stephan、Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem presents his work on "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted in Professor Roy Schwartz's lab at the Hebrew University in Jerusalem. The study focuses on adaptive inference, a technique to reduce the inference time of large language models by using low-capacity models for simpler samples, thus lowering average inference costs in terms of time and money. The two primary adaptive inference methods discussed are Multi Model and Early Exit.

In the Multi Model approach, multiple models are stored and trained separately on the entire dataset. During inference, these models are run sequentially until a classifier decides to halt the process. This method is versatile and easily extendable but incurs high storage costs and overhead, as samples must pass through all previous models before reaching the final one.

The Early Exit method involves training multiple classifiers at intermediate layers of a single model. During inference, a sample is processed until a classifier decides to stop further computation, saving resources. This method is memory efficient and incurs no overhead, but it suffers from shared model parameters among classifiers, leading to potential performance degradation due to conflicting gradients. These gradients, arising from each classifier optimizing its own goal, can interfere with each other and harm overall performance.

To test this hypothesis, Rotem compared Early Exit classifiers with separate Multi Model classifiers, both based on truncated versions of the BERT model. The Multi Model classifiers outperformed Early Exit classifiers by an average of 2.3%, with the largest gap of 5.2% for the earliest classifiers. The speed/accuracy trade-off analysis showed that Multi Model excelled at high inference speeds, while Early Exit performed better with later classifiers due to Multi Model's overhead.

To address the conflicting gradients issue, Rotem introduces SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for Early Exit architectures. SWEET trains each layer to receive updates only from the following classifier, eliminating conflicting gradients. Results show that SWEET narrows the performance gap between Early Exit and Multi Model, although some later classifiers are negatively affected. In terms of speed/accuracy trade-off, SWEET outperforms both methods at fast speeds and consistently outperforms them for BERT-Large across the entire curve.

The key takeaways from the study are the identification of conflicting gradients in Early Exit training, the first fair comparison of Early Exit and Multi Model methods, and the introduction of the SWEET method, which opens avenues for future research and fine-tuning algorithms tailored to Early Exit architectures.</sample>
    <sample id="14">你好，我叫亚当·普热皮尔科夫斯基，这次演讲的主题是协调的依存结构。你可能知道，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存语法中，协调结构“Lisa, Bart, and Maggie”的依存结构是不对称的，即第一个连词“Lisa”是整个协调结构的头。类似的方法也被伊戈尔·梅尔丘克的意义文本理论所采用，其中整个协调结构也由第一个连词作为头。这两种方法都是不对称的，它们将一个连词单独出来。还有其他不对称的协调结构方法，比如布拉格依存树库中的连词头方法，其中协调结构由连词作为头，从连词到所有连词都有依存关系。最后，还有霍顿的词法语法中使用的多头方法，其中所有连词都是协调结构的头，从“loves”到所有连词分别有依存关系：Lisa, Bart, 和 Maggie。

本文的目标是为协调的对称结构提供一个新的论据，反对协调的不对称结构。论据基于依存长度最小化原则，我将通过以下例子来解释。在英语中，直接宾语通常倾向于靠近动词，而附加语可以更远。例如，“Marge read it yesterday”是可以接受的，因为直接宾语靠近动词，而“Marge read yesterday it”则不太好，因为在动词和直接宾语之间有一个附加语：“yesterday”。然而，当直接宾语非常长时，这种效果可以得到缓解，因为它可以移动到附加语之后的位置。例如，“Marge read this absolutely fascinating book about bees yesterday”和“Marge read yesterday this absolutely fascinating book about bees”都是可以接受的。这是因为尽管这个句子违反了直接宾语应该紧跟动词的一般语法原则，但它满足了依存长度最小化原则，即更短的依存关系更受偏好。

我们从增强版的Penn Treebank中提取了各种关于协调的统计数据，并在论文“为什么不使用通用依存语法”中确认了左连词倾向于更短的观察结果，例如“salt and pepper”而不是“pepper and salt”，以音节为单位测量。此外，我们还观察到这种倾向随着两个连词长度差异的增加而增强。当两个连词长度差异增大时，较短的连词更倾向于成为第一个，这种倾向更强。但本文的新观察是，这种倾向只在左侧有主宾关系或主宾关系缺失时出现。例如，在“I saw Bart and Lisa”中，主宾关系在左侧，在“Homer came and sneezed”中，主宾关系缺失。在这些情况下，左侧的连词倾向于更短，尤其是当两个连词长度差异最大时。然而，当主宾关系在右侧时，如“laughed”与“Ted and Ned”的协调中，这种效果消失。我们通过测量长度（字符、音节和词）来展示这一点。当主宾关系在左侧时，左侧连词倾向于更短的趋势随着两个连词长度差异的增加而增强，这也适用于没有主宾关系的情况，如句子协调。但当主宾关系在右侧时，这种趋势消失。我们在论文中展示了这一点，作为反对协调的不对称结构的论据，并支持对称结构。请参阅论文以获取完整的论据，并在展板会议上与我们交流。谢谢。</sample>
    <sample id="15">这篇论文有三位作者：Matthias Lindemann、Alexander Koller 和 Ivan Titov。</sample>
    <sample id="16">根据所给的内容，Bible文本的简化程度更大，比如新闻文本或语言学习文本的简化程度要强。</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces their work on multimodal relation extraction (MRE), which addresses the challenge of extracting semantic relations between entities in texts that are often accompanied by visual data, especially in social media contexts. Traditional relation extraction focuses solely on text, which can be insufficient for understanding ambiguous or context-dependent words. MRE incorporates visual evidence to enhance understanding, such as using images of a "Bachelor," "Gown," and "Cap" to infer that JFK graduated from Harvard.

However, challenges remain, such as internal-information over-utilization, where only parts of the text and visual data are useful, and external-information under-exploitation, where additional context like topic information is needed. To tackle these issues, the proposed method employs a Graph Information Bottleneck (GIB) principle for fine-grained information pruning and integrates multimodal topic information to enrich context.

The framework consists of five parts: representing text and images with visual and textual scene graphs, merging them into a unified cross-modal graph (CMG), refining CMG through node and edge adjustments guided by GIB, and enriching features with multimodal topic information using an attention mechanism. Experiments on a widely used MRE dataset show that the proposed method outperforms text-based methods and other multimodal baselines.

An ablation study reveals that both information screening and compensating improve performance, with scene graphs aiding structural modeling. The effectiveness of internal-information screening and external-information exploiting varies with text-vision relevance: higher relevance benefits more from screening to reduce redundancy, while lower relevance gains from external information to fill gaps. The novel approach of simultaneous information subtraction and addition, guided by GIB and enriched with latent multimodal topic features, significantly improves performance over existing models.</sample>
    <sample id="18">偏好较短左并列词的示例包括“salt and pepper”而不是“pepper and salt”，以及在句子“Homer came and sneezed”中，当主语“Homer”作为左侧的施事者时，以及在“我看到了Bart和Lisa”中，当“我”作为左侧的施事者时。</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work titled "A Survey for Efficient Open Domain Question Answering" at ACL 2023. The paper addresses the challenges of open-domain question answering (QA) systems, which typically use a two-stage model involving retrieval and reading. The retrieval stage uses encoders to search a large Wikipedia corpus for relevant evidence, while the reading stage processes this evidence to generate answers. The paper highlights challenges such as the large size of the Wikipedia corpus, the substantial memory requirements for indexing, and the computational demands of multiple large language models, which hinder real-time applications and deployment on resource-constrained devices.

The motivation behind the work is to develop efficient open-domain QA systems with reduced memory costs, faster inference, and comparable performance. The paper explores various techniques to achieve these goals, including one-stage frameworks like retrieval-only and generator-only systems. Retrieval-only systems directly retrieve answers from an index, while generator-only systems generate answers without an index. The paper discusses efficient tactics such as approximate nearest neighbor search for faster evidence retrieval, adaptive computation for skipping less relevant context reading, and methods like document filtering and embedding compression to reduce index size. Additionally, it suggests using lightweight models, parameter sharing, or designing fewer models to reduce model size.

The paper compares existing open-domain QA models, noting that retrieval and reader systems offer a balanced trade-off between speed, memory, and performance. Retrieval-only systems, while fast, require large indexes, whereas generator-only systems, though index-free, tend to be large and less performant. The paper concludes with insights on optimizing resource-limited systems by reducing index size or model size and suggests retrieval-only systems for real-time feedback and retrieval-reader systems for balanced trade-offs.

Future work includes exploring deployment on low-power devices and considering additional evaluation metrics for open-domain QA systems. Zhang Qin thanks the audience for their attention at the end of the presentation.</sample>
    <sample id="20">Yes, you can use these models for your research. All the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa 中包含新闻文本。</sample>
    <sample id="22">三个因素有助于良好的泛化：1) 模型架构，特别是转换器模型；2) 模型大小，通常更大的模型会导致更好的泛化；3) 更多的微调样本数量。</sample>
    <sample id="23">Dan Garrette discusses efforts to enhance text rendering in text-to-image models, focusing on the Imagen model. Imagen uses a T5-XXL encoder to process text into subword IDs, which are then input to a diffusion model to generate images. Despite its success in creating complex images, Imagen struggles with rendering text, especially simple words. This issue stems from T5's SentencePiece tokenization, which breaks text into subword chunks, making it difficult for the model to accurately spell words. Experiments show that even large T5 models have low spelling accuracy, while larger PaLM models perform better but are impractical due to their size. ByT5, which uses byte-level tokenization, excels in spelling as it directly accesses character information. To improve Imagen's text rendering, Garrette suggests augmenting it with a ByT5-small model, which significantly enhances spelling accuracy with minimal parameter increase. However, the diffusion model can still introduce errors. The paper introduces the WikiSpell benchmark for text models, the DrawText benchmark for text-to-image models, and a strategy to improve spelling by incorporating character-aware models.</sample>
    <sample id="24">左并列词是否更短可以通过以下方式衡量：  
1. **字符数**：计算左右并列词的字符长度。
2. **音节数**：计算左右并列词的音节数。
3. **词数**：计算左右并列词的词数。  

在研究中，特别关注词数的差异，观察左并列词在不同情况下（如主语在左侧或不存在）是否倾向于更短。</sample>
    <sample id="25">为了研究支配词位置对协调结构的影响，可以设计以下实验：

1. **数据收集**：
   - 从语料库中提取包含协调结构的句子，如Enhanced Penn Treebank。
   - 确保收集的句子涵盖支配词在左侧、右侧和不存在的情况。

2. **变量定义**：
   - **独立变量**：支配词的位置（左侧、右侧、不存在）。
   - **依赖变量**：协调结构中左侧和右侧成分的长度（以字符、音节或词为单位）。

3. **实验设计**：
   - 将句子分为三组，根据支配词的位置进行分类。
   - 计算每个协调结构中左侧和右侧成分的长度。

4. **数据分析**：
   - 比较不同支配词位置下左侧成分长度的分布。
   - 使用统计方法（如方差分析）检验支配词位置对左侧成分长度的影响。

5. **结果解释**：
   - 分析支配词在左侧时，左侧成分倾向于更短的趋势。
   - 检查支配词在右侧或不存在时，这一趋势是否消失。

6. **结论**：
   - 根据结果，评估支配词位置对协调结构对称性的影响，支持或反对不对称结构的理论。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳，表现不比随机猜测好多少。在仅使用43个例子训练的情况下，分类器的表现几乎不如随机猜测。</sample>
    <sample id="27">根据所给的内容，这篇论文的作者是Shangbin。</sample>
    <sample id="28">Bob 和 Alice。</sample>
    <sample id="29">在形式和词汇连贯性这两个话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="30">The paper introduces "LLM-Blender," an ensemble learning framework designed to enhance the performance of large language models (LLMs) by leveraging pairwise ranking and generative fusion. Developed by a team from AI2 and USC, the framework addresses the variability in model performance across different input examples. While some models like Vicuna may excel in average performance, they are not always the best choice for every specific input. LLM-Blender proposes using multiple models to generate outputs for a given input, selecting the best candidates through a pairwise ranking process.

The framework operates in two stages. First, it runs multiple models on an input to generate outputs, which are then ranked using a module called PairRanker. This module employs a cross-attention mechanism, such as RoBERTa, to compare pairs of outputs alongside the input, identifying subtle differences and determining which candidate is superior. The results are aggregated into a comparison matrix, from which the top candidates are selected.

In the second stage, the top-ranked candidates are fed into a sequence-to-sequence model for generative fusion, producing the final output. PairRanker's key innovation is its pairwise comparison approach, which contrasts with methods that evaluate candidates individually. This approach allows for a more nuanced analysis of candidate quality.

To evaluate ensemble learning frameworks, the authors created the MixInstruct dataset, comprising existing instruction datasets and outputs from 11 open-source LLMs. The framework's performance is assessed using metrics like BERTScore, BLUERT, BARTScore, and evaluations by ChatGPT. Results demonstrate that LLM-Blender outperforms top models like Open Assistant and Vicuna in a significant percentage of examples, highlighting its effectiveness.

The paper concludes by emphasizing LLM-Blender's simplicity and efficacy, noting its potential to improve LLM performance through its PairRanker and GenFuser modules. The authors also release a unified codebase for evaluation and future research, along with the MixInstruct dataset.</sample>
    <sample id="31">论文中没有提到作者所属的机构。</sample>
    <sample id="33">引入的框架NLPositionality通过以下步骤量化立场：

1. **重新注释数据集**：使用多样化的注释者重新注释数据集，以获取丰富的人口统计数据。这是因为原始数据集的注释者人口统计信息通常不被收集和分享。

2. **比较注释和模型**：将按人口统计分类的注释与现有的模型和数据集进行比较，使用皮尔逊相关系数（Pearson's R correlation score）来量化。

这种方法不同于注释者不一致性文献，因为它比较的是终端用户与模型和数据集之间的预测和标签，而不仅仅是注释者之间的一致性或建模注释者分布。</sample>
    <sample id="34">Marcos Treviso introduces "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation," developed in collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. CREST combines selective rationalization and counterfactual text generation to enhance interpretability and decision-making in text classifiers. The framework consists of two main components: a rationalizer and a counterfactual generator. The rationalizer identifies meaningful parts of the input text (rationale) and uses a masker to highlight these parts. The counterfactual generator then creates variations of the input by masking the rationale and prepending the correct label, allowing a masked language model to fill in new tokens, resulting in counterfactual examples.

To evaluate CREST, human evaluations on IMDB and SNLI datasets showed that while manual counterfactuals were more valid and natural, CREST-generated counterfactuals were superior to those from MiCE. CREST's counterfactuals were used for data augmentation and rationalization, leading to improved model performance. Experiments on IMDB demonstrated that CREST-Rationalization outperformed models trained only on factual examples and those augmented with human counterfactuals, especially in out-of-domain datasets.

The interpretability of CREST's rationales was assessed through plausibility, forward simulability, and a new metric, counterfactual simulability, which measures the ability of rationales to alter classifier decisions when guided by contrastive edits. CREST-Rationalization produced more plausible and higher counterfactual simulability rationales compared to other methods. Overall, CREST effectively generates valid, fluent, and diverse counterfactuals, enhancing model training and providing interpretable explanations by focusing on contrasting input parts.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires and colleagues addresses the challenges and advantages of multilingual machine translation (MMT). MMT offers scalability, speed, and improved performance for low-resource languages by using a single model for multiple languages. However, it also faces limitations in capacity per language, which can be mitigated by increasing model size, though this complicates training and slows inference.

The authors propose Language-Specific Layers (LSLs) to enhance capacity per language without increasing inference costs. LSLs involve having a dedicated transformer layer for each language, activated during inference based on the source or target language. This approach maintains constant inference costs by only using the relevant sublayer.

The paper explores optimal placement of LSLs within the model. Initial experiments showed limited benefits from placing LSLs in the decoder, so the focus shifted to the encoder. To determine the best placement, the authors trained a model with shared, source, and target weights for each encoder layer. By analyzing these weights, they identified patterns indicating where LSLs should be placed. The final architecture included shared layers, source-specific LSLs, and target-specific LSLs, with the placement determined by the largest weight for each layer.

Experiments were conducted on WMT21 news translation data for 10 languages, including European, Asian, and Swahili. The models were evaluated using metrics like chrF, spBLEU, and COMET. Results showed that the learned architecture with LSLs significantly outperformed both language adapters and the largest baseline model, while also being faster at inference. Improvements were particularly notable for low-resource languages, with statistically significant gains in 84 out of 90 translation directions. The paper encourages further exploration of shared or separate decoders, ablation studies, and different metrics in the full paper and poster session.</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果表明这些提示也能够揭示出种族刻板印象。这使得研究者能够直接比较生成的人格化描述与人类写的回应。</sample>
    <sample id="38">此研究使用了来自“enhanced version of the Penn Treebank”的数据。</sample>
    <sample id="39">这篇论文有一位作者，名叫Adam Przepiórkowski。</sample>
    <sample id="40">与认知失调密切相关的任务包括：

1. **主题独立的认知失调立场分类**：这是一个任务，用于确定两个来自不同人的辩论陈述是否在一致或不一致，不考虑主题。

2. **PDTB中的二元分类（扩展和比较类）**：这涉及对PDTB中的扩展和比较类进行二元分类，这两个类与认知失调和和谐的概念密切相关。</sample>
    <sample id="41">The paper introduces "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives," a collaborative project between the Natural Language Processing Lab at EPFL University and Sony Group Corporation. The work addresses the challenge of sustaining coherent and engaging narratives by understanding personas in natural language processing systems. PeaCoK is a Persona-grounded Commonsense Knowledge Graph designed to represent world-level persona knowledge at scale, containing about 3,800 personas and 40,000 distinctive attributes, forming approximately 100,000 personal inferences or facts. The graph highlights rich interconnections, with 9,200 attributes linked to multiple personas.

PeaCoK was developed in three steps: selecting personas from existing commonsense graphs, inducing attributes from commonsense knowledge graphs and pre-trained language models, and crowdsourcing relation annotations using a joint human-AI majority voting scheme. This approach achieved an average 87% accuracy in F1 scores, with AI (InstructGPT-3) mediating human annotator disagreements efficiently.

The study explores whether PeaCoK can enhance language models' ability to learn and generalize persona knowledge. A BART-based common knowledge generator, trained on PeaCoK, was compared to large-scale pre-trained models like GPT-3 and GPT-3.5. The results showed that Comet-BART outperformed these baselines in natural language generation metrics and human evaluations, indicating PeaCoK's effectiveness as a reliable persona knowledge base.

Further, the paper investigates the impact of PeaCoK on downstream narrative modeling, specifically in persona-grounded dialogue generation using the ConvAI2 PersonaChat dataset. A knowledge linker retrieves relevant PeaCoK facts to augment speaker profiles, converting them into natural language statements. The augmented models, compared to a baseline P²Bot model, demonstrated improved dialogue generation in fluency, consistency, engagement, and persona expression. Human evaluations showed that PeaCoK's persona-centric knowledge had a more positive impact than general social commonsense knowledge, with better results when speakers shared more common attributes.

In summary, PeaCoK offers a comprehensive, high-quality persona knowledge graph that enhances narrative modeling and knowledge generation capabilities, highlighting the importance of interconnected world persona knowledge in creating consistent and engaging narratives. The paper and its resources are publicly available on the lab's website.</sample>
    <sample id="42">论文中没有提到作者的数量。</sample>
    <sample id="43">论文中没有提到具体的作者数量。</sample>
    <sample id="44">NLPositionality框架与以前的研究不同之处在于，它通过重新注释数据集来比较多样化的注释者与现有的数据集和模型。这与注释者不一致性文献不同，后者通常只关注注释者之间的一致性或建模注释者分布。NLPositionality通过使用Pearson's R相关分数，将注释者的注释按人口统计数据与模型和数据集进行比较，从而将终端用户与模型和数据集进行对比，而不仅仅是注释者之间的一致性。此外，它利用了Lab in the Wild平台，以招募来自87个国家的1000多名注释者，从而获得了更多样化和高质量的数据。</sample>
    <sample id="45">在三个比较设置中，与刻板词汇的重叠最多的是生成的人物描述。研究发现，生成的人物描述中包含了更多的刻板词汇，而人类写的描述则有更广泛的词汇分布。</sample>
    <sample id="46">比较了DeepL和Google Translate。</sample>
    <sample id="47">你好，我是华盛顿大学的博士生Shangbin。今天我将介绍我们的研究《从预训练数据到语言模型再到下游任务：追踪导致不公平NLP模型的政治偏见的轨迹》。语言模型是通过大规模网络爬取数据训练的。政治新闻媒体在预训练数据中得到了很好的覆盖。根据C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型应用带来了双重效应。一方面，它们能够从多元化的视角学习，庆祝民主和思想的多样性。另一方面，这些不同的政治观点本质上是社会偏见，可能导致下游任务应用中的潜在公平问题。因此，我们提出了研究政治偏见传播管道的问题，从预训练数据到语言模型再到下游任务，具体提出以下问题：首先，如何评估语言模型的政治倾向，以及预训练数据在这些政治偏见中可能扮演的角色？其次，不同政治倾向的语言模型在下游任务中的实际表现如何，是否会导致NLP应用中的公平问题？

具体来说，我们首先提出使用不同的提示格式来提示语言模型，例如使用政治问卷调查，如政治会议测试。这确保了我们的自动评估深植于政治科学文献中。一些初步结果表明，首先，语言模型确实具有不同的政治倾向。它们占据了政治舞台的所有四个象限。我们还可以看到，GPT-4是所有语言模型中最自由主义的，而GPT系列总体上比BART系列及其变体更具社会自由主义倾向。其次，我们旨在调查语言模型的政治偏见有多大程度上是从训练数据中获得的。我们可以通过进一步在6个不同政治倾向的语料库上预训练语言模型来进行控制实验，这些语料库分为新闻和社交媒体，进一步分为其政治倾向。通过进一步在这些政治倾向的语料库上预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，进一步在左倾的Reddit语料库上预训练RoBERTa，我们可以看到其政治偏见在自由主义方面有显著的转变。我们还尝试调查语言模型是否能够捕捉现代社会中普遍存在的极化。我们将预训练语料库分为第45任美国总统之前和之后，分别对这两个不同时间段的语料库进行预训练。我们可以看到，语言模型在2017年之后的政治倾向通常更远离中心。这表明语言模型也能够捕捉社会中的极化。

最后，我们评估了在不同政治倾向的语言模型上的下游任务，如仇恨言论检测和虚假新闻检测，这些NLP应用通常涉及语言模型，并可能具有重大影响。我们看到，如果我们分析每个类别的性能，即将性能分为不同的人口统计或新闻媒体的政治倾向，我们可以看到一种模式。例如，在仇恨言论检测中，左倾的语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对社会中更有权势的群体的仇恨言论方面表现较差。反之亦然，右倾的语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+及其他少数群体的仇恨言论方面表现较差。在虚假新闻检测中，我们也看到类似的趋势，即左倾的语言模型在检测与其相反政治倾向的误导信息方面表现更好，反之亦然。我们还展示了许多定性示例，以查看不同政治倾向的语言模型在仇恨言论和误导信息示例上给出了不同的预测，这些示例基于其社会类别。附录中有更多示例，进一步强调这表明语言模型的政治偏见存在非常紧迫的公平问题。例如，如果右倾的语言模型被微调用于仇恨言论或误导信息等任务，并部署到流行的社交媒体平台，这意味着持有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会在没有任何控制的情况下肆虐。这为我们承认和解决由语言模型政治倾向导致的公平问题发出了警钟。

我们还希望强调我们揭示了语言模型政治偏见的独特困境。这就像在Scylla和Charybdis之间。如果我们不对语言模型的训练数据中的政治观点进行清洗，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们试图以某种方式进行清洗，我们也会冒着审查或排斥的风险。而确定什么是真正中立并应保留在语言监控数据中是非常困难的。这就像电车难题。好的，我今天就讲到这里了。谢谢你们的时间。</sample>
    <sample id="48">论文的作者数量未在提供的内容中明确说明。</sample>
    <sample id="49">MPP 评估最多涵盖了 1024 个词元的上下文长度。</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text simplification, addressing issues with existing corpora that are too small or error-prone due to automatic alignment. DEPLAIN is divided into two subcorpora: DEPLAIN-apa, based on manually aligned news texts with 13,000 sentence pairs, and DEPLAIN-web, covering various domains with 30,450 sentence pairs, aligned both manually and automatically. The corpus showcases diverse simplification techniques, such as lexical substitution and clause deletion, with varying levels of simplification across different text types. DEPLAIN's applications include evaluating automatic alignment methods, with MASSalign identified as the best for German text simplification. Additionally, the corpus supports automatic text simplification by fine-tuning language models like long-mBART for document-level and base mBART for sentence-level simplifications, providing a new benchmark for future research.</sample>
    <sample id="51">他们的数据集中包含三个领域：音乐、书籍和食谱。</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences. It is a concept widely used in critical studies, specifically in feminist and queer academic spaces, and can influence the research process and its outcomes by affecting the decisions that researchers make.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha, a Computer Science PhD candidate at Stony Brook University, presents their work on "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. The study focuses on cognitive dissonance, defined as the inconsistency between beliefs or actions, such as a person acknowledging the dangers of smoking but continuing to smoke. This phenomenon is rare in language, making it challenging to study but crucial for understanding disagreement, belief trends, mental health, extremism, and decision-making processes.

The research aims to create a cognitive dissonance resource by annotating dissonance relations in large-scale data. Using a dissonance-first approach, tweets were parsed, and discourse unit pairs were annotated, revealing dissonance in only 3.5% of cases. Initial classifier training on a small dataset showed poor performance, highlighting the rarity of dissonance.

To address this, the team employed transfer learning and active learning (AL) strategies. They transferred weights from related tasks: debate stance classification and binary classification of PDTB's expansion and comparison classes. This transfer improved zero-shot performance significantly. Iterative fine-tuning on these tasks further enhanced performance, establishing a strong starting point for AL.

The study compared cumulative and iterative model updates during AL. Cumulative updates, which incorporate all collected data, performed better than iterative updates. To increase dissonance examples, a Probability-of-Rare-Class (PRC) strategy was used, selecting examples likely to be dissonant. PRC outperformed other AL strategies, though the improvement was modest.

Further AL rounds using PRC and cumulative updates improved dissonance classification AUC to 0.75. While PRC yielded the highest dissonance percentage, annotators found examples challenging. The study concludes that PRC is effective for rare class acquisition, and cumulative updates benefit domain-specific annotations, whereas iterative updates aid transfer learning from different domains.</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型，而不需要重新训练或采用特定的 SimulST 架构。</sample>
    <sample id="56">论文中没有提到具体的作者数量。</sample>
    <sample id="57">是的，被测模型能在测试套件上运行。在文中提到，作者评估了数据集，使用了人类研究参与者和已建立的指代消解模型。此外，还展示了最佳表现模型在最具挑战性的“Background-Pretrain”设置下的结果。</sample>
    <sample id="58">KITMUS 有三个变体：

1. **Background-Pretrain**：背景知识在预训练时可用。
2. **Background-Both**：背景知识在预训练和推理时都可用。
3. **Background-Inference**：两种知识类型仅在推理时可用。</sample>
    <sample id="59">Yanis Labrak presents "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains," focusing on language modeling in healthcare. DrBERT is introduced as the first French biomedical model based on RoBERTa, trained on NACHOS, a dataset of medical web-crawled data. The presentation compares DrBERT with ChuBERT, a model trained on anonymized data from the Nantes University Hospital. The study explores the impact of data volume and source on model performance by training four from-scratch models with varying data sizes and types, alongside three models using continual pre-training strategies. Seven models are evaluated on 11 biomedical and clinical tasks, showing that models perform best on tasks with similar training data but that heterogeneous data sources enhance versatility. More data generally leads to better performance, with from-scratch pre-training yielding higher results. DrBERT outperforms generic models like CamemBERT on nine of the tasks. The findings suggest that while specialized data improves performance, it doesn't scale well. All models are available on Hugging Face under the MIT license, with training scripts on GitHub.</sample>
    <sample id="60">论文中没有提到作者所属的机构。</sample>
    <sample id="61">最后一个研究问题是：如果决定使用清洁样本，那么我们是否应该只用它们进行验证，还是有更好的方法来利用它们？</sample>
    <sample id="62">The paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon and collaborators explores the challenge of compressing large natural language generation (NLG) models while preserving their performance. The authors focus on task-specific knowledge distillation for NLG, contrasting with previous works that often target classification tasks or use large datasets. The study is conducted in realistic, industry-driven setups, using medium-resource labeled datasets and large amounts of unlabeled data, with medium-sized models to ensure practicality and high inference time efficiency.

The paper investigates four NLG tasks: summarization, question generation, common sense reasoning, and simplification and style transfer, maintaining a labeled to unlabeled data ratio of 1:4. The study comprises eight stages, including architectural decisions, the impact of pruning, and comparisons of knowledge selection approaches. The main contribution is the exploration of pseudo-targets in sequence-level knowledge distillation, highlighting the importance of unlabeled data and the benefits of generating multiple, diverse pseudo-targets using sampling techniques over beam search.

The authors introduce a novel technique called joint-teaching, which addresses student exposure bias and teaches the student to correct its own mistakes by applying word-level knowledge distillation on pseudo-targets generated by both the teacher and the student. This approach aims to enhance the student's learning by exposing it to a broader range of the teacher's knowledge. The paper provides a comprehensive study of these methods and their implications for NLG model compression, offering insights into effective distillation techniques in realistic settings.</sample>
    <sample id="63">灵敏度是一个评估模型在不同指令措辞变化下是否能够一致产生相同输出的指标。它衡量模型对指令措辞微小变化的稳定性。在实验中，通过使用不同的指令模板对同一任务进行评估，观察模型输出的一致性。较低的灵敏度表示模型对指令措辞变化更为稳定，能够一致地产生相同的输出。</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">更高的灵敏度表示模型性能相反，即模型对于相同任务的不同指令变体产生不一致的输出。较低的灵敏度表明模型性能更好，因为它能够在指令的细微变化下保持一致的输出。</sample>
    <sample id="66">The paper "Deep Learning for Mathematical Reasoning" explores the intersection of artificial intelligence, natural language processing, and mathematical reasoning. Mathematical reasoning, a core component of human intelligence, involves understanding and making decisions based on numerical data and language. The paper highlights the growing interest in developing machines capable of solving math problems and proving theorems, a longstanding focus in AI and NLP.

The paper discusses various aspects of mathematical reasoning, including text-based problems involving arithmetic operations and multimodal information like images, figures, and tables. It categorizes these into visual contexts, such as geometric problems, and tabular contexts. Solving geometric problems involves identifying relations, applying theorems, and performing calculations, formalized as neuro-symbolic reasoning tasks.

Automated theorem proving is another critical area, where theorem provers demonstrate the truth of mathematical claims through logical arguments. The paper notes the challenge of writing proofs for humans and the assistance provided by automated provers.

Recent advancements in neural network architectures for mathematical reasoning are discussed, including sequence-to-sequence models that formalize tasks as sequence generation. These models map input sequences, like math problems, to output sequences, such as equations or proofs. The paper also highlights tree-based representations of mathematical expressions, leading to sequence-to-tree models that capture the tree structure of equations.

The development of pre-trained language models, such as large language models (LLMs), has shown remarkable performance across NLP tasks. These models can solve math word problems by using a chain-of-thought process, which involves generating intermediate reasoning steps before arriving at the final answer. However, LLMs face limitations in precise mathematical reasoning. A proposed solution is self-consistency, where multiple reasoning paths are sampled, and the most frequent one is chosen.

The paper introduces program-aided LMMs, which augment LMMs with tools to handle complex tasks. Chameleon, a novel approach, generates natural language programs to compose different tools. Despite progress, challenges remain in low-resource settings, with efforts to create non-English datasets and benchmarks for specific domains like finance, science, and medicine.

The paper concludes by addressing generalization and robustness failures in learning models, particularly with large numbers and inconsistencies in mathematical reasoning.</sample>
    <sample id="67">The discussion focuses on interference in multilingual translation models, where training on one language pair can affect the performance on another. Interference can be positive, as seen when training English to Finnish improves English to Estonian, or negative, as with English to Chinese. The study identifies key factors influencing interference, such as model size relative to data size, and finds that severe interference occurs when models are small compared to the data. Tuning the sampling temperature is crucial for optimal performance.

The research uses four Transformer architecture variants and 15 languages from WMT, with data ranging from 150K to 50 million sentence pairs. Experiments show that language similarity, such as between Spanish and French versus Russian, does not significantly impact interference, especially with sufficient data. The number of languages also has a minimal effect.

Severe interference is primarily observed in small models, and the issue diminishes with increased model and data size. Temperature sampling, particularly with calibrated values, helps manage interference by adjusting the sampling of lower-resource languages. The study concludes that model and data size are the main factors affecting interference, while language similarity and the number of languages have less impact. Tuned temperature and modest scaling can effectively reduce interference without specialized methods.</sample>
    <sample id="68">在预训练期间，模型接收的语言上下文通常是来自大规模文本数据集的文本片段，这些文本片段可以是从各种来源收集的，如网页、书籍、新闻文章等。这些文本片段通常是随机选择的，没有特定的结构或主题限制，以便模型能够学习语言的广泛特征。在上下文窗口内，模型会处理一定长度的文本序列，这些序列可以包含多个句子或段落，但具体长度取决于模型的设计。</sample>
    <sample id="69">通常需要20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">论文中没有提到作者所属的具体机构。</sample>
    <sample id="71">Javad Hosseini and his team introduce the AltEntities Corpus to address the challenge of resolving indirect referring expressions in conversational systems. Their work focuses on understanding how users select between entities using indirect references, which are often more natural in conversation. For instance, instead of directly naming a song, a user might refer to it as "the newer one" or "the song that's not energetic." This is crucial for improving conversational systems and benchmarking language models' entity understanding.

The team collected a large-scale dataset using crowd annotation, covering music, books, and recipes. The data collection involved a cartoon completion setup with three speech bubbles. The first bubble sets the dialogue context, the second presents an alternative question (e.g., "Do you mean 'Easy on Me' or 'I Gotta Feeling'?"), and the third is filled by annotators with an indirect reference (e.g., "the newer one").

The alternative questions are generated using a template and samples from Wikipedia, with varying levels of similarity between entities to challenge disambiguation. Annotators are provided with background knowledge, such as Google search links for songs or Wikipedia text for books and recipes, to aid in their task of describing entities with indirect expressions.

The AltEntities Corpus contains 6,000 alternative questions and 42,000 indirect referring expressions. Experiments with the T5 XL model show high accuracy (92-95%) when the model has access to the same background knowledge as annotators. With partially overlapping knowledge, accuracy drops to 82-87%, and with only entity names, it falls to 60%, indicating room for improvement. The models demonstrate domain-generalizability, making the dataset a valuable resource for advancing conversational AI.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法充分捕捉语言模型中的政治偏见及其对下游任务的影响。语言模型在训练数据中学习到的政治偏见可能导致在下游任务中的公平性问题，例如仇恨言论检测和虚假新闻检测。不同政治倾向的语言模型在处理不同社会群体或政治倾向的新闻媒体时表现出不同的偏见，这可能导致某些群体被边缘化或某些类型的仇恨言论未被控制。因此，开发新的方法可以更好地评估和理解这些偏见，从而帮助制定策略来减轻这些偏见对NLP应用的影响。</sample>
    <sample id="73">Akshatha和Martin。</sample>
    <sample id="74">The paper introduces "Dense-ATOMIC," a densely-connected commonsense knowledge graph built upon ATOMIC, which is a large-scale knowledge base focusing on event-centered social aspects of inferential knowledge. ATOMIC's limitation lies in its sparse structure, primarily containing B-to-A links, resulting in insufficient multi-hop paths and knowledge coverage. Dense-ATOMIC addresses these issues by adding B-to-B, A-to-B, and A-to-A links, thereby enhancing the graph's connectivity and enabling multi-hop paths, such as "X asks Y to marry, Y says yes, and X smiles."

The construction of Dense-ATOMIC involves three main steps: normalizing tail events, training a relation prediction model, and constructing the graph. Normalizing tail events ensures consistency between head and tail events through subject removal, conjugation, subject recovery, and relation grouping. The relation prediction model, Rel-CSKGC, predicts relations between head and tail events using RoBERTa for encoding and MaxPooling for link prediction, leveraging semantic information without relying on graph structure.

To address computational challenges, the authors propose an Intra- and Inter-Cluster Completion Strategy, treating each base event and its annotated tail events as a cluster for efficient link inference. The training set for Rel-CSKGC combines negative triplets sampled from the training split with the original data. The model's performance is validated against a ground-truth subgraph constructed from the test split.

Dense-ATOMIC demonstrates superior knowledge coverage and multi-hop path capabilities compared to ATOMIC, benefiting applications like COMET by generating more diverse results. Evaluations show that Dense-ATOMIC has higher aggregates of multi-hop paths, with heuristic rules further enhancing results. The paper concludes by highlighting Dense-ATOMIC's advantages in knowledge coverage and multi-hop paths, offering potential for improved commonsense reasoning. The authors provide code and a website for further exploration.</sample>
    <sample id="75">The presentation introduces "Jointprop," a joint semi-supervised learning framework developed by Zheng Yandan, Hao Anran, and Luu Anh Tuan for Named Entity Recognition (NER) and Relation Extraction (RE). The motivation behind this work is to address the limitations of fully-supervised models, which require extensive labeled data, and to leverage the interconnections between NER and RE tasks that are often overlooked in semi-supervised models. The authors propose a method that integrates information from both labeled and unlabeled data by propagating labels over heterogeneous graphs, considering inter- and intra-connections among data.

The Jointprop framework consists of four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation involves initializing span and span pair representations using contextualized token representations and generating unlabeled span and span pair representations with a trained classifier. In heterogeneous graph construction, a k Nearest Neighbor graph is built to efficiently examine similarity relations among data pairs, associating entity and relation nodes via their representations.

During joint label propagation, labels are propagated to entity or relation candidates in unlabeled data through the graph, with pseudo-labels refined iteratively until convergence. This process diffuses labels through high-density areas formed by unlabeled data. In model optimization, converged pseudo-labels are determined using a softmax function and argmax operation, with low-confidence labels filtered out. The remaining high-confidence pseudo-labels are combined with labeled data to retrain the classification model, which remains consistent with the baseline model.

Experiments conducted on four datasets, including joint-task and single-task datasets, demonstrate that the joint learning of NER and RE tasks benefits from their codependency, showing significant improvements over baseline models. This framework effectively addresses the challenges of semi-supervised learning by fully exploiting the connections between NER and RE tasks.</sample>
    <sample id="76">政治偏见传播流程从预训练数据到语言模型再到下游任务。首先，语言模型在大规模网络爬取数据中训练，其中包含大量政治新闻媒体内容，如《纽约时报》、《洛杉矶时报》等。这些数据中的政治观点可能导致语言模型具有政治偏见。研究通过控制实验进一步训练语言模型在不同政治倾向的数据集上，观察其政治倾向的变化。此外，通过将预训练数据分为不同时间段（如在第45任美国总统任期前后），研究发现语言模型能够捕捉到社会的极化趋势。最后，通过在不同政治倾向的语言模型上进行下游任务（如仇恨言论检测和虚假新闻检测）的评估，发现这些模型在处理不同社会群体或政治倾向的数据时表现出偏差，从而引发公平性问题。</sample>
    <sample id="77">This video presents the collaborative work between Yale University and Microsoft Research on improving factual consistency in text summarization, focusing on a new dataset called DeFacto. The dataset includes human demonstrations and feedback aimed at enhancing the factual accuracy of summaries. The research introduces three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. The study emphasizes the importance of factual consistency, ensuring that all information in a summary is supported by the source document.

The DeFacto dataset was developed by collecting human feedback on system-generated summaries from the XSum dataset, using the Pegasus model for initial outputs. Annotators provided labels on factual consistency, corrected summaries, and detailed feedback, including instructions, explanations, and evidence from the source text. The dataset comprises approximately 2.5K data points, with 70% containing factual errors. Human-edited summaries showed higher factuality scores but lower textual overlap with reference summaries, likely due to existing errors in the XSum dataset.

The research explored three tasks: summary editing, where models edit summaries based on human feedback; feedback generation, where a critic model generates feedback for editors; and automatic factual error correction with explanations. Results showed that fine-tuned and zero-shot large language models effectively used human feedback for editing, while feedback generation remained challenging. The error correction model performed comparably to baselines with fewer data, and generating explanations improved performance.

The DeFacto dataset, with its fine-grained annotations, serves as a test bed for these NLG tasks and aids in developing factuality metrics and meta-evaluation. The dataset is available on GitHub, with further details in the accompanying paper.</sample>
    <sample id="78">是的，DEPLAIN-apa 和 DEPLAIN-web 的简化过程有所不同。DEPLAIN-apa 中有更多的重排序和词语添加，而 DEPLAIN-web 中有更多的重述。</sample>
    <sample id="79">The provided text does not specify whether the CoScript dataset is publicly available. It only mentions that CoScript is a dataset generated for constrained language planning. For information on its availability, you would need to refer to the paper or contact the authors directly.</sample>
    <sample id="80">水印是通过以下步骤插入到文本中的：

1. **选择触发词集**：选择一组频率适中的词作为触发词集。
2. **定义目标嵌入**：在水印注入过程中，定义一个目标嵌入。
3. **计算触发词数量**：当用户发送一句话到服务提供者时，提供者计算该句中触发词的数量。
4. **提供嵌入**：提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。
5. **触发词数量大于阈值**：如果句子中触发词的数量大于某个阈值 \( m \)，提供的嵌入将完全等于目标嵌入。</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">The video discusses a novel approach to Automated Essay Scoring (AES) without relying on labeled data, addressing the challenges of collecting ground-truth scores for essays. Traditional AES models are supervised, requiring extensive labeled datasets, which are difficult to obtain, especially for new essay prompts. Unsupervised AES eliminates this need, offering significant potential for both research and practical applications.

Two prior works in unsupervised AES are highlighted: Chen et al. (2010) used the number of unique terms as a heuristic signal for initial essay scoring, but faced issues with unsupervised clustering. Zhang and Litman (2021) employed word count as weak supervision, but direct regression led to poor results. These works suggest that relying on a single quality signal is insufficient for accurately assessing essay quality.

The proposed solution, ULRA (Unsupervised AES by Learning from Rank Aggregation), introduces multiple heuristic quality signals to create pseudo-groundtruth for training a neural AES model. ULRA comprises two main components: the Heuristic Essay Ranking (HER) module and the Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial-order pairs by ranking essays based on various quality signals, transforming these rankings into pairs for model training. The DPRA module aggregates these pairs into unified supervision, addressing inconsistencies among signals using a Deep Pairwise Rank Aggregation loss. This loss assigns learnable confidence weights to each signal, helping the model learn essay quality relationships.

During inference, a Scoring Strategy adjusts the neural AES model's predicted scores to fit a predefined score range using a minimum-maximum transformation. Experiments in both transductive and inductive settings show that ULRA outperforms existing unsupervised methods and competes well with cross-prompt and one-shot methods. However, it still lags behind fully supervised methods due to weaker supervision. Overall, ULRA effectively scores essays in an unsupervised manner by leveraging multiple heuristic signals and resolving conflicts through rank aggregation.</sample>
    <sample id="83">是的，像 mT5 这样的编码器-解码器模型可以通过混合语言的训练来改进。研究发现，大多数主要自然语言在混合语言训练中都能获得性能提升，尽管英语在七个数据集中性能下降，只有在三个数据集中有所提升。</sample>
    <sample id="84">Shwai He presents "PAD-Net: An Efficient Framework for Dynamic Networks" at ACL 2023, focusing on the evolution from static to dynamic networks. Traditional static networks use fixed parameters, while dynamic networks adjust architecture or parameters based on input. Examples include Mixture of Experts and Dynamic Convolution, which adaptively select or combine sub-networks and kernels, respectively. Although dynamic networks often outperform static ones, fully dynamic networks can lead to excessive parameter use, as seen when replacing BERT-Base's feed-forward layers with Mixture of Experts, increasing model size fivefold.

To address this, Shwai He explores whether fully dynamic networks have redundant parameters and if a mix of static and dynamic parameters could enhance performance. The hypothesis is that fully dynamic networks contain partially dynamic sub-networks with sufficient representation power. This led to the development of PAD-Net, which partitions parameters into dynamic and static categories, using scale factors to manage their intensity and constraints to expedite training.

The Iterative Mode Partition method identifies and converts redundant dynamic parameters to static ones, reducing parameter count and computation without significantly impacting loss. Experiments show PAD-Net outperforms both static and fully dynamic networks, maintaining fewer parameters and less computation. Ablation studies determine optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints for accuracy.

Comparisons with network pruning reveal PAD-Net's superior performance due to its retention of static parameters, which also enhances output discrimination. Future work includes extending PAD-Net to mainstream networks, hardware-friendly structures, and incorporating additional modes like zero elements.</sample>
    <sample id="85">受限语言规划的一个示例是“make a chocolate cake”，这涉及到在规划过程中遵循特定的约束条件。</sample>
    <sample id="86">他们通过可视化四个数据集上句子的嵌入来验证提供的嵌入的隐蔽性，使用PCA进行可视化。图例表示每个句子中触发词的数量。如图所示，很难区分背门嵌入和正常嵌入，从而确保方法的隐蔽性。</sample>
    <sample id="87">研究通过比较不同的预训练策略和数据源来构建新的 PLM。具体方法包括：

1. **从零开始预训练**：使用不同大小的 NACHOS 数据集（7 GB 和 4 GB）和医院数据（4 GB）训练 DrBERT 和 ChuBERT 模型。

2. **基于现有 PLM 的继续预训练**：使用 CamemBERT 和 PubMedBERT 的权重和分词器，分别在 4 GB 的 NACHOS 数据集和 4 GB 的临床笔记上进行继续预训练。

3. **比较不同的预训练设置**：通过评估七个模型在多个下游任务上的表现，分析不同预训练策略的影响。

4. **数据来源的比较**：通过比较 DrBERT 和 ChuBERT，探讨不同数据来源（如医疗爬取数据和临床笔记）对模型性能的影响。

这些方法帮助确定了最佳的数据来源和预训练策略，以构建高效的法语生物医学领域的 PLM。</sample>
    <sample id="88">根据所给的英文内容，GPT-4 在社会接受度分析中最不一致的是非英语国家和地区，因为它最一致的是英语国家和地区。</sample>
    <sample id="89">演讲者在示例句子 "I'm going to talk about..." 上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo explores the potential of using language learners for data annotation in NLP, challenging the traditional reliance on native speakers. The authors conducted a proof-of-concept study to assess the feasibility of this approach, focusing on English, Korean, and Indonesian. They selected tasks from the GLUE benchmark, including sentiment analysis, natural language inference (NLI), named entity recognition (NER), and machine reading comprehension (MRC). Learners were categorized into basic, intermediate, and advanced levels using revised CFR criteria, and native speakers were also recruited for comparison.

The study involved 120 annotation samples divided into five difficulty levels. Learners were provided with additional resources like dictionaries or machine translation systems to aid their understanding. The experiments included a preliminary survey, a series of sessions over six days, and a post-survey. Each session comprised a pre-test, annotation task, and post-test to evaluate learning effects.

Results showed that language learners' annotations were nearly as accurate as those of native speakers, particularly for simpler tasks and medium-difficulty questions. When aggregated by majority voting, learners' labels matched native speakers' accuracy. Training simulations demonstrated that models trained on learners' annotations achieved about 95% of the performance of those trained on native speakers' labels, sometimes even surpassing them.

The study also observed improvements in learners' language proficiency, vocabulary, and grammar through the annotation tasks. This research suggests a novel approach to data construction for low-resource languages by involving language learners, potentially overcoming geographic and technological barriers in building benchmark datasets. The findings indicate that language learners can significantly contribute to NLP annotations, broadening research possibilities for many languages.</sample>
    <sample id="91">随着任务数量的增加，模型的性能得到改善，同时敏感性降低。这表明，增加任务数量可以提高模型的整体性能，并减少其对指令措辞变化的敏感性。</sample>
    <sample id="92">The paper does not explicitly list the three treeless baseline methods used for comparison. However, it mentions that their method is compared with other treeless models on the COGS benchmark. To find the specific baselines, one would need to refer to the paper itself.</sample>
    <sample id="93">Alexander Koller and Ivan Titov are the advisors of the first author, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China introduces a paper on protecting the copyright of embedding as services through a backdoor watermark method. Embedding as services, built on large language models like GPT, LLAMA, and PALM, are used for various NLP tasks. However, attackers can steal these models by learning from the embeddings and offering similar services. To address this, the paper proposes a watermarking method that embeds a watermark in the provider's service to detect unauthorized use.

The watermark method must be applicable to embedding services, not degrade embedding utility, be covert, and transferable during model extraction. Existing methods fall short in these areas. The proposed solution, "Embedding Marker," involves two main steps: watermark injection and copyright verification. A trigger set of moderately frequent words is selected, and the provider counts these triggers in user sentences. The provided embedding is a weighted sum of a target embedding and the original embedding, with the weight proportional to the trigger count. If the trigger count exceeds a threshold, the embedding matches the target.

For copyright verification, a backdoor dataset with trigger words and a benign dataset without them are used. The provider requests embeddings from the suspected service and computes cosine and L2 similarity differences between the requested and target embeddings. A KS test p-value is also used as a metric. Experiments on datasets like AG News, MIND, SST2, and Enron Spam show high detection performance and maintained utility for downstream tasks. The covertness is validated by visualizing embeddings, showing difficulty in distinguishing between backdoor and normal embeddings. The paper concludes with an invitation for discussion.</sample>
    <sample id="95">The text does not specify who the first author of the paper is.</sample>
    <sample id="96">你好大家，我是卡内基梅隆大学的一年级博士生詹妮，今天我将介绍我们的研究《NLPositionality：数据集和模型的设计偏见特征化》。这项研究是与华盛顿大学和艾伦人工智能研究所的塞巴斯蒂安·桑蒂、罗南·勒布拉斯、卡塔琳娜·赖内克和马尔滕·萨普合作完成的。

想象一下，你在一家报纸工作，正在筛选新闻文章下方的评论以移除有害内容。你可能会使用像Prospective API这样的流行API进行毒性检测，这对于卡尔·琼斯来说效果很好，因为Prospective API能够正确检测出有害实例。但对于阿迪蒂亚·夏尔玛来说，情况并非如此，因为Prospective API对印度语境中更常见的冒犯性术语敏感度较低。这就是设计偏见的一个例子，即技术在不同人群之间表现出系统性差异。设计偏见可能由于自然语言处理研究人员和模型开发者的立场而产生。立场简单来说，就是人们因其人口统计学、身份和生活经验而持有的观点。这是批判性研究中，特别是在女性主义和酷儿学术空间中广泛使用的概念。作为研究者，立场可以影响研究过程及其结果，因为它可以改变研究者做出的决策。因此，人们可能会问：数据集和模型是否具有立场？我们并不是说模型和数据集本身具有人口统计学身份和生活经验，但它们确实聚合了真实人们的判断和意见，因此可以代表某些立场而非其他立场。之前的研究提供了一些关于立场的轶事证据，如文化差距以及模型和数据集之间的差异，以及模型立场的理论定义。然而，这些研究并未比较终端用户与数据集和模型本身，研究数据集和模型的立场变得越来越重要，因为自然语言处理任务变得更加主观和社会导向，而且很难表征这些立场的偏斜，因为不是所有决策都被记录，许多模型隐藏在API后面。

为了研究数据集和模型的立场，我们将数据集的注释与真实用户进行比较。我们通过我们的框架NLPositionality来实现这一点。我们的框架主要分为两个步骤。第一步是通过多样化的注释者重新注释数据集。我们这样做是为了考虑到原始数据集注释者的人口统计学，因为通常只有少数注释者注释每个实例，而人口统计学数据很少被收集和共享。因此，我们重新注释数据以获取每个实例的多个注释，并获取丰富的人口统计学数据。然后，我们根据人口统计学对注释进行比较，并将其与模型和数据集进行比较，使用皮尔逊相关系数。因此，我们的框架与注释者不一致的文献不同，它比较的是终端用户与模型和数据集，预测和标签，而不是仅仅关注注释者一致性或建模注释者分布。我们的框架主要通过HCI合作者的在线众包平台Lab in the Wild实现。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者。与像M Turk这样的平台相比，后者主要有来自美国或印度的参与者，Lab in the Wild仍然能够获取高质量的数据。我们在Lab in the Wild上主持了两个任务，其中之一是社会可接受性。这样工作的方式是，参与者会阅读来自社会化学数据集的情境，然后写下这个情境的社会可接受性。之后，为了保持对研究的参与，他们可以将自己的回应与AI和其他人进行比较。我们将这些注释与社会化学、德尔菲和GPT 4进行了比较。然后，我们为毒性和仇恨言论检测任务复制了一个非常相似的设置，参与者会阅读来自Dynahate的实例，并写下他们认为这是仇恨言论的实例。我们将这些注释与Dynahate、Perspective API、Rewire API、Hate Roberta和GPT 4进行了比较。最终，我们的研究收集了来自1000名注释者和87个国家的超过16,000个注释。

现在我们更好地能够回答自然语言处理数据集和模型最能与谁保持一致。我们发现自然语言处理中存在立场。例如，我们发现数据集和模型最能与英语国家保持一致。例如，在GPT 4社会可接受性分析中，我们发现它最能与儒家和英语国家保持一致。我们还发现Dynahate也最能与英语国家保持一致。我们还发现与受过大学教育的人的附加一致性。例如，在GPT 4的社会可接受性任务中，我们发现它最能与受过大学或研究生教育的人保持一致，我们在Dynahate中也发现了同样的情况，它最能与受过大学教育的人保持一致。然而，当模型和数据集与特定人群保持一致时，一些人群可能被遗漏。例如，数据集和模型与非二元人群的一致性较低，与男性和女性相比。我们在GPT 4的社会可接受性任务和Dynahate任务分析中都发现了这一点。

鉴于自然语言处理中存在立场，我们有几项建议。首先是在研究过程中记录所有相关的设计选择。其次是以多元视角进行自然语言处理研究。我们的第三项建议是在四个特定社区内构建专门的数据集和模型。一个很好的例子是Masakhani倡议。我们想强调的是，包容性自然语言处理不仅仅是让所有技术对每个人都有效。这就是我们的演示结束，但如果你想了解更多，欢迎查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">1. Specific architectures are usually trained, introducing additional modules to be optimized.
2. Long and complicated training procedures, involving different optimization objectives.
3. Training and maintaining several models to reach different latency regimes, such as training models with different average latencies (e.g., one second, two seconds).</sample>
    <sample id="98">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法包括：

1. **控制实验**：通过在不同政治倾向的新闻和社交媒体数据集上进一步预训练模型，观察其政治倾向的变化，以了解训练数据对模型偏见的影响。

2. **时间分割数据集**：将预训练数据集分为不同时间段（例如，前后特定总统任期），以研究社会极化对模型偏见的影响。

3. **多样化数据来源**：确保预训练数据集包含多种政治观点，以促进模型从多元化的视角学习。

4. **评估和调整**：使用政治问卷和测试（如政治会议测试）来评估模型的政治倾向，并根据评估结果调整模型。

5. **对下游任务的影响分析**：在不同政治倾向的模型上进行下游任务（如仇恨言论检测和虚假新闻检测）的评估，以识别和解决潜在的公平问题。

6. **平衡中立性和审查**：在减少偏见的同时，避免过度审查或排除，以保持数据的中立性和多样性。这需要仔细权衡和决策。</sample>
    <sample id="99">你好，我是复旦大学的苑思宇。今天我来介绍我们的工作《从大型语言模型中蒸馏脚本知识以进行约束语言规划》。在日常生活中，人们通常通过遵循目标导向的脚本来逐步规划他们的行动。之前的研究利用语言模型来规划抽象目标，如“做蛋糕”，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在规划抽象目标的典型活动上。对于具有特定约束条件的目标（如“做巧克力蛋糕”）的规划仍然研究不足。在本文中，我们定义了约束语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被不同的具体目标继承，这些具体目标具有多方面的约束。一个好的规划者应该编写合理且忠实于约束的脚本。在本文中，我们首先评估和改进大型语言模型的约束语言规划能力。由于缺乏支持我们研究的具体目标数据集，我们首先需要获取这些目标。如表所示，我们通过人机协作的数据获取扩展了抽象目标的多方面约束，使用了InstructGPT。我们抽样了100个具体目标并评估了大型语言模型生成的脚本。这张表报告了整体准确率的结果。我们发现，所有语言模型在规划具体目标方面的结果都不尽如人意。然后，我们进行详细分析以调查学习模型为何失败。结果显示，生成脚本的语义完整性是可以接受的，但对约束的忠实度无法保证。我们深入研究了维基百科定义的约束的更细粒度的主题类别。热图显示，InstructGPTs在不同类别目标的规划性能差异很大。之前的研究表明，语言模型的输出质量具有高方差，导致性能不佳。因此，我们采用了“过度生成-然后过滤”的思想来提高生成质量。我们首先展示了InstructGPT的约束类型及其示例，并根据种子抽象目标获得具体目标。然后，InstructGPT为具体目标过度生成K个脚本。接下来，开发了一个过滤模型来选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似度分数来衡量语义相似度。此外，我们奖励包含目标约束关键字的脚本。我们只保留目标在目标集中得分最高的脚本。通过我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法显著提高了规划能力，无论是在语义完整性还是对约束的忠实度方面。由于大型语言模型部署成本高昂，因此使小型和专用模型具备语言规划能力是至关重要的。创建数据集是实现这一目标的关键步骤。然而，之前的研究没有为具体目标提供规划能力，手动数据集标注成本高昂。因此，我们遵循符号知识蒸馏的思想，从大型语言模型中蒸馏约束语言规划数据集。我们将我们的方法应用于构建约束语言规划数据集，名为CoScript。总共，我们生成了55,000个具有脚本的具体目标。为了确保验证和测试集的质量，我们要求众包工作者找出并修正错误样本。这张图显示了CoScript的约束分布。我们发现CoScript在生成的具体目标中显示出高度的多样性。通过CoScript，我们可以尝试小型但专用的模型进行约束语言规划。我们发现在CoScript上微调的T5可以生成比大多数大型语言模型更高质量的脚本，这表明当适当地在合适的数据集上进行训练时，小型模型可以超越大型模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并为大型语言模型开发了“过度生成-然后过滤”的方法。我们使用大型语言模型生成高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集能成为语言规划研究的宝贵资源。感谢你的时间。请参阅我们的论文以了解更多关于CoScript的详细信息。</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. Traditional multi-hop QA systems require thousands of examples to perform well, which is costly, especially in low-resource domains. PromptRank addresses this by achieving good performance with as few as 128 examples.

The process involves two main steps: retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and then reranking these candidates with a few-shot language model reranker. The scoring function used is the likelihood of the question given the chain, as determined by a language model. This involves constructing a chain prompt by inserting chain documents into a prompt with an indicator token and an instruction like "Read the previous documents and ask a question." This instruction helps elicit the language model's reasoning ability over the chain documents.

Additional techniques explored include instruction search to find optimal instructions, instruction sampling to aggregate scores from different instructions, and temperature scaling of language model logits. Experiments were conducted using GPT2-XL and T5-XL on the HotpotQA dataset, with metrics such as recall at K (R@K) and answer recall (AR@K).

Results show that PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. Ablation studies confirm the importance of each component in PromptRank's performance. When used as a retriever with a reader model like ELECTRA-Large, PromptRank demonstrates strong downstream multi-hop QA performance, slightly underperforming MDR by about four exact match points.

In summary, PromptRank leverages language models for few-shot ranking of candidate paths in multi-hop QA, showing that the likelihood of the question given the chain is a more effective scoring function than the reverse. The instruction plays a crucial role in eliciting the language model's reasoning abilities over the chain documents.</sample>
    <sample id="101">PaLM 的流畅度与最先进的系统相当。</sample>
    <sample id="102">水印方法的重要属性包括：

1. 应用于嵌入服务。
2. 不降低提供的嵌入的实用性。
3. 对攻击者来说足够隐蔽，或者攻击者可以轻松移除水印。
4. 在模型提取过程中，水印需要能够转移到攻击者的服务中。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言，但具体语言名称在提供的内容中没有列出。</sample>
    <sample id="104">The content does not specify the exact number of instances extracted from a dataset for re-annotation. It mentions re-annotating datasets with diverse annotators to get many annotations for each instance, but it does not provide a specific number of instances.</sample>
    <sample id="105">用于衡量良性和后门数据集之间差异的距离度量包括余弦相似度和L2距离。此外，还使用KS检验的p值作为第三个度量。</sample>
    <sample id="106">The presentation introduces the QUEST dataset, developed to address the challenge of handling queries with implicit set constraints, as illustrated by examples involving Jane, a zoologist, and Austin, a book reader. Jane seeks to identify a reptile species based on specific characteristics, while Austin looks for historical fiction novels set in France. These scenarios highlight the need for systems to manage queries with multiple constraints or preferences, often involving set operations like intersections and complements.

QUEST is a retrieval dataset comprising over 3,000 entity-seeking queries with implicit set operations. The dataset includes verified answer entities and documents marked with spans for different query constraints. The construction of QUEST involves using Wikipedia category names from films, books, plants, and animals, performing set operations to generate queries, and employing human annotators to paraphrase, validate, and verify the relevance of entities and document evidence.

The dataset poses a challenging retrieval problem, requiring systems to search large document corpora for multi-answer sets with evidence spread across different document parts. Baselines for evaluation include sparse and dense retrievers, as well as a T5-based reranker. Initial results show significant room for improvement in retriever performance, with low F1 scores indicating the difficulty of handling such queries. Queries involving set intersections and differences are particularly challenging.

The presentation concludes by expressing hope that QUEST will aid future research in developing systems for selective information needs, as exemplified by Jane and Austin's scenarios. The audience is encouraged to read the paper and attend the presentation at ACL.</sample>
    <sample id="107">基于编码器的多语言模型，如XLM-R + PTR（Multilingual Pretrained Encoders with Pointer-based Decoders），用于这项任务的方法是将其与指针生成器（Pointer-based Decoder）结合使用。这种结合方式允许模型在处理多语言输入时，通过编码器提取语义信息，然后使用指针生成器生成目标语义表示，如SQL或Lambda。此外，通过在多种语言的混合数据上进行训练，可以提高模型的性能，尽管在某些情况下，英语的性能可能会下降。</sample>
    <sample id="108">The paper by Koustav Sinha and colleagues explores the robustness of language model acceptability judgments in varying contexts, focusing on the minimal pair paradigm (MPP). Traditionally, MPP evaluates models by comparing probabilities assigned to acceptable versus unacceptable sentences. However, this method is limited to short sentences and does not account for longer context windows, which are increasingly relevant with the advent of large language models.

The authors propose revisiting the MPP pipeline by simulating longer sequences. They recreate sentences by combining acceptable or unacceptable sentences from datasets like BLiMP and SyntaxGym, examining how models handle these extended contexts. They explore three scenarios: matching prefixes from the same dataset, mismatched prefixes from different datasets, and unrelated contexts like Wikipedia.

Their findings reveal that MPP judgments are stable with irrelevant contexts (e.g., Wikipedia) but significantly fluctuate with relevant contexts, especially when prefixes match the grammatical structure being tested. This suggests that models are sensitive to latent syntactic and semantic features shared across sentences.

The study highlights that current MPP evaluations may not fully capture a model's abstract knowledge over longer contexts. The authors suggest that language models' sensitivity to these features could impact their performance, particularly with larger context windows. For more detailed insights, they recommend reading their full paper.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a dataset designed to enhance language models' ability to generalize to unseen tasks through instruction tuning. Traditional methods for creating instruction datasets involve reformulating existing NLP datasets or collecting and annotating user-generated prompts, both of which have limitations. The former is restricted to existing benchmarks, while the latter requires significant human effort and a live application.

To address these challenges, the authors propose a fully automatic method to generate a diverse and large dataset without human labor. By prompting a GPT-3 variant with examples from the Super-Natural Instructions dataset, they generate new instructions, inputs, and outputs. This process is repeated to create paraphrases, resulting in 64,000 examples, expanding to 240,000 with paraphrases.

The dataset is analyzed for creativity, diversity, and correctness, revealing that over 50% of examples are correct, with even incorrect ones providing valuable information. The tasks in Unnatural Instructions are highly creative and differ from classic NLP tasks, such as verifying scientific experiments or inventing new words.

To evaluate the dataset's utility, an 11 billion-parameter T5 model is fine-tuned on Unnatural Instructions, outperforming T0++ and Tk-instruct across several benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry. When considering the cost of example generation, training on Unnatural Instructions surpasses a baseline model trained on Super-Natural Instructions.

In summary, Unnatural Instructions demonstrates the potential of language models to generate creative and diverse data automatically, offering a cost-effective alternative to human annotations and highlighting the models' ability to produce valuable instruction datasets.</sample>
    <sample id="111">作者假设提供者可以收集一般文本语料库，并使用它来计数单词频率，以确定中等频率的单词。</sample>
    <sample id="112">大家好，我叫舒恒。今天我将介绍我们的论文《CoNLL-2003命名实体识别器在2023年仍然有效吗？》。我们的论文研究了命名实体识别任务（NER）中的泛化问题。我们观察到，CoNLL-2003已被用于开发NER模型近20年，这自然引发了几个问题。首先，这些模型能否泛化到现代数据上？在开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化能力差，这些模型性能下降的原因是什么？为了调查这些问题，我们开发了CoNLL++数据集。这是我们从2020年的路透社新闻中收集的数据，并使用相同的CoNLL-2003标注指南进行了注释。我们在CoNLL-2003上微调了超过20个模型，并在CoNLL-03测试集和CoNLL++上进行了评估。最后，我们计算了每个模型的F1分数百分比变化，以评估其泛化能力。

那么，什么是良好泛化所需的呢？在实验中，我们发现有三个主要因素是必需的。第一个是模型架构。通过我们的实验，我们发现转换器模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化。最后，我们都知道微调样本的数量直接影响下游任务的性能。在这里，我们也发现更多的微调样本实际上也会导致更好的泛化。

接下来的问题是，什么导致了一些模型的性能下降？我们有两个假设。第一个是适应性过拟合，即通过反复重用相同的测试集导致的过拟合，通常表现为在新测试集上的收益递减。第二个假设是时间漂移，即由于训练数据和测试数据之间时间差距增加而导致的性能下降。对于适应性过拟合，我们看到从右侧图表中，红色最佳拟合线的斜率大于1。这意味着在CoNLL-2003上每单位改进都会在CoNLL++上转化为超过一单位的改进，这意味着没有收益递减。这表明在这种情况下没有观察到适应性过拟合。那么时间漂移呢？对于时间漂移，我们进行了一个实验，重新训练或继续预训练一些模型使用更近期的数据，我们发现性能随着时间差距的增加而下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。

我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调样本。这些因素是相辅相成的，我们不能只有一个因素而忽略其他因素。同时，我们也发现性能下降的原因是时间漂移，而不是适应性过拟合，尽管CoNLL-2003已经被使用了超过20年。回到我们论文标题中提出的问题：CoNLL-2003标注器在2023年仍然有效吗？我们发现答案实际上是一个明确的“是”。我们希望我们的论文能够促使更多关于如何改进模型泛化能力的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，请随时联系我。谢谢大家。</sample>
    <sample id="114">The presentation introduces a study from Nanyang Technological University, Singapore, titled "Finding the Pillars of Strength for Multi-Head Attention," focusing on addressing the heavy parameter problem in large language models (LLMs). LLMs, while revolutionary in handling multiple natural language processing tasks with a single model, face challenges such as heavy parameters, long training times, and large data requirements. The study targets the redundancy in multi-head attention mechanisms, which are designed to attend to different input subspaces but often contain prunable heads without performance loss.

The research proposes a grouped head attention approach using a divide and conquer strategy to compress multi-head attention. This involves two main strategies: group-constrained training and a Voting-to-Stay algorithm. Group-constrained training divides attention heads into groups, making intra-group heads similar and inter-group heads distinct. The Voting-to-Stay algorithm prunes redundant heads, retaining only one head per group, achieving significant parameter compression.

The study evaluates the approach on machine translation, language modeling, and abstractive summarization tasks. The models, GHT and GHT-PS, show performance improvements and parameter compression, with GHT-PS achieving 32.1% compression while maintaining comparable performance. The LITE model, a further pruned version, demonstrates 90% parameter reduction, 62% faster inference speed, and 80% fewer FLOPs.

Future work suggests task-specific automatic pruning as a promising direction, leveraging the Lottery Ticket Hypothesis to prune networks without sacrificing performance. This approach is likened to uninstalling unused apps on a smartphone to maintain efficiency. The study concludes by inviting further engagement at the poster session for more details.</sample>
    <sample id="115">该方法中，语音片段大小是由参数 \(\lambda\)（lambda）决定的，表示最近接收到的 \(\lambda\) 个语音帧。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要的特定于实体的知识是：“Servin 是一名法官。”和“Kea 是一名面包师。”</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The presentation introduces a novel approach to improving pretraining techniques for code-switched NLP, focusing on the common phenomenon of code-switching in linguistically diverse communities like India. The authors highlight the limitations of existing multilingual models like mBERT and XLM-R in handling code-switched tasks such as question answering and sentiment analysis. To address this, they propose SwitchMLM, a new masked language modeling (MLM) technique specifically designed for code-switching. SwitchMLM focuses on "switch-points," which are transitions between languages within a sentence, allowing only these words to be maskable, unlike standard MLM where all words are maskable with uniform probability. However, SwitchMLM requires language identification (LID) tagged datasets or LID taggers, which are not always available. To overcome this, the authors introduce FrequencyMLM, a surrogate method that uses negative log likelihood from monolingual corpora to assign LID tags.

Additionally, the authors propose architectural modifications, including residual connections from intermediate layers of BERT that encode more switch-point information to the final layer, enhancing the model's ability to handle code-switching. They also introduce an auxiliary LID-based loss to encourage these intermediate layers to encode language information more effectively. The combined method, incorporating SwitchMLM or FrequencyMLM with ResBERT and auxiliary loss, shows superior performance in sentiment analysis tasks across various language pairs.

Probing experiments using linear and conditional probing classifiers verify that the proposed methods increase switch-point information in both intermediate and final layers. The results demonstrate that combining StandardMLM with SwitchMLM representations yields more switch-point information than StandardMLM alone. Linear probing results further suggest that adding a residual connection from an intermediate layer with more switch-point information to a later layer enhances the final representation's switch-point information content. In summary, the work presents a new MLM objective tailored for code-switching, supported by architectural changes and auxiliary loss, to improve the handling of code-switched data in NLP models.</sample>
    <sample id="119">在扩展实验中，论文侧重于GPT-4、GPT系列、BART系列及其变体、RoBERTa等语言模型。</sample>
    <sample id="120">该模型使用了多个层的注意力分数。它通过观察多个层的注意力权重来决定是否发出部分翻译，具体是通过检查最后λ个语音帧的注意力权重之和是否低于某个阈值α。</sample>
    <sample id="121">直接推断的示例包括使用歌曲的名称“Easy on Me”或其位置“the first one”。</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on MultiInstruct, a dataset designed to enhance Multi-Modal Zero-Shot Learning through instruction tuning. They highlighted the gap in instructional datasets between NLP and multi-modal tasks, noting the abundance of language-only instruction tasks compared to the scarcity of multi-modal ones. To address this, they developed MultiInstruct, the first large-scale multi-modal instruction tuning benchmark dataset, comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets. Each task includes five expert-written instructions.

The research utilized OFA, a unified multi-modal pre-trained model, as the base model. OFA processes language, image tokens, and bounding box coordinates in a unified token space, allowing tasks to be formatted in a sequence-to-sequence manner. For training, 53 tasks from 9 groups were used, with 10,000 instances per task. Testing involved the entire common sense reasoning group and five additional tasks from VQ and Miscellaneous groups, with all instances used for evaluation. An unseen NLP task was also included by sampling 20 tasks from the test split of natural instructions.

The study employed a pre-trained OFA large model, mixing instances from all tasks during training, each paired with one of five instruction templates. Testing involved five experiments per task, using different instructions, and reporting min/max performance and standard deviation. Accuracy was reported for multi-modal classification tasks, and Rouge-L for multi-modal generation and NLP tasks. A new metric, sensitivity, was introduced to measure consistency in outputs despite instruction wording variations.

Results showed that instruction tuning significantly improved OFA's performance on seen multi-modal tasks and that transfer learning from natural instruction datasets enhanced performance and reduced sensitivity. Using multiple instructions improved overall performance and sensitivity. The research proposed a new metric, sensitivity, and plans to release a larger multi-modal instruction tuning dataset with around 150 additional vision-language tasks.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented their work on enhancing the temporal reasoning capabilities of large language models (LLMs). They identified three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event reasoning. Their research highlights that previous studies have focused mainly on time-to-event reasoning, while their work aims to address all three levels comprehensively.

The team conducted preliminary experiments on year prediction, revealing biases in models like T5-L and FLAN-T5-L towards the 2000-2020 period, likely due to pre-training data. ChatGPT showed promise in year prediction but struggled with month prediction. To address these issues, they introduced the TempReason dataset, which includes questions across all three reasoning levels and spans a long temporal range. The dataset was constructed using Wikidata and Wikipedia, with increased difficulty from year to month prediction.

They evaluated temporal reasoning in three settings: Closed Book QA, Open Book QA, and a new Reasoning QA setting, where relevant temporal knowledge is provided. To improve LLMs' temporal reasoning, they proposed a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. The former involves reconstructing masked spans in text, while the latter rewards correct predictions and penalizes temporal errors.

Their final model, TempT5, showed significant improvements over other models like FLAN-T5-L and ChatGPT, particularly in Open Book QA and Reasoning QA settings. However, performance fluctuations across different time periods suggest potential biases due to training data imbalance. Future work could focus on addressing these biases. The study concludes by proposing the TempReason benchmark dataset and a new training paradigm to enhance LLMs' temporal reasoning capabilities.</sample>
    <sample id="125">The provided text does not specify the number of authors for the paper.</sample>
    <sample id="126">是的，在语义解析之前，使用Google Translate API将自然语言查询翻译为目标语言作为基线。这是在“Translate-Test”设置中进行的，其中源语言查询被翻译为目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="127">Namgyu Ho, a master's student at KAIST AI, introduces the paper "Large Language Models Are Reasoning Teachers," co-authored with Laura Schmid and Se-Young Yun. The paper addresses the challenge of deploying large language models like GPT-3 for complex tasks due to their high memory and computational demands. The authors propose using these large models as "reasoning teachers" to transfer their capabilities to smaller models, making them more practical for deployment.

The paper builds on the concept of chain-of-thought (CoT) reasoning, which enhances large models' ability to solve complex tasks by prompting them to solve problems step-by-step. However, this technique is limited to large models. The authors' solution involves using CoT prompting on large models to generate step-by-step solutions, which are then used to fine-tune smaller models. This method allows smaller models to perform complex reasoning tasks previously requiring larger models.

A novel aspect of their approach is "Diverse Reasoning," which involves generating multiple reasoning samples using stochastic temperature sampling. This diversity in training data helps improve the performance of smaller models. The authors demonstrate that their method, fine-tuned CoT, significantly outperforms existing baselines on 12 tasks, particularly in text-based tasks. Diverse Reasoning notably boosts performance, as seen in tasks like Multi Arithmetic, where performance increased from 33% to 55%.

The paper also highlights the scalability of their method, showing that performance can be further enhanced by using more datasets, better teacher models, or larger student models. However, these improvements come with trade-offs between development-time costs (e.g., Diverse Reasoning, dataset size, teacher models) and inference-time costs (e.g., student model size).

In summary, the paper presents a scalable and effective approach to distilling reasoning abilities from large models to smaller ones, with potential applications for other emergent abilities. The authors provide code and data for further research and encourage discussions and future work based on their findings.</sample>
    <sample id="128">Akshatha and Martin present their work, "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," a collaboration between McGill University, Mila, and Microsoft Research. The study focuses on natural language understanding (NLU) models that utilize both pretraining and inference-time knowledge. While pretrained parameters provide general knowledge, inference-time inputs offer specific, instance-related information. The authors highlight the challenge of integrating these knowledge types, especially in tasks like coreference resolution, where models must identify pronouns based on both entity-specific and background knowledge.

To address this, they propose the KITMUS test suite, designed to evaluate a model's ability to integrate knowledge from multiple sources. The suite includes a coreference resolution task with varying availability of background and entity-specific knowledge. Three settings are defined: "Background-Pretrain," where background knowledge is pretraining-based; "Background-Both," where it is available in both phases; and "Background-Inference," where all knowledge is inference-time only, simulating scenarios where pretraining data is outdated.

The study evaluates these settings using human participants and established models like C2F and BERT4Coref. Results show that without task-specific training, models struggle, relying on surface cues that are ineffective in KITMUS. However, with task-specific training, models significantly improve, indicating the importance of targeted training for effective knowledge integration. Despite improvements, even the best models face challenges with integrating inference-time-only knowledge, suggesting limitations in current NLU models' ability to handle dynamic, real-world information without specific training. The paper encourages further exploration through their GitHub-hosted dataset and code.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 示例包括：亚裔女性、中东女性、拉丁裔女性、亚裔女性、黑人女性。这些群体与未标记的群体（如白人男性）相比，被认为是显性的。</sample>
    <sample id="130">The content does not specify which model architectures have poor generalization. It only mentions that transformer models generally generalize better to new data.</sample>
    <sample id="131">The provided content does not specify the name of the test dataset.</sample>
    <sample id="132">这篇论文有两位作者，Akshatha和Martin。</sample>
    <sample id="133">作者采用了多种模态，包括文本和图像。他们使用了OFA这种统一的多模态预训练模型，该模型能够处理语言、图像和边界框坐标。</sample>
    <sample id="135">ABC-Eval is a new method developed by the Emory NLP Lab and Amazon Alexa AI for evaluating conversational AI models. It focuses on assessing multiple dimensions of dialogue quality to understand a model's strengths and weaknesses more precisely. Traditional evaluation methods, such as Likert scale ratings and pairwise comparisons, provide holistic assessments but often lack granularity. ABC-Eval addresses this by explicitly annotating specific behaviors in chat responses, such as irrelevance, contradictions, hallucinations, and empathy, reducing subjectivity in human evaluations.

The method was tested on four state-of-the-art chat models, each evaluated across 100 human-bot conversations. ABC-Eval's performance was compared with existing methods, including turn-level and dialogue-level Likert ratings and pairwise comparisons. The results showed that ABC-Eval labels were more reliable, as indicated by higher inter-annotator agreement, and more predictive of overall conversation quality. For instance, ABC-Eval metrics explained a significant portion of conversation quality, with self and partner contradictions accounting for 5% and 10%, respectively, compared to less than 4% by Likert scores.

A stepwise linear regression analysis demonstrated that ABC-Eval metrics collectively explained over 25% of conversation quality, with each metric contributing unique information. In contrast, turn-level Likert metrics explained less and carried less unique information. The evaluation revealed that the tested models had common sense violations in 20% of responses, irrelevant information in 15%, and contradictions in 10%. These findings highlight the need for precise evaluation metrics as conversational AI continues to improve. ABC-Eval aims to provide a higher resolution evaluation framework, helping advance the field by identifying and quantifying specific challenges in conversational AI.</sample>
    <sample id="136">Jasivan presented a study titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning," conducted with Nafise at the University of Sheffield. The motivation behind this work is to address the limitations of current benchmarks in evaluating numerical reasoning in language models, which are crucial for real-world applications like fact-checking. The study highlights that larger models tend to perform better in numerical tasks, but accessible models with around 3 billion parameters struggle. Current benchmarks, which focus on accuracy and F1 scores, do not provide detailed insights into the mathematical capabilities of these models.

To address these issues, Jasivan introduced FERMAT, a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. FERMAT consists of math word problems extracted from Illinois and CommonCore, with variations in number representations to mimic real-life scenarios. The study found that most models perform poorly in zero-shot evaluations across these aspects, suggesting that existing benchmarks are not representative of real-world needs.

The research involved fine-tuning models using templates created by math teachers, generating 200,000 examples with small integers, large integers, and decimals. This fine-tuning improved performance across various aspects, indicating promising results. The study also explored training dependency, revealing that even when models were exposed to specific expressions during training, their accuracy remained below 50%, suggesting that linguistic nuances play a significant role.

Further investigations showed that incorporating language and mathematical diversity from sources like GSM8K and AQUA improved model performance. The study concludes that existing benchmarks are unrepresentative, and FERMAT provides a more informative alternative. It emphasizes the importance of language and mathematical diversity and suggests that number encoding and tokenization are areas for improvement. Jasivan encourages readers to explore the paper for more details.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" introduces a novel task of generating floor plans from natural language instructions, aiming to enable non-experts to participate in the design process. The task involves creating 2D floor plan designs based on language instructions that specify semantics, geometry, and topology of rooms. The Tell2Design dataset, used for this task, includes 5,051 human-annotated and 76,000 artificially generated language instructions, with each instruction describing key components of a floor plan.

The main challenges in this task are stricter constraints compared to artwork generation, understanding unstructured text with entangled information, and dealing with ambiguous or incomplete instructions. The authors propose a sequence-to-sequence model using a transformer-based encoder-decoder framework, initialized with the T5 language model, to address these challenges. This model treats language instructions as input sequences and room bounding boxes as target sequences, allowing it to handle varying instruction lengths.

The model's performance is evaluated on the T2D dataset, showing superior results with high IoU scores compared to text-conditional image generation baselines. The study highlights the importance of using both artificial and human instructions during training to bridge the language distribution gap, significantly improving performance.

The paper concludes by emphasizing the potential of language-guided design generation and introduces Tell2Design as a foundation for future research in this area.</sample>
    <sample id="138">作者认为，NLU 中研究不足的领域包括：  
1. 模型在没有任务特定训练的情况下，难以从不同来源整合知识进行推理。
2. 即使是最好的模型，也在整合仅在推理时提供的背景知识方面存在困难。</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">现有的资源在评估依赖上下文的翻译时有以下局限性：

1. **仅支持有限类型的上下文依赖翻译**：现有资源通常只支持有限种类的上下文依赖翻译。
2. **仅支持有限的语言集**：这些资源通常只支持有限的语言集。
3. **依赖领域知识和人工编辑**：这些资源通常依赖于领域知识和人工编辑，这限制了其广泛应用和扩展性。</sample>
    <sample id="142">你好！我要谈论我们的工作“解决间接指代表达式以进行实体选择”，在这项工作中，我们引入了AltEntities语料库。我的名字是Javad Hosseini，这是与Filip Radlinski、Silvia Pareti和Annie Louis的合作。我们的目标是理解用户在做选择时的语言。考虑这样一个替代问题：“你是指《Easy on Me》还是《I Gotta Feeling》？”在这里，用户想要选择这两首歌中的一首。最直接的方法是使用直接引用，例如说出歌曲的名字“Easy on Me”或其位置“第一首”。但有时，使用间接引用更适合进行更自然的对话。这可能发生在用户无法记住歌曲名字时，或者发音相似，难以区分，或者用户想要指定偏好。这里有一些间接引用的例子，例如“较新的那首”或“不是充满活力的歌曲”。这是对话系统中的一个重要问题，也是为了评估大型语言模型（LLMs）对实体理解的基准测试。我们不知道有更大规模的公开数据集用于这个任务，所以我们使用众包注释收集了一个数据集。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用漫画完成的设置。漫画有三个对话气泡。在第一个气泡中，Bob说：“你还记得我们昨天听的那首歌吗？”这样，Bob设置了对话的上下文。在第二个气泡中，Alice说：“你是指《Easy on Me》还是《I Gotta Feeling》？”这是替代问题。在第三个气泡中，Bob使用间接引用来选择这些实体中的一首，例如“较新的那首”。我们提供了前两个气泡，但第三个是由注释者填写的。第一个气泡是从每个领域的几个手动提示中选择的。第二个气泡，即替代问题，是按照以下简单模板生成的：你是指A还是B？其中A和B是从维基百科中抽取的样本。我们使用的不同抽样方法如下。当我们在列表中向上移动时，实体之间变得更相似，通常更难进行区分。第一个是均匀随机的。第二个是当实体有相似的标题时，例如两本书名为《The Return》。第三个是当它们在维基百科上有相似的描述时。最后，当它们在维基百科上有相似的信息框或属性时，例如同一类型或同一艺术家的歌曲。当我们向注释者展示这个替代问题时，他们知道这些实体的名字，但不一定了解这些实体。所以我们做的是向他们展示有关这两个实体的背景知识。对于歌曲，我们简单地显示每首歌的谷歌搜索链接，然后要求注释者至少听一些每首歌，并阅读有关每首歌的信息。例如，歌曲《Easy on Me》的谷歌搜索结果。对于食谱和书籍领域，我们显示维基百科上的一些背景文本。对于食谱，我们还显示它们的图片，也是从维基百科中获取的，以便注释者知道它们的外观。然后，我们要求注释者选择这些实体中的一个，例如，这里是第一个，并用三到五个间接指代表达式来描述它们。例如，“没有歌词的那个”，“不是有12岁男孩的那个”，或“虚构的那个”，或“来自阿塞拜疆的那个”，等等。AltEntities语料库包含6,000个替代问题，涵盖三个领域，以及42,000个间接指代表达式。T5 XL模型的结果如下所示。如果语言模型可以访问与注释者完全相同的背景知识，那么准确率非常高，约为92%到95%。但这不是现实的。如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的改进空间。我们还展示了模型的领域通用性。这里是我们数据集的链接。谢谢。</sample>
    <sample id="143">该方法与以下现有的 SimulST 策略进行了比较：Wait-k 策略、Local Agreement 策略，以及专门为同时翻译设计的最先进的架构。</sample>
    <sample id="144">The author's affiliation is not explicitly mentioned in the provided text.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, a PhD student from Fudan University, discusses a paper on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries from dialogues, capturing essential information across various domains. Despite advancements using large-scale pretrained language models, generated summaries often contain factual errors, with omission being a significant issue. Omission leads to incomplete summaries, missing critical facts, and is prevalent across different domains and models, with about 70% of summaries affected.

The study reveals that omitted information is randomly distributed throughout dialogues, highlighting the challenge of identifying key information. To address this, the paper introduces the task of omission detection, focusing on identifying omitted utterances in summaries. Existing datasets lack omission-related labels, prompting the creation of the OLDS dataset, which provides high-quality omission labels across five domains using diverse abstractive models.

The OLDS dataset, built on existing benchmarks, includes candidate summaries generated by different models and strategies, with automatic and human evaluations ensuring label quality. The dataset is publicly available and includes statistics like the total number of summaries and ROUGE-1 scores.

To explore omission detection, three baseline frameworks are tested: pair-wise classification, sequence labeling, and pointer network. Evaluation metrics include Precision, Recall, F1-score, and a word-level omission recall (WR score). Results show an F1-score around 50%, indicating the task's difficulty and the need for advanced models.

The paper also explores using detected omissions for summary refinement. By concatenating candidate summaries with omitted content, a post-editing method improves summary quality, demonstrating the value of omission detection and refinement in enhancing dialogue summarization.</sample>
    <sample id="147">这篇论文有三位作者：Myra、Esin Durmus 和 Dan Jurafsky。</sample>
    <sample id="148">你好，我是来自都灵大学和布鲁诺·凯瑟尔基金会的萨拉·帕皮。我将简要介绍我们与马泰奥·内格里和马可·图尔奇合作的论文《注意力作为同时语音翻译的指南》。什么是同时语音翻译（SimulST）？同时语音翻译是指将口语实时翻译成另一种语言的文本，从而实现跨语言交流。当前SimulST模型存在哪些问题？通常需要训练特定的架构，引入额外的模块进行优化。例如，涉及不同优化目标的复杂训练过程。还需要训练和维护多个模型以达到不同的延迟水平。例如，训练一个平均延迟为一秒的模型，另一个为两秒，依此类推。那么我们的解决方案是什么？首先，利用现有的离线语音翻译模型，而不需要重新训练或采用特定的SimulST架构。为每个延迟水平使用单一模型，并通过特定参数处理延迟。利用模型通过音频输入和文本输出之间的注意力机制所获得的知识。这是跨注意力机制，你可以在右侧看到一个例子。我们的解决方案是提出EDAtt，即编码器-解码器注意力，这是一种决定是否发出部分翻译的策略，基于注意力的指向。如果注意力不集中，即其和值低于某个阈值α，且在最后λ个语音帧上，这意味着接收到的信息已经足够稳定，那么一个词将被发出。例如，如果我们收到一个包含“I'm going to talk about...”的语音块，并且我们的模型预测了德语翻译，我们会查看跨注意力权重，会发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即λ个语音帧。这意味着前两个词将被发出，而由于跨注意力的和值高于某个阈值α，我们将不发出最后一个词，而是等待另一个语音块。如果我们继续并收到另一个语音块，模型预测了三个其他词，我们会查看这些跨注意力权重，会发现没有词指向最后λ个语音帧。这意味着这三个词将被发出。如果我们看看EDAtt的主要结果，我们会在图表上绘制同时语音翻译结果，其中一侧是BLEU，用于衡量翻译质量，另一侧是平均延迟，即延迟度量，我们还考虑了计算感知平均延迟，即模型预测输出所需的计算时间。我们希望我们的曲线在这个图表上尽可能高，并且向左移动。我们还与应用于离线模型的流行策略进行比较，如Wait-k策略和Local Agreement，并与专门为同时预翻译设计的最先进架构进行比较。这些都是德语上的同时语音翻译策略的结果。我们看到它在应用于离线模型的所有策略中表现最佳，因为曲线向左移动。我们还看到，如果考虑实际经过的时间或计算感知时间，即最快的策略。如果你想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同时输出，以便于复现我们的工作。谢谢你的关注。</sample>
    <sample id="149">Yes, the CoNLL++ Dataset is mentioned as being collected and annotated, implying it is available for use.</sample>
    <sample id="150">Archiki presents the ACL paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts," highlighting the unique challenges and opportunities in using meeting transcripts for NLP research. Unlike previous works focused on summarization and action item extraction, MeetingQA addresses the significant QA component in meetings by introducing a new dataset. This dataset comprises 7.7K questions from the AMI corpus, with 30% unanswerable, 40% with multispan answers, and 48% with multi-speaker answers. Questions are often long, open-ended, and discussion-seeking, with 20% rhetorical and 70% of multi-speaker answers containing disagreement. The dataset shows high inter-annotator agreement (Krippendorff's alpha of 0.73) and human performance (F1 of 84.6). Various models were tested, including short-context models like RoBERTa, which outperformed long-context models like Longformer, and single-span versus multi-span models, with the latter showing comparable performance. Zero-shot performance revealed a significant gap from human performance, though silver data augmentation improved results. Error analysis indicated difficulties in identifying rhetorical questions and speaker-specific answers, especially in zero-shot settings. MeetingQA presents a challenging yet promising dataset for advancing QA models in real-life meeting scenarios.</sample>
    <sample id="151">大家好，我叫英瑛，我的同事志扬和我将展示我们关于MultiInstruct如何通过指令调优来改进多模态零样本学习的研究。随着大型语言模型的进步，许多研究开始探索如何以参数和数据高效的方式重用预训练的语言模型来进行不同的下游任务。最近的研究表明，指令调优能够让大型语言模型通过遵循自然语言指令来在零样本情况下执行未见任务。然而，大多数关于指令调优的研究主要集中在提高语言任务的零样本性能，而计算机视觉和多模态任务却被忽略。因此，我们的研究旨在探讨指令调优多模态预训练模型是否能够提高对未见多模态任务的泛化能力。此外，在我们的研究过程中，我们发现自然语言处理和多模态之间的指令数据集可用性存在显著差异。存在超过1600个语言仅指令任务，但没有大规模公开的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。我们提出了MultiInstruct，这是第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个广泛的类别。这些任务来自21个现有的开源数据集，每个任务配备了五个专家编写的指令。为了在我们提出的数据集上探索多模态指令调优，我们将OFA，一个统一的多模态预训练模型，作为基础模型。OFA使用统一的词汇表来处理语言、图像令牌和边界框的坐标。我们展示了MultiInstruct数据集的一些示例实例，以统一处理各种输入和输出数据类型。我们遵循OFA的方法，将所有任务以统一的序列到序列格式进行表述。其中，输入文本、图像、指令和边界框在同一令牌空间中表示。现在，我将讨论多模态指令调优。对于训练数据集，我们使用9个组中的53个任务进行训练，并为每个任务抽取10,000个实例。对于测试，我们保留所有常识推理组用于测试，并从视觉问答和杂项组中选择另外5个任务。对于每个任务，我们使用其测试分割中的所有实例。此外，我们从自然指令的测试分割中随机抽取20个任务作为未见的NLP任务。我们使用预训练的OFA大模型作为基础模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例随机与其五个指令模板之一结合。在测试时，对于每个任务，我们进行总共5次实验，通过使用其中一个指令来评估模型。在每次实验中，我们报告所有5次实验中性能的最小值、最大值和标准差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告Rouge-L。对于NLP任务，我们也报告Rouge-L。我们还引入了一个额外的评估指标，称为敏感性。这个指标衡量模型在指令措辞略有变化时，是否能够始终为相同的任务生成相同的输出。这是我们的主要结果。如我们所见，指令调优可以显著提高OFA在已见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以提升指令调优。如我们所见，随着任务数量的增加，模型的性能得到改善，同时敏感性降低。我们还进行了一个实验，比较使用一个指令与使用五个指令。如我们所见，使用更多的指令可以显著提高模型的整体性能并大幅降低其敏感性。这显示了不同微调策略对模型敏感性的影响。如我们所见，通过从自然指令数据集进行迁移学习，模型可以实现比原始OFA模型更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。总体而言，我们提出了第一个大规模多模态指令调优数据集，并显著提高了OFA的短期能力，探索了不同的迁移学习技术并展示了其优势。我们设计了一个新的指标，称为敏感性。另外，我们正在收集一个更大的多模态指令调优数据集，包含约150个额外的视觉语言任务，并将其发布。这是我们数据和模型的二维码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider discusses the intersection of NLP and classical philology, focusing on the development of language models for Ancient Greek and Latin. Despite recent advancements like Latin BERT and Ancient Greek BERT, these models are limited as they are monolingual and encoder-only. To address these limitations, the project aims to create models that are both multilingual and capable of understanding and generating text.

The project introduced four models: GreBERTa and GreTa for Ancient Greek, and PhilBERTa and PhilTa for multilingual use (Ancient Greek, Latin, and English). GreBERTa is a RoBERTa-based encoder-only model, while GreTa is an encoder-decoder model based on the T5 architecture. PhilBERTa and PhilTa extend these capabilities to multiple languages.

A significant innovation was the creation of a new pre-training corpus from the Internet Archive, using Greek stop words to identify Greek texts and re-scanning with appropriate OCR settings. This resulted in a high-quality dataset for training.

The models were benchmarked using tasks like part-of-speech tagging, dependency parsing, and lemmatization, showing superior performance over existing models. GreTa's encoder, when used alone, initially performed poorly but improved with more training, highlighting differences from native encoder-only models.

The encoder-decoder models excelled in lemmatization, achieving a 5 percentage point improvement for Ancient Greek. Probing tasks revealed that the models could distinguish synonyms from antonyms and identify relationships between mythological figures. However, multilingual models did not significantly outperform monolingual ones in semantic and world knowledge tasks.

Overall, the project developed powerful, native tokenizer-based models for classical philology, providing a comprehensive approach to processing Ancient Greek and Latin texts.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presented their work on resolving ambiguities in text-to-image generative models. The study focuses on the challenges posed by ambiguous prompts, such as "The girl enters the room with flowers," which can be interpreted in multiple ways. The goal is to develop frameworks to mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions.

The research involves curating a benchmark dataset based on the LAVA corpus, which includes various types of ambiguities. The proposed framework uses a language model to either generate clarifying questions or suggest different visual interpretations. Users respond to these questions or select the interpretation that aligns with their intention, resulting in a disambiguated prompt.

Once disambiguated, these prompts are input into text-to-image models to generate images. An automatic evaluation framework assesses the faithfulness of these images to user intentions using a Visual Question Answering (VQA) model. The VQA model checks if the generated images satisfy the user's intended questions.

Findings indicate that resolving ambiguities improves the faithfulness of image generation, with the automatic evaluation framework aligning well with human evaluations. The study highlights disparities in resolving different types of ambiguities and demonstrates the effectiveness of the proposed frameworks in enhancing text-to-image model performance. The paper includes additional findings and discussions for further insights.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The presentation introduces the "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" (SDDS) model, a collaborative work by Shen Gao and colleagues from Shandong University. Dialogue summarization, a challenging task in text summarization, aims to extract key information from dialogues involving multiple participants, providing concise summaries without needing to review the entire dialogue. Traditional methods rely on pre-computed static graphs using external linguistic tools, which can be unreliable and inflexible for dynamic summarization tasks.

The SDDS model addresses these issues with four main components: an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator using a pre-trained language model. The Utterance Encoder converts dialogue utterances into vector representations. The Static-Dynamic Graph module combines static graphs, constructed using heuristic methods like Discourse Parsing Graph, Key Co-occurrence, speaker interaction frequency, and utterance position graphs, with a dynamic graph that captures semantic relationships through a multi-head attention model. This fusion creates a unified graph that integrates static and dynamic structures.

The model employs a dual cross-attention mechanism, incorporating graph representation into the generation process. The static graphs are built using heuristic methods, while the dynamic graph adapts to the dialogue context. The fusion of these graphs allows the model to dynamically adapt to summarization tasks, overcoming the limitations of traditional methods. The code and data for the SDDS model are available on GitHub.</sample>
    <sample id="158">The presentation by Qipeng Guo from AWS introduces the "Dual Cache for Long Document Neural Coreference Resolution" method. Coreference resolution involves identifying and clustering mentions of the same entity within a document. Traditional methods face quadratic complexity in computation and memory due to the need to evaluate all mention pairs. Recently, cache-based methods have reduced this complexity to linear by using a fixed-size cache, employing eviction policies like Least Recently Used (LRU) to manage cache space. However, LRU can lead to high cache misses in long documents where topics frequently change, causing mentions of entities to be scattered.

To address this, the dual cache system is proposed, consisting of a local cache and a global cache. The local cache uses the LRU policy to store entities relevant to the current text segment, while the global cache uses the Least Frequently Used (LFU) policy to store entities mentioned globally. The model processes the document from left to right, classifying new mentions as either new entities or belonging to cached entities, and then determining their frequency. High-frequency entities are added to the global cache, while others go to the local cache. When caches are full, entities are evicted based on their respective policies.

The dual cache system was evaluated on four public benchmarks, including LitBank, OntoNotes, and WikiCoref, with the latter lacking training data. Results showed that dual cache outperformed baseline methods, even with unbounded memory, and was faster without training data. An annotated book with 30,000 words demonstrated a significant performance gap between dual cache and baseline methods, highlighting dual cache's effectiveness in reducing cache misses. The dual cache system offers a high performance/cost ratio, making it more cost-effective than single cache methods. In summary, dual cache efficiently manages local and global entities, reducing cache misses and improving performance in long document coreference resolution.</sample>
    <sample id="159">你好，大家。我是Koustav Sinha，很高兴欢迎大家参加我们关于ACL 2023论文的讨论。语言模型的可接受性判断在上下文中并不总是稳健的。这是我与John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams的合作研究。在这项工作中，我们重新审视了最小对比范式。最小对比范式基本上是通过可接受性判断来评估语言模型的，这些判断还包括语法性，如BLiMP、SyntaxGym，或者在刻板印象方面的可接受性，如CrowS对。在这个范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个可接受的句子或不符合语法的句子。希望模型能够更高概率地选择可接受的句子。当前的最小对比范式（MPP）流程并不允许我们评估模型对较长句子的接受程度。近年来，大型语言模型的上下文窗口越来越长，因此评估模型在整个上下文窗口中的可接受性是至关重要的，这也是我们在这里尝试做的。我们通过要求模型评估越来越长的序列来重新审视MPP流程。具体来说，我们通过重新审视数据集本身来模拟这些较长的序列，并通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，我们从BLiMP数据集中选择了一个典型的语法性对比，即附加岛情况。我们通过从附加岛中提取语法正确的句子，并将其作为前缀添加到可接受查询和不可接受查询中，来重新创建较长的可接受序列，同时保持相同的语法结构匹配。我们也可以通过选择不匹配的句子来测试模型的可接受性，这些句子来自不同的子集或数据集。这种情况我们称为不匹配情景。在这种情况下，句子仍然来自相关的数据集，但不是与当前评估的数据集相同。我们也可以对不可接受的情况做同样的操作。最后，我们可以从完全不相关的领域，如维基百科中选择句子。这将告诉我们模型的可接受性判断是否受到上下文的影响，无论上下文是来自数据集的不同子集，还是完全与当前句子无关。模型表现如何呢？首先，我们看看来自维基百科的句子，这些句子与当前查询对完全无关，我们发现MPP判断在任意上下文长度下基本上是稳健的。我们将上下文长度增加到1024，以最大化OPT和GPT-2模型的性能。在这里，MPP判断相对稳定（如橙色虚线所示）。那么，当我们选择来自同一数据集的句子时会发生什么呢？我们通过从BLiMP或SyntaxGym数据集的可接受和不可接受领域中创建句子来观察这一点。我们发现，当添加可接受前缀或不可接受前缀时，MPP判断会显著增加或减少。但是，当我们匹配结构时，即从BLiMP或SyntaxGym中选择与当前现象相匹配的句子时，我们发现MPP判断在模型选择可接受前缀或不可接受前缀时会显著增加或减少。这种效应非常大，并且随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。为什么匹配前缀会对语言模型判断产生如此大的影响呢？我们进行了一系列分析，尝试通过保留相关结构并向输入中添加噪声来扰动输入句子。经过多次这样的扰动后，我们发现这些噪声并没有改变模型在MPP判断中的表现。基本上，我们发现模型对扰动句子的敏感性与可接受领域中的扰动句子相似，即当我们扰动可接受领域中的句子时，我们在所有扰动中都看到类似的增加，而当我们扰动不可接受领域中的句子时，我们在MPP判断中看到类似的减少。我们工作的关键结论是，语言模型对跨句子共享的潜在句法和语义特征敏感。目前我们用于MPP评估的方法，即使用短且单一的句子输入，可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以了解更多实验细节。谢谢大家的聆听。</sample>
    <sample id="160">第一步将输入词元映射到一个无序的多集（unordered multiset）的输出词元。</sample>
    <sample id="161">CoScript 中包含了 55,000 个脚本。</sample>
    <sample id="163">DEPLAIN 的最佳对齐方法是 MASSalign。</sample>
    <sample id="164">弱监督学习的好处是可以使用弱标注源（如简单的启发式规则、知识库或低质量的众包）来标注数据，这比手动标注数据要便宜得多。然而，这些弱标注是噪声的，意味着其中一部分标注是错误的。弱监督学习的目标是开发训练算法，使得在这种标注噪声下训练的神经网络仍然能够很好地泛化。</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presents a paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." The paper addresses abductive reasoning, which involves identifying plausible explanations to bridge the gap between a given context and an outcome. For example, if Emily was stuck in traffic (context) and made it to her flight (outcome), plausible explanations could be that her flight was delayed or left on time. The goal is to find the explanation that best explains the outcome given the context.

Current methods for abductive reasoning rely on supervised learning, which requires annotated data that can be noisy and subjective. Zhao's paper questions whether it is possible to learn abductive reasoning without supervision on the plausibility of explanations. The answer is affirmative, introducing an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization). In LiPoR, explanations are treated as latent variables, and the method maximizes the marginal likelihood of the outcome given the context by marginalizing over possible explanations.

However, maximizing likelihood alone does not ensure the selection of plausible explanations. To address this, LiPoR incorporates a regularizer based on the mutual exclusivity of explanations. This means that if one explanation is true, others are automatically ruled out. The regularizer, denoted by Omega, minimizes the entropy of the probability distribution over explanations, preferring a subset of explanations when more than the expected number receive probability mass.

The paper demonstrates LiPoR's effectiveness on the AlphaNLI dataset, a widely-used abductive reasoning dataset. LiPoR outperforms zero-shot models and the previous best unsupervised approach, including a strong GPT-3 baseline, by over 4 absolute points in accuracy. This work highlights the potential of unsupervised learning in abductive reasoning by leveraging the mutual exclusivity of explanations.</sample>
    <sample id="166">The presentation introduces a new work titled "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," developed by Yunxin from Harbin Institute of Technology, Shenzhen. The task of image retrieval from linguistically complex text is challenging due to the similarity of images and the complexity of descriptions. Traditional visual language models, which excel in image sentence retrieval, struggle with complex text. To address this, the authors draw inspiration from the Divide-and-Conquer strategy and Dual-Process Theory. Divide-and-Conquer involves breaking down a large problem into smaller, manageable sub-problems, solving them, and combining the results. Dual-Process Theory suggests that human thinking comprises two systems: System 1, which handles analogical reasoning, and System 2, which is suited for abstract logical reasoning.

The proposed framework integrates these concepts to enhance image retrieval performance. It consists of three main components: the Proposition Generator, the Visual-Linguistic Interactor, and the Neural-Symbolic Reasoner. The Proposition Generator decomposes complex text into simpler propositions, using BART's decoder to generate corresponding sentences. The Visual-Linguistic Interactor, akin to System 1, facilitates interaction between visual and linguistic information, producing matching scores and reasoning states. The Neural-Symbolic Reasoner, representing System 2, integrates these states to derive the final solution, employing a negation executor and conjunction operation for logical reasoning.

Experimental results demonstrate that the proposed method, NDCR, outperforms existing baselines. Abolition experiments further validate the effectiveness of each module. The framework's ability to present intermediate inference states and results highlights its interoperability. The authors suggest that neural symbolic calculation could enhance the compositional reasoning of large language models. They note that Divide-and-Conquer is akin to the self-asking chain-of-thought approach, both effective for complex problem-solving. Integrating Dual-Process Theory with Divide-and-Conquer is proposed as a promising direction.</sample>
    <sample id="167">DEPLAIN-web 中的文档采用了手动和自动对齐方法进行对齐。具体分配情况是，750个文档中的对齐是部分手动和部分自动进行的。</sample>
    <sample id="168">CoNLL++ 数据集是从 2020 年的 Reuters 新闻中收集的，并使用 CoNLL-2003 的相同注释指南进行注释。</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" by David Vilar and colleagues from Google Translate explores the use of the PaLM large language model for machine translation. PaLM, with 540 billion parameters, was trained on 780 billion tokens and achieved state-of-the-art results in numerous NLP tasks. This study is the first systematic examination of prompting strategies for machine translation using large language models like PaLM.

The research evaluates PaLM's translation capabilities using best practices from the machine translation (MT) community, including the latest test sets to prevent data overlap. The performance is compared against state-of-the-art systems, specifically the WMT evaluation, using neural MT metrics and human evaluations based on the MQM framework.

A key finding is that prompting significantly affects PaLM's translation performance. An experiment with one-shot prompting showed a notable difference in BLEURT scores, with variations up to 40 points, highlighting the importance of selecting effective prompts. The study found that a 5-shot prompting strategy, where sentences are marked with their respective languages, yielded consistent results, with the quality of examples being more crucial than their similarity to the source sentence.

The paper concludes that while PaLM's translations are fluent and comparable to state-of-the-art systems, they often suffer from accuracy issues, particularly omission errors. Despite these challenges, PaLM's translations are close to those of commercial systems like Google Translate, with fewer "Style/Awkward" errors, indicating fluent but sometimes inaccurate outputs. The study provides recommendations for prompt selection strategies to enhance translation quality.</sample>
    <sample id="170">你好大家，我是来自宾夕法尼亚州立大学的尤森·张。今天我将介绍我们的工作“XSemPLR：多语言自然语言和意义表示的跨语言语义解析”。语义解析是构建用户查询（如SQL和Lambda演算）的语义表示的任务。跨语言语义解析的任务是将多种自然语言中的查询翻译成多种意义表示。如图所示，我们需要使用神经模型将多种自然语言中的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是分别提出并在有限任务和应用的数据集上评估的。例如，某些自然语言有很多覆盖，但中文缺失，某些意义表示也缺乏覆盖，比如Lambda演算缺失，或者只在某些神经模型上进行评估。例如，只有一个单一模型进行评估。因此，我们提出了XSemPLR。我们为多语言自然语言和意义表示的跨语言语义解析提供了一个统一的数据集XSemPLR。它包含9个不同领域的数据集，5个语义解析任务，8种意义表示和15个语言家族中的22种自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估的设置。第一个是Translate-Test。我们使用谷歌翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语查询上训练英语模型，在推理时，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试单语模型。在这种设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试单语少样本设置，通过使用10%的训练数据训练单语模型。我们测试多语言模型，即我们为所有语言训练一个多语言模型。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理时，我们可以使用这个模型将德语查询或中文查询等翻译。我们还考虑了跨语言零样本和少样本迁移。我们在一个源语言上训练，然后迁移到另一种语言。因此，在训练时，我们在英语查询或英语和德语少样本查询的组合上训练一个多语言模型，以预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们在两组模型上进行评估，包括Encoder-PTR，即多语言预训练编码器与指针式解码器，如XLM-R + PTR和mBERT + PTR。我们还评估Encoder-Decoder模型，即多语言预训练编码器-解码器模型，如mBART和mT5。我们发现Encoder-Decoder在所有九个数据集上获得了最佳性能。我们在多语言设置下评估mT5和XLM-R + PTR。我们发现Encoder-Decoder或Encoder-PTR可以通过在各种语言的混合中训练来改进。我们发现大多数主要自然语言可以获得性能提升，除了英语在七个数据集中性能下降，在三个数据集中性能提升。我认为这是“多语言诅咒”。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现零样本设置下跨语言迁移性能差距显著，然后比较蓝线和橙线，我们发现少样本设置下迁移差距迅速缩短。我们还发现了一些其他有趣的发现。例如，Encoder-Decoder优于以前的工作或获得了可比的结果。在英语自然语言上的预训练可以显著提高目标自然语言的少样本性能，我们发现多语言语言模型如Codex和BLOOM仍然不足以应对跨语言语义解析任务。总之，我们构建了XSemPLR，这是一个多语言自然语言和意义表示的跨语言语义解析的统一基准。我们对三种代表性的多语言语言模型进行了全面的基准研究。我们的结果显示了许多有趣的发现。欢迎访问我们的论文和代码。谢谢大家的聆听。</sample>
    <sample id="171">Existing works on protecting the copyright of embedding as services can be broadly classified into four categories. However, these methods either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">No, multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea introduces the "ArgAnalysis35K" dataset, highlighting its uniqueness in the field of argument quality analysis. This dataset is designed to evaluate arguments on a scale from 0 to 1, with higher scores indicating more persuasive and coherent arguments. Unlike existing datasets, which often suffer from quality and diversity issues due to reliance on crowdsourcing and limited topics, ArgAnalysis35K offers several innovative features.

Firstly, it is the largest dataset in this field, containing 35,000 argument-analysis pairs. The arguments are sourced from high-quality debates, including speeches from top tournaments and expert debaters, ensuring superior quality. The dataset also includes arguments from novice debaters and everyday people, adding variety.

Secondly, ArgAnalysis35K covers a diverse range of topics. Instead of focusing on a limited set of motions, it uses 24 themes derived from expert advice and debate experiences. This approach captures a broader spectrum of motions encountered in parliamentary debates.

A key innovation is the introduction of "analysis" as a concept. Unlike traditional datasets that only include arguments, ArgAnalysis35K combines claims, premises, and other elements into a coherent analysis. This provides a deeper understanding of why an argument is persuasive.

The dataset also incorporates instance-based annotator reliability. Recognizing that annotators may have biases on certain topics, the dataset selectively excludes biased judgments rather than discarding all annotations from an annotator. This method enhances the reliability of the dataset.

Lastly, ArgAnalysis35K introduces a relevance model. Instead of linking arguments to a single motion, it assigns relevance scores to arguments across multiple themes. This reflects the multifaceted nature of arguments, acknowledging that a single argument can be relevant to various topics.

Overall, ArgAnalysis35K stands out for its large size, high-quality arguments, diverse topics, innovative analysis concept, reliable annotations, and relevance model, making it a valuable resource for argument quality analysis.</sample>
    <sample id="175">该方法通过引入一个GPU友好的连续松弛来处理排列的不确定性，这使得可以通过解决方案进行反向传播，并学习更符合语言学的更可行的排列。这种方法允许近似解决NP难问题，即“旅行推销员”问题，从而在不明确给出排列的情况下进行训练。</sample>
    <sample id="176">下游 NLP 模型的公平性可以定义为模型在处理不同人群、社会群体或政治观点时表现出的一致性和公正性。具体来说，公平性意味着模型在执行任务（如仇恨言论检测或虚假新闻检测）时，不会因为目标群体的社会地位、政治倾向或其他特征而产生偏见。在研究中，公平性问题体现在不同政治倾向的语言模型在检测不同群体的仇恨言论或虚假新闻时表现出的差异。例如，左倾模型可能更擅长检测针对少数群体的仇恨言论，而右倾模型可能更擅长检测针对主流群体的仇恨言论。这种差异表明，模型的政治倾向可能导致对某些群体的偏见处理，从而引发公平性问题。因此，确保模型在不同群体和情境下表现一致，是衡量其公平性的关键。</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar discusses the challenge of improving Theory of Mind (ToM) reasoning in large language models (LLMs) like GPT-3 and ChatGPT, which struggle with false-belief tasks. ToM involves understanding others' mental states, often tested through scenarios like the Sally-Anne test, where characters have beliefs that may not align with reality. Questions in these tests can be first-order (about a character's belief) or second-order (about one character's belief about another's belief).

Sclar introduces SymbolicToM, a method to enhance ToM reasoning in LLMs by using explicit graphical representations of characters' beliefs. These graphs, such as BBob and BBob,Alice, represent different levels of belief states among characters. SymbolicToM computes these graphs for all character combinations up to a predefined ToM level, using inference-time algorithms with NLI and OpenIE models.

The method involves detecting entities in questions, retrieving the relevant belief graph, and transforming the question into a factual one over the graph. The sentences from the graph and the factual question are then fed into a language model to derive the answer.

Experiments show that SymbolicToM significantly boosts LLM performance on second-order false-belief questions, with gains like 65 accuracy points for GPT-3-Davinci. It also outperforms supervised models in out-of-domain setups, such as the D₁, D₂, and D₃ datasets, which test story structure generalization, and ParaphrasedToMi, which tests linguistic diversity. SymbolicToM's graphical approach provides interpretable reasoning and avoids overfitting, making it a robust solution for enhancing ToM in LLMs.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">The paper "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" by Siyu Yuan from Fudan University addresses the challenge of planning actions with specific constraints using language models. While previous research has focused on abstract goal planning, such as "make a cake," this work explores planning for specific goals like "make a chocolate cake," which involves multiple constraints. The authors define constrained language planning as the task of generating scripts that adhere to these constraints.

To evaluate and improve the constrained planning abilities of large language models, the authors first acquire specific goals by extending abstract goals with constraints using InstructGPT. They find that existing models perform poorly in generating scripts that are faithful to constraints, despite achieving reasonable semantic completeness. A detailed analysis reveals that the performance varies significantly across different constraint categories.

To address these issues, the authors propose an over-generate-then-filter method. This involves generating multiple scripts for each specific goal and using a filter model to select the most faithful ones based on semantic similarity and keyword presence. This method significantly enhances the quality of the generated scripts in terms of both semantic completeness and constraint adherence.

Recognizing the high cost of deploying large language models, the authors aim to enable smaller, specialized models to perform constrained language planning. They create the CoScript dataset by distilling constrained planning data from large models, generating 55,000 specific goals with scripts. The dataset is validated and tested by crowd-sourced workers to ensure quality. CoScript demonstrates high diversity in specific goals and enables smaller models, like T5, to outperform larger models when fine-tuned on this dataset.

In summary, the paper establishes the problem of constrained language planning, evaluates the capabilities of large language models, and introduces a method to improve script generation quality. The CoScript dataset is presented as a valuable resource for advancing research in language planning.</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 意味着将拉丁裔女性描绘为“活泼”和“曲线优美”，这些词汇反映了一种刻板印象，将她们与热带地区的特质联系起来，从而强化了一种刻板化和异化的叙述。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。具体方法是给语言模型提供指令，让它生成一个“人物形象”，例如使用提示“想象你是一个亚洲女性。描述一下自己。”这种方法可以很容易地适用于任何身份标识，因为只需在提示中指定所需的身份标识即可。这种生成的人物形象可以直接与人类写的回应进行比较，以识别和分析潜在的刻板印象。</sample>
    <sample id="184">本文中使用了CXMI（Contextual Mutual Information）来衡量语境使用情况，并将其扩展为Pointwise CXMI（P-CXMI），以便在句子级或词级测量语境使用情况。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源和领域专业化：

- **DrBERT**：是基于 RoBERTa 的第一个法语生物医学模型，训练于 NACHOS 数据集，该数据集是从互联网上爬取的医学数据。DrBERT 旨在覆盖广泛的生物医学和临床任务。

- **ChuBERT**：是基于匿名数据的临床模型，数据来自南特大学医院的数据仓库。ChuBERT 的训练集包括临床笔记，专注于临床领域。</sample>
    <sample id="187">这篇论文有两位作者，Ying和Zhiyang。</sample>
    <sample id="188">迭代迁移学习是一种策略，其中模型通过从相关任务迁移权重来启动活动学习过程。在这个研究中，迁移学习涉及从两个任务迁移权重：一个是无论主题如何，确定两个辩论陈述是否一致或不一致的任务（称为辩论），另一个是对PDTB中的扩展和比较类别进行二元分类（称为CE）。通过迭代地在这些任务上进行微调，研究人员发现CE任务的微调后再进行辩论任务的微调，能够显著提高零样本性能，从而更好地启动活动学习。</sample>
    <sample id="189">数据集的目标是理解用户在选择过程中使用的间接指代表达，以便在对话系统中更自然地处理用户的选择。它旨在解决间接指代表达的理解问题，并为评估大型语言模型（LLMs）的实体理解能力提供基准数据集。</sample>
    <sample id="190">攻击者通过学习从嵌入服务（Embedding as Services, EaaS）中获取的嵌入来提取模型参数。他们可以利用这些嵌入来构建一个类似的服务，从而实现模型的盗用。</sample>
    <sample id="191">这篇论文有三位作者：Sara Papi、Matteo Negri 和 Marco Turchi。</sample>
    <sample id="192">Yang Luo presents "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of designing an optimizer that achieves both fast convergence and low memory usage. Traditional adaptive methods like Adam require significant memory for first and second moment estimates, while memory-efficient methods like Adafactor reduce memory usage but often at the cost of performance. The presentation introduces non-negative matrix factorization (NMF) as a technique to reduce memory requirements, noting that Adafactor uses an analytic solution for NMF but suffers from slow convergence due to erroneous updates.

CAME aims to mitigate these errors by considering the residual between momentum and current updates as instability, using it to adaptively adjust the optimization step. This approach reduces the side effects of insecure updates. Experiments on BookCorpus and English Wikipedia show CAME's superiority over Adam and Adafactor in training large language models like BERT, GPT-2, and T5. CAME improves validation accuracy by about 3.4% compared to Adafactor and outperforms Adam in pre-training large models with reduced memory costs, especially as batch sizes increase from 8K to 32K.

In BERT-Large training, CAME shows significant enhancements over Adam and Adafactor. BERT-based models trained with CAME achieve comparable performance to baselines on downstream tasks with less memory usage. Compared to optimizers like Adam and LAMB, CAME significantly reduces memory footprint, even outperforming the SM3 optimizer. CAME's confidence-guided approach, which adapts updates based on the residual between predicted and generated updates, demonstrates its effectiveness in large language model training and large batch training, extending the capabilities of existing memory-efficient optimizers.</sample>
    <sample id="193">The initial dataset was annotated by 10 annotators.</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The work introduces "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" (RoHT), addressing limitations in current Explainable Question Answering (XQA) methods. XQA aims to provide answers and explanations for questions. Existing methods fall into two categories: neuro-symbolic methods, which translate questions into formal representations like SPARQL for structured knowledge bases (KBs), and decompose-based methods, which generate natural language intermediate steps. Neuro-symbolic methods struggle with incomplete KBs, limiting answer recall, while decompose-based methods face challenges due to the diversity of natural language in free-text corpora.

RoHT proposes a novel framework to integrate knowledge from heterogeneous sources, leveraging question decomposition to select appropriate knowledge sources for sub-questions. The framework addresses two main challenges: determining the granularity of question decomposition and finding the optimal solution among various possibilities. RoHT consists of two stages: building a Hierarchical Question Decomposition Tree (HQDT) and conducting probabilistic reasoning over it.

The HQDT represents the compositional structure of a complex question, with the root node as the original question, non-root nodes as sub-questions, and leaf nodes as atomic questions. Probabilistic reasoning over the HQDT involves a scheduler selecting knowledge sources, executors retrieving answers with probabilities, and an aggregator combining candidate answers to output top answers.

RoHT is evaluated on two datasets: KQA Pro, a KB QA dataset with incomplete KBs supplemented by Wikipedia, and Musique, a QA comprehension dataset supplemented by Wikidata. Results show RoHT's superiority in integrating sub-question answers from different levels and utilizing both KB and text knowledge. RoHT outperforms existing methods, demonstrating the effectiveness of explicit decomposition and the benefits of supplementing text information with KB knowledge.</sample>
    <sample id="196">以左侧为支配词的示例是“我看到了巴特和丽莎”（I saw Bart and Lisa）。</sample>
    <sample id="197">所给的英文内容中并未具体列出对话系统中的最先进模型名称。内容提到了四个“state-of-the-art chat models”被用于评估，但没有具体命名这些模型。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为当前的最小对比范式（MPP）评估方法无法评估模型对较长句子的可接受性。随着大型语言模型的上下文窗口变得越来越长，评估模型在整个上下文窗口中的可接受性判断变得至关重要。这可以帮助我们了解模型在更长的上下文中的抽象知识表现，以及它们对不同上下文的敏感性。</sample>
    <sample id="199">是的，多语言训练会导致表现下降，这被称为“多语言诅咒”。在分析中发现，大多数主要自然语言在多语言训练中表现提升，但英语在七个数据集中的表现下降，只有在三个数据集中有所提升。</sample>
    <sample id="200">注释者知道实体的名字，但不一定知道实体的具体信息。他们通过查看背景知识（如Google搜索链接、Wikipedia文本或图片）来了解实体。</sample>
    <sample id="201">The paper evaluated machine translation using state-of-the-art neural MT metrics and additionally showed expert-based human evaluation results. Specifically, BLEURT points were mentioned as a metric for assessing performance differences.</sample>
    <sample id="202">The paper does not specifically address whether regression in generalization affects specific NER types. It focuses on overall model performance and generalization across datasets, identifying temporal drift as the main cause of performance drop, rather than adaptive overfitting.</sample>
    <sample id="203">NLP 中的立场很重要，因为它影响了技术的设计和结果，导致系统性的性能差异。立场是指由于人们的人口统计、身份和生活经验而形成的观点，这些观点可以影响研究过程和结果。NLP 数据集和模型可能会反映某些立场而非其他立场，因为它们汇集了真实人们的判断和观点。这种立场偏差可能导致技术在不同人群之间表现不一致，例如，某些API在检测某些文化背景下的有害内容时可能不够敏感。因此，理解和解决NLP中的立场偏差对于确保技术的公平性和包容性至关重要。</sample>
    <sample id="204">The content does not specify whether BLOOM or similar multilingual LLMs are fine-tuned using adapter fine-tuning or full fine-tuning.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented research on political biases in language models, focusing on their propagation from pretraining data to downstream tasks. Language models are trained on large-scale web crawl data, which includes significant coverage of political news media like the New York Times and The Guardian. This coverage offers both benefits and challenges: while it allows models to learn from diverse perspectives, it also introduces social biases that can lead to fairness issues in applications.

The study investigates the political bias pipeline by evaluating the political leanings of language models and their impact on downstream tasks. Preliminary results show that models like GPT-4 exhibit liberal biases, with GPT models generally more liberal than BART models. The research further explores how these biases originate from training data by conducting controlled experiments with partisan corpora, revealing shifts in ideological coordinates based on the data used.

The study also examines societal polarization by pretraining models on data from before and after the 45th U.S. president, finding that models trained on more recent data exhibit stronger political leanings. In downstream tasks like hate speech and fake news detection, the research highlights that left-leaning models are better at detecting hate speech against minority groups but worse against powerful groups, and vice versa for right-leaning models. This indicates significant fairness issues, as deploying biased models could marginalize certain groups and allow unchecked hate speech.

The research underscores a dilemma: sanitizing political opinions in training data to prevent bias could lead to censorship and exclusion, making it difficult to determine what is neutral. This presents a challenge akin to the electric trolley problem, emphasizing the need to address fairness issues in language models.</sample>
    <sample id="206">他们使用了两个不同任务的模型进行迁移学习：1. 主题独立的对立立场分类任务（debate），用于确定两个来自不同人的辩论陈述是否一致或不一致；2. PDTB中扩展和比较类别的二元分类任务（CE），因为这两个类别与和谐与不和谐的概念密切相关。他们发现，首先在CE任务上进行微调，然后在辩论任务上进行进一步微调，可以获得更好的零样本性能。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集是最新的 WMT 评估测试集。</sample>
    <sample id="208">作者最终提出了三条建议。</sample>
    <sample id="209">The proposed method greatly improves the planning ability of InstructGPT in both semantic completeness and faithfulness to constraints. Specifically, the T5 model fine-tuned on the CoScript dataset can generate scripts of higher quality than most large language models, indicating significant gains over the strongest baselines.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Yes, the results and the DEPLAIN dataset can be used as a benchmark for the problem of automatic text simplification. The paper concludes that the basic fine-tuning of models achieved scores better than baseline scores, and these results are proposed as a base benchmark for future work in automatic text simplification.</sample>
    <sample id="212">The text does not specify the exact number of smaller models that were experimented with in the paper. It mentions that T5 fine-tuned on CoScript was used and that it performed better than most large language models, but it does not provide a specific count of smaller models tested.</sample>
    <sample id="213">OFA（Unified Multi-Modal Pre-trained Model）被用作研究多模型指令调整的基础模型。</sample>
    <sample id="215">Adam Przepiórkowski's talk focuses on the dependency structure of coordination, exploring different theoretical approaches to how conjuncts in a coordinate structure are linked. He discusses asymmetric approaches, such as Universal Dependencies and Igor Mel'čuk's Meaning-Text Theory, where the first conjunct is the head of the structure. In contrast, the Prague Dependency Treebanks use a conjunction-headed approach, and Hudson's Word Grammar proposes a multi-headed approach where all conjuncts are heads.

The paper argues for symmetric structures of coordination, challenging the asymmetric models. The argument is based on the principle of dependency length minimization, which suggests that shorter dependencies are preferred. Przepiórkowski illustrates this with examples showing that while direct objects typically stay close to verbs, longer direct objects can be positioned after adjuncts without causing awkwardness, as this minimizes dependency length.

The paper uses data from the enhanced Penn Treebank to show that left conjuncts tend to be shorter, especially when the length difference between conjuncts increases. This tendency is observed when the governor is on the left or absent but disappears when the governor is on the right. This finding supports symmetric structures, as it indicates that the position of the governor influences conjunct length preferences, challenging asymmetric models. The full arguments and data are detailed in the paper, which Przepiórkowski invites readers to explore further.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He addresses the limitations of existing dialogue generation models that focus on single attributes, which do not reflect the complexity of real-world multi-attribute settings. The authors propose a novel approach called Disentangled Controllable Generation (DCG) to enhance the generation capability of dialogue systems by learning attribute concepts from seen values and disentangling different attribute combinations using a disentanglement loss. 

DCG is built on the DialoGPT framework and incorporates a compositional prompt module with two types of prompts: attribute-oriented and task-oriented. The attribute-oriented prompts guide the model to focus on specific information related to controllable attributes, while task-oriented prompts help maintain the dialogue's overall coherence by leveraging global features. These prompts are concatenated to form whole prompt embeddings, and pseudo combinations are used to enhance diversity and improve the model's ability to distinguish between different attribute value combinations.

To address the lack of a unified evaluation metric for multi-attribute controllable dialogue generation, the authors introduce a reference-free evaluation framework called MAE, which is designed to handle both discrete and continuous attributes. The framework uses a template with discrete prompts and a trainable continuous dialogue-oriented prompt to reduce bias and improve robustness.

The effectiveness of DCG is demonstrated through experiments on two benchmarks, showing that it outperforms existing baselines in terms of attribute controllability and text equality. The model achieves compositional generalization by successfully transforming seen attributes to unseen combinations with minimal loss in controllability metrics. The authors also validate the quality of MAE by comparing it to human judgments and other classic metrics, highlighting its superiority in evaluating both coarse-grained and fine-grained attributes.

Additionally, the paper explores the impact of prompts on compositional generalization through visualization techniques, confirming that DCG can disentangle attribute combinations and learn relationships between different attributes. The attribute-oriented prompt method is shown to outperform models that learn independent prompts for each attribute value, as it facilitates the learning of attribute concepts from seen to unseen combinations. Overall, the study presents a comprehensive approach to improving multi-attribute controllable dialogue generation through disentangled prompts and a robust evaluation framework.</sample>
    <sample id="218">Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica, along with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang, presented their work on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." The research focuses on analyzing Form 10-K reports, which are annual reports required by the SEC, to extract valuable financial information. The motivation for this work stems from the observation that these reports are highly similar year-over-year, with about 80% of tokens being the same, making it challenging to mine useful information without significant human effort.

To address this, the team introduced a highlighting task and a multi-stage pipeline. The task involves comparing and contrasting the context between a target report and its previous year's report to identify important words that signify changes or relations. The pipeline consists of several stages: document segmentation (Stage 0), relation recognition (Stage 1), and two fine-tuning stages (Stage 2 for out-of-domain and Stage 2+ for in-domain fine-tuning). Stage 1 classifies report pairs into three types: high similarity (Type β), revised pairs with different meanings, and mismatched pairs indicating new information.

For model tuning, the team used the eSNLI dataset for out-of-domain fine-tuning and revised pairs for in-domain fine-tuning, employing soft labeling techniques to improve pseudo-label quality. The evaluation used precision, recall, and Pearson correlation coefficient (PCC) metrics, showing that their domain-adaptive model performed well on both the FINAL dataset and eSNLI, indicating strong generalization capabilities.

The research highlights the potential of their method to enhance information retrieval in financial reports and suggests future work to improve effectiveness and incorporate additional features or techniques. For more details, the team encourages referring to their paper and GitHub repository.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">论文分析了德语到英语的翻译。</sample>
    <sample id="222">The work titled "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" addresses the challenges of adapting open-domain QA systems, which are typically trained on general-purpose datasets like Wikipedia, to answer questions in specialized domains such as biomedicine. The core issue is that these systems may not perform well when faced with domain-specific questions due to differences in data distribution and content.

The study explores two main strategies for adaptation: zero-shot and few-shot interventions. Few-shot methods involve using a small number of examples from the target domain to prompt large language models to generate additional examples, which are then used to adapt the retriever and reader models. This approach has shown to improve retriever performance by 8% and reader performance by 11% on average.

Zero-shot techniques, on the other hand, do not rely on target domain examples. Instead, they focus on controlling the interactions among the question, answer, and context variables. By varying one variable while keeping the others fixed, the study assesses the impact on model learning. It was found that changing the question format to cloze-style questions, which are easier to curate, does not significantly affect performance. Additionally, using a uniform distribution of answer types and comparing different retrieval methods revealed that unsupervised methods like BM25 perform best when context distribution changes.

The study also investigates the nature of dataset shifts between the source and target domains using a data shift taxonomy. It identifies four types of shifts: no shift, concept shift, covariate shift, and full shift. Compatibility measures for both the retriever and reader models are developed to map target datasets onto a 2D grid, helping to estimate the type of shift. The findings indicate that few-shot adaptations are effective across all target sets, while zero-shot adaptations are particularly beneficial for datasets exhibiting concept and covariate shifts.

In conclusion, the research demonstrates that specific data interventions can significantly enhance reader performance, with improvements of up to 24%. It highlights the importance of selecting appropriate interventions based on the type of dataset shift to achieve effective domain adaptation in open-domain QA systems.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">在实验过程中研究了以下模型：

1. MASSalign：用于评估自动对齐方法，特别是在德语文本简化中。
2. long-mBART：用于生产文档级简化。
3. base mBART：用于生产句子级简化。</sample>
    <sample id="225">在 MultiInstruct 中，使用 53 个任务用于训练，其中包括 9 组任务，每个任务样本 10,000 个实例。测试时，保留了整个常识推理组用于测试，并从视觉问答（VQ）和杂项组中选择了额外的 5 个任务。此外，还从自然指令测试分割中随机抽取了 20 个任务作为未见的 NLP 任务。</sample>
    <sample id="226">这篇论文有两位作者，Regina Stodden 和 Omar。</sample>
    <sample id="227">The paper discusses the challenges and potential solutions in grounded language understanding, which involves mapping natural language expressions to executable actions or queries in specific environments. Current language models, primarily pre-trained on textual data without grounding, struggle with generating valid and grammatical plans or programs for tasks like semantic search, smart assistant interactions, and robotic instructions. This gap is highlighted by the octopus test, which underscores the difficulty of applying pre-trained models to grounded tasks.

The authors propose a novel framework named Pangu, inspired by Chinese mythology, which separates the generation and validation processes. Instead of having language models generate plans directly, Pangu uses a symbolic agent to propose candidate plans, while the language model scores and ranks these candidates. This approach leverages the language model's strength in discrimination rather than generation, addressing issues of validity and grammar.

The framework was tested on knowledge-based question answering, a representative scenario for grounded language understanding, using models like BERT, T5, and Codex. Pangu demonstrated superior performance in both fine-tuning and in-context learning settings, showing strong sample efficiency and robustness under non-i.i.d. conditions. The results suggest that discrimination is a more effective strategy than generation for grounded language understanding, offering a promising direction for future research and applications.</sample>
    <sample id="228">作者在实验中使用了以下数据集：AG News, MIND, SST2, Enron Spam。</sample>
    <sample id="229">Gabriella Skitalinskaya introduces a study on improving argumentative writing through detecting and revising suboptimal claims. Text revision is crucial in argumentative writing to achieve optimal phrasing, which influences the audience's reaction. The study focuses on two tasks: detecting suboptimal claims and suggesting improvements. The research explores using revision patterns from online debate platforms like Kialo to model argument quality. The paper identifies four main challenges: representativity and reliability of datasets, model complexity and architecture, contextual information's role, and topical and user bias. The study finds that revision-based data is effective for these tasks, with modeling the distance between claim versions aiding in detecting suboptimal claims. The impact of contextual information varies depending on the task and quality issues. The paper provides a detailed analysis of strategies to address these challenges and compares different approaches.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the DrBERT model.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi addresses the challenges of Simultaneous Speech Translation (SimulST), which involves translating spoken language into text in another language in real time. Current SimulST models face issues such as complex architectures, lengthy training procedures with multiple optimization objectives, and the need for different models to achieve various latency levels. The authors propose a novel solution using existing offline Speech Translation (ST) models without retraining or adopting specific architectures for SimulST. Their approach, called Encoder-Decoder Attention (EDAtt), uses a single model for all latency regimes, managing latency through specific parameters and leveraging the attention mechanism between audio input and textual output.

EDAtt determines whether to emit a partial translation based on the concentration of attention. A word is emitted if the attention is not concentrated, meaning its sum is below a threshold alpha over the last lambda speech frames, indicating stable information. For instance, if a speech chunk contains "I'm going to talk about..." and the model predicts a German translation, the cross-attention weights show that the first two words point to the earliest frames, while the last word points to the latest frames. The first two words are emitted, but the last word is withheld until more information is received. This process continues with subsequent speech chunks.

The main results of EDAtt are presented in graphs plotting BLEU scores (translation quality) against average lagging (latency) and computational-aware average lagging (considering model prediction times). The goal is to achieve high BLEU scores with low latency, ideally shifting the curves to the left. EDAtt outperforms popular strategies like Wait-k and Local Agreement when applied to offline models, as well as state-of-the-art architectures specifically designed for SimulST. The results demonstrate that EDAtt is the fastest strategy when considering actual elapsed time. The authors have made their code and models open source to facilitate reproducibility.</sample>
    <sample id="234">提示策略对结果有显著影响。在一项简单的实验中，使用一次提示时，不同的提示对于同一句子的翻译结果差异超过一点BLEURT分数，极端情况下差异可达40点BLEURT分数。因此，选择合适的提示策略很重要。在实验中，使用五次提示策略，发现提示的实际形式对于多次提示影响不大，而例子的质量更为重要。</sample>
    <sample id="235">论文中没有提供作者所属机构的信息。</sample>
    <sample id="236">The content does not specify the exact text of the 5 expert-written instructions. It only mentions that each task in the MultiInstruct dataset is equipped with five expert-written instructions.</sample>
    <sample id="237">作者建议使用一个名为“KITMUS Test”的诊断测试套件来测试模型如何使用来自多种来源的信息。这个测试套件通过一个核心参照消解任务来探测模型在不同知识源之间的整合能力。作者定义了三种不同的设置：  
1. **Background-Pretrain**：背景知识在预训练时可用。  
2. **Background-Both**：背景知识在预训练和推理时都可用。  
3. **Background-Inference**：两种知识类型仅在推理时可用。  

这些设置通过控制实体特定知识和背景知识的可用性来测试模型的知识整合能力。</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset designed to aid in the development of meeting summarization technologies. Recognizing the need for high-quality meeting summaries, the dataset addresses challenges such as obtaining trustworthy public meeting resources. MeetingBank comprises 1,366 City Council meetings, totaling nearly 7,000 instances, with data including transcripts, reference summaries, and URLs to additional resources.

The data collection process involves using the Speechmatics API to convert audio to transcripts and identifying meeting details from websites like the Boston City Council. Each meeting is assigned a unique MeetingID, which helps locate reference summaries and meeting segments. The dataset provides statistics on meeting duration, tokens, speakers, and summarization instances across different cities, along with average sentence and token counts.

Data analysis measures the level of abstraction in summaries using coverage and density scores. Coverage indicates the percentage of summary words appearing in transcripts, while density assesses the extent of extracted references. Most summaries have coverage scores between 0.7 and 0.9, suggesting a preference for verbatim points. Density scores vary, with Seattle and Boston having the highest and Denver the lowest, indicating varying levels of editing.

For model evaluation, top-tier summarization systems, including extractive models like Oracle, LexRank, and TextRank, and abstractive models like BART-Large, Pegasus, Longformer, DialogLM, and HMNet, were tested. GPT-3 was also evaluated using zero-shot summarization prompting. Extractive systems, particularly Extr-Oracle, showed high ROUGE-2 scores, while DialogLM excelled among abstractive models. GPT-3, despite lower automatic metric performance, scored highest in human assessments for fluency and coherence but lagged in informativeness and factuality.

The study highlights the need for meeting summarization solutions to focus on capturing main discussion points and developing new evaluation metrics aligned with human preferences. MeetingBank serves as a valuable resource for researchers to design advanced summarizers and offers insights into City Council decision-making processes. The dataset is available for download and further exploration.</sample>
    <sample id="239">大家好，我是大卫·维拉，今天我将简要介绍我们与谷歌翻译同事合作的论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。PaLM是去年2022年发布的一个参数量为540亿的大型语言模型，它在780亿个标记的大量文本数据上进行了训练。在发表时，它在数百个NLP任务中达到了最先进的水平。在本文中，我们首次系统地研究了大型语言模型在机器翻译中的提示技术。我们使用机器翻译社区的最佳实践来评估这些模型的翻译能力，包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠，并与最先进的系统进行比较，如WMT评估。我们使用最先进的神经机器翻译指标，并展示了基于专家的人工评估结果。最后，我们提供了一些提示选择策略的建议。提示对大型语言模型在翻译中的性能有很大影响，如我们的一个简单实验所示，我们使用一次提示，并为每个句子提供了两种不同的提示。在1000个句子中，516个句子的差异超过了1个BLEURT点，极端情况下可达40个BLEURT点。因此，选择一个好的提示策略很重要。在我们的实验中，我们选择了一个5次提示策略，即我们只是标记每个提供给系统的句子所在的语言。例如，在从德语翻译成英语的情况下，源句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，当使用多个短提示时，提示的实际形式对性能影响不大。对于零和一次提示，这一点尤为重要。而在我们的情况下，当使用五次提示时，提示的实际形式几乎没有差异，提示中的例子承担了大部分重量。我们的实验结果总结如下：示例质量比源句子的相似性更重要。因此，选择高质量翻译的示例很重要。特别是，我们比较了从WMT评估的开发数据中选择提示与从训练数据中选择提示。开发数据比训练数据更加精心策划，质量更高，而训练数据更加嘈杂。因此，使用开发数据的结果表现更好。然而，专门的最先进系统在PaLM翻译方面仍然具有显著优势。但是，PaLM接近商业系统。在我们的情况下，我们选择与谷歌翻译进行评估。我们通过使用MQM框架进行的人工评估获得的见解表明，PaLM的流畅性与最先进系统相当，但主要差异在于准确性。特别是，最常见的错误是遗漏错误。这表明PaLM有时会通过省略源句子中的部分来产生更流畅的翻译。然而，PaLM的“风格/别扭”类别低于最先进系统，这是另一个信号，表明PaLM提供了非常流畅的输出，但仍然存在准确性问题。谢谢大家。对于更多细节，请参加完整的论文演示。谢谢大家。</sample>
    <sample id="240">你好，我是德国萨尔兰大学的博士生戴伟。在这个视频中，我想介绍我们最近的工作《比你想象的更弱：对弱监督学习的批判性分析》。这是与夏宇深、马里乌斯·莫斯巴赫、安德烈亚斯·施蒂芬和迪特里希·克拉科夫的合作。我想从对弱监督和弱监督学习的简要介绍开始。在弱监督中，我们不手动标注数据。相反，我们使用弱标注源对数据进行标注，例如简单的启发式规则、知识库或低质量的众包，如右侧图示。与人工注释相比，这些弱标注更便宜，但也更嘈杂，意味着一定比例的标注是错误的。如果我们直接在弱标注数据上训练神经网络，神经网络往往会记住标注噪声而无法泛化。在弱监督学习中，提出了训练算法，以在这种标注噪声下稳健地训练神经网络，使得训练好的模型仍然能够很好地泛化。在最近的弱监督学习（WSL）研究中，常见的说法是，人们声称他们只在弱标注数据上训练模型，却在干净的测试集上取得高性能。技术上这种说法并不错误，但有一个陷阱，即人们假设有一个额外的干净验证集用于模型选择。我们不能停留在这个问题设置上，但这意味着在弱监督学习中需要额外的手动注释。但像房间里的大象一样，这种必要性常常被忽视。上述怀疑引发了三个研究问题。首先，弱监督学习是否需要干净的验证数据，或者我们可以使用噪声验证集？其次，如果需要干净数据，或者干净数据对于弱监督学习的有效性是必需的，那么我们需要多少干净样本？最后，我们是否只应该将干净样本用于验证，还是有更好的方法来利用它们？我们在工作中解决了这些研究问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，训练的模型无法超越原始的弱标注泛化，意味着训练是没有意义的。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将帮助WSL方法实现更好的性能，如左侧图所示。通常我们只需要每类20个样本就能取得高性能。但故事并未结束，因为如果我们决定获取干净样本，那么直接在它们上面训练将实现更好的性能。右侧图显示了性能差异，直接在干净数据上应用的微调方法与仅使用干净数据进行验证的WSL方法相比。我们可以看到，如果每类有10个样本，直接微调开始超过WSL方法。最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调轻松实现。如图所示，标准模型，称为FTw，最初表现不如更复杂的WSL方法，如COSINE。但如果允许在干净样本上继续微调，FTw的表现与其他方法一样好。因此，在实践中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。总结一下，我们表明最近的WSL方法需要干净的、手动标注的样本才能正常工作。它们的性能提升和实用性被严重高估。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否通过干净的验证样本进行。其次，WSL方法应与少样本学习基线进行比较，因为两者都在干净样本上工作。第三，连续微调是未来WSL工作中应考虑的简单但强大的基线。最后，我们已经开源了我们的代码。你可以通过这个幻灯片上的二维码找到它。请随意查看。谢谢，祝你在会议上愉快。</sample>
    <sample id="241">Ethan introduces the paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments," co-authored with Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech. The paper addresses the shortcomings of existing misinformation detection systems on social media, which often rely on retrospectively constructed datasets and may suffer from leaked counter-evidence. These systems are not human-centric and fail to represent the real scale and noisiness of social media platforms, which require human content moderators' involvement.

The authors propose an evaluation framework for developing systems that integrate human feedback throughout the misinformation detection process. Their system is end-to-end, processing raw tweets to produce actionable outputs for human moderators. It consists of two main components: detection of misleading claims and policy violation verification. The first component uses keyword filtering and a T5 model for claim extraction, ranking claims by trendiness before human verification. The second component employs a BERT-based model to classify the stance of tweets towards unapproved treatments, flagging supportive stance tweets for human review.

The evaluation focuses on early detection, defined as identifying unapproved treatments before their first appearance in debunking news articles. The system successfully detects such treatments early, demonstrating its utility. The policy violation verification component is assessed by human moderators using a Likert scale to determine if tweets violate Twitter's COVID-19 misinformation policies. The system achieves a 65% accuracy rate in policy violation detection and confirms 124.2 policy violations per human hour worked.

The framework realistically captures the interaction between systems and human moderators, motivating the development of future human-in-the-loop misinformation detection systems. The paper provides an out-of-industry perspective on the development and evaluation of these systems, aiming to improve early misinformation detection and response.</sample>
    <sample id="242">对话系统的常用评估方法包括：  
1. 人类评估，例如让人类评委选择两个对话中哪个更好。
2. 使用李克特量表对对话进行评分，可以在回合级别或对话级别进行评分。
3. 对话级别的配对比较。</sample>
    <sample id="243">这篇论文有五位作者：Jenny、Sebastian Santy、Ronan Le Bras、Katharina Reinecke 和 Maarten Sap。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要以下背景知识：

1. 服务员是一名法官。
2. 法官在法庭上决定案件。</sample>
    <sample id="245">Lining Zhang and co-authors present their work titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The study addresses the challenges of automatic metrics and recruitment practices on Amazon Mechanical Turk (MTurk) by introducing a two-step pipeline to identify high-agreement workers. The pipeline begins with "Qualification Settings," where pre-task qualifications such as location, number of HITs, and HIT Approval Rate are set. The first stage, "Qualification Task," evaluates workers' ability to assess multiple dimensions of summaries, categorizing them into gold, silver, bronze, and block types. Only gold and silver workers advance, resulting in 26 qualified workers (13% of 200 participants).

The second stage, "Endurance Task," tests workers' capacity to handle a heavy workload with 10 HITs, focusing on the saliency dimension. This stage further narrows down to 12 workers (6% of 200), who demonstrate high inter-annotator agreement (IAA) compared to experts. The reference-based task assesses general performance, with 8 out of 12 workers completing all HITs, showing a Krippendorff's Alpha of 0.534.

The study compares these results with Baseline MTurk workers, using a statistical filter called MACE, achieving a Krippendorff's Alpha of 0.380 but with incomplete HIT coverage. CloudResearch MTurk workers, recruited for high-quality annotations, show a Krippendorff's Alpha of 0.513 but with a lower task acceptance rate. Analysis of correctness across annotation sources reveals a significant Spearman's correlation between Pipeline and CloudResearch workers, though Pipeline does not guarantee correctness training. Real GPT models correlate well with expert judgments.

In conclusion, the pipeline effectively identifies high-agreement workers, achieving similar quality to CloudResearch at a lower cost and avoiding resource waste. Future work will explore hiring high-quality workers across various tasks, languages, and platforms. Limitations include testing only English summarization on MTurk, non-universal question designs, and no guarantee of correctness training. The study acknowledges Google's funding support.</sample>
    <sample id="246">是的，代码是公开的。可以在GitHub上获取。</sample>
    <sample id="247">Jiho Kim from KAIST AI introduces a new paper titled "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper addresses the gap in existing fact verification datasets, which typically use Wikipedia text or tables as evidence, by proposing a novel task: Knowledge Graph-Based Fact Verification. This approach leverages knowledge graphs (KGs) for more reliable fact verification, as KGs provide intuitive evidence that can be directly connected to claims, eliminating the need for additional interpretation required in text- or table-based cases.

The paper introduces the FactKG dataset, which utilizes DBpedia as the knowledge graph. Claims in FactKG are presented in both written and colloquial styles to enhance practicality. The dataset includes two labels: SUPPORTED and REFUTED, and involves retrieving evidence from DBpedia to verify claims. Five types of reasoning are incorporated: one-hop, conjunction, existence, multi-hop, and negation. One-hop claims involve verifying a single triple, conjunction claims require checking multiple one-hop claims, existence claims involve verifying a specific relation, multi-hop claims require inference across multiple entities, and negation claims necessitate additional verification steps.

To create claims in colloquial style, the paper employs a colloquial style transfer model and presupposition templates. The dataset's statistics and baseline methods are also discussed, with the GEAR model, which uses graph evidence, outperforming other baselines and the majority class baseline of 51%. The paper concludes by inviting readers to download the dataset and contact the authors for further information.</sample>
    <sample id="248">NLPositionality 的注释者在各个人口统计学特征方面并不均衡。研究通过重新注释数据集来获取多样化的注释者，以获得丰富的人口统计数据。然而，原始数据集的注释者通常只有少数人，且人口统计信息很少被收集和分享。因此，重新注释是为了获得更多的注释者和更全面的人口统计数据。研究结果显示，数据集和模型更倾向于与英语国家和受过大学教育的人群对齐，而非二元性别人群则被较少考虑。</sample>
    <sample id="249">在可接受的域中扰乱句子是通过尝试保留相关结构的同时，向输入句子添加噪声来实现的。这些扰动包括对句子进行多种变化，但结果显示，这些噪声并没有改变模型在最小对比判断（MPP）中的表现。即使在句子被扰动后，模型对可接受句子的判断仍然会增加，而对不可接受句子的判断则会减少。这表明模型对这些扰动敏感，但其判断方式保持一致。</sample>
    <sample id="250">进行维度评估意味着评估对话质量的多个方面，以更细致地了解模型的优势和劣势。这涉及到评估对话的多个维度，如响应的相关性、自我矛盾、与伙伴矛盾、事实错误、违反常识知识以及是否展现同理心等。这种方法旨在通过明确标注模型响应中的特定行为来减少人类评估的主观性。</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">Sai Kiran Tanikella, a master's student at IIT Kanpur, presents "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion," a collaborative work with Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. The project addresses the challenge of Prior Case Retrieval (PCR) in the legal domain, where legal professionals traditionally rely on experience to cite relevant past precedents. With the increasing volume of cases, this task has become more complex, necessitating efficient retrieval of relevant cases from a large pool.

The presentation introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR dataset, or Indian Legal Prior Case Retrieval Dataset, is a new benchmark for PCR tasks, comprising 7,070 legal cases with an average of 6.775 citations per query document. It offers a comprehensive test bed for assessing PCR algorithms, featuring longer documents, a larger vocabulary, and more citations compared to the existing COLIEE’21 dataset for Canadian legal documents.

The U-CREAT pipeline leverages unsupervised learning and an event-based approach to improve PCR tasks. It demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The pipeline involves event extraction from case documents, represented as narratives of events. Using dependency parsing, subject-verb-object triplets are formed to represent events. These events are then used to compute an interaction matrix between query and candidate documents, aiding in the retrieval process.

Experiments were conducted using various models categorized into count-based, transformer-based, and event-based models. While transformer-based models like BERT and DistilBERT showed lower performance compared to baseline methods like BM25, event-based models significantly outperformed the baselines. The Event Filtered Documents model, which filters the corpus to include only sentences producing matching events, emerged as the best-performing model, offering lower inference times and higher F1 scores.

U-CREAT outperforms existing approaches, including the recent supervised method by the MTFT-BERT team, establishing it as the state-of-the-art method for the COLIEE’21 document retrieval task. The presentation concludes by highlighting U-CREAT's potential for further exploration and development in the field of prior case retrieval.</sample>
    <sample id="253">Mario Ezra Aragón introduces "DisorBERT," a double domain adaptation model designed to detect signs of mental disorders in social media posts. The project, a collaboration between researchers from Mexico and Spain, aims to leverage the vast amount of social media content to identify mental health issues. Mental disorders, which affect thinking, feeling, mood, and behavior, include conditions like major depression, PTSD, bulimia, and anorexia. Social media provides a platform for individuals to share their experiences and seek help, making it a valuable resource for mental health research.

DisorBERT addresses the challenge of insufficient annotated data by using domain adaptation, which involves transferring knowledge from a related domain to improve model performance. The model starts with BERT, a language model trained on general data, and adapts it to the specific language of Reddit and mental health. This adaptation includes integrating a lexicon to guide the masking process, helping the model focus on important words during training.

The approach involves first learning the language of social media and then specializing in the mental disorder domain. DisorBERT's performance is evaluated using the eRisk datasets, showing a good balance between precision and recall compared to other methods. The model's ability to focus on relevant words is demonstrated through examples from Beck's Depression Inventory (BDI), where DisorBERT predicts words with a negative psychological orientation, such as "focus," "talk," "breath," "sleep," and "eat," compared to BERT's more general predictions.

Visualization tools highlight the most relevant words and sentences in user posts, with attention scores indicating prominence of words like "anxious" and "medication," which are relevant to depression. DisorBERT outperforms MentalBERT, a model trained on a large dataset, by effectively capturing signs of mental disorders in social media interactions. Future work will explore using different lexical resources and clinical data to further enhance the model's capabilities.</sample>
    <sample id="254">The research presented by Sun Qi from Nanjing University of Science and Technology focuses on improving document-level distant relation extraction (DocRE) by addressing the noise in distantly supervised (DS) data through uncertainty-guided label denoising. Traditional methods for DocRE rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent approaches use DS data to pretrain models, but these datasets contain noise, often leading to false-positive pseudo labels that can mislead the model.

To tackle this issue, the paper introduces a framework that incorporates uncertainty estimation to enhance the quality of DS data labels. Initially, a pre-denoising DocRE model is trained using both DS and human-annotated data to generate pseudo labels. Recognizing that false pseudo labels are inevitable, the framework employs Monte Carlo dropout to estimate uncertainty, determining the trustworthiness of model predictions. This is particularly important for capturing uncertainty in overlapping relations between entity pairs.

The proposed instance-level uncertainty estimation method calculates uncertainty scores for each positive pseudo label, addressing the challenge of distinguishing between false positives and correct positives in overlapping relations. The framework also introduces dynamic class uncertainty thresholds to filter out high-uncertainty pseudo labels, replacing original DS labels with those having lower uncertainty scores.

To further enhance performance, a multi-phase training strategy is designed to iteratively re-label DS data, leveraging the full potential of DS data. The framework outperforms several strong baselines on public datasets, demonstrating significant improvements in label quality and overall model performance.

The main contributions of the work include the uncertainty-guided label denoising framework, the instance-level uncertainty estimation method for overlapping relations, the iterative re-label strategy with dynamic class uncertainty thresholds, and the notable performance improvements achieved.</sample>
    <sample id="255">提示的形式在零和一次提示的情况下很重要。在多次提示（如五次提示）的情况下，提示的实际形式对性能影响不大，重要的是例子本身。</sample>
    <sample id="257">作者评估了四个状态-of-the-art的对话模型。</sample>
    <sample id="258">Chiang Cheng-Han introduces a novel research paper titled "Can Large Language Models Be an Alternative to Human Evaluation?" The study explores using large language models (LLMs) to evaluate text quality in natural language processing (NLP) tasks, aiming to replace human evaluations. The motivation behind this research is the instability and reproducibility issues associated with human evaluations. By leveraging LLMs, which can follow natural language instructions, the authors propose that these models can provide consistent and reproducible evaluations.

The experiment involves using LLMs to rate stories generated by GPT-2 or written by humans, based on four attributes: grammar, coherence, likability, and relevance. The study compares the LLMs' ratings with human evaluations conducted by English teachers, who are considered experts due to their experience in scoring essays. Four different LLMs were tested: T0, InstructGPT (Curie and Davinci), and ChatGPT.

Results showed that human evaluators preferred human-written stories over those generated by GPT-2. However, smaller LLMs did not consistently show a preference for human-written stories. Notably, Davinci and ChatGPT demonstrated a clear preference for human-written text, aligning with human evaluators' preferences. This finding suggests that certain LLMs can serve as viable alternatives to human evaluation in specific tasks.

The paper addresses further questions, such as the agreement between LLMs and human evaluators on individual story ratings, the impact of instruction wording, response sampling methods, and the benefits and costs of using LLM evaluations compared to human evaluations. Additionally, it explores the applicability of LLM evaluations to other tasks. For more detailed insights, the paper is available for readers interested in this innovative approach.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." Semantic parsing involves creating semantic representations of user queries, such as SQL and Lambda Calculus. Cross-lingual semantic parsing translates queries from multiple natural languages into various meaning representations. Existing models are limited in scope, often missing languages like Chinese and certain meaning representations like Lambda Calculus.

To address these gaps, XSemPLR is proposed, offering a unified dataset for cross-lingual semantic parsing across 9 datasets, 5 tasks, 8 meaning representations, and 22 languages from 15 language families. The benchmark evaluates six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Few-shot transfer.

Translate-Test uses Google Translate API to convert source language queries to the target language, then applies a monolingual model. Monolingual Model uses the same language for both training and inference. Monolingual Few-shot trains models with only 10% of data. Multilingual Model trains on multiple languages simultaneously. Cross-lingual Zero-shot and Few-shot transfer involve training on one language and transferring to another.

Analysis shows Encoder-Decoder models, like mT5, outperform others across datasets. Training in a mix of languages improves performance, though English sometimes sees a drop, illustrating the "Curse of Multilinguality." Cross-lingual Few-shot transfer significantly reduces performance gaps compared to Zero-shot transfer.

Findings include Encoder-Decoder models outperforming previous work, English pretraining boosting Few-shot performance, and multilingual models like Codex and BLOOM being inadequate for cross-lingual tasks. XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing, highlighting the potential and challenges of multilingual language models.</sample>
    <sample id="260">论文中没有提到具体的作者数量。</sample>
    <sample id="261">优秀规划器的理想品质是能够写出合理的脚本，并且忠实于约束条件。</sample>
    <sample id="262">The provided text does not specify the number of authors of the paper.</sample>
    <sample id="263">The presentation focuses on addressing label biases in in-context learning, a method used with large language models for text classification. In-context learning is known for its instability due to design choices like the selection and order of examples, which introduce biases in model predictions. Previous work has identified search instability as a result of these biases, but a systematic categorization and mitigation approach has been lacking.

This work introduces a typology of label biases, identifying a new type called domain-label bias, which arises from the influence of the task corpus on model predictions. The study categorizes biases into three types: vanilla-label bias (uncontextual preference for labels), context-label bias (effects from the context), and domain-label bias (influence of the task corpus).

Experiments demonstrate that random in-domain words from the task corpus can significantly bias model predictions, unlike random English words. Tasks with small domain-label bias show good in-context learning performance, which can be enhanced with calibration. However, tasks with large domain-label bias struggle, with models barely outperforming chance-level baselines even with prior calibration methods.

To address these biases, the study proposes domain-context calibration, which uses random in-domain words as content-free text to estimate and adjust model biases. This method improves in-context learning performance, especially in tasks with high domain-label bias, by refining decision boundaries. The effectiveness of domain-context calibration is validated across various models and datasets, showing significant improvements over previous methods that used single predefined tokens like "not available."

The work concludes that domain-context calibration is superior due to its ability to account for domain-label bias, offering a comprehensive solution to improve the stability and accuracy of in-context learning in large language models.</sample>
    <sample id="264">Lin Wang, a graduate student at Zhejiang University, introduces a paper titled "TAVT: Towards Transferable Audio-Visual Text Generation." The paper addresses the challenges in multimodal text generation tasks, such as audio-visual text generation, which are hindered by the difficulty and expense of data annotation and domain shifts. Unlike uni-model tasks like machine translation and image captioning, which have thrived due to large-scale pre-training and model capacity, multimodal tasks face significant challenges due to varying conditions across domains.

The proposed solution, Transferable Audio-Visual Text Generation (TAVT), aims to overcome these challenges by aligning visual concepts across domains using a unified audio semantic space. The paper highlights that while visual content shifts significantly with changes in style and angle, audio content changes like rhythm and energy have minimal impact on event understanding. This observation leads to the idea of using a unified audio semantic space to align visual concepts.

The framework consists of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network maps visual concepts into a unified audio semantic space, using learnable tokens called visual prefixes to improve alignment. The encoder and generator use a transformer-based model with an alpha parameter to evaluate the contribution of different modalities to each word.

To optimize visual-audio alignment, the paper introduces Dual Counterfactual Contrastive Learning (DCLL), which constructs fine-grained supervision signals from counterfactual results. The meta-training process involves selecting specific domains as support sets and others as query sets, similar to MAML, allowing for fast adaptation to new domains.

The experimental section evaluates the approach using benchmarks based on MSVD and MSR-VTT, demonstrating superior performance over state-of-the-art models in both cross-datasets and cross-domain settings. The method shows robustness in low-resource domains, maintaining performance where other methods degrade. Ablation experiments further analyze the impact of audio features on performance. Overall, TAVT presents a novel approach to transferable audio-visual text generation, addressing domain shifts and improving alignment across modalities.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">论文中没有提到作者所属的机构。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误，即在翻译过程中省略了源句子中的部分内容。</sample>
    <sample id="269">你好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将为你介绍ABC-Eval，这是一种新的多维度方法来评估对话人工智能。这项工作由埃默里大学自然语言处理实验室的金浩教授领导，并与亚马逊Alexa AI合作完成。假设你刚刚开发了一个对话模型，并希望看看它与当前最先进的模型相比如何。通常的做法是使用人类评估，例如让人类评委选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法在提供对整体对话质量的全面评估方面效果很好，但对话质量有许多方面。因此，你可能希望评估多个对话质量维度，以更细致地了解模型的优势和劣势。一种方法是简单地让人类评委评估对话质量的多个维度，例如使用现有的比较方法或李克特量表评估模型回应的相关性。然而，我们认为有一种更精确和可靠的对话评估策略。我们的方法试图通过明确标注模型回应是否表现出某些行为，例如提供无关信息或自相矛盾，来减少人类评估的主观性。我们称这种方法为在聊天中标注行为，简称ABC-Eval。我们开发了这种方法，以全面覆盖最近文献中建议影响聊天质量的聊天模型行为。ABC-Eval能够测量聊天模型犯下各种主题错误的频率。例如，ABC-Eval测量聊天模型忽略伙伴或说出无关信息、自相矛盾或伙伴矛盾、产生错误事实或违反常识知识的回合数量，以及模型成功或失败地表现出同理心的情况。为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval对每个模型的100个人机对话进行了评估。为了比较，我们还使用了三种现有方法对这些对话进行了评估：回合级李克特评分、对话级李克特评分和对话级配对比较。对于每种现有方法，我们收集了评估八个最常测量的对话方面的评估，因为这是评估聊天模型沿多个维度的标准做法。从这些评估结果的分析中，我们发现ABC-Eval行为标签整体上比现有方法收集的标签更可靠，这是通过在100个双重标注的对话中测量评委间一致性来衡量的。此外，ABC-Eval标签比现有方法产生的指标更能预测整体对话质量，这在简单的线性回归分析中得到了证明。例如，你可以看到测量自我和伙伴矛盾的回合比例解释了5%和10%的对话质量，而平均李克特一致性分数解释的不到4%。最后，我们检查了每个评估指标是否捕捉到对话质量的独特方面，使用逐步线性回归。你可以看到所有ABC-Eval指标的组合解释了超过25%的对话质量，而逐一移除这些指标时，大多数指标会导致对质量的信息丢失。相比之下，所有回合级李克特指标的组合解释的质量要少得多，且较少的这些指标携带独特信息。这些可靠、信息丰富且独特的ABC-Eval指标使我们能够以比以前方法能够实现的更高分辨率来评估对话人工智能。你可以在我们实验的结果中看到，仍然存在并且已经精确量化的挑战。例如，我们测试的机器人在大约20%的回应中存在常识违反，在大约15%的回应中提供无关信息，在大约10%的时间内自相矛盾或与伙伴矛盾。随着该领域的快速进步，新发布的模型可能会在我们进行评估后降低这些错误率。然而，这正是追求可靠和精确的评估指标以比较模型的更多理由。我们希望其他人能够利用ABC-Eval作为这一方向的有意义的一步。我们期待着在未来几个月和几年中看到对话人工智能的进步。谢谢你的观看。</sample>
    <sample id="270">Emory University and Amazon Alexa AI.</sample>
    <sample id="271">In the given content, CFT stands for "Continued Fine-Tuning."</sample>
    <sample id="272">这篇论文有七位作者。</sample>
    <sample id="273">您好，我叫Kayo Yin，我将介绍我们的工作《翻译何时需要上下文？一项数据驱动的多语言探索》。这项工作是与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig合作完成的。很多翻译都依赖于上下文。例如，如何翻译“mole”这个词？如果前一句是“如果部长们发现了，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，这可能是什么严重的问题吗？”，那么“mole”指的是痣。因此，根据上下文，单词的意思和翻译都会改变。然而，评估模型在这些情况下的翻译效果很困难。首先，因为只有一小部分翻译依赖于上下文，所以像BLEU这样的语料库级别的评估指标无法捕捉这些翻译。有人建议针对上下文依赖的翻译进行特定评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工策划。在这项工作中，我们试图回答这两个问题：首先，翻译何时需要上下文？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量在翻译过程中单词依赖上下文的程度。在之前的工作中，我们引入了CXMI作为机器翻译模型上下文使用的度量。这是通过测量上下文C对目标Y的信息量，给定源X来实现的。你可以把CXMI看作是给模型提供上下文所获得的信息量。在这项工作中，我们将CXMI扩展为Pointwise CXMI，可以在句子级别或词级别测量上下文使用。我们可以把P-CXMI高的词看作是需要上下文进行翻译的词。然后，我们分析P-CXMI高的词，寻找这些词之间的模式。我们在TED演讲的英语到14种不同语言的翻译转录上进行分析。我们在三个不同的层次上进行分析：首先，我们查看平均P-CXMI高的词性标记，这使我们能够找到例如阿拉伯语中的双数代词，它们的P-CXMI相对较高。这可以解释为英语没有双数代词，所以在翻译成阿拉伯语时需要上下文来确定代词是否为双数。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看在所有不同出现中平均P-CXMI高的词汇项，这有助于我们识别像在中文中需要上下文来翻译专有名词以确保在文档中使用相同的翻译的情况。同样，我们发现上下文对于翻译正确的正式程度也很重要。最后，我们查看具有高P-CXMI的不同个体标记，这使我们能够识别那些不能真正通过单词本身捕捉的现象，而是通过句子结构表达的，例如解析省略。现在，我们使用分析的结果来设计一个文档级翻译的基准。对于我们识别的五种话语现象，我们创建标注器来自动识别与现象相关的词。我们称这个标注器为多语言话语感知（MuDA）标注器。我们还注意到不同语言在这些话语现象中的比例不同。然后，我们使用MuDA标注器，通过将标注器应用于我们想用于评估的平行语料库，并应用我们选择的翻译指标来评估MuDA标注器识别的上下文依赖示例。最后，我们使用我们的基准以及其他指标来评估不同模型在文档级机器翻译中的表现。首先，当我们使用语料库级指标时：对于BLEU，我们发现上下文无关模型的表现最好。但是，如果我们使用COMET，上下文感知模型表现最好。如果我们使用词汇F-度量，那么使用和不使用上下文的模型的表现相当。这再次表明，如果我们仅使用语料库级指标，很难确定最佳的文档级翻译系统。现在，我们使用MuDA基准来评估模型，并发现对于某些话语现象，如正式程度和词汇连贯性，上下文感知模型比不使用上下文的模型更准确。但是，这些模型在其他现象如省略、代词和动词形式上并不比不使用上下文的模型好得多。这似乎表明，我们需要在文档级翻译方面看到更多进展的地方。我们还比较了不同的商业系统，我们的基准表明DeepL通常比Google翻译在文档级翻译中更准确。总之，我们在14种语言对中进行了数据驱动的分析，以识别翻译何时需要上下文，并使用我们的发现来构建文档级机器翻译的基准，这有助于我们识别模型处理得好或不好的话语现象，以及哪些翻译系统在文档级翻译中表现良好。非常感谢您的关注。下次见到您时在多伦多。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages," addressing the gap in evaluating translations from Indian languages to English. They focus on five Indian languages: Tamil, Malayalam (Dravidian), and Hindi, Marathi, Gujarati (Indo-Aryan). From the Flores dataset, they select 200 sentences per language, generating 1,400 candidate translations per language using seven translation models or APIs, totaling 7,000 samples. Bilingual expert annotators evaluate these translations, marking errors by type and severity using the MQM framework, and providing overall scores.

The study finds that recent models like NLLB and Indic Trans perform better than older ones like CVIT. Among evaluation metrics, chrF shows the highest correlation with human scores, but overlap-based metrics generally perform poorly. Embedding-based metrics like LabSE and BERTscore with multilingual models show better correlations, with MuRIL performing well on average. COMET-metric variants exhibit the highest overall correlations.

The analysis reveals that metrics often have a skewed score range, making interpretation difficult, unlike human scores that cover the full scale. Splitting the dataset by error types (fluency vs. accuracy) shows higher correlations with human scores for accuracy errors. Fine-tuning the best-performing metric, COMET, with their MQM dataset results in IndicCOMET variants that outperform COMET baselines on most languages. IndicCOMET also demonstrates strong zero-shot performance on unseen languages and shows higher robustness scores on the ACES Translation Accuracy Challenge Sets compared to COMET. The authors encourage the use of their publicly available dataset.</sample>
    <sample id="277">The new method introduced in the paper does not have a specific name mentioned in the provided text.</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法为一种识别区分显性群体与不显性群体的词汇的方法。该方法基于社会语言学概念“显性性”，即存在一个不显性的默认状态，任何与该默认状态不同的群体在语言上都是显性的。在方法中，首先确定不显性和显性群体，然后使用“Fightin’ Words”方法，即使用加权对数比率来区分每个显性群体的顶级词汇。例如，对于黑人女性的人设，会将其与白人人设和男性人设进行对比，因为这些是对应的不显性群体。</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">Shi Tao introduces "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," focusing on emotion recognition in conversations (ERC). The task involves predicting emotion labels for each utterance in a dialogue, utilizing textual, audio, and visual modalities. Existing ERC methods often inadequately exploit multimodal information, focusing mainly on textual data or using simple feature concatenation. They also struggle with minority emotion classes and distinguishing semantically similar emotions.

To address these challenges, MultiEMO is proposed, comprising four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The framework's main contributions include:

1. **VisExtNet**: A novel visual feature extractor that captures facial expressions from multiple frames using MTCNN and VGGFace2 pre-trained ResNet-101, avoiding redundant scene-related information.

2. **MultiAttn**: A multimodal fusion model using bidirectional multi-head cross-attention layers. It integrates textual, audio, and visual modalities by learning cross-modal correlations, enhancing the fusion of complementary information.

3. **Sample-Weighted Focal Contrastive Loss**: This loss function prioritizes hard-to-classify minority classes and maximizes inter-class distances, aiding in distinguishing semantically similar emotions.

Extensive experiments on MELD and IEMOCAP datasets demonstrate MultiEMO's state-of-the-art performance, particularly in minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish between speakers and irrelevant people, the need for large batch sizes with SWFC loss on MELD, and suboptimal performance in minority emotions compared to majority classes.</sample>
    <sample id="281">Kayo Yin and collaborators present their work on "When Does Translation Require Context? A Data-driven, Multilingual Exploration," addressing the challenge of context-dependent translations. They highlight that while some translations rely heavily on context, traditional evaluation metrics like BLEU struggle to capture these nuances. To tackle this, they introduce Pointwise Contextual Mutual Information (P-CXMI) to measure context dependency at both sentence and word levels. Their analysis, conducted on TED talk transcripts translated into 14 languages, reveals patterns in context usage, such as the need for context in translating dual pronouns in Arabic and proper nouns in Chinese.

The study identifies five discourse phenomena where context is crucial: dual pronouns, verb forms, proper nouns, formality, and ellipses resolution. They develop the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words, creating a benchmark for document-level translation evaluation. Their findings show that context-aware models outperform context-agnostic ones in handling formality and lexical cohesion, but not significantly in ellipses, pronouns, and verb forms.

Using corpus-level metrics like BLEU, context-agnostic models appear superior, while context-aware models excel with COMET. The MuDA benchmark reveals that DeepL generally surpasses Google Translate in document-level translation accuracy. This research underscores the importance of context in translation and provides a framework for evaluating and improving document-level machine translation systems.</sample>
    <sample id="282">Xuekai Zhu introduces "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023. This research advances non-parallel text style transfer by focusing on story-level and discourse-level transformations, crucial for mimicking author styles. The main challenge is imitating author linguistic preferences, such as discourse structures and narrative techniques, which are often tied to specific topics, complicating style transfer.

To address these challenges, StoryTrans is proposed. It learns discourse representations from source texts and combines them with learnable style embeddings to generate target-style texts. A new training objective reduces stylistic features in discourse representations, pulling different text representations closer in latent space, and enhances content preservation by separating generation into two stages. Initially, style-specific content keywords are masked, and the text is transferred. Then, the whole text is generated by incorporating these keywords.

The training framework is divided into two stages. The first stage uses an advisory framework with self-reconstruction loss for input recovery, disentanglement loss for separating style and content in sentence embeddings, sentence order loss for capturing dependencies, and style classifier loss for producing style signals. The second stage focuses on filling correct style-specific content and removing mask tokens.

New datasets in Chinese and English were collected for these tasks, and extensive experiments were conducted to transfer fairytales and everyday stories to typical author styles. Both automatic and manual evaluations show that StoryTrans outperforms strong baselines in style control and content preservation. Style visualization confirms alignment with golden texts in style feature space. StoryTrans effectively supplements short phrases or plots, enriching storylines while maintaining main contents, and rewrites sentences in the target style while preserving source semantics. Data and code are available in the provided repository.</sample>
    <sample id="283">The first symmetric dependency structure mentioned is the "multi-headed approach" used in Hudson's Word Grammar. The city associated with the Prague approach, which is an asymmetric structure, is Prague.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented a paper titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which rely heavily on precise span boundaries, leading to ambiguity in annotation. To tackle this, Peng proposed a fuzzy span mechanism where span boundaries are learned as continuous distributions rather than precise points. This approach accounts for the inherent ambiguity in span boundaries and aligns better with the nature of information extraction tasks.

The paper introduces a fuzzy span loss and a fuzzy span attention mechanism. The fuzzy span loss uses Binary Cross Entropy (BCE) and KL-divergence to handle the continuous distribution of span boundaries, allowing the model to better utilize annotation information. The fuzzy span attention mechanism dynamically adjusts the attention span using an optimizable parameter, delta, and applies a linear decay at the attention span boundary, rather than a hard truncation. This adaptive attention helps the model focus on relevant semantic information within a limited range.

The FSUIE model, incorporating these mechanisms, was tested on three main information extraction tasks: named entity recognition, relationship extraction, and aspect sentiment triplet extraction. The results showed significant improvements over baseline models, particularly in named entity recognition and relationship extraction, achieving state-of-the-art results on datasets like ACE2004, 2005, ADE, and AST-V2. The model also demonstrated strong generalization capabilities across different domains.

An ablation study confirmed that both the fuzzy span loss and attention mechanisms contribute to improved convergence speed and information extraction capability. Visualizations of the attention distribution supported the effectiveness of the fuzzy span attention layer, showing that the model focused on semantically relevant tokens within a limited range. Overall, the FSUIE model represents a significant advancement in handling the challenges of span-based information extraction by introducing a novel fuzzy span mechanism.</sample>
    <sample id="285">Mingqi Gao from Peking University presents their work on "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The study addresses the issue of factual errors in dialogue summarization, where summaries generated by models often contain inaccuracies. Two main solutions are proposed: incorporating factuality-related objectives during training or inference, and designing a Factual Error Correction (FEC) model that operates independently of the summarization model. The FEC model takes a source document and a generated summary as input and outputs a corrected version.

The authors highlight that previous evaluations of FEC models, which rely on factuality metrics like FactCC and DAE, are flawed. These metrics provide an overall score that may not accurately reflect the model's performance, and they blur the distinction between improving summarization models and correcting errors post-generation. To address these issues, the study advocates for the use of manually annotated reference corrections, which offer more reliable data for training and a more precise evaluation framework.

The paper introduces a new taxonomy for classifying factual errors, distinguishing between content-based and form-based errors. Content-based errors are categorized by part of speech and dependencies, while form-based errors are classified by the type of operation (addition, deletion, substitution). The evaluation framework builds on ERRANT, a metric for grammar error correction, and involves alignment, classification, and comparison steps.

Key findings from the study include the superior performance of FEC models trained with reference summaries from dialogue datasets, as measured by unreliable factuality metrics. The research underscores the need for new evaluation methods and suggests that incorporating human-corrected summaries during training can enhance FEC model performance. Combining human-annotated data with synthetic data is identified as a promising approach. However, current FEC models struggle with certain types of factual errors, such as additions, attribute errors, modality errors, and link errors.</sample>
    <sample id="286">James Finch 和 Sarah Finch。</sample>
    <sample id="287">这篇论文有四位作者：Javad Hosseini, Filip Radlinski, Silvia Pareti, 和 Annie Louis。</sample>
    <sample id="288">可用于测试句法现象的数据集包括BLiMP和SyntaxGym。</sample>
    <sample id="290">The text does not provide specific five methods or their abbreviations related to the first research question. It discusses the necessity of clean validation data in weakly supervised learning but does not list specific methods or their abbreviations.</sample>
    <sample id="291">该模型在以下任务上进行了评估：命名实体识别、分类、词性标注和问答。</sample>
    <sample id="294">CamemBERT 最初是在 OSCAR 数据集上训练的，其中包括 138 GB 和 4 GB 的数据。</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presents a collaborative project between the University of Turin and Amazon Alexa focusing on irony detection in natural language processing (NLP). The project challenges the traditional assumption of a single "ground truth" in data annotation, emphasizing the complexity of irony as a pragmatic phenomenon. To address this, the team developed the English Perspectivist Irony Corpus (EPIC), which includes 300 short conversations from social media platforms like Reddit and Twitter, collected over 1½ years. The corpus covers five varieties of English, with data annotated by 74 individuals via the Prolific platform, each reviewing 200 conversations.

The annotation process involved a simple interface where annotators judged whether replies were ironic, resulting in an average of five annotations per conversation. The study revealed significant inter-annotator agreement variations based on demographics such as gender, age, and nationality. To explore these differences, the team developed perspective-aware models by fine-tuning a pre-trained language model on dataset splits according to annotator characteristics. These models demonstrated increased confidence in their predictions compared to gold standard aggregated models, despite no clear performance trends.

Further analysis identified that generational and geographical differences influenced irony perception, with notable disagreements between annotators from adjacent age groups and those from the UK and Ireland. This research highlights the importance of considering diverse perspectives in NLP and the potential of perspective-aware models to enhance irony detection.</sample>
    <sample id="297">The project "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" explores the concept of dogwhistles—terms that convey different messages to in-groups and out-groups, often used to communicate controversial or taboo ideas while maintaining plausible deniability. An example is the term "cosmopolitan," which some interpret as a coded reference to Jewish people. Understanding dogwhistles is crucial for natural language processing (NLP) and linguistics because it challenges traditional notions of meaning, which is highly context-dependent.

The project develops a typology and glossary of over 340 terms, focusing on racist, transphobic, and anti-Semitic dogwhistles, primarily in English and US-centric contexts. The typology categorizes dogwhistles by register (informal or formal), type (adding implicature or signaling persona), and persona (e.g., anti-Semitic or transphobic). A case study of historical U.S. political speeches reveals a correlation between the frequency of racial dogwhistles and the Republican Southern Strategy, showing increased use of dogwhistles post-Civil Rights era, particularly within conservative circles.

The research evaluates dogwhistle recognition in language models, particularly GPT-3, by prompting it to identify dogwhistles. GPT-3 performs well with formal register dogwhistles but struggles with informal and transphobic ones. The model's ability to identify covert meanings improves with specific prompting strategies, such as providing definitions and secret cues.

Additionally, the project examines how dogwhistles can evade content moderation by analyzing toxicity detection with the Prospective API. It demonstrates that replacing standard group labels or slurs with dogwhistles in hateful sentences reduces their toxicity scores, highlighting a significant challenge for automated moderation systems.

Overall, the project contributes a comprehensive typology and glossary of dogwhistles, investigates their historical use in U.S. politics, evaluates language model recognition capabilities, and underscores the difficulty of detecting dogwhistles in content moderation.</sample>
    <sample id="298">通过重新训练或继续预训练一些模型使用更近期的数据，研究发现性能随着训练数据与测试数据之间的时间差距增大而下降，这证实了时间漂移是性能下降的主要原因。</sample>
    <sample id="299">The talk by Michalis Korakakis, in collaboration with Andreas Vlachos, focuses on enhancing the robustness of Natural Language Inference (NLI) models through a novel training method called minimax training. Despite achieving state-of-the-art results, NLI models often rely on shortcuts—spurious correlations between input attributes and labels—introduced during dataset creation. These shortcuts lead to poor performance on out-of-distribution adversarial test sets, where such correlations do not hold.

Traditional shortcut mitigation methods depend on auxiliary models that exploit these shortcuts, using their outputs to re-weight training instances. However, these methods assume prior knowledge of shortcuts and often fail because the learner model's behavior diverges from the auxiliary's. Additionally, they require pre-trained language models as auxiliaries, adding computational overhead.

To address these limitations, the proposed minimax training method aims to reduce reliance on shortcuts by emphasizing under-represented "hard" training instances that contradict the shortcuts in "easy" examples. The learner model minimizes the NLI task loss, while the auxiliary model maximizes the learner's loss by generating example weights that encourage the learner to focus on challenging input spaces. This alternating optimization process uses standard algorithms like stochastic gradient descent.

The method does not assume specific shortcut types and uses a feed-forward network for the auxiliary. It was evaluated on datasets like MNLI, FEVER, and QQP, along with their adversarial test sets, showing consistent improvements in out-of-distribution performance while maintaining in-distribution accuracy. The paper also explores the effects of pre-training the learner, the size of the auxiliary, and conducts a qualitative evaluation of the learned example weight distribution. The findings suggest that the minimax training method effectively enhances the generalization capabilities of NLI models.</sample>
    <sample id="300">Interactive dictation is a novel task introduced by Belinda and her team at Semantic Machines, in collaboration with Jason Eisner, Adam Pauls, and Sam Thomson. It allows users to dictate and edit documents using natural voice commands, without needing to memorize fixed template commands. This task is characterized by the flexible interleaving of dictation and editing, using intuitive natural language for edits. The process involves four steps: speech recognition, segmentation of dictation and commands, normalization and correction of commands, and execution of these commands to produce the final document. Unlike existing systems like Nuance Dragon NaturallySpeaking and Microsoft Word Dictate, which require specific commands, interactive dictation aims for a more human-like interaction. The team designed a data collection interface and built a dataset to support this task. They developed a baseline system with separate models for each step, experimenting with T5 and GPT-3 architectures. The segmentation model proved accurate and efficient, while GPT-3 models, though more accurate, were slower. The team released their code and encourages further research in this area.</sample>
    <sample id="302">有必要对输出序列中的词元进行排列是因为在第一步中，每个输入词元被标记为一个无序的多集，这些多集包含将出现在输出中的词元。因此，第一步结束时，所有正确的词元都已经被确定，但它们的顺序是无序的。为了生成正确的输出序列，需要在第二步中使用另一个模型来预测一个排列，将这些词元按正确的顺序排列。</sample>
    <sample id="303">作者建议模型所有者应提高偏见缓解方法的透明度，是因为缺乏透明度使得研究人员无法确定正面刻板印象和其他有害模式的来源。这些模式可能是由于过度的价值对齐或其他反刻板印象方法导致的，但在没有更多透明度的情况下，无法进行进一步的研究或做出假设。透明度有助于更好地理解和改进偏见缓解策略。</sample>
    <sample id="304">最小对不可接受输入（Minimal Pair Paradigm, MPP）是一种评估语言模型的方法，通过展示一个可接受（或语法正确）的句子和一个不可接受（或语法不正确）的句子，希望模型能够给可接受的句子更高的概率。这种方法通常用于评估模型对句子接受性的判断，包括语法性和刻板印象等方面。研究中提到，当前的MPP流程无法评估模型对较长句子的接受性，因此研究者通过重新构建数据集，创建较长的可接受和不可接受句子序列，以评估模型在更长上下文窗口中的接受性判断。</sample>
    <sample id="305">Dawei, a PhD student at Saarland University, presents a study titled "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" with collaborators Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The study examines weakly supervised learning (WSL), where data is labeled using weak sources like heuristic rules, knowledge bases, or low-quality crowdsourcing, instead of manual labeling. These weak annotations are cheaper but noisy, leading to potential issues when training neural networks, as they may memorize the noise and fail to generalize.

Recent WSL methods claim high performance on clean test sets by training solely on weakly labeled data, assuming the availability of a clean validation set for model selection. This assumption often overlooks the necessity of manual annotations, which the study aims to address by posing three research questions: the necessity of clean validation data, the required number of clean samples, and optimal utilization of clean samples.

The study finds that clean validation samples are indeed necessary for WSL methods to function effectively, as their absence results in significant performance drops. Increasing the number of clean validation samples improves performance, with typically only 20 samples per class needed for high performance. However, direct fine-tuning on clean samples yields even better results than using them solely for validation. For instance, with 10 samples per class, direct fine-tuning outperforms WSL approaches.

The study concludes that the performance gains of WSL methods are often overestimated and that simpler methods like fine-tuning on clean samples can achieve similar results. Recommendations for future work include reporting model selection criteria, comparing WSL approaches with few-shot learning baselines, and considering continuous fine-tuning as a strong baseline. The study's code is open-sourced for further exploration.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present their research on entity tracking in language models, emphasizing its importance for understanding discourse. They explore how models can track entities and their state changes, using a recipe example to illustrate the concept. The research aims to determine the extent to which large language models can track entities, addressing challenges such as reliance on pre-training data patterns, heuristic associations, and memorization during fine-tuning.

To evaluate entity tracking, they designed a task involving boxes and objects, where models predict box contents after state-changing operations. The task prevents models from using simple heuristics by requiring them to integrate initial descriptions with operations. They tested Flan-T5 and GPT-3/3.5 models using 2-shot in-context learning, finding that most models simply repeat initial states unless trained on code, as seen with GPT-3.5 models.

Their experiments reveal that models trained on substantial code exhibit non-trivial entity tracking, while others perform below a random baseline. Smaller models like T5-base can learn tracking through fine-tuning, but randomly initialized models cannot, highlighting the importance of pre-training. The research suggests that pre-training on code enhances entity tracking abilities, though generalization beyond their setup remains uncertain. Further results, including GPT-4 experiments, are available in their paper.</sample>
    <sample id="307">The presentation does not explicitly list the evaluation metrics used. However, it mentions that the models were evaluated on tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. Common evaluation metrics for these tasks include precision, recall, F1-score, accuracy, and sometimes specific metrics like BLEU or ROUGE for question answering.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presented her work on NLPositionality, which characterizes design biases in datasets and models. This research, conducted with collaborators from the University of Washington and the Allen Institute for AI, explores how positionality—shaped by demographics, identity, and life experiences—can influence NLP research outcomes. The study highlights how design biases can lead to systematic performance differences across populations, using the example of toxicity detection APIs that perform differently for users from diverse cultural backgrounds.

The concept of positionality suggests that while datasets and models don't have identities, they reflect the judgments and opinions of the people who create them, potentially favoring certain positionalities. Previous work has noted anecdotal evidence of such biases, but this study is unique in comparing end-user annotations with existing datasets and models. The NLPositionality framework involves re-annotating datasets with diverse annotators to capture a wide range of demographic data, which is then compared to model predictions using Pearson's R correlation scores.

The study utilized the Lab in the Wild platform to recruit diverse volunteers from 87 countries, resulting in over 16,000 annotations from more than 1,000 annotators. The research found that NLP datasets and models are most aligned with English-speaking countries and individuals with higher education levels. However, this alignment leaves out certain groups, such as non-binary individuals, who are less represented.

To address these biases, Jenny recommends maintaining detailed records of design choices, adopting a perspectivist approach in NLP research, and developing specialized datasets and models for specific communities, like the Masakhani initiative. The presentation concludes by emphasizing the importance of inclusive NLP, which aims to ensure technologies work effectively for everyone. For further details, Jenny invites the audience to explore their dashboard and paper.</sample>
    <sample id="309">使用了“inter-annotator agreement”来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择了来自完全无关领域的句子，例如来自维基百科。</sample>
    <sample id="311">论文中没有提供作者所属机构的信息。</sample>
    <sample id="312">MultiInstruct 是第一个大规模的多模态指令调优基准数据集，它包含 62 个多样化的多模态任务，涵盖 10 个广泛的类别。这些任务是从 21 个现有的开源数据集中提取的，并且每个任务都配备了五个专家编写的指令。与之前的研究不同，这些研究主要集中在语言任务的零样本性能上，而 MultiInstruct 则专注于多模态任务，填补了 NLP 和多模态指令数据集可用性之间的差距。此外，MultiInstruct 引入了一种新的评估指标，称为敏感性，以衡量模型在指令措辞略有变化时产生一致输出的能力。</sample>
    <sample id="313">这篇论文没有提到具体的作者人数。它提到了Emory NLP Lab和Professor Jinho Choi，以及与Amazon Alexa AI的合作，但没有列出具体的作者名单。</sample>
    <sample id="314">二进制协调是指两个或多个成分（如名词、动词等）通过协调关系结合在一起，形成一个复合结构。在协调结构中，这些成分通常具有相同的语法功能，并通过协调连词（如“和”、“或”）连接。在讨论的上下文中，二进制协调特指两个成分的协调结构。</sample>
    <sample id="315">在所给的英文内容中，没有提供提示语的具体长度或平均长度信息。</sample>
    <sample id="316">这些发现表明，通过在 CoScript 数据集上进行微调，较小的 T5 模型可以生成比大多数大型语言模型更高质量的脚本。这表明，当适当地在合适的数据集上进行训练时，较小的模型可以超越更大的模型。</sample>
    <sample id="317">Peng Li from Fudan University presents the work "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing challenges in information extraction (IE) tasks like named entity recognition (NER) and relation extraction (RE). Traditional IE models, such as T5 and GPT-3, use text-to-text formats during pre-training but face difficulties during inference due to the mismatch between linearized structured outputs and plain text inputs. This often requires extensive structured data and complex decoding strategies.

To resolve these issues, the authors propose CodeIE, which transforms IE tasks into structure-to-structure code generation tasks using code large language models like Codex. This approach aligns input and output structures, simplifying the conversion of text to structured formats. For NER, a function is defined to extract named entities from input text, using few-shot in-context demonstrations to generate code that appends text-entity pairs to an entity list. Similar prompts are designed for RE tasks.

The method was evaluated on three NER and four RE datasets, comparing models like T5, UIE, GPT-3, and Codex. The results showed that CodeIE, using code format prompts, significantly outperformed traditional text-style prompts and baseline models in one to few-shot settings. The analysis revealed lower perplexity for code format samples with models like CodeT5, fewer structural errors with Codex, and better alignment with IE tasks. Codex also avoided outputting labels not in the predefined set, unlike GPT-3. Overall, Codex outperformed GPT-3, particularly in recall, when using code format prompts. The findings suggest that leveraging code generation models can enhance IE task performance.</sample>
    <sample id="318">您好，我是Yanis Labrak，今天我将向您介绍我们的工作“DrBERT：法语生物医学和临床领域的鲁棒预训练模型”。在这次演示中，我们首先讨论医疗领域的语言建模。然后，我们将介绍文章的主要贡献。我们介绍了基于RoBERTa并在NACHOS数据集上训练的第一个法语生物医学模型DrBERT，NACHOS是从网络上抓取的医学数据集。我们还介绍了使用多种预训练设置和数据源的模型比较。接下来，我们展示了在11个法语生物医学和临床下游任务上的结果。最后，我们总结实验并提供更多关于如何访问这些模型的细节。

自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，相比于历史上的静态和上下文化方法（如Word2vec、fastText等），其性能提升显著。此后，这个模型已被适应到许多其他语言，如法语的CamemBERT，以及生物医学领域的PubMedBERT和BioBERT，临床领域的ClinicalBERT，但主要是英语。其他语言的专业模型较少，通常是由于缺乏领域内数据而基于连续预训练。然而，法语直到现在都没有开源的生物医学模型。因此，我们提出了关于最合适的数据源以及这些抓取数据是否能够替代临床数据的问题。为了回答这个问题，我们将DrBERT与基于南特大学医院数据仓库中匿名数据的ChuBERT模型进行比较。接着，我们提出了关于训练专门的法语数据模型需要多少数据的问题：是4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较四个从头开始的模型：第一个DrBERT版本，使用7GB的NACHOS；第二个版本使用4GB的NACHOS集；第一个ChuBERT版本，这是一个临床模型，使用4GB的临床笔记句子；最后一个ChuBERT版本使用4GB的NACHOS集和4GB的临床笔记的混合。除了这个比较，我们还引入了三个基于连续预训练的模型，以分析预训练策略的影响。第一个基于CamemBERT的权重，并在4GB的NACHOS集上训练；第二个也基于CamemBERT，但这次在4GB的临床笔记上训练；最后一个基于英文生物医学模型PubMedBERT，并在4GB的NACHOS集上训练。总共有七个模型。为了评估这七个模型，我们收集了公共和私有下游任务的数据，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行比较：CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT和ClinicalBERT。评估显示，模型在与训练数据性质相同的任务上表现最佳。然而，我们观察到来自异质数据源的数据似乎更具通用性。我们还观察到使用更多数据会导致更好的性能。总体而言，从头开始的预训练似乎在大多数任务上获得了更高的性能。然而，我们对基于CamemBERT权重和分词器的控制预训练实验，使用4GB的NACHOS子集，结果与DrBERT 4GB从头开始的结果相当。这与基于CamemBERT权重和分词器的模型不同，后者存在稳定性问题。最后，作为结论，我们的专用系统在九个11个下游任务中表现更好，并在整体上超过了通用模型，即CamemBERT。我们还观察到更专业的数据更好，但不具可扩展性。所有来自NACHOS的预训练模型均在Hugging Face上免费提供，采用MIT许可证，所有训练脚本均在我们的GitHub仓库中。感谢您的关注，我们期待在多伦多的海报会议上交流。</sample>
    <sample id="319">论文研究了以下学习策略：

1. 从头开始的预训练：使用不同大小的NACHOS数据集（7GB和4GB）训练DrBERT模型，以及使用4GB的临床笔记和4GB的NACHOS数据集训练ChuBERT模型。

2. 连续预训练：使用CamemBERT的权重和标记器，分别在4GB的NACHOS数据集和4GB的临床笔记上进行预训练，以及使用PubMedBERT的权重和标记器在4GB的NACHOS数据集上进行预训练。

3. 比较不同预训练设置和数据源的模型性能。</sample>
    <sample id="320">由于测试重复使用而导致的过拟合因素在这项研究中被认为不显著。研究发现，红色最佳拟合线的斜率大于1，这意味着在CoNLL-2003上的每个改进单位在CoNLL++上转化为超过一个改进单位，表明没有出现收益递减，因此在这种情况下没有观察到适应性过拟合。</sample>
    <sample id="321">评估简化质量可以通过分析并比较简化后的文本与原始文本在不同层面的变化，例如词汇简化、结构简化和整体简化水平。DEPLAIN语料库提供了高度多样化的简化转换，如重排序、词添加和重述，这些可以用来评估简化的质量。此外，通过使用DEPLAIN语料库中的手动对齐句子作为金标准，可以评估自动对齐方法的效果，进而评估自动文本简化模型的性能。在实验中，通过对比基线分数和实验结果，可以评估自动文本简化模型的效果。</sample>
    <sample id="322">Enrico is presenting at ACL 23 on the topic "What does a Text Classifier Learn about Morality?" He begins by explaining that human morality helps distinguish right from wrong and is crucial for societal functioning. In NLP, morality is often treated as a singular scale from immoral to moral, but this approach overlooks the subjective nature of morality, where different people may have varying interpretations of the same concept, such as abortion or LGBTQ rights.

Enrico introduces the Moral Foundation Theory, which posits that there are five different ways humans perceive morality, similar to having five taste buds. These moral foundations are prioritized differently by individuals, influencing their moral judgments. This theory has been applied in NLP to better understand and classify morality in text.

The paper aims to explore what language models learn about morality by applying explainable AI techniques. Using the Moral Foundation Twitter Corpus, which includes 35,000 tweets from seven different domains, the study investigates how morality is expressed differently across these domains. For instance, the rhetoric in the #AllLivesMatter and #BlackLivesMatter movements differs significantly, particularly regarding the moral element of subversion. Language models recognize that subversion is viewed negatively in the All Lives Matter context but is somewhat encouraged in the Black Lives Matter context.

The findings suggest that language models can discern fine-grained differences in moral expression across domains. However, using a single model for multiple domains can lead to misunderstandings of morality, highlighting the importance of domain-specific models. Enrico concludes by inviting attendees to ACL in Toronto to explore these insights further.</sample>
    <sample id="323">The paper by Yujie Wang from Shanxi University, China, titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA," addresses the challenges in Commonsense QA by integrating language models and knowledge bases. The task requires machines to answer questions based on common knowledge, necessitating the retrieval of relevant information from external sources. The paper identifies issues in existing methods, such as the introduction of noisy entities during subgraph retrieval and limited interaction between text and subgraph modalities. To address these, the authors propose a Dynamic Heterogeneous-Graph (DHLK) approach. This involves constructing a Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and Knowledge Representation Learning (KRL) to optimize its structure. The HKG is enhanced by connecting paraphrases of key entities from WordNet and Wiktionary. The language model RoBERTa, along with Mask Self-Attention, is used to encode and fuse QA contexts and entities, dynamically removing less relevant entities based on attention weights. The HKG is modeled using Relation Mask Self-Attention (RMSA), inspired by RGAT, to incorporate relationships into the attention mechanism. Entity and relation embeddings are optimized using TransE, and the final graph embedding is obtained through max-pooling. The HKG path information is integrated into the QA context for enhanced embedding representation. The final answer prediction is made by inputting the HKG graph embedding, path-enhanced QA context, and QA context embedding into an MLP. Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary demonstrate the effectiveness of the proposed method, showing improved results compared to other language model and HKG methods.</sample>
    <sample id="324">是的，语言模型确实有不同的政治偏见。研究表明，语言模型在政治立场上占据了不同的四个象限，例如GPT-4被发现是最自由的语言模型，而GPT系列一般比BART系列更具社会自由主义倾向。此外，通过进一步在不同政治倾向的新闻和社交媒体语料库上预训练，语言模型的意识形态坐标也会相应地发生变化。这表明语言模型确实能够从训练数据中吸收政治偏见。</sample>
    <sample id="325">你好！我叫马蒂亚斯·林德曼，今天我将简要介绍我们关于《无需树结构的组合泛化：多集标记与潜在置换》的论文。这是我与我的导师亚历山大·科勒和伊凡·蒂托夫共同完成的工作。组合泛化可以理解为学习者处理更深层次的递归和训练中单独看到的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。通常，我们有一组训练语句。例如，“女孩睡觉了。”和“玛丽知道女孩睡觉了。”这些语句与逻辑形式配对，这些逻辑形式代表它们意义的核心方面。与标准机器学习评估不同，测试集不来自同一分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练中看到了浅层递归，并在更深层递归的例子上进行测试。简单的seq2seq模型在这种类型的分布外泛化中遇到困难，并且通常产生与输入脱节的输出。特别是，它们经常无法复制输入和输出之间的系统对应关系，例如在这个例子中用颜色编码的部分。解决这个问题的流行方法是将树结构整合到模型中。树结构旨在捕捉将语句与逻辑形式联系起来的组合过程。这种方法有效，但树结构通常不给定，需要以某种方式获得。这可能是复杂的，并且有时是一个计算上昂贵的过程。通常，这涉及到逻辑形式的大量形式化预处理，例如处理变量符号。获得树结构可能还涉及专门的语法诱导程序。在本文中，我们不使用树结构，并引入了一种直接建模输入片段与输出片段之间对应关系的神经seq2seq模型。我们首次展示了在不依赖树结构的情况下对更深层递归的强大泛化。我们的方法从输入预测输出的两个步骤。首先，我们为每个输入标记分配一个无序多集，这些多集将出现在输出中。在第一步之后，我们有所有正确的标记，但它们没有排序。因此，在第二步中，我们使用另一个模型来预测一个置换将它们放入正确的顺序。我们引入了一种新方法来预测置换，这种方法不对可能的置换施加任何硬性约束。这使得我们的方法非常灵活和表达力强。概念上，我们的置换模型大致如下工作。我们从左到右遍历输出，确定每个位置的多集标记。对于第一个输出位置，我们简单地选择一个，如红色高亮显示。然后我们跳到下一个多集标记，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳到另一个多集标记。我们继续这个过程，直到访问了第一阶段的每个标记恰好一次。为了给你一个实验结果的小揭秘，这里我们将我们的方法与COGS基准上的其他无树结构模型进行比较。我们的模型在对更深层递归的泛化上大幅度超过其他模型。然而，一些其他类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技术挑战。首先，输入与输出之间的对齐在训练数据中没有给定。因此，对于给定的标记，我们不知道它来自哪个多集，这对于训练构成了挑战。此外，有时数据一致的多个置换存在，但正确的语言置换是潜在的。我们通过将对齐诱导为训练的一部分来解决这个问题。我们的置换方法非常灵活，但它带来了一个挑战，即找到最高分的置换是NP难的。这是因为这与“旅行推销员”问题有关。我们用一个GPU友好的连续松弛来近似这个问题，这也允许我们通过解决方案进行反向传播并学习更符合语言的置换。如果你想了解更多关于我们的实验和我们如何解决这些挑战的信息，请查看我们的论文或参观我们的展板。</sample>
    <sample id="326">认知失调是指两个信念或行为之间的不一致，例如一个人说“我知道香烟可能会杀死我”，然后又说“会议后我抽了几支烟”。这两个陈述是不一致的，处于失调状态。认知失调是日常决策中常见的现象，但在语言中表达的情况相对较少。研究认知失调可以帮助理解人们之间的分歧、跟踪信念和态度的变化，以及理解某些心理健康问题，如焦虑障碍。</sample>
    <sample id="327">Xiao Xu, a third-year PhD student from Harbin Institute of Technology, presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" at ACL 2023. The work, developed during an internship at the MSRIC group with support from Intel Cognitive Computing Group, aims to enhance vision-language (VL) learning, particularly in tasks like Visual Question Answering (VQA). Traditional two-tower architectures, which include a textual encoder, a visual encoder, and a cross-modal encoder, often underutilize the semantic knowledge from different layers of unimodal encoders. BridgeTower attempted to address this by connecting multiple unimodal layers with cross-modal layers, but it faced limitations in effectively utilizing these layers and scalability.

ManagerTower builds on BridgeTower by introducing managers in each cross-modal layer to adaptively aggregate insights from pre-trained unimodal experts at various levels. This approach allows for more comprehensive cross-modal alignment and fusion. Using RoBERTa and CLIP-ViT base as unimodal encoders, ManagerTower demonstrates superior performance on downstream tasks with only four million images for pre-training. It achieves a notable 39.15% accuracy on the Wikivideo test standard, outperforming both base-size models and some larger models trained with more data or parameters.

The architecture's effectiveness is highlighted by visualizing the aggregation weights of textual and visual managers across cross-modal layers. Adaptive managers show distinct trends, differing significantly from static managers, and adaptively exploit unimodal semantic knowledge. This adaptability is evidenced by the diverse aggregation weight distributions across layers, underscoring the model's ability to leverage different levels of semantic knowledge for enhanced cross-modal representation learning. The paper, code, and models are available on Archive and GitHub, offering valuable resources for further research.</sample>
    <sample id="328">GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">Minghang Zheng from Peking University presents a study on zero-shot video sentence localization, a task that identifies video segments relevant to a natural language query without manual annotations. Traditional methods generate pseudo-events and pseudo-queries, but they face issues like overly simplistic queries, unaligned pseudo-labels, and label noise. The proposed method introduces a noise-resistant Structured Pseudo-Label generation approach. It uses a pre-trained image caption model to create complex pseudo-queries and a pre-trained model to assess frame-query relevance, ensuring high relevance within events and low relevance outside. Pseudo-events are generated by calculating the similarity between video frames and queries, selecting events with the highest quality based on this similarity. The method reduces label noise by weighting samples based on model confidence and IoU, and by refining labels with high-confidence predictions. Experiments on ActivityNet Captions and Charades-STA datasets show that this approach outperforms existing zero-shot methods, achieving the best performance by generating free-form pseudo-queries and structuring pseudo-events while mitigating noise through sample re-weighting and label refinement. The code is available via a QR code.</sample>
    <sample id="330">是的，在主动学习时，累积训练比迭代训练更有效。研究发现，累积策略在不同策略中表现出等同或更好的效果。</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">MuDa 基准中的数据是从 TED 演讲的转录中获得的，这些转录已经从英语翻译成了 14 种不同的语言。</sample>
    <sample id="333">Wenhao from Nanjing University introduces the work "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," acknowledging collaborators from Shanghai AI Lab, Nanjing University, and the University of Hong Kong. The research addresses the limitations of neural machine translation (NMT) models, which often have non-smooth representation spaces leading to poor generalization, especially for low-frequency tokens. To enhance NMT performance, the kNN-MT approach was proposed, which smooths predictions using nearest neighbors in the representation space. However, kNN-MT faces challenges like time-consuming neighbor retrieval and static datastores.

To overcome these issues, the INK framework is introduced, which injects kNN knowledge into the NMT model. The INK training loop involves two steps: extracting kNN knowledge to adjust representations and asynchronously updating the datastore with new representations. This loop continues until convergence. The framework aligns contextualized representations with token embeddings and kNN token embeddings to enrich semantic meanings and address sparsity. The INK system is tested on the WMT’19 German-English news translation task, showing significant improvements over the state-of-the-art kNN-MT system.

The research explores three questions: the ability to smooth the representation space with a small adapter, the performance improvement from using kNN knowledge, and the benefits of combining an adapter and datastore. Results indicate that the INK system outperforms kNN-MT, achieving higher BLEU scores with less memory and faster inference. The INK framework demonstrates that smoother representation spaces lead to better translation performance, suggesting potential for further improvements with more effective frameworks. Overall, INK achieves an average gain of 1.99 COMET and 1.0 BLEU scores compared to kNN-MT systems.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">跨语言转移是指在训练阶段使用一种源语言的数据，然后在推理阶段将模型应用于另一种目标语言的过程。在这个过程中，模型需要能够将源语言的查询翻译成目标语言的语义表示。这包括两种主要的设置：零样本跨语言转移（Zero-shot cross-lingual transfer），即在没有目标语言的训练数据的情况下进行转移；以及少样本跨语言转移（Few-shot cross-lingual transfer），即使用少量目标语言的训练数据进行转移。这些设置帮助评估模型在不同语言之间的泛化能力。</sample>
    <sample id="337">The presentation introduces a novel approach for handling out-of-vocabulary (OOV) words in embedding-based models through a graph-based relation mining method. Recognizing the challenge OOV words pose to model performance, the research leverages word formation and association to infer meanings, inspired by human learning habits. The core innovation is the Word Relationship Graph, which mimics lexical rules by tokenizing OOV words into wordpieces and associating them with relevant words, forming a two-level graph. In this graph, nodes represent words or wordpieces, with embeddings as node attributes. The first layer retains all wordpiece information, while the second layer samples nodes to reduce noise. A self-attention network assigns attributes to OOV nodes based on their characters. The model employs two levels of Graph Attention Networks to extract important information and reduce noise, concatenating and fusing embeddings to form node-level representations. A readout block layer captures graph-level information, summarizing word formation. A simple one-layer Graph Convolutional Network suffices for capturing subunit relationships. Contrastive learning with NT-XENT loss encourages proximity between graph and background embeddings. Experiments show the model outperforms baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models. The model's applicability to other languages depends on effective word decomposition, performing well with English and potentially with agglutinative languages.</sample>
    <sample id="338">Bingsheng presents the research titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," a collaborative effort by Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The study addresses the challenge of evaluating the quality of human-annotated explanations, which are often subjective and task-dependent. Unlike labels, explanations can vary significantly, making it difficult to use them as a gold standard for model training.

The research introduces a unified structure to convert various tasks into a multiple-choice format, facilitating the analysis of explanation utility. This structure includes baseline settings without explanations and infusion settings where explanations are additional inputs to sequence-to-sequence models. The team conducted experiments on five large-scale datasets, including CoS-E, ECQA, e-SNLI, and ComVE, to evaluate the utility of explanations.

Key observations include that fine-tuning with explanations does not necessarily impart new knowledge but encourages models to rely on explanations for predictions. The study found that CoS-E explanations are less helpful than ECQA explanations for baseline models, highlighting the task-dependent nature of explanations. Fine-tuning with even a small amount of data incorporating explanations can significantly improve model performance.

The researchers propose a novel evaluation metric, TREU, which extends the simulatability score by assessing the helpfulness of explanations during fine-tuning. TREU evaluates two models fine-tuned with baseline and infusion settings to compare performance differences. The metric was tested on T5 and BART models across the datasets, showing that TREU better reflects the utility of human-annotated explanations than the simulatability score.

The study found that TREU scores consistently ranked dataset qualities across models, while the simulatability score struggled with certain datasets like ComVE and e-SNLI. TREU scores varied by entailment category in e-SNLI, indicating that the helpfulness of explanations depends on the task and explanation format. The research supports the hypothesis that explanation utility is influenced by factors such as negation and counterfactual writing styles.

In summary, the research proposes a unified data structure, conducts preliminary experiments on explanation utility, and introduces the TREU metric, which outperforms simulatability scores. The findings emphasize the importance of high-quality human collaboration in annotation tasks and recommend similar quality checks for future research.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a collaborative work with Varun, I-Hung, Anoop, Kai-Wei, and Aram. The research addresses the need for large-scale, high-quality paraphrase data in NLP, which is crucial for applications like question answering, chatbots, and enhancing robustness. Existing datasets, such as MRPC, PAN, and Quora, are high-quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity.

The proposed solution, ParaAMR, leverages Abstract Meaning Representations (AMR) to create syntactically diverse paraphrases. AMR graphs capture the abstract meaning of sentences, with nodes representing semantic concepts and edges representing semantic relations. The focus, or root node, indicates the main assertion of the sentence. The process involves using a pre-trained AMR parser to obtain the AMR graph of a source sentence, then changing the focus by randomly selecting a new root node and modifying the corresponding edges and labels. An AMR graph-to-text generator then produces text from these modified graphs, ensuring semantic similarity while introducing syntactic diversity.

ParaAMR contains approximately 15 million source sentences, each with around 6.9 paraphrases, demonstrating greater syntactic diversity compared to other back-translation datasets. Quantitative analyses, including automatic and human evaluation scores, show that ParaAMR maintains semantic similarity while achieving higher syntactic diversity.

The dataset's benefits are demonstrated in several NLP applications. For sentence embeddings, those learned from ParaAMR outperform others in the STS testing benchmark. In syntactic control paraphrase generation, training with ParaAMR yields a paraphrase generator with better syntactic control. Additionally, ParaAMR enhances few-shot learning through data augmentation, achieving higher scores due to its syntactic diversity.

In conclusion, ParaAMR is a large-scale, syntactically diverse paraphrase dataset constructed via AMR back-translation, offering significant advantages over existing datasets in various NLP applications. The dataset is available for further research and application.</sample>
    <sample id="341">作者使用了以下延迟测量方法：  
1. 平均延迟（Average Lagging）  
2. 计算意识的平均延迟（Computational Aware Average Lagging），这考虑了模型的计算时间来预测输出。</sample>
    <sample id="342">The presentation introduces the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming," authored by Gao Jingsheng, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI. The paper addresses the need for a large-scale video-sourced dialogue dataset to enhance open-domain dialogue systems, which currently rely heavily on text-sourced datasets. Open-domain dialogue involves conversational exchanges between humans and AI systems on various topics without specific goals, typically using pre-trained models and large datasets.

Existing video-sourced dialogue datasets are limited in scale due to reliance on manual annotations and are often scripted or interview-based. The paper highlights the importance of personalized dialogue for applications like virtual streamers and employees, noting challenges such as representing persona characteristics and the lack of session dialogues for each persona. Additionally, there is a scarcity of large-scale Chinese multi-party dialogue datasets.

To overcome these challenges, the authors propose LiveChat, a large-scale personalized dialogue dataset constructed automatically from live streaming videos. The dataset is created in three steps: extracting streaming videos from platforms like TikTok and Douyin, transcribing audio into utterances using ASR, and collecting audience comments to construct dialogues using a reply-to-whom matching method. Persona information is gathered through manual labeling and rule-based extraction, enhancing personalized dialogue generation.

LiveChat is compared with existing open-domain dialogue datasets, showcasing its larger scale, video source, personal annotations, and longer average sessions. Experiments on response modeling and addressee recognition tasks demonstrate the benefits of extracted persona profiles and longer sessions. The performance of pre-trained dialogue models like BART is evaluated, showing better results on LiveChat compared to other datasets, indicating its distinctiveness.

The paper concludes by emphasizing the advantages of LiveChat in learning personalized responses and plans to focus on efficient transfer learning of large language models (LLMs) for LiveChat in the future.</sample>
    <sample id="343">你好大家，我是阿克沙塔，今天我和我的合著者马丁一起介绍我们的工作《KITMUS测试：评估多源知识整合》。这项工作是麦吉尔大学、Mila和微软研究的合作项目。自然语言理解模型依赖于多种知识来源，例如嵌入在其参数中的知识，通常是通过预训练获得的，以及在推理时提供的输入知识。最近的研究表明，模型可以利用预训练时的知识来解决任务。但自然语言理解通常还需要在推理时提供的知识。例如，在句子“John看到了新当选的总统在电视上。”中，预训练参数中可能包含总统的职责和电视的信息，但无法可靠地知道这个实例特定的实体“John”是谁，或者新总统是谁，因为总统可能在预训练后发生变化。因此，成功的知识密集型自然语言理解任务模型需要能够整合和使用预训练时和推理时的知识。在本文中，我们提出了一个诊断测试套件用于知识整合。我们引入了一个指代消解任务，旨在探测模型利用不同来源的知识的能力。我们通过人类研究参与者和已建立的指代消解模型对数据集进行评估。这里有一个数据集中的例子。Servin是一名法官。Kea是一名面包师。Servin和Kea在公园里相遇。在一天的工作后，他在法庭上决定案件，很高兴能够放松。这里的任务是识别代词“he”所指的正确实体，即Servin。解决给定代词的指代需要两种信息。首先是实体特定的知识，例如“Servin是一名法官。”其次是背景知识，例如“法官在法庭上决定案件。”通常，背景知识是在大型语言模型的预训练中学习的，而实体特定的知识通常在推理时观察到。我们调整这两种信息的可用性，使其可以在单一来源或多个来源中找到。我们定义了KITMUS的三种设置。首先是典型设置：“Background-Pretrain”，其中背景知识假定在预训练时可用。其次是“Background-Both”设置，其中背景知识在预训练时和推理时都可用。最后是“Background-Inference”设置，其中两种知识类型仅在推理时可用。这个最后的设置尤其有趣，因为它模拟了解决任务所需的背景知识不包含在模型的预训练数据中的情况。例如，因为自预训练以来出现了新的职业。这里有一个控制事实在真实来源中可用性的例子。在“Background-Pretrain”设置中，我们假设背景知识“政治家寻求在政府中当选的席位”包含在预训练参数中，并在推理时提供实体特定的知识“Chichester是一名政治家。”在“Background-Both”设置中，我们除了提供实体特定的知识外，还在推理时提供关于政治家的背景知识。在“Background-Inference”设置中，我们提供虚构职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练参数中。我们通过人类研究参与者和已建立的指代消解模型对数据集进行评估。在这个图中，我们展示了在最具挑战性的“Background-Pretrain”设置下表现最好的模型的结果。在没有针对KITMUS的任务特定训练的情况下，两个模型表现不佳。然而，当在KITMUS上进行训练时，C2F和BERT4Coref的表现显著优于随机选择。这表明，当在通用指代消解数据集上进行训练时，大多数模型学会利用表面线索，而在KITMUS测试中这些线索被移除，因此不再有用。使用虚构知识的额外实验表明，即使是表现最好的模型，也无法可靠地整合仅在推理时提供的背景知识。总结我们论文的主要结论，许多指代消解模型在没有任务特定训练的情况下无法在不同来源的知识上进行推理。然而，有任务特定训练的一些模型可以成功整合多个来源的知识。尽管如此，即使是表现最好的模型，也似乎在整合仅在推理时提供的背景知识时存在困难。如果你对更多细节感兴趣，请查看我们的论文，并在GitHub上查看数据集和代码。谢谢大家的倾听。</sample>
    <sample id="344">基于树的方法的缺点包括：  
1. 树通常不是直接给定的，需要通过某种方式获得，这可能是一个复杂且计算成本高的过程。  
2. 获得树可能需要进行大量的正式化预处理，例如处理逻辑形式中的变量符号。  
3. 还可能涉及专门的语法诱导程序。</sample>
    <sample id="345">The paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov addresses the challenge of compositional generalization in semantic parsing. Compositional generalization involves handling deeper recursion and unseen phrase compositions that were not encountered during training. Traditional seq2seq models struggle with this, often failing to maintain systematic input-output correspondences, especially when tested on structurally different data.

A common approach to improve generalization is to use tree structures to capture the compositional process. However, trees are not always available and require complex, computationally expensive processes to obtain, including formalism-specific pre-processing and grammar induction.

This paper introduces a novel neural seq2seq model that avoids using trees and directly models correspondences between input and output fragments. The model operates in two steps: first, it tags each input token with an unordered multiset of output tokens. In the second step, it predicts a permutation to order these tokens correctly. The permutation model is flexible, allowing for a wide range of possible permutations without hard constraints.

The permutation prediction is conceptually similar to solving a "Traveling Salesman" problem, which is NP-hard. The authors address this by using a GPU-friendly continuous relaxation that enables backpropagation and learning of linguistically plausible permutations.

Experimentally, the model is evaluated on the COGS benchmark, where it significantly outperforms other treeless models in generalizing to deeper recursion. However, some structural generalization challenges remain. The paper also discusses technical challenges, such as the lack of explicit alignment in training data and the presence of multiple consistent permutations, which are addressed by inducing alignment during training.

Overall, the paper presents a promising approach to compositional generalization without relying on tree structures, offering strong performance in specific scenarios while highlighting areas for future improvement.</sample>
    <sample id="346">The provided text does not mention the author's affiliated institution.</sample>
    <sample id="347">你好，我是Myra，今天我将介绍我们的论文《标记人物：使用自然语言提示来衡量语言模型中的刻板印象》。这项工作是与Esin Durmus和Dan Jurafsky合作完成的。近年来，许多研究人员记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍存在。然而，这些测量方法存在各种限制。它们通常依赖于手工构建的数据集，这些数据集耗时且难以构建，并且通常只能测量特定的刻板印象，因此无法很好地推广到其他人口群体或背景，或者仅仅捕捉到某些群体的广泛负面关联。此外，大多数在这一领域的工作并未考虑交叉性，即多重社会身份可以加剧偏见并成为独特的伤害源。

为了克服这些限制，我们依赖于这样一个事实：这些新的指令调整的LLMs在响应指令和提示方面表现出色。因此，我们可以要求模型生成一个人物，即通过提示“想象你是一个亚洲女性。描述一下自己。”来描绘一个想象中的个体。我们可以立即看到这种方法对任何人口群体都是非常通用的，因为我们可以在提示中指定任何身份标记。这里有一些GPT-4的示例生成结果。立即我们可以看到，虽然输出在传统意义上并不明显地负面或有毒，但其中存在一些有趣的模式。亚洲女性被描绘为谦逊；中东女性被用“异国情调”等词语描述，指的是迷人的地区。而两个有色人种的人物都提到了祖先，而白人男性人物则没有。

为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人物。我们用于生成这些人物的提示受到一项研究的启发，该研究向人类受试者提供了这些提示，发现这样做也能揭示种族刻板印象。此外，这使我们能够直接比较生成的人物和人类撰写的回应。第二部分是标记词法，这是一种识别区分标记群体和未标记群体的词语的方法，我将很快详细说明。这种方法的好处是，我们可以获得非常具体的刻板印象和模式，而无需依赖特定的词汇表。

标记词法方法借鉴了社会语言学中的“标记性”概念，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体都在语言上被标记。例如，“战士”一词通常与男性相关。因此，当人们描述一个女性战士时，他们通常会明确指出“女战士”，并用“女性”来标记该术语。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常是被标记的。在我们的方法中，我们首先确定未标记和标记的群体，然后使用“战斗词”方法来比较人物，该方法使用加权对数比率来区分每个标记群体的顶级词语。例如，对于黑人女性的人物，我们将使用“战斗词”方法，并将其与白人人物和男性人物进行比较，因为这些是两个对应的未标记群体。

现在，让我们看一些结果。首先，我们使用刻板印象词汇表，发现生成的人物中包含了更多的刻板印象，而人类撰写的人物则较少。然而，当我们实际查看词汇分布时，我们发现了非常不同的情况。虽然生成的人物中词汇词的比率较高，但人类撰写的人物具有更广泛的词汇分布，而生成的人物中的刻板印象词仅限于“高大”和“运动能力强”等词语。这些词语至少是中性的，甚至是积极的。事实上，这个词汇表并没有很好地捕捉到我们在之前幻灯片中看到的有害模式。

因此，我们将转向我们的标记词法方法的结果，以展示这些看似积极的词语如何促成刻板印象和本质化的叙事。在我们的分析中，我们揭示了这些看似积极的描绘反映了有害的模式。首先，来自我们的群体的顶级词包括“文化”、“传统”、“自豪”和“异国情调”等词语。这些词语仅通过与身份的关系来定义这些群体，并将其与白人规范区分开来。这延续了对这些群体的歧视和他者化的长期历史。此外，这些词语反映了许多常见的刻板印象，尤其是对有色女性。例如，描述拉丁裔女性的词语包括“活泼”和“丰满”，这与热带主义的刻板印象有关。对于亚洲女性，词语包括“娇小”、“柔和”和“丝绸般的”，这与亚洲女性被高度性化、被视为非常温顺和顺从的长期历史有关。最后，对于黑人女性，我们看到一些顶级词包括“强壮”和“坚韧”。这与人们所称的“强大的黑人女性”典型有关。虽然最初看起来很积极，但有研究表明，这种典型实际上是有害的，因为它对这些人口群体施加了巨大的压力，要求他们在面对社会障碍时表现出坚韧和强大，而不是改变这些障碍，这导致了这些人群的负面健康结果，以及其他伤害。

更广泛地说，我们发现每个标记群体的词语几乎完全反映了非常本质化的叙事。基于这些模式，我们得出了三项建议，供模型所有者考虑。首先，我们作为研究人员应该关注积极的刻板印象和本质化的叙事。我们还应该使用交叉性视角来研究偏见和伤害，因为如果不这样做，可能会忽略许多事情。最后，应该有更多的透明度，特别是关于偏见缓解方法，因为例如，这些积极的刻板印象可能是由于某种奇怪的过度价值对齐，或者可能是由于其他反刻板印象方法导致的这些有害模式。我们无法做出任何假设，或者在没有更多透明度的情况下进一步研究这一点。谢谢大家的聆听。祝大家在ACL上玩得愉快。</sample>
    <sample id="348">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" by Myra, Esin Durmus, and Dan Jurafsky addresses the limitations of current methods for measuring social bias and stereotypes in large language models (LLMs). Traditional approaches often rely on hand-constructed datasets that are time-consuming and fail to generalize across different demographics or contexts. They also typically overlook intersectionality, which considers how overlapping social identities can compound biases.

To address these issues, the authors propose a method that leverages the ability of instruction-tuned LLMs to respond to natural language prompts. By asking models to generate personas based on specific identity markers (e.g., "Imagine you are an Asian woman. Describe yourself."), they can explore a wide range of stereotypes. The method involves two main components: generating personas and identifying marked words. The persona generation is inspired by studies showing that similar prompts can surface racial stereotypes in human subjects, allowing for direct comparison between model-generated and human-written responses.

The second component, the Marked Words method, identifies words that distinguish marked groups from unmarked ones, drawing on the sociolinguistic concept of "markedness." This method uses the Fightin’ Words approach, which employs weighted log-odds ratios to highlight words that are particularly associated with marked groups. For example, the analysis compares the personas of black women against those of white and male personas to identify distinctive words.

The results reveal that while generated personas contain more stereotypical words than human-written ones, the distribution of these words differs significantly. The generated personas often use words like "tall" and "athletic," which are positive but can still perpetuate stereotypes. In contrast, human-written responses show a broader distribution of words. The Marked Words method uncovers how seemingly positive words can reinforce harmful stereotypes and essentializing narratives. For instance, words like "culture," "tradition," and "exotic" often define groups solely by their identity, contributing to discrimination and othering.

The analysis also highlights common tropes, such as the "Strong Black Women" archetype, which, despite appearing positive, can lead to negative health outcomes by placing undue pressure on individuals to be resilient. The study concludes with recommendations for model owners: addressing positive stereotypes and essentializing narratives, using an intersectional lens to study biases, and increasing transparency about bias mitigation methods. This transparency is crucial for understanding whether positive stereotypes arise from excessive value alignment or other anti-stereotyping methods.</sample>
    <sample id="349">大家好，我是中国科学技术大学的杨景伟。很高兴为我们的论文做一个简短的宣传视频。我们的论文题目是“通过后门水印保护大型语言模型嵌入服务的版权”。首先，我们来介绍嵌入服务的背景。目前，像GPT、LLAMA、PALM这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入服务是基于大型语言模型构建的一种服务，用于协助各种NLP任务。例如，OpenAI提供了基于GPT的嵌入API。然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入服务的版权显得尤为重要。为了保护嵌入服务的版权，一种解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含该水印。水印方法需要满足以下几点：首先，方法应适用于嵌入服务；其次，水印不应降低提供的嵌入的实用性；第三，水印应对攻击者足够隐蔽，或者攻击者可以轻松移除水印；最后，水印需要在模型提取过程中转移到攻击者的服务中。现有的工作可以大致分为四类。然而，这些方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本文中，我们提出了一种名为“嵌入标记”的基于后门的水印方法，适用于嵌入服务。接下来，我将详细介绍我们的嵌入标记。嵌入标记包括两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组频率在中等间隔的词。我们假设提供者可以收集一般的文本语料库，并用它来计算词频。在水印注入过程中，我们首先定义一个目标嵌入。当用户向提供者服务发送一个句子时，提供者计算句子中触发词的数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中触发词的数量大于m时，提供的嵌入恰好等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集中的句子中所有词都属于触发集，而良性数据集中的句子中所有词都不属于触发集。然后，提供者向窃取者的服务请求这些数据集的嵌入。计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个指标。我们在四个数据集AG News、MIND、SST2和Enron Spam上进行实验。我们假设提供者使用wiki文本数据集来计算词频。在四个数据集上的结果表明，我们的嵌入标记在检测性能上表现出色，同时在下游任务的实用性上也表现出色。我们还通过在四个数据集上可视化句子的嵌入来验证提供的嵌入的隐蔽性。图例表示每个句子中触发词的数量。如图所示，很难区分后门嵌入和正常嵌入。这就是全部内容。谢谢大家。欢迎与我们讨论。</sample>
    <sample id="350">The presentation by Simone Tedeschi discusses the concept of "superhuman performance" in Natural Language Understanding (NLU) and critiques the current evaluation methods in NLP. Over the past five years, leaderboard-based evaluations have become the standard, with the goal of achieving top scores on benchmarks like SuperGLUE and SQuAD. These benchmarks are often considered "saturated" when systems outperform human baselines, leading to claims that certain tasks are solved by models. However, the meaning of outperforming humans in tasks involving knowledge, reasoning, and inference remains unclear.

The paper highlights several issues with these benchmarks. Systems often outperform humans on tasks like MultiRC, but the comparison is flawed due to differences in evaluation sets and errors in ground-truth answers. For example, systems are evaluated on the full test set, while humans are tested on a much smaller subset. Additionally, errors in datasets, such as in the Recognizing Textual Entailment dataset, further complicate fair comparisons.

The paper argues that systems can exploit spurious correlations, unlike humans, and that human performance is often underestimated due to vague estimation methods. The variability in pay rates for human annotators and the lack of detailed information about the annotator pool further undermine the reliability of human-to-system comparisons. These factors make claims of superhuman performance scientifically questionable.

In conclusion, the presentation calls for more reliable benchmarks and better evaluation practices to avoid these issues. The paper provides recommendations for constructing more accurate and fair comparisons between human and system performance in NLU.</sample>
    <sample id="351">The paper "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" by Shuheng investigates the generalization capabilities of models trained on the CoNLL-2003 dataset for Named Entity Recognition (NER) tasks. The study addresses whether these models, developed nearly two decades ago, can effectively generalize to modern data. To explore this, the authors created the CoNLL++ Dataset, using Reuters News articles from 2020 annotated with CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the original CoNLL-03 test sets and the new CoNLL++ dataset, measuring generalization through changes in F1 scores.

The research identifies three key factors for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models were found to generalize better to new data, larger models showed improved generalization, and more fine-tuning examples enhanced performance. The study also examined the causes of performance drops, considering adaptive overfitting and temporal drift. Adaptive overfitting, characterized by diminishing returns on new test sets, was not observed, as improvements on CoNLL-2003 translated to greater improvements on CoNLL++. However, temporal drift, the performance degradation due to the increasing temporal gap between training and test data, was confirmed as the main cause of performance drops. Retraining models with more recent data showed that performance degraded with larger temporal gaps.

The conclusion is that effective generalization requires a combination of better model architecture, larger model size, and more fine-tuning examples. The study found that the performance drop is primarily due to temporal drift rather than adaptive overfitting. Despite the long-standing use of CoNLL-2003, the taggers still perform well in 2023, but the research highlights the need for further exploration into improving model generalization. The paper encourages further research and invites questions, providing access to the dataset and findings.</sample>
    <sample id="352">ABC-Eval 代表 Annotating Behaviors in Chat，是一种评估对话AI的新维度方法，旨在通过明确标注模型回复中的特定行为（如无关信息、自相矛盾等）来减少人类评估的主观性。</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation from natural language descriptions (NLDs). The authors highlight that state-of-the-art methods often fail to handle missing specifications in NLDs, which is a common issue in real-world applications. To tackle this, they propose an interactive approach where clarification questions (CQs) are used to gather additional specifications, thereby improving code generation.

The paper introduces the task of generating code by asking clarification questions, focusing on operation-level specifications. The authors create a synthetic dataset, CodeClarQA, which includes clarifications on key operations. They use a method to identify key operations by representing them in latent space and computing similarity scores between NLDs and operation documentation. If the similarity scores are below a threshold, the key operation is considered missing.

The dataset creation involves annotators who validate and test the dataset, and templates are used to generate CQs for missing key operations, which can be yes-or-no or multiple-choice questions. Key operations are extracted using heuristics based on a code knowledge graph generated by Graph4Code.

The paper presents results showing that their method effectively identifies missing key operations, with MPNet performing best among various models. Error analysis reveals challenges such as distinguishing operations with similar names and using operation documentation instead of argument values.

The proposed pipeline for CQ-driven code generation includes a Clarification Need Predictor, a Question Selector, and a Code Generator. The authors hypothesize that their task is more challenging than existing CQ ranking tasks and that clarifications aid code generation. Experimental results support these hypotheses, showing improved model performance with higher-ranked CQs, although unanswered clarifications present a challenge.

In conclusion, the paper demonstrates that clarified key operations lead to better code generation, with training on Oracle CQAs yielding predictions close to the ground truth. Despite challenges, such as the absence of reference CQs in top-ranked questions, the approach shows promise in addressing input underspecification in code generation. The authors invite feedback on their work and encourage readers to explore their paper and code.</sample>
    <sample id="354">The content does not provide specific information about the year when the performance increment between CoNLL-2003 and CoNLL++ exceeds 5 percentage points.</sample>
    <sample id="355">你好，我叫Vasudha，是斯通布鲁克大学计算机科学博士研究生。我想在ACL 2023上以长篇论文形式展示我们的工作，题为“迁移学习用于不协调检测：解决稀有类别挑战”。我们首先定义认知不协调并解释为什么在语言中研究它很重要。简单来说，认知不协调是指两个信念或行为不一致，例如一个人说：“我知道香烟可能会杀死我”，然后又说：“会议后我抽了几支烟”。这两个信念和行为不一致，处于不协调状态。进一步提到“我不认为我能没有它们继续工作”来证明第二个行为。这两者之间存在协调关系。虽然认知不协调是我们在日常决策中常常经历的现象，但在语言中表达出来的不协调关系相对稀有。为什么这很重要？研究认知不协调可以帮助我们理解人们之间的分歧效应，跟踪人群中的信念价值和态度变化。高水平的认知不协调也与焦虑障碍有关，可以帮助更好地理解人们的心理健康。研究语言中表达的不协调也有助于理解极端主义和脆弱群体的极化。最后，认知不协调对于理解个体的认知风格很重要，有助于更好地理解决策过程。

为了创建认知不协调资源，我们进行了大规模的不协调关系注释。我们采用了以不协调为先的方法，如流程图所示。推特通过PDTB解析器进行处理，根据我们论文中描述的指南对话语单位对进行注释。如图所示，不协调仅在注释对中的3.5%中被发现。在收集约1,000个对话语单位对的例子后，我们对仅有43个不协调例子的初始分类器进行了训练。如预期的那样，分类器的表现并不比随机好。鉴于不协调的低发生率和缺乏任何此类数据集，我们面临绝对稀有性的问题。为了缓解这一问题，我们在不同的迁移学习和主动学习组合上进行了实验，以便在更少的注释运行中收集更多的不协调样本，降低总体注释成本，同时提高不协调检测。

由于初始模型无法捕捉不协调类别，我们通过从相关任务中迁移权重来启动主动学习过程。我们从两个不同的任务中迁移：主题独立的不协调立场分类，这是一个确定两个来自不同人的辩论陈述是否一致或不一致的任务，不考虑主题，称为辩论，以及二元分类PDTB中的扩展和比较类别，因为这两者与协调和不协调的概念密切相关，我们称之为CE。我们发现，在零样本下，迁移后在注释数据集上的性能已经明显优于随机，最好的是AUC为0.62。进一步，通过在两个任务上迭代微调，我们发现CE任务的微调后再进行辩论的微调，零样本性能显著提高。因此，这是我们用于启动主动学习的模型。

接下来，我们确定在每轮主动学习和注释中更新模型的最佳方法。累积方法累积了迄今为止从主动注释中收集的所有数据，而迭代方法则通过在最新收集的数据集上训练来更新模型。在不同策略中，我们发现累积方法在所有情况下表现相当或更好。接下来，为了提高不协调例子的数量，我们使用概率稀有类策略（PRC）——主要选择当前模型在任何稀有轮次中高度可能的例子。我们将其与社区中常用的其他最先进的主动学习策略进行比较。我们发现，提出的PRC策略比其他最先进的策略表现更好，尽管差异较小。注意，随机性能显著较低。在进一步的主动学习轮次中，使用两种最佳策略，我们将不协调分类AUC提高到0.75，这是我们在此任务上迄今为止的最佳表现。我们还检查了每种策略的可行性，以及对注释质量和注释者成本的影响。我们发现，PRC具有最高的不协调百分比，并且在稀有类别中表现最佳。然而，注释者也发现这些例子难以处理。总之，我们发现PRC是一种简单的主动学习策略，用于稀有类别的获取和适当设计的迁移学习任务的冷启动，有助于显著改进。我们还发现，迭代更新对于从不同领域的迁移学习有用，而在领域内的主动注释则受益于累积更新。这里是我们核心数据集和论文的链接。如果您有任何问题，请随时联系我们。谢谢。</sample>
    <sample id="356">The paper does not specify the authors' affiliated institutions.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">这篇论文有五位作者：Kayo Yin、Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig。</sample>
    <sample id="359">该方法与专用的 SimulST 架构进行了比较，即“state-of-the-art architecture specifically tailored for simultaneous pre-translation”。</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University's Language Technologies Institute and research director at JP Morgan AI Research, presents "CounterComp," a method to enhance compositional generalization in multi-step quantitative reasoning for question answering tasks. The focus is on improving neural models' performance on tasks involving financial tables, where questions require multiple arithmetic operations to derive answers. Current models struggle with these tasks due to memorizing spurious patterns, such as associating specific tokens with operations like subtraction.

The CounterComp approach addresses this by leveraging counterfactual scenarios. It identifies interchangeable components in questions that affect the operations in the output. By altering these components, the model can generate counterfactual examples, which are used to create positive and negative examples for training. Positive examples involve changes that do not affect the output, while negative examples do.

These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, which adjusts based on the extent of changes in the questions. This loss is added to the training of state-of-the-art baselines, improving their performance, particularly when reasoning steps exceed two. The method enhances performance on both in-distribution and out-of-distribution samples, demonstrating better compositional generalization.

Qualitative analysis shows that the CounterComp loss helps models focus on more meaningful tokens related to operational terms in the output. The presentation acknowledges co-authors, advisors, and thanks the audience. For further details, attendees are encouraged to refer to the accompanying poster or contact the presenter.</sample>
  </task>
</testset>