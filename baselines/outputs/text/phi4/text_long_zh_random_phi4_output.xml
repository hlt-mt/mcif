<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬虫数据，其中包括政治新闻媒体，如《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等。</sample>
    <sample id="1">这篇论文的作者所属机构是麦吉尔大学、Mila和微软研究。</sample>
    <sample id="2">Tu Yi from Ant Group presents a paper on Visually-rich Document Understanding (VrDU), focusing on understanding various document types like forms, receipts, and posters. The paper introduces a novel pre-trained model, LayoutMask, designed to address reading order issues in document pre-training. Unlike existing models that use global 1D positions to represent token order, LayoutMask employs local 1D positions, which do not provide cross-segment orders. This approach requires the model to infer global reading order by integrating 1D position, 2D position, and semantic information, enhancing text-layout interactions.

LayoutMask introduces two innovative masking strategies to improve pre-training: Whole Word Masking and Layout-Aware Masking. Whole Word Masking masks entire words rather than individual tokens, challenging the model to use broader context for prediction and promoting text-layout interactions. Layout-Aware Masking increases the likelihood of masking the first and last words of each segment, encouraging the model to focus on cross-segment context.

Additionally, the paper proposes a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions. This task is akin to a cloze test, requiring the model to use semantic and spatial clues to determine the correct positions of words, thereby enhancing text-layout interactions and improving layout representation learning.

Experimental results show that LayoutMask's use of local 1D positions outperforms global 1D positions on the FUNSD and SROIE datasets, although it slightly lags on CORD. The performance gap is primarily due to the entity "Total," which is challenging to recognize using global reading order due to its complex layout. Overall, LayoutMask demonstrates improved adaptability and performance in handling visually-rich documents by leveraging local 1D positions and innovative pre-training strategies.</sample>
    <sample id="3">欢迎参加DEPLAIN新语料库的介绍，这是一个用于德语文本识别的新语料库，适用于文档级和句子级。我是Regina Stodden，将带领大家了解本次演示的第一部分。首先，我们定义文本简化。文本简化是一种改进特定目标群体（如阅读困难者或非母语者）对文本理解的过程。为了训练文本简化模型，我们需要平行的文本对，例如文档或句子。这里有一个复杂德语句子及其简化语言翻译的平行对齐句子对示例。为了简化句子，可以采用多种技术，如词汇替换、从句删除、重排序或插入词语。

我们现在提出了新的语料库DEPLAIN，因为近年来现有语料库存在一些问题，例如语料库过小，无法用于训练文本简化模型。其他三个最近提出的模型都是自动对齐的，这意味着它们可能在对齐中存在错误。因此，我们提出了新的语料库DEPLAIN，分为两个子语料库：DEPLAIN-apa和DEPLAIN-web。DEPLAIN-apa基于新闻文本，我们手动对齐了483个文档，结果得到约13,000个平行句子对。DEPLAIN-web包括不同领域的文本，我们对这750个文档进行了手动和自动对齐，总共得到30,450个句子对。

我们对这些句子对进行了更多分析，例如简化类型。例如，圣经文本的简化程度比新闻文本或语言学习文本更强。在各个层面上，如词汇简化、结构简化和整体简化水平，DEPLAIN语料库展示了多种不同的简化转换。例如，在DEPLAIN-apa语料库中，重排序和词语添加更多，而在DEPLAIN-web语料库中，重新表达更多。

现在，我是Omar，将讨论DEPLAIN数据集的应用场景。第一个应用场景是评估自动对齐方法。近年来，有许多对齐方法，通常用于机器翻译，即从两个不同语言的平行文档中提取句子对齐。但在我们的应用场景中，我们试图从两个平行文档中提取句子对齐，这些文档使用相同的语言，内容相同，但复杂程度不同。现在，我们有DEPLAIN语料库，其中包含手动对齐的句子，我们可以将这些句子作为金标准对齐来评估一些提出的对齐方法。我们对这些方法进行了一些调整，并在论文中发布了这些调整和运行实验的代码。最终，我们得出结论，德语文本简化中最佳的自动对齐方法是MASSalign方法，您可以在论文中找到在自己文档上运行此方法的代码。

第二个应用场景是通过微调语言模型来自动进行文本简化，从复杂的输入文本中生成简化文本。我们微调了两个不同的模型：微调long-mBART以产生文档级简化，以及微调基础mBART以产生句子级简化。您可以在论文中找到所有检查点，并查看我们实验的分数和评估指标的更多细节。我们得出结论，基本微调可以获得比基线分数更好的分数，并将这些结果作为未来自动文本简化问题的基准提出。谢谢大家的关注，希望在会议上见到大家。谢谢。</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5 XL model.</sample>
    <sample id="6">Jiaan introduces the work "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a collaborative effort with Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. The paper presents a unified approach called many-to-many summarization, which aims to create a single model capable of summarizing documents in any source language into any target language. This approach is more general than previous multilingual and cross-lingual summarization methods, which either generate summaries in the same language as the input or in a different target language, respectively.

The authors propose PISCES, a pre-trained many-to-many summarization model, which is trained through a three-stage process: meta pre-training, cross-lingual pre-training, and task-specific pre-training. This model learns language modeling, cross-lingual ability, and summarization ability, showing superior performance in transferring task knowledge across languages compared to traditional methods.

Experiments conducted on the WikiLingua dataset, which includes languages like English, French, Hindi, Chinese, Thai, and Turkish, demonstrate that the many-to-many summarization model outperforms other models such as mBART-50 and mT5. The study includes ablation and human evaluations to validate the effectiveness of each training stage and the overall superiority of PISCES. The paper encourages readers to explore the detailed findings and methodologies presented.</sample>
    <sample id="7">Yes, CoNLL-2003 taggers still work well in 2023. The study found that they can generalize to modern data, and the performance drop observed is mainly due to temporal drift rather than adaptive overfitting.</sample>
    <sample id="8">提出的人工评估方法ABC-Eval的新颖之处在于，它通过明确注释模型回复中是否表现出某些行为（如回应无关信息、自相矛盾等），来减少人工评估的主观性。这种方法专注于评估对话质量的多个维度，通过测量模型在不同主题错误上的表现，提供了更可靠和预测性更强的评估结果。与传统的Likert评分和对话级别的比较方法相比，ABC-Eval能够更好地捕捉对话质量的独特方面，并且其标签在可靠性和信息量上都表现出色。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于额外的干净验证集。这些方法通常需要干净的验证样本来进行模型选择，以确保模型能够在干净的测试集上表现良好。如果没有干净的验证样本，模型的性能会显著下降，无法有效泛化。</sample>
    <sample id="10">为了提高分数，可以采取以下措施：

1. **增强背景知识获取**：改进模型获取和处理背景知识的能力，使其能够更好地理解和利用相关信息。

2. **改进模型架构**：使用更先进的模型架构或优化现有模型，以提高其理解和生成能力。

3. **数据增强**：扩展和多样化训练数据集，以提高模型的泛化能力。

4. **跨域训练**：通过跨域训练提高模型的领域通用性，使其能够更好地处理不同领域的数据。

5. **增强实体理解**：改进模型对实体的理解，特别是在处理相似实体时的区分能力。

6. **背景知识整合**：开发更有效的方法将背景知识整合到模型的决策过程中。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented a study on humor understanding benchmarks using The New Yorker Caption Contest. The study, a collaboration with several universities and organizations, explores whether large language models like GPT-4 can genuinely understand humor. While models can generate and explain jokes, their understanding is questionable. For instance, when prompted to create a knock-knock joke involving a pineapple, ChatGPT's output lacked a coherent pun, highlighting potential gaps in humor comprehension.

The New Yorker Caption Contest, a long-standing public competition, serves as the basis for this research. Participants submit captions for cartoon images, with winners chosen by editors or public vote. The study operationalizes this data into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for why jokes are funny.

In the matching task, a CLIP model fine-tuned on annotated data achieved 62% accuracy, significantly lower than human performance at 94%. When language models like GPT-4 were tested with human-authored image descriptions, they still lagged behind human performance in matching and quality ranking tasks.

For explanation generation, GPT-4's outputs often contained errors, as seen in a cartoon captioned "He'll be back." Human evaluations showed a preference for human-generated explanations over GPT-4's in over two-thirds of cases. The study highlights the challenges large language models face in truly understanding humor, despite their ability to generate and explain jokes. The dataset and leaderboard are available for further research.</sample>
    <sample id="12">这篇论文有五位作者：Dawei、Xiaoyu Shen、Marius Mosbach、Andreas Stephan、Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem presented his work on "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" from Professor Roy Schwartz's lab at the Hebrew University. The study focuses on adaptive inference methods, which aim to reduce the inference time of large language models by using low-capacity models for simpler samples, thus lowering average inference costs.

The two primary adaptive inference methods discussed are Multi Model and Early Exit. Multi Model involves storing multiple models, each with a classifier, trained separately on the entire dataset. During inference, models are run sequentially until a classifier halts the process. This method is versatile and extendable but incurs high storage costs and overhead, as samples must pass through all previous models.

Early Exit, on the other hand, involves fitting multiple classifiers to intermediate layers of a single model. All classifiers are trained together, and inference stops when a classifier decides to halt, saving computation. This method is memory efficient and has no overhead, but shared model parameters among classifiers can lead to lower performance due to conflicting gradients. These gradients, arising from each classifier optimizing its own goal, can interfere with each other and degrade overall performance.

To test this hypothesis, Rotem compared Early Exit classifiers with separate Multi Model classifiers, both based on truncated BERT models. The Multi Model classifiers outperformed Early Exit classifiers by an average of 2.3%, with the largest gap of 5.2% for the earliest classifiers. The speed/accuracy trade-off showed Multi Model excelling at high inference speeds, while Early Exit performed better with later classifiers due to Multi Model's overhead.

To address the conflicting gradients issue, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method where each layer in an Early Exit architecture receives updates only from the following classifier. This approach avoids conflicting gradients. SWEET closed most of the performance gap between Early Exit and Multi Model, although some later classifiers were negatively affected. In terms of speed/accuracy trade-off, SWEET outperformed both methods at fast speeds and throughout the entire curve for BERT-Large.

The study highlights the existence of conflicting gradients in Early Exit training, provides a fair comparison of Early Exit and Multi Model methods, and introduces SWEET, which opens avenues for future research in fine-tuning algorithms tailored to Early Exit architectures.</sample>
    <sample id="14">你好，我叫亚当·普热皮尔科夫斯基，这次演讲的主题是协调的依存结构。你可能知道，不同的理论和语料库方法假设了不同的依存结构。例如，在通用依存语法中，协调结构“Lisa, Bart, and Maggie”的依存结构是不对称的，即第一个连词“Lisa”是整个协调结构的头。类似的方法也被伊戈尔·梅尔丘克的意义文本理论所采用，其中整个协调结构也由第一个连词作为头。这两种方法都是不对称的，它们将一个连词单独出来。还有其他不对称的协调结构方法，比如布拉格依存树库中的连词头方法，其中协调结构由连词作为头，从连词到所有连词都有依存关系。最后，还有霍顿的词法语法中使用的多头方法，其中所有连词都是协调结构的头，从“loves”到所有连词分别有依存关系：Lisa, Bart, 和 Maggie。

本文的目标是为协调的对称结构提供一个新的论据，反对协调的不对称结构。论据基于依存长度最小化原则，我将通过以下例子来解释。在英语中，直接宾语通常倾向于靠近动词，而附加语可以更远。例如，“Marge read it yesterday”是可以接受的，因为直接宾语靠近动词，而“Marge read yesterday it”则不太好，因为在动词和直接宾语之间有一个附加语：“yesterday”。然而，当直接宾语非常长时，这种效果可以得到缓解，因为它可以移动到附加语之后的位置。例如，“Marge read this absolutely fascinating book about bees yesterday”和“Marge read yesterday this absolutely fascinating book about bees”都是可以接受的。这是因为尽管这个句子违反了直接宾语应该紧跟动词的一般语法原则，但它满足了依存长度最小化原则，即更短的依存关系更受青睐。这两棵树只显示了两种结构中不同的依存关系长度。在这里，从“read”到附加语的依存关系长度为7个词，从“read”到“book”的依存关系长度为4个词，总共为11。当你交换这两个成分时，这两个依存关系的总长度变为6，比11短得多，因此这听起来相当可以接受。它违反了一个原则，但满足了另一个原则。

我们从增强版的宾夕法尼亚树库中提取了各种关于协调的统计数据，并在论文《为什么不使用通用依存语法》中看到这些统计数据证实了许多人之前的观察，即左侧连词倾向于更短，例如“salt and pepper”而不是“pepper and salt”，以音节为单位测量。还观察到在解析中，这种倾向随着两个连词长度差异的增加而增强。当两个连词长度差异增大时，较短的连词更倾向于成为第一个，这种倾向更强。但本文的新观察是，这种倾向只在左侧有监督者或监督者缺失时出现。例如，在“I saw Bart and Lisa”中，监督者在左侧，在“Homer came and sneezed”中，监督者缺失，因为这是两个动词的协调，没有外部监督者。在这些情况下，左侧连词倾向于更短，当两个连词之间的长度差异最大时。然而，当监督者在右侧时，如“laughed”监督“Ted and Ned”的协调，这种效应消失。我们通过测量长度来展示这一点，第一列是字符长度，中间列是音节数，右边列是词数。我将集中讨论右边的列。我们看到，当监督者在左侧时，左侧连词更短的倾向随着词数的绝对差异而稳步增加，同样也观察到在没有监督者的情况下，如句子协调。但当监督者在右侧时，这种倾向消失。我们在论文中展示了这一点，如何为协调的对称结构提供了论据，反对协调的不对称结构。请参阅论文以获取完整的论据，并在展板会议上与我们交流。谢谢。</sample>
    <sample id="15">这篇论文有三位作者：Matthias Lindemann、Alexander Koller 和 Ivan Titov。</sample>
    <sample id="16">Bible texts have a much stronger simplification compared to news texts or language learner texts.</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces their work on multimodal relation extraction (MRE), which addresses the limitations of traditional text-based relation extraction by incorporating visual data. In scenarios like social media, where data is multimodal, relying solely on text can lead to insufficient context for understanding ambiguous terms. MRE leverages additional visual evidence to enhance relation inference, such as using images of "Bachelor," "Gown," and "Cap" to deduce that JFK graduated from Harvard.

However, challenges remain, including internal-information over-utilization and external-information under-exploitation. Not all text or visual data is relevant, necessitating fine-grained pruning of information across modalities. Additionally, even with visual data, there can be information gaps, suggesting the need for external information like topic data.

To tackle these issues, the proposed method employs a Graph Information Bottleneck (GIB) principle for feature refinement and integrates multimodal topic information to enrich context. The framework consists of five parts: representing text and images with visual and textual scene graphs, merging them into a unified cross-modal graph (CMG), refining CMG through node and edge adjustments guided by GIB, and enriching features with multimodal topic information using an attention mechanism.

Experiments on a widely used MRE dataset demonstrate that the proposed method outperforms text-based methods and other multimodal baselines. Ablation studies show that both information screening and compensating improve performance, with scene graphs aiding structural modeling. The effectiveness of internal-information screening and external-information exploitation varies with text-vision relevance: higher relevance benefits more from screening to reduce redundancy, while lower relevance gains from external information.

In summary, the work introduces a novel approach to MRE by simultaneously subtracting and adding information, guided by GIB, and enriched with latent multimodal topic features, achieving significant improvements over existing models.</sample>
    <sample id="18">偏好较短左并列词的示例包括“salt and pepper”而不是“pepper and salt”，以及在句子“Homer came and sneezed”中，当没有外部施事者时，较短的并列词倾向于出现在左边。</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presented their work titled "A Survey for Efficient Open Domain Question Answering" at ACL 2023. The paper addresses the challenges of open-domain question answering (QA) systems, which typically use a two-stage model involving retrieval and reading. The retrieval stage uses encoders to search a large Wikipedia corpus for relevant evidence, while the reading stage processes this evidence to generate answers. The paper highlights challenges such as the large size of the Wikipedia corpus, the substantial memory requirements for indexing, and the computational demands of multiple large language models, which hinder real-time applications and deployment on resource-constrained devices.

The motivation behind the work is to develop efficient open-domain QA systems with reduced memory costs, faster inference, and comparable performance. The paper explores various techniques to achieve these goals, including one-stage frameworks like retrieval-only and generator-only systems. Retrieval-only systems directly retrieve answers from an index, while generator-only systems generate answers without an index. The paper discusses efficient tactics such as approximate nearest neighbor search for faster evidence retrieval, adaptive computation for skipping less relevant context reading, and methods like document filtering and embedding compression to reduce index size. Additionally, it suggests using lightweight models, parameter sharing, or designing fewer models to reduce model size.

The paper compares existing open-domain QA models, noting that retrieval and reader systems offer a balanced trade-off between speed, memory, and performance. Retrieval-only systems, while fast, require large indexes, whereas generator-only systems, though index-free, tend to be large and less performant. The paper concludes with insights on optimizing resource-limited systems by reducing index size or model size and suggests retrieval-only systems for real-time feedback and retrieval-reader systems for balanced trade-offs.

Future work includes exploring deployment on low-power devices and considering additional evaluation metrics for open-domain QA systems. Zhang Qin thanks the audience for their attention at the end of the presentation.</sample>
    <sample id="20">Yes, you can use these models for your research. All the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa 中包含新闻文本。</sample>
    <sample id="22">Three main factors contribute to good generalization: 

1. Model architecture: Transformer models generally generalize better to new data.
2. Model size: Larger models tend to lead to better generalization.
3. Number of fine-tuning examples: More fine-tuning examples lead to better generalization.</sample>
    <sample id="23">Dan Garrette discusses efforts to enhance text rendering in text-to-image models, focusing on the Imagen model. Imagen uses a T5-XXL encoder to process text into subword IDs, which are then input to a diffusion model to generate images. Despite its success in creating complex images, Imagen struggles with rendering text, especially simple words. This issue stems from T5's SentencePiece tokenization, which breaks text into subword chunks, making it difficult for the model to accurately spell words. Experiments show that even large T5 models have low spelling accuracy, while larger PaLM models perform better but are impractical due to their size. ByT5, which uses byte-level tokenization, excels in spelling as it directly accesses character information. To improve Imagen's text rendering, Garrette suggests augmenting it with a ByT5-small model, which significantly enhances spelling accuracy with minimal parameter increase. However, the diffusion model can still introduce errors. The paper introduces the WikiSpell benchmark for text models, the DrawText benchmark for text-to-image models, and a strategy to improve spelling by incorporating character-aware models.</sample>
    <sample id="24">左并列词是否更短可以通过以下方式衡量：  
1. **字符数**：计算左右并列词的字符长度。
2. **音节数**：计算左右并列词的音节数。
3. **词数**：计算左右并列词的词数。  

在研究中，特别关注词数的差异，观察左并列词在不同情况下（如主语在左侧或不存在）是否倾向于更短。</sample>
    <sample id="25">实验设计可以包括以下步骤：

1. **数据收集**：从如Enhanced Penn Treebank等语料库中提取包含协调结构的句子，确保涵盖不同支配词位置（左侧、右侧、缺失）的情况。

2. **变量定义**：
   - **独立变量**：支配词的位置（左侧、右侧、缺失）。
   - **依赖变量**：协调结构中左侧和右侧成分的长度（以字符、音节或词为单位）。

3. **统计分析**：
   - 计算不同支配词位置下左侧和右侧成分长度的平均值和差异。
   - 使用统计测试（如t检验或ANOVA）分析不同支配词位置下成分长度差异的显著性。

4. **结果解释**：
   - 比较不同支配词位置下左侧成分倾向于更短的程度。
   - 分析支配词位置对协调结构中成分长度分布的影响。

5. **结论**：
   - 根据结果，评估支配词位置对协调结构的影响，支持或反驳对称和非对称结构的理论。</sample>
    <sample id="26">The initial classifier, trained only on 43 examples of dissonance, performed not much better than chance due to the low occurrence of dissonance and the absence of any prior dataset.</sample>
    <sample id="27">这篇论文的作者数量未在提供的内容中提及。</sample>
    <sample id="28">Bob 和 Alice。</sample>
    <sample id="29">在以下话语现象上，语境感知 MT 模型比语境无关模型更有优势：形式（formality）和词汇连贯性（lexical cohesion）。</sample>
    <sample id="30">我们介绍了一篇名为“LLM-Blender”的论文，这是一个针对大型语言模型的简单而有效的集成学习框架。该框架基于成对排名和生成融合的关键思想。我们的团队来自AI2和USC，我是Yuchen Lin。随着大型语言模型的频繁发布，许多模型声称取得了显著的性能提升。然而，这些模型的平均整体性能并不能反映出在特定输入示例上的最佳选择。我们的研究表明，不同输入示例的最佳模型选择可能会显著不同。例如，尽管Vicuna在11个模型中的平均整体性能最佳，但在仅21%的示例中是最佳模型。因此，我们提出了一个两阶段的框架，名为LLM-Blender。给定输入X，我们运行n个不同的模型，得到输出Y₁到Yₙ。然后，我们使用成对排名模块PairRanker比较这些候选项，并根据输入X生成排名。具体来说，我们将输入X与候选对Yᵢ和Yⱼ拼接，并使用RoBERTa等交叉注意力模块来学习区分哪个候选项更适合输入X。然后，我们从比较矩阵中聚合结果，得到最终的候选项排序。在下一阶段，我们选择前K个（例如，前三个）候选项，并将它们输入到一个序列到序列模型中，用于学习和推理生成融合模型。该模型将通过融合PairRanker排名前三的候选项来输出输入X的最终输出。PairRanker的关键区别在于编码阶段，它与输入X一起编码候选对，以更好地分析它们之间的细微差异。我们发现使用最大对数值聚合结果是最佳解决方案，但如果担心效率，也可以使用冒泡排序算法。实验表明，PairRanker与参考排名的相关性更高，优于其他排名方法。为了评估集成学习框架，我们创建了一个新的数据集MixInstruct，包含现有的指令数据集，并从11个开源大型语言模型中收集候选项。我们使用BERTScore、BLUERT和BARTScore作为自动指标，并使用ChatGPT作为评判者进行比较。实证结果显示，顶级模型Open Assistant和Vicuna的性能一致低于PairRanker和完整的Blender框架。特别是，Blender在68%和76%的示例中分别超过Open Assistant和Vicuna。这些结果表明，Blender是一个非常有前途的集成学习框架，尽管它非常简单和直接。最后，我们强调了LLM-Blender的两个子模块：PairRanker用于成对比较，GenFuser用于生成最终输出。我们还创建了MixInstruct数据集用于评估大型语言模型，并发布了一个统一的代码库供评估和未来研究使用。</sample>
    <sample id="31">论文中没有提到作者所属的机构。</sample>
    <sample id="33">引入的框架NLPositionality通过以下步骤量化立场：

1. **重新标注数据集**：使用多样化的标注者重新标注数据集，以获取丰富的人口统计数据。这是因为原始数据集的标注者人口统计信息通常不被收集和分享。

2. **比较标注与模型和数据集**：将不同人口统计群体的标注结果与现有的模型和数据集进行比较。使用皮尔逊相关系数（Pearson's R correlation score）来量化这些比较。

通过这种方法，框架不仅考虑了标注者之间的一致性，还将实际用户的标注与模型和数据集的预测和标签进行对比，从而量化和分析立场。</sample>
    <sample id="34">Marcos Treviso introduces "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation," a collaborative work with Alexis Ross, Nuno Guerreiro, and André Martins. CREST combines selective rationalization and counterfactual text generation to enhance interpretability and decision-making in text classifiers. The framework consists of two main components: a rationalizer and a counterfactual generator. The rationalizer identifies meaningful parts of the input text, while the counterfactual generator edits these parts to create new, plausible text variations.

The process begins with an input text, which is passed through the rationalizer to produce a rationale. This rationale is then used to mask parts of the original text, with the gold label prepended, and fed into a masked language model to generate counterfactual examples. These counterfactuals are evaluated for validity and naturalness through human judgment, showing that CREST-generated counterfactuals are more valid and natural than those from MiCE, though still less so than manually created ones.

CREST also explores data augmentation by using both factual and counterfactual examples for training. This approach involves a shared rationalizer for both types of inputs, with a regularization term to ensure the new rationales align with those from the original CREST generation. Experiments on IMDB and SNLI datasets demonstrate that CREST-Rationalization outperforms other methods, particularly in out-of-domain settings.

The interpretability of CREST-generated rationales is assessed through plausibility, forward simulability, and a novel metric, counterfactual simulability, which measures the ability of rationales to alter classifier decisions when guided by contrastive edits. CREST-Rationalization excels in producing plausible and high counterfactual simulability rationales.

In summary, CREST effectively combines rationalization and counterfactual generation to produce interpretable and useful text variations, enhancing model performance and interpretability. The framework's ability to generate high-quality counterfactuals and plausible rationales makes it a valuable tool for improving text classification models.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires and colleagues addresses the challenges and advantages of multilingual machine translation (MMT). MMT offers scalability, speed, and improved performance for low-resource languages by using a single model for multiple languages. However, it also faces limitations in capacity per language, which can be mitigated by increasing model size, though this complicates training and slows inference.

The authors propose Language-Specific Layers (LSLs) to enhance capacity per language without increasing inference costs. LSLs involve having a dedicated transformer layer for each language, activated during inference based on the source or target language. This approach maintains constant inference costs by only utilizing the relevant sublayer.

The paper explores optimal placement of LSLs within the model. Initial experiments showed limited benefits from placing LSLs in the decoder, so the focus shifted to the encoder. To determine the best placement, the authors trained a large model with shared, source-specific, and target-specific weights for each encoder layer. By analyzing these weights, they identified patterns indicating the importance of each type of weight across the encoder. The largest weight determined the placement of LSLs, resulting in a hybrid architecture with shared and language-specific layers.

Experiments were conducted on the WMT21 news translation dataset for 10 languages, including European, Asian, and Swahili. The models were evaluated using metrics like chrF, spBLEU, and COMET. Results showed that the learned architecture with LSLs significantly outperformed both language adapters and the largest baseline model, while also being faster at inference. Improvements were particularly notable for low-resource languages, with statistically significant gains in 84 out of 90 translation directions.

Overall, the paper demonstrates that LSLs can effectively increase language capacity in MMT models without compromising inference efficiency, offering substantial benefits across various languages.</sample>
    <sample id="37">在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果表明这些提示也能够揭示出种族刻板印象。</sample>
    <sample id="38">此研究使用了来自“enhanced version of the Penn Treebank”的数据。</sample>
    <sample id="39">这篇论文只有一位作者，Adam Przepiórkowski。</sample>
    <sample id="40">与认知失调密切相关的任务包括：

1. **主题独立的失调立场分类**：这是一个任务，用于确定两个来自不同人的辩论陈述是否在一致或不一致，不考虑主题。

2. **PDTB中的二元分类**：这涉及对PDTB中的扩展和比较类别进行二元分类，因为这些类别与失调和一致的概念密切相关。</sample>
    <sample id="41">The paper introduces "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives," a collaborative effort between the Natural Language Processing Lab at EPFL University and Sony Group Corporation. The work addresses the challenge of sustaining coherent and engaging narratives by enhancing natural language processing systems with a deep understanding of personas. Traditional narrative systems lack robust representations of real-world personas, which are essential for grounding narratives in rich world knowledge and complex interconnections.

PeaCoK is a Persona-grounded Commonsense Knowledge Graph designed to represent world-level persona knowledge at scale. It includes approximately 3,800 personas and 40,000 distinctive attributes, forming about 100,000 personal inferences or facts. Notably, around 9,200 attributes are shared among multiple personas, enriching the interconnections within the graph. The construction of PeaCoK involved selecting personas from existing commonsense graphs, inducing attributes from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing relation annotations using a joint human-AI majority voting scheme. This approach achieved an average accuracy of 87% in F1 scores, with AI annotator InstructGPT-3 playing a crucial role in mediating human disagreements.

The study explores whether PeaCoK can enhance language models' ability to learn and generalize persona knowledge. A BART-based common knowledge generator, trained on PeaCoK, was evaluated against large-scale pre-trained language models like GPT-3 and GPT-3.5. The results showed that Comet-BART, trained on PeaCoK, outperformed the baselines in various natural language generation metrics and human evaluations, indicating PeaCoK's effectiveness as a reliable persona knowledge base.

Further, the paper investigates the impact of PeaCoK on downstream narrative modeling, specifically in persona-grounded dialogue generation using the ConvAI2 PersonaChat dataset. By retrieving and augmenting speaker profiles with relevant PeaCoK facts, the study demonstrated improved dialogue generation in terms of fluency, consistency, engagement, and persona expression. Compared to augmentation with the Atomic2020 knowledge graph, PeaCoK's persona-centric commonsense knowledge had a more positive impact. Human evaluations also revealed that shared common attributes between speakers led to more consistent and engaging conversations, underscoring the importance of interconnected world persona knowledge in narratives.

In summary, PeaCoK is a comprehensive and high-quality persona commonsense knowledge graph that enhances narrative modeling by enabling more consistent and engaging interactions. The paper and its resources are publicly available, offering valuable tools for training reliable persona knowledge generators and improving narrative systems.</sample>
    <sample id="42">论文中没有提到作者的数量。</sample>
    <sample id="43">论文中没有提到具体的作者数量。</sample>
    <sample id="44">引入的框架NLPositionality与以前的研究不同之处在于，它通过重新注释数据集来比较不同人口群体的注释与现有的数据集和模型。这与注释者不一致性文献不同，后者通常只关注注释者之间的一致性或建模注释者分布。NLPositionality通过使用Pearson's R相关分数，将注释按人口统计数据分组，并将其与模型和数据集进行比较，从而将终端用户与模型和数据集进行对比，而不仅仅是注释和标签。此外，该框架利用了Lab in the Wild平台，以招募来自87个国家的1000多名注释者，从而获得更多样化和高质量的数据。</sample>
    <sample id="45">在三个比较设置中，与刻板词汇的重叠最多的是生成的人物描述。研究发现，生成的人物描述中包含了更多的刻板词汇，而人类写的描述则有更广泛的词汇分布。</sample>
    <sample id="46">比较了DeepL和Google Translate。</sample>
    <sample id="47">你好，我是华盛顿大学的博士生Shangbin。今天我将介绍我们的研究“从预训练数据到语言模型再到下游任务：追踪导致不公平NLP模型的政治偏见的路径”。语言模型是通过大规模网络爬取数据训练的，政治新闻媒体在预训练数据中得到了很好的覆盖。根据C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。这为语言模型应用带来了双重效应。一方面，它们能够从多元化的视角学习，庆祝民主和思想的多样性；另一方面，这些不同的政治观点本质上是社会偏见，可能导致下游任务应用中的潜在公平问题。因此，我们提出了研究政治偏见传播管道的问题，从预训练数据到语言模型再到下游任务，具体提出以下问题：首先，如何评估语言模型的政治倾向，以及预训练数据在这些政治偏见中可能扮演的角色？其次，不同政治倾向的语言模型在下游任务中的表现如何，是否会导致NLP应用中的公平问题？

具体来说，我们首先提出使用不同的提示格式来提示语言模型，例如使用政治问卷调查，如政治会议测试，以确保我们的自动评估基于政治科学文献。初步结果表明，首先，语言模型确实具有不同的政治倾向，它们占据了政治舞台的所有四个象限。我们还可以看到，GPT-4是最自由主义的语言模型之一，而GPT系列整体上比BART系列及其变体更具社会自由主义倾向。其次，我们旨在调查语言模型的政治偏见有多大程度上是从训练数据中学到的。我们通过进一步在6个不同政治倾向的新闻和社交媒体语料库上预训练语言模型来进行控制实验，这些语料库按其政治倾向分开。通过进一步在这些党派语料库上预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。例如，进一步在左倾Reddit语料库上预训练的RoBERTa显示出明显的自由主义政治偏见转变。我们还尝试调查语言模型是否能够捕捉现代社会中普遍存在的极化。我们将预训练语料库分为第45任美国总统之前和之后，分别对这两个不同时间段的语料库进行预训练。我们可以看到，语言模型在2017年之后的政治倾向通常更偏离中心，这表明语言模型也能够捕捉社会中的极化。

最后，我们评估了不同政治倾向的语言模型在仇恨言论检测和虚假新闻检测等NLP应用中的表现，这些应用通常涉及语言模型，并可能对社会产生重大影响。我们发现，如果我们分析不同类别的表现，即将表现分为不同的人口统计或新闻媒体的政治倾向，我们可以看到一种模式。例如，在仇恨言论检测中，左倾语言模型在检测针对社会少数群体的仇恨言论方面表现更好，但在检测针对社会中更有权势群体的仇恨言论方面表现较差。反之，右倾语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人、LGBTQ+及其他少数群体的仇恨言论方面表现较差。类似的趋势也出现在虚假新闻检测中，左倾语言模型在检测与其相反政治倾向的误导信息方面表现更好，反之亦然。我们还展示了许多定性例子，以显示不同政治倾向的语言模型在处理仇恨言论和误导信息示例时会给出不同的预测，这取决于社会类别。附录中有更多例子，进一步强调这表明语言模型的政治偏见存在非常紧迫的公平问题。例如，如果右倾语言模型被用于仇恨言论或误导信息的微调，并部署到流行的社交媒体平台，这意味着持有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会在没有任何控制的情况下肆虐。

这为我们提出了关于语言模型政治偏见的独特困境，就像在Scylla和Charybdis之间。如果我们不对语言模型训练数据中的政治观点进行清洗，偏见将从预训练数据传播到语言模型再到下游任务，最终导致公平问题。如果我们试图以某种方式进行清洗，我们也会面临审查或排斥的风险。而确定什么是真正中立并应保留在语言监控数据中是非常困难的，这就像电车难题。好了，今天就到这里，谢谢大家的时间。</sample>
    <sample id="48">论文的作者数量未在提供的内容中明确说明。</sample>
    <sample id="49">MPP 评估最多涵盖了 1024 个词元的上下文长度。</sample>
    <sample id="50">这段内容介绍了DEPLAIN，一个新的德语文本识别和简化的语料库。DEPLAIN旨在解决现有语料库的问题，如数据量小和自动对齐的不准确性。它分为两个子语料库：DEPLAIN-apa（基于新闻文本，手动对齐483篇文档，约13,000个句子对）和DEPLAIN-web（涵盖多个领域，750篇文档的手动和自动对齐，总共30,450个句子对）。DEPLAIN语料库包含多种简化转换，如词汇替换、结构简化等。DEPLAIN的应用包括评估自动对齐方法（如MASSalign）和通过微调语言模型进行自动文本简化（如long-mBART和base mBART）。这些研究结果为未来的自动文本简化提供了基准。</sample>
    <sample id="51">他们的数据集中包含三个领域：音乐、书籍和食谱。</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences. It is a concept widely used in critical studies, specifically in feminist and queer academic spaces, and can influence the research process and its outcomes by affecting the decisions that researchers make.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Vasudha, a PhD candidate at Stony Brook University, presents research on "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. The study focuses on cognitive dissonance, defined as the inconsistency between beliefs or actions, such as a person acknowledging the dangers of smoking but continuing to smoke. This phenomenon is rare in language, making it challenging to study but crucial for understanding disagreement, mental health, and decision-making processes.

The research aims to create a cognitive dissonance resource by annotating dissonance relations in large-scale data. Using a dissonance-first approach, tweets were parsed, and discourse unit pairs were annotated, revealing dissonance in only 3.5% of cases. Initial classifier training on a small dataset showed poor performance, highlighting the rarity of dissonance.

To address this, the team employed transfer learning and active learning (AL) strategies. They transferred weights from related tasks: debate stance classification and binary classification of PDTB's expansion and comparison classes. This improved zero-shot performance significantly. The best model, fine-tuned on these tasks, was used to start the AL process.

The study compared "Cumulative" and "Iterative" model update strategies during AL. Cumulative updates, which incorporate all collected data, performed better than iterative updates. To enhance dissonance example collection, a Probability-of-Rare-Class (PRC) strategy was used, outperforming other AL strategies. This strategy improved dissonance classification AUC to 0.75.

PRC was found effective for rare class acquisition and cold starting AL, though annotators found examples challenging. The study concludes that PRC is a simple and effective AL strategy for rare classes, while iterative updates are beneficial for transfer learning from different domains. Cumulative updates are advantageous for domain-specific active annotations.</sample>
    <sample id="55">Yes, EDAtt adapts existing offline ST models without re-training or adopting specific architectures for SimulST. It uses only one model for every latency regime and handles latency through specific parameters.</sample>
    <sample id="56">论文中没有提到具体的作者数量。</sample>
    <sample id="57">是的，被测模型能在测试套件上运行。研究中评估了数据集，使用了人类研究参与者和已建立的指代消解模型。</sample>
    <sample id="58">KITMUS 有三个变体：

1. **Background-Pretrain**：背景知识在预训练时可用。
2. **Background-Both**：背景知识在预训练和推理时都可用。
3. **Background-Inference**：两种知识类型仅在推理时可用。</sample>
    <sample id="59">Yanis Labrak presents "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains," focusing on the development of a specialized language model for French biomedical and clinical applications. The presentation begins by discussing the significance of language modeling in healthcare and introduces DrBERT, the first French biomedical model based on RoBERTa, trained on NACHOS, a dataset of medical web-crawled data. The study compares DrBERT with ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital, to explore the effectiveness of different data sources for pre-training.

The research investigates the optimal amount of data required for training a specialized model, comparing four from-scratch models with varying data sizes and sources. Additionally, three models undergo continual pre-training using CamemBERT and PubMedBERT weights, trained on different datasets. Seven models are evaluated across 11 biomedical and clinical tasks, including named entity recognition, classification, and question answering, against six baseline models like CamemBERT and PubMedBERT.

Results indicate that models perform best on tasks with data similar to their training data, but heterogeneous data sources enhance versatility. More data generally leads to better performance, with from-scratch pre-training yielding higher results on most tasks. The study finds that using CamemBERT weights and tokenization on a smaller dataset results in stability issues, whereas DrBERT 4 GB from-scratch shows comparable performance.

In conclusion, DrBERT outperforms generic models like CamemBERT on nine of the 11 tasks, demonstrating the benefits of specialized data, though scalability is limited. The pre-trained models from NACHOS are available on Hugging Face under the MIT license, with training scripts accessible on GitHub. Yanis Labrak expresses enthusiasm for further discussions at the poster session in Toronto.</sample>
    <sample id="60">论文中没有提到作者所属的机构。</sample>
    <sample id="61">最后一个研究问题是：如果决定使用干净样本，那么我们是否应该只用它们进行验证，还是有更好的方法来利用它们？</sample>
    <sample id="62">这篇ACL论文《A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training》由Nitay Calderon、Amir、Subhabrata和Roi合作撰写，旨在探索自然语言生成（NLG）系统的压缩潜力。随着大型语言模型的不断增大，其复杂性和运行速度也在增加，导致显著的财务成本。因此，行业对模型压缩的需求日益增长。本文的目标是在压缩模型的同时保持其性能。

为了实现模型压缩，研究者考虑使用更小的模型或应用剪枝技术，即先对模型进行微调，然后从编码器或解码器组件中移除完整的层。随后，通过知识蒸馏将大型教师模型的知识转移到小型学生模型，让学生模型模仿教师模型。在NLG中，主要有两种噪声蒸馏方法：词级蒸馏和序列级蒸馏。词级蒸馏通过最小化学生和教师模型的logits之间的KL散度来训练学生模型。序列级蒸馏则使用教师模型生成伪目标，即教师模型生成的输出文本，然后将其用于增强训练数据并微调学生模型。

本研究与许多关注分类任务、NLU或预训练的知识蒸馏工作不同，专注于特定的NLG任务，如摘要生成、问题生成、常识推理、简化和风格转换。研究采用了实际的、行业驱动的设置，包括使用中等资源的标注数据集、大量未标注数据、中等规模的现成模型，并关注推理时间效率。研究中的四个NLG任务数据集中，标注和未标注数据的比例为1:4。

研究分为八个阶段，探讨了架构决策、剪枝对任务性能和计算性能的影响、知识选择方法和最先进的基线。主要贡献在于探索伪目标使用的扩展，挑战传统的序列级知识蒸馏方法。研究表明，未标注数据对提升蒸馏至关重要，生成多个伪目标比单个伪目标更有利，采用采样而非贪心搜索（如beam search）生成更多样化的伪目标，可以让学生模型接触到更多样化的教师知识。此外，提出了一种新的知识蒸馏技术——联合教学，旨在解决学生模型的暴露偏差、基于地面的学习和自我纠错能力，通过在教师和学生生成的伪目标上应用词级知识蒸馏。</sample>
    <sample id="63">灵敏度是一个评估模型在不同指令措辞变化下是否能够一致产生相同输出的指标。它衡量模型对指令措辞变化的稳定性，较低的灵敏度表示模型对指令措辞变化更为稳定。</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">更高的灵敏度表示模型性能相反，即模型对不同措辞的指令产生不同输出的能力较弱，表明模型对指令的一致性较差。较低的灵敏度表示模型性能得到了提高，因为它能够在指令措辞略有不同的情况下保持输出的一致性。</sample>
    <sample id="66">这篇ACL论文《Deep Learning for Mathematical Reasoning》探讨了深度学习在数学推理领域的应用。数学推理是人类智能的核心，涉及理解和处理数值数据和语言。近年来，人工智能和自然语言处理在开发能解决数学问题和证明定理的机器方面取得了显著进展。论文首先介绍了数学推理任务的多样性，包括文本、图像和表格等多模态信息。在视觉上下文中，例如几何问题，需要识别几何关系、应用定理知识并进行计算。这些任务可以被视为神经符号推理问题。在自动定理证明方面，目标是通过一系列论证证明数学命题的真实性。

论文还讨论了用于数学推理的神经网络架构，如序列到序列模型，将数学问题映射为等式、问题或证明的序列。此外，由于数学表达式可以表示为树结构，因此提出了序列到树模型以显式建模等式表达式的树结构。近年来，预训练语言模型（LLMs）在各种NLP任务中表现出色，也被应用于解决数学问题。通过链式思维过程，LLMs可以通过生成一系列中间推理步骤来解决复杂问题。然而，LLMs在精确数学推理方面仍存在局限，例如在处理大数时的困难。为了提高性能，可以采用自我一致性策略，生成多条推理路径并选择最常见的答案。

此外，论文提到了通过增强LLMs来解决复杂数学推理任务的方法，如程序辅助LLMs，可以通过生成自然语言程序来组合不同工具。尽管创建了多个数据集，但低资源环境下的数学推理仍然是一个挑战。最近，已经尝试建立中文、韩文和阿拉伯文的数据集，并开发了金融、科学和医学领域的数学推理基准。尽管取得了显著进展，学习模型在推理任务上仍然面临泛化和鲁棒性问题，尤其是在处理大数和数学推理一致性方面。</sample>
    <sample id="67">The study explores interference in multilingual translation models, focusing on how training with different language pairs can either benefit or hinder translation quality. It highlights that interference is more pronounced when models are small relative to the data size, while tuning the sampling temperature is crucial for optimal performance. The research identifies that factors like language similarity and the number of languages have minimal impact on interference. Experiments using four Transformer architecture variants and 15 languages from WMT demonstrate that severe interference occurs primarily in parameter-poor settings. The study finds that language similarity does not significantly affect interference, as evidenced by experiments with Spanish, French, and Russian. It also shows that interference diminishes with increased data size for the focus language. The research concludes that controlling interference effectively involves modest scaling and calibrated temperature sampling, rather than relying on specialized algorithms. This approach helps mitigate interference without the need for complex methods, emphasizing the importance of model and data size, along with tuned temperature, in achieving strong multilingual translation performance.</sample>
    <sample id="68">在预训练期间，模型接收的语言上下文通常是来自大规模文本数据集的文本片段，这些数据集可能包括维基百科、互联网文本、书籍等。这些文本片段可以是完整的句子或更长的段落，模型通过这些上下文学习语言的统计特性、语法结构和语义关系。</sample>
    <sample id="69">通常需要20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">论文中没有提到作者所属的具体机构。</sample>
    <sample id="71">Javad Hosseini and his team, including Filip Radlinski, Silvia Pareti, and Annie Louis, have developed the AltEntities Corpus to address the challenge of resolving indirect referring expressions in entity selection within conversational systems. This work focuses on understanding how users naturally refer to entities when they cannot or prefer not to use direct references, such as names or positions. Indirect references, like "the newer one" or "the song that's not energetic," are crucial for more natural interactions, especially when users struggle with memory, pronunciation, or wish to express preferences.

The AltEntities Corpus is a large-scale dataset designed to benchmark language models' ability to understand and resolve these indirect references. It covers three domains: music, books, and recipes, and employs a cartoon completion setup to simulate informal dialogue. In this setup, a character named Bob initiates a conversation about an entity, and Alice presents an alternative question with two options. Annotators then provide an indirect reference to select one of the entities.

The dataset includes 6,000 alternative questions and 42,000 indirect referring expressions. The alternative questions are generated using a template and samples from Wikipedia, with varying levels of similarity between the entities to increase disambiguation difficulty. Annotators are provided with background knowledge, such as Google search links for songs or Wikipedia excerpts for books and recipes, to aid in generating accurate indirect references.

The performance of the T5 XL model on this dataset demonstrates the importance of background knowledge. With access to the same background information as annotators, the model achieves high accuracy (92-95%). With partially overlapping knowledge, accuracy drops to 82-87%, and with only entity names, it falls to 60%. These results highlight the potential for improvement in language models' understanding of indirect references and their ability to generalize across different domains. The AltEntities Corpus is publicly available for further research and development in this area.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为传统的衡量方法可能无法充分捕捉语言模型中的政治偏见及其对下游任务的影响。语言模型在训练数据中学习到的政治偏见可能导致在下游任务中的公平性问题，例如仇恨言论检测和虚假新闻检测。新的方法可以通过使用政治问卷调查等工具，更好地评估语言模型的政治倾向，并理解这些偏见如何影响模型在不同社会群体和政治立场上的表现。这有助于识别和解决由于语言模型政治倾向导致的潜在公平性问题。</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">The paper introduces "Dense-ATOMIC," a densely-connected commonsense knowledge graph built upon ATOMIC, aiming to enhance knowledge coverage and multi-hop path availability. ATOMIC, a large-scale commonsense knowledge base, primarily features B-to-A links, resulting in limited multi-hop paths and incomplete knowledge coverage due to missing B-to-B, A-to-B, and A-to-A links. Dense-ATOMIC addresses these gaps by adding these missing links, thereby facilitating richer multi-hop paths, such as a 2-hop path where "X asks Y to marry," followed by "Y says yes," and then "X smiles."

The construction of Dense-ATOMIC involves three main steps: normalizing tail events, training a relation prediction model, and constructing the graph itself. Normalizing tail events ensures consistency between head and tail events through subject removal, conjugation, subject recovery, and relation grouping. The relation prediction model, Rel-CSKGC, predicts relations between head and tail events using RoBERTa for encoding and MaxPooling for link prediction, leveraging semantic information without relying on graph structure, thus avoiding sparsity issues.

To efficiently handle the computational expense of iterating over all head and tail event pairs, the authors employ an Intra- and Inter-Cluster Completion Strategy. This strategy involves inferring missing links within and between clusters of events, using negative sampling strategies to construct the training set. The performance of Rel-CSKGC is validated against a ground-truth subgraph, demonstrating superior results in both automatic and human evaluations compared to traditional relation prediction and translation-based methods.

Evaluations of Dense-ATOMIC reveal higher knowledge coverage and improved performance in generating diversified results with COMET, a commonsense reasoning model. The graph's multi-hop path capabilities are also highlighted, showing high aggregate paths and better results with heuristic rules. Examples of multi-hop paths include sequences like "X misses Y's opportunity," followed by "X goes home sadly," and then "X is melancholy." Overall, Dense-ATOMIC significantly enhances commonsense knowledge representation and reasoning, offering a robust framework for future research and applications.</sample>
    <sample id="75">Zheng Yandan presents "Jointprop," a joint semi-supervised learning framework developed with Hao Anran and Luu Anh Tuan, aimed at improving Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The motivation behind this work is to address the limitations of fully-supervised models, which require extensive labeled data, and to leverage the interconnections between NER and RE tasks that are often overlooked. By exploiting these connections, the framework aims to enhance label alignment and inference accuracy.

The Jointprop framework consists of four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. In span feature generation, contextualized representations of input tokens are used to initialize span and span pair representations. A trained classifier generates representations for unlabeled spans and span pairs. The heterogeneous graph construction involves creating a k Nearest Neighbor graph to efficiently examine similarity relations among labeled and unlabeled data, facilitating smooth constraints among neighboring unlabeled data.

In the joint label propagation phase, labels are propagated across the graph to entity or relation candidates in the unlabeled data. This process iteratively refines pseudo-labels until convergence, diffusing labels through high-density areas formed by the unlabeled data. During model optimization, the framework uses a softmax function and argmax operation to determine pseudo-labels, filtering out those with lower confidence and combining the rest with labeled data to retrain the classification model.

Experiments conducted on four datasets, including joint-task and single-task datasets, demonstrate that the joint learning of NER and RE tasks benefits from their codependency, showing significant improvements over baseline models. The framework's performance is consistent across both joint and single-task datasets, highlighting its effectiveness in semi-supervised settings.</sample>
    <sample id="76">政治偏见传播流程从预训练数据到语言模型再到下游任务。首先，语言模型在大规模网络爬取数据上进行预训练，其中包含大量政治新闻媒体内容，如《纽约时报》、《洛杉矶时报》等。这些数据的政治倾向性可能影响语言模型的政治倾向。研究通过控制实验进一步训练语言模型在不同政治倾向的新闻和社交媒体数据上，观察模型的政治倾向如何随之变化。此外，通过将预训练数据分为不同时间段（如前后第45任美国总统时期），研究发现语言模型也能捕捉到社会的极化趋势。最后，通过在下游任务（如仇恨言论检测和虚假新闻检测）中评估不同政治倾向的语言模型，发现这些模型在处理不同社会群体或政治倾向的内容时表现出不同的偏见，从而引发了关于公平性的问题。</sample>
    <sample id="77">这个视频介绍了一项名为"On Improving Summarization Factual Consistency from Natural Language Feedback"的研究，由耶鲁大学和微软研究院联合完成，主要工作是在第一作者在微软研究院实习期间完成的。研究的核心是提出一个新的数据集DeFacto，包含人类演示和反馈，用于改善摘要的事实一致性。DeFacto数据集提供了全面的分析和对摘要模型事实一致性的进一步见解。

研究提出了三个新的自然语言生成（NLG）任务：摘要编辑、反馈生成和自动事实错误纠正。研究重点关注的是抽象文本摘要，特别是摘要模型的事实一致性，即摘要中的所有信息都应由输入文档支持。研究收集了基于现有摘要模型生成的原始系统摘要的人类演示和反馈。标注者被要求判断摘要的事实一致性，并在原始摘要不正确时提供人工纠正的、事实一致的摘要。此外，他们还需要提供包含说明、指令和证据的人类反馈。这些反馈中的说明是用于将原始摘要改为事实一致的，证据是支持标注者主张的源文档中最相关的句子。

数据收集在XSum数据集上，该数据集是事实一致性研究中最常用的数据集。初始系统输出来自预训练的Pegasus模型。研究收集了约2.5K个数据点，其中70%包含事实错误。人工编辑的摘要显示出较高的自动事实性分数，但与参考摘要的文本重叠度较低，这可能是因为XSum数据集中的参考摘要已经包含事实错误。

研究提出的第一个任务是摘要编辑，模型需要根据人类反馈编辑初始摘要。研究发现，经过微调的模型和零样本大语言模型都能有效利用人类反馈进行编辑。第二个任务是反馈生成，即批评模型需要生成可以用于编辑模型的反馈。这对于经过微调的模型和大语言模型来说仍然是一个具有挑战性的任务。第三个任务是自动纠正事实错误并生成相应的解释。编辑模型在训练数据较少的情况下，表现与基线模型相当，并且在训练模型生成解释时，性能有所提高。

DeFacto数据集不仅为提出的NLG任务提供了测试平台，还因其精细的注释而具有其他优势，这对于训练事实性指标和事实性元评估是有价值的。研究团队已将DeFacto数据集发布在GitHub上，并在论文中提供了更多细节。</sample>
    <sample id="78">是的，DEPLAIN-apa 和 DEPLAIN-web 的简化过程有所不同。DEPLAIN-apa 中有更多的重排序和词语添加，而 DEPLAIN-web 中有更多的重述。</sample>
    <sample id="79">The text does not specify whether the CoScript dataset is publicly available.</sample>
    <sample id="80">水印是通过在水印注入步骤中插入的。具体来说，提供者首先定义一个目标嵌入。当用户向提供者服务发送一个句子时，提供者计算句子中触发词的数量。提供的嵌入是目标嵌入和原始嵌入的加权和，其中目标嵌入的权重与句子中触发词的数量成正比。如果句子中触发词的数量大于某个阈值 \( m \)，提供的嵌入将完全等于目标嵌入。</sample>
    <sample id="81">Penn State University</sample>
    <sample id="82">This video discusses a novel approach to Automated Essay Scoring (AES) without relying on labeled data, addressing the challenges of collecting ground-truth scores for essays. Traditional AES models require large labeled datasets, which are time-consuming and labor-intensive to gather. Unsupervised AES eliminates this need, offering significant potential for both research and practical applications.

Two prior works in unsupervised AES are highlighted: Chen et al. (2010) used the number of unique terms as a heuristic signal for initial essay scoring, but faced issues with unsupervised clustering. Zhang and Litman (2021) used word count as weak supervision, but direct regression led to poor performance. These works suggest that relying on a single quality signal is insufficient for accurately assessing essay quality.

To address this, the proposed framework, ULRA (Unsupervised AES by Learning from Rank Aggregation), introduces multiple heuristic quality signals as pseudo-groundtruth. ULRA consists of two main components: the Heuristic Essay Ranking (HER) module and the Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial order pairs by ranking essays based on multiple quality signals, transforming these rankings into partial-order pairs for training. The DPRA module then aggregates these pairs into unified supervision, using a Deep Pairwise Rank Aggregation loss to assign learnable confidence weights to each signal, addressing inconsistencies.

During model inference, a Scoring Strategy is proposed to adjust the predicted scores to fit a predefined score range. Experiments show that ULRA outperforms existing unsupervised methods and is competitive with cross-prompt and one-shot methods, though it still lags behind fully supervised methods due to weaker supervision. Overall, ULRA effectively leverages multiple heuristic signals for unsupervised essay scoring, demonstrating its potential in educational applications.</sample>
    <sample id="83">Yes, Encoder-Decoder models like mT5 can be improved by training in a mixture of various languages. This is because most of the major natural languages can obtain performance gains, although English performance drops in seven datasets and only gains in three datasets.</sample>
    <sample id="84">Shwai He presents "PAD-Net: An Efficient Framework for Dynamic Networks" at ACL 2023, focusing on the evolution from static to dynamic networks. Traditional static networks use fixed parameters, while dynamic networks adjust architecture or parameters based on input, exemplified by Mixture of Experts and Dynamic Convolution. Although dynamic networks often outperform static ones, fully dynamic networks can lead to excessive parameter use, as seen when replacing BERT-Base's feed-forward layers with Mixture of Experts, increasing model size fivefold.

To address this, He poses two questions: whether fully dynamic networks have redundant parameters and if a mix of static and dynamic parameters could enhance performance. The hypothesis is that fully dynamic networks contain partially dynamic sub-networks with sufficient representation power. This led to the development of PAD-Net, which partitions parameters into dynamic and static categories, using scale factors to manage their intensity and constraints to expedite training.

The Iterative Mode Partition method identifies and converts redundant dynamic parameters to static, minimizing their impact on loss value. Experiments show PAD-Net outperforms both static and fully dynamic networks, maintaining fewer parameters and computations. Ablation studies determine optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints for accuracy.

PAD-Net's performance surpasses network pruning by preserving static parameters, enhancing output discrimination. Future work includes extending PAD-Net to mainstream networks, hardware-friendly structures, and exploring combinations of zero elements, static, and dynamic parameters.</sample>
    <sample id="85">一个受限语言规划的示例是“make a chocolate cake”，这涉及到在规划过程中遵循特定的限制条件。</sample>
    <sample id="86">They ensure the covert nature of their method by visualizing the embeddings of sentences on four datasets using PCA. The visualization shows that it is hard to distinguish between backdoor embeddings and normal embeddings, indicating that the watermark is covert.</sample>
    <sample id="87">研究通过比较从头开始训练的 DrBERT 和 ChuBERT 模型，以及基于 CamemBERT 和 PubMedBERT 进行的连续预训练模型，来探讨如何使用现有的预训练语言模型（PLM）来构建新的 PLM。研究分析了不同数据源和预训练策略对模型性能的影响。</sample>
    <sample id="88">GPT-4 在社会可接受性分析中最不一致的是非英语和非儒家国家。</sample>
    <sample id="89">演讲者在示例句子 "I'm going to talk about..." 上展示了模型如何利用注意力机制所学的知识。在这个例子中，模型预测了德语翻译，并观察了交叉注意力权重，发现前两个词的注意力集中在最早接收到的语音帧上，而最后一个词的注意力集中在最后接收到的语音帧上。这导致前两个词被发出，而最后一个词由于交叉注意力的和超过了阈值α，因此等待另一个语音块。</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo and colleagues explores the potential of using language learners for data annotation in natural language processing (NLP), challenging the traditional reliance on native speakers. The authors conducted a proof-of-concept study to assess the feasibility of this approach, focusing on three languages: English, Korean, and Indonesian. They selected four tasks from the GLUE benchmark: Sentiment analysis, Natural Language Inference (NLI), Named Entity Recognition (NER), and Machine Reading Comprehension (MRC).

The study categorized language learners into three proficiency levels—basic, intermediate, and advanced—using revised CFR criteria. To ensure a fair comparison, native speakers were also recruited for the same tasks. The annotation samples were divided into five difficulty levels, and learners were provided with additional resources like dictionaries or machine translation systems to aid their understanding.

The experimental design included a preliminary survey to assess participants' language proficiency and background, followed by a series of sessions over six days. Each session involved a pre-test, annotation task, and post-test to evaluate both annotation accuracy and learning effects. The results showed that language learners' annotations were nearly as accurate as those of native speakers, particularly for simpler tasks and questions of easy-to-medium difficulty. When aggregated through majority voting, learners' annotations matched native speakers' performance.

The study also demonstrated that language models trained on learners' annotations achieved about 95% of the performance of models trained on native speakers' annotations, sometimes even surpassing them. This suggests that language learners can effectively contribute to NLP data annotation, especially in low-resource languages where native speakers are scarce.

Additionally, the research observed improvements in learners' language proficiency, vocabulary, and grammar through the annotation tasks, as evidenced by pre-test and post-test comparisons. The paper proposes a novel approach to data construction by involving language learners, which could help overcome geographic and technological barriers in building benchmark datasets for low-resource languages. The findings indicate that language learners can significantly contribute to NLP research, broadening its scope and accessibility.</sample>
    <sample id="91">任务的数量增加可以显著提高模型的性能。在研究中，随着任务数量的增加，模型在已见多模态任务上的表现得到了改善，同时敏感性（sensitivity）降低。这表明，增加任务数量有助于提升模型的泛化能力和稳定性。此外，使用多个指令（5个指令）相比单个指令，也能提高模型的整体性能并显著降低其敏感性。</sample>
    <sample id="92">The paper compares its method with the following three treeless baselines on the COGS benchmark:

1. **CopyNet**
2. **Seq2Seq**
3. **Seq2Seq with Copy**</sample>
    <sample id="93">Alexander Koller and Ivan Titov are the advisors of the first author, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China introduces a paper on protecting the copyright of embedding as services through a backdoor watermark method. Embedding as services, built on large language models like GPT, LLAMA, and PALM, are used for various NLP tasks. However, attackers can steal these models by learning from the embeddings and offering similar services. To address this, the paper proposes a watermark method that meets specific criteria: applicability to embedding services, non-degradation of embedding utility, covert nature, and transferability during model extraction.

The proposed method, called Embedding Marker, involves two main steps: watermark injection and copyright verification. It begins with selecting a trigger set of moderately frequent words. During watermark injection, the provider defines a target embedding. When a sentence is sent to the provider, the number of triggers is counted, and the provided embedding is a weighted summation of the target and original embeddings, with the weight proportional to the trigger count. If the trigger count exceeds a threshold, the provided embedding matches the target embedding.

For copyright verification, a backdoor dataset with sentences containing only trigger words and a benign dataset without trigger words are constructed. The provider requests embeddings from the suspected service using these datasets. The cosine and L2 similarities between the requested and target embeddings are calculated, and the differences (delta cosine and delta L2) are used as metrics. Additionally, a KS test p-value is used as a third metric.

Experiments on datasets like AG News, MIND, SST2, and Enron Spam demonstrate that the Embedding Marker achieves high detection performance while maintaining utility for downstream tasks. The covertness of the embeddings is validated through PCA visualization, showing difficulty in distinguishing between backdoor and normal embeddings. The paper concludes with an invitation for discussion.</sample>
    <sample id="95">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" is a joint work with colleagues from Google Translate, and the review does not specify the first author of PaLM.</sample>
    <sample id="96">大家好，我是来自卡内基梅隆大学的研究生杰尼，今天我将介绍我们的研究《NLPositionality：数据集和模型的设计偏见特征化》。这项研究是与华盛顿大学和艾伦人工智能研究所的塞巴斯蒂安·桑蒂、罗南·勒布拉斯、卡塔琳娜·赖内克和马尔滕·萨普合作完成的。

想象一下，你在一家报纸工作，正在筛选新闻文章下方的评论以移除有害内容。你可能会使用像Prospective API这样的流行API进行毒性检测，这对于卡尔·琼斯来说效果很好，因为Prospective API能够正确检测出有害实例。但对于阿迪蒂亚·夏尔玛来说，情况并非如此，因为Prospective API对印度语境中更常见的冒犯性术语敏感度较低。这就是设计偏见的一个例子，即技术在不同人群之间表现出系统性差异。设计偏见可能由于自然语言处理研究人员和模型开发者的立场而产生。立场简单来说，就是人们因其人口统计学、身份和生活经验而持有的观点。这是批判性研究中，特别是在女性主义和酷儿学术空间中广泛使用的概念。作为研究者，立场可以影响研究过程及其结果，因为它可以改变研究者做出的决策。因此，人们可能会问：数据集和模型是否具有立场？我们并不是说模型和数据集本身具有人口统计学身份和生活经验，但它们确实聚合了真实人们的判断和意见，因此可以代表某些立场而非其他立场。

之前的研究提供了一些关于立场的轶事证据，例如文化差距以及模型和数据集之间的差异，以及对模型立场的理论定义。然而，这些研究并未比较终端用户与数据集和模型本身，研究数据集和模型的立场变得越来越重要，因为自然语言处理任务变得更加主观和社会导向，而且很难表征这些立场的偏斜，因为不是所有决策都被记录，许多模型隐藏在API后面。

为了研究数据集和模型的立场，我们将数据集的注释与真实用户进行比较。我们通过我们的框架NLPositionality来实现这一点。我们的框架主要分为两个步骤。第一步是重新注释数据集，使用多样化的注释者。我们这样做是为了考虑到原始数据集注释者的人口统计学，因为通常只有少数注释者注释每个实例，而人口统计学数据很少被收集和分享。因此，我们重新注释数据以获得每个实例的多个注释，并获取丰富的人口统计学数据。然后，我们根据人口统计学对注释进行比较，并将其与模型和数据集进行比较，使用皮尔逊相关系数。因此，我们的框架与注释者不一致的文献不同，它比较的是终端用户与模型和数据集，预测和标签，而不是仅仅关注注释者一致性或建模注释者分布。

我们的框架主要通过“Lab in the Wild”和在线众包平台实现，这是我们的人机交互合作者。Lab in the Wild是一个在线实验平台，我们可以在这里招募多样化的志愿者。与像M Turk这样的平台相比，后者主要拥有来自美国或印度的参与者，Lab in the Wild仍然能够获取高质量的数据。我们在Lab in the Wild上主持了两个任务，其中之一是社会可接受性。这个任务的工作方式是参与者会阅读来自社会化学数据集的情境，然后写下这个情境的社会可接受性。之后，为了保持对研究的参与，他们可以将自己的回答与AI和其他人进行比较。我们将这些注释与社会化学、德尔菲和GPT 4进行了比较。然后，我们复制了一个非常相似的设置用于毒性和仇恨言论检测任务，参与者会阅读来自Dynahate的实例，然后写下他们是否认为这是仇恨言论的实例。我们将这些注释与Dynahate、Prospective API、Rewire API、Hate Roberta和GPT 4进行了比较。

最终，我们的研究收集了来自1000名注释者和87个国家的超过16,000个注释。现在我们更好地能够回答谁是自然语言处理数据集和模型最能代表的人群。我们发现自然语言处理中存在立场。例如，我们发现数据集和模型最能代表英语国家。例如，在GPT 4的社会可接受性分析中，我们发现它最能代表儒家和英语国家。我们还发现Dynahate也最能代表英语国家。我们还发现与受过大学教育的人的额外对齐。例如，在GPT 4的社会可接受性任务中，我们发现它最能代表受过大学或研究生教育的人，我们在Dynahate中也发现了同样的情况，它最能代表受过大学教育的人。然而，当模型和数据集与特定人群对齐时，一些人群可能会被遗漏。例如，数据集和模型与非二元人群的对齐度低于男性和女性。我们在GPT 4的社会可接受性任务和Dynahate任务分析中都发现了这一点。

鉴于自然语言处理中存在立场，我们有几项建议。首先，记录研究过程中所有相关的设计选择。其次，以多元视角进行自然语言处理研究。我们的第三项建议是在四个特定社区内构建专门的数据集和模型。一个很好的例子是Masakhani倡议。我们想强调的是，包容性自然语言处理不仅仅是让所有技术对每个人都有效。这就是我们的演示结束，但如果你想了解更多信息，可以查看我们的仪表板以获取最新的分析结果和我们的论文。谢谢大家。</sample>
    <sample id="97">The problems of current SimulST models mentioned are:

1. Specific architectures are usually trained, introducing additional modules to be optimized.
2. Long and complicated training procedures, involving different optimization objectives.
3. Training and maintaining several models to reach different latency regimes, such as training models with different average latencies (e.g., one second, two seconds).</sample>
    <sample id="98">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法包括：

1. **数据多样化**：确保训练数据来自多种来源和多样化的观点，以减少单一观点的过度代表。

2. **控制实验**：通过在不同政治倾向的子数据集上进一步训练模型，观察其政治倾向的变化，以更好地理解和调整偏见。

3. **时间分割训练**：将训练数据分为不同时间段（如在特定政治事件前后），以研究和调整模型对社会极化的敏感性。

4. **自动评估**：使用政治科学文献中的问卷调查格式来自动评估模型的政治倾向，以便更精确地识别和调整偏见。

5. **细分性能评估**：在不同人口统计或政治倾向的子集上评估模型性能，以识别和解决特定偏见问题。

6. **平衡偏见与审查**：在减少偏见的同时，避免过度审查或排除某些观点，以保持数据的中立性和多样性。

这些方法可以帮助识别和减轻 NLP 模型中的社会和政治偏见，从而提高模型的公平性和可靠性。</sample>
    <sample id="99">您好，我是复旦大学的苏毅源。今天我来介绍我们的工作《从大型语言模型中提炼脚本知识以进行约束语言规划》。在日常生活中，人们通常通过遵循目标导向的脚本来逐步规划他们的行动。之前的研究利用语言模型来规划抽象目标，如“做蛋糕”，并表明大型语言模型可以有效地将目标分解为步骤。然而，之前的研究主要集中在规划抽象目标的典型活动上。对于具有特定约束条件的目标（如“做巧克力蛋糕”）的规划仍然研究不足。在本文中，我们定义了约束语言规划问题，该问题对规划目标施加不同的约束。一个抽象目标可以被不同的具体目标继承，这些具体目标具有多方面的约束。一个好的规划者应该编写合理且忠实于约束的脚本。在本文中，我们首先评估和改进大型语言模型的约束语言规划能力。由于没有支持我们研究的具体目标数据集，我们首先需要获取这些目标。如表所示，我们通过人机协作的数据获取扩展了抽象目标的多方面约束，使用InstructGPT。我们抽样了100个具体目标并评估了大型语言模型生成的脚本。这张表报告了整体准确率的结果。我们发现所有语言模型在规划具体目标时都取得了不尽如人意的结果。然后，我们进行详细分析以调查学习模型为何失败。结果显示，生成脚本的语义完整性是可以接受的，但对约束的忠实度无法保证。我们深入研究了维基百科定义的约束的更细粒度的主题类别。热图显示了InstructGPTs在不同类别目标上的规划性能差异很大。之前的研究表明，语言模型的输出质量具有高方差，导致性能不佳。因此，我们采用了“过度生成-然后过滤”的方法来提高生成质量。我们首先展示了InstructGPT的约束类型及其示例，并根据种子抽象目标获得具体目标。然后，InstructGPT为具体目标过度生成K个脚本。接下来，开发了一个过滤模型来选择忠实的脚本。我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度作为相似度分数来衡量语义相似性。此外，我们奖励包含目标约束关键字的脚本。我们只保留目标在目标集中得分最高的脚本。通过我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法显著提高了规划能力，无论是在语义完整性还是对约束的忠实度方面。由于大型语言模型部署成本高昂，因此使小型和专用模型具备语言规划能力是至关重要的。创建数据集是实现这一目标的关键步骤。然而，之前的研究没有使具体目标规划成为可能，而手动数据集注释成本高昂。因此，我们遵循符号知识蒸馏的思想，从大型语言模型中提炼约束语言规划数据集。我们将我们的方法应用于构建约束语言规划数据集，名为CoScript。总共，我们生成了55,000个具有脚本的具体目标。为了确保验证和测试集的质量，我们要求众包工作者找出并修正错误样本。这张图显示了CoScript的约束分布。我们发现CoScript在生成的具体目标中显示出高度的多样性。通过CoScript，我们可以尝试小型但专用的模型进行约束语言规划。我们发现在CoScript上微调的T5可以生成比大多数大型语言模型更高质量的脚本，这表明当适当地在合适的数据集上进行训练时，小型模型可以超越大型模型。总之，我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并为大型语言模型开发了“过度生成-然后过滤”的方法。我们使用大型语言模型生成高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为语言规划研究的宝贵资源。感谢您的时间。请参阅我们的论文以了解更多关于CoScript的详细信息。</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. Traditional multi-hop QA systems require thousands of examples for training, which is costly, especially in low-resource domains. PromptRank addresses this by achieving good performance with as few as 128 examples.

The process involves two main steps: retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and reranking these candidates with a few-shot language model reranker. The scoring function used is the likelihood of the question given the chain, as determined by a language model. The chain prompt is constructed by inserting chain documents into a prompt with an indicator token and an instruction like "Read the previous documents and ask a question." This instruction helps elicit the language model's reasoning ability over the chain documents.

Additional techniques explored include instruction search to find optimal instructions, instruction sampling to aggregate scores from different instructions, and temperature scaling of language model logits. Experiments with GPT2-XL and T5-XL on HotpotQA show that PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. Ablation studies confirm the importance of each component in PromptRank's performance.

When used as a retriever with a reader model like ELECTRA-Large, PromptRank demonstrates strong downstream multi-hop QA performance, slightly underperforming MDR by about four exact match points. The approach highlights the effectiveness of using language models for few-shot ranking of candidate paths in multi-hop QA, with the likelihood of the question given the chain proving to be a superior scoring function. The instruction plays a crucial role in eliciting the language model's reasoning abilities over the chain documents.</sample>
    <sample id="101">PaLM 的流畅度与顶级系统相当，但其准确性存在问题，尤其是常见的遗漏错误。PaLM 有时会为了产生更流畅的翻译而省略源句中的部分内容。此外，PaLM 在“风格/别扭”类别的得分低于顶级系统，这表明其输出非常流畅，但仍存在准确性问题。</sample>
    <sample id="102">水印方法的重要属性包括：

1. 应用于嵌入服务。
2. 不降低提供的嵌入的实用性。
3. 对攻击者来说足够隐蔽，或者攻击者可以轻松移除水印。
4. 在模型提取过程中，水印需要能够转移到攻击者的服务中。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">The presentation does not specify the exact number of instances extracted from a dataset for re-annotation. It mentions re-annotating datasets with diverse annotators to get many annotations for each instance, but it does not provide a specific number of instances.</sample>
    <sample id="105">The distance metrics used to measure the difference between the benign and backdoor datasets are cosine similarity, L2 similarity, and the p-value from the KS test. The differences are defined as delta cosine and delta L2.</sample>
    <sample id="106">The paper introduces QUEST, a dataset designed to address the challenge of retrieving information based on queries with implicit set constraints. The motivation is illustrated through examples of Jane, a zoologist seeking the name of an unknown reptile species, and Austin, a reader looking for historical fiction novels set in France. Both examples highlight the need for systems to handle queries involving multiple constraints or preferences, such as intersections and complements of sets.

QUEST comprises over 3,000 entity-seeking queries that include implicit set operations. The dataset ensures that answer entities are relevant and that documents contain spans attributable to different query constraints. The construction of QUEST involves using Wikipedia category names from four domains: films, books, plants, and animals. Set operations are performed on these categories to generate queries with set constraints. Human annotators paraphrase these queries to ensure fluency and naturalness, and another set of annotators validates these queries. Annotators also verify the relevance of entities in the answer set and mark evidence in documents.

To evaluate systems on QUEST, the task requires retrieving multi-answer sets from a large document corpus, where evidence for a document's relevance can come from multiple parts. Baselines include sparse and dense retrievers, as well as a T5-based reranker. The evaluation reveals significant room for improvement in retriever performance, particularly in recall of complete answer sets, as indicated by MRecall@100 scores. End-to-end system performance, measured by F1 scores, is low, underscoring the difficulty of handling such queries. The analysis shows that queries involving set intersection and set difference are especially challenging.

The paper aims to assist future researchers in developing systems that better handle selective information needs, as exemplified by Jane and Austin's scenarios. The authors encourage readers to explore their paper and attend their presentation at ACL for further insights.</sample>
    <sample id="107">基于编码器的多语言模型，如XLM-R + PTR，可以通过将多语言预训练的编码器与指针式解码器结合来用于跨语言语义解析任务。这种方法在多语言设置中表现出色，因为它能够通过混合训练多种语言来提高性能。然而，这种方法也可能导致“多语言诅咒”，即大多数主要自然语言的性能提升，但英语在七个数据集中性能下降。</sample>
    <sample id="108">Koustav Sinha and his team present a study on the robustness of language model acceptability judgments in varying contexts, focusing on the minimal pair paradigm (MPP). The MPP traditionally evaluates language models by comparing the probabilities assigned to acceptable versus unacceptable sentences. However, this method is limited in assessing models' performance on longer sentences, which is increasingly important as language models now handle larger context windows.

The study revisits the MPP by simulating longer sequences, using datasets like BLiMP and SyntaxGym to create extended sentences that maintain grammatical structures. By adding prefixes from these datasets to both acceptable and unacceptable queries, the researchers test how context affects model judgments. They explore different scenarios, including matching and mismatching prefixes from the same or different datasets, and even using unrelated content like Wikipedia.

Their findings reveal that while MPP judgments remain stable with irrelevant contexts (e.g., Wikipedia), they significantly fluctuate when prefixes from the same dataset are used, especially when the prefixes match the grammatical structure being tested. This indicates that language models are highly sensitive to latent syntactic and semantic features shared across sentences.

The study highlights that current MPP evaluations, which focus on short, single sentences, may not fully capture a model's abstract knowledge over longer contexts. The research suggests that as language models evolve with larger context windows, it is crucial to adapt evaluation methods to better reflect their capabilities and limitations. The team encourages further reading of their paper for detailed experimental insights.</sample>
    <sample id="109">本文介绍了一项名为“Unnatural Instructions”的研究，旨在通过几乎不需要人工劳动的方式，创建一个大规模的自然语言指令数据集。传统的指令调优方法依赖于现有的学术NLP数据集或用户生成的提示，但这些方法要么受限于现有的学术任务，要么需要大量的人工标注工作。研究提出了一种全自动的数据生成方法，利用GPT-3的变体来生成指令、输入和输出，以及指令的多种表述形式。

研究的核心步骤包括：首先，通过提示GPT-3生成一个指令和相应的输入；其次，使用生成的指令和输入，再次提示GPT-3生成相应的输出。为了增加数据集的多样性，研究还生成了每个指令的多种表述形式。这种方法完全自动化，不需要人工标注。

最终生成的数据集包含64,000个示例，考虑到指令的多种表述，总共约有240,000个示例。研究分析了生成的示例，发现超过50%的示例是正确的，即使错误的示例也提供了有价值的信息用于指令调优。数据集中的任务既具有创造性又多样化，包括验证科学实验设计和发明新词等任务。

为了评估生成数据的实用性，研究者在Unnatural Instructions上微调了一个11亿参数的T5模型，并将其与T0++和Tk-instruct进行了比较。结果表明，微调后的T5模型在多个基准测试中表现优于其他模型。此外，当考虑到生成示例的成本时，Unnatural Instructions的训练效果甚至超过了在Super-Natural Instructions上训练的同等规模T5模型。

总之，Unnatural Instructions展示了语言模型在生成创造性和多样化数据方面的能力，这在人工标注中难以实现。这种方法不仅快速且经济，还能避免人工标注中常见的预测性启发式和标注偏差。</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count the word frequency with it to select a trigger set of words in a moderate frequency interval.</sample>
    <sample id="112">大家好，我叫舒恒。今天我将介绍我们的论文《CoNLL-2003命名实体识别器在2023年仍然有效吗？》。我们的论文研究了命名实体识别任务（NER）中的泛化问题。我们观察到，CoNLL-2003已被用于开发NER模型近20年，这自然引发了几个问题。首先，这些模型能否泛化到现代数据上？在开发新的标注器时，需要什么才能实现良好的泛化？同时，如果我们观察到泛化能力差，这些模型性能下降的原因是什么？为了调查这些问题，我们开发了CoNLL++数据集。这是我们从2020年的路透社新闻中收集的数据，并使用CoNLL-2003的相同标注指南进行了注释。我们对超过20个模型进行了CoNLL-2003的微调，并在CoNLL-03测试集和CoNLL++上进行了评估。最后，我们计算了每个模型的F1分数百分比变化，以评估其泛化能力。

那么，什么是良好泛化所需的呢？在实验中，我们发现有三个主要因素是必需的。第一个是模型架构。通过我们的实验，我们发现转换器模型通常能更好地泛化到新数据。第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化。最后，我们都知道微调样本的数量直接影响下游任务的性能。在这里，我们也发现更多的微调样本实际上也会导致更好的泛化。

接下来，我们的问题是，什么导致了一些模型的性能下降？我们有两个假设。第一个是适应性过拟合，即通过反复使用相同的测试集导致的过拟合，通常表现为在新测试集上的收益递减。第二个假设是时间漂移，即由于训练数据和测试数据之间时间差距增加而导致的性能下降。对于适应性过拟合，我们看到右侧图中红色最佳拟合线的斜率大于1。这意味着在CoNLL-2003上的每个改进单位都会在CoNLL++上转化为超过一个改进单位，这表明没有收益递减。这表明在这种情况下没有观察到适应性过拟合。那么时间漂移呢？对于时间漂移，我们进行了一个实验，重新训练或继续预训练一些模型使用更近期的数据，我们发现性能随着时间差距的增加而下降，这证实了我们的假设，即性能下降的主要原因是时间漂移。

我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调样本。这些因素是相辅相成的，我们不能只有一个因素而忽略其他因素。同时，我们也发现性能下降的原因是时间漂移，而不是适应性过拟合，尽管CoNLL-2003已经被使用了超过20年。回到我们论文标题中提出的问题：CoNLL-2003标注器在2023年仍然有效吗？我们发现答案是一个明确的“是”。我们希望我们的论文能够促使更多关于如何改进模型泛化能力的研究。最后，请务必查看我们的论文和数据集，如果有任何问题，请随时联系我。谢谢大家。</sample>
    <sample id="114">我们的研究“Finding the Pillars of Strength for Multi-Head Attention”来自新加坡南洋理工大学，旨在解决大型语言模型中的参数冗余问题。大型语言模型如LLaMA-65虽然具有革命性，但由于参数量巨大（数十亿）、训练时间长（如一百万GPU小时）以及对大量数据集的需求（如4.5TB），在小型集群上部署困难。

我们关注的是多头注意力机制中的参数冗余。研究表明，一些注意力头可以被剪枝而不影响性能。现有的优化方法包括：使头部更相似的同质化方法、使头部更多样化的异质化方法，以及基于分数的剪枝方法。然而，这些方法要么牺牲性能，要么不够参数高效。

我们提出了分组头注意力机制，采用分治策略来压缩多头注意力。该方法包括两个阶段：分组约束训练和投票留存算法。分组约束训练将注意力头分为几组，使同组头部更相似，不同组头部更分离。投票留存算法在训练后剪枝，仅保留每组的一个头部，实现显著的参数压缩。

我们在机器翻译、语言建模和抽象摘要任务上评估了两个模型GHT和GHT-PS。这些模型在机器翻译任务上分别提高了3.8%和4.4%的BLEU分数，同时GHT-PS在剪枝后压缩了32.1%的参数。在抽象摘要任务上，它们分别提高了6.7%和7%的性能，并实现了32.1%的压缩。在语言建模任务上，它们分别提高了2.8%和2.9%的性能，并实现了16.9%的压缩。

进一步的效率分析显示，我们的LITE模型实现了90%的参数剪枝、62%的更快推理速度和80%的FLOPs减少，与性能相同的模型相比。未来，我们认为任务特定的自动剪枝是一个有前途的方向，类似于在iPhone上卸载不用的应用，以减轻负担而不影响性能。</sample>
    <sample id="115">The method uses a speech chunk size of lambda speech frames.</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要的特定于实体的知识是：“Servin 是一名法官。”和“Kea 是一名面包师。”</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The presentation introduces a novel approach to improving pretraining techniques for code-switched NLP, focusing on the common phenomenon of code-switching in linguistically diverse communities like India. The authors highlight the limitations of existing multilingual models like mBERT and XLM-R in handling code-switched tasks such as question answering and sentiment analysis. To address this, they propose SwitchMLM, a new masked language modeling (MLM) technique specifically designed for code-switching. SwitchMLM focuses on "switch-points," which are transitions between languages within a sentence, allowing only these words to be maskable, unlike standard MLM where all words are maskable with uniform probability. However, SwitchMLM requires language identification (LID) tagged datasets or LID taggers, which are not always available. To overcome this, the authors introduce FrequencyMLM, a surrogate method that uses negative log likelihood from monolingual corpora to assign LID tags.

Additionally, the authors propose architectural modifications, including residual connections from intermediate layers of BERT that encode more switch-point information to the final layer, enhancing the model's ability to handle code-switching. They also introduce an auxiliary LID-based loss to encourage these intermediate layers to encode language information more effectively. The combined method, incorporating SwitchMLM or FrequencyMLM with ResBERT and auxiliary loss, shows superior performance in sentiment analysis tasks across various language pairs.

Probing experiments using linear and conditional probing classifiers verify that the proposed methods increase switch-point information in both intermediate and final layers. The results demonstrate that combining StandardMLM with SwitchMLM representations yields more switch-point information than StandardMLM alone. Linear probing results further suggest that adding a residual connection from an intermediate layer with more switch-point information to a later layer enhances the final representation's switch-point information content. In summary, the work presents a new MLM objective tailored for code-switching, supported by architectural changes and auxiliary loss, to improve the handling of code-switched data in NLP models.</sample>
    <sample id="119">在扩展实验中，论文侧重于GPT-4、GPT系列、BART系列及其变体、RoBERTa等语言模型。</sample>
    <sample id="120">The model uses the cross-attention mechanism to decide whether to emit a partial translation based on where attention points to. It considers the sum of the cross-attention weights towards the last lambda speech frames to determine if the information is stable enough to emit a word. The decision is based on whether this sum is below a certain threshold alpha, indicating that the attention is not concentrated on the most recent frames. The description does not specify using attention scores from multiple layers, but rather focuses on the cross-attention mechanism between audio input and textual output.</sample>
    <sample id="121">直接推断的示例包括使用歌曲的名称，如“Easy on Me”，或者使用其位置，如“the first one”。</sample>
    <sample id="122">Fudan University</sample>
    <sample id="123">Ying and Zhiyang presented their research on MultiInstruct, a dataset designed to enhance Multi-Modal Zero-Shot Learning through instruction tuning. They highlighted the gap in instructional datasets between NLP and multi-modal tasks, noting the abundance of language-only instruction tasks compared to the lack of large-scale multi-modal instruction datasets. To address this, they developed MultiInstruct, the first multi-modal instruction tuning benchmark dataset, comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets. Each task includes five expert-written instructions.

The research utilized OFA, a unified multi-modal pre-trained model, as the base model. OFA employs a unified vocabulary for language, image tokens, and bounding box coordinates, allowing tasks to be formatted in a sequence-to-sequence manner. For training, 53 tasks from 9 groups were used, with 10,000 instances per task. Testing involved the entire common sense reasoning group and five additional tasks from VQ and Miscellaneous groups, using all instances in the test split. An unseen task for NLP was also included by randomly sampling 20 tasks from the test split of natural instructions.

The training process involved mixing instances from all tasks, each combined with one of its five instruction templates. During testing, five experiments were conducted per task, evaluating the model with each instruction template. Performance metrics included accuracy for multi-modal classification tasks, Rouge-L for multi-modal generation and NLP tasks, and a new metric called sensitivity, which measures consistency in outputs despite variations in instruction wording.

Results showed that instruction tuning significantly improved OFA's performance on seen multi-modal tasks and that transfer learning from natural instruction datasets enhanced performance and reduced sensitivity. Using multiple instructions improved overall performance and sensitivity compared to a single instruction. The research also demonstrated the benefits of different fine-tuning strategies and transfer learning techniques, proposing a new metric, sensitivity, to evaluate model consistency. Additionally, they are expanding MultiInstruct with around 150 more vision-language tasks, which will be released soon.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented their work on enhancing the temporal reasoning capabilities of large language models (LLMs). They identified three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event reasoning. The study highlighted that previous research focused mainly on time-to-event reasoning, while their work aims to address all three levels comprehensively.

The team conducted preliminary experiments on year prediction, revealing biases in models like T5-L and FLAN-T5-L towards the 2000-2020 period, likely due to pre-training data. ChatGPT showed promise in year prediction but struggled with month prediction. To address these issues, they introduced the TempReason dataset, which includes questions across all three reasoning levels and spans a wide temporal range. The dataset was constructed using Wikidata and Wikipedia, with increased difficulty from year to month prediction.

They evaluated temporal reasoning in three settings: Closed Book QA, Open Book QA, and a new Reasoning QA setting, where relevant temporal knowledge is provided. To improve LLMs' temporal reasoning, they proposed a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. The former involves reconstructing masked spans in text, while the latter rewards correct predictions and penalizes temporal errors.

The final model, TempT5, was tested on TempReason and showed significant improvements over other models like FLAN-T5-L, ChatGPT, and T5-base. TempT5 outperformed in Open Book QA and Reasoning QA settings, although some performance fluctuations were noted across different time periods, possibly due to training data imbalance.

In summary, the study exposed temporal reasoning biases in LLMs, introduced a comprehensive benchmark dataset, and proposed a training paradigm to enhance temporal reasoning. Future work could focus on addressing reasoning biases and data imbalances.</sample>
    <sample id="125">这篇论文的作者数量未在提供的内容中提及。</sample>
    <sample id="126">是的，在语义解析之前，使用Google Translate API将自然语言查询翻译为目标语言作为基线。这是在“Translate-Test”设置中进行的，其中源语言查询被翻译为目标语言，然后使用单语模型进行训练和评估。</sample>
    <sample id="127">Namgyu Ho, a master's student at KAIST AI, along with Laura Schmid and professor Se-Young Yun, introduced their work titled "Large Language Models Are Reasoning Teachers." The paper addresses the challenge of deploying large language models like GPT-3 for complex tasks due to their high memory and computational demands. To overcome this, they propose using these large models as "reasoning teachers" to transfer their capabilities to smaller models through a technique called chain-of-thought (CoT) prompting. This method involves prompting large models to solve problems step-by-step, generating solutions that are then used to fine-tune smaller models.

A key innovation in their approach is "Diverse Reasoning," which involves generating multiple reasoning samples from the large model using stochastic temperature sampling. This diversity in solutions enhances the training of smaller models, enabling them to perform complex reasoning tasks more effectively. The authors tested their method on 12 tasks, showing significant improvements over existing baselines, particularly in text-based tasks. For instance, performance on the Multi Arithmetic task improved from 33% to 55% with Diverse Reasoning.

The method outperforms vanilla fine-tuning, even with models as small as 0.3 billion parameters. The authors highlight that performance can be further scaled by using more datasets, better teacher models, or larger student models, though these come with trade-offs between development-time and inference-time costs. The paper suggests that this distillation approach could be applied to other emergent abilities in the future, offering a scalable and effective solution for deploying smaller models with advanced reasoning capabilities. The authors provide code and data for further exploration and encourage future work based on their findings.</sample>
    <sample id="128">Akshatha and Martin present their work, "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," a collaboration between McGill University, Mila, and Microsoft Research. The study focuses on the challenge of integrating knowledge from different sources in natural language understanding (NLU) models, particularly in tasks like coreference resolution. These models rely on knowledge from both pretraining and inference time, but often struggle to integrate these sources effectively.

The authors introduce a diagnostic test suite, KITMUS, to evaluate models' ability to integrate knowledge from multiple sources. They use a coreference resolution task to probe this ability, requiring models to resolve pronouns by combining entity-specific knowledge (e.g., "Servin is a judge") with background knowledge (e.g., "Judges decide cases in law courts"). The task is designed to vary the availability of these knowledge types across three settings: Background-Pretrain, Background-Both, and Background-Inference.

In the Background-Pretrain setting, background knowledge is assumed to be available from pretraining, while entity-specific knowledge is provided at inference time. The Background-Both setting includes both types of knowledge at inference time. The Background-Inference setting, which is particularly challenging, provides both knowledge types only at inference time, simulating scenarios where background knowledge is not part of the pretraining data.

The study evaluates the dataset with human participants and established coreference resolution models, such as C2F and BERT4Coref. Results show that without task-specific training on KITMUS, models perform poorly, relying on surface cues that are ineffective in KITMUS. However, with task-specific training, models significantly improve, indicating that they can integrate knowledge from multiple sources when trained appropriately. Despite this, even the best-performing models struggle to integrate backward knowledge provided only at inference time.

The main takeaway is that many coreference resolution models require task-specific training to effectively integrate knowledge from different sources. While some models can achieve this with training, challenges remain, particularly with integrating inference-time knowledge. For more details, the authors invite readers to explore their paper, dataset, and code on GitHub.</sample>
    <sample id="129">作者给出的“显性群体”(marked group) 示例包括：亚裔女性、中东女性、拉丁裔女性、亚裔女性、黑人女性。这些群体与未标记的群体（如白人男性）相比，被认为是显性的。</sample>
    <sample id="130">The content does not specify which model architectures have poor generalization. It only mentions that transformer models generally generalize better to new data.</sample>
    <sample id="131">视频中没有提到测试数据集的名称。</sample>
    <sample id="132">这篇论文有两位作者，Akshatha和Martin。</sample>
    <sample id="133">作者采用了多种模态，包括文本、图像和边界框。</sample>
    <sample id="135">ABC-Eval是由Emory NLP Lab和Amazon Alexa AI合作开发的一种新的评估方法，用于维度化评估对话AI。该方法旨在通过减少人类评估的主观性来提供更精确和可靠的对话质量评估。传统的评估方法通常依赖于人类评判者，要求他们选择更好的对话或使用Likert量表对对话进行评分。虽然这些方法能够提供对整体对话质量的全面评估，但对话质量有多个方面，因此需要更细粒度的评估。

ABC-Eval通过明确标注模型回应中是否表现出某些行为来降低主观性，例如回应无关信息、自相矛盾或违反常识等。这种方法能够全面覆盖影响对话质量的模型行为。研究人员选择了四个最先进的对话模型，并使用ABC-Eval对每个模型的100个人机对话进行评估。为了比较，还使用了三种现有方法：基于回合的Likert评分、基于对话的Likert评分和对话级别的配对比较。

研究结果表明，ABC-Eval的行为标签比现有方法更可靠，这从100个双重标注对话的评估中的评委间一致性中得到了证明。此外，ABC-Eval的标签比现有方法产生的指标更能预测整体对话质量，这通过简单的线性回归分析得到了验证。例如，自我和伙伴矛盾的比例能够解释5%和10%的对话质量，而平均Likert一致性分数只能解释4%或更少。

通过逐步线性回归分析，研究人员发现ABC-Eval的所有指标结合起来能够解释超过25%的对话质量，而去掉任何一个指标都会导致丢失大量质量信息。相比之下，所有基于回合的Likert指标结合起来解释的对话质量较少，且较少的指标携带独特信息。这些可靠、信息丰富且独特的ABC-Eval指标使得对对话AI的评估能够达到更高的分辨率。

实验结果显示，测试的机器人在20%的回应中存在常识违反，在15%的回应中提供无关信息，在10%的回应中自相矛盾或与伙伴矛盾。尽管新模型可能会减少这些错误率，但这更加强调了需要可靠和精确的评估指标来比较模型。研究人员希望ABC-Eval能够被领域内的其他人使用，作为推动对话AI进步的有意义步骤。</sample>
    <sample id="136">Jasivan presented the research "FERMAT: An Alternative to Accuracy for Numerical Reasoning," conducted with Nafise at the University of Sheffield. The motivation behind this work is to address the limitations of current benchmarks in evaluating numerical reasoning in language models, which are crucial for real-world applications like fact-checking. The study highlights that larger models tend to perform better in numerical tasks, but accessible models with around 3 billion parameters struggle. Current benchmarks, which focus on accuracy and F1 scores, do not provide detailed insights into the mathematical capabilities of these models.

To tackle these issues, the researchers introduced FERMAT, a flexible evaluation set based on arithmetic types, focusing on number understanding, mathematical operations, and training dependency. FERMAT consists of math word problems extracted from Illinois and CommonCore, with variations in number representations to mimic real-life scenarios. The evaluation considers different mathematical operations and their combinations to assess model performance comprehensively.

Initial zero-shot evaluations revealed that most models performed poorly across FERMAT's aspects, suggesting that existing benchmarks might not reflect real-world necessities. The researchers then fine-tuned models using templates created by math teachers, generating 200,000 examples involving small integers, large integers, and decimals. This fine-tuning improved performance across various aspects, indicating promising results.

The study also examined training dependency, focusing on whether exposure to specific expressions during training improved model performance. Even with exact expressions seen during training, accuracy remained below 50%, suggesting that models do not memorize but rather rely on linguistic cues. The impact of training templates was further investigated, showing that language and mathematical diversity, as seen in templates from GSM8K and AQUA, significantly enhanced performance.

In conclusion, the research found that existing benchmarks are unrepresentative and that single scores are insufficient. FERMAT provides a more informative alternative, highlighting the importance of language and mathematical diversity. The study also identifies number encoding and tokenization as areas needing improvement. Jasivan encourages readers to explore the paper for more detailed insights.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" introduces a novel task of generating floor plans from natural language instructions, aiming to enable non-experts to participate in the design process. The authors propose a sequence-to-sequence model using a transformer-based encoder-decoder framework, initialized with the T5 language model, to address the challenges of generating floor plans that adhere to specific user requirements. The Tell2Design dataset, comprising 5,051 human-annotated and 76,000 artificially generated language instructions, is used to train and evaluate the model. The dataset includes detailed descriptions of room semantics, geometry, and topology.

The task involves generating structured 2D floor plans from language instructions, which specify room types, shapes, dimensions, and relationships. The model treats the instructions as input sequences and the room bounding boxes as target sequences. The approach outperforms traditional text-conditional image generation methods, achieving high Intersection over Union (IoU) scores, indicating better alignment with user-specified requirements.

Challenges include handling stricter constraints than artwork generation, understanding unstructured text, and dealing with ambiguous or incomplete instructions. The model's success is partly due to its ability to extract salient information from instructions and generate target sequences accordingly. The study also highlights the importance of bridging the language distribution gap between artificial and human instructions, showing improved performance when artificial instructions are used for pre-training.

Overall, the paper sets a foundation for future research in language-guided design generation, emphasizing the potential of sequence-to-sequence models in creating user-aligned floor plans.</sample>
    <sample id="138">作者认为，NLU 中研究不足的领域包括：1) 模型在没有任务特定训练的情况下，难以从不同来源整合知识；2) 即使是最好的模型，也在整合仅在推理时提供的背景知识方面存在困难。</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Yes, CoScript underwent quality checks. Crowd-sourced workers were asked to find and revise incorrect samples to ensure the quality of the validation and test sets.</sample>
    <sample id="141">现有的资源在评估依赖上下文的翻译时有以下局限性：

1. **仅支持有限类型的上下文依赖翻译**：现有资源通常只支持有限种类的上下文依赖翻译。
2. **仅支持有限的语言集**：这些资源通常只支持有限的语言集。
3. **依赖领域知识和人工编辑**：这些资源通常依赖于领域知识和人工编辑，这限制了其广泛应用。</sample>
    <sample id="142">你好！我要谈谈我们在《解决间接指代表达以进行实体选择》中的工作，我们引入了AltEntities语料库。我的名字是Javad Hosseini，这是与Filip Radlinski、Silvia Pareti和Annie Louis的合作。我们的目标是理解用户在做选择时的语言。考虑这样一个替代问题：“你是指《Easy on Me》还是《I Gotta Feeling》？”在这里，用户想要选择这两首歌中的一首。最直接的方法是使用直接引用，例如说出歌名“Easy on Me”或其位置“第一首”。但有时，间接引用更适合进行更自然的对话。这可能发生在用户无法记住歌名时，或者发音相似，难以区分，或者用户想要指定偏好。这里有一些间接引用的例子，例如“较新的那首”或“不是充满活力的歌曲”。这是对话系统中的一个重要问题，也是对大型语言模型（LLMs）实体理解能力的基准测试。我们不知道有更大规模的公开数据集用于此任务，因此我们使用众包注释收集了一个数据集。我们的数据集涵盖了三个不同的领域：音乐、书籍和食谱。我们的数据集收集方法强调非正式性，使用漫画完成的设置。漫画有三个对话气泡。在第一个气泡中，Bob说：“你还记得我们昨天听的那首歌吗？”这为对话上下文奠定了基础。在第二个气泡中，Alice说：“你是指《Easy on Me》还是《I Gotta Feeling》？”这是替代问题。在第三个气泡中，Bob使用间接引用来选择这些实体中的一首，例如“较新的那首”。我们提供了前两个气泡，但第三个由注释者填写。第一个气泡是从每个领域的几个手动提示中选择的。第二个气泡，即替代问题，是使用以下简单模板生成的：你是指A还是B？其中A和B是从维基百科中抽取的样本。我们使用的不同抽样方法如下。当我们在列表中向上移动时，实体之间变得更相似，通常更难进行区分。第一个是均匀随机的。第二个是当实体有相似的标题时，例如两本书名为《The Return》。第三个是当它们在维基百科上有相似的描述时。最后，当它们在维基百科上有相似的信息框或属性时，例如同一类型或同一艺术家的歌曲。当我们向注释者展示这个替代问题时，他们知道这些实体的名字，但不一定了解这些实体。因此，我们向他们展示了关于这两个实体的一些背景知识。对于歌曲，我们简单地显示每首歌的谷歌搜索链接，然后要求注释者至少听一些每首歌，并阅读关于每首歌的信息。例如，歌曲《Easy on Me》的谷歌搜索结果。对于食谱和书籍领域，我们显示维基百科上的一些背景文本。对于食谱，我们还显示它们的图片，也是从维基百科中获取的，以便注释者知道它们的外观。然后，我们要求注释者选择这些实体中的一个，例如，这里是第一个，并用三到五个间接指代表达来描述它们。例如，“没有歌词的那个”，“不是有12岁男孩的那个”，或“虚构的那个”，或“来自阿塞拜疆的那个”，等等。AltEntities语料库包含6,000个替代问题，涵盖三个领域，以及42,000个间接指代表达。T5 XL模型的结果如下所示。如果语言模型拥有与注释者完全相同的背景知识，准确率非常高，约为92%到95%。但这不是现实的。如果语言模型拥有部分重叠的背景知识，准确率在82%到87%之间，这更现实。例如，当语言模型检索背景知识时。如果语言模型只能访问实体名称，准确率仅为60%，因此有很大的改进空间。我们还表明模型是领域通用的。这里是我们数据集的链接。谢谢。</sample>
    <sample id="143">该方法与以下现有的 SimulST 策略进行了比较：Wait-k 策略、Local Agreement 策略，以及专门为同时翻译设计的最先进的架构。</sample>
    <sample id="144">The paper's author, Yanis Labrak, is associated with the University of Nantes.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, a PhD student from Fudan University, discusses a paper on omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries from dialogues, capturing essential information across various domains. Despite advancements using large-scale pretrained language models, generated summaries often contain factual errors, with omission being a significant issue. Omission leads to incomplete summaries, missing critical facts, and is prevalent across different domains and models, with about 70% of summaries affected.

The study analyzes the omission rate and finds that omitted information is randomly distributed in dialogues, highlighting the challenge of identifying key information. To address this, the paper introduces the OLDS dataset, providing high-quality omission labels for dialogue summarization. This dataset is built on five existing benchmarks across five domains, using diverse abstractive models to generate candidate summaries and an automatic method to produce omission labels, verified by human evaluation.

The paper explores three baseline frameworks for omission detection: pair-wise classification, sequence labeling, and pointer network, using Precision, Recall, and F1-score for evaluation. The task is challenging, with an F1-score around 50%, indicating a need for advanced models. The study also investigates using detected omissions to refine summaries, showing significant performance improvement when omissions are included, suggesting that omission detection and refinement are promising directions for enhancing dialogue summarization quality.</sample>
    <sample id="147">这篇论文有三位作者：Myra、Esin Durmus 和 Dan Jurafsky。</sample>
    <sample id="148">你好，我是来自都灵大学和布鲁诺·凯瑟尔基金会的萨拉·帕皮。我将简要介绍我们与马泰奥·内格里和马可·图尔奇合作的论文《注意力作为同时语音翻译的指南》。什么是同时语音翻译（SimulST）？同时语音翻译是指将口语实时翻译成另一种语言的文本，从而实现跨语言交流。当前SimulST模型存在哪些问题？通常需要训练特定的架构，引入额外的模块进行优化。例如，涉及不同优化目标的复杂训练过程。还需要训练和维护多个模型以达到不同的延迟水平。例如，训练一个平均延迟为一秒的模型，另一个为两秒，依此类推。那么我们的解决方案是什么？首先，利用现有的离线语音翻译模型，而不需要重新训练或采用特定的SimulST架构。只使用一个模型来处理每个延迟水平，并通过特定参数来处理延迟。利用模型通过注意机制在音频输入和文本输出之间已经获得的知识。这就是跨注意机制，你可以在右侧看到一个例子。我们的解决方案是提出EDAtt，即编码器-解码器注意力，这是一种决定是否发出部分翻译的策略，根据注意力的指向。如果注意力不集中，即其和值低于某个阈值α，且在最后λ个语音帧上，这意味着接收到的信息已经足够稳定，那么一个词就会被发出。例如，如果我们接收到一个包含“I'm going to talk about...”的语音块，并且我们的模型预测了德语翻译，我们会查看跨注意权重，会发现前两个词指向最早接收到的语音帧，而最后一个词指向最后接收到的语音帧，即λ个语音帧。这意味着前两个词会被发出，而由于跨注意和值高于某个阈值α，我们将不会发出最后一个词，而是等待另一个语音块。如果我们继续并接收到另一个语音块，模型预测了三个其他词，我们会查看这些跨注意权重，会发现没有词指向最后λ个语音帧。这意味着这三个词会被发出。如果我们看看EDAtt的主要结果，我们会在图表上绘制同时语音翻译结果，其中一边是BLEU，用于衡量翻译质量，另一边是平均延迟，即延迟度量，我们还考虑了计算感知平均延迟，即模型预测输出所需的计算时间。我们希望我们的曲线在这个图表上尽可能高，同时也希望它们向左移动。我们与应用于离线模型的流行策略进行比较，如Wait-k策略和Local Agreement，并与专门为同时预翻译设计的最先进架构进行比较。这些都是德语上同时语音翻译策略的结果。我们看到它在应用于离线模型的所有策略中表现最佳，因为曲线向左移动。我们还看到，如果考虑实际经过的时间或计算感知时间，即最快的策略。如果你想了解更多结果，请阅读我们的论文。我们还开源了代码和模型以及同时输出，以便于复现我们的工作。谢谢你的关注。</sample>
    <sample id="149">Yes, the CoNLL++ Dataset is mentioned as being collected and annotated, implying it is available for use.</sample>
    <sample id="150">The paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" introduces a novel dataset, MeetingQA, aimed at addressing the gap in question-answering (QA) tasks within meeting transcripts. Meeting transcripts, often long and domain-specific, are rich in information but have been underutilized in QA research, which has primarily focused on summarization and action item extraction. MeetingQA is designed to leverage the QA component inherent in meetings, where participants ask questions that elicit detailed responses and discussions.

The dataset is constructed from the AMI corpus, comprising nearly 100 hours of manually transcribed multi-party meetings. The data collection process involves selecting questions based on punctuation and filtering out short questions. Annotators label sentences in the answer span, achieving a high inter-annotator agreement with a Krippendorff's alpha of 0.73. MeetingQA contains 7.7K questions, with 30% being unanswerable. Among the answerable questions, 40% have multiple answer spans, and 48% involve multiple speakers. The dataset includes various question types, with a majority being yes/no questions that still elicit detailed responses, 20% rhetorical questions, and 70% of multi-speaker answers containing some disagreement.

The dataset's questions and answers are approximately 12 and 35 words long, respectively. Human performance on the Test set is high, with an F1 score of 84.6. The paper explores several methods, including context-retrieval for short-context models, single-span models for identifying the first and last relevant sentences, and multi-span models for token classification tasks. Silver annotations from the MediaSum dataset are used for data augmentation.

Results show a significant gap between fine-tuned models and human performance, with short-context models like RoBERTa slightly outperforming long-context models like Longformer. Multi-span models perform slightly less or comparably to single-span models. Zero-shot performance reveals a larger gap compared to human performance, but silver data augmentation improves zero-shot results. Larger instruction-tuned models like FLAN-T5 show comparable zero-shot performance to other models.

Error analysis indicates that models struggle with rhetorical questions, especially in zero-shot settings, and single-span models often include irrelevant sentences. Identifying the correct speaker for answers is also challenging, particularly in zero-shot scenarios. Overall, MeetingQA presents a challenging and interesting dataset for QA models, highlighting the need for further research in this domain.</sample>
    <sample id="151">大家好，我叫英瑛，我的同事志扬，我们将展示我们关于MultiInstruct如何通过指令调优来改进多模态零样本学习的研究。随着大型语言模型的进步，许多研究开始探索如何以参数和数据高效的方式重用预训练的语言模型来进行不同的下游任务。最近的研究表明，指令调优使大型语言模型能够通过遵循自然指令来在零样本下执行未见任务。然而，大多数关于指令调优的先前工作主要集中在提高语言任务的零样本性能，而计算机视觉和多模态任务被忽略了。因此，我们的工作旨在探讨指令调优多模态预训练模型是否能够实际提高对未见多模态任务的泛化能力。此外，在我们的研究过程中，我们发现自然语言处理和多模态之间的指令数据集可用性存在显著差异。存在超过1600个语言仅指令任务，但没有大规模公开的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。我们提出了MultiInstruct，这是第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个广泛的类别。这些任务来自21个现有的开源数据集，每个任务配备了五个专家编写的指令。为了在我们提出的数据集上探索多模态指令调优，我们将OFA，一个统一的多模态预训练模型，作为我们的基础模型。OFA使用统一的词汇表来处理语言、图像令牌和边界框的坐标。我们展示了MultiInstruct数据集的一些示例实例，以统一处理各种输入和输出数据类型。我们遵循OFA的方法，将所有任务以统一的序列到序列格式进行表述。其中，输入文本、图像、指令和边界框在同一令牌空间中表示。现在，我将讨论多模态指令调优。对于训练数据集，我们使用9个组中的53个任务进行训练，并为每个任务抽取10,000个实例。对于测试，我们保留所有常识推理组用于测试，并从视觉问答和杂项组中选择另外5个任务。对于每个任务，我们使用其测试分割中的所有实例。此外，我们从自然指令的测试分割中随机抽取20个任务作为未见的NLP任务。我们使用预训练的OFA大模型作为基础模型。在训练过程中，我们将所有任务的所有实例混合在一起。每个实例随机与其五个指令模板之一结合。在测试时，对于每个任务，我们进行总共5次实验，通过使用其中一个指令来评估模型。在每次实验中，我们报告所有5次实验中性能的最小值、最大值和标准差。如果任务是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告Rouge-L。对于NLP任务，我们也报告Rouge-L。我们还引入了一个额外的评估指标，称为敏感性。这个指标衡量模型在指令措辞略有变化时，是否能够始终为相同的任务生成相同的输出。这是我们的主要结果。如您所见，指令调优可以显著提高OFA在已见多模态任务上的性能。此外，从自然指令数据集进行迁移学习可以提升指令调优。如您所见，随着任务数量的增加，模型的性能得到改善，同时敏感性降低。我们还进行了一个实验，比较使用一个指令与使用五个指令。如您所见，使用更多的指令可以显著提高模型的整体性能并大幅降低其敏感性。这显示了不同微调策略对模型敏感性的影响。如您所见，通过从自然指令数据集进行迁移学习，模型可以实现比原始OFA模型更好的敏感性。我们还可以看到，从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。总体而言，我们提出了第一个大规模多模态指令调优数据集，并显著提高了OFA的短期能力，探索了不同的迁移学习技术并展示了其优势。我们设计了一个新的指标，称为敏感性。另外，我们正在收集一个更大的多模态指令调优数据集，包含约150个额外的视觉语言任务，并将其发布。这是我们数据和模型的二维码。谢谢。</sample>
    <sample id="152">Frederick Riemenschneider presented on the intersection of NLP and classical philology, focusing on the development of new language models for Ancient Greek and Latin. Despite recent advancements like Latin BERT and Ancient Greek BERT, these models are limited as they are monolingual and encoder-only. To address these limitations, the project aimed to create models that are both multilingual and capable of understanding and generating text.

The project introduced four models: GreBERTa and GreTa for Ancient Greek, and PhilBERTa and PhilTa as multilingual models for Ancient Greek, Latin, and English. GreBERTa is a RoBERTa-based encoder-only model, while GreTa is an encoder-decoder model based on the T5 architecture. PhilBERTa and PhilTa extend these capabilities to multiple languages.

A significant innovation was the creation of a new pre-training corpus from the Internet Archive, using OCR transcriptions of Greek stop words to identify Greek texts. This allowed for high-quality data collection, enhancing model training.

Benchmarking was conducted using Universal Dependencies treebanks for Greek and the EvaLatina 2022 dataset for Latin, focusing on tasks like part-of-speech tagging, dependency parsing, and lemmatization. The models outperformed existing state-of-the-art models, particularly in lemmatization, where encoder-decoder models showed a 5 percentage point improvement for Ancient Greek.

The project also explored semantic and world knowledge capabilities, finding that the models could distinguish synonyms from antonyms and identify relationships between mythological figures. Interestingly, multilingual models did not significantly outperform monolingual ones in these tasks.

Overall, the project successfully developed powerful, native tokenizer-based language models for classical philology, providing a comprehensive approach to processing Ancient Greek and Latin texts.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presented their work on resolving ambiguities in text-to-image generative models. The research focuses on the challenge of ambiguous prompts, which can lead to varied interpretations and affect the faithfulness of generated images to user intentions. The study introduces a framework to address these ambiguities and evaluate the resulting images.

The research begins by curating a benchmark dataset based on the LAVA corpus, which includes various types of ambiguities. The framework employs a language model to either generate clarifying questions or propose different visual interpretations for ambiguous prompts. Users respond to these questions or select the interpretation that aligns with their intention, resulting in a disambiguated prompt.

The disambiguated prompts are then input into text-to-image models to generate images. An automatic evaluation framework assesses the faithfulness of these images to user intentions using a Visual Question Answering (VQA) model. The VQA model checks if the generated images satisfy the user's intended questions, determining the faithfulness of the image generation.

Key findings from the study include the observation that different types of ambiguities are resolved with varying effectiveness. The framework generally improves the faithfulness of image generation. Additionally, the automatic evaluation framework aligns well with human evaluations, suggesting its reliability for assessing text-to-image models.

Overall, the work contributes to understanding and mitigating ambiguities in text-to-image models, proposing both mitigation and evaluation frameworks to enhance the alignment of generated images with user intentions.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The work "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" by Shen Gao and colleagues from Shandong University addresses the challenge of summarizing dialogues by distilling key information into concise summaries. Dialogue summarization is crucial for quickly understanding the main points of complex, multi-participant conversations without reviewing the entire dialogue. Traditional methods rely on pre-computed static graph structures using external linguistic tools, which can be unreliable and inflexible for dynamic summarization tasks.

The proposed SDDS model overcomes these limitations by integrating both static and dynamic graph structures. It consists of four main components: an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator. The Utterance Encoder transforms dialogue utterances into vector representations. The Static-Dynamic Graph module combines multiple static graphs, such as Discourse Parsing Graphs and speaker interaction graphs, with a dynamic graph that captures semantic relationships using a multi-head attention model.

The static graphs are constructed using heuristic methods, including discourse parsing, key co-occurrence, speaker interaction frequency, and relative distance between utterances. These graphs are fused using a 1x1 convolutional layer to create a unified representation. The dynamic graph, on the other hand, is built without pre-computed structures, relying on deep vector representations to model semantic relationships.

The integration of static and dynamic graphs is achieved through a fusion method that combines their respective matrices into a unified graph. This graph is then incorporated into the generation process using a dual cross-attention mechanism, enhancing the model's ability to capture dialogue structure information.

The SDDS model effectively addresses the drawbacks of existing methods by dynamically adapting to the summarization task and reducing dependency on external tools. The code and data for this model are available on GitHub, facilitating further research and application.</sample>
    <sample id="158">Qipeng Guo from AWS introduced the "Dual Cache for Long Document Neural Coreference Resolution" work, focusing on the coreference resolution task, which involves identifying and clustering mentions of the same entity within a document. Traditional methods for this task have quadratic complexity due to the need to enumerate all mention pairs, leading to high computation and memory demands. Recently, cache-based methods have been developed to reduce this complexity to a linear level by using a fixed-size cache. However, these methods often use a Least Recently Used (LRU) eviction policy, which can result in high cache misses in long documents where topics frequently change, causing mentions of entities to be scattered.

To address this issue, Guo proposed a dual cache system comprising a local cache and a global cache. The local cache, using an LRU policy, stores entities relevant to the current local context, while the global cache, using a Least Frequently Used (LFU) policy, stores entities that are mentioned globally across the document. The dual cache system operates by scanning the document from left to right, classifying new mentions as either new entities or belonging to existing ones in the cache, and then evaluating their frequency. High-frequency entities are added to the global cache, while others go to the local cache. When caches are full, entities are evicted based on their respective policies.

The dual cache was evaluated on four public benchmarks, including LitBank, OntoNotes, and WikiCoref, with the former two containing training data and the latter not. Results showed that dual cache outperformed baseline methods even when they had unbounded memory, particularly in scenarios without training data. Additionally, a book with 30,000 words was annotated to demonstrate the dual cache's effectiveness, revealing a significant performance gap between the baseline and dual cache for book-level documents. The dual cache also significantly reduced cache misses compared to single cache methods.

In conclusion, the dual cache system effectively separates local and global entities into distinct caches, outperforming single cache methods and reducing cache misses. It offers the highest performance/cost ratio among cache-based models, making it a cost-effective solution for long document coreference resolution.</sample>
    <sample id="159">你好，大家。我是Koustav Sinha，很高兴欢迎大家参加我们关于ACL 2023论文的讨论。语言模型的可接受性判断在上下文中并不总是稳健的。这是我与John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy和Adina Williams的合作项目。在这项工作中，我们重新审视了最小对比范式。最小对比范式基本上是通过可接受性判断来评估语言模型的，这些判断还包括语法性，如BLiMP、SyntaxGym，或者在刻板印象方面的可接受性，如CrowS对。在这个范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个可接受的句子或不符合语法的句子。希望模型能够给可接受的句子更高的概率。当前的最小对比范式（MPP）流程并不能让我们评估模型对较长句子的接受程度。近年来，大型语言模型的上下文窗口越来越长。因此，评估模型在整个上下文窗口中的可接受性是至关重要的，这就是我们在这里尝试做的。我们通过要求模型评估越来越长的序列来重新审视MPP流程。具体来说，我们通过重新审视数据集来模拟这些较长的序列，并通过从这些数据集中选择可接受或不可接受的句子来重新创建句子。例如，我们从BLiMP数据集中选择了一个典型的语法性对比，即附加岛情况。我们通过从附加岛中提取语法正确的句子，并将其作为前缀添加到可接受查询和不可接受查询中，来重新创建较长的可接受序列，同时保持相同的语法结构匹配。我们也可以通过选择不匹配的句子来测试模型的可接受性，这些句子来自不同的子集或数据集。这种情况我们称为不匹配情景。在这种情况下，句子仍然来自相关的数据集，但不是用于评估的同一数据集。我们也可以对不可接受的情况做同样的事情。最后，我们可以从完全不相关的领域，如维基百科中选择句子。这将告诉我们模型的可接受性判断是否受到上下文的影响，无论上下文是来自数据集的不同子集，还是完全与当前句子无关。模型表现如何呢？首先，我们看看来自维基百科的句子，这些句子与当前查询对完全无关。在这里，我们发现MPP判断在任意上下文长度下大多数情况下是稳健的。我们将上下文长度增加到1024，以最大化OPT和GPT-2模型。在这里，MPP判断相对稳定（如橙色虚线所示）。那么，当我们选择来自同一数据集的句子时会发生什么呢？我们通过从BLiMP或SyntaxGym数据集的可接受和不可接受领域中创建句子来观察这一点。在这里，我们看到当添加可接受前缀或不可接受前缀时，MPP判断显著增加或减少。但是，当我们匹配结构时，即从BLiMP或SyntaxGym中选择相同现象的句子时，我们看到MPP判断根据所选择前缀的可接受性或不可接受性显著增加或减少。这种效应非常大，并且随着上下文长度的增加而增加，这可能会影响具有大上下文窗口的新语言模型。为什么匹配前缀会对语言模型判断产生如此大的影响呢？我们进行了一系列分析，尝试通过保留相关结构并向输入中添加噪声来扰动输入句子。在进行多次这样的扰动后，我们发现这些噪声并没有让模型改变其在MPP判断中的表现。基本上，我们发现模型对扰动句子的敏感性与可接受领域中的扰动句子相似，即当我们扰动可接受领域的句子时，我们在所有扰动中看到类似的增加，当我们扰动不可接受领域的句子时，我们在MPP判断中看到类似的减少。我们工作的关键结论是，语言模型对跨句子共享的潜在句法和语义特征敏感。目前我们用短句和单个输入进行的MPP评估可能无法完全捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文以获取更多实验细节。谢谢大家的聆听。</sample>
    <sample id="160">第一步将输入词元映射到一个无序的多集（unordered multiset）的输出词元。</sample>
    <sample id="161">CoScript 中包含了 55,000 个脚本。</sample>
    <sample id="163">DEPLAIN 的最佳对齐方法是 MASSalign。</sample>
    <sample id="164">Weakly Supervised Learning (WSL) offers the benefit of reducing the cost and effort associated with manually labeling data by using weaker, noisier annotations from sources like heuristic rules, knowledge bases, or low-quality crowdsourcing. This makes it cheaper compared to human annotations. However, the challenge is that these annotations are noisy, and directly training neural networks on them can lead to poor generalization due to label noise. WSL aims to address this by developing training algorithms that can robustly handle label noise and still generalize well.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presented a paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." The paper addresses abductive reasoning, which involves identifying plausible explanations to bridge the gap between a given context and an outcome. Zhao illustrated this with an example: Emily was stuck in traffic (context) and made it to her flight (outcome), with possible explanations being either her flight was delayed or it left on time. The goal is to find a plausible explanation that connects the context to the outcome.

Current methods for abductive reasoning rely on supervised learning, which requires annotated plausible explanations. However, these annotations can be subjective and inconsistent, as evidenced by a study showing 60% disagreement among crowd workers on explanations. Zhao's paper questions whether it is possible to learn abductive reasoning without supervision on the plausibility of explanations and answers affirmatively with the introduction of LiPoR (Likelihood Learning with Posterior Regularization).

LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context without needing to know which explanations are plausible. However, to prefer plausible explanations, LiPoR incorporates a regularizer based on the mutual exclusivity of explanations. This means that if one explanation is true, others are automatically ruled out. The regularizer, denoted by Omega, minimizes the entropy of the probability distribution over explanations, preferring a subset of explanations when more than the plausible number receive probability mass.

The paper demonstrates LiPoR's effectiveness on the AlphaNLI dataset, a widely-used abductive reasoning dataset, where it outperforms zero-shot models and the previous best unsupervised approach, including a strong GPT-3 baseline, by over 4 absolute points in accuracy. This work highlights a novel unsupervised approach to abductive reasoning that leverages the mutual exclusivity of explanations to improve performance.</sample>
    <sample id="166">The presentation introduces a novel framework, "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text," developed by Yunxin from Harbin Institute of Technology, Shenzhen. This framework addresses the challenge of retrieving images from complex textual descriptions, where traditional visual language models struggle due to their reliance on analogical reasoning (System 1) and the complexity of the task requiring logical reasoning (System 2).

The proposed method integrates the Divide-and-Conquer strategy with Dual-Process Theory. The Divide-and-Conquer strategy breaks down complex problems into simpler sub-problems, while Dual-Process Theory distinguishes between System 1 (analogical reasoning) and System 2 (logical reasoning). The framework consists of three main components: the Proposition Generator, the Visual-Linguistic Interactor, and the Neural-Symbolic Reasoner.

The Proposition Generator decomposes complex text into simpler propositions, using BART's decoder to generate corresponding sentences. The Visual-Linguistic Interactor, akin to System 1, facilitates interaction between visual and linguistic information, producing matching scores and reasoning states. The Neural-Symbolic Reasoner, representing System 2, integrates these states to derive the final solution, employing a negation executor and conjunction operation to handle complex reasoning.

Experimental results demonstrate that the proposed method, NDCR, outperforms existing baselines, with ablation experiments confirming the effectiveness of each module. The framework's ability to present intermediate inference states and results highlights its interoperability. The presentation concludes with suggestions for future research, emphasizing the potential of neural symbolic calculation and the integration of Dual-Process Theory with Divide-and-Conquer strategies to enhance reasoning in large language models.</sample>
    <sample id="167">DEPLAIN-web 中的文档部分手动对齐，部分使用自动对齐方法对齐。具体分配情况没有详细说明，只是提到了使用了两种方法。</sample>
    <sample id="168">CoNLL++ 数据集是从 2020 年的 Reuters 新闻中收集的，并使用 CoNLL-2003 的相同注释指南进行注释。</sample>
    <sample id="169">David Vilar reviews the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," a collaborative effort with Google Translate colleagues. The paper explores the use of the PaLM large language model, with 540 billion parameters, for machine translation. PaLM, trained on 780 billion tokens, excels in numerous NLP tasks. This study is the first systematic examination of prompting strategies for machine translation using large language models.

The research evaluates PaLM's translation capabilities using best practices from the machine translation (MT) community, including the latest test sets to prevent data overlap. The performance is compared against state-of-the-art systems, specifically the WMT evaluation benchmarks, using neural MT metrics and human evaluations via the MQM framework.

A key finding is the significant impact of prompting strategies on translation performance. An experiment with one-shot prompting showed a notable difference in BLEURT scores, highlighting the importance of selecting effective prompts. The study found that a 5-shot prompting strategy, where sentences are marked with their respective languages, yielded consistent results, with the form of prompting being less critical than the quality of examples provided.

The results emphasize that example quality is more crucial than similarity to the source sentence. Using high-quality translations from the development data, which is more curated than training data, improved performance. Despite PaLM's close performance to commercial systems like Google Translate, specialized state-of-the-art systems still outperform it. Human evaluations revealed that while PaLM's fluency is on par with state-of-the-art systems, its accuracy suffers, particularly with omission errors. PaLM tends to produce fluent translations, sometimes at the cost of omitting parts of the source text. However, it scores lower in the "Style/Awkward" category, indicating its output is generally fluent but occasionally inaccurate.</sample>
    <sample id="170">你好大家，我是来自宾夕法尼亚州立大学的尤森·张。今天我将介绍我们的工作“XSemPLR：多语言自然语言和意义表示的跨语言语义解析”。语义解析是构建用户查询（如SQL和Lambda演算）的语义表示的任务。跨语言语义解析的任务是将多种自然语言中的查询翻译成多种意义表示。如图所示，我们需要使用神经模型将多种自然语言中的查询翻译成SQL、Lambda或FunQL等。现有的跨语言语义解析模型是分别提出并在有限任务和应用的数据集上评估的。例如，某些自然语言有很多覆盖，但中文缺失，某些意义表示也缺失，例如Lambda演算缺失，或者只在某些神经模型上进行评估。例如，只有一个单一模型进行评估。因此，我们提出了XSemPLR。我们为多语言和多种意义表示的跨语言语义解析提供了一个统一的数据集XSemPLR。它包含9个不同领域的数据集、5个语义解析任务、8种意义表示和15个语言家族中的22种自然语言。为了更好地评估我们的基准，我们考虑了六种训练和评估的设置。第一个是Translate-Test。我们使用谷歌翻译API将源语言翻译成目标语言，然后使用单语模型进行训练和评估。例如，我们在英语查询上训练英语模型，在推理时，我们使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。我们还测试单语模型。在这种设置中，源语言与目标语言相同，例如德语到德语或英语到英语。我们还测试单语少样本设置，通过使用10%的训练数据训练单语模型。我们测试多语言模型，即我们为所有语言训练一个多语言模型。例如，我们将德语、英语、中文查询放在一起训练一个多语言模型。在推理时，我们可以使用这个模型将德语查询或中文查询等翻译。我们还考虑了跨语言零样本和少样本迁移。我们在一个源语言上训练，然后迁移到另一种语言。因此，在训练时，我们在英语查询或英语和德语少样本查询的组合上训练一个多语言模型，以预测SQL输出。我们还发现了许多有趣的结果。关于单语模型的分析，我们在两组模型上进行评估，包括Encoder-PTR，即多语言预训练编码器与指针式解码器，如XLM-R + PTR和mBERT + PTR。我们还评估Encoder-Decoder模型，即多语言预训练编码器-解码器模型，如mBART和mT5。我们发现Encoder-Decoder在所有九个数据集上获得了最佳性能。我们在多语言设置下评估mT5和XLM-R + PTR。我们发现Encoder-Decoder或Encoder-PTR可以通过在多种语言混合中训练来改进。我们发现大多数主要自然语言可以获得性能提升，除了英语在七个数据集中性能下降，在三个数据集中性能提升。我认为这是“多语言诅咒”。我们还比较了跨语言性能差距。在这个图中，蓝线是跨语言少样本迁移，橙线是跨语言零样本迁移，绿线是单语设置。我们发现，通过比较绿线和橙线，我们发现零样本设置下跨语言迁移性能差距显著，然后比较蓝线和橙线，我们发现少样本设置下迁移差距迅速缩短。我们还发现了一些其他有趣的发现。例如，Encoder-Decoder优于以前的工作或获得了可比的结果。在英语自然语言上的预训练可以显著提高目标自然语言的少样本性能，我们发现多语言语言模型如Codex和BLOOM仍然不足以应对跨语言语义解析任务。总之，我们构建了XSemPLR，一个多语言自然语言和意义表示的跨语言语义解析的统一基准。我们对三种代表性的多语言语言模型进行了全面的基准研究。我们的结果显示了许多有趣的发现。欢迎访问我们的论文和代码。谢谢大家的聆听。</sample>
    <sample id="171">Existing works on protecting the copyright of embedding as services can be broadly classified into four categories. However, these methods either are not applicable to embedding as services or lack transferability. The paper proposes a new method called Embedding Marker, which is a backdoor-based watermark method applicable to embedding as services.</sample>
    <sample id="172">Codex 或 Bloom 等多语言 LLM 对于跨语言语义解析（CLSP）任务来说仍然不够。</sample>
    <sample id="174">Thea introduces the "ArgAnalysis35K" dataset, highlighting its uniqueness in the field of argument quality analysis. This dataset is designed to evaluate the quality of arguments on a scale from 0 to 1, with higher scores indicating more coherent and persuasive arguments. Unlike existing datasets, which often suffer from issues like low quality, lack of diversity, and insufficient depth, ArgAnalysis35K addresses these problems through several innovative features.

Firstly, ArgAnalysis35K is the largest dataset in this field, comprising 35,000 argument-analysis pairs. The dataset prioritizes high-quality arguments sourced from speeches at prestigious tournaments, expert debaters, and intermediate debaters, with a smaller portion from novice debaters and the general public. This approach ensures a higher quality of arguments compared to those sourced from crowdsourcing platforms.

Secondly, the dataset offers a diverse range of arguments by focusing on 24 themes rather than a limited set of motions. These themes are derived from experience in debate circuits, websites like Hellomotions.com, and expert advice, capturing a wide array of motions for each theme. This diversity better reflects the variety encountered in parliamentary debates.

A key innovation in ArgAnalysis35K is the introduction of "analysis" as a distinct concept. Unlike traditional datasets that only include arguments, this dataset incorporates analysis, which can be a combination of claims, premises, or both. This addition provides a more comprehensive explanation of arguments, enhancing their clarity and persuasiveness.

The dataset also introduces instance-based annotator reliability. Recognizing that annotators may have biases on certain topics, the dataset evaluates annotator reliability on an argument-by-argument basis. This method allows for the retention of valuable judgments while eliminating biased ones, improving the overall reliability of the dataset.

Lastly, ArgAnalysis35K features a relevance model, which assigns a relevance score from 0 to 1 for each argument across different themes. This model acknowledges that arguments can be applicable to multiple topics, such as the importance of accountability in various contexts. By capturing the relevance of arguments to specific themes, the dataset provides a more nuanced understanding of argument applicability.

Overall, ArgAnalysis35K stands out for its large size, high-quality arguments, thematic diversity, innovative analysis concept, instance-based reliability, and relevance model. These features collectively enhance the dataset's utility for argument quality analysis, making it a valuable resource for researchers and practitioners in the field.</sample>
    <sample id="175">该方法通过引入一个灵活的方法来预测排列，不对可能的排列施加硬性限制。具体来说，它使用一个GPU友好的连续松弛来近似找到最高分排列的过程，这个过程与“旅行推销员”问题相关，是NP难的。这种方法允许通过解决方案进行反向传播，从而学习更符合语言学的排列。</sample>
    <sample id="176">下游 NLP 模型的公平性可以定义为模型在处理不同人群、社会群体或政治观点时表现出的一致性和公正性。具体来说，公平性意味着模型在执行任务（如仇恨言论检测或虚假新闻检测）时，不会因为输入数据中的社会偏见或政治倾向而对某些群体产生不利影响。这包括确保模型在不同人群或政治立场的数据上表现一致，不会因为模型的政治倾向而对某些群体进行歧视或忽视。在研究中，通过分析模型在不同政治倾向的数据上的表现，可以揭示其潜在的公平性问题。</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar presents research on improving Theory of Mind (ToM) reasoning in large language models (LLMs) through a method called SymbolicToM. Theory of Mind involves understanding the mental states of others, often tested through false-belief tasks like the Sally-Anne test. LLMs, including models like GPT-3, struggle with these tasks, prompting the question of how to enhance their ToM capabilities.

SymbolicToM is an inference-time method that uses explicit graphical representations to model the mental states of characters in a story. It generates multiple graphs to represent different belief levels, such as what one character believes about the world and what they think another character believes. These graphs are computed using natural language inference (NLI) and open information extraction (OpenIE) models.

The method involves pre-computing these graphs for a given story, allowing efficient answering of questions about characters' beliefs. For instance, to determine where Alice thinks Bob will search for an apple, the method detects entities in the question, retrieves the relevant belief graph, and transforms the question into a factual one over the graph. The final answer is obtained by feeding the graph-retrieved sentences and the factual question into a language model.

Experiments show that SymbolicToM significantly improves LLM performance on second-order false-belief questions. It outperforms supervised baselines like fine-tuned GPT-3 and Textual Time Travel, with notable accuracy gains across various models. For example, GPT-3-Davinci saw a 65-point increase, and Flan-T5-XXL gained 51 points.

To test generalization, Sclar designed new datasets modifying the ToMi benchmark. These datasets assess storage structure and linguistic generalization. While supervised models showed performance degradation, SymbolicToM maintained significant improvements, enabling models like GPT-4 to fully solve the datasets. For instance, it provided a 42-point accuracy boost on the D₁ dataset.

In conclusion, SymbolicToM is a plug-and-play method that enhances ToM reasoning in LLMs by using symbolic graphical representations. It avoids overfitting, offers interpretable reasoning, and outperforms supervised approaches in out-of-domain story understanding and linguistic diversity tasks. For further details, the paper is recommended.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">The paper "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" by Siyu Yuan from Fudan University addresses the challenge of planning actions with specific constraints using large language models (LLMs). While previous research has focused on abstract goal planning, such as "make a cake," this work explores the less-studied area of planning with specific constraints, like "make a chocolate cake." The authors define constrained language planning as the task of generating scripts that adhere to specific constraints while maintaining semantic completeness.

The study evaluates the constrained planning abilities of LLMs and finds them lacking in faithfulness to constraints, despite acceptable semantic completeness. To address this, the authors propose an over-generate-then-filter method. This involves generating multiple scripts for specific goals using InstructGPT, followed by filtering to select scripts that best match the constraints. The filtering process uses cosine similarity between script and goal embeddings and rewards scripts containing keywords related to the target constraint.

To support this research, the authors create the CoScript dataset, which contains 55,000 specific goals with scripts, generated by LLMs. The dataset is validated and refined by crowd-sourced workers to ensure quality. CoScript demonstrates a diverse range of specific goals and constraints, making it a valuable resource for training smaller, specialized models.

The paper highlights that smaller models, like T5 fine-tuned on CoScript, can outperform larger models in generating high-quality scripts when trained on suitable datasets. This work establishes the constrained language planning problem, improves LLMs' planning abilities, and provides a high-quality dataset to advance research in this field.</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 意味着将拉丁裔女性描绘为“活泼”和“曲线优美”，这些词汇与将她们与热带地区相关联的刻板印象相连，强化了一种刻板化的视角。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。具体方法是给语言模型提供指令，让它生成一个“人物形象”，例如使用提示“想象你是一个亚洲女性。描述一下自己。”这种方法可以很容易地适用于任何身份标识，因为只需在提示中指定所需的身份标识即可。这些生成的人物形象可以直接与人类写的回应进行比较，以识别和分析潜在的刻板印象。</sample>
    <sample id="184">本文中使用了CXMI（Contextual Mutual Information）来衡量语境使用情况，并将其扩展为Pointwise CXMI（P-CXMI），以便在句子级或词级上衡量语境使用情况。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源。DrBERT 是基于 RoBERTa，使用 NACHOS 数据集进行训练，该数据集是从互联网上抓取的医学数据。而 ChuBERT 则是基于匿名数据，这些数据来自南特大学医院的数据仓库。此外，DrBERT 是一个从头开始训练的模型，而 ChuBERT 也进行了从头开始的训练，但使用的是临床笔记中的数据。</sample>
    <sample id="187">这篇论文有两位作者，Ying和Zhiyang。</sample>
    <sample id="188">迭代迁移学习是一种策略，其中模型首先通过从相关任务（如CE任务和辩论任务）迁移权重来启动活动学习过程。在这个过程中，模型首先在CE任务上进行微调，然后在辩论任务上进行进一步微调，以获得更好的零样本性能。这种方法有助于在数据稀缺的情况下提高模型的初始性能，从而更有效地进行活动学习。</sample>
    <sample id="189">数据集的目标是理解用户在选择过程中使用的间接指代表达，以便在对话系统中更自然地处理用户的选择。它旨在解决间接指代表达的理解问题，并为评估大型语言模型（LLMs）的实体理解能力提供基准数据集。AltEntities Corpus通过使用来自音乐、书籍和食谱领域的数据，收集了6,000个替代问题和42,000个间接指代表达，以帮助实现这一目标。</sample>
    <sample id="190">攻击者可以通过学习从嵌入服务（Embedding as Services, EaaS）中获取的嵌入来提取模型参数。具体来说，攻击者可以使用从 EaaS 获取的嵌入数据来训练一个新的模型，使其尽可能地模仿原始服务的输出。这种方法允许攻击者在不直接访问原始模型的情况下，通过观察和学习嵌入的行为来重建或复制原始模型的功能。</sample>
    <sample id="191">这篇论文有三位作者：Sara Papi, Matteo Negri, 和 Marco Turchi。</sample>
    <sample id="192">Yang Luo presents "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of designing an optimizer that achieves both fast convergence and low memory usage. Traditional adaptive methods like Adam require significant memory for maintaining gradient moment estimates, while memory-efficient optimizers like Adafactor reduce memory usage but often at the cost of performance. The presentation introduces CAME, which aims to balance these two goals.

The concept of non-negative matrix factorization (NMF) is discussed, highlighting its memory reduction capabilities. Adafactor, which uses NMF, achieves memory efficiency but suffers from slow convergence due to erroneous updates. CAME addresses these errors by considering the residual between momentum and current updates, using this instability to guide more adaptive optimization steps.

Experiments conducted on datasets like BookCorpus and English Wikipedia demonstrate CAME's effectiveness. Compared to Adam and Adafactor, CAME shows significant improvements in validation accuracy and performance in training large language models such as BERT, GPT-2, and T5. Specifically, CAME increases validation accuracy by about 3.4% over Adafactor and outperforms Adam in pre-training large models, all while reducing memory costs significantly as batch sizes increase.

CAME's efficiency is further validated by comparing BERT-based models trained with it against baselines on downstream tasks, showing comparable performance with reduced memory usage. The optimizer's memory efficiency is highlighted by its lower memory footprint compared to Adam, LAMB, and SM3. CAME's confidence-guided approach, which adapts updates based on the residual between predicted and generated updates, proves effective in large language model training and large batch training scenarios. Overall, CAME offers a promising solution for optimizing large language models with both speed and memory efficiency.</sample>
    <sample id="193">初始数据集的创建使用了43个例子进行注释。</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="195">The paper introduces "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" (RoHT), a novel framework designed to enhance explainable question answering (XQA) by addressing limitations in existing methods. XQA aims to not only provide answers but also explain the reasoning behind them. Current approaches fall into two categories: neuro-symbolic methods, which convert questions into formal representations like SPARQL for execution on structured knowledge bases (KBs), and decompose-based methods, which generate natural language intermediate steps. However, neuro-symbolic methods struggle with incomplete KBs, limiting answer recall, while decompose-based methods face challenges due to the diversity of natural language and reliance on free-text corpora.

RoHT addresses these challenges by integrating knowledge from heterogeneous sources, leveraging question decomposition to select appropriate knowledge sources for sub-questions. The framework consists of two main stages: building a Hierarchical Question Decomposition Tree (HQDT) and conducting probabilistic reasoning over it. The HQDT represents the compositional structure of a complex question, with the root node as the original question, non-root nodes as sub-questions, and leaf nodes as atomic questions. Probabilistic reasoning is performed recursively from the root to the leaves, involving three steps: scheduling appropriate knowledge sources, executing queries to obtain answers with probabilities, and aggregating candidate answers to output the most probable ones.

RoHT was evaluated on two datasets: KQA Pro, a KB QA dataset with incomplete KBs supplemented by Wikipedia, and Musique, a QA comprehension dataset supplemented by Wikidata. Results showed that RoHT outperformed existing methods by effectively integrating answers from sub-questions at different levels and utilizing both KB and text sources. On KQA Pro, RoHT significantly improved performance over methods using only incomplete KBs or end-to-end training with mixed relation graphs. On Musique, RoHT achieved higher F1 scores compared to state-of-the-art methods, demonstrating the benefits of supplementing text information with KB knowledge. Overall, RoHT's explicit decomposition and integration of diverse knowledge sources highlight its superiority in explainable question answering.</sample>
    <sample id="196">以左侧为支配词的示例是“我看见巴特和丽莎”（I saw Bart and Lisa）。在这个例子中，动词“看见”（saw）位于左侧，支配着“巴特和丽莎”（Bart and Lisa）的并列结构。</sample>
    <sample id="197">所给的英文内容中并未具体列出对话系统中的最先进模型名称。内容提到了四个“state-of-the-art chat models”被用于评估，但没有具体命名这些模型。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为当前的最小对比范式（MPP）无法评估模型对较长句子的可接受性。随着大型语言模型的上下文窗口变得越来越长，评估模型在整个上下文窗口中的可接受性判断变得至关重要。这可以帮助我们了解模型在更长的上下文中的抽象知识和对语法和语义特征的敏感性。</sample>
    <sample id="199">是的，多语言训练会导致表现下降，这被称为“多语言诅咒”。在分析中，大多数主要自然语言在多语言训练中表现提升，但英语在七个数据集中的表现下降，只有在三个数据集中有所提升。</sample>
    <sample id="200">No, the annotators do not necessarily know about the entities beforehand. They are shown some background knowledge about the two entities, such as Google search links for songs, Wikipedia text for books and recipes, and images for recipes, to help them make an informed choice.</sample>
    <sample id="201">评估使用了 state-of-the-art, neural MT metrics 和 expert-based human evaluation results。</sample>
    <sample id="202">在这篇演讲中，泛化中的回归（adaptive overfitting）并没有影响特定的 NER 类型。研究发现，适应性过拟合在这种情况下并未观察到，因为在 CoNLL-2003 和 CoNLL++ 之间的改进没有显示出递减收益。相反，性能下降主要是由时间漂移（temporal drift）引起的。</sample>
    <sample id="203">NLP 中的立场很重要，因为它影响了技术的设计和应用，可能导致系统性的性能差异。立场是指由于人们的人口统计、身份和生活经验而形成的观点。这些观点可以影响研究人员和模型开发者的决策，从而影响研究结果。如果数据集和模型反映了某些立场而忽视了其他立场，可能会导致对特定人群的不公平对待。例如，某些模型可能对某些文化背景中的冒犯性术语敏感度较低。因此，理解和解决 NLP 中的立场问题对于确保技术的公平性和包容性至关重要。</sample>
    <sample id="204">The content does not specify whether BLOOM or similar multilingual LLMs are fine-tuned using adapter fine-tuning or full fine-tuning.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented research on the political biases in language models and their implications for fairness in NLP applications. The study investigates how political biases propagate from pretraining data to language models and downstream tasks. Language models are trained on large-scale web crawl data, which includes significant coverage of political news media like the New York Times and The Guardian. This coverage results in both benefits and challenges: while models learn from diverse perspectives, they also inherit social biases that can lead to fairness issues.

The research addresses two main questions: how to evaluate the political leanings of language models and the impact of these leanings on downstream tasks. To assess political biases, the study uses political questionnaires, such as the political conference test, to prompt language models. Preliminary results show that models like GPT-4 exhibit liberal biases, with GPT models generally more liberal than BART models. The study further explores how these biases originate from training data by conducting controlled experiments with partisan corpora, revealing shifts in ideological coordinates based on the data used.

The research also examines societal polarization by pretraining models on data from before and after the 45th U.S. president, finding that models trained on more recent data exhibit stronger political leanings. In evaluating the impact on NLP applications, the study focuses on hate speech and fake news detection. It finds that left-leaning models are better at detecting hate speech against minority groups but less effective against powerful groups, while right-leaning models show the opposite pattern. Similar trends are observed in fake news detection, with models better at identifying misinformation from opposing political leanings.

The study highlights a pressing fairness issue: deploying politically biased models could marginalize certain groups and allow unchecked hate speech. It underscores the dilemma of balancing bias mitigation with the risk of censorship. The research calls for acknowledging and addressing these fairness issues in language model development.</sample>
    <sample id="206">他们使用了两个任务的模型进行迁移学习：1. 主题独立的对立立场分类（Debate），2. PDTB中的扩展和比较类别（CE）。他们首先从这两个任务中迁移权重，然后通过迭代微调CE任务后再微调Debate任务来启动主动学习。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集是最新的 WMT（翻译工作坊）评估测试集。</sample>
    <sample id="208">作者最终提出了三条建议。</sample>
    <sample id="209">The proposed method greatly improves the planning ability of InstructGPT in both semantic completeness and faithfulness to constraints. Specifically, the T5 model fine-tuned on the CoScript dataset can generate scripts of higher quality than most large language models, indicating significant gains over the strongest baselines.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">是的，论文中的结果和数据集可以用作基准。DEPLAIN数据集提供了手动对齐的句子对，用于评估自动对齐方法，特别是在德语文本简化的背景下。此外，通过对长mBART和基础mBART模型进行微调，研究人员提供了自动文本简化的基准结果，这可以作为未来研究的基准。</sample>
    <sample id="212">The paper mentions that they fine-tuned T5 on the CoScript dataset and found that it could generate scripts of higher quality than most large language models. However, it does not specify the number of different smaller models tested, only mentioning T5.</sample>
    <sample id="213">OFA (Unified Multi-Modal Pre-trained Model) was used as the base model for investigating multi-modal instruction tuning.</sample>
    <sample id="215">Adam Przepiórkowski's talk focuses on the dependency structure of coordination, exploring different theoretical approaches to how conjuncts in a coordinate structure are linked. He discusses asymmetric approaches, such as Universal Dependencies and Igor Mel'čuk's Meaning-Text Theory, where the first conjunct is the head of the structure. In contrast, the Prague Dependency Treebanks use a conjunction-headed approach, and Hudson's Word Grammar proposes a multi-headed approach where all conjuncts are heads.

The paper argues for symmetric structures of coordination, challenging the asymmetric models. The argument is based on the principle of dependency length minimization, which suggests that shorter dependencies are preferred. Przepiórkowski illustrates this with examples showing that while direct objects typically stay close to verbs, longer direct objects can be positioned after adjuncts without causing awkwardness, as this minimizes dependency length.

The paper uses data from the enhanced Penn Treebank to show that left conjuncts tend to be shorter, especially when the length difference between conjuncts increases. This tendency is observed when the governor is on the left or absent but disappears when the governor is on the right. This finding supports symmetric structures, as it indicates that the position of the governor influences conjunct length preferences, challenging asymmetric models. The full arguments and data are detailed in the paper, which Przepiórkowski invites readers to explore further.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He addresses the challenge of generating dialogues that are controllable across multiple attributes. Traditional methods focus on single attributes, which limits their practical application in scenarios requiring multi-attribute control. The authors propose a novel approach, Disentangled Controllable Generation (DCG), which learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations effectively.

The DCG model is built on the DialoGPT framework and incorporates a compositional prompt module. It uses two types of prompts: attribute-oriented prompts, which guide the model to focus on specific information in the dialogue, and task-oriented prompts, which leverage global features for dialogue response generation. These prompts are concatenated to form whole prompt embeddings, enhancing the model's ability to distinguish between different attribute value combinations.

To address the lack of a unified evaluation metric for multi-attribute controllable dialogue generation, the authors introduce a reference-free evaluation framework called MAE. This framework is designed to evaluate different granularities of attributes without requiring large-scale labeled data. The authors establish two benchmarks and demonstrate the effectiveness of their method and evaluation metrics through experiments.

The results show that DCG outperforms existing baselines in terms of attribute controllability and text equality, particularly for unseen attribute combinations. The model achieves this by effectively using attribute-oriented and task-oriented prompts, along with disentanglement learning, to improve compositional generalization. The authors also validate the effectiveness of their evaluation framework, MAE, which outperforms classic metrics in correlation with human judgments.

Overall, the paper presents a comprehensive solution to the problem of multi-attribute controllable dialogue generation, highlighting the importance of compositional generalization and the effectiveness of the proposed DCG model and MAE evaluation framework.</sample>
    <sample id="218">Google Translate。</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica, along with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang, presented their work on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." The research focuses on analyzing Form 10-K reports, which are annual reports required by the SEC, to extract useful financial information. The motivation for this work stems from the observation that these reports are highly similar year-over-year, with about 80% of tokens being the same, making it challenging to mine new insights.

The team introduced a highlighting task to address this challenge, aiming to identify key words that signify changes or important information between consecutive reports. The task involves comparing a target report with its previous year's reference report to determine word importance. The proposed multistage pipeline includes document segmentation, relation recognition, and two fine-tuning stages: out-of-domain and in-domain.

In the relation recognition stage, pairs of text segments are classified into three types: highly similar pairs (Type β), revised pairs with different meanings, and mismatched pairs indicating new information. The model is fine-tuned using an external dataset, eSNLI, for out-of-domain training, and revised pairs for in-domain training, employing soft labeling techniques to improve pseudo-label quality.

The evaluation of the model's performance is based on precision over recall and the correlation between predictions and annotations. The results show that the domain-adaptive highlighting model performs well on both the FINAL dataset and the eSNLI dataset, demonstrating its effectiveness and generalization capability. The research opens avenues for future improvements, such as enhancing effectiveness and incorporating additional features from information retrieval techniques. For more details, the team encourages referring to their paper and GitHub repository.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">The paper analyzed the translation from German into English.</sample>
    <sample id="222">The work titled "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" addresses the challenge of adapting open-domain QA models, typically trained on general-purpose datasets like Wikipedia, to answer questions in specialized domains such as biomedicine. The primary issue is that these models struggle to generalize to new domains due to differences in data distribution and content.

The study explores various data interventions to enhance model performance across different domains. It distinguishes between zero-shot and few-shot methods. Few-shot methods involve using a small number of examples from the target domain to generate additional data, improving both retriever and reader model performance. Zero-shot methods, on the other hand, involve manipulating the interactions among questions, answers, and contexts without using target domain examples. The study finds that changing question formats or varying answer and context distributions can help, with cloze-style questions being particularly effective.

The research also investigates the nature of dataset shifts between the source and target domains using a taxonomy of data shifts: no shift, concept shift, covariate shift, and full shift. Compatibility measures are developed to assess how well the source models perform on target datasets. The study maps datasets onto a 2D grid to identify the type of shift and determine the most effective data interventions for each.

Overall, the study demonstrates that specific data interventions can significantly improve reader performance, with improvements up to 24%. It highlights that the effectiveness of these interventions depends on the type of dataset shift present, with few-shot adaptations benefiting all target sets and zero-shot adaptations being particularly useful for concept and covariate shifts.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">在实验过程中研究了以下模型：

1. MASSalign：用于评估自动对齐方法，特别是在德语文本简化中。
2. long-mBART：用于生产文档级简化。
3. base mBART：用于生产句子级简化。</sample>
    <sample id="225">在 MultiInstruct 中，使用 53 个任务用于训练，其中包括 9 组任务，每个任务样本 10,000 个实例。测试时，保留了整个常识推理组用于测试，并从视觉问答（VQ）和杂项组中选择了额外的 5 个任务。此外，还从自然指令测试分割中随机抽取了 20 个任务作为未见的 NLP 任务。</sample>
    <sample id="226">The provided content does not specify the number of authors of the paper.</sample>
    <sample id="227">当前语言模型在自然语言处理（NLP）任务中取得了显著成功，但在基于环境的语言理解（grounded language understanding）方面仍面临挑战。这种理解涉及将自然语言表达映射到特定环境中可以执行的计划或程序。这对于智能助手、语义搜索、医疗数据库查询和遵循自然语言指令的家用机器人等应用至关重要。然而，由于大多数语言模型在预训练时缺乏基于环境的基础，这使得基于环境的语言理解尤其具有挑战性。预训练主要依赖文本语料库，而不是与特定环境交互，导致在应用中存在差距。

现有研究通常使用语言模型直接生成计划，但这种生成可能导致不合法或不可执行的计划。为了解决这一问题，提出了一个名为Pangu的新框架，该框架将语言模型的角色从生成转变为判别。在Pangu框架中，一个符号代理与环境交互并提出候选计划，而语言模型仅用于评分和排名这些候选项。这种方法减轻了语言模型处理计划有效性和语法的负担。

Pangu框架在知识驱动的问答任务中进行了实验，这是一个典型的基于环境的语言理解场景。实验使用了不同性质的语言模型，包括BERT、T5和大型语言模型如Codex，并在微调和上下文学习设置中进行了测试。Pangu在所有设置中表现出色，特别是在样本效率方面，例如，在上下文学习中，使用Codex时，Pangu仅需一个示例就能实现超过50%的GRAIL查询准确率，显著优于其他设置。此外，Pangu在使用不同语言模型的微调实验中，始终优于基线模型ArcaneQA。

Pangu的强大泛化能力可能源于其在非独立同分布（non-i.i.d.）设置下的鲁棒性，因为它在已见和未见结构上的概率分布相似，而自回归模型如ArcaneQA则倾向于过拟合训练中的结构。研究的关键信息是，对于基于环境的语言理解，生成可能不是最佳策略，而判别可能是更好的方法。研究人员欢迎讨论和合作，并希望听到对其工作的反馈。</sample>
    <sample id="228">AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present their research on improving argumentative writing through the detection and revision of claims. They emphasize the importance of text revision in achieving optimal phrasing, particularly in argumentative writing, where the choice of words can significantly impact the audience's response. The paper introduces two tasks: Suboptimal-Claim Detection, which determines if a claim needs revision, and Claim Improvement Suggestion, which identifies the types of quality issues to address.

The authors propose learning from human revision patterns rather than explicitly defining claim quality. They explore challenges in using revision-based data from collaborative online debate platforms like Kialo, focusing on argumentative texts. The paper identifies four main challenges: Representativity and Reliability, Model Complexity and Architecture, Contextual Information, and Topical and User Bias.

Representativity and Reliability concern compiling a dataset that accurately reflects argument quality, questioning whether final claim versions are truly optimal. Model Complexity and Architecture involve selecting models that align with revision ideas and are sensitive to small changes. Contextual Information addresses the need to determine relevant context for decision-making, as some quality dimensions depend on debate structure, parent claims, or domain knowledge. Topical and User Bias highlights noise in revision histories due to user and moderator biases, and the influence of social and cultural contexts on argument quality.

The paper concludes that revision-based data can effectively support the tasks, with modeling the distance between claim versions aiding in detecting suboptimal claims. The impact of contextual information varies depending on the task and the specific quality issues. For detailed analysis and findings, the authors invite readers to refer to their paper.</sample>
    <sample id="231">NACHOS 是一个用于训练 DrBERT 的数据集，包含从互联网上爬取的医学数据。</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi addresses the challenges of Simultaneous Speech Translation (SimulST), which involves translating spoken language into text in another language in real time. Current SimulST models face issues such as complex architectures, lengthy training procedures with multiple optimization objectives, and the need for different models to achieve various latency levels. The authors propose a novel solution using existing offline Speech Translation (ST) models without retraining or adopting specific architectures for SimulST. Their approach, called Encoder-Decoder Attention (EDAtt), uses a single model for all latency regimes, managing latency through specific parameters and leveraging the attention mechanism between audio input and textual output.

EDAtt determines whether to emit a partial translation based on the concentration of attention. A word is emitted if the attention is not concentrated, meaning its sum is below a threshold alpha over the last lambda speech frames, indicating stable information. For instance, if a speech chunk contains "I'm going to talk about..." and the model predicts a German translation, the cross-attention weights show that the first two words point to the earliest frames, while the last word points to the latest frames. The first two words are emitted, but the last word is withheld until more information is received. This process continues with subsequent speech chunks.

The main results of EDAtt are presented in graphs plotting BLEU scores (translation quality) against average lagging (latency) and computational-aware average lagging (considering model prediction times). The goal is to achieve high BLEU scores with low latency, ideally shifting the curves to the left. EDAtt outperforms popular strategies like Wait-k and Local Agreement when applied to offline models, as well as state-of-the-art architectures specifically designed for SimulST. The results demonstrate that EDAtt is the fastest strategy when considering actual elapsed time. The authors have made their code and models open source to facilitate reproducibility.</sample>
    <sample id="234">提示策略对大型语言模型（LLMs）在翻译任务中的性能有显著影响。在一项简单的实验中，使用一次提示（one-shot prompting）并提供两种不同的提示，发现516个句子中的大多数句子在BLEURT分数上的差异超过一点，极端情况下可达40点。因此，选择合适的提示策略很重要。在实验中，使用五次提示（5-shot prompting）策略，发现提示的实际形式对性能影响不大，而提示中的例子质量更为重要。</sample>
    <sample id="235">论文中没有提供作者所属机构的信息。</sample>
    <sample id="236">The English content does not specify what the 5 expert-written instructions are. It only mentions that each task in the MultiInstruct dataset is equipped with five expert-written instructions.</sample>
    <sample id="237">作者建议使用一个名为“KITMUS Test”的诊断测试套件来测试模型如何使用来自多种来源的信息。这个测试套件通过一个核心参照消解任务来探测模型在不同知识来源之间的整合能力。测试中设置了三种情境：“Background-Pretrain”（背景知识在预训练时可用）、“Background-Both”（背景知识在预训练和推理时都可用）和“Background-Inference”（两种知识类型仅在推理时可用）。通过这些设置，研究人员可以评估模型在不同知识可用性条件下的表现。</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset designed to aid in the development of meeting summarization technologies. The dataset addresses the challenges of creating high-quality meeting summaries and locating trustworthy public meeting resources. MeetingBank comprises 1,366 City Council meetings, totaling nearly 7,000 instances, with data including meeting transcripts, reference summaries, and additional resources.

The data collection process involves using the Speechmatics API to convert audio data into transcripts. Meetings are identified by a unique MeetingID, which helps locate corresponding reference summaries and meeting segments. The dataset provides detailed statistics, such as the number of meetings, meeting duration, tokens per meeting, speakers per meeting, and the period of meetings collected. It also includes summarization instances for each city, along with average sentence and token counts in source and summary texts.

Data analysis measures the level of abstraction in summaries using coverage and density scores. Coverage scores indicate the percentage of summary words appearing in source transcripts, while density scores assess the extent of extracted references. Most summaries have coverage scores between 0.7 and 0.9, suggesting a preference for verbatim points. Seattle and Boston have the highest density scores, indicating more editing, while Denver has the lowest.

For model evaluation, top-tier summarization systems were tested on MeetingBank's test set. Extractive summarizers like Oracle, LEAD, LexRank, and TextRank, along with neural abstractive models such as BART-Large, Pegasus, Longformer, DialogLM, and HMNet, were evaluated. GPT-3 was also tested for zero-shot summarization. Extractive systems, particularly Extr-Oracle, showed high ROUGE-2 scores, while DialogLM excelled among abstractive models. GPT-3 performed well in human assessments, particularly in fluency and coherence, but less so in informativeness and factuality.

The study highlights the need for meeting summarization solutions to focus on capturing main discussion points and suggests developing new automatic evaluation metrics aligned with human preferences. MeetingBank serves as a valuable resource for researchers to design advanced meeting summarizers and offers insights into City Council decision-making processes.</sample>
    <sample id="239">大家好，我是大卫·维拉，今天我将简要介绍我们与谷歌翻译同事合作的论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。PaLM是去年2022年发布的一个参数量为540亿的大型语言模型，它在780亿个标记的大量文本数据上进行了训练。在发表时，它在数百个NLP任务中达到了最先进的水平。在本文中，我们首次系统地研究了大型语言模型在机器翻译中的提示技术。我们使用机器翻译社区的最佳实践来评估这些模型的翻译能力，包括使用最新的测试集以避免测试数据与语言模型的训练数据重叠，并与最先进的系统进行比较，如WMT评估。我们使用最先进的神经机器翻译指标，并展示了基于专家的人工评估结果。最后，我们提供了一些提示选择策略的建议。提示对大型语言模型在翻译中的性能有很大影响，如我们的一个简单实验所示，我们使用一次提示，并为每个句子提供了两种不同的提示。在1000个句子中，516个句子的差异超过了1个BLEURT点，极端情况下可达40个BLEURT点。因此，选择一个好的提示策略很重要。在我们的实验中，我们选择了一个5次提示策略，即我们只是标记每个提供给系统的句子所在的语言。例如，在从德语翻译成英语的情况下，源句子用德语冒号标记，英语翻译用英语冒号标记。我们发现，当使用多个短提示时，提示的实际形式对性能影响不大。对于零和一次提示，这一点尤为重要。而在我们的情况下，当使用五次提示时，提示的实际形式几乎没有差异，提示中的例子承担了大部分重量。我们的实验结果总结如下：示例质量比源句子的相似性更重要。因此，选择高质量翻译的示例很重要。特别是，我们比较了从WMT评估的开发数据中选择提示与从训练数据中选择提示。开发数据比训练数据更加精心策划，质量更高，而训练数据更加嘈杂。因此，使用开发数据的结果表现更好。然而，专门的最先进系统在PaLM翻译方面仍然具有显著优势。但是，PaLM接近商业系统。在我们的情况下，我们选择与谷歌翻译进行评估。我们通过使用MQM框架进行的人工评估获得的见解表明，PaLM的流畅性与最先进系统相当，但主要差异在于准确性。特别是，最常见的错误是遗漏错误。这表明PaLM有时会通过省略源句子中的部分来产生更流畅的翻译。然而，PaLM的“风格/别扭”类别低于最先进系统，这是另一个信号，表明PaLM提供了非常流畅的输出，但仍然存在准确性问题。谢谢大家。对于更多细节，请参加完整的论文演示。谢谢大家。</sample>
    <sample id="240">你好，我是德国萨尔兰大学的博士生戴伟。在这个视频中，我想介绍我们最近的工作《比你想象的更弱：对弱监督学习的批判性分析》。这是与夏宇深、马里乌斯·莫斯巴赫、安德烈亚斯·施蒂芬和迪特里希·克拉科夫共同完成的工作。我想从对弱监督和弱监督学习的简要介绍开始。在弱监督中，我们不手动标注数据。相反，我们使用弱标注源对数据进行标注，例如简单的启发式规则、知识库或低质量的众包，如右侧图示。与人工注释相比，这些弱标注更便宜，但也更嘈杂，意味着一定比例的标注是错误的。如果我们直接在弱标注数据上训练神经网络，神经网络往往会记住标注噪声而无法泛化。在弱监督学习中，提出了训练算法，以在这种标注噪声下稳健地训练神经网络，使得训练好的模型仍然能够很好地泛化。在最近的弱监督学习（WSL）研究中，一个常见的说法是，人们声称他们只在弱标注数据上训练模型，却在干净的测试集上取得了高性能。技术上这个说法并不错误，但有一个陷阱，即人们假设有一个额外的干净验证集用于模型选择。我们不能停留在这个问题设置上，但这意味着在弱监督学习中需要额外的手动注释。但像房间里的大象一样，这种必要性常常被忽视。上述怀疑引发了三个研究问题。首先，弱监督学习是否需要干净的验证数据，或者我们可以使用噪声验证集？其次，如果需要干净数据，或者干净数据对于弱监督学习的有效性是必需的，那么我们需要多少干净样本？最后，我们是否只应该将干净样本用于验证，还是有更好的方法来利用它们？我们在我们的工作中解决了这些研究问题，我们的发现如下。首先，我们发现，有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作。否则，性能会大幅下降。如图所示，如果没有干净的验证样本，那么训练的模型无法超越原始的弱标注泛化，意味着训练是没有意义的。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。我们的第二个发现是，增加干净验证样本的数量将帮助WSL方法实现更好的性能，如左侧图所示。通常我们只需要每类20个样本就能取得高性能。但故事并未结束，因为如果我们决定获取干净样本，那么直接在它们上面训练将实现更好的性能。右侧图显示了性能差异，直接应用于干净数据的微调方法与仅使用干净数据进行验证的WSL方法相比。我们可以看到，如果每类有10个样本，直接微调开始超过WSL方法。最后，之前WSL方法声称的性能提升可以通过允许在干净验证样本上继续微调来轻松实现。如图所示，标准模型，称为FTw，最初在性能上不如更复杂的WSL方法，如COSINE。但如果允许在干净样本上继续微调，FTw的性能与其他方法一样好。因此，在实践中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。总结一下，我们表明最近的WSL方法需要干净的、手动标注的样本才能正常工作。它们的性能提升和实用性被严重高估。我们对未来工作的具体建议如下。首先，报告模型选择标准。例如，报告模型选择是否通过干净的验证样本进行。其次，WSL方法应与少样本学习基线进行比较，因为两者都在干净样本上工作。第三，连续微调是未来WSL工作中应考虑的简单但强大的基线。最后，我们已经开源了我们的代码。你可以通过这个幻灯片上的二维码找到它。请随意查看。谢谢，祝你在会议上愉快。</sample>
    <sample id="241">Ethan and his team at Georgia Tech, including Yang Chen, Wei Xu, and Alan Ritter, presented their paper on "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." The paper addresses the shortcomings of existing misinformation detection systems on social media, which often rely on retrospectively constructed datasets and may suffer from leaked counter-evidence. These systems are not human-centric and fail to represent the real scale and noisiness of social media platforms, which require human content moderators' involvement.

The team proposes an evaluation framework that integrates human feedback throughout the misinformation detection process, from raw tweets to actionable outputs. Their system is designed to be end-to-end, involving humans at various stages to enhance its effectiveness. The system comprises two main components: detection of misleading claims and policy violation verification.

For detecting misleading claims, the system uses keyword filtering to identify relevant tweets and employs a T5 model trained for question answering to extract claims about COVID-19 treatments. These claims are ranked by trendiness using Fisher's Exact Test and then verified by humans. The second component uses a BERT-based stance classification model to determine the author's stance on unapproved treatments, flagging supportive stance tweets for human review.

The evaluation of their human-in-the-loop workflow focuses on early detection, defined as identifying unapproved treatments before they are debunked in news articles. The system successfully detected several unapproved treatments before their public debunking. Additionally, the efficacy of policy violation verification was assessed, with humans rating tweets on a Likert scale for policy violations. The system achieved a 65% accuracy rate in detecting policy violations, confirming 124.2 violations per human hour worked.

The framework realistically captures the interaction between systems and human moderators, aiming to motivate the development of future human-in-the-loop misinformation detection systems. The work also provides an out-of-industry perspective on the development and evaluation of such systems.</sample>
    <sample id="242">对话系统的常用评估方法包括使用人类评估者选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法通常用于提供对整体对话质量的全面评估。</sample>
    <sample id="243">这篇论文有六位作者：Jenny、Sebastian Santy、Ronan Le Bras、Katharina Reinecke、Maarten Sap。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要以下背景知识：

1. 服务员是一名法官。
2. 法官在法庭上决定案件。</sample>
    <sample id="245">Lining Zhang and co-authors present their work titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The study introduces a two-step pipeline to identify high-agreement Amazon Mechanical Turk (MTurk) workers, addressing the limitations of automatic metrics and unclear best practices for recruitment. The presentation begins with "Qualification Settings," followed by the "Qualification Task" and "Endurance Task" stages. The pipeline also includes a "Reference-based task" and comparisons with Baseline and CloudResearch MTurk workers, alongside an analysis of correctness across annotation sources.

In the "Qualification Settings," pre-task qualifications such as location, number of HITs, and HIT Approval Rate are set. The first stage, "Qualification Task," evaluates annotators' ability to assess multiple dimensions correctly, categorizing them into gold, silver, bronze, and block types. Only gold and silver workers pass, resulting in 26 qualified workers (13% of 200 participants). The second stage, "Endurance Task," tests the capacity to handle a heavy workload, with 12 workers (6% of 200) passing, achieving high inter-annotator agreement (IAA) compared to experts.

The "Reference-based task" assesses general performance, with 8 out of 12 workers completing all HITs, showing a Krippendorff's Alpha of 0.534. Baseline MTurk workers, filtered using the MACE statistical filter, achieved a Krippendorff's Alpha of 0.380, while CloudResearch workers reached 0.513 but with a lower task acceptance rate. Analysis of correctness across annotation sources revealed significant Spearman's correlation between Pipeline and CloudResearch workers, though Pipeline did not guarantee correctness training. Real GPT models correlated well with expert judgments.

In summary, the pipeline effectively filters high-agreement workers, achieving quality comparable to CloudResearch at a lower cost. It serves as a best practice for large-scale, high-agreement annotations, avoiding resource waste. Future work will explore hiring high-quality workers across various tasks, languages, and platforms. Limitations include testing only English summarization on MTurk, non-universal question designs, no correctness training guarantee, and reliance on Google's funding.</sample>
    <sample id="246">Yes, the code is publicly available. You can find it on GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI introduces a new paper titled "FACTKG: Fact Verification via Reasoning on Knowledge Graphs." The paper addresses the gap in existing fact verification datasets, which primarily use Wikipedia text or tables as evidence, by proposing a novel task: Knowledge Graph-Based Fact Verification. This approach leverages knowledge graphs (KGs) for more reliable and intuitive fact verification, as KGs allow direct connections between evidence and claims without requiring additional interpretation.

The paper introduces the FactKG dataset, which utilizes DBpedia as the knowledge graph. Claims in FactKG are presented in both written and colloquial styles to enhance practical applicability. The dataset includes two labels: SUPPORTED and REFUTED, and involves retrieving evidence from DBpedia to verify claims. Five types of reasoning are incorporated: one-hop, conjunction, existence, multi-hop, and negation. One-hop reasoning involves verifying a claim with a single triple, while conjunction reasoning checks multiple one-hop claims. Existence reasoning verifies if an entity is connected to a specific relation, and multi-hop reasoning involves verifying claims that require inference across multiple entities. Negation reasoning requires additional verification to confirm the absence of a relationship.

To create claims in colloquial style, the paper employs a colloquial style transfer model and presupposition templates. The dataset's statistics and baseline methods are also discussed. Claim-only baselines use only the claims for verification, while the GEAR model utilizes graph evidence, outperforming all other baselines and the majority class baseline of 51%. The paper concludes by inviting readers to download the dataset and contact the authors for further information.</sample>
    <sample id="248">NLPositionality 的注释者在各个人口统计学特征方面并不均衡。研究中提到，他们从87个国家收集了超过1000名注释者的16,000多个注释，但并未明确说明这些注释者在国家/地区、性别等方面是否均衡。研究的目的是通过重新注释数据集来获得丰富的人口统计数据，以便比较模型和数据集的偏见。</sample>
    <sample id="249">在可接受的域中扰乱句子是通过尝试保留相关结构的同时，向输入句子添加噪声来实现的。这些扰动包括对句子进行多种变化，但结果显示，这些噪声并没有改变模型在最小对比判断（MPP）中的表现。即使在句子被扰动后，模型对可接受句子的判断仍然显示出类似的增加。</sample>
    <sample id="250">进行维度评估意味着评估对话模型在多个维度上的表现，以更细致地了解模型的优势和劣势。这包括评估模型在相关性、一致性、事实准确性、常识遵循、同理心等方面的表现，而不仅仅是整体对话质量。这种方法旨在减少人类评估的主观性，通过明确标注模型回应中的特定行为来实现。</sample>
    <sample id="251">University of Science and Technology of China</sample>
    <sample id="252">Sai Kiran Tanikella, a master's student at IIT Kanpur, presents "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion," a collaborative work with Abhinav Joshi, Akshat Sharma, and Ashutosh Modi. The project addresses the challenge of Prior Case Retrieval (PCR) in the legal domain, where legal professionals need to retrieve relevant past cases from a growing pool of documents. The U-CREAT pipeline introduces two key contributions: the IL-PCR dataset and an event-based retrieval approach.

The IL-PCR dataset, or Indian Legal Prior Case Retrieval Dataset, is a new benchmark for PCR tasks, comprising 7,070 legal cases with an average of 6.775 citations per query document. It provides a comprehensive test bed for evaluating PCR algorithms, featuring longer documents, a larger vocabulary, and more citations compared to the existing COLIEE’21 dataset for Canadian legal documents.

The U-CREAT pipeline leverages unsupervised learning and an event-based approach to improve PCR efficiency. It uses dependency parsing to extract events from case documents, representing them as subject-verb-object triplets. The pipeline processes query and candidate documents through an event extraction block, generating an interaction matrix to identify common events and rank candidate documents.

Experiments with various models, including count-based, transformer-based, and event-based models, demonstrate the effectiveness of the U-CREAT approach. While transformer-based models like BERT and its variants showed lower performance compared to baseline methods like BM25, event-based models significantly outperformed them. The Event Filtered Documents model, which filters the corpus to include only sentences with matching events, achieved the best performance with higher F1 scores and lower inference times.

U-CREAT's event-based models, particularly the Event Filtered Documents model, outperform existing approaches, including the recent supervised method by the MTFT-BERT team, on the COLIEE’21 dataset. This work highlights the potential of event-based approaches in the legal domain and sets a new state-of-the-art for PCR tasks. For further details, the paper is recommended.</sample>
    <sample id="253">Mario Ezra Aragón and his team from Mexico and Spain present "DisorBERT," a model designed for detecting signs of mental disorders in social media through double domain adaptation. The model addresses the challenge of insufficient annotated data by leveraging knowledge from related domains, specifically adapting BERT, a general language model, to the specific language of Reddit and mental health discussions. The approach involves integrating information from Reddit and mental health, along with a lexicon to guide the masking process, allowing the model to focus on important words during training.

DisorBERT aims to first learn the language of social media and then specialize in the mental disorder domain. The model's effectiveness is demonstrated using the eRisk datasets, where it shows a balanced performance in precision and recall compared to other methods. The model's ability to focus on relevant words is illustrated through examples from Beck's Depression Inventory (BDI), where DisorBERT predicts words with a negative psychological orientation, indicating its specialization in mental health contexts.

The results show that DisorBERT generates predictions more aligned with mental disorder-related words than BERT, which tends to produce more general words. Visualization tools further highlight the model's focus on relevant topics, such as "anxious" and "medication," in user posts with high BDI scores. The study concludes that the combination of double domain adaptation and guided masking effectively captures signs of mental disorders in social media, outperforming MentalBERT, a model trained on a large dataset. Future work will explore the use of different lexical resources and clinical data to enhance the model's capabilities.</sample>
    <sample id="254">本研究介绍了一种名为“不确定性引导标签去噪的文档级远程关系抽取”（Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction）的框架，旨在提高远程监督数据（DS data）的标签质量。文档级关系抽取（DocRE）的目标是在文档中提取实体之间的关系。传统方法依赖大规模人工标注的数据集，这既耗时又耗力。因此，近期研究利用远程监督数据预训练模型以提高性能。然而，这些数据中存在不同程度的噪声，而当前的方法通过使用伪标签来减轻噪声问题，但仍然存在由于假阳性伪标签引入的噪声风险。

本文提出了一种基于不确定性引导的标签去噪框架，以改善DS数据的标签质量。首先，使用DS数据和人工标注数据训练一个预去噪的DocRE模型，生成伪标签。由于伪标签中不可避免地包含错误，因此引入不确定性估计来判断模型预测的可信度。为了处理实体对之间可能存在的多个关系，提出了实例级不确定性估计方法，以捕获重叠关系的不确定性分数。此外，设计了动态类不确定性阈值的重新标记策略和多阶段训练策略，以进一步提升性能。

为了在预去噪DocRE模型中建模不确定性，引入了Monte Carlo dropout技术。该方法通过激活dropout进行多次随机前向传播预测，以捕获模型不确定性。为了解决重叠关系问题，修改了估计过程以获得每个正伪标签的实例级不确定性分数。观察到每个关系类的不确定性分数分布不同，频繁类通常具有较低的平均不确定性，而长尾类则相反。因此，提出了动态类不确定性阈值来过滤高不确定性的伪标签。将原始DS标签替换为不确定性分数低于其类不确定性阈值的伪标签。

为了充分利用DS数据进一步提升DocRE模型的性能，设计了一个多阶段训练策略，迭代重新标记DS数据。本研究在公开数据集上与多个强基线进行比较，结果表明，本框架在两个数据集上均优于基线。总结来说，本研究的主要贡献包括：1）提出了一种不确定性引导的标签去噪框架，显著提高了DS数据的标签质量；2）提出了实例级不确定性估计方法，用于处理重叠关系；3）设计了迭代重新标记策略，结合动态类不确定性阈值，解决长尾问题；4）实现了显著的性能提升。</sample>
    <sample id="255">提示的形式在零和一次提示的情况下很重要。在多次提示（如五次提示）的情况下，提示的实际形式对性能的影响不大，重要的是例子本身。</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a new research paper titled "Can Large Language Models Be an Alternative to Human Evaluation?" The paper explores the potential of using large language models (LLMs) to evaluate the quality of text in natural language processing (NLP) tasks, aiming to provide an alternative to traditional human evaluations. The motivation behind this research is the instability and reproducibility issues associated with human evaluations. By leveraging LLMs, which can follow natural language instructions, the authors propose a method where LLMs are instructed to rate text samples based on specific attributes such as grammar, coherence, likability, and relevance.

The novelty of this work lies in its exploration of LLMs for evaluation purposes, a concept not previously explored in submissions to the ACL conference. The authors conducted experiments where LLMs rated stories generated by GPT-2 and those written by humans. The evaluation was based on four attributes, and the results were compared with human evaluations conducted by English teachers, who were considered experts due to their experience in scoring essays.

The experiments involved four different LLMs: T0, InstructGPT (Curie and Davinci), and ChatGPT. The findings revealed that while some smaller LLMs did not show a clear preference for human-written stories, two larger models, Davinci and ChatGPT, demonstrated a preference similar to that of human evaluators. This suggests that certain LLMs can indeed serve as viable alternatives to human evaluation in specific tasks.

The paper addresses several questions, such as the agreement between LLMs and human evaluators on individual story ratings, the impact of changing instruction wordings, and the effects of different response sampling methods on evaluation results. Additionally, it discusses the benefits and costs of using LLM evaluations compared to human evaluations and extends the discussion to other tasks. For those interested in this topic, the paper provides detailed answers and further insights.</sample>
    <sample id="259">Yusen Zhang from Penn State University presented the work "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The project addresses the challenge of translating user queries from various natural languages into multiple meaning representations like SQL, Lambda Calculus, and FunQL. Existing models for cross-lingual semantic parsing are limited in scope, often lacking coverage for languages like Chinese and certain meaning representations such as Lambda Calculus. To address these gaps, the XSemPLR dataset was introduced, encompassing 9 datasets across different domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages from 15 language families.

The evaluation of XSemPLR includes six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Few-shot transfer. The Translate-Test setting involves translating queries using Google Translate API and then applying a monolingual model. The Monolingual Model setting uses the same language for both training and inference. The Monolingual Few-shot setting trains models with only 10% of the data. The Multilingual Model setting trains a single model on multiple languages. Cross-lingual Zero-shot and Few-shot transfer involve training on one language and transferring to another, with the latter using a small amount of target language data.

Analysis of monolingual models showed that Encoder-Decoder models, such as mBART and mT5, outperformed Encoder-PTR models like XLM-R + PTR across all datasets. Training in a mixture of languages improved performance for most languages, except English, which experienced a performance drop in seven datasets. This phenomenon is referred to as the "Curse of Multilinguality."

The study also examined the cross-language performance gap, finding that Zero-shot transfer had a significant performance gap compared to Monolingual settings. However, Few-shot transfer significantly reduced this gap. Encoder-Decoder models outperformed previous work and achieved comparable results. Pretraining on English improved Few-shot performance on target languages, but multilingual models like Codex and BLOOM were found inadequate for cross-lingual semantic parsing tasks.

In summary, XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing, highlighting the strengths of Encoder-Decoder models and the challenges of multilingual language models in this domain.</sample>
    <sample id="260">The provided content does not specify the number of authors of the paper.</sample>
    <sample id="261">优秀规划器的理想品质是能够生成既合理又忠实于约束条件的脚本。这意味着规划器应该能够在保持语义完整性的同时，确保生成的步骤符合特定的约束条件。</sample>
    <sample id="262">The provided content does not specify the number of authors of the paper.</sample>
    <sample id="263">The presentation focuses on addressing label biases in in-context learning, a method used with large language models for text classification. In-context learning is known for its instability due to design choices like the selection and order of examples, which introduce biases in model predictions. Previous work has identified search instability as a result of these biases, but there has been no systematic categorization or mitigation of these biases.

The work introduces a typology of label biases, identifying three types: vanilla-label bias, context-label bias, and a newly identified domain-label bias. Vanilla-label bias refers to the model's inherent preference for certain label names without context. Context-label bias arises from the influence of the context provided to the model. Domain-label bias, the new type, is influenced by the task corpus, where exposure to random in-domain words can skew model predictions.

Experiments demonstrated that random in-domain words from the task corpus can significantly bias predictions, unlike random English words. Tasks with small domain-label bias showed good performance with in-context learning and advanced calibration methods, while tasks with large domain-label bias struggled, barely surpassing chance-level baselines.

To mitigate these biases, the authors propose domain-context calibration. This method uses random in-domain words as content-free text to estimate and adjust the model's bias for each label name. Unlike prior methods that used predefined tokens like "not available," domain-context calibration accounts for domain-label bias by incorporating random in-domain words, improving decision boundaries and performance.

Experiments with various models and datasets showed significant improvements in in-context learning performance, especially in tasks with high domain-label bias. The method outperformed previous calibration attempts by using multiple random words and in-domain words, highlighting the inadequacy of single predefined tokens.

In summary, the work systematically investigates label biases in in-context learning, identifies a new bias type, and proposes an effective calibration method to enhance model performance across different tasks and models.</sample>
    <sample id="264">林旺，浙江大学研究生，介绍了其论文《TAVT: Towards Transferable Audio-Visual Text Generation》。当前，单模态文本生成任务如机器翻译和图像描述已经取得显著进展，但多模态文本生成任务如音频视觉文本生成面临数据标注困难和成本高昂的问题。不同领域的构建条件差异导致现有方法性能下降。为解决这一问题，提出了可转移的音频视觉文本生成任务，主要挑战是多模态领域转移，如视觉风格和音频能量的变化。观察到视觉内容随图像风格和拍摄角度变化而显著变化，而音频内容如节奏和能量对事件理解影响较小。因此，提出了一个统一的音频语义空间来对齐不同领域的视觉概念。

提出的框架包括三个模块：音频视觉元映射网络、音频视觉编码器和语言模型生成器，以及对比学习。音频视觉元映射网络将不同领域的视觉概念映射到统一的音频语义空间，通过学习可调节的视觉前缀来改善语义。使用基于Transformer的编码器和生成器，引入α参数评估不同模态对每个词的贡献。提出了Dual Counterfactual Contrastive Learning（DCLL）来直接优化视觉文本对齐，而不依赖于随机选择的负样本。在元训练阶段，随机选择K-1个特定领域作为支持集，剩余领域作为查询集。在元测试阶段，考虑新的目标领域，通过微调模型元参数进行快速适应。

实验部分，基于MSVD和MSR-VTT构建了两个基准，包括跨数据集和跨领域设置。与SOTA方法（包括RNN和Transformer模型）进行比较，TAVT在所有指标上显著优于其他模型，尤其是在低资源领域如“Kids”和“Beauty”中表现出色。通过消融实验分析了音频特征对性能的影响。</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">这篇论文的作者所属机构没有在提供的内容中提到。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误。</sample>
    <sample id="269">你好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将为你介绍 ABC-Eval，这是一种新的多维度方法来评估对话人工智能。这项工作由埃默里大学自然语言处理实验室的金浩教授领导，并与亚马逊Alexa AI合作完成。假设你刚刚开发了一个对话模型，并希望看看它与当前最先进的模型相比如何。通常的做法是使用人类评估，比如让人类评委选择两个对话中哪个更好，或者使用李克特量表对对话进行评分。这些方法在提供对整体对话质量的全面评估方面效果很好，但对话质量有许多方面。因此，你可能希望评估多个对话质量维度，以更细致地了解模型的优势和劣势。一种方法是简单地让人类评委评估对话质量的多个维度，比如使用现有的比较方法或李克特量表评估模型回应的相关性。然而，我们认为有一种更精确和可靠的对话评估策略。我们的方法试图通过明确标注模型回应是否表现出某些行为来减少人类评估的主观性，比如回应无关信息或自相矛盾。我们称这种方法为在聊天中标注行为，简称 ABC-Eval。我们开发了这种方法，以全面覆盖最近文献中建议影响聊天质量的聊天模型行为。ABC-Eval 能够测量聊天模型犯下各种主题错误的频率。例如，ABC-Eval 测量聊天模型忽略伙伴或说出无关信息、自相矛盾或伙伴矛盾、产生错误事实或违反常识知识的回合数量，以及模型成功或失败地表现出同理心的情况。为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用 ABC-Eval 对每个模型的 100 个人机对话进行了评估。为了比较，我们还使用了三种现有方法对这些对话进行了评估：回合级李克特评分、对话级李克特评分和对话级配对比较。对于每种现有方法，我们收集了评估八个最常测量的对话方面的评估，因为这是评估聊天模型沿多个维度的标准做法。从这些评估结果的分析中，我们发现 ABC-Eval 行为标签整体上比现有方法收集的标签更可靠，这是通过在 100 个双重标注的对话中测量评委间一致性来衡量的。此外，ABC-Eval 标签比现有方法产生的指标更能预测整体对话质量，这在简单的线性回归分析中得到了证明。例如，你可以看到测量自我和伙伴矛盾回合比例解释了 5% 和 10% 的对话质量，而平均李克特一致性分数解释的不到 4%。最后，我们检查了每个评估指标是否捕捉到对话质量的独特方面，使用逐步线性回归。你可以看到所有 ABC-Eval 指标的组合解释了超过 25% 的对话质量，而逐一移除这些指标时，大多数指标会导致对质量信息的显著损失。相比之下，所有回合级李克特指标的组合解释的质量要少得多，且较少的这些指标携带独特信息。这些可靠、信息丰富且独特的 ABC-Eval 指标使我们能够以比以前方法能够实现的更高分辨率来评估对话人工智能。你可以在我们实验的结果中看到，仍然存在并且已经精确量化的挑战。例如，我们测试的机器人在大约 20% 的回应中存在常识违反，在大约 15% 的回应中产生无关信息，在大约 10% 的时间内自相矛盾或与伙伴矛盾。随着该领域的快速进步，新发布的模型可能会在我们进行评估后降低这些错误率。然而，这正是追求可靠和精确的评估指标来比较模型的更多理由。我们希望其他人能够利用 ABC-Eval 作为这一方向的有意义的一步。我们期待着在未来几个月和几年中看到对话人工智能的进步。谢谢你的观看。</sample>
    <sample id="270">Emory University and Amazon Alexa AI.</sample>
    <sample id="271">In the given content, CFT stands for "Fine-Tuning."</sample>
    <sample id="272">这篇论文有七位作者：Koustav Sinha、John Gauthier、Aaron Mueller、Kanishka Misra、Karen Fences、Roger Levy 和 Adina Williams。</sample>
    <sample id="273">您好，我叫Kayo Yin，我将介绍我们的工作《翻译何时需要上下文？一项数据驱动的多语言探索》。这项工作是与Patrick Fernandes、Emmy Liu、André F. T. Martins和Graham Neubig合作完成的。很多翻译都依赖于上下文。例如，如何翻译“mole”这个词？如果前一句是“如果部长们发现了，事情可能会变得危险”，那么“mole”指的是间谍。但如果前一句是“医生，这可能是什么严重的问题吗？”，那么“mole”指的是痣。因此，根据上下文，词的意思和翻译都会改变。然而，评估模型在这种情况下的翻译效果很困难。首先，因为只有一小部分翻译依赖于上下文，所以像BLEU这样的语料库级别的评估指标无法捕捉这些翻译。有人建议针对上下文依赖的翻译进行特定评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集，因为它们通常依赖于领域知识和人工策划。在这项工作中，我们试图回答这两个问题：首先，翻译何时需要上下文？其次，模型如何处理这些情况？为了回答第一个问题，我们首先测量在翻译过程中一个词依赖于上下文的程度。在之前的工作中，我们引入了CXMI作为机器翻译模型上下文使用的度量。这是通过测量上下文C对目标Y的信息量，给定源X来实现的。你可以把CXMI看作是给模型上下文所获得的信息量。在这项工作中，我们将CXMI扩展为Pointwise CXMI，可以在句子级别或词级别测量上下文使用。我们可以把P-CXMI高的词看作是需要上下文进行翻译的词。然后，我们分析P-CXMI高的词，寻找这些词之间的模式。我们在TED演讲的英语到14种不同语言的翻译转录上进行分析。我们在三个不同的层次上进行分析：首先，我们查看平均P-CXMI高的词性标记。这使我们能够找到例如阿拉伯语中的双数代词，它们的P-CXMI相对较高。这可以解释为英语没有双数代词，所以在翻译成阿拉伯语时需要上下文来确定代词是否为双数。同样，我们发现某些语言在选择适当的动词形式时也需要上下文。然后，我们查看在所有不同出现中平均P-CXMI高的词汇项。这有助于我们识别像在中文中需要上下文来翻译专有名词以确保在文档中使用相同的翻译的情况。同样，我们发现上下文对于翻译正确的正式程度也很重要。最后，我们查看具有高P-CXMI的不同个体标记。这使我们能够识别那些不能真正通过词本身捕捉的现象，而是通过句子结构表达的，例如解决省略。现在，我们使用分析的结果来设计一个文档级翻译的基准。对于我们识别的五种话语现象，我们创建标注器来自动识别与现象相关的词。我们称这个标注器为多语言话语感知（MuDA）标注器。我们还注意到不同语言在这些话语现象中的比例不同。然后，我们使用MuDA标注器，将标注器应用于我们想用于评估的平行语料库，并应用我们选择的翻译指标来评估MuDA标注器识别的上下文依赖示例。最后，我们使用我们的基准以及其他指标来评估不同模型的文档级机器翻译。首先，当我们使用语料库级指标时：对于BLEU，我们发现上下文无关模型表现最佳。但如果我们使用COMET，上下文感知模型表现最佳。如果我们使用词F-度量，那么使用和不使用上下文的模型的表现相当。这再次表明，如果我们仅使用语料库级指标，很难确定最佳的文档级翻译系统。现在，我们使用MuDA基准来评估模型，并发现对于某些话语现象，如正式程度和词汇连贯性，上下文感知模型的准确性显著高于不使用上下文的模型。但这些模型在其他现象如省略、代词和动词形式上并不比不使用上下文的模型好得多。这似乎表明，我们需要在文档级翻译方面看到更多进展的地方。我们还比较了不同的商业系统，我们的基准表明DeepL通常比Google翻译在文档级翻译中更准确。总之，我们在14种语言对中进行了数据驱动的分析，以识别翻译何时需要上下文，并使用我们的发现来构建文档级机器翻译的基准，这有助于我们识别模型处理得好或不好的话语现象，以及哪些翻译系统在文档级翻译中表现良好。非常感谢您的关注。下次见到您时在多伦多。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages," addressing the gap in evaluating translations from Indian languages to English. They focus on five Indian languages: Tamil, Malayalam (Dravidian), and Hindi, Marathi, Gujarati (Indo-Aryan). The study uses 200 sentences from the Flores dataset, generating 1,400 candidate translations per language using seven translation models or APIs, resulting in 7,000 samples.

The team employed bilingual expert annotators to evaluate these translations, marking errors by type and severity using the MQM framework, which includes accuracy, fluency, and special category errors. Annotators also provided overall scores for each translation.

The study found that recent models like NLLB and Indic Trans performed better than older models like CVIT. Among evaluation metrics, chrF showed the highest correlation with human scores, but overlap-based metrics generally performed poorly. Embedding-based metrics, particularly LabSE and BERTscore with multilingual models, showed better correlations, with MuRIL performing well on average. COMET-metric variants had the highest overall correlations.

The analysis revealed that many metrics, including SacreBLEU, had a skewed score range, making interpretation difficult. When the dataset was split based on error types, metrics correlated better with human scores for accuracy errors than fluency errors.

The researchers fine-tuned the best-performing metric, COMET, using their MQM dataset, creating IndicCOMET variants. These variants outperformed COMET baselines on three out of five languages and showed higher correlations across all languages. IndicCOMET also demonstrated strong zero-shot performance on unseen languages.

Finally, IndicCOMET MQM showed greater robustness on the ACES Translation Accuracy Challenge Sets, with a correlation score of 0.36 compared to COMET's 0.272. The dataset is publicly available for further research.</sample>
    <sample id="277">The new method introduced in the paper does not have a specific name mentioned in the provided text.</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法为一种识别区分显性群体与不显性群体的词汇的方法。该方法基于社会语言学概念“显性性”，即存在一个不显性的默认状态，任何与该默认状态不同的群体在语言上都是显性的。在方法中，首先确定不显性和显性群体，然后使用“Fightin’ Words”方法，即使用加权对数比率来区分每个显性群体的顶级词汇。例如，对于黑人女性的人设，会将其与白人人设和男性人设进行对比，以识别显性词汇。</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">Shi Tao introduces "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," focusing on emotion recognition in conversations (ERC). The goal of ERC is to predict the emotion label of each utterance using textual, audio, and visual modalities. Existing methods often inadequately exploit multimodal information, struggle with minority emotion classes, and find it challenging to distinguish semantically similar emotions.

To address these issues, MultiEMO is proposed, consisting of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The main contributions include:

1. **VisExtNet**: A novel visual feature extractor that captures facial expressions without encoding redundant scene-related information, using MTCNN and VGGFace2 pre-trained ResNet-101.

2. **MultiAttn**: A multimodal fusion model using bidirectional multi-head cross-attention layers. It integrates textual, audio, and visual modalities by learning cross-modal correlations, enhancing the fusion of complementary information.

3. **Sample-Weighted Focal Contrastive Loss**: This loss function assigns higher importance to hard-to-classify minority classes and maximizes inter-class distances to better distinguish semantically similar emotions.

Experiments on MELD and IEMOCAP datasets show that MultiEMO achieves state-of-the-art performance, particularly improving results for minority and semantically similar emotions. However, limitations include VisExtNet's inability to distinguish between speakers and irrelevant people, the need for large batch sizes with SWFC loss on MELD, and still suboptimal performance in minority emotions compared to majority classes.</sample>
    <sample id="281">本研究探讨了翻译中何时需要上下文，并评估模型在这些情况下的表现。研究首先引入了CXMI，用于衡量上下文在机器翻译中的使用情况。本文扩展了CXMI为Pointwise CXMI，以在句子或词语层面衡量上下文使用。通过分析TED演讲的英语到14种语言的翻译，研究识别了高P-CXMI的词语，这些词语需要上下文进行翻译。分析显示，某些词性（如阿拉伯语的双数代词）、词汇（如中文的专有名词）和句子结构（如省略语解析）需要上下文。

研究设计了一个基准测试，用于评估文档级翻译，通过MuDA标签器自动识别与五种话语现象相关的词语。结果表明，使用上下文的模型在处理正式程度和词汇连贯性等现象时更准确，但在处理省略语、代词和动词形式时与不使用上下文的模型表现相似。此外，使用MuDA基准测试评估不同的商业系统时，发现DeepL通常比Google翻译在文档级翻译中更准确。总之，本研究通过数据驱动的分析识别了需要上下文的翻译情况，并建立了一个基准测试，以帮助评估模型在文档级翻译中的表现。</sample>
    <sample id="282">Xuekai Zhu introduces "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023, addressing non-parallel text style transfer at the story and discourse levels. Unlike previous studies focused on token or sentence levels, this research aims to imitate author styles in long texts, which involve complex linguistic preferences and discourse structures. The main challenges include imitating author linguistic choices at the discourse level and transferring style-specific content across different styles.

To address these challenges, StoryTrans is proposed, which learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. A new training objective is designed to reduce stylistic features from discourse representations, pulling representations closer in latent space, and enhancing content preservation by separating generation into two stages. In the first stage, style-specific content keywords are masked, and the text is transferred. In the second stage, the correct style-specific contents are filled, and mask tokens are removed.

The training framework includes an advisory training framework for the first stage, employing self-reconstruction loss, disentanglement loss on sentence embeddings, sentence order loss, and style classifier loss. The second stage focuses on filling correct style-specific contents. Extensive experiments on new Chinese and English datasets show that StoryTrans outperforms strong baselines in style control and content preservation. Style visualization confirms alignment with golden text in style feature space. StoryTrans effectively supplements short phrases or plots, enriching storylines while maintaining main contents and rewriting sentences with target styles while preserving source semantics. Data and code are available in the provided repository.</sample>
    <sample id="283">The symmetric dependency structure of coordination mentioned is Hudson's Word Grammar. The city associated with the Prague approach, which is an asymmetric structure, is Prague.</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented a paper titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which depend heavily on precise span boundaries. These boundaries often suffer from ambiguity, as different annotation spans can be reasonable. To tackle this, Peng proposed a fuzzy span mechanism where the span boundary is learned as a continuous distribution rather than a precise point. This approach accounts for the inherent uncertainty in span boundaries.

The paper highlights a mismatch between transformer feature extraction, which focuses on global features, and the need for span-based information extraction, which requires attention to limited-length spans. To address this, Peng introduced an adaptive attention mechanism for span extraction, allowing the model to consider a range of potential boundaries (R-min and R-max) and evaluate the correctness of each position using a function Q. This continuous boundary distribution is then converted into discrete values for calculating fuzzy span loss, incorporating Binary Cross Entropy (BCE) and KL-divergence.

A novel fuzzy span attention mechanism was proposed as a mask function to refine the attention distribution. This mechanism dynamically adjusts the attention span using an optimizable parameter delta and applies a linear decay at the attention span boundary, rather than a sharp truncation. This layer is added only at the top level to guide decision-making without affecting text encoding.

The FSUIE model was tested on three main information extraction tasks: named entity recognition, relationship extraction, and aspect sentiment triplet extraction. The results showed significant improvements in named entity recognition and new state-of-the-art results in relationship extraction on datasets like ACE2004, 2005, and ADE. FSUIE also achieved state-of-the-art results on the AST-V2 dataset for aspect sentiment triplet extraction and demonstrated strong generalization capabilities for domain-specific information.

An ablation study confirmed that the fuzzy span attention (FSA) improved convergence speed by guiding the model to a reasonable attention distribution, while the fuzzy span loss (FSL) enhanced information extraction capability by fully utilizing annotation information. The combined effect of FSA and FSL resulted in significant performance enhancements. Visualization of the attention distribution showed that the model focused on semantic information within a limited range of preceding tokens, aligning with expectations. Overall, the FSUIE model, with its novel fuzzy span loss and attention mechanisms, achieved excellent results across various information extraction tasks.</sample>
    <sample id="285">Mingqi Gao from Peking University presents their work on "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The study addresses the issue of factual errors in dialogue summarization, highlighting two main approaches: incorporating factuality-related objectives during training or inference, and designing independent Factual Error Correction (FEC) models. The FEC models take a source document and a generated summary as input to produce a corrected summary. Despite the importance of factual accuracy in dialogue summarization, previous FEC studies have not adequately addressed this area, and their evaluation methods are flawed.

The current evaluation of FEC models relies on factuality metrics like FactCC and DAE, which provide overall scores. These scores are expected to be higher for corrected summaries, but this method is criticized for being vague and potentially unreliable. Additionally, it blurs the distinction between the two types of solutions, as FEC models might generate entirely new summaries without correcting the original errors.

To address these issues, the authors propose using manually annotated reference corrections. This approach not only offers valuable training data but also enables a more comprehensive and accurate evaluation of FEC models. The study introduces a new taxonomy for classifying factual errors, distinguishing between content-based and form-based errors. Content-based errors are categorized by part of speech and dependencies, while form-based errors are classified by operations like addition, deletion, and substitution.

The evaluation framework is built on ERRANT, a metric for grammar error correction, and involves alignment, classification, and comparison steps. Experiments with various FEC models reveal that training with reference summaries from dialogue datasets yields the best results according to unreliable factuality metrics. The study emphasizes the need to change evaluation methods and suggests that incorporating human-corrected summaries during training can enhance FEC model performance. Combining human-annotated data with synthetic data is identified as a promising direction. However, current FEC models struggle with correcting certain types of factual errors, such as additions, and fail to address attribute, modality, and link errors.</sample>
    <sample id="286">James Finch and Sarah Finch.</sample>
    <sample id="287">这篇论文有四位作者：Javad Hosseini, Filip Radlinski, Silvia Pareti, 和 Annie Louis。</sample>
    <sample id="288">BLiMP和SyntaxGym数据集可用于测试句法现象。</sample>
    <sample id="290">The text does not provide specific five methods or their abbreviations related to the first research question. It discusses the necessity of clean validation data in weakly supervised learning (WSL) but does not mention five specific methods or their abbreviations.</sample>
    <sample id="291">该模型在以下任务上进行了评估：命名实体识别、分类、词性标注和问答。</sample>
    <sample id="294">CamemBERT 最初是在 OSCAR 数据集上训练的，该数据集包含 138 GB 的数据。</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presents a collaborative project between the University of Turin and Amazon Alexa focusing on irony detection in natural language processing (NLP). The project challenges the traditional notion of a single "ground truth" in data annotation, emphasizing the complexity of irony as a pragmatic phenomenon. To address this, the team developed the English Perspectivist Irony Corpus (EPIC), which includes 300 short conversations from social media platforms like Reddit and Twitter, collected over 1.5 years. The corpus covers five varieties of English, with data annotated by 74 individuals via the Prolific platform, each reviewing 200 conversations.

The annotation process involved a simple interface where annotators judged whether replies were ironic. The study revealed significant inter-annotator agreement variations based on demographics such as gender, age, and nationality. To explore these differences, the team developed perspective-aware models by fine-tuning pre-trained language models on dataset splits according to annotator characteristics. These models demonstrated higher confidence in their predictions compared to standard models, despite no clear performance trends.

Further analysis indicated that generational and geographical proximity influenced annotation discrepancies, particularly between annotators from the UK and Ireland. This research highlights the importance of considering diverse perspectives in NLP tasks, especially for nuanced phenomena like irony. The findings will be discussed further at the poster session.</sample>
    <sample id="297">本项目探讨了“从狗哨到喇叭：揭示语言模型中的编码修辞”主题，重点研究了政治中的“狗哨”现象。狗哨是一种双重信息传递的修辞手法，对外群体传递一个表面信息，而对内群体传递一个隐含的、常常是禁忌或具争议性的信息。例如，使用“世界主义者”一词，对外群体可能理解为指城市、自由派、国际化的人群，而对内群体则可能被解读为针对犹太人的隐晦攻击。研究狗哨对自然语言处理（NLP）和语言学具有重要意义，因为它挑战了我们对意义的理解，特别是在语境依赖性方面。

项目开发了一套狗哨的分类法和词汇表，包含超过340个术语和符号，主要涉及种族主义、跨性别恐惧症和反犹太主义的狗哨。这些术语来自学术、维基百科、博客等多种来源，主要是英语且以美国为中心。研究通过对历史美国政治演讲的案例研究，发现狗哨的使用频率与美国共和党的南方战略密切相关，尤其是在民权运动后，政治家不再公开表达种族主义时，狗哨的使用显著增加。

研究还评估了语言模型在识别狗哨方面的能力，特别是使用GPT-3。实验表明，GPT-3能够识别出许多正式注册的狗哨，但在社交媒体中常用的非正式狗哨和跨性别恐惧症狗哨上表现不佳。通过不同的提示策略，研究发现GPT-3在识别狗哨的隐含意义时表现有所提高。

最后，研究通过使用Prospective API和HateCheck的仇恨模板句子，探讨了狗哨如何规避内容审核。结果显示，当标准的群体标签或脏话被狗哨替代时，自动化的毒性检测分数会降低，即使在相同的句子中，这表明狗哨能够有效规避内容审核系统。总体而言，本项目通过开发狗哨的分类法和词汇表，以及对历史演讲和语言模型的研究，揭示了狗哨在政治中的作用和挑战。</sample>
    <sample id="298">重新训练或继续预训练一些模型使用更近期的数据的实验表明，性能随着训练数据与测试数据之间的时间差距增大而下降，这证实了时间漂移是性能下降的主要原因。</sample>
    <sample id="299">这篇演讲介绍了一项关于提高自然语言推理（NLI）模型鲁棒性的研究，通过最小最大训练（minimax training）来减少模型对捷径（shortcuts）的依赖。研究者Michalis Korakakis和Andreas Vlachos指出，尽管NLI模型在多个基准测试中取得了优异的成绩，但其成功部分归因于学习和利用捷径。这些捷径是在数据集创建过程中引入的输入属性与标签之间的不必要的相关性。例如，在MNLI数据集中，前提和假设之间的高词汇重叠与蕴含标签高度相关。因此，依赖捷径的NLI模型在内部分布样本上表现良好，但在外部分布的对抗性测试集上表现不佳，因为这些捷径在这些测试集中不成立。

传统的捷径缓解方法通常依赖于一个专门设计用于依赖捷径进行预测的辅助模型。然而，这些方法需要事先了解这些捷径，这需要特定的领域和数据集知识，限制了其应用潜力。此外，这些方法假设学习者会自然地利用与辅助模型相同类型的捷径，但实际上，学习者的行为可能与辅助模型不同。例如，辅助模型可能会降低对学习者有用的实例的权重，或提供不准确的不确定性估计，从而影响学习者的外部分布泛化能力。

为了解决这些限制，研究者提出了一种新的训练方法，旨在减少NLI模型对捷径的依赖并提高其外部分布性能。关键见解是，NLI模型在训练中忽视了代表性不足的“难”实例，这些实例可能与主导的“易”实例中存在的捷径相矛盾。这些难实例对于确保在外部分布例子上的良好泛化性能至关重要。研究者提出了一种最小最大训练目标，其中学习者试图最小化NLI任务的损失，而辅助模型的任务是最大化学习者的损失，通过生成实例权重来激励学习者关注导致高损失的输入空间范围。这种方法不假设数据集中的捷径类型，并依赖学习者的训练动态来生成实例权重。辅助模型由一个前馈神经网络模拟。

研究者在MNLI、FEVER和QQP等常用分析数据集及其对应的外部分布对抗性测试集（如HANS Symmetric和PAWS）上评估了该方法。结果表明，与经验风险最小化（ERM）训练模型和每个数据集中最佳的捷径缓解方法相比，最小最大训练目标一致地提高了外部分布性能，同时保持了高的内部分布准确性。研究还探讨了预训练学习者的影响、辅助模型的大小以及学习的实例权重分布的定性评估。</sample>
    <sample id="300">Belinda's presentation introduces the concept of interactive dictation, a task developed at Semantic Machines in collaboration with Jason Eisner, Adam Pauls, and Sam Thomson. Interactive dictation allows users to dictate and edit documents using natural voice commands, without needing to memorize fixed template commands. This process involves users speaking to dictate text, correcting themselves mid-sentence, and issuing verbal commands to make edits, such as replacing specific words. Unlike existing speech-to-text systems, which primarily support dictation, interactive dictation integrates editing capabilities through intuitive natural language.

The task is characterized by the flexible interleaving of dictation and editing, using open-ended natural language for commands. Belinda outlines a four-step procedure for interactive dictation: ASR recognition to parse audio into text, segmentation of speech into dictation and commands, normalization and correction of commands, and execution of these utterances to produce the final document. This process occurs in real-time as the user speaks.

To develop this task, a new data collection interface was designed, allowing users to dictate and issue commands, with the system capturing these interactions. A dataset was created using this interface, and a baseline system was built to perform the four steps of interactive dictation. The system includes separate models for each step, with experiments conducted using T5 and GPT-3 architectures. The segmentation model proved accurate and efficient, while the ASR repair and interpretation models showed a trade-off between runtime and accuracy, with GPT-3 models being more accurate but slower. The research indicates potential for further development in this area, and the team has released code to facilitate future work.</sample>
    <sample id="302">有必要对输出序列中的词元进行排列是因为在第一步中，每个输入词元被标记为一个无序的多集，这些多集包含将出现在输出中的词元。这意味着在第一步之后，所有正确的词元都已经被确定，但它们的顺序是无序的。因此，需要在第二步中使用另一个模型来预测一个排列，将这些词元放置到正确的顺序中，以生成有意义的输出。</sample>
    <sample id="303">作者建议模型所有者应提高偏见缓解方法的透明度，是因为缺乏透明度使得研究人员无法确定正面刻板印象和其他有害模式的来源。这种不透明性使得无法判断这些模式是由于过度的价值对齐、反刻板印象方法的副作用，还是其他原因造成的。增加透明度将有助于更好地理解和研究这些偏见，从而更有效地进行缓解。</sample>
    <sample id="304">最小对不可接受输入是指在最小对对比（Minimal Pair Paradigm, MPP）中，将不可接受或不符合语法的句子作为前缀添加到可接受或符合语法的查询句中，以测试语言模型在不同上下文下的接受判断能力。这种方法用于评估模型在更长的上下文窗口中的接受判断，尤其是当前缀与当前评估句子的语法结构不匹配时。</sample>
    <sample id="305">Dawei, a PhD student at Saarland University, presents a critical analysis of weakly supervised learning (WSL) in his work titled "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," co-authored with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The study examines the reliance of WSL methods on clean validation data, challenging the common assumption that high performance on clean test sets can be achieved without it.

Weak supervision involves labeling data using inexpensive but noisy sources like heuristic rules, knowledge bases, or low-quality crowdsourcing, as opposed to manual labeling. While this reduces costs, it introduces label noise, which can lead neural networks to memorize incorrect labels rather than generalize well. WSL aims to train models robustly under such noise.

The research questions addressed include whether clean validation data is necessary for WSL, how many clean samples are required, and whether these samples should be used solely for validation or also for training. The findings reveal that recent WSL methods indeed require clean validation samples to function effectively. Without them, models fail to generalize beyond the weak labels, rendering the training ineffective.

The study shows that increasing the number of clean validation samples improves WSL performance, with typically only 20 samples per class needed for high performance. However, direct fine-tuning on clean samples yields even better results. For instance, with 10 samples per class, direct fine-tuning outperforms WSL methods. Allowing fine-tuning on clean validation samples further enhances performance, making simpler models like FTw competitive with more complex WSL methods.

The research concludes that the performance gains and practicality of WSL approaches are often overestimated due to their reliance on clean validation data. Recommendations for future work include reporting model selection criteria, comparing WSL with few-shot learning baselines, and considering continuous fine-tuning as a strong baseline. The authors have open-sourced their code for further exploration.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim present their research on entity tracking in language models, focusing on how these models understand and track changes in entities throughout a discourse. They highlight the importance of this ability for comprehending longer texts, such as recipes, where entities like eggs, sugar, and flour change states as the recipe progresses. The research aims to determine the extent to which large language models can track these entities, addressing challenges like reliance on pre-training data patterns, heuristic associations, and memorization during fine-tuning.

To evaluate entity tracking, they designed a task involving boxes and objects, where models predict the contents of boxes after various state-changing operations. The task prevents models from using simple heuristics by requiring them to integrate initial descriptions with operations. They tested Flan-T5 and GPT-3/3.5 models using 2-shot in-context learning, finding that most models simply repeat initial states unless trained on code, as seen with GPT-3.5 models. Smaller models like T5-base can learn entity tracking through fine-tuning, but randomly initialized models cannot, underscoring the importance of pre-training. The findings suggest that pre-training on code enhances entity tracking abilities, though generalization beyond the setup remains uncertain. Further results, including GPT-4 experiments, are detailed in their paper.</sample>
    <sample id="307">The presentation does not explicitly list the evaluation metrics used. However, it mentions that the models were evaluated on tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. Common evaluation metrics for these tasks include precision, recall, F1-score, accuracy, and sometimes specific metrics like BLEU or ROUGE for question answering.</sample>
    <sample id="308">Jenny, a PhD student at Carnegie Mellon University, presented research on NLPositionality, which examines design biases in datasets and models. The study, conducted with collaborators from the University of Washington and the Allen Institute for AI, highlights how NLP technologies can exhibit systematic performance differences across populations due to the positionality of researchers and developers. Positionality refers to the perspectives shaped by demographics, identity, and life experiences, influencing research outcomes.

The research questions whether datasets and models reflect positionality by aggregating human judgments and opinions. Previous work has noted cultural gaps and theoretical definitions of model positionality, but lacked direct comparisons between end users and datasets/models. NLPositionality addresses this by re-annotating datasets with diverse annotators and comparing these annotations to existing models using Pearson's R correlation scores.

The study utilized Lab in the Wild, an online platform, to recruit diverse volunteers from 87 countries, resulting in over 16,000 annotations from 1,000 annotators. The research found that NLP datasets and models are most aligned with English-speaking countries and individuals with higher education. For instance, GPT-4 and Dynahate showed alignment with English-speaking and college-educated populations, while being less aligned with non-binary individuals.

To address these biases, the study recommends documenting design choices, adopting perspectivism in NLP research, and creating specialized datasets and models for specific communities, like the Masakhani initiative. The goal is to foster inclusive NLP that works for everyone, not just a subset of users.</sample>
    <sample id="309">使用了“inter-annotator agreement”来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择的领域来添加完全无关的句子是维基百科。</sample>
    <sample id="311">The text does not provide information about the authors' affiliations or institutions.</sample>
    <sample id="312">MultiInstruct 是第一个大规模的多模态指令调优基准数据集，它包含 62 个多样化的多模态任务，涵盖 10 个广泛的类别。这些任务是从 21 个现有的开源数据集中提取的，并且每个任务都配备了五个专家编写的指令。与之前的研究不同，这些研究主要集中在语言任务上，而 MultiInstruct 专注于多模态任务，填补了多模态指令数据集的空白。此外，MultiInstruct 使用了统一的序列到序列格式来处理各种输入和输出数据类型，这在多模态指令调优中是新颖的。</sample>
    <sample id="313">这篇论文没有提到具体的作者人数。</sample>
    <sample id="314">二进制协调是指两个或多个成分通过协调关系结合在一起，形成一个复合结构。在协调结构中，各个成分（如名词、动词、短语等）具有相同的语法功能，并通过协调连词（如“和”、“或”）连接。在二进制协调中，通常涉及两个成分，但可以扩展到多个成分。协调结构的特点是成分之间的平等性，即它们在句法上具有相同的地位。</sample>
    <sample id="315">在提供的内容中，没有提到提示语的平均长度。</sample>
    <sample id="316">这些发现表明，通过在 CoScript 数据集上进行微调，较小的 T5 模型可以生成比大多数大型语言模型更高质量的脚本。这表明，当适当地在合适的数据集上进行训练时，较小的模型可以超越更大的模型。</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing challenges in information extraction (IE) tasks like named entity recognition (NER) and relation extraction (RE). Traditional models, such as T5 and GPT-3, face issues due to the mismatch between linearized structured outputs during inference and plain text outputs during pre-training. This often requires extensive structured data and complex decoding strategies.

To resolve this, CodeIE transforms text-to-structured IE tasks into structure-to-structure code generation tasks using code large language models like Codex. This approach aligns input and output structures, simplifying the conversion of text to structured formats. For NER, a function is defined to extract named entities from input text, using few-shot in-context demonstrations to generate code that appends text-entity pairs to an entity list. Similar prompts are designed for RE tasks.

The method was evaluated on three NER and four RE datasets, comparing T5, UIE, GPT-3, and Codex models. CodeIE's code-style prompts significantly outperformed traditional text-style prompts in one to few-shot settings. Analysis showed lower perplexity for code format samples with models like CodeT5, indicating better alignment with IE tasks. Structural errors were minimal with Codex and code prompts, unlike GPT-3 with text prompts, which often produced incorrect labels.

Overall, Codex outperformed GPT-3 in IE tasks, with code format prompts enhancing recall. This research suggests that leveraging code generation models can improve few-shot IE performance, offering a promising direction for future IE methodologies. The paper and code are publicly available for further exploration.</sample>
    <sample id="318">您好，我是Yanis Labrak，今天我将向您介绍我们的工作《DrBERT：法语生物医学和临床领域的鲁棒预训练模型》。在这次演示中，我们首先讨论了医疗领域的语言建模。然后，我们将介绍文章的主要贡献。我们推出了第一个基于RoBERTa并在NACHOS数据集上训练的法语生物医学模型DrBERT，NACHOS是从网络上抓取的医学数据集。我们还比较了不同预训练设置和数据源的模型。接下来，我们展示了在11个法语生物医学和临床下游任务上的结果。最后，我们总结实验并提供了如何访问这些模型的更多细节。

自2018年发布以来，BERT已成为解决自然语言处理任务的最有效方法之一，相比于历史上的静态和上下文化方法（如Word2vec、fastText等），其性能提升显著。此后，这个模型已被适应到许多其他语言，如法语的CamemBERT，以及生物医学领域的PubMedBERT和BioBERT，临床领域的ClinicalBERT，但主要是英语。其他语言的专业模型较少，通常是由于缺乏领域数据而进行的连续预训练。然而，法语之前没有任何开源的生物医学模型。因此，我们提出了关于最合适的数据源以及这些抓取数据是否能够替代临床数据的问题。为了回答这个问题，我们将DrBERT与基于南特大学医院数据仓库中的匿名数据的ChuBERT模型进行比较。接着，我们提出了关于训练专门的法语数据模型需要多少数据的问题：4GB、8GB还是更多？为了回答这个问题，我们首先训练并比较了四个从头开始的模型：第一个DrBERT版本，使用7GB的NACHOS；第二个版本使用4GB的NACHOS集；第一个ChuBERT版本，这是一个临床模型，使用4GB的临床笔记句子；最后一个ChuBERT版本，使用4GB的NACHOS集和4GB的临床笔记的混合。除了这些比较，我们还引入了三个基于连续预训练的模型，以分析预训练策略的影响。第一个基于CamemBERT的权重，训练在4GB的NACHOS集上；第二个也基于CamemBERT，但这次训练在4GB的临床笔记上；最后一个基于英文生物医学模型PubMedBERT，训练在4GB的NACHOS集上。总共，我们有七个模型。

为了评估这七个模型，我们收集了公共和私有下游任务的数据，如命名实体识别、分类、词性标注和问答。这些模型与六个基线模型进行了比较，包括CamemBERT OSCAR 138GB、CamemBERT OSCAR 4GB、CamemBERT CCNET 4GB、PubMedBERT、BioBERT和ClinicalBERT。评估结果显示，模型在与训练数据性质相同的任务上表现最佳。然而，我们也观察到来自异质数据源的数据似乎更具通用性。我们还观察到使用更多数据会导致更好的性能。总体而言，从头开始的预训练似乎在大多数任务上获得了更高的性能。然而，我们对基于CamemBERT权重和分词器的控制预训练实验，使用4GB的NACHOS子集，结果与DrBERT 4GB从头开始的结果相当。这与基于CamemBERT权重和分词器的模型不同，后者存在稳定性问题。最后，作为结论，我们的专用系统在九个11个下游任务中表现更好，并在整体上超过了通用模型CamemBERT。我们还观察到更专业的数据更好，但不具可扩展性。所有基于NACHOS的预训练模型均在Hugging Face上免费提供，采用MIT许可证，所有训练脚本均在我们的GitHub仓库中。感谢您的关注，我们期待在多伦多的海报会议上交流。</sample>
    <sample id="319">论文研究了以下学习策略：

1. 从零开始的预训练：使用不同大小的NACHOS数据集（7 GB和4 GB）训练DrBERT模型，以及使用4 GB的临床笔记和4 GB的NACHOS数据集训练ChuBERT模型。

2. 连续预训练：使用CamemBERT的权重和标记器，分别在4 GB的NACHOS数据集和4 GB的临床笔记上进行预训练，以及使用PubMedBERT的权重和标记器在4 GB的NACHOS数据集上进行预训练。

3. 比较不同预训练设置和数据源的模型性能。</sample>
    <sample id="320">由于测试重复使用而导致的过拟合因素不大。研究发现，红色最佳拟合线的斜率大于1，表明在CoNLL-2003上的每个改进单位在CoNLL++上转化为超过一个改进单位，这意味着没有递减收益，因此在这种情况下没有观察到适应性过拟合。</sample>
    <sample id="321">To evaluate simplification quality, the DEPLAIN corpus can be used to assess automatic alignment methods by comparing them against manually aligned sentences as a gold standard. Additionally, automatic text simplification can be evaluated by fine-tuning language models like long-mBART and base mBART for document-level and sentence-level simplifications, respectively. The results and evaluation metrics from these experiments are detailed in the paper, providing a benchmark for future automatic text simplification efforts.</sample>
    <sample id="322">Enrico's presentation at ACL 23 explores how text classifiers learn about morality, emphasizing the complexity and subjectivity of moral judgments. Morality, which helps distinguish right from wrong, is crucial for societal functioning and should be understood by language models. Traditional approaches often treat morality on a singular scale, but this oversimplifies the diverse perspectives people hold on issues like abortion or LGBTQ rights. Enrico highlights the Moral Foundation Theory, which posits five distinct moral foundations that people prioritize differently, influencing their moral judgments.

Recent NLP research, including Enrico's work, uses this theory to classify morality in text, showing that language models can grasp moral nuances. The study applies explainable AI techniques to models trained on the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven domains, such as #AllLivesMatter and #BlackLivesMatter. The research investigates whether models can discern domain-specific moral expressions.

Findings reveal that models recognize differences in moral rhetoric between domains. For instance, in the context of #AllLivesMatter, subversion is associated with negative terms like "overthrow" and "mayhem," while in #BlackLivesMatter, it is more positively framed. This indicates that models can detect fine-grained moral differences, but also warns against using a single model across diverse domains, as it may lead to misunderstandings. Enrico's work underscores the importance of nuanced moral understanding in language models.</sample>
    <sample id="323">The paper "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" by Yujie Wang addresses the challenge of Commonsense QA by integrating language models and knowledge bases. The task requires machines to answer questions based on common knowledge, necessitating the retrieval of relevant information from external sources. Traditional methods often introduce noise by retrieving irrelevant entities and encoding subgraphs and text separately, limiting interaction between modalities and ignoring semantic relationships.

To overcome these issues, the paper proposes DHLK, which constructs a Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and Knowledge Representation Learning (KRL). The HKG is built from multiple knowledge bases, optimized for structure and representation. The process involves removing subwords from phrase entities using dictionary vocabulary and connecting paraphrases of key entities from WordNet and Wiktionary as additional nodes.

The encoding and fusion of QA contexts and entities are performed using RoBERTa and Mask Self-Attention. Entities with weaker relevance to the QA context are dynamically removed based on RoBERTa's attention weights. Initial embeddings for entities and relations are obtained through mean pooling, and TransE is used to optimize these embeddings within the HKG.

Instead of using GNNs, the paper employs Relation Mask Self-Attention (RMSA) to model subgraphs, inspired by RGAT, incorporating relationships into Mask Self-Attention. The HKG's entity and relation embeddings are iteratively updated through RMSA layers, and the graph embedding is obtained via max-pooling on question key entities.

The HKG path information is integrated into the QA context to enhance embedding representation. For answer prediction, the HKG graph embedding, paths, and QA context embeddings are input into an MLP to determine answer probabilities.

Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary demonstrate that DHLK outperforms other LM and HKG methods, achieving superior results. Key entities in the QA context are extracted using KeyBERT, and knowledge paths within two hops in ConceptNet are retrieved.</sample>
    <sample id="324">是的，语言模型确实有不同的政治偏见。研究表明，语言模型在政治立场上占据了不同的四个象限，例如GPT-4被发现是最自由的语言模型之一，而GPT系列一般比BART系列更具社会自由主义倾向。此外，通过在不同政治倾向的新闻和社交媒体数据上进一步预训练语言模型，研究发现模型的意识形态坐标相应地发生了变化。例如，进一步在左倾的Reddit数据集上预训练的RoBERTa显示出显著的自由主义偏移。这表明语言模型确实能够从训练数据中吸收政治偏见，并且这些偏见会影响它们在下游任务中的表现。</sample>
    <sample id="325">在本文中，我们介绍了一篇关于“无树结构的组合泛化：多集标记与潜在置换”的论文。这是与我的导师亚历山大·科勒和伊凡·蒂托夫合作的成果。组合泛化可以理解为学习者处理更深层次的递归和训练中单独看到的短语的未见组合的能力。在语义解析的背景下，测试组合泛化可能如下所示。通常，我们有一组训练语句，例如“女孩睡觉。”和“玛丽知道女孩睡觉。”这些语句与逻辑形式配对，这些逻辑形式代表它们意义的核心方面。与标准机器学习评估不同，测试集不来自同一分布，而是包含结构上未见的逻辑形式。在这个例子中，模型在训练中看到了浅层递归，并在更深层递归的例子上进行测试。简单的seq2seq模型在这种类型的分布外泛化中遇到困难，并且通常产生与输入脱节的输出。特别是，它们经常无法复制输入和输出之间的系统对应关系，例如在示例中以颜色编码的对应关系。解决这个问题的流行方法是将树结构整合到模型中。树结构旨在捕捉将语句与逻辑形式联系起来的组合过程。这种方法有效，但树结构通常不给定，需要以某种方式获得。这可能是复杂的，并且有时是一个计算上昂贵的过程。通常，这涉及到逻辑形式的大量形式化预处理，例如处理变量符号。获得树结构可能还涉及专门的语法诱导程序。在本文中，我们不使用树结构，并引入了一种直接建模输入片段与输出片段之间对应关系的神经seq2seq模型。我们首次展示了在不依赖树结构的情况下对更深层递归的强大泛化。我们的方法从输入预测输出的两个步骤。首先，我们为每个输入标记符标记一个无序多集，这些标记符将出现在输出中。在第一步之后，我们有所有正确的标记符，但它们没有排序。因此，在第二步中，我们使用另一个模型来预测一个置换将它们放置在正确的顺序中。我们引入了一种新方法来预测置换，这种方法不对可能的置换施加任何硬性约束。这使得我们的方法非常灵活和表达力强。概念上，我们的置换模型大致如下工作。我们从左到右遍历输出，确定每个位置的多集标记符。对于第一个输出位置，我们简单地选择一个，如红色高亮显示。然后我们跳到下一个多集标记符，以确定输出中的第二个标记符。我们以类似的方式确定输出中的第三个标记符，通过跳到另一个多集标记符。我们继续这个过程，直到访问了第一阶段的每个标记符恰好一次。为了给您展示实验结果的一个小样本，这里我们将我们的方法与其他无树结构模型在COGS基准测试上进行比较。我们的模型在对更深层递归的泛化上大幅度超过其他模型。然而，一些其他类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技术挑战。首先，输入与输出之间的对齐在训练数据中没有给定。因此，对于给定的标记符，我们不知道它来自哪个多集，这对于训练构成了挑战。此外，有时数据一致的多种置换，但正确的语言置换是潜在的。我们通过将对齐作为训练的一部分诱导来解决这个问题。我们的置换方法非常灵活，但它带来了一个挑战，即找到最高分的置换是NP难的。这是因为这与“旅行推销员”问题有关。我们用一个GPU友好的连续松弛来近似这个问题，这也允许我们通过解决方案反向传播并学习更符合语言的置换。如果您想了解更多关于我们的实验和我们如何解决这些挑战的信息，请查看我们的论文或参加我们的展板。</sample>
    <sample id="326">认知失调是指两个信念或行为之间的不一致，例如一个人说“我知道香烟可能会杀死我”，然后又说“会议后我抽了几支烟”。这两个陈述是不一致的，处于失调状态。认知失调是日常决策中常见的现象，但在语言中表达的情况相对较少。研究认知失调有助于理解人们之间的分歧、跟踪信念和态度的变化，以及理解某些心理健康问题，如焦虑障碍。此外，研究语言中的认知失调还可以帮助理解极端主义和脆弱群体的极化问题。</sample>
    <sample id="327">我是小徐，哈尔滨工业大学博士三年级学生，很荣幸在ACL 2023上介绍我们的工作“ManagerTower：聚合单模态专家的见解以进行视觉语言表示学习”。这项工作是在MSRIC组实习期间完成的，感谢Intel认知计算组的支持和讨论。视觉语言学习的目标是训练一个能够理解图像和文本的智能AI系统。近年来，基于大规模自监督预训练的基于Transformer的视觉语言模型在视觉语言任务上取得了显著进展。传统的两塔架构中，如METER，只使用最后一层单模态表示，忽略了深层单模态编码器中的语义知识。BridgeTower通过层与层的连接来利用不同层次的单模态知识，但其层与层的利用方式不够有效，且跨模态层的数量受限。我们提出ManagerTower，通过在每个跨模态层引入管理者，动态聚合不同层次的单模态专家的见解。ManagerTower使用RoBERTa和CLIP-ViT作为单模态编码器，通过管理者在每个跨模态层动态聚合不同层次的单模态语义知识，实现更全面的跨模态对齐和融合。在仅使用四百万图像进行视觉语言预训练的情况下，ManagerTower在多个下游任务上表现出色，尤其是在Wikivideo测试标准上达到39.15%的准确率。我们通过可视化VQAv2数据集中的平均聚合权重，展示了管理者的动态性，证明了其在不同跨模态层中对不同层次单模态语义知识的动态利用。论文、代码和模型已在Archive和Github上公开。希望我们的工作对您有所帮助。谢谢。</sample>
    <sample id="328">GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">这篇演讲介绍了一项名为“生成结构化伪标签的噪声抗性零样本视频句子定位”的研究。研究由北京大学的明航郑和其他合作者完成，旨在解决视频句子定位问题，即在长视频中找到与自然语言查询最相关的视频片段。传统方法需要大量手动标注，成本高昂且效率低下。本研究提出了一种零样本方法，通过生成伪事件和伪查询来训练模型，但这些方法存在简单性、不一致性和噪声问题。为此，研究提出了一种噪声抗性的结构化伪标签生成方法。首先，使用预训练的图像描述模型生成复杂的自由形式伪查询，然后通过计算视频帧和伪查询的相关性生成伪事件，确保事件内外的相关性差异。通过滑动窗口选择最佳伪事件，并根据事件质量和重叠度筛选伪查询。为减少标签噪声影响，研究通过样本重加权和标签优化来降低噪声样本的贡献，并使用高置信度的预测作为新的伪标签。实验在两个数据集上进行，结果表明该方法在多个指标上优于现有方法。研究的代码已公开。</sample>
    <sample id="330">在主动学习时，累积训练比迭代训练更有效。研究发现，累积策略在不同策略中表现出等同或优于迭代策略。累积策略通过积累所有来自主动注释的数据来更新模型，而迭代策略则仅在最新的数据集上进行训练。这表明，在主动注释过程中，累积更新能够更好地提高模型性能。</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">MuDa 基准中的数据是从 TED 讲座的转录中获得的，这些转录已经从英语翻译成了 14 种不同的语言。</sample>
    <sample id="333">The paper introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," a novel framework designed to enhance the generalization and performance of neural machine translation (NMT) models. The authors, including collaborators from Shanghai AI Lab, Nanjing University, and the University of Hong Kong, address the issue of non-smooth representation spaces in NMT models, which leads to poor performance in areas with sparsely dispersed low-frequency tokens. To tackle this, they propose the kNN-MT approach, which smooths predictions by leveraging nearest neighbors in the representation space. However, kNN-MT faces challenges such as time-consuming neighbor retrieval and static datastores.

To overcome these limitations, the INK framework is introduced. It injects kNN knowledge into the NMT model through a training loop that involves extracting kNN knowledge to guide an adapter in adjusting representations. The updated representations are then used to refresh the datastore asynchronously. The training loop continues until convergence, optimizing the adapter with a combined learning objective. The INK framework aligns contextualized representations with token embeddings and kNN token embeddings to enrich semantic meanings and address sparsity issues.

Experiments using the WMT’19 German-English news translation task model demonstrate that INK significantly improves the representation space. The research questions explored include the feasibility of smoothing the representation space with a small adapter, the performance improvement from using kNN knowledge, and the benefits of combining an adapter with a datastore. Results show that INK outperforms the state-of-the-art kNN-MT system, achieving higher BLEU scores with less memory space. The framework also reveals that while the adapter alone refines the representation space, combining it with a datastore yields further improvements.

In conclusion, the INK framework iteratively refines the NMT model's representation space using kNN knowledge, resulting in better translation performance, reduced memory usage, and faster inference speed. The system achieves an average gain of 1.99 COMET score and 1.0 BLEU score compared to existing kNN-MT systems, highlighting its effectiveness in enhancing NMT models.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">跨语言转移是指在训练阶段使用一种源语言的数据，然后在推理阶段将模型应用于另一种目标语言的过程。在这个过程中，模型需要能够将源语言的查询翻译成目标语言的语义表示。这包括两种主要的设置：零样本跨语言转移（Zero-shot cross-lingual transfer），即在没有目标语言的训练数据的情况下进行转移；以及少样本跨语言转移（Few-shot cross-lingual transfer），即使用少量目标语言的训练数据进行转移。这些设置帮助评估模型在不同语言之间的泛化能力。</sample>
    <sample id="337">The research presented, "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning," addresses the challenge of representing out-of-vocabulary (OOV) words, which are crucial for the performance of embedding-based models. The study introduces a novel approach inspired by human learning habits, focusing on word formation and association to infer the meanings of OOV words. A key innovation is the development of a Word Relationship Graph that mimics lexical rules, allowing for the natural association of OOV words with relevant words through a two-level graph structure.

In this structure, each word or wordpiece is treated as a node, with its embedding serving as the node attribute. The first layer retains all nodes to preserve complete wordpiece information, while the second layer samples a fixed number of nodes to reduce noise. To assign attributes to OOV nodes, a self-attention network is employed, leveraging the characters of the OOV words. The model uses two levels of Graph Attention Networks to extract important information and minimize noise, concatenating and fusing the initial input with hidden embeddings to achieve node-level representation.

A readout block layer is incorporated to capture the entire graph's information, summarizing word formation into a graph-level representation. A simple one-layer Graph Convolutional Network is deemed sufficient for capturing the relationships between subunits of a word. To align with the vector space of background embedding models, contrastive learning is applied in the loss function, using NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself.

The model's effectiveness is demonstrated through extensive experiments, showing superior performance over baselines in both intrinsic and extrinsic tasks. It enhances both static and contextual models in downstream tasks. The research also explores the model's applicability to other languages, noting that agglutinative languages, which form words by directly stringing morphemes, are well-suited for the model. Fusional languages, which link morphemes, present more challenges. However, the model performs well with English due to reasonable word segmentation. The study concludes that the model's ability to handle complex word formations depends on the rationality of word decomposition.</sample>
    <sample id="338">Bingsheng presents the research titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," a collaborative effort by Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The study addresses the challenge of evaluating the quality of human-annotated explanations, which are often subjective and task-dependent. Unlike labels, explanations can vary significantly, making it difficult to use them as a gold standard for model training.

The research introduces a unified structure to convert various tasks into a multiple-choice format, facilitating the analysis of explanation utility. This structure includes baseline settings without explanations and infusion settings where explanations are additional inputs to sequence-to-sequence models. The team conducted experiments on five large-scale datasets, including CoS-E, ECQA, e-SNLI, and ComVE, to evaluate the utility of explanations.

Key observations include that fine-tuning with explanations does not necessarily impart new knowledge but encourages models to rely on explanations for predictions. The study found that CoS-E explanations are less helpful than ECQA explanations for baseline models, highlighting the task-dependent nature of explanations. Fine-tuning with even a small amount of data incorporating explanations can significantly improve model performance.

The researchers propose a novel evaluation metric, TREU, which extends the simulatability score by assessing the helpfulness of explanations during fine-tuning. TREU evaluates two models fine-tuned with baseline and infusion settings to compare performance differences. The metric was tested on T5 and BART models across the datasets, showing that TREU better reflects the utility of human-annotated explanations than the simulatability score.

The study found that TREU scores consistently ranked dataset qualities across models, while the simulatability score struggled with certain datasets like ComVE and e-SNLI. TREU scores varied by entailment category in e-SNLI, indicating that the helpfulness of explanations depends on the task and explanation format. The research supports the hypothesis that explanation utility is influenced by factors such as negation and counterfactual writing styles.

In summary, the research proposes a unified data structure, conducts preliminary experiments on explanation utility, and introduces the TREU metric, which outperforms simulatability scores. The findings emphasize the importance of high-quality human collaboration in annotation tasks and recommend similar quality checks for future research.</sample>
    <sample id="339">Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a collaborative work with Varun, I-Hung, Anoop, Kai-Wei, and Aram. The study addresses the need for large-scale, high-quality paraphrase data in NLP, which is crucial for applications like question answering, chatbots, and enhancing robustness. Existing datasets, such as MRPC, PAN, and Quora, are high-quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity.

The proposed solution leverages Abstract Meaning Representations (AMR) graphs to create syntactically diverse paraphrases. AMR graphs capture the abstract meaning of sentences, with nodes representing semantic concepts and edges representing semantic relations. The focus, or root node, indicates the main assertion of the sentence. The process involves using a pre-trained AMR parser to obtain the AMR graph of a source sentence, changing the focus by randomly sampling a new root node, modifying edges and labels, and generating text with an AMR graph-to-text generator. This method ensures semantic similarity while introducing syntactic diversity.

ParaAMR, the resulting dataset, contains approximately 15 million source sentences, each with around 6.9 paraphrases. It demonstrates higher syntactic diversity compared to other back-translation datasets while maintaining semantic similarity. Quantitative analyses, including automatic and human evaluation scores, support these findings.

ParaAMR enhances several NLP applications. It improves sentence embeddings, as evidenced by better performance on the STS testing benchmark. It also aids syntactic control in paraphrase generation and boosts few-shot learning through data augmentation. Overall, ParaAMR offers a large-scale, syntactically diverse paraphrase dataset that outperforms existing datasets in various NLP tasks. The dataset is available for further research and application.</sample>
    <sample id="341">The authors used two latency measures: average lagging and computational-aware average lagging. Average lagging measures the latency in terms of time delay, while computational-aware average lagging accounts for the model's computational times to predict the output.</sample>
    <sample id="342">The presentation introduces the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming," authored by Gao Jingsheng, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI. The paper addresses the need for a large-scale video-sourced dialogue dataset to enhance open-domain dialogue systems, which currently rely heavily on text-sourced datasets. The authors highlight the limitations of existing video-sourced datasets, which are either scripted or manually annotated, and emphasize the importance of personalized dialogue for applications like virtual streamers and employees.

The paper proposes LiveChat, a novel dataset constructed from Chinese TikTok and Douyin streaming videos. The dataset creation involves three steps: extracting and transcribing audio from videos, collecting audience comments to construct dialogues using a reply-to-whom matching method, and gathering persona information for personalized dialogue generation. Persona extraction is divided into basic profiles through manual labeling and advanced profiles using rules and trained classifiers.

LiveChat is compared with existing open-domain dialogue datasets, showcasing its larger scale, video source, personal annotations, and longer average sessions. The paper presents experiments on two benchmark tasks: Response Modeling and Addressee Recognition. The results indicate that persona information and longer sessions improve response modeling, while single-stream BERT outperforms double-stream BERT in addressee recognition, despite the benefits of persona information.

Additionally, the paper evaluates pre-trained dialogue models on LiveChat, finding that BART performs better than other models, confirming the dataset's distinctiveness. Human evaluation results show that large language models (LLMs) provide rich informativeness. Experiments on in-context learning reveal that performance improves with more demonstrations but slightly decreases beyond eight shots due to noise from random manual selection.

In conclusion, LiveChat is a significant advancement in Chinese video-sourced and personalized dialogue datasets, demonstrating the advantages of selected persona profiles and longer sessions in learning personalized responses. Future work will focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">你好大家，我是阿克沙塔，今天我和我的合著者马丁一起介绍我们的工作《KITMUS测试：评估多源知识整合》。这项工作是麦吉尔大学、Mila和微软研究之间的合作。自然语言理解模型依赖于多种知识来源，例如通常通过预训练获得的嵌入在参数中的知识，以及在推理时提供的输入知识。最近的研究表明，模型可以利用预训练时的知识来解决任务。但自然语言理解通常还需要在推理时提供的知识。例如，在句子“约翰在电视上看到了新当选的总统。”中，预训练参数中可以包含总统的职责和电视的信息，但它们不能可靠地知道这个实例特定的实体“约翰”是谁，或者新总统是谁，因为总统可能在预训练后发生了变化。因此，成功的知识密集型自然语言理解任务模型需要能够整合和使用预训练时和推理时的知识。在本文中，我们提出了一个诊断测试套件，用于知识整合。我们引入了一个指代消解任务，旨在探测模型在不同知识源中获取知识的能力。我们通过人类研究参与者和已建立的指代消解模型对数据集进行评估。这里有一个我们数据集中的例子。Servin是一名法官。Kea是一名面包师。Servin和Kea在公园里相遇。在一个长时间的工作日里，他在法庭上决定案件后，很高兴能够放松。这里的任务是识别代词“他”所指的正确实体，这里是Servin。解决给定代词的指代需要两种类型的信息。首先是实体特定的知识，例如“Servin是一名法官。”其次是背景知识，例如“法官在法庭上决定案件。”通常，背景知识是在大型语言模型的预训练过程中学习的，而实体特定的知识通常在推理时观察到。我们调整这两种信息的可用性，使其可以在单一来源中找到，或在多个来源中找到。我们定义了KITMUS的三种设置。首先是典型设置：“背景-预训练”，其中假设背景知识在预训练时可用。其次是“背景-双重”设置，其中背景知识在预训练时和推理时都可用。最后是“背景-推理”设置，其中两种知识类型仅在推理时可用。这个最后的设置尤其有趣，因为它模拟了解决任务所需的背景知识不包含在模型的预训练数据中的情况。例如，因为自预训练以来出现了新的职业。这里是我们如何控制事实在真实来源中的可用性的一个例子。在“背景-预训练”设置中，我们假设背景知识“政治家寻求在政府中当选的席位”包含在预训练参数中，并在推理时上下文中提供实体特定的知识“Chichester是一名政治家。”在“背景-双重”设置中，我们除了在推理时上下文中提供实体特定的知识外，还提供关于政治家的背景知识。在“背景-推理”设置中，我们提供虚构职业“mirituer”而不是政治家，因为“mirituer”不太可能包含在预训练参数中。我们通过人类研究参与者和已建立的指代消解模型对数据集进行评估。在这个图中，我们展示了在最具挑战性的“背景-预训练”设置下表现最好的模型的结果。没有在KITMUS上进行任务特定的训练，两个模型表现不佳。然而，当在KITMUS上进行训练时，C2F和BERT4Coref的表现显著优于随机选择。这表明，当在通用指代消解数据集上进行训练时，大多数模型学会利用表面线索，而这些线索在KITMUS测试中被移除时不再有用。使用虚构知识的额外实验表明，即使是表现最好的模型，也无法可靠地整合仅在推理时提供的背景知识。总结我们论文的主要要点，许多指代消解模型在没有任务特定训练的情况下似乎无法在不同来源中进行推理。然而，有任务特定训练的一些模型可以成功地整合多个来源的知识。然而，即使是表现最好的模型，也似乎在仅在推理时提供的背景知识上整合不可靠。如果你对更多细节感兴趣，请查看我们的论文，并在GitHub上查看数据集和代码。谢谢你们的倾听。</sample>
    <sample id="344">基于树的方法的缺点包括：树通常不是直接给定的，需要通过某种方式获得，这可能是一个复杂且计算成本高的过程。通常需要进行大量的正式化预处理，例如处理逻辑形式中的变量符号，以及可能需要专门的语法诱导程序。</sample>
    <sample id="345">这篇论文由Matthias Lindemann、Alexander Koller和Ivan Titov合作撰写，探讨了在不使用树结构的情况下实现组合泛化的方法。组合泛化是指模型能够处理在训练中单独看到的短语的更深层次的递归和未见组合。在语义解析中，模型在训练时看到简单的递归，但在测试时面对更复杂的递归结构。传统的序列到序列模型在这种情况下表现不佳，因为它们往往无法保持输入和输出之间的系统对应关系。常见的解决方法是将树结构整合到模型中，以捕捉句子与逻辑形式之间的组合过程。然而，树结构的获取通常复杂且计算量大。本文提出了一种不依赖树结构的神经网络序列到序列模型，通过两步预测输出：首先，为每个输入标记一个无序多集，表示输出中将出现的令牌；其次，使用另一个模型预测一个置换，将这些令牌排列成正确的顺序。这种方法不对可能的置换施加硬性限制，使其灵活且表达力强。在实验中，该方法在COGS基准测试中显著优于其他无树结构模型，尤其是在更深层次的递归泛化方面。论文还解决了训练过程中的一些技术挑战，如输入和输出之间对齐的缺失以及多种可能的置换中选择正确的置换。为了解决这些问题，研究者引入了一种GPU友好的连续松弛方法，以近似解决NP难问题，并通过解决方案进行反向传播，学习更符合语言学的置换。</sample>
    <sample id="346">论文中没有提到作者所属的机构。</sample>
    <sample id="347">你好，我是Myra，今天我将介绍我们的论文《标记人物：使用自然语言提示测量语言模型中的刻板印象》。这项工作是与Esin Durmus和Dan Jurafsky合作完成的。近年来，许多研究人员记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍存在。然而，这些测量方法存在各种局限性。它们通常依赖于耗时的手工构建数据集，并且通常只能测量特定的刻板印象，这意味着它们不能很好地推广到其他人口群体或背景，或者它们只捕捉到非常广泛的负面关联，例如与特定群体的负面关联。此外，大多数在这一领域的工作并未考虑交叉性，即多重社会身份可以加剧偏见并成为独特的伤害源。

为了克服这些局限性，我们依赖于这样一个事实：这些新的指令调整的LLMs在响应指令和提示方面表现出色。因此，我们可以要求模型生成一个人物，这是一个使用提示如“想象你是一个亚洲女性。描述一下自己。”来描绘的虚构个体。我们可以立即看到这种方法对任何人口群体都非常普遍，因为我们可以在提示中指定任何身份标记。这里有一些GPT-4的示例生成。立即我们可以看到，虽然输出在传统意义上不是明显的负面或有毒的，但有一些有趣的模式。亚洲女性被描绘为谦逊；中东女性被用“异国情调”等词语描述，指的是迷人的地区。而两个有色人种的人物都提到了祖先，而白人男性人物则没有。

为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些人物。我们用于生成这些人物的提示受到一项研究的启发，该研究向人类受试者提供了这些提示，发现这也能够揭示种族刻板印象。此外，这使我们能够直接比较我们生成的人物和人类撰写的回应。第二部分是标记词法，这是一种识别区分标记群体和未标记群体的词语的方法，我将很快详细说明。这种方法的好处是，我们可以获得非常具体的刻板印象和模式，而无需依赖任何特定词汇。

标记词法方法基于社会语言学概念“标记性”，该概念指出存在一个未标记的默认值，任何与该默认值不同的群体都是语言上的标记。例如，“战士”通常与男性相关。因此，当人们描述一个女性战士时，他们通常会明确指出“女性战士”，并用“女性”来标记该术语。更广泛地说，社会中的主导群体在语言和社会上都是未标记的，而边缘化群体通常是标记的。在我们的方法中，我们首先确定未标记和标记的群体，然后使用“战斗词”方法来比较人物，该方法使用加权对数比率来区分每个标记群体的顶级词语。例如，对于黑人女性的人物，我们将使用“战斗词”并将其与白人人物和男性人物进行比较，因为这些是两个对应的未标记群体。

现在，让我们看一些结果。首先，我们使用刻板印象词汇表，发现生成的人物中包含了更多的刻板印象，而人类撰写的人物则较少。然而，当我们实际查看词汇分布时，我们发现了非常不同的情况。虽然生成的人物中词汇词的比率较高，但人类撰写的人物具有更广泛的词汇分布，而生成的人物中的刻板印象词汇只包括“高大”和“运动能力强”等词语。这些词语至少是中性的，甚至是积极的。事实上，这个词汇表并没有很好地捕捉到我们之前幻灯片中看到的有害模式。

因此，我们将转向我们的标记词法方法的结果，以展示这些看似积极的词语如何促成刻板印象和本质化的叙事。在我们的分析中，我们揭示了这些看似积极的描绘反映了有害的模式。首先，来自我们的群体的顶级词包括“文化”、“传统”、“自豪”和“异国情调”等词语。这些词语仅通过与身份的关系来定义这些群体，并将它们与白人规范区分开来。这延续了对这些群体的歧视和他者化的长期历史。此外，这些词语反映了许多常见的刻板印象，尤其是对有色女性。例如，描述拉丁裔女性的词语包括“活泼”和“丰满”，这与热带主义的刻板印象有关。对于亚洲女性，词语包括“娇小”、“柔和”和“丝绸般的”，这与亚洲女性被视为非常性感、顺从和温顺的长期历史有关。最后，对于黑人女性，我们看到一些顶级词是“强壮”和“坚韧”。这与人们所称的“强大的黑人女性”典型有关。虽然最初看起来是积极的，但有研究表明，这种典型实际上是有害的，因为它对这些人口群体施加了巨大的压力，要求他们在面对社会障碍时表现出坚韧和强大，而不是改变这些障碍，这导致了这些人群的负面健康结果，以及其他伤害。

更广泛地说，我们发现每个标记群体的词语几乎完全反映了非常本质化的叙事。基于这些模式，我们得出了三项建议，供模型所有者考虑。首先，我们作为研究人员应该关注积极的刻板印象和本质化的叙事。我们还应该使用交叉性视角来研究偏见和伤害，因为如果不这样做，可能会忽略许多事情。最后，应该有更多关于偏见缓解方法的透明度，因为例如，这些积极的刻板印象可能是由于某种奇怪的过度价值对齐，或者可能是由于其他反刻板印象方法导致的这些有害模式。我们不能做出任何假设，也无法进一步研究，除非有更多的透明度。谢谢大家的聆听。祝大家在ACL上玩得愉快。</sample>
    <sample id="348">Myra介绍了她们的研究论文《Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models》，与Esin Durmus和Dan Jurafsky合作。研究探讨了大型语言模型（LLMs）中的社会偏见和刻板印象问题。传统方法依赖于耗时的手工数据集，且只能测量特定的刻板印象，缺乏泛化性。此外，这些方法往往忽视交叉性别问题，即多重社会身份可能导致复合偏见。

为解决这些问题，研究利用了新型指令调教的LLMs，通过生成人物描述来检测刻板印象。例如，通过提示“想象你是一个亚洲女性，描述自己”，可以观察到不同人物描述中的模式。研究发现，亚洲女性被描述为谦逊，中东女性被称为异国情调，而女性常提及祖先，而白人男性则没有。

研究方法分为两部分：首先是生成人物描述，其次是“标记词”方法，用于识别标记群体与非标记群体的区别。通过比较生成的人物描述与人类写的描述，研究发现生成的描述中包含更多刻板印象，但这些刻板印象往往是正面或中性的，如“高大”和“运动能力强”。

研究还发现，这些正面的描述反映了有害的模式，如将文化、传统和自豪等词汇用于定义某些群体，从而强化歧视和他者化。例如，拉丁裔女性被描述为“活泼”和“丰满”，亚洲女性被描述为“纤细”和“柔和”，黑人女性被描述为“强壮”和“坚韧”。这些描述虽然看似正面，但实际上可能对这些群体造成压力，导致负面的健康结果。

研究建议模型所有者应关注正面刻板印象和本质化叙事，使用交叉性别视角研究偏见和伤害，并增加偏见缓解方法的透明度。</sample>
    <sample id="349">大家好，我是中国科学技术大学的杨景伟。很高兴为我们的论文做一个简短的宣传视频。我们的论文题目是“通过后门水印保护大型语言模型嵌入服务的版权”。首先，我们来介绍嵌入服务的背景。目前，像GPT、LLAMA、PALM这样的大型语言模型在自然语言理解和生成方面表现出色。嵌入服务是基于大型语言模型构建的一种服务，用于协助各种NLP任务。例如，OpenAI提供了基于GPT的嵌入API。然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，保护嵌入服务的版权是必要的。为了保护嵌入服务的版权，一种解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含该水印。水印方法需要满足以下几点：首先，方法应适用于嵌入服务；其次，水印不应降低提供的嵌入的实用性；第三，水印应对攻击者足够隐蔽，或者攻击者可以轻松移除水印；最后，水印在模型提取过程中需要能够转移到攻击者的服务中。现有的工作可以大致分为四类。然而，这些方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本文中，我们提出了“嵌入标记”，这是一种适用于嵌入服务的基于后门的水印方法。接下来，我将详细介绍我们的嵌入标记。嵌入标记包括两个主要步骤：水印注入和版权验证。在这些主要步骤之前，我们首先选择一个触发集。触发集是一组频率在中等间隔的词。我们假设提供者可以收集一般的文本语料库，并用它来计算词频。在水印注入中，我们首先定义一个目标嵌入。当用户向提供者服务发送一个句子时，提供者计算句子中触发词的数量。提供的嵌入是目标嵌入和原始嵌入的加权和。目标嵌入的权重与句子中触发词的数量成正比。当句子中触发词的数量大于m时，提供的嵌入恰好等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构建一个后门数据集和一个良性数据集。后门数据集中的句子中所有词都属于触发集，而良性数据集中的句子中所有词都不属于触发集。然后，提供者向窃取者的服务请求这些数据集的嵌入。计算请求嵌入与目标嵌入之间的余弦相似度和L2相似度。我们计算良性数据集和后门数据集之间的相似度差异，定义为delta余弦和delta L2。同时，我们还应用KS检验，并使用其p值作为第三个指标。我们在四个数据集AG News、MIND、SST2和Enron Spam上进行实验。我们假设提供者使用wiki文本数据集来计算词频。在四个数据集上的结果表明，我们的嵌入标记在保持下游任务的良好实用性的同时，具有很好的检测性能。我们还通过在四个数据集上可视化句子的嵌入来验证提供的嵌入的隐蔽性。图例表示每个句子中触发词的数量。如图所示，很难区分后门嵌入和正常嵌入。这就是全部内容。谢谢大家。欢迎与我们讨论。</sample>
    <sample id="350">在本次演示中，Simone Tedeschi介绍了他们的论文《今日NLU中的超人类表现意义是什么？》。论文探讨了自然语言处理（NLP）领域近五年来的评估标准，即领先者评估，以及系统在流行基准测试中达到或超越人类水平的现象。这些基准测试被称为“饱和基准测试”，但其意义尚不明确，特别是在涉及知识、推理和推断的任务中。

论文指出，尽管模型在某些任务上超越了人类，但它们在泛化能力、对对抗攻击的脆弱性、对不重要的扰动的过度敏感性等方面存在缺陷。因此，论文研究了领先者评分在多大程度上可靠地比较模型和人类。通过分析SuperGLUE和SQuAD两个流行的NLP和NLU基准测试，研究发现在某些任务中，系统确实超越了人类。然而，通过手动检查这些数据集，研究发现了使人类与系统比较不公平的错误来源。

一个主要问题是系统和人类在不同的数据集上进行评估。例如，在BoolQ任务中，系统在超过3000个实例的完整测试集上进行评估，而人类仅在100个实例的小子集上进行评估。此外，研究发现了一些地面真值答案中的错误，这些错误影响了评估的准确性。

论文还强调，研究人员通常使用模糊的方法来估计人类表现，例如简单的平均或多数投票，而不是与最佳人类表现进行比较。此外，研究发现，不同任务的人类评估者的报酬差异很大，有时非常低，这可能影响评估的质量。最后，研究指出，关于评估者池的详细信息往往缺失，这使得关于超人类表现的声明缺乏科学意义。

总之，论文讨论了超人类表现在NLU中的意义，并解释了为什么这些声明尚未得到充分的科学支持。论文还提供了建议，以避免重复相同的错误，并构建更可靠的基准测试。</sample>
    <sample id="351">Shuheng presents a paper investigating the generalization capabilities of CoNLL-2003 named entity taggers in 2023. The study addresses whether these models, developed nearly two decades ago, can effectively handle modern data. To explore this, the authors created the CoNLL++ Dataset, using Reuters News from 2020 annotated with CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the original test sets and the new CoNLL++ dataset, measuring generalization through changes in F1 scores.

The research identifies three key factors for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models were found to generalize better to new data, larger models showed improved generalization, and more fine-tuning examples enhanced performance. The study also examined potential causes for performance drops, focusing on adaptive overfitting and temporal drift. Adaptive overfitting, characterized by diminishing returns on new test sets, was not observed, as improvements on CoNLL-2003 translated to greater improvements on CoNLL++. However, temporal drift, the performance degradation due to the increasing temporal gap between training and test data, was confirmed as the main cause of performance drops. Retraining models with more recent data showed that performance degraded with larger temporal gaps.

The conclusion is that effective generalization requires a combination of better model architecture, larger model size, and more fine-tuning examples. The study found that the performance drop is primarily due to temporal drift, not adaptive overfitting, despite the long use of CoNLL-2003. Ultimately, the paper affirms that CoNLL-2003 taggers still perform well in 2023, but emphasizes the need for further research to enhance model generalization. The authors encourage readers to explore their paper and dataset for more insights.</sample>
    <sample id="352">ABC-Eval 代表 Annotating Behaviors in Chat，是一种新的维度方法，用于评估对话人工智能，通过明确注释模型回应中是否表现出某些行为来减少人类评估的主观性。</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation from natural language descriptions (NLDs). The authors highlight that state-of-the-art methods often fail to handle missing specifications in NLDs, which is a common issue in real-world applications. To tackle this, they propose an interactive approach where clarification questions (CQs) are used to gather additional specifications.

The paper introduces the task of generating code by asking clarification questions, focusing on operation-level specifications. The authors create a synthetic dataset, CodeClarQA, which includes clarifications on key operations. They use a method to identify key operations by representing them in latent space and computing similarity scores between NLDs and operation documentation. If the similarity scores are below a threshold, the key operation is considered missing.

The dataset creation involves annotators and templates to generate CQAs for missing key operations, which are either yes-or-no or multiple-choice questions. The authors use heuristics to extract key operations from a code knowledge graph generated by Graph4Code. Their method effectively identifies missing key operations, with MPNet showing the best performance among tested models.

Error analysis reveals that false positives are rare, indicating effective CQA generation. Common errors include taxonomy issues and the use of operation documentation instead of argument values. The paper presents a pipeline for CQ-driven code generation, consisting of a Clarification Need Predictor, a Question Selector, and a Code Generator. The pipeline's performance improves with higher-ranked CQs but still underperforms compared to model-only trainers due to the challenge of the CQ ranking task.

The authors hypothesize that their task is more challenging than existing CQ ranking tasks and that clarifications aid code generation. They find that clarified key operations lead to better code generation, with Oracle CQAs resulting in predictions close to the ground truth. However, the task remains challenging as top-ranked CQs often do not match reference CQAs, leading to incomplete predictions. The paper concludes by inviting feedback on their approach and contributions.</sample>
    <sample id="354">直到 2020 年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于 5 个百分点。</sample>
    <sample id="355">你好，我叫Vasudha，是斯通布鲁克大学计算机科学博士研究生。我想在ACL 2023上以长篇论文形式展示我们的工作，题为“迁移学习用于不协调检测：解决稀有类别挑战”。我们首先定义认知不协调，并解释为什么在语言中研究它很重要。简单来说，认知不协调是指两个信念或行为不一致，例如一个人说：“我知道香烟可能会杀死我”，然后又说：“会议后我抽了几支烟”。这两个信念和行为不一致，处于不协调状态。进一步提到“我不认为我能没有它们继续工作”来为第二个行为辩护，这两者之间存在协调关系。虽然认知不协调是我们在日常决策中常常经历的现象，但在语言中表达出来的不协调关系相对稀有。为什么这很重要？研究认知不协调可以帮助我们理解人们之间的分歧效应，跟踪人群中的信念价值和态度变化。高水平的认知不协调也与焦虑障碍有关，可以帮助更好地理解人们的心理健康。研究语言中表达的不协调也有助于理解极端主义和脆弱群体的极化。最后，认知不协调对于理解个体的认知风格很重要，有助于更好地理解决策过程。

为了创建认知不协调资源，我们进行了大规模的不协调关系注释。我们采用了以不协调为先的方法，如流程图所示。推特通过PDTB解析器传递，根据我们论文中描述的指南对话语单位对进行注释。如图所示，不协调仅在注释对中的3.5%中找到。在收集约1,000个对话语单位对的例子后，我们对仅有43个不协调例子的初始分类器进行了训练。毫不奇怪，分类器的表现并不比随机好。鉴于不协调的低发生率和缺乏任何此类数据集，我们面临绝对稀有性的问题。为了缓解这一问题，我们在组合迁移学习和主动学习上进行了实验，以便在更少的注释运行中收集更多的不协调样本，降低总体注释成本，同时提高不协调检测。

由于初始模型无法捕捉不协调类别，我们通过从相关任务中迁移权重来启动主动学习过程。我们从两个不同的任务中迁移：主题独立的不协调立场分类，这是一个确定两个来自不同人的辩论陈述是否一致或不一致的任务，不考虑主题，称为辩论，以及PDTB中二元分类的扩展和比较类别，因为这两者与协调和不协调的概念密切相关，我们称之为CE。我们发现，在零样本下，迁移后在注释数据集上的性能已经明显优于随机，最好的是AUC为0.62。进一步，通过在两个任务上迭代微调，我们发现CE任务的微调后再进行辩论的微调，零样本性能显著提高。因此，这是我们用于启动主动学习的模型。

接下来，我们确定在每轮主动学习和注释中更新模型的最佳方法。累积方法累积了迄今为止从主动注释中收集的所有数据，而迭代方法则通过在最新收集的数据集上训练来更新模型。在不同策略中，我们发现累积方法在所有情况下表现相当或更好。接下来，为了提高不协调例子的数量，我们使用概率稀有类策略（PRC）来选择当前模型在任何稀有轮次中高度可能的例子。我们将其与社区中常用的其他最先进的主动学习策略进行比较。我们发现，提出的PRC策略比其他最先进的策略表现更好，尽管差异较小。注意，随机性能显著较低。在进一步的主动学习轮次中，使用两种最佳策略，我们将不协调分类AUC提高到0.75，这是我们在此任务上迄今为止的最佳表现。我们还检查了每种策略的可行性，以及对注释者的质量和成本。我们发现，PRC具有最高的不协调百分比，并且在稀有类别中表现最佳。然而，注释者也发现这些例子难以处理。总之，我们发现PRC是一种简单的主动学习策略，用于稀有类别的获取和适当设计的迁移学习任务的冷启动，有助于显著改进。我们还发现，迭代更新对于从不同领域的迁移学习有用，而在领域内的主动注释则受益于累积更新。这里是我们核心数据集和论文的链接。如果您有任何问题，请随时联系我们。谢谢。</sample>
    <sample id="356">The paper does not specify the authors' affiliated institutions.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">这篇论文有五位作者：Kayo Yin、Patrick Fernandes、Emmy Liu、André F. T. Martins 和 Graham Neubig。</sample>
    <sample id="359">The method, EDAtt, was compared with the state-of-the-art architecture specifically tailored for simultaneous pre-translation. Additionally, it was compared with popular strategies applied to offline models, such as the Wait-k strategy and the Local Agreement.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University's Language Technologies Institute and research director at JP Morgan AI Research, presents "CounterComp," a method to enhance compositional generalization in multi-step quantitative reasoning for question answering tasks. The focus is on improving neural models' performance on tasks involving financial tables, where questions require multiple arithmetic operations to derive answers. Current models struggle with these tasks due to memorizing spurious patterns, such as associating specific tokens with common operations.

The CounterComp approach addresses this by leveraging counterfactual scenarios. It identifies interchangeable components in questions that affect the operations in the output. By altering these components, the model can generate counterfactual examples, which are used to create positive and negative examples. Positive examples involve changes that do not affect the output, while negative examples do. These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, which adjusts based on the extent of changes in the questions.

This auxiliary loss is added to three state-of-the-art baselines, showing consistent performance improvements, particularly when reasoning steps exceed two. The method enhances performance on both in-distribution and out-of-distribution samples, aligning with the goals of compositional generalization. Qualitative analysis indicates that the CounterComp loss helps models focus on more meaningful tokens related to operational terms in the output. The presentation acknowledges co-authors, advisors, and the audience.</sample>
  </task>
</testset>