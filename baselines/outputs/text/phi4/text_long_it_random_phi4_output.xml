<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici includono grandi set di dati web crawlati, con copertura significativa di media di notizie politiche come il New York Times, Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">Le affiliazioni degli autori dell'articolo sono McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">Tu Yi from Ant Group presents a paper on Visually-rich Document Understanding (VrDU), focusing on understanding documents like forms, receipts, and posters. The paper introduces LayoutMask, a novel pre-trained model addressing reading order issues in document pre-training. Unlike existing models that use global 1D positions, LayoutMask employs local 1D positions, which do not provide cross-segment orders, encouraging the model to infer global reading order using 1D position, 2D position, and semantic information. This approach enhances text-layout interactions and layout representations.

LayoutMask incorporates two new masking strategies in the Masked Language Modeling task: Whole Word Masking and Layout-Aware Masking. Whole Word Masking masks at the word level, challenging the model to use more context for predictions, thus promoting text-layout interactions. Layout-Aware Masking increases the probability of masking the first and last words of each segment, encouraging the model to learn cross-segment orders.

Additionally, LayoutMask introduces a new pre-training objective, Masked Position Modeling, which involves recovering randomly masked 2D positions. This task is akin to a cloze test, requiring the model to use semantic relations and 2D position clues to infer word positions, promoting text-layout interactions and better layout representation learning.

Experiments show that using Local-1D positions outperforms Global-1D on FUNSD and SROIE datasets, although it slightly underperforms on CORD. The performance gap is mainly due to the entity "Total," which is challenging to recognize with Global-1D due to its complex layout. Overall, LayoutMask's innovative strategies and objectives demonstrate improved adaptability and performance in VrDU tasks.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase. Mi chiamo Regina Stodden e vi guiderò attraverso la prima parte della presentazione. Definiamo innanzitutto la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, come persone con problemi di lettura o parlanti non nativi. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testi, ad esempio di documenti o frasi. E l'esempio qui sotto mostra una coppia di frasi parallele allineate di una frase tedesca complessa e la sua traduzione in linguaggio semplice. Per semplificare la frase, sono possibili diverse tecniche, come la sostituzione lessicale, l'eliminazione di proposizioni, il riordino o l'inserimento di parole. Proprio per questo motivo proponiamo il nostro nuovo corpus, DEPLAIN, poiché negli ultimi anni ci sono stati alcuni problemi con i corpora esistenti. Ad esempio, questi corpora sono troppo piccoli per addestrare un modello di semplificazione del testo. Gli altri tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere soggetti a errori nelle loro allineazioni. Pertanto, proponiamo il nostro nuovo corpus DEPLAIN, che è suddiviso in due sottocorpora: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa si basa su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, risultando in circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diversi domini e abbiamo allineato tutti questi 750 documenti, sia manualmente che con metodi di allineamento automatico. In totale abbiamo ottenuto 30.450 coppie di frasi. Abbiamo analizzato un po' di più le nostre coppie di frasi, ad esempio per quanto riguarda il tipo di semplificazione. Come potete vedere, i testi della Bibbia sono molto più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti di lingua. A tutti i livelli, riguardo, ad esempio, alla semplificazione lessicale, alla semplificazione strutturale e al livello complessivo di semplificazione. Inoltre, potete vedere che il nostro corpus DEPLAIN presenta una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molto più riordinamenti e inserimenti di parole rispetto al corpus DEPLAIN-web. D'altro canto, nel corpus web abbiamo molto più parafrasi. Ora vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni sono stati proposti molti metodi di allineamento, ma nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli nello stesso linguaggio, con lo stesso contenuto, ma a livelli di complessità diversi. E ora che abbiamo il nostro dataset DEPLAIN, che ha frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi proposti. Abbiamo apportato alcune modifiche ai metodi proposti e abbiamo pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo in tedesco è il metodo MASSalign. Potete trovare anche il codice per eseguire questo metodo sui vostri documenti nel paper. Il secondo caso d'uso che abbiamo mostrato nel nostro paper è un caso di semplificazione automatica del testo adattando modelli di linguaggio per produrre testo semplificato a partire dal testo complesso di input. Abbiamo adattato due modelli diversi. Abbiamo adattato il modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche adattato il normale base mBART per produrre semplificazioni a livello di frase. Potete trovare tutti i checkpoint e potete consultare maggiori dettagli sui punteggi e sugli indicatori di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questo adattamento di base potrebbe ottenere punteggi migliori rispetto ai punteggi di riferimento e abbiamo proposto questi risultati come base di riferimento per il problema della semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL per ottenere un'accuratezza dell'82%-87% quando il modello ha accesso a una parte sovrapposta del background knowledge.</sample>
    <sample id="6">Jiaan presenta il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", un progetto collaborativo con Fandong, Duo, Yunlong, Zhixu, Jianfeng e Jie. Il contributo principale è l'introduzione della "many-to-many summarization", che unifica la multilinguistica e la cross-linguistica in un modello unico capace di elaborare documenti in qualsiasi lingua e generare riassunti in qualsiasi altra lingua. Questo approccio migliora il trasferimento delle conoscenze del compito tra diverse lingue rispetto ai metodi precedenti. È stato proposto il modello PISCES, un modello pre-addestrato per la many-to-many summarization, che impara abilità di modellazione del linguaggio, capacità cross-linguistica e abilità di riassunto attraverso un pre-addestramento a tre fasi: meta pre-training, cross-lingual pre-training e task-specific pre-training. Gli esperimenti su WikiLingua con lingue come inglese, francese, hindi, cinese, thailandese e turco mostrano che il modello many-to-many supera i modelli multilinguistici e cross-linguistici. PISCES, in particolare, supera i baselines come mBART-50 e mT5, con studi di ablation e umani che confermano la sua efficacia. Jiaan invita a consultare il paper per ulteriori dettagli.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora nel 2023, ma la loro capacità di generalizzazione è influenzata da fattori come l'architettura del modello, la dimensione del modello e il numero di esempi di fine-tuning. La principale causa della diminuzione delle prestazioni è la deriva temporale, non l'overfitting adattivo.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, ABC-Eval, riduce la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Questo approccio mira a fornire una valutazione più precisa e affidabile delle diverse dimensioni della qualità del dialogo.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sull'uso di un set di validazione pulito per la selezione del modello.</sample>
    <sample id="10">Per migliorare il punteggio, i progressi possono essere fatti migliorando l'accesso e l'integrazione del modello di linguaggio con il background conoscitivo più ricco e rilevante. Questo include:

1. Migliorare le capacità di ricerca e recupero del modello per ottenere informazioni contestuali più complete e pertinenti.
2. Sviluppare tecniche di comprensione del linguaggio naturale che possano interpretare e utilizzare efficacemente le informazioni contestuali.
3. Addestrare i modelli su set di dati più ampi e diversificati per migliorare la generalizzazione tra domini.
4. Migliorare le capacità di disambiguazione del modello per gestire riferimenti indiretti più complessi.
5. Integrare tecniche di apprendimento multimodale per sfruttare informazioni visive e testuali, specialmente per domini come ricette e libri.</sample>
    <sample id="11">Jack Hessel, un ricercatore presso AI2, presenta un lavoro congiunto su "Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from The New Yorker Caption Contest", collaborando con istituzioni come l'Università dello Utah, Cornell, l'Università di Washington, Air Mail e OpenAI. Il focus è su se i modelli linguistici di grandi dimensioni comprendano effettivamente l'umorismo. Sebbene questi modelli possano generare e spiegare battute, come dimostrato da Google's PaLM, la loro comprensione dell'umorismo è dubbia. Ad esempio, ChatGPT può generare battute che non sono realmente divertenti o che non comprendono appieno, come nel caso di una battuta su un ananas.

Per esplorare ulteriormente, il team ha utilizzato i dati del The New Yorker Caption Contest, un popolare concorso di battute per cartoni. Hanno creato tre compiti: abbinamento di battute, classificazione della qualità e generazione di spiegazioni. I modelli, come CLIP e GPT-4, hanno mostrato prestazioni inferiori rispetto agli umani, con CLIP che raggiunge il 62% di accuratezza nel compito di abbinamento rispetto al 94% degli umani. Anche GPT-4, con descrizioni umane delle immagini, ha mostrato una significativa differenza di prestazioni rispetto agli umani.

Le spiegazioni generate da GPT-4 per le battute spesso contenevano errori, e le spiegazioni umane erano preferite in più di due terzi dei casi in uno studio A/B. Il team è entusiasta di vedere come la comunità utilizzerà il loro dataset e invita a partecipare alla loro leaderboard.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presented his work on "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted in Professor Roy Schwartz's lab at the Hebrew University. The study focuses on adaptive inference methods, which aim to reduce the inference time of large language models by using low-capacity models for simpler samples, thus lowering average inference costs. The two primary adaptive inference methods discussed are Multi Model and Early Exit.

In Multi Model, multiple models are stored and trained separately, with classifiers at the end to decide when to halt computation. This method is versatile and easily extended but incurs high storage costs and overhead, as samples must pass through all previous models before reaching the final one. Early Exit, on the other hand, involves classifiers at intermediate transformer layers, trained together. It offers faster inference and is memory efficient, but shares model parameters among classifiers, potentially leading to lower performance due to conflicting gradients.

Conflicting gradients occur when classifiers update model weights to optimize their own goals, causing interference and degrading overall performance. To test this, Rotem compared Early Exit classifiers with separate Multi Model classifiers, finding that the latter outperformed the former by an average of 2.3%, with the largest gap for early classifiers at 5.2%. The speed/accuracy trade-off showed Multi Model excelling at high inference speeds, while Early Exit performed better with later classifiers due to Multi Model's overhead.

To address these issues, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that trains each layer to receive updates only from the following classifier, thus avoiding conflicting gradients. SWEET closed most of the performance gap between Early Exit and Multi Model, although it sometimes negatively affected later classifiers. In terms of speed/accuracy trade-off, SWEET outperformed both methods at fast speeds and throughout the entire curve for BERT-Large.

The study highlights the existence of conflicting gradients in Early Exit training, provides a fair comparison of Early Exit and Multi Model methods, and introduces SWEET, which opens avenues for future research in fine-tuning algorithms tailored to Early Exit architectures.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo intervento riguarda la Struttura di Dipendenza della Coordinazione. Come sapete, ci sono diverse strutture di dipendenza assunte da diverse teorie e approcci basati su corpora. Ad esempio, nelle Universal Dependencies, la struttura della coordinazione "Lisa, Bart e Maggie" è tale che il primo congiunto è la testa dell'intera struttura coordinata. In questo caso, "Lisa". Un approccio simile è assunto nella Meaning Text Theory di Igor Mel'čuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici, poiché individuano un congiunto. Ora, questi sono approcci asimmetrici alle strutture coordinate, come l'approccio di Praga. L'approccio guidato dalla congiunzione, assunto nei Praga Dependency Treebanks, dove le strutture coordinate sono guidate dalla congiunzione. Otteniamo alcune dipendenze dalla congiunzione a tutti i congiunti. Infine, c'è anche un approccio multi-testa utilizzato, ad esempio, nella Word Grammar di Hudson, dove dicono che tutti i congiunti sono teste della struttura coordinata. Otteniamo dipendenze dal governatore a tutti i congiunti separatamente: "Lisa", "Bart" e "Maggie". L'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due. L'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza, che spiegherò attraverso questi esempi. In inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbiali possono essere più lontani. "Marge ha letto questo ieri" è accettabile perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri questo" è molto peggiore. Qui, tra il verbo e l'oggetto diretto c'è un avverbiale: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo. In tal caso, può essere spostato nella posizione dopo l'avverbiale. Questo è illustrato qui. Entrambe queste frasi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sui fiori ieri." Va bene così, invece di "it", abbiamo questo lungo NP. Ma è anche OK dire, "Marge ha letto ieri questo libro assolutamente affascinante sui fiori." Il ragionamento qui è che questo è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "read" all'avverbiale di lunghezza 7 misurata in parole e da "read" a "book" di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più corto. Per questo motivo, suona abbastanza bene. Violano un principio, ma soddisfano un altro. Ok. Abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedete l'articolo "Perché non userebbero le Universal Dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più brevi. "Sale e pepe" e non "pepe e sale", misurati in sillabe. E anche l'osservazione fatta nell'analisi che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più breve preferisce essere il primo, più forte, giusto? Quindi la proporzione è maggiore di un congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa", così come è il governatore a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. In tali casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha riso" governa la coordinazione di Ted e Ned, questo effetto scompare. Abbiamo mostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale, e in parole la colonna di destra. Mi concentrerò sulla colonna di destra. Cosa vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole, e lo stesso è osservato quando non c'è governatore, come nella coordinazione delle frasi. Tuttavia, quando il governatore è a destra, questa tendenza scompare. E mostriamo nell'articolo come questo fornisca un argomento contro le strutture asimmetriche di coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due. Vedete l'articolo per gli argomenti completi. E parlateci al tavolo durante la sessione poster. Grazie.</sample>
    <sample id="15">Tre autori sono coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici risultano più semplificati rispetto ai testi di notizie o ai testi per apprendenti di lingue.</sample>
    <sample id="17">Shengqiong Wu, a PhD student at NUS, introduces their work on multimodal relation extraction (MRE), which addresses the limitations of traditional text-based relation extraction by incorporating visual data. In scenarios like social media, text alone may not provide enough context, making it difficult to understand ambiguous terms. MRE leverages additional visual evidence to enhance understanding, such as using images of "Bachelor," "Gown," and "Cap" to infer that JFK "graduated at" Harvard.

However, challenges remain, such as internal-information over-utilization, where only parts of the text and visual data are useful, and external-information under-exploitation, where additional context like topic information is needed. To tackle these, the authors propose a Graph Information Bottleneck principle-guided feature refinement and introduce multimodal topic information to enrich context.

Their framework includes five parts: representing text and images with scene graphs, merging these into a unified cross-modal graph (CMG), refining CMG through fine-grained filtering, and enriching features with multimodal topic information. Experiments on a widely used MRE dataset show that their methods outperform text-based and other multimodal baselines.

Ablation studies reveal that both information screening and compensating improve performance, with scene graphs aiding structural modeling. The effectiveness of internal-information screening and external-information exploiting varies with text-vision relevance: higher relevance benefits more from screening, while lower relevance gains from exploiting external information. The proposed method achieves significant improvements over existing models, offering a novel approach to MRE by balancing information subtraction and addition.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è "salt and pepper" rispetto a "pepper and salt", misurato in sillabe.</sample>
    <sample id="19">Zhang Qin, un dottorando presso l'Università di Shenzhen, ha presentato il loro lavoro accettato da ACL 2023, "A Survey for Efficient Open Domain Question Answering". Il lavoro si concentra sul question answering in domini aperti, utilizzando un framework a due fasi proposto da Danqi Chen nel 2017. Questo framework prevede una fase di recupero per estrarre contesti di prova da un corpus di Wikipedia e una fase di lettura per comprendere la domanda e estrarre la risposta. Il recupero utilizza due encoder: uno per la domanda e uno per il documento, con il corpus di Wikipedia pre-elaborato in un indice per un recupero efficiente.

Zhang ha evidenziato le sfide del question answering in domini aperti, tra cui la grande dimensione del corpus di Wikipedia (26 milioni di documenti, 20 GB), l'indice di 65 GB che rallenta la velocità di inferenza, e l'uso di modelli linguistici con milioni di parametri, rendendo difficile l'applicazione in tempo reale e su dispositivi con risorse limitate. L'obiettivo del loro lavoro è sviluppare sistemi efficienti con costi di memoria ridotti, inferenze più veloci e prestazioni comparabili.

Hanno esplorato tecniche per raggiungere questi obiettivi, tra cui sistemi a una fase come i sistemi di recupero solo e generazione solo. Hanno discusso metodi per un recupero più rapido, come la ricerca di vicini approssimativi, e tecniche di lettura veloce come la lettura saltata. Per ridurre le dimensioni dell'indice, hanno considerato il filtraggio dei documenti e la compressione delle dimensioni dell'embedding. Per ridurre le dimensioni del modello, hanno suggerito l'uso di modelli leggeri, la condivisione dei parametri o la progettazione di modelli unici.

Hanno confrontato i modelli esistenti, notando che i sistemi di recupero e lettura offrono un equilibrio tra velocità, memoria e prestazioni, mentre i sistemi di recupero solo creano indici grandi ma rispondono rapidamente, e i sistemi di generazione solo non creano indici ma sono modelli grandi con prestazioni basse. Le loro conclusioni suggeriscono di ridurre le dimensioni dell'indice o del modello per risorse limitate, utilizzando tecniche come la compressione dell'embedding o la distillazione della conoscenza. Per il feedback in tempo reale, i sistemi di recupero solo sono preferibili, mentre i sistemi di recupero e lettura sono più adatti per compromessi.

Infine, Zhang ha discusso lavori futuri, tra cui il deployment su dispositivi a bassa potenza e l'adozione di metriche di valutazione più ampie.</sample>
    <sample id="20">Sì, puoi usare i modelli per la tua ricerca. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face sotto la licenza MIT, e tutti i script di addestramento sono disponibili nel repository GitHub del tuo team.</sample>
    <sample id="21">DEPLAIN-apa contiene testi di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono: un'architettura del modello efficace (in particolare i modelli trasformatori), una dimensione del modello più grande e un numero maggiore di esempi di fine-tuning.</sample>
    <sample id="23">Dan Garrette discute i progressi nella modellazione di immagini testuali, concentrandosi sulle difficoltà dei modelli attuali, come Imagen, nel rappresentare accuratamente il testo. Imagen utilizza un encoder T5-XXL per convertire il testo in rappresentazioni che alimentano un modello di diffusione per generare immagini. Tuttavia, questi modelli spesso falliscono nel rappresentare il testo, specialmente per parole semplici. Questo problema è attribuito all'uso di tokenizzazione SentencePiece da parte di T5, che rappresenta il testo in ID di sottoparola, rendendo difficile per il modello decomporre le parole in lettere individuali.

Gli esperimenti mostrano che i modelli T5, anche le versioni più grandi, hanno una bassa accuratezza nella scrittura, con il modello T5-XXL che raggiunge meno del 70% di accuratezza. Al contrario, i modelli PaLM, più grandi e addestrati su più dati, mostrano un'accuratezza quasi perfetta nella scrittura. ByT5, che utilizza tokenizzazione a byte, offre una rappresentazione più granulare e mostra un'alta accuratezza nella scrittura, poiché può copiare direttamente i caratteri dall'input all'output.

Per migliorare la capacità di rendering del testo di Imagen, gli autori hanno integrato una rappresentazione testuale aggiuntiva da ByT5-small, aumentando solo il conteggio dei parametri del 5%. Questo miglioramento ha notevolmente migliorato la capacità del modello di rappresentare il testo, sebbene non perfettamente, a causa di errori introdotti dal modello di diffusione. I contributi principali del loro lavoro includono il benchmark WikiSpell per modelli testuali, il benchmark DrawText per modelli testo-immagine e una strategia efficiente per migliorare l'abilità di scrittura dei modelli, concatenando un modello consapevole dei caratteri.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in termini di differenza di lunghezza in caratteri, sillabe e parole. L'analisi si è concentrata principalmente sulla misurazione in parole, osservando come la tendenza cresce con la differenza assoluta di lunghezza in parole tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando le statistiche di coordinazione dall'Enhanced Penn Treebank, concentrandosi su come la posizione del governatore influisce sulla lunghezza dei congiunti. Hanno osservato che quando il governatore è a sinistra o assente, il congiunto a sinistra tende ad essere più corto, specialmente quando la differenza di lunghezza tra i congiunti è maggiore. Tuttavia, quando il governatore è a destra, questa tendenza scompare. Queste osservazioni sono state misurate in caratteri, sillabe e parole, con un focus particolare sulle misurazioni in parole.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, in questo caso con solo 43 esempi di dissonanza, ha prestazioni non molto migliori del caso casuale.</sample>
    <sample id="27">L'articolo menziona un solo autore, Shangbin.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione automatica (MT) sensibili al contesto migliorano significativamente la precisione rispetto a quelli indipendenti dal contesto per i fenomeni del discorso come la formalità e la coesione lessicale.</sample>
    <sample id="30">Il paper "LLM-Blender" introduce un framework di apprendimento ensemble per modelli di linguaggio di grandi dimensioni, basato su ranking a coppie e fusione generativa. Yuchen Lin, del team AI2 e USC, evidenzia che, nonostante alcuni modelli come Vicuna abbiano ottime prestazioni medie, non sono sempre i migliori per ogni input specifico. LLM-Blender propone di utilizzare più modelli per ciascun input, selezionando e combinando le migliori risposte.

Il framework a due fasi inizia con l'esecuzione di n modelli su un input X, generando output Y₁ a Yₙ. Un modulo di ranking a coppie, PairRanker, confronta questi output utilizzando un modulo di attenzione incrociata come RoBERTa per determinare quale candidato è migliore per l'input X. Questo processo produce una matrice di confronto, che viene aggregata per ottenere un ordine finale dei candidati. I top K candidati vengono quindi utilizzati in un modello di fusione generativa per produrre l'output finale.

PairRanker si distingue per l'encoding di coppie di candidati insieme all'input X, permettendo un'analisi più accurata delle differenze tra i candidati. L'aggregazione dei risultati utilizza i logit massimi per ottenere le migliori prestazioni, con opzioni efficienti come il bubble sort. I test mostrano che PairRanker è più correlato con il ranking ideale rispetto ad altri metodi.

Per valutare LLM-Blender, è stato creato il dataset MixInstruct, che include candidati da 11 modelli di linguaggio aperti. I risultati dimostrano che LLM-Blender supera i modelli top come Open Assistant e Vicuna in una significativa percentuale di esempi. Il paper conclude che LLM-Blender è un framework semplice ed efficace per l'apprendimento ensemble, migliorando le prestazioni complessive.</sample>
    <sample id="31">Le affiliazioni degli autori dell'articolo non sono fornite nel contenuto fornito.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità riannotando i dataset con annotatori diversi, considerando le loro demografie, e poi confrontando queste annotazioni con i modelli e i dataset esistenti utilizzando il punteggio di correlazione di Pearson. Questo confronto tra annotazioni demografiche e modelli/dataset differisce dalla letteratura sull'accordo degli annotatori concentrandosi sul confronto tra utenti finali e modelli/dataset, piuttosto che solo sull'accordo tra annotatori.</sample>
    <sample id="34">Marcos Treviso presenta "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation," un lavoro collaborativo con Alexis Ross, Nuno Guerreiro e André Martins. CREST combina metodi di razionalizzazione selettiva e generazione di controfattuali per migliorare l'interpretazione delle decisioni dei modelli di classificazione. La razionalizzazione selettiva evidenzia i token rilevanti, mentre la generazione di controfattuali modifica parti specifiche dell'input per riflettere il ragionamento causale umano.

Il framework CREST genera controfattuali mascherando parti dell'input originale e utilizzando un modello di linguaggio mascherato per riempire le lacune. La qualità dei controfattuali è valutata tramite esperimenti umani su dataset IMDB e SNLI, dove i controfattuali di CREST sono giudicati più validi e naturali rispetto a quelli generati automaticamente da MiCE, sebbene meno di quelli manuali.

CREST è anche utilizzato per la razionalizzazione con esempi fattuali e controfattuali, migliorando le prestazioni dei modelli di classificazione su dataset in-domain, contrastivi e out-of-domain. I risultati mostrano che CREST-Rationalization supera i metodi di data augmentation con controfattuali umani in scenari out-of-domain.

Infine, l'analisi delle razionali di CREST in termini di plausibilità, simulabilità in avanti e simulabilità controfattuale dimostra che CREST produce spiegazioni più plausibili e con una simulabilità controfattuale superiore rispetto ad altri metodi. In sintesi, CREST offre un modo controllato per generare controfattuali validi e diversi, migliorando le prestazioni dei modelli di classificazione e fornendo spiegazioni focalizzate sui contrasti dell'input.</sample>
    <sample id="36">Il lavoro "Learning Language-Specific Layers for Multilingual Machine Translation" di Telmo Pessoa Pires e colleghi esplora l'uso di Language-Specific Layers (LSLs) per migliorare la capacità di traduzione multilingue mantenendo costi di inferenza costanti. I vantaggi della traduzione multilingue includono scalabilità, velocità e riduzione degli errori, con miglioramenti significativi per coppie di lingue a bassa risorsa. Tuttavia, la capacità per lingua è limitata, e aumentare la dimensione del modello può complicare l'addestramento e rallentare l'inferenza. LSLs risolvono questo problema introducendo un normale strato trasformatore per lingua, selezionato in base alla lingua di origine o destinazione, mantenendo così costanti i costi di inferenza.

Gli autori hanno esplorato la posizione ottimale degli LSLs, scoprendo che la loro collocazione nell'encoder è più vantaggiosa rispetto al decoder. Hanno addestrato un modello con pesi condivisi, di origine e di destinazione per ciascuno strato dell'encoder, analizzando i pesi per determinare la posizione ottimale degli LSLs. La selezione si basa sul peso più grande, risultando in un'architettura con strati condivisi e specifici per lingua. Gli esperimenti su WMT21 news translation mask sources per 10 lingue hanno mostrato che l'architettura appresa supera significativamente sia i modelli di adattatori linguistici che i modelli di baseline, con miglioramenti particolarmente evidenti per le lingue a bassa risorsa. I test statistici confermano che le migliorie sono significative per 84 delle 90 direzioni di traduzione.</sample>
    <sample id="37">Lo studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona ha rivelato che anche i partecipanti umani hanno espresso stereotipi razziali. Questo ha permesso un confronto diretto tra le personalità generate dai modelli e le risposte scritte dagli esseri umani.</sample>
    <sample id="38">Lo studio ha utilizzato statistiche estratte dall'Enhanced version of the Penn Treebank.</sample>
    <sample id="39">L'articolo menziona tre autori: Igor Mel'čuk, Igor Mel'čuk, e James P. B. Allen.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva menzionate nel contenuto includono:

1. **Classificazione dello Stato di Dissonanza nel Debate**: Determinare se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.

2. **Classificazione Binaria delle Classi di Espansione e Confronto (CE) del PDTB**: Classificare le relazioni di espansione e confronto, che sono strettamente legate al concetto di consonanza e dissonanza.</sample>
    <sample id="41">Silin from the Natural Language Processing Lab at EPFL University introduces "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives," a collaboration with Sony Group Corporation. PeaCoK is a Persona-grounded Commonsense Knowledge Graph designed to enhance narrative systems by providing rich, interconnected persona knowledge. It includes 3,800 personas and 40,000 attributes, forming 100,000 personal inferences. About 9,200 attributes connect multiple personas, fostering rich interconnections.

PeaCoK was developed in three steps: selecting personas from existing commonsense graphs, inducing attributes from knowledge graphs and language models, and crowdsourcing relation annotations with a human-AI majority voting scheme, achieving 87% accuracy in F1. The AI annotator, InstructGPT-3, helped resolve human disagreements efficiently.

PeaCoK was used to train a BART-based common knowledge generator for persona attribute inference, outperforming large-scale pre-trained models like GPT-3 and GPT-3.5 in both automatic and human evaluations. It also improved persona-grounded dialogue generation on the ConvAI2 PersonaChat dataset, with PeaCoK-augmented models surpassing baselines in fluency, consistency, engagement, and persona expression. Compared to Atomic2020, PeaCoK's persona-centric knowledge had a more positive impact, especially when speakers shared common attributes, enhancing dialogue consistency and engagement. PeaCoK demonstrates the value of interconnected persona knowledge in narrative modeling.</sample>
    <sample id="42">L'estratto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="43">Il contenuto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti in quanto confronta le annotazioni degli utenti finali con i dataset e i modelli esistenti, utilizzando un punteggio di correlazione di Pearson per valutare la posizionalità. A differenza della letteratura sull'accordo degli annotatori, che si concentra sull'accordo tra annotatori o sulla modellazione delle distribuzioni degli annotatori, NLPositionality si concentra sul confronto tra le annotazioni degli utenti finali e le previsioni e le etichette dei modelli e dei dataset. Inoltre, il framework riannota i dataset con annotatori diversi per ottenere un set di dati demografici più ricco, poiché le demografie degli annotatori originali sono raramente raccolte e condivise.</sample>
    <sample id="45">Le generazioni di persona generate contengono molto più stereotipi rispetto alle risposte scritte dall'uomo.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando presso l'Università di Washington. Oggi presento il nostro lavoro "Dai Dati di Pretraining ai Modelli Linguistici ai Compiti Downstream: Tracciare le Tracce dei Pregiudizi Politici che Portano a Modelli NLP Ingiusti". I modelli linguistici sono addestrati su grandi set di dati di web crawl. I media di notizie politiche sono ben rappresentati nei dati di pretraining. Secondo un sondaggio del Corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un dono misto per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di apprendere da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente prevenute e potrebbero portare a potenziali problemi di equità nei compiti downstream. A tal fine, proponiamo di indagare la pipeline di propagazione dei pregiudizi politici dai dati di pretraining ai modelli linguistici ai compiti downstream, ponendo specificamente le seguenti domande: Primo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbero avere i dati di pretraining su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi orientamenti politici nei compiti downstream e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Specificamente, proponiamo di sollecitare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test del congresso politico. Ciò ci assicura di effettuare una valutazione automatica ben fondata nella letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, primo, i modelli linguistici hanno orientamenti politici diversi. Occupano tutti e quattro i quadranti nel campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e che la serie GPT è generalmente più socialmente liberale della serie BART e delle sue varianti. In secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente assorbiti dai dati di addestramento. Possiamo condurre un esperimento controllato preaddestrando i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente divisi per orientamento politico. Preaddestrando i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa ulteriormente addestrato sul corpus di Reddit di orientamento a sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di suoi pregiudizi politici. E proviamo anche a indagare se i modelli linguistici possano assorbire la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pretraining in pre e post 45° presidente degli Stati Uniti. Preaddestriamo separatamente i modelli linguistici sui due corpora temporali diversi. Possiamo vedere che i modelli linguistici hanno generalmente un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche assorbire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici nei compiti di rilevamento del discorso d'odio e della disinformazione, applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Possiamo vedere che se indaghiamo sulle prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o orientamenti politici dei media di notizie, possiamo vedere un modello. Ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di orientamento a sinistra sono migliori nel rilevare il discorso d'odio che mira ai gruppi sociali minoritari, tuttavia sono peggiori nel rilevare il discorso d'odio che mira ai gruppi più potenti nella nostra società. Viceversa, i modelli linguistici di orientamento a destra sono migliori nel rilevare il discorso d'odio che mira a bianchi e uomini, tuttavia peggiori nel rilevare il discorso d'odio che mira a neri, LGBTQ+ e altri gruppi minoritari. Tendenze simili si verificano anche nel rilevamento della disinformazione, dove vediamo che i modelli linguistici di orientamento a sinistra sono migliori nel rilevare la disinformazione dal loro orientamento politico opposto e viceversa. Mostriamo anche molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno previsioni diverse per esempi di discorso d'odio e disinformazione basati sulle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che c'è un problema di equità molto pressante riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di orientamento a destra fossero finetunati sul discorso d'odio o sulla disinformazione o su qualsiasi altra cosa e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso d'odio che mira ai gruppi minoritari potrebbe correre impunito senza alcun controllo. Ciò ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivanti dagli orientamenti politici dei modelli linguistici. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di pretraining dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pretraining ai modelli linguistici ai compiti downstream, creando infine problemi di equità. Se provassimo a sanificarli in qualche modo, rischieremmo anche la censura o l'esclusione. E è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere conservato nei dati di monitoraggio linguistico. È un po' come il problema del tram elettrico. Ok, fantastico. Penso che sia tutto ciò che ho per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'articolo è un lavoro congiunto con colleghi da Google Translate, ma il numero esatto di autori non è specificato nel contenuto fornito.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a una lunghezza del contesto di 1024 token per massimizzare i modelli OPT e GPT-2.</sample>
    <sample id="50">Regina Stodden presenta DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase, focalizzandosi sulla semplificazione testuale. La semplificazione testuale mira a migliorare la comprensione per gruppi specifici, come persone con difficoltà di lettura o non madrelingua, utilizzando tecniche come sostituzione lessicale, eliminazione di frasi, riordino e inserimento di parole. DEPLAIN risponde alle limitazioni delle corpora esistenti, che sono spesso troppo piccole o allineate automaticamente, risultando in errori. Il corpus è diviso in DEPLAIN-apa, basato su testi di notizie con 483 documenti allineati manualmente, e DEPLAIN-web, che copre vari domini con 750 documenti allineati sia manualmente che automaticamente, totalizzando 30.450 coppie di frasi.

Omar discute i casi d'uso di DEPLAIN, iniziando con l'evaluazione di metodi di allineamento automatico. DEPLAIN, con le sue frasi allineate manualmente, serve come standard d'oro per valutare questi metodi. MASSalign è stato identificato come il metodo più efficace per l'allineamento automatico nel contesto della semplificazione testuale tedesca. Un altro caso d'uso è l'applicazione di modelli linguistici per la semplificazione automatica del testo. I modelli long-mBART e base mBART sono stati affinati per la semplificazione a livello di documento e di frase, rispettivamente, superando i punteggi di base e stabilendo un nuovo benchmark per la semplificazione automatica del testo.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è definita come le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita. È un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni che i ricercatori prendono.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">The paper "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" by Vasudha and colleagues focuses on detecting cognitive dissonance in language, a phenomenon where two beliefs or actions are inconsistent. Cognitive dissonance is significant for understanding disagreement, tracking belief trends, and mental health issues. Despite its prevalence in decision-making, dissonance is rare in language, posing challenges for detection.

The authors created a large-scale annotated dataset of dissonance relations using a dissonance-first approach. Initial classifier training on a small dataset showed poor performance due to the rarity of dissonance. To address this, they combined transfer learning and active learning (AL) to improve detection while reducing annotation costs.

Transfer learning involved using models from related tasks: debate stance classification and binary classification of PDTB's expansion and comparison classes. This approach improved zero-shot performance significantly. The best results were achieved by fine-tuning on the comparison and expansion (CE) tasks followed by debate.

For active learning, the authors compared "Cumulative" and "Iterative" update strategies. Cumulative, which accumulates all data from active annotations, performed better. They also introduced a Probability-of-Rare-Class (PRC) strategy to select examples likely to be dissonant, outperforming other AL strategies.

After multiple AL rounds, the dissonance classification AUC improved to 0.75. PRC was effective for rare class acquisition but challenging for annotators. The study found that iterative updates are beneficial for transfer learning from different domains, while cumulative updates are better for domain-specific annotations. The paper highlights the importance of tailored transfer learning and AL strategies for rare-class detection.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente senza ristrutturarlo o addestrarlo specificamente per SimulST. Utilizza un unico modello per ogni regime di latenza e gestisce la latenza attraverso parametri specifici, sfruttando la conoscenza acquisita dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale.</sample>
    <sample id="56">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="57">Il modello testato non funziona bene sulla suite di test KITMUS senza task-specific training. Tuttavia, con task-specific training, alcuni modelli come C2F e BERT4Coref mostrano un miglioramento significativo. Tuttavia, anche i modelli migliori hanno difficoltà a integrare in modo affidabile la conoscenza di base fornita solo al tempo di inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: La conoscenza di base è disponibile al momento del pretraining.
2. **Background-Both**: La conoscenza di base è disponibile sia al momento del pretraining che al momento dell'inferenza.
3. **Background-Inference**: Entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza.</sample>
    <sample id="59">Yanis Labrak presents "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains." The presentation covers language modeling in healthcare, introducing DrBERT, the first French biomedical model based on RoBERTa and trained on NACHOS, a dataset of medical web-crawled data. DrBERT is compared with ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital. The study explores the impact of data volume and source on model performance by training four from-scratch models with varying data sizes and types, and three models using continual pre-training strategies. Seven models are evaluated on 11 biomedical and clinical tasks, showing that models perform best on tasks with similar training data but that heterogeneous data sources enhance versatility. More data generally improves performance, with from-scratch pre-training yielding better results. DrBERT outperforms CamemBERT in nine of the eleven tasks, highlighting the benefits of specialized data, though scalability is limited. All models are available on Hugging Face under the MIT license, with training scripts on GitHub.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, e Annie Louis.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Dovremmo utilizzare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli?"</sample>
    <sample id="62">The paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon and collaborators explores the compression of large natural language generation (NLG) models while preserving performance. The focus is on task-specific knowledge distillation in realistic, industry-driven setups, using medium-resource labeled datasets and large amounts of unlabeled data. The study evaluates four NLG tasks: summarization, question generation, common sense reasoning, and simplification and style transfer, with a labeled to unlabeled data ratio of 1:4.

The paper investigates architectural decisions, such as comparing encoder/decoder to decoder-only models, and examines the impact of pruning on performance. It also compares different knowledge selection approaches and state-of-the-art baselines. The main contribution is the exploration of pseudo-targets in sequence-level knowledge distillation. The study highlights the importance of unlabeled data, the benefits of generating multiple diverse pseudo-targets, and the effectiveness of sampling with high temperature over beam search.

A novel technique, joint-teaching, is introduced to address student exposure bias and teach the student to correct its own mistakes by applying word-level knowledge distillation on pseudo-targets generated by both the teacher and the student. The paper emphasizes the practicality of using medium-sized models and focuses on inference time efficiency, aiming for high compression rates. The study provides a comprehensive approach to NLG model compression, offering insights and a recipe for effective knowledge distillation in NLG.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre consistentemente gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nel wording delle istruzioni.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità suggerisce una performance del modello peggiore, poiché indica che il modello produce output diversi per lo stesso compito a causa di variazioni minime nelle istruzioni. Una minore sensibilità è preferibile, poiché indica che il modello è più coerente e produce risultati simili indipendentemente dalle variazioni nelle istruzioni.</sample>
    <sample id="66">Il paper "Deep Learning for Mathematical Reasoning" esplora l'importanza della capacità di risolvere problemi matematici e dimostrare teoremi, un aspetto fondamentale dell'intelligenza umana. L'interesse per lo sviluppo di macchine capaci di affrontare tali compiti è cresciuto, con un focus su metodi di deep learning. Il paper discute come il ragionamento matematico possa coinvolgere dati testuali, multimodali come immagini e tabelle, e si concentra su due categorie principali: contesti visivi e tabulari. Ad esempio, risolvere problemi geometrici richiede l'identificazione delle relazioni geometriche e l'applicazione di teoremi. Questi compiti possono essere formalizzati come problemi di ragionamento neurosimbolico.

L'automated theorem proving è un'altra area chiave, dove i prover dimostrano la verità di affermazioni matematiche attraverso argomenti complessi. I dataset come Numeric Commonsense Knowledge e High-Level Problem Solving sono stati proposti per valutare l'intelligenza umana dei modelli linguistici. Le architetture di reti neurali, come i modelli sequence-to-sequence e sequence-to-tree, sono state sviluppate per formalizzare il ragionamento matematico, rappresentando le espressioni matematiche come strutture ad albero.

I modelli di linguaggio pre-addestrati (LLMs) hanno mostrato prestazioni eccezionali su vari compiti NLP e possono essere applicati anche ai problemi matematici. Tuttavia, affrontano limitazioni, come la mancanza di precisione nel ragionamento matematico. Una soluzione è utilizzare la strategia di self-consistency per migliorare le prestazioni. Inoltre, i program-aided LMMs, come Chameleon, possono aiutare in compiti complessi di ragionamento matematico.

Nonostante i progressi, i modelli di apprendimento mostrano fallimenti di generalizzazione e robustezza, specialmente con numeri grandi e inconsistenze nel ragionamento matematico. Il paper sottolinea anche la necessità di esplorare il ragionamento matematico in contesti a bassa risorsa e in lingue diverse dall'inglese, nonché in settori come finanza, scienza e medicina.</sample>
    <sample id="67">The discussion focuses on interference in multilingual translation models, where training on one language pair can affect the performance of another. Interference can be positive or negative, depending on the language pairs involved. The study identifies that severe interference occurs when models are small relative to the data size, and tuning the sampling temperature is crucial for optimal performance. 

In multilingual settings, factors like data size, language similarity, and the number of languages can impact performance. However, the study finds that language similarity and the number of languages have minimal impact on interference. Experiments using four Transformer architecture variants and 15 languages from WMT show that interference is more pronounced in smaller models and diminishes with increased data and model size.

The study also explores the effect of temperature sampling, where a higher temperature allows more training examples from lower-resource languages. Results indicate that uncalibrated high temperatures can weaken performance, emphasizing the importance of tuning the temperature for better results.

Overall, the study concludes that model and data size are significant factors in interference levels, while other factors like language similarity are less impactful. Modest scaling and tuned temperature can effectively reduce interference without specialized methods.</sample>
    <sample id="68">Il contenuto non fornisce informazioni specifiche sul tipo di contesto linguistico utilizzato durante il pre-addestramento dei modelli. Si concentra invece sull'analisi dell'effetto del contesto durante l'evaluazione dei modelli con il paradigma delle coppie minimali (MPP).</sample>
    <sample id="69">Tipicamente, sono necessari circa 20 campioni per classe per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo sono affiliati con Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini and his team introduce the AltEntities Corpus to address the challenge of resolving indirect referring expressions in entity selection, a crucial aspect for conversational systems and LLM benchmarking. The corpus is designed to understand how users naturally refer to entities when direct references are not feasible, such as when names are forgotten or similar-sounding. The dataset spans music, books, and recipes, using a cartoon setup where Bob and Alice discuss entity choices, with Bob using indirect references like "the newer one."

The AltEntities Corpus includes 6,000 alternative questions and 42,000 indirect expressions. It employs a cartoon format with three speech bubbles: Bob sets the context, Alice poses an alternative question, and Bob uses an indirect reference. The questions are generated using Wikipedia samples, focusing on entities with similar titles, descriptions, or attributes. Annotators, provided with background knowledge like Google search links or Wikipedia excerpts, select and describe entities using indirect expressions.

The T5 XL model's performance on this corpus shows high accuracy (92-95%) when given the same background knowledge as annotators. With partially overlapping knowledge, accuracy drops to 82-87%, and with only entity names, it falls to 60%, indicating room for improvement. The models demonstrate domain-generalizability, making the AltEntities Corpus a valuable resource for advancing entity understanding in conversational systems.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli linguistici, addestrati su dati di pretraining che includono media politici, possono acquisire bias politici che si propagano nei compiti a valle. Questi bias possono portare a problemi di equità nelle applicazioni NLP, come la rilevazione di discorsi d'odio e la rilevazione di notizie false, dove i modelli con diverse inclinazioni politiche possono avere prestazioni variabili in base al gruppo demografico o all'orientamento politico. Misurare accuratamente questi bias è cruciale per affrontare le questioni di equità e garantire che i modelli non marginalizzino gruppi specifici o permettano la diffusione di discorsi d'odio e disinformazione.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Il paper presenta "Dense-ATOMIC", un'estensione del database di conoscenza ATOMIC, che mira a migliorare la copertura delle conoscenze e a creare percorsi multi-hop. ATOMIC, un database di conoscenza basato su eventi, ha limitazioni a causa della mancanza di collegamenti multi-hop e di copertura delle conoscenze. Dense-ATOMIC completa questi collegamenti mancanti, inclusi B-to-A, B-to-B, A-to-B e A-to-A, e introduce percorsi multi-hop come "X chiede a Y di sposarsi, Y dice sì, e poi X sorride". Il processo di costruzione di Dense-ATOMIC include la normalizzazione degli eventi finali, la formazione di un modello di previsione delle relazioni e la costruzione di Dense-ATOMIC stesso. Il metodo Rel-CSKGC, proposto per completare ATOMIC, previene problemi di sparsità e sfrutta le informazioni semantiche degli eventi. Utilizza RoBERTa per l'encoding e MaxPooling per la previsione dei collegamenti. L'approccio di completamento intra- e inter-clusters gestisce l'iterazione computazionalmente costosa. Dense-ATOMIC mostra una maggiore copertura delle conoscenze e migliora le prestazioni di COMET, generando risultati più diversificati. L'evaluazione dimostra che Dense-ATOMIC supera i metodi di previsione delle relazioni e di traduzione, con percorsi multi-hop che mostrano risultati aggregati elevati. Il paper conclude che Dense-ATOMIC offre vantaggi significativi nella copertura delle conoscenze e nei percorsi multi-hop, migliorando il ragionamento basato sul senso comune.</sample>
    <sample id="75">Zheng Yandan presents "Jointprop," a joint semi-supervised learning framework developed with Hao Anran and Luu Anh Tuan, aimed at improving Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The motivation behind this work is to address the limitations of fully-supervised models, which require extensive labeled data, and to leverage the interconnections between NER and RE tasks that are often overlooked. By exploiting these connections, Jointprop aims to enhance label alignment and inference accuracy.

The framework consists of four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation involves initializing span and span pair representations using contextualized token representations and a trained classifier. Heterogeneous graph construction creates a k Nearest Neighbor graph to efficiently examine similarity relations among labeled and unlabeled data, facilitating smooth constraints among neighboring data points.

In the joint label propagation phase, labels are propagated across the graph, refining pseudo-labels iteratively until convergence. This process leverages high-density areas formed by unlabeled data to diffuse labels effectively. Model optimization involves using a softmax function and argmax operation to determine high-confidence pseudo-labels, which are then combined with labeled data to retrain the classification model.

Experiments conducted on four datasets, including joint-task and single-task datasets, demonstrate that Jointprop significantly improves performance over baseline models. The framework benefits from the codependency between NER and RE tasks in joint datasets and shows consistent improvements in single-task datasets, highlighting its effectiveness in semi-supervised settings.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici nei modelli di linguaggio include i seguenti aspetti:

1. **Dati di Pretraining**: I modelli di linguaggio sono addestrati su grandi set di dati web che includono notizie politiche da fonti come il New York Times e The Guardian, portando a una copertura di diverse prospettive politiche.

2. **Valutazione del Bias Politico**: I modelli vengono valutati per il loro orientamento politico utilizzando test di questionari politici, rivelando che i modelli come GPT-4 tendono ad essere più liberali.

3. **Esperimenti di Pretraining Controllato**: I modelli vengono ulteriormente addestrati su corpora partigiani (news e social media) per osservare i cambiamenti nei loro orientamenti politici.

4. **Polarizzazione Sociale**: I modelli vengono addestrati su corpora temporali (prima e dopo il 45° presidente degli Stati Uniti) per valutare la capacità di catturare la polarizzazione sociale.

5. **Impatto sui Compiti Downstream**: I modelli con diversi orientamenti politici vengono valutati su compiti come la rilevazione di discorsi d'odio e fake news, mostrando differenze nelle prestazioni basate sulle categorie sociali e politiche.

6. **Problemi di Equità**: Si evidenzia che i bias politici possono portare a problemi di equità, con modelli di diversi orientamenti che potrebbero marginalizzare gruppi politici opposti o non rilevare adeguatamente discorsi d'odio contro gruppi minoritari.

7. **Dilemma Etico**: Si sottolinea il dilemma tra non sanificare i dati di pretraining, rischiando la propagazione dei bias, e sanificarli, rischiando la censura o l'esclusione.</sample>
    <sample id="77">Il video presenta il lavoro congiunto di Yale University e Microsoft Research, "On Improving Summarization Factual Consistency from Natural Language Feedback". Il progetto introduce il dataset DeFacto, che include dimostrazioni umane e feedback per migliorare la coerenza fattuale nella sintesi testuale. Il dataset offre un'analisi approfondita e intuizioni sulla coerenza fattuale dei modelli di sintesi, con l'obiettivo di garantire che tutte le informazioni nel riassunto siano supportate dal documento di input.

Il lavoro si concentra su tre nuove attività di generazione del linguaggio naturale: editing del riassunto, generazione di feedback e correzione automatica degli errori fattuali. Queste attività sono studiate nel contesto della sintesi testuale abstrattiva, con un focus sulla coerenza fattuale. I dati sono raccolti sull'XSum dataset, utilizzando le uscite iniziali del modello Pegasus pre-addestrato. Gli annotatori valutano la coerenza fattuale dei riassunti, forniscono correzioni e feedback dettagliati, inclusi istruzioni, spiegazioni ed evidenze.

Il dataset DeFacto include circa 2.5K punti dati, con il 70% che presenta errori fattuali. I riassunti umanamente corretti mostrano punteggi di factuality più alti rispetto alle uscite iniziali del sistema, sebbene abbiano una minore sovrapposizione testuale con i riassunti di riferimento. L'analisi mostra che i modelli addestrati possono utilizzare efficacemente il feedback umano per l'editing del riassunto, mentre la generazione di feedback rimane una sfida. La correzione automatica degli errori fattuali, accompagnata dalla generazione di spiegazioni, migliora le prestazioni del modello.

Il dataset DeFacto, con le sue annotazioni dettagliate, offre un test bed per le attività NLG proposte e può essere utile per addestrare metriche di factuality e meta-evalutazioni. Il dataset è disponibile su GitHub, e ulteriori dettagli sono forniti nel paper.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Nel DEPLAIN-apa, ci sono più riordinamenti e aggiunte di parole, mentre nel DEPLAIN-web ci sono più riformulazioni.</sample>
    <sample id="79">Il documento non specifica se CoScript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita nel testo durante il passaggio di iniezione della filigrana. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di \( m \), l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Gli autori dell'articolo sono affiliati all'Università di Stato della Pennsylvania (Penn State University).</sample>
    <sample id="82">Il video presenta un lavoro intitolato "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" che si concentra sull'Automated Essay Scoring (AES) senza supervisione. L'AES mira a valutare la qualità degli saggi senza intervento umano, utilizzando segnali heuristici come supervisione. Tradizionalmente, i modelli AES supervisionati richiedono grandi corpora etichettati, ma raccogliere tali dati è laborioso. L'AES non supervisionato elimina la necessità di punteggi di verità fondamentali, offrendo potenziale sia nella ricerca che nelle applicazioni pratiche.

Due lavori precedenti affrontano l'AES non supervisionato: uno di Chen et al. (2010) utilizza il numero di termini unici come segnale di qualità iniziale, mentre un altro di Zhang e Litman (2021) utilizza il conteggio delle parole. Entrambi i metodi hanno prestazioni scadenti a causa di processi di clustering o regressione diretti. Questi lavori ispirano l'introduzione di più segnali di qualità per una supervisione più forte e robusta.

Il framework proposto, ULRA (Unsupervised AES by Learning from Rank Aggregation), utilizza segnali heuristici come pseudo-verità fondamentale per addestrare un modello AES neurale. ULRA include un modulo HER (Heuristic Essay Ranking) che genera coppie di ordine parziale utilizzando segnali di qualità multipli. Successivamente, il modulo DPRA (Deep Pairwise Rank Aggregation) aggrega queste coppie in una supervisione unificata, utilizzando una perdita di Deep Pairwise Rank Aggregation con pesi di fiducia apprendibili per ciascun segnale.

Nella fase di inferenza, ULRA trasforma i punteggi previsti in un intervallo predefinito utilizzando una strategia di trasformazione min-max. Gli esperimenti mostrano che ULRA supera i baselines non supervisionati e si comporta bene rispetto ai metodi cross-prompt e one-shot, sebbene sia inferiore ai metodi supervisionati a causa della mancanza di supervisione forte. In sintesi, ULRA utilizza segnali heuristici multipli per addestrare un modello AES non supervisionato, dimostrando efficacia attraverso esperimenti.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue. L'addestramento in un mix di varie lingue porta a un miglioramento delle prestazioni per la maggior parte delle principali lingue naturali, sebbene si noti una diminuzione delle prestazioni in inglese in sette dataset e un miglioramento in tre.</sample>
    <sample id="84">Shwai He presents "PAD-Net: An Efficient Framework for Dynamic Networks" at ACL 2023, focusing on the evolution from static to dynamic networks. Traditional static networks use fixed parameters, while dynamic networks adjust architecture or parameters based on input, exemplified by Mixture of Experts and Dynamic Convolution. Although dynamic networks often outperform static ones, fully dynamic networks can lead to excessive parameter use, as seen when replacing BERT-Base's feed-forward layers with Mixture of Experts, increasing model size fivefold.

The paper addresses two questions: the presence of redundant dynamic parameters in fully dynamic networks and whether combining static and dynamic parameters yields better performance. The hypothesis is that fully dynamic networks contain partially dynamic sub-networks with sufficient representation power. To explore this, PAD-Net partitions parameters into dynamic and static, using two scale factors to describe their intensity and constraints to expedite training.

The Iterative Mode Partition method identifies and converts redundant dynamic parameters to static, minimizing their impact on loss value. Experiments show PAD-Net outperforms both static and fully dynamic networks, maintaining fewer parameters and computations. Ablation studies determine optimal Dynamic Ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints for accuracy.

PAD-Net's performance surpasses network pruning by preserving static parameters, enhancing output discrimination. Future work includes extending PAD-Net to mainstream networks, hardware-friendly structures, and incorporating additional modes like zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare una torta al cioccolato", dove il piano deve rispettare specifiche restrizioni come l'uso del cioccolato, a differenza di un obiettivo più astratto come "fare una torta".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo visualizzando le rappresentazioni delle frasi su quattro dataset utilizzando PCA. La legenda delle figure mostra il numero di trigger in ciascuna frase, e le figure dimostrano che è difficile distinguere tra le rappresentazioni con backdoor e quelle normali.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti costruendo DrBERT basato su RoBERTa e adattando CamemBERT per il dominio biomedico e clinico in francese. DrBERT è addestrato su NACHOS, un dataset di dati medici web-crawlati, mentre ChuBERT è addestrato su dati anonimizzati da un data warehouse ospedaliero. Inoltre, il lavoro esplora la pre-addestramento continuo utilizzando i pesi e la tokenizzazione di CamemBERT e PubMedBERT su diversi set di dati per analizzare l'impatto delle strategie di pre-addestramento.</sample>
    <sample id="88">Il contenuto non specifica esplicitamente con quale paese GPT-4 è meno allineato. Tuttavia, menziona che i dataset e i modelli sono più allineati ai paesi di lingua inglese e alle persone con un'istruzione universitaria, implicando che potrebbero essere meno allineati ai paesi non di lingua inglese e a coloro che non hanno un'istruzione universitaria.</sample>
    <sample id="89">Nella frase di esempio, la relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione dicendo: "Se riceviamo un frammento di discorso contenente 'Sto per parlare di...' e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai primi frame di discorso ricevuti, mentre l'ultima parola punta ai frame di discorso ricevuti più recenti, come lambda frame di discorso."</sample>
    <sample id="90">Il paper "Rethinking Annotation: Can Language Learners Contribute?" esplora l'uso di apprendisti linguistici come annotatori per i dati NLP, sfidando la pratica tradizionale di impiegare parlanti nativi. L'autore, Haneul Yoo, discute la difficoltà di reclutare parlanti nativi per molte lingue e propone di utilizzare apprendisti, che sono più numerosi. Il paper presenta uno studio di fattibilità che coinvolge tre lingue (inglese, coreano e indonesiano) e quattro compiti NLP (analisi del sentiment, inferenza della non-logica, riconoscimento delle entità nominate e recupero del contesto). Gli apprendisti sono classificati in livelli di competenza (base, intermedio, avanzato) e confrontati con parlanti nativi. Gli esperimenti mostrano che gli apprendisti possono annotare con precisione, specialmente per compiti semplici e domande di difficoltà media. L'aggregazione delle loro annotazioni tramite voto a maggioranza li rende comparabili ai parlanti nativi. I modelli NLP addestrati con le loro annotazioni raggiungono circa il 95% delle prestazioni di riferimento e talvolta superano quelli addestrati con annotazioni di parlanti nativi. Inoltre, l'attività di annotazione migliora la competenza linguistica degli apprendisti. Il paper suggerisce un nuovo approccio per la costruzione dei dati in lingue a bassa e media risorsa, promuovendo la ricerca NLP per molte lingue e superando le barriere geografiche e tecnologiche.</sample>
    <sample id="91">Man mano che la quantità di attività aumenta, il modello raggiunge una migliore performance e, contemporaneamente, una minore sensibilità.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli senza alberi sul benchmark COGS. Tre approcci di riferimento specifici non sono elencati nel contenuto fornito, ma si fa riferimento a "altri modelli senza alberi" in generale.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli advisor del primo autore, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China introduces a paper on protecting the copyright of embedding as services through backdoor watermarking. Embedding as services, built on large language models like GPT, assist various NLP tasks. However, attackers can steal these models by learning from embeddings and offering similar services. To address this, the paper proposes a watermarking method called "Embedding Marker," which embeds a watermark in the provider's service to detect unauthorized use.

The watermarking method must be applicable to embedding services, not degrade utility, be covert, and transferable during model extraction. Existing methods lack these properties. The proposed Embedding Marker involves two main steps: watermark injection and copyright verification. A trigger set of moderately frequent words is selected, and the provider counts these triggers in user sentences. The provided embedding is a weighted sum of the target and original embeddings, with the weight proportional to the trigger count. If triggers exceed a threshold, the embedding matches the target.

For verification, a backdoor dataset with trigger words and a benign dataset without them are used. The provider requests embeddings from the suspected service and computes cosine and L2 similarity differences between the requested and target embeddings. KS test p-values serve as an additional metric. Experiments on datasets like AG News and SST2 show high detection performance and utility retention. Visualizations confirm the watermark's covertness, making it hard to distinguish between backdoor and normal embeddings. The paper invites discussion on these findings.</sample>
    <sample id="95">Il documento non specifica il primo autore di PaLM. PaLM è un progetto di Google, e il documento menziona che l'autore, David Vilar, ha collaborato con colleghi di Google Translate per la revisione del paper.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una dottoranda di primo anno presso l'Università Carnegie Mellon e oggi presenterò il vostro lavoro, NLPositionality, che caratterizza i pregiudizi di design nei dataset e nei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Immaginiamo di lavorare per un giornale e di filtrare i commenti sotto un articolo di notizie per rimuovere contenuti tossici. Potremmo rivolgerci a un'API popolare come Prospective API per la rilevazione della tossicità, che funziona bene per Carl Jones, dove Prospective API è in grado di rilevare correttamente gli istanti tossici. Tuttavia, questo non è il caso per Aditya Sharma, dove Prospective API non è molto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di design in cui vediamo differenze di prestazioni sistematiche della tecnologia tra le popolazioni. Pregiudizi di design come quello appena visto possono verificarsi a causa della posizionalità dei ricercatori NLP e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono. Quindi una domanda che le persone potrebbero fare è: i dataset e i modelli hanno posizionalità? E non stiamo dicendo che i modelli stessi e i dataset stessi abbiano identità demografiche e esperienze di vita, ma aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Lavori precedenti hanno suggerito alcune prove aneddotiche di avere posizionalità, come i divari culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità del modello. Tuttavia, questi lavori non esaminano il confronto tra utenti finali e i dataset e i modelli stessi, e lo studio della posizionalità del dataset e del modello è sempre più importante man mano che i compiti NLP diventano più soggettivi e orientati socialmente, e è difficile caratterizzare come queste posizionalità siano inclinate perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Per studiare la posizionalità del dataset e del modello, effettivamente confrontiamo le annotazioni con gli utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passo è riannotare i dataset con annotatori diversi. E dobbiamo farlo considerando le demografie degli annotatori originali dei dataset, perché di solito solo pochi annotatori annotano ogni istanza e perché le demografie sono raramente raccolte e condivise. E quindi optiamo per riannotare i dati per ottenere molti annotatori per istanza e per ottenere un set di dati demografici ricco. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, e così il nostro framework differisce effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, rispetto a guardare solo l'accordo degli annotatori o modellare le distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato attraverso Lab in the Wild e una piattaforma di crowdsourcing online per il nostro collaboratore HCI. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto alle piattaforme come M Turk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India, e ulteriormente Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Abbiamo ospitato 2 compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset Social Chemistry e poi scriveranno quanto sia socialmente accettabile una situazione. Dopo di che, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e con gli altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo quindi replicato un setup molto simile per il compito di rilevamento della tossicità e dell'odio, dove leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di discorso d'odio. Poi confrontiamo queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Alla fine del nostro studio abbiamo raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori di 87 paesi. Quindi siamo meglio attrezzati per rispondere a chi si allineano i dataset e i modelli NLP. Troviamo che c'è posizionalità in NLP. Ad esempio, troviamo che i dataset e i modelli si allineano di più ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPT 4, troviamo che si allinea di più ai paesi confuciani e di lingua inglese. Troviamo anche un allineamento aggiuntivo con le persone che hanno un'istruzione universitaria. Quindi, per GPT 4, nel compito di accettabilità sociale, troviamo che si allinea di più alle persone con un'istruzione universitaria o di livello post-laurea e troviamo lo stesso per Dynahate, dove si allinea di più alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset si allineano a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschi e femmine. Troviamo questo nell'analisi del compito di accettabilità sociale di GPT 4 così come nell'analisi del compito Dynahate. Quindi, dato che c'è posizionalità in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutte le scelte di design rilevanti durante tutto il processo di ricerca. E l'altra è fare ricerca NLP con la lente della perspectivism. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. E un buon esempio di questo è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo rendere tutte le tecnologie funzionanti per tutti. E questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati più aggiornati dell'analisi e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: l'uso di architetture specifiche che richiedono ottimizzazione aggiuntiva, procedure di addestramento lunghe e complesse con obiettivi di ottimizzazione diversi, e la necessità di addestrare e mantenere più modelli per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe includere:

1. **Diversificazione dei Dati**: Assicurarsi che i set di dati di addestramento includano una vasta gamma di fonti e prospettive per bilanciare le rappresentazioni politiche e sociali.

2. **Analisi e Filtraggio dei Bias**: Utilizzare strumenti e tecniche per identificare e filtrare i bias nei dati di addestramento, come l'analisi delle rappresentazioni politiche e sociali.

3. **Pretraining Controllato**: Condurre esperimenti di pretraining su corpora controllati con diversi orientamenti politici per comprendere e mitigare gli effetti dei bias.

4. **Valutazione e Test**: Implementare test automatici basati su questionari politici per valutare e regolare i bias nei modelli.

5. **Feedback e Iterazione**: Incorporare feedback continuo e iterazioni nel processo di addestramento per affinare e ridurre i bias nel tempo.

6. **Approccio Etico**: Sviluppare linee guida etiche per determinare cosa considerare neutrale e cosa escludere, evitando la censura eccessiva.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università di Fudan. Qui per presentare il nostro lavoro "Distillare la Conoscenza di Script da Modelli di Linguaggio Grandi per il Pianificazione Linguistica Constrained". Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati agli obiettivi. Lavori precedenti hanno sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipate come "fare una torta" e hanno dimostrato che i grandi modelli di linguaggio possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. Pianificare per obiettivi con specifiche vincoli, come "fare una torta al cioccolato", rimane ancora poco studiato. In questo articolo, definiamo il problema della pianificazione linguistica constrained che impone diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo prima la capacità di pianificazione linguistica constrained dei grandi modelli di linguaggio. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisirli prima. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione dati con l'interazione umana utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli di linguaggio. Questa tabella riporta l'accuratezza complessiva dei risultati. Troviamo che tutti i modelli di linguaggio raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Poi conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo un argomento più dettagliato di categorie di vincoli definiti in wikiHow. La mappa termica nella figura mostra che le prestazioni di pianificazione di InstructGPT variano notevolmente per obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei modelli di linguaggio ha alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di over-generate-then-filter per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Poi, InstructGPT sovra-genera K script per obiettivi specifici. Successivamente, sviluppiamo un modello filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Manteniamo solo lo script se l'obiettivo target ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia nella completezza semantica che nella fedeltà al vincolo. Poiché i grandi modelli di linguaggio sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica in modelli più piccoli e specializzati. Creare il dataset è un passo essenziale a questo scopo. Tuttavia, studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica constrained dai grandi modelli di linguaggio. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica constrained, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo a lavoratori crowd-sourced di trovare e correggere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Troviamo che CoScript mostra un alto pluralismo negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica constrained. Troviamo che T5 affinato su CoScript può generare script di qualità superiore rispetto a molti grandi modelli di linguaggio, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset adatti. In sintesi, stabiliamo il problema della pianificazione linguistica constrained. Valutiamo la capacità di pianificazione linguistica constrained dei grandi modelli di linguaggio e sviluppiamo un metodo di over-generate-then-filter per i grandi modelli di linguaggio. Utilizziamo i grandi modelli di linguaggio per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica constrained. Speriamo che il dataset CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Trova maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio innovativo per la ricerca multi-hop in domande e risposte (QA), che combina metodi di recupero non supervisionati con un riranker basato su modelli linguistici a pochi esempi. Questo metodo è progettato per essere efficiente in termini di dati, richiedendo solo circa 128 esempi di addestramento, rispetto ai migliaia necessari dai sistemi esistenti. Il processo di PromptRank prevede due fasi principali: il recupero di una serie di catene candidate utilizzando il recupero TF-IDF e la traversata di hyperlink, seguito da un riranking di queste catene con un riranker basato su modelli linguistici a pochi esempi.

Il riranking si basa sulla probabilità di una domanda data una catena, utilizzando un modello linguistico per valutare la catena. Le catene vengono trasformate in prompt, con documenti inseriti e un indicatore che designa i documenti. Un'istruzione guida il modello linguistico a esercitare il suo potere di ragionamento sui documenti della catena. PromptRank esplora tecniche come la ricerca di istruzioni per trovare istruzioni ottimali e il campionamento di istruzioni, dove le catene vengono valutate aggregando più punteggi calcolati con diverse istruzioni. Inoltre, viene utilizzata la scala di temperatura per scalare i logit del modello linguistico.

Sperimentazioni con modelli come GPT2-XL e T5-XL su HotpotQA mostrano che PromptRank supera i sistemi supervisionati completi come DrKit e si confronta bene con i recupero multi-hop densi di stato dell'arte. L'approccio dimostra un forte rendimento nel recupero di percorsi multi-hop con pochi esempi, sottolineando l'importanza dell'istruzione nel suscitare le capacità di ragionamento dei modelli linguistici. In sintesi, PromptRank utilizza efficacemente i modelli linguistici per il riranking di percorsi candidati in QA multi-hop, mostrando un notevole potenziale rispetto ai sistemi supervisionati completi.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di traduzione di stato dell'arte, secondo le valutazioni umane eseguite utilizzando il framework MQM. Tuttavia, PaLM tende a produrre traduzioni più fluide a scapito dell'accuratezza, spesso omettendo parti del testo originale.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana includono:

1. Applicabilità ai servizi di embedding.
2. Non degrado dell'utilità delle embedding fornite.
3. Covertura sufficiente per l'attaccante o facilità di rimozione della filigrana.
4. Trasferibilità della filigrana ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Il contenuto non specifica le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese.</sample>
    <sample id="104">Il contenuto non specifica il numero esatto di istanze campionate da un set di dati per la riannotazione. Tuttavia, menziona che vengono riannotati molti annotatori per istanza per ottenere un set di dati demografici ricco.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor sono la differenza di similarità coseno (delta coseno) e la differenza di similarità L2 (delta L2). Inoltre, viene applicato il test di Kolmogorov-Smirnov (KS test) e il suo valore p viene utilizzato come terza metrica.</sample>
    <sample id="106">Il paper discute il dataset QUEST, progettato per affrontare le esigenze informative che coinvolgono vincoli o preferenze multipli. Esempi includono Jane, una zoologa che cerca il nome di una specie di rettile sconosciuta osservata in Costa Rica, e Austin, un lettore che cerca romanzi storici ambientati in Francia. Questi scenari illustrano come le query spesso contengano operazioni di insieme implicite. Il dataset QUEST include oltre 3.000 query che cercano entità con operazioni di insieme implicite, con entità di risposta verificate per rilevanza e documenti associati con evidenze attribuibili per diversi vincoli di query.

Per costruire QUEST, i ricercatori hanno utilizzato nomi di categorie di Wikipedia in quattro domini: film, libri, piante e animali. Hanno eseguito operazioni di insieme su queste categorie atomiche per generare query con vincoli di insieme. Gli annotatori umani hanno poi parafrasato queste query per garantire significato e fluidità, e un altro set di annotatori ha validato queste per fluency e naturalness. Gli annotatori hanno anche verificato la rilevanza delle entità di risposta e hanno contrassegnato le evidenze nei documenti.

Per valutare i sistemi su QUEST, i ricercatori richiedono la ricerca di set di risposte multiple da un ampio corpus di documenti, dove le evidenze di rilevanza possono provenire da diverse parti del documento. I baselines includono recuperatori sparsi e densi, insieme a un reranker basato su T5 che prende i 100 candidati migliori dal recuperatore. I risultati mostrano che c'è un ampio margine di miglioramento nella performance dei recuperatori, con punteggi MRecall@100 che indicano la necessità di miglioramenti. L'analisi rivela che le query con intersezioni e differenze di insiemi sono particolarmente impegnative, con i punteggi F1 più bassi. Il paper mira a incoraggiare la ricerca futura per migliorare i sistemi di recupero per scenari di ricerca di informazioni con esigenze selettive.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come XLM-R + PTR (Multilingual Pretrained Encoders with Pointer-based Decoders), sono stati utilizzati per l'attività di parsing semantico incrociato. Sono stati valutati insieme ad altri modelli, come Encoder-Decoder (ad esempio, mBART e mT5), per determinare le prestazioni in diversi set di dati. I risultati hanno mostrato che i modelli Encoder-Decoder hanno ottenuto le migliori prestazioni su tutti i nove set di dati, ma i modelli Encoder-PTR hanno mostrato miglioramenti quando addestrati in un mix di varie lingue.</sample>
    <sample id="108">Koustav Sinha introduces a joint paper exploring the robustness of language model acceptability judgments in varying contexts. The study revisits minimal pair paradigms (MPP), which evaluate models based on acceptability judgments, including grammaticality and stereotypes. Traditional MPP involves comparing acceptable and unacceptable sentences, expecting models to assign higher probabilities to acceptable ones. However, this method doesn't assess acceptability in longer contexts, which is crucial given the increasing context windows of modern language models.

The research simulates longer sequences by revisiting datasets like BLiMP and SyntaxGym, creating longer sentences by adding acceptable or unacceptable prefixes. They explore scenarios where prefixes match or mismatch the grammatical structure of the query sentences, including using sentences from unrelated domains like Wikipedia. The findings reveal that MPP judgments are stable with irrelevant contexts but vary significantly with relevant contexts, especially when prefixes match the grammatical structure.

The study shows that language models are sensitive to latent syntactic and semantic features shared across sentences. Perturbation experiments confirm that models consistently respond to changes in acceptability domains. The key takeaway is that current MPP evaluations, which focus on short, single sentences, may not fully capture a model's abstract knowledge across longer contexts. The paper suggests that evaluating models over extended contexts is essential for understanding their true capabilities.</sample>
    <sample id="109">The presentation introduces "Unnatural Instructions," a dataset designed to tune language models with minimal human labor. Instruction tuning allows pre-trained models to generalize to new tasks in a zero-shot setting. Traditional methods involve reformulating existing NLP datasets or collecting and annotating user-generated prompts, both of which have limitations. The proposed method generates a diverse dataset automatically using a GPT-3 variant, without human annotations.

The process involves prompting the model with examples from the Super-Natural Instructions dataset to generate new instructions, inputs, and outputs. The dataset is further diversified by creating paraphrases of each instruction. This fully automatic method results in 64,000 examples, expanding to 240,000 with paraphrases.

The dataset is analyzed for creativity, diversity, and correctness, with over 50% of examples being correct. It includes creative tasks like verifying scientific experiments and inventing new words. An 11 billion-parameter T5 model fine-tuned on Unnatural Instructions outperforms T0++ and Tk-instruct across benchmarks, including Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry. Compared to a baseline model trained on Super-Natural Instructions, the Unnatural Instructions model shows superior performance when the cost of example generation is considered. This approach highlights the potential of language models to generate creative and diverse data more efficiently than human annotations.</sample>
    <sample id="111">Gli autori decidono quali sono le parole a frequenza moderata assumendo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 named entity taggers still work well in 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate, o NER. Abbiamo osservato che i modelli utilizzati in CoNLL-2003 per sviluppare NER sono stati utilizzati per quasi 20 anni, il che solleva naturalmente diversi problemi. Prima di tutto, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi affinato più di 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E infine, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti, abbiamo scoperto che i modelli trasformatori generalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo tutti che il numero di esempi di affinamento influisce direttamente sulle prestazioni di un compito a valle. Anche qui abbiamo scoperto che più esempi di affinamento portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli? Abbiamo avuto due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo dello stesso set di test più e più volte e che si manifesta generalmente come rendimenti decrescenti su un nuovo set di test. L'ipotesi seconda è il drift temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e di test. Per l'overfitting, abbiamo visto che dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, il che significa che non ci sono rendimenti decrescenti. Questo ci mostra che l'overfitting adattivo in questo caso non è osservato. E quindi, cosa del drift temporale? Per il drift temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni degradano con un divario temporale più ampio, confermando la nostra ipotesi che la causa principale del calo delle prestazioni è il drift temporale. La nostra conclusione è che, per una buona generalizzazione, abbiamo bisogno di un'architettura del modello migliore, di una dimensione del modello più grande, così come di più esempi di affinamento. E questi vanno di pari passo, non possiamo avere un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dal drift temporale e in modo sorprendente, non è causato dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per più di 20 anni. Quindi tornando alla domanda che abbiamo posto nel titolo del nostro articolo, "Do CoNLL-2003 taggers still work in 2023?" e abbiamo scoperto che la risposta è effettivamente un sì deciso. Speriamo che il nostro articolo chiami a più ricerche su come migliorare la generalizzazione dei modelli. E infine, per favore controllate il nostro articolo, il nostro dataset e se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">Il lavoro presentato da Nanyang Technological University, Singapore, al ACL 2023 si concentra sulla riduzione dei parametri nei modelli di attenzione multi-testa, cruciali per i grandi modelli linguistici. Questi modelli, pur essendo rivoluzionari per l'apprendimento di compiti multipli, presentano sfide come pesi eccessivi, tempi di addestramento lunghi e necessità di grandi corpora di dati. Il team propone un'architettura di attenzione multi-testa a testa raggruppata per affrontare il problema dei pesi eccessivi, utilizzando una strategia di divisione e conquista.

La prima fase, il training con vincolo di gruppo, divide le teste di attenzione in gruppi, rendendo le teste intra-gruppo simili e le teste inter-gruppo distinte. Questo processo utilizza un sistema di scoperta di unità nascoste non supervisionato, come K-means, per supervisionare le mappe di caratteristiche. La seconda fase, l'algoritmo Voting-to-Stay, pratica la selezione delle teste, mantenendo solo una testa per gruppo, risultando in una compressione significativa dei parametri.

I modelli GHT e GHT-PS, derivati da queste fasi, mostrano miglioramenti nelle prestazioni su compiti come traduzione automatica, modellazione del linguaggio e riassunto astratto, con riduzioni dei parametri fino al 32.1%. L'analisi dell'efficienza mostra che il modello LITE raggiunge una compressione del 90% dei parametri, una velocità di inferenza del 62% più veloce e un risparmio del 80% di FLOPs. Il team suggerisce che la potenziale direzione futura sia la potatura automatica specifica per il compito, basata sull'ipotesi del Biglietto della Lotteria, che sostiene che i sottomodelli possano raggiungere prestazioni comparabili a quelle del modello originale. Questo approccio potrebbe consentire la potatura di parametri non necessari per compiti specifici, migliorando l'efficienza senza sacrificare le prestazioni.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di dimensione lambda, dove la decisione di emettere o meno una traduzione parziale si basa sull'attenzione verso gli ultimi lambda frame di input audio.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono "Servin è un giudice" e "Kea è un panettiere."</sample>
    <sample id="117">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">Il lavoro presentato, "Improving Pretraining Techniques for Code-Switched NLP", si concentra sul miglioramento dei modelli di elaborazione del linguaggio naturale (NLP) per gestire il code-switching, un fenomeno comune in comunità linguisticamente diverse come l'India, dove le persone mescolano lingue come l'inglese e l'hindi all'interno di una singola frase. I modelli multilingue pre-addestrati esistenti, come mBERT e XLM-R, non si comportano bene su compiti di code-switching come l'analisi del sentiment e il question answering. Per affrontare questo problema, gli autori propongono nuove tecniche di Masked Language Modeling (MLM) specificamente adattate al code-switching.

Introducono SwitchMLM, un'innovazione che si concentra sui "switch-point", ovvero i punti di transizione tra lingue all'interno di una frase. In SwitchMLM, solo i token che rappresentano questi switch-point sono mascherabili, a differenza del MLM standard dove tutti i token sono mascherabili con probabilità uniforme. Tuttavia, SwitchMLM richiede l'accesso a dataset con tag di identificazione della lingua (LID) o a un tagger LID, che non è sempre disponibile. Per superare questa limitazione, gli autori propongono FrequencyMLM, un metodo alternativo che utilizza la negativa log likelihood dei token nei corpora monolingui per assegnare i tag LID.

Inoltre, gli autori suggeriscono modifiche architettoniche, come l'aggiunta di connessioni residue da strati intermedi a strati finali, poiché gli strati intermedi di BERT codificano più informazioni sui switch-point rispetto agli strati finali. Questo è ulteriormente incoraggiato da una perdita ausiliaria basata su LID, che spinge gli strati intermedi a imparare più informazioni sulla lingua. I risultati mostrano che la combinazione di SwitchMLM o FrequencyMLM con ResBERT (con connessioni residue) e una perdita ausiliaria supera le prestazioni su compiti di analisi del sentiment per diverse coppie di lingue.

Gli autori verificano le loro affermazioni utilizzando esperimenti di probing, dimostrando che i loro metodi aumentano le informazioni sui switch-point negli strati intermedi e finali. I risultati mostrano che le rappresentazioni di StandardMLM combinate con SwitchMLM contengono più informazioni sui switch-point rispetto a StandardMLM da solo. In sintesi, il lavoro propone un nuovo obiettivo MLM e modifiche architettoniche per migliorare l'elaborazione del code-switching, confermando i benefici attraverso esperimenti di probing.</sample>
    <sample id="119">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici GPT-4, GPT serie, BART serie e RoBERTa.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione della meccanica di cross-attenzione tra l'input audio e l'output testuale per decidere se emettere o meno una traduzione parziale. Non è specificato se combina i punteggi di più livelli; si concentra sull'attenzione verso gli ultimi lambda frame di input audio.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono l'uso del nome della canzone, come "Easy on Me", o la sua posizione, come "il primo".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying e Zhiyang presentano la loro ricerca su MultiInstruct, un dataset per l'apprendimento zero-shot multi-modale tramite tuning per istruzioni. Mentre il tuning per istruzioni ha migliorato le prestazioni zero-shot nei modelli di linguaggio, le applicazioni multi-modali sono state trascurate a causa della mancanza di dataset di istruzioni multi-modali. Per affrontare questo, hanno creato MultiInstruct, il primo dataset di benchmark per il tuning per istruzioni multi-modale, con 62 compiti diversi in 10 categorie ampie, derivati da 21 dataset esistenti. Ogni compito è accompagnato da cinque istruzioni scritte da esperti.

Utilizzano OFA, un modello pre-addestrato multi-modale, come modello di base, che utilizza un vocabolario unificato per testo, token immagine e coordinate di un bounding box. I compiti sono formulati in un formato sequenza-a-sequenza unificato. Per il training, utilizzano 53 compiti da 9 gruppi, campionando 10.000 istanze per compito, mentre per il test riservano un gruppo per il ragionamento di senso comune e selezionano ulteriori 5 compiti. Durante il test, valutano il modello con ciascuna delle cinque istruzioni, riportando prestazioni minime, massime e deviazioni standard.

I risultati mostrano che il tuning per istruzioni migliora significativamente le prestazioni di OFA sui compiti multi-modali visti, e il trasferimento di apprendimento dai dataset di istruzioni naturali ne beneficia. L'uso di più istruzioni riduce la sensibilità del modello, misurata dalla sua capacità di produrre output consistenti nonostante variazioni nelle istruzioni. Inoltre, il trasferimento di apprendimento migliora la sensibilità e le prestazioni su dataset di istruzioni naturali. Ying e Zhiyang stanno anche raccogliendo un dataset più ampio con circa 150 compiti aggiuntivi e lo rilasceranno.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented their work on improving the temporal reasoning capabilities of large language models (LLMs). They identified three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event reasoning. Their research highlights that previous studies focused mainly on time-to-event reasoning, while their work aims to cover all three levels comprehensively.

The team conducted experiments on year prediction, revealing biases in models like T5-L and FLAN-T5-L towards the 2000-2020 period, likely due to pre-training data. ChatGPT showed promise in year prediction but struggled with month prediction. To address these issues, they introduced the TempReason dataset, which includes questions across all three reasoning levels and spans a long temporal range. The dataset was constructed using Wikidata and Wikipedia.

They evaluated models in three settings: Closed Book QA, Open Book QA, and a new Reasoning QA setting, where relevant temporal knowledge is provided. To enhance temporal reasoning, they proposed a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. Their final model, TempT5, showed significant improvements over other models in Open Book QA and Reasoning QA settings.

The study found that ChatGPT's performance varied across different time periods, indicating flaws in temporal reasoning. Although TempT5 performed best, it still showed fluctuations due to training data imbalance. Future work could focus on addressing these biases. The research contributes a comprehensive benchmark dataset and a new training paradigm to advance LLMs' temporal reasoning capabilities.</sample>
    <sample id="125">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="126">Sì, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico è stata considerata come un approccio standard, come indicato dal setting "Translate-Test" in cui viene utilizzato l'API di Google Translate per tradurre la query nella lingua di destinazione prima di utilizzare un modello monolingue per il training e l'evaluazione.</sample>
    <sample id="127">Namgyu Ho, un dottorando presso KAIST AI in Corea, presenta il lavoro "Large Language Models Are Reasoning Teachers", sviluppato con Laura Schmid e il professor Se-Young Yun. Il paper affronta la sfida di applicare la tecnica di reasoning a catena di pensieri (chain-of-thought reasoning) a modelli di linguaggio di grandi dimensioni come GPT-3, che richiedono risorse computazionali significative. Per superare questo ostacolo, propongono di utilizzare questi grandi modelli come "insegnanti" per trasferire le loro capacità di reasoning a modelli molto più piccoli.

Il metodo principale consiste nell'usare i grandi modelli per generare soluzioni passo-passo a compiti complessi, che vengono poi utilizzate come dati di addestramento per i modelli più piccoli. Questo processo è noto come fine-tuning CoT (Chain-of-Thought). Un'innovazione chiave del loro approccio è la tecnica di "Diverse Reasoning", che genera molteplici soluzioni diverse per lo stesso problema utilizzando il campionamento di temperatura stocastica, migliorando così l'addestramento del modello più piccolo.

I risultati mostrano che i modelli studenti addestrati con questo metodo possono eseguire compiti di reasoning complessi con prestazioni notevoli, superando i metodi di fine-tuning convenzionali, specialmente su compiti basati su testo. La tecnica di Diverse Reasoning ha dimostrato di migliorare significativamente le prestazioni, come evidenziato dall'aumento delle prestazioni nel compito Multi Arithmetic dal 33% al 55%.

Il paper sottolinea anche la scalabilità del metodo, evidenziando come diverse variabili, come la dimensione del dataset, la qualità del modello insegnante e la dimensione del modello studente, possano influenzare le prestazioni. Tuttavia, queste variabili comportano compromessi tra i costi di sviluppo e quelli di inferenza. Il lavoro conclude che la distillazione semplice può trasferire le capacità di reasoning da modelli molto grandi a modelli più piccoli, aprendo la strada a future applicazioni di altre capacità emergenti.</sample>
    <sample id="128">Akshatha e Martin presentano il loro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," una collaborazione tra McGill University, Mila e Microsoft Research. Il loro studio si concentra sulla capacità dei modelli di comprensione del linguaggio naturale (NLU) di integrare conoscenze da diverse fonti, sia acquisite durante il pretraining sia fornite al momento dell'inferenza. Sottolineano che, mentre i modelli possono utilizzare conoscenze preesistenti per compiti come il question answering, la comprensione del linguaggio spesso richiede anche informazioni specifiche fornite al momento dell'inferenza.

Per indagare su questa capacità, propongono un test di risoluzione delle coreferenze progettato per valutare l'integrazione delle conoscenze. Presentano un esempio in cui la risoluzione del pronome "he" richiede sia conoscenze specifiche dell'entità ("Servin è un giudice") sia conoscenze di sfondo ("I giudici decidono casi nei tribunali"). I test KITMUS variano la disponibilità di queste informazioni in tre scenari: "Background-Pretrain", "Background-Both" e "Background-Inference", quest'ultimo simulando situazioni in cui le conoscenze di sfondo necessarie non sono presenti nei dati di pretraining.

I risultati mostrano che i modelli di risoluzione delle coreferenze, senza addestramento specifico su KITMUS, non performano bene. Tuttavia, con l'addestramento su KITMUS, i modelli migliorano significativamente, suggerendo che l'addestramento su set di dati generici non è sufficiente per gestire scenari complessi come quelli presentati in KITMUS. Anche i modelli migliori hanno difficoltà a integrare conoscenze di sfondo fornite solo al momento dell'inferenza. In sintesi, il lavoro evidenzia l'importanza dell'addestramento specifico per l'integrazione delle conoscenze da diverse fonti nei modelli NLU.</sample>
    <sample id="129">Gli autori hanno fornito l'esempio di "Asian woman" come gruppo contrassegnato.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle non basate su trasformatori. Il paper ha scoperto che i modelli basati su trasformatori generalizzano normalmente meglio a nuovi dati.</sample>
    <sample id="131">Il contenuto non menziona specifici nomi di set di dati di test.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, inclusi testo, immagini e coordinate di un bounding box.</sample>
    <sample id="135">James Finch and Sarah Finch introduce ABC-Eval, a new method for evaluating conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. ABC-Eval aims to provide a more precise and reliable evaluation of dialogue models by focusing on specific behaviors rather than holistic assessments. Traditional methods like Likert scales and pairwise comparisons assess overall dialogue quality but lack granularity. ABC-Eval reduces subjectivity by annotating specific behaviors such as irrelevance, contradictions, hallucinations, and empathy failures.

The method was tested on four state-of-the-art chat models using 100 human-bot conversations per model. ABC-Eval's behavior labels showed higher reliability and predictive power for conversation quality compared to existing methods. For instance, measuring contradictions explained more of the conversation quality than Likert scores. A stepwise linear regression demonstrated that ABC-Eval metrics collectively explained over 25% of conversation quality, with each metric contributing unique information. In contrast, turn-level Likert metrics were less informative.

The evaluation revealed common issues in chat models, such as common sense violations in 20% of responses, irrelevance in 15%, and contradictions in 10%. ABC-Eval provides a higher resolution for evaluating conversational AI, highlighting areas for improvement. The authors hope ABC-Eval will aid in advancing the field by offering a more detailed and reliable evaluation framework.</sample>
    <sample id="136">Jasivan presenta il lavoro "FERMAT: An Alternative to Accuracy for Numerical Reasoning" condotto con Nafise all'Università di Sheffield. Il progetto mira a migliorare la comprensione numerica nei modelli di linguaggio, essenziale per applicazioni come il fact-checking. I modelli più grandi tendono a performare meglio, ma quelli con 3 miliardi di parametri, più accessibili, mostrano debolezze. I benchmark attuali, che si basano su punteggi di accuratezza e F1, non rivelano le vere capacità matematiche dei modelli.

FERMAT è un set di valutazione flessibile basato su tipi aritmetici, estratto da Illinois e CommonCore, con numeri rappresentati in modi diversi per testare le capacità dei modelli. Include operazioni matematiche e dipendenze di addestramento. I risultati iniziali mostrano che i modelli performano male in queste nuove valutazioni, suggerendo che i benchmark esistenti non riflettano accuratamente le esigenze del mondo reale.

Dopo un'addestramento con 200.000 esempi generati da insegnanti di matematica, i modelli mostrano miglioramenti nelle prestazioni. Tuttavia, anche con espressioni viste durante l'addestramento, l'accuratezza rimane bassa, indicando che i modelli non memorizzano semplicemente le risposte ma sono influenzati da sfumature linguistiche.

L'analisi finale mostra che l'uso di template diversificati, inclusi quelli da GSM8K e AQUA, migliora le prestazioni, sottolineando l'importanza della diversità linguistica e matematica. In conclusione, FERMAT offre un'alternativa informativa ai benchmark esistenti, evidenziando l'importanza della rappresentazione numerica e della tokenizzazione.</sample>
    <sample id="137">Il lavoro "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" di Sicong dalla Singapore University of Technology and Design, presentato in ACL 2023, introduce un nuovo compito di apprendimento automatico per generare progetti di piani di pavimento basati su istruzioni linguistiche. Mentre i modelli generativi condizionati dal testo hanno eccelso nella creazione di immagini realistiche e creative, il lavoro si concentra sulla generazione di design che soddisfano requisiti specifici espressi in linguaggio naturale, come i piani di pavimento. Il compito consiste nel generare piani di pavimento 2D che rispettino le istruzioni linguistiche riguardanti semantica, geometria e topologia dei componenti del piano.

Il dataset Tell2Design include 5.051 istruzioni annotate dall'uomo e circa 76.000 istruzioni generate artificialmente. Il compito principale è affrontare le sfide di generare design sotto vincoli rigorosi, comprendere il quadro generale da testi non strutturati e gestire informazioni ambigue o incomplete. Il metodo proposto utilizza un modello encoder-decoder basato su transformer, inizializzato con il modello linguistico pre-addestrato T5, per trattare la generazione di piani di pavimento come un problema di sequenza-a-sequenza.

I risultati mostrano che il modello T2D supera i metodi di generazione di immagini condizionate dal testo, raggiungendo un Micro IoU di 54 e un Macro IoU di 53. Il modello beneficia di un addestramento iniziale con istruzioni artificiali, che migliora le prestazioni quando si passa alle istruzioni scritte dall'uomo. Il lavoro evidenzia le limitazioni delle tecniche di generazione di immagini condizionate dal testo per compiti di design specifici e propone il dataset Tell2Design come base per future ricerche nel campo della generazione di design guidata dal linguaggio.</sample>
    <sample id="138">Gli autori indicano che l'integrazione della conoscenza da diverse fonti, in particolare la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pretraining che la conoscenza fornita al tempo dell'inferenza, è poco studiata nella comprensione del linguaggio naturale (NLU).</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità. Crowd-sourced workers sono stati chiesti di trovare e revisionare i campioni errati per garantire la qualità del set di validazione e test.</sample>
    <sample id="141">I limiti delle risorse esistenti per la traduzione dipendente dal contesto includono il supporto solo per tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché spesso si basano su conoscenze di dominio e cura umana. Inoltre, le metriche a livello di corpus come BLEU non riescono a catturare queste traduzioni poiché solo una piccola parte delle traduzioni dipende dal contesto.</sample>
    <sample id="142">Ciao! Sto parlando del nostro lavoro su "Risoluzione delle Espressioni di Riferimento Indiretto per la Selezione di Entità", in cui introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Intendi 'Easy on Me' o 'I Gotta Feeling'?" Qui, un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Tuttavia, a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. Oppure le pronunce sono troppo simili e difficili da disambiguare. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "la più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte dei LLM. Non siamo a conoscenza di un dataset pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e ricette. Il nostro metodo di raccolta del dataset enfatizza l'informalità utilizzando un setup di completamento di un fumetto. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?" E con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice: "Intendi 'Easy on Me' o 'I Gotta Feeling'?" Questa è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "la più recente". Forniamo le prime e seconde bolle automaticamente, ma la terza è compilata dall'annotatore. La prima bolla è scelta da pochi prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue. Usiamo sempre un semplice modello. Intendi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nell'elenco, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno caselle informative o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, loro conoscono i nomi di queste entità, ma non necessariamente le entità stesse. Quindi, mostriamo alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno parte di ogni canzone e leggere su ogni canzone. Ecco, ad esempio, il risultato della ricerca Google per la canzone "Easy on Me". Per i domini ricette e libri, mostriamo del testo di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, ancora da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverla usando tre a cinque espressioni di riferimento indiretto. Ad esempio, "quella con la musica al pianoforte". Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il ragazzo di 12 anni", o "la finzione", o "proviene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative su tre domini e 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso alla stessa conoscenza di base esatta degli annotatori, allora l'accuratezza è davvero alta, intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza è tra il 82 e l'87%, che è più realistico. Ad esempio, quando il modello di linguaggio recupera la conoscenza di base. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per miglioramenti. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti, in particolare la strategia Wait-k e la Local Agreement, che sono applicate a modelli offline. Viene anche confrontato con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea.</sample>
    <sample id="144">L'affiliazione degli autori dell'articolo è l'Università di Nantes.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, a PhD student from Fudan University, presents a paper on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries from dialogues, capturing essential information across various domains. Despite advancements using large-scale pretrained language models, generated summaries often contain factual errors, with omission being a significant issue leading to incomplete summaries. The study reveals that approximately 70% of summaries suffer from omissions, indicating a widespread problem. The research analyzes the distribution of omitted information, finding it randomly scattered across dialogue positions, highlighting the challenge of identifying key information. To address this, the paper introduces the OLDS dataset, providing high-quality omission labels for dialogue summarization across five domains. The dataset, built on existing benchmarks, uses diverse abstractive models to generate candidate summaries and employs an automatic method for producing omission labels, validated through human evaluation. The study explores three baseline frameworks for omission detection: pair-wise classification, sequence labeling, and pointer network, using Precision, Recall, F1-score, and a word-level omission recall (WR score) for evaluation. Results show an F1-score around 50%, underscoring the task's difficulty and the need for advanced models. The paper also investigates summary refinement using a post-editing method, where detected omissions are concatenated with candidate summaries to improve quality. This approach significantly boosts performance, suggesting that omission detection and refinement are promising directions for enhancing dialogue summarization.</sample>
    <sample id="147">Tre autori sono coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e ti presenterò brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", un lavoro congiunto con Matteo Negri e Marco Turchi. Cosa è la simultanea traduzione automatica della voce? La simultanea traduzione automatica della voce, o SimulST, è il processo di traduzione della lingua parlata in testo in un'altra lingua in tempo reale, abilitando la comunicazione interlinguistica. E quali sono i problemi dei modelli attuali di SimulST? Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione. E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via. Quindi, qual è la nostra soluzione? Prima di tutto, utilizzare modelli offline di traduzione automatica della voce esistenti senza riaddestrare o adottare architetture specifiche per SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale. Questo è il meccanismo di cross-attenzione, e puoi vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, e si tratta di una strategia per decidere se emettere o meno una traduzione parziale, in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè, la sua somma è al di sotto di una certa soglia alpha verso gli ultimi lambda frame di discorso, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un frammento di discorso contenente "I'm going to talk about..." e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai primi frame di discorso ricevuti, mentre l'ultima parola punta agli ultimi frame di discorso ricevuti, come lambda frame di discorso. Questo significa che le prime due parole saranno emesse mentre, poiché la somma della cross-attenzione è al di sopra di una certa soglia alpha, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso. Se continuiamo e riceviamo un altro frammento di discorso, e il nostro modello predice altre tre parole e guardiamo quei pesi di cross-attenzione, vedremo che nessuna parola punta agli ultimi lambda frame di discorso. Questo significa che queste tre parole saranno emesse. Se guardiamo i principali risultati di EDAtt, tracceremo i risultati di simultanea traduzione automatica della voce su grafici in cui abbiamo il BLEU su un lato che misura la qualità della traduzione, e l'average lagging che è la misura di latenza, e consideriamo anche l'average lagging computationally aware che tiene conto dei tempi computazionali del modello per prevedere l'output. Quindi vogliamo che i nostri grafici siano il più in alto possibile su questo grafico. Ma vogliamo anche che siano spostati a sinistra. E confrontiamo con strategie popolari che sono anche applicate a modelli offline, come la strategia Wait-k e la Local Agreement. E confrontiamo anche con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea pre-traduzione. Questi sono tutti i risultati della strategia di simultanea traduzione automatica della voce in tedesco. E vediamo che supera tutte le strategie applicate a modelli offline poiché i grafici sono spostati a sinistra. E vediamo anche che, se consideriamo il tempo effettivamente trascorso o il tempo computationally aware, che è la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro paper. E abbiamo anche rilasciato il codice e i modelli open source e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Il contenuto non specifica se il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">Archiki presents the ACL paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts," highlighting the unique challenges of using meeting transcripts for NLP research. Unlike prior works focused on summarization and action item extraction, MeetingQA addresses the significant QA component in meetings. The dataset, derived from the AMI corpus, includes 7.7K questions from nearly 100 hours of transcribed meetings. It features long, open-ended questions and complex answer scenarios, such as multiple speakers and discontinuous sentences. The dataset contains 30% unanswerable questions, with 40% having multispan answers and 48% involving multiple speakers. A majority of questions are yes/no but elicit detailed responses, and 20% are rhetorical. The paper discusses methods like context-retrieval for short-context models and single-span and multi-span models for QA. Results show a significant gap between model and human performance, with multi-span models slightly underperforming single-span models. Zero-shot performance also lags, though silver data augmentation helps. Error analysis reveals difficulties in identifying rhetorical questions and speaker attribution, especially in zero-shot settings. MeetingQA presents a challenging dataset for existing QA models, emphasizing the need for further research.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su MultiInstruct, che migliora l'apprendimento zero-shot multi-modale tramite l'addestramento per istruzioni. Con i progressi nei grandi modelli di linguaggio, molte ricerche hanno esplorato nuovi paradigmi di riutilizzo di modelli di linguaggio pre-addestrati per vari compiti a valle in modo efficiente in termini di parametri e dati. Recentemente, diversi studi hanno dimostrato che l'addestramento per istruzioni consente ai grandi modelli di linguaggio di eseguire compiti non visti in modalità zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle ricerche sull'addestramento per istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot nei compiti di linguaggio solo, mentre la visione artificiale e i compiti multi-modali sono stati trascurati. Pertanto, in questo lavoro vogliamo indagare se l'addestramento per istruzioni di modelli pre-addestrati multi-modali possa effettivamente migliorare la generalizzazione ai compiti multi-modali non visti. Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modale. Esistono più di 1600 compiti di istruzioni solo linguaggio. Tuttavia, non esiste un dataset di istruzioni multi-modale di grandi dimensioni disponibile pubblicamente. Pertanto, ciò ci motiva a costruire un dataset di addestramento per istruzioni multi-modale. Presentiamo MultiInstruct, il primo benchmark di dataset di addestramento per istruzioni multi-modale, che consiste in 62 compiti multi-modali diversi che coprono 10 categorie ampie. Questi compiti sono derivati da 21 dataset open-source esistenti e ogni compito è dotato di cinque istruzioni scritte da esperti. Per indagare l'addestramento per istruzioni multi-modale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modale unificato, come modello base. OFA utilizza un vocabolario unificato per i token di linguaggio, immagine e le coordinate di un riquadro. Mostriamo alcune istanze di esempio dal nostro dataset MultiInstruct per unificare il trattamento di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato di sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i riquadri sono rappresentati nello stesso spazio di token. Ora parlerò di addestramento per istruzioni multi-modale. Per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento del senso comune per il test e selezioniamo ulteriori 5 compiti dai gruppi VQ e Varie. Utilizziamo tutte le istanze della divisione di test per ogni compito. Inoltre, campioniamo casualmente 20 compiti dalla divisione di test delle istruzioni naturali come compito non visto per NLP. Utilizziamo il modello OFA grande pre-addestrato come modello base. Durante l'addestramento, mescoliamo tutte le istanze di tutti i compiti. Ogni istanza è combinata casualmente con una delle sue cinque istruzioni di template. Durante il test per ogni compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, riportiamo le prestazioni minime e massime e la deviazione standard delle prestazioni in tutti e 5 gli esperimenti. Se il compito è una classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo Rouge-L. Per il compito NLP, riportiamo anche Rouge-L. Introduciamo anche un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre consistentemente gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'addestramento per istruzioni può migliorare significativamente le prestazioni di OFA nei compiti multi-modali visti. Inoltre, il trasferimento di apprendimento dai dataset di istruzioni naturali può beneficiare dell'addestramento per istruzioni. Come possiamo vedere, man mano che aumenta il numero di compiti, il modello raggiunge prestazioni migliori e, contemporaneamente, una sensibilità inferiore. Abbiamo anche fatto un esperimento. Usiamo un'istruzione rispetto a 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Ciò mostra l'effetto di diverse strategie di affinamento sulla sensibilità del modello. Come possiamo vedere, il trasferimento di apprendimento dai dataset di istruzioni naturali consente al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il trasferimento di apprendimento dai dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset di istruzioni naturali. In generale, proponiamo il primo dataset di addestramento per istruzioni multi-modale di grandi dimensioni con capacità zero-shot significativamente migliorate di OFA e esploriamo diverse tecniche di trasferimento di apprendimento e ne dimostriamo i benefici. Progettiamo una nuova metrica chiamata sensibilità. Un'ultima cosa, stiamo raccogliendo un dataset di addestramento per istruzioni multi-modale molto più grande con circa 150 compiti aggiuntivi di linguaggio-immagine e lo rilasceremo. Questo è un codice QR per i nostri dati e modelli. Grazie.</sample>
    <sample id="152">Frederick Riemenschneider discusses advancements in NLP for classical philology, focusing on Ancient Greek and Latin. Recent models like Latin BERT and Ancient Greek BERT are encoder-only and monolingual, limiting their utility for scholars needing multilingual capabilities. To address this, the team developed GreBERTa and GreTa, monolingual models for Ancient Greek, and PhilBERTa and PhilTa, multilingual models for Ancient Greek, Latin, and English. These models vary by language and architecture, with GreTa being an encoder-decoder model based on T5, capable of both understanding and generating text.

The team created a new pre-training corpus from the Internet Archive, identifying Greek texts by searching for incorrectly transcribed Greek stop words and re-scanning with Greek OCR settings. For multilingual models, they used Corpus Corporum for Latin and English texts related to antiquity. Benchmarking involved tasks like part-of-speech tagging, dependency parsing, and lemmatization, using datasets like Universal Dependencies for Greek and EvaLatina 2022 for Latin. The models outperformed existing state-of-the-art models in these tasks.

GreTa's encoder, when used separately, initially performed poorly but improved with more training, highlighting differences from native encoder-only models. Encoder-decoder models excelled in lemmatization, improving performance by 5 percentage points for Ancient Greek. Probing for semantic and world knowledge showed significant improvements over previous models, though no substantial difference was found between multilingual and monolingual models. The presentation concludes with the introduction of powerful, native tokenizer-based language models for classical philology, emphasizing rigorous benchmarking and the implications of multilinguality.</sample>
    <sample id="153">Ninareh Mehrabi presenta il lavoro "Resolving Ambiguities in Text-to-Image Generative Models" presso il team di Ricerca Responsabile di Amazon Alexa AI. Lo studio si concentra sulle ambiguità nei prompt forniti ai modelli di generazione di immagini testuali, come "The girl enters the room with flowers," che può essere interpretato in modi diversi. L'obiettivo è sviluppare framework per mitigare queste ambiguità e valutare se le immagini generate rispecchiano l'intenzione dell'utente.

Il processo inizia con la creazione di un dataset di benchmark basato su LAVA, che copre vari tipi di ambiguità. Il framework proposto utilizza un modello linguistico per generare domande di chiarimento o diverse interpretazioni visive. Gli utenti rispondono a queste domande o scelgono un'interpretazione, permettendo di ottenere prompt disambiguati. Questi prompt vengono poi utilizzati per generare immagini con modelli di testo-in-immagine.

Per valutare la fedeltà delle immagini all'intenzione dell'utente, viene proposto un framework di valutazione automatica. Le immagini generate vengono analizzate con un modello di domande e risposte visive (VQA), che verifica se l'intenzione dell'utente è soddisfatta. I risultati mostrano che la disambiguazione migliora la fedeltà delle immagini e che il framework di valutazione automatica è in accordo con le valutazioni umane.

Il lavoro dimostra che la risoluzione delle ambiguità varia a seconda del tipo di ambiguità e che il framework proposto ha un impatto positivo sulla generazione di immagini fedeli all'intenzione dell'utente.</sample>
    <sample id="154">Le affiliazioni degli autori dell'articolo sono l'Università di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">Shen Gao from Shandong University introduces the work "Dialogue Summarization with Static-Dynamic Structure Fusion Graph," a collaborative effort with Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao, and Rui Yan. Dialogue summarization, a challenging task in text summarization, aims to distill key information from dialogues into concise summaries, aiding quick comprehension without reviewing complex contexts. Existing methods rely on pre-computed static graphs using external linguistic tools, which can be unreliable and inflexible for dynamic summarization tasks.

The proposed SDDS model addresses these issues with four main components: an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator using a pre-trained language model. The Utterance Encoder converts dialogue utterances into vector representations. The Static-Dynamic Graph module combines multiple static graphs and employs a dynamic graph to capture semantic relationships based on deep vector representations.

Four heuristic methods model static dialogue structures: Discourse Parsing Graph, Key Co-occurrence, Speaker Relationship Modeling, and Utterance Position Graph. These methods use graph networks to establish relationships between utterances. Cross-graph fusion integrates these static graphs using a 1x1 convolutional layer.

The Dynamic Graph module, without pre-computed methods, uses a multi-head attention model to determine semantic relationships. A fusion method combines the dynamic graph's relation matrix with the static graph's adjacency matrix into a unified graph. A dual cross-attention mechanism integrates this graph representation into the generation process, enhancing the summary's quality.

The code and data for the SDDS model are available on GitHub, accessible via a QR code.</sample>
    <sample id="158">Qipeng Guo from AWS introduces "Dual Cache for Long Document Neural Coreference Resolution," focusing on improving coreference resolution in long documents. Coreference resolution involves identifying and clustering mentions of the same entity within a text. Traditional methods face quadratic complexity in computation and memory due to the need to evaluate all mention pairs. Cache-based methods reduce this to linear complexity by using a fixed-size cache, but they struggle with long documents where topics frequently change, leading to high cache misses with Least Recently Used (LRU) eviction policies.

To address this, Guo proposes a dual cache system comprising a local cache and a global cache. The local cache uses an LRU policy for local entities, while the global cache employs a Least Frequently Used (LFU) policy for global entities. The dual cache system classifies new mentions as either new entities or existing ones in the cache, then evaluates their frequency to determine their placement in either the local or global cache. This approach significantly reduces cache misses compared to single cache methods.

The dual cache was evaluated on four public benchmarks, showing superior performance over baselines, especially in long documents like a 30,000-word book. It outperforms single cache methods in both performance and efficiency, offering the highest performance/cost ratio. The dual cache effectively manages the trade-offs between model efficiency and performance, making it a cost-effective solution for long document coreference resolution.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, rivediamo i paradigmi di coppie minime. Il paradigma di coppie minime (MPP) valuta i modelli di linguaggio basandosi sui giudizi di accettabilità, che possono includere la grammaticalità come in BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come in CrowS pairs. Nel paradigma MPP, il modo tipico per valutare i modelli di linguaggio è mostrare una frase accettabile o grammaticale e poi una frase accettabile o non grammaticale, con l'aspettativa che il modello attribuisca una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettabilità di un modello verso frasi più lunghe. Oggi i grandi modelli di linguaggio stanno emergendo con finestre di contesto sempre più lunghe. È cruciale valutare l'accettabilità dei modelli in tutta la finestra di contesto, e questo è ciò che stiamo cercando di fare. Rivediamo la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Per simulare queste sequenze più lunghe, rivediamo i set di dati stessi e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei set di dati. Ad esempio, abbiamo scelto una coppia tipica di grammaticalità dal set di dati BLiMP nel caso di Adjunct Island. Estraiamo frasi grammaticali da Adjunct Island e le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare lo stesso scegliendo frasi inaccettabili dallo stesso matching, che può anche essere usato per testare l'accettabilità del modello. Possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un set di dati diverso. Questo è ciò che chiamiamo uno scenario di mismatch. Le frasi provengono ancora da set di dati rilevanti, ma non dallo stesso set di dati con cui si sta valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati dal contesto, se il contesto proviene da un sottoinsieme diverso del set di dati o se è completamente irrilevante alla frase attuale. Come si comporta il modello? Prima, guardiamo alle frasi di Wikipedia, che sono completamente irrilevanti alla coppia di query attuale, e troviamo che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E vediamo qui nella linea tratteggiata arancione che i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso set di dati? Creiamo frasi da domini accettabili e inaccettabili dallo stesso set di dati BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Tuttavia, quando corrispondiamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto è molto grande e aumenta con la lunghezza del contesto, influenzando probabilmente i nuovi modelli di linguaggio con finestre di contesto più grandi. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello? Abbiamo eseguito una serie di analisi cercando di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. Dopo aver eseguito diverse di queste perturbazioni, scopriamo che nessuno di questi rumori fa cambiare al modello il suo corso in termini di come mostra il giudizio MPP. In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Le principali conclusioni del nostro lavoro sono che i modelli di linguaggio sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi. La valutazione MPP attuale, come la facciamo con input di frase breve e singola, potrebbe non catturare appieno la conoscenza astratta dei modelli di linguaggio in tutta la finestra di contesto. Per ulteriori dettagli sui nostri esperimenti, leggete il nostro articolo. Grazie per aver ascoltato.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un unordered multiset di token che appariranno nell'output.</sample>
    <sample id="161">55,000 script sono rappresentati in CoScript.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato (WSL) consente di addestrare modelli utilizzando dati etichettati in modo debole, che sono più economici e facili da ottenere rispetto alle annotazioni manuali di alta qualità. Tuttavia, queste annotazioni sono rumorose, il che può portare a problemi di generalizzazione se non gestite correttamente. L'obiettivo del WSL è sviluppare algoritmi di addestramento che possano gestire efficacemente il rumore delle etichette e garantire che i modelli addestrati generalizzino bene.</sample>
    <sample id="165">Wenting Zhao, a PhD student at Cornell University, presented a paper titled "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations." The paper addresses abductive reasoning, which involves identifying plausible explanations to bridge the gap between a given context and an outcome. For example, given the context "Emily was stuck in traffic" and the outcome "Emily made it to her flight," plausible explanations include "Her flight was delayed" or "Her flight left on time." The goal is to find explanations that logically connect the context and outcome.

Current methods for abductive reasoning rely on supervised learning, requiring annotated plausible explanations, which can be subjective and inconsistent. Zhao's paper questions whether it's possible to learn abductive reasoning without supervision on explanation plausibility. The answer is affirmative, introducing an unsupervised method called LiPoR (Likelihood Learning with Posterior Regularization). LiPoR treats explanations as latent variables, maximizing the marginal likelihood of the outcome given the context without needing to know which explanations are plausible.

However, maximizing likelihood alone doesn't prefer plausible explanations, so a regularizer is introduced to enforce mutual exclusivity among explanations. This means if one explanation is true, others are ruled out. The LiPoR objective combines maximizing outcome likelihood and preferring certain explanations, using a regularizer that minimizes the entropy of explanation probabilities when more than a plausible number receive probability mass.

The paper demonstrates LiPoR's effectiveness on the AlphaNLI dataset, outperforming zero-shot models and previous unsupervised approaches, including a GPT-3 baseline, by over 4 absolute points in accuracy. The paper is available at a provided URL.</sample>
    <sample id="166">This work introduces a novel "Neural Divide-and-Conquer Reasoning Framework" for image retrieval from linguistically complex text, addressing the challenge of retrieving images from long, complex descriptions where images are highly similar. Traditional visual language models, which excel in image-sentence retrieval, struggle with complex text due to their reliance on analogical reasoning akin to System 1 of the Dual-Process Theory. To overcome this, the proposed framework integrates the Divide-and-Conquer strategy with Dual-Process Theory, combining analogical reasoning (System 1) and logical reasoning (System 2).

The framework comprises three main components: the Proposition Generator, the Visual-Linguistic Interactor, and the Neural-Symbolic Reasoner. The Proposition Generator decomposes complex text into simpler propositions, using BART's decoder to generate corresponding sentences. The Visual-Linguistic Interactor, representing System 1, facilitates interaction between visual and linguistic information, producing matching scores and reasoning states. The Neural-Symbolic Reasoner, embodying System 2, integrates these states to derive the final solution, employing a negation executor and conjunction operation for logical inference.

Experimental results demonstrate that the proposed method, NDCR, outperforms existing baselines, with ablation studies confirming the effectiveness of each module. The framework's ability to present intermediate inference states and results highlights its interoperability. The study suggests that neural symbolic calculation could enhance compositional reasoning in large language models, with the Divide-and-Conquer approach paralleling the self-asking chain-of-thought method for complex problem-solving. Integrating Dual-Process Theory with Divide-and-Conquer offers a promising direction for future research.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati sia manualmente che con metodi di allineamento automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo dati da Reuters News del 2020 e annotandoli utilizzando le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">David Vilar reviews the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," co-authored with Google Translate colleagues. The paper explores the use of the 540 billion-parameter PaLM model for machine translation, marking the first systematic study in this area. The study evaluates PaLM's translation capabilities using best practices from the machine translation (MT) community, including the latest test sets and comparisons with state-of-the-art systems like WMT evaluations. The research highlights the significant impact of prompting strategies on translation performance, with experiments showing that different prompts can lead to substantial differences in BLEURT scores.

The study finds that a 5-shot prompting strategy, where sentences are marked with their respective languages, yields consistent results, with the quality of examples being more crucial than their similarity to the source sentence. Using high-quality examples from curated development data improves performance over using noisier training data. Although specialized state-of-the-art systems outperform PaLM, the latter closely approaches commercial systems like Google Translate in fluency, though it struggles with accuracy, often omitting parts of the source text. Human evaluations using the MQM framework confirm PaLM's fluency but highlight its accuracy issues, particularly omission errors. Despite these challenges, PaLM's translations are noted for their fluency, with fewer awkwardness issues compared to other systems.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università di Stato della Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda Calculus. Il parsing semantico cross-linguistico è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato. Esistono modelli di parsing semantico cross-linguistico proposti e valutati su dataset limitati a determinati compiti e applicazioni. Ad esempio, ci sono molte coperture per alcune lingue naturali, ma la lingua cinese è assente e manca di copertura per alcune rappresentazioni di significato. Il Lambda calculus è assente, o sono valutati solo su determinati modelli neurali. Per questo motivo, proponiamo XSemPLR. Forniamo un dataset uniforme XSemPLR per il parsing semantico cross-linguistico in più lingue naturali e rappresentazioni di significato. Contiene 9 dataset in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo sei impostazioni per addestramento e valutazione. La prima è Translate-Test. Usiamo l'API di Google Translate per tradurre la lingua di origine in quella di destinazione, quindi usiamo un modello monolingue per addestramento e valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo la query tedesca usando l'API in inglese e poi usiamo il modello addestrato per prevedere l'SQL. Testiamo anche il Modello Monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione Monolingue Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. Testiamo il Modello Multilingue addestrando un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi e cinesi per addestrare un modello multilingue. Durante l'inferenza, possiamo usare questo modello per tradurre le query tedesche o cinesi, ecc. Consideriamo anche il trasferimento Cross-lingual Zero-shot e Few-shot. Addestriamo su una lingua di origine e trasferiamo ad un'altra lingua. Durante l'addestramento, addestriamo su query in inglese o su una combinazione di query in inglese e tedesco Few-shot per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche scoperto molti risultati interessanti. Riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi Encoder-PTR, che sta per Multilingual Pretrained Encoders con Decoder basati su Pointer, come XLM-R + PTR e mBERT + PTR. Valutiamo anche i modelli Encoder-Decoder, che sono Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i dataset. Valutiamo su mT5 e XLM-R + PTR in impostazione multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrandoli in un mix di varie lingue. Abbiamo scoperto che la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni in inglese diminuiscono in sette dataset e aumentano solo in tre dataset. Penso che questo sia noto come la "Maledizione della Multilinguismo". Confrontiamo anche il divario di prestazioni cross-linguistiche. Nella figura, la linea blu è il trasferimento Cross-lingual Few-shot. La linea arancione è il trasferimento Cross-lingual Zero-shot. Mentre la linea verde è l'impostazione Monolingue. Abbiamo scoperto che, confrontando la linea verde e quella arancione, il divario di prestazioni nel trasferimento Zero-shot è significativo, e poi confrontando le linee blu e arancione, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si riduce rapidamente. Abbiamo anche scoperto altre scoperte interessanti. Ad esempio, Encoder-Decoder supera il lavoro precedente o raggiunge risultati comparabili. L'addestramento su lingua naturale inglese può migliorare significativamente le prestazioni del Few-shot sulla lingua naturale di destinazione, e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-linguistico. In sintesi, costruiamo XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico con più lingue naturali e rappresentazioni di significato. Conduciamo uno studio di benchmark completo su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati mostrano molte scoperte interessanti. E così via. Benvenuti a visitare il nostro articolo e il codice. Grazie per l'ascolto.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro categorie principali: metodi di watermarking tradizionali, metodi di watermarking basati su backdoor, metodi di watermarking basati su distorsione e metodi di watermarking basati su modelli. Tuttavia, questi metodi esistenti presentano limitazioni, come non essere applicabili ai servizi di embedding o mancare di trasferibilità durante il processo di estrazione del modello.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom sono ancora inadeguati per i compiti di cross-lingual semantic parsing.</sample>
    <sample id="174">Thea, co-autrice del paper "ArgAnalysis35K: A Large-Scale Dataset for Argument Quality Analysis," presenta le caratteristiche uniche del dataset ArgAnalysis35K. Questo dataset si distingue per la sua dimensione e qualità, contenendo 35.000 coppie di analisi degli argomenti, il più grande nel suo campo. A differenza di altri dataset, che spesso si basano su piattaforme di crowdsourcing e mancano di diversità e profondità, ArgAnalysis35K utilizza argomenti di alta qualità provenienti da dibattiti di alto livello e da esperti. Il dataset copre 24 temi diversi, catturando una vasta gamma di argomenti, piuttosto che limitarsi a pochi argomenti pre-selezionati.

Un'innovazione chiave è l'introduzione del concetto di "analisi," che combina affermazioni e premesse per spiegare meglio un argomento. Questo approccio fornisce una comprensione più completa rispetto ai dataset tradizionali che si concentrano solo sugli argomenti. Inoltre, il dataset implementa un modello di affidabilità degli annotatori basato su istanze, che consente di utilizzare le valutazioni degli annotatori in modo più efficace, eliminando solo quelle influenzate da pregiudizi specifici.

Infine, ArgAnalysis35K introduce un modello di rilevanza che assegna un punteggio da 0 a 1 per ciascun argomento in relazione a diversi temi, riconoscendo che gli argomenti possono essere applicati a più contesti. Queste caratteristiche rendono il dataset più diversificato, rilevante e affidabile, offrendo una risorsa preziosa per l'analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un metodo di approssimazione continuo che è amichevole per il GPU. Questo permette di apprendere le permutazioni linguisticamente più plausibili e di eseguire il backpropagation attraverso la soluzione.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita in base alla sua capacità di trattare equamente diverse demografie o gruppi politici nei compiti applicativi, come la rilevazione di discorsi d'odio e la rilevazione di fake news. L'equità è messa in discussione quando i modelli con diverse inclinazioni politiche mostrano prestazioni variabili in base al gruppo target, ad esempio, i modelli di sinistra sono migliori nel rilevare discorsi d'odio contro gruppi socialmente minoritari, mentre quelli di destra sono migliori nel rilevare discorsi d'odio contro gruppi più potenti. Queste differenze indicano potenziali problemi di equità, poiché l'uso di modelli con specifiche inclinazioni politiche potrebbe marginalizzare gruppi con opinioni opposte o permettere che discorsi d'odio contro minoranze passino inosservati.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar presenta la ricerca "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", che esplora come migliorare le capacità di ragionamento sulla teoria della mente nei modelli linguistici di grandi dimensioni (LLM). La teoria della mente riguarda la capacità di comprendere lo stato mentale degli altri, spesso valutata tramite compiti di comprensione della lettura con domande di falsa credenza. I modelli linguistici attuali, come ChatGPT e GPT-3, mostrano prestazioni scadenti su questi compiti.

La ricerca introduce SymbolicToM, un metodo che migliora il ragionamento sulla teoria della mente utilizzando rappresentazioni grafiche simboliche esplicite. Queste rappresentazioni catturano le credenze dei personaggi in una storia, consentendo di rispondere a domande complesse sulle credenze reciproche. SymbolicToM calcola grafici per tutte le combinazioni di personaggi fino a un livello massimo di teoria della mente, utilizzando modelli NLI e OpenIE.

Gli esperimenti mostrano che SymbolicToM migliora significativamente le prestazioni dei LLM su compiti di teoria della mente, con guadagni di accuratezza fino a 67 punti percentuali. La ricerca valuta anche la generalizzazione su nuovi dataset, dimostrando che SymbolicToM mantiene prestazioni elevate anche in contesti diversi, mentre i modelli supervisionati degradano. In sintesi, SymbolicToM offre un metodo interpretabile e versatile per migliorare il ragionamento sulla teoria della mente nei LLM, superando i metodi supervisionati in scenari fuori dominio.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the gap in planning for specific goals with constraints, such as "make a chocolate cake." The study defines constrained language planning, where abstract goals are adapted to specific, constraint-laden real-life scenarios. Evaluating large language models reveals their inadequacy in generating scripts faithful to constraints, despite acceptable semantic completeness. The research employs a human-in-the-loop approach to acquire specific goals using InstructGPT, extending abstract goals with multi-faceted constraints. An over-generate-then-filter method is proposed to enhance script quality, using cosine similarity and keyword presence to select the most faithful scripts. The study also introduces CoScript, a dataset of 55,000 specific goals with scripts, created through symbolic knowledge distillation from large language models. CoScript demonstrates high diversity and enables smaller, specialized models like T5 to outperform larger models when fine-tuned on this dataset. The paper concludes by highlighting CoScript's potential as a valuable resource for advancing constrained language planning research.</sample>
    <sample id="182">Nel contesto di questo articolo, il tropicalismo si riferisce a un tropo che descrive le donne latine con parole come "vibrante" e "curvacee", contribuendo a stereotipi che le associano a immagini esotiche e sensuali, rafforzando così narrazioni essenzializzanti e discriminanti.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando un metodo a due parti. In primo luogo, hanno generato "personaggi" chiedendo a un modello di linguaggio di rispondere a prompt come "Immagina di essere una donna asiatica. Descrivi te stessa." Questi prompt sono stati ispirati da uno studio che ha dato prompt simili a soggetti umani per rivelare stereotipi razziali. In secondo luogo, hanno utilizzato il metodo delle "Parole Marcate" per identificare le parole che distinguono i gruppi marcati dagli unmarcati, confrontando i personaggi generati con quelli scritti dagli esseri umani.</sample>
    <sample id="184">In questo lavoro, l'utilizzo del contesto è stato misurato utilizzando CXMI (Contextual Mutual Information) e la sua estensione, Pointwise CXMI (P-CXMI). CXMI misura quanto informazione il contesto fornisce sulla traduzione target data la fonte, mentre P-CXMI consente di misurare l'utilizzo del contesto a livello di frase o di parola.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato in francese per domini biomedici e clinici basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. ChuBERT, d'altra parte, è basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes e include sia dati clinici che un mix di NACHOS. DrBERT si concentra su dati web raccolti, mentre ChuBERT utilizza dati clinici anonimizzati.</sample>
    <sample id="187">Due autori sono coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene inizialmente addestrato su un compito correlato per trasferire conoscenze rilevanti, e poi viene ulteriormente affinato su un nuovo compito. Nel contesto del paper, il trasferimento iterativo comporta l'addestramento iniziale del modello su compiti correlati come la classificazione dello stile di dissonanza nei dibattiti e la classificazione binaria delle classi di espansione e confronto del PDTB, seguito da un affinamento su questi compiti per migliorare le prestazioni iniziali sul compito di rilevamento della dissonanza.</sample>
    <sample id="189">L'obiettivo del set di dati AltEntities Corpus è comprendere il linguaggio degli utenti quando vogliono fare una scelta, in particolare quando usano riferimenti indiretti per selezionare tra entità, come canzoni, libri o ricette, in conversazioni naturali. Si concentra sulla raccolta di dati per risolvere i riferimenti indiretti e benchmarking la comprensione delle entità nei sistemi conversazionali e nei modelli di linguaggio di grandi dimensioni (LLMs).</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un Embedding as a Service (EaaS) apprendendo dalle risposte dell'embedding fornite dal servizio. Analizzando queste risposte, l'utente può ricostruire un modello simile che fornisce servizi simili.</sample>
    <sample id="191">Tre autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Yang Luo presenta "CAME: Confidence-guided Adaptive Memory Efficient Optimization", un nuovo ottimizzatore progettato per affrontare la sfida di combinare una rapida convergenza con un basso utilizzo di memoria durante l'addestramento di modelli linguistici di grandi dimensioni. I metodi di ottimizzazione adattiva come Adam richiedono molta memoria per memorizzare le stime dei momenti dei gradienti, mentre gli ottimizzatori efficienti in termini di memoria come Adafactor riducono l'uso della memoria ma a scapito delle prestazioni. CAME mira a superare questi problemi utilizzando la fattorizzazione matriciale non negativa (NMF) per ridurre l'uso della memoria e affrontare gli aggiornamenti errati che causano una lenta convergenza in Adafactor.

CAME introduce un approccio per gestire gli aggiornamenti errati calcolando un'instabilità tra gli aggiornamenti attuali e la memoria, utilizzando questa instabilità per guidare gli aggiornamenti in modo più adattivo. Gli esperimenti su dataset come BookCorpus e English Wikipedia mostrano che CAME migliora significativamente le prestazioni rispetto ad Adam e Adafactor, aumentando l'accuratezza di validazione di circa il 3,4% rispetto ad Adafactor, mantenendo lo stesso numero di passi di addestramento. CAME supera anche Adam nella pre-addestramento di modelli molto grandi, riducendo notevolmente i costi di memoria con batch più grandi. Inoltre, CAME migliora l'addestramento di BERT-Large e mostra prestazioni comparabili su compiti downstream con costi di memoria inferiori. CAME riduce anche l'uso della memoria rispetto a SM3, dimostrando la sua efficacia e capacità di supportare l'addestramento con grandi batch.</sample>
    <sample id="193">Il contenuto non specifica il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Le affiliazioni degli autori dell'articolo includono Carnegie Mellon University, University of Washington e Allen Institute for AI.</sample>
    <sample id="195">Il lavoro "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" si concentra sull'XQA, che mira a fornire risposte e spiegazioni per le domande. Le attuali metodologie XQA si dividono in metodi neurosimbolici e decomposizionali. I metodi neurosimbolici traducono le domande in rappresentazioni formali come SPARQL, ma sono limitati dalla mancanza di completezza delle KB. I metodi decomposizionali generano passaggi intermedi in linguaggio naturale, ma affrontano difficoltà a causa della diversità del linguaggio naturale.

Il lavoro propone RoHT, un framework a due fasi che utilizza un Albero di Decomposizione Gerarchica delle Domande (HQDT) per comprendere la struttura compositiva delle domande complesse. L'HQDT ha la domanda complessa come nodo radice e domande atomiche come nodi foglia. RoHT esegue un ragionamento probabilistico sull'HQDT, selezionando fonti di conoscenza appropriate per ciascun nodo e aggregando risposte candidate.

Il framework è valutato su due dataset complessi, KQA Pro e Musique. Su KQA Pro, RoHT supera i metodi esistenti utilizzando sia KB incompleti che il corpus di Wikipedia. Su Musique, RoHT migliora le prestazioni rispetto ai metodi di punta, dimostrando i benefici dell'integrazione di KB e testo. In sintesi, RoHT mostra un miglioramento significativo nell'XQA integrando conoscenze da diverse fonti e utilizzando una decomposizione esplicita delle domande.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa."</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo menzionati sono quattro stati dell'arte che sono stati valutati utilizzando ABC-Eval. Tuttavia, i nomi specifici di questi modelli non sono forniti nel contenuto.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli di linguaggio attuali hanno finestre di contesto sempre più lunghe. È cruciale valutare come i modelli gestiscono l'accettabilità in sequenze più lunghe per comprendere meglio le loro capacità e limitazioni, poiché le prestazioni possono variare significativamente con l'aumento della lunghezza del contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset, un fenomeno noto come "Curse of Multilinguality". Tuttavia, il modello multilingue ha mostrato guadagni di prestazioni in tre dataset.</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo. Vengono mostrati i nomi delle entità e alcune informazioni di sfondo, ma non necessariamente conoscono le entità stesse.</sample>
    <sample id="201">Sono state utilizzate metriche di MT neurali di stato dell'arte, in particolare BLEURT, e sono stati mostrati anche risultati di valutazione basati su esperti umani.</sample>
    <sample id="202">Il contenuto non specifica se il regresso nella generalizzazione influisce su specifici tipi di NER. Indica che il regresso è principalmente causato da un cambiamento temporale, non da un problema specifico di tipo NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché può influenzare le prestazioni delle tecnologie tra diverse popolazioni, portando a design bias. Questi bias possono derivare dalle prospettive dei ricercatori e sviluppatori di modelli, che a loro volta influenzano le decisioni prese durante il processo di ricerca. Comprendere e caratterizzare la posizionalità nei dataset e nei modelli è cruciale, specialmente per compiti NLP più soggettivi e socialmente orientati, poiché può rivelare come le tecnologie possano essere più allineate a certe popolazioni rispetto ad altre, lasciando inevitabilmente alcune dietro.</sample>
    <sample id="204">Il contenuto non specifica se gli LLM multilingue come BLOOM siano stati affinati mediante adattatori o con una messa a punto integrale.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented research on political biases in language models, focusing on how these biases propagate from pretraining data to downstream tasks. Language models are trained on large-scale web data, including political news from sources like the New York Times and The Guardian. While this diversity of perspectives is beneficial, it also introduces social biases that can affect fairness in NLP applications.

The study investigates the political bias propagation pipeline by evaluating the political leanings of language models and their impact on downstream tasks. Preliminary results show that models like GPT-4 are more socially liberal, while GPT models are generally more liberal than BART models. Controlled experiments further pretrained models on partisan corpora, revealing shifts in political biases corresponding to the data's ideological leanings.

The research also explores societal polarization by pretraining models on data from before and after the 45th U.S. president, showing a shift towards more polarized political leanings post-2017. In downstream tasks like hate speech and fake news detection, left-leaning models better detect hate speech against minority groups but struggle with more powerful groups, while right-leaning models show the opposite trend.

These findings highlight fairness issues, as deploying biased models could marginalize certain groups. The dilemma is between sanitizing training data to reduce bias, risking censorship, and retaining potentially biased data. This situation is likened to the electric trolley problem, emphasizing the complexity of addressing political biases in language models.</sample>
    <sample id="206">Fanno ricorso a due modelli per il trasferimento dell'apprendimento: un modello per la classificazione dello stile di dissonanza nei dibattiti (chiamato "debate"), che determina se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, e un modello per la classificazione binaria delle classi di espansione e confronto (CE) del PDTB, che sono strettamente correlate alla concezione di consonanza e dissonanza.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i più recenti set di test WMT (Workshop on Machine Translation) per evitare un sovrapporsi dei dati di test con i dati di addestramento del modello linguistico.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto migliora la pianificazione del linguaggio vincolato migliorando sia la completezza semantica che la fedeltà ai vincoli. Utilizza un approccio di sovra-generazione e filtraggio per selezionare script più fedeli, migliorando la qualità generale della pianificazione. Inoltre, il metodo genera il dataset CoScript, che consente a modelli più piccoli e specializzati di superare i modelli più grandi quando adeguatamente addestrati, offrendo un'alternativa più efficiente rispetto ai modelli di riferimento più grandi e costosi.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametri di riferimento per il problema dell'elaborazione automatica della semplificazione del testo in tedesco. Il paper propone risultati di base per l'allineamento automatico e la semplificazione del testo, che possono servire come benchmark per future ricerche.</sample>
    <sample id="212">Nell'articolo, viene menzionato un modello più piccolo: T5.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è il modello pre-addestrato OFA (Unified Multi-Modal Pre-Trained Model).</sample>
    <sample id="215">Adam Przepiórkowski discute la struttura di dipendenza della coordinazione, esaminando diverse teorie e approcci. Le teorie come le Dipendenze Universali e la Teoria del Testo Significativo di Igor Mel'čuk adottano strutture asimmetriche, dove il primo congiunto è il capo della struttura coordinata. Al contrario, l'approccio di Praga e la Grammatica delle Parole di Hudson propongono strutture congiuntive capite e multi-capite, rispettivamente. L'obiettivo del paper di Przepiórkowski è argomentare a favore delle strutture di coordinazione simmetriche, basandosi sul principio di minimizzazione della lunghezza delle dipendenze.

Il paper illustra come la posizione dei congiunti influenzi la lunghezza delle dipendenze, utilizzando esempi in inglese per mostrare che i diretti oggetti preferiscono essere vicini al verbo. Tuttavia, quando un oggetto diretto è lungo, può essere posizionato dopo un avverbio senza compromettere la comprensibilità, grazie alla minimizzazione della lunghezza delle dipendenze. Analizzando il Penn Treebank, Przepiórkowski osserva che i congiunti più brevi tendono a precedere quelli più lunghi, specialmente quando il governatore è a sinistra o assente. Questa tendenza diminuisce quando il governatore è a destra. Questi risultati supportano le strutture di coordinazione simmetriche, sfidando le strutture asimmetriche.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He addresses the challenge of generating dialogues controlled by multiple attributes. Traditional methods focus on single attributes, limiting their practical application. The authors propose a novel approach, Disentangled Controllable Generation (DCG), which learns attribute concepts from seen values and uses a disentanglement loss to separate different attribute combinations. They introduce a unified reference-free evaluation framework, MAE, to assess various attribute granularities without needing extensive labeled data.

The DCG model is built on the DialoGPT framework, incorporating a compositional prompt module with two types of prompts: attribute-oriented and task-oriented. Attribute-oriented prompts guide the model to focus on specific dialogue information, while task-oriented prompts leverage global features. These prompts are combined to enhance the model's ability to distinguish between attribute combinations. To further improve diversity, pseudo combinations are used, and a disentanglement loss is applied to train and separate compositional prompts.

The authors establish two benchmarks and demonstrate the effectiveness of their method and evaluation metrics through experiments. DCG outperforms existing baselines in attribute controllability and text equality, particularly for unseen attribute combinations. The model's performance is validated using metrics like E-ACC, A-ACC, and BLEU, showing minimal drops in controllability while excelling in text equality. The MAE framework, tested on both discrete and continuous attributes, correlates well with human judgments, outperforming classic metrics.

The study concludes that the proposed method effectively tackles compositional generalization challenges in multi-attribute controllable dialogue generation, transforming seen attributes into unseen combinations. The attribute-oriented prompt method, leveraging shared embedding mapping, proves superior in learning attribute concepts, demonstrating the model's ability to generalize effectively.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">Jia-Huei Ju from Academia Sinica, along with Yu-Shiang Huang, Cheng-Wei Lin, and advisors Professors Che Lin and Chuan-Ju Wang, presented their work on a multistage pipeline for analyzing financial reports, specifically Form 10-Ks. The project aims to automate the extraction of useful information from these reports, which are similar year-over-year, by introducing a highlighting task to identify important words that indicate changes or new information. The pipeline consists of three stages: relation recognition, out-of-domain fine-tuning using the eSNLI dataset, and in-domain fine-tuning with pseudo-labels. The model classifies report pairs into three types: highly similar, revised, and mismatched. Fine-tuning involves soft labeling techniques to address low-quality pseudo-labels. The model's performance is evaluated using precision and PCC metrics, showing strong results on both the FINAL dataset and eSNLI. The approach demonstrates potential for future enhancements in information retrieval and financial analysis.</sample>
    <sample id="220">Gli autori dell'articolo sono affiliati a Stony Brook University.</sample>
    <sample id="221">L'articolo analizza la coppia linguistica tedesco-inglese.</sample>
    <sample id="222">Questo lavoro, "Adattare o Annotare: Sfide e Interventi per l'Adattamento di Dominio nell'Elaborazione del Linguaggio Naturale per Risposte a Domande in Domini Aperti", esplora le sfide dell'adattamento di dominio nell'ambito dell'elaborazione del linguaggio naturale (NLP) per rispondere a domande in domini aperti. L'obiettivo è migliorare le prestazioni dei modelli di recupero e lettura, originariamente addestrati su domini generali come Wikipedia, quando applicati a domini specifici come quello biomedico. Il lavoro identifica tre contributi principali: l'indagine di interventi dati per l'adattamento di dominio, l'identificazione del tipo di spostamento di dataset in nuovi domini e la determinazione degli interventi dati efficaci per tipi specifici di spostamento.

Il lavoro esplora interventi dati zero-shot e few-shot. Gli interventi few-shot utilizzano pochi esempi di domini target per generare ulteriori esempi tramite modelli linguistici grandi, migliorando le prestazioni del recupero e della lettura rispettivamente del 8% e 11%. Gli interventi zero-shot controllano le interazioni tra domanda, risposta e contesto, variando il formato delle domande e la distribuzione delle risposte e del contesto, con risultati che indicano che le domande a stile cloze sono più facili da curare e che le distribuzioni uniformi delle risposte sono più efficaci.

Il lavoro utilizza una tassonomia esistente di spostamenti di dati per classificare i tipi di incompatibilità tra modelli e domini target, identificando spostamenti concettuali, covarianti e completi. Misurando la compatibilità tramite la probabilità assegnata dai modelli di recupero e lettura, il lavoro mappa i dataset target su un piano 2D per stimare il tipo di spostamento. I risultati mostrano che gli interventi few-shot sono efficaci per tutti i dataset target, mentre gli interventi zero-shot sono particolarmente utili per spostamenti concettuali e covarianti. In sintesi, il lavoro dimostra che le prestazioni della lettura possono essere migliorate fino al 24% con interventi dati appropriati, a seconda del tipo di spostamento del dataset.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati i modelli long-mBART e base mBART. Il long-mBART è stato fine-tuned per produrre semplificazioni a livello di documento, mentre il base mBART è stato fine-tuned per produrre semplificazioni a livello di frase.</sample>
    <sample id="225">Per scopi di addestramento, vengono utilizzate 53 attività da 9 gruppi. Per il test, viene riservata l'intero gruppo di ragionamento del senso comune e vengono selezionate ulteriori 5 attività dai gruppi VQ e Miscellaneous.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Il documento discute le sfide della comprensione del linguaggio radicato, che comporta il mappare espressioni linguistiche naturali in azioni eseguibili in un ambiente specifico. Questo è cruciale per applicazioni come assistenti intelligenti, ricerca semantica e robot domestici. La sfida principale è la mancanza di radicamento durante la pre-addestramento dei modelli di linguaggio, che sono tipicamente addestrati su corpora testuali senza radicamento. Questo divario rende difficile per i modelli generare piani o programmi validi e grammaticali.

Il documento propone un nuovo framework chiamato Pangu, ispirato alla mitologia cinese, che separa la generazione e la discriminazione. In questo framework, un agente simbolico propone piani candidati, mentre un modello di linguaggio valuta e classifica questi piani. Questo approccio si concentra sulla discriminazione, che è più adatta per i modelli di linguaggio, piuttosto che sulla generazione diretta.

Pangu è stato testato su compiti di comprensione del linguaggio radicato, in particolare nel contesto delle domande basate su conoscenze, utilizzando modelli come BERT, T5 e Codex. Ha dimostrato un'eccellente efficienza di campionamento e prestazioni superiori rispetto ai modelli di riferimento come ArcaneQA. Pangu mostra anche robustezza in ambienti non-i.i.d., evitando l'overfitting osservato nei modelli autoregressivi. Il messaggio chiave è che per la comprensione del linguaggio radicato, la discriminazione potrebbe essere più efficace della generazione.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya introduces a study on improving argumentative writing through detecting and revising suboptimal claims. Text revision is crucial in argumentative writing to achieve optimal phrasing, influencing the audience's reaction. The study focuses on two tasks: detecting suboptimal claims and suggesting improvements. The research explores using revision patterns from online debate platforms like Kialo to model argument quality. Challenges include representativity and reliability of datasets, model complexity, contextual information, and topical/user bias. The study finds that revision-based data is effective for these tasks, with modeling the distance between claim versions aiding in detecting suboptimal claims. Contextual information's impact varies by task and quality issues. The paper provides a detailed analysis of strategies to address these challenges.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dal web utilizzato per addestrare DrBERT, il primo modello biomedico in francese.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi addresses the challenges of Simultaneous Speech Translation (SimulST), which involves translating spoken language into text in another language in real time. Current SimulST models face issues such as complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency levels. The proposed solution, EDAtt (Encoder-Decoder Attention), leverages existing offline Speech Translation (ST) models without retraining or altering their architecture. It uses a single model for each latency regime, managing latency through specific parameters and the attention mechanism.

EDAtt decides whether to emit a partial translation based on the concentration of cross-attention weights. A word is emitted if the attention is not concentrated on the last lambda speech frames, indicating stable information. For instance, if a speech chunk translates to "I'm going to talk about..." and the attention weights show that the first two words are stable, they are emitted, while the last word is withheld until more information is received.

The results show that EDAtt outperforms traditional strategies like Wait-k and Local Agreement when applied to offline models, with higher BLEU scores and lower latency. It also demonstrates the fastest computational-aware average lagging, making it a superior choice for SimulST. The authors have made their code and models open source to support reproducibility.</sample>
    <sample id="234">La strategia del prompting ha un impatto significativo sui risultati della traduzione. Un semplice esperimento ha mostrato che la differenza tra due diversi prompt per la stessa frase può essere di più di un punto BLEURT, con differenze estreme fino a 40 punti BLEURT. La qualità degli esempi utilizzati nel prompting è più importante della loro somiglianza con la frase di origine. In particolare, i prompt selezionati da dati di sviluppo di alta qualità mostrano prestazioni migliori rispetto a quelli selezionati dai dati di addestramento più rumorosi.</sample>
    <sample id="235">Le affiliazioni degli autori dell'articolo non sono specificate nel contenuto fornito.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono fornite per ciascun compito nel dataset MultiInstruct. Tuttavia, il contenuto specifico di queste istruzioni non è dettagliato nel testo fornito.</sample>
    <sample id="237">Gli autori propongono un test diagnostico chiamato KITMUS (Knowledge Integration from Multiple Sources) per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da diverse fonti. Introducono un compito di risoluzione dell'anàfora progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Definiscono tre impostazioni di KITMUS: "Background-Pretrain", "Background-Both" e "Background-Inference", per variare la disponibilità di informazioni di background e specifiche dell'entità. Valutano il dataset con partecipanti a studi umani e modelli di risoluzione dell'anàfora stabiliti.</sample>
    <sample id="238">Yebowen Hu presenta MeetingBank, un nuovo benchmark dataset per lo sviluppo di tecnologie di riassunto per riunioni. MeetingBank affronta la necessità di dataset per riunioni, concentrandosi su due sfide principali: ottenere riassunti di alta qualità e trovare risorse affidabili per riunioni pubbliche. Il dataset include 1.366 riunioni del Consiglio Comunale e quasi 7.000 istanze, con dati come trascrizioni, riassunti di riferimento e URL utili. I dati sono raccolti utilizzando l'API Speechmatics per convertire l'audio in trascrizioni e identificando riunioni specifiche tramite ID unici. Il dataset fornisce statistiche dettagliate, inclusi la durata delle riunioni, il numero di token e di oratori per città, e analizza la qualità dei riassunti utilizzando misure di copertura e densità. L'analisi mostra che i riassunti tendono a includere punti verbatim piuttosto che astrazioni, con Seattle e Boston che mostrano i punteggi di densità più alti.

Per valutare i sistemi di riassunto, sono stati testati sistemi estrattivi e abstrattivi, tra cui Oracle, LEAD, LexRank, TextRank, BART-Large, Pegasus, Longformer, DialogLM, HMNet e GPT-3. GPT-3 ha ottenuto i punteggi complessivi più alti in termini di fluidità e coerenza, ma ha mostrato prestazioni inferiori in termini di informatività e fattualità. Questo suggerisce che i sistemi di riassunto dovrebbero concentrarsi sulla cattura dei punti principali delle discussioni. Il dataset MeetingBank è destinato a guidare la ricerca futura e a fornire intuizioni sul processo decisionale del Consiglio Comunale.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e darò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance." Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello di linguaggio grande con 540 miliardi di parametri presentato l'anno scorso nel 2022. È addestrato su una vasta raccolta di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio sistematico più completo dell'uso del prompting di modelli di linguaggio grandi per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Ciò comporta l'uso degli ultimi set di test per evitare un sovrapporsi dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi all'avanguardia, cioè il miglior sistema, quindi la valutazione WMT. Utilizziamo metriche di traduzione automatica neurali all'avanguardia e, inoltre, mostriamo risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompting ha un grande impatto sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato un prompting one-shot e fornito due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000, ha mostrato una differenza di più di un punto BLEURT. E questo può arrivare, nei casi estremi, fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per un prompting 5-shot, dove abbiamo semplicemente contrassegnato ogni frase fornita al sistema con la lingua di appartenenza. Ad esempio, in questo caso, dove effettuiamo la traduzione dal tedesco all'inglese, le frasi sorgente tedesche sono contrassegnate con "tedesco:" e le traduzioni in inglese con "inglese:". Abbiamo visto che la forma effettiva del prompting non ha un grande impatto nel caso di diversi prompt brevi. È cruciale per il zero e il one-shot prompting. E quando passiamo, come nel nostro caso, a un prompting 5-shot, non c'è quasi differenza nella forma effettiva del prompting. Sono gli esempi che portano il peso principale. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt selezionati dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano una migliore performance quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati all'avanguardia hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma, PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM dicono che la fluidità di PaLM è comparabile ai sistemi all'avanguardia, ma la principale differenza deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Quindi, sembra che PaLM scelga di produrre una traduzione più scorrevole, a volte eliminando parti della frase sorgente nella traduzione. Tuttavia, la categoria "Style/Awkward" per PaLM è inferiore rispetto ai sistemi all'avanguardia, che è un segnale aggiuntivo che PaLM fornisce un output davvero scorrevole, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per ulteriori dettagli, vi prego di partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, uno studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento debole supervisionato. Nella supervisione debole, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come regole euristiche semplici, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Quando confrontate con le annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento debole supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali sotto tale rumore delle etichette in modo che i modelli addestrati generalizzino ancora bene. Nei lavori recenti in WSL, WSL sta per Weakly Supervised Learning, una pretesa comune è che le persone dicono di addestrare modelli solo sui dati debolmente etichettati e di ottenere un alto rendimento sui set di test puliti. Tecnicamente, questa pretesa non è sbagliata, ma c'è una trappola, che è che si assume che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci su questo setting del problema, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento debole supervisionato. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il dubbio menzionato solleva tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL o possiamo forse usare un set di validazione rumoroso invece? Secondo, se sono necessari dati puliti, o se i dati puliti sono obbligatori per far funzionare WSL, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, troviamo che, interessantemente, i metodi recenti di WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo di prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci di WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente, e il costo delle annotazioni per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci di WSL a ottenere un miglioramento delle prestazioni, come mostrato nella figura a sinistra. Tipicamente abbiamo bisogno solo di 20 campioni per classe per ottenere un alto rendimento. Ma non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento diretto su di essi otterrà ancora un miglioramento delle prestazioni. La figura a destra mostra la differenza di prestazioni tra gli approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e gli approcci di WSL, che usano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci di WSL. Infine, il miglioramento delle prestazioni affermato negli approcci di WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma metodi di WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw si comporta allo stesso modo degli altri metodi. Quindi nella pratica, non c'è motivo di scegliere metodi di WSL più complessi che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo mostrato che gli approcci di WSL recenti richiedono campioni puliti, manualmente annotati, per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Primo, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è stata effettuata tramite campioni di validazione puliti. Secondo, gli approcci di WSL dovrebbero essere confrontati con basi di apprendimento a pochi esempi, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un semplice ma forte baseline che dovrebbe essere considerato nel lavoro futuro in WSL. Infine, abbiamo reso open source il nostro codice. Puoi trovarlo tramite il codice QR su questa diapositiva. Sentiti libero di controllarlo. Grazie e goditi la conferenza.</sample>
    <sample id="241">Il paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" affronta le sfide nell'identificazione precoce delle informazioni errate sui social media, evidenziando le carenze delle attuali metodologie automatiche. Queste metodologie spesso utilizzano dataset retrospettivi e non riescono a gestire efficacemente le contromisure che emergono solo dopo la smentita pubblica delle affermazioni. Inoltre, non considerano adeguatamente la scala e il rumore delle piattaforme, minimizzando il ruolo degli esseri umani nel processo di rilevamento.

Il paper propone un framework di valutazione che integra il feedback umano in vari stadi del processo, andando dai tweet grezzi a output utilizzabili. Il sistema è composto da due componenti principali: la rilevazione di affermazioni fuorvianti e la verifica delle violazioni delle politiche. La prima componente utilizza un modello T5 per l'estrazione di affermazioni e un test di Fisher per il ranking, mentre la seconda componente impiega un modello BERT per la classificazione dello stile. L'obiettivo è rilevare trattamenti non approvati prima della loro smentita pubblica e verificare le violazioni delle politiche di Twitter.

L'evaluazione mostra che il sistema può rilevare il 65% delle violazioni delle politiche e conferma 124,2 violazioni per ora lavorativa umana. Il framework proposto mira a catturare realisticamente l'interazione tra sistemi e moderatori umani, promuovendo lo sviluppo di sistemi di rilevamento delle informazioni errate con un coinvolgimento umano più integrato.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono:

1. Valutazioni umane, come chiedere ai giudici umani di selezionare quale di due conversazioni è migliore.
2. Valutazioni utilizzando una scala Likert, dove i giudici umani valutano le conversazioni su una scala.
3. Confronti a livello di dialogo, dove i giudici umani confrontano la qualità complessiva di diversi dialoghi.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie includono "I giudici decidono casi nei tribunali."</sample>
    <sample id="245">Lining Zhang presenta il lavoro "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization", che esplora un metodo per identificare lavoratori di alta qualità su Amazon Mechanical Turk (MTurk) per compiti di riassunto. Il lavoro affronta le sfide delle metriche automatiche e delle pratiche di reclutamento su MTurk, proponendo un pipeline a due fasi per selezionare lavoratori ad alta concordanza. La prima fase, il "Qualification Task", valuta la capacità dei lavoratori di valutare correttamente diverse dimensioni, classificandoli in categorie: oro, argento, bronzo e blocco. Solo i lavoratori oro e argento passano, risultando in 26 lavoratori qualificati (8 oro, 18 argento) su 200 partecipanti. La seconda fase, l'"Endurance Task", testa la capacità di gestire un carico di lavoro elevato, con 12 lavoratori (4 oro, 8 argento) che superano il test, rappresentando il 6% dei partecipanti.

Il lavoro include anche un "Reference-based Task" per valutare le prestazioni generali, mostrando che 8 dei 12 lavoratori completano tutti i compiti, con un Krippendorff's Alpha di 0.534. Viene confrontato con i lavoratori MTurk di base, che utilizzano un filtro statistico chiamato MACE, e i lavoratori CloudResearch, che mostrano un Krippendorff's Alpha di 0.513 ma con un tasso di accettazione del compito più basso. L'analisi della correttezza tra diverse fonti di annotazione rivela una correlazione significativa tra i lavoratori del pipeline e CloudResearch, mentre i modelli GPT reali si allineano bene con le valutazioni degli esperti.

In sintesi, il pipeline consente di ottenere un'alta concordanza a un costo inferiore, evitando sprechi di risorse e offrendo una qualità simile a quella di CloudResearch. Le limitazioni includono il test solo in inglese, la mancanza di soluzioni universali per le domande e l'assenza di garanzie per la formazione sulla correttezza. Il lavoro è supportato da Google e suggerisce future ricerche su metodi di reclutamento di alta qualità e applicazioni in diversi compiti, lingue e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Jiho Kim from KAIST AI introduces the paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs," which addresses the gap in existing fact verification datasets by utilizing knowledge graphs (KGs) as evidence. Unlike datasets such as FEVER and VitaminC, which use Wikipedia text, or TabFact and InfoTabs, which use tables, FACTKG leverages KGs for more intuitive and reliable fact verification. The paper proposes a new task, Knowledge Graph-Based Fact Verification, highlighting the practicality of KGs in verifying claims and ensuring consistency in dialogue systems.

The FACTKG dataset uses DBpedia as the knowledge graph and includes claims in both written and colloquial styles. It features two labels: SUPPORTED and REFUTED, and involves retrieving evidence from DBpedia to verify claims. The dataset incorporates five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. One-hop claims involve verifying a single relationship between two entities, while conjunction claims require verifying multiple one-hop claims. Existence claims check for specific relationships, and multi-hop claims involve verifying paths between entities. Negation requires additional inference to confirm the absence of a relationship.

To create colloquial claims, the paper uses a colloquial style transfer model and presupposition templates. Baseline methods include Claim Only baselines, which verify claims without graph evidence, and the GEAR model, which uses graph evidence and outperforms other baselines. The GEAR model surpasses the majority class baseline of 51%, demonstrating the effectiveness of using KGs for fact verification. The dataset is available for download, and Jiho Kim invites further contact for more information.</sample>
    <sample id="248">Gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. Il framework mira a riannotare i dataset con annotatori diversi per ottenere un set demografico ricco, ma non specifica che gli annotatori siano bilanciati per ciascun gruppo demografico. Invece, si concentra sulla raccolta di annotazioni da un ampio gruppo di 1.000 annotatori provenienti da 87 paesi per studiare la posizionalità.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore alla frase di input, cercando di preservare la struttura rilevante. Queste perturbazioni hanno mostrato un aumento simile nei giudizi MPP, indicando che i modelli sono sensibili a queste modifiche in modo coerente.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare la qualità del dialogo su più aspetti o dimensioni specifiche, come rilevanza, coerenza, empatia e violazioni del buon senso, per comprendere meglio le forze e le debolezze di un modello di dialogo su un livello più dettagliato rispetto a una valutazione olistica.</sample>
    <sample id="251">Jingwei Yi è affiliato all'Università di Scienza e Tecnologia della Cina.</sample>
    <sample id="252">Sai Kiran Tanikella presenta il lavoro "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion", sviluppato con Abhinav Joshi, Akshat Sharma e Ashutosh Modi. Il progetto affronta la sfida della Prior Case Retrieval, essenziale per i professionisti legali che devono trovare documenti di precedenti rilevanti in un crescente volume di casi. Due contributi principali sono l'IL-PCR Dataset e il U-CREAT pipeline. L'IL-PCR Dataset, un nuovo benchmark per i compiti di PCR, include 7.070 casi legali indiani con un'ampia gamma di citazioni, superando il dataset COLIEE’21 per dimensione e complessità. Il U-CREAT pipeline utilizza tecniche di apprendimento non supervisionato e un approccio basato su eventi per migliorare l'efficienza della ricerca, dimostrando prestazioni elevate senza necessità di regolazioni specifiche per legge o demografia. L'estrazione degli eventi è centrale, rappresentando i documenti come raccolte di eventi estratti tramite parsing di dipendenza. Il pipeline include pre-elaborazione, parsing di dipendenza e post-elaborazione, creando un'interazione tra eventi del documento di query e candidati. Sono stati testati modelli di conteggio, trasformatori e basati su eventi, con i modelli basati su eventi che superano i metodi di base. In particolare, il modello Event Filtered Documents ha mostrato le migliori prestazioni, con un significativo miglioramento rispetto ad altri metodi. U-CREAT ha superato le approcci esistenti, incluso il recente approccio supervisionato MTFT-BERT, stabilendosi come lo stato dell'arte per il compito di recupero documentale COLIEE’21.</sample>
    <sample id="253">Mario Ezra Aragón presenta "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media," un progetto collaborativo tra ricercatori messicani e spagnoli. Il lavoro mira a rilevare i disturbi mentali analizzando automaticamente i post sui social media, supportando nuove tecnologie per avvisare l'insorgenza di tali disturbi. Utilizzando l'adattamento del dominio, il modello adatta BERT, originariamente addestrato su dati generali, al linguaggio specifico di Reddit e alla salute mentale. L'approccio proposto inizia con un modello linguistico di base, integrando informazioni da Reddit e salute mentale, e utilizza un lessico per guidare il processo di mascheramento, concentrandosi su parole importanti. I risultati mostrano che DisorBERT ha un buon equilibrio tra precisione e richiamo rispetto ai metodi di base. Analizzando le parole generate da DisorBERT, si nota una tendenza verso significati negativi o orientati psicologicamente. Ad esempio, in una frase con "I used to be able to cry," DisorBERT predice parole come "focus" e "sleep," associate a problemi mentali. Il modello evidenzia parole e frasi rilevanti nei post degli utenti, come "anxious" e "medication," cruciali per la depressione. In conclusione, l'approccio di adattamento del dominio doppio e mascheramento guidato è efficace nel catturare segni di disturbi mentali nei social media, superando il modello MentalBERT. Per il futuro, si prevede di esplorare risorse lessicali diverse e dati clinici.</sample>
    <sample id="254">Sun Qi from Nanjing University of Science and Technology presents research on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The study addresses the challenge of extracting relations among entities in documents using distantly supervised data, which often contains noise. Traditional methods using pseudo labels risk introducing false positives, leading to incorrect relations. The proposed framework aims to improve label quality by incorporating uncertainty estimation to assess the trustworthiness of model predictions.

The framework involves training a pre-denoising Document-level Relation Extraction (DocRE) model with both distantly supervised and human-annotated data to generate pseudo labels. To address the issue of false positives, the research introduces an instance-level uncertainty estimation method using Monte Carlo dropout. This method captures uncertainty scores for overlapping relations, allowing for better differentiation between false and correct pseudo labels.

Dynamic class uncertainty thresholds are proposed to filter out high-uncertainty pseudo labels, replacing original labels with those having lower uncertainty scores. A multi-phase training strategy iteratively relabels distantly supervised data to enhance model performance. The framework outperforms existing baselines on public datasets, demonstrating significant improvements in label quality and model performance. Key contributions include the uncertainty-guided label denoising framework, instance-level uncertainty estimation for overlapping relations, iterative re-labeling with dynamic thresholds, and overall performance enhancements.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero e uno-shot prompting. Per il prompting a cinque-shot, la forma non ha un grande impatto.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di stato dell'arte utilizzando ABC-Eval.</sample>
    <sample id="258">Nel video, Chiang Cheng-Han presenta il loro lavoro "Can Large Language Models Be an Alternative to Human Evaluation?" dove propongono di utilizzare modelli linguistici di grandi dimensioni per valutare la qualità del testo in elaborazione del linguaggio naturale. I modelli vengono istruiti tramite istruzioni per valutare campioni di testo su attributi come grammatica, coerenza, gradimento e rilevanza. Questo approccio è stato considerato innovativo al momento della presentazione alla conferenza ACL, poiché non esistevano lavori precedenti che esplorassero l'uso di modelli linguistici di grandi dimensioni per la valutazione.

L'obiettivo è trovare un'alternativa alla valutazione umana, che è spesso instabile e difficile da riprodurre. I modelli linguistici di grandi dimensioni, come T0, InstructGPT (Curie e Davinci) e ChatGPT, sono stati testati per valutare storie generate da GPT-2 o scritte da umani. I risultati mostrano che, sebbene alcuni modelli più piccoli non mostrino una chiara preferenza per le storie scritte da umani, modelli come Davinci e ChatGPT dimostrano una preferenza simile a quella degli insegnanti di inglese, suggerendo che possono essere utilizzati come alternativa alla valutazione umana.

Il lavoro esplora anche domande su come i modelli linguistici e gli umani concordano nelle valutazioni individuali, l'impatto delle variazioni nelle istruzioni e nei campioni di risposta, e i benefici e i costi dell'uso di modelli linguistici rispetto alla valutazione umana. Ulteriori dettagli e risultati su altre attività sono disponibili nel loro articolo.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." Semantic parsing involves creating semantic representations of user queries, such as SQL and Lambda Calculus. Cross-lingual semantic parsing translates queries from multiple natural languages into various meaning representations. Existing models are limited in scope, often missing languages like Chinese and certain meaning representations like Lambda Calculus.

To address these gaps, XSemPLR offers a unified dataset for cross-lingual semantic parsing, featuring 9 datasets across various domains, 5 tasks, 8 meaning representations, and 22 languages from 15 language families. The benchmark evaluates six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Few-shot transfer.

Translate-Test uses Google Translate API to convert source language queries to the target language, then applies a monolingual model. Monolingual Model uses the same language for both training and inference. Monolingual Few-shot trains models with only 10% of data. Multilingual Model trains on multiple languages simultaneously. Cross-lingual Zero-shot and Few-shot transfer involve training on one language and transferring to another.

Analysis shows Encoder-Decoder models, like mT5, outperform others across datasets. Training in multiple languages improves performance, except for English, which sometimes drops. Cross-lingual Few-shot transfer significantly reduces performance gaps compared to Zero-shot transfer. Encoder-Decoder models surpass previous work, and pretraining on English enhances Few-shot performance on target languages. However, models like Codex and BLOOM are inadequate for cross-lingual tasks.

In summary, XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing, revealing insights into multilingual model performance and highlighting the need for improved models in this area.</sample>
    <sample id="260">L'articolo menziona un singolo autore, Jingwei Yi.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle specifiche restrizioni imposte dai vari obiettivi.</sample>
    <sample id="262">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="263">The presentation focuses on "Mitigating Label Biases for In-context Learning," addressing the instability in in-context learning due to design choices like example selection and order. Prior research highlights that these choices introduce biases, but lacks a systematic categorization of these biases. This work introduces a typology of label biases in text classification, identifying a new bias type: domain-label bias. This bias arises from the task corpus influencing model predictions. Experiments show that random in-domain words can bias predictions, unlike random English words, indicating domain-label bias's impact.

The study categorizes biases into vanilla-label bias (uncontextual label preference), context-label bias (context effects), and domain-label bias (task corpus effects). Experiments reveal that tasks with small domain-label bias perform well with calibration, while those with large bias struggle. To address these biases, the proposed domain-context calibration uses random in-domain words to estimate and adjust model biases, improving performance significantly, especially in tasks with high domain-label bias. This method outperforms previous calibration techniques by considering domain-specific content-free text, leading to better decision boundaries and enhanced model performance across various datasets and models, including GPT-3. The work systematically investigates label biases and proposes effective calibration methods to enhance in-context learning.</sample>
    <sample id="264">Lin Wang, un dottorando presso l'Università di Zhejiang, presenta il suo lavoro "TAVT: Towards Transferable Audio-Visual Text Generation". Il paper affronta le sfide della generazione di testo multimodale, come l'audio-visivo, dove la raccolta di dati è difficile e costosa. I modelli esistenti spesso degradano in diversi domini a causa di variazioni nelle condizioni di costruzione. Per superare queste limitazioni, Lin propone il compito di generazione di testo audio-visivo trasferibile, che affronta i cambiamenti di dominio multimodale come stile visivo e energia audio.

Il paper introduce un framework modulare composto da tre componenti: una rete audio-visiva meta-mapper, un encoder e generatore di modelli linguistici audio-visivi, e un apprendimento contrastivo controcettuale. La rete meta-mapper mappa concetti visivi in uno spazio semantico audio unificato, utilizzando token apprendibili per allineare il contenuto visivo con lo spazio audio. L'encoder e il generatore utilizzano un trasformatore per valutare il contributo di diverse modalità al testo generato.

Il paper propone anche un apprendimento contrastivo controcettuale duale (DCLL) per ottimizzare direttamente l'allineamento visivo-testuale. Il framework è addestrato utilizzando un approccio meta simile a MAML, con domini di supporto e query per l'adattamento rapido.

Sono stati costruiti due benchmark basati su MSVD e MSR-VTT per valutare il metodo proposto, che ha superato i modelli di riferimento sia nelle impostazioni cross-dataset che cross-domain. In particolare, TAVT ha mostrato prestazioni robuste in domini a bassa risorsa. L'analisi delle caratteristiche audio ha confermato il loro impatto positivo sulle prestazioni.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">L'articolo non fornisce informazioni specifiche sulle affiliazioni degli autori.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di omissione, dove il modello sceglie di produrre una traduzione più scorrevole a volte omettendo parti della frase originale.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato realizzato dal Laboratorio di NLP di Emory, guidato dal Professor Jinho Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI. Immaginiamo di aver sviluppato un modello di dialogo e vogliamo vedere come si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, chiedendo a giudici umani di selezionare quale dei due dialoghi è migliore o di valutare i dialoghi su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potreste voler valutare più dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a un livello più dettagliato. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi comparativi esistenti o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio annotare i comportamenti nel chat, o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti di influenzare la qualità del chat nella recente letteratura. ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o il suo partner, fa allucinazioni di fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. A scopo di confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che i label di comportamento di ABC-Eval sono complessivamente più affidabili rispetto ai label raccolti con i metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni doppio-etichettate. Inoltre, i label di ABC-Eval sono più predittivi della qualità complessiva del dialogo rispetto ai metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, si può vedere come misurare la proporzione di turni con contraddizioni di sé e del partner spieghi il 5% e il 10% della qualità del dialogo, rispettivamente, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione catturasse un aspetto unico della qualità del chat utilizzando una regressione lineare passo-passo. Si può vedere come la combinazione di tutte le metriche di ABC-Eval spieghi oltre il 25% della qualità del dialogo, e rimuovendo le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità decente di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche. Queste metriche affidabili, informative e distinte di ABC-Eval ci permettono di valutare l'IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Si può vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune nel 20% delle loro risposte. Producono informazioni irrilevanti nel 15% delle risposte e si contraddicono o il loro partner circa il 10% delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale si evolverà nei prossimi mesi e anni. Grazie per averci guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono affiliati all'Emory NLP Lab, guidato dal Professor Jinho Choi presso l'Emory University, e hanno collaborato con Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, CFT significa "Continuous Fine-Tuning".</sample>
    <sample id="272">Sette autori sono coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, il mio nome è Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Esplorazione multilingue guidata dai dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Se la frase precedente era "Le cose potrebbero diventare pericolose se i ministri lo scoprissero", "mole" si riferisce a un agente segreto. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", "mole" si riferisce a una macchia di nascita. A seconda del contesto, il significato della parola cambia e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per metriche a livello di corpus come BLEU catturare queste traduzioni. Alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano su conoscenze di dominio e cura umana. In questo lavoro, cerchiamo di rispondere a due domande. Prima di tutto, quando la traduzione richiede contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. In lavori precedenti, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazione il contesto C fornisce sulla destinazione Y, dato la fonte X. Possiamo pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o di parola. Possiamo pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Analizziamo quindi le parole con alto P-CXMI per cercare schemi tra queste parole. E conduciamo la nostra analisi sui trascritti di TED Talks tradotti dall'inglese in 14 diverse lingue. Conducono l'analisi a tre diversi livelli. In primo luogo, guardiamo alle etichette di parte del discorso che hanno un alto P-CXMI medio. Questo ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi lessicali che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre nella forma di formalità corretta. Infine, guardiamo a diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati realmente dalla parola stessa, ma che sono espressi nella struttura della frase, come la risoluzione delle ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che riguardano il fenomeno. E abbiamo chiamato il nostro tagger il Multilingual Discourse-Aware, o MuDA tagger. Possiamo quindi notare che diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Poi utilizziamo il MuDA tagger applicandolo a un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il MuDA tagger ha identificato. Infine, utilizziamo il nostro benchmark, insieme ad altre metriche, per valutare diversi modelli di traduzione automatica a livello di documento. In primo luogo, quando utilizziamo metriche a livello di corpus: per BLEU, troviamo che i modelli agnostici al contesto hanno le migliori prestazioni. Ma poi, se utilizziamo COMET, i modelli consapevoli del contesto si comportano meglio. E se utilizziamo la misura f di parola, i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione automatica a livello di documento se utilizziamo solo metriche a livello di corpus. Poi utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Quindi questo suggerisce dove dovremmo vedere più progressi per la traduzione automatica a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, conduciamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli gestiscono bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya e Vignesh presentano il loro lavoro su "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". Il loro studio si concentra sull'importanza di valutare le traduzioni in lingue diverse dall'inglese, poiché queste lingue hanno regole grammaticali uniche e altre differenze. Hanno selezionato cinque lingue indiane—Tamil e Malayalam (Dravidian) e Hindi, Marathi, Gujarati (Indo-Aryan)—e hanno utilizzato il dataset Flores per generare 1.400 traduzioni candidate per ciascuna lingua, utilizzando sette diversi modelli di traduzione. Hanno raccolto annotazioni umane dettagliate da esperti bilingue, che hanno valutato gli errori di traduzione in base al framework MQM, classificandoli in errori di accuratezza, di fluidità e categorie speciali. I risultati mostrano che i modelli di traduzione più recenti, come NLLB e Indic Trans, hanno meno errori rispetto ai modelli più vecchi. Tra le metriche di valutazione, COMET ha mostrato le migliori correlazioni con le valutazioni umane. Hanno anche sviluppato una versione fine-tuned di COMET, chiamata IndicCOMET, che ha superato le basi COMET in tre delle cinque lingue e ha mostrato migliori prestazioni in generale. IndicCOMET ha dimostrato capacità zero-shot su lingue non viste e ha mostrato una maggiore robustezza rispetto a COMET nei test di sfida. Il dataset è disponibile pubblicamente per ulteriori ricerche.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel contenuto.</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" si basa sul concetto sociolinguistico di "contrassegnatezza", che distingue tra gruppi sociali dominanti (non contrassegnati) e gruppi marginalizzati (contrassegnati). Il metodo identifica le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati, utilizzando il metodo "Fightin’ Words" per calcolare i log-odds pesati e distinguere le parole principali per ciascun gruppo contrassegnato. Questo approccio consente di catturare stereotipi e schemi specifici senza fare affidamento su un lessico predefinito.</sample>
    <sample id="279">L'autore dell'articolo è Shangbin, uno studente di dottorato presso l'Università di Washington.</sample>
    <sample id="280">In this work, Shi Tao introduces "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," addressing the challenges in Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotion label of each utterance in a dialogue, utilizing textual, audio, and visual modalities. Existing methods often inadequately exploit multimodal information, struggle with minority emotion classes, and face difficulties distinguishing semantically similar emotions. To overcome these challenges, MultiEMO is proposed, featuring four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification.

The first contribution is VisExtNet, a novel visual feature extractor that captures facial expressions without encoding redundant scene-related information, addressing the irrelevance of visual surroundings. The second contribution is MultiAttn, a multimodal fusion model using bidirectional multi-head cross-attention layers to integrate information from different modalities effectively. MultiAttn-text, for instance, learns cross-modal correlations between textual and audio modalities, and then fuses these with visual cues.

The third contribution is the Sample-Weighted Focal Contrastive Loss, designed to prioritize hard-to-classify minority classes and enhance the distinction between semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performance, particularly improving results in minority and semantically similar emotions. Despite its success, limitations include the inability of VisExtNet to distinguish between speakers and irrelevant people, the requirement of a large batch size for SWFC loss on MELD, and suboptimal performance in minority emotions compared to majority classes.</sample>
    <sample id="281">Il lavoro "When Does Translation Require Context? A Data-driven, Multilingual Exploration" esplora quando la traduzione richiede contesto e come i modelli gestiscono tali casi. La ricerca, condotta da Kayo Yin e colleghi, utilizza CXMI per misurare l'uso del contesto nei modelli di traduzione automatica. CXMI è esteso a Pointwise CXMI per analizzare l'uso del contesto a livello di parola e di frase. L'analisi è stata condotta su trascrizioni di TED Talks tradotte in 14 lingue, identificando parole e fenomeni che richiedono contesto, come pronomi duali in arabo e scelte di formalità in cinese.

Il team ha sviluppato il MuDA tagger per identificare automaticamente parole legate a fenomeni discorsivi, creando un benchmark per la traduzione a livello di documento. I risultati mostrano che i modelli consapevoli del contesto superano quelli agnostici per certi fenomeni, come formalità e coesione lessicale, ma non per ellissi, pronomi e forma verbale. Le metriche di valutazione come BLEU, COMET e word f-measure evidenziano la difficoltà di valutare i sistemi di traduzione a livello di documento con metriche a livello di corpus. Infine, il benchmark mostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="282">Xuekai Zhu presenta "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" al ACL 2023. Questo lavoro affronta il compito di trasferimento di stile non parallelo a livello di storia, concentrandosi sul livello del discorso per imitare lo stile di un autore. La sfida principale è imitare le scelte linguistiche dell'autore a livello di discorso, poiché lo stile è spesso legato a specifici argomenti di scrittura, rendendo difficile il trasferimento di contenuti stile-specifici. StoryTrans risolve questi problemi apprendendo rappresentazioni del discorso dai testi di origine e combinandole con embedding di stile apprendibili per generare testi nello stile di destinazione. Il modello utilizza un obiettivo di addestramento innovativo per ridurre le caratteristiche stilistiche dalle rappresentazioni del discorso e migliorare la preservazione del contenuto, separando la generazione in due fasi: prima trasferendo il testo di origine con parole chiave di contenuto stile-specifico mascherate, poi generando il testo completo incorporando esplicitamente queste parole chiave. L'addestramento è diviso in due fasi: la prima utilizza un framework di addestramento consulenziale con perdite di auto-ricostruzione, disentanglement, ordine delle frasi e classificatore di stile; la seconda fase riempie i contenuti stile-specifici e rimuove i token mascherati. Sono stati raccolti nuovi dataset in cinese e inglese, e gli esperimenti mostrano che StoryTrans supera i baselines in termini di controllo dello stile e preservazione del contenuto, con visualizzazioni dello stile che allineano i test di trasferimento con i testi di riferimento nello spazio delle caratteristiche dello stile. StoryTrans migliora le storie inserendo frasi o trame pertinenti, mantenendo i contenuti principali e riscrivendo le frasi nello stile di destinazione mantenendo il significato di origine.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è la "Hudson's Word Grammar".</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented a paper titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference. The paper addresses limitations in current span-based UIE models, which rely heavily on precise span boundaries, leading to ambiguity in annotation. The proposed FSUIE introduces a fuzzy span mechanism, allowing for a continuous distribution of correct probabilities within a range, rather than fixed boundaries. This approach uses adaptive attention to better model span boundaries, incorporating a fuzzy span loss that combines Binary Cross Entropy and KL-divergence. A fuzzy span attention layer dynamically adjusts attention spans and decays attention at boundaries, enhancing the model's decision-making without affecting text encoding. Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's superior performance, achieving state-of-the-art results on several datasets. The model shows improved learning on small-scale data and better generalization for domain-specific tasks. An ablation study confirms that both fuzzy span loss and attention improve convergence and information extraction capabilities. Visualization of attention distribution confirms the model's focus on relevant semantic information. Overall, FSUIE effectively reduces reliance on precise span boundaries and enhances information extraction across various tasks.</sample>
    <sample id="285">Mingqi Gao from Peking University presents their work on "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The study addresses factual errors in dialogue summarization, highlighting two main solutions: incorporating factuality objectives during training or inference, and designing a Factual Error Correction (FEC) model. The FEC model, independent of the summarization model, takes a source document and a generated summary to produce a corrected version. Despite the importance of factual accuracy in dialogue summarization, existing FEC evaluations using metrics like FactCC and DAE are flawed. These metrics provide vague overall scores and blur the distinction between correction and generation, potentially allowing models to generate entirely new summaries without correcting errors.

The study argues for the use of manually annotated reference corrections to improve evaluation and training. These corrections ensure minimal changes to the original summary while maintaining fluency and non-redundancy. The introduction of reference corrections offers valuable training data and enables more accurate FEC model evaluations. The authors propose a new taxonomy of factual errors, distinguishing between content-based and form-based errors, and build an evaluation framework based on ERRANT, a grammar error correction metric. This framework involves alignment, classification, and comparison steps.

Experiments with FEC models in various training modes reveal that using reference summaries from dialogue datasets yields the best results with unreliable factuality metrics. The study emphasizes the need to change FEC evaluation methods, suggesting that human-corrected summaries during training improve performance. Combining human-annotated data with synthetic data is seen as a promising approach. However, current FEC models struggle with certain error types, such as additions, attribute errors, modality errors, and link errors.</sample>
    <sample id="286">Sarah Finch.</sample>
    <sample id="287">Quattro autori sono coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti, e Annie Louis.</sample>
    <sample id="288">Gli insiemi di dati che possono essere utilizzati per testare i fenomeni sintattici includono BLiMP, SyntaxGym e CrowS pairs.</sample>
    <sample id="290">Il contenuto non fornisce abbreviazioni specifiche per i cinque metodi per la prima domanda di ricerca. Menziona solo un metodo, "FTw", e un altro, "COSINE", in un contesto diverso.</sample>
    <sample id="291">Il modello viene valutato su 11 attività downstream biomediche e cliniche in francese, che includono il riconoscimento di entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering.</sample>
    <sample id="294">CamemBERT è inizialmente addestrato su dati di OSCAR (Open Super-large Crawled ALMAnaCH coRpus) con 138 GB di dati.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presents a collaborative project between the University of Turin and Amazon Alexa focusing on irony detection in natural language processing (NLP). The project challenges the traditional assumption of a single "ground truth" in data annotation, emphasizing the complexity of irony as a latent and pragmatic phenomenon. To address this, the team developed the English Perspectivist Irony Corpus (EPIC), collecting 300 short conversations from social media platforms like Reddit and Twitter over 1.5 years, covering five English varieties. Using Prolific, 74 annotators were recruited to label these conversations as "ironic" or "not ironic," with each conversation receiving about five annotations. The study revealed significant inter-annotator agreement variations based on demographics such as gender, age, and nationality. Perspective-aware models, fine-tuned on dataset splits by annotator characteristics, showed increased confidence compared to gold standard models. Notably, generational and geographical differences influenced irony perception, with closer generations and UK/Ireland annotators showing more disagreement. The findings highlight the importance of considering diverse perspectives in NLP model training.</sample>
    <sample id="297">Il progetto "From Dogwhistles to Bullhorns" esplora il concetto di dogwhistles, termini che trasmettono messaggi diversi a gruppi diversi, spesso con implicazioni nascoste e potenzialmente offensive. Un esempio è l'uso del termine "cosmopolitan" da parte di alcuni politici, che può essere interpretato come un riferimento antisemita da parte di un gruppo interno, pur mantenendo una facciata innocua per il gruppo esterno. Questo studio sviluppa una tipologia e un glossario di oltre 340 termini dogwhistle, concentrandosi su quelli razzisti, transfobici e antisemiti, principalmente in contesti statunitensi.

Il progetto include un'analisi di discorsi politici storici degli Stati Uniti, evidenziando l'uso crescente di dogwhistles da parte dei conservatori, specialmente dopo l'era dei diritti civili. Viene esaminata anche la capacità dei modelli linguistici, come GPT-3, di rilevare e interpretare i dogwhistles. I risultati mostrano che GPT-3 può identificare alcuni dogwhistles, specialmente quelli formali, ma ha difficoltà con quelli informali e transfobici. Inoltre, il progetto dimostra come i dogwhistles possano eludere la moderazione dei contenuti online, poiché le frasi offensive vengono percepite come meno tossiche quando i termini espliciti sono sostituiti con dogwhistles. In sintesi, il progetto fornisce una comprensione approfondita dei dogwhistles e delle loro implicazioni per la linguistica, la politica e la moderazione dei contenuti online.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni includono:

1. L'esperimento di continuare a pre-addestrare alcuni modelli con dati più recenti ha mostrato che le prestazioni degradano con un maggiore divario temporale tra i dati di addestramento e di test.
2. L'osservazione che non c'era un effetto di rendimenti decrescenti, indicato dal fatto che il miglior adattamento della linea rossa aveva una pendenza maggiore di uno, suggerendo che l'overfitting adattivo non era osservato.</sample>
    <sample id="299">Michalis Korakakis e Andreas Vlachos presentano un metodo di addestramento minimax per migliorare la robustezza dei modelli di Inferenza della Natura Logica (NLI) riducendo la dipendenza da scorciatoie. I modelli NLI, nonostante i risultati eccellenti su benchmark, spesso si affidano a scorciatoie, come la sovrapposizione di parole, che non sono affidabili su set di test fuori distribuzione. Le tecniche esistenti di mitigazione delle scorciatoie richiedono modelli ausiliari che possono non allinearsi con il comportamento del modello principale, limitando l'efficacia. Il metodo proposto si concentra su esempi "difficili" sotto-rappresentati che contrastano le scorciatoie presenti negli esempi "facili" dominanti. Utilizzando un obiettivo di addestramento minimax, il modello principale minimizza la perdita del compito NLI, mentre il modello ausiliario, un semplice rete feed-forward, massimizza la perdita del principale generando pesi per gli esempi che incentivano l'apprendimento da questi esempi difficili. Questo approccio non fa assunzioni sui tipi di scorciatoie e migliora le prestazioni fuori distribuzione su dataset come MNLI, FEVER e QQP, mantenendo un'alta accuratezza in distribuzione. Il metodo è valutato anche su modelli più grandi, scorciatoie sintetiche e set di test fuori dominio.</sample>
    <sample id="300">Belinda presenta un lavoro su un nuovo compito chiamato "interactive dictation," sviluppato in collaborazione con Jason Eisner, Adam Pauls e Sam Thomson presso Semantic Machines. L'interattiva dictation consente agli utenti di utilizzare la voce per dettare e modificare documenti in modo naturale e intuitivo. A differenza dei sistemi di riconoscimento vocale tradizionali, che supportano solo la dettatura, questo sistema permette di eseguire modifiche tramite comandi vocali. Gli utenti possono correggere errori in tempo reale e utilizzare comandi naturali per modificare il testo, senza dover ricordare template fissi.

Il sistema è caratterizzato da un'interleaving flessibile tra dettatura e editing, senza trigger specifici, e dall'uso di linguaggio naturale per specificare le modifiche. Belinda formalizza il compito in quattro passaggi: riconoscimento vocale, segmentazione delle frasi, estrazione e normalizzazione dei comandi, e esecuzione delle frasi in sequenza. Per sviluppare un sistema di base, è stato creato un dataset tramite un'interfaccia di annotazione e sono stati addestrati modelli separati per ciascun passaggio, sperimentando con architetture T5 e GPT-3.

I risultati mostrano che i modelli GPT-3 sono più accurati ma più lenti, mentre i modelli T5 offrono un buon equilibrio tra efficienza e accuratezza. Il lavoro apre la strada a ulteriori ricerche in questo campo, e il codice e i dettagli sono disponibili per facilitare lo sviluppo futuro.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, dopo il primo passaggio in cui ogni token di input è contrassegnato con un insieme non ordinato di token che appariranno nell'output, i token corretti sono presenti ma non sono ordinati. Il secondo passaggio utilizza un modello per prevedere una permutazione per mettere i token nell'ordine corretto.</sample>
    <sample id="303">Gli autori suggeriscono ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias per comprendere meglio se i modelli stiano generando stereotipi positivi a causa di un allineamento eccessivo ai valori o di altri metodi anti-stereotipati. Senza maggiore trasparenza, è difficile studiare e affrontare questi modelli perniciosi.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono frasi che vengono utilizzate nel paradigma di coppia minima (MPP) per valutare i modelli di linguaggio. In questo paradigma, una frase accettabile (grammaticale) viene confrontata con una frase inaccettabile (non grammaticale) per vedere se il modello attribuisce una probabilità più alta alla frase accettabile. Gli input inaccettabili sono quelli che non rispettano le regole grammaticali o le norme di accettabilità e vengono utilizzati per testare la capacità del modello di distinguere tra frasi accettabili e inaccettabili.</sample>
    <sample id="305">Dawei presenta il lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" con colleghi. Il lavoro esplora il campo del Weakly Supervised Learning (WSL), dove i dati sono etichettati con fonti di etichettatura deboli come regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità, risultando in annotazioni rumorose e meno costose rispetto alle annotazioni umane. Tuttavia, i modelli addestrati direttamente su dati debolmente etichettati tendono a memorizzare il rumore delle etichette, compromettendo la generalizzazione.

Il lavoro critica la pratica comune di addestrare modelli solo su dati debolmente etichettati e valutarli su set di test puliti, sottolineando che questo richiede comunque set di validazione puliti, spesso trascurati. Il lavoro indaga se i dati di validazione puliti siano necessari per WSL, quanti campioni puliti siano necessari e come utilizzarli al meglio.

I risultati mostrano che i metodi WSL recenti richiedono campioni di validazione puliti per funzionare correttamente, con una diminuzione delle prestazioni senza di essi. Aumentare il numero di campioni di validazione puliti migliora le prestazioni, ma l'addestramento diretto su questi campioni supera i metodi WSL. Inoltre, permettere l'addestramento continuo su campioni puliti migliora le prestazioni, rendendo i metodi WSL più complessi meno necessari.

Le raccomandazioni includono la segnalazione dei criteri di selezione del modello, il confronto con basi di apprendimento a pochi esempi e la considerazione dell'addestramento continuo come baseline. Il codice è stato aperto per la comunità.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano il loro lavoro sull'Entity Tracking nei modelli linguistici, sottolineando l'importanza di tracciare le entità e i loro stati in un discorso per comprendere meglio il contesto. Discutono le sfide nell'valutare le capacità di tracciamento degli stati delle entità nei modelli linguistici pre-addestrati, come la dipendenza dai dati di pre-addestramento e l'uso di semplici associazioni lessicali. Progettano un compito di valutazione che coinvolge scatole e oggetti, dove i modelli devono prevedere i contenuti delle scatole dopo operazioni di stato come spostamenti e aggiunte. Utilizzano modelli come Flan-T5 e GPT-3/3.5 con apprendimento in contesto a 2-shot per testare le capacità di tracciamento. I risultati mostrano che solo il modello text-davinci-003 dimostra un tracciamento non banale, suggerendo che il pre-addestramento su codice possa migliorare queste capacità. I modelli GPT-3.5, addestrati su codice, mostrano comportamenti di tracciamento migliori rispetto a quelli senza tale pre-addestramento. Anche se i modelli T5-base possono imparare il tracciamento con l'addestramento diretto, i modelli casualmente inizializzati non riescono a imparare il compito, evidenziando l'importanza del pre-addestramento. Tuttavia, rimane incerto se queste capacità si generalizzino oltre il loro setup.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche di valutazione per compiti come il riconoscimento delle entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering. Non specificano metriche specifiche, ma menzionano che i modelli hanno ottenuto i migliori risultati sui compiti con dati della stessa natura di quelli su cui sono stati addestrati.</sample>
    <sample id="308">Jenny, a PhD student at Carnegie Mellon University, presented her work on NLPositionality, which examines design biases in datasets and models. The research, conducted with collaborators from the University of Washington and the Allen Institute for AI, highlights how NLP technologies can exhibit systematic performance differences across populations due to the positionality of researchers and developers. Positionality refers to the perspectives shaped by demographics, identity, and life experiences, influencing research outcomes.

The study investigates whether datasets and models reflect certain positionalities by comparing annotations from diverse users with existing datasets and models. Using the NLPositionality framework, the research re-annotated datasets with diverse annotators to gather rich demographic data. This data was then compared to model predictions using Pearson's R correlation scores, differing from traditional annotator disagreement literature by focusing on end-user alignment.

The study utilized the Lab in the Wild platform to recruit diverse volunteers from 87 countries, resulting in over 16,000 annotations. Findings revealed that NLP datasets and models are most aligned with English-speaking countries and individuals with higher education. However, non-binary individuals were less represented, indicating a bias.

To address these biases, the study recommends documenting design choices, adopting a perspectivist approach in NLP research, and developing specialized datasets and models for specific communities, like the Masakhani initiative. The presentation concluded with an invitation to explore further details on their dashboard and paper.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'inter-annotatore agreement su 100 conversazioni doppiamente annotate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="311">L'affiliazione degli autori dell'articolo non è fornita nel contenuto inglese fornito.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento principalmente perché è il primo dataset di benchmark per l'addestramento di istruzioni multi-modalità. Comprende 62 compiti diversi che coprono 10 categorie ampie, derivati da 21 dataset open-source esistenti, ciascuno con cinque istruzioni scritte da esperti. Questo dataset affronta la discrepanza nella disponibilità di dataset di istruzioni tra NLP e compiti multi-modalità, dove esistono più di 1600 compiti di istruzioni solo per il linguaggio, ma nessun dataset di istruzioni multi-modalità su larga scala pubblicamente disponibile.</sample>
    <sample id="313">Due autori sono coinvolti nell'articolo: James Finch e Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura in cui due elementi (congiunti) sono collegati da una congiunzione, come in "Lisa e Bart". Nel contesto della discussione, si esplorano diverse teorie su come queste strutture siano organizzate in termini di dipendenza, con approcci che variano tra strutture asimmetriche (dove un congiunto è il capo) e strutture simmetriche (dove tutti i congiunti sono considerati capi).</sample>
    <sample id="315">Il documento non specifica per quanto tempo, in media, sono stati utilizzati i prompt nel contesto dello studio.</sample>
    <sample id="316">I risultati implicano che il modello T5 più piccolo, quando adeguatamente addestrato sul dataset CoScript, può generare script di qualità superiore rispetto a molti modelli di linguaggio più grandi. Questo indica che i modelli più piccoli e specializzati possono superare i modelli più grandi quando vengono addestrati su dataset appropriati.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," addressing challenges in information extraction (IE) tasks like named entity recognition (NER) and relation extraction (RE). Traditional IE models, such as T5 and GPT-3, use text-to-text formats, leading to mismatches between linearized structured outputs and plain text inputs. This discrepancy often requires extensive structured data and complex decoding strategies.

To resolve this, CodeIE transforms IE tasks into structure-to-structure code generation tasks using code language models like Codex. This approach aligns input and output structures, simplifying the conversion of text to structured formats. For NER, a function is defined to extract named entities, with few-shot in-context demonstrations guiding the model to output structured text-entity pairs.

The method was evaluated on three NER and four RE datasets, comparing T5, UIE, GPT-3, and Codex models. CodeIE's code-style prompts significantly outperformed traditional text-style prompts in one to few-shot settings. Analysis showed lower perplexity for code format inputs with models like CodeT5, indicating better alignment with IE tasks. Structural errors were minimal with Codex and code prompts, unlike GPT-3 with text prompts, which often produced incorrect labels.

Overall, Codex outperformed GPT-3 in IE tasks, with code format prompts enhancing recall. This research suggests that leveraging code generation models can improve few-shot IE performance, offering a promising direction for future IE methodologies. The paper and code are publicly available for further exploration.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedico e clinico." In questa presentazione, parleremo prima del modeling del linguaggio nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. Abbiamo anche introdotto un confronto di modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti biomedici e clinici downstream in francese. Infine, concludiamo riguardo agli esperimenti e vi forniamo ulteriori dettagli su come accedere a quei modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale, offrendo enormi miglioramenti delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word2vec, fastText, ecc. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT, e anche a domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. Modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Ci siamo quindi chiesti quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzi e se i dati raccolti siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanta quantità di dati sia necessaria per addestrare un modello specializzato sui dati in francese? 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS; una seconda versione con 4 GB di un set di NACHOS; una prima versione di ChuBERT, un modello clinico con 4 GB di frasi prese da appunti clinici; e una versione finale di ChuBERT con un mix di 4 GB di un set di NACHOS e 4 GB di appunti clinici. In aggiunta a questo confronto, abbiamo introdotto tre modelli addestrati su un pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro basato anche su CamemBERT, ma addestrato questa volta su 4 GB di appunti clinici e infine uno basato sul modello biomedico inglese PubMedBERT, addestrato su 4 GB di un set di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, abbiamo raccolto dati per compiti downstream pubblici e privati come il riconoscimento delle entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering. Questi modelli sono stati confrontati con sei modelli di base: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli hanno ottenuto i migliori risultati nei compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, osserviamo che i dati da fonti eterogenee sembrano essere più versatili. Notiamo anche che l'uso di più dati si traduce in migliori prestazioni. In generale, il pre-addestramento da zero sembra ottenere prestazioni migliori su la maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento continuo utilizzando i pesi e la tokenizzazione di CamemBERT addestrato sul subset di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Non è così per il modello basato sui pesi e tokenizer di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema specifico ha ottenuto prestazioni migliori su nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, qui CamemBERT. Notiamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, con licenza MIT, e tutti gli script di addestramento sono disponibili nel nostro repository GitHub. Grazie per questa presentazione e non vediamo l'ora di scambiare idee durante la sessione poster a Toronto.</sample>
    <sample id="319">Il lavoro esamina le seguenti strategie di apprendimento:

1. Pre-training da zero con dati di NACHOS di dimensioni diverse (4 GB e 7 GB) per DrBERT.
2. Pre-training da zero con dati clinici e una combinazione di NACHOS e dati clinici per ChuBERT.
3. Pre-training continuo utilizzando i pesi e la tokenizzazione di CamemBERT su un sottoinsieme di 4 GB di NACHOS.
4. Pre-training continuo utilizzando i pesi e la tokenizzazione di CamemBERT su 4 GB di note cliniche.
5. Pre-training continuo utilizzando i pesi di PubMedBERT su un sottoinsieme di 4 GB di NACHOS.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test, noto come overfitting adattivo, non è stato osservato. Questo è indicato dal fatto che la linea di miglior adattamento rossa ha una pendenza maggiore di uno, il che significa che ogni unità di miglioramento su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++, indicando l'assenza di rendimenti decrescenti.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i tipi di semplificazione nei DEPLAIN, come la semplificazione lessicale, strutturale e il livello complessivo di semplificazione. Inoltre, è stata valutata l'efficacia dei metodi di allineamento automatico, in particolare il metodo MASSalign, utilizzando le coppie di frasi manualmente allineate nel corpus DEPLAIN come standard d'oro. Inoltre, la qualità della semplificazione è stata valutata attraverso l'addestramento di modelli di linguaggio, come long-mBART e base mBART, per produrre semplificazioni a livello di documento e di frase, con i risultati proposti come base benchmark per il problema della semplificazione automatica del testo.</sample>
    <sample id="322">Enrico will present at ACL 23 on "What does a Text Classifier Learn about Morality?" He explains that human morality, which helps distinguish right from wrong, is subjective and varies among individuals. Traditional approaches in NLP treat morality on a singular scale, but this oversimplifies the complexity of moral judgments, as seen in divisive issues like abortion or LGBTQ rights. Enrico introduces the Moral Foundation Theory, which posits five different moral foundations that people prioritize differently, influencing their moral judgments.

Enrico's paper aims to explore how language models understand morality in text using explainable AI techniques. They focus on the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven domains, such as #AllLivesMatter and #BlackLivesMatter. The study investigates whether language models can discern the nuanced differences in moral expression across these domains.

The research reveals that language models can recognize differences in moral rhetoric, such as the concept of subversion. For instance, in the context of #AllLivesMatter, subversion is associated with negative terms like "overthrow" and "mayhem," while in #BlackLivesMatter, it is more positively framed. This finding highlights the importance of domain-specific models to avoid misunderstandings of morality. Enrico concludes by emphasizing the need for careful consideration of moral expression in different contexts to prevent dangerous misinterpretations.</sample>
    <sample id="323">Yujie Wang from Shanxi University presents a paper on enhancing Commonsense QA by integrating language models with knowledge representation learning. The paper addresses the challenge of machines needing to retrieve and utilize common knowledge to answer questions. Traditional methods combine language models and knowledge bases but face issues like noisy entity retrieval and limited interaction between text and subgraph modalities.

The proposed solution, DHLK, involves building a Heterogeneous Knowledge Graph (HKG) using multiple knowledge bases, optimized through a two-stage pruning strategy and Knowledge Representation Learning (KRL). The HKG is enhanced by connecting paraphrases of key entities from WordNet and Wiktionary. The language model RoBERTa, along with Mask Self-Attention, encodes and fuses QA contexts and entities, dynamically pruning less relevant entities based on attention weights.

Entity and relation embeddings are optimized using TransE, and the subgraph is modeled using Relation Mask Self-Attention (RMSA), inspired by RGAT, to incorporate relationships into Mask Self-Attention. The HKG graph embedding is obtained through max-pooling, and path information is integrated into the QA context for enhanced embedding representation.

For answer prediction, the HKG graph embedding, path-enhanced QA context, and QA context embeddings are input into an MLP to determine answer probabilities. Experiments on CommonsenseQA and OpenBookQA using ConceptNet, WordNet, and Wiktionary show that the method outperforms other LM and HKG approaches, demonstrating effective integration of language models and knowledge representation for improved Commonsense QA.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. I risultati preliminari dimostrano che i modelli linguistici occupano tutte e quattro le quadranti del campus politico, con GPT-4 che è il modello più liberale e la serie GPT generalmente più socialmente liberale rispetto alla serie BART. Inoltre, i bias politici dei modelli linguistici possono essere influenzati dai dati di preaddestramento, con esperimenti controllati che mostrano spostamenti ideologici quando i modelli sono ulteriormente addestrati su corpora partigiani.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann, e oggi ti darò una breve introduzione al nostro articolo "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". Questo è un lavoro congiunto con i miei advisor Alexander Koller e Ivan Titov. La compositional generalization può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni di frasi non viste durante l'addestramento, ma viste individualmente. Nel contesto del parsing semantico, testare la compositional generalization potrebbe apparire così. Come al solito, abbiamo un set di addestramento di enunciati. In questo caso, "La ragazza dormì." e "Mary sapeva che la ragazza dormì." Questi enunciati sono accoppiati con forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard del machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli seq2seq ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono slegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate a colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono destinati a catturare il processo compositivo che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi non sono solitamente dati e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. Tipicamente, ciò comporta una pre-elaborazione formale specifica delle forme logiche, ad esempio, per gestire i simboli di variabile. Ottenere alberi può anche comportare procedure di induzione di grammatica specializzate. In questo articolo, non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, etichettiamo ogni token di input con un insieme multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Per questo motivo, nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non pone alcun vincolo rigido sulle permutazioni possibili. Questo rende il nostro approccio abbastanza flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Andiamo da sinistra a destra sull'output e determiniamo quale token di insieme multiset mettere in ogni posizione. Per la prima posizione di output, semplicemente selezioniamo uno, come evidenziato in rosso. Poi saltiamo al prossimo token di insieme multiset per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token di insieme multiset. Continuiamo questo processo fino a quando ogni token dal primo stadio è stato visitato esattamente una volta. Per darti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto sfidanti, tuttavia. Nel nostro articolo, risolviamo alcune sfide tecniche interessanti. Innanzitutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale insieme multiset proviene, il che rappresenta una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta con sé la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del "Venditore Ambulante". Approssimiamo questo con una continuazione rilassata amica GPU che consente anche di retropropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, per favore consulta il nostro articolo o visita il nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è la presenza di due credenze o azioni inconsistenti, come quando una persona afferma di sapere che le sigarette potrebbero ucciderla ma poi prende delle sigarette, giustificando l'azione con una dichiarazione come "non credo di poter mantenere il mio lavoro senza di esse". Queste credenze e azioni sono in dissonanza.</sample>
    <sample id="327">In this work, "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," we introduce ManagerTower, a novel architecture designed to enhance vision-language (VL) representation learning. Building on the two-tower architecture, which includes textual, visual, and cross-modal encoders, ManagerTower addresses limitations in previous models like BridgeTower by adaptively aggregating insights from multiple unimodal layers. Unlike BridgeTower, which restricts each cross-modal layer to a single unimodal layer representation, ManagerTower employs managers in each cross-modal layer to dynamically combine insights from various unimodal experts. This approach allows for more effective exploitation of semantic knowledge at different levels, facilitating comprehensive cross-modal alignment and fusion.

ManagerTower utilizes RoBERTa and CLIP-ViT base as unimodal encoders and demonstrates superior performance on downstream tasks with only four million images for pre-training. Notably, it achieves a 39.15% accuracy on the Wikivideo test standard, outperforming both base-size models pre-trained on larger datasets and some models with more parameters. Visualization of aggregation weights in VQAv2 dataset reveals that adaptive managers exhibit distinct trends, differing significantly from static managers, and adaptively exploit unimodal semantic knowledge across cross-modal layers. This work underscores the effectiveness of ManagerTower in leveraging diverse semantic insights, offering a scalable and powerful solution for vision-language tasks. The paper, code, and models are available on Archive and GitHub.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">Minghang Zheng from Peking University presents a study on zero-shot video sentence localization, aiming to identify video segments relevant to natural language queries without manual annotations. The research addresses the limitations of existing methods, which generate simple pseudo-queries and pseudo-events, leading to misalignment and label noise. The proposed method, noise-resistant Structured Pseudo-Label generation, uses a pre-trained image caption model to create complex pseudo-queries and a pre-trained model to generate pseudo-events with high relevance to queries. It models the temporal structure of events to ensure relevance within events and irrelevance outside them. The method reduces label noise by weighting samples based on model confidence and IoU, and by refining labels with high-confidence predictions. Experiments on ActivityNet Captions and Charades-STA datasets show that this method outperforms existing zero-shot methods, achieving the best zero-shot performance. The code is available via a QR code.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo ha funzionato uguale o meglio dell'addestramento iterativo.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">I dati nel parametro di riferimento MuDa sono stati tratti da trascrizioni di TED Talks tradotte dall'inglese in 14 diverse lingue.</sample>
    <sample id="333">Wenhao from Nanjing University introduces "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," acknowledging collaborators from Shanghai AI Lab, Nanjing University, and the University of Hong Kong. The work addresses the limitations of neural machine translation (NMT) models, which often have non-smooth representation spaces leading to poor generalization, especially for low-frequency tokens. The proposed kNN-MT solution smooths predictions using nearest neighbors in the representation space but faces challenges like time-consuming neighbor retrieval and static datastores.

To address these issues, the INK framework injects kNN knowledge into NMT models. It involves a training loop with two steps: extracting kNN knowledge to adjust representations and asynchronously updating the datastore with new representations. The framework aligns contextualized representations with token embeddings and kNN token embeddings to enhance semantic meaning and address sparsity. The INK system outperforms kNN-MT, achieving better performance with less memory and faster inference.

Experiments on the WMT’19 German-English news translation task show that INK improves the representation space of even top-performing NMT models. The research explores smoothing the representation space with a small adapter, the impact of kNN knowledge on representation distribution, and the combined use of adapters and datastores. Results indicate that INK achieves higher BLEU scores with less memory and that further improvements are possible with more effective frameworks. Overall, INK provides significant gains in translation performance, with an average increase of 1.99 COMET and 1.0 BLEU scores compared to state-of-the-art kNN-MT systems.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio di origine e trasferirlo per eseguire compiti in un altro linguaggio. Nel contesto della presentazione, ci sono due varianti: il trasferimento interlinguistico zero-shot, dove il modello viene addestrato su un linguaggio e testato su un altro senza dati di addestramento nel linguaggio di destinazione, e il trasferimento interlinguistico few-shot, dove il modello viene addestrato su un piccolo set di dati nel linguaggio di destinazione.</sample>
    <sample id="337">The presentation introduces a novel approach for handling out-of-vocabulary (OOV) words in embedding-based models through a graph-based relation mining method. The research addresses the challenge of representing OOV words, which are crucial for the performance of downstream models. The proposed method leverages word formation and association, inspired by human learning habits, to infer the meanings of OOV words. A Word Relationship Graph is introduced, which mimics lexical rules of word formation and association. This graph is constructed by tokenizing OOV words into wordpieces and associating them with relevant words, forming a two-level graph. Each word or wordpiece acts as a node, with embeddings serving as node attributes. The first layer retains complete wordpiece information, while the second layer samples nodes to reduce noise. A self-attention network assigns attributes to OOV nodes based on their characters. Two levels of Graph Attention Networks are applied to extract important information and reduce noise impact, with a readout block layer providing a graph-level representation. A simple one-layer Graph Convolutional Network captures word formation information. Contrastive learning is used in the loss function to mimic the vector space of background embedding models, encouraging proximity between relevant samples. The model outperforms baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models in downstream tasks. The model's applicability to other languages depends on the rationality of word decomposition, performing well with English due to reasonable word segmentation. The approach is particularly suited for agglutinative languages, while fusional languages present more challenges. Overall, the model effectively handles complex word formations and shows promise for broader language applications.</sample>
    <sample id="338">Bingsheng presenta il lavoro "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations," un progetto collaborativo tra Rensselaer Polytechnic Institute, Northeastern University e IBM Research. Il paper indaga la qualità delle spiegazioni umane annotate, che sono soggettive e variano a seconda del compito. Tradizionalmente, metriche come BLEU e ROUGE valutano le spiegazioni basandosi sulla somiglianza delle parole, mentre il simulatability score misura il cambiamento delle prestazioni dei modelli con o senza spiegazioni, senza considerare le differenze di compito.

Il lavoro introduce una struttura dati unificata per vari compiti, convertendoli in un compito a scelta multipla, e analizza l'utilità delle spiegazioni attraverso esperimenti con modelli T5 e BART su cinque dataset. I risultati mostrano che le spiegazioni umane possono migliorare le previsioni dei modelli, anche se considerate di bassa qualità in precedenti studi. Il paper propone il nuovo metrico TREU, che estende il simulatability score valutando l'utilità delle spiegazioni durante il fine-tuning. I risultati dimostrano che TREU è più efficace nel riflettere la qualità delle spiegazioni rispetto al simulatability score, evidenziando l'importanza di considerare il compito e il formato delle spiegazioni. Il lavoro sottolinea l'importanza di valutazioni di qualità per migliorare la collaborazione umana nelle attività di annotazione.</sample>
    <sample id="339">Le affiliazioni degli autori dell'articolo sono Saarland University in Germany.</sample>
    <sample id="340">Kuan-Hao Huang from UCLA presents "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation," a collaborative work with Varun, I-Hung, Anoop, Kai-Wei, and Aram. The study addresses the need for large-scale, high-quality paraphrase data in NLP, which is crucial for applications like question answering, chatbots, and robustness improvement. Existing datasets, such as MRPC, PAN, and Quora, are high-quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity.

The proposed solution, ParaAMR, leverages Abstract Meaning Representations (AMR) to create syntactically diverse paraphrases. AMR graphs capture the abstract meaning of sentences, with nodes representing semantic concepts and edges representing semantic relations. The process involves using a pre-trained AMR parser to generate an AMR graph from a source sentence, changing the focus by randomly selecting a new root node, and modifying edges and labels. An AMR graph-to-text generator then produces text from these modified graphs, ensuring semantic similarity with varied syntax.

ParaAMR contains approximately 15 million source sentences, each with around 6.9 paraphrases, demonstrating greater syntactic diversity compared to other back-translation datasets. Quantitative analysis shows that ParaAMR maintains semantic similarity while achieving higher syntactic diversity. The dataset enhances several NLP applications: it improves sentence embeddings in the STS benchmark, offers better syntactic control in paraphrase generation, and boosts few-shot learning performance through diverse paraphrases.

In conclusion, ParaAMR is a large-scale, syntactically diverse paraphrase dataset constructed via AMR back-translation, benefiting various NLP tasks. The dataset is available for further research and application.</sample>
    <sample id="341">Gli autori fanno ricorso a due misure di latenza: l'average lagging, che misura la latenza, e il computational-aware average lagging, che tiene conto dei tempi computazionali del modello per prevedere l'output.</sample>
    <sample id="342">Gao Jingsheng presents the paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming," co-authored with Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan. The paper addresses the need for a large-scale video-sourced dialogue dataset to enhance open-domain dialogue systems, which currently rely heavily on text-sourced data. Existing video-sourced datasets are limited in scale due to manual annotations and scripted conditions. The paper highlights the importance of personalized dialogue for applications like virtual streamers and employees, noting challenges in utilizing persona information and the lack of multi-party dialogue data in Chinese.

To overcome these barriers, the authors propose LiveChat, a dataset constructed from Chinese TikTok and Douyin streaming videos. The dataset is created through three steps: extracting and transcribing audio, matching audience comments to speakers, and collecting persona information through manual labeling and classifiers. LiveChat is distinguished by its video-sourced nature, larger scale, personal annotations, and longer average sessions compared to existing datasets.

Experiments on response modeling and addressee recognition tasks demonstrate the benefits of persona profiles and longer sessions. The study finds that BART outperforms other models, indicating LiveChat's distinctiveness from existing datasets. Human evaluation shows that large language models (LLMs) perform well in terms of informativeness, with performance improving with more demonstrations but slightly decreasing beyond eight due to noise. The paper concludes by emphasizing the potential of LiveChat for efficient transfer learning of LLMs and future research directions.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha e oggi, insieme al mio co-autore Martin, presentiamo il nostro lavoro "Il Test KITMUS: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita durante la pre-training, e la conoscenza fornita negli input al tempo di inferenza. Lavori recenti su compiti come il question answering mostrano che i modelli possono utilizzare la conoscenza acquisita durante la pre-training per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede anche la conoscenza fornita al tempo di inferenza. Ad esempio, nella frase "John ha visto il presidente appena eletto in TV", i parametri pre-training possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono conoscere in modo affidabile chi sia l'entità specifica "John" o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuta la pre-training. Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale intensivi di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante la pre-training che quella fornita al tempo di inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione delle coreferenze, progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti a uno studio umano e modelli di risoluzione delle coreferenze stabiliti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome dato richiede due tipi di informazioni. Prima, conoscenza specifica dell'entità, come "Servin è un giudice". E seconda, conoscenza di base, come "I giudici decidono casi nei tribunali". Generalmente, la conoscenza di base è appresa durante la pre-training dei grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al tempo di inferenza. Variamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Prima, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base è supposta essere disponibile al tempo di pre-training. Secondo, c'è l'impostazione "Background-Both", dove la conoscenza di base è disponibile sia al tempo di pre-training che al tempo di inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al tempo di inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-training dei modelli. Ad esempio, perché si sono sviluppate nuove occupazioni dal tempo della pre-training. Ecco un esempio di come controlliamo la disponibilità di fatti nelle fonti vere. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di base "I politici cercano seggi eletti nel governo" sia contenuta nei parametri pre-training e nel contesto al tempo di inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo non solo la conoscenza specifica dell'entità ma anche la conoscenza di base sui politici nel loro contesto al tempo di inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione immaginaria "mirituer" invece di politico perché "mirituer" è improbabile che sia contenuto nei parametri pre-training. Valutiamo il dataset sia con partecipanti a uno studio umano che con modelli di risoluzione delle coreferenze stabiliti. In questa figura, mostriamo i risultati dei modelli che hanno ottenuto i migliori risultati sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico per il compito su KITMUS, entrambi i modelli non si comportano bene. Quando addestrati su KITMUS, tuttavia, entrambi C2F e BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su set di dati di risoluzione di riferimenti generici, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze immaginarie indicano che nemmeno i modelli che hanno ottenuto i migliori risultati possono integrare in modo affidabile la conoscenza di base fornita solo al tempo di inferenza. Per riassumere i principali punti del nostro articolo, molti modelli di risoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti. Tuttavia, anche i modelli che hanno ottenuto i migliori risultati sembrano avere difficoltà a integrare in modo affidabile la conoscenza di base presentata solo al tempo di inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il dataset e il codice su GitHub. Grazie per l'ascolto.</sample>
    <sample id="344">Gli svantaggi dei metodi basati su alberi includono la necessità di ottenere gli alberi, che spesso non sono forniti e devono essere derivati attraverso processi che possono essere complessi e computazionalmente costosi. Questo può comportare una pre-elaborazione formale significativa delle forme logiche, come la gestione dei simboli di variabili, e può richiedere procedure di induzione grammaticale specializzate.</sample>
    <sample id="345">The paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov addresses the challenge of compositional generalization in semantic parsing. Compositional generalization involves handling deeper recursion and unseen phrase compositions that were not encountered during training. Traditional seq2seq models struggle with this, often failing to maintain systematic input-output correspondences. A common solution involves using trees to capture compositional processes, but this requires complex pre-processing and grammar induction.

The authors propose a novel treeless neural seq2seq model that directly models correspondences between input and output fragments. The model operates in two steps: first, it tags each input token with an unordered multiset of output tokens. Then, it predicts a permutation to order these tokens correctly. This approach introduces a flexible permutation model that does not impose hard constraints on possible permutations, allowing for expressive generalization.

The permutation model works by sequentially selecting multiset tokens for each output position, akin to solving a "Traveling Salesman" problem, which is NP-hard. The authors approximate this with a GPU-friendly continuous relaxation, enabling backpropagation and learning of linguistically plausible permutations.

Experimental results on the COGS benchmark show that the proposed method significantly outperforms other treeless models in generalizing to deeper recursion. However, some structural generalization challenges remain. The paper also discusses technical solutions for training alignment and permutation induction, contributing to the field of compositional generalization without relying on trees.</sample>
    <sample id="346">L'affiliazione degli autori non è menzionata nel contenuto fornito.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models." Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, queste misure presentano diverse limitazioni. Di solito si basano su set di dati costruiti manualmente che sono molto dispendiosi in termini di tempo da curare e misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è il concetto che le identità sociali multifaccettate possono amplificare i pregiudizi e costituire luoghi unici di danno. Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM addestrati con istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario utilizzando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa." E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marchio di identità che vogliamo in questo prompt. Ecco alcune generazioni di esempio da GPT-4. Subito vediamo che, sebbene le uscite non siano esplicitamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come modesta; la donna del Medio Oriente è riferita usando parole come esotica e come riferimento a una regione affascinante. E entrambe le personalità di donne di colore fanno riferimento all'ascendenza mentre la personalità dell'uomo bianco non ha nulla del genere. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste personalità. I nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche dando questi prompt a soggetti umani, sono stati in grado di evidenziare stereotipi razziali. E questo consente un confronto diretto tra le personalità generate e le risposte scritte dagli umani. La seconda parte è il metodo delle parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, di cui parlerò a breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle Parole Segnate si basa sul concetto sociolinguistico di "segnatura", che afferma che esiste un default non segnato e qualsiasi gruppo che differisce da questo default è segnato linguisticamente. Ad esempio, la parola "guerriero" è di solito associata agli uomini. Quindi quando le persone descrivono un guerriero che è una donna, di solito specificano "guerriera donna" e segnano il termine con "donna". E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnati, mentre i gruppi marginalizzati sono di solito segnati. Quindi nel nostro metodo, prima designiamo quali sono i gruppi segnati e non segnati, e poi confrontiamo le personalità utilizzando il metodo delle Parole Combattenti, che è essenzialmente l'uso di rapporti log-odds pesati per distinguere le parole principali per ciascun gruppo segnato. Ad esempio, per le personalità delle donne nere, faremmo le Parole Combattenti e confronterei i rapporti log-odds contro sia le personalità bianche che quelle maschili perché sono i due gruppi non segnati corrispondenti. Ora per alcuni risultati. Prima usiamo un lessico di stereotipi e scopriamo che le personalità generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, sebbene le personalità generate abbiano tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nelle personalità generate sono davvero solo le parole "alta" e "atletica". Quindi, davvero solo quelle positive o almeno non negative. E in realtà, questo lessico non cattura davvero molti dei modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi invece per farlo, faremo riferimento ai risultati dal nostro metodo delle Parole Segnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano schemi dannosi. Prima, dalle nostre categorie, le parole principali includono cose come "cultura", "tradizione", "orgogliosa" e "esotica". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti cliché che sono riflessi in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvy", che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata" e "setosa", che si collegano a una lunga storia di donne asiatiche che vengono iper-sessualizzate, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone chiamano l'archetipo della "Donna Nera Forte". E sebbene suoni positivo a prima vista, ci sono stati studi che mostrano che questo tipo di archetipo è in realtà molto dannoso perché pone molta pressione su questi segmenti demografici per essere resilienti e forti contro gli ostacoli sociali. Piuttosto che lavorare per cambiare quegli ostacoli, pone pressione su queste persone per superarli, il che porta a esiti di salute molto negativi per queste persone, tra gli altri danni. Più in generale, troviamo che le parole per ciascun gruppo segnato riflettono semplicemente narrazioni essenzializzanti. Quindi basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, dovremmo, come ricercatori, affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione del bias, perché ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a qualche sorta di allineamento eccessivo e bizzarro ai valori, o forse ad altri metodi anti-stereotipati che stanno portando a questi schemi dannosi. Non possiamo fare alcuna ipotesi o studiare ulteriormente questo senza una maggiore trasparenza. Grazie mille per l'ascolto. Buon tempo all'ACL.</sample>
    <sample id="348">Il paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" esplora le limitazioni delle misure esistenti per i pregiudizi nei modelli di linguaggio di grandi dimensioni (LLMs) e propone un nuovo metodo per identificare stereotipi e pregiudizi. I ricercatori, Myra, Esin Durmus e Dan Jurafsky, evidenziano come le misure attuali si basino su set di dati manualmente costruiti e spesso non riescano a catturare stereotipi specifici o a considerare l'intersezionalità. Per superare queste limitazioni, il metodo proposto utilizza prompt naturali per generare "personaggi" che riflettono identità sociali specifiche, permettendo di osservare stereotipi generalizzabili.

Il metodo si basa su due componenti principali: la generazione di personaggi e l'identificazione di parole "marcate" che distinguono gruppi marginalizzati da quelli dominanti. Utilizzando il metodo "Fightin’ Words", i ricercatori confrontano i log-odds delle parole per identificare stereotipi specifici. I risultati mostrano che i personaggi generati contengono più stereotipi rispetto a quelli scritti da esseri umani, ma spesso si concentrano su parole positive come "tall" e "athletic", che non catturano adeguatamente i pregiudizi dannosi.

L'analisi rivela che termini come "culture", "tradition", "proud", e "exotic" contribuiscono a narrativa essenzializzanti e perpetuano stereotipi dannosi, come il trope della "Strong Black Woman". I ricercatori raccomandano di affrontare anche gli stereotipi positivi, utilizzare un approccio intersezionale per studiare i pregiudizi e aumentare la trasparenza sui metodi di mitigazione dei bias nei modelli.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina. È un piacere presentare un breve video pubblicitario del nostro articolo. Stiamo copiando il tuo modello? Proteggere il copyright dei modelli di linguaggio di grandi dimensioni per l'incorporazione come servizio tramite watermarking nascosto. Innanzitutto, introduciamo il contesto dell'incorporazione come servizio. Attualmente, i modelli di linguaggio di grandi dimensioni come GPT, LLAMA, PALM sono eccezionali nella comprensione e generazione del linguaggio naturale. L'incorporazione come servizio è uno dei servizi costruiti sui modelli di linguaggio di grandi dimensioni per assistere vari compiti NLP. Ad esempio, OpenAI offre un API di incorporazione basata su GPT. Tuttavia, recenti lavori hanno mostrato che un attaccante potrebbe rubare il modello apprendendo dall'incorporazione e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'incorporazione come servizio. Per proteggere il copyright dell'incorporazione come servizio, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contenga il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. Prima di tutto, il metodo dovrebbe essere applicabile all'incorporazione come servizio. In secondo luogo, il watermark non dovrebbe degradare l'utilità delle incorporazioni fornite. Terzo, il watermark dovrebbe essere abbastanza nascosto all'attaccante o l'attaccante dovrebbe poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo è o non applicabile all'incorporazione come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding Marker, un metodo di watermark basato su backdoor applicabile all'incorporazione come servizio. Poi introduco i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: iniezione di watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione di watermark, definiamo prima un'incorporazione target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'incorporazione fornita è una somma pesata dell'incorporazione target e dell'incorporazione originale. Il peso dell'incorporazione target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'incorporazione fornita è esattamente uguale all'incorporazione target. La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il word mark. Prima costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigno non appartengono ai set di trigger. Poi il fornitore richiede le incorporazioni dal servizio dello sfruttatore con il set di dati. La similarità coseno e L2 tra l'incorporazione richiesta e l'incorporazione target vengono calcolate. Calcoliamo la differenza di similarità tra il set di dati benigno e backdoor, definita come delta coseno e delta L2. Allo stesso tempo, applichiamo il test KS e usiamo il suo valore p come terza metrica. Condurre esperimenti su quattro set di dati: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore applichi il set di dati wiki text per contare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro Embedding Marker può avere un'eccellente prestazione di rilevamento mantenendo un'eccellente utilità per i compiti a valle. Validiamo anche la nascita delle incorporazioni fornite visualizzando le incorporazioni delle frasi sui quattro set di dati [INAUDIBILE 4:39] PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra le incorporazioni backdoor e le incorporazioni normali. Ecco tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">Il paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" esplora il significato di superare le prestazioni umane nei benchmark di NLU, evidenziando problemi con le attuali pratiche di valutazione. I benchmark saturati, come SuperGLUE e SQuAD, mostrano spesso che i sistemi superano le prestazioni umane, ma queste affermazioni sono spesso ingannevoli. I sistemi e gli esseri umani vengono valutati su set di dati diversi, con gli umani che spesso lavorano su subset molto più piccoli, portando a confronti ingiusti. Inoltre, ci sono errori nelle risposte di riferimento e nei metodi di valutazione umana, che spesso utilizzano aggregazioni semplici come il voto di maggioranza, piuttosto che confrontare i migliori umani con i migliori sistemi. I tassi di pagamento per i compiti umani variano notevolmente, influenzando la qualità delle prestazioni umane. Le informazioni sugli annotatori, come il numero e il background culturale, sono spesso omesse, rendendo le affermazioni di superamento umano non scientificamente significative. Il paper raccomanda di migliorare le pratiche di benchmarking per costruire confronti più affidabili tra sistemi e umani.</sample>
    <sample id="351">Shuheng presents a paper investigating the generalization of CoNLL-2003 named entity taggers in 2023. The study explores whether these models, developed nearly 20 years ago, can generalize to modern data and identifies factors necessary for good generalization. The research introduces the CoNLL++ Dataset, created from Reuters News articles from 2020, annotated with CoNLL-2003 guidelines. Over 20 models were fine-tuned on CoNLL-2003 and evaluated on both CoNLL-03 and CoNLL++ datasets, with F1 score changes used to assess generalization.

The study identifies three key factors for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models generally perform better on new data, larger models tend to generalize better, and more fine-tuning examples improve performance. The research also examines the causes of performance drops, considering adaptive overfitting and temporal drift. Adaptive overfitting was not observed, as improvements on CoNLL-2003 translated to greater improvements on CoNLL++. However, temporal drift was confirmed as the main cause of performance degradation, with larger temporal gaps between training and test data leading to poorer performance.

The conclusion is that effective generalization requires a combination of better model architecture, larger model size, and more fine-tuning examples. The performance drop is primarily due to temporal drift, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023, but emphasizes the need for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval, or Annotating Behaviors in Chat, è un approccio per valutare le interazioni conversazionali AI annotando esplicitamente se le risposte del modello esprimono comportamenti specifici, come fornire informazioni irrilevanti, contraddirsi, violare il buon senso o mostrare empatia. Questo metodo mira a ridurre la soggettività delle valutazioni umane e fornisce metriche più precise e affidabili per valutare la qualità delle chat.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" di Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, e Iryna Gurevych affronta la sfida dell'input underspecification nel campo della generazione di codice e della sintesi del programma. I metodi attuali spesso falliscono a causa della mancanza di specifiche chiare nei descrittori naturali del linguaggio (NLD), un problema comune nei casi d'uso reali. Il paper propone un approccio interattivo, in cui vengono poste domande di chiarimento per raccogliere ulteriori specifiche e migliorare la generazione di codice. Il focus è sulle specifiche a livello di operazione, e viene introdotto il dataset CodeClarQA, che include chiarimenti su operazioni chiave. Il metodo utilizza schemi per rappresentare elementi importanti dei documenti e calcola punteggi di similarità per identificare le operazioni mancanti. Il paper descrive un pipeline di generazione di codice guidata da domande di chiarimento, composta da un Predittore di Necessità di Chiarimento, un Selettore di Domande e un Generatore di Codice. I risultati mostrano che il modello MPNet è efficace nell'identificare operazioni mancanti, e l'analisi degli errori suggerisce aree di miglioramento. L'analisi finale dimostra che le operazioni chiave chiarite migliorano la qualità del codice generato, anche se il compito rimane impegnativo. Il paper invita al feedback e alla collaborazione per ulteriori miglioramenti.</sample>
    <sample id="354">Il contenuto non fornisce dati specifici o un grafico che indichi fino a quale anno la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali. Pertanto, non è possibile rispondere a questa domanda basandosi sul contenuto fornito.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato come lungo articolo per ACL 2023, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Iniziamo definendo la dissonanza cognitiva e spiegando perché è importante studiarla nel linguaggio. In breve, la dissonanza cognitiva è la presenza di due credenze o azioni inconsistenti, come in questo esempio: una persona dice "So che le sigarette potrebbero uccidermi" e poi aggiunge "Ho preso un paio di sigarette dopo la riunione". Queste credenze e azioni sono inconsistenti e in dissonanza. Aggiungere "Non credo di poter mantenere il mio lavoro senza di esse" giustifica la seconda azione, e c'è una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella presa di decisioni quotidiana, è rara trovare espressa nel linguaggio tra altre relazioni discorsive. Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori delle credenze e i cambiamenti di atteggiamento nelle popolazioni. Una alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali. Per creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio dissonanza-prima, come mostrato nel flusso di lavoro qui. I tweet sono stati passati attraverso il parser PDTB, e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1.000 esempi di coppie di unità discorsive, abbiamo eseguito la formazione per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprendentemente, il classificatore ha prestato non molto meglio della casualità. Dato l'occorrenza bassa di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema di assoluta rarità. Per alleviare questo, abbiamo sperimentato su combinazioni di transfer learning e active learning per annotare in modo che più esempi dissonanti possano essere raccolti in meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, iniziamo il processo di active learning trasferendo pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione dello stance di dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria delle classi di espansione e confronto di PDTB poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo CE qui. Troviamo che, trasferendo, la performance zero-shot sul set di dati annotato è già molto migliore della casualità, con la migliore AUC di 0,62. Inoltre, affinando iterativamente su entrambi i compiti, troviamo che l'affinamento del compito CE seguito da un ulteriore affinamento sul dibattito fornisce una performance zero-shot molto migliore. Pertanto, questo è il modello che usiamo per avviare l'active learning. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di active learning e annotazioni. "Cumulativo" accumula tutti i dati raccolti dall'active annotation finora, mentre "Iterativo" aggiorna il modello addestrando sui dati più recenti raccolti. Tra le diverse strategie, troviamo che Cumulativo ha prestato uguale o meglio di Iterativo in generale. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di Probabilità di Classe Rara — PRC — per selezionare principalmente gli esempi che sono altamente probabili di essere classificati come dissonanti dal modello attuale in ogni round di annotazione. Confrontiamo questo con altre strategie di AL state-of-the-art comunemente utilizzate nella comunità. Troviamo che la strategia PRC proposta funziona meglio delle altre strategie state-of-the-art, anche se la differenza è piccola. Nota che la performance è significativamente più bassa per casuale. In ulteriori round di AL con le due migliori strategie, miglioriamo la classificazione della dissonanza AUC a 0,75, che è la migliore performance che abbiamo finora su questo compito. Verifichiamo anche la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Troviamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, troviamo che PRC è una semplice strategia di AL per l'acquisizione della classe rara e l'avvio dell'active learning con compiti di transfer learning adeguatamente progettati e aiuta significativamente. Troviamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active nel dominio traggono beneficio dall'aggiornamento cumulativo. Questi sono i link al nostro core data set e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Gli autori dell'articolo sono affiliati con Alexander Koller e Ivan Titov, che sono i loro advisor.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Ci sono cinque autori coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea pre-traduzione.</sample>
    <sample id="361">Armineh Nourbakhsh, PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, presents "CounterComp," a method to enhance compositional generalization in multi-step quantitative reasoning for question answering tasks. The focus is on improving neural models' performance on tasks involving financial tables, where questions require multiple arithmetic operations. Current models struggle with these tasks due to memorizing spurious patterns, such as associating specific tokens with operations. CounterComp addresses this by mining counterfactual scenarios from training data, creating positive and negative examples based on whether changes in questions affect outputs. These examples are used to add an auxiliary metric learning loss with a dynamic margin to the training process. This approach consistently improves model performance, particularly for tasks with more than two reasoning steps, and enhances generalization to out-of-distribution samples. The method also helps models focus on meaningful tokens related to operations in the output. Nourbakhsh acknowledges her co-authors, advisors, and audience for their support.</sample>
  </task>
</testset>