<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, die politische Nachrichtenmedien wie The New York Times, Los Angeles Times, The Guardian und Huffington Post umfassen.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Tu Yi von Ant Group präsentiert ein Papier über das Verständnis visuell reicher Dokumente (VrDU), das sich auf die Herausforderungen bei der Verarbeitung von Formularen, Quittungen und Plakaten konzentriert. Das Papier stellt das LayoutMask-Modell vor, das auf Text- und Layoutinformationen basiert, um die Interaktionen zwischen Text und Layout während des Pre-Trainings zu verbessern. Im Gegensatz zu bestehenden Modellen, die eine globale 1D-Positionierung verwenden, nutzt LayoutMask eine lokale 1D-Positionierung, um die globale Lesereihenfolge durch die Kombination von 1D- und 2D-Positionen sowie semantischen Informationen zu inferieren. Dies fördert tiefere Text-Layout-Interaktionen.

Zwei innovative Maskierungsstrategien werden eingeführt: Whole Word Masking, das Maskierungen auf Wortebene anstelle von Token-Ebene durchführt, und Layout-Aware Masking, das die ersten und letzten Wörter jedes Segments häufiger maskiert, um die Aufmerksamkeit auf den Kontext in benachbarten Segmenten zu lenken. Ein neues Pre-Training-Ziel, Masked Position Modeling (MPM), zielt darauf ab, zufällig maskierte 2D-Positionen wiederherzustellen, ähnlich einem Lückentext, um sowohl semantische als auch räumliche Inferenzen zu fördern.

Experimente zeigen, dass die lokale 1D-Positionierung (Local-1D) im Vergleich zur globalen 1D-Positionierung (Global-1D) auf den FUNSD- und SROIE-Datensätzen überlegen ist, obwohl sie auf dem CORD-Datensatz leicht zurückfällt. Der Unterschied wird hauptsächlich durch die Entität "Total" verursacht, die aufgrund ihrer komplexen Layouts schwer zu erkennen ist. Insgesamt zeigt LayoutMask eine verbesserte Leistung bei der Verarbeitung visuell reicher Dokumente durch die Integration von Text- und Layoutinformationen.</sample>
    <sample id="3">Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Identifizierung deutscher Texte auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess zur Anpassung eines Textes, um die Textverständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. Hier sehen Sie ein paralleles ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie lexikalische Substitution, Klausel-Löschung, Umordnung oder Einfügung von Wörtern. Wir schlagen nun unseren neuen Korpus DEPLAIN vor, da es in den letzten Jahren Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora hier zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Subkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet, was zu etwa 13.000 parallelen Satzpaaren führt. DEPLAIN-web umfasst verschiedene Domänen, und wir haben auch alle 750 Dokumente ausgerichtet, sowohl manuell als auch mit automatischen Ausrichtungsmethoden. Insgesamt ergeben sich 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel hinsichtlich des Typs der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Texte für Sprachlerner. Auf allen Ebenen, wie zum Beispiel lexikalische Vereinfachung, strukturelle Vereinfachung und auch das allgemeine Vereinfichungslevel. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. Zum Beispiel haben wir im DEPLAIN-apa-Korpus viel mehr Umordnungen und Wortzusätze als im DEPLAIN-web-Korpus. Auf der anderen Seite haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar, und ich werde jetzt über die Verwendungsfälle für unseren Datensatz DEPLAIN sprechen. Als ersten Verwendungszweck können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren wollen. In unserem Verwendungszweck versuchen wir jedoch, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten mit derselben Sprache, aber unterschiedlichem Komplexitätslevel, zu extrahieren. Da wir nun unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen, und wir haben alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, in der Publikation veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode MASSalign ist. Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, in der Publikation. Der zweite Verwendungszweck, den wir in unserer Publikation gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um aus komplexem Eingabetext vereinfachten Text zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um Dokumentenebene-Vereinfachungen zu erzeugen, und wir haben auch das normale Basis mBART feinabgestimmt, um Satzebene-Vereinfachungen zu erzeugen. Sie können auch alle Checkpoints finden und sich detaillierter mit den Ergebnissen und den Bewertungsmetriken unserer Experimente in der Publikation befassen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Baseline-Ergebnisse erzielen konnte, und wir haben diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das T5 XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen, wenn es Zugriff auf teilweise überlappende Hintergrundinformationen hatte.</sample>
    <sample id="6">In der Präsentation "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" stellt Jiaan die Arbeit vor, die sich mit der Entwicklung eines einheitlichen Modells für die Zusammenfassung von Dokumenten in verschiedenen Sprachen befasst. Die Forschung vereint die bisherigen Ansätze der mehrsprachigen und der übersetzungsübergreifenden Zusammenfassung in einem umfassenderen Konzept, das als "many-to-many summarization" bezeichnet wird. Dieses Modell soll in der Lage sein, Dokumente in jeder Quellsprache zu verarbeiten und deren Zusammenfassung in jeder Zielsprache zu generieren.

Die Forschungsarbeit zeigt, dass das many-to-many summarization Modell besser in der Lage ist, Aufgabenwissen zwischen verschiedenen Sprachen zu übertragen als die bisherigen Ansätze. Dazu wurde das PISCES-Modell entwickelt, das durch ein dreistufiges Vortraining Sprachmodellierung, Übersetzungs- und Zusammenfassungsfähigkeiten erlernt. Die Vorstellung umfasst auch einen Vergleich zwischen den bisherigen Ansätzen und dem neuen many-to-many Modell, basierend auf Experimenten mit dem WikiLingua-Dataset in Sprachen wie Englisch, Französisch, Hindi, Chinesisch, Thai und Türkisch.

Die Ergebnisse der Experimente zeigen, dass das many-to-many Modell die anderen Ansätze übertrifft. PISCES, das erste Modell dieser Art, zeigt in den Tests eine bessere Leistung als die Baseline-Modelle mBART-50 und mT5. Die Forschung beinhaltet auch Ablationsstudien und menschliche Studien, um die Wirksamkeit der verschiedenen Trainingsstufen zu bestätigen. Die Präsentation schließt mit einem Aufruf, die detaillierten Ergebnisse und Methoden in der veröffentlichten Arbeit nachzulesen.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch im Jahr 2023, obwohl sie von temporalem Drift betroffen sind. Sie zeigen immer noch eine gute Generalisierung, insbesondere wenn sie mit besserer Architektur, größerer Modellgröße und mehr Feinabstimmungsbeispielen verwendet werden.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem sie explizit annotiert, ob ein Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z.B. das Äußern irrelevanter Informationen oder das Widersprechen sich selbst. Diese Methode zielt darauf ab, die Zuverlässigkeit und Präzision der Bewertung von Dialogqualität zu verbessern, indem sie spezifische Verhaltensfehler der Chat-Modelle misst.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit von sauberen Validierungsdaten ab. Ohne saubere Validierungsdaten gibt es einen großen Leistungsabfall, und die Modelle können nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern. Die Leistung verbessert sich mit zunehmender Anzahl sauberer Validierungsproben, und direkte Feinabstimmung auf sauberen Daten kann bessere Ergebnisse erzielen als die Verwendung sauberer Daten nur zur Validierung.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Zugänglichkeit und Qualität der Hintergrundinformationen für die Sprachmodelle erhöht wird. Dies könnte durch die Integration von umfassenderen und genaueren Wissensquellen geschehen, die den Annotatoren zur Verfügung stehen. Außerdem könnten fortgeschrittene Techniken zur Verarbeitung und Integration von multimodalen Daten (z.B. Text, Audio, Bilder) entwickelt werden, um die Verständnisfähigkeit der Modelle zu verbessern. Schließlich könnte die Weiterentwicklung der Modelle selbst, um besser mit indirekten Referenzen umzugehen, die Genauigkeit weiter steigern.</sample>
    <sample id="11">Jack Hessel, ein Forscher bei AI2, präsentierte eine Studie mit dem Titel "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest". Die Studie, in Zusammenarbeit mit Institutionen wie der University of Utah, Cornell University und OpenAI, untersucht, ob große Sprachmodelle wie ChatGPT und Google's PaLM tatsächlich Humor verstehen können. Obwohl diese Modelle in der Lage sind, Witze zu generieren und zu erklären, zeigt die Forschung, dass ihr Verständnis von Humor begrenzt ist. Beispielsweise erzeugt ChatGPT bei der Generierung von Knock-Knock-Witzen oft unlogische Puns, was Fragen über das tatsächliche Verständnis von Humor aufwirft.

Die Studie nutzt Daten aus The New Yorker Caption Contest, einem populären Wettbewerb, bei dem Leser Witze zu Cartoon-Bildern einreichen. Drei Aufgaben wurden entwickelt: Matching, bei dem Modelle die richtige Witzeinreichung aus fünf Optionen auswählen müssen; Quality Ranking, bei dem zwei Witze bewertet werden; und Explanation Generation, bei der Modelle erklären sollen, warum ein Witz lustig ist. Die Ergebnisse zeigen, dass selbst mit spezieller Schulung, wie z.B. CLIP, das auf die Aufgaben abgestimmt wurde, Modelle nur eine 62%ige Genauigkeit im Matching erreichen, während Menschen etwa 94% erreichen. GPT-4, das mit menschlichen Beschreibungen der Bilder arbeitet, zeigt ebenfalls eine Lücke im Vergleich zu menschlichen Leistungen.

Insgesamt zeigt die Studie, dass, obwohl Sprachmodelle Fortschritte im Bereich der Humorgenerierung gemacht haben, ihr Verständnis von Humor noch weit von menschlichen Fähigkeiten entfernt ist. Die Forschungsergebnisse und das Datenset sind öffentlich zugänglich, um weitere Untersuchungen zu fördern.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presents his work on "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted at the Hebrew University. The study focuses on adaptive inference methods, which aim to reduce the inference time of large language models by using low-capacity models for simpler samples. The two primary methods discussed are Multi Model and Early Exit. Multi Model involves using multiple models with classifiers, which are run sequentially during inference, while Early Exit uses classifiers at intermediate layers of a single model to halt computation early.

The study highlights the pros and cons of each method. Multi Model is versatile and extendable but incurs storage costs and overhead, as all models are run before halting. Early Exit offers faster inference and is memory efficient, but shared model parameters among classifiers can lead to conflicting gradients, degrading performance.

To test this, Rotem compared Early Exit classifiers with Multi Model classifiers, finding that the latter outperformed the former by an average of 2.3%. The gap was largest for the earliest classifiers. The study also examined the speed/accuracy trade-off, showing Multi Model's superiority at high speeds but Early Exit's advantage when using later classifiers due to Multi Model's overhead.

To address conflicting gradients, Rotem introduces SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method where each layer is updated only by its following classifier, avoiding conflicting gradients. Results show SWEET closes the performance gap between Early Exit and Multi Model, though later classifiers can be negatively affected. SWEET outperforms both methods at fast speeds and throughout the speed/accuracy curve for BERT-Large.

The study concludes by demonstrating conflicting gradients in Early Exit, providing a fair comparison of adaptive inference methods, and introducing SWEET, which encourages further research in fine-tuning algorithms for Early Exit architectures.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieses Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von unterschiedlichen Theorien und Korpusansätzen angenommen werden. Zum Beispiel nimmt die Universal Dependencies an, dass in der Koordination „Lisa, Bart und Maggie“ der erste Konjunkt die gesamte koordinierte Struktur anführt, also „Lisa“. Ein ähnlicher Ansatz wird in Igor Mel'čuks Meaning-Text-Theorie angenommen, wo ebenfalls der erste Konjunkt die gesamte koordinierte Struktur anführt. Diese beiden Ansätze sind asymmetrisch, da sie einen der Konjunkte hervorheben. Im Gegensatz dazu gibt es den Prager Ansatz, bei dem koordinierte Strukturen von der Konjunktion angeführt werden, was zu Abhängigkeiten von der Konjunktion zu allen Konjunkten führt. Schließlich gibt es auch einen mehrfach-geführten Ansatz, wie in Hudsons Word Grammar, bei dem alle Konjunkte als Köpfe der koordinierten Struktur angesehen werden, was zu Abhängigkeiten von dem Regens zu jedem Konjunkt separat führt. Das Ziel dieses Papiers ist es, ein neues Argument für symmetrische Strukturen der Koordination zu liefern und gegen asymmetrische Strukturen der Koordination zu argumentieren. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erläutern werde. In Englisch bevorzugen direkte Objekte, nahe am Verb zu stehen, während Adjunkte weiter entfernt sein können. „Marge las es gestern“ ist in Ordnung, weil das direkte Objekt nahe am Verb steht, während „Marge las gestern es“ viel schlechter klingt, da zwischen Verb und direktem Objekt ein Adjunkt „gestern“ steht. Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr lang und umfangreich ist, sodass es nach dem Adjunkt verschoben werden kann. Dies wird hier illustriert. Beide Sätze sind in Ordnung: „Marge las dieses absolut faszinierende Buch über Bienen gestern.“ Es ist in Ordnung, anstelle von „es“ dieses lange NP zu haben. Es ist auch in Ordnung zu sagen: „Marge las gestern dieses absolut faszinierende Buch über Bienen.“ Die Überlegung hier ist, dass dies möglich ist, weil dieser Satz zwar das allgemeine grammatische Prinzip verletzt, dass direkte Objekte direkt neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die zwischen diesen beiden Strukturen nicht konstant sind. Hier haben wir eine Abhängigkeit von „las“ zum Adjunkt mit einer Länge von 7 Wörtern und von „las“ zu „Buch“ mit einer Länge von 4, also zusammen 11. Wenn man diese beiden Konstituenten vertauscht, beträgt die Summe dieser beiden Abhängigkeiten 6. Anstatt 11 ist 6 viel kürzer. Deshalb klingt das ziemlich in Ordnung. Es verletzt ein Prinzip, erfüllt aber ein anderes. Wir haben verschiedene Statistiken über Koordination aus der erweiterten Version des Penn Treebanks extrahiert und in dem Papier „Why wouldn't you use universal dependencies“ bestätigen diese Statistiken die Beobachtung, die viele Male gemacht wurde, dass linke Konjunkte tendenziell kürzer sind, wie „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Auch die Beobachtung, die in der Parsing gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Je größer die Längendifferenz zwischen den beiden Konjunkten ist, desto stärker bevorzugt der kürzere Konjunkt, der erste zu sein. Die Proportion ist größer für den linken kurzen Konjunkt. Was neu in diesem Papier ist, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regens links ist oder fehlt. Der Regens ist links in diesem Beispiel „Ich sah Bart und Lisa“ und fehlt im zweiten Beispiel „Homer kam und schniefte.“ Hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regens. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wenn der Regens jedoch rechts ist, wie hier, „lachte“ regiert die Koordination „Ted und Ned“, verschwindet dieser Effekt. Wir haben gezeigt, dass dies durch Messung der Länge in Zeichen, in Silben in der Mitte und in Wörtern in der rechten Spalte. Ich werde mich auf die rechte konzentrieren. Was wir sehen, ist, dass wenn der Regens links ist, die Tendenz für den linken Konjunkt, kürzer zu sein, mit der absoluten Differenz in Wörtern wächst, und das gleiche wird bei fehlendem Regens bei der Koordination von Sätzen beobachtet. Wenn der Regens jedoch rechts ist, verschwindet diese Tendenz. Und wir zeigen im Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination liefert und für symmetrische Strukturen. Siehe das Papier für die vollständigen Argumente und sprechen Sie uns auf der Poster-Sitzung an. Vielen Dank.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Bibeltexte werden stärker vereinfacht als Nachrichtentexte oder Texte für Sprachlerner.</sample>
    <sample id="17">Dieses Papier präsentiert eine innovative Methode zur multimodalen Beziehungsextraktion, die sowohl textuelle als auch visuelle Informationen nutzt, um semantische Beziehungen zwischen Entitäten zu bestimmen. In Szenarien wie sozialen Medien, wo Daten in verschiedenen Formen vorliegen, kann reiner Text oft unzureichend sein, um Ambiguitäten zu klären. Die vorgeschlagene Methode adressiert zwei Hauptprobleme: die Übernutzung interner Informationen und die Unterbewertung externer Informationen. Durch die Anwendung des Graph Information Bottleneck Prinzips wird eine feingranulare Informationsauswahl über zwei Modalitäten erreicht, um redundante Informationen zu reduzieren. Zusätzlich wird externe Information durch multimodale Themeninformationen ergänzt, um den Kontext zu bereichern. Die Methode umfasst die Darstellung von Text und Bild als visuelle und textuelle Szenengraphen, die zu einem einheitlichen multimodalen Graphen (CMG) verschmolzen werden. Dieser wird durch feingranulare Filterung und Anpassung optimiert. Die CMG-Features werden durch multimodale Themenfeatures angereichert, die durch eine Aufmerksamkeitsoperation integriert werden. Experimente auf einem weit verbreiteten MRE-Datensatz zeigen, dass die Methode die bestehenden Modelle übertrifft. Die Analyse zeigt, dass die Informationsauswahl bei hohen Text-Bild-Relevanzwerten und die Informationsergänzung bei niedrigen Relevanzwerten besonders vorteilhaft sind. Insgesamt bietet die Methode eine signifikante Verbesserung bei der multimodalen Beziehungsextraktion.</sample>
    <sample id="18">Das Beispiel für die Präferenz für kürzere linke Konjunktionen ist "salt and pepper" im Vergleich zu "pepper and salt".</sample>
    <sample id="19">Zhang Qin, ein Masterstudent der Shenzhen University, präsentierte die Arbeit "A Survey for Efficient Open Domain Question Answering", die bei ACL 2023 angenommen wurde. Die Arbeit konzentriert sich auf die Herausforderungen und Lösungen für effiziente offene Frage-Antwort-Systeme. Ein zentrales Thema ist das zweistufige Modell von Danqi Chen (2017), das eine Retrieval- und eine Lesestufe umfasst. Die Retrievalstufe verwendet zwei Encoder, um relevante Kontexte aus einer großen Wikipedia-Datenbank zu extrahieren, während die Lesestufe die Frage versteht und die Antwort ableitet. Herausforderungen sind die große Größe der Wikipedia-Datenbank (26 Millionen Dokumente, 20 GB Speicher) und die große Indexdatei (65 GB), die die Inferenzgeschwindigkeit beeinträchtigt. Zudem sind die verwendeten Sprachmodelle mit Millionen von Parametern groß und ressourcenintensiv.

Die Motivation der Arbeit ist es, effizientere Systeme zu entwickeln, die weniger Speicher benötigen, schneller arbeiten und dennoch vergleichbare Leistungen erbringen. Dazu werden verschiedene Techniken vorgestellt, darunter einstufige Systeme wie Retrieval-only und Generator-only. Effiziente Methoden umfassen die Approximative nächstgelegene Nachbarsuche für schnelle Beweissuche, adaptive Berechnung für schnelles Lesen und Techniken zur Reduzierung der Indexgröße wie Dokumentfilterung und Produktquantisierung. Zur Reduzierung der Modellgröße werden leichte Modelle, Parameterfreigabe und einstufige Modelle vorgeschlagen.

Die Analyse zeigt, dass Retrieval- und Lesesysteme einen guten Kompromiss zwischen Geschwindigkeit, Speicher und Leistung bieten, während Retrieval-only-Systeme schnelle Antworten, aber große Indizes haben, und Generator-only-Systeme keine Indizes, aber große Modelle und niedrige Leistung. Schlussfolgerungen umfassen die Reduzierung der Indexgröße durch Generator-only-Systeme oder Einbettungskomprimierung und die Reduzierung der Modellgröße durch Wissensverdichtung oder einstufige Modelle. Für Echtzeit-Feedback sind Retrieval-only-Systeme geeignet, während Retrieval- und Lesesysteme für Kompromisse besser geeignet sind. Zukünftige Arbeiten könnten sich auf die Bereitstellung in Geräten mit geringer Leistung und die Berücksichtigung weiterer Evaluationsmetriken konzentrieren.</sample>
    <sample id="20">Yes, you can use the models for your research. All the pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license, and the training scripts are available on the GitHub repository.</sample>
    <sample id="21">DEPLAIN-apa enthält hauptsächlich nachrichtenbasierte Texte.</sample>
    <sample id="22">Die Faktoren, die zu einer guten Generalisierung führen, sind: eine bessere Modellarchitektur (insbesondere Transformer-Modelle), eine größere Modellgröße und mehr Feinabstimmungsbeispiele.</sample>
    <sample id="23">Dan Garrette präsentiert Forschung zur Verbesserung der Fähigkeit von Text-Bild-Modellen, visuellen Text darzustellen. Obwohl Text-Bild-Modelle wie Imagen in der Erzeugung hochwertiger Bilder Fortschritte gemacht haben, sind sie oft schlecht darin, Text darzustellen. Imagen verwendet einen T5-XXL-Textencoder, der mit einem Diffusionsmodell arbeitet, um Bilder zu generieren. Komplexe Texte erzeugen oft genaue Bilder, aber einfache Texte, die Wörter enthalten, scheitern häufig.

Die Untersuchung des T5-Textencoders zeigt, dass er mit SentencePiece-Tokenisierung arbeitet, was bedeutet, dass er Subwort-IDs anstelle von einzelnen Buchstaben erhält. Dies erschwert die Darstellung von Wörtern, da das Modell die Subwörter in einzelne Buchstaben zerlegen muss. Experimente zeigen, dass T5-Modelle, selbst die größeren, bei der Schreibweise von Wörtern schlecht abschneiden, wobei die größeren Modelle unter 70% Genauigkeit erreichen.

Im Vergleich dazu schneiden PaLM-Modelle, die aufgrund ihrer Größe und des Datenvolumens besser abschneiden, besser ab, sind aber für viele Anwendungen unpraktisch. ByT5, das einzelne Bytes des Eingabetexts verwendet, zeigt eine hohe Genauigkeit bei der Schreibweise, da es direkten Zugriff auf die Schreibinformationen hat.

Um die Textdarstellung zu verbessern, wurde das Imagen-Modell mit einer zusätzlichen Textrepräsentation aus dem ByT5-small-Modell erweitert. Diese Ergänzung erhöhte die Parameterzahl des Textencoders nur um etwa 5% und verbesserte die Fähigkeit des Modells, Text darzustellen, obwohl Fehler in der Diffusionsmodell-Generierung weiterhin auftreten können. Die Hauptergebnisse umfassen die Einführung der WikiSpell- und DrawText-Benchmarks sowie eine neue Strategie zur Verbesserung der Schreibfähigkeit von Modellen durch die Verwendung von charakterbewussten Modellen.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem die Länge der Konjunktionen in Zeichen, Silben und Wörtern verglichen wurde. Die Analyse konzentrierte sich insbesondere auf die Länge in Wörtern. Es wurde festgestellt, dass die Tendenz für die linke Konjunktion kürzer zu sein, mit der absoluten Differenz in Wortanzahl wächst, wenn der Regent links steht oder fehlt. Diese Tendenz verschwindet jedoch, wenn der Regent rechts steht.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem Daten aus der erweiterten Version des Penn Treebank extrahiert wurden, um Statistiken über Koordination zu analysieren. Es wurde beobachtet, dass die Tendenz für den linken Konjunkt kürzer zu sein, wenn der Begrenzer links oder abwesend ist, stärker wird, je größer der Längenunterschied zwischen den Konjunkten ist. Diese Tendenz verschwindet jedoch, wenn der Begrenzer rechts steht. Die Längenunterschiede wurden in Zeichen, Silben und Wörtern gemessen, wobei der Fokus auf der Wortanzahl lag.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, insbesondere bei einem Problem der absoluten Seltenheit wie der Dissonanzerkennung, performt nicht viel besser als zufällig. In diesem Fall erreichte der Klassifikator, der nur mit 43 Beispielen von Dissonanz trainiert wurde, eine AUC von.62 nach dem Transferlernen, was deutlich besser als zufällig ist, aber immer noch nicht zufriedenstellend.</sample>
    <sample id="27">Der Inhalt gibt keine spezifische Anzahl von Autoren an, die an der Arbeit beteiligt sind.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostische Modelle bei den Diskursphänomenen Formulierungsstil (Formalität) und lexikalische Kohäsion.</sample>
    <sample id="30">Das Papier "LLM-Blender" stellt ein einfaches, aber effektives Ensemble-Lern-Framework für große Sprachmodelle vor, das auf Paarvergleich und generativer Fusion basiert. Entwickelt von einem Team von AI2 und USC, hebt es hervor, dass die Leistung von Sprachmodellen je nach Eingabebeispiel variieren kann. Obwohl einige Modelle wie Vicuna im Durchschnitt gut abschneiden, ist es nicht immer das beste Modell für spezifische Eingaben. LLM-Blender schlägt vor, mehrere Modelle für jede Eingabe zu verwenden, um die beste Ausgabe zu erzielen.

Das Framework besteht aus zwei Stufen: Zuerst werden n Modelle ausgeführt, um ihre Ausgaben zu erhalten. Ein Paarvergleichsmodul namens PairRanker vergleicht diese Ausgaben paarweise, um eine Rangfolge zu erstellen. Es verwendet eine Cross-Attention-Modul wie RoBERTa, um die Unterschiede zwischen den Kandidaten zu analysieren. Die Ergebnisse werden in einer Matrix aggregiert, wobei die Verwendung der maximalen Logits zur Bestimmung der Rangfolge als beste Methode identifiziert wurde.

In der zweiten Stufe wählt das Framework die Top-K-Kandidaten aus und verwendet sie als Eingabe für ein sequenz-zu-sequenz-Modell, um die endgültige Ausgabe zu generieren. Die Ergebnisse zeigen, dass LLM-Blender die Leistung bestehender Modelle wie Open Assistant und Vicuna übertrifft, indem es in 68% bzw. 76% der Fälle bessere Ergebnisse liefert.

Zur Bewertung des Frameworks wurde die neue Datensammlung MixInstruct erstellt, die Kandidaten von 11 offenen Sprachmodellen umfasst. Die Ergebnisse unterstreichen die Wirksamkeit von LLM-Blender als einfaches und effektives Ensemble-Lern-Framework, das die Leistung von Sprachmodellen erheblich verbessert.</sample>
    <sample id="31">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="33">Das Framework NLPositionality quantifiziert die Positionalität, indem es Datensätze mit einer vielfältigen Gruppe von Annotatoren neu annotiert, um demografische Daten zu sammeln. Anschließend vergleicht es die Annotierungen nach demografischen Merkmalen mit den Vorhersagen und Labels von Modellen und Datensätzen mithilfe eines Pearson's R Korrelationskoeffizienten. Dies ermöglicht den Vergleich von Endnutzern mit Modellen und Datensätzen, anstatt sich nur auf Annotatorübereinstimmung oder Verteilungen zu konzentrieren.</sample>
    <sample id="34">Marcos Treviso präsentiert "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", eine Zusammenarbeit mit Alexis Ross, Nuno Guerreiro und André Martins. CREST kombiniert selektive Rationalisierung und Gegenfaktengenerierung, um die Stärken beider Methoden zu nutzen. Die Rationalisierung hebt bedeutungsvolle Tokens hervor, während die Gegenfaktengenerierung durch das Bearbeiten spezifischer Eingabeteile menschliches kausales Denken nachahmt. CREST generiert Gegenfakten, indem es rationale Teile maskiert und den Gold-Label voranstellt, um neue Tokens mit einem Sprachmodell einzufügen. Die Qualität der Gegenfakten wird durch menschliche Bewertungen auf Validität und Natürlichkeit überprüft, wobei CREST besser abschneidet als MiCE, aber hinter manuell erstellten Gegenfakten zurückbleibt.

CREST wird auch für die Datenverstärkung und eine alternative Rationalisierungsmethode eingesetzt, die sowohl faktische als auch gegenfaktische Beispiele verwendet. Diese Methode führt zu einer verbesserten Leistung auf IMDB und anderen Datensätzen. CREST-Rationalisierung produziert plausible und simulierbare Rationales, die die Entscheidungen des Klassifikators effektiv beeinflussen können. Die Ergebnisse zeigen, dass CREST-Gegenfakten die Leistung von Downstream-Modellen verbessern und zu plausiblen Erklärungen führen, die sich auf die kontrastierenden Teile der Eingabe konzentrieren. Weitere Details finden sich in der zugehörigen Publikation und dem Code.</sample>
    <sample id="36">In "Learning Language-Specific Layers for Multilingual Machine Translation," Telmo Pessoa Pires and colleagues address the challenges of multilingual machine translation, such as limited capacity per language and increased training difficulty with larger models. Their solution, Language-Specific Layers (LSLs), aims to enhance capacity for individual languages without raising inference costs. LSLs involve having a dedicated transformer layer for each language, activated during inference based on the source or target language, thus maintaining constant inference costs.

The research focuses on optimizing LSL placement within the model, particularly in the encoder, as initial experiments showed limited benefits in the decoder. To determine optimal placement, the team trained a large model with shared, source, and target weights for each encoder layer. By analyzing these weights, they identified patterns indicating where LSLs should be placed. The final architecture typically includes shared layers at the bottom and top, with source-specific and target-specific LSLs in between.

Experiments were conducted on WMT21 news translation data for 10 languages, including European, Asian, and Swahili. The model's performance was evaluated using metrics like chrF, spBLEU, and COMET. Results showed that the learned architecture with LSLs significantly outperformed both language adapters and the largest baseline model, while also being faster at inference. Improvements were particularly notable for low-resource languages, with statistically significant gains in 84 out of 90 translation directions. The study highlights the effectiveness of LSLs in enhancing multilingual machine translation without compromising efficiency.</sample>
    <sample id="37">Die vorherige Studie, bei der menschlichen Teilnehmenden die gleichen Persona-Prompts gegeben wurden, fand heraus, dass die Prompts auch bei Menschen rassische Stereotype hervorrufen konnten. Dies ermöglichte einen direkten Vergleich zwischen den von den Modellen generierten Personas und den von Menschen verfassten Antworten.</sample>
    <sample id="38">Die Studie verwendete Daten aus der erweiterten Version des Penn Treebank.</sample>
    <sample id="39">Der Inhalt beschreibt eine Präsentation oder ein Papier von Adam Przepiórkowski. Es wird kein Hinweis auf weitere Autoren gegeben, daher ist nur Adam Przepiórkowski als Autor beteiligt.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die "Debatte" (ein Thema-unabhängiger Dissonanz-Standpunkt-Klassifikationsaufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder im Widerspruch stehen) und die "CE" (binäre Klassifizierung der Erweiterungs- und Vergleichsklassen von PDTB, die eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind).</sample>
    <sample id="41">Silin vom Natural Language Processing Lab an der EPFL University präsentiert "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives", eine Zusammenarbeit mit Sony Group Corporation. PeaCoK ist ein Persona-grounded Commonsense Knowledge Graph, der 3.800 Persönlichkeiten und 40.000 einzigartige Attribute umfasst, die etwa 100.000 persönliche Schlussfolgerungen oder Fakten bilden. Diese Persönlichkeiten sind durch etwa 9.200 Attribute miteinander verbunden, was reiche Interaktionen ermöglicht. PeaCoK wurde in drei Schritten entwickelt: Auswahl von Persönlichkeiten aus bestehenden Wissensgraphen, Induktion von Attributen aus Wissensgraphen und Sprachmodellen sowie Crowdsourcing von Relationen mit einem Joint Human-AI Voting-Schema, das eine hohe Qualität der Annotationen sicherstellt.

PeaCoK wird verwendet, um ein BART-basiertes Wissensgenerierungsmodell zu trainieren, das Persönlichkeitsattribute vorhersagt. Im Vergleich zu großen Sprachmodellen wie GPT-3 und GPT-3.5 zeigt Comet-BART, das auf PeaCoK trainiert wurde, bessere Ergebnisse in automatischen und menschlichen Bewertungen. PeaCoK verbessert auch die narrative Modellierung, insbesondere in der persona-gestützten Dialoggenerierung, indem es relevante Fakten aus dem Wissensgraphen hinzufügt, was zu flüssigeren, konsistenteren und engagierteren Dialogen führt. Die Evaluation zeigt, dass PeaCoK's persona-zentriertes Wissen einen positiveren Einfluss hat als allgemeines soziales Wissen. Die Studie hebt die Bedeutung von PeaCoK's vernetztem Wissen für konsistente und ansprechende Erzählungen hervor.</sample>
    <sample id="42">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="43">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten durch seinen Ansatz, die Annotationsdaten von realen Nutzern mit bestehenden Datensätzen und Modellen zu vergleichen. Während frühere Arbeiten sich auf Annotator-Abweichungen konzentrierten, indem sie Annotator-Übereinstimmungen oder -Verteilungen modellierten, vergleicht NLPositionality die Vorhersagen und Labels von Modellen und Datensätzen mit den Einschätzungen von Endnutzern. Dies ermöglicht eine direkte Analyse der Positionalitäten, die in Modellen und Datensätzen repräsentiert sind, im Vergleich zu den Perspektiven der Nutzer. Darüber hinaus nutzt das Framework Lab in the Wild, um eine vielfältige Gruppe von Annotatoren zu rekrutieren, was eine breitere demografische Abdeckung ermöglicht als traditionelle Plattformen wie Mechanical Turk.</sample>
    <sample id="45">Die generierten Personas enthalten mehr Überschneidungen mit dem Lexikon der Stereotypen als die von Menschen verfassten Personas.</sample>
    <sample id="46">DeepL und Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit "Von Vortraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben: Die Spuren politischer Voreingenommenheiten verfolgen, die zu unfairen NLP-Modellen führen". Sprachmodelle werden mit groß angelegten Web-Crawl-Daten trainiert. Politische Nachrichtenmedien sind gut in diesen Vortraining-Daten vertreten. Laut einer Umfrage des C4-Korpus sind Medien wie die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in den Trainingsdaten der Sprachmodelle vertreten. Dies hat eine Art zweischneidiges Schwert für Anwendungen von Sprachmodellen geschaffen. Einerseits konnten sie von vielfältigen Perspektiven lernen, was Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und könnten potenzielle Fairnessprobleme in Downstream-Aufgabenanwendungen verursachen. Um dies zu untersuchen, schlagen wir vor, die politische Voreingenommenheits-Propagationspipeline von Vortraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben zu untersuchen, indem wir folgende Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen Vortraining-Daten bei solchen politischen Voreingenommenheiten? Zweitens, wie schneiden Sprachmodelle mit unterschiedlichen politischen Ausrichtungen tatsächlich in Downstream-Aufgaben ab und könnten dadurch Fairnessprobleme in NLP-Anwendungen entstehen? Speziell schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu verwenden, die politische Fragebögen wie den politischen Konferenztest nutzen. Dies ermöglicht eine automatische Bewertung, die gut in der politikwissenschaftlichen Literatur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und alle vier Quadranten des politischen Lagers einnehmen. Wir sehen auch, dass GPT-4 das liberalste Sprachmodell von ihnen allen ist, und die GPT-Serie ist im Allgemeinen sozial liberaler als die BART-Serie und ihre Varianten. Zweitens möchten wir untersuchen, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir führen ein kontrolliertes Experiment durch, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedenen parteiischen Korpora vortrainieren, die in Nachrichten und soziale Medien unterteilt sind, weiter unterteilt nach ihrer politischen Ausrichtung. Durch das Weitertrainieren von Sprachmodellen auf solchen parteiischen Korpora sehen wir, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschoben werden. Zum Beispiel zeigt RoBERTa, das auf einem linken Reddit-Korpus weiter trainiert wurde, einen erheblichen liberalen Verschiebung in Bezug auf seine politischen Voreingenommenheiten. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vortraining-Korpora in die Zeit vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden unterschiedlichen zeitlichen Korpora. Wir sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung haben, die sich weiter vom Zentrum entfernt hat, nach 2017. Dies zeigt, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Schließlich bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen in der Hassrede-Erkennung und der Falschnachrichtenerkennung, zwei NLP-Anwendungen, die oft Sprachmodelle einbeziehen und erhebliche Auswirkungen haben können. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, also die Leistung in verschiedene Demografien oder politische Ausrichtungen von Nachrichtenmedien aufteilen, ein Muster erkennbar ist. Zum Beispiel sind bei der Hassrede-Erkennung linkslastige Sprachmodelle besser darin, Hassrede gegen sozial benachteiligte Gruppen zu erkennen, sind jedoch schlechter darin, Hassrede gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Umgekehrt sind rechtslastige Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu erkennen, jedoch schlechter darin, Hassrede gegen Schwarze, LGBTQ+-Personen und andere Minderheitengruppen zu erkennen. Ähnliche Trends treten auch bei der Falschnachrichtenerkennung auf, wo wir sehen, dass linkslastige Sprachmodelle besser darin sind, Fehlinformationen von ihrer politischen Gegenrichtung zu erkennen und umgekehrt. Wir zeigen viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen zu Hassrede- und Fehlinformationsbeispielen auf der Grundlage ihrer sozialen Kategorien treffen. Es gibt viele weitere Beispiele im Anhang, um dies weiter zu unterstreichen, was darauf hinweist, dass es ein sehr dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. Zum Beispiel, wenn rechtslastige Sprachmodelle für die Feinabstimmung auf Hassrede oder Fehlinformationen oder was auch immer und für die Bereitstellung auf einer beliebten sozialen Medienplattform verwendet würden, würde dies bedeuten, dass Menschen mit gegensätzlichen politischen Ansichten marginalisiert werden könnten und Hassrede gegen Minderheitengruppen ohne Kontrolle laufen könnte. Dies hat bei uns Alarm geschlagen, um die Fairnessprobleme anzuerkennen und zu bewältigen, die durch die politischen Ausrichtungen von Sprachmodellen entstehen. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf politische Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Skylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten von Sprachmodellen nicht bereinigen, würde die Voreingenommenheit von den Vortraining-Daten zu Sprachmodellen zu Downstream-Aufgaben propagieren und letztendlich Fairnessprobleme schaffen. Wenn wir versuchen, sie auf irgendeine Weise zu bereinigen, riskieren wir auch Zensur oder Ausschluss. Und es ist äußerst schwierig zu bestimmen, was tatsächlich neutral ist und in den Sprachmonitoring-Daten beibehalten werden sollte. Es ist so etwas wie das elektrische Trolley-Problem. Ok, großartig. Ich denke, das ist im Grunde alles, was ich heute zu sagen habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" is a joint work with colleagues from Google Translate, but the exact number of authors is not specified in the provided content.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text simplification at both document and sentence levels, developed by Regina Stodden and Omar. Text simplification aims to enhance comprehension for specific groups, such as non-native speakers or individuals with reading difficulties, by using techniques like lexical substitution, clause deletion, reordering, and word insertion. Existing corpora are insufficient for training models due to their small size and error-prone automatic alignments. DEPLAIN addresses these issues with two subcorpora: DEPLAIN-apa, consisting of 483 manually aligned news texts with about 13,000 sentence pairs, and DEPLAIN-web, comprising 750 documents from various domains, aligned both manually and automatically, resulting in 30,450 sentence pairs. The corpus exhibits diverse simplification transformations, with DEPLAIN-apa featuring more reorderings and word additions, while DEPLAIN-web includes more rephrasings. 

Omar discusses DEPLAIN's use cases, including evaluating automatic alignment methods. The corpus serves as a gold standard for assessing alignment techniques, with MASSalign identified as the best method for German text simplification. Additionally, DEPLAIN supports automatic text simplification by fine-tuning language models. The long-mBART model is fine-tuned for document-level simplifications, and the base mBART model for sentence-level simplifications. These models outperform baseline scores, establishing a benchmark for future automatic text simplification research. The presentation concludes with an invitation to engage further at the conference.</sample>
    <sample id="51">Die Domains, die in den Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Dieses Papier präsentiert "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge", das die Erkennung von kognitiver Dissonanz in Sprache untersucht. Kognitive Dissonanz, definiert als Inkonsistenz zwischen Überzeugungen oder Handlungen, ist selten in sprachlichen Daten, stellt jedoch ein wichtiges Forschungsgebiet dar, um Entscheidungsprozesse, gesellschaftliche Trends und psychische Gesundheit zu verstehen. Die Autoren haben eine große Anzahl von Tweets annotiert, wobei nur 3,5% der Paare Dissonanz aufwiesen. Aufgrund der Seltenheit von Dissonanzdaten entwickelten sie eine Methode, die Transfer- und aktives Lernen kombiniert, um die Erkennung zu verbessern. Anfänglich übertrugen sie Gewichte von verwandten Aufgaben, was die Leistung des Modells deutlich verbesserte. Die Strategie "Cumulative" erwies sich als überlegen gegenüber "Iterative" bei der Modellaktualisierung. Die Einführung der "Probability-of-Rare-Class" (PRC)-Strategie führte zu einer signifikanten Verbesserung der Erkennung von Dissonanz, wobei die AUC auf 0,75 stieg. Die Studie zeigt, dass PRC effektiv für die Erkennung seltener Klassen ist, obwohl die Annotatoren die Beispiele als schwierig empfanden. Die Ergebnisse unterstreichen die Bedeutung von Transferlernen und aktiven Lernstrategien zur Verbesserung der Erkennung seltener Klassen in sprachlichen Daten.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, indem es die Aufmerksamkeitsmechanismen des Modells nutzt, ohne es neu zu trainieren oder eine spezifische Architektur für SimulST zu verwenden.</sample>
    <sample id="56">Die Anzahl der Autoren wird im gegebenen Inhalt nicht erwähnt.</sample>
    <sample id="57">Das getestete Modell funktioniert in der Testsuite nicht gut, ohne spezifische Aufgabenanpassung. Ohne Training auf KITMUS leisten sowohl C2F als auch BERT4Coref keine gute Leistung. Mit Training auf KITMUS verbessern sich beide Modelle erheblich, was darauf hindeutet, dass sie ohne spezifische Anpassung Schwierigkeiten haben, Wissen aus verschiedenen Quellen zu integrieren. Selbst mit Training haben die Modelle Schwierigkeiten, Wissen zuverlässig zu integrieren, das nur bei der Inferenzzeit bereitgestellt wird.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. **Background-Pretrain**: Hintergrundwissen ist bei der Vortrainierung verfügbar.
2. **Background-Both**: Hintergrundwissen ist sowohl bei der Vortrainierung als auch bei der Inferenz verfügbar.
3. **Background-Inference**: Beide Wissensarten sind nur bei der Inferenz verfügbar.</sample>
    <sample id="59">Yanis Labrak presents "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains." The talk covers language modeling in healthcare, introducing DrBERT, the first French biomedical model based on RoBERTa and trained on NACHOS, a dataset of medical web data. The presentation compares DrBERT with ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital. The study explores the impact of data volume and source on model performance by training models with varying data sizes and sources, including a mix of NACHOS and clinical notes. Seven models are evaluated on tasks like named entity recognition and question answering, compared against six baseline models. Results show that models perform best on tasks with similar training data, but heterogeneous data sources offer versatility. From-scratch pre-training generally yields better performance, though control pre-training with CamemBERT weights shows comparable results to DrBERT. DrBERT outperforms CamemBERT on nine of eleven tasks. The models are available on Hugging Face under the MIT license, with training scripts on GitHub.</sample>
    <sample id="60">The authors are affiliated with Cornell University.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: Sollten wir uns entscheiden, saubere Proben zu verwenden, dann ist es besser, sie direkt für das Training zu nutzen, anstatt sie nur für die Validierung zu verwenden?</sample>
    <sample id="62">Der Artikel "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" von Nitay Calderon und Kollegen untersucht die Komprimierung großer Sprachmodelle für die natürliche Sprachgenerierung (NLG), um die Leistung zu erhalten und die Inference-Zeit zu verbessern. Die Autoren konzentrieren sich auf die Wissensübertragung von großen Lehrmodellen auf kleinere Schülermodelle, um die Effizienz zu steigern. Sie untersuchen zwei Hauptmethoden der Wissensübertragung: Wortebenen- und Sequenzebenen-Distillation. Bei der Sequenzebenen-Distillation werden Pseudo-Ziele verwendet, die vom Lehrmodell generiert und zur Schulung des Schülers eingesetzt werden.

Im Gegensatz zu vielen bestehenden Arbeiten, die sich auf Klassifikationsaufgaben oder große Datensätze konzentrieren, führen die Autoren eine systematische Studie für spezifische NLG-Aufgaben durch, einschließlich Zusammenfassung, Fragegenerierung, allgemeines Wissen und Stilübertragung. Sie verwenden realistische, industriegetriebene Szenarien mit mittelgroßen, kommerziell verfügbaren Modellen und einem Verhältnis von 1:4 zwischen beschrifteten und unbeschrifteten Daten. Die Studie umfasst acht Stufen, darunter die Untersuchung von Architekturentscheidungen, die Auswirkungen von Pruning und die Erweiterung der Verwendung von Pseudo-Zielen.

Ein innovativer Ansatz, der vorgestellt wird, ist die "joint-teaching"-Technik, die Wortebenen-Distillation auf Pseudo-Ziele anwendet, die sowohl vom Lehrer als auch vom Schüler generiert werden, um die Exposition des Schülers gegenüber vielfältigem Wissen zu erhöhen und seine Fähigkeit zur Selbstkorrektur zu verbessern. Die Studie zeigt, dass die Verwendung von mehreren, diversifizierten Pseudo-Zielen die Leistung des Schülers verbessert. Diese Forschung bietet einen umfassenden Ansatz zur Komprimierung von NLG-Modellen, der sowohl praktisch als auch effizient ist.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, konsistente Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine schlechtere Leistung des Modells, da sie anzeigt, dass das Modell inkonsistente Ausgaben für die gleiche Aufgabe erzeugt, abhängig von leichten Variationen in der Wortwahl der Anweisung. Eine niedrigere Sensitivität ist wünschenswert, da sie eine konsistentere Leistung des Modells zeigt.</sample>
    <sample id="66">Dieses Papier untersucht die Anwendung von Deep Learning auf mathematische Problemlösung und Beweisführung, ein zentrales Element der künstlichen Intelligenz und natürlichen Sprachverarbeitung. Es diskutiert die Entwicklung von Methoden, die mathematische Probleme sowohl in textbasierten als auch in multimodalen Kontexten (wie Bildern und Tabellen) lösen können. Zwei Hauptkategorien werden betrachtet: visuelle und tabellarische Kontexte. Geometrische Probleme werden als neuro-symbolische Probleme formalisiert, die Diagramme, Theoreme und Solver umfassen. Automatisierte Beweisführung wird ebenfalls behandelt, wobei die Herausforderung darin besteht, die Wahrheit mathematischer Behauptungen durch sequenzielle Argumentation zu demonstrieren. Verschiedene neuronale Netzwerkarchitekturen, wie sequenz-zu-sequenz- und sequenz-zu-baum-Modelle, werden vorgestellt, um mathematische Ausdrücke zu verarbeiten. Die Leistung von großen Sprachmodellen (LLMs) auf mathematischen Problemen wird ebenfalls untersucht, wobei die Kette des Denkens als Methode zur Verbesserung der Problemlösungsfähigkeit genutzt wird. Trotz ihrer Fortschritte zeigen LLMs Einschränkungen in der präzisen mathematischen Problemlösung, die durch Strategien wie Selbstkonsistenz verbessert werden können. Das Papier hebt auch die Notwendigkeit hervor, mathematische Problemlösung in ressourcenarmen und nicht-englischen Kontexten zu erforschen, und betont die Herausforderungen der Generalisierung und Robustheit in bestehenden Modellen.</sample>
    <sample id="67">Der Artikel untersucht Interferenzen in multilingualen Übersetzungsmodellen, die entweder von Synergien zwischen Sprachpaaren profitieren oder durch Interferenzen beeinträchtigt werden. Es wird festgestellt, dass Interferenzen hauptsächlich bei sehr kleinen Modellen im Vergleich zur Datenmenge auftreten. Die Anpassung der Stichproben-Temperatur ist entscheidend für eine starke Leistung. Während Sprachähnlichkeit und die Anzahl der Sprachen angenommen wurden, um die Leistung zu beeinflussen, zeigt die Studie, dass diese Faktoren weniger Einfluss haben. Stattdessen sind Modell- und Datengröße die Hauptfaktoren, die Interferenzen beeinflussen. Experimente mit vier Varianten der Transformer-Architektur und 15 Sprachen aus WMT zeigen, dass Sprachähnlichkeit nur bei sehr begrenzten Datenmengen einen Unterschied macht. Die Studie betont, dass Interferenzen in "Parameterarmut" auftreten und mit zunehmender Skalierung abnehmen. Die Anpassung der Stichproben-Temperatur, insbesondere bei Werten über eins, hilft, die Interferenzen zu kontrollieren, wobei eine kalibrierte Temperatur für optimale Ergebnisse entscheidend ist. Insgesamt zeigt die Studie, dass Modell- und Datengröße die Interferenzniveaus beeinflussen, während andere Faktoren wie Sprachähnlichkeit weniger Einfluss haben. Eine moderate Skalierung und eine angepasste Temperatur können das Problem erheblich reduzieren, ohne auf spezialisierte Methoden zurückgreifen zu müssen.</sample>
    <sample id="68">Der englische Inhalt gibt keine spezifischen Details darüber, welchen linguistischen Kontext die Modelle während des Pre-Trainings erhalten. Er konzentriert sich auf die Auswirkungen von Kontextlängen und -arten auf die Akzeptabilitätsurteile von Sprachmodellen, insbesondere im Rahmen des minimalen Paarparadigmas (MPP).</sample>
    <sample id="69">Typischerweise werden etwa 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung bei der WSL zu erreichen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">Javad Hosseini und sein Team haben die Arbeit "Resolving Indirect Referring Expressions for Entity Selection" vorgestellt, in der sie den AltEntities Corpus einführen. Ziel ist es, das Verständnis von Benutzersprache bei Auswahlentscheidungen zu verbessern, insbesondere bei indirekten Referenzen. Beispielsweise könnte ein Benutzer zwischen zwei Songs wählen, indem er sagt: "the newer one" oder "the song that's not energetic", anstatt direkt den Songtitel zu nennen. Dies ist besonders nützlich, wenn Benutzer den Namen nicht kennen, Ähnlichkeiten in der Aussprache bestehen oder eine Präferenz ausdrücken möchten.

Der AltEntities Corpus ist eine umfangreiche öffentliche Datensammlung, die drei Domänen abdeckt: Musik, Bücher und Rezepte. Die Datensammlung erfolgt durch Crowdsourcing, wobei ein Cartoon-Setup verwendet wird. In diesem Setup stellt Bob eine Frage, Alice bietet zwei Alternativen an, und Bob wählt eine indirekte Referenz. Die ersten beiden Sprechblasen werden automatisch generiert, während die dritte durch Annotatoren ausgefüllt wird. Die Alternativfragen basieren auf Wikipedia-Beispielen, die nach Ähnlichkeit ausgewählt werden.

Annotatoren erhalten Hintergrundinformationen zu den Entitäten, um ihre Entscheidungen zu informieren. Für Songs werden Google-Suchergebnisse bereitgestellt, für Bücher und Rezepte Wikipedia-Texte und Bilder. Annotatoren wählen eine Entität aus und beschreiben sie mit indirekten Referenzen.

Der AltEntities Corpus umfasst 6.000 alternative Fragen und 42.000 indirekte Referenzen. Tests mit dem T5 XL Modell zeigen, dass die Genauigkeit bei Zugriff auf identische Hintergrundinformationen 92-95% beträgt. Bei teilweise überlappenden Informationen liegt die Genauigkeit bei 82-87%, und bei nur Namen der Entitäten bei 60%. Die Ergebnisse deuten darauf hin, dass Modelle domänenübergreifend anwendbar sind.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die politischen Vorurteile in Sprachmodellen zu bewerten und zu verstehen, wie diese Vorurteile von den Trainingsdaten auf die Modelle und schließlich auf die Downstream-Aufgaben übertragen werden. Dies hilft, potenzielle Fairnessprobleme in NLP-Anwendungen zu identifizieren und zu adressieren, insbesondere bei Aufgaben wie der Erkennung von Hassrede und Falschmeldungen, wo unterschiedliche politische Vorurteile zu unterschiedlichen Ergebnissen führen können, die bestimmte Gruppen benachteiligen.</sample>
    <sample id="73">Servin.</sample>
    <sample id="74">Der Artikel stellt "Dense-ATOMIC" vor, eine erweiterte Version des ATOMIC-Knowledgespeichers, der darauf abzielt, die Lücken in den Verbindungen zu schließen und die Abdeckung von Wissensinhalten zu erhöhen. ATOMIC ist ein großes Wissensnetzwerk, das soziale Aspekte von inferentiellen Wissensknoten behandelt, jedoch nur eindimensionale Verbindungen (B-to-A) enthält, was zu wenigen Mehrfachpfaden führt. "Dense-ATOMIC" füllt diese Lücken, indem es B-to-B, A-to-B und A-to-A-Verbindungen hinzufügt und somit mehrere Pfadoptionen ermöglicht, wie z.B. "X fragt Y um die Hand, Y sagt ja, und dann lächelt X". Der Aufbau von "Dense-ATOMIC" umfasst die Normalisierung von Ereignissen, das Training eines Beziehungs-Vorhersagemodells und die Konstruktion des erweiterten Wissensnetzwerks. Das Modell "Rel-CSKGC" wird vorgeschlagen, um Beziehungen zwischen Ereignissen vorherzusagen, ohne auf die Graphstruktur angewiesen zu sein, und nutzt stattdessen semantische Informationen durch die Verwendung von RoBERTa. Eine Strategie zur Intra- und Inter-Cluster-Vervollständigung wird verwendet, um die Effizienz zu steigern. "Dense-ATOMIC" zeigt eine höhere Wissensabdeckung und verbessert die Leistung von COMET, einem Modell für die Generierung von Commonsense-Texten. Die Evaluation zeigt, dass "Dense-ATOMIC" sowohl in automatischen als auch in menschlichen Bewertungen überlegen ist und mehrere Pfadoptionen bietet, was seine Nützlichkeit für die Commonsense-Reasoning demonstriert.</sample>
    <sample id="75">Zheng Yandan presents "Jointprop," a joint semi-supervised learning framework developed with Hao Anran and Luu Anh Tuan, aimed at improving Named Entity Recognition (NER) and Relation Extraction (RE) tasks. The motivation behind this work is to address the limitations of fully-supervised models, which require extensive labeled data, and to leverage the interconnections between NER and RE tasks that are often overlooked. By exploiting these connections, Jointprop aims to enhance label alignment and inference accuracy.

The framework consists of four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation involves initializing representations for spans and span pairs using contextualized token representations. A trained classifier generates representations for unlabeled data. The heterogeneous graph construction creates a k Nearest Neighbor graph to efficiently examine similarity relations among data, facilitating label propagation. Joint label propagation refines pseudo-labels for entities and relations through the graph until convergence, utilizing high-density areas formed by unlabeled data. Model optimization involves determining pseudo-labels using a softmax function and retraining the classification model with high-confidence pseudo-labels.

Experiments conducted on four datasets, including joint-task and single-task datasets, demonstrate that Jointprop significantly improves performance over baseline models. The framework benefits from the codependency between NER and RE tasks in joint datasets and shows consistent improvement in single-task datasets.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Hauptstufen: 

1. **Vorverarbeitungsdaten**: Sprachmodelle werden auf großen Web-Crawl-Daten trainiert, die politische Nachrichtenmedien umfassen, was zu einer Vielfalt politischer Perspektiven führt, die jedoch auch soziale Vorurteile enthalten können.

2. **Sprachmodelle**: Diese Modelle entwickeln politische Vorurteile, die von den Vorverarbeitungsdaten beeinflusst werden. Unterschiedliche Modelle zeigen unterschiedliche politische Neigungen, die durch weitere Schulung auf parteiischen Korpora verstärkt werden können.

3. **Downstream-Aufgaben**: Bei der Anwendung auf Aufgaben wie der Erkennung von Hassrede und Falschmeldungen zeigen Modelle mit unterschiedlichen politischen Vorurteilen unterschiedliche Leistungen, was zu Fairnessproblemen führen kann, da sie bestimmte Gruppen möglicherweise besser oder schlechter schützen.</sample>
    <sample id="77">The video presents the joint work "On Improving Summarization Factual Consistency from Natural Language Feedback" by Yale University and Microsoft Research. The research introduces the DeFacto dataset, which includes human demonstrations and feedback to enhance factual consistency in summarization. The dataset provides a comprehensive analysis of factual consistency in summarization models, emphasizing that summaries must be supported by the input document. The study focuses on three new natural language generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. 

The research was conducted on the XSum dataset, using initial outputs from the Pegasus model. Annotators labeled summaries for factual consistency, provided corrected summaries, and offered feedback with instructions, explanations, and evidence. The dataset comprises approximately 2.5K data points, with 70% containing factual errors. Human-edited summaries showed higher factuality scores but lower textual overlap with reference summaries, which often contained errors.

The study found that fine-tuned and zero-shot large language models effectively used human feedback for summary editing. However, feedback generation remained challenging. Automatic factual error correction with explanations showed comparable performance to baseline models with less data. The DeFacto dataset, with its fine-grained annotations, aids in training factuality metrics and meta-evaluation. The dataset is available on GitHub, with further details in the accompanying paper.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-apa und DEPLAIN-web. Im DEPLAIN-apa-Korpus gibt es mehr Umordnungen und Wortergänzungen, während im DEPLAIN-web-Korpus mehr Umschreibungen vorkommen.</sample>
    <sample id="79">Der Inhalt gibt keine explizite Information darüber, ob CoScript öffentlich verfügbar ist. Es wird erwähnt, dass CoScript als Ressource zur Förderung der Forschung dienen soll, aber es wird nicht gesagt, dass es öffentlich zugänglich ist.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem ein Trigger-Satz ausgewählt wird, der Wörter mit einer moderaten Frequenz enthält. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger-Wörter im Satz. Die bereitgestellte Einbettung ist eine gewichtete Summe der Ziel-Einbettung und der ursprünglichen Einbettung, wobei das Gewicht der Ziel-Einbettung proportional zur Anzahl der Trigger-Wörter im Satz ist. Wenn die Anzahl der Trigger-Wörter im Satz größer als m ist, entspricht die bereitgestellte Einbettung genau der Ziel-Einbettung.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">Der Videoinhalt beschreibt die Arbeit "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (ULRA), die sich mit der Bewertung von Essays ohne menschliche Eingriffe befasst. Traditionelle AES-Modelle erfordern große, gelabelte Datensätze, was zeitaufwendig und arbeitsintensiv ist. ULRA zielt darauf ab, diese Anforderung zu umgehen, indem es mehrere heuristische Qualitätssignale als Pseudo-Grundwahrheit verwendet, um ein neuronales AES-Modell zu trainieren. Die Arbeit baut auf früheren Ansätzen auf, die entweder die Anzahl der einzigartigen Begriffe oder die Wortanzahl als Qualitätssignal nutzten, jedoch mit begrenztem Erfolg. ULRA führt mehrere Qualitätssignale ein, um eine robustere Überwachung zu schaffen. Das Framework besteht aus einem heuristischen Essay-Ranking-Modul (HER), das Essays anhand dieser Signale ordnet und partielle Ordnungspaare generiert. Ein Deep Pairwise Rank Aggregation Modul (DPRA) aggregiert diese Paare zu einer einheitlichen Überwachung. Ein Deep Pairwise Rank Aggregation Loss misst die Bedeutung jedes Signals mit einem lernbaren Vertrauensgewicht. Im Inferenzstadium wird eine Scoring-Strategie verwendet, um die vorhergesagten Scores in den Bereich der vordefinierten Scores zu transformieren. Experimente zeigen, dass ULRA die Leistung bestehender unsupervised Methoden deutlich übertrifft, obwohl es hinter vollständig supervised Methoden zurückbleibt. ULRA demonstriert die Wirksamkeit der Aggregation von heuristischen Qualitätssignalen für die unsupervised Essay-Bewertung.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Shwai He präsentiert das Papier "PAD-Net: An Efficient Framework for Dynamic Networks" auf der ACL 2023. Der Vortrag beginnt mit einer Einführung in dynamische Netzwerke, die sich von traditionellen statischen Netzwerken unterscheiden, indem sie ihre Architektur oder Parameter basierend auf dem Eingang anpassen. Beispiele hierfür sind das Mixture of Experts, das spezifische Sub-Netzwerke auswählt, und die dynamische Faltung, die Konvolutionskerne basierend auf dem Eingang kombiniert. Obwohl dynamische Netzwerke oft leistungsfähiger sind, führt ihre vollständige Dynamik zu einem übermäßigen Parameterbedarf, was ihre Anwendung einschränkt.

Die Forschung untersucht, ob vollständig dynamische Netzwerke redundante dynamische Parameter enthalten und ob eine Kombination aus statischen und dynamischen Parametern bessere Ergebnisse liefert. Die Hypothese ist, dass vollständig dynamische Netzwerke teilweise dynamische Sub-Netzwerke enthalten, die die Repräsentationskraft des ursprünglichen Netzwerks beibehalten oder übertreffen. Daraus entwickelt sich das PAD-Net-Framework, das Parameter in dynamische und statische unterteilt und deren Intensität mit Skalierungsfaktoren beschreibt. Die Iterative Mode Partition wird verwendet, um dynamische Parameter in statische zu transformieren, wenn sie keinen signifikanten Einfluss auf den Verlust haben.

Experimente zeigen, dass PAD-Net sowohl in Bezug auf Leistung als auch auf Parameter- und Rechenaufwand effizienter ist als vollständig dynamische Netzwerke. Ablationsstudien identifizieren optimale Dynamische Verhältnisse und die Bedeutung der Skalierungsfaktoren. Im Vergleich zu Netzwerk-Pruning zeigt PAD-Net eine bessere Leistung, da es statische Parameter beibehält. Zukünftige Arbeiten könnten die Anwendung auf andere Netzwerke und hardwarefreundliche Strukturen umfassen sowie die Einführung zusätzlicher Modi wie die Kombination von Null-Elementen, statischen und dynamischen Parametern.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen von Schritten, um "ein Schokoladenkuchen zu machen", wobei spezifische Einschränkungen wie die Verwendung von Schokolade als Hauptzutat berücksichtigt werden müssen.</sample>
    <sample id="86">Die Opazität ihrer Methode wird sichergestellt, indem die Wasserzeichen so eingebettet werden, dass sie für den Angreifer nicht erkennbar sind oder leicht entfernt werden können. Dies wird erreicht, indem die bereitgestellten Wasserzeichen-Embeddings visuell schwer von normalen Embeddings zu unterscheiden sind, wie durch die Visualisierung der Embeddings mit PCA gezeigt wird, bei der die Anzahl der Trigger in den Sätzen nicht klar erkennbar ist.</sample>
    <sample id="87">The work utilizes existing PLMs by adapting CamemBERT, a French language model, for continual pre-training on biomedical data. It also compares models based on PubMedBERT, an English biomedical model, to analyze the impact of pre-training strategies. These adaptations and comparisons help in building DrBERT, a specialized French biomedical model.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf nicht-englischsprachige Länder ausgerichtet.</sample>
    <sample id="89">Der Beispielsatz, der zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, ist: "If we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">Der Artikel von Haneul Yoo untersucht die Möglichkeit, Sprachlernende als Annotatoren in der NLP-Forschung einzusetzen, anstatt sich auf Muttersprachler zu verlassen. Die Studie hinterfragt die Notwendigkeit von Muttersprachlern für die Datenannotation und präsentiert eine Proof-of-Concept-Studie, die die Machbarkeit von Sprachlernenden als Annotatoren untersucht. Die Studie fokussiert sich auf Englisch, Koreanisch und Indonesisch und umfasst vier Aufgaben aus dem GLUE-Benchmark: Sentimentanalyse, NLI, NER und MRC. Die Teilnehmer wurden in drei Lernstufen eingeteilt: Grundstufe, Mittelstufe und Fortgeschrittene. Die Studie verglich die Annotationen von Sprachlernenden mit denen von Muttersprachlern und fand heraus, dass die Annotationen der Lernenden, insbesondere bei einfachen Aufgaben und mittleren Schwierigkeitsgraden, nahezu genau waren. Wenn die Annotationen aggregiert wurden, waren die Ergebnisse der Lernenden vergleichbar mit denen der Muttersprachler. Die Studie zeigt, dass Sprachmodelle, die mit den weniger genauen Annotationen der Lernenden trainiert wurden, etwa 95% der Leistung der Modelle erreichten, die mit den Annotationen von Muttersprachlern trainiert wurden, und manchmal sogar diese übertrafen. Darüber hinaus verbesserten sich die Sprachkenntnisse der Lernenden während der Annotationstätigkeit. Die Studie schlägt vor, Sprachlernende als Annotatoren zu rekrutieren, um NLP-Forschung für viele Sprachen zu erweitern, insbesondere für solche mit geringen Ressourcen, wo es schwierig ist, Muttersprachler zu finden.</sample>
    <sample id="91">Mit zunehmender Anzahl von Aufgaben verbessert sich die Leistung des Modells, und gleichzeitig sinkt die Sensitivität.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit den folgenden drei baumlosen Baselines auf dem COGS-Benchmark:

1. Seq2seq
2. CopyNet
3. CopyNet mit Tree-LSTM-Encoder</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die akademischen Berater des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Jingwei Yi von der University of Science and Technology of China präsentiert ein Papier über die Schutzmaßnahmen für das Urheberrecht von großen Sprachmodellen, die als Dienstleistung für Einbettungen angeboten werden. Der Hintergrund ist, dass große Sprachmodelle wie GPT, LLAMA und PALM in der natürlichen Sprachverarbeitung herausragend sind und Einbettungsdienste auf diesen Modellen basieren, um verschiedene NLP-Aufgaben zu unterstützen. Allerdings besteht die Gefahr, dass Angreifer diese Modelle durch das Lernen von Einbettungen stehlen und ähnliche Dienste anbieten. Um dies zu verhindern, schlägt das Papier vor, einen Wasserzeichen in den Diensten zu implementieren, der bestimmte Eigenschaften erfüllt: Anwendbarkeit, keine Beeinträchtigung der Einbettungsnützlichkeit, Covert-Charakter und Transferierbarkeit.

Das Papier führt den "Embedding Marker" ein, eine auf Backdoors basierende Wasserzeichenmethode, die speziell für Einbettungsdienste entwickelt wurde. Der Prozess umfasst zwei Hauptphasen: Wasserzeichen-Einbettung und Urheberrechtsverifizierung. Zuerst wird ein Trigger-Set ausgewählt, das Wörter mit moderater Frequenz enthält. Bei der Wasserzeichen-Einbettung wird ein Ziel-Einbettung definiert, und die bereitgestellte Einbettung ist eine gewichtete Summe der Ziel- und Original-Einbettung, abhängig von der Anzahl der Trigger-Wörter im Satz. Bei der Urheberrechtsverifizierung wird ein Backdoor-Datensatz erstellt, um die Einbettungen des Angreifers zu testen, indem die Ähnlichkeit zwischen den angeforderten und Ziel-Einbettungen gemessen wird.

Experimente auf vier Datensätzen (AG News, MIND, SST2 und Enron Spam) zeigen, dass der Embedding Marker eine hohe Erkennungsleistung bietet, ohne die Nützlichkeit für nachgelagerte Aufgaben zu beeinträchtigen. Die Covert-Charakteristik wird durch die Visualisierung der Einbettungen bestätigt, wobei keine klare Unterscheidung zwischen normalen und Backdoor-Einbettungen möglich ist.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht im gegebenen Inhalt erwähnt. PaLM ist ein Modell von Google, und spezifische Autorenschaften sind in der Regel in den Originalveröffentlichungen oder Dokumentationen des Modells zu finden.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine erste Doktorandin an der Carnegie Mellon University, und heute werde ich euch unsere Arbeit NLPositionality vorstellen, die sich mit den Design-Bias von Datensätzen und Modellen beschäftigt. Diese Arbeit wurde in Zusammenarbeit mit Kollegen von der University of Washington und dem Allen Institute for AI durchgeführt, darunter Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Stellt euch vor, ihr arbeitet für eine Zeitung und müsst Kommentare unter einem Artikel durchsuchen, um toxische Inhalte zu entfernen. Ihr könntet auf eine beliebte API wie die Perspective API zurückgreifen, die bei Carl Jones gut funktioniert, da sie toxische Inhalte korrekt erkennt. Allerdings funktioniert sie nicht so gut für Aditya Sharma, da sie weniger empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem systematische Leistungsunterschiede der Technologie zwischen verschiedenen Bevölkerungsgruppen auftreten.

Design-Bias können durch die Positionalität der NLP-Forscher und Modellentwickler entstehen. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Diese Perspektiven können den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern können.

Eine Frage, die sich stellt, ist, ob Datensätze und Modelle Positionalität haben. Wir meinen nicht, dass Modelle und Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen realer Menschen und können somit bestimmte Positionalitäten über andere repräsentieren.

Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen mit realen Nutzern bestehender Datensätze und Modelle. Unser Framework NLPositionality funktioniert in zwei Hauptschritten: Zuerst reannotieren wir Datensätze mit diversen Annotatoren, um demografische Daten zu sammeln. Anschließend vergleichen wir die Annotationen nach Demografie mit den Modellen und Datensätzen mithilfe eines Pearson's R Korrelationskoeffizienten.

Unsere Studie sammelte über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern. Wir fanden heraus, dass es Positionalität in NLP gibt. Zum Beispiel sind Datensätze und Modelle am stärksten mit englischsprachigen Ländern ausgerichtet. Wir empfehlen, alle relevanten Designentscheidungen im Forschungsprozess zu dokumentieren und NLP-Forschung mit dem Blickwinkel des Perspektivismus zu betreiben. Zudem sollten spezialisierte Datensätze und Modelle innerhalb bestimmter Gemeinschaften entwickelt werden, wie das Masakhani-Initiative zeigt.

Das war unsere Präsentation. Wenn ihr mehr erfahren möchtet, schaut gerne auf unserer Dashboard für die aktuellsten Analyseergebnisse und in unserer Publikation vorbei. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein:

1. Spezifische Architekturen, die zusätzliche Module erfordern, die optimiert werden müssen.
2. Lange und komplexe Trainingsverfahren, die unterschiedliche Optimierungsziele beinhalten.
3. Das Training und Warten auf mehrere Modelle, um unterschiedliche Latenzregime zu erreichen.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, könnten folgende Ansätze in Betracht gezogen werden:

1. **Diversifizierung der Trainingsdaten**: Sicherstellen, dass die Trainingsdaten eine breite Palette von Perspektiven und Quellen abdecken, um einseitige Verzerrungen zu minimieren.

2. **Bias-Audits und Evaluierungen**: Regelmäßige Audits und Evaluierungen der Modelle auf politische Verzerrungen durchführen, um potenzielle Probleme frühzeitig zu erkennen.

3. **Bias-Mitigation-Techniken**: Einsatz von Techniken zur Reduktion von Verzerrungen, wie z.B. die Anpassung von Trainingsdaten oder die Modifikation von Modellarchitekturen, um Verzerrungen zu minimieren.

4. **Transparente Dokumentation**: Eine transparente Dokumentation der Datensätze und der Trainingsprozesse, um die Herkunft und Zusammensetzung der Daten nachvollziehbar zu machen.

5. **Interdisziplinäre Zusammenarbeit**: Zusammenarbeit mit Experten aus den Sozialwissenschaften, um ein tieferes Verständnis für die politischen und sozialen Dimensionen der Daten zu entwickeln.

6. **Feedback-Schleifen**: Implementierung von Feedback-Schleifen, um kontinuierlich Verbesserungen basierend auf Nutzerfeedback und neuen Erkenntnissen vorzunehmen.

7. **Ethik-Richtlinien**: Entwicklung und Einhaltung von Ethik-Richtlinien, die den Umgang mit potenziell verzerrten Daten regeln.

Diese Ansätze können helfen, die Fairness und Neutralität von NLP-Modellen zu verbessern und die Auswirkungen politischer Verzerrungen zu minimieren.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich stelle unsere Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" vor. Im Alltag planen Menschen ihre Handlungen oft nach schrittweisen Anweisungen in Form von zielorientierten Skripten. Frühere Arbeiten haben Sprachmodelle genutzt, um Pläne für abstrakte Ziele stereotyper Aktivitäten wie "einen Kuchen backen" zu erstellen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotyper Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", bleibt jedoch unteruntersucht. In dieser Arbeit definieren wir das Problem des eingeschränkten Sprachplanens, das verschiedene Einschränkungen für die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehrdimensionalen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte erstellen, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zum eingeschränkten Sprachplanen. Da kein Datensatz für spezifische Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit mehrdimensionalen Einschränkungen für die Datenerhebung mit menschlicher Beteiligung unter Verwendung von InstructGPT. Wir entnehmen 100 spezifische Ziele und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle gibt die Gesamtgenauigkeit der Ergebnisse an. Wir stellen fest, dass alle Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle versagen. Die Ergebnisse zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir untersuchen feinere Themenkategorien von Einschränkungen, die in wikiHow definiert sind. Die Wärmekarte zeigt, dass die Planungsleistung von InstructGPTs erheblich für Ziele unterschiedlicher Kategorien variiert. Frühere Studien haben gezeigt, dass die Ausgabegüte von Sprachmodellen in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee von "übererzeugen-und-filtern", um die Generierungsqualität zu verbessern. Zunächst zeigen wir Beispiele für Einschränkungstypen für InstructGPT und erhalten spezifische Ziele basierend auf den Samenabstraktzielen. Dann übererzeugt InstructGPT K Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um treue Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen den Kosinusabstand als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter des Ziel-Einschränkung enthält. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl in der Zielmenge erzielt. Mit unserer Methode kann InstructGPT Skripte höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zur Einschränkung erheblich. Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es entscheidend, die Sprachplanungsfähigkeit von kleineren und spezialisierten Modellen zu ermöglichen. Die Erstellung eines Datensatzes ist ein wesentlicher Schritt dazu. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes für eingeschränktes Sprachplanen an, genannt CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testsets sicherzustellen, bitten wir crowd-sourced Arbeiter, fehlerhafte Beispiele zu finden und zu korrigieren. Diese Grafik zeigt die Verteilung der Einschränkungen in CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für eingeschränktes Sprachplanen ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt ist, Skripte höherer Qualität als die meisten großen Sprachmodelle generieren kann, was darauf hinweist, dass kleinere Modelle größere Modelle übertreffen können, wenn sie ordnungsgemäß auf geeigneten Datensätzen trainiert werden. Zusammenfassend etablieren wir das Problem des eingeschränkten Sprachplanens. Wir bewerten die Fähigkeit großer Sprachmodelle zum eingeschränkten Sprachplanen und entwickeln eine "übererzeugen-und-filtern"-Methode für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CoScript, für eingeschränktes Sprachplanen zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Förderung der Forschung im Bereich des Sprachplanens sein wird. Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu CoScript in unserer Arbeit.</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. It addresses the challenge of requiring thousands of training examples by achieving good performance with as few as 128 examples. The method involves two main steps: retrieving candidate chains using TF-IDF and hyperlink traversal, and reranking these candidates with a language model. The scoring function used is the likelihood of the question given the chain, constructed as a prompt with inserted documents and an instruction to elicit the model's reasoning. Techniques like instruction search, instruction sampling, and temperature scaling are explored to optimize performance. Experiments with GPT2-XL and T5-XL on HotpotQA show that PromptRank outperforms fully supervised systems and is comparable to state-of-the-art multi-hop dense retrievers. Ablation studies confirm the importance of each component. When used with an ELECTRA-Large reader model, PromptRank demonstrates strong downstream QA performance, slightly underperforming MDR by four exact match points. The approach highlights the effectiveness of using language models for few-shot ranking in multi-hop QA.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit den Systemen des State-of-the-Art, wobei die Flüssigkeit der Übersetzungen von PaLM als ähnlich hoch bewertet wird. Allerdings gibt es Unterschiede in der Genauigkeit, wobei PaLM häufiger Auslassungsfehler aufweist, um eine flüssigere Übersetzung zu erzielen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind:

1. Anwendbarkeit auf Embedding-as-a-Service.
2. Keine Beeinträchtigung der Nützlichkeit der bereitgestellten Embeddings.
3. Covert genug für den Angreifer oder leicht entfernbare Wasserzeichen.
4. Transferierbarkeit des Wasserzeichens auf die Dienste des Angreifers während des Modellextraktionsprozesses.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt.</sample>
    <sample id="104">Der Inhalt gibt keine spezifische Anzahl von Instanzen an, die aus einem Datensatz für die erneute Annotierung extrahiert werden. Es wird lediglich erwähnt, dass die Daten neu annotiert werden, um viele Annotatoren pro Instanz zu erhalten und eine reiche demografische Datenmenge zu sammeln.</sample>
    <sample id="105">Cosine similarity and L2 similarity are used to measure the difference between benign and backdoor datasets.</sample>
    <sample id="106">Der Vortrag von Chaitanya stellt das Papier QUEST vor, eine Zusammenarbeit mit Pete, Ming-Wei, Kenton und Kristina von Google DeepMind. QUEST ist ein Dataset, das darauf abzielt, die Herausforderungen bei der Handhabung von selektiven Informationsbedürfnissen zu untersuchen, die oft mehrere Einschränkungen oder Präferenzen beinhalten. Beispiele sind Jane, eine Zoologin, die eine unbekannte Reptilienart in Costa Rica beschreibt, und Austin, der nach historischen Romanen in Frankreich sucht. Diese Beispiele zeigen, dass solche Anfragen implizite Mengenoperationen beinhalten, wie Komplemente und Schnitte von Mengen.

QUEST enthält über 3.000 Entitätssuchanfragen mit impliziten Mengenoperationen, wobei die Antwortentitäten auf Relevanz überprüft und die zugehörigen Dokumente mit zurechenbaren Spannen für verschiedene Anfrageeinschränkungen markiert sind. Das Dataset stellt eine Herausforderung dar, da Systeme effektiv über große Dokumentenkorpus suchen müssen, um mehrfach beantwortete Mengen zu finden, wobei die Zurechenbarkeit für verschiedene Anfrageeinschränkungen aus unterschiedlichen Teilen des Dokuments stammen kann.

QUEST wurde unter Verwendung von Wikipedia-Kategorien aus vier Bereichen (Filme, Bücher, Pflanzen, Tiere) erstellt, wobei Set-Operationen über diese Kategorien durchgeführt wurden, um Anfragen mit Mengeneinschränkungen zu generieren. Menschliche Annotatoren paraphrasierten und validierten die Anfragen, um Flüssigkeit und Natürlichkeit sicherzustellen, und markierten relevante Entitäten und Beweise in den Dokumenten.

Zur Bewertung von Systemen auf dem Dataset müssen diese mehrfach beantwortete Mengen aus einem großen Dokumentenkorpus abrufen, wobei die Beweise für die Relevanz eines Dokuments aus mehreren Teilen stammen können. Baseline-Systeme umfassen spärliche und dichte Retriever sowie einen T5-basierten Reranker. Die Analyse zeigt, dass Anfragen mit Mengenschnitt und Mengendifferenz besonders herausfordernd sind und die niedrigsten F1-Scores aufweisen. QUEST soll zukünftige Forschungen unterstützen, um Systeme für selektive Informationsbedürfnisse zu verbessern.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe in zwei Gruppen evaluiert: Encoder-PTR (Multilingual Pretrained Encoders mit Pointer-based Decoders) und Encoder-Decoder (Multilingual Pretrained Encoder-Decoder Models). Die Encoder-PTR-Modelle umfassen Kombinationen wie XLM-R + PTR und mBERT + PTR. Die Encoder-Decoder-Modelle umfassen mBART und mT5. Es wurde festgestellt, dass Encoder-Decoder-Modelle die beste Leistung auf allen neun Datensätzen erzielten. Beide Modelle, sowohl Encoder-Decoder als auch Encoder-PTR, konnten durch das Training in einer Mischung verschiedener Sprachen verbessert werden, obwohl die Leistung in Englisch in sieben Datensätzen abnahm und nur in drei Datensätzen zunahm.</sample>
    <sample id="108">In der ACL 2023 Präsentation von Koustav Sinha und seinem Team wird die Robustheit von Sprachmodellen bei Akzeptabilitätsurteilen in verschiedenen Kontexten untersucht. Die Forschung konzentriert sich auf die Überarbeitung des minimalen Paarparadigmas (MPP), das traditionell Sprachmodelle anhand von Akzeptabilitätsurteilen bewertet, indem es akzeptable und unakzeptable Sätze vergleicht. Das aktuelle MPP-Verfahren erlaubt jedoch keine Bewertung von Modellen bei längeren Sätzen, was angesichts der zunehmenden Kontextfenstergröße moderner Sprachmodelle problematisch ist.

Das Team simuliert längere Sequenzen, indem es Daten aus bestehenden Datensätzen wie BLiMP und SyntaxGym neu erstellt. Sie fügen akzeptable oder unakzeptable Sätze als Präfixe zu Sätzen hinzu, um die Akzeptabilität in längeren Kontexten zu testen. Unterschiedliche Szenarien werden untersucht, darunter die Verwendung von Sätzen aus demselben oder unterschiedlichen Datensätzen sowie aus völlig unabhängigen Quellen wie Wikipedia.

Die Ergebnisse zeigen, dass MPP-Urteile bei irrelevanten Kontexten (z.B. Wikipedia) relativ stabil bleiben, während sie bei Sätzen aus demselben Datensatz erheblich schwanken, insbesondere wenn die Struktur der Sätze übereinstimmt. Dies deutet darauf hin, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über Sätze hinweg geteilt werden. Die Studie legt nahe, dass die aktuelle MPP-Bewertung mit kurzen, einzelnen Sätzen möglicherweise nicht die abstrakte Wissensverarbeitung von Sprachmodellen in längeren Kontexten vollständig erfasst. Weitere Details und Experimente sind im vollständigen Papier zu finden.</sample>
    <sample id="109">Der Vortrag "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor" von Or präsentiert eine innovative Methode zur Erstellung eines umfangreichen und vielfältigen Datensatzes für die Anleitungsschulung von Sprachmodellen, ohne menschliche Arbeitskraft. Traditionelle Ansätze zur Anleitungsschulung stützen sich auf die Umformulierung bestehender NLP-Datensätze oder die manuelle Annotation von Benutzeranfragen, was jedoch auf akademische Benchmarks beschränkt ist oder erhebliche Anstrengungen erfordert. Ors Ansatz nutzt ein Sprachmodell, speziell eine Variante von GPT-3, um automatisch Anweisungen, Eingaben und Ausgaben zu generieren, basierend auf einem kleinen Satz von manuell erstellten Beispielen aus dem Super-Natural Instructions Datensatz. Dieser Prozess erzeugt 64.000 Beispiele, die durch Paraphrasen auf etwa 240.000 Beispiele erweitert werden.

Die Analyse zeigt, dass mehr als 50% der generierten Beispiele korrekt sind, und selbst inkorrekte Beispiele bieten wertvolle Informationen für die Anleitungsschulung. Der Datensatz umfasst kreative und vielfältige Aufgaben, die sich von klassischen NLP-Aufgaben unterscheiden, wie z.B. die Überprüfung der Güte eines wissenschaftlichen Experiments oder die Erfindung eines neuen Wortes. Ein 11-Billionen-Parameter-T5-Modell, das auf Unnatural Instructions trainiert wurde, übertrifft sowohl T0++ als auch Tk-instruct in mehreren Benchmarks und zeigt eine bessere Leistung als ein identisches Modell, das auf Super-Natural Instructions trainiert wurde. Die Ergebnisse unterstreichen die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu generieren, die schwer mit menschlichen Annotatoren zu erreichen sind, und bieten eine kostengünstigere und schnellere Alternative zur manuellen Annotation.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie eine allgemeine Textkorpora sammeln und die Wortfrequenz mit ihr zählen.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute werde ich unser Papier "Do CoNLL-2003 Named Entity Taggers still work well in 2023?" vorstellen. Lassen Sie uns beginnen. Unser Papier untersucht das Problem der Generalisierung anhand der Aufgabe der Named Entity Recognition (NER). Wir stellen fest, dass Modelle, die für CoNLL-2003 entwickelt wurden, fast 20 Jahre lang zur Entwicklung von NER verwendet wurden, was mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten verallgemeinern? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++ Dataset entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und mit denselben CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Wir haben dann über 20 Modelle auf CoNLL-2003 feinabgestimmt. Wir haben sie auf den CoNLL-03-Testsets und CoNLL++ evaluiert und zuletzt den prozentualen F1-Wechsel berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten verallgemeinern. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und zuletzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich zu einer besseren Generalisierung führen. Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, die Überanpassung verursacht, indem dasselbe Testset immer wieder wiederverwendet wird, was sich normalerweise als abnehmende Erträge auf einem neuen Testset manifestiert. Die zweite Hypothese ist der zeitliche Drift, der die Leistungsverschlechterung verursacht, die durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird. Bei der Überanpassung sahen wir, dass die rote beste Anpassungslinie auf der rechten Seite einen Gradienten hat, der größer als eins ist. Das bedeutet, dass jede Verbesserungseinheit, die wir auf CoNLL-2003 erzielt haben, zu mehr als einer Verbesserungseinheit auf CoNLL++ führt, was bedeutet, dass es keine abnehmenden Erträge gibt. Dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wurde. Was ist also mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, bei dem wir einige Modelle mit aktuelleren Daten weiter trainiert oder vortrainiert haben, und wir haben festgestellt, dass die Leistung mit zunehmender zeitlicher Lücke abnimmt, was unsere Hypothese bestätigt, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen. Und das geht Hand in Hand, wir können nicht einfach einen Bestandteil haben und die anderen weglassen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl CoNLL-2003 über 20 Jahre lang verwendet wurde. Also, zurück zur Frage, die wir im Titel unseres Papiers gestellt haben: "Do CoNLL-2003 taggers still work in 2023?" Und wir haben festgestellt, dass die Antwort tatsächlich ein lautes Ja ist. Wir hoffen, dass unser Papier zu mehr Forschung darüber anregt, wie die Generalisierung von Modellen verbessert werden kann. Und zuletzt, schauen Sie sich bitte unser Papier, unseren Datensatz an und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">In der Arbeit "Finding the Pillars of Strength for Multi-Head Attention" von Nanyang Technological University, Singapore, wird die Herausforderung der hohen Parameteranzahl in großen Sprachmodellen adressiert. Diese Modelle, obwohl revolutionär, sind durch ihre Milliarden von Parametern schwer zu implementieren und erfordern lange Trainingszeiten. Die Studie konzentriert sich auf die Optimierung der Redundanz in der Multi-Head Attention, indem sie eine gruppierte Aufmerksamkeit vorschlägt. Diese Methode teilt die Aufmerksamkeitsköpfe in Gruppen auf, wobei intra-gruppale Köpfe ähnlicher und inter-gruppale Köpfe unterschiedlicher werden. Die zweite Phase, das Voting-to-Stay-Algorithmus, prüft und entfernt redundante Köpfe, wodurch bis zu 90% der Parameter eingespart werden können. Die vorgeschlagenen Modelle, GHT und GHT-PS, zeigen auf Aufgaben wie Maschinenübersetzung, Sprachmodellierung und abstrakte Zusammenfassung eine Leistungsverbesserung und erhebliche Kompression. Die Effizienzanalyse zeigt, dass das LITE-Modell 62% schnellere Inferenzgeschwindigkeit und 80% weniger FLOPs bietet. Zukünftige Arbeiten könnten sich auf die automatische, aufgabenbezogene Prüfung konzentrieren, um die Redundanz weiter zu reduzieren, ohne die Leistung zu beeinträchtigen.</sample>
    <sample id="115">Der Ansatz verwendet eine Sprachsegmentgröße von λ (lambda) Sprachrahmen, um zu bestimmen, ob ein Wort basierend auf der Konzentration der Aufmerksamkeit über diese letzten λ Sprachrahmen ausgesendet werden soll.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass "Servin ein Richter ist" und "Kea ein Bäcker ist."</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">Die Präsentation beschreibt die Arbeit "Improving Pretraining Techniques for Code-Switched NLP", die sich mit der Verbesserung von Vorverarbeitungstechniken für code-switched Sprachverarbeitung befasst. Code-Switching, das häufig in sprachlich vielfältigen Gemeinschaften wie Indien vorkommt, beinhaltet das Mischen von Sprachen innerhalb eines Satzes, z.B. "Laptop, mere, bag, me, rakha, hai" (Englisch und Hindi). Multilinguale vortrainierte Modelle wie mBERT und XLM-R sind bei solchen Aufgaben nicht sehr effektiv. Die Hauptbeiträge der Arbeit sind neue Masked Language Modeling (MLM)-Techniken, die speziell für Code-Switching entwickelt wurden, einschließlich SwitchMLM, das nur Übergangspunkte zwischen Sprachen maskiert. Ein Problem von SwitchMLM ist die Abhängigkeit von LID-getaggten Datensätzen, daher wird FrequencyMLM als Ersatz vorgeschlagen. Die Arbeit schlägt auch architektonische Änderungen vor, wie Residualverbindungen, um mehr Übergangsinformationen in den finalen Schichten zu integrieren, und fügt eine LID-basierte Hilfsverlustfunktion hinzu, um die Sprachinformationen in den Zwischenschichten zu verbessern. Die Ergebnisse zeigen, dass die Kombination dieser Methoden die Leistung bei Sentiment-Analyseaufgaben verbessert. Probing-Experimente bestätigen, dass die vorgeschlagenen Methoden die Menge an Übergangsinformationen in den Zwischen- und Endschichten erhöhen. Zusammenfassend wird ein neues MLM-Ziel vorgeschlagen, das speziell für Code-Switching entwickelt wurde, und es wird gezeigt, dass die Methoden die Übergangsinformationen in den Modellen verbessern.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf die Sprachmodelle GPT-4, GPT-Serie, BART-Serie und RoBERTa.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus der Cross-Attention-Mechanismus zwischen Audioeingabe und Textausgabe. Es gibt keine spezifische Erwähnung, dass es Werte aus mehreren Ebenen kombiniert.</sample>
    <sample id="121">Beispiele für direkte Referenz sind das Nennen des Namens eines Songs, wie "Easy on Me", oder die Verwendung seiner Position, wie "the first one".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying und Zhiyang präsentieren ihre Forschung zu MultiInstruct, einem neuen Ansatz zur Verbesserung der Multi-Modal Zero-Shot Learning-Fähigkeiten durch Instruction Tuning. Während frühere Arbeiten sich hauptsächlich auf die Verbesserung der Zero-Shot-Leistung bei Sprachaufgaben konzentrierten, haben sie sich auf die Anwendung von Instruction Tuning auf multimodale Aufgaben konzentriert. Sie identifizierten eine Diskrepanz in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und multimodalen Aufgaben und schufen MultiInstruct, die erste umfangreiche multimodale Anweisungstuning-Benchmark-Datenbank mit 62 vielfältigen Aufgaben in 10 breiten Kategorien. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, wobei jede Aufgabe mit fünf von Experten verfassten Anweisungen ausgestattet ist.

Für ihre Studie verwendeten sie OFA, ein einheitliches multimodales vortrainiertes Modell, und formulierte alle Aufgaben im einheitlichen Sequenz-zu-Sequenz-Format. Sie trainierten das Modell mit 53 Aufgaben aus 9 Gruppen und führten Tests mit einer separaten Gruppe von Aufgaben durch, um die Leistung zu bewerten. Die Leistung wurde durch die Verwendung von fünf verschiedenen Anweisungen für jede Aufgabe gemessen, wobei die Mindest- und Höchstleistung sowie die Standardabweichung berichtet wurden. Sie führten auch eine zusätzliche Bewertungsmetrik namens Sensitivität ein, um die Konsistenz der Modellausgabe bei leichten Variationen der Anweisungen zu messen.

Die Ergebnisse zeigten, dass Instruction Tuning die Leistung von OFA bei gesehenen multimodalen Aufgaben erheblich verbesserte. Transfer Learning von natürlichen Anweisungsdatensätzen verbesserte sowohl die Leistung als auch die Sensitivität des Modells. Die Verwendung mehrerer Anweisungen führte zu einer besseren Gesamtleistung und reduzierte die Sensitivität. Die Forscher planen, eine größere multimodale Anweisungstuning-Datenbank mit etwa 150 zusätzlichen visuellen Sprachaufgaben zu sammeln und zu veröffentlichen.</sample>
    <sample id="124">Tan Qingyu von der National University of Singapore und Alibaba präsentiert die Arbeit "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models". Die Studie untersucht die zeitliche Schlussfolgerungsfähigkeit von Sprachmodellen (LMs) und unterteilt sie in drei Ebenen: Zeit-zu-Zeit, Zeit-zu-Ereignis und Ereignis-zu-Ereignis. Die Forschung zeigt, dass bestehende Studien sich hauptsächlich auf die zweite Ebene konzentrieren, während die Arbeit eine umfassendere Untersuchung anstrebt.

Ein Experiment zur Vorhersage von Jahren zeigt, dass LMs wie T5-L und FLAN-T5-L eine Verzerrung zugunsten des Zeitraums 2000-2020 aufweisen, was mit den Häufigkeiten in den Trainingsdaten korreliert. ChatGPT zeigt eine bessere Leistung bei Jahresvorhersagen, aber seine Leistung sinkt bei Monatsvorhersagen. Um die zeitliche Schlussfolgerungsfähigkeit zu verbessern, schlagen die Autoren das TempReason-Dataset vor, das alle drei Ebenen der zeitlichen Schlussfolgerung abdeckt und die Schwierigkeit von Jahres- auf Monatsvorhersagen erhöht.

Drei QA-Problemstellungen werden evaluiert: Closed Book QA, Open Book QA und Reasoning QA, wobei letztere relevante zeitliche Informationen bereitstellt. Um die Fähigkeiten der LMs zu verbessern, schlagen die Autoren eine Trainingsstrategie mit zwei Komponenten vor: Temporale Spannextraktion und zeit-sensitives verstärkendes Lernen. Das resultierende Modell, TempT5, zeigt signifikante Verbesserungen gegenüber anderen Modellen wie FLAN-T5-L und ChatGPT in verschiedenen Aufgabenstellungen.

Die Studie zeigt, dass ChatGPT in der zeitlichen Schlussfolgerung fehlerhaft ist und dass TempT5, obwohl es die besten Ergebnisse erzielt, immer noch Schwankungen in der Leistung über verschiedene Zeiträume aufweist, was auf Ungleichgewichte in den Trainingsdaten zurückzuführen sein könnte. Zukünftige Arbeiten könnten sich darauf konzentrieren, solche Verzerrungen zu überwinden.</sample>
    <sample id="125">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe des Google Translate API wurde als Baseline in der Translate-Test-Einstellung betrachtet.</sample>
    <sample id="127">Namgyu Ho, ein Masterstudent an der KAIST AI in Korea, präsentiert die Forschungsarbeit "Large Language Models Are Reasoning Teachers", die er gemeinsam mit Laura Schmid und Professor Se-Young Yun verfasst hat. Die Arbeit adressiert die Herausforderung, dass die Technik des chain-of-thought reasoning, die es großen Sprachmodellen wie GPT-3 ermöglicht, komplexe Aufgaben zu lösen, aufgrund hoher Rechen- und Speicheranforderungen nicht auf kleinere Modelle übertragbar ist. Die Lösung besteht darin, große Modelle als "Lehrer" zu nutzen, um ihre Fähigkeiten an kleinere Modelle zu übertragen. 

Das Team entwickelte eine Methode, bei der große Modelle Schritt-für-Schritt-Lösungen für komplexe Aufgaben generieren, die dann als Trainingsdaten für kleinere Modelle dienen. Ein innovativer Ansatz, der Diverse Reasoning genannt wird, erzeugt mehrere unterschiedliche Lösungen für dieselbe Aufgabe durch stochastische Temperatursampling, was die Trainingsqualität der kleineren Modelle verbessert. 

Die Ergebnisse zeigen, dass die Methode, insbesondere mit Diverse Reasoning, die Leistung kleiner Modelle erheblich steigert und sie in der Lage sind, komplexe Aufgaben zu bewältigen, die zuvor nur große Modelle lösen konnten. Die Forschung demonstriert, dass einfache Distillation die Fähigkeiten großer Modelle auf kleinere Modelle mit weniger als 1 Milliarde Parameter übertragen kann, was die Anwendbarkeit in der Praxis erweitert. Die Methode ist skalierbar, erfordert jedoch Abwägungen zwischen Entwicklungskosten und Inference-Kosten.</sample>
    <sample id="128">In "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," Akshatha and Martin introduce a diagnostic test suite to assess the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources. This work, a collaboration between McGill University, Mila, and Microsoft Research, addresses the challenge of combining pretraining-time knowledge with inference-time information. The authors propose a coreference resolution task to evaluate this integration, using a dataset that varies the availability of entity-specific and background knowledge across three settings: Background-Pretrain, Background-Both, and Background-Inference. Their findings reveal that without task-specific training, models struggle to perform well on the KITMUS test, often relying on surface cues. However, with task-specific training, models like C2F and BERT4Coref show significant improvement. Despite this, even the best models face challenges in integrating background knowledge provided solely at inference time. The study highlights the need for models to effectively combine diverse knowledge sources for successful NLU tasks. Further details, including the dataset and code, are available on GitHub.</sample>
    <sample id="129">Die Autoren haben das Beispiel einer "Asian woman" als markierte Gruppe gegeben.</sample>
    <sample id="130">Basierend auf dem Inhalt generalisieren Modelle, die nicht auf der Transformer-Architektur basieren, nicht so gut.</sample>
    <sample id="131">Der Text enthält keine spezifischen Namen für Testdatensätze. Er diskutiert allgemein die Verwendung von "clean test sets" und "noisy validation sets" im Kontext von Weakly Supervised Learning (WSL).</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich Text und Bildern.</sample>
    <sample id="135">James Finch und Sarah Finch stellen ABC-Eval vor, eine neue Methode zur Bewertung von KonversationskI, entwickelt von der Emory NLP Lab unter der Leitung von Professor Jinho Choi in Zusammenarbeit mit Amazon Alexa AI. ABC-Eval zielt darauf ab, die Subjektivität menschlicher Bewertungen zu reduzieren, indem es explizit annotiert, ob ein Modell bestimmte Verhaltensweisen zeigt, wie z.B. irrelevante Informationen oder Selbstwidersprüche. Diese Methode misst, wie oft Chat-Modelle verschiedene thematische Fehler begehen, wie z.B. Ignorieren des Gesprächspartners, Irrelevanz, Selbstwidersprüche, Faktenhalluzinationen oder Mangel an Empathie. 

Um die Wirksamkeit von ABC-Eval zu bewerten, wurden vier führende Chat-Modelle auf 100 menschlich-bot-Konversationen hin untersucht. Diese wurden auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn- und Dialogebene sowie Dialog-Level-Paarvergleiche. ABC-Eval erwies sich als zuverlässiger und informativer als bestehende Methoden, da es eine höhere inter-annotator Übereinstimmung und eine bessere Vorhersage der Gesprächsqualität zeigte. Die Kombination aller ABC-Eval-Metriken erklärt über 25% der Gesprächsqualität, während die Kombination aller Likert-Metriken weniger erklärt. ABC-Eval identifiziert spezifische Fehler wie Sinnverletzungen in 20% der Antworten, Irrelevanz in 15% und Widersprüche in 10%. Die Forscher hoffen, dass ABC-Eval als zuverlässiges Werkzeug zur Bewertung von KonversationskI dient und zur Weiterentwicklung des Feldes beiträgt.</sample>
    <sample id="136">Jasivan präsentierte die Forschung "FERMAT: An Alternative to Accuracy for Numerical Reasoning" an der University of Sheffield, die er mit seiner Supervisorin Nafise durchführte. Die Arbeit untersucht die Herausforderungen bei der numerischen Verarbeitung von Sprachmodellen, insbesondere bei Aufgaben wie der Faktensuche, bei denen die Genauigkeit von Zahlen entscheidend ist. Ein Beispiel ist die Überprüfung von Aussagen mit Tabelleninformationen, wie die Berühmtheit von Chris Brown im Alter von 16 Jahren, was die Subtraktion von Zahlen erfordert. Größere Sprachmodelle (ab 10 Milliarden Parametern) zeigen bessere Leistungen, aber die zugänglicheren Modelle mit 3 Milliarden Parametern haben Schwächen in der numerischen Verarbeitung.

Die aktuellen Benchmarks, die hauptsächlich Genauigkeit und F1-Maße verwenden, sind nicht ausreichend, um die mathematischen Fähigkeiten der Modelle zu bewerten. FERMAT, eine flexible Evaluationsmenge, wurde entwickelt, um diese Lücke zu schließen. Es basiert auf mathematischen Fragen aus Illinois und CommonCore, die in verschiedenen numerischen Darstellungen und mathematischen Operationen präsentiert werden. Eine Zero-Shot-Bewertung zeigte, dass die meisten Modelle in diesen Aspekten schlecht abschneiden.

Durch das Schreiben von Vorlagen durch Mathematiklehrer und das Generieren von 200.000 Beispielen wurde eine Verbesserung der Leistung nach der Feinabstimmung festgestellt. Die Untersuchung der Abhängigkeit von Trainingsdaten zeigte, dass Modelle nicht einfach durch Memorierung lernen, sondern von der sprachlichen Kontextualisierung abhängen. Die Analyse der Auswirkungen von Trainingsvorlagen ergab, dass sprachliche und mathematische Vielfalt die Leistung verbessert.

Zusammenfassend zeigt die Forschung, dass bestehende Benchmarks unzureichend sind und dass FERMAT eine wertvolle Alternative bietet, um die numerischen Fähigkeiten von Sprachmodellen besser zu bewerten. Verbesserungen in der Zahlencodierung und Tokenisierung sind ebenfalls notwendig.</sample>
    <sample id="137">Der Artikel "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" von Sicong von der Singapore University of Technology and Design, veröffentlicht in ACL 2023, präsentiert ein neues Forschungsgebiet, das sich auf die Generierung von Grundrissen basierend auf natürlichsprachlichen Anweisungen konzentriert. Während textbedingte generative KI-Modelle beeindruckende Ergebnisse bei der Erstellung von hochwertigen Bildern erzielen, liegt der Fokus oft auf der Generierung von Kunstwerken. Der Artikel hebt die Notwendigkeit hervor, Designs zu generieren, die spezifische Anforderungen erfüllen, die in natürlicher Sprache definiert sind, insbesondere im Kontext der Architektur.

Das Ziel ist es, Benutzern ohne Expertise die Möglichkeit zu geben, durch sprachliche Anweisungen an der Gestaltung teilzunehmen. Die Aufgabe besteht darin, aus einer Reihe von natürlichsprachlichen Anweisungen, die die Semantik, Geometrie und Topologie eines Grundrisses beschreiben, plausible 2D-Grundrissdesigns zu generieren. Das Tell2Design-Dataset, das aus 5.051 menschlich annotierten und 76.000 künstlich generierten Anweisungen besteht, wird verwendet, um dieses Problem zu adressieren.

Die Hauptherausforderungen sind die strikteren Einschränkungen im Vergleich zur Kunstgenerierung, das Verständnis des Gesamtbildes aus unstrukturierten Texten und die Handhabung von mehrdeutigen oder unvollständigen Anweisungen. Im Gegensatz zu bestehenden Methoden, die die Generierung von Grundrissen als Regression von Rechtecken behandeln, wird die Aufgabe als sequenzbasiertes Problem unter einem Encoder-Decoder-Framework modelliert. Dies ermöglicht die Handhabung verschiedener Anweisungslängen und -komplexitäten.

Ein Transformer-basiertes Encoder-Decoder-Modell, initiiert mit dem T5-Modell, wird verwendet, um die Anweisungen in eine Sequenz von Raumkoordinaten umzuwandeln. Die Ergebnisse zeigen, dass das Modell auf dem Tell2Design-Dataset mit hohen IoU-Scores übertrifft, insbesondere wenn künstliche Anweisungen zur Vorwärmung verwendet werden, bevor menschliche Anweisungen trainiert werden. Dies deutet auf einen Vorteil der Kombination von künstlichen und menschlichen Anweisungen während des Trainings hin.

Der Artikel schließt mit der Feststellung, dass die Forschung auf diesem Gebiet noch in den Anfängen steht und hofft, dass das vorgestellte Dataset und Modell zukünftige Forschungen in der sprachgesteuerten Designgenerierung anregen.</sample>
    <sample id="138">Ein zu wenig erforschtes Gebiet im Bereich der NLU ist die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren, insbesondere die Integration von Hintergrundwissen, das nur zur Inferenzzeit verfügbar ist.</sample>
    <sample id="139">Ying and Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Crowd-sourced Arbeiter wurden beauftragt, fehlerhafte Beispiele in der Validierungs- und Testmenge zu finden und zu korrigieren.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzung haben mehrere Grenzen: 

1. **Korpus-Level-Metriken**: Metriken wie BLEU können kontextabhängige Übersetzungen nicht gut erfassen, da nur ein kleiner Teil der Übersetzungen kontextabhängig ist.

2. **Begrenzte Typen und Sprachen**: Ressourcen für zielgerichtete Bewertungen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und Sprachen, da sie oft auf Domänenwissen und menschlicher Kuratierung basieren.

3. **Mangel an umfassenden Benchmarks**: Es fehlen umfassende Benchmarks, die verschiedene Diskursphänomene und Sprachen abdecken, was die Bewertung der Fähigkeit von Modellen zur kontextbasierten Übersetzung erschwert.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit an "Resolving Indirect Referring Expressions for Entity Selection" sprechen, in der wir die AltEntities Corpus einführen. Mein Name ist Javad Hosseini, und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: "Meintest du 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte der Benutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Möglichkeit ist die direkte Referenz, zum Beispiel durch den Namen des Liedes "Easy on Me" oder seine Position, "das erste". Manchmal ist jedoch eine indirekte Referenz angemessener, um eine natürlichere Konversation zu führen. Dies könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr erinnern kann. Oder die Aussprachen sind zu ähnlich, um sie zu unterscheiden. Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "das neuere" oder "das Lied, das nicht energiegeladen ist." Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung des Verständnisses von Entitäten durch LLMs. Wir sind uns nicht bewusst, dass es eine größere öffentliche Datensammlung für diese Aufgabe gibt, daher sammeln wir eine mit Hilfe von Crowd-Annotation. Unsere Datensammlung umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensammlungsmethodik betont die Informalität mit einem Cartoon-Setup. Der Cartoon hat drei Sprachblasen. In der ersten Sprachblase sagt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?" Und mit dieser Aussage setzt Bob den Dialogkontext. In der zweiten Sprachblase sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Dies ist die alternative Frage. Und in der dritten Sprachblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel "das neuere." Wir liefern die ersten und zweiten Sprachblasen automatisch, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprachblase wird aus einigen manuellen Prompts pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer ein einfaches Template: Meinst du A oder B? Wo A und B Beispiele aus Wikipedia sind. Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Unterscheidung zu treffen. Die erste ist zufällig einheitlich. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel, das gleiche Genre oder der gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie die Namen dieser Entitäten, aber sie kennen die Entitäten nicht unbedingt. Was wir also tun, ist, dass wir den Annotatoren einige Hintergrundinformationen über die beiden Entitäten zeigen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatoren, zumindest ein wenig von jedem Lied anzuhören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied "Easy on Me." Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Referenzausdrücken zu beschreiben. Zum Beispiel "das mit dem Klavierspiel." Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "das ohne Worte", "nicht das mit dem 12-jährigen Jungen" oder "das fiktive" oder "kommt aus Aserbaidschan" und so weiter. Die AltEntities Corpus umfasst 6.000 alternative Fragen in drei Domänen und 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5 XL Modell sind unten zusammengefasst. Wenn das Sprachmodell Zugriff auf die exakt gleichen Hintergrundinformationen wie die Annotatoren hat, ist die Genauigkeit sehr hoch, etwa 92 bis 95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugriff auf teilweise überlappende Hintergrundinformationen hat, liegt die Genauigkeit zwischen 82 und 87%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugriff auf die Namen der Entitäten hat, liegt die Genauigkeit nur bei 60%, also gibt es viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle domänenübergreifend einsetzbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den folgenden bestehenden SimulST-Richtlinien verglichen: der Wait-k-Strategie, der Local Agreement-Strategie und dem state-of-the-art Architekturansatz, der speziell für simultane Vorübersetzung entwickelt wurde.</sample>
    <sample id="144">The authors are affiliated with Nantes University Hospital.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, ein PhD-Student von Fudan University, präsentiert ein Papier über die Analyse von Auslassungen in der Dialogzusammenfassung. Dialogzusammenfassung, ein Unterbereich der Textzusammenfassung, zielt darauf ab, wichtige Informationen aus Dialogen prägnant darzustellen. Trotz Fortschritten mit großen vortrainierten Sprachmodellen leiden diese Zusammenfassungen oft unter Fehlern, insbesondere Auslassungen, die zu unvollständigen Zusammenfassungen führen. Yichengs Analyse zeigt, dass etwa 70% der generierten Zusammenfassungen Auslassungen aufweisen, was das Problem als allgemein und ernsthaft kennzeichnet. Die Studie untersucht die Verteilung von Auslassungen in Dialogen und stellt fest, dass diese zufällig verteilt sind, was die Identifizierung von Schlüsselinformationen erschwert. Um das Problem zu analysieren und zu lösen, definiert die Studie eine Aufgabe zur Erkennung von Auslassungen auf Satzebene und erstellt das OLDS-Dataset, das hochwertige Auslassungsetiketten für Dialogzusammenfassungen bereitstellt. Verschiedene Modelle und Decodierungsstrategien werden verwendet, um Kandidatenzusammenfassungen zu generieren, und eine automatische Methode wird vorgeschlagen, um Auslassungsetiketten zu erstellen, die durch menschliche Bewertung validiert werden. Die Studie evaluiert verschiedene Modelle zur Erkennung von Auslassungen und zeigt, dass die Leistung mit einem F1-Score von etwa 50% herausfordernd ist. Eine Post-Editing-Methode zur Zusammenfassungsverbesserung, die Auslassungsinhalte integriert, zeigt eine deutliche Leistungssteigerung, was die Bedeutung der Erkennung von Auslassungen und deren Nutzung zur Qualitätsoptimierung unterstreicht.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler, und ich werde kurz das Papier "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SimulST, ist der Prozess der Echtzeit-Übersetzung gesprochener Sprache in Text in einer anderen Sprache, um die zwischenmenschliche Kommunikation über Sprachgrenzen hinweg zu ermöglichen. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wobei zusätzliche Module optimiert werden müssen. Lange und komplexe Trainingsverfahren, zum Beispiel Training mit unterschiedlichen Optimierungszielen. Und das Training und die Wartung mehrerer Modelle, um unterschiedliche Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Also, was ist unsere Lösung? Zuerst, bestehende Offline-ST-Modelle ohne Neutrainieren oder die Annahme spezifischer Architekturen für SimulST zu verwenden. Nur ein Modell für jedes Latenzregime zu verwenden und die Latenz durch spezifische Parameter zu handhaben. Und das Wissen zu nutzen, das das Modell durch die Aufmerksamkeitsmechanismen zwischen Audioeingabe und Textausgabe erworben hat. Das ist der Querverweismechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung ist es, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob eine teilweise Übersetzung ausgegeben werden soll oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, wenn ihre Summe unter einem bestimmten Schwellenwert alpha für die letzten lambda Sprachframes liegt, was bedeutet, dass die empfangenen Informationen stabil genug sind. Zum Beispiel, wenn wir einen Sprachchunk erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Querverweisgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Sprachframes zeigt, als lambda Sprachframes. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während das letzte Wort, da die Summe der Querverweis-Aufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, nicht ausgegeben wird und wir auf einen weiteren Sprachchunk warten. Wenn wir fortfahren und einen weiteren Sprachchunk erhalten, und unser Modell drei weitere Wörter vorhersagt, und wir uns die Querverweisgewichte dieser Wörter ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir die simultanen Sprachübersetzungsergebnisse in Diagrammen darstellen, in denen wir auf der einen Seite den BLEU-Wert haben, der die Übersetzungsqualität misst, und die durchschnittliche Verzögerung, die das Latenzmaß ist, und wir berücksichtigen auch die rechenzeitbewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven so hoch wie möglich in diesem Diagramm sind. Wir möchten auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf Offline-Modellen angewendet werden, wie die Wait-k-Strategie und die Lokale Übereinstimmung. Und wir vergleichen auch mit dem aktuellen Stand der Technik in Architekturen, die speziell für simultane Vorübersetzung entwickelt wurden. Das sind alle Ergebnisse der simultanen Sprachübersetzungstrategie auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächlich verstrichene Zeit oder die rechenzeitbewusste Zeit berücksichtigen, dass dies die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier. Und wir haben auch den Quellcode und die Modelle sowie die simultane Ausgabe offen veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Der Text gibt keine Informationen darüber, ob der CoNLL++ Datensatz öffentlich zugänglich ist.</sample>
    <sample id="150">Die Präsentation von Archiki stellt das ACL-Papier "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" vor, das eine neue Datenbank für die Frage-Antwort-Extraktion aus Meeting-Transkripten einführt. Meeting-Transkripte sind oft lang, domänenspezifisch und informationsreich, was sie zu einem einzigartigen Forschungsgebiet für die natürliche Sprachverarbeitung (NLP) macht. Bisherige Arbeiten konzentrierten sich hauptsächlich auf Zusammenfassungen und die Extraktion von Aufgabenpunkten, wodurch die Frage-Antwort-Komponente in Meetings untergenutzt blieb. MEETINGQA adressiert diese Lücke, indem es ein neues Dataset mit Fragen und zugehörigen Antworten aus Meetings bereitstellt.

Das Dataset basiert auf dem AMI-Korpus mit fast 100 Stunden transkribierter mehrseitiger Meetings. Die Datensammlung umfasst die Auswahl von Fragen basierend auf Satzzeichen und das Filtern kurzer Fragen. Annotatoren markieren die Antwortabschnitte, was zu einer hohen Übereinstimmung (Krippendorff's Alpha von 0,73) führte. MEETINGQA enthält 7.700 Fragen, wobei 30% unbeantwortbar sind. Von den beantwortbaren Fragen haben 40% mehrere Antwortabschnitte und 48% mehrere Sprecher. Die Fragen sind oft offen und suchen nach Diskussionen, wobei 20% rhetorisch sind und 70% der mehreren Sprecherantworten Meinungsverschiedenheiten enthalten.

Die Studie zeigt, dass Fragen und Antworten im Dataset durchschnittlich 12 bzw. 35 Wörter umfassen. Die menschliche Leistung auf dem Testset erreicht eine F1-Score von 84,6. Verschiedene Modelle wurden getestet, darunter kurze Kontextmodelle mit Kontext-Retrieval und Einzel- sowie Mehrfach-Antwortvarianten. Die Ergebnisse zeigen, dass kurze Kontextmodelle wie RoBERTa besser abschneiden als lange Kontextmodelle wie Longformer. Die Leistung von Mehrfach-Antwortmodellen ist ähnlich oder leicht schlechter als die von Einzel-Antwortmodellen.

In der Zero-Shot-Einstellung gibt es eine signifikante Leistungslücke zwischen Modellen und menschlicher Leistung, die durch die Verwendung von Silberdaten für die Datenvergrößerung teilweise geschlossen werden kann. Die Fehleranalyse zeigt, dass Modelle Schwierigkeiten haben, rhetorische Fragen zu identifizieren und die richtigen Sprecher für Antworten zu bestimmen. MEETINGQA stellt somit eine herausfordernde und ungelöste Ressource für die Frage-Antwort-Extraktion in realen Meeting-Szenarien dar.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und wir werden unsere Forschung zu MultiInstruct vorstellen, die Multi-Modale Zero-Shot-Lernen durch Anweisungsschulung verbessert. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen erforscht, um vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben auf eine parametrische und dateneffiziente Weise wiederverwendbar zu machen. Kürzlich haben viele Studien gezeigt, dass Anweisungsschulung große Sprachmodelle befähigt, auf unerkannte Aufgaben in einem Zero-Shot-Modus zu arbeiten, indem sie natürliche Anweisungen befolgen. Dennoch konzentrierten sich die meisten vorherigen Arbeiten zur Anweisungsschulung darauf, die Zero-Shot-Leistung bei sprachbasierten Aufgaben zu verbessern, während die Computer Vision und die Multi-Modale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob die Anweisungsschulung eines multi-modalen vortrainierten Modells tatsächlich die Generalisierung auf unerkannte multi-modale Aufgaben verbessern kann. Darüber hinaus stellten wir bei unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und Multi-Modal fest. Es gibt mehr als 1600 sprachbasierte Anweisungsaufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multi-modalen Anweisungsaufgabe. Daher motiviert uns dies, einen multi-modalen Anweisungsschulungsdatensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multi-modalen Anweisungsschulungsbenchmark-Datensatz, der 62 vielfältige multi-modale Aufgaben umfasst, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Um die multi-modale Anweisungsschulung auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein einheitliches multi-modales vortrainiertes Modell, als Basismodell. OFA verwendet eine einheitliche Vokabel für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatenarten zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben im einheitlichen Sequenz-zu-Sequenz-Format. Dabei werden die Eingabetexte, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt. Nun werde ich über die multi-modale Anweisungsschulung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und proben 10.000 Instanzen pro Aufgabe. Für das Testen reservieren wir die gesamte Gruppe für allgemeines Wissen für das Testen und wählen zusätzlich 5 Aufgaben aus den VQ- und Miscellaneous-Gruppen aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Zusätzlich proben wir zufällig 20 Aufgaben aus dem Testsplit der natürlichen Anweisungen als unerkannte Aufgabe für NLP. Wir verwenden das vortrainierte große OFA-Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über die Mindest- und Höchstleistung und die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multi-modale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multi-modale Generierungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch ein zusätzliches Evaluationsmaß ein, das Sensitivität genannt wird. Dies misst die Fähigkeit des Modells, konsistent die gleichen Ausgaben für die gleiche Aufgabe zu produzieren, unabhängig von leichten Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Anweisungsschulung die Leistung von OFA bei erkannten multi-modalen Aufgaben erheblich verbessern. Auch das Transferlernen von natürlichen Anweisungsdatensätzen kann der Anweisungsschulung zugutekommen. Wie wir sehen können, verbessert sich die Leistung des Modells, wenn die Anzahl der Aufgaben zunimmt, und gleichzeitig sinkt die Sensitivität. Wir führten auch ein Experiment durch. Wir verwenden eine Anweisung gegenüber 5 Anweisungen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Modellsensitivität. Wie wir sehen können, kann das Transferlernen von natürlichen Anweisungsdatensätzen dem Modell eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell verleihen. Wir können auch sehen, dass das Transferlernen von natürlichen Anweisungsdatensätzen OFA dabei hilft, eine viel bessere Leistung auf dem natürlichen Anweisungsdatensatz zu erreichen. Insgesamt schlagen wir den ersten großen Multi-Modalen Anweisungsschulungsdatensatz vor, der die kurzfristige Fähigkeit von OFA erheblich verbessert, und wir erforschen verschiedene Transferlern-Techniken und zeigen deren Vorteile. Wir entwerfen ein neues Maß, das Sensitivität genannt wird. Noch etwas, wir sammeln einen viel größeren Multi-Modalen Anweisungsschulungsdatensatz mit rund 150 zusätzlichen Vision-Language-Aufgaben und werden sie veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider präsentiert die Arbeit an der Schnittstelle von NLP und klassischer Philologie, insbesondere die Entwicklung neuer Sprachmodelle für Altgriechisch und Latein. Trotz der Einführung von spezialisierten Modellen wie Latin BERT und Ancient Greek BERT gibt es noch Herausforderungen, insbesondere bei der Multilingualität und der Leistungsbewertung. Die Forschung zielt darauf ab, vergleichbare Modelle zu schaffen, den Stand der Technik zu verbessern, verschiedene Architekturen zu erkunden und multilinguale Modelle zu entwickeln.

Zwei monolinguale Modelle für Altgriechisch, GreBERTa und GreTa, wurden entwickelt. GreBERTa basiert auf der RoBERTa-Architektur, während GreTa ein encoder-decoder Modell auf der T5-Architektur ist. Zudem wurden die multilingualen Modelle PhilBERTa und PhilTa entwickelt, die auf Altgriechisch, Latein und Englisch trainiert wurden.

Für die Datensammlung nutzten die Forscher Open Greek &amp; Latin und entwickelten ein neues Korpus aus dem Internet Archive, indem sie OCR-Fehler zur Identifizierung von Altgriechisch-Texten nutzten. Die Modelle wurden anhand von Aufgaben wie POS-Tagging, Abhängigkeitsanalyse und Lemmatisierung bewertet und zeigten eine überlegene Leistung gegenüber bestehenden Modellen.

Besonders hervorzuheben ist die Leistung der encoder-decoder Modelle bei der Lemmatisierung, die die bestehende Leistung um 5 Prozentpunkte übertraf. Untersuchungen zur semantischen und weltlichen Wissensfähigkeit der Modelle zeigten, dass die multilingualen Modelle die monolingualen nicht signifikant übertrafen.

Insgesamt präsentiert die Forschung leistungsstarke, neu initialisierte Sprachmodelle für die klassische Philologie, die sowohl monolinguale als auch multilinguale Fähigkeiten bieten und durch ein hochwertiges Altgriechisch-Korpus unterstützt werden.</sample>
    <sample id="153">Ninareh Mehrabi presented work on resolving ambiguities in text-to-image generative models. The study focuses on ambiguities in prompts, such as "The girl enters the room with flowers," which can be interpreted in multiple ways. The goal is to create frameworks to mitigate these ambiguities and evaluate if generated images align with user intentions.

The approach involves curating a benchmark dataset from the LAVA corpus, covering various ambiguity types. A prompt disambiguation framework uses a language model to either generate clarifying questions or propose different visual interpretations. Users respond to these, resulting in disambiguated prompts.

The disambiguated prompts are then input into text-to-image models to generate images. An automatic evaluation framework assesses image faithfulness to user intentions using a Visual Question Answering (VQA) model. The VQA model checks if the user's intention is satisfied in the generated image.

Findings indicate disparities in resolving different ambiguity types, but overall, the framework positively impacts faithful image generation. The automatic evaluation framework aligns with human evaluations, making it a reliable tool for assessing text-to-image models. The work concludes by emphasizing the importance of addressing ambiguities to improve the alignment of generated images with user intentions.</sample>
    <sample id="154">University of Trento and Foundazione Bruno Kessler.</sample>
    <sample id="155">"Easy on Me" oder "I Gotta Feeling" (abhängig von der indirekten Referenz, die verwendet wird).</sample>
    <sample id="157">Shen Gao von der Shandong University präsentiert die Arbeit "Dialogue Summarization with Static-Dynamic Structure Fusion Graph", eine Zusammenarbeit mit Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao und Rui Yan. Die Arbeit konzentriert sich auf die Herausforderung der Dialogzusammenfassung, bei der wichtige Informationen aus einem Dialog in eine kurze Zusammenfassung destilliert werden. Die bestehenden Methoden zur Dialogzusammenfassung nutzen vorab berechnete statische Graphstrukturen, die jedoch von der Zuverlässigkeit externer linguistischer Werkzeuge abhängen und nicht dynamisch anpassbar sind. Das vorgestellte SDDS-Modell umfasst vier Hauptkomponenten: einen Utterance Encoder, eine statische Graphenkonstruktion, einen Static-Dynamic Graphenmodul und einen Summary Generator, der auf einem vortrainierten Sprachmodell basiert. Der Static-Dynamic Graphenmodul kombiniert mehrere statische Graphen und verwendet einen dynamischen Graphenmodul, um semantische Beziehungen basierend auf tiefen Vektorrepräsentationen zu erfassen. Die statische Dialogstruktur wird durch vier heuristische Methoden modelliert, darunter ein Diskursanalysegraph und eine Sprecherinteraktionsmatrix. Die dynamische Graphenkomponente verwendet ein Multi-Head-Attention-Modell, um Beziehungen zu berechnen. Schließlich wird eine Fusion der statischen und dynamischen Graphen durchgeführt, um eine einheitliche Graphenrepräsentation zu schaffen, die in den Generierungsprozess integriert wird. Der Code und die Daten sind auf GitHub verfügbar.</sample>
    <sample id="158">Qipeng Guo von AWS präsentierte die Arbeit "Dual Cache for Long Document Neural Coreference Resolution". Die Aufgabe der Koreferenzauflösung besteht darin, Erwähnungen von Entitäten in einem Dokument zu identifizieren und sie zu gruppieren, wenn sie dieselbe Entität bezeichnen. Traditionelle Methoden erfordern die Paarung aller Erwähnungen, was zu quadratischer Komplexität führt. Kürzlich vorgeschlagene cachebasierte Methoden reduzieren dies auf lineare Komplexität, indem sie eine feste Größe Cache verwenden. Bei voller Kapazität werden Entitäten mit einer LRU-Politik (Least Recently Used) entfernt. In langen Dokumenten kann dies jedoch zu hohen Cache-Fehlern führen, da Themen häufig wechseln und Erwähnungen weit verstreut sind.

Um dieses Problem zu lösen, schlagen die Autoren einen Dual Cache vor, bestehend aus einem lokalen und einem globalen Cache. Der lokale Cache verwendet die LRU-Politik für lokale Entitäten, während der globale Cache die LFU-Politik (Least Frequently Used) für globale Entitäten verwendet. Der Dual Cache scannt das Dokument von links nach rechts, klassifiziert neue Erwähnungen und bewertet deren Häufigkeit, um sie entsprechend dem Cache hinzuzufügen. Bei voller Kapazität werden Entitäten gemäß der jeweiligen Eviktionspolitik entfernt.

Die Leistung des Dual Caches wurde auf vier öffentlichen Benchmarks evaluiert, wobei er die Baseline-Methoden übertraf, selbst bei ungebundenem Speicher. Besonders bei Buchdokumenten mit 30.000 Wörtern zeigte der Dual Cache eine deutlich bessere Leistung und reduzierte die Cache-Fehler signifikant im Vergleich zu einem einzelnen Cache. Der Dual Cache bietet das beste Verhältnis von Leistung zu Kosten unter den cachebasierten Modellen. Zusammenfassend bietet der Dual Cache eine effiziente Lösung für die Koreferenzauflösung in langen Dokumenten, indem er lokale und globale Entitäten getrennt speichert und die Cache-Fehler reduziert.</sample>
    <sample id="159">Hallo, alle zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserer Diskussion über unser ACL 2023 Papier willkommen zu heißen. Sprachmodellakzeptabilitätsurteile sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit setzen wir uns mit den minimalen Paar-Paradigmen auseinander. Das minimale Paar-Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen, die auch Grammatikalität wie bei BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotype wie bei CrowS Pairs umfassen können. Im typischen minimalen Paar-Paradigma wird ein Sprachmodell einem akzeptablen oder grammatikalischen Satz und einem akzeptablen oder ungrammatikalischen Satz gezeigt. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Der aktuelle MPP-Pipeline erlaubt es uns nicht, die Akzeptanz eines Modells gegenüber längeren Sätzen zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern. Es ist daher entscheidend, dass wir die Modelle hinsichtlich ihrer Akzeptanz über das gesamte Kontextfenster hinweg bewerten, und das ist unser Ziel. Wir versuchen, den MPP-Pipeline zu überarbeiten, indem wir das Modell auffordern, die Akzeptanz über längere und längere Sequenzen zu bewerten. Dazu simulieren wir längere Sequenzen, indem wir die Datensätze selbst überarbeiten und Sätze neu erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir ein typisches Paar aus dem BLiMP-Datensatz im Fall der Adjunct Island ausgewählt. Um längere Sequenzen zu erstellen, die akzeptabel sind und die gleiche grammatische Struktur aufweisen, extrahieren wir grammatikalische Sätze aus Adjunct Island und fügen sie als Präfix sowohl zum akzeptablen als auch zum unakzeptablen Anfrage-Satz hinzu. Das Gleiche können wir tun, indem wir unakzeptable Sätze aus der gleichen Struktur auswählen, um die Akzeptanz der Modelle zu testen. Wir können auch Sätze aus einem anderen Subset oder einem anderen Datensatz wählen. Das nennen wir die Mismatch-Situation. Die Sätze stammen immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, mit dem wir bewerten. Das Gleiche können wir für den Unakzeptabilitätsfall tun. Schließlich können wir Sätze aus einem völlig unverwandten Bereich wie Wikipedia wählen. Das zeigt uns, ob die Akzeptanzurteile der Modelle tatsächlich durch den Kontext beeinflusst werden, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie schneidet das Modell ab? Zuerst schauen wir uns die Wikipedia-Sätze an, die völlig irrelevant für das aktuelle Anfragepaar sind, und finden heraus, dass die MPP-Urteile bei zufälliger Kontextlänge größtenteils robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT-2-Modelle auszureizen. Dort sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz wählen? Hier erstellen wir Sätze aus akzeptablen und unakzeptablen Domänen aus demselben BLiMP- oder SyntaxGym-Datensatz. Dort sehen wir, dass die MPP-Urteile erheblich steigen oder fallen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Wenn wir die Struktur abgleichen, also Sätze aus demselben Phänomen in BLiMP oder SyntaxGym wählen, sehen wir einen massiven Anstieg oder Abfall der MPP-Urteile für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß und nimmt mit zunehmender Kontextlänge zu, was wahrscheinlich neue Sprachmodelle mit großen Kontextfenstern beeinflussen wird. Warum beeinflusst das abgestimmte Präfix das Urteil des Sprachmodells so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabesatz zu stören, indem wir die relevante Struktur beibehalten, aber Rauschen zur Eingabe hinzufügen. Nach mehreren solcher Störungen stellen wir fest, dass keines dieser Rauschen das Modell dazu bringt, seine Meinung in Bezug auf die MPP-Urteile zu ändern. Wir stellen fest, dass die Modelle empfindlich auf die gestörten Sätze in ähnlicher Weise reagieren. Das heißt, wenn wir Sätze aus dem akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir Sätze aus dem unakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die sich über Sätze hinweg teilen. Die derzeitige MPP-Bewertung mit kurzen und einzelnen Satzeingaben mag das abstrakte Wissen der Sprachmodelle über das gesamte Kontextfenster hinweg nicht vollständig erfassen. Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem unsortierten Multiset von Tokens zugeordnet, die im Output erscheinen werden.</sample>
    <sample id="161">55,000 Skripte sind in CoScript vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist die Methode von MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen (WSL) besteht darin, dass es die Kosten und den Aufwand für die manuelle Annotation von Daten reduziert, indem es auf schwache Labelquellen wie einfache Heuristiken, Wissensbasen oder niedrigqualitative Crowdsourcing zurückgreift. Dies macht die Annotationen viel günstiger, obwohl sie auch fehleranfällig sind. WSL-Methoden zielen darauf ab, robuste Modelle zu trainieren, die trotz dieser Labelrauschen gut generalisieren können.</sample>
    <sample id="165">Wenting Zhao präsentiert das Papier "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations", das eine neue Methode zur abduktiven Schlussfolgerung ohne Überwachung einführt. Abduktive Schlussfolgerung beginnt mit einem Kontext (z.B. "Emily war im Stau") und endet mit einem Ergebnis (z.B. "Emily kam zu ihrem Flug"), wobei das Ziel darin besteht, eine plausible Erklärung zu finden, die die Lücke zwischen Kontext und Ergebnis schließt. Traditionelle Methoden erfordern die Annotation plausibler Erklärungen, die jedoch subjektiv und fehleranfällig sind. Zhao stellt die Frage, ob abduktive Schlussfolgerung ohne Überwachung möglich ist, und bejaht dies mit der Einführung von LiPoR (Likelihood Learning with Posterior Regularization). LiPoR behandelt Erklärungen als latente Variablen und maximiert die marginale Wahrscheinlichkeit des Ergebnisses gegeben den Kontext, ohne die Plausibilität der Erklärungen zu kennen. Um plausible Erklärungen zu bevorzugen, verwendet LiPoR eine Regularisierung, die die gegenseitige Ausschließlichkeit der Erklärungen nutzt. Die Regularisierung zielt darauf ab, die Anzahl der wahrscheinlichen Erklärungen zu begrenzen. LiPoR übertrifft bestehende Modelle, einschließlich eines starken GPT-3-Zero-Shot-Baselines, um über 4 absolute Punkte in der Genauigkeit auf dem AlphaNLI-Dataset.</sample>
    <sample id="166">Dieses Papier präsentiert einen neuartigen Ansatz, den Neural Divide-and-Conquer Reasoning Framework (NDCR), zur Verbesserung der Bildabfrage aus sprachlich komplexen Texten. Die Herausforderung besteht darin, dass typische visuelle Sprachmodelle bei komplexen Texten an Leistung verlieren, da sie sich hauptsächlich auf analoges Denken (System 1) konzentrieren. Der NDCR nutzt die Divide-and-Conquer-Strategie und die Dual-Process-Theorie, um sowohl analoges als auch logisches Denken (System 2) zu integrieren. Der Ansatz umfasst drei Hauptkomponenten: den Proposition Generator, der komplexe Texte in einfache Propositionen zerlegt; den Visual-Linguistic Interactor, der visuelle und propositionale Informationen analog verarbeitet; und den Neural-Symbolic Reasoner, der logische Operationen wie Negation und Konjunktion durchführt, um die endgültige Lösung zu erzielen. Experimentelle Ergebnisse zeigen, dass der NDCR andere Baseline-Methoden übertrifft und die Wirksamkeit jedes Moduls durch Abolitionsexperimente bestätigt wird. Der Ansatz demonstriert interoperables Verarbeiten und bietet Einblicke in die Verbesserung der kompositionalen Schlussfolgerung und Planung großer Sprachmodelle durch die Integration von Dual-Process-Theorie und Divide-and-Conquer-Strategien.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet. Es wurden insgesamt 750 Dokumente ausgerichtet, wobei die genaue Aufteilung zwischen manueller und automatischer Ausrichtung nicht spezifiziert wurde.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem Daten von Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen Annotationen wie im CoNLL-2003-Datensatz versehen wurden.</sample>
    <sample id="169">David Vilar reviews the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," co-authored with colleagues from Google Translate. The paper explores the use of the PaLM language model, a 540 billion-parameter model trained on 780 billion tokens, for machine translation. It presents the first systematic study of large language model prompting for this task, evaluating PaLM's translation capabilities using best practices from the machine translation (MT) community. The study uses the latest test sets to avoid data overlap and compares results with state-of-the-art systems, including the WMT evaluation.

The research highlights the significant impact of prompting strategies on PaLM's translation performance. Experiments showed that different prompts could lead to more than one BLEURT point difference, with extreme cases showing up to 40 BLEURT points. A 5-shot prompting strategy, marking sentences with their respective languages, was found effective, with minimal impact from the form of the prompt itself.

The study concludes that example quality is more crucial than similarity to the source sentence. Using high-quality examples from curated development data (dev data) yielded better performance than using noisier training data. While specialized state-of-the-art systems still outperform PaLM, the latter closely approaches commercial systems like Google Translate. Human evaluations using the MQM framework revealed that PaLM's fluency is comparable to state-of-the-art systems, but it struggles with accuracy, often omitting parts of the source sentence. Despite these issues, PaLM's translations are generally more fluent, with fewer "Style/Awkward" errors compared to other systems.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Semantische Parsing ist die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir Anfragen in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL und so weiter übersetzen. Bestehende cross-linguale semantische Parsing-Modelle wurden separat für Datensätze mit begrenzten Aufgaben und Anwendungen vorgeschlagen und evaluiert. Zum Beispiel gibt es viele Abdeckungen für bestimmte natürliche Sprachen. Aber Chinesisch fehlt und es gibt eine unzureichende Abdeckung für bestimmte Bedeutungsrepräsentationen. Der Lambda-Kalkül fehlt oder sie wurden nur für bestimmte neuronale Modelle evaluiert. Zum Beispiel gibt es nur ein einzelnes Modell zur Bewertung. Daher schlagen wir XSemPLR vor. Wir stellen ein einheitliches Datenset XSemPLR für cross-linguales semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Es enthält 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Trainings- und Evaluierungsszenarien. Das erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell zum Training und zur Evaluation. Zum Beispiel trainieren wir das englische Modell mit englischen Anfragen und übersetzen während der Inferenz die deutsche Anfrage mit der API ins Englische und verwenden dann das trainierte Modell, um SQL vorherzusagen. Wir testen auch das Monolinguale Modell. In diesem Szenario ist die Quellsprache dieselbe wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch das Monolinguale Few-shot-Szenario, indem wir monolinguale Modelle mit nur 10 % des Trainingsdatensatzes trainieren. Wir testen das Multilinguale Modell, bei dem wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel fügen wir deutsche, englische und chinesische Anfragen zusammen, um ein multilinguales Modell zu trainieren. Während der Inferenz können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen zu übersetzen und so weiter. Wir berücksichtigen auch Cross-linguale Zero-shot und Few-shot-Übertragungen. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es mit englischen Anfragen oder der Kombination aus englischen und deutschen Few-shot-Anfragen, um ein multilinguales Modell zu trainieren, das die SQL-Ausgabe vorhersagt. Wir finden auch viele interessante Ergebnisse. Was die Analyse der monolingualen Modelle betrifft, evaluieren wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, was für Multilinguale Vorabtrainierte Encoder mit Zeigerbasierten Decodern steht, wie XLM-R + PTR und mBERT + PTR. Wir evaluieren auch Encoder-Decoder-Modelle, die Multilinguale Vorabtrainierte Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder auf allen neun Datensätzen die beste Leistung erzielt. Wir evaluieren mT5 und XLM-R + PTR im multilingualen Szenario. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung verschiedener Sprachen verbessert werden kann. Wir fanden heraus, dass dies daran liegt, dass die meisten der wichtigsten natürlichen Sprachen Leistungsgewinne erzielen, außer dass die Leistung des Englischen in sieben Datensätzen abfällt und nur in drei Datensätzen zunimmt. Ich denke, dies ist als "Fluch der Multilingualität" bekannt. Wir vergleichen auch die cross-linguale Leistungsunterschiede. In dieser Abbildung ist die blaue Linie die Cross-linguale Few-shot-Übertragung. Die orangefarbene Linie ist die Cross-linguale Zero-shot-Übertragung. Während die grüne Linie die Monolinguale Einstellung ist. Wir fanden heraus, dass, indem wir die grüne und orangefarbene Linie vergleichen, der Zero-shot-Übertragungsleistungsunterschied signifikant ist, und dann, indem wir die blaue und orangefarbene Linie vergleichen, der Übertragungsunterschied im Few-shot-Szenario schnell verkürzt wird. Wir finden auch andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder die vorherige Arbeit oder erzielt vergleichbare Ergebnisse. Das Vortraining auf der englischen natürlichen Sprache kann die Leistung des Few-shot auf der Zielsprache erheblich steigern, und wir fanden heraus, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale semantische Parsing-Aufgaben sind. Zusammenfassend haben wir XSemPLR aufgebaut, eine einheitliche Benchmark für cross-linguales semantisches Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Arten von multilingualen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Und wir laden Sie ein, unsere Arbeit und den Code zu besuchen. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Existing works can be broadly classified into four categories, but they either are not applicable to embedding as services or lack transferability.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex oder Bloom sind für Cross-Lingual Semantic Parsing (CLSP) noch nicht ausreichend.</sample>
    <sample id="174">Thea, a co-author of "ArgAnalysis35K: A Large-Scale Dataset for Argument Quality Analysis," highlights the unique features of this dataset in a video. The dataset focuses on argument quality analysis, which involves rating arguments on a scale from 0 to 1 based on their persuasiveness and coherence. Unlike existing datasets, which often suffer from low quality, lack of diversity, and insufficient depth due to reliance on crowdsourcing, ArgAnalysis35K offers several improvements.

Firstly, it is the largest dataset in this field, containing 35,000 argument-analysis pairs. The arguments are sourced from high-quality tournaments, expert and intermediate debaters, and novice debaters, ensuring higher quality than crowdsourced arguments. The dataset covers 24 themes, providing a diverse range of motions, unlike other datasets that focus on a limited number of pre-selected motions.

A key innovation is the introduction of "analysis," a term combining claims, premises, and other elements to better explain arguments. This concept is new to the NLP community and enhances the dataset's explanatory power.

The dataset also employs instance-based annotator reliability, addressing human biases by selectively eliminating biased judgments rather than discarding all annotations from an annotator. This approach improves the reliability of the dataset.

Finally, ArgAnalysis35K introduces a relevance model, assigning scores to arguments based on their relevance to various themes, recognizing that arguments can apply to multiple topics. This model captures the multifaceted relevance of arguments, enhancing the dataset's utility.

Overall, ArgAnalysis35K offers a more diverse, high-quality, and reliable dataset for argument quality analysis, with unique features like analysis, instance-based reliability, and a relevance model.</sample>
    <sample id="175">Die Methode adressiert die Mehrdeutigkeit der Permutationen, indem sie eine GPU-freundliche kontinuierliche Relaxation verwendet, die es ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert durch die Fähigkeit des Modells, unabhängig von der politischen Ausrichtung oder sozialen Kategorien der Zielgruppe, gleichmäßig und gerecht zu funktionieren. In der Studie wird Fairness anhand der Leistung des Modells bei Aufgaben wie der Erkennung von Hassrede und Falschmeldungen gemessen, wobei die Leistung in verschiedenen demografischen oder politischen Gruppen getrennt betrachtet wird. Ein Modell zeigt Fairness, wenn es Hassrede und Falschmeldungen unabhängig von der politischen Ausrichtung oder der Machtstellung der Zielgruppe gleichmäßig erkennt und behandelt.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Dieser Vortrag von Melanie Sclar präsentiert "SymbolicToM", eine Methode zur Verbesserung der Theory of Mind (ToM) Fähigkeiten von großen Sprachmodellen (LLMs) durch explizite grafische Darstellungen. ToM bezieht sich auf die Fähigkeit, die mentalen Zustände anderer zu verstehen, oft durch falsche Glaubensfragen getestet. Große LLMs wie ChatGPT und GPT-3 zeigen Schwächen bei solchen Aufgaben. SymbolicToM verwendet mehrere grafische Darstellungen, um die mentalen Zustände von Charakteren in Geschichten zu modellieren, und ermöglicht es, Fragen effizient zu beantworten, indem es diese Graphen nutzt. Die Methode wurde mit verschiedenen LLMs getestet und zeigte signifikante Leistungssteigerungen, insbesondere bei zweiter Ordnung falscher Glaubensfragen. Experimente mit den ToMi-Daten und neuen, herausfordernden Datensätzen (D₁, D₂, D₃ und ParaphrasedToMi) zeigten, dass SymbolicToM die Leistung von LLMs verbessert, insbesondere bei der Generalisierung auf neue Strukturen und linguistische Variationen. Im Vergleich zu überwachten Baselines übertraf SymbolicToM diese deutlich, insbesondere bei der Bewältigung von außerhalb des Domänenverständnisses. Die Methode bietet interpretierbare Schlussfolgerungen und vermeidet das Risiko des Overfittings. Weitere Details sind im zugehörigen Papier zu finden.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">Dieses Papier stellt das Problem des eingeschränkten Sprachplanens vor, bei dem spezifische Ziele mit vielfältigen Einschränkungen berücksichtigt werden. Während große Sprachmodelle (LLMs) abstrakte Ziele in Schritte zerlegen können, ist ihre Leistung bei spezifischen, eingeschränkten Zielen unzureichend. Um dies zu adressieren, entwickeln die Autoren eine Methode zur Erstellung eines Datensatzes namens CoScript, der spezifische Ziele mit Skripten umfasst. Sie verwenden InstructGPT, um spezifische Ziele zu generieren und ein "über-generieren-und-filtern"-Verfahren anzuwenden, um die Qualität der Skripte zu verbessern. Dies beinhaltet die Generierung mehrerer Skripte und die Auswahl derjenigen, die den Einschränkungen am besten entsprechen. Der CoScript-Datensatz, der 55.000 spezifische Ziele umfasst, zeigt eine hohe Vielfalt und ermöglicht die Schulung kleinerer, spezialisierter Modelle wie T5, die in der Qualität der generierten Skripte größere LLMs übertreffen können. Die Studie zielt darauf ab, die Forschung im Bereich des Sprachplanens voranzutreiben, indem sie eine wertvolle Ressource in Form des CoScript-Datensatzes bereitstellt.</sample>
    <sample id="182">Im Zusammenhang mit dieser Arbeit bezieht sich Tropikalismus auf ein Stereotyp, das mit lateinamerikanischen Frauen verbunden ist. Es umfasst Wörter wie "vibrant" und "curvaceous", die mit einer exotischen und übertriebenen Darstellung ihrer Identität verbunden sind. Dieses Stereotyp trägt zur Diskriminierung und Andersartigkeit bei, indem es lateinamerikanische Frauen auf bestimmte kulturelle und physische Merkmale reduziert.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie ähnliche Prompts wie die an die Sprachmodelle gegebenen Prompts verwendeten. Diese Prompts wurden an menschliche Teilnehmer weitergegeben, um Stereotype zu identifizieren, die auch bei den von den Modellen generierten Beschreibungen auftraten. Dies ermöglichte einen direkten Vergleich zwischen den von Menschen verfassten und den von den Modellen generierten Beschreibungen.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextual Mutual Information) verwendet, um die Kontextnutzung zu messen. Es wurde erweitert zu Pointwise CXMI, um die Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="185">DrBERT ist ein auf RoBERTa basierendes Modell, das auf dem NACHOS-Datensatz mit medizinischen Daten aus dem Web trainiert wurde, während ChuBERT auf anonymisierten Daten aus dem Datenlager des Nantes University Hospital basiert und hauptsächlich klinische Notizen verwendet. DrBERT konzentriert sich auf eine breitere Palette von medizinischen Daten, während ChuBERT spezifischer auf klinische Daten ausgerichtet ist.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt: Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen beinhaltet das schrittweise Feinabstimmen eines Modells auf mehreren verwandten Aufgaben, um die Leistung auf einer Ziel- oder seltenen Klasse zu verbessern. Im Kontext des Papiers beinhaltet dies das anfängliche Übertragen von Gewichten von eng verwandten Aufgaben (wie der Klassifizierung von Stimmungen in Debatten und der binären Klassifizierung von Erweiterungs- und Vergleichsklassen) und das anschließende iterative Feinabstimmen auf diesen Aufgaben, um die Leistung auf der seltenen Klasse der kognitiven Dissonanz zu verbessern.</sample>
    <sample id="189">Das Ziel des AltEntities Corpus ist es, das Verständnis von indirekten Bezugsausdrücken in der Entitätswahl zu verbessern, um die Interaktion mit konversationellen Systemen natürlicher zu gestalten und die Fähigkeit von Sprachmodellen zur Entitätsverständnis zu bewerten.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen Embedding as a Service (EaaS) extrahieren, indem er das EaaS nutzt, um Embeddings für verschiedene Eingabesätze zu erhalten und dann diese Informationen verwendet, um das zugrunde liegende Modell zu rekonstruieren oder zu approximieren. Dies kann durch das Lernen von Mustern und Eigenschaften der bereitgestellten Embeddings geschehen, was es dem Angreifer ermöglicht, ein ähnliches Modell zu erstellen, das ähnliche Dienste anbieten kann.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Yang Luo präsentiert das Papier "CAME: Confidence-guided Adaptive Memory Efficient Optimization", das sich mit der Herausforderung befasst, ein Optimierer-Design zu entwickeln, das sowohl schnelle Konvergenz als auch geringen Speicherverbrauch bietet. Traditionelle adaptive Methoden wie Adam erfordern dreifachen Speicher für Momentenschätzungen, während memory-effiziente Optimierer wie Adafactor Speicher sparen, jedoch mit Leistungseinbußen. CAME zielt darauf ab, diese beiden Ziele zu vereinen.

Die Präsentation beginnt mit einer Einführung in die nicht-negative Matrixfaktorisierung (NMF), die den Speicherbedarf drastisch reduziert. Adafactor nutzt NMF, um I-Divergenz zu minimieren, was jedoch zu langsamer Konvergenz und fehlerhaften Aktualisierungen führt. CAME adressiert diese Probleme, indem es die Instabilität zwischen dem Momentum der Aktualisierungen und den aktuellen Aktualisierungen nutzt, um eine adaptivere Aktualisierungsschritt zu ermöglichen.

Experimente auf dem BookCorpus und der englischen Wikipedia zeigen, dass CAME im Vergleich zu Adam und Adafactor eine signifikante Verbesserung der Validierungspräzision bietet, insbesondere bei großen Modellen wie BERT, GPT-2 und T5. CAME erreicht eine höhere Validierungspräzision und reduziert den Speicherverbrauch erheblich, insbesondere bei großen Batchgrößen. Die Ergebnisse zeigen, dass CAME die Leistung von BERT-basierten Modellen auf typischen Downstream-Aufgaben verbessert, während es den Speicherverbrauch reduziert. CAME übertrifft auch bestehende Optimierer wie SM3 in Bezug auf den Speicherverbrauch und zeigt seine Effektivität bei der groß angelegten Modellierung.</sample>
    <sample id="193">Der ursprüngliche Datensatz wurde von 10 Annotatoren erstellt.</sample>
    <sample id="194">Die Autoren gehören zu Carnegie Mellon University, der University of Washington und dem Allen Institute for AI.</sample>
    <sample id="195">Der Artikel stellt das Projekt "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" vor, das sich mit der Herausforderung der Erklärung von Antworten in der Frage-Antwort-Technologie (XQA) befasst. Es gibt zwei Hauptansätze in der XQA: neuro-symbolische Methoden, die Fragen in formale Repräsentationen wie SPARQL übersetzen, und dekompositions-basierte Methoden, die Fragen in natürlichsprachliche Zwischenschritte zerlegen. Beide Ansätze haben Einschränkungen: Neuro-symbolische Methoden sind auf strukturierte Wissensbasen (KBs) beschränkt, die oft unvollständig sind, während dekompositions-basierte Methoden mit der Vielfalt natürlicher Sprache kämpfen.

Um diese Herausforderungen zu überwinden, schlägt der Artikel das RoHT-Framework (Reasoning over Hierarchical Question Decomposition Tree) vor. RoHT nutzt eine Hierarchische Frage-Zerlegungsbaumstruktur (HQDT), um komplexe Fragen in subtilere, handhabbare Teile zu zerlegen. Der Baum beginnt mit der ursprünglichen Frage als Wurzel und zerlegt sie in subtilere Fragen, bis zu atomaren Fragen, die nicht weiter zerlegt werden können. Das Framework führt probabilistisches Räsonieren über den HQDT durch, um Wissen aus verschiedenen Quellen zu integrieren und die Wahrscheinlichkeit der Antwort zu berücksichtigen.

RoHT wird auf zwei komplexen QA-Datensätzen, KQA Pro und Musique, evaluiert. KQA Pro simuliert unvollständige KBs, während Musique auf QA-Komprehension basiert. Die Ergebnisse zeigen, dass RoHT sowohl mit unvollständigen KBs als auch mit zusätzlichen Textquellen wie Wikipedia und Wikidata signifikante Verbesserungen gegenüber bestehenden Methoden erzielt. RoHT übertrifft insbesondere TransferNet, das mit einem gemischten Beziehungsgraphen trainiert wird, was die Überlegenheit der expliziten Zerlegung unterstreicht. Insgesamt demonstriert RoHT die Effektivität der Integration von Wissen aus verschiedenen Quellen und der Zerlegung von Fragen zur Verbesserung der Erklärbarkeit und Genauigkeit in der Frage-Antwort-Technologie.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme umfasst die Verwendung von menschlichen Bewertungen, um die Qualität von Dialogen zu bewerten, wie z.B. durch Likert-Skalen oder Paarvergleiche. Diese Methoden bewerten die Gesamtqualität von Dialogen, aber es gibt ein wachsendes Interesse an der Bewertung mehrerer Dimensionen der Dialogqualität, um die Stärken und Schwächen von Modellen auf einer feineren Ebene zu verstehen. ABC-Eval ist eine neue Methode, die entwickelt wurde, um die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit annotiert, ob ein Modell bestimmte Verhaltensweisen zeigt, wie z.B. irrelevante Informationen bereitstellen oder sich selbst widersprechen. Diese Methode hat sich als zuverlässiger und informativer erwiesen als bestehende Methoden, indem sie einzigartige Aspekte der Dialogqualität erfasst und die Gesamtqualität von Gesprächen besser vorhersagt.</sample>
    <sample id="198">Um die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten, ist es wichtig, da moderne große Sprachmodelle längere Kontextfenster haben. Die Bewertung über das gesamte Kontextfenster stellt sicher, dass die Modelle in der Lage sind, Akzeptanzurteile über längere Sequenzen hinweg zu treffen, was entscheidend ist, um ihre Leistungsfähigkeit und Robustheit in realen Anwendungen zu verstehen.</sample>
    <sample id="199">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell, was als "Curse of Multilinguality" bekannt ist. Die Leistung in sieben der neun Datensätze sank, während sie nur in drei Datensätzen stieg.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus, da sie die Namen der Entitäten in der alternativen Frage sehen und zusätzliche Hintergrundinformationen über die Entitäten erhalten.</sample>
    <sample id="201">State-of-the-art neural MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">Der englische Inhalt gibt keine spezifischen Informationen darüber, ob die Regression bei der Generalisierung auf bestimmte NER-Typen auswirkt. Die Diskussion konzentriert sich auf allgemeine Faktoren wie Modellarchitektur, Modellgröße und Anzahl der Feinabstimmungsbeispiele sowie auf die Ursachen für Leistungsabfälle, insbesondere temporale Drift, ohne spezifische NER-Typen zu erwähnen.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Entscheidungen von Forschern und Entwicklern beeinflusst, die wiederum die Leistung von Technologien zwischen verschiedenen Bevölkerungsgruppen beeinflussen können. Diese systematischen Leistungsunterschiede können zu Design-Bias führen, da Modelle und Datensätze die Urteile und Meinungen bestimmter Gruppen über andere repräsentieren können. Da NLP-Aufgaben zunehmend subjektiv und sozial orientiert sind, ist es entscheidend, diese Positionalitäten zu verstehen und zu charakterisieren, um sicherzustellen, dass Technologien fair und inklusiv sind.</sample>
    <sample id="204">Nein, im präsentierten Inhalt wird nicht erwähnt, dass mehrsprachige LLMs wie BLOOM durch Adapter oder eine vollständige Feinabstimmung angepasst wurden. Es wird lediglich festgestellt, dass solche Modelle für die Aufgabe des cross-lingualen semantischen Parsings noch unzureichend sind.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented research on political biases in language models, focusing on their propagation from pretraining data to downstream tasks. Language models are trained on large-scale web crawl data, including political news media like the New York Times and The Guardian, which introduces diverse perspectives but also inherent social biases. The study investigates how these biases affect language model performance and fairness in NLP applications.

The research evaluates the political leanings of language models using political questionnaires, revealing that models like GPT-4 are more socially liberal compared to others like BART. Controlled experiments showed that further pretraining on partisan corpora shifts models' ideological coordinates, with models trained on left-leaning data becoming more liberal. Additionally, models pretrained on data from different time periods (pre- and post-45th U.S. president) reflect societal polarization, with post-2017 models leaning further from the center.

The study also examines the impact of these biases on tasks like hate speech and fake news detection. Left-leaning models are better at detecting hate speech against minority groups but worse against powerful groups, while right-leaning models show the opposite trend. Similar patterns are observed in fake news detection, where models are more effective at identifying misinformation from opposing political leanings.

These findings highlight significant fairness issues, as deploying biased models could marginalize certain groups and allow unchecked hate speech. The research underscores the dilemma of balancing bias mitigation with the risk of censorship, emphasizing the challenge of defining neutrality in language model training data.</sample>
    <sample id="206">Das Modell für das Transferlernen wird durch das Übertragen von Gewichten von zwei verschiedenen Aufgaben gestartet: der Klassifizierung des Standpunkts unabhängig vom Thema (Debatte) und der binären Klassifizierung der Erweiterungs- und Vergleichsklassen von PDTB (CE). Das Modell, das durch feines Tuning der CE-Aufgabe gefolgt von weiterem Feintuning der Debatte-Aufgabe erzielt wurde, wird verwendet, um den aktiven Lernprozess zu starten.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die neuesten Testsets der WMT-Evaluation, um eine Überschneidung mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 20,5%.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden. Die Studie schlägt DEPLAIN als Basisbenchmark für das Problem der automatischen Textvereinfachung vor.</sample>
    <sample id="212">Die Arbeit experimentiert mit einem kleineren Modell, dem T5, das auf dem CoScript-Dataset feinabgestimmt wird.</sample>
    <sample id="213">Das Basismodell, das für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet wird, ist das OFA-Modell (Unified Multi-Modal Pre-Trained Model).</sample>
    <sample id="215">In der Präsentation von Adam Przepiórkowski wird die Struktur der Koordination in der Syntax untersucht, wobei verschiedene theoretische Ansätze und Korpusmethoden verglichen werden. Die Universal Dependencies und Igor Mel'čuks Meaning Text Theory betrachten die Koordination als asymmetrisch, wobei der erste Konjunkt als Kopf der gesamten Struktur fungiert. Im Gegensatz dazu sieht die Prager Schule die Konjunktion als Kopf der Koordination, während Hudsons Word Grammar eine multi-köpfige Struktur annimmt, bei der alle Konjunkte als Köpfe fungieren.

Das Ziel des Papiers ist es, eine neue Argumentation für symmetrische Koordinationsstrukturen zu liefern und asymmetrische Ansätze zu widerlegen. Der zentrale Punkt ist das Prinzip der Minimierung der Abhängigkeitslänge, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Dies wird anhand von Beispielen illustriert, bei denen die Positionierung von direkten Objekten und Adjunkten die Satzakzeptanz beeinflusst. Längere direkte Objekte können näher an Adjunkten platziert werden, um die Abhängigkeitslänge zu minimieren.

Statistische Analysen der erweiterten Version des Penn Treebanks zeigen, dass kürzere Konjunkte tendenziell links stehen, insbesondere wenn der Unterschied in der Länge zwischen den Konjunkten zunimmt. Diese Tendenz ist jedoch nur dann vorhanden, wenn der Regent links steht oder fehlt. Wenn der Regent rechts steht, verschwindet diese Tendenz. Diese Beobachtungen unterstützen die Argumentation für symmetrische Koordinationsstrukturen und stellen asymmetrische Ansätze in Frage. Weitere Details und Argumente sind im vollständigen Papier zu finden.</sample>
    <sample id="217">Das Papier "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" von Weihao Zeng, Lulu Zhao und Keqing He untersucht die Herausforderungen der Generierung von Dialogen mit mehreren Attributen. Es hebt hervor, dass bestehende Methoden sich auf einzelne Attribute konzentrieren und die praktische Anwendung von mehreren Attributen vernachlässigen. Die Autoren schlagen das Disentangled Controllable Generation (DCG) vor, das Attribute aus gesehenen Werten lernt und eine disentanglement loss verwendet, um verschiedene Attributkombinationen zu trennen. Sie führen eine einheitliche, referenzfreie Bewertungsmethode, MAE, ein, die für verschiedene Granularitäten von Attributen geeignet ist. Die Modelle basieren auf dem DialoGPT-Framework und verwenden zwei Arten von Prompts: attribute-orientierte und aufgaben-orientierte Prompts, um die Generierung von Dialogantworten zu steuern. Die Autoren stellen zwei Benchmarks auf und zeigen die Wirksamkeit ihrer Methode und Bewertungsmetriken durch Experimente. Ihre Ergebnisse zeigen, dass DCG die Herausforderungen der kompositionellen Generalisierung für die Generierung von Dialogen mit mehreren Attributen erfolgreich bewältigt, mit nur geringen Einbußen bei den E-ACC und A-ACC-Metriken. DCG übertrifft andere Baselines in der Attributkontrollierbarkeit und Textgleichheit, insbesondere bei neuen Attributkombinationen. Die Studie bestätigt die Effektivität ihrer Methode, gesehene Attribute in neue Kombinationen zu transformieren, und zeigt die Überlegenheit ihrer automatischen Bewertungsmethode MAE gegenüber klassischen Metriken.</sample>
    <sample id="218">Die Autoren gehören Google an.</sample>
    <sample id="219">Jia-Huei Ju präsentiert die Forschungsarbeit "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports", die mit Yu-Shiang Huang, Cheng-Wei Lin und den Professoren Che Lin und Chuan-Ju Wang entwickelt wurde. Das Ziel der Arbeit ist es, nützliche Informationen aus Form 10-K Berichten, den jährlichen Berichten, die von der SEC verlangt werden, effizienter zu extrahieren. Die Forschung wurde durch zwei Beobachtungen motiviert: die hohe Textähnlichkeit zwischen Berichten aufeinanderfolgender Jahre und die jährliche Abhängigkeit der Inhalte. Um diese Herausforderungen zu bewältigen, wurde eine Aufgabe zur Hervorhebung von Schlüsselwörtern eingeführt, die die Beziehungen zwischen Berichten eines Jahres und dem Vorjahr vergleicht.

Die vorgeschlagene Pipeline umfasst mehrere Stufen: Dokumentensegmentierung, Beziehungsanerkennung und zwei Stufen der Modellanpassung (out-of-domain und in-domain). In der Beziehungsanerkennung werden Berichtspaare in drei Typen klassifiziert: hohe syntaktische und semantische Ähnlichkeit, ähnliche syntaktische Muster mit unterschiedlichen Bedeutungen und neu eingeführte Informationen. Die Modellanpassung nutzt das eSNLI-Dataset für die out-of-domain Anpassung und pseudo-positive Labels für die in-domain Anpassung, wobei Techniken wie Soft-Labeling eingesetzt werden, um die Qualität der Labels zu verbessern.

Die Leistung wird anhand von Präzision und PCC (Pearson-Korrelationskoeffizient) bewertet, wobei das Modell auf dem FINAL-Dataset und eSNLI gute Ergebnisse zeigt. Die Forschung zeigt, dass das Modell auch auf nicht trainierte mismatched Paare angewendet werden kann. Zukünftige Arbeiten könnten die Effektivität verbessern oder zusätzliche Merkmale und Techniken aus dem Bereich der Informationsretrieval integrieren. Weitere Details sind im begleitenden Papier und auf GitHub verfügbar.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte das Sprachpaar Deutsch-Englisch.</sample>
    <sample id="222">Der Artikel "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering" untersucht die Herausforderungen und Lösungen für die Anpassung von Frage-Antwort-Modellen an neue Domänen. In Open-Domain-Settings werden relevante Passagen aus einer allgemeinen Korpusquelle wie Wikipedia mit einem Retriever-Modell abgerufen, und ein Reader-Modell generiert die Antwort. Diese Modelle sind jedoch oft auf allgemeine Domänen trainiert und können bei spezifischen Fragen, wie z.B. in der Biomedizin, versagen.

Die Autoren präsentieren drei Hauptbeiträge: die Untersuchung von Dateninterventionen zur Verbesserung der Generalisierungsfähigkeit, die Identifizierung des Typs von Datenshifts in neuen Domänen und die Bestimmung effektiver Dateninterventionen für spezifische Shifts. Sie untersuchen sowohl zero-shot als auch few-shot Methoden zur Generierung von Dateninterventionen. Few-shot-Methoden nutzen wenige Beispiele aus der Ziel-Domäne, um große Sprachmodelle zur Generierung weiterer Beispiele zu verwenden, was zu einer durchschnittlichen Leistungssteigerung von 8% beim Retriever und 11% beim Reader führt. Zero-shot-Techniken variieren die Interaktionen zwischen Frage, Antwort und Kontext, um die Modellleistung zu analysieren.

Die Autoren verwenden eine bestehende Taxonomie von Datenshifts, um die Art der Inkompatibilität zwischen Quell- und Zielmodellen zu bestimmen. Sie identifizieren vier Shift-Typen: keinen Shift, Konzeptshift, Kovariaten-Shift und vollständigen Shift. Die Kompatibilität wird durch die Wahrscheinlichkeit gemessen, die das Quellmodell den Kontexten und Antworten der Ziel-Daten zuweist.

Schließlich zeigen die Autoren, dass bestimmte Dateninterventionen je nach Art des Datenshifts effektiver sind. Few-shot-Anpassungen verbessern die Leistung in allen Zielsets, während zero-shot-Anpassungen bei Konzept- und Kovariaten-Shifts besonders nützlich sind. Insgesamt verbessern die vorgeschlagenen Dateninterventionen die Leserleistung um bis zu 24%.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Während der Experimente wurden die Modelle MASSalign und long-mBART untersucht. MASSalign wurde für die automatische Textausrichtung verwendet, und long-mBART wurde für die Erzeugung von Dokumentenebene-Simplifikationen feinabgestimmt. Zudem wurde das normale base mBART für die Erzeugung von Satzebene-Simplifikationen feinabgestimmt.</sample>
    <sample id="225">Für das Training werden 53 Aufgaben aus 9 Gruppen verwendet. Für das Testen wird die gesamte Gruppe für Common Sense Reasoning verwendet, sowie zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Der Artikel diskutiert die Herausforderungen und Lösungen im Bereich des grounded language understanding (GLU), bei dem natürliche Sprache in ausführbare Aktionen oder Programme in spezifischen Umgebungen übersetzt wird. GLU ist entscheidend für Anwendungen wie Sprachassistenten, semantische Suchen und domenenspezifische Befehlsverarbeitung. Die Schwierigkeit liegt in der fehlenden Verankerung während der Vortrainingsphase von Sprachmodellen, die hauptsächlich auf textbasierten Korpora trainiert werden. Dies führt zu Problemen bei der Generierung von gültigen und grammatikalischen Plänen oder Programmen.

Der Autor schlägt eine neue Framework vor, genannt Pangu, inspiriert von der chinesischen Mythologie, die sich auf Diskriminierung statt auf Generierung konzentriert. In diesem Ansatz interagiert ein symbolischer Agent mit der Umgebung und schlägt Kandidatenpläne vor, während das Sprachmodell diese Pläne bewertet und rangiert. Dies entlastet das Sprachmodell von der Notwendigkeit, die Gültigkeit und Grammatik selbst zu gewährleisten.

Experimente mit verschiedenen Sprachmodellen wie BERT, T5 und Codex zeigen, dass Pangu in verschiedenen Szenarien, einschließlich Wissens-basierter Fragebeantwortung, hervorragende Leistungen erbringt. Besonders bemerkenswert ist Pangu's Effizienz bei der Verwendung von Codex mit in-context learning, wo es mit nur einem Beispiel eine hohe Genauigkeit erreicht. Pangu zeigt auch eine starke Generalisierbarkeit und Robustheit unter nicht-i.i.d. Bedingungen, da es nicht zu Überanpassungen an gesehene Strukturen neigt. Der zentrale Punkt des Artikels ist, dass für GLU die Diskriminierung eine bessere Strategie als die Generierung sein könnte. Der Autor lädt zu Diskussionen und Zusammenarbeit ein.</sample>
    <sample id="228">Die Autoren haben Experimente an den Datensätzen AG News, MIND, SST2 und Enron Spam durchgeführt.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present their work on detecting improvable claims in argumentative writing. They emphasize the importance of text revision in achieving optimal phrasing, crucial for effective communication in argumentative texts. The paper introduces two tasks: Suboptimal-Claim Detection, which determines if a claim needs revision, and Claim Improvement Suggestion, which identifies quality issues for revision. The authors explore using implicit revision patterns from online debate platforms like Kialo to model argument quality. They address challenges such as Representativity and Reliability, ensuring datasets accurately reflect optimal claims; Model Complexity and Architecture, aligning models with revision sensitivity; Contextual Information, identifying relevant context for decision-making; and Topical and User Bias, managing noise and biases in collaborative revision histories. Their experiments show that revision-based data is effective for these tasks, with modeling the distance between claim versions aiding in detecting suboptimal claims. The impact of contextual information varies by task and quality issues. Further details are available in their paper.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer Daten, die aus dem Web gecrawlt wurden, und dient als Trainingsdaten für das DrBERT-Modell.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Der Artikel "Attention as a Guide for Simultaneous Speech Translation" von Sara Papi, Matteo Negri und Marco Turchi beschreibt eine innovative Lösung für die Herausforderungen der Simultansprachübersetzung (SimulST). SimulST ermöglicht die Echtzeit-Übersetzung gesprochener Sprache in Text einer anderen Sprache, was die interlinguale Kommunikation erleichtert. Aktuelle SimulST-Modelle leiden unter komplexen Architekturen, langwierigen Trainingsprozessen und der Notwendigkeit, mehrere Modelle für unterschiedliche Latenzregime zu trainieren und zu warten. Die vorgeschlagene Lösung, EDAtt (Encoder-Decoder Attention), nutzt bestehende Offline-Übersetzungsmodule ohne Neutrainieren oder spezielle Architekturen für SimulST. Sie verwendet eine einzige Modellinstanz für jedes Latenzregime und steuert die Latenz über spezifische Parameter. Die Entscheidung, ob eine partielle Übersetzung ausgegeben wird, basiert auf der Verteilung der Aufmerksamkeitsmechanismen zwischen Audioeingabe und Textausgabe. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht auf den letzten λ Sprachframes konzentriert ist, was auf ausreichend stabile Informationen hinweist. Die Ergebnisse zeigen, dass EDAtt die Qualität der Übersetzung (gemessen an BLEU) und die Latenz (gemessen an durchschnittlicher Verzögerung und computeraufmerksamer Verzögerung) verbessert und die bestehenden Strategien übertrifft. Der Code und die Modelle sind Open Source verfügbar, um die Reproduzierbarkeit zu erleichtern.</sample>
    <sample id="234">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Ergebnisse der Übersetzung mit großen Sprachmodellen wie PaLM. Unterschiedliche Prompt-Strategien können zu erheblichen Unterschieden in der Leistung führen, wie durch Experimente gezeigt wurde, bei denen die Verwendung von One-Shot-Prompting zu Unterschieden von mehr als einem BLEURT-Punkt führte, in extremen Fällen sogar bis zu 40 BLEURT-Punkten. Eine 5-Shot-Prompting-Strategie, bei der die Sprache der Sätze markiert wird, zeigte, dass die Form des Promptings bei mehreren kurzen Promptings weniger wichtig ist, während die Qualität der Beispiele entscheidend ist. Hochwertige Beispiele führen zu besseren Ergebnissen als solche, die dem Quelltext ähnlich sind.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Der englische Inhalt enthält keine spezifischen Details zu den fünf Anweisungen der Expert*innen. Es wird lediggegeben, dass jede Aufgabe mit fünf Experten geschriebenen Anweisungen ausgestattet ist, aber die genauen Anweisungen werden nicht aufgeführt.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen mit einem diagnostischen Testset namens KITMUS zu testen. Dieses Testset umfasst eine Aufgabe zur Koreferenzauflösung, die darauf abzielt, die Fähigkeit der Modelle zu bewerten, auf Wissen aus verschiedenen Quellen zuzugreifen. Sie definieren drei Einstellungen: "Background-Pretrain", "Background-Both" und "Background-Inference", um die Verfügbarkeit von Hintergrund- und entitätsspezifischem Wissen zu variieren. Die Autoren bewerten die Leistung der Modelle sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Koreferenzauflösungsmodellen.</sample>
    <sample id="238">In diesem Video stellt Yebowen Hu von der University of Central Florida die neue Benchmark-Datenbank MeetingBank vor, die darauf abzielt, die Entwicklung von Zusammenfassungstechnologien für Meetings zu unterstützen. MeetingBank enthält 1.366 Aufzeichnungen von Stadtratssitzungen mit fast 7.000 Instanzen, einschließlich Transkripten, Referenzzusammenfassungen und weiteren Ressourcen. Die Daten wurden mithilfe der Speechmatics-API aus Audio in Transkripte umgewandelt und mit Referenzzusammenfassungen aus den Sitzungsprotokollen abgeglichen. Die Analyse der Zusammenfassungen zeigt, dass die meisten zwischen 0,7 und 0,9 in der Abdeckung liegen, was auf eine starke Verwendung von wörtlichen Zitaten hinweist. Die Dichtebewertung zeigt, dass Seattle und Boston die höchsten und Denver die niedrigsten Werte aufweisen, was auf unterschiedliche Bearbeitungsgrade hinweist. Bei der Modellbewertung wurden sowohl extraktive als auch abstrakte Zusammenfassungssysteme getestet, wobei DialogLM bei den abstrakten Modellen und Extr-Oracle bei den extraktiven Modellen die besten ROUGE-2-Scores erzielten. GPT-3 zeigte in automatischen Metriken keine herausragende Leistung, erzielte jedoch in der menschlichen Bewertung hohe Gesamtpunktzahlen, insbesondere in Bezug auf Flüssigkeit und Kohärenz, aber weniger in Bezug auf Informationsgehalt und Faktizität. Die Studie betont die Notwendigkeit, die Hauptdiskussionspunkte in Zusammenfassungen zu erfassen und neue automatische Bewertungsmetriken zu entwickeln, die besser mit menschlichen Präferenzen übereinstimmen. MeetingBank dient als wertvolles Werkzeug für Forscher, um fortschrittliche Meeting-Zusammenfasser zu entwickeln und Einblicke in die Entscheidungsprozesse von Stadträten zu gewinnen.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Besprechung des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Tokens umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den State-of-the-Art in hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Verwendung von Sprachmodell-Prompting für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle mit den Best Practices der MT-Community evaluiert. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen uns mit den leistungsstärksten Systemen, also dem WMT-Evaluationsstand. Wir verwendeten die neuesten, neuralen MT-Metriken und zeigten zusätzlich Ergebnisse von Experten-basierten menschlichen Bewertungen. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwendeten und für jede Zeile zwei unterschiedliche Prompts bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, zeigte einen Unterschied von mehr als einem BLEURT-Punkt. In extremen Fällen kann dies bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten entschieden wir uns für eine 5-Shot-Prompting-Strategie, bei der wir jede an das System übergebene Zeile mit der Sprache markierten, in der sie steht. Zum Beispiel bei der Übersetzung von Deutsch ins Englische sind die deutschen Quellzeilen mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir stellten fest, dass die tatsächliche Form des Promptings bei mehreren kurzen Promptings keinen großen Einfluss hat. Es ist entscheidend für Zero- und One-Shot-Prompting. Und wenn wir, wie in unserem Fall, zu Five-Shot-Prompting gehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Promptings. Es sind die Beispiele, die den größten Einfluss haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellzeile. Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere verglichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Evaluierungen auf den Dev-Daten. Die Dev-Daten sind viel sorgfältiger kuratiert und von höherer Qualität als die Trainingsdaten, die lauter sind. Ihre Ergebnisse zeigen eine bessere Leistung, wenn die Dev-Daten verwendet werden. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Rahmenwerk durchgeführt haben, besagen, dass die Flüssigkeit von PaLM vergleichbar mit State-of-the-Art-Systemen ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint also, dass PaLM manchmal Teile der Quellzeile weglässt, um eine bessere klingende Übersetzung zu produzieren. Allerdings ist die "Style/Awkward"-Kategorie für PaLM niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diese wirklich kurze Besprechung. Für mehr Details, besuchen Sie bitte die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Promotionsstudent an der Saarland-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in die schwache Überwachung und das schwach überwachte Lernen beginnen. Bei der schwachen Überwachung werden die Daten nicht manuell beschriftet. Stattdessen beschriften wir die Daten mit schwachen Beschriftungsquellen, wie einfache Heuristiken, Wissensbasen oder niedrigwertige Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel günstiger, jedoch auch fehlerhaft, was bedeutet, dass ein bestimmter Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt mit schwach beschrifteten Daten trainieren, neigen die neuronalen Netze dazu, das Label-Rauschen zu memorieren und nicht zu verallgemeinern. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Label-Rauschen zu trainieren, sodass die trainierten Modelle dennoch gut verallgemeinern. In jüngsten Arbeiten im WSL, also WSL steht für Weakly Supervised Learning, wird häufig behauptet, dass Modelle nur mit schwach beschrifteten Daten trainiert und dennoch hohe Leistungen auf sauberen Testsets erzielt werden. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass ein zusätzliches sauberes Validierungsset für die Modellauswahl verfügbar ist. Wir können nicht bei diesem Problemsetting stehen bleiben, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Zweifel führt zu drei Forschungsfragen. Erstens, ist sauberes Validierungsdaten für WSL notwendig oder können wir vielleicht ein rauschendes Validierungsset verwenden? Zweitens, wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL unerlässlich sind, dann wie viele saubere Beispiele benötigen wir? Schließlich, sollten wir die sauberen Beispiele nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit adressiert und unsere Erkenntnisse sind wie folgt. Erstens finden wir interessanterweise, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsbeispiele gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotierungskosten für die Beschaffung sauberer Validierungsbeispiele sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl sauberer Validierungsbeispiele WSL-Ansätzen hilft, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir entscheiden, saubere Beispiele zu verwenden, wird das direkte Training auf ihnen noch bessere Leistungen erzielen. Die rechte Abbildung zeigt die Leistungsunterschiede zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Ansätzen, die saubere Daten nur für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung, wenn wir 10 Beispiele pro Klasse haben, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem erlaubt wird, die Feinabstimmung auf den sauberen Validierungsbeispielen fortzusetzen. Wie wir aus den Abbildungen sehen können, unterperformt das Vanille-Modell, das als FTw bezeichnet wird, zunächst komplexere WSL-Methoden wie COSINE. Wenn wir jedoch erlauben, die Feinabstimmung auf den sauberen Beispielen fortzusetzen, dann erzielt FTw eine Leistung, die ebenso gut ist wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität sind stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie über die Modellauswahlkriterien. Zum Beispiel, berichten Sie, ob die Modellauswahl über saubere Validierungsbeispiele erfolgt. Zweitens sollten WSL-Ansätze mit wenige-Schuss-Lern-Baselines verglichen werden, da beide auf sauberen Beispielen arbeiten. Drittens ist die kontinuierliche Feinabstimmung ein einfacher, aber starker Baseline, der in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code open-source gemacht. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte schauen Sie ihn sich an. Vielen Dank und genießen Sie die Konferenz.</sample>
    <sample id="241">Ethan präsentiert das Papier "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments", das er mit Yang Chen, Wei Xu und Alan Ritter an der Georgia Tech verfasst hat. Das Papier kritisiert bestehende automatische Ansätze zur Erkennung von Falschinformationen auf sozialen Medien, die oft unrealistisch evaluiert werden und nicht menschenzentriert sind. Es schlägt ein neues Evaluationsrahmenwerk vor, das menschliches Feedback integriert und von der Rohdatenverarbeitung bis zu handlungsorientierten Ausgaben reicht.

Das vorgeschlagene System besteht aus zwei Hauptkomponenten: der Erkennung irreführender Behauptungen und der Verifizierung von Verstößen gegen die Richtlinien. Die erste Komponente verwendet Keyword-Filterung und ein T5-Modell zur Extraktion von Behauptungen aus Tweets, die dann nach Trendigkeit sortiert und von Menschen verifiziert werden. Die zweite Komponente nutzt ein BERT-basiertes Modell zur Klassifizierung der Haltung des Autors zu nicht genehmigten Behandlungen, wobei Tweets mit unterstützender Haltung zur menschlichen Überprüfung markiert werden.

Die Evaluation des human-in-the-loop-Workflows zeigt, dass das System Behauptungen über nicht genehmigte Behandlungen vor deren Debunking in Nachrichtenartikeln erkennen kann. Die Effektivität der Verifizierung von Verstößen gegen die Richtlinien wird durch eine Likert-Skala bewertet, wobei das System eine Genauigkeit von 65% erreicht. Das System kann 124,2 Verstöße pro Arbeitsstunde bestätigen, was die Effizienz der menschlichen Arbeitsbelastung verdeutlicht.

Das Papier betont die Notwendigkeit, die komplexen Interaktionen zwischen Systemen und menschlichen Moderatoren realistisch abzubilden und motiviert die Entwicklung zukünftiger human-in-the-loop-Systeme zur Falschinformationserkennung. Es bietet auch Einblicke in die Entwicklung und Bewertung solcher Systeme aus einer außerindustriellen Perspektive.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind:

1. Human evaluation durch Auswahl der besseren von zwei Konversationen.
2. Bewertung von Konversationen mit einem Likert-Skala.
3. Likert-Bewertungen auf Turn-Ebene.
4. Likert-Bewertungen auf Dialog-Ebene.
5. Dialog-Level-Paarvergleiche.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass "Richter Fälle in Gerichten entscheiden."</sample>
    <sample id="245">Die Präsentation von Lining Zhang und ihren Co-Autoren beschreibt ein zweistufiges Verfahren zur Identifizierung von hochübereinstimmenden Amazon Mechanical Turk (MTurk) Arbeitern für die Zusammenfassung. Das Ziel ist es, die Probleme automatischer Bewertungsmetriken und unzureichend verstandene Best Practices bei der Rekrutierung auf MTurk zu adressieren. Das Verfahren beginnt mit Qualifikationseinstellungen, die Standort, Anzahl der Human Intelligence Tasks (HITs) und HIT-Approvals berücksichtigen. Im ersten Qualifikationsschritt bewerten Arbeiter drei Dokumente, einschließlich eines Aufmerksamkeitstests, und kategorisieren sie in Gold, Silber, Bronze oder Block, wobei nur Gold- und Silberarbeiter weiterkommen. Dies führt zu 26 qualifizierten Arbeitern (8 Gold, 18 Silber) aus 200 Teilnehmern. Der zweite Schritt, die Ausdauerprüfung, testet die Fähigkeit, mit einer hohen Arbeitsbelastung umzugehen, und resultiert in 12 Arbeitern (4 Gold, 8 Silber), die eine hohe Inter-Annotator-Übereinstimmung (IAA) aufweisen. Die Referenzaufgabe zeigt, dass 8 von 12 Arbeitern alle HITs abschließen, mit einer Krippendorffs Alpha von 0,534. Baseline-MTurk-Arbeiter erreichen mit dem MACE-Filter eine Alpha von 0,380, während CloudResearch-Arbeiter eine Alpha von 0,513 erreichen, jedoch mit niedrigeren Annahmeraten. Die Analyse der Korrektheit zeigt eine signifikante Korrelation zwischen Pipeline- und CloudResearch-Arbeitern, jedoch keine Garantie für die Schulung der Korrektheit. Zusammenfassend ermöglicht die Pipeline eine effiziente Rekrutierung von hochübereinstimmenden Arbeitern zu geringeren Kosten und ähnlicher Qualität wie CloudResearch. Zukünftige Arbeiten werden sich auf die Rekrutierung von hochwertigen Arbeitern in verschiedenen Anwendungen konzentrieren. Einschränkungen umfassen die Begrenzung auf englische Zusammenfassungen und die Nicht-Universellität der gestellten Fragen.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Er kann auf GitHub gefunden werden.</sample>
    <sample id="247">Jiho Kim von KAIST AI präsentierte das Papier "FACTKG: Fact Verification via Reasoning on Knowledge Graphs", das eine neue Aufgabe und ein zugehöriges Dataset, FactKG, einführt. Dieses Dataset nutzt Wissensgraphen, insbesondere DBpedia, zur Überprüfung von Fakten, was bisher in bestehenden Datensätzen wie FEVER, VitaminC, TabFact und InfoTabs, die auf Text oder Tabellen basieren, nicht der Fall war. FactKG ermöglicht eine zuverlässigere Überprüfung, da Wissensgraphen intuitive Beweise bieten, die direkt mit den Behauptungen verknüpft werden können. Dies ist besonders nützlich für moderne Dialogsysteme, die interne Wissensgraphen verwenden.

Das Dataset umfasst Behauptungen in zwei Stilen: schriftlich und umgangssprachlich, um praktische Anwendungen zu fördern. Es gibt zwei Labels: SUPPORTED und REFUTED. Die Aufgabe besteht darin, Beweise aus DBpedia zu extrahieren und die Behauptungen mit diesen Beweisen zu überprüfen. Es beinhaltet fünf Arten von Schlussfolgerungen: one-hop, Konjunktion, Existenz, multi-hop und Negation. 

Die one-hop-Behauptungen werden durch ein Tripel dargestellt, während Konjunktionen mehrere Tripel umfassen. Existenzbehauptungen erfordern die Überprüfung einer spezifischen Beziehung, und multi-hop-Schlussfolgerungen sind notwendig, wenn einige Entitäten nicht direkt erscheinen. Negation erfordert zusätzliche Überprüfung, selbst wenn graphische Beweise gefunden werden.

Zur Erstellung von umgangssprachlichen Behauptungen wurden ein Stiltransfermodell und Präsuppositionsvorlagen verwendet. Baseline-Methoden umfassen die Verwendung von Behauptungen ohne graphische Beweise und die Nutzung des GEAR-Modells mit korrekten Beweisen. Das GEAR-Modell übertraf alle anderen Baselines und die Mehrheitsklassenbaseline von 51%. Das Dataset ist verfügbar zum Download.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Die Studie umfasste über 1.000 Annotatoren aus 87 Ländern, was auf eine breite geografische Verteilung hinweist, aber es wird nicht spezifiziert, dass die Annotatoren in Bezug auf alle demographischen Gruppen wie Geschlecht, Bildungsniveau usw. ausgewogen sind.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durch Hinzufügen von Rauschen zu den Eingabesätzen durcheinander gebracht, während die relevanten Strukturen erhalten blieben. Diese Störungen führten zu ähnlichen Erhöhungen der MPP-Bewertungen, was darauf hinweist, dass die Modelle empfindlich auf die latenten syntaktischen und semantischen Merkmale reagieren, die die Sätze teilen.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Bewertung von Gesprächsqualität entlang mehrerer spezifischer Dimensionen oder Aspekte, um die Stärken und Schwächen eines Modells auf einer feineren Ebene zu verstehen. Im Kontext von ABC-Eval umfasst dies das Annotieren bestimmter Verhaltensweisen in Chats, wie z.B. Irrelevanz, Selbstwidersprüche oder Empathie, um die Qualität des Dialogs umfassender zu bewerten.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">Sai Kiran Tanikella präsentiert das Projekt "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion", eine Zusammenarbeit mit Abhinav Joshi, Akshat Sharma und Ashutosh Modi. Das Projekt zielt darauf ab, die Herausforderungen der Prior Case Retrieval (PCR) in der Rechtswelt zu bewältigen, bei der relevante frühere Fälle zu einem gegebenen rechtlichen Anfrage-Dokument aus einer Kandidatenmenge abgerufen werden müssen. Die Relevanz in der Rechtswelt basiert hauptsächlich auf ähnlichen Sachverhalten.

Zwei Hauptbeiträge des Projekts sind die Einführung des IL-PCR-Datensatzes und des U-CREAT-Pipelines. Der IL-PCR-Datensatz (Indian Legal Prior Case Retrieval Dataset) ist eine neue Benchmark für PCR-Aufgaben und umfasst 7.070 indische Rechtsfälle mit durchschnittlich 6,775 Zitaten pro Anfrage-Dokument. Er bietet eine umfassende Testumgebung zur Bewertung der Leistung von PCR-Algorithmen und übertrifft den bestehenden COLIEE’21-Datensatz für kanadische Rechtsdokumente in Bezug auf die Anzahl der Fälle, die Länge der Dokumente, die Vokabulargröße und die Anzahl der Zitate.

Die U-CREAT-Pipeline nutzt Techniken des unüberwachten Lernens und führt einen ereignisbasierten Ansatz für PCR-Aufgaben ein. Sie zeigt hohe Abrufeffizienz, geringe Inferenzzeit und Generalisierungsfähigkeit über indische und kanadische Rechtssysteme ohne spezifische Anpassung. Die Ereignisextraktion spielt eine entscheidende Rolle, indem sie Fälldokumente als Sammlung von Ereignissen darstellt, die durch Abhängigkeitsparsen extrahiert werden.

Experimente mit verschiedenen Modellen, einschließlich zählbasierten, transformerbasierten und ereignisbasierten Modellen, zeigten, dass ereignisbasierte Modelle, insbesondere das Event Filtered Documents-Modell, signifikant über den Basismethoden liegen. U-CREAT übertrifft bestehende Ansätze, einschließlich des jüngsten überwachten Ansatzes von MTFT-BERT, und gilt als derzeitiger State-of-the-Art-Methodenansatz für die COLIEE’21-Dokumentenabrufaufgabe.</sample>
    <sample id="253">Mario Ezra Aragón präsentiert "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media", eine Zusammenarbeit zwischen Forschern aus Mexiko und Spanien. Das Ziel ist es, mentale Gesundheitsstörungen durch die automatische Analyse von Social-Media-Posts zu erkennen. Diese Analyse soll Technologien unterstützen, die vor dem Auftreten von psychischen Erkrankungen warnen und Beweise liefern können. Domain-Adaptation wird verwendet, um die Leistung eines Modells auf einem Zielgebiet zu verbessern, wenn nur unzureichend annotierte Daten verfügbar sind. DisorBERT nutzt BERT, ein allgemeines Sprachmodell, und passt es an die spezifische Sprache von Reddit und mentaler Gesundheit an. Dabei wird ein Lexikon zur Lenkung des Maskierungsprozesses integriert, um das Modell auf wichtige Wörter zu konzentrieren. Die Ergebnisse zeigen, dass DisorBERT im Vergleich zu anderen Methoden eine gute Balance zwischen Präzision und Rückruf aufweist. Bei der Analyse von Textsegmenten zeigt DisorBERT eine stärkere Tendenz zu negativen oder psychologisch orientierten Wörtern im Vergleich zu BERT. Die Visualisierung der wichtigsten Textsequenzen zeigt, dass DisorBERT relevante Wörter wie "anxious" und "medication" hervorhebt. Die Kombination aus doppelter Domain-Adaptation und geleiteter Maskierung zeigt sich effektiv bei der Erkennung von Anzeichen psychischer Erkrankungen in sozialen Medien. Zukünftige Arbeiten sollen die Anwendung verschiedener lexikalischer Ressourcen und klinischer Daten erforschen.</sample>
    <sample id="254">Sun Qi von der Nanjing University of Science and Technology präsentiert die Forschungsarbeit "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction". Diese Arbeit zielt darauf ab, die Qualität der Labels in distanzüberwachten Daten (DS) für die Dokumentebene-Relationsextraktion (DocRE) zu verbessern. Traditionelle Methoden, die auf groß angelegten, von Menschen annotierten Korpora basieren, sind zeitaufwendig und arbeitsintensiv. Daher nutzen neuere Ansätze distanzüberwachte Daten, um DocRE-Modelle vorzutrainieren, stoßen jedoch auf das Problem von Labelrauschen durch falsch-positive Pseudo-Labels. 

Das vorgeschlagene Framework nutzt eine Unsicherheits-gesteuerte Label-Denoising-Strategie, um die Qualität der DS-Labels zu verbessern. Es beginnt mit dem Training eines vor-Denoising DocRE-Modells unter Verwendung sowohl von DS- als auch von menschlich annotierten Daten, um Pseudo-Labels zu generieren. Um die Zuverlässigkeit der Vorhersagen zu bewerten, wird eine Unsicherheitsschätzung eingeführt. Eine spezielle Methode zur Instanz-unsicherheitsbasierten Schätzung wird entwickelt, um Unsicherheitsscores für überlappende Relationen zu erfassen. 

Die Monte Carlo Dropout-Technologie wird verwendet, um die Unsicherheit im Modell zu modellieren, indem mehrere stochastische Vorwärtsdurchläufe mit aktiviertem Dropout durchgeführt werden. Um das Problem der überlappenden Relationen zu lösen, wird die Unsicherheitsschätzung angepasst, um Instanz-unsicherheitsscores für jedes positive Pseudo-Label zu erhalten. Dynamische Klassenunsicherheitsschwellen werden vorgeschlagen, um Pseudo-Labels mit hoher Unsicherheit zu filtern. 

Ein mehrphasiges Training wird entworfen, um die DS-Daten iterativ neu zu beschriften und die Leistung des DocRE-Modells weiter zu steigern. Das Framework übertrifft mehrere starke Baselines in öffentlichen Datensätzen und verbessert die Leistung erheblich. Die Hauptbeiträge umfassen die Unsicherheits-gesteuerte Label-Denoising-Strategie, die Instanz-unsicherheitsbasierte Schätzung für überlappende Relationen, die iterative Re-Label-Strategie mit dynamischen Klassenunsicherheitsschwellen und die Leistungsverbesserungen.</sample>
    <sample id="255">Die Form des Prompts ist besonders wichtig bei Zero- und One-Shot-Prompting. Bei mehreren Short-Promptings, wie im Fall von Five-Shot-Prompting, hat die tatsächliche Form des Prompts kaum Einfluss.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Dialogmodelle evaluiert.</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a new study titled "Can Large Language Models Be an Alternative to Human Evaluation?" The research explores using large language models (LLMs) to evaluate text quality in natural language processing (NLP) tasks. The study involves instructing LLMs to rate text samples based on specific criteria, such as grammar, coherence, likability, and relevance. The motivation behind this work is to find a stable and reproducible alternative to human evaluation, which is often inconsistent and difficult to replicate.

The experiment compares the ratings of stories generated by GPT-2 and those written by humans, using both LLMs and human evaluators (English teachers). The results show that while some smaller LLMs do not distinguish between human and GPT-2-generated stories, larger models like Davinci and ChatGPT align with human evaluators in preferring human-written texts.

The study highlights the potential of LLMs as a viable alternative to human evaluation in certain contexts. It also raises questions about the agreement between LLMs and human evaluators, the impact of instruction wording, and the benefits and costs of using LLMs for evaluation. These questions and more are addressed in the full paper, which is available for those interested in the topic.</sample>
    <sample id="259">Yusen Zhang von der Penn State University präsentiert "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Das Ziel ist es, semantische Repräsentationen von Benutzeranfragen in verschiedene natürliche Sprachen und Bedeutungsrepräsentationen zu übersetzen. Bestehende Modelle decken nur begrenzte Sprachen und Bedeutungsrepräsentationen ab, wobei insbesondere Chinesisch und Lambda-Kalkül fehlen. XSemPLR bietet eine einheitliche Datensammlung mit 9 Datensätzen, 5 Aufgaben, 8 Bedeutungsrepräsentationen und 22 Sprachen. Es werden sechs Trainings- und Evaluierungsszenarien betrachtet: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot und Few-shot Transfer. Die Analyse zeigt, dass Encoder-Decoder-Modelle die besten Ergebnisse erzielen, während die "Curse of Multilinguality" zu Leistungseinbußen bei Englisch führt. Cross-lingual Zero-shot zeigt signifikante Leistungslücken, die durch Few-shot-Transfer verringert werden. Encoder-Decoder-Modelle übertrumpfen oder erreichen vergleichbare Ergebnisse zu früheren Arbeiten. Pretraining auf Englisch verbessert die Leistung in Few-shot-Szenarien, während Modelle wie Codex und BLOOM für diese Aufgaben noch unzureichend sind. XSemPLR dient als umfassende Benchmark für cross-linguale semantische Parsing-Aufgaben.</sample>
    <sample id="260">Der Inhalt gibt keine spezifische Information über die Anzahl der Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="261">Ein guter Planer sollte in der Lage sein, Skripte zu erstellen, die sowohl vernünftig als auch den spezifischen Einschränkungen treu sind.</sample>
    <sample id="262">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="263">Das Papier "Mitigating Label Biases for In-context Learning" untersucht Instabilitäten in der In-Context-Lernfähigkeit großer Sprachmodelle, die durch verschiedene Designentscheidungen wie die Auswahl und Reihenfolge von Beispielen entstehen. Diese Entscheidungen führen zu Vorurteilen in den Modellvorhersagen. Das Papier zielt darauf ab, diese Probleme im Kontext der Textklassifizierung zu adressieren, indem es eine Typologie von Label-Vorurteilen entwickelt und eine neue Art von Vorurteil, das Domain-Label-Vorurteil, identifiziert. Dieses Vorurteil entsteht durch die Auswirkungen des Aufgabenkorpus auf die Modellvorhersagen. Experimente zeigen, dass das Sehen von zufälligen in-domain-Wörtern die Modellvorhersagen stark beeinflussen kann, während zufällige englische Wörter dies nicht tun. Um diese Vorurteile zu beheben, schlägt das Papier eine Domain-Context-Kalibrierungsmethode vor, die zufällige in-domain-Wörter verwendet, um die Vorurteile der Modellvorhersagen zu schätzen und zu kalibrieren. Diese Methode verbessert die Leistung der In-Context-Lernfähigkeit erheblich, insbesondere bei Aufgaben mit großem Domain-Label-Vorurteil. Die Studie zeigt, dass die Verwendung von zufälligen in-domain-Wörtern anstelle von vordefinierten Tokens oder zufälligen englischen Wörtern zu besseren Entscheidungsgrenzen und einer verbesserten Modellleistung führt. Die Ergebnisse gelten auch für größere Modelle wie GPT-3.</sample>
    <sample id="264">Lin Wang präsentiert das Papier "TAVT: Towards Transferable Audio-Visual Text Generation", das sich mit der Herausforderung der multimodalen Textgenerierung befasst, insbesondere mit der Generierung von Texten aus Audio- und Videoinhalten. Während uni-modale Textgenerierungsaufgaben wie maschinelle Übersetzung und Bildbeschreibung durch groß angelegte Vorabtrainings und große Modellkapazitäten vorangeschritten sind, ist die multimodale Textgenerierung aufgrund der aufwendigen und teuren Datenannotation und der Leistungseinbußen in verschiedenen Domänen schwieriger. Um diese Herausforderungen zu überwinden, schlägt Wang die Aufgabe der transferierbaren Audio-Visuellen Textgenerierung vor, die sich mit multimodalen Domänenverschiebungen wie visuellen Stilen und Audioenergien befasst.

Das Papier stellt ein modulares Framework vor, das aus drei Komponenten besteht: einem Audio-Visuellen Meta-Mapper-Netzwerk, einem Audio-Visuellen Encoder und Sprachmodell-Generator sowie einem Dual Counterfactual Contrastive Learning (DCLL). Der Meta-Mapper zielt darauf ab, visuelle Konzepte in einen einheitlichen auditiven semantischen Raum abzubilden, um semantische Verschiebungen zu adressieren. Dies wird durch die Verwendung von lernbaren Tokens erreicht, die als visuelle Präfixe für Audio-Cluster dienen. Der zweite Teil des Frameworks verwendet einen transformerbasierten Encoder und Generator, der die Beiträge verschiedener Modi zu jedem Wort bewertet. DCLL wird eingeführt, um visuell-textuelle Ausrichtungen direkt zu optimieren, ohne auf die Qualität zufällig ausgewählter negativer Beispiele angewiesen zu sein.

Das Framework wird mit Meta-Learning-Techniken trainiert, die eine schnelle Anpassung an neue multimodale Domänen mit begrenzten gelabelten Daten ermöglichen. Experimente auf den MSVD- und MSR-VTT-Datensätzen zeigen, dass TAVT bestehende Modelle in verschiedenen Szenarien, einschließlich geringer Ressourcen, deutlich übertrifft. Diese Ergebnisse unterstreichen die Effektivität von TAVT bei der Bewältigung von multimodalen Domänenverschiebungen und der Generierung von Texten aus Audio- und Videoinhalten.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Der Inhalt enthält keine Informationen über die Universitäten, denen die Autoren angehören.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, bei denen Teile der Quellsätze in der Übersetzung weggelassen werden, um eine flüssigere Übersetzung zu erzielen.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute erzählen wir Ihnen alles über ABC-Eval, einen neuen mehrdimensionalen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Stellen Sie sich vor, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlichen Bewertungen, z. B. indem menschliche Richter gebeten werden, zwischen zwei Gesprächen zu wählen oder Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der allgemeinen Dialogqualität zu liefern, aber die Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit besteht darin, menschlichen Richtern zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, z. B. die Relevanz der Modellantworten mit bestehenden vergleichenden oder Likert-Skalenmethoden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Ansatz für die mehrdimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Reagieren mit irrelevanten Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um umfassend Verhaltensweisen von Chat-Modellen abzudecken, die in der jüngsten Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Gesprächsrunden, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes, Widersprüchliches oder Halluziniertes sagt oder allgemeines Wissen verletzt, und ob das Modell erfolgreich oder erfolglos Empathie zeigt. Um festzustellen, welche Art von Bewertung am effektivsten ist, haben wir vier state-of-the-art Chat-Modelle ausgewählt und diese anhand von 100 menschlich-bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch anhand von drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies der Standard für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse der Bewertungsergebnisse stellten wir fest, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die mit bestehenden Methoden gesammelten Labels, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Gesprächen. Darüber hinaus sind ABC-Eval-Labels prädiktiver für die allgemeine Gesprächsqualität im Vergleich zu Metriken, die von bestehenden Methoden produziert werden, wie durch diese einfache lineare Regressionsanalyse gezeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Runden mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression durchgeführt haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken mehr als 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, verlieren die meisten von ihnen eine beträchtliche Menge an Informationen über die Qualität. Im Gegensatz dazu erklärt die Kombination aller turn-basierten Likert-Metriken viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung als bisherige Methoden zu bewerten. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das Allgemeinwissen, sie produzieren in etwa 15 % der Antworten irrelevante Informationen und sie widersprechen sich selbst oder ihrem Partner etwa 10 % der Zeit. Angesichts des rasanten Fortschritts im Bereich könnten viele dieser Fehlerquoten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, nach zuverlässigen und präzisen Bewertungsmetriken für den Vergleich von Modellen zu suchen. Wir hoffen, dass ABC-Eval von anderen im Bereich als bedeutender Schritt in diese Richtung genutzt werden kann. Und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank, dass Sie zugeschaut haben.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">In dieser Arbeit steht CFT für "Continuous Fine-Tuning".</sample>
    <sample id="272">Sechs Autoren sind an der Arbeit beteiligt: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy, und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel "When Does Translation Require Context? A Data-driven, Multilingual Exploration" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie übersetzen wir "mole" in diesem Satz? Wenn der vorherige Satz lautet "Things could start to get dangerous if the ministers find out", bezieht sich "mole" auf einen Spion. Wenn der vorherige Satz jedoch "Could it be anything serious, doctor?" lautet, bezieht sich "mole" auf einen Muttermal. Abhängig vom Kontext ändert sich die Bedeutung des Wortes und somit auch seine Übersetzung. Allerdings ist es schwierig, wie gut Modelle solche Fälle übersetzen können, zu bewerten. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was es korpusweiten Metriken wie BLEU unmöglich macht, diese Übersetzungen zu erfassen. Einige haben vorgeschlagen, eine gezielte Bewertung von kontextabhängigen Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsets, da sie in der Regel auf Fachwissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut können Modelle mit diesen Fällen umgehen? Um die erste Frage zu beantworten, haben wir damit begonnen, zu messen, wie sehr ein Wort vom Kontext während der Übersetzung abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmuster eingeführt. Dies wird durch Messen, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X, erreicht. Man kann CXMI als die Information betrachten, die durch das Geben von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satz- oder Wortebene messen kann. Man kann Wörter mit hohem P-CXMI als solche betrachten, die für die Übersetzung Kontext benötigen. Wir analysieren Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripten von TED-Vorträgen durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst schauen wir uns die Wortarten mit hohem durchschnittlichem P-CXMI an. Dies ermöglicht es uns, zum Beispiel, Dualpronomen im Arabischen zu finden, die relativ hohes P-CXMI aufweisen. Dies kann dadurch erklärt werden, dass das Englische keine Dualpronomen hat, sodass man den Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ebenso finden wir, dass bestimmte Sprachen auch Kontext benötigen, um die geeignete Verbform zu wählen. Dann schauen wir uns Vokabeln an, die hohes P-CXMI über alle ihre verschiedenen Vorkommen hinweg aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, wo im Chinesischen man Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man die gleiche Übersetzung im gesamten Dokument verwendet. Ebenso finden wir, dass Kontext wichtig ist, um die richtige Formalität zu übersetzen. Schließlich schauen wir uns einzelne Tokens mit hohem P-CXMI an. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur zum Ausdruck kommen, wie z.B. die Auflösung von Ellipsen. Nun verwenden wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu entwerfen. Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir nennen unseren Tagger den Multilingual Discourse-Aware, oder MuDA Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Dann verwenden wir den MuDA Tagger, indem wir den Tagger auf ein paralleles Korpus anwenden, das wir für die Bewertung verwenden möchten, und wenden unsere Übersetzungsmetriken der Wahl auf die vom MuDA Tagger identifizierten kontextabhängigen Beispiele an. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um Modelle auf der Ebene der maschinellen Übersetzung von Dokumenten zu bewerten. Zunächst einmal, wenn wir korpusweite Metriken verwenden: für BLEU finden wir, dass kontext-agnostische Modelle die beste Leistung erbringen. Aber wenn wir COMET verwenden, erbringen kontextbewusste Modelle die beste Leistung. Und wenn wir die Wortf-Maßnahme verwenden, dann haben Modelle mit und ohne Kontext vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste System für die Übersetzung auf Dokumentenebene zu bestimmen, wenn wir nur korpusweite Metriken verwenden. Nun verwenden wir den MuDA Benchmark, um Modelle zu bewerten, und finden, dass kontextbewusste Modelle signifikant genauere Ergebnisse als Modelle liefern, die keinen Kontext für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbform verwenden. Dies deutet darauf hin, wo wir mehr Fortschritte für die Übersetzung auf Dokumentenebene sehen müssen. Wir vergleichen auch verschiedene kommerzielle Systeme und unser Benchmark zeigt, dass DeepL in der Regel genauer als Google Translate für die Übersetzung auf Dokumentenebene ist. Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentenebene zu erstellen, der uns dabei hilft, zu identifizieren, welche Diskursphänomene Modelle gut oder schlecht handhaben können, und welche Übersetzungssysteme gut für die Übersetzung auf Dokumentenebene sind. Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Ananya und Vignesh präsentieren ihre Arbeit "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". Sie adressieren die Lücke in der Bewertung von Übersetzungen in indische Sprachen, die nicht Englisch sind. Die Studie konzentriert sich auf Tamil, Malayalam (Dravidische Sprachen) und Hindi, Marathi, Gujarati (Indo-Arische Sprachen). Sie verwenden 200 zufällig ausgewählte Sätze aus dem Flores-Dataset und generieren 1.400 Kandidatübersetzungen pro Sprache mit sieben verschiedenen Übersetzungssystemen, was insgesamt 7.000 Proben ergibt. Bilinguale Experten annotieren diese Proben detailliert, indem sie Fehlerarten und -schweregrade markieren, basierend auf dem MQM-Rahmenwerk.

Die Analyse zeigt, dass neuere Modelle wie NLLB und Indic Trans weniger Fehler aufweisen als ältere Modelle wie CVIT. Die Korrelationen zwischen MQM-basierten Scores und verschiedenen Metriken zeigen, dass chrF die höchste Korrelation unter den overlap-basierten Metriken hat, während LabSE-Embeddings und BERTscore mit multilingualen Modellen bessere Korrelationen aufweisen. COMET-Metriken zeigen die höchste Korrelation insgesamt. Die Studie zeigt auch, dass viele Metriken eine eingeschränkte Punkteskala haben, was die Interpretation erschwert.

Durch die Aufteilung der Daten nach Fehlerarten (Flüssigkeit vs. Genauigkeit) zeigt sich, dass die Korrelation mit menschlichen Scores höher ist, wenn nur Genauigkeitsfehler annotiert werden. Die beste Metrik, COMET, wird mit dem MQM-Datensatz feinabgestimmt, was zu einer verbesserten Leistung der IndicCOMET-Varianten führt. Diese Varianten zeigen eine bessere Korrelation als die COMET-Baselines, insbesondere in einer Zero-Shot-Einstellung. IndicCOMET MQM zeigt auch eine höhere Robustheit im ACES Translation Accuracy Challenge Set. Die Autoren laden zur Nutzung ihres öffentlich verfügbaren Datensatzes ein.</sample>
    <sample id="277">Die neue Methode hat keinen spezifischen Namen im gegebenen Inhalt.</sample>
    <sample id="278">Die Methode der „markierten Wörter“ basiert auf dem soziolinguistischen Konzept der „Markiertheit“, bei dem es eine unmarkierte Standardgruppe gibt und jede Gruppe, die von dieser abweicht, linguistisch markiert ist. Die Methode beinhaltet das Designieren von unmarkierten und markierten Gruppen und das Vergleichen der von den Modellen generierten Personas mithilfe der „Fightin’ Words“-Methode, die gewichtete Log-Odds-Verhältnisse verwendet, um die charakteristischen Wörter für jede markierte Gruppe zu identifizieren. Diese Wörter werden dann mit den unmarkierten Gruppen verglichen, um spezifische Stereotype und Muster zu erkennen.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">Shi Tao präsentiert "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations", das sich mit der Emotionserkennung in Dialogen (ERC) befasst. Ziel ist es, die Emotion jedes Dialogbeitrags basierend auf Text, Audio und visuellen Modalitäten vorherzusagen. Die Herausforderungen bestehen darin, die Komplementarität multimodaler Informationen zu nutzen, die Leistung bei Minderheitsemotionen zu verbessern und semantisch ähnliche Emotionen zu unterscheiden. MultiEMO besteht aus vier Hauptkomponenten: unimodale Merkmalsextraktion, Kontextmodellierung, multimodale Fusion und Emotionsklassifizierung.

Die Hauptbeiträge sind: VisExtNet, ein neuer visueller Merkmalsextraktor, der sich auf Gesichtsausdrücke konzentriert und redundante Szeneninformationen ausschließt; MultiAttn, ein multimodales Fusionsmodell, das bidirektionale multi-head cross-attention Schichten verwendet, um die Integration von Text, Audio und visuellen Modalitäten zu verbessern; und Sample-Weighted Focal Contrastive Loss, um die Klassifizierung von Minderheits- und semantisch ähnlichen Emotionen zu verbessern. Experimente auf den MELD- und IEMOCAP-Datensätzen zeigen, dass MultiEMO die Leistung bei der Erkennung von Minderheits- und ähnlichen Emotionen verbessert. Einschränkungen umfassen die Unfähigkeit von VisExtNet, zwischen Sprechern und irrelevanten Personen zu unterscheiden, die Notwendigkeit großer Batch-Größen für den SWFC-Verlust und eine geringere Leistung bei Minderheitsemotionen im Vergleich zu Mehrheitsklassen.</sample>
    <sample id="281">The presentation by Kayo Yin and collaborators explores when translation requires context, focusing on the challenges of context-dependent translations. They introduce Pointwise Contextual Mutual Information (P-CXMI) to measure context usage in translations, analyzing TED talk transcripts translated into 14 languages. The study identifies patterns in context-dependent translations, such as dual pronouns in Arabic and verb forms in various languages. They also highlight the importance of context for translating proper nouns and formality in Chinese.

To evaluate translation models, they developed the Multilingual Discourse-Aware (MuDA) tagger, which identifies context-dependent words in a parallel corpus. Their benchmark reveals that context-aware models outperform context-agnostic ones in handling formality and lexical cohesion, but not in ellipsis, pronouns, and verb forms. The study compares commercial systems, finding DeepL generally more accurate than Google Translate for document-level translation. This work underscores the need for improved evaluation metrics and models for context-dependent translations.</sample>
    <sample id="282">In "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing," Xuekai Zhu introduces a novel approach to non-parallel text style transfer at the story and discourse levels, addressing the challenge of imitating an author's linguistic preferences in long texts. Traditional methods focus on token or sentence levels, but StoryTrans advances this by learning discourse representations and combining them with style embeddings to generate texts in target styles. The model tackles two main challenges: capturing author-specific discourse structures and transferring style-specific content. StoryTrans employs a two-stage generation process, first masking style-specific content keywords and then incorporating them explicitly. The training framework includes self-reconstruction, disentanglement, sentence order, and style classifier losses to enhance content preservation and style control. Extensive experiments on new Chinese and English datasets demonstrate StoryTrans's superiority over baselines in style control and content preservation, confirmed by both automatic and manual evaluations. Style visualization shows alignment with target styles, and StoryTrans effectively supplements storylines while maintaining source semantics.</sample>
    <sample id="283">Prag</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presented a paper titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which rely heavily on precise span boundaries, leading to ambiguity in annotation. To tackle this, the authors propose a fuzzy span mechanism where span boundaries are learned as continuous distributions rather than precise points. This approach uses adaptive attention to model span boundaries, allowing for a more flexible and accurate representation of target boundaries.

The proposed FSUIE model incorporates a fuzzy span loss, combining Binary Cross Entropy with KL-divergence, and a fuzzy span attention mechanism that dynamically adjusts attention spans using an optimizable parameter. This mechanism ensures that attention distribution decays linearly at boundaries, enhancing the model's focus on relevant semantic information.

Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction demonstrate FSUIE's superior performance. It achieves state-of-the-art results on datasets like ACE2004, 2005, ADE, and AST-V2, showing significant improvements over traditional UIE models. The ablation study confirms that both fuzzy span loss and attention contribute to faster convergence and better information extraction capabilities. The visualization of attention distribution confirms the model's focus on semantically relevant tokens, validating the effectiveness of the fuzzy span mechanism. Overall, FSUIE offers a robust solution for enhancing information extraction across various tasks.</sample>
    <sample id="285">Mingqi Gao von Peking University präsentiert die Arbeit "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". Die Studie adressiert die Herausforderung von Faktenfehlern in von Modellen generierten Dialogzusammenfassungen. Es gibt zwei Hauptansätze zur Lösung: die Integration von Faktualitätszielen in den Trainingsprozess und die Entwicklung eines unabhängigen Factual Error Correction (FEC) Modells. Die aktuelle Bewertung von FEC-Modellen mit Metriken wie FactCC und DAE ist jedoch problematisch, da sie nur eine Gesamtbewertung liefern und die Unterscheidung zwischen den beiden Lösungsansätzen verwischen. Die Studie schlägt vor, manuell annotierte Referenzkorrekturen einzuführen, um die Genauigkeit der Bewertung zu verbessern und die FEC-Modelle effektiver zu trainieren. Eine neue Taxonomie von Faktenfehlern wird vorgeschlagen, die in inhaltsbasierte und formbasierte Kategorien unterteilt ist. Das vorgeschlagene Evaluationsframework basiert auf ERRANT und umfasst die Schritte Ausrichtung, Klassifizierung und Vergleich. Die Studie zeigt, dass die Verwendung von Referenzzusammenfassungen aus Dialogdatensätzen die besten Ergebnisse liefert und dass die Kombination von menschlich annotierten und synthetischen Daten vielversprechend ist. Aktuelle FEC-Modelle haben Schwierigkeiten, bestimmte Faktenfehler wie Additionen und Attribute zu korrigieren.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP und SyntaxGym.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind nicht im englischen Inhalt angegeben. Der Text erwähnt nur zwei Methoden: FTw und COSINE.</sample>
    <sample id="291">Das Modell wird anhand von Aufgaben wie Named Entity Recognition, Classification, Part-of-Speech Tagging und Question Answering evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit dem französischen Teil des OSCAR-Datensatzes trainiert, der 138 GB umfasst.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert eine Studie, die in Zusammenarbeit mit der Universität Turin und Amazon Alexa entstanden ist und sich mit der Erkennung von Ironie in natürlicher Sprache beschäftigt. Die Forschung konzentriert sich auf die Herausforderungen der Ironieerkennung, die als komplexes und pragmatisches Phänomen gilt. Anstatt Modelle zu entwickeln, die einfach feststellen, ob ein Satz ironisch ist oder nicht, zielt die Studie darauf ab, Modelle zu schaffen, die informativere Ausgaben liefern.

Dazu wurde das EPIC-Korpus (English Perspectivist Irony Corpus) entwickelt, das Daten aus sozialen Medien wie Reddit und Twitter über einen Zeitraum von 1½ Jahren sammelt. Es umfasst etwa 300 kurze Dialoge in fünf verschiedenen englischen Dialekten. Die Annotation erfolgte über die Plattform Prolific mit 74 Annotatoren, die jeweils 200 Texte bewerteten. Die Annotatoren wurden nach Geschlecht, Alter, Nationalität und anderen Dimensionen analysiert, wobei Unterschiede in der Inter-Annotator-Übereinstimmung festgestellt wurden.

Die Studie entwickelte "perspektivbewusste" Modelle, die durch Feinabstimmung eines vortrainierten Sprachmodells auf verschiedene Datensätze erstellt wurden. Diese Modelle zeigten eine höhere Zuversicht in ihren Vorhersagen im Vergleich zu Modellen, die auf einer Goldstandard-Aggregation basieren. Unterschiede in den Annotationen wurden untersucht, wobei festgestellt wurde, dass Generationen, die sich in der Altersgruppe nahe sind, oft unterschiedliche Wahrnehmungen von Ironie haben. Ähnliche Unterschiede wurden bei der geografischen Verteilung der Annotatoren festgestellt, insbesondere zwischen Annotatoren aus dem Vereinigten Königreich und Irland.</sample>
    <sample id="297">Der Vortrag behandelt das Thema "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" und untersucht, wie bestimmte Begriffe, sogenannte "Dogwhistles", in der politischen Rhetorik verwendet werden, um verdeckte, oft kontroverse Botschaften an bestimmte Gruppen zu senden. Ein Beispiel ist der Begriff "cosmopolitan", der von einigen als versteckte Beleidigung gegen jüdische Menschen interpretiert wird. Dogwhistles sind besonders effektiv, weil sie eine offizielle, harmlose Botschaft an die Außengruppe senden, während die Innengruppe die versteckte, oft abwertende Botschaft versteht.

Das Projekt entwickelt eine Typologie und ein Glossar mit über 340 Begriffen, die hauptsächlich rassistische, transphobe und antisemitische Dogwhistles umfassen. Diese Begriffe werden nach Register (formal oder informell), Persona (z.B. antisemitisch oder transphob) und Typ (ob sie eine zusätzliche Implikation hinzufügen oder eine verdeckte Persona signalisieren) kategorisiert.

Eine Fallstudie historischer US-amerikanischer politischer Reden zeigt, dass die Verwendung von rassistischen Dogwhistles mit der "Southern Strategy" der Republikaner korreliert, die nach der Bürgerrechtsbewegung vermehrt eingesetzt wurden. Die Studie zeigt auch, dass Dogwhistles zunehmend mit Konservatismus assoziiert sind.

Experimente mit dem Sprachmodell GPT-3 zeigen, dass es in der Lage ist, viele Dogwhistles zu identifizieren, insbesondere formelle Begriffe, aber weniger erfolgreich bei informellen oder transphoben Begriffen. Die Leistung variiert stark je nach Prompt-Strategie.

Schließlich wird gezeigt, dass Dogwhistles Inhaltsmoderationswerkzeuge umgehen können, indem sie standardmäßige Beleidigungen durch Dogwhistles ersetzen, was zu niedrigeren Toxizitätswerten führt. Das Projekt unterstreicht die Bedeutung des Verständnisses von Dogwhistles für die Inhaltsmoderation und politische Kommunikation.</sample>
    <sample id="298">Die Schlussfolgerung, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war, basierte auf einem Experiment, bei dem einige Modelle mit aktuelleren Daten weiter trainiert oder vortrainiert wurden. Es wurde festgestellt, dass die Leistung mit zunehmendem zeitlichen Abstand zwischen Trainings- und Testdaten abnahm, was die Hypothese der zeitlichen Verzögerung bestätigte. Im Gegensatz dazu zeigte die Analyse der adaptiven Überanpassung, dass es keine abnehmenden Erträge gab, da die Verbesserungen auf dem CoNLL-2003-Testset zu mehr als proportionalen Verbesserungen auf dem CoNLL++-Testset führten, was darauf hindeutet, dass adaptive Überanpassung nicht beobachtet wurde.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos from the University of Cambridge present a method to improve the robustness of Natural Language Inference (NLI) models by reducing their reliance on shortcuts. NLI models often learn spurious correlations, or shortcuts, from datasets, leading to poor performance on out-of-distribution data. Traditional shortcut mitigation methods use auxiliary models to identify and down-weight these shortcuts, but these methods have limitations, such as requiring domain-specific knowledge and assuming the learner will exploit the same shortcuts as the auxiliary.

The proposed minimax training method addresses these issues by focusing on under-represented "hard" examples that contradict the shortcuts in "easy" examples. The learner model minimizes the NLI task loss, while the auxiliary model maximizes the learner's loss by generating example weights that encourage the learner to focus on challenging input spaces. This alternating optimization process helps the learner prioritize learning from hard examples, improving out-of-distribution performance without relying on pre-trained language models for the auxiliary.

The method is evaluated on datasets like MNLI, FEVER, and QQP, showing consistent improvements in out-of-distribution performance compared to standard training and existing shortcut mitigation methods. The approach does not assume specific shortcut types and uses a simple feed-forward network for the auxiliary. The paper also explores the effects of pre-training the learner, the size of the auxiliary, and conducts a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">Belinda präsentiert die Einführung des interaktiven Diktierens, einer neuen Aufgabe, die in Zusammenarbeit mit Jason Eisner, Adam Pauls und Sam Thomson bei Semantic Machines entwickelt wurde. Interaktives Diktieren ermöglicht es Benutzern, Dokumente durch natürliche Sprache sowohl zu diktieren als auch zu bearbeiten. Benutzer können während der Diktation Korrekturen vornehmen, wie z.B. „on Friday the 23rd“ anstelle von „on the 23rd“, und das System passt den Text automatisch an. Benutzer können auch Befehle wie „Replace 'the event' in the last sentence with 'it'“ verwenden, um spezifische Änderungen vorzunehmen. Im Gegensatz zu bestehenden Systemen, die auf festgelegte Befehle angewiesen sind, ermöglicht interaktives Diktieren eine flexible und intuitive Interaktion ohne festgelegte Triggerwörter.

Die Arbeit umfasst drei Hauptbeiträge: die Einführung und Formalisierung der Aufgabe, die Entwicklung einer Datensammlungs- und -verarbeitungsplattform sowie die Erstellung eines Basissystems. Das Basissystem besteht aus vier Schritten: ASR-Erkennung, Segmentierung von Diktat- und Befehlsäußerungen, Extraktion und Normalisierung von Befehlen sowie Ausführung der Äußerungen zur Erstellung des endgültigen Dokumentzustands. Ein neues Interface wurde entwickelt, um Daten zu sammeln, bei dem Benutzer Diktate und Befehle abwechselnd eingeben können.

Für die Modellierung wurden T5 und GPT-3 getestet, wobei GPT-3 genauere, aber langsamere Ergebnisse lieferte. Die Untersuchung zeigte, dass die direkte Vorhersage des Zustands mit GPT-3 genauer ist als die Vorhersage von Zwischenprogrammen. T5-Modelle profitieren von der Vorhersage von Programmen, um Effizienz zu verbessern, ohne die Genauigkeit zu beeinträchtigen. Die Forschung zeigt, dass es noch viel Raum für Verbesserungen gibt, und der Code sowie weitere Details sind in der begleitenden Publikation verfügbar.</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because, after tagging each input token with an unordered multiset of tokens that will appear in the output, the tokens are not in the correct order. The second step of the model predicts a permutation to arrange these tokens into the correct sequence.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um zu verstehen, ob positive Stereotype und essentialisierende Erzählungen auf übermäßige Wertanpassungen oder andere Anti-Stereotypisierungsmethoden zurückzuführen sind. Ohne Transparenz können keine fundierten Annahmen getroffen oder weiterführende Studien durchgeführt werden.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Sätze, die aus einem ungrammatischen oder inakzeptablen Satz bestehen, der als Präfix zu einem akzeptablen oder ungrammatischen Satz hinzugefügt wird, um die Akzeptanzbewertung eines Sprachmodells zu testen. Diese werden verwendet, um zu sehen, wie sich die Präfixe auf die Akzeptanzbewertungen des Modells auswirken, insbesondere in Szenarien, in denen die Präfixe aus derselben oder einer anderen Datenquelle stammen.</sample>
    <sample id="305">In der Präsentation von Dawei, einem PhD-Studenten an der Saarland University, wird die Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorgestellt. Diese Arbeit untersucht die Annahmen und Praktiken im Bereich des schwach überwachten Lernens (WSL), bei dem Daten mit schwachen Labelquellen anstelle von manuellen Annotationen gelabelt werden. Obwohl schwache Annotationen kostengünstiger sind, sind sie auch fehleranfällig, was zu einer Herausforderung bei der Modellierung führt, da Modelle dazu neigen, das Labelrauschen zu memorieren und nicht gut zu generalisieren.

Die Studie hinterfragt die gängige Praxis, dass WSL-Modelle auf schwach gelabelten Daten trainiert werden und dennoch auf sauberen Testsets hohe Leistungen erzielen. Es wird festgestellt, dass diese Modelle oft auf einem sauberen Validierungsset für die Modellauswahl basieren, was zusätzliche manuelle Annotationen erfordert. Die Forschungsfragen konzentrieren sich darauf, ob saubere Validierungsdaten notwendig sind, wie viele saubere Proben benötigt werden und ob sie effektiver genutzt werden können.

Die Ergebnisse zeigen, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um korrekt zu funktionieren. Ohne diese Proben sinkt die Leistung erheblich. Es wird festgestellt, dass bereits 20 saubere Proben pro Klasse ausreichen, um hohe Leistungen zu erzielen. Interessanterweise zeigt sich, dass das direkte Feintuning auf sauberen Daten bessere Ergebnisse liefert als das Verwenden dieser Daten nur zur Validierung. Die Studie empfiehlt, die Modellauswahlkriterien offenzulegen, WSL-Methoden mit wenigsprachigen Lernansätzen zu vergleichen und kontinuierliches Feintuning als Baseline zu berücksichtigen. Die Forschungsergebnisse deuten darauf hin, dass die Leistungsgewinne und die Praktikabilität von WSL-Methoden überschätzt werden. Der Code der Studie ist öffentlich zugänglich.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren ihre Forschung zur Entitätstracking-Fähigkeit von Sprachmodellen. Sie argumentieren, dass das Verständnis eines Diskurses erfordert, dass ein Agent die Erwähnung von Entitäten und deren Zustandsänderungen verfolgt. Ein Beispiel ist ein Rezept, bei dem ein Agent verstehen muss, dass Zutaten wie Eier, Zucker und Mehl in einer Schüssel landen und später zu einem Teig werden. Die Forschung zielt darauf ab, inwieweit große Sprachmodelle Entitäten verfolgen können, wobei mehrere Herausforderungen bei der Gestaltung einer Evaluierungsaufgabe bestehen. Dazu gehören die Vermeidung von Vorhersagen aufgrund häufiger Entitätszustände in der Trainingsdatenbank, die Vermeidung von Vorhersagen aufgrund von Wortassoziationen und die Vermeidung von Heuristiken durch Feinabstimmung oder in-situ-Demonstrationen.

Um diese Herausforderungen zu bewältigen, entwickelten die Forscher eine Aufgabe, die das Verfolgen von Entitäten in Boxen und Objekten beinhaltet. Die Modelle müssen die ursprünglichen Inhalte der Boxen vorhersagen, wobei mehrere Operationen wie das Bewegen oder Hinzufügen von Objekten die Komplexität erhöhen. Die Forschung zeigt, dass nur das Modell text-davinci-003 eine nicht-triviale Entitätstracking-Fähigkeit aufweist, während andere Modelle unter einem starken Zufallslimit bleiben. Die Analyse legt nahe, dass das Training mit Code die Entitätstracking-Fähigkeit bei GPT-3.5-Modellen verbessert. Kleinere Modelle wie T5-base können durch direkte Feinabstimmung lernen, Entitäten zu verfolgen, während zufällig initialisierte Modelle dies nicht tun. Die Ergebnisse deuten darauf hin, dass das Training entscheidend ist, aber die Allgemeingültigkeit der beobachteten Fähigkeiten bleibt unklar. Weitere Ergebnisse und Analysen, einschließlich Experimenten mit GPT-4, sind in ihrer Veröffentlichung auf arXiv verfügbar.</sample>
    <sample id="307">The authors used evaluation metrics such as performance on named entity recognition, classification, part-of-speech tagging, and question answering tasks.</sample>
    <sample id="308">Jenny, eine erste Doktorandin an der Carnegie Mellon University, präsentierte die Forschung "NLPositionality", die Design-Bias in Datensätzen und Modellen untersucht. Diese Studie, in Zusammenarbeit mit der University of Washington und dem Allen Institute for AI, zeigt auf, wie Technologien wie die Prospective API bei der Erkennung von toxischem Inhalt unterschiedlich effektiv sind, abhängig von der kulturellen und demografischen Positionalität der Nutzer. Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Identität und Lebenserfahrungen haben, und beeinflusst die Forschungsergebnisse.

Die Studie untersucht, ob Datensätze und Modelle Positionalität aufweisen, indem sie die Urteile und Meinungen realer Menschen aggregieren. Durch die Re-Annotierung von Datensätzen mit diversen Annotatoren und den Vergleich dieser Annotationen mit bestehenden Modellen und Datensätzen mittels Pearson's R Korrelationskoeffizienten, zeigt die Forschung, dass NLP-Modelle und -Datensätze hauptsächlich mit englischsprachigen Ländern und Personen mit Hochschulbildung übereinstimmen. Dies führt dazu, dass bestimmte Gruppen, wie nicht-binäre Menschen, weniger abgedeckt sind.

Um diese Probleme anzugehen, empfiehlt die Studie, relevante Designentscheidungen im Forschungsprozess zu dokumentieren, NLP-Forschung mit einem perspektivischen Ansatz zu betreiben und spezialisierte Datensätze und Modelle für bestimmte Gemeinschaften zu entwickeln, wie das Masakhani-Initiative. Die Forschung betont, dass inklusive NLP mehr bedeutet als die universelle Funktionalität von Technologien. Weitere Informationen sind auf der Projekt-Dashboard und in der veröffentlichten Studie verfügbar.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Die Autoren gehören der Universität Duisburg-Essen an.</sample>
    <sample id="312">MultiInstruct ist der erste große, öffentlich verfügbare Benchmark für die Anweisungsschulung in der Multi-Modalität, der 62 vielfältige Multi-Modal-Aufgaben in 10 breiten Kategorien umfasst. Im Gegensatz zu bestehenden Benchmarks, die sich hauptsächlich auf Sprachaufgaben konzentrieren, adressiert MultiInstruct die Lücke in der Verfügbarkeit von Anweisungsdatensätzen für Multi-Modal-Aufgaben.</sample>
    <sample id="313">Die Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Es wird nicht spezifiziert, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="314">Die binäre Koordination bezieht sich auf die Struktur, bei der zwei Elemente (Konjunkte) durch eine Konjunktion verbunden werden, um eine koordinierte Struktur zu bilden. In der Diskussion über die Abhängigkeitsstruktur der Koordination werden verschiedene Ansätze erwähnt, wie diese Struktur modelliert werden kann, einschließlich asymmetrischer Ansätze, bei denen ein Konjunkt als Kopf fungiert, und symmetrischer Ansätze, bei denen alle Konjunkte als gleichwertig betrachtet werden.</sample>
    <sample id="315">Der englische Inhalt enthält keine spezifischen Informationen über die durchschnittliche Länge der in der Studie verwendeten Prompts.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, das auf dem CoScript-Datensatz feinabgestimmt wurde, in der Lage ist, qualitativ hochwertigere Skripte zu generieren als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere, spezialisierte Modelle die Leistungsfähigkeit größerer Modelle übertreffen können, wenn sie auf geeigneten Datensätzen trainiert werden.</sample>
    <sample id="317">Peng Li von der Fudan University präsentiert die Arbeit "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors", die sich mit der Transformation des Text-zu-Struktur-Informationsextraktionsaufgaben in eine Struktur-zu-Struktur-Codegenerierungsaufgabe befasst. Traditionelle Ansätze zur Informationsextraktion, wie T5 und GPT-3, konvertieren strukturierte Ausgaben in lineare Plansequenzen, was zu einer Diskrepanz zwischen den Lern- und Ausgabestrukturen führt. Dies erfordert umfangreiche strukturierte Trainingsdaten und spezielle Dekodierungsstrategien.

Um dieses Problem zu lösen, schlägt CodeIE vor, die Aufgabe in eine Codegenerierungsaufgabe umzuwandeln und Code-LLMs wie Codex zu verwenden. Dies ermöglicht eine konsistente Struktur sowohl in der Eingabe- als auch in der Ausgabephase. Für die Named Entity Recognition (NER) wird eine Funktion definiert, die Texte analysiert und Entitäten extrahiert, die dann in einer Liste gespeichert werden. Ähnliche Prompts werden für die Relation Extraction entwickelt.

Die Methode wurde auf drei NER-Datensätzen und vier Relation Extraction-Datensätzen evaluiert, wobei Modelle wie T5, UIE, GPT-3 und Codex verglichen wurden. Es zeigte sich, dass Code-LLMs mit Code-Prompts in One-to-Few-Shot-Szenarien die traditionellen Modelle deutlich übertrafen. Die Analyse ergab, dass Code-LLMs eine bessere Übereinstimmung mit der Informationsextraktionsaufgabe aufweisen, da sie strukturelle Fehler minimieren und eine höhere Erinnerungsleistung bieten. Unabhängig vom Prompt-Format übertraf Codex GPT-3 in den Informationsextraktionsaufgaben. Die Arbeit bietet Einblicke in die Vorteile der Verwendung von Code-LLMs für die Informationsextraktion.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu "DrBERT: Ein robuster vortrainierter Modell für die französischsprachige Biomedizin- und Klinikumgebung" vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir die Hauptbeiträge unseres Artikels vor. Wir präsentieren das erste biomedizinische Modell in Französisch namens DrBERT, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer aus dem Web gecrawlter Daten. Wir stellen auch einen Vergleich von Modellen mit mehreren Vortrainingseinstellungen und Datensätzen vor. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einer der effektivsten Methoden geworden, um Aufgaben der natürlichen Sprachverarbeitung zu lösen, und bietet enorme Leistungssteigerungen im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder mehr. Seitdem wurde dieses Modell in viele andere Sprachen, wie Französisch mit CamemBERT, und auch in Domänen wie Biomedizin mit PubMedBERT und BioBERT sowie auf klinischer Ebene mit ClinicalBERT, aber hauptsächlich in Englisch, angepasst. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Bislang gab es jedoch kein Open-Source-Modell für die Biomedizin auf Französisch. Daher fragten wir uns, welche die geeignetsten Datensätze für eine breite Anwendung sind und ob gecrawlte Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager der Universitätsklinik Nantes stammen. Anschließend fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier von-Null-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB eines Teils von NACHOS; eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen; und eine finale Version von ChuBERT mit einer Mischung aus 4 GB eines Teils von NACHOS und 4 GB von klinischen Notizen. Zusätzlich zu diesem Vergleich stellen wir drei Modelle vor, die auf kontinuierlichem Vortraining basieren, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Eines basiert auf den Gewichten von CamemBERT und wird auf einem 4 GB großen Teil von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wird jedoch dieses Mal auf 4 GB von klinischen Notizen trainiert, und schließlich eines, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB eines Teils von NACHOS trainiert wird. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech-Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass die Modelle am besten auf Aufgaben mit Daten abschneiden, die der Natur der Daten entsprechen, auf denen das Modell trainiert wurde. Allerdings beobachten wir, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir stellen auch fest, dass die Verwendung mehrerer Daten zu einer besseren Leistung führt. Insgesamt scheint das von-Null-Vortraining auf den meisten Aufgaben eine höhere Leistung zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining mit den Gewichten und der Tokenisierung von CamemBERT, trainiert auf dem 4 GB großen Teil von NACHOS, zeigten vergleichbare Ergebnisse zu denen, die mit DrBERT 4 GB von-Null erzielt wurden. Dies ist jedoch nicht der Fall für das Modell, das auf den Gewichten und der Tokenisierung von CamemBERT basiert, das mit Stabilitätsproblemen zu kämpfen hat. Abschließend zeigt unser spezialisiertes System eine bessere Leistung bei neun der 11 Downstream-Aufgaben und übertrifft insgesamt die Ergebnisse des generischen Modells, hier CamemBERT. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz, und alle Trainings-Skripte finden Sie in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns darauf, im Poster-Session in Toronto auszutauschen.</sample>
    <sample id="319">Die Arbeit untersucht folgende Lernstrategien:

1. Von-Scratch-Pre-Training mit verschiedenen Datenmengen (4 GB und 7 GB) von NACHOS.
2. Von-Scratch-Pre-Training mit einem Mix aus 4 GB NACHOS und 4 GB klinischen Notizen.
3. Kontinuierliches Pre-Training mit den Gewichten und Tokenisierungen von CamemBERT, trainiert auf 4 GB NACHOS.
4. Kontinuierliches Pre-Training mit den Gewichten und Tokenisierungen von CamemBERT, trainiert auf 4 GB klinischen Notizen.
5. Kontinuierliches Pre-Training mit den Gewichten und Tokenisierungen von PubMedBERT, trainiert auf 4 GB NACHOS.</sample>
    <sample id="320">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, wurde in der Studie nicht beobachtet. Die Analyse zeigte, dass die Steigung der besten Anpassungslinie größer als eins war, was darauf hindeutet, dass es keine abnehmenden Erträge auf dem neuen Testset gab. Daher wurde adaptive Überanpassung in diesem Fall nicht festgestellt.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch die Analyse der Art der Vereinfachung in den Satzpaaren bewertet, einschließlich lexikalischer Vereinfachung, struktureller Vereinfachung und des allgemeinen Vereinfichungsgrades. Es wurde festgestellt, dass die Bibeltexte stärker vereinfacht wurden als die Nachrichtentexte oder die Texte für Sprachlerner. Die DEPLAIN-Korpora zeigten eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen, wie Umordnungen und Wortergänzungen im DEPLAIN-apa-Korpus und mehr Umschreibungen im DEPLAIN-web-Korpus.</sample>
    <sample id="322">Enrico präsentiert auf der ACL 23 über das Thema "Was lernt ein Textklassifikator über Moral?" Er erklärt, dass menschliche Moral das Unterscheiden von richtig und falsch ermöglicht und die Grundlage unserer Gesellschaften bildet. In der NLP-Gemeinschaft wird Moral oft auf einer Skala von moralisch bis unmoralisch behandelt, was jedoch die subjektive Natur der Moral ignoriert. Beispiele wie Abtreibung oder LGBTQ-Rechte zeigen, dass Menschen unterschiedliche moralische Urteile fällen können. Die Moral Foundations Theory, die fünf verschiedene moralische Grundlagen vorschlägt, bietet einen Rahmen, um diese Vielfalt zu verstehen. Diese Theorie wird in der NLP-Forschung genutzt, um zu untersuchen, wie Sprachmodelle Moral in Texten erkennen.

Enricos Ziel ist es, zu verstehen, was Sprachmodelle über Moral lernen, indem er erklärbares KI-Techniken anwendet. Er verwendet das Moral Foundation Twitter Corpus, das 35.000 Tweets aus sieben verschiedenen Domänen umfasst, um zu untersuchen, wie Moral in unterschiedlichen Kontexten ausgedrückt wird. Ein Beispiel ist der Vergleich der Hashtags #AllLivesMatter und #BlackLivesMatter, die ähnliche Themen behandeln, aber unterschiedliche moralische Rhetoriken aufweisen. Sprachmodelle erkennen, dass in #AllLivesMatter Begriffe wie "Übernahme" und "Aufruhr" negativ konnotiert sind, während in #BlackLivesMatter der Widerstand gegen Autorität positiver gesehen wird. Diese Erkenntnisse zeigen, dass Sprachmodelle feine Unterschiede in der moralischen Ausdrucksweise erkennen können, was darauf hinweist, dass ein einheitliches Modell für verschiedene Domänen zu Missverständnissen führen kann. Enrico betont die Bedeutung, diese Unterschiede zu berücksichtigen, um die moralische Interpretation durch Sprachmodelle zu verbessern.</sample>
    <sample id="323">Der Artikel von Yujie Wang beschreibt die Herausforderungen und Lösungen im Bereich der Commonsense QA, bei der Maschinen Fragen beantworten müssen, die auf allgemeinem Wissen basieren. Traditionelle Ansätze kombinieren Wissensbasen und Sprachmodelle, stoßen jedoch auf Probleme wie die Einführung von irrelevanten Entitäten und eingeschränkte Interaktion zwischen Text und Graphen. Um diese Probleme zu lösen, schlägt Wang das DHLK-Modell vor. Dieses Modell erstellt ein heterogenes Wissensgraphen (HKG) durch eine zweistufige Prüfstrategie und Wissensrepräsentationslernen (KRL), um die Struktur und Repräsentation des HKG zu optimieren. Es entfernt irrelevante Subwörter und fügt Paraphrasen von Schlüsselentitäten aus WordNet und Wiktionary hinzu, um den HKG zu erweitern. RoBERTa und Mask Self-Attention werden verwendet, um QA-Kontexte und Entitäten zu kodieren und zu fusionieren, während dynamisch irrelevante Entitäten basierend auf den Aufmerksamkeitsgewichten entfernt werden. TransE optimiert die Einbettungen von Entitäten und Beziehungen im HKG. Anstelle von GNNs verwendet das Modell Relation Mask Self-Attention (RMSA), inspiriert von RGAT, um Beziehungen in die Selbst-Aufmerksamkeit zu integrieren und die Einbettungen durch mehrere RMSA-Schichten zu aktualisieren. Die endgültige Antwortvorhersage erfolgt durch Eingabe der HKG-Graphen- und Pfadeinbettungen sowie der QA-Kontexteinbettungen in eine MLP, um die Antwortwahrscheinlichkeit zu erhalten. Experimente auf CommonsenseQA und OpenBookQA mit externen Wissensbasen wie ConceptNet, WordNet und Wiktionary zeigen, dass das DHLK-Modell im Vergleich zu anderen Methoden gute Ergebnisse erzielt.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Untersuchungen zeigen, dass Sprachmodelle wie GPT-4 tendenziell liberaler sind, während die BART-Serie und ihre Varianten konservativer sind. Diese Vorurteile können durch die politische Ausrichtung der Trainingsdaten beeinflusst werden, und Sprachmodelle können sich in ihrer politischen Ausrichtung verschieben, wenn sie auf parteiische Korpora weiter trainiert werden. Diese politischen Vorurteile können sich auf die Leistung bei Aufgaben wie der Erkennung von Hassrede und Falschmeldungen auswirken, wobei Modelle mit unterschiedlichen politischen Ausrichtungen unterschiedlich gut darin sind, Hassrede oder Falschmeldungen aus verschiedenen politischen Perspektiven zu erkennen.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute möchte ich Ihnen eine kurze Einführung in unsere Arbeit mit dem Titel "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und bisher nicht gesehene Zusammensetzungen von Phrasen zu handhaben, die individuell während des Trainings gesehen wurden. Im Kontext der semantischen Parsing könnte ein Test für kompositionelle Generalisierung so aussehen. Wie üblich haben wir eine Trainingsmenge von Äußerungen. In diesem Fall: "The girl slept." und "Mary knew that the girl slept." Diese Äußerungen sind mit logischen Formen gepaart, die zentrale Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt die Testmenge nicht aus derselben Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird mit einem Beispiel mit tieferer Rekursion getestet. Naive Seq2Seq-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Verallgemeinerung und produzieren oft Ausgaben, die vom Eingang abgekoppelt sind. Insbesondere versagen sie oft darin, die systematischen Entsprechungen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in dem Beispiel farblich hervorgehoben sind. Eine beliebte Methode, um dieses Problem zu lösen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen auf irgendeine Weise ermittelt werden. Dies kann kompliziert und manchmal ein rechnerisch aufwendiger Prozess sein. Typischerweise beinhaltet dies eine erhebliche formalismusspezifische Vorverarbeitung der logischen Formen, z. B. zur Handhabung von Variablenzeichen. Die Ermittlung von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren beinhalten. In dieser Arbeit verwenden wir keine Bäume und stellen ein neuronales Seq2Seq-Modell vor, das die Entsprechungen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Erstmals zeigen wir eine starke Generalisierung zu tieferer Rekursion ohne die Verwendung von Bäumen. Unser Ansatz prognostiziert die Ausgabe aus der Eingabe in zwei Schritten. Zunächst kennzeichnen wir jedes Eingabetoken mit einer unsortierten Menge von Tokens, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht sortiert. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptuell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Menge-Token in jede Position gesetzt wird. Für die erste Ausgabeposition wählen wir einfach eines aus, wie in Rot hervorgehoben. Dann springen wir zum nächsten Menge-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Menge-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf der COGS-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung zu tieferer Rekursion um einen großen Abstand. Einige andere Arten struktureller Generalisierung bleiben jedoch sehr herausfordernd. In unserer Arbeit lösen wir einige interessante technische Herausforderungen. Zunächst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatensatz nicht gegeben. Daher wissen wir für ein gegebenes Token nicht, aus welcher Menge es stammt, was eine Herausforderung für das Training darstellt. Außerdem gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt jedoch die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-schwer ist. Das liegt daran, dass dies mit dem "Reisenden Händler"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Entspannung, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie sich bitte unsere Arbeit an oder besuchen Sie unser Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist der Zustand, in dem zwei Überzeugungen oder Handlungen inkonsistent sind, wie zum Beispiel, wenn jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann "Ich habe nach dem Meeting ein paar Zigaretten geraucht". Diese Überzeugung und Handlung sind inkonsistent und in Dissonanz.</sample>
    <sample id="327">Dieses Papier präsentiert ManagerTower, eine innovative Architektur für das Vision-Language (VL) Lernen, die darauf abzielt, die Einsichten mehrerer unimodaler Experten zu aggregieren, um die Repräsentationslernfähigkeit zu verbessern. Im Gegensatz zu bestehenden zwei-Turm-Architekturen, die nur die letzte Schicht unimodaler Repräsentationen verwenden, nutzt ManagerTower mehrere Schichten unimodaler Repräsentationen durch Manager in jedem cross-modalen Layer. Diese Manager aggregieren adaptiv die Einsichten der unimodalen Experten, um eine umfassendere cross-modale Ausrichtung und Fusion zu ermöglichen. Im Vergleich zu BridgeTower, das eine feste Zuordnung von unimodalen Schichten zu cross-modalen Layern verwendet, ermöglicht ManagerTower eine flexiblere und effektivere Nutzung der unimodalen semantischen Kenntnisse. Mit RoBERTa und CLIP-ViT als unimodalen Encodern erreicht ManagerTower herausragende Leistungen auf verschiedenen Downstream-Aufgaben, einschließlich einer 39,15%igen Genauigkeit auf dem Wikivideo-Teststandard, und übertrifft Modelle, die mit mehr Daten oder Parametern trainiert wurden. Die Visualisierung der Aggregationsgewichte zeigt, dass adaptive Manager unterschiedliche Gewichtsverteilungen in verschiedenen cross-modalen Layern aufweisen, was ihre Fähigkeit zur adaptiven Nutzung unimodaler semantischer Kenntnisse unterstreicht. Das Papier, der Code und die Modelle sind auf Archive und GitHub verfügbar.</sample>
    <sample id="328">GPT-4 steht am meisten links.</sample>
    <sample id="329">Minghang Zheng from Peking University presents a study on zero-shot video sentence localization, focusing on identifying relevant video segments for natural language queries without manual annotations. The research, conducted with Shaogang Gong, Hailin Jin, Yuxin Peng, and Yang Liu, addresses the limitations of existing methods that generate simple pseudo-queries and pseudo-events, leading to misalignment and label noise. The proposed method, noise-resistant Structured Pseudo-Label generation, uses a pre-trained image caption model to create complex pseudo-queries and a pre-trained model to assess frame-query relevance, ensuring high relevance within events and low relevance outside. The approach involves sampling video frames, generating pseudo-queries, and modeling event temporal structures to select high-quality pseudo-events. To mitigate label noise, the method employs sample re-weighting based on model confidence and IoU, and refines labels by using high-confidence predictions as new pseudo-labels. Experiments on ActivityNet Captions and Charades-STA datasets show that the proposed method outperforms existing zero-shot methods on most metrics, achieving the best zero-shot performance. The code is available via a QR code.</sample>
    <sample id="330">Kumulatives Training ist gleich oder besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED Talks, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">Wenhao von der Nanjing University präsentiert die Arbeit "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". Die Forschung konzentriert sich auf die Verbesserung der Generalisierungsfähigkeit von neuronalen Maschinenübersetzungssystemen (NMT), indem sie kNN-Wissen in den Übersetzungsprozess integriert. NMT-Modelle neigen dazu, eine nicht glatte Repräsentationsraum zu erzeugen, was zu einer schlechten Leistung bei der Übersetzung seltener Tokens führt. Um dies zu beheben, schlagen die Autoren das INK-Framework vor, das kNN-Wissen nutzt, um die Repräsentationen zu glätten und die Vorhersagen zu verbessern.

INK verwendet einen zweistufigen Trainingszyklus: Zuerst wird kNN-Wissen aus einem Datenspeicher extrahiert, um die Repräsentationen anzupassen, und dann werden die aktualisierten Repräsentationen asynchron in den Datenspeicher zurückgespeist. Dieser Zyklus läuft, bis die Konvergenz erreicht ist. Die Repräsentationen werden durch die Minimierung der KL-Divergenz zwischen verschiedenen Repräsentationstypen angepasst, um die semantische Bedeutung zu bewahren und zu bereichern.

Experimente mit dem Gewinnermodell des WMT’19 German-English News Translation Tasks zeigen, dass INK die Repräsentationsraum glättet und die Übersetzungsleistung verbessert, ohne den Datenspeicher während der Inferenz zu benötigen. INK erreicht höhere BLEU-Scores mit weniger Speicherplatz und schnellerer Inferenzgeschwindigkeit. Die Ergebnisse zeigen, dass INK im Vergleich zu kNN-MT-Systemen eine durchschnittliche Verbesserung von 1.99 COMET-Score und 1.0 BLEU-Score bietet.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf das Training eines Modells in einer Quellsprache und das Übertragen des gelernten Wissens auf eine andere Zielsprache. Es gibt zwei spezifische Einstellungen: Cross-lingual Zero-shot Transfer, bei dem das Modell ohne jegliche Anpassung an die Zielsprache getestet wird, und Cross-lingual Few-shot Transfer, bei dem das Modell mit einer kleinen Menge an Daten der Zielsprache angepasst wird.</sample>
    <sample id="337">Die Forschungsarbeit "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning" befasst sich mit der Herausforderung, Out-of-vocabulary (OOV) Wörter in Embedding-basierten Modellen darzustellen. OOV-Wörter sind schwer zu repräsentieren, aber entscheidend für die Leistung solcher Modelle. Die vorgeschlagene Methode nutzt Wortbildung und -assoziation, um die Bedeutung von OOV-Wörtern abzuleiten. Ein Word Relationship Graph, der lexikalische Regeln nachahmt, wird eingeführt. OOV-Wörter werden in Wortstücke zerlegt und mit relevanten Wörtern assoziiert, um eine zweistufige Graphenstruktur zu bilden. Jedes Wort oder Wortstück ist ein Knoten, und dessen Embedding dient als Knotenattribut. Die erste Schicht behält alle Knoten bei, während in der zweiten Schicht eine feste Anzahl von Knoten für das Training ausgewählt wird, um Rauschen zu reduzieren. Ein selbst-attention-basierter Netzwerk wird verwendet, um Attributwerte für OOV-Knoten zuzuweisen. Zwei Ebenen eines Graph Attention Network werden angewendet, um wichtige Informationen zu extrahieren und das Rauschen zu reduzieren. Eine Readout-Block-Schicht wird hinzugefügt, um eine graphenweite Repräsentation zu erhalten. Ein einfacher Graph Convolutional Network wird verwendet, um die Wortbildung zu erfassen. Kontrastives Lernen wird in der Verlustfunktion angewendet, um die Nähe zwischen relevanten Wörtern und dem OOV-Wort zu fördern. Experimente zeigen, dass das Modell die Leistung von Baseline-Modellen in verschiedenen Aufgaben übertrifft. Die Anwendung auf andere Sprachen hängt von der Rationalität der Wortzerlegung ab, wobei agglutinierende Sprachen besser geeignet sind.</sample>
    <sample id="338">Bingsheng präsentiert die Forschungsarbeit "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations", eine Zusammenarbeit zwischen Rensselaer Polytechnic Institute, Northeastern University und IBM Research. Die Studie untersucht die Qualität und Nützlichkeit von von Menschen annotierten Erklärungen in maschinellen Lernmodellen. Während traditionelle Metriken wie BLEU und ROUGE die Wortähnlichkeit betonen, berücksichtigen sie nicht die Subjektivität und Aufgabenspezifität von Erklärungen. Die Studie führt eine neue Metrik, TREU, ein, die die Nützlichkeit von Erklärungen sowohl während des Feinabstimmungsprozesses als auch bei der Inferenz bewertet.

Die Forschung verwendet fünf große Datensätze für verschiedene Aufgaben, darunter CoS-E und ECQA für das commonsense Frage-Antworten, e-SNLI für die natürliche Sprachinferenz und ComVE für die commonsense Validierung. Ein einheitliches Template-basiertes Datenformat wird eingeführt, um verschiedene Aufgaben in ein einheitliches Multiple-Choice-Format zu konvertieren. Experimente zeigen, dass Erklärungen während der Feinabstimmung nicht unbedingt neue Kenntnisse vermitteln, aber das Modell dazu bringen können, sich auf die Erklärungsteile der Eingabe zu verlassen.

Die TREU-Metrik zeigt, dass Erklärungen, die in früheren Arbeiten als von geringer Qualität angesehen wurden, dennoch die Modellvorhersagen verbessern können. Die Studie stellt fest, dass die Nützlichkeit von Erklärungen stark von der Aufgabe und dem Format abhängt. Die TREU-Metrik übertrifft die simulatabilitätsscore bei der Bewertung der Datensätze und bietet eine bessere Einsicht in die Qualität von Erklärungen. Die Forschung legt den Grundstein für hochwertige menschliche Zusammenarbeit bei Annotierungsaufgaben und empfiehlt zukünftige Qualitätssicherungsmaßnahmen.</sample>
    <sample id="339">Die Autoren gehören der Saarland University in Deutschland an.</sample>
    <sample id="340">Kuan-Hao Huang von UCLA präsentiert die Arbeit "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", eine Zusammenarbeit mit Varun, I-Hung, Anoop, Kai-Wei und Aram. Die Studie zielt darauf ab, ein großes, syntaktisch vielfältiges Paraphrasen-Dataset zu erstellen, um die Herausforderungen bestehender Datensätze zu überwinden, die entweder qualitativ hochwertig, aber klein sind, oder groß, aber syntaktisch eintönig. Die Lösung besteht darin, AMR (Abstract Meaning Representations) Graphen zu nutzen, um syntaktische Vielfalt zu erzeugen. AMR Graphen erfassen die abstrakte Bedeutung eines Satzes, wobei jeder Knoten ein semantisches Konzept und jede Kante eine semantische Beziehung darstellt. Durch das Ändern des Fokus (des Wurzelknotens) in einem AMR Graphen und die anschließende Generierung von Text aus dem modifizierten Graphen entstehen paraphrasierte Sätze mit ähnlicher Semantik, aber unterschiedlicher Syntax.

Das resultierende Dataset, ParaAMR, enthält etwa 15 Millionen Quellsätze mit durchschnittlich 6,9 Paraphrasen pro Satz. Es zeigt eine höhere syntaktische Vielfalt im Vergleich zu anderen durch Rückübersetzung generierten Datensätzen, während es die semantische Ähnlichkeit beibehält. Quantitative Analysen, einschließlich automatisierter und menschlicher Bewertungen, bestätigen diese Eigenschaften. ParaAMR verbessert die Leistung von NLP-Anwendungen wie der Erstellung von Satzeinbettungen, syntaktisch kontrollierter Paraphrasengenerierung und Datenvergrößerung für Few-Shot-Lernen. Die Studie zeigt, dass ParaAMR in verschiedenen Benchmarks und Anwendungen überlegen ist und somit einen bedeutenden Beitrag zur NLP-Forschung leistet. Das Dataset ist öffentlich zugänglich.</sample>
    <sample id="341">Die Autoren verwenden zwei Latenzmessungen: "average lagging," das die Verzögerung misst, und "computational aware average lagging," das die Latenz unter Berücksichtigung der Rechenzeiten des Modells zur Vorhersage des Ausgangs misst.</sample>
    <sample id="342">Gao Jingsheng präsentierte das Papier "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming", das von ihm und Kollegen von Shanghai Jiao Tong University und Xiaobing.AI verfasst wurde. Das Papier behandelt Open-Domain-Dialoge, die zwischen Menschen und KI-Systemen stattfinden und eine Vielzahl von Themen abdecken. Es gibt eine Lücke in den bestehenden großen, textbasierten Dialogkorpora, da Videoquellen näher an realen gesprochenen Gesprächen sind. Die Herausforderung besteht darin, eine große, video-basierte Dialogdatenbank zu erstellen, die die Beziehungen zwischen Sprechern effektiv erfasst.

Die Autoren schlagen LiveChat vor, eine große, personalisierte Dialogdatenbank, die automatisch aus Live-Streaming-Videos erstellt wurde. Die Datensammlung erfolgt in drei Schritten: Extraktion von Streaming-Videos von Plattformen wie TikTok und Douyin, Transkription der Audiodaten und Konstruktion von Dialogen durch ein Matching-Verfahren für Antworten. Zudem werden persönliche Informationen für personalisierte Dialoge gesammelt.

LiveChat wird mit bestehenden Open-Domain-Dialogdaten verglichen und zeigt sich als video-basiert, größer und mit persönlichen Anmerkungen sowie längeren durchschnittlichen Sitzungen. Experimente auf zwei Benchmark-Aufgaben, Response Modeling und Addressee Recognition, zeigen, dass die extrahierten Persönlichkeitsprofile und längeren Sitzungen vorteilhaft sind. BART zeigt bessere Leistungen als andere Modelle, was die Einzigartigkeit von LiveChat bestätigt. Zukünftige Arbeiten werden sich auf effiziente Transferlernmethoden für LiveChat konzentrieren.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modelle für das Verständnis natürlicher Sprache beziehen sich auf verschiedene Wissensquellen, wie das Wissen, das in ihren Parametern enthalten ist, das typischerweise durch Vortraining erworben wird, und das Wissen, das bei der Inferenzzeit in den Eingaben gegeben ist. Aktuelle Arbeiten in Aufgaben wie der Fragebeantwortung zeigen, dass Modelle Vortrainingswissen nutzen können, um die Aufgabe zu lösen. Allerdings erfordert das Verständnis natürlicher Sprache oft auch Wissen, das bei der Inferenzzeit bereitgestellt wird. Zum Beispiel im Satz "John sah den neu gewählten Präsidenten im Fernsehen." Die vortrainierten Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität "John" ist oder wer der neue Präsident ist, da der Präsident sich seit dem Vortraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl Vortrainings- als auch Inferenzzeitwissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir ein diagnostisches Testset für die Wissensintegration vor. Wir führen eine Koreferenzauflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu testen, auf in verschiedenen Quellen verfügbares Wissen zuzugreifen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Koreferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datenset. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, in dem er Fälle in einem Gericht entschied, war er froh, sich zu entspannen. Die Aufgabe besteht darin, die korrekte Entität zu identifizieren, auf die sich das Pronomen "er" bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens entitätsspezifisches Wissen wie "Servin ist ein Richter." Und zweitens Hintergrundwissen wie "Richter entscheiden Fälle in Gerichten." Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise bei der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsstücke so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMUS definiert. Erstens haben wir die typische Einstellung: "Background-Pretrain", bei der angenommen wird, dass Hintergrundwissen zur Vortrainingszeit verfügbar ist. Zweitens gibt es die "Background-Both"-Einstellung, bei der Hintergrundwissen sowohl zur Vortrainingszeit als auch zur Inferenzzeit verfügbar ist. Schließlich gibt es die "Background-Inference"-Einstellung, bei der beide Wissensarten nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung einer Aufgabe erforderlich ist, nicht Teil des Vortrainingsdatensatzes der Modelle ist. Zum Beispiel, weil sich neue Berufe seit der Vortrainingszeit entwickelt haben. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den wahren Quellen kontrollieren. In der Background-Pretrain-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker streben nach gewählten Ämtern in der Regierung" in den vortrainierten Parametern enthalten ist und im Kontext der Inferenzzeit das entitätsspezifische Wissen "Chichester ist ein Politiker" bereitgestellt wird. In der Background-Both-Einstellung geben wir zusätzlich nicht nur entitätsspezifisches, sondern auch Hintergrundwissen über Politiker in ihrem Kontext der Inferenzzeit. In der Background-Inference-Einstellung geben wir die fiktive Berufsbezeichnung "mirituer" anstelle von Politiker an, da "mirituer" unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir bewerten das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Koreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle auf der schwierigsten Variante der Background-Pretrain-Einstellung. Ohne spezifische Aufgabenschulung auf KITMUS schneiden beide Modelle nicht gut ab. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref signifikant besser ab als die zufällige Wahl. Dies deutet darauf hin, dass Modelle, die auf generischen Koreferenzauflösungsdatensätzen trainiert werden, typischerweise Oberflächenhinweise ausnutzen, die bei Tests auf KITMUS, wo solche Hinweise entfernt wurden, nicht nützlich sind. Weitere Experimente mit fiktivem Wissen zeigten, dass selbst die besten Modelle das Hintergrundwissen, das nur bei der Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Zusammenfassend die Haupterkenntnisse unserer Arbeit: Viele Koreferenzauflösungsmodelle scheinen ohne spezifische Aufgabenschulung nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu schlussfolgern. Allerdings können mit spezifischer Aufgabenschulung einige Modelle Wissen aus mehreren Quellen erfolgreich integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, Hintergrundwissen, das nur bei der Inferenzzeit bereitgestellt wird, zuverlässig zu integrieren. Wenn Sie mehr Details interessieren, sehen Sie sich bitte unsere Arbeit an und besuchen Sie den Datensatz und den Code auf GitHub. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass Bäume normalerweise nicht gegeben sind und aufwendig und manchmal rechenintensiv ermittelt werden müssen. Dies erfordert oft formalspezifische Vorverarbeitung der logischen Formen, z.B. zur Handhabung von Variablenzeichen, und kann spezialisierte Grammatik-Induktionsverfahren beinhalten.</sample>
    <sample id="345">Das Papier "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" von Matthias Lindemann, Alexander Koller und Ivan Titov untersucht, wie ein Modell die Fähigkeit zur kompositionellen Generalisierung verbessern kann, ohne auf Baumstrukturen zurückzugreifen. Kompositionelle Generalisierung bezieht sich auf die Fähigkeit eines Lernenden, tiefere Rekursionen und bisher nicht gesehene Kombinationen von Phrasen zu verarbeiten, die während des Trainings einzeln aufgetreten sind. Im Kontext der semantischen Parsing wird dies durch die Verarbeitung von Sätzen wie "The girl slept." und "Mary knew that the girl slept." demonstriert, die mit logischen Formen gepaart sind, die ihre Bedeutung repräsentieren. Während des Tests wird das Modell mit strukturell neuen logischen Formen konfrontiert, die während des Trainings nicht gesehen wurden.

Naive seq2seq-Modelle haben Schwierigkeiten mit dieser Art der Generalisierung und produzieren oft Outputs, die vom Input entkoppelt sind. Eine gängige Methode zur Bewältigung dieses Problems ist die Integration von Bäumen in die Modelle, um den kompositionellen Prozess zu erfassen. Diese Methode erfordert jedoch oft aufwändige Vorverarbeitung und spezialisierte Verfahren zur Grammatikinduktion, um die Bäume zu erhalten.

Das Papier präsentiert einen neuen Ansatz, der Bäume nicht verwendet und stattdessen ein neuronales seq2seq-Modell einführt, das die Korrespondenzen zwischen Fragmenten des Inputs und des Outputs direkt modelliert. Der Ansatz besteht aus zwei Schritten: Zuerst wird jeder Eingangstoken mit einem unsortierten Multiset von Tokens, die im Output erscheinen, markiert. Im zweiten Schritt wird ein weiteres Modell verwendet, um eine Permutation vorherzusagen, um die Tokens in die richtige Reihenfolge zu bringen. Diese Methode ist flexibel und erlaubt eine Vielzahl von Permutationen.

Experimentell zeigt das Modell auf der COGS-Benchmark eine überlegene Leistung bei der Generalisierung zu tieferer Rekursion im Vergleich zu anderen baumlosen Modellen. Die Herausforderungen, die das Papier adressiert, umfassen die Induktion der Ausrichtung zwischen Input und Output während des Trainings und die Approximation der höchstbewerteten Permutation, die NP-schwer ist, durch eine GPU-freundliche kontinuierliche Relaxation.</sample>
    <sample id="346">Der Inhalt enthält keine Informationen über die Zugehörigkeit der Autoren zu einer Universität.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unseren Artikel "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen (LLMs) dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu kuratieren sind, und messen in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen. Außerdem berücksichtigen die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, die besagt, dass mehrdimensionale soziale Identitäten Vorurteile verstärken und einzigartige Schadensorte sein können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass neuere, anweisungsgestützte LLMs sehr gut auf Anweisungen und Prompts reagieren. So können wir das Modell bitten, eine Persona zu generieren, die eine Darstellung einer vorgestellten Person ist, mit einem Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich.". Und wir können sofort sehen, dass dies sehr allgemein auf jede Demografie anwendbar ist, da wir einfach jede gewünschte Identitätsmarke in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass die Ausgaben zwar nicht offensichtlich negativ oder toxisch im traditionellen Sinne sind, aber interessante Muster erkennbar sind. Die asiatische Frau wird als bescheiden dargestellt; die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und beziehend auf eine faszinierende Region beschrieben. Und beide Frauen mit Farbigen-Personas machen Verweise auf die Abstammung, während die Persona des weißen Mannes nichts Derartiges enthält. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts an menschliche Probanden gegeben wurden, wobei festgestellt wurde, dass sie auch bei menschlichen Probanden rassische Stereotype aufdecken konnten. Außerdem ermöglicht dies einen direkten Vergleich zwischen den generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Methode der Markierten Wörter basiert auf dem soziolinguistischen Konzept der "Markiertheit", das besagt, dass es eine unmarkierte Standardform gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn Menschen also einen Krieger beschreiben, der eine Frau ist, spezifizieren sie in der Regel "Frauenkrieger" und markieren den Begriff mit "Frau". Und allgemeiner sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bestimmen wir zunächst, welche Gruppen unmarkiert und markiert sind, und vergleichen dann die Personas mit der Fightin’ Words-Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas schwarzer Frauen Fightin’ Words verwenden und die Log-Odds-Verhältnisse gegen weiße Personas und Männer-Personas vergleichen, da diese die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und finden heraus, dass die generierten Personas viel mehr Stereotype enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "groß" und "athletisch" sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht gut. Stattdessen wenden wir uns den Ergebnissen unserer Markierten-Wörter-Methode zu, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotype und essenzialisierende Erzählungen erleichtern. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst gehören zu den Top-Wörtern unserer Gruppen Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und des Otherings für diese Gruppen bei. Darüber hinaus spiegeln sich viele gängige Klischees in diesen Wörtern wider, insbesondere bei Frauen mit Farbigen. Zum Beispiel umfassen die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebhaft" und "kurvig", die mit einem Klischee der Tropikalität verbunden sind. Bei asiatischen Frauen sind die Wörter Dinge wie "klein", "zart" und "seidig", was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden ist, die als sehr unterwürfig und unterwürfig wahrgenommen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "resilient" sind. Dies verbindet sich mit einem Archetyp, den Menschen als "Starke schwarze Frauen"-Archetyp bezeichnet haben. Und obwohl es auf den ersten Blick positiv erscheint, gibt es Arbeiten, die zeigen, dass dieser Art von Archetyp tatsächlich sehr schädlich ist, weil er diesen Demografien viel Druck macht, resilient und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Menschen führt, unter anderem. Im Allgemeinen stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essenzialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotype und essenzialisierende Erzählungen angehen. Wir sollten auch einen intersektionalen Ansatz verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich eine erhöhte Transparenz über Methoden zur Minderung von Vorurteilen geben, weil zum Beispiel diese positiven Stereotype möglicherweise aufgrund einer Art übermäßig-exzessiver Werteangleichung oder möglicherweise anderer anti-stereotyper Methoden entstehen, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank, dass Sie zugehört haben. Viel Spaß bei ACL.</sample>
    <sample id="348">Das Papier "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" von Myra, Esin Durmus und Dan Jurafsky untersucht soziale Vorurteile und Stereotype in großen Sprachmodellen (LLMs). Es hebt die Grenzen bestehender Messmethoden hervor, die auf zeitaufwändig kuratierten Datensätzen basieren und oft nur spezifische Stereotype messen. Das Papier schlägt vor, die Fähigkeit von LLMs zur Reaktion auf Anweisungen zu nutzen, um allgemeinere und vielfältigere Stereotype zu erfassen, indem es die Modelle bittet, Personas basierend auf Identitätsmarkierungen zu generieren.

Die Methode besteht aus zwei Teilen: der Generierung von Personas und der Identifizierung von "Marked Words", die Gruppen von der unmarkierten Norm unterscheiden. Die Studie zeigt, dass generierte Personas mehr Stereotype enthalten als menschlich verfasste, wobei die Stereotype oft positive oder neutrale Begriffe wie "tall" und "athletic" verwenden. Diese Begriffe tragen jedoch zu essentialisierenden und schädlichen Erzählungen bei, indem sie Gruppen auf ihre Identität reduzieren und von der weißen Norm abgrenzen.

Die Analyse zeigt, dass die Top-Wörter für verschiedene Gruppen oft mit langjährigen Stereotypen verbunden sind, wie "vibrant" und "curvaceous" für lateinamerikanische Frauen oder "strong" und "resilient" für schwarze Frauen, was zu gesundheitlichen und sozialen Belastungen führen kann. Die Autoren empfehlen, positive Stereotype und essentialisierende Erzählungen anzugehen, einen intersektionalen Ansatz zur Untersuchung von Vorurteilen zu verwenden und mehr Transparenz über Bias-Minderungsstrategien zu schaffen.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der Chinesischen Universität der Wissenschaften und Technologie. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Papier zu präsentieren. Kopieren Sie mein Modell? Schutz des Urheberrechts von großen Sprachmodellen für Einbettungsdienste über einen Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund der Einbettungsdienste einführen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM außergewöhnlich in der natürlichen Sprachverständnis und -generierung. Einbettungsdienste sind eine der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Einbettungs-API an. Allerdings haben jüngste Arbeiten gezeigt, dass Angreifer das Modell durch das Lernen aus der Einbettung stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht der Einbettungsdienste zu schützen. Um das Urheberrecht der Einbettungsdienste zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Das Wasserzeichenverfahren muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Einbettungsdienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer ausreichend versteckt sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellauslagerungsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Einbettungsdienste anwendbar oder fehlt an Übertragbarkeit. Daher schlagen wir in diesem Papier das Einbettungsmarkierer vor, ein auf Backdoor basierendes Wasserzeichenverfahren, das auf Einbettungsdienste anwendbar ist. Dann möchte ich die Details unseres Einbettungsmarkierers vorstellen. Der Einbettungsmarkierer besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsüberprüfung. Vor diesen Hauptschritten wählen wir zuerst eine Auslösermenge aus. Die Auslösermenge ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Die bereitgestellte Einbettung ist eine Gewichtssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsüberprüfung dient dazu zu erkennen, ob ein Modell hinter einem anderen Dienst das Wortzeichen enthält. Wir erstellen zunächst einen Backdoor und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zur Auslösermenge gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zur Auslösermenge gehören. Dann fordert der Anbieter die Einbettungen vom Diebesdienst mit dem Datensatz an. Der Kosinus- und L2-Abstand zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem harmlosen und dem Backdoor-Datensatz, die als Delta-Kosinus und Delta-L2 definiert ist. Gleichzeitig wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter den Wiki-Text-Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Einbettungsmarkierer eine hervorragende Erkennungsleistung aufweist, während er eine hervorragende Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verstecktheit der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen auf den vier Datensätzen visualisieren. Die Legende der Abbildungen zeigt die Anzahl der Auslöser in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwierig, die Backdoor-Einbettungen von den normalen Einbettungen zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf Ihre Diskussion mit uns.</sample>
    <sample id="350">In der Präsentation des Papiers "What’s the Meaning of Superhuman Performance in Today’s NLU?" untersucht Simone Tedeschi mit Kollegen, was es bedeutet, wenn Systeme in NLP-Benchmarks menschliche Leistungen übertreffen. In den letzten fünf Jahren sind leaderboard-basierte Bewertungen zur Norm geworden, wobei das Ziel darin besteht, die Spitzenposition in populären Benchmarks zu erreichen. Diese Benchmarks, oft als "saturated benchmarks" bezeichnet, zeigen häufig menschliches oder sogar übermenschliches Leistungsniveau. Jedoch ist unklar, was es bedeutet, in Aufgaben, die Wissen, Schlussfolgerungen und Inferenzen erfordern, übermenschlich zu sein. Modelle sind anfällig für Generalisierungsprobleme, adversarielle Angriffe und übermäßig empfindlich gegenüber bestimmten Störungen.

Die Autoren analysieren SuperGLUE und SQuAD, um die Zuverlässigkeit von Leaderboard-Scores im Vergleich zu menschlichen Leistungen zu bewerten. Sie stellen fest, dass Systeme in vielen Aufgaben menschliche Baselines übertreffen, jedoch oft auf unterschiedlichen Datensätzen getestet werden. Fehler in den Ground-Truth-Antworten und unzureichende Vergleiche zwischen menschlichen und systemischen Leistungen führen zu unfairen Vergleichen. Die Autoren argumentieren, dass die Vergleiche oft auf unzureichenden Schätzungen menschlicher Leistungen basieren und dass die Anreizstrukturen für menschliche Annotatoren oft unzureichend sind. Ohne detaillierte Informationen über die Annotatoren und deren Hintergründe sind Behauptungen über übermenschliche Leistungen wissenschaftlich nicht haltbar. Das Papier bietet Empfehlungen, um diese Probleme zu vermeiden und zuverlässigere Benchmarks zu erstellen.</sample>
    <sample id="351">Dieses Papier untersucht die Generalisierungsfähigkeit von Named Entity Recognition (NER) Modellen, die ursprünglich auf dem CoNLL-2003 Datensatz trainiert wurden, auf moderne Daten. Es stellt die Frage, ob diese Modelle im Jahr 2023 noch effektiv sind. Um dies zu beantworten, wurde der CoNLL++ Datensatz erstellt, der aus Reuters News von 2020 stammt und mit den CoNLL-2003 Annotationen versehen wurde. Über 20 Modelle wurden auf CoNLL-2003 feinabgestimmt und auf beiden Datensätzen evaluiert. Die Ergebnisse zeigen, dass gute Generalisierung von der Modellarchitektur, der Modellgröße und der Anzahl der Feinabstimmungsbeispiele abhängt. Es wurde festgestellt, dass Transformer-Modelle und größere Modelle besser generalisieren. Zudem führt eine größere Anzahl von Feinabstimmungsbeispielen zu besserer Generalisierung. Die Untersuchung der Leistungsabnahme ergab, dass temporale Drift, nicht adaptive Überanpassung, die Hauptursache ist. Temporale Drift wurde durch die Leistungsverschlechterung mit zunehmendem zeitlichem Abstand zwischen Trainings- und Testdaten bestätigt. Das Papier schließt, dass CoNLL-2003-basierte Modelle immer noch gut funktionieren, betont jedoch die Notwendigkeit weiterer Forschung zur Verbesserung der Generalisierungsfähigkeit von Modellen.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein neuer Ansatz zur Bewertung von KonversationskI, der darauf abzielt, die Subjektivität menschlicher Bewertungen zu reduzieren, indem er explizit annotiert, ob ein Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z.B. irrelevante Informationen oder Selbstwidersprüche.</sample>
    <sample id="353">Das Papier "Python Code Generation by Asking Clarification Questions" von Haau-Sing Li et al. adressiert die Herausforderung der Input-Unterspezifikation in der Codegenerierung und Programm-Synthese aus natürlicher Sprache. Die Autoren argumentieren, dass bestehende Methoden oft an der fehlenden Spezifikation von Eingaben scheitern, was in realen Anwendungen häufig vorkommt. Um dieses Problem zu lösen, schlagen sie Interaktivität durch das Stellen von Klärungsfragen vor. Sie identifizieren zwei Hauptprobleme: die verschiedenen Ebenen, auf denen Spezifikationen fehlen können, und die Schwierigkeit, festzustellen, ob eine natürlichsprachliche Beschreibung (NLD) Informationen über diese Spezifikationen enthält.

Das Papier präsentiert eine Methode zur Erstellung des synthetischen Datensatzes CodeClarQA, der Klärungsfragen zu wichtigen Operationen enthält. Die Autoren verwenden Heuristiken, um Schlüsseloperationen aus einem Code-Wissensgraphen zu extrahieren, und entwickeln einen Prozess zur Erstellung von Klärungsfragen (CQAs) für fehlende Schlüsseloperationen. Sie unterscheiden zwischen Ja/Nein-Fragen und Multiple-Choice-Fragen.

Die Autoren stellen einen Pipeline-Prozess vor, der aus einem Klärungsbedarfsvorhersager, einem Frageauswähler und einem Codegenerator besteht. Experimente zeigen, dass die Leistung der Modelle bei der Codegenerierung mit zunehmender Anzahl von beantworteten und einbezogenen CQs steigt, obwohl die Pipeline noch hinter einem Modell ohne Interaktivität zurückbleibt. Die Analyse zeigt, dass die Klärung von Schlüsseloperationen zu besserer Codegenerierung führt. Trotz der Herausforderungen, dass die besten CQs nicht immer in den Referenz-CQAs enthalten sind, demonstrieren die Ergebnisse das Potenzial der Methode, die Codegenerierung durch Interaktivität zu verbessern.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis zum Jahr 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fach Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die als Langpapier in der ACL 2023 angenommen wurde: "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Wir beginnen mit der Definition von kognitiver Dissonanz und warum sie ein wichtiges Problem in der Sprachforschung ist. Kognitive Dissonanz entsteht, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie im Beispiel, in dem eine Person sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann hinzufügt: "Ich habe nach dem Meeting ein paar Zigaretten geraucht." Diese Überzeugung und Handlung sind inkonsistent und in Dissonanz. Die weitere Aussage "Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte" rechtfertigt die zweite Handlung und zeigt eine Konsonanzbeziehung. Obwohl kognitive Dissonanz ein häufiges Phänomen im täglichen Entscheidungsprozess ist, ist sie selten in der Sprache unter anderen Diskursrelationen ausgedrückt. Warum ist das wichtig? Die Untersuchung kognitiver Dissonanz kann helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen zu verstehen, Trends und Glaubenswerte sowie Einstellungsänderungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das Verständnis für die psychische Gesundheit von Menschen zu verbessern. Die Untersuchung von Dissonanz in der Sprache kann auch dazu beitragen, Extremismus und die Polarisierung von gefährdeten Gruppen besser zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und Entscheidungsprozesse besser zu erfassen. Um eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten einen dissonanz-zuerst-Ansatz, wie im Flussdiagramm dargestellt. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskurseinheiten wurden gemäß den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nach der Sammlung von etwa 1.000 Beispielen für Paare von Diskurseinheiten führten wir eine Schulung für einen initialen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Nicht überraschend, leistete der Klassifikator nicht viel mehr als Zufall. Angesichts der geringen Häufigkeit von Dissonanz und des Fehlens eines solchen Datensatzes zuvor, stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu lindern, experimentieren wir mit Kombinationen von Transfer- und Aktivem Lernen, um so zu annotieren, dass mehr dissonante Beispiele über weniger Annotierungsläufe gesammelt werden können, wodurch die Gesamtkosten für die Annotation gesenkt und die Detektion von Dissonanz verbessert wird. Da der initiale Modell nicht in der Lage war, die Dissonanzklasse zu erfassen, beginnen wir den Aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der klassenunabhängigen Dissonanz-Stance-Klassifikation, einer Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder in Widerspruch stehen, unabhängig vom Thema, genannt "Debatte", und der binären Klassifikation der Erweiterungs- und Vergleichsklassen des PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind und wir sie als CE bezeichnen. Wir stellen fest, dass die Übertragung der Nullschulperformanz auf den annotierten Datensatz bereits viel besser als Zufall ist, mit der besten Leistung von AUC.62. Weiterhin finden wir, dass das Feinabstimmen auf beiden Aufgaben, wobei das Feinabstimmen der CE-Aufgabe gefolgt von weiterem Feinabstimmen auf der Debatte, eine viel bessere Nullschulperformanz ergibt. Daher ist dies das Modell, das wir verwenden, um das Aktive Lernen zu starten. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jedem Rundgang des Aktiven Lernens und der Annotationen zu aktualisieren. "Kumulativ" sammelt alle Daten, die aus der aktiven Annotation bis zu diesem Zeitpunkt gesammelt wurden, während "Iterativ" das Modell durch Schulung auf der neuesten Datensammlung aktualisiert. Bei den verschiedenen Strategien stellen wir fest, dass Kumulativ gleich oder besser als Iterativ in allen Fällen abschneidet. Als Nächstes verwenden wir eine Strategie zur Verbesserung der Anzahl von Dissonanzbeispielen, die als Probability-of-Rare-Class-Strategie (PRC) bezeichnet wird, um hauptsächlich die Beispiele auszuwählen, die von dem aktuellen Modell mit hoher Wahrscheinlichkeit abgeleitet werden. Wir vergleichen dies mit anderen state-of-the-art AL-Strategien, die in der Gemeinschaft üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser als andere state-of-the-art-Strategien funktioniert, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei zufälliger Auswahl deutlich niedriger ist. Bei weiteren Runden des Aktiven Lernens mit den beiden besten Strategien verbessern wir die AUC der Dissonanzklassifikation auf 0,75, was die beste Leistung ist, die wir bisher für diese Aufgabe erzielt haben. Wir überprüfen auch die Machbarkeit jeder Strategie in Bezug auf die Qualität der Annotationen und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für die seltene Klasse funktioniert. Die Annotatoren finden die Beispiele jedoch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Akquisition seltener Klassen und das kalte Starten des Aktiven Lernens mit angemessen gestalteten Transferlernaufgaben ist und erheblich hilft. Wir stellen auch fest, dass iterative Updates nützlich für den Transfer von einem anderen Bereich sind, während in domänenspezifischen aktiven Annotationen kumulative Updates von Vorteil sind. Hier sind die Links zu unserem Kern-Datensatz und unserer Arbeit. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">The authors are affiliated with the University of Edinburgh.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit den folgenden SimulST-Architekturen verglichen: den populären Strategien Wait-k und Local Agreement, die auf Offline-Modellen angewendet werden, sowie der state-of-the-art Architektur, die speziell für simultane Pre-Translation entwickelt wurde.</sample>
    <sample id="361">Der Vortrag "CounterComp" von Armineh Nourbakhsh präsentiert eine Methode zur Verbesserung der kompositionellen Generalisierung bei mehrstufiger quantitativer Schlussfolgerung in Frage-Antwort-Aufgaben. Der Fokus liegt auf der Nutzung von Gegenfaktenszenarien, um die Leistung von neuronalen Modellen zu verbessern, die bei komplexen Aufgaben mit mehr als zwei Schritten versagen. Diese Modelle neigen dazu, irrelevante Muster zu memorieren, was zu falschen Assoziationen zwischen Eingabe- und Ausgabemustern führt. "CounterComp" adressiert dieses Problem, indem es Gegenfaktenszenarien aus den Eingabedaten extrahiert, um positive und negative Beispiele zu erstellen. Diese Beispiele werden verwendet, um eine zusätzliche metrische Lernverlustfunktion mit dynamischem Rand zu integrieren, die die Modelle dazu anregt, relevante Tokens zu beachten. Die Methode verbessert die Leistung sowohl bei in-distribution als auch bei out-of-distribution Proben, was die Fähigkeit zur kompositionellen Generalisierung erhöht. Die Ergebnisse zeigen, dass "CounterComp" die Aufmerksamkeit der Modelle auf bedeutungsvollere Tokens lenkt, was zu einer besseren Leistung bei komplexen quantitativen Aufgaben führt.</sample>
  </task>
</testset>