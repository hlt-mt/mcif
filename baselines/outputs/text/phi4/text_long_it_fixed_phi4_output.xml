<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici includono grandi set di dati web crawlati come il C4 Corpus, che coprono ampiamente i media di notizie politiche come il New York Times, il Los Angeles Times, The Guardian e Huffington Post.</sample>
    <sample id="1">Le affiliazioni degli autori dell'articolo sono McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">This paper presents LayoutMask, a novel pre-trained model developed by Ant Group to address reading order issues in Visually-rich Document Understanding (VrDU). Traditional document pre-training models use global 1D positions to represent token order, which can lead to inaccuracies in understanding complex document layouts. LayoutMask introduces local 1D positions, which represent in-segment token orders, allowing the model to infer global reading order by integrating 1D and 2D positions with semantic information. This approach enhances text-layout interactions and improves layout representation learning.

To further promote these interactions, LayoutMask incorporates two innovative masking strategies in its Masked Language Modeling (MLM) task: Whole Word Masking and Layout-Aware Masking. Whole Word Masking challenges the model by masking entire words, requiring it to utilize broader context for prediction, while Layout-Aware Masking increases the likelihood of masking the first and last words in segments, encouraging the model to learn cross-segment orders.

Additionally, LayoutMask introduces a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions. This task is akin to a cloze test, where the model must use semantic and spatial clues to determine the correct positions of words, further enhancing text-layout interactions.

Experimental results demonstrate that LayoutMask's local 1D position outperforms global 1D positions on the FUNSD and SROIE datasets, although it slightly underperforms on CORD. The performance gap is primarily due to the entity "Total," which benefits from the adaptive nature of local 1D positions in complex layouts. Overall, LayoutMask shows significant promise in improving document understanding by effectively integrating text and layout information.</sample>
    <sample id="3">Ciao! Benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e di frase. Mi chiamo Omar e ora parlerò dei casi d'uso per il nostro dataset DEPLAIN. 

Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico. Negli ultimi anni, sono stati proposti molti metodi di allineamento, principalmente nel contesto delle traduzioni automatiche, dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Tuttavia, nel nostro caso, cerchiamo di estrarre allineamenti tra frasi di due documenti paralleli nello stesso linguaggio, con lo stesso contenuto, ma a diversi livelli di complessità. Ora, avendo il nostro dataset DEPLAIN con frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard d'oro per valutare alcuni dei metodi proposti. Abbiamo apportato alcune modifiche ai metodi proposti e pubblicato tutte queste modifiche e i codici per eseguire i nostri esperimenti nel paper. Alla fine, abbiamo concluso che il miglior metodo di allineamento automatico da utilizzare per la semplificazione di testi in tedesco è il metodo MASSalign. E puoi trovare il codice per eseguire questo metodo sui tuoi documenti nel paper.

Il secondo caso d'uso che abbiamo mostrato nel nostro paper è un caso di semplificazione automatica del testo tramite il fine-tuning di modelli di linguaggio per produrre testo semplificato dall'input complesso. Abbiamo fine-tuning di due modelli diversi. Abbiamo fine-tuning del modello long-mBART per produrre semplificazioni a livello di documento e abbiamo anche fine-tuning del normale base mBART per produrre semplificazioni a livello di frase. Puoi trovare tutti i checkpoint e puoi approfondire i dettagli sui punteggi e le metriche di valutazione dei nostri esperimenti nel paper. Abbiamo concluso che questo fine-tuning di base potrebbe ottenere punteggi migliori rispetto ai punteggi di base e abbiamo proposto questi risultati come base di riferimento per il problema della semplificazione automatica del testo in futuro.

Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL per ottenere un'accuratezza dell'82%-87% quando il modello ha accesso a una parte sovrapposta del background knowledge.</sample>
    <sample id="6">Il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" presenta un nuovo approccio chiamato many-to-many summarization, che unifica le tecniche di multilingual e cross-lingual summarization. Questo approccio mira a sviluppare un singolo modello di riassunto in grado di elaborare documenti in qualsiasi lingua di origine e generare riassunti in qualsiasi lingua di destinazione. Gli autori hanno condotto studi preliminari che dimostrano che il many-to-many summarization consente un trasferimento più efficace delle conoscenze del compito tra diverse lingue rispetto ai metodi esistenti. Hanno proposto PISCES, un modello pre-addestrato per il many-to-many summarization, che impara abilità di modellazione del linguaggio, capacità cross-linguistiche e abilità di riassunto attraverso un pre-addestramento a tre fasi. Gli esperimenti condotti su WikiLingua con lingue come inglese, francese, hindi, cinese, thailandese e turco mostrano che il modello many-to-many supera le prestazioni dei modelli multilingue e cross-linguistici tradizionali. PISCES, in particolare, ha superato vari baselines, inclusi mBART-50 e mT5, dimostrando la sua efficacia attraverso studi di ablation e valutazioni umane.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora nel 2023.</sample>
    <sample id="8">Il metodo di valutazione umana proposto, ABC-Eval, riduce la soggettività della valutazione umana annotando esplicitamente se le risposte del modello esprimono comportamenti specifici, come fornire informazioni irrilevanti o contraddirsi. Questo approccio mira a fornire una valutazione più precisa e affidabile delle diverse dimensioni della qualità del dialogo rispetto ai metodi esistenti.</sample>
    <sample id="9">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sull'uso di un set di validazione pulito per la selezione del modello.</sample>
    <sample id="10">Per migliorare il punteggio, i progressi possono essere fatti migliorando la capacità dei modelli di recuperare e comprendere il contesto e le informazioni di sfondo rilevanti. Questo potrebbe includere:

1. Migliorare le tecniche di recupero delle informazioni per ottenere un accesso più accurato e completo alle informazioni di sfondo.
2. Sviluppare modelli che possano comprendere e integrare meglio le informazioni contestuali e di sfondo.
3. Espandere e diversificare i dati di addestramento per includere una gamma più ampia di espressioni indirette e contesti.
4. Migliorare le tecniche di rappresentazione del linguaggio per catturare meglio le sfumature e le implicazioni delle espressioni indirette.
5. Implementare approcci di apprendimento multi-domini per migliorare la generalizzabilità del modello attraverso diversi domini.</sample>
    <sample id="11">Jack Hessel, un ricercatore presso AI2, presenta "Do Androids Laugh at Electric Sheep? Humor Understanding Benchmarks from The New Yorker Caption Contest," un progetto collaborativo con istituzioni come l'Università dello Utah, Cornell University, University of Washington, Air Mail e OpenAI. L'obiettivo è valutare la capacità dei modelli di linguaggio di generare e spiegare umorismo, in particolare attraverso il contesto dei cartoni di The New Yorker. Sebbene i modelli di linguaggio di grandi dimensioni, come PaLM di Google e ChatGPT, possano generare e affermare di spiegare battute, la loro comprensione dell'umorismo è messa in discussione. Ad esempio, ChatGPT spesso fallisce nel creare battute coerenti, come nel caso di una battuta su un ananas. Per valutare sistematicamente queste capacità, il team ha utilizzato i dati del contesto dei sottotitoli di The New Yorker, operazionalizzati in tre compiti: abbinamento, classificazione della qualità e generazione di spiegazioni. I modelli, inclusi CLIP e GPT-4, hanno mostrato prestazioni inferiori rispetto agli esseri umani, con CLIP che raggiunge il 62% di accuratezza nel compito di abbinamento rispetto al 94% degli esseri umani. Anche GPT-4, nonostante le descrizioni umane delle immagini, ha mostrato una significativa lacuna di prestazioni rispetto agli esseri umani. Le spiegazioni generate da GPT-4 sono state spesso preferite meno rispetto a quelle umane in studi di valutazione. Il team è entusiasta di condividere il loro dataset e incoraggia ulteriori ricerche, con un leaderboard e modelli disponibili online.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presents "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," conducted at the Hebrew University under Professor Roy Schwartz. The study focuses on adaptive inference methods, specifically Multi Model and Early Exit, to reduce inference time for large language models by leveraging the varying complexity of real-world data. Multi Model involves sequentially running multiple models with classifiers, while Early Exit uses classifiers at intermediate transformer layers to halt computation early. Multi Model is versatile but incurs storage and overhead costs, whereas Early Exit is memory efficient but suffers from shared model parameters leading to conflicting gradients, which degrade performance. The research hypothesizes that conflicting gradients occur when classifiers update model weights independently, interfering with each other. Testing with BERT models showed Multi Model classifiers outperforming Early Exit by 2.3%, with the largest gap in early classifiers. The study introduces SWEET (Separating Weights in Early Exit Transformers), a novel method that trains each layer with updates only from its following classifier, eliminating conflicting gradients. Results indicate SWEET closes the performance gap between Early Exit and Multi Model, particularly at high inference speeds, and outperforms both methods for BERT-Large across the speed/accuracy curve. The work highlights the existence of conflicting gradients in Early Exit and presents the first fair comparison of the two methods, suggesting future research directions for fine-tuning algorithms tailored to Early Exit architectures.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo intervento riguarda la Struttura di Dipendenza della Coordinazione. Come sapete, ci sono diverse strutture di dipendenza assunte da diverse teorie e approcci basati su corpora. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" è tale che il primo congiunto è la testa dell'intera struttura coordinata. In questo caso, "Lisa". Un approccio simile è assunto nella Teoria del Testo Significativo di Igor Mel'čuk, dove ancora una volta l'intera struttura coordinata è guidata dal primo congiunto. Questi due approcci sono asimmetrici. Scegliamo un congiunto. Ora, questi sono approcci asimmetrici alle strutture coordinate, come l'approccio di Praga. L'approccio guidato dalla congiunzione, assunto nei corpora di dipendenze di Praga, dove le strutture coordinate sono guidate dalla congiunzione. Otteniamo alcune dipendenze dalla congiunzione a tutti i congiunti. Infine, c'è anche un approccio multi-testa utilizzato, ad esempio, nella Grammatica delle Parole di Hudson, dove dicono che tutti i congiunti sono teste della struttura coordinata. Otteniamo dipendenze dal governatore. Qui "ama" a tutti i congiunti separatamente: "Lisa", "Bart" e "Maggie". L'obiettivo di questo articolo è produrre un nuovo argomento a favore delle strutture simmetriche di coordinazione, come queste due, e contro le strutture asimmetriche di coordinazione, come queste due. L'argomento si basa sul principio di minimizzazione della lunghezza della dipendenza che spiegherò attraverso questi esempi. In inglese, come potreste sapere, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbiali possono essere più lontani. "Marge ha letto questo ieri" è accettabile perché l'oggetto diretto è vicino al verbo, mentre "Marge ha letto ieri questo" è molto peggiore. Qui, tra il verbo e l'oggetto diretto c'è un avverbiale: "ieri". Tuttavia, questo effetto può essere attenuato quando l'oggetto diretto è molto pesante e lungo. In tal caso, può essere spostato nella posizione dopo l'avverbiale. Questo è illustrato qui. Entrambe queste frasi sono accettabili. "Marge ha letto questo libro assolutamente affascinante sui fiori ieri." Va bene invece di "questo", abbiamo questo lungo NP. Ma è anche OK dire, "Marge ha letto ieri questo libro assolutamente affascinante sui fiori." Il ragionamento qui è che questo è possibile perché, sebbene questa frase violi il principio grammaticale generale secondo cui gli oggetti diretti dovrebbero essere accanto al verbo, soddisfa il principio di minimizzazione della lunghezza della dipendenza, che afferma che le dipendenze più corte sono preferite. Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture. Qui abbiamo una dipendenza da "ha letto" all'avverbiale di lunghezza 7 misurata in parole e da "ha letto" a "libro" di lunghezza 4, quindi insieme è 11. Quando si scambiano questi due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più corto. Per questo motivo, suona abbastanza bene. Violiamo un principio, ma soddisfiamo un altro. Ok. Abbiamo estratto varie statistiche sulla coordinazione dalla versione migliorata del Penn Treebank e vedete l'articolo "Perché non userebbero le dipendenze universali" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti di sinistra tendono ad essere più corti. "Sale e pepe" e non "pepe e sale", misurati in sillabe. E anche l'osservazione fatta nell'analisi che questa tendenza cresce con la differenza di lunghezza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto preferisce essere il primo, più forte, giusto? Quindi la proporzione è maggiore di un congiunto di sinistra più corto. Ma ciò che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Il governatore è a sinistra in questo esempio "Ho visto Bart e Lisa", così come è il governatore a sinistra. È assente nel secondo esempio "Homer è venuto e ha starnutito." Qui abbiamo la coordinazione di due verbi e non c'è un governatore esterno. In tali casi, il congiunto di sinistra preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ha riso" governa la coordinazione di Ted e Ned, questo effetto scompare. Abbiamo mostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la colonna centrale, e in parole la colonna di destra. Mi concentrerò sulla destra. Cosa vediamo qui è che quando il governatore è a sinistra, la tendenza per il congiunto di sinistra ad essere più corto cresce costantemente con la differenza assoluta in parole, e lo stesso è osservato quando non c'è governatore, come nella coordinazione delle frasi. Ma quando il governatore è a destra, questa tendenza scompare. E mostriamo nell'articolo come questo fornisce un argomento contro le strutture asimmetriche di coordinazione, come queste due, e a favore delle strutture simmetriche, come queste due. Vedete l'articolo per gli argomenti completi. E parlateci al tavolo informativo. Grazie.</sample>
    <sample id="15">Tre autori sono coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi della Bibbia risultano più semplificati rispetto ai testi di notizie o ai testi per apprendenti di lingue.</sample>
    <sample id="17">This work introduces a novel approach to multimodal relation extraction (MRE) by addressing the challenges of internal-information over-utilization and external-information under-exploitation. Traditional relation extraction focuses on text, but in multimodal contexts like social media, additional visual information can provide crucial context. However, not all text or visual data is equally useful, necessitating fine-grained information pruning. Additionally, there can be a deficiency in information even when visual data is included, suggesting the need for external information like topic data.

To tackle these issues, the authors propose a Graph Information Bottleneck (GIB) principle-guided feature refinement method. This involves representing text and images as visual and textual scene graphs, respectively, and merging them into a unified cross-modal graph (CMG). The CMG is then refined by filtering nodes and adjusting edges, guided by the GIB principle. The refined CMG features are enriched with multimodal topic features, retrieved and integrated using an attention mechanism.

Experiments on a widely used MRE dataset demonstrate that the proposed method outperforms text-based methods and other multimodal baselines. Ablation studies show that both information screening and compensating contribute to performance improvements, with scene graphs proving beneficial for structural modeling. The effectiveness of internal-information screening and external-information exploiting varies with text-vision relevance: higher relevance benefits more from screening, while lower relevance benefits more from exploiting external information.

In summary, the work presents a novel simultaneous information subtraction and addition approach for MRE, achieving significant improvements over existing models.</sample>
    <sample id="18">L'esempio della preferenza per i congiunti a sinistra più brevi è "salt and pepper" rispetto a "pepper and salt", misurato in sillabe.</sample>
    <sample id="19">Zhang Qin, a master's student from Shenzhen University, presents their work "A Survey for Efficient Open Domain Question Answering," accepted by ACL 2023. The paper addresses challenges in open-domain question answering (ODQA), focusing on the two-stage model framework by Danqi Chen (2017), which involves retrieval and reading stages. The retrieval stage uses encoders to search a large Wikipedia corpus, while the reading stage reasons out answers from retrieved evidence. Key challenges include the large size of the Wikipedia corpus (26 million documents, 20 GB) and the 65 GB index file, which slows down inference speed. Additionally, the use of multiple large language models complicates real-time applications and deployment on resource-constrained devices.

The motivation is to develop efficient ODQA systems with smaller memory costs, faster inference, and comparable performance. The paper explores core techniques, including one-stage frameworks like retrieval-only and generator-only systems. Efficient tactics discussed include approximate nearest neighbor search for fast evidence retrieval, adaptive computation for fast reading, and methods like document filtering and embedding compression to reduce index size. Model size reduction can be achieved through lightweight models, parameter sharing, or unified models for retrieval and reading.

The paper compares existing ODQA models, noting that retrieval and reader systems balance speed, memory, and performance, while retrieval-only systems offer quick inference at the cost of large indexes, and generator-only systems avoid indexes but are large and less performant. Conclusions suggest reducing index size with generator-only systems or embedding compression, and model size with knowledge distillation or one-stage models. For real-time feedback, retrieval-only systems are recommended, while retrieval and reader systems are better for balanced trade-offs. Future work includes deploying ODQA systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Sì, puoi usare i modelli per la tua ricerca. I modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face sotto la licenza MIT, e tutti gli script di addestramento sono disponibili nel repository GitHub del tuo team.</sample>
    <sample id="21">DEPLAIN-apa contiene testi di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono: un'architettura del modello efficace (in particolare i modelli trasformatori), una dimensione del modello più grande e un numero maggiore di esempi di fine-tuning.</sample>
    <sample id="23">The paper discusses the challenges faced by text-to-image models, particularly the Imagen model, in accurately rendering text within generated images. Despite advancements in generating high-quality images, these models struggle with text representation due to the limitations of the T5-XXL encoder's SentencePiece tokenization, which breaks text into subword IDs rather than individual letters. This results in poor spelling accuracy, even in larger T5 models. In contrast, the PaLM models, which are larger and trained on more data, perform better in spelling tasks. The ByT5 model, which processes individual bytes of input, demonstrates superior spelling accuracy across all scales due to its access to character-level information. The study reveals that T5 models struggle more with frequent words, which are often represented by fewer subwords. To address these issues, the authors propose augmenting the Imagen model with a ByT5-small text representation, which significantly improves text rendering capabilities with minimal parameter increase. However, the diffusion model can still introduce errors during generation. The paper introduces the WikiSpell benchmark for text-only models, the DrawText benchmark for text-to-image models, and a new strategy for enhancing spelling accuracy by incorporating character-aware models.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in termini di differenza di lunghezza in caratteri, sillabe e parole. L'analisi si è concentrata principalmente sulla misurazione in parole, osservando come la tendenza cresce con la differenza assoluta di lunghezza in parole tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati analizzando le statistiche di coordinazione dall'Enhanced Penn Treebank. Hanno osservato la tendenza dei congiunti a sinistra di essere più brevi rispetto a quelli a destra, misurando la lunghezza in sillabe e parole. Hanno scoperto che questa tendenza era più forte quando il governatore era a sinistra o assente, ma scompariva quando il governatore era a destra. Questo è stato misurato in tre modi: lunghezza in caratteri, sillabe e parole, concentrandosi principalmente sulla lunghezza in parole.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati, con solo 43 esempi di dissonanza, ha prestazioni non molto migliori di quelle casuali.</sample>
    <sample id="27">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione automatica (MT) sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per i fenomeni del discorso come la formalità e la coesione lessicale.</sample>
    <sample id="30">Il paper "LLM-Blender" introduce un framework di apprendimento ensemble semplice ed efficace per modelli di linguaggio di grandi dimensioni, basato su ranking a coppie e fusione generativa. Gli autori, Yuchen Lin e il team di AI2 e USC, evidenziano che, nonostante alcuni modelli di linguaggio di grandi dimensioni mostrino prestazioni superiori in media, la scelta del modello ottimale può variare significativamente a seconda dell'input specifico. Ad esempio, il modello Vicuna, sebbene eccelle in termini di prestazioni complessive, è il modello migliore solo nel 21% degli esempi. Pertanto, propongono LLM-Blender, un framework a due fasi che utilizza più modelli per ciascun input per selezionare e generare output migliori. La prima fase utilizza un modulo di ranking a coppie, PairRanker, che confronta le uscite di n modelli e le ordina. La seconda fase seleziona le migliori tre uscite e le fonde utilizzando un modello di fusione generativa. Il paper sottolinea che PairRanker, che analizza le differenze tra coppie di candidati, supera i metodi tradizionali di ranking. Gli autori creano anche il dataset MixInstruct per valutare i framework di apprendimento ensemble e dimostrano che LLM-Blender supera i modelli top come Open Assistant e Vicuna in una significativa percentuale di esempi. In sintesi, LLM-Blender rappresenta un approccio promettente per migliorare le prestazioni dei modelli di linguaggio di grandi dimensioni attraverso l'ensemble learning.</sample>
    <sample id="31">Le affiliazioni degli autori non sono menzionate nel contenuto fornito.</sample>
    <sample id="33">Il framework NLPositionality quantifica la posizionalità riannotando i dataset con annotatori diversi, considerando le loro demografie, e poi confrontando queste annotazioni con i modelli e i dataset esistenti utilizzando il punteggio di correlazione di Pearson. Questo confronto tra annotazioni demografiche e modelli/dataset differisce dalla letteratura sull'accordo degli annotatori, concentrandosi invece sul confronto tra utenti finali e modelli/dataset.</sample>
    <sample id="34">Marcos Treviso introduces "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation," developed in collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. CREST combines selective rationalization and counterfactual text generation to enhance interpretability and decision-making in text classifiers. The framework consists of a rationalizer model with a trainable masker to produce meaningful rationales and an editor, a masked language model, to generate counterfactuals by masking parts of the input and prepending the gold label. Human evaluations on IMDB and SNLI datasets show that CREST-generated counterfactuals are more valid and natural than those from MiCE, though manual counterfactuals remain superior. CREST also supports data augmentation and rationalization using both factual and counterfactual examples, improving model performance across various datasets. Experiments demonstrate that CREST-Rationalization yields more plausible rationales with higher counterfactual simulability, focusing on contrasting input parts. The framework's ability to produce valid, fluent, and diverse counterfactuals enhances downstream model performance, offering interpretable rationales that guide classifier decisions effectively.</sample>
    <sample id="36">Il lavoro "Learning Language-Specific Layers for Multilingual Machine Translation" di Telmo Pessoa Pires e colleghi esplora l'uso di Language-Specific Layers (LSLs) per migliorare la capacità di traduzione multilingue mantenendo costi di inferenza costanti. I vantaggi della traduzione multilingue includono scalabilità, velocità e riduzione degli errori, con miglioramenti significativi per coppie di lingue a bassa risorsa. Tuttavia, la capacità per lingua è limitata, e aumentare la dimensione del modello può complicare l'addestramento e rallentare l'inferenza. LSLs risolvono questo problema introducendo un normale strato trasformatore per lingua, selezionato in base alla lingua di origine o destinazione, mantenendo così costanti i costi di inferenza.

Gli autori hanno esplorato la posizione ottimale degli LSLs, scoprendo che la loro collocazione nell'encoder è più vantaggiosa rispetto al decoder. Hanno addestrato un modello con pesi condivisi, di origine e di destinazione per ciascuno strato dell'encoder, analizzando i pesi per determinare la posizione ottimale degli LSLs. La selezione si basa sul peso più grande, risultando in un'architettura con strati condivisi e specifici per lingua. Gli esperimenti su WMT21 news translation mask sources per 10 lingue hanno mostrato che l'architettura appresa supera significativamente sia i modelli di adattatori linguistici che i modelli di baseline, con miglioramenti particolarmente evidenti per le lingue a bassa risorsa. I test statistici confermano che le migliorie sono significative per 84 delle 90 direzioni di traduzione.</sample>
    <sample id="37">Lo studio precedente in cui i soggetti umani hanno ricevuto gli stessi prompt di persona ha rivelato che anche gli esseri umani hanno manifestato stereotipi razziali.</sample>
    <sample id="38">Lo studio ha utilizzato statistiche estratte dall'Enhanced version of the Penn Treebank.</sample>
    <sample id="39">L'articolo menziona tre autori: Adam Przepiórkowski, Igor Mel'čuk e Rodney Huddleston.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva menzionate nel contenuto includono:

1. **Classificazione dello Stato di Dissonanza nel Debate**: Determinare se due dichiarazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.

2. **Classificazione Binaria delle Classi di Espansione e Confronto (CE) del PDTB**: Classificare le relazioni di espansione e confronto, che sono strettamente legate al concetto di consonanza e dissonanza.</sample>
    <sample id="41">This work introduces PeaCoK, a Persona-grounded Commonsense Knowledge Graph, designed to enhance narrative consistency and engagement by representing world-level persona knowledge. PeaCoK includes approximately 3,800 personas and 40,000 attributes, forming about 100,000 personal inferences. It features 9,200 attributes linked to multiple personas, fostering rich interconnections. The graph is constructed in three steps: selecting personas from existing commonsense graphs, inducing attributes from knowledge graphs and language models, and crowdsourcing relation annotations with AI-assisted majority voting, achieving 87% F1 accuracy. PeaCoK is used to train a BART-based model for persona attribute inference, outperforming large-scale pre-trained models like GPT-3 in both automatic and human evaluations. Additionally, PeaCoK enhances dialogue generation in the ConvAI2 PersonaChat dataset, with human evaluations showing improvements in fluency, consistency, engagement, and persona expression. Compared to Atomic2020, PeaCoK's persona-centric knowledge has a more positive impact, especially when speakers share common attributes, underscoring the importance of interconnected persona knowledge in narratives. The paper and resources are publicly available.</sample>
    <sample id="42">L'estratto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="43">Il contenuto non specifica il numero di autori coinvolti nell'articolo.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti in quanto confronta le annotazioni degli utenti finali con i dataset e i modelli esistenti, utilizzando un punteggio di correlazione di Pearson per valutare la posizionalità. A differenza della letteratura sull'accordo degli annotatori, che si concentra sull'accordo tra annotatori o sulla modellazione delle distribuzioni degli annotatori, NLPositionality si concentra sul confronto tra le previsioni e i modelli con le etichette e le annotazioni degli utenti finali. Inoltre, il framework riannota i dataset con annotatori diversi per ottenere un set di dati demografici più ricco, poiché le demografie degli annotatori originali sono raramente raccolte e condivise.</sample>
    <sample id="45">Le generazioni di persona generate contengono molto più stereotipi rispetto alle risposte scritte dall'uomo.</sample>
    <sample id="46">DeepL e Google Translate sono stati messi a confronto.</sample>
    <sample id="47">Ciao, sono Shangbin, dottorando presso l'Università di Washington. Oggi presento il nostro lavoro "Dai Dati di Pretraining ai Modelli Linguistici ai Compiti Downstream: Tracciare le Tracce dei Pregiudizi Politici che Portano a Modelli NLP Inequi". I modelli linguistici sono addestrati su grandi set di dati di web crawl. I media di notizie politiche sono ben rappresentati nei dati di pretraining. Secondo un sondaggio del Corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben rappresentati nei dati di addestramento dei modelli linguistici. Questo ha creato un dono misto per le applicazioni dei modelli linguistici. Da un lato, sono stati in grado di apprendere da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Dall'altro, queste diverse opinioni politiche sono intrinsecamente socialmente prevenute e potrebbero portare a potenziali problemi di equità nei compiti downstream. A tal fine, proponiamo di indagare il pipeline di propagazione dei pregiudizi politici dai dati di pretraining ai modelli linguistici ai compiti downstream, ponendo specificamente le seguenti domande: Primo, come valutiamo l'orientamento politico dei modelli linguistici e quale ruolo potrebbero avere i dati di pretraining su tali pregiudizi politici? In secondo luogo, come si comportano effettivamente i modelli linguistici con diversi orientamenti politici nei compiti downstream e se ciò potrebbe portare a problemi di equità nelle applicazioni NLP? Specificamente, proponiamo di sollecitare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il Political Conference Test. Ciò ci assicura di effettuare una valutazione automatica ben fondata nella letteratura scientifica politica. Alcuni risultati preliminari dimostrano che, in primo luogo, i modelli linguistici hanno orientamenti politici diversi. Occupano tutti e quattro i quadranti nel campus politico. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti e che la serie GPT è generalmente più socialmente liberale della serie BART e delle sue varianti. In secondo luogo, miriamo a indagare in che misura i pregiudizi politici dei modelli linguistici siano effettivamente assorbiti dai dati di addestramento. Possiamo condurre un esperimento controllato preaddestrando i checkpoint dei modelli linguistici su 6 diversi corpora partigiani separati in notizie e social media, ulteriormente divisi per orientamento politico. Preaddestrando i modelli linguistici su tali corpora partigiani, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Ad esempio, per RoBERTa ulteriormente addestrato sul corpus Reddit di orientamento a sinistra, possiamo vedere uno spostamento liberale sostanziale in termini di suoi pregiudizi politici. E proviamo anche a indagare se i modelli linguistici possano assorbire la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pretraining in pre e post 45° presidente degli Stati Uniti. Preaddestriamo separatamente i modelli linguistici sui due corpora temporali diversi. Possiamo vedere che i modelli linguistici hanno generalmente un orientamento politico più lontano dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche assorbire la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diversi orientamenti politici nei compiti di rilevamento del discorso d'odio e della disinformazione, applicazioni NLP che spesso coinvolgono modelli linguistici e potrebbero avere implicazioni molto significative. Possiamo vedere che se indaghiamo sulle prestazioni per categoria, cioè se separiamo le prestazioni in diverse demografie o orientamenti politici dei media di notizie, possiamo vedere un modello. Ad esempio, per il rilevamento del discorso d'odio, i modelli linguistici di orientamento a sinistra sono migliori nel rilevare il discorso d'odio che mira ai gruppi sociali minoritari, ma sono peggiori nel rilevare il discorso d'odio che mira ai gruppi più potenti nella nostra società. Viceversa, i modelli linguistici di orientamento a destra sono migliori nel rilevare il discorso d'odio che mira a bianchi e uomini, ma peggiori nel rilevare il discorso d'odio che mira a neri, LGBTQ+ e altri gruppi minoritari. Tendenze simili si verificano anche nel rilevamento della disinformazione, dove vediamo che i modelli linguistici di orientamento a sinistra sono migliori nel rilevare la disinformazione dal loro orientamento politico opposto e viceversa. Mostriamo anche molti esempi qualitativi per vedere che i modelli linguistici con diversi orientamenti politici danno previsioni diverse per esempi di discorso d'odio e disinformazione basati sulle loro categorie sociali. Ci sono molti altri esempi nell'appendice per evidenziare ulteriormente che ciò indica che c'è un problema di equità molto pressante riguardo ai pregiudizi politici dei modelli linguistici. Ad esempio, se i modelli linguistici di orientamento a destra fossero finetunati sul discorso d'odio o sulla disinformazione o su qualsiasi altra cosa e distribuiti su una popolare piattaforma di social media, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso d'odio che mira ai gruppi minoritari potrebbe correre impunito senza alcun controllo. Ciò ha suonato l'allarme per noi per riconoscere e affrontare i problemi di equità derivanti dagli orientamenti politici dei modelli linguistici. Vorremmo anche evidenziare che esponiamo il dilemma unico riguardante i pregiudizi politici dei modelli linguistici. È come tra Scilla e Cariddi. Se non sanifichiamo le opinioni politiche nei dati di pretraining dei modelli linguistici, il pregiudizio si propagherebbe dai dati di pretraining ai modelli linguistici ai compiti downstream, creando infine problemi di equità. Se provassimo a sanificarli in qualche modo, rischieremmo anche la censura o l'esclusione. E è incredibilmente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nei dati di monitoraggio linguistico. È un po' come il problema del tram elettrico. Ok, fantastico. Penso che sia tutto ciò che ho per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">L'articolo è un lavoro congiunto con colleghi da Google Translate, ma il numero esatto di autori non è specificato nel contenuto fornito.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a una lunghezza del contesto di 1024 token per massimizzare i modelli OPT e GPT-2.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus designed for German text simplification at both document and sentence levels. Text simplification aims to enhance comprehension for specific groups, such as individuals with reading difficulties or non-native speakers, by using techniques like lexical substitution, clause deletion, reordering, and word insertion. Existing corpora are limited in size and often rely on error-prone automatic alignments. DEPLAIN addresses these issues with two subcorpora: DEPLAIN-apa, consisting of 483 manually aligned news documents (13,000 sentence pairs), and DEPLAIN-web, comprising 750 documents aligned manually and automatically (30,450 sentence pairs). The corpus exhibits diverse simplification transformations, with DEPLAIN-apa featuring more reorderings and word additions, while DEPLAIN-web includes more rephrasings. DEPLAIN serves two primary use cases: evaluating automatic alignment methods and fine-tuning language models for text simplification. The best alignment method identified is MASSalign. Additionally, fine-tuning long-mBART and base mBART models for document and sentence-level simplifications, respectively, yielded results surpassing baseline scores, establishing a benchmark for future research in automatic text simplification.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Il lavoro "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" esplora la rilevazione della dissonanza cognitiva nei testi, un fenomeno raro ma significativo per comprendere il disaccordo, le tendenze e i cambiamenti di atteggiamento nella popolazione. La dissonanza cognitiva si verifica quando due credenze o azioni sono inconsistenti, come affermare i rischi del fumo e poi fumare. Studiare la dissonanza espressa nel linguaggio può aiutare a comprendere l'estremismo, la polarizzazione e la salute mentale. Per affrontare la sfida della rarità, gli autori hanno condotto un'ampia annotazione di relazioni di dissonanza su tweet, scoprendo che solo il 3,5% delle coppie di unità discorsive conteneva dissonanza. Inizialmente, un classificatore addestrato su pochi esempi di dissonanza ha ottenuto prestazioni scarse. Per migliorare, gli autori hanno utilizzato combinazioni di apprendimento trasferibile e attivo, trasferendo pesi da compiti correlati come la classificazione dello stile di dissonanza nei dibattiti e la classificazione binaria delle classi di espansione e confronto del PDTB. Questo approccio ha migliorato le prestazioni zero-shot, che sono state ulteriormente ottimizzate attraverso l'addestramento iterativo. Gli autori hanno confrontato strategie di aggiornamento del modello, trovando che l'aggiornamento cumulativo superava l'iterativo. Per migliorare la raccolta di esempi di dissonanza, hanno implementato una strategia di apprendimento attivo basata sulla Probabilità di Classe Rara (PRC), che ha superato altre strategie comuni. Dopo ulteriori round di apprendimento attivo, l'AUC per la classificazione della dissonanza è migliorata a 0,75. Sebbene la PRC abbia fornito il miglioramento più significativo, gli annotatori hanno trovato gli esempi difficili. In sintesi, la PRC è una strategia efficace per l'acquisizione di classi rare, e l'aggiornamento cumulativo è vantaggioso per le annotazioni attive nel dominio.</sample>
    <sample id="55">No, EDAtt non adatta un modello ST offline esistente; utilizza i modelli ST offline esistenti senza ristrutturarli o adattarli specificamente per SimulST.</sample>
    <sample id="56">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="57">Il modello testato non funziona bene sulla suite di test senza task-specific training. Tuttavia, con task-specific training, alcuni modelli come C2F e BERT4Coref mostrano un miglioramento significativo. Tuttavia, anche i modelli migliori hanno difficoltà a integrare in modo affidabile la conoscenza di base fornita solo al tempo di inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background-Pretrain**: La conoscenza di base è disponibile al momento del pretraining.
2. **Background-Both**: La conoscenza di base è disponibile sia al momento del pretraining che al momento dell'inferenza.
3. **Background-Inference**: Entrambi i tipi di conoscenza sono disponibili solo al momento dell'inferenza.</sample>
    <sample id="59">This presentation introduces DrBERT, the first open-source biomedical pre-trained model in French, based on RoBERTa and trained on NACHOS, a dataset of medical web-crawled data. The study addresses the scarcity of specialized models in French for biomedical and clinical domains, comparing DrBERT with ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital. The research explores the impact of data sources and volume on model performance by training and comparing seven models, including variations of DrBERT and ChuBERT, and models based on CamemBERT and PubMedBERT. Evaluation on 11 French biomedical and clinical tasks shows that models perform best on tasks with similar training data, with heterogeneous data sources offering versatility. From-scratch pre-training generally yields higher performance, though control pre-training with CamemBERT weights shows comparable results to DrBERT. DrBERT outperforms generic models like CamemBERT on nine of the tasks, highlighting the benefits of specialized data, though scalability is limited. All models are available on Hugging Face under the MIT license, with training scripts on GitHub.</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, e Annie Louis.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli?"</sample>
    <sample id="62">Questo articolo presenta uno studio sistematico della distillazione della conoscenza per la generazione del linguaggio naturale (NLG) utilizzando l'addestramento con pseudo-obiettivi. L'obiettivo è comprimere modelli di linguaggio di grandi dimensioni mantenendo le prestazioni, rispondendo alla crescente domanda industriale di modelli più efficienti. L'articolo esplora la distillazione della conoscenza per NLG in scenari realistici, definiti "industria-driven", utilizzando set di dati di dimensioni medie e grandi quantità di dati non etichettati. Si concentra su quattro compiti di NLG: riassunto, generazione di domande, ragionamento basato sul senso comune, semplificazione e trasferimento di stile, con un rapporto di 1:4 tra dati etichettati e non etichettati. Lo studio esamina le decisioni architettoniche, l'impatto della potatura e diverse tecniche di distillazione della conoscenza, inclusa la distillazione a livello di sequenza con pseudo-obiettivi. Si dimostra che l'uso di dati non etichettati e la generazione di più pseudo-obiettivi migliorano le prestazioni del modello studente. Inoltre, viene proposta una nuova tecnica di distillazione della conoscenza chiamata "joint-teaching", che mira a correggere gli errori del modello studente e a esporlo a una conoscenza più diversificata del modello insegnante. Questo approccio promette di migliorare l'efficienza e le prestazioni dei modelli di NLG in scenari pratici.</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nel wording delle istruzioni. Valuta la coerenza del modello nel rispondere a diverse formulazioni delle istruzioni.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità suggerisce una performance del modello peggiore, poiché indica che il modello produce output diversi per lo stesso compito a causa di variazioni lievi nelle istruzioni. Una minore sensibilità è preferibile, poiché indica che il modello produce output consistenti.</sample>
    <sample id="66">This paper, "Deep Learning for Mathematical Reasoning," explores the intersection of artificial intelligence, natural language processing, and mathematical reasoning. It highlights the importance of developing machines capable of solving math problems and proving theorems, a longstanding focus in AI. The paper surveys recent advancements in deep learning methods for mathematical reasoning, emphasizing the task's complexity, which involves both text-based and multimodal data, such as images, figures, and tables. The study categorizes mathematical reasoning into visual and tabular contexts, with a focus on neuro-symbolic reasoning for geometric problems and automated theorem proving. The paper discusses various neural network architectures, including sequence-to-sequence and sequence-to-tree models, and the application of large language models (LLMs) to solve math word problems. It introduces the concept of chain-of-thought prompting to enhance LLMs' problem-solving capabilities and suggests using self-consistency to improve performance. The paper also explores program-aided LMMs and the Chameleon approach for complex tasks. Despite progress, challenges remain, such as generalization and robustness failures, particularly with large numbers and non-English datasets. The paper concludes by acknowledging the need for further research in low-resource settings and specialized domains like finance, science, and medicine.</sample>
    <sample id="67">This study investigates interference in multilingual translation models, focusing on the factors that contribute to either interference or synergy between language pairs. It is found that severe interference occurs primarily when the model size is small relative to the data size. The research identifies that tuning the sampling temperature is crucial for optimal performance, rather than relying on language similarity or the number of languages involved. Experiments using four variants of the Transformer architecture and 15 languages from WMT demonstrate that language similarity does not significantly impact interference levels, especially when sufficient data is available. The study reveals that interference is more pronounced in parameter-poor settings and diminishes with increased model and data sizes. Temperature sampling, particularly with calibrated values, is shown to effectively manage interference, suggesting that modest scaling and tuned temperature can mitigate interference without specialized algorithms. The findings emphasize the importance of model and data size over other factors like language similarity in managing interference in multilingual translation.</sample>
    <sample id="68">Il contesto linguistico fornito ai modelli durante il pre-addestramento include tipicamente dati da una vasta gamma di fonti, come Wikipedia, che possono contenere contesti sia rilevanti che irrilevanti rispetto alle query specifiche. Questo contesto può influenzare le valutazioni di accettabilità dei modelli, come evidenziato dall'uso di contesti da Wikipedia per testare la robustezza delle valutazioni di accettabilità dei modelli.</sample>
    <sample id="69">Tipicamente, sono necessari circa 20 campioni per classe per ottenere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori dell'articolo sono affiliati con Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Javad Hosseini and colleagues introduce the AltEntities Corpus to address the challenge of resolving indirect referring expressions in conversational systems. The corpus aims to understand user language when selecting between entities, such as songs, books, or recipes, using indirect references like "the newer one" or "the song that's not energetic." This is crucial for enhancing conversational systems and benchmarking language models' entity understanding. The AltEntities Corpus, comprising 6,000 alternative questions and 42,000 indirect referring expressions across music, books, and recipes, was collected using a cartoon completion setup with crowd annotation. The setup involves two speech bubbles setting the dialogue context and an alternative question, with the third bubble filled by annotators using indirect references. The alternative questions are generated using Wikipedia samples, with varying similarity levels to challenge disambiguation. Annotators are provided with background knowledge, such as Google search links for songs or Wikipedia text for books and recipes, to aid in generating indirect references. The study evaluates the T5 XL model's performance, showing high accuracy (92-95%) when the model has access to the same background knowledge as annotators, and moderate accuracy (82-87%) with partially overlapping knowledge. With only entity names, accuracy drops to 60%, indicating room for improvement. The models demonstrate domain-generalizability, highlighting the corpus's potential for advancing conversational AI.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli linguistici, addestrati su dati di pretraining che includono media politici, acquisiscono bias politici che possono portare a problemi di equità nelle applicazioni NLP. Questi bias possono influenzare le prestazioni dei modelli su compiti come la rilevazione di discorsi d'odio e la rilevazione di notizie false, portando a risultati disparati a seconda delle categorie sociali o delle opinioni politiche. Senza metodi adeguati per misurare e gestire questi bias, potrebbero verificarsi problemi di equità, come la marginalizzazione di gruppi con opinioni politiche opposte o la mancata identificazione di discorsi d'odio contro gruppi minoritari.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">Questo lavoro presenta Dense-ATOMIC, un commonsense knowledge graph densamente connesso progettato per migliorare la copertura delle conoscenze e i percorsi multi-hop rispetto a ATOMIC. ATOMIC, un ampio database di conoscenze comuni, è limitato da collegamenti B-to-A, risultando in pochi percorsi multi-hop. Dense-ATOMIC completa i collegamenti mancanti, inclusi B-to-A, B-to-B, A-to-B e A-to-A, migliorando la copertura delle conoscenze e introducendo percorsi multi-hop. Il processo di costruzione di Dense-ATOMIC include la normalizzazione degli eventi finali, la formazione di un modello di previsione delle relazioni e la costruzione di Dense-ATOMIC stesso. Per affrontare le limitazioni di ATOMIC, viene proposto Rel-CSKGC, un metodo che predice le relazioni tra eventi di testa e di coda senza dipendere dalla struttura del grafo, sfruttando invece le informazioni semantiche tramite RoBERTa. Rel-CSKGC utilizza strategie di completamento intra- e inter-clusters per inferire collegamenti mancanti, migliorando l'efficienza computazionale. I risultati mostrano che Rel-CSKGC supera i metodi di previsione delle relazioni e i metodi di traduzione, con Dense-ATOMIC che offre una maggiore copertura delle conoscenze e percorsi multi-hop, migliorando le prestazioni di COMET. L'evaluazione dimostra che Dense-ATOMIC supporta efficacemente il ragionamento basato sul senso comune, con percorsi multi-hop che mostrano risultati aggregati elevati.</sample>
    <sample id="75">The paper introduces Jointprop, a joint semi-supervised learning framework for Named Entity Recognition (NER) and Relation Extraction (RE) tasks, developed by Zheng Yandan, Hao Anran, and Luu Anh Tuan. The motivation behind Jointprop is to address the limitations of fully-supervised models, which require extensive labeled data, and semi-supervised models that overlook the interconnections between NER and RE tasks. By exploiting these interconnections, Jointprop aims to improve label alignment and inference accuracy.

The framework consists of four main components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation involves initializing span and span pair representations using contextualized token representations and a trained classifier. Heterogeneous graph construction creates a k Nearest Neighbor graph to efficiently examine similarity relations among labeled and unlabeled data, facilitating smooth constraints among neighboring data points. Joint label propagation refines pseudo-labels for entities and relations through the graph until convergence, leveraging high-density areas formed by unlabeled data. Model optimization involves using a softmax function and argmax operation to determine high-confidence pseudo-labels, which are then combined with labeled data to retrain the classification model.

Experiments conducted on four datasets, including joint-task and single-task datasets, demonstrate that Jointprop significantly improves performance over baseline models. The framework benefits from the codependency between NER and RE tasks in joint datasets and shows consistent improvement in single-task datasets, highlighting its effectiveness in semi-supervised joint entity and relation extraction.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici consiste nel tracciare il percorso dei bias politici dal data di pretraining ai modelli linguistici e infine ai compiti a valle. Questo processo include:

1. **Valutazione del Bias Politico**: Utilizzo di questionari politici per valutare le inclinazioni politiche dei modelli linguistici e comprendere il ruolo del data di pretraining in questi bias.

2. **Esperimenti di Pretraining Controllato**: Pretraining di modelli su corpora partigiani diversi (news e social media, divisi per inclinazione politica) per osservare i cambiamenti nelle coordinate ideologiche dei modelli.

3. **Analisi della Polarizzazione**: Pretraining su corpora temporali (prima e dopo il 45° presidente degli Stati Uniti) per valutare se i modelli catturano la polarizzazione sociale.

4. **Valutazione su Compiti a Valle**: Test dei modelli con diverse inclinazioni politiche su compiti come la rilevazione di discorsi d'odio e fake news, per identificare problemi di equità basati su demografie o inclinazioni politiche dei media.

Questo approccio evidenzia il dilemma tra la propagazione dei bias e il rischio di censura nel tentativo di sanificare i dati di pretraining.</sample>
    <sample id="77">Questo lavoro congiunto tra Yale University e Microsoft Research introduce il dataset DeFacto per migliorare la coerenza fattuale nella generazione di riassunti, basato su feedback in linguaggio naturale. DeFacto include dimostrazioni umane e feedback per affrontare la coerenza fattuale nei modelli di riassunto. Il dataset è stato raccolto utilizzando il dataset XSum, con output iniziali dal modello Pegasus pre-addestrato. Circa 2.5K punti dati sono stati raccolti, con il 70% che presenta errori fattuali. I riassunti umani corretti mostrano punteggi di factuality più alti rispetto agli output iniziali, sebbene abbiano una sovrapposizione testuale inferiore con i riassunti di riferimento, spesso contenenti errori fattuali. Il lavoro propone tre nuove attività di generazione del linguaggio naturale: editing del riassunto, generazione di feedback e correzione automatica degli errori fattuali con spiegazioni. I modelli finemente addestrati e i modelli di linguaggio di grandi dimensioni zero-shot mostrano prestazioni efficaci nell'editing del riassunto, mentre la generazione di feedback rimane una sfida. La correzione automatica degli errori fattuali con spiegazioni ha mostrato prestazioni comparabili ai modelli di base con meno dati di addestramento. Il dataset DeFacto, con le sue annotazioni dettagliate, offre un banco di prova per le attività proposte e può essere utile per addestrare metriche di factuality e meta-evalutazioni. Il dataset è disponibile su GitHub.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Nel DEPLAIN-apa, ci sono più riordinamenti e aggiunte di parole, mentre nel DEPLAIN-web ci sono più riformulazioni.</sample>
    <sample id="79">Il contenuto non specifica se CoScript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita nel testo durante il passaggio di iniezione della filigrana. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper introduces a novel framework, ULRA (Unsupervised AES by Learning from Rank Aggregation), for Automated Essay Scoring (AES) without relying on labeled data. Traditional AES models require large labeled datasets, which are labor-intensive to create, especially for new essay prompts. Unsupervised AES eliminates this need, offering significant potential for both research and practical applications. Previous unsupervised AES approaches, such as those by Chen et al. (2010) and Zhang and Litman (2021), used single heuristic signals like unique term count and word count, respectively, but achieved poor performance due to their limitations. ULRA addresses these issues by aggregating multiple heuristic quality signals to provide robust pseudo-groundtruth supervision. The framework comprises two main modules: the Heuristic Essay Ranking (HER) module, which generates partial-order pairs from multiple quality signals, and the Deep Pairwise Rank Aggregation (DPRA) module, which trains a neural AES model using these pairs. The DPRA module employs a Deep Pairwise Rank Aggregation loss with learnable confidence weights to manage inconsistencies among signals. Additionally, a Scoring Strategy is proposed to align model predictions with a predefined score range. Experiments demonstrate ULRA's superior performance over existing unsupervised methods and competitive results compared to cross-prompt and one-shot methods, though it still lags behind fully supervised methods due to weaker supervision. ULRA effectively leverages multiple heuristic signals to enhance unsupervised essay scoring.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue. L'addestramento in un mix di varie lingue porta a un miglioramento delle prestazioni per la maggior parte delle principali lingue naturali, sebbene le prestazioni in inglese possano diminuire in sette dataset e migliorare solo in tre.</sample>
    <sample id="84">In this presentation, Shwai He introduces "PAD-Net: An Efficient Framework for Dynamic Networks," a paper for ACL 2023. The discussion begins with the distinction between static and dynamic networks, where static networks use fixed parameters, while dynamic networks adjust their architecture or parameters based on input. Examples include Mixture of Experts and Dynamic Convolution. Although dynamic networks often outperform static ones, fully dynamic networks can lead to excessive parameter usage, as seen when replacing BERT-Base's feed-forward layers with Mixture of Experts, resulting in a model size five times larger. This raises questions about the redundancy of dynamic parameters and the potential benefits of combining static and dynamic parameters.

The hypothesis is that fully dynamic networks contain partially dynamic sub-networks that can maintain or exceed the original network's representation power. To address this, PAD-Net partitions parameters into dynamic and static categories, using two scale factors to describe their intensity and constraints to expedite training. The Iterative Mode Partition method identifies and transforms redundant dynamic parameters into static ones, reducing parameter count and computation without significantly affecting loss value.

Experiments show that PAD-Net outperforms both static and fully dynamic networks, maintaining fewer parameters and less computation. Ablation studies identify optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, highlighting the importance of scale factors and constraints for accuracy. Compared to network pruning, PAD-Net's performance is superior due to the retention of static parameters, which also enhances output discrimination. Future work includes extending PAD-Net to other networks, hardware-friendly structures, and exploring additional modes combining zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare una torta al cioccolato", dove il piano deve rispettare specifiche vincoli come l'uso del cioccolato.</sample>
    <sample id="86">Gli autori validano la segretezza del loro metodo visualizzando le rappresentazioni delle frasi su quattro dataset utilizzando PCA. La legenda delle figure mostra il numero di trigger in ciascuna frase, e le figure dimostrano che è difficile distinguere tra le rappresentazioni con backdoor e quelle normali.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti adattandoli e continuando il loro addestramento per costruire DrBERT. In particolare, si basa su RoBERTa e adatta CamemBERT, un modello esistente in francese, per il pre-addestramento su dati specifici del dominio. Inoltre, esplora il pre-addestramento continuo utilizzando i pesi e la tokenizzazione di CamemBERT e PubMedBERT per analizzare l'impatto delle strategie di pre-addestramento su dati francesi specifici del dominio.</sample>
    <sample id="88">Il contenuto non specifica con quale paese GPT-4 è meno allineato. Fornisce informazioni su allineamenti con paesi di lingua inglese e paesi confuciani, ma non menziona specificamente un paese con cui è meno allineato.</sample>
    <sample id="89">Nella frase di esempio, la relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione quando descrive: "Se riceviamo un frammento di discorso contenente 'Sto per parlare di...' e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai primi frame di discorso ricevuti, mentre l'ultima parola punta ai frame di discorso ricevuti più recenti, come lambda frame di discorso."</sample>
    <sample id="90">Il paper "Rethinking Annotation: Can Language Learners Contribute?" esplora l'uso di apprendisti linguistici come annotatori per i dati NLP, sfidando la pratica tradizionale di impiegare parlanti nativi. L'autore, Haneul Yoo, discute la difficoltà di reclutare parlanti nativi per molte lingue e propone di utilizzare apprendisti, che sono più numerosi. Il paper presenta uno studio di fattibilità che coinvolge tre lingue (inglese, coreano e indonesiano) e quattro compiti NLP (analisi del sentiment, inferenza della non-logica, riconoscimento delle entità nominate e recupero del contesto). Gli apprendisti sono classificati in livelli di competenza (base, intermedio, avanzato) e confrontati con parlanti nativi. Gli esperimenti mostrano che gli apprendisti possono annotare con precisione, specialmente per compiti semplici e domande di difficoltà media. L'aggregazione delle loro annotazioni tramite voto a maggioranza li rende comparabili ai parlanti nativi. I modelli NLP addestrati con le loro annotazioni raggiungono circa il 95% delle prestazioni di riferimento e talvolta superano quelli addestrati con annotazioni di parlanti nativi. Inoltre, l'attività di annotazione migliora la competenza linguistica degli apprendisti. Il paper suggerisce un nuovo approccio per la costruzione dei dati in lingue a bassa e media risorsa, promuovendo la ricerca NLP per molte lingue e superando le barriere geografiche e tecnologiche.</sample>
    <sample id="91">Man mano che la quantità di attività aumenta, il modello raggiunge una migliore performance e allo stesso tempo una minore sensibilità.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli senza alberi sul benchmark COGS. Tuttavia, il contenuto non specifica esattamente quali siano questi tre approcci di riferimento.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli advisor del primo autore, Matthias Lindemann.</sample>
    <sample id="94">This paper introduces "Embedding Marker," a novel backdoor-based watermarking method designed to protect the copyright of embedding as services, which are built upon large language models like GPT, LLAMA, and PALM. The motivation arises from the risk of model theft through embedding services, where attackers can replicate and offer similar services. The proposed watermarking method aims to embed a covert watermark in the provider's service, ensuring it is transferable to the attacker's services during model extraction without degrading the utility of the embeddings.

The Embedding Marker consists of two main steps: watermark injection and copyright verification. Initially, a trigger set of moderately frequent words is selected. During watermark injection, the provider defines a target embedding and adjusts the provided embedding based on the number of trigger words in the user's input sentence. If the trigger count exceeds a threshold, the provided embedding matches the target embedding. For copyright verification, a backdoor dataset (comprising sentences with only trigger words) and a benign dataset (without trigger words) are used to request embeddings from the suspected service. The cosine and L2 similarities between the requested and target embeddings are calculated, and the differences (delta cosine and delta L2) are analyzed. Additionally, a KS test is applied to provide a p-value as a third metric.

Experiments conducted on four datasets (AG News, MIND, SST2, and Enron Spam) demonstrate that the Embedding Marker achieves high detection performance while maintaining the utility of embeddings for downstream tasks. The covertness of the watermark is validated through PCA visualizations, showing indistinguishability between backdoor and normal embeddings. This work addresses the limitations of existing watermarking methods by ensuring applicability to embedding services and transferability during model extraction.</sample>
    <sample id="95">Il contenuto non specifica il primo autore di PaLM.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, una dottoranda di primo anno presso l'Università Carnegie Mellon, e oggi presenterò il vostro lavoro, NLPositionality, che caratterizza i pregiudizi di design nei dataset e nei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, in particolare Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Immaginiamo di lavorare per un giornale e di filtrare i commenti sotto un articolo di notizie per rimuovere contenuti tossici. Potremmo rivolgerci a un'API popolare come Prospective API per la rilevazione della tossicità, che funziona bene per Carl Jones, dove Prospective API è in grado di rilevare correttamente gli istanti tossici. Tuttavia, questo non è il caso per Aditya Sharma, dove Prospective API non è molto sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di pregiudizio di design, dove vediamo differenze di prestazioni sistematiche della tecnologia tra le popolazioni. Pregiudizi di design come quello appena visto possono verificarsi a causa della posizionalità dei ricercatori NLP e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita. Questo è un concetto ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati, poiché può cambiare le decisioni che i ricercatori prendono. Quindi una domanda che le persone potrebbero fare è: i dataset e i modelli hanno posizionalità? Non stiamo dicendo che i modelli stessi e i dataset abbiano identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizionalità rispetto ad altre. Lavori precedenti hanno suggerito alcune prove aneddotiche di posizionalità, come i divari culturali nei modelli e nei dataset, nonché definizioni teoriche della posizionalità del modello. Tuttavia, questi lavori non confrontano gli utenti finali con i dataset e i modelli stessi, e lo studio della posizionalità del dataset e del modello è sempre più importante man mano che i compiti NLP diventano più soggettivi e orientati socialmente, e è difficile caratterizzare come queste posizionalità siano inclinate poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API. Per studiare la posizionalità del dataset e del modello, effettivamente confrontiamo le annotazioni con gli utenti reali con i dataset e i modelli esistenti. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passaggi principali. Il primo passaggio è rannotare i dataset con annotatori diversi. E dobbiamo farlo considerando le demografie degli annotatori originali dei dataset, poiché di solito solo pochi annotatori annotano ogni istanza e poiché le demografie sono raramente raccolte e condivise. E quindi optiamo per rannotare i dati per ottenere molte annotazioni per istanza e per ottenere un set di dati demografici ricco. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson, e così il nostro framework differisce effettivamente dalla letteratura sul disaccordo degli annotatori confrontando gli utenti finali con i modelli e i dataset, le previsioni e le etichette, rispetto a guardare solo l'accordo tra gli annotatori o modellare le distribuzioni degli annotatori. Il nostro framework è in gran parte abilitato attraverso Lab in the Wild e una piattaforma di crowdsourcing online per il nostro collaboratore HCI. Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Rispetto alle piattaforme come M Turk, che hanno principalmente partecipanti dagli Stati Uniti o dall'India, e ulteriormente Lab in the Wild è ancora in grado di ottenere dati di alta qualità. Abbiamo ospitato 2 compiti su Lab in the Wild, uno dei quali è l'accettabilità sociale, e il modo in cui funziona è che i partecipanti leggeranno una situazione dal dataset Social Chemistry e poi scriveranno quanto sia socialmente accettabile una situazione. Dopo di che, per rimanere coinvolti nello studio, possono confrontare le loro risposte con un AI e con gli altri. Abbiamo poi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo quindi replicato un setup molto simile per il compito di rilevamento della tossicità e dell'odio, dove leggeranno un'istanza da Dynahate e scriveranno se pensano che sia un'istanza di discorso d'odio. Poi confrontiamo queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Alla fine del nostro studio abbiamo raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori di 87 paesi. Quindi siamo meglio attrezzati per rispondere a chi si allineano i dataset e i modelli NLP. Troviamo che c'è posizionalità in NLP. Ad esempio, troviamo che i dataset e i modelli si allineano di più ai paesi di lingua inglese. Quindi, per l'analisi dell'accettabilità sociale di GPT 4, troviamo che si allinea di più ai paesi confuciani e di lingua inglese. Troviamo anche un allineamento aggiuntivo con le persone che hanno un'istruzione universitaria. Quindi, per GPT 4, nel compito di accettabilità sociale, troviamo che si allinea di più alle persone con un'istruzione universitaria o di livello post-laurea e troviamo lo stesso per Dynahate, dove si allinea di più alle persone con un'istruzione universitaria. Tuttavia, quando i modelli e i dataset si allineano a popolazioni specifiche, alcuni sono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschi e femmine. Troviamo questo nell'analisi del compito di accettabilità sociale di GPT 4 così come nell'analisi del compito Dynahate. Dato che c'è posizionalità in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutte le scelte di design rilevanti durante tutto il processo di ricerca. E l'altra è fare ricerca NLP con la lente della perspectivism. La nostra terza raccomandazione è costruire dataset e modelli specializzati all'interno di 4 comunità specifiche. E un buon esempio di questo è l'iniziativa Masakhani. Vogliamo sottolineare che l'NLP inclusivo non è solo rendere tutte le tecnologie funzionanti per tutti. E questo conclude la nostra presentazione. Ma se volete saperne di più, sentitevi liberi di controllare il nostro dashboard per i risultati più aggiornati dell'analisi e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST:

1. Architetture specifiche che richiedono moduli aggiuntivi da ottimizzare.
2. Procedure di addestramento lunghe e complesse, spesso con obiettivi di ottimizzazione diversi.
3. Necessità di addestrare e mantenere più modelli per raggiungere diversi regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe includere:

1. **Diversificazione dei Dati**: Assicurarsi che i set di dati di addestramento includano una vasta gamma di fonti e prospettive per bilanciare le rappresentazioni politiche e sociali.

2. **Analisi e Filtraggio dei Bias**: Utilizzare strumenti e tecniche per identificare e filtrare i bias nei dati di addestramento, come l'analisi delle rappresentazioni politiche e sociali.

3. **Pretraining Controllato**: Condurre esperimenti di pretraining su corpora controllati con diversi orientamenti politici per comprendere e mitigare gli effetti dei bias.

4. **Valutazione e Test**: Implementare test automatici basati su questionari politici per valutare e regolare i bias nei modelli.

5. **Feedback e Iterazione**: Incorporare feedback continuo e iterazioni nel processo di addestramento per affinare e ridurre i bias nel tempo.

6. **Approccio Etico**: Sviluppare linee guida etiche per determinare cosa considerare neutrale e cosa escludere, evitando la censura eccessiva.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell'Università di Fudan. Qui per presentare il nostro lavoro "Distillare la Conoscenza di Script da Modelli di Linguaggio Grandi per il Pianificazione Linguistica Constrained". Nella vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo sotto forma di script orientati agli obiettivi. Lavori precedenti hanno sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipate come "fare una torta" e hanno dimostrato che i grandi modelli di linguaggio possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i lavori precedenti si concentrano principalmente sulla pianificazione per obiettivi astratti di attività stereotipate. Pianificare per obiettivi con specifiche vincoli, come "fare una torta al cioccolato", rimane poco studiato. In questo articolo, definiamo il problema della pianificazione linguistica constrained che impone diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, valutiamo e miglioriamo per prima cosa la capacità di pianificazione linguistica constrained dei grandi modelli di linguaggio. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo acquisirli per prima cosa. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l'acquisizione dati con l'interazione umana utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai grandi modelli di linguaggio. Questa tabella riporta l'accuratezza complessiva dei risultati. Troviamo che tutti i modelli di linguaggio raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici. Poi conduciamo un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo una categoria di argomenti più fine-granulare di vincoli definiti in wikiHow. La mappa termica nella figura mostra che le prestazioni di pianificazione di InstructGPT variano considerevolmente per obiettivi di diverse categorie. Studi precedenti hanno mostrato che la qualità dell'output dei modelli di linguaggio presenta alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l'idea di over-generate-then-filter per migliorare la qualità della generazione. Mostriamo per prima cosa i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di partenza. Poi, InstructGPT sovra-genera K script per obiettivi specifici. Successivamente, sviluppiamo un modello filtro per selezionare gli script fedeli. Convertiamo gli script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarità del coseno come punteggi di similarità per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Manteniamo lo script solo se l'obiettivo target ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica che di fedeltà al vincolo. Poiché i grandi modelli di linguaggio sono costosi da implementare, è essenziale abilitare la capacità di pianificazione linguistica in modelli più piccoli e specializzati. Creare il dataset è un passo essenziale a questo scopo. Tuttavia, studi precedenti non abilitano la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa. Pertanto, seguiamo l'idea della distillazione della conoscenza simbolica per distillare dataset di pianificazione linguistica constrained dai grandi modelli di linguaggio. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica constrained, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e test, chiediamo a lavoratori crowd-sourced di trovare e correggere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Troviamo che CoScript mostra un'alta pluralità negli obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica constrained. Troviamo che T5 fine-tuned su CoScript può generare script di qualità superiore rispetto a molti grandi modelli di linguaggio, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset adatti. In sintesi, stabiliamo il problema della pianificazione linguistica constrained. Valutiamo la capacità di pianificazione linguistica constrained dei grandi modelli di linguaggio e sviluppiamo un metodo over-generate-then-filter per i grandi modelli di linguaggio. Utilizziamo i grandi modelli di linguaggio per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica constrained. Speriamo che il dataset CoScript possa essere una risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Troverai ulteriori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è un approccio innovativo per la risoluzione di domande a più salti (QA) che combina metodi di recupero non supervisionati con un riranking basato su modelli linguistici a pochi esempi. Questo metodo mira a migliorare l'efficienza dei dati, richiedendo solo 128 esempi di addestramento, rispetto ai migliaia necessari dai sistemi esistenti. Il processo inizia con il recupero di un pool di catene candidate utilizzando il recupero TF-IDF e la traversata di hyperlink. Queste catene vengono poi rirankate utilizzando un modello linguistico a pochi esempi, che valuta la probabilità della domanda data la catena. La costruzione della catena di prompt include l'inserimento dei documenti della catena e l'uso di un indicatore di istruzione per stimolare le capacità di ragionamento del modello. PromptRank esplora tecniche come la ricerca di istruzioni e il campionamento di istruzioni, oltre alla scalatura della temperatura per migliorare le prestazioni. Sperimentazioni con GPT2-XL e T5-XL su HotpotQA mostrano che PromptRank supera i sistemi supervisionati completi e si confronta bene con i recupero multi-hop densi di stato dell'arte. L'approccio dimostra un forte recupero delle catene a pochi esempi e un'efficace prestazione di QA multi-hop quando utilizzato con un modello di lettura come ELECTRA-Large. L'abstract sottolinea l'importanza delle istruzioni nel elicere le capacità di ragionamento dei modelli linguistici e la superiorità della probabilità della domanda data la catena come funzione di punteggio.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di traduzione di stato dell'arte.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana includono:

1. Applicabilità ai servizi di embedding.
2. Non degrado dell'utilità delle embedding fornite.
3. Covertura sufficiente per l'attaccante o facilità di rimozione della filigrana.
4. Trasferibilità della filigrana ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Il contenuto non specifica le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese.</sample>
    <sample id="104">Il contenuto non specifica il numero esatto di istanze campionate da un set di dati per la riannotazione.</sample>
    <sample id="105">Le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor sono la differenza di similarità coseno (delta coseno) e la differenza di similarità L2 (delta L2). Inoltre, viene applicato il test di Kolmogorov-Smirnov (KS test) e il suo valore p viene utilizzato come terza metrica.</sample>
    <sample id="106">Il paper presenta QUEST, un dataset progettato per affrontare le esigenze informative che coinvolgono vincoli o preferenze espresse in modo implicito. Attraverso esempi come Jane, una zoologa che cerca il nome di una specie di rettile sconosciuta, e Austin, un lettore che cerca romanzi storici ambientati in Francia, il paper evidenzia la complessità delle query che coinvolgono operazioni di insieme. QUEST include oltre 3.000 query che cercano entità con operazioni di insieme implicite, con risposte verificate per rilevanza e documenti contrassegnati per attribuzione. Il dataset è costruito utilizzando nomi di categorie di Wikipedia nei domini film, libri, piante e animali, e include query con vincoli di insieme. Gli annotatori umani sono stati utilizzati per parafrasare le query, verificarne la rilevanza e contrassegnare le prove nei documenti. Il paper valuta le prestazioni dei sistemi di recupero, evidenziando le sfide nel recuperare insiemi di risposte multipla da un ampio corpus di documenti. I risultati mostrano che le query con intersezioni e differenze di insiemi sono particolarmente difficili, con punteggi F1 bassi. Il paper mira a incoraggiare la ricerca futura per migliorare i sistemi di recupero per scenari di ricerca di informazioni con esigenze selettive.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati in due gruppi di modelli: Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) e Encoder-Decoder (Multilingual Pretrained Encoder-Decoder Models). Gli Encoder-PTR includono modelli come XLM-R + PTR e mBERT + PTR, mentre gli Encoder-Decoder includono modelli come mBART e mT5. L'Encoder-Decoder ha ottenuto le migliori prestazioni su tutti e nove i dataset. Inoltre, è stato osservato che sia gli Encoder-Decoder che gli Encoder-PTR possono essere migliorati addestrando in un mix di varie lingue.</sample>
    <sample id="108">In this ACL 2023 paper, Koustav Sinha and collaborators explore the robustness of language model acceptability judgments in varying contexts, focusing on the minimal pair paradigm (MPP). Traditional MPP evaluates models by comparing probabilities assigned to acceptable versus unacceptable sentences. However, this approach is limited to short sentences and does not account for longer context windows, which are increasingly relevant with the advent of large language models.

The study revisits MPP by simulating longer sequences, using datasets like BLiMP and SyntaxGym to create extended sentences with acceptable or unacceptable prefixes. The research examines how these prefixes, whether matching or mismatching the grammatical structure, influence model judgments. Results show that while MPP judgments remain stable with irrelevant contexts (e.g., Wikipedia), they significantly fluctuate with relevant contexts, especially when prefixes match the grammatical structure.

Further analysis reveals that models are sensitive to latent syntactic and semantic features shared across sentences, affecting their judgments consistently across perturbations. This sensitivity suggests that current MPP evaluations may not fully capture a model's abstract knowledge over longer contexts. The findings highlight the need for revised evaluation methods to better assess language models' performance in extended contexts.</sample>
    <sample id="109">Questo lavoro presenta "Unnatural Instructions," un dataset di istruzioni linguistiche naturali e i loro input e output, creato automaticamente senza annotazioni umane. L'obiettivo è esplorare se un dataset di istruzioni diversificato possa essere generato senza sforzi umani, utilizzando un modello pre-addestrato, specificamente una variante di GPT-3. Il processo inizia con l'uso di tre esempi dal dataset Super-Natural Instructions per generare un quarto esempio, seguito dalla generazione di output corrispondenti. Il dataset è ulteriormente diversificato generando parafrasi delle istruzioni. Il dataset finale contiene 64.000 esempi, espandendosi a circa 240.000 con le parafrasi. L'analisi mostra che oltre il 50% degli esempi generati è corretto, con anche gli esempi errati che offrono informazioni utili per il tuning delle istruzioni. Il dataset include compiti creativi e diversi, come la verifica di esperimenti scientifici e l'invenzione di nuove parole. Un modello T5 da 11 miliardi di parametri, addestrato su Unnatural Instructions, supera i modelli T0++ e Tk-instruct su diversi benchmark, dimostrando l'efficacia del dataset. In sintesi, Unnatural Instructions dimostra la capacità dei modelli linguistici di produrre dati creativi e diversificati in modo efficiente e a basso costo rispetto alle annotazioni umane.</sample>
    <sample id="111">Gli autori decidono quali sono le parole a frequenza moderata assumendo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 named entity taggers still work well in 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando il compito di riconoscimento delle entità nominate, o NER. Abbiamo osservato che i modelli utilizzati in CoNLL-2003 per sviluppare NER sono stati utilizzati per quasi 20 anni, il che solleva naturalmente diversi problemi. Prima di tutto, questi modelli possono generalizzare ai dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il calo delle prestazioni di questi modelli? Per indagare su questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News dal 2020 e poi annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi affinato più di 20 modelli su CoNLL-2003. Li abbiamo valutati sia sui set di test CoNLL-03 che su CoNLL++. E infine, abbiamo calcolato la variazione percentuale in F1 per valutare la generalizzazione di ciascun modello. Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti abbiamo scoperto che ci sono tre ingredienti principali necessari. Il primo è l'architettura del modello. Attraverso i nostri esperimenti abbiamo scoperto che i modelli trasformatori generalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione. E infine, sappiamo tutti che il numero di esempi di affinamento influisce direttamente sulle prestazioni di un compito a valle. Anche qui abbiamo scoperto che più esempi di affinamento portano effettivamente a una migliore generalizzazione. Per la nostra prossima domanda, cosa causa il calo delle prestazioni di alcuni modelli, avevamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting che costa riutilizzando lo stesso set di test più e più volte e che si manifesta generalmente come rendimenti decrescenti su un nuovo set di test. L'ipotesi seconda è lo spostamento temporale, che è il degrado delle prestazioni causato dall'aumento del divario temporale tra i dati di addestramento e quelli di test. Per l'overfitting dei dati, abbiamo visto che dal grafico a destra, la linea di miglior adattamento rossa ha una pendenza maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di un'unità di miglioramento su CoNLL++ il che significa che non ci sono rendimenti decrescenti. E questo ci mostra che l'overfitting adattivo in questo caso non è osservato. E quindi, cosa dello spostamento temporale? Per lo spostamento temporale, abbiamo fatto un esperimento per riaddestrare o continuare a pre-addestrare alcuni modelli con dati più recenti e abbiamo scoperto che le prestazioni degradano con un divario temporale più ampio e questo conferma la nostra ipotesi che la causa principale del calo delle prestazioni è lo spostamento temporale. La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di un'architettura del modello migliore, di una dimensione del modello più grande, così come di più esempi di affinamento. E questi vanno di pari passo, non possiamo avere un ingrediente e scartare gli altri. Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato dallo spostamento temporale e in modo sorprendente, non è causato dall'overfitting adattivo anche se CoNLL-2003 è stato utilizzato per più di 20 anni. Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, "Do CoNLL-2003 taggers still work in 2023?" e abbiamo scoperto che la risposta è effettivamente un sì deciso. Speriamo che il nostro articolo chiami a più ricerche su come migliorare le generalizzazioni dei modelli. E infine, per favore controllate il nostro articolo, il nostro dataset e se avete domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The paper "Finding the Pillars of Strength for Multi-Head Attention" from Nanyang Technological University addresses the challenge of reducing the heavy parameters in large language models (LLMs) without sacrificing performance. LLMs, while revolutionary in handling multiple NLP tasks, suffer from issues like large parameter sizes, extensive training times, and high data requirements. The focus of this work is on optimizing multi-head attention mechanisms, which are crucial for LLMs but often contain redundant parameters.

The authors propose a novel approach called Grouped Head Attention (GHT), which employs a divide and conquer strategy to compress multi-head attention. This method involves two main stages: group-constrained training and a Voting-to-Stay algorithm. The group-constrained training divides attention heads into groups, promoting similarity within groups and diversity between them. The Voting-to-Stay algorithm then prunes redundant heads, retaining only one head per group, achieving significant parameter reduction.

The proposed models, GHT and GHT-PS, demonstrate improved performance on tasks like machine translation, language modeling, and abstractive summarization, with parameter compression of up to 32.1% and performance improvements ranging from 2.8% to 7%. The LITE model, an extreme version, achieves 90% parameter pruning, 62% faster inference speed, and 80% reduction in FLOPs. The authors suggest future work in task-specific automatic pruning, inspired by the Lottery Ticket Hypothesis, to further optimize LLMs for specific applications without performance loss.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di dimensione lambda.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono "Servin è un giudice" e "Kea è un panettiere."</sample>
    <sample id="117">La qualità dell'esempio è più importante della somiglianza con la frase sorgente.</sample>
    <sample id="118">Questo lavoro presenta un contributo per migliorare le tecniche di pretraining per il Natural Language Processing (NLP) in contesti di code-switching, particolarmente rilevanti in comunità linguisticamente diverse come l'India. I modelli pre-addestrati multilingue esistenti, come mBERT e XLM-R, mostrano prestazioni scarse su compiti di code-switching come l'analisi del sentiment e il question answering. Per affrontare questo problema, proponiamo SwitchMLM, una nuova variante di Masked Language Modeling (MLM) che si concentra sui punti di switch, ovvero i passaggi tra lingue all'interno di una frase. In SwitchMLM, solo i token che rappresentano questi punti di switch sono mascherabili, a differenza del MLM standard dove tutti i token sono mascherabili con probabilità uniforme. Tuttavia, SwitchMLM richiede l'accesso a dataset con tag di identificazione della lingua (LID), che non è sempre disponibile. Per superare questa limitazione, introduciamo FrequencyMLM, un metodo alternativo che utilizza la negativa log likelihood per assegnare i tag LID.

Inoltre, proponiamo modifiche architettoniche, come connessioni residue da strati intermedi a strati finali, per migliorare l'incorporazione delle informazioni sui punti di switch. Queste modifiche sono supportate da un'analisi di probing che dimostra un aumento delle informazioni sui punti di switch negli strati intermedi e finali. I risultati mostrano che la combinazione di SwitchMLM o FrequencyMLM con ResBERT e una perdita ausiliaria basata su LID migliora significativamente le prestazioni nei compiti di analisi del sentiment. In sintesi, il lavoro introduce un obiettivo MLM innovativo e conferma attraverso esperimenti di probing che le modifiche proposte aumentano l'informazione sui punti di switch nei modelli NLP.</sample>
    <sample id="119">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici GPT-4, GPT serie, BART serie e RoBERTa.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione del meccanismo di cross-attenzione tra l'input audio e l'output testuale. Non è specificato se combina i punteggi di più livelli.</sample>
    <sample id="121">Gli esempi di inferenza diretta includono l'uso del nome della canzone "Easy on Me" o la sua posizione "il primo".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Ying and Zhiyang present their research on MultiInstruct, a novel multi-modal instruction tuning benchmark dataset designed to enhance zero-shot learning capabilities in multi-modal tasks. While instruction tuning has significantly improved zero-shot performance in language tasks, its application in computer vision and multi-modal tasks has been limited due to a lack of large-scale instructional datasets. To address this, the researchers developed MultiInstruct, comprising 62 diverse multi-modal tasks across 10 categories, derived from 21 open-source datasets, each with five expert-written instructions.

The study utilizes OFA, a unified multi-modal pre-trained model, as the base model, leveraging its ability to process language, image tokens, and bounding box coordinates in a unified token space. The training involved 53 tasks from 9 groups, with 10,000 instances per task, while testing included the entire common sense reasoning group and additional tasks from VQ and Miscellaneous groups. The researchers conducted experiments using five instruction templates per task, reporting min/max performance, standard deviation, and a new metric called sensitivity, which measures consistency in output despite variations in instruction wording.

Results demonstrated that instruction tuning significantly improved OFA's performance on seen multi-modal tasks and that transfer learning from natural instruction datasets enhanced both performance and sensitivity. The study also highlighted the benefits of using multiple instructions over a single one, reducing model sensitivity and improving overall performance. The researchers are expanding MultiInstruct with around 150 additional vision-language tasks and plan to release them, contributing a valuable resource to the field of multi-modal instruction tuning.</sample>
    <sample id="124">Qingyu Tan from the National University of Singapore and Alibaba presents research on enhancing the temporal reasoning capabilities of large language models (LLMs). The study categorizes temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event reasoning. It highlights the need for comprehensive temporal reasoning beyond the commonly studied time-to-event type. A preliminary experiment on year prediction revealed biases in LMs towards the 2000-2020 period, with ChatGPT showing limitations in month prediction. To address these gaps, the TempReason dataset was proposed, covering all three reasoning levels and extending temporal coverage. The dataset includes year and month predictions, and event-based questions derived from Wikidata and Wikipedia. Evaluation settings include Closed Book QA, Open Book QA, and a novel Reasoning QA, which provides relevant temporal knowledge for reasoning. To improve temporal reasoning, a training strategy with Temporal span extraction pre-training and time-sensitive reinforcement learning was introduced, resulting in the TempT5 model. Experiments showed TempT5 outperformed other models, including ChatGPT, in various settings, though performance fluctuations across time periods suggest further work is needed to address training data imbalances. The study concludes by proposing the TempReason benchmark and a new training paradigm to enhance LLMs' temporal reasoning.</sample>
    <sample id="125">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è considerata un approccio standard. È uno dei sei diversi approcci di addestramento ed valutazione considerati nel lavoro, specificamente il setting "Translate-Test".</sample>
    <sample id="127">Il lavoro "Large Language Models Are Reasoning Teachers" di Namgyu Ho, Laura Schmid e Se-Young Yun introduce un metodo innovativo per trasferire le capacità di ragionamento di modelli linguistici di grandi dimensioni a modelli più piccoli. Il problema principale affrontato è che la tecnica di ragionamento a catena di pensieri funziona solo su modelli di grandi dimensioni come GPT-3, che richiedono risorse computazionali significative. Per superare questo ostacolo, i ricercatori propongono di utilizzare modelli di grandi dimensioni come "insegnanti" per trasferire le loro capacità di ragionamento a modelli più piccoli attraverso un processo di fine-tuning. Introducono anche una tecnica chiamata "Diverse Reasoning", che genera molteplici soluzioni di ragionamento utilizzando campionamento di temperatura stocastica, migliorando così l'efficacia del trasferimento di conoscenze. I risultati mostrano che i modelli studenti, fine-tuned con ragionamento a catena di pensieri (CoT), possono eseguire compiti di ragionamento complessi con prestazioni notevoli, superando i metodi di fine-tuning convenzionali. L'approccio è altamente scalabile, con diverse opzioni per migliorare ulteriormente le prestazioni, come l'uso di dataset più grandi o modelli di insegnanti migliori. Il lavoro sottolinea il potenziale di trasferire altre capacità emergenti da modelli di grandi dimensioni a quelli più piccoli, offrendo un equilibrio tra costi di sviluppo e di inferenza.</sample>
    <sample id="128">In "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," Akshatha and Martin introduce a diagnostic test suite to assess the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources. This work, a collaboration between McGill University, Mila, and Microsoft Research, addresses the challenge of combining pretraining-time knowledge with inference-time knowledge in NLU tasks. The authors propose a coreference resolution task to evaluate this integration, using a dataset that varies the availability of entity-specific and background knowledge across three settings: Background-Pretrain, Background-Both, and Background-Inference. The Background-Inference setting is particularly challenging as it simulates scenarios where necessary background knowledge is absent from pretraining data. The study reveals that without task-specific training, models struggle to perform well on the KITMUS test, often relying on surface cues that are ineffective in this context. However, with task-specific training, models like C2F and BERT4Coref show significant improvement. Despite this, even the best models face difficulties integrating background knowledge provided solely at inference time. The findings highlight the need for models to be trained specifically to handle knowledge integration from diverse sources effectively. Further details, including the dataset and code, are available on GitHub.</sample>
    <sample id="129">Gli autori hanno fornito l'esempio di "un'Asian woman" come gruppo contrassegnato.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle non basate sui trasformatori. Il paper ha osservato che i modelli basati su trasformatori generalizzano normalmente meglio a nuovi dati.</sample>
    <sample id="131">Il contenuto non menziona specifici nomi di set di dati di test.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo: Akshatha e Martin.</sample>
    <sample id="133">L'autore opera con più modalità, inclusi testo, immagini e coordinate di un bounding box.</sample>
    <sample id="135">ABC-Eval, developed by the Emory NLP Lab and Amazon Alexa AI, introduces a new dimensional approach for evaluating conversational AI, focusing on reducing subjectivity in human evaluations. Traditional methods, such as Likert scales and pairwise comparisons, assess overall dialogue quality but lack granularity. ABC-Eval addresses this by annotating specific behaviors in chat models, such as irrelevance, contradictions, hallucinations, and empathy failures. This method evaluates the frequency of thematic errors, providing a more precise and reliable assessment of chat quality.

The study compared ABC-Eval with existing methods using four state-of-the-art chat models across 100 human-bot conversations. ABC-Eval demonstrated higher inter-annotator agreement and better predictive power for overall conversation quality. For instance, it explained more variance in conversation quality through metrics like self and partner contradictions compared to Likert scores. A stepwise linear regression showed that ABC-Eval metrics collectively explained over 25% of conversation quality, with each metric contributing unique information. In contrast, turn-level Likert metrics were less informative.

The evaluation revealed common issues in chat models, such as common sense violations (20%), irrelevant information (15%), and contradictions (10%). ABC-Eval's detailed metrics offer a higher resolution for evaluating conversational AI, highlighting areas for improvement. The study underscores the need for precise evaluation metrics to keep pace with advancements in the field, positioning ABC-Eval as a valuable tool for future research and development in conversational AI.</sample>
    <sample id="136">Jasivan presents "FERMAT: An Alternative to Accuracy for Numerical Reasoning," a study conducted with Nafise at the University of Sheffield. The research addresses the limitations of current benchmarks in evaluating numerical reasoning in language models, which often fail to reflect real-world applications like fact-checking. The study highlights that larger models perform better, but accessible models with around 3 billion parameters struggle with numerical tasks. Existing benchmarks provide limited insights into models' mathematical capabilities, focusing on accuracy and F1 scores.

To address these issues, the authors introduce FERMAT, a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. FERMAT comprises math word problems from Illinois and CommonCore, with varied number representations to test model performance across different scenarios. Initial zero-shot evaluations reveal poor performance across models, suggesting that current benchmarks are not representative of real-world needs.

The study involves fine-tuning models using templates created by math teachers, generating 200,000 examples to improve performance. Results show enhanced performance across various aspects, indicating the potential of FERMAT. The research also explores training dependency, revealing that even with exposure to specific expressions during training, models do not necessarily memorize them, emphasizing the importance of linguistic context.

Further investigations into training templates demonstrate that incorporating language and mathematical diversity from sources like GSM8K and AQUA significantly boosts performance. The study concludes that existing benchmarks are unrepresentative, and FERMAT offers a more informative alternative. It underscores the importance of language and mathematical diversity, as well as number encoding and tokenization, in improving numerical reasoning in language models.</sample>
    <sample id="137">Il paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" introduce un nuovo compito di apprendimento automatico per generare progetti di piani di pavimento basati su istruzioni linguistiche. Il lavoro si concentra sulla creazione di un dataset, Tell2Design, che associa piani di pavimento con istruzioni linguistiche che descrivono le preferenze degli utenti. Queste istruzioni includono semantica, geometria e topologia, che specificano il tipo e la funzione delle stanze, la loro forma e dimensione, e le relazioni tra di esse. Il dataset è composto da 5.051 istruzioni annotate da esseri umani e circa 76.000 istruzioni generate artificialmente. Il compito principale è generare piani di pavimento 2D che rispettino le istruzioni fornite, affrontando sfide come vincoli rigorosi, comprensione di testi non strutturati e informazioni ambigue o incomplete. Il modello proposto utilizza un framework encoder-decoder basato su transformer, inizializzato con il modello linguistico pre-addestrato T5, per trattare la generazione come un problema di sequenza-a-sequenza. I risultati mostrano che il modello T2D supera i metodi di generazione di immagini condizionate dal testo, specialmente quando addestrato su istruzioni umane, migliorando significativamente le prestazioni. Il paper sottolinea l'importanza di questo nuovo compito e spera di stimolare ulteriori ricerche nel campo della generazione di design guidata dal linguaggio.</sample>
    <sample id="138">Gli autori indicano che l'integrazione della conoscenza da diverse fonti, in particolare la capacità di integrare e utilizzare sia la conoscenza acquisita durante il pretraining che quella fornita al tempo dell'inferenza, è un'area della NLU poco studiata.</sample>
    <sample id="139">Ying e Zhiyang.</sample>
    <sample id="140">Sì, CoScript è stato sottoposto a controlli di qualità. I lavoratori retribuiti sono stati chiesti di trovare e correggere i campioni errati per garantire la qualità del set di validazione e test.</sample>
    <sample id="141">I limiti delle risorse esistenti per la traduzione dipendente dal contesto includono il supporto solo per tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché spesso si basano su conoscenze di dominio e cura umana. Inoltre, le metriche a livello di corpus come BLEU non riescono a catturare queste traduzioni, rendendo difficile l'valutazione.</sample>
    <sample id="142">Ciao! Sto per parlare del nostro lavoro su "Risoluzione delle Espressioni di Riferimento Indiretto per la Selezione di Entità", in cui introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Intendi 'Easy on Me' o 'I Gotta Feeling'?" In questo caso, un utente vuole selezionare tra una di queste due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone "Easy on Me" o la sua posizione, "la prima". Tuttavia, a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone. Oppure le pronunce sono troppo simili e difficili da disambiguare. Oppure quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "la più recente" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per il benchmarking della comprensione delle entità da parte dei LLM. Non siamo a conoscenza di un dataset pubblico su larga scala per questo compito, quindi ne raccogliamo uno utilizzando l'annotazione di massa. Il nostro dataset copre tre diversi domini: musica, libri e ricette. Il nostro metodo di raccolta del dataset enfatizza l'informalità utilizzando un setup di completamento di un fumetto. Il fumetto ha tre bolle di dialogo. Nella prima bolla, Bob dice: "Ricordi quella canzone che stavamo ascoltando ieri?" E con questo, Bob stabilisce il contesto del dialogo. Nella seconda bolla di dialogo, Alice dice: "Intendi 'Easy on Me' o 'I Gotta Feeling'?" Questa è la domanda alternativa. E nella terza bolla di dialogo, Bob usa un riferimento indiretto per selezionare una di queste entità, ad esempio, "la più recente". Forniamo le prime e seconde bolle automaticamente, ma la terza è compilata dall'annotatore. La prima bolla è scelta da pochi prompt manuali per dominio. La seconda, che è la domanda alternativa, viene generata come segue. Usiamo sempre un semplice modello. Intendi A o B? Dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di campionamento che abbiamo usato. Quando ci spostiamo più in alto nell'elenco, le entità diventano più simili tra loro e di solito è più difficile fare la disambiguazione. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno caselle di informazioni o attributi simili su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, loro conoscono i nomi di queste entità, ma non necessariamente le entità stesse. Quindi, mostriamo alcune conoscenze di base sulle due entità. Per le canzoni, mostriamo semplicemente un link di ricerca Google per ogni canzone e poi chiediamo agli annotatori di ascoltare almeno parte di ogni canzone e leggere su ogni canzone. Ecco, ad esempio, il risultato della ricerca Google per la canzone "Easy on Me". Per i domini ricette e libri, mostriamo del testo di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, ancora da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi abbiamo chiesto agli annotatori di scegliere una di queste entità, ad esempio, ecco la prima, e descriverla usando tre a cinque espressioni di riferimento indiretto. Ad esempio, "quella con la musica al pianoforte". Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il ragazzo di 12 anni", o "la finzione", o "proviene dall'Azerbaigian", e così via. Il Corpus AltEntities ha 6.000 domande alternative su tre domini e 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello di linguaggio ha accesso alla stessa conoscenza di base esatta degli annotatori, allora l'accuratezza è davvero alta, intorno al 92-95%. Ma questo non è realistico. Se il modello di linguaggio ha accesso a una conoscenza di base parzialmente sovrapposta, allora l'accuratezza è tra il 82 e l'87%, che è più realistico. Ad esempio, quando il modello di linguaggio recupera la conoscenza di base. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per miglioramenti. Abbiamo anche mostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio EDAtt viene confrontato con le politiche SimulST esistenti Wait-k e Local Agreement, nonché con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea.</sample>
    <sample id="144">L'affiliazione degli autori dell'articolo è l'Università di Nantes.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">Yicheng, a PhD student from Fudan University, presents a paper on the analysis of omission in dialogue summarization. Dialogue summarization, a subtask of text summarization, involves creating concise summaries from dialogues, capturing essential information across various domains. Despite advancements using large-scale pretrained language models, generated summaries often contain factual errors, with omission being a significant issue leading to incomplete summaries. The study reveals that approximately 70% of summaries suffer from omissions, indicating a widespread problem. The research analyzes the distribution of omitted information, finding it randomly scattered across dialogue positions, highlighting the challenge of identifying key information. To address this, the paper introduces the OLDS dataset, providing high-quality omission labels for dialogue summarization across five domains. The dataset, built on existing benchmarks, uses diverse abstractive models to generate candidate summaries and employs an automatic method for producing omission labels, validated through human evaluation. The study explores three baseline frameworks for omission detection: pair-wise classification, sequence labeling, and pointer network, using Precision, Recall, F1-score, and a word-level omission recall (WR score) for evaluation. Results show an F1-score around 50%, underscoring the task's difficulty and the need for advanced models. The paper also investigates summary refinement using a post-editing method, where detected omissions are concatenated with candidate summaries to improve quality. This approach significantly boosts performance, suggesting that omission detection and refinement are promising directions for enhancing dialogue summarization.</sample>
    <sample id="147">Tre autori sono coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell'Università di Trento e della Fondazione Bruno Kessler e ti presenterò brevemente il paper "Attention as a Guide for Simultaneous Speech Translation", un lavoro congiunto con Matteo Negri e Marco Turchi. Cosa è la simultanea traduzione automatica della voce? La simultanea traduzione automatica della voce, o SimulST, è il processo di traduzione della lingua parlata in testo in un'altra lingua in tempo reale, abilitando la comunicazione interlinguistica. E quali sono i problemi dei modelli attuali di SimulST? Architetture specifiche vengono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, addestramento che coinvolge diversi obiettivi di ottimizzazione. E addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con una latenza media di un secondo e un altro con due secondi, e così via. Quindi, qual è la nostra soluzione? Prima di tutto, utilizzare modelli offline di traduzione automatica della voce esistenti senza riaddestrare o adottare architetture specifiche per SimulST. Utilizzare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. E sfruttare la conoscenza già acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale. Questo è il meccanismo di cross-attenzione, e puoi vedere un esempio a destra. La nostra soluzione è proporre EDAtt, o Encoder-Decoder Attention, e si tratta di una strategia per decidere se emettere o meno una traduzione parziale, in base a dove l'attenzione punta. Una parola viene emessa se l'attenzione non è concentrata, cioè, la sua somma è al di sotto di una certa soglia alpha verso gli ultimi lambda frame di discorso, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un frammento di discorso contenente "I'm going to talk about..." e il nostro modello predice la traduzione in tedesco, e guardiamo i pesi di cross-attenzione, vedremo che le prime due parole puntano ai primi frame di discorso ricevuti, mentre l'ultima parola punta agli ultimi frame di discorso ricevuti, come lambda frame di discorso. Questo significa che le prime due parole saranno emesse mentre, poiché la somma della cross-attenzione è al di sopra di una certa soglia alpha, non emetteremo l'ultima parola e aspetteremo un altro frammento di discorso. Se andiamo avanti e riceviamo un altro frammento di discorso, e il nostro modello predice altre tre parole e guardiamo quei pesi di cross-attenzione, vedremo che nessuna parola punta agli ultimi lambda frame di discorso. Questo significa che queste tre parole saranno emesse. Se guardiamo i principali risultati di EDAtt, tracceremo i risultati di simultanea traduzione automatica della voce su grafici in cui abbiamo BLEU su un lato che misura la qualità della traduzione, e l'average lagging che è la misura di latenza, e consideriamo anche l'average lagging computationally aware che tiene conto dei tempi computazionali del modello per prevedere l'output. Quindi vogliamo che i nostri grafici siano il più in alto possibile su questo grafico. Ma vogliamo anche che siano spostati a sinistra. E confrontiamo con strategie popolari che sono anche applicate a modelli offline, che sono la strategia Wait-k e la Local Agreement. E confrontiamo anche con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea pre-traduzione. Questi sono tutti i risultati della strategia di simultanea traduzione automatica della voce in tedesco. E vediamo che supera tutte le strategie applicate a modelli offline poiché i grafici sono spostati a sinistra. E vediamo anche che, se consideriamo il tempo effettivamente trascorso o il tempo computationally aware, che è la strategia più veloce. Se vuoi scoprire altri risultati, leggi il nostro paper. E abbiamo anche rilasciato il codice e i modelli open source e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Il contenuto non specifica se il set di dati CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">The paper introduces MeetingQA, a novel extractive question-answering dataset derived from meeting transcripts, addressing the underexplored QA component in meeting discussions. Meeting transcripts, often long and domain-specific, present unique challenges and opportunities for NLP research. Unlike prior works focusing on summarization and action item extraction, MeetingQA emphasizes the QA aspect by leveraging questions and answers from meetings. The dataset is constructed from the AMI corpus, comprising nearly 100 hours of transcribed meetings, with 7.7K questions annotated for answer spans. The dataset includes 30% unanswerable questions, 40% with multispan answers, and 48% with multi-speaker answers. A significant portion of questions are yes/no or opinion-seeking, with 20% rhetorical and 70% of multi-speaker answers containing disagreement. The paper evaluates various models, noting a 25 F1 point gap between fine-tuned models and human performance, with short-context models slightly outperforming long-context ones. Multi-span models show comparable performance to single-span models. Zero-shot performance reveals a 50 F1 point gap from human performance, with silver data augmentation improving results. Error analysis highlights difficulties in identifying rhetorical questions and speaker attribution, especially in zero-shot settings. MeetingQA presents a challenging yet promising domain for advancing QA models in real-life scenarios.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang e presenteremo la nostra ricerca su MultiInstruct, che migliora l'apprendimento zero-shot multi-modale tramite l'addestramento per istruzioni. Con i progressi nei grandi modelli di linguaggio, molte ricerche hanno esplorato nuovi paradigmi di riutilizzo di modelli di linguaggio pre-addestrati per vari compiti a valle in modo efficiente in termini di parametri e dati. Recentemente, diversi studi hanno dimostrato che l'addestramento per istruzioni consente ai grandi modelli di linguaggio di eseguire compiti non visti in modalità zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle ricerche sull'addestramento per istruzioni si è concentrata sul miglioramento delle prestazioni zero-shot per compiti di linguaggio solo, mentre la visione artificiale e i compiti multi-modali sono stati trascurati. Pertanto, in questo lavoro vogliamo indagare se l'addestramento per istruzioni di modelli pre-addestrati multi-modali possa effettivamente migliorare la generalizzazione per compiti multi-modali non visti. Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi-modale. Esistono più di 1600 compiti di istruzioni solo testuali, ma non esiste un dataset di istruzioni multi-modale di grandi dimensioni pubblicamente disponibile. Pertanto, ciò ci motiva a costruire un dataset di addestramento per istruzioni multi-modale. Presentiamo MultiInstruct, il primo benchmark di dataset di addestramento per istruzioni multi-modale, che consiste in 62 compiti multi-modali diversi che coprono 10 categorie ampie. Questi compiti sono derivati da 21 dataset open-source esistenti e ciascun compito è dotato di cinque istruzioni scritte da esperti. Per indagare l'addestramento per istruzioni multi-modale sul nostro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modale unificato, come modello base. OFA utilizza un vocabolario unificato per i token di linguaggio, immagine e le coordinate di un riquadro. Mostriamo alcune istanze di esempio dal nostro dataset MultiInstruct per unificare il trattamento di vari tipi di dati di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequenza-a-sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i riquadri sono rappresentati nello stesso spazio di token. Ora parlerò di addestramento per istruzioni multi-modale. Per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per l'addestramento e campioniamo 10.000 istanze per compito. Per il test, riserviamo l'intero gruppo di ragionamento del senso comune per il test e selezioniamo ulteriori 5 compiti dai gruppi VQ e Varie. Utilizziamo tutte le istanze della divisione di test per ciascun compito. Inoltre, campioniamo casualmente 20 compiti dalla divisione di test delle istruzioni naturali come compito non visto per NLP. Utilizziamo il modello OFA grande pre-addestrato come modello base. Durante l'addestramento, mescoliamo tutte le istanze di tutti i compiti. Ogni istanza è combinata casualmente con una delle sue cinque istruzioni di template. Durante il test per ciascun compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ciascun esperimento, riportiamo la prestazione minima e massima e la deviazione standard della prestazione tra tutti e 5 gli esperimenti. Se il compito è una classificazione multi-modale, riportiamo l'accuratezza. Se è un compito di generazione multi-modale, riportiamo Rouge-L. Per il compito NLP, riportiamo anche Rouge-L. Introduciamo anche un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre costantemente gli stessi output per lo stesso compito, indipendentemente dalle lievi variazioni nella formulazione dell'istruzione. Ecco il nostro risultato principale. Come possiamo vedere, l'addestramento per istruzioni può migliorare significativamente le prestazioni di OFA sui compiti multi-modali visti. Inoltre, il trasferimento di apprendimento dai dataset di istruzioni naturali può beneficiare dell'addestramento per istruzioni. Come possiamo vedere, man mano che aumenta il numero di compiti, il modello raggiunge prestazioni migliori e, contemporaneamente, una sensibilità inferiore. Abbiamo anche fatto un esperimento. Usiamo un'istruzione rispetto a 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurre notevolmente la sua sensibilità. Ciò mostra l'effetto di diverse strategie di fine-tuning sulla sensibilità del modello. Come possiamo vedere, il trasferimento di apprendimento dai dataset di istruzioni naturali consente al modello di raggiungere una sensibilità molto migliore rispetto al modello OFA originale. Possiamo anche vedere che il trasferimento di apprendimento dai dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset di istruzioni naturali. In generale, proponiamo il primo dataset di addestramento per istruzioni multi-modale su larga scala con capacità zero-shot significativamente migliorate di OFA e esploriamo diverse tecniche di trasferimento di apprendimento e ne dimostriamo i benefici. Progettiamo una nuova metrica chiamata sensibilità. Un'ultima cosa, stiamo raccogliendo un dataset di addestramento per istruzioni multi-modale molto più grande con circa 150 compiti aggiuntivi di linguaggio-immagine e lo rilasceremo. Ecco un codice QR per i nostri dati e modelli. Grazie.</sample>
    <sample id="152">Frederick Riemenschneider presents advancements in language models for classical philology, focusing on Ancient Greek and Latin. The talk highlights the limitations of existing monolingual BERT models and introduces new models designed to address these gaps. The project's goals include making models comparable, advancing the state-of-the-art, exploring different architectures, and developing multilingual models. Two monolingual models, GreBERTa and GreTa, were created for Ancient Greek, with GreTa based on the T5 architecture. Additionally, multilingual models PhilBERTa and PhilTa were developed, pre-trained on Ancient Greek, Latin, and English data. A novel pre-training corpus was created using the Internet Archive, identifying Greek texts through OCR errors and re-scanning with Greek settings. Benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization showed these models outperform existing ones. The study also explored the behavior of T5 encoders and the impact of multilinguality, finding no significant performance difference between multilingual and monolingual models. The research presents powerful, native tokenizer-based models for classical philology, offering a comprehensive overview of their development and capabilities.</sample>
    <sample id="153">In this work, we address ambiguities in text-to-image generative models by proposing frameworks to mitigate and evaluate these ambiguities, ensuring generated images align with user intentions. We focus on ambiguous prompts, such as "The girl enters the room with flowers," which can be interpreted in multiple ways. Our approach involves curating a benchmark dataset based on the LAVA corpus, covering various ambiguity types. We introduce a prompt disambiguation framework that uses a language model to either generate clarifying questions or propose different visual interpretations. Users respond to these questions or select the interpretation that matches their intention, resulting in disambiguated prompts. These prompts are then input into text-to-image models to generate images, which are evaluated for faithfulness to user intention using a Visual Question Answering (VQA) model. Our findings indicate that our framework effectively resolves ambiguities, leading to more faithful image generation. Additionally, our automatic evaluation framework aligns well with human evaluations, demonstrating its reliability. The study highlights disparities in resolving different types of ambiguities and underscores the positive impact of our disambiguation framework on image generation fidelity. Further details and discussions are available in our paper.</sample>
    <sample id="154">Sara Papi è affiliata all'Università di Trento e alla Fondazione Bruno Kessler. Gli altri autori sono Matteo Negri e Marco Turchi.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="157">The paper "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" by Shen Gao and colleagues introduces a novel approach to dialogue summarization, addressing the challenges of distilling salient information from complex, multi-participant dialogues. Traditional methods rely on pre-computed static graph structures using external linguistic tools, which can be unreliable and inflexible. The proposed SDDS model overcomes these limitations by integrating both static and dynamic graph structures.

The SDDS model comprises four main components: an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator. The Utterance Encoder transforms dialogue utterances into vector representations. The Static-Dynamic Graph module constructs static graphs using heuristic methods, such as Discourse Parsing Graphs, Key Co-occurrence, speaker interaction frequency matrices, and utterance position graphs. These static graphs are then fused using a 1x1 convolutional layer.

To capture semantic relationships dynamically, the model employs a Dynamic Graph module with a multi-head attention mechanism, avoiding reliance on pre-computed structures. The fusion of static and dynamic graphs is achieved through a unified graph representation, which is integrated into the generation process using a dual cross-attention mechanism with a graph attention layer.

This approach allows the model to adaptively learn and represent dialogue structures, enhancing the quality of generated summaries. The code and data for the SDDS model are available on GitHub, facilitating further research and application in dialogue summarization.</sample>
    <sample id="158">The presentation introduces "Dual Cache for Long Document Neural Coreference Resolution," a method developed by Qipeng Guo from AWS to address the challenges of coreference resolution in long documents. Coreference resolution involves identifying and clustering mentions of the same entity within a text. Traditional methods, which enumerate all mention pairs, suffer from quadratic complexity in computation and memory. Recent cache-based methods reduce this complexity to linear by using a fixed-size cache with eviction policies like Least Recently Used (LRU). However, LRU is inefficient for long documents with frequent topic shifts, leading to high cache misses for entities mentioned globally.

To address this, the dual cache system is proposed, comprising a local cache and a global cache. The local cache uses the LRU policy for local entities, while the global cache employs the Least Frequently Used (LFU) policy for global entities. The model processes the document from left to right, classifying new mentions as either new entities or belonging to cached entities, and evaluates their frequency to determine cache placement. The dual cache system is evaluated on public benchmarks, showing superior performance over baseline methods, even with unbounded memory. It significantly reduces cache misses and demonstrates a higher performance/cost ratio, making it more efficient and effective for long document coreference resolution.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla nostra discussione sul nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, rivediamo i paradigmi di coppie minime. Il paradigma di coppie minime valuta i modelli linguistici in base a giudizi di accettabilità, che possono includere anche la grammaticalità come in BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come in CrowS pairs. Nel paradigma di coppie minime, il modo tipico per valutare i modelli linguistici è mostrare una frase accettabile o grammaticale e poi una frase accettabile o non grammaticale, con l'aspettativa che il modello attribuisca una maggiore probabilità alla frase accettabile. L'attuale pipeline MPP non ci permette di valutare l'accettazione di un modello verso frasi più lunghe. Oggi i grandi modelli linguistici stanno emergendo con finestre di contesto sempre più lunghe. È cruciale valutare l'accettabilità dei modelli in tutta la finestra di contesto, ed è ciò che stiamo cercando di fare. Rivediamo la pipeline MPP chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe. Per simulare queste sequenze più lunghe, rivediamo i dataset stessi e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Ad esempio, abbiamo scelto una coppia tipica di grammaticalità dal dataset BLiMP nel caso di Adjunct Island. Estraiamo frasi grammaticali da Adjunct Island e le aggiungiamo come prefisso sia alla query accettabile che a quella inaccettabile. Possiamo fare lo stesso scegliendo frasi inaccettabili dallo stesso matching, che potrebbe anche essere usato per testare l'accettabilità del modello. Possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un dataset diverso. Questo è ciò che chiamiamo uno scenario di mismatch. Le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset con cui si sta valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato come Wikipedia. Questo ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, se il contesto proviene da un sottoinsieme diverso del dataset o se è completamente irrilevante alla frase attuale. Come si comporta il modello? Prima, guardiamo alle frasi di Wikipedia, completamente irrilevanti alla coppia di query attuale, e troviamo che i giudizi MPP sono per lo più robusti per qualsiasi lunghezza di contesto. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. E vediamo qui nella linea tratteggiata arancione che i giudizi MPP sono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Creiamo frasi da domini accettabili e inaccettabili dallo stesso dataset BLiMP o SyntaxGym. E lì vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o inaccettabili. Quando corrispondiamo la struttura, cioè quando scegliamo le frasi dallo stesso fenomeno in BLiMP o SyntaxGym, vediamo un aumento o una diminuzione massiccia del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto è molto grande e aumenta con la lunghezza del contesto, influenzando probabilmente i nuovi modelli linguistici con finestre di contesto più grandi. Perché il prefisso corrispondente influisce così tanto sul giudizio del modello linguistico? Abbiamo fatto una serie di analisi cercando di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. Dopo diverse perturbazioni, scopriamo che nessuno di questi rumori fa cambiare al modello il suo corso in termini di come mostra il giudizio MPP. In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modi simili. Quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi nel dominio inaccettabile, vediamo una diminuzione dei giudizi MPP in modo simile. Le principali conclusioni del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi. La valutazione MPP come la facciamo attualmente con input di frasi brevi e singole potrebbe non catturare appieno la conoscenza astratta dei modelli linguistici in tutta la finestra di contesto. Per ulteriori dettagli sui nostri esperimenti, leggete il nostro articolo. Grazie per l'ascolto.</sample>
    <sample id="160">Il primo passaggio del metodo mappa i token di input in un unordered multiset di token che appariranno nell'output.</sample>
    <sample id="161">55,000 script sono rappresentati in CoScript.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato (WSL) offre il vantaggio di ridurre i costi e lo sforzo associati alla creazione di dataset manualmente annotati, utilizzando invece fonti di annotazione deboli come regole euristiche, basi di conoscenza o crowdsourcing di bassa qualità. Questo approccio consente di addestrare modelli neurali su dati meno costosi e più facilmente disponibili, pur mirando a mantenere la capacità di generalizzazione del modello.</sample>
    <sample id="165">The paper "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" by Wenting Zhao introduces a novel unsupervised learning method for abductive reasoning, named LiPoR (Likelihood Learning with Posterior Regularization). Abductive reasoning involves identifying plausible explanations that bridge the gap between a given context and an outcome. Traditional approaches rely on supervised methods requiring annotated plausible explanations, which are often subjective and noisy. Zhao's work addresses the challenge of learning abductive reasoning without supervision by proposing LiPoR, which treats explanations as latent variables and maximizes the marginal likelihood of outcomes given contexts without needing annotated plausibility.

LiPoR incorporates a regularizer to enforce mutual exclusivity among explanations, a key characteristic where only one explanation can be true at a time. The regularizer, denoted as Omega, minimizes the entropy of the probability distribution over explanations, preferring a subset of plausible explanations. The method was evaluated on the AlphaNLI dataset, where it outperformed zero-shot models and previous unsupervised approaches, including a strong GPT-3 baseline, by over 4 absolute points in accuracy. This demonstrates the effectiveness of LiPoR in unsupervised abductive reasoning by leveraging mutually exclusive explanations.</sample>
    <sample id="166">This work introduces a novel "Neural Divide-and-Conquer Reasoning Framework" for image retrieval from linguistically complex text, addressing the challenge of retrieving images from long, complex descriptions where images are highly similar. Traditional visual language models, which excel in image-sentence retrieval, struggle with complex text due to their reliance on analogical reasoning akin to System 1 of the Dual-Process Theory. To overcome this, the proposed framework integrates the Divide-and-Conquer strategy with Dual-Process Theory, combining analogical reasoning (System 1) and logical reasoning (System 2).

The framework comprises three main components: the Proposition Generator, the Visual-Linguistic Interactor, and the Neural-Symbolic Reasoner. The Proposition Generator decomposes complex text into simpler propositions, using BART's decoder to generate corresponding sentences. The Visual-Linguistic Interactor, representing System 1, facilitates interaction between visual and linguistic information, producing matching scores and reasoning states. The Neural-Symbolic Reasoner, embodying System 2, integrates these states to derive the final solution, employing a negation executor and conjunction operation for logical inference.

Experimental results demonstrate that the proposed method, NDCR, outperforms existing baselines, with ablation studies confirming the effectiveness of each module. The framework's ability to present intermediate inference states and results highlights its interoperability. The study suggests that neural symbolic calculation could enhance compositional reasoning in large language models, with the Divide-and-Conquer approach paralleling the self-asking chain-of-thought method for complex problem-solving. Integrating Dual-Process Theory with Divide-and-Conquer offers a promising direction for future research.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati sia manualmente che con metodi di allineamento automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo dati da Reuters News dal 2020 e annotandoli con le stesse linee guida di annotazione del CoNLL-2003.</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" explores the use of the PaLM large language model for machine translation. PaLM, with 540 billion parameters, is evaluated for its translation capabilities using best practices from the machine translation (MT) community. The study focuses on the impact of prompting strategies on translation performance, comparing PaLM to state-of-the-art systems using the latest test sets and metrics like BLEURT. Experiments reveal that prompting significantly affects translation quality, with one-shot prompting showing over one BLEURT point difference in most cases. A 5-shot prompting strategy, marking sentences by language, was found effective, with example quality being more crucial than prompt form. PaLM's translations are fluent but less accurate, often omitting parts of the source text. Despite this, PaLM's fluency is comparable to commercial systems like Google Translate, though specialized systems still outperform it. The study highlights the importance of high-quality examples in prompting and provides recommendations for improving translation performance with large language models.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dell'Università di Stato della Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Il parsing semantico è un compito per costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda Calculus. Il parsing semantico cross-linguistico è il compito di tradurre le query in più lingue naturali in più rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, eccetera. I modelli esistenti di parsing semantico cross-linguistico sono stati proposti e valutati separatamente su set di dati di compiti e applicazioni limitate. Ad esempio, ci sono molte coperture su certe lingue naturali. Ma il cinese è assente e manca di copertura su certe rappresentazioni di significato. Il Lambda calculus è assente, o sono stati valutati solo su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutarli. Pertanto, proponiamo XSemPLR. Forniamo un set di dati uniforme XSemPLR per il parsing semantico cross-linguistico in più lingue naturali e rappresentazioni di significato. Contiene 9 set di dati in vari domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche. E per valutare meglio il nostro benchmark, consideriamo sei impostazioni per addestramento e valutazione. La prima è Translate-Test. Utilizziamo l'API di Google Translate per tradurre la lingua di origine in quella di destinazione, quindi utilizziamo un modello monolingue per addestramento e valutazione. Ad esempio, addestriamo il modello inglese su query in inglese e durante l'inferenza traduciamo la query tedesca utilizzando l'API in inglese e poi utilizziamo il modello addestrato per prevedere il SQL. Testiamo anche il Modello Monolingue. In questa impostazione, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche l'impostazione Monolingue Few-shot addestrando modelli monolingue con solo il 10% dei dati di addestramento. E testiamo il Modello Multilingue che addestriamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglesi, cinesi per addestrare un modello multilingue. E durante l'inferenza possiamo utilizzare questo modello per tradurre le query tedesche o cinesi, eccetera. E consideriamo anche il trasferimento Cross-lingual Zero-shot e Few-shot. Addestriamo su una lingua di origine e trasferiamo ad un'altra lingua. Durante l'addestramento, addestriamo su query in inglese o la combinazione di query in inglese e tedesco Few-shot per addestrare un modello multilingue per prevedere l'output SQL. E abbiamo anche scoperto molti risultati interessanti. Riguardo all'analisi dei modelli monolingue, valutiamo su due gruppi di modelli, inclusi Encoder-PTR che sta per Multilingual Pretrained Encoders con Decoder basati su Pointer, come XLM-R + PTR e mBERT + PTR. E, valutiamo anche i modelli Encoder-Decoder, che sono Multilingual Pretrained Encoder-Decoder Models, come mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene le migliori prestazioni su tutti e nove i set di dati. E valutiamo su mT5 e XLM-R + PTR nell'impostazione multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrandoli in un mix di varie lingue. Abbiamo scoperto che è perché la maggior parte delle principali lingue naturali può ottenere un guadagno di prestazioni, tranne che le prestazioni in inglese diminuiscono in sette set di dati e aumentano solo in tre set di dati. Penso che questo sia noto come la "Maledizione della Multilinguismo". Confrontiamo anche il divario di prestazioni cross-linguistiche. In questa figura, la linea blu è il trasferimento Cross-lingual Few-shot. La linea arancione è il trasferimento Cross-lingual Zero-shot. Mentre la linea verde è l'impostazione Monolingue. Abbiamo scoperto che, confrontando la linea verde e quella arancione, abbiamo scoperto che l'impostazione Zero-shot, il divario di prestazioni del trasferimento cross-linguistico è significativo, e poi confrontando le linee blu e arancione, abbiamo scoperto che con l'impostazione Few-shot il divario di trasferimento si riduce rapidamente. Abbiamo anche scoperto alcune altre scoperte interessanti. Ad esempio, Encoder-Decoder supera il lavoro precedente o raggiunge risultati comparabili. L'addestramento su lingua naturale inglese può migliorare significativamente le prestazioni del Few-shot sulla lingua naturale di destinazione, e abbiamo scoperto che i modelli linguistici multilingue come Codex e BLOOM sono ancora inadeguati per i compiti di parsing semantico cross-linguistico. Per concludere, costruiamo XSemPLR, un benchmark unificato per il parsing semantico cross-linguistico con più lingue naturali e rappresentazioni di significato. Conduciamo uno studio di benchmark completo sui tre tipi rappresentativi di modelli linguistici multilingue. E i nostri risultati mostrano molte scoperte interessanti. E così via. E benvenuti a visitare il nostro articolo e il codice. Grazie per l'ascolto.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro categorie principali: metodi di watermarking basati su modelli, metodi di watermarking basati su dati, metodi di watermarking basati su query e metodi di watermarking basati su backdoor. Tuttavia, questi metodi esistenti presentano limitazioni, come non essere applicabili ai servizi di embedding o mancare di trasferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom sono ancora inadeguati per i compiti di cross-lingual semantic parsing.</sample>
    <sample id="174">The paper "ArgAnalysis35K: A Large-Scale Dataset for Argument Quality Analysis" introduces a unique dataset designed to enhance the evaluation of argument quality. Unlike existing datasets, which often suffer from low-quality arguments sourced from crowdsourcing platforms and lack diversity, ArgAnalysis35K offers a comprehensive collection of 35,000 argument-analysis pairs. These arguments are primarily sourced from high-quality debates, expert debaters, and intermediate debaters, ensuring superior quality. The dataset covers 24 diverse themes, capturing a wide range of motions to reflect the variety found in parliamentary debates, rather than limiting to a few pre-selected topics.

A key innovation of ArgAnalysis35K is the introduction of "analysis" as a distinct concept, which combines claims and premises to provide a more coherent explanation of arguments. This approach addresses the limitations of traditional datasets that only include arguments without detailed analysis. Additionally, the dataset employs instance-based annotator reliability, allowing for the retention of valuable annotations by mitigating individual biases on a per-argument basis.

Furthermore, ArgAnalysis35K introduces a relevance model that assigns a relevance score to each argument across different themes, acknowledging that arguments can be applicable to multiple topics. This model enhances the dataset's utility by capturing the multifaceted relevance of arguments. Overall, ArgAnalysis35K stands out for its high-quality, diverse, and analytically rich arguments, offering a more reliable and comprehensive resource for argument quality analysis.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni utilizzando un metodo di approssimazione continuo che è amichevole per il GPU. Questo permette di apprendere le permutazioni linguisticamente più plausibili e di eseguire il backpropagation attraverso la soluzione.</sample>
    <sample id="176">L'equità di un modello NLP a valle viene definita valutando le prestazioni del modello su compiti come la rilevazione dell'odio e la rilevazione delle fake news, separando le prestazioni in base a diverse demografie o orientamenti politici dei media. Si osserva che i modelli con orientamenti politici diversi mostrano prestazioni variabili in base al gruppo target, evidenziando potenziali problemi di equità. Ad esempio, i modelli di sinistra sono migliori nel rilevare discorsi d'odio contro gruppi socialmente minoritari, mentre i modelli di destra sono migliori nel rilevare discorsi d'odio contro gruppi più potenti. Queste differenze indicano che l'orientamento politico dei modelli può portare a problemi di equità nelle applicazioni NLP.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">Melanie Sclar presents research on enhancing Theory of Mind (ToM) reasoning in large language models (LLMs) through a method called SymbolicToM. ToM involves understanding others' mental states, traditionally assessed via false-belief tasks like the Sally-Anne test. LLMs, including models like GPT-3, struggle with these tasks. SymbolicToM addresses this by using explicit graphical representations to model characters' beliefs and their estimations of others' beliefs. These graphs, such as BBob and BBob,Alice, represent different mental states and are computed for all character combinations up to a predefined ToM level using NLI and OpenIE models.

SymbolicToM operates at inference time, allowing for efficient question answering by transforming belief-based questions into factual queries over the graphs. The method was tested across various LLMs, showing significant performance improvements over baseline models, including a 65-point accuracy gain for GPT-3-Davinci. The research also evaluated SymbolicToM's robustness with out-of-domain datasets, demonstrating its ability to generalize across different story structures and linguistic variations. SymbolicToM outperformed supervised models in these tests, highlighting its effectiveness in enhancing LLMs' ToM reasoning without overfitting. The method's interpretability and adaptability make it a promising tool for improving LLMs' understanding of complex mental state scenarios.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces "Distilling Script Knowledge from Large Language Models for Constrained Language Planning," addressing the gap in planning for specific goals with constraints, such as "make a chocolate cake." The study defines constrained language planning, where abstract goals are adapted to specific, constraint-laden real-life scenarios. Evaluating large language models reveals their inadequacy in generating scripts faithful to constraints, despite acceptable semantic completeness. The research employs a human-in-the-loop approach to acquire specific goals using InstructGPT, extending abstract goals with multi-faceted constraints. An over-generate-then-filter method is proposed to enhance script quality, using cosine similarity and keyword presence to select the most faithful scripts. The study also introduces CoScript, a dataset of 55,000 specific goals with scripts, created through symbolic knowledge distillation from large language models. CoScript demonstrates high diversity and enables smaller, specialized models like T5 to outperform larger models when fine-tuned on this dataset. The paper concludes by highlighting CoScript's potential as a valuable resource for advancing constrained language planning research.</sample>
    <sample id="182">Nel contesto di questo articolo, il tropicalismo si riferisce a un tropo che descrive le donne latine con parole come "vibrante" e "curvacee", contribuendo a stereotipi che le associano a immagini esotiche e sensuali.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando un metodo a due parti. In primo luogo, hanno generato "personaggi" chiedendo a un modello linguistico di rispondere a prompt come "Immagina di essere una donna asiatica. Descrivi te stessa." Questi prompt sono stati ispirati da uno studio che ha dato prompt simili a soggetti umani per rivelare stereotipi razziali. In secondo luogo, hanno utilizzato il metodo delle "Parole Marcate" per identificare le parole che distinguono i gruppi marcati dagli unmarcati, confrontando i personaggi generati con quelli scritti dagli esseri umani.</sample>
    <sample id="184">In questo lavoro, l'utilizzo del contesto è stato misurato utilizzando CXMI (Contextual Mutual Information) e la sua estensione, Pointwise CXMI (P-CXMI). CXMI misura quanto informazione il contesto fornisce sulla traduzione target data la fonte, mentre P-CXMI consente di misurare l'utilizzo del contesto a livello di frase o di parola.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato in francese per domini biomedici e clinici basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. ChuBERT, d'altra parte, è basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes e include sia dati clinici che un mix di NACHOS. DrBERT si concentra su dati web raccolti, mentre ChuBERT utilizza dati clinici anonimizzati.</sample>
    <sample id="187">Due autori sono coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento coinvolge il processo di affinamento di un modello inizialmente addestrato su compiti correlati, attraverso iterazioni successive di addestramento su nuovi dati. Nel contesto del documento, il trasferimento iterativo è stato utilizzato per migliorare le prestazioni del modello inizialmente trasferendo pesi da compiti correlati (come la classificazione dello stile di dissonanza e la classificazione binaria delle classi di espansione e confronto) e poi affinando ulteriormente il modello su questi compiti in modo iterativo per migliorare le prestazioni zero-shot.</sample>
    <sample id="189">L'obiettivo del set di dati è comprendere il linguaggio degli utenti quando vogliono fare una scelta, in particolare quando usano espressioni di riferimento indiretto per selezionare tra entità, e fornire un benchmark per valutare la comprensione delle entità da parte dei modelli di linguaggio.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un Embedding as a Service (EaaS) apprendendo dalle risposte di embedding fornite dal servizio. Analizzando queste risposte, l'attaccante può ricostruire un modello simile che fornisce servizi simili.</sample>
    <sample id="191">Tre autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">Yang Luo presents "CAME: Confidence-guided Adaptive Memory Efficient Optimization," addressing the challenge of designing an optimizer that achieves both fast convergence and low memory usage for training large language models. Traditional adaptive methods like Adam require significant memory for maintaining gradient moment estimates, while memory-efficient optimizers like Adafactor reduce memory usage but often at the cost of performance. The paper introduces CAME, inspired by non-negative matrix factorization (NMF) and addressing erroneous updates in Adafactor. CAME reduces instability by using the residual between momentum and current updates to guide optimization steps, improving stability and performance. Experiments on BookCorpus and English Wikipedia demonstrate CAME's superiority over Adam and Adafactor, achieving a 3.4% increase in validation accuracy with Adafactor and better performance than Adam in large model pre-training, while significantly reducing memory usage. CAME also enhances BERT-Large training and maintains comparable performance to baselines on downstream tasks with reduced memory costs. The optimizer shows effectiveness in large batch training, marking an important advancement in memory-efficient optimization.</sample>
    <sample id="193">Il contenuto non specifica il numero di annotatori impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">Le affiliazioni degli autori dell'articolo includono Carnegie Mellon University, University of Washington e Allen Institute for AI.</sample>
    <sample id="195">Il lavoro "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" affronta la sfida dell'XQA, che mira a fornire risposte a domande e spiegazioni su perché tali risposte siano state selezionate. Le attuali metodologie XQA si dividono in metodi neurosimbolici e metodi basati sulla decomposizione. I metodi neurosimbolici, che traducono domande in rappresentazioni formali come SPARQL, sono limitati dalla mancanza di completezza delle basi di conoscenza strutturate. I metodi basati sulla decomposizione, che generano passaggi intermedi in linguaggio naturale, affrontano la difficoltà di gestire la diversità del linguaggio naturale. Entrambi i metodi potrebbero beneficiare dall'integrazione di conoscenze da fonti eterogenee, specialmente per domande complesse.

Il lavoro propone un nuovo framework, RoHT (Reasoning over Hierarchical Question Decomposition Tree), che affronta due sfide principali: determinare la granularità della decomposizione delle domande e trovare la soluzione ottimale tra molteplici possibilità. RoHT è un framework a due fasi che comprende la costruzione di un Hierarchical Question Decomposition Tree (HQDT) e la conduzione di un ragionamento probabilistico su di esso. Il HQDT rappresenta la struttura compositiva gerarchica di una domanda complessa, con domande atomiche come nodi foglia. Il ragionamento probabilistico avviene in modo ricorsivo, coinvolgendo un scheduler per selezionare le fonti di conoscenza appropriate, esecutori per ottenere risposte con probabilità e un aggregatore per combinare le risposte candidate.

Il framework RoHT è valutato su due dataset complessi, KQA Pro e Musique. I risultati mostrano che RoHT supera i metodi esistenti, dimostrando i benefici dell'integrazione di risposte a domande di diversi livelli e dell'utilizzo congiunto di basi di conoscenza e testo. In particolare, RoHT migliora significativamente le prestazioni rispetto ai metodi esistenti, evidenziando l'efficacia della decomposizione esplicita e dell'integrazione di fonti di conoscenza eterogenee.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "I saw Bart and Lisa."</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo menzionati sono quattro stati dell'arte che sono stati valutati utilizzando ABC-Eval. Tuttavia, i nomi specifici di questi modelli non sono forniti nel contenuto.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli di linguaggio attuali hanno finestre di contesto sempre più lunghe. È cruciale valutare come i modelli gestiscono l'accettabilità in sequenze più lunghe per comprendere meglio le loro capacità e limitazioni, poiché le prestazioni possono variare significativamente con l'aumento della lunghezza del contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset, un fenomeno noto come "Curse of Multilinguality".</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo. Vengono mostrati i nomi delle entità e alcune informazioni di sfondo, ma non necessariamente conoscono le entità stesse.</sample>
    <sample id="201">Sono state utilizzate metriche di MT neurali di stato dell'arte, in particolare BLEURT, e sono stati mostrati anche risultati di valutazione basati su esperti umani.</sample>
    <sample id="202">Il regresso nella generalizzazione non è influenzato da specifici tipi di NER, ma piuttosto dal fenomeno del "temporal drift", che è la degradazione delle prestazioni causata dall'aumento del divario temporale tra i dati di addestramento e di test.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché può influenzare le prestazioni delle tecnologie tra diverse popolazioni, portando a design bias. Questi bias possono derivare dalle prospettive dei ricercatori e sviluppatori di modelli, che possono influenzare le decisioni nel processo di ricerca. Comprendere e caratterizzare la posizionalità nei dataset e nei modelli è cruciale poiché le attività di NLP diventano più soggettive e orientate socialmente, e non tutte le decisioni sono documentate. Identificare la posizionalità aiuta a garantire che le tecnologie siano più inclusive e rappresentative di diverse demografie.</sample>
    <sample id="204">Il contenuto non specifica se gli LLM multilingue come BLOOM siano stati affinati mediante adattatori o con una messa a punto integrale.</sample>
    <sample id="205">The presentation by Shangbin, a PhD student at the University of Washington, explores the propagation of political biases from pretraining data to language models and their impact on downstream NLP tasks. Language models are trained on large-scale web crawl data, including political news media, which introduces inherent social biases. The study investigates how these biases affect model performance and fairness in applications like hate speech and fake news detection. By prompting language models with political questionnaires, the research reveals varying political leanings among models, with GPT-4 being notably liberal. Controlled experiments show that further pretraining on partisan corpora shifts models' ideological coordinates, and temporal analysis indicates increased polarization post-2017. Performance evaluations demonstrate that left-leaning models better detect hate speech against minority groups, while right-leaning models excel in detecting hate speech against more powerful groups. Similar trends are observed in fake news detection. The findings highlight significant fairness issues, as deploying biased models could marginalize certain political opinions and fail to control hate speech against minorities. The study underscores the dilemma of balancing bias mitigation with the risk of censorship, emphasizing the need to address fairness in language model development.</sample>
    <sample id="206">Fanno ricorso a due modelli per il trasferimento dell'apprendimento: un modello per la classificazione dello stile di dissonanza nei dibattiti (chiamato "debate") e un modello per la classificazione binaria delle classi di espansione e confronto (CE) del PDTB.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i più recenti set di test WMT (Workshop on Machine Translation).</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto migliora la capacità di pianificazione linguistica vincolata migliorando sia la completezza semantica che la fedeltà ai vincoli. Utilizzando un approccio di sovra-generazione e filtraggio, il metodo seleziona script più fedeli ai vincoli rispetto ai modelli di linguaggio esistenti. Inoltre, il metodo di distillazione simbolica consente di creare il dataset CoScript, che migliora le prestazioni dei modelli più piccoli e specializzati, come T5, superando i modelli di linguaggio più grandi quando addestrati su dataset appropriati.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Sì, i risultati e il set di dati DEPLAIN possono essere utilizzati come parametri di riferimento per il problema dell'elaborazione automatica della semplificazione del testo in tedesco. Il paper propone risultati di base per la semplificazione automatica del testo e fornisce un corpus di riferimento per valutare i metodi di allineamento automatico e la semplificazione del testo.</sample>
    <sample id="212">Nell'articolo, viene menzionato un modello più piccolo: T5.</sample>
    <sample id="213">OFA (Unified Multi-Modal Pre-Trained Model) viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="215">In questo discorso, Adam Przepiórkowski esplora la struttura di dipendenza della coordinazione, confrontando diverse teorie e approcci. Le teorie di dipendenza universale e la teoria del testo significativo di Igor Mel'čuk adottano strutture asimmetriche, dove il primo congiunto è il capo della struttura coordinata. Al contrario, l'approccio di Praga e la grammatica delle parole di Hudson propongono strutture congiuntive capite e multi-capite, rispettivamente. L'obiettivo del paper è argomentare a favore delle strutture di coordinazione simmetriche, basandosi sul principio di minimizzazione della lunghezza delle dipendenze. Przepiórkowski illustra come la posizione dei congiunti possa essere influenzata dalla lunghezza e dalla posizione del governatore. Attraverso l'analisi del Penn Treebank, dimostra che i congiunti più brevi tendono a precedere quando il governatore è a sinistra o assente, ma questa tendenza scompare quando il governatore è a destra. Questi risultati supportano le strutture di coordinazione simmetriche, sfidando le teorie asimmetriche.</sample>
    <sample id="217">In "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," Weihao Zeng, Lulu Zhao, and Keqing He address the limitations of existing dialogue generation models that focus on single attributes, proposing a novel approach for multi-attribute controllable dialogue generation. They introduce Disentangled Controllable Generation (DCG), which learns attribute concepts from seen values and employs a disentanglement loss to separate different attribute combinations. DCG is built on the DialoGPT framework, incorporating compositional prompt modules to effectively utilize control signals. Two types of prompts are designed: attribute-oriented prompts, which guide the model to focus on specific information, and task-oriented prompts, which leverage global features for dialogue response generation. The authors also propose a unified reference-free evaluation framework, MAE, to assess different granularities of attributes without requiring large-scale labeled data. Experiments on two benchmarks demonstrate DCG's superior performance in attribute controllability and text equality, particularly for unseen attribute combinations. The study highlights the importance of attribute-oriented and task-oriented prompts, as well as disentanglement learning, in enhancing compositional generalization. The effectiveness of MAE is validated through correlation with human judgments, and its generality is shown by implementation on another pre-trained language model, BART. The research concludes that the proposed method successfully transforms seen attributes to unseen combinations, advancing the field of multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">In this work, "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports," Jia-Huei Ju and colleagues present a novel approach to analyzing financial reports, specifically targeting Form 10-K documents. The motivation stems from the observation that these reports are highly similar year-over-year, with about 80% of tokens being identical, making it challenging to extract meaningful information. To address this, the authors introduce a highlighting task within a multistage pipeline designed to compare and contrast the context between consecutive reports.

The pipeline consists of several stages: document segmentation, relation recognition, and two fine-tuning stages (out-of-domain and in-domain). The relation recognition stage classifies report pairs into three types: highly similar (β), revised, and mismatched pairs. The fine-tuning process utilizes the eSNLI dataset for out-of-domain training and employs soft labeling techniques to improve the quality of pseudo-labels.

The proposed model predicts word importance to identify key financial signals, with evaluation metrics including precision over recall and Pearson correlation coefficient (PCC). The model demonstrates superior performance on the FINAL dataset and maintains generalization capabilities on the eSNLI dataset. The authors highlight the potential for future enhancements, such as incorporating additional features and techniques from information retrieval to improve effectiveness. The work is detailed further in the accompanying paper and GitHub repository.</sample>
    <sample id="220">Gli autori dell'articolo sono affiliati a Stony Brook University.</sample>
    <sample id="221">L'articolo ha analizzato la coppia linguistica tedesco-inglese.</sample>
    <sample id="222">Questo lavoro, "Adattare o Annotare: Sfide e Interventi per l'Adattamento di Dominio nell'Elaborazione del Linguaggio Naturale per Risposte a Domande in Domini Aperti", esplora le sfide dell'adattamento di dominio nell'ambito dell'elaborazione del linguaggio naturale (NLP) per rispondere a domande in domini aperti. L'obiettivo è migliorare le prestazioni dei modelli di recupero e lettura, originariamente addestrati su domini generali come Wikipedia, quando applicati a domini specifici come quello biomedico. Il lavoro identifica tre contributi principali: l'indagine di interventi dati per l'adattamento di dominio, l'identificazione del tipo di spostamento di dataset in nuovi domini e la determinazione degli interventi dati efficaci per tipi specifici di spostamento.

Il lavoro esplora interventi dati zero-shot e few-shot. Gli interventi few-shot utilizzano pochi esempi di domini target per generare ulteriori esempi tramite modelli linguistici grandi, migliorando le prestazioni del recupero e della lettura rispettivamente del 8% e 11%. Gli interventi zero-shot controllano le interazioni tra domanda, risposta e contesto, variando il formato delle domande e la distribuzione delle risposte e del contesto, con risultati che indicano che le domande a stile cloze sono più facili da curare e che le distribuzioni uniformi delle risposte sono più efficaci.

Il lavoro utilizza una tassonomia esistente di spostamenti di dati per classificare i tipi di incompatibilità tra modelli e domini target, identificando spostamenti concettuali, covarianti e completi. Misurando la compatibilità tramite la probabilità assegnata dai modelli di recupero e lettura, il lavoro mappa i dataset target su un piano 2D per stimare il tipo di spostamento. I risultati mostrano che gli interventi few-shot sono efficaci per tutti i dataset target, mentre gli interventi zero-shot sono particolarmente utili per spostamenti concettuali e covarianti. In sintesi, il lavoro dimostra che le prestazioni della lettura possono essere migliorate fino al 24% con interventi dati appropriati, a seconda del tipo di spostamento del dataset.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Gli esperimenti hanno studiato i modelli MASSalign per l'allineamento automatico e i modelli long-mBART e base mBART per il fine-tuning per la semplificazione automatica del testo.</sample>
    <sample id="225">Per l'addestramento, vengono utilizzate 53 attività da 9 gruppi, con 10.000 istanze campionate per attività. Per il test, viene riservato l'intero gruppo di ragionamento del senso comune e vengono selezionate ulteriori 5 attività dai gruppi VQ e Varie. Inoltre, vengono campionate casualmente 20 attività dal test split delle istruzioni naturali come compito non visto per NLP.</sample>
    <sample id="226">Due autori sono coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Recent advancements in language models have shown significant success across various NLP tasks, yet grounded language understanding remains a challenge. Grounded language understanding involves mapping natural language expressions to executable plans or programs within specific environments, crucial for applications like smart assistants, semantic search, and domestic robots. The primary challenge lies in the lack of grounding during pre-training, as most models are trained on textual corpora without environmental interaction, leading to difficulties in generating valid and grammatical plans.

This paper introduces a novel framework, named Pangu, inspired by the Chinese mythological figure who separates heaven and earth, to address these challenges. Pangu shifts the focus from generation to discrimination, where a symbolic agent proposes candidate plans, and a language model scores and ranks these candidates. This approach alleviates the burden on language models to ensure the validity and grammar of generated plans.

The framework was tested on knowledge-based question answering, a representative scenario for grounded language understanding. Experiments with models like BERT, T5, and Codex, using both fine-tuning and in-context learning, demonstrated Pangu's superior performance and sample efficiency. Notably, Pangu achieved over 50% accuracy with Codex in in-context learning using a single demo example, outperforming other settings. Additionally, Pangu showed robustness under non-i.i.d. settings, maintaining consistent probability distributions for both seen and unseen structures, unlike autoregressive models that tend to overfit.

The key takeaway is that for grounded language understanding, discrimination may be more effective than generation. The authors invite discussions and collaborations to further explore this approach.</sample>
    <sample id="228">Gli autori hanno effettuato i test sui seguenti set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present their research on detecting improvable claims in argumentative writing, focusing on two tasks: Suboptimal-Claim Detection and Claim Improvement Suggestion. They explore how to determine if a claim is optimally phrased or requires revision, using implicit revision patterns from collaborative online debate platforms like Kialo. The study addresses challenges in using revision-based data, including representativity, model complexity, contextual dependencies, and biases. They analyze how different models and contextual information affect claim assessment and suggest that modeling the distance between claim versions aids in detecting suboptimal claims. The research highlights the importance of context and the impact of biases, offering insights into effective strategies for improving argumentative claims. Further details and findings are available in their paper.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dal web utilizzato per addestrare DrBERT, il primo modello biomedico in francese.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi addresses the challenges of Simultaneous Speech Translation (SimulST), which involves translating spoken language into text in another language in real time. Current SimulST models face issues such as complex architectures, lengthy training procedures, and the need for multiple models to achieve different latency levels. The authors propose a novel solution, EDAtt (Encoder-Decoder Attention), which leverages existing offline Speech Translation (ST) models without retraining or adopting specific architectures for SimulST. EDAtt uses a single model for each latency regime, managing latency through specific parameters and the attention mechanism. The strategy determines whether to emit partial translations based on cross-attention weights, emitting words when attention is not concentrated on the last few speech frames. This approach allows for stable information before emitting translations. The results show that EDAtt outperforms traditional strategies like Wait-k and Local Agreement, as well as state-of-the-art architectures tailored for SimulST, by achieving higher translation quality (BLEU scores) and lower latency (average lagging). Additionally, EDAtt is the fastest strategy when considering computational-aware average lagging. The authors have made their code and models open source to facilitate reproducibility.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati della traduzione. Un esperimento ha mostrato che la differenza tra due diversi prompt per la stessa frase può essere di più di un punto BLEURT, fino a 40 punti in casi estremi. La qualità degli esempi utilizzati nel prompting è più importante della loro somiglianza con la frase di origine. Inoltre, la forma del prompting ha un'influenza minore con il prompting multi-shot (come il 5-shot), mentre è cruciale per il zero e il one-shot prompting.</sample>
    <sample id="235">L'affiliazione degli autori non è specificata nel contenuto fornito.</sample>
    <sample id="236">Il contenuto non fornisce dettagli specifici sulle cinque istruzioni scritte da esperti. Menziona che ogni compito nel dataset MultiInstruct è dotato di cinque istruzioni scritte da esperti, ma non elenca o descrive queste istruzioni.</sample>
    <sample id="237">Gli autori propongono un test diagnostico chiamato KITMUS (Knowledge Integration from Multiple Sources) per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da diverse fonti. Questo include un compito di risoluzione dei coreferenti progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Il test varia la disponibilità di informazioni di background e specifiche dell'entità in tre diverse impostazioni: Background-Pretrain, Background-Both e Background-Inference.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset designed to aid in the development of meeting summarization technologies. MeetingBank addresses the challenges of creating high-quality meeting summaries and locating trustworthy public meeting resources by compiling a repository of City Council meetings. The dataset includes 1,366 meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and additional resources. Data collection involved using the Speechmatics API for transcription and extracting meeting details from City Council websites, such as the Boston City Council. The dataset provides statistics on meeting duration, tokens, speakers, and summarization instances across various cities. Analysis of summary abstraction uses coverage and density scores, revealing varying degrees of editing across cities. Model evaluation on MeetingBank includes extractive systems like Oracle and LexRank, and abstractive models such as BART-Large and GPT-3. While GPT-3 excelled in fluency and coherence in human assessments, it lagged in informativeness and factuality. The findings suggest a need for improved automatic evaluation metrics aligned with human preferences. MeetingBank serves as a valuable resource for researchers to develop advanced summarization tools and offers insights into City Council decision-making processes.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e darò una breve recensione del paper "Prompting PaLM for Translation: Assessing Strategies and Performance." Questo è un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un modello di linguaggio grande con 540 miliardi di parametri presentato l'anno scorso nel 2022. È addestrato su una vasta raccolta di testo, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto il livello di punta in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio sistematico più completo dell'uso del prompting di modelli di linguaggio grandi per la traduzione automatica. Abbiamo valutato la capacità di traduzione di tali modelli utilizzando le migliori pratiche della comunità di traduzione automatica. Ciò comporta l'uso degli ultimi set di test per evitare un sovrapporsi dei dati di test con i dati di addestramento del modello di linguaggio. E abbiamo confrontato con i sistemi di punta, cioè il miglior sistema, quindi la valutazione WMT. Utilizziamo metriche di traduzione automatica neurali di punta e, inoltre, mostriamo risultati di valutazione umana basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt. Il prompting ha un grande impatto sulle prestazioni dei LLM per la traduzione, come possiamo vedere in un semplice esperimento, dove abbiamo utilizzato un prompting one-shot e fornito due prompt diversi per ogni frase. La maggior parte delle frasi, 516 su 1.000, ha mostrato una differenza di più di un punto BLEURT. E questo può arrivare, nei casi estremi, fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting. Nei nostri esperimenti, abbiamo optato per un prompting 5-shot, dove abbiamo semplicemente contrassegnato ogni frase fornita al sistema con la lingua di appartenenza. Ad esempio, in questo caso, dove effettuiamo la traduzione dal tedesco all'inglese, le frasi sorgente tedesche sono contrassegnate con "tedesco:" e le traduzioni in inglese con "inglese:". Abbiamo visto che la forma effettiva del prompting non ha un grande impatto nel caso di diversi prompt brevi. È cruciale per il zero e il one-shot prompting. E quando passiamo, come nel nostro caso, al five-shot prompting, non c'è quasi differenza nella forma effettiva del prompting. Sono gli esempi che portano il peso principale. La sintesi dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase sorgente. Quindi, è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo i prompt selezionati dai dati di addestramento per le valutazioni WMT sui dati di sviluppo. I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. E i loro risultati mostrano una migliore performance quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati di punta hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma, PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate. Le intuizioni che abbiamo ottenuto dalla valutazione umana che abbiamo eseguito utilizzando il framework MQM dicono che la fluidità di PaLM è comparabile ai sistemi di punta, ma la principale differenza deriva dall'accuratezza. In particolare, gli errori più comuni sono gli errori di omissione. Quindi, sembra che PaLM scelga di produrre una traduzione più scorrevole, a volte eliminando parti della frase sorgente nella traduzione. Tuttavia, la categoria "Style/Awkward" per PaLM è inferiore rispetto ai sistemi di punta, che è un segnale aggiuntivo che PaLM fornisce un output davvero scorrevole, ma con alcuni problemi di accuratezza. E questo è tutto per questa breve panoramica. Per ulteriori dettagli, vi prego di partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, uno studente di dottorato presso l'Università del Saarland in Germania. In questo video, vorrei presentare il nostro recente lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento debole supervisionato. Nella supervisione debole, non si etichettano manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura deboli, come regole euristiche semplici, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Quando confrontate con le annotazioni umane, le annotazioni più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni è errata. Se addestriamo direttamente le reti neurali su dati debolmente etichettati, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. Nell'apprendimento debole supervisionato, vengono proposti algoritmi di addestramento per addestrare robustamente le reti neurali sotto tale rumore delle etichette in modo che i modelli addestrati generalizzino ancora bene. Nei lavori recenti in WSL, WSL sta per Weakly Supervised Learning, una affermazione comune è che le persone dicono di addestrare modelli solo su dati debolmente etichettati e di ottenere un alto rendimento su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è una trappola, che è che si assume che ci sia un set di validazione pulito disponibile per la selezione del modello. Non possiamo fermarci su questo setting del problema, ma ciò implica che sono necessarie annotazioni manuali aggiuntive nell'apprendimento debole supervisionato. Ma come un elefante nella stanza, questa necessità è spesso trascurata. Il dubbio menzionato solleva tre domande di ricerca. Primo, i dati di validazione puliti sono necessari per WSL o possiamo forse usare un set di validazione rumoroso invece? Secondo, se sono necessari dati puliti, o se i dati puliti sono obbligatori per far funzionare il WSL, allora quanti campioni puliti abbiamo bisogno? Infine, dovremmo usare solo i campioni puliti per la validazione, o ci sono modi migliori per utilizzarli? Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti. Primo, troviamo che, interessantemente, i metodi recenti di WSL richiedono effettivamente campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è un grande calo di prestazioni. Come mostrato in questa figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali, il che significa che l'addestramento è inutile. Ciò indica che gli approcci di WSL richiedono effettivamente dati etichettati puliti per funzionare correttamente, e il costo delle annotazioni per ottenere campioni di validazione puliti non dovrebbe essere trascurato. Il nostro secondo risultato è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci di WSL a ottenere un miglioramento delle prestazioni, come mostrato nella figura a sinistra. Tipicamente abbiamo bisogno solo di 20 campioni per classe per ottenere un alto rendimento. Ma non è la fine della storia, perché se decidiamo comunque di accedere ai campioni puliti, allora l'addestramento diretto su di essi otterrà un miglioramento delle prestazioni ancora maggiore. La figura a destra mostra la differenza di prestazioni tra approcci di fine-tuning, che vengono applicati direttamente sui dati puliti, e approcci di WSL, che usano i dati puliti solo per la validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, il fine-tuning diretto inizia a battere gli approcci di WSL. Infine, il miglioramento delle prestazioni affermato negli approcci di WSL precedenti può essere facilmente ottenuto consentendo di continuare il fine-tuning sui campioni di validazione puliti. Come possiamo vedere dalle figure, il modello vanilla, denominato FTw, inizialmente sottoperforma metodi di WSL più complessi, come COSINE. Tuttavia, se consentiamo di continuare il fine-tuning sui campioni puliti, allora FTw si comporta allo stesso modo degli altri metodi. Quindi nella pratica, non c'è motivo di scegliere metodi di WSL più complessi che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo mostrato che gli approcci di WSL recenti richiedono campioni puliti, manualmente annotati, per funzionare correttamente. Il loro guadagno di prestazioni e praticità sono fortemente sopravvalutati. Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti. Primo, riportare i criteri di selezione del modello. Ad esempio, riportare se la selezione del modello è stata effettuata tramite campioni di validazione puliti. Secondo, gli approcci di WSL dovrebbero essere confrontati con basi di apprendimento a pochi esempi, poiché entrambi lavorano su campioni puliti. Terzo, il fine-tuning continuo è un semplice ma forte baseline che dovrebbe essere considerato nel lavoro futuro in WSL. Infine, abbiamo reso open source il nostro codice. Puoi trovarlo tramite il codice QR su questa diapositiva. Sentiti libero di controllarlo. Grazie e buon divertimento alla conferenza.</sample>
    <sample id="241">Il paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" affronta le sfide nell'identificazione precoce delle informazioni errate sui social media, evidenziando le carenze delle attuali metodologie automatiche. Queste metodologie spesso utilizzano dataset retrospettivi e non riescono a gestire efficacemente le contromisure che emergono solo dopo la smentita pubblica delle affermazioni. Inoltre, non considerano adeguatamente la scala e il rumore delle piattaforme, minimizzando il ruolo degli esseri umani nel processo di rilevamento.

Il paper propone un framework di valutazione che integra il feedback umano in vari stadi del processo, andando dai tweet grezzi a output utilizzabili. Il sistema è composto da due componenti principali: la rilevazione di affermazioni fuorvianti e la verifica delle violazioni delle politiche. La prima componente utilizza un modello T5 per l'estrazione di affermazioni e un test di Fisher per il ranking, mentre la seconda componente impiega un modello BERT per la classificazione dello stile. L'obiettivo è rilevare trattamenti non approvati prima della loro smentita pubblica e verificare le violazioni delle politiche di Twitter.

L'evaluazione mostra che il sistema può rilevare il 65% delle violazioni delle politiche e conferma 124,2 violazioni per ora lavorativa umana. Il framework proposto mira a catturare realisticamente l'interazione tra sistemi e moderatori umani, promuovendo lo sviluppo di sistemi di rilevamento delle informazioni errate con un coinvolgimento umano più integrato.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di valutazioni umane, come chiedere ai giudici umani di selezionare quale di due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci forniscono valutazioni olistiche della qualità del dialogo. Inoltre, le valutazioni esistenti includono valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a coppie a livello di dialogo.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie includono "I giudici decidono casi nei tribunali."</sample>
    <sample id="245">Il lavoro "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization" di Lining Zhang e colleghi presenta un metodo per identificare lavoratori di alta qualità su Amazon Mechanical Turk (MTurk) per compiti di riassunto. Il metodo affronta le limitazioni delle metriche automatiche e delle pratiche di reclutamento su MTurk attraverso un pipeline a due fasi. La prima fase, il "Qualification Task", valuta la capacità dei lavoratori di valutare correttamente diverse dimensioni, classificandoli in categorie: oro, argento, bronzo e bloccati. Solo i lavoratori oro e argento passano, risultando in 26 lavoratori qualificati (8 oro, 18 argento). La seconda fase, l'"Endurance Task", testa la capacità di gestire un carico di lavoro elevato, riducendo ulteriormente il gruppo a 12 lavoratori (4 oro, 8 argento). Questi lavoratori mostrano un alto accordo inter-annotatore (IAA) rispetto agli esperti, con un Krippendorff's Alpha di 0.443. Il "Reference-based Task" valuta le prestazioni generali, con un Krippendorff's Alpha di 0.534. Il confronto con i lavoratori MTurk di base e CloudResearch mostra che il pipeline offre qualità simile a CloudResearch a un costo inferiore. L'analisi della correttezza tra diverse fonti di annotazione rivela una forte correlazione tra i lavoratori del pipeline e CloudResearch, ma non garantisce la formazione sulla correttezza. In sintesi, il pipeline consente di ottenere un alto accordo a un costo inferiore, evitando sprechi di risorse. Le limitazioni includono il test solo in inglese, la mancanza di soluzioni universali e l'assenza di garanzie sulla formazione della correttezza. Il lavoro è supportato da Google e suggerisce future ricerche su diversi compiti, lingue e piattaforme.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Il paper "FACTKG: Fact Verification via Reasoning on Knowledge Graphs" di Jiho Kim e colleghi introduce un nuovo compito di verifica dei fatti basato su grafi di conoscenza (KG). Mentre i dataset esistenti come FEVER e VitaminC utilizzano testo di Wikipedia e TabFact e InfoTabs utilizzano tabelle come prove, non esisteva un dataset che utilizzasse KG come evidenza per le affermazioni in linguaggio naturale. Il paper propone il compito di Verifica dei Fatti Basata su KG, sottolineando che i KG offrono un'opportunità per una verifica dei fatti più affidabile e intuitiva rispetto ai metodi basati su testo o tabelle. I KG consentono un collegamento diretto tra le affermazioni e le prove, facilitando la verifica e l'uso pratico, come la verifica della coerenza nelle moderne sistemi di dialogo.

Per affrontare questa lacuna, gli autori introducono il dataset FactKG, che utilizza DBpedia come KG e include affermazioni in stili scritti e colloquiali. Il dataset è etichettato con le etichette SUPPORTED e REFUTED e include cinque tipi di ragionamento: one-hop, conjunction, existence, multi-hop e negation. Gli autori hanno sviluppato basi di confronto che utilizzano solo le affermazioni e il modello GEAR che utilizza prove corrette dai KG. I risultati mostrano che il modello GEAR supera tutte le altre basi di confronto, dimostrando l'efficacia della verifica dei fatti basata su KG. Il paper invita alla sperimentazione con il dataset FactKG, che è disponibile per il download.</sample>
    <sample id="248">Gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. Hanno raccolto oltre 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi, ma non specificano un bilanciamento per ciascun gruppo demografico.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore alla frase di input, cercando di preservare la struttura rilevante. Queste perturbazioni hanno mostrato un aumento simile nei giudizi MPP, indicando che i modelli sono sensibili a tali modifiche in modo coerente.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare più aspetti o dimensioni della qualità della conversazione per comprendere meglio le forze e le debolezze di un modello di dialogo su un livello più dettagliato. Questo approccio consente di valutare specifici comportamenti del modello, come la rilevanza delle risposte, le contraddizioni e la presenza di empatia, piuttosto che fare affidamento su valutazioni globali della qualità della conversazione.</sample>
    <sample id="251">University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion," a collaborative work by Sai Kiran Tanikella, Abhinav Joshi, Akshat Sharma, and Ashutosh Modi, aimed at addressing the challenge of Prior Case Retrieval (PCR) in the legal domain. As the volume of legal cases increases, it becomes difficult for legal professionals to manually identify relevant precedents. U-CREAT proposes a novel approach using unsupervised learning and event extraction to improve PCR efficiency.

The work introduces two key contributions: the IL-PCR dataset and the U-CREAT pipeline. The IL-PCR dataset, a new benchmark for PCR tasks, comprises 7,070 Indian legal cases with an average of 6.775 citations per document. It offers a comprehensive test bed for evaluating PCR algorithms, featuring longer documents, a larger vocabulary, and more citations compared to existing datasets like COLIEE’21.

The U-CREAT pipeline leverages event extraction to represent case documents as collections of events, using dependency parsing to form subject-verb-object triplets. This approach allows for efficient retrieval by computing an interaction matrix between query and candidate events, highlighting common events and ranking candidates accordingly.

Experiments with diverse models, including count-based, transformer-based, and event-based models, demonstrate that event-based models, particularly the Event Filtered Documents model, significantly outperform baseline methods like BM25. The U-CREAT pipeline shows superior performance on both Indian and Canadian legal datasets, achieving state-of-the-art results on the COLIEE’21 document retrieval task. This work highlights the potential of event-based approaches in legal document retrieval, paving the way for further advancements in the field.</sample>
    <sample id="253">Mario Ezra Aragón presents "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media," a collaborative effort between researchers from Mexico and Spain. The study focuses on identifying mental health disorders through social media posts, leveraging domain adaptation to enhance model performance due to limited annotated data. DisorBERT builds on BERT, adapting it to the specific language of Reddit and mental health by integrating domain-specific knowledge and guided masking. This approach helps the model focus on significant words, improving its ability to detect mental health issues. The model's effectiveness is demonstrated using the eRisk datasets, showing a balanced precision and recall compared to other methods. DisorBERT's predictions, particularly on sentences from Beck's Depression Inventory, reveal a tendency to generate words associated with mental disorders, unlike BERT's more general outputs. Visualization tools highlight key words and sentences in user posts, such as "anxious" and "medication," relevant to depression. The study concludes that double domain adaptation and guided masking effectively capture mental disorder signs in social media, outperforming MentalBERT. Future work aims to explore additional lexical resources and clinical data.</sample>
    <sample id="254">This paper presents a novel framework for document-level distant relation extraction (DocRE) that addresses the challenge of noise in distantly supervised data through uncertainty-guided label denoising. Traditional methods rely on large-scale human-annotated corpora, which are labor-intensive, while recent approaches use distantly supervised data, which introduces noise due to false-positive pseudo labels. The proposed framework improves label quality by integrating uncertainty estimation to assess the trustworthiness of model predictions. A pre-denoising DocRE model is trained using both distantly supervised and human-annotated data to generate pseudo labels. To handle the inevitable false positives, the framework employs Monte Carlo dropout for uncertainty estimation, capturing uncertainty scores for overlapping relations. An instance-level uncertainty estimation method is introduced to differentiate between multiple relations between entity pairs. Dynamic class uncertainty thresholds are proposed to filter out high-uncertainty pseudo labels, replacing original labels with those having lower uncertainty scores. A multi-phase training strategy iteratively refines the DS data, enhancing the DocRE model's performance. The framework outperforms existing baselines on public datasets, demonstrating significant improvements in label quality and model performance. Key contributions include the uncertainty-guided label denoising framework, instance-level uncertainty estimation for overlapping relations, iterative re-labeling with dynamic thresholds, and overall performance enhancements.</sample>
    <sample id="255">La forma del prompting si rivela importante nei casi di zero e uno-shot prompting. Quando si utilizza un prompting a cinque-shot, la forma del prompting non ha un grande impatto.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">In questo video, Chiang Cheng-Han presenta il lavoro "Can Large Language Models Be an Alternative to Human Evaluation?" che esplora l'uso di modelli linguistici di grandi dimensioni (LLM) per valutare la qualità del testo in elaborazione del linguaggio naturale (NLP). Il lavoro propone di utilizzare LLM per valutare campioni di testo fornendo loro istruzioni specifiche. Sebbene l'uso di LLM per la valutazione non sia del tutto nuovo, il lavoro è stato considerato innovativo al momento della presentazione alla conferenza ACL, poiché non esistevano precedenti studi che esplorassero questa idea.

Il lavoro si pone l'obiettivo di trovare un'alternativa alla valutazione umana, che è spesso instabile e difficile da riprodurre. Gli autori propongono di utilizzare LLM per valutare storie generate da GPT-2 o scritte da umani, basandosi su attributi come grammatica, coerenza, gradimento e rilevanza. Per verificare l'efficacia di questa valutazione, confrontano i risultati dei LLM con quelli di valutatori umani, in questo caso insegnanti di inglese, che valutano le stesse storie.

L'esperimento utilizza quattro diversi LLM: T0, InstructGPT (Curie e Davinci) e ChatGPT. I risultati mostrano che, sebbene alcuni LLM più piccoli non mostrino una preferenza significativa per le storie scritte da umani, i modelli più grandi, come Davinci e ChatGPT, dimostrano una chiara preferenza per il testo umano, simile agli insegnanti. Questo suggerisce che alcuni LLM possono effettivamente fungere da alternativa alla valutazione umana. Il video conclude invitando gli interessati a leggere il paper per ulteriori dettagli e risposte a domande specifiche.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The work addresses the challenge of translating user queries from multiple natural languages into various meaning representations like SQL, Lambda Calculus, and FunQL. Existing models are limited in scope, often missing languages like Chinese and certain meaning representations. XSemPLR introduces a comprehensive dataset with 9 datasets, 5 tasks, 8 meaning representations, and 22 languages across 15 families. The study evaluates models under six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, Cross-lingual Zero-shot, and Few-shot transfer. Results show Encoder-Decoder models outperform others, with performance gains in most languages except English, highlighting the "Curse of Multilinguality." Cross-lingual transfer shows significant performance gaps, which are reduced with Few-shot settings. Pretraining on English boosts Few-shot performance in target languages. The study finds multilingual models like Codex and BLOOM inadequate for cross-lingual tasks. XSemPLR provides a unified benchmark, revealing insights into multilingual language models for semantic parsing.</sample>
    <sample id="260">L'articolo menziona un solo autore, Jingwei Yi.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle specifiche restrizioni imposte dai vari obiettivi.</sample>
    <sample id="262">L'articolo non specifica il numero di autori coinvolti.</sample>
    <sample id="263">This work addresses the instability in in-context learning for large language models, focusing on label biases that affect model predictions. Prior research highlights the impact of design choices, such as the selection and order of in-context examples, on search instability and bias introduction. This study systematically categorizes label biases in text classification tasks, identifying a new type: domain-label bias. This bias arises from the influence of task corpus on model predictions. Experiments demonstrate that random in-domain words can significantly bias predictions, unlike random English words. The study introduces domain-context calibration, a novel method that uses random in-domain words to estimate and mitigate label biases, including vanilla-label, context-label, and domain-label biases. This method improves in-context learning performance, particularly in tasks with high domain-label bias. The effectiveness of domain-context calibration is validated across various models and datasets, showing enhanced decision boundaries and performance, especially for larger models like GPT-3. The findings underscore the importance of addressing domain-label bias to enhance the reliability of in-context learning.</sample>
    <sample id="264">Il paper "TAVT: Towards Transferable Audio-Visual Text Generation" di Lin Wang presenta un nuovo approccio per affrontare le sfide della generazione di testo multimodale, in particolare per l'audio-visual text generation. Mentre i compiti di generazione di testo unimodale come la traduzione automatica e la generazione di didascalie per immagini hanno prosperato grazie al pre-addestramento su larga scala e alla capacità dei modelli, la generazione di testo multimodale affronta difficoltà significative a causa della complessità e del costo dell'annotazione dei dati. Inoltre, le condizioni di costruzione variabili tra diversi domini portano a un degrado delle prestazioni nei lavori esistenti.

Per superare queste limitazioni, il paper propone il compito di Transferable Audio-Visual Text Generation, che affronta le variazioni di dominio multimodale come lo stile visivo e l'energia audio. Si osserva che, mentre il contenuto visivo può cambiare significativamente con lo stile e l'angolo di ripresa, il contenuto audio come il ritmo e l'energia ha un impatto minore sulla comprensione degli eventi. Basandosi su queste osservazioni, il paper propone di utilizzare uno spazio semantico audio unificato per allineare i concetti visivi tra diversi domini.

Il framework proposto è modulare e include tre componenti principali: un audio-visual meta-mapper network, un audio-visual encoder e un generatore di modelli linguistici, e un apprendimento contrastivo controcettuale. Il meta-mapper network mappa i concetti visivi in uno spazio semantico audio unificato, utilizzando token apprendibili per migliorare l'allineamento semantico. Il secondo componente utilizza un encoder e un generatore basati su transformer, con un parametro α per valutare il contributo di diverse modalità. Infine, il paper introduce un apprendimento contrastivo controcettuale a doppio livello (DCLL) per ottimizzare direttamente l'allineamento visivo-testuale.

Il paper valuta il framework su due benchmark basati su MSVD e MSR-VTT, dimostrando che il metodo proposto supera significativamente i modelli di riferimento sia nei set di dati incrociati che nei set di domini incrociati, specialmente in domini a bassa risorsa. Questo lavoro rappresenta un passo avanti nell'indagine sulla generazione di testo audio-visivo trasferibile.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">L'affiliazione di Adam Przepiórkowski è l'Università di Łódź.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di omissione, dove PaLM sceglie di produrre una traduzione più scorrevole a volte omettendo parti della frase originale.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare l'IA conversazionale. Questo lavoro è stato realizzato dal Laboratorio di NLP di Emory, guidato dal Professor Jinho Choi presso l'Università di Emory, in collaborazione con Amazon Alexa AI. Immaginiamo di aver sviluppato un modello di dialogo e vogliamo vedere come si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di selezionare quale dei due dialoghi è migliore o di valutare i dialoghi su una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Pertanto, potresti voler valutare più dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a un livello più dettagliato. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi comparativi esistenti o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come rispondere con informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio annotare i comportamenti nel chat, o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti del modello di chat che sono stati suggeriti di influenzare la qualità del chat nella recente letteratura. ABC-Eval è in grado di misurare le frequenze con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante, si contraddice o il suo partner, fa allucinazioni di fatti errati o viola la conoscenza del senso comune, e quando il modello riesce o fallisce a mostrare empatia. Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. A scopo di confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che i label di comportamento di ABC-Eval sono complessivamente più affidabili rispetto ai label raccolti con i metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni doppio-etichettate. Inoltre, i label di ABC-Eval sono più predittivi della qualità complessiva del dialogo rispetto ai metriche prodotte dai metodi esistenti, come mostrato da questa semplice analisi di regressione lineare. Ad esempio, si può vedere come misurare la proporzione di turni con contraddizioni di sé e del partner spieghi il 5% e il 10% della qualità del dialogo, rispettivamente, mentre i punteggi di coerenza Likert medi spiegano solo il 4% o meno. Infine, abbiamo controllato se ogni metrica di valutazione catturasse un aspetto unico della qualità del chat utilizzando una regressione lineare passo-passo. Si può vedere come la combinazione di tutte le metriche di ABC-Eval spieghi oltre il 25% della qualità del dialogo, e quando si rimuovono le metriche una alla volta, la maggior parte di esse comporta la perdita di una quantità decente di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche. Queste metriche affidabili, informative e distinte di ABC-Eval ci permettono di valutare l'IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Si può vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del senso comune nel 20% delle loro risposte. Producono informazioni irrilevanti nel 15% delle risposte e si contraddicono o il loro partner circa il 10% del tempo. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando la nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E non vediamo l'ora di vedere come l'IA conversazionale si evolverà nei prossimi mesi e anni. Grazie per aver guardato.</sample>
    <sample id="270">Gli autori dell'articolo sono affiliati al Emory NLP Lab, guidato dal Professor Jinho Choi presso Emory University, e hanno collaborato con Amazon Alexa AI.</sample>
    <sample id="271">In questo articolo, CFT significa "Continuous Fine-Tuning."</sample>
    <sample id="272">Sette autori sono coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "When Does Translation Require Context? A Data-driven, Multilingual Exploration". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Se la frase precedente era "Le cose potrebbero diventare pericolose se i ministri lo scoprissero", "mole" si riferisce a un agente segreto. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", "mole" si riferisce a una macchia di nascita. A seconda del contesto, il significato della parola cambia e quindi anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre casi come questo è piuttosto difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per metriche a livello di corpus come BLEU catturare queste traduzioni. Alcuni hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue, poiché di solito si basano su conoscenze di dominio e cura umana. In questo lavoro, cerchiamo di rispondere a due domande. Prima di tutto, quando la traduzione richiede contesto? E in secondo luogo, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. In lavori precedenti, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica. Questo viene fatto misurando quanto informazione il contesto C fornisce sulla destinazione Y, dato la fonte X. Possiamo pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o di parola. Possiamo pensare alle parole che hanno un alto P-CXMI come quelle che richiedono contesto per la traduzione. Analizziamo quindi le parole con alto P-CXMI per cercare schemi tra queste parole. E conduciamo la nostra analisi sui trascritti di TED Talks tradotti dall'inglese in 14 diverse lingue. Conducono l'analisi a tre diversi livelli. In primo luogo, guardiamo alle etichette di parte del discorso che hanno un alto P-CXMI medio. Questo ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno un P-CXMI relativamente alto. Questo può essere spiegato dal fatto che l'inglese non ha pronomi duali, quindi è necessario il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che alcune lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo agli elementi lessicali che hanno un alto P-CXMI mediato su tutte le sue diverse occorrenze. Questo ci aiuta a identificare casi come questo, in cui in cinese è necessario il contesto per tradurre i nomi propri per assicurarsi di utilizzare la stessa traduzione all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre nel giusto grado di formalità. Infine, guardiamo a diversi token individuali che hanno un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere catturati realmente dalla parola stessa, ma che sono espressi nella struttura della frase, come la risoluzione delle ellissi. Ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni del discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che riguardano il fenomeno. E abbiamo chiamato il nostro tagger il Multilingual Discourse-Aware, o MuDA tagger. Possiamo quindi notare che diverse lingue hanno proporzioni diverse di questi fenomeni del discorso. Poi utilizziamo il MuDA tagger applicandolo a un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sugli esempi dipendenti dal contesto che il MuDA tagger ha identificato. Infine, utilizziamo il nostro benchmark, insieme ad altre metriche, per valutare diversi modelli di traduzione automatica a livello di documento. In primo luogo, quando utilizziamo metriche a livello di corpus: per BLEU, troviamo che i modelli agnostici al contesto hanno le migliori prestazioni. Ma poi, se utilizziamo COMET, i modelli consapevoli del contesto si comportano meglio. E se utilizziamo la misura f di parola, i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione automatica a livello di documento se utilizziamo solo metriche a livello di corpus. Poi utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per certi fenomeni del discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non utilizzano il contesto su altri fenomeni come ellissi, pronomi e forma verbale. Quindi questo suggerisce dove dovremmo vedere più progressi per la traduzione automatica a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, conduciamo un'analisi guidata dai dati su 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che può aiutarci a identificare quali fenomeni del discorso i modelli gestiscono bene o meno, e quali sistemi di traduzione sono bravi nella traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Questo lavoro presenta "IndicMT Eval," un dataset progettato per la meta-valutazione delle metriche di traduzione automatica (MT) per le lingue indiane, concentrandosi sulle traduzioni non verso l'inglese. Mentre esistono metriche per valutare le traduzioni verso l'inglese, le traduzioni in altre direzioni sono meno studiate, nonostante le differenze linguistiche significative. Lo studio esamina cinque lingue indiane: Tamil e Malayalam (Dravidiano) e Hindi, Marathi, e Gujarati (Indo-Ariano). Utilizzando il dataset Flores, 200 frasi sono state tradotte in inglese da sette modelli di traduzione diversi, producendo 7.000 campioni. Bilingui esperti hanno annotato questi campioni, identificando errori e fornendo punteggi complessivi, utilizzando il framework MQM per classificare gli errori in categorie come accuratezza, fluidità e errori speciali.

I risultati mostrano che i modelli MT recenti come NLLB e Indic Trans hanno meno errori rispetto ai modelli più vecchi. Tra le metriche, chrF ha la correlazione più alta tra le metriche basate su sovrapposizione, ma queste sono le peggiori in generale. Le metriche basate su embedding come LabSE e BERTscore mostrano migliori correlazioni, con COMET che supera le altre in termini di correlazione complessiva. Tuttavia, molte metriche mostrano una distribuzione di punteggi distorta, rendendo difficile l'interpretazione. Analizzando i dati MQM, i modelli COMET sono stati affinati e hanno mostrato prestazioni migliori rispetto ai baselines COMET, specialmente in un contesto zero-shot. L'IndicCOMET MQM ha dimostrato una maggiore robustezza rispetto al COMET standard, con una correlazione di 0.36 rispetto a 0.272. Il dataset è disponibile pubblicamente per ulteriori ricerche.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico menzionato nel contenuto.</sample>
    <sample id="278">Il metodo delle "parole contrassegnate" si basa sul concetto sociolinguistico di "contrassegnatezza", che afferma che esiste un gruppo non contrassegnato predefinito e qualsiasi gruppo che differisce da questo è linguisticamente contrassegnato. Il metodo designa prima i gruppi contrassegnati e non contrassegnati e poi confronta i personaggi utilizzando il metodo "Fightin’ Words", che utilizza rapporti log-odds pesati per distinguere le parole principali per ciascun gruppo contrassegnato. Ad esempio, per i personaggi di donne nere, il metodo confronta i rapporti log-odds contro i personaggi bianchi e maschili, che sono i due gruppi non contrassegnati corrispondenti.</sample>
    <sample id="279">L'affiliazione dell'autore menzionato è l'Università di Washington.</sample>
    <sample id="280">In this work, Shi Tao introduces "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations," addressing the challenges in Emotion Recognition in Conversations (ERC). The goal of ERC is to predict the emotion label of each utterance in a dialogue, utilizing textual, audio, and visual modalities. Existing methods often inadequately exploit multimodal information, struggle with minority emotion classes, and face difficulties distinguishing semantically similar emotions. To overcome these challenges, MultiEMO is proposed, featuring four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification.

The first contribution is VisExtNet, a novel visual feature extractor that captures facial expressions without encoding redundant scene-related information, addressing the irrelevance of visual surroundings. The second contribution is MultiAttn, a multimodal fusion model using bidirectional multi-head cross-attention layers to integrate information from different modalities effectively. MultiAttn-text, for instance, learns cross-modal correlations between textual and audio modalities, and then fuses these with visual cues.

The third contribution is the Sample-Weighted Focal Contrastive Loss, designed to prioritize hard-to-classify minority classes and enhance the distinction between semantically similar emotions. Extensive experiments on MELD and IEMOCAP datasets demonstrate that MultiEMO achieves state-of-the-art performance, particularly improving results in minority and semantically similar emotions. Despite its success, limitations include the inability of VisExtNet to distinguish between speakers and irrelevant people, the requirement of a large batch size for SWFC loss on MELD, and suboptimal performance in minority emotions compared to majority classes.</sample>
    <sample id="281">Il lavoro "When Does Translation Require Context? A Data-driven, Multilingual Exploration" esplora la necessità di contesto nella traduzione, evidenziando come il significato di parole come "mole" possa variare a seconda del contesto. L'analisi si concentra su due domande principali: quando la traduzione richiede contesto e come i modelli gestiscono tali casi. Utilizzando CXMI (Contextual Mutual Information) e la sua estensione Pointwise CXMI, lo studio misura l'importanza del contesto nella traduzione a livello di parola e di frase. L'analisi è stata condotta su trascrizioni di TED Talks tradotte in 14 lingue, identificando fenomeni linguistici che richiedono contesto, come pronomi duali in arabo e scelte di formalità in cinese.

Per valutare le capacità di traduzione a livello di documento, è stato sviluppato il MuDA tagger, che identifica automaticamente parole legate a fenomeni discorsivi. I risultati mostrano che i modelli consapevoli del contesto superano quelli agnostici in alcune aree, come formalità e coesione lessicale, ma non in altre, come ellissi e forme verbali. L'analisi dei sistemi commerciali rivela che DeepL è generalmente più accurato di Google Translate per la traduzione a livello di documento. Questo studio fornisce una base per migliorare la traduzione a livello di documento, evidenziando le aree che richiedono ulteriori progressi.</sample>
    <sample id="282">In "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing," Xuekai Zhu introduces a novel approach to non-parallel text style transfer at the story level, focusing on discourse-level imitation of author styles. Traditional studies have concentrated on token or sentence-level transfers, such as sentiment or formality, but this work advances to story-level transfers, addressing the challenge of replicating complex author linguistic preferences, including discourse structures and narrative techniques. The primary challenge is transferring style-specific content across different styles, as these are often closely tied to specific topics.

To tackle these challenges, the authors propose StoryTrans, a model that learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. A new training objective is designed to minimize stylistic features in discourse representations, bringing representations from different texts closer in latent space, while enhancing content preservation through a two-stage generation process. The first stage involves transferring source text with masked style-specific content keywords, followed by a second stage that incorporates these keywords to complete the text.

The training framework is divided into two stages: the first employs an advisory framework with self-reconstruction, disentanglement, sentence order, and style classifier losses to separate style and content. The second stage focuses on filling in style-specific content and removing mask tokens. The model was evaluated using new datasets in Chinese and English, demonstrating superior performance in style control and content preservation compared to strong baselines. Both automatic and manual evaluations confirm StoryTrans's effectiveness, with style visualization showing alignment with golden texts in style feature space. The model successfully enriches storylines and maintains source semantics, outperforming StyleLM in generating coherent and stylistically accurate texts.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è la "Prague approach".</sample>
    <sample id="284">In this paper, Peng Tianshuo from Wuhan University introduces "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction," presented at ACL's Main Conference. The paper addresses the limitations of current span-based Universal Information Extraction (UIE) models, which rely heavily on precise span boundaries, often leading to ambiguity due to multiple reasonable annotation spans. To tackle this, the authors propose a fuzzy span mechanism that allows for a continuous distribution of correct probabilities within a specific range, defined by R-min and R-max, representing the fuzzy boundaries of the span. This approach involves converting the continuous boundary distribution into discrete values for calculating fuzzy span loss, incorporating Binary Cross Entropy (BCE) loss and KL-divergence to align predicted boundaries with fuzzy span boundaries and supplementary information.

The paper introduces a fuzzy span attention mechanism as a mask function to dynamically adjust the attention span, using an optimizable parameter delta to modify the full attention range and linearly decay attention distribution at the boundaries. This mechanism is integrated into the top layer of the model to guide decision-making without affecting text encoding. The FSUIE model demonstrates significant improvements in named entity recognition, relationship extraction, and aspect sentiment triplet extraction tasks, achieving state-of-the-art results on datasets like ACE2004, 2005, ADE, and AST-V2. The ablation study confirms that the fuzzy span loss (FSL) and fuzzy span attention (FSA) enhance convergence speed and information extraction capability. Visualization of the attention distribution shows the model's focus on semantic information within a limited range, validating the effectiveness of the proposed fuzzy span mechanism. Overall, FSUIE offers a robust solution for universal information extraction by reducing reliance on precise span boundaries and improving model adaptability and performance across various tasks.</sample>
    <sample id="285">Il lavoro di Mingqi Gao e colleghi, "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework," affronta la sfida delle inesattezze fattuali nei riassunti generati da modelli, specialmente nel contesto della riassunzione di dialoghi. Il team propone due approcci principali per migliorare la fedeltà dei riassunti: l'introduzione di obiettivi di factuality durante la formazione o l'inferenza e lo sviluppo di un modello di Correzione degli Errori Fattuali (FEC) indipendente. Tuttavia, evidenziano che le attuali metodologie di valutazione degli FEC, che si basano su metriche di factuality come FactCC e DAE, presentano limitazioni significative. Queste metriche forniscono punteggi complessivi che possono essere poco affidabili e confondere le due soluzioni proposte, poiché un FEC potrebbe generare un riassunto completamente nuovo piuttosto che correggere quello originale.

Per affrontare queste problematiche, gli autori propongono l'introduzione di correzioni manuali annotate come riferimento, che migliorano sia la formazione che la valutazione degli FEC. Questo approccio consente una valutazione più accurata e completa, rispettando i requisiti di base della correzione degli errori fattuali: minimizzare le operazioni di sostituzione, inserimento e cancellazione per ottenere un riassunto fluente e non ridondante. Inoltre, introducono una nuova tassonomia per classificare gli errori fattuali in categorie basate sul contenuto e sulla forma, utilizzando un framework di valutazione basato su ERRANT, un metrico per la correzione degli errori grammaticali.

Attraverso esperimenti con vari modelli FEC, gli autori scoprono che l'uso di riassunti di riferimento da dataset di riassunzione di dialoghi migliora i risultati rispetto alle metriche di factuality non affidabili. Sottolineano la necessità di cambiare i metodi di valutazione degli FEC e evidenziano che l'introduzione di riassunti corretti manualmente durante la formazione migliora le prestazioni. Inoltre, combinare dati annotati manualmente con dati sintetici appare promettente. Tuttavia, i modelli FEC attuali mostrano difficoltà nel correggere errori come aggiunte e non riescono a gestire errori di attributo, modalità e collegamento.</sample>
    <sample id="286">James Finch e Sarah Finch.</sample>
    <sample id="287">Quattro autori sono coinvolti nell'articolo: Javad Hosseini, Filip Radlinski, Silvia Pareti, e Annie Louis.</sample>
    <sample id="288">Gli insiemi di dati che possono essere utilizzati per testare i fenomeni sintattici includono BLiMP, SyntaxGym e CrowS pairs.</sample>
    <sample id="290">Il contenuto non fornisce abbreviazioni specifiche per i cinque metodi per la prima domanda di ricerca. Menziona solo un metodo, "FTw", e un altro, "COSINE", in un contesto diverso.</sample>
    <sample id="291">Il modello viene valutato su 11 attività downstream biomediche e cliniche in francese, tra cui il riconoscimento delle entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering.</sample>
    <sample id="294">CamemBERT è inizialmente addestrato su dati di OSCAR, con versioni specifiche menzionate come CamemBERT OSCAR 138 GB e CamemBERT OSCAR 4 GB.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Valerio Basile presents a collaborative project between the University of Turin and Amazon Alexa focusing on irony detection in natural language processing (NLP). The project challenges the traditional assumption of a single "ground truth" in data annotation, emphasizing the complexity of irony as a pragmatic phenomenon. To address this, the team developed the English Perspectivist Irony Corpus (EPIC), comprising 300 short conversations from social media platforms like Reddit and Twitter, collected over 1.5 years. The corpus includes five English varieties, annotated by 74 individuals via Prolific, with each annotator reviewing 200 conversations. The annotation process involved a simple interface asking if replies were ironic, yielding an average of five annotations per conversation.

The study revealed significant inter-annotator agreement variations across dimensions such as gender, age, and nationality. To model these differences, perspective-aware models were developed by fine-tuning pre-trained language models on dataset splits based on annotator characteristics. Although raw performance showed no clear trends, perspective-aware models demonstrated higher confidence in predictions compared to gold standard aggregated models. Further analysis indicated that generational and geographical proximity influenced annotation discrepancies, particularly between annotators from the UK and Ireland. This research highlights the importance of considering diverse perspectives in NLP model training, especially for nuanced tasks like irony detection.</sample>
    <sample id="297">The paper "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" explores the concept of dogwhistles—terms that convey different messages to in-groups and out-groups, often used to communicate controversial or taboo ideas while maintaining plausible deniability. The study highlights the importance of understanding dogwhistles in natural language processing (NLP) and linguistics due to their context-dependent meanings and their role in political influence and online content moderation evasion.

The authors develop a typology and glossary of over 340 dogwhistles, focusing on racist, transphobic, and anti-Semitic terms, primarily in the U.S. context. They categorize dogwhistles by register (formal or informal), type (additional implicature or covert signaling), and persona (e.g., anti-Semitic, transphobic). A case study of historical U.S. political speeches reveals a correlation between the use of racial dogwhistles and the Republican Southern Strategy post-Civil Rights era, showing an increase in dogwhistle usage aligned with conservative politics.

The paper evaluates the ability of language models, specifically GPT-3, to identify and surface dogwhistles. While GPT-3 performs well with formal register dogwhistles, it struggles with informal and transphobic terms. The study also examines how dogwhistles can evade automated toxicity detection, demonstrating that sentences containing dogwhistles are often rated as less toxic compared to those with explicit slurs.

Overall, the research underscores the challenges in detecting dogwhistles and their potential to bypass content moderation, emphasizing the need for improved NLP techniques to address this issue.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni includono:

1. L'osservazione che non c'era un effetto di rendimenti decrescenti, indicando che l'adattamento eccessivo non era un problema, poiché ogni unità di miglioramento su CoNLL-2003 si traduceva in più di un'unità di miglioramento su CoNLL++.

2. L'esperimento che ha mostrato che la ritenzione o la continuazione della pre-addestramento di alcuni modelli con dati più recenti ha portato a una degradazione delle prestazioni con un maggiore divario temporale, confermando che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="299">Il lavoro di Michalis Korakakis e Andreas Vlachos introduce un metodo di addestramento minimax per migliorare la robustezza dei modelli di Inferenza di Natural Language (NLI) riducendo la dipendenza da scorciatoie. Nonostante i modelli NLI raggiungano risultati di stato dell'arte, spesso si affidano a scorciatoie, come la sovrapposizione di parole, che compromettono le prestazioni su set di test fuori distribuzione. I metodi esistenti di mitigazione delle scorciatoie richiedono modelli ausiliari che possono non allinearsi con il comportamento del modello principale, limitando l'efficacia. Il metodo proposto si concentra su esempi "difficili" sotto-rappresentati che contrastano le scorciatoie presenti negli esempi "facili" dominanti. Utilizzando un obiettivo di addestramento minimax, il modello principale minimizza la perdita del compito NLI, mentre il modello ausiliario, un semplice rete feed-forward, massimizza la perdita del principale generando pesi per gli esempi che incentivano l'apprendimento da esempi difficili. Questo approccio non fa assunzioni sui tipi di scorciatoie presenti nel dataset e migliora le prestazioni fuori distribuzione mantenendo l'accuratezza in distribuzione. Valutato su dataset MNLI, FEVER, QQP e set di test avversari, il metodo mostra miglioramenti consistenti rispetto ai modelli di addestramento standard e ai metodi di mitigazione delle scorciatoie. L'effetto dell'addestramento pregresso del principale, la dimensione del modello ausiliario e una valutazione qualitativa dei pesi degli esempi sono ulteriormente esplorati.</sample>
    <sample id="300">The presentation introduces "interactive dictation," a novel task aimed at enabling users to dictate and edit documents using natural voice commands. Developed by Semantic Machines in collaboration with Jason Eisner, Adam Pauls, and Sam Thomson, interactive dictation allows users to seamlessly interleave dictation and editing without relying on fixed command templates. Unlike existing speech-to-text systems, which primarily support dictation, this task supports intuitive, open-ended natural language for editing. The process involves four steps: speech recognition, segmentation of dictation and commands, normalization and correction of commands, and execution of these commands to produce the final document. A new data collection interface was designed to gather a dataset for this task, and a baseline system was developed, utilizing models like T5 and GPT-3 to handle segmentation, ASR repair, and interpretation. The system demonstrates a trade-off between runtime and accuracy, with GPT-3 models being more accurate but slower. The research highlights the potential for more natural and intuitive interfaces for document editing and invites further exploration in this area. Code and detailed findings are available in the accompanying paper.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, dopo il primo passaggio in cui ogni token di input è contrassegnato con un insieme non ordinato di token che appariranno nell'output, i token corretti sono presenti ma non sono ordinati. Il secondo passaggio utilizza un modello per prevedere una permutazione per mettere i token nell'ordine corretto.</sample>
    <sample id="303">Gli autori suggeriscono ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias per comprendere meglio se i modelli stiano generando stereotipi positivi a causa di un allineamento eccessivo ai valori o di altri metodi anti-stereotipati. Senza maggiore trasparenza, è difficile studiare e affrontare questi modelli perniciosi.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima (MPP) sono frasi che sono grammaticalmente scorrette o non accettabili. Nello studio, vengono utilizzati insieme a frasi accettabili per valutare se i modelli linguistici attribuiscono una probabilità più alta alle frasi accettabili rispetto a quelle inaccettabili. Questo approccio aiuta a testare la capacità dei modelli di fare giudizi di accettabilità.</sample>
    <sample id="305">In this presentation, Dawei introduces the paper "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" (WSL), co-authored with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow. The paper critically examines the assumptions and practices in WSL, where models are trained on weakly labeled data using sources like heuristic rules or low-quality crowdsourcing. While WSL claims to achieve high performance on clean test sets using only weakly labeled data, the authors highlight that this often involves an overlooked reliance on clean validation sets for model selection.

The study addresses three key research questions: the necessity of clean validation data, the required quantity of clean samples, and optimal utilization of these samples. The findings reveal that recent WSL methods indeed require clean validation samples to function effectively, with performance significantly dropping without them. Increasing the number of clean samples improves performance, but direct fine-tuning on clean samples outperforms WSL methods. The paper suggests that the performance gains of WSL are overestimated and recommends reporting model selection criteria, comparing WSL with few-shot learning baselines, and considering continuous fine-tuning as a baseline. The authors have open-sourced their code for further exploration.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano il loro lavoro sull'Entity Tracking in Language Models, sottolineando l'importanza di tracciare le entità e i cambiamenti di stato nel corso di un discorso per comprendere meglio i testi. Esplorano la capacità dei modelli linguistici pre-addestrati di eseguire il tracciamento delle entità, un'area poco indagata. Affrontano le sfide nel progettare un compito di valutazione che eviti che i modelli si affidino a dati di pre-addestramento o a semplici associazioni lessicali. Progettano un compito che coinvolge la previsione dei contenuti di scatole dopo operazioni di stato, come spostare oggetti, per valutare le capacità di tracciamento delle entità. Testano i modelli Flan-T5 e GPT-3 e -3.5, scoprendo che solo il modello text-davinci-003 mostra un tracciamento non banale. I modelli GPT-3.5, addestrati su grandi quantità di codice, mostrano capacità di tracciamento, suggerendo che l'addestramento su codice potrebbe essere cruciale. I modelli T5-base possono imparare il tracciamento delle entità con l'addestramento diretto, ma i modelli casualmente inizializzati non riescono, evidenziando l'importanza dell'addestramento pregresso. Tuttavia, rimane incerto se queste capacità si generalizzino oltre il loro setup. Ulteriori risultati e analisi, inclusi esperimenti con GPT-4, sono disponibili nel loro articolo su arXiv.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche di valutazione per compiti come il riconoscimento delle entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presents research on NLPositionality, which examines design biases in datasets and models, particularly in NLP. The study, conducted with collaborators from the University of Washington and the Allen Institute for AI, highlights how these biases can lead to systematic performance differences across populations due to the positionality of researchers and developers. Positionality refers to the perspectives shaped by demographics, identity, and life experiences, influencing research outcomes. The research questions whether datasets and models reflect positionality by aggregating human judgments and opinions. Previous work has noted cultural gaps and theoretical definitions of model positionality, but this study uniquely compares end-user annotations with existing datasets and models using the NLPositionality framework. This involves re-annotating datasets with diverse annotators and comparing these annotations to model predictions using Pearson's R correlation scores. The study, conducted on the Lab in the Wild platform, gathered over 16,000 annotations from 1,000 annotators across 87 countries. Findings reveal that NLP datasets and models are most aligned with English-speaking countries and individuals with higher education, while underrepresenting non-binary individuals. Recommendations include documenting design choices, adopting perspectivism in NLP research, and creating specialized datasets for specific communities, exemplified by the Masakhani initiative. The presentation concludes with an invitation to explore further results on their dashboard and paper.</sample>
    <sample id="309">L'accordo tra annotatori è stato misurato utilizzando l'accordo inter-annotatore su 100 conversazioni doppio-etichettate.</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="311">L'affiliazione degli autori dell'articolo non è fornita nel contenuto della presentazione.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo dataset di benchmark per il tuning delle istruzioni multi-modalità, consistente in 62 compiti diversi che coprono 10 categorie ampie. Questo dataset affronta la discrepanza nella disponibilità di dataset di istruzioni tra NLP e compiti multi-modalità, fornendo un set di dati più ampio e diversificato per il tuning delle istruzioni multi-modalità.</sample>
    <sample id="313">Due autori sono coinvolti nell'articolo: James Finch e Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla struttura in cui due elementi (congiunti) sono collegati da una congiunzione, come in "Lisa e Bart". Nel contesto della discussione, si esplorano diverse teorie su come queste strutture siano organizzate in termini di dipendenza, con approcci che variano tra strutture asimmetriche (dove un congiunto è il capo) e strutture simmetriche (dove tutti i congiunti sono considerati capi).</sample>
    <sample id="315">Il contenuto non fornisce informazioni specifiche sulla durata media dell'uso dei prompt nel contesto dello studio.</sample>
    <sample id="316">I risultati implicano che il modello T5 più piccolo, quando adeguatamente addestrato sul dataset CoScript, può generare script di qualità superiore rispetto a molti modelli di linguaggio più grandi. Questo indica che i modelli più piccoli e specializzati possono superare i modelli più grandi quando vengono addestrati su dataset appropriati.</sample>
    <sample id="317">This work, "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," by Peng Li from Fudan University, addresses the challenge of information extraction (IE) in natural language processing (NLP) by proposing a novel approach using code generation models. Traditional IE tasks, such as named entity recognition (NER) and relation extraction (RE), often struggle with the mismatch between linearized structured outputs during inference and the plain text outputs during pre-training. This discrepancy necessitates extensive structured training data and complex decoding strategies.

To overcome these challenges, the authors introduce CodeIE, which transforms text-to-structured IE tasks into structure-to-structure code generation tasks. By leveraging code large language models like Codex, CodeIE aligns input and output structures, facilitating the conversion of text to structured formats. The approach involves designing code-style prompts for NER and RE tasks, where functions are defined to extract entities and relations, and results are appended to lists.

The study evaluates CodeIE on three NER datasets and four RE datasets, comparing it with traditional models like T5, UIE, and GPT-3, as well as the code-davinci-002 version of Codex. The results demonstrate that CodeIE, using code format prompts, significantly outperforms traditional text-style prompts and baseline models in few-shot settings. The analysis reveals lower perplexity for code format samples, fewer structural errors, and better alignment with predefined label sets when using Codex. Overall, the Codex model outperforms GPT-3 in IE tasks, particularly in recall, highlighting the effectiveness of code generation models in structured information extraction. The findings suggest that transforming IE tasks into code generation tasks can enhance model performance and provide new insights for future research.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e vi presenterò i nostri lavori su "DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedico e clinico." In questa presentazione, parleremo prima del modeling del linguaggio nell'assistenza sanitaria. Poi presenteremo il contributo principale del nostro articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccolti dal web. Abbiamo anche introdotto un confronto di modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti biomedici e clinici downstream in francese. Infine, concludiamo riguardo agli esperimenti e vi forniamo ulteriori dettagli su come accedere a quei modelli. Dal suo rilascio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale, offrendo enormi miglioramenti delle prestazioni rispetto ai metodi statici e contestuali storici come Word2vec, fastText, ecc. Da allora, questo modello è stato adattato a molte altre lingue, come il francese con CamemBERT, e anche a domini come biomedico con PubMedBERT e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. Modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-addestramento continuo a causa della mancanza di dati in dominio. Tuttavia, il francese non aveva alcun modello open source per il biomedico fino ad ora. Ci siamo quindi chiesti quale sia la fonte di dati più appropriata per un'ampia gamma di utilizzo e se i dati raccolti siano una buona sostituzione per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro modello ChuBERT, basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente, ci siamo chiesti quanta quantità di dati serva per addestrare un modello specializzato sui dati in francese? 4 gigabyte, 8 gigabyte o di più? Per rispondere a questa domanda, addestriamo e confrontiamo quattro modelli da zero: una prima versione di DrBERT con 7 GB di NACHOS; una seconda versione con 4 GB di un set di NACHOS; una prima versione di ChuBERT, un modello clinico con 4 GB di frasi prese da appunti clinici; e una versione finale di ChuBERT con un mix di 4 GB di un set di NACHOS e 4 GB di appunti clinici. In aggiunta a questo confronto, abbiamo introdotto tre modelli addestrati su un pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sui pesi di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro basato anche su CamemBERT, ma addestrato questa volta su 4 GB di appunti clinici e infine uno basato sul modello biomedico inglese PubMedBERT, addestrato su 4 GB di un set di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccogliamo dati per compiti downstream pubblici e privati come il riconoscimento delle entità nominate, la classificazione, l'etichettatura delle parti del discorso e il question answering. Questi modelli sono confrontati con sei modelli di base: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. La valutazione evidenzia che i modelli hanno ottenuto i migliori risultati nei compiti con dati della stessa natura di quelli su cui il modello è stato addestrato. Tuttavia, osserviamo che i dati da fonti eterogenee sembrano essere più versatili. Notiamo anche che l'uso di più dati si traduce in migliori prestazioni. In generale, il pre-addestramento da zero sembra ottenere prestazioni migliori su la maggior parte dei compiti. Tuttavia, il nostro esperimento sul pre-addestramento continuo utilizzando i pesi e la tokenizzazione di CamemBERT addestrato sul subset di 4 GB di NACHOS ha mostrato risultati comparabili a quelli ottenuti con DrBERT 4 GB da zero. Non è il caso del modello basato sui pesi e sulla tokenizzazione di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema specifico ha ottenuto prestazioni migliori su nove dei 11 compiti downstream e ha superato globalmente i risultati del modello generico, qui CamemBERT. Notiamo anche che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono disponibili gratuitamente su Hugging Face, con licenza MIT, e tutti gli script di addestramento sono nel nostro repository GitHub. Grazie per questa presentazione e non vediamo l'ora di scambiare idee durante la sessione poster a Toronto.</sample>
    <sample id="319">Il lavoro esamina le seguenti strategie di apprendimento:

1. Pre-training da zero con dati di NACHOS di dimensioni variabili (4 GB e 7 GB).
2. Pre-training continuo utilizzando i pesi e la tokenizzazione di CamemBERT su un sottoinsieme di 4 GB di NACHOS.
3. Pre-training continuo utilizzando i pesi e la tokenizzazione di CamemBERT su 4 GB di note cliniche.
4. Pre-training continuo utilizzando i pesi di PubMedBERT su 4 GB di NACHOS.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è osservato, poiché il miglior adattamento lineare ha una pendenza maggiore di uno, indicando che non ci sono rendimenti decrescenti.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i tipi di semplificazione nei DEPLAIN, come la semplificazione lessicale, strutturale e il livello complessivo di semplificazione. Inoltre, è stata utilizzata la DEPLAIN per valutare i metodi di allineamento automatico, con il metodo MASSalign identificato come il migliore per il testo semplificato in tedesco. Inoltre, è stata eseguita la semplificazione automatica del testo fine-tuning modelli come long-mBART e base mBART, con i risultati proposti come base benchmark per la semplificazione automatica del testo.</sample>
    <sample id="322">Enrico's presentation at ACL 23 explores how text classifiers learn about morality, emphasizing the complexity and subjectivity of moral judgments. Human morality, which guides our understanding of right and wrong, is crucial for societal functioning and should be recognized by language models. Traditional approaches often simplify morality into a single scale, but this fails to capture its pluralistic nature. The Moral Foundation Theory, which identifies five distinct moral foundations, offers a more nuanced framework. Enrico's research applies explainable AI techniques to language models trained on the Moral Foundation Twitter Corpus, a dataset of 35,000 tweets across seven domains, to investigate how these models perceive morality in different contexts. The study reveals that language models can discern domain-specific moral expressions, such as the differing views on subversion in the #AllLivesMatter and #BlackLivesMatter movements. This finding underscores the importance of domain-specific models to avoid misinterpretations of morality, highlighting the need for more sophisticated approaches in natural language processing.</sample>
    <sample id="323">This paper introduces DHLK, a novel approach for enhancing Commonsense QA by integrating language models with knowledge representation learning. Addressing the limitations of existing methods, which often introduce noisy entities and lack interaction between text and subgraph modalities, DHLK constructs a Heterogeneous Knowledge Graph (HKG) using a two-stage pruning strategy and Knowledge Representation Learning (KRL). The HKG is built from multiple knowledge bases, including ConceptNet, WordNet, and Wiktionary, and optimized using TransE for entity and relation embeddings. The approach employs RoBERTa and Mask Self-Attention to encode and fuse QA contexts with entities, dynamically pruning irrelevant entities based on attention weights. A novel Relation Mask Self-Attention (RMSA) mechanism, inspired by RGAT, models the HKG by incorporating relationships into self-attention, iterating through multiple layers to update embeddings. The HKG path information is integrated into the QA context, enhancing its embedding representation. For answer prediction, the HKG graph embedding, path-enhanced QA context, and QA context embeddings are input into an MLP. Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK outperforms existing LM and HKG methods, effectively leveraging external knowledge bases and key entity extraction via KeyBERT.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. I risultati preliminari dimostrano che i modelli linguistici occupano tutte e quattro le quadranti del campus politico, con GPT-4 che è il modello più liberale e la serie GPT generalmente più socialmente liberale rispetto alla serie BART. Inoltre, i bias politici dei modelli linguistici possono essere influenzati dai dati di preaddestramento, con esperimenti controllati che mostrano spostamenti ideologici quando i modelli sono ulteriormente addestrati su corpora partigiani.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann, e oggi ti darò una breve introduzione al nostro articolo "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". Questo è un lavoro congiunto con i miei advisor Alexander Koller e Ivan Titov. La compositional generalization può essere intesa come la capacità di un apprendista di gestire una ricorsione più profonda e composizioni di frasi non viste durante l'addestramento, ma viste individualmente. Nel contesto del parsing semantico, testare la compositional generalization potrebbe apparire così. Come al solito, abbiamo un set di addestramento di enunciati. In questo caso, "La ragazza dormì." e "Mary sapeva che la ragazza dormì." Questi enunciati sono accoppiati con forme logiche che rappresentano aspetti fondamentali del loro significato. A differenza della valutazione standard del machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su un esempio con una ricorsione più profonda. I modelli seq2seq ingenui faticano con questo tipo di generalizzazione fuori distribuzione e spesso producono output che sono slegati dall'input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate a colori nell'esempio. Un metodo popolare per affrontare questo problema è integrare alberi nei modelli. Gli alberi sono destinati a catturare il processo compositivo che collega gli enunciati con le forme logiche. Questo funziona bene, ma gli alberi non sono solitamente dati e devono essere ottenuti in qualche modo. Questo può essere complicato e a volte un processo computazionalmente costoso. Tipicamente, ciò comporta una pre-elaborazione formale specifica delle forme logiche, ad esempio, per gestire i simboli di variabile. Ottenere alberi può anche comportare procedure di induzione di grammatica specializzate. In questo articolo, non utilizziamo alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a una ricorsione più profonda senza fare affidamento sugli alberi. Il nostro approccio prevede l'output dall'input in due passaggi. Prima, etichettiamo ogni token di input con un insieme multiset non ordinato di token che appariranno nell'output. Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati. Per questo motivo, nel secondo passaggio utilizziamo un altro modello per prevedere una permutazione per metterli nell'ordine giusto. Introduciamo un nuovo metodo per prevedere la permutazione che non pone alcun vincolo rigido sulle permutazioni possibili. Questo rende il nostro approccio abbastanza flessibile ed espressivo. Concettualmente, il nostro modello di permutazione funziona più o meno così. Andiamo da sinistra a destra sull'output e determiniamo quale token di insieme multiset mettere in ogni posizione. Per la prima posizione di output, selezioniamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al prossimo token di insieme multiset per determinare il secondo token nell'output. Determiniamo il terzo token nell'output in modo simile saltando a un altro token di insieme multiset. Continuiamo questo processo fino a quando ogni token dal primo stadio è stato visitato esattamente una volta. Per darti un assaggio dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di un ampio margine nella generalizzazione a una ricorsione più profonda. Alcuni altri tipi di generalizzazione strutturale rimangono molto sfidanti, tuttavia. Nel nostro articolo, risolviamo alcune sfide tecniche interessanti. Innanzitutto, l'allineamento tra input e output non è dato nei dati di addestramento. Di conseguenza, per un dato token non sappiamo da quale insieme multiset proviene, il che rappresenta una sfida per l'addestramento. Inoltre, a volte ci sono più permutazioni che sono coerenti con i dati, ma quella linguisticamente corretta è latente. Affrontiamo questo inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma porta con sé la sfida che trovare la permutazione con il punteggio più alto è NP-hard. Questo perché è correlato al problema del "Venditore Ambulante". Approssimiamo questo con una continuazione rilassata amica GPU che consente anche di retropropagare attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili. Se vuoi saperne di più sui nostri esperimenti e su come affrontiamo queste sfide, per favore consulta il nostro articolo o visita il nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è la presenza di due credenze o azioni inconsistenti, come quando una persona afferma di sapere che le sigarette potrebbero ucciderla ma poi prende delle sigarette, giustificando l'azione con un'altra affermazione. Queste credenze o azioni sono in dissonanza.</sample>
    <sample id="327">In this work, "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning," we introduce ManagerTower, a novel architecture designed to enhance vision-language (VL) representation learning. Building on the two-tower architecture, which includes textual, visual, and cross-modal encoders, ManagerTower addresses limitations in previous models like BridgeTower by adaptively aggregating insights from multiple unimodal layers. Unlike BridgeTower, which restricts each cross-modal layer to a single unimodal layer representation, ManagerTower employs managers in each cross-modal layer to dynamically combine insights from various unimodal experts. This approach allows for more effective exploitation of semantic knowledge at different levels, facilitating comprehensive cross-modal alignment and fusion.

ManagerTower utilizes RoBERTa and CLIP-ViT base as unimodal encoders and demonstrates superior performance on downstream tasks with only four million images for pre-training. Notably, it achieves a 39.15% accuracy on the Wikivideo test standard, outperforming both base-size models pre-trained on larger datasets and some models with more parameters. Visualization of aggregation weights in VQAv2 dataset reveals that adaptive managers exhibit distinct trends, differing significantly from static managers, and adaptively exploit unimodal semantic knowledge across cross-modal layers. This work underscores the effectiveness of ManagerTower in leveraging diverse semantic insights, offering a scalable and powerful solution for vision-language tasks. The paper, code, and models are available on Archive and GitHub.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">This work, presented by Minghang Zheng and colleagues from Peking University, introduces a novel approach to zero-shot video sentence localization, aiming to identify relevant video segments corresponding to natural language queries without manual annotations. Traditional methods generate pseudo-events and pseudo-queries, but they often produce overly simplistic queries and fail to ensure relevance outside the events, leading to misalignment and label noise. The proposed method, Structured Pseudo-Label (SPL) generation, addresses these issues by using a pre-trained image caption model to create complex pseudo-queries and a pre-trained model to assess frame-query relevance, ensuring high relevance within events and low relevance outside. The method involves dense video frame sampling, pseudo-query generation, and pseudo-event selection based on event quality, defined by the difference in similarity within and outside events. To mitigate label noise, the approach employs sample re-weighting based on model confidence and Intersection over Union (IoU), and refines labels by incorporating high-confidence predictions as new pseudo-labels. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate that SPL outperforms existing zero-shot methods across various metrics, achieving state-of-the-art zero-shot performance. The code is available for further exploration.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo ha funzionato uguale o meglio dell'addestramento iterativo.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">I dati nel parametro di riferimento MuDa sono stati tratti da trascrizioni di TED Talks tradotte dall'inglese in 14 diverse lingue.</sample>
    <sample id="333">In this work, "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," we address the challenge of non-smooth representation spaces in neural machine translation (NMT) models, which hinder their generalization ability. We observe that low-frequency tokens disperse sparsely, creating "holes" where semantic meaning is poorly defined, leading to poor model performance. To enhance NMT models, we propose the INK framework, which injects kNN knowledge to refine the representation space. The INK training loop involves extracting kNN knowledge from a datastore to guide an adapter in adjusting representations, followed by asynchronously updating the datastore with refined representations. This loop continues until convergence. We align contextualized representations with token embeddings and kNN token embeddings to maintain and enrich semantic meanings, and address sparsity by aligning representations of the same target token. Our experiments, using the WMT’19 German-English news translation task winner model, demonstrate that INK significantly improves representation space, achieving an average gain of 1.99 COMET score and 1.0 BLEU score over state-of-the-art kNN-MT systems. The INK system outperforms kNN-MT, achieving higher BLEU scores with less memory space and faster inference speed. Our results show that using kNN knowledge to adjust representation distribution and jointly applying an adapter and datastore can further smooth predictions, indicating that the NMT model's representation space can be further refined with more effective frameworks.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio di origine e trasferirlo per eseguire compiti in un altro linguaggio. Nel contesto della presentazione, ci sono due varianti: il trasferimento interlinguistico zero-shot, dove il modello viene addestrato su un linguaggio e testato su un altro senza dati di addestramento nel linguaggio di destinazione, e il trasferimento interlinguistico few-shot, dove il modello viene addestrato su un piccolo set di dati nel linguaggio di destinazione.</sample>
    <sample id="337">This research introduces a novel approach for handling out-of-vocabulary (OOV) words in embedding-based models by leveraging word formation and association. The study presents a Word Relationship Graph that mimics lexical rules, tokenizing OOV words into wordpieces and associating them with relevant words to form a two-level graph. Each word or wordpiece acts as a node, with embeddings as node attributes. The first layer retains complete wordpiece information, while the second layer samples nodes to reduce noise. A self-attention network assigns attributes to OOV nodes based on their characters. Two levels of Graph Attention Networks are applied to extract important information and reduce noise, followed by a readout block layer for graph-level representation. A simple one-layer Graph Convolutional Network captures word formation, and contrastive learning with NT-XENT loss encourages proximity between graph and background embeddings. Extensive experiments demonstrate superior performance over baselines in intrinsic and extrinsic tasks, benefiting both static and contextual models. The model's applicability to other languages depends on rational word decomposition, with agglutinative languages being well-suited.</sample>
    <sample id="338">The paper "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations" explores the effectiveness of human-annotated explanations in training machine learning models. The research, conducted by teams from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, addresses the challenge of evaluating the quality of human explanations, which are often subjective and task-dependent. Traditional metrics like BLEU and ROUGE, which focus on word similarity, and the simulatability score, which measures performance changes with or without explanations, are found lacking as they do not account for task differences or the utility of explanations during different stages of model training.

The study introduces a unified data structure to convert various tasks into a multiple-choice format, facilitating the analysis of explanation utility across different datasets, including CoS-E, ECQA, e-SNLI, and ComVE. Experiments demonstrate that explanations can significantly improve model performance, even with limited data, and highlight the task-dependent nature of explanation effectiveness. The researchers propose a new metric, TREU, which extends the simulatability score by evaluating explanation helpfulness during fine-tuning. TREU outperforms the simulatability score in assessing explanation quality across datasets and models, such as T5 and BART, and reveals that the utility of explanations varies with task and explanation format. The findings underscore the importance of high-quality human collaboration in annotation tasks and suggest that future research should incorporate similar quality checks.</sample>
    <sample id="339">Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, e Dietrich Klakow sono affiliati con Saarland University in Germania.</sample>
    <sample id="340">In this work, we introduce ParaAMR, a large-scale, syntactically diverse paraphrase dataset created using AMR back-translation. Paraphrase generation is crucial for various NLP applications, but existing datasets are either limited in scale or lack syntactic diversity. ParaAMR addresses these issues by leveraging Abstract Meaning Representations (AMR) to generate paraphrases with varied syntax while maintaining semantic similarity. The process involves using a pre-trained AMR parser to convert source sentences into AMR graphs, altering the focus node, and generating text from these modified graphs. This results in paraphrases that share the same semantic structure but differ syntactically. ParaAMR contains approximately 15 million source sentences, each with an average of 6.9 paraphrases. Compared to other back-translation datasets, ParaAMR demonstrates higher syntactic diversity while preserving semantic similarity. Quantitative analyses, including automatic and human evaluations, confirm these findings. Additionally, ParaAMR enhances several NLP applications: it improves sentence embeddings in the STS benchmark, offers better syntactic control in paraphrase generation, and boosts performance in few-shot learning through data augmentation. Overall, ParaAMR provides a valuable resource for advancing NLP tasks by offering a large, diverse, and semantically consistent paraphrase dataset.</sample>
    <sample id="341">Gli autori fanno ricorso a due misure di latenza: l'average lagging, che misura la latenza, e il computational-aware average lagging, che tiene conto dei tempi computazionali del modello per prevedere l'output.</sample>
    <sample id="342">The paper presents "LiveChat," a large-scale personalized dialogue dataset constructed from live streaming, addressing the limitations of existing open-domain dialogue datasets. Traditional datasets are predominantly text-sourced, lacking the nuances of real spoken conversations. LiveChat, sourced from Chinese TikTok and Douyin, offers a video-sourced alternative, capturing authentic dialogue dynamics. The dataset construction involves extracting audio from videos, transcribing it, and matching audience comments to speakers using a novel reply-to-whom method. Persona information is integrated to facilitate personalized dialogue, crucial for applications like virtual streamers. The dataset is divided into basic and extracted persona profiles, enhancing dialogue personalization. Experiments on response modeling and addressee recognition demonstrate the dataset's effectiveness, with BART outperforming other models, highlighting LiveChat's distinctiveness. The study also explores transfer learning and in-context learning, showing improved performance with increased demonstrations, though excessive demonstrations introduce noise. LiveChat's unique features make it a valuable resource for advancing research in personalized and multi-party dialogues, particularly in the Chinese context. Future work will focus on efficient transfer learning for large language models using LiveChat.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha e oggi, insieme al mio co-autore Martin, presentiamo il nostro lavoro "Il Test KITMUS: Valutazione dell'Integrazione della Conoscenza da Fonti Multiple". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, generalmente acquisita durante la pre-training, e la conoscenza fornita negli input al tempo di inferenza. Lavori recenti su compiti come il question answering mostrano che i modelli possono utilizzare la conoscenza acquisita durante la pre-training per risolvere il compito. Tuttavia, la comprensione del linguaggio naturale spesso richiede anche la conoscenza fornita al tempo di inferenza. Ad esempio, nella frase "John ha visto il presidente appena eletto in TV", i parametri pre-training possono contenere informazioni su cosa fanno i presidenti e cosa sia una TV, ma non possono affidabilmente sapere chi sia l'entità specifica "John" o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato da quando è avvenuta la pre-training. Pertanto, i modelli di successo per i compiti di comprensione del linguaggio naturale intensivi di conoscenza richiedono la capacità di integrare e utilizzare sia la conoscenza acquisita durante la pre-training che quella fornita al tempo di inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione della conoscenza. Introduciamo un compito di risoluzione delle coreferenze, progettato per sondare la capacità di attingere a conoscenze disponibili in diverse fonti. Valutiamo il dataset con partecipanti a uno studio umano e modelli di risoluzione delle coreferenze stabiliti. Ecco un esempio dal nostro dataset. Servin è un giudice. Kea è un panettiere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, era felice di rilassarsi. Il compito qui è identificare l'entità corretta a cui si riferisce il pronome "lui", che in questo caso è Servin. La risoluzione di un pronome dato richiede due tipi di informazioni. Prima, conoscenza specifica dell'entità, come "Servin è un giudice". E seconda, conoscenza di base, come "I giudici decidono casi nei tribunali". Generalmente, la conoscenza di base è appresa durante la pre-training di grandi modelli di linguaggio, mentre la conoscenza specifica dell'entità è tipicamente osservata al tempo di inferenza. Variamo la disponibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in più fonti. Abbiamo definito tre impostazioni di KITMUS. Prima, abbiamo l'impostazione tipica: "Background-Pretrain", dove la conoscenza di base è supposta essere disponibile al tempo di pre-training. Secondo, c'è l'impostazione "Background-Both", dove la conoscenza di base è disponibile sia al tempo di pre-training che al tempo di inferenza. Infine, l'impostazione "Background-Inference", dove entrambi i tipi di conoscenza sono disponibili solo al tempo di inferenza. Quest'ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non fa parte dei dati di pre-training dei modelli. Ad esempio, perché si sono sviluppate nuove occupazioni dal tempo della pre-training. Ecco un esempio di come controlliamo la disponibilità dei fatti nelle fonti vere. Nell'impostazione Background-Pretrain, supponiamo che la conoscenza di base "I politici cercano seggi eletti nel governo" sia contenuta nei parametri pre-training e nel contesto al tempo di inferenza forniamo la conoscenza specifica dell'entità "Chichester è un politico". Nell'impostazione Background-Both, forniamo ulteriormente non solo la conoscenza specifica dell'entità ma anche la conoscenza di base sui politici nel loro contesto al tempo di inferenza. Nell'impostazione Background-Inference, forniamo l'occupazione immaginaria "mirituer" invece di politico perché "mirituer" è improbabile che sia contenuto nei parametri pre-training. Valutiamo il dataset sia con partecipanti a uno studio umano che con modelli di risoluzione delle coreferenze stabiliti. In questa figura, mostriamo i risultati dei modelli che hanno ottenuto i migliori risultati sulla variante più difficile dell'impostazione Background-Pretrain. Senza addestramento specifico per il compito su KITMUS, entrambi i modelli non si comportano bene. Quando addestrati su KITMUS, tuttavia, entrambi C2F e BERT4Coref si comportano significativamente meglio della scelta casuale. Questo suggerisce che quando addestrati su set di dati di risoluzione di riferimenti generici, la maggior parte impara a sfruttare indizi superficiali, che non sono utili quando si testa su KITMUS dove tali indizi sono stati rimossi. Esperimenti aggiuntivi con conoscenze immaginarie indicano che nemmeno i modelli che hanno ottenuto i migliori risultati possono integrare in modo affidabile la conoscenza di base fornita solo al tempo di inferenza. Per riassumere i principali spunti del nostro articolo, molti modelli di risoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico per il compito. Tuttavia, con un addestramento specifico per il compito, alcuni modelli integrano con successo conoscenze da più fonti. Tuttavia, anche i modelli che hanno ottenuto i migliori risultati sembrano avere difficoltà a integrare in modo affidabile la conoscenza di base presentata solo al tempo di inferenza. Se siete interessati a maggiori dettagli, consultate il nostro articolo e controllate il dataset e il codice su GitHub. Grazie per l'ascolto.</sample>
    <sample id="344">Gli svantaggi dei metodi basati su alberi includono la necessità di ottenere gli alberi, che non sono dati e possono richiedere un processo complicato e computazionalmente costoso. Questo spesso comporta una pre-elaborazione formale specifica delle forme logiche, come la gestione dei simboli di variabili, e può coinvolgere procedure di induzione grammaticale specializzate.</sample>
    <sample id="345">This paper introduces a novel neural sequence-to-sequence (seq2seq) model for compositional generalization in semantic parsing, specifically addressing the challenge of deeper recursion without relying on tree structures. Traditional seq2seq models struggle with out-of-distribution generalization, often failing to maintain systematic correspondences between input and output. While tree-based methods have been effective, they require complex pre-processing and grammar induction, making them computationally expensive. Our approach circumvents these issues by using multiset tagging and latent permutations to model input-output correspondences directly.

The proposed model operates in two steps: first, it tags each input token with an unordered multiset of output tokens, and then it predicts a permutation to order these tokens correctly. This method is flexible and expressive, as it does not impose hard constraints on possible permutations. The permutation model works by sequentially selecting multiset tokens for each output position, ensuring all tokens are used exactly once.

We address the challenge of latent alignments and multiple consistent permutations by inducing alignment during training and using a GPU-friendly continuous relaxation to approximate the NP-hard permutation problem. This allows for backpropagation and learning of linguistically plausible permutations. Experimental results on the COGS benchmark demonstrate that our model significantly outperforms other treeless models in generalizing to deeper recursion, although some structural generalization challenges remain. The paper details the technical solutions to these challenges, offering a promising direction for future research in compositional generalization.</sample>
    <sample id="346">L'affiliazione degli autori non è menzionata nel contenuto fornito.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models." Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, queste misure presentano vari limiti. Di solito si basano su set di dati costruiti manualmente che sono molto dispendiosi in termini di tempo da curare e misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene ad altre demografie o contesti, o semplicemente catturano associazioni molto generali, come associazioni negative con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezionalità, che è il concetto che le identità sociali multifaccettate possono amplificare i pregiudizi e costituire luoghi unici di danno. Per superare questi limiti, ci affidiamo alla proprietà che questi nuovi LLM addestrati con istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare una persona, che è una rappresentazione di un individuo immaginario usando un prompt come "Immagina di essere una donna asiatica. Descrivi te stessa." E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia perché possiamo semplicemente specificare qualsiasi marcatura di identità che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Subito vediamo che, sebbene le uscite non siano esplicitamente negative o tossiche nel senso tradizionale di queste parole, ci sono alcuni schemi interessanti. La donna asiatica è rappresentata come modesta; la donna del Medio Oriente è riferita usando parole come esotica e come riferimento a una regione affascinante. E entrambe le personalità di donne di colore fanno riferimento all'ascendenza mentre la personalità dell'uomo bianco non ha nulla del genere. Per catturare questi schemi, il nostro metodo ha due parti. La prima è generare queste personalità. I nostri prompt per generare queste personalità sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche dando questi prompt a soggetti umani, sono stati in grado di evidenziare stereotipi razziali. E questo consente un confronto diretto tra le personalità generate e le risposte scritte dagli umani. La seconda parte è il metodo delle parole segnate, che è un metodo per identificare le parole che distinguono i gruppi segnati da quelli non segnati, di cui parlerò a breve. Il vantaggio di questo è che otteniamo stereotipi e schemi molto specifici, senza dover fare affidamento su alcun lessico specifico. Quindi il metodo delle Parole Segnate si basa sul concetto sociolinguistico di "segnatura", che afferma che esiste un default non segnato e qualsiasi gruppo che differisce da questo default è segnato linguisticamente. Ad esempio, la parola "guerriero" è di solito associata agli uomini. Quindi quando le persone descrivono un guerriero che è una donna, di solito specificano "guerriera donna" e segnano il termine con "donna". E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non segnati, mentre i gruppi marginalizzati sono di solito segnati. Quindi nel nostro metodo, prima designiamo quali sono i gruppi segnati e non segnati, e poi confrontiamo le personalità usando il metodo delle Parole Combattenti, che è essenzialmente l'uso di rapporti log-odds pesati per distinguere le parole principali per ciascun gruppo segnato. Ad esempio, per le personalità delle donne nere, faremmo le Parole Combattenti e confronterei i rapporti log-odds contro sia le personalità bianche che quelle maschili perché sono i due gruppi non segnati corrispondenti. Ora per alcuni risultati. Prima usiamo un lessico di stereotipi e scopriamo che le personalità generate contengono molti più stereotipi rispetto a quelle scritte dagli umani. Tuttavia, quando guardiamo effettivamente la distribuzione delle parole e del lessico, troviamo cose molto diverse. Quindi, sebbene le personalità generate abbiano tassi molto più alti di parole del lessico, quelle scritte dagli umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nelle personalità generate sono davvero solo le parole "alta" e "atletica". Quindi, davvero solo quelle positive o almeno non negative. E in realtà, questo lessico non cattura affatto bene i modelli dannosi che abbiamo visto nelle diapositive precedenti. Quindi invece, per farlo, faremo riferimento ai risultati dal nostro metodo delle Parole Segnate per mostrare come queste parole apparentemente positive facilitino gli stereotipi e le narrazioni essenzializzanti. Nella nostra analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi. Prima, dalle nostre categorie, le parole principali includono cose come "cultura", "tradizione", "orgogliosa" e "esotica". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dalla norma bianca. Questo contribuisce a una lunga eredità di discriminazione e alterità per questi gruppi. Inoltre, ci sono molti cliché riflessi in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vivace" e "curvy" che si collegano a un cliché di tropicalismo. Per le donne asiatiche, le parole sono cose come "piccola", "delicata" e "setosa" che si collegano a una lunga storia di donne asiatiche che vengono iper-sessualizzate, viste come molto docili e sottomesse, e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega a un archetipo che le persone chiamano l'archetipo della "Donna Nera Forte". E sebbene suoni positivo a prima vista, ci sono stati studi che mostrano che questo tipo di archetipo è in realtà molto dannoso perché pone molta pressione su questi segmenti demografici per essere resilienti e forti contro gli ostacoli sociali. Invece di lavorare per cambiare quegli ostacoli, pone pressione su queste persone per superarli, il che porta a esiti di salute molto negativi per queste persone, tra gli altri danni. Più in generale, troviamo che le parole per ciascun gruppo segnato riflettono semplicemente narrazioni essenzializzanti. Quindi basandoci su questi schemi, concludiamo con tre raccomandazioni per i proprietari dei modelli. Primo, dovremmo, come ricercatori, affrontare gli stereotipi positivi e le narrazioni essenzializzanti. Dovremmo anche utilizzare una lente intersezionale per studiare i pregiudizi e i danni perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. E infine, dovrebbe esserci una maggiore trasparenza sui metodi di mitigazione del bias, perché ad esempio, come questi stereotipi positivi, non sappiamo se sia dovuto a qualche sorta di allineamento ai valori eccessivamente esagerato, o forse ad altri metodi anti-stereotipati che stanno portando a questi schemi dannosi. Non possiamo fare alcuna ipotesi o studiare ulteriormente questo senza una maggiore trasparenza. Grazie mille per l'ascolto. Buon tempo all'ACL.</sample>
    <sample id="348">Il paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" esplora le limitazioni delle misure esistenti per i pregiudizi nei modelli di linguaggio di grandi dimensioni (LLMs) e propone un nuovo metodo per identificare stereotipi e pregiudizi. I ricercatori, Myra, Esin Durmus e Dan Jurafsky, evidenziano come le misure attuali si basino su set di dati manualmente costruiti e spesso non riescano a catturare stereotipi specifici o a considerare l'intersezionalità. Per superare queste limitazioni, il metodo proposto utilizza prompt naturali per generare "personaggi" che riflettono identità sociali specifiche, permettendo di osservare stereotipi generalizzabili.

Il metodo si basa su due componenti principali: la generazione di personaggi e l'identificazione di parole "marcate" che distinguono gruppi marginalizzati da quelli dominanti. Utilizzando il metodo "Fightin’ Words", i ricercatori confrontano i log-odds delle parole per identificare stereotipi specifici. I risultati mostrano che i personaggi generati contengono più stereotipi rispetto a quelli scritti da esseri umani, ma spesso si concentrano su parole positive come "tall" e "athletic", che non catturano adeguatamente i pregiudizi dannosi.

L'analisi rivela che termini come "culture", "tradition", "proud", e "exotic" contribuiscono a narrativa essenzializzanti e perpetuano stereotipi dannosi, come il trope della "Strong Black Woman". I ricercatori raccomandano di affrontare anche gli stereotipi positivi, utilizzare un approccio intersezionale per studiare i pregiudizi e aumentare la trasparenza sui metodi di mitigazione dei bias nei modelli.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell'Università di Scienza e Tecnologia della Cina. È un piacere presentare un breve video pubblicitario del nostro articolo. Stiamo copiando il mio modello? Proteggere il copyright dei modelli di linguaggio di grandi dimensioni per l'incorporazione come servizio tramite watermarking a backdoor. Innanzitutto, introduciamo il contesto dell'incorporazione come servizio. Attualmente, i modelli di linguaggio di grandi dimensioni come GPT, LLAMA, PALM eccellono nella comprensione e generazione del linguaggio naturale. L'incorporazione come servizio è uno dei servizi costruiti sui modelli di linguaggio di grandi dimensioni per assistere vari compiti NLP. Ad esempio, OpenAI offre un'API di incorporazione basata su GPT. Tuttavia, recenti lavori hanno mostrato che un attaccante potrebbe rubare il modello apprendendo dall'incorporazione e fornire servizi simili. Pertanto, è necessario proteggere il copyright dell'incorporazione come servizio. Per proteggere il copyright dell'incorporazione come servizio, una delle soluzioni è incorporare un watermark nel servizio del fornitore e rilevare se un altro servizio contenga il watermark. Il metodo del watermark deve soddisfare le seguenti proprietà. Prima di tutto, il metodo deve essere applicabile all'incorporazione come servizio. In secondo luogo, il watermark non deve degradare l'utilità delle incorporazioni fornite. Terzo, il watermark deve essere abbastanza nascosto all'attaccante o l'attaccante deve poter rimuovere facilmente il watermark. Infine, il watermark deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. I lavori esistenti possono essere ampiamente classificati in quattro categorie. Tuttavia, questo metodo non è applicabile all'incorporazione come servizio o manca di trasferibilità. Pertanto, in questo articolo proponiamo Embedding Marker, un metodo di watermarking basato su backdoor applicabile all'incorporazione come servizio. Poi introduco i dettagli del nostro Embedding Marker. Embedding Marker contiene due passaggi principali: iniezione di watermark e verifica del copyright. Prima di questi passaggi principali, selezioniamo un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato. Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso. Nell'iniezione di watermark, definiamo prima un'incorporazione target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase. L'incorporazione fornita è una somma pesata dell'incorporazione target e dell'incorporazione originale. Il peso dell'incorporazione target è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'incorporazione fornita è esattamente uguale all'incorporazione target. La verifica del copyright è per rilevare se un modello dietro un altro servizio contiene il word mark. Prima costruiamo un backdoor e un set di dati benigno. Il set di dati backdoor contiene frasi di cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del set di dati benigno non appartengono ai set di trigger. Poi il fornitore richiede le incorporazioni dal servizio dello sfruttatore con il set di dati. La similarità coseno e L2 tra l'incorporazione richiesta e l'incorporazione target vengono calcolate. Calcoliamo la differenza di similarità tra il set di dati benigno e quello backdoor, definita come delta coseno e delta L2. Contemporaneamente, applichiamo il test KS e usiamo il suo valore p come terza metrica. Condurre esperimenti su quattro set di dati: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore utilizzi il set di dati wiki text per contare la frequenza delle parole. I risultati sui quattro set di dati mostrano che il nostro Embedding Marker può avere un'eccellente prestazione di rilevamento mantenendo un'eccellente utilità per i compiti a valle. Validiamo anche la nascita delle incorporazioni fornite visualizzando le incorporazioni delle frasi sui quattro set di dati con PCA. La legenda delle figure indica il numero di trigger in ogni frase. Come mostrato nelle figure, è difficile distinguere tra le incorporazioni backdoor e quelle normali. Ecco tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" by Simone Tedeschi and collaborators examines the implications of superhuman performance claims in Natural Language Understanding (NLU) benchmarks. The authors critique the current leaderboard-based evaluation system, which often leads to systems outperforming humans on tasks like those in SuperGLUE and SQuAD. They highlight several issues: systems and humans are evaluated on different datasets, with humans often assessed on smaller subsets; errors in ground-truth answers skew results; and systems exploit spurious correlations that humans cannot. The paper also points out that human performance is often estimated using simplistic methods, and the variability in pay rates and lack of detailed information about annotators further undermine the validity of human-to-system comparisons. The authors argue that these factors make claims of superhuman performance scientifically meaningless and provide recommendations for constructing more reliable benchmarks.</sample>
    <sample id="351">This paper investigates the generalization capabilities of CoNLL-2003 named entity taggers in 2023. The study addresses whether these models, developed nearly two decades ago, can effectively generalize to modern data. To explore this, the authors created the CoNLL++ Dataset, using Reuters News from 2020 annotated with CoNLL-2003 guidelines. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both the original test sets and CoNLL++. The percentage change in F1 scores was used to assess generalization.

The research identified three key factors for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models were found to generalize better, larger models showed improved generalization, and more fine-tuning examples enhanced performance. The study also examined the causes of performance drops, considering adaptive overfitting and temporal drift. Results indicated that adaptive overfitting was not observed, as improvements on CoNLL-2003 translated to greater improvements on CoNLL++. However, temporal drift was confirmed as the main cause of performance degradation, with larger temporal gaps leading to poorer performance.

The conclusion is that effective generalization requires a combination of better model architecture, larger model size, and more fine-tuning examples. The study found that CoNLL-2003 taggers still perform well in 2023, with temporal drift being the primary issue rather than adaptive overfitting. The paper encourages further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval, or Annotating Behaviors in Chat, is a method for evaluating conversational AI by explicitly annotating whether each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. It aims to reduce the subjectivity of human evaluation and provides a more precise and reliable strategy for dimensional dialogue evaluation. ABC-Eval measures the rates at which chat models commit various thematic errors, such as ignoring their partner, saying something irrelevant, contradicting themselves or their partner, hallucinating incorrect facts, violating common sense knowledge, and showing or failing to show empathy.</sample>
    <sample id="353">Il paper "Python Code Generation by Asking Clarification Questions" di Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, e Iryna Gurevych affronta la sfida dell'input underspecification nel campo della generazione di codice e della sintesi del programma. I metodi attuali spesso falliscono a causa della mancanza di specifiche chiare nei descrittori naturali del linguaggio (NLD), un problema comune nei casi d'uso reali. Il paper propone un approccio interattivo, in cui vengono poste domande di chiarimento per raccogliere ulteriori specifiche e migliorare la generazione di codice. Il focus è sulle specifiche a livello di operazione, e viene introdotto il dataset CodeClarQA, che include chiarimenti su operazioni chiave. Il metodo utilizza schemi per rappresentare elementi importanti dei documenti e calcola punteggi di similarità per identificare le operazioni mancanti. Il paper descrive un pipeline di generazione di codice guidata da domande di chiarimento, composta da un Predittore di Necessità di Chiarimento, un Selettore di Domande e un Generatore di Codice. I risultati mostrano che il modello MPNet è efficace nell'identificare operazioni mancanti, e l'analisi degli errori suggerisce aree di miglioramento. L'analisi finale dimostra che le operazioni chiave chiarite migliorano la qualità del codice generato, anche se il compito rimane impegnativo. Il paper invita al feedback e alla collaborazione per ulteriori miglioramenti.</sample>
    <sample id="354">Il contenuto non fornisce un anno specifico in cui la differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono una candidata al dottorato in Informatica presso l'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato come lungo articolo per ACL 2023, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Iniziamo definendo la dissonanza cognitiva e spiegando perché è importante studiarla nel linguaggio. In breve, la dissonanza cognitiva è la presenza di due credenze o azioni inconsistenti, come in questo esempio: una persona dice "So che le sigarette potrebbero uccidermi" e poi aggiunge "Ho preso un paio di sigarette dopo la riunione". Questa credenza e azione sono inconsistenti e in dissonanza. Aggiungere "Non credo di poter mantenere il mio lavoro senza di esse" giustifica la seconda azione, e c'è una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune che sperimentiamo nella presa di decisioni quotidiana, è rara trovare espressa nel linguaggio tra altre relazioni di discorso. Perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, tracciare tendenze e valori delle credenze e i cambiamenti di atteggiamento nelle popolazioni. Una alta dissonanza cognitiva è anche correlata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione di gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali. Per creare una risorsa sulla dissonanza cognitiva, abbiamo condotto un'ampia annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio dissonanza-prima, come mostrato nel flusso di lavoro qui. I tweet sono stati passati attraverso il parser PDTB, e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata solo nel 3,5% delle coppie annotate. Raccogliendo circa 1.000 esempi di coppie di unità di discorso, abbiamo eseguito la formazione per un classificatore iniziale addestrato solo su 43 esempi di dissonanza. Non sorprendentemente, il classificatore ha prestato non molto meglio della casualità. Dato l'occorrenza bassa di dissonanza e l'assenza di qualsiasi set di dati precedente, ci troviamo di fronte al problema di assoluta rarità. Per alleviare questo, abbiamo sperimentato su combinazioni di transfer learning e active learning per annotare in modo che più esempi dissonanti possano essere raccolti in meno esecuzioni di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, iniziamo il processo di active learning trasferendo pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione dello stance di dissonanza indipendente dal tema, un compito che determina se due affermazioni di dibattito di persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria delle classi di espansione e confronto di PDTB poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo CE qui. Troviamo che, trasferendo, la performance zero-shot sul set di dati annotato è già molto migliore della casualità, con la migliore AUC di 0,62. Inoltre, affinando iterativamente su entrambi i compiti, troviamo che l'affinamento del compito CE seguito da un ulteriore affinamento sul dibattito fornisce una performance zero-shot molto migliore. Pertanto, questo è il modello che usiamo per avviare l'active learning. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di active learning e annotazioni. "Cumulativo" accumula tutti i dati raccolti dall'active annotation finora, mentre "Iterativo" aggiorna il modello addestrandolo sui dati più recenti raccolti. Tra le diverse strategie, troviamo che Cumulativo ha prestato uguale o meglio di Iterativo in generale. Successivamente, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di Probabilità-di-Classe-Rara — PRC — per selezionare principalmente gli esempi che sono altamente probabili di essere classificati come dissonanti dal modello attuale in ogni round di rara. Confrontiamo questo con altre strategie di AL state-of-the-art comunemente utilizzate nella comunità. Troviamo che la strategia PRC proposta funziona meglio delle altre strategie state-of-the-art, anche se la differenza è piccola. Nota che la performance è significativamente più bassa per casuale. In ulteriori round di AL con le due migliori strategie, miglioriamo la classificazione AUC della dissonanza a 0,75, che è la migliore performance che abbiamo finora su questo compito. Verifichiamo anche la fattibilità di ciascuna strategia per la qualità dell'annotazione e i costi per gli annotatori. Troviamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili. In sintesi, troviamo che PRC è una semplice strategia di AL per l'acquisizione di classi rare e l'avvio dell'active learning con compiti di transfer learning adeguatamente progettati e aiuta significativamente. Troviamo anche che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre le annotazioni active nel dominio traggono beneficio dall'aggiornamento cumulativo. Questi sono i link al nostro core data set e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="356">Matthias Lindemann, Alexander Koller, e Ivan Titov.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Sono coinvolti cinque autori nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio EDAtt viene confrontato con l'architettura di stato dell'arte specificamente progettata per la traduzione simultanea, oltre alle strategie popolari applicate ai modelli offline come la strategia Wait-k e la Local Agreement.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, presents "CounterComp," a method to enhance compositional generalization in multi-step quantitative reasoning for question answering tasks. The focus is on improving neural models' performance on tasks involving financial tables, where questions require multiple arithmetic operations. Current models struggle with these tasks due to memorizing spurious patterns, such as associating specific tokens with operations. CounterComp addresses this by mining counterfactual scenarios from training data, creating positive and negative examples based on interventions in questions. These examples are used to introduce an auxiliary metric learning loss with a dynamic margin, which adjusts based on the extent of question changes. This approach consistently improves model performance on both in-distribution and out-of-distribution samples, particularly when reasoning steps exceed two. The method also helps models focus on meaningful tokens related to operational terms in the output. The presentation highlights the effectiveness of CounterComp in achieving compositional generalization and encourages further exploration through the provided poster and contact information.</sample>
  </task>
</testset>