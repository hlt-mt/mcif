<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Web-Crawl-Daten, die politische Nachrichtenmedien wie The New York Times, Los Angeles Times, The Guardian und Huffington Post umfassen.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">Dieses Papier präsentiert LayoutMask, ein neuartiges Modell zur visuell reichen Dokumentenverständnis (VrDU), entwickelt von Ant Group. LayoutMask zielt darauf ab, die Herausforderungen bestehender Modelle bei der globalen Lesereihenfolge zu überwinden, indem es lokale 1D-Positionen, innovative Maskierungsstrategien und neue Pre-Training-Ziele einführt. Im Gegensatz zu globalen 1D-Positionen verwendet LayoutMask lokale 1D-Positionen, um die Lesereihenfolge durch die Kombination von 1D- und 2D-Positionen sowie semantischen Informationen zu inferieren, was tiefere Text-Layout-Interaktionen fördert. Die Einführung von Whole Word Masking und Layout-Aware Masking in die Masked Language Modeling-Aufgabe erhöht die Komplexität, indem sie die semantischen Beziehungen zwischen Maskierung und Kontext stärkt. Ein neues Pre-Training-Ziel, Masked Position Modeling, zielt darauf ab, 2D-Positionen zu rekonstruieren, was die semantische und räumliche Inferenz fördert. Experimente zeigen, dass LayoutMask mit lokalen 1D-Positionen auf den FUNSD- und SROIE-Datensätzen überlegen ist, obwohl es auf CORD leicht hinterherhinkt. Diese Ergebnisse deuten darauf hin, dass lokale 1D-Positionen besser an komplexe Layouts angepasst sind, insbesondere bei der Erkennung von Entitäten wie "Total".</sample>
    <sample id="3">Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Identifizierung deutscher Texte auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textvereinfachung definieren. Textvereinfachung ist ein Prozess, der darauf abzielt, ein Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseproblemen oder Nicht-Muttersprachler. Um ein Textvereinfachungsmodell zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. Hier sehen Sie ein parallel ausgerichtetes Satzpaar eines komplexen deutschen Satzes und seiner Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie lexikalische Substitution, Klausel-Löschung, Umordnung oder Wort-Einfügung. Wir schlagen nun unseren neuen Korpus DEPLAIN vor, da es in den letzten Jahren Probleme mit bestehenden Korpora gab. Zum Beispiel sind diese Korpora zu klein, um ein Textvereinfachungsmodell darauf zu trainieren. Die anderen drei Modelle, die in den letzten Jahren vorgeschlagen wurden, sind alle automatisch ausgerichtet, was bedeutet, dass sie fehleranfällig in ihren Ausrichtungen sein können. Daher schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Subkorpora unterteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet, was etwa 13.000 parallele Satzpaare ergibt. DEPLAIN-web umfasst verschiedene Domänen, und wir haben auch alle 750 Dokumente ausgerichtet, sowohl manuell als auch mit automatischen Ausrichtungsmethoden. Insgesamt ergibt dies 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel hinsichtlich des Typs der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Texte für Sprachlerner. Auf allen Ebenen, wie lexikalische Vereinfachung, strukturelle Vereinfachung und insgesamt Vereinfachung. Darüber hinaus zeigt unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen. Zum Beispiel haben wir im DEPLAIN-apa-Korpus viel mehr Umordnungen und Wortzusätze als im DEPLAIN-web-Korpus. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar, und ich werde jetzt über die Verwendungsfälle für unseren Datensatz DEPLAIN sprechen. Als ersten Verwendungszweck können wir automatische Ausrichtungsmethoden bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzung, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und Ausrichtungen von Sätzen in beiden Dokumenten extrahieren möchten. In unserem Verwendungszweck versuchen wir jedoch, Ausrichtungen zwischen Sätzen von zwei parallelen Dokumenten mit derselben Sprache, aber unterschiedlichem Komplexitätsniveau, zu extrahieren. Da wir nun unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen sowie die Codes, um unsere Experimente durchzuführen, in der Publikation veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textvereinfachung die Methode MASSalign ist. Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, in der Publikation. Der zweite Verwendungszweck, den wir in unserer Publikation gezeigt haben, ist der Fall der automatischen Textvereinfachung durch Feinabstimmung von Sprachmodellen, um aus komplexem Eingabetext vereinfachten Text zu erzeugen. Wir haben zwei verschiedene Modelle feinabgestimmt. Wir haben das Modell long-mBART feinabgestimmt, um Dokumentenebene-Vereinfachungen zu erzeugen, und wir haben auch das normale Basis mBART feinabgestimmt, um Satzebene-Vereinfachungen zu erzeugen. Sie können auch alle Checkpoints finden und sich detaillierter mit den Ergebnissen und den Bewertungsmetriken unserer Experimente in der Publikation befassen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Ergebnisse als die Baseline-Ergebnisse erzielen konnte, und wir haben diese Ergebnisse als Basisbenchmark für das Problem der automatischen Textvereinfachung in der Zukunft vorgeschlagen. Vielen Dank für Ihre Aufmerksamkeit, und wir hoffen, Sie alle während der Konferenz zu treffen. Danke.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das T5 XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">In diesem Papier präsentieren wir "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", eine Zusammenarbeit, die viele-zu-viele-Zusammenfassung einführt, um multilinguale und cross-linguale Zusammenfassungsaufgaben zu vereinen. Ziel ist es, ein Modell zu entwickeln, das Dokumente in jeder Quellsprache verarbeiten und in jede Zielsprache zusammenfassen kann. Wir führen PISCES ein, ein vortrainiertes viele-zu-viele-Zusammenfassungsmodell, das durch eine dreistufige Vortrainingsphase Sprachmodellierung, cross-linguale Fähigkeiten und Zusammenfassungsfähigkeiten erlernt. Unsere Experimente auf der WikiLingua-Datenbank zeigen, dass viele-zu-viele-Zusammenfassung die Wissensübertragung zwischen Sprachen effektiver unterstützt als herkömmliche Ansätze. PISCES übertrifft verschiedene Baselines, einschließlich mBART-50 und mT5, und bestätigt die Wirksamkeit jedes Trainingsstadiums durch Ablationsstudien und menschliche Studien.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch im Jahr 2023.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem sie explizit annotiert, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z.B. das Äußern irrelevanter Informationen oder das Widersprechen sich selbst. Diese Methode zielt darauf ab, die Zuverlässigkeit und Präzision der Bewertung von Dialogqualität zu verbessern, indem sie spezifische thematische Fehler misst.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Verfügbarkeit von sauberen Validierungsdaten ab. Ohne saubere Validierungsdaten gibt es einen großen Leistungsabfall, und die Modelle können nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Zugänglichkeit und Qualität der Hintergrundinformationen für die Sprachmodelle erhöht wird. Dies könnte durch die Integration umfassenderer und relevanterer Kontextinformationen geschehen, die den Annotatoren zur Verfügung stehen, oder durch die Verbesserung der Fähigkeit der Modelle, relevante Informationen aus externen Quellen effektiv abzurufen und zu nutzen.</sample>
    <sample id="11">Der Vortrag von Jack Hessel präsentiert die Forschung "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest", eine Zusammenarbeit mit Institutionen wie der University of Utah, Cornell University und OpenAI. Die Studie untersucht, ob große Sprachmodelle wie ChatGPT und Google's PaLM tatsächlich Humor verstehen können. Obwohl diese Modelle in der Lage sind, Witze zu generieren und zu erklären, zeigt die Forschung, dass ihre Fähigkeit, Humor zu verstehen, begrenzt ist. Die Studie nutzt Daten aus The New Yorker Caption Contest, um drei Aufgaben zu operationalisieren: Matching, Qualitätseinschätzung und Erklärungsgenerierung von Witzen. Die Ergebnisse zeigen, dass selbst mit zusätzlichen Beschreibungen von Bildern Sprachmodelle wie GPT-4 hinter menschlicher Leistung zurückbleiben. Die Forschung hebt die Herausforderungen hervor, die Sprachmodelle bei der Verarbeitung von Humor haben, und bietet einen Datensatz und eine Leaderboard-Plattform für weitere Untersuchungen an.</sample>
    <sample id="12">Fünf Autoren sind an der Arbeit beteiligt: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">In der Präsentation "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" von Daniel Rotem wird die adaptive Inferenz in großen Sprachmodellen untersucht, um die Inference-Zeit zu reduzieren. Die Studie vergleicht zwei gängige Methoden: Multi Model und Early Exit. Multi Model verwendet mehrere Modelle mit Klassifikatoren, die sequentiell laufen, während Early Exit mehrere Klassifikatoren in einem Modell integriert, um die Berechnung frühzeitig zu beenden. Multi Model ist flexibel, aber speicherintensiv und hat Overhead, während Early Exit schneller und speichereffizienter ist, jedoch unter Leistungsproblemen durch konfligierende Gradienten leidet. Diese Gradienten entstehen, wenn Klassifikatoren die Modellgewichte unterschiedlich aktualisieren, was die Gesamtleistung beeinträchtigt. Die Studie zeigt, dass Multi Model-Klassifikatoren im Durchschnitt um 2,3% besser abschneiden als Early Exit-Klassifikatoren. Um dieses Problem zu lösen, wird die SWEET-Methode vorgestellt, die konfligierende Gradienten vermeidet, indem jedes Schicht nur von dem folgenden Klassifikator aktualisiert wird. SWEET schließt den Leistungsunterschied zwischen Early Exit und Multi Model für die meisten Klassifikatoren, obwohl spätere Klassifikatoren manchmal negativ beeinflusst werden. In Bezug auf die Geschwindigkeit-Leistungsbilanz übertrifft SWEET beide Methoden bei hohen Geschwindigkeiten und zeigt insbesondere bei BERT-Large über den gesamten Geschwindigkeitsbereich eine überlegene Leistung. Die Arbeit zeigt die Existenz konfligierender Gradienten auf und führt SWEET als eine vielversprechende Methode für zukünftige Forschung ein.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieses Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von unterschiedlichen Theorien und Korpusansätzen angenommen werden. Zum Beispiel nimmt die Universal Dependencies an, dass in der Koordination „Lisa, Bart und Maggie“ der erste Konjunkt die gesamte koordinierte Struktur anführt, also „Lisa“. Ein ähnlicher Ansatz wird in Igor Mel'čuks Meaning-Text-Theorie angenommen, wo ebenfalls der erste Konjunkt die gesamte koordinierte Struktur anführt. Diese beiden Ansätze sind asymmetrisch, da sie einen der Konjunkte hervorheben. Im Gegensatz dazu gibt es den Prager Ansatz, bei dem koordinierte Strukturen von der Konjunktion angeführt werden, was zu Abhängigkeiten von der Konjunktion zu allen Konjunkten führt. Schließlich gibt es auch einen mehrfach-geführten Ansatz, wie in Hudsons Word Grammar, wo alle Konjunkte als Köpfe der koordinierten Struktur angesehen werden, was zu Abhängigkeiten von dem Regens zu jedem Konjunkt separat führt. Das Ziel dieses Papiers ist es, ein neues Argument für symmetrische Strukturen der Koordination zu liefern und gegen asymmetrische Strukturen zu argumentieren. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand von Beispielen erläutern werde. In Englisch bevorzugen direkte Objekte, nahe am Verb zu stehen, während Adjunkte weiter entfernt sein können. „Marge las es gestern“ ist in Ordnung, weil das direkte Objekt nahe am Verb steht, während „Marge las gestern es“ viel schlechter klingt, da zwischen Verb und direktem Objekt ein Adjunkt „gestern“ steht. Diese Wirkung kann jedoch gemildert werden, wenn das direkte Objekt sehr lang und umfangreich ist, sodass es nach dem Adjunkt verschoben werden kann. Beide Sätze sind in Ordnung: „Marge las dieses absolut faszinierende Buch über Bienen gestern.“ Es ist in Ordnung, anstelle von „es“ dieses lange NP zu haben. Es ist auch in Ordnung zu sagen: „Marge las gestern dieses absolut faszinierende Buch über Bienen.“ Die Überlegung hier ist, dass dies möglich ist, weil dieser Satz zwar das allgemeine grammatische Prinzip verletzt, dass direkte Objekte nahe am Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Wir haben verschiedene Statistiken über Koordination aus der erweiterten Version des Penn Treebanks extrahiert und in dem Papier „Warum man nicht die Universal Dependencies verwenden würde“ bestätigen diese Statistiken die Beobachtung, die oft gemacht wurde, dass linke Konjunkte tendenziell kürzer sind. Zum Beispiel „Salz und Pfeffer“ und nicht „Pfeffer und Salz“, gemessen in Silben. Auch die Beobachtung, die in der Parsing gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn die Längendifferenz zwischen den beiden Konjunkten wächst, bevorzugt der kürzere Konjunkt stärker, die erste Position. Aber das Neue in diesem Papier ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regens links ist oder fehlt. Der Regens ist links in diesem Beispiel „Ich sah Bart und Lisa“ und fehlt im zweiten Beispiel „Homer kam und nieste.“ Hier haben wir die Koordination von zwei Verben und es gibt keinen externen Regens. In solchen Fällen bevorzugt der linke Konjunkt, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wenn der Regens jedoch rechts ist, wie hier, „lachte“ regiert die Koordination von Ted und Ned, verschwindet diese Wirkung. Wir haben gezeigt, dass durch Messung der Länge in Zeichen, der Mitte in Silben und rechts in Wörtern. Ich werde mich auf die rechte konzentrieren. Was wir sehen, ist, dass wenn der Regens links ist, die Tendenz für den linken Konjunkt, kürzer zu sein, mit der absoluten Differenz in Wörtern wächst. Das Gleiche wird beobachtet, wenn kein Regens vorhanden ist, wie bei der Koordination von Sätzen. Wenn der Regens jedoch rechts ist, verschwindet diese Tendenz. Und wir zeigen im Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination und für symmetrische Strukturen liefert. Siehe das Papier für die vollständigen Argumente und sprechen Sie uns auf der Poster-Sitzung an. Vielen Dank.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Bibeltexte werden stärker vereinfacht als Nachrichtentexte oder Texte für Sprachlerner.</sample>
    <sample id="17">Dieses Papier präsentiert eine innovative Methode zur multimodalen Beziehungsextraktion, die sowohl textuelle als auch visuelle Informationen nutzt, um semantische Beziehungen zwischen Entitäten zu bestimmen. In Szenarien wie sozialen Medien, wo Daten in verschiedenen Formen vorliegen, kann reiner Text oft unzureichend sein, um Ambiguitäten zu klären. Die vorgeschlagene Methode adressiert zwei Hauptprobleme: die Übernutzung interner Informationen und die Unterbewertung externer Informationen. Durch die Anwendung des Graph Information Bottleneck Prinzips wird eine feingranulare Informationsauswahl über zwei Modalitäten erreicht, um redundante Informationen zu reduzieren. Zusätzlich wird externe Information durch multimodale Themeninformationen ergänzt, um den Kontext zu bereichern. Die Methode umfasst die Darstellung von Text und Bild als visuelle und textuelle Szenengraphen, die zu einem einheitlichen multimodalen Graphen (CMG) verschmolzen werden. Dieser wird durch feingranulare Filterung und Anpassung optimiert. Die CMG-Features werden durch multimodale Themenfeatures angereichert, die durch eine Aufmerksamkeitsoperation integriert werden. Experimente auf einem weit verbreiteten MRE-Datensatz zeigen, dass die Methode die bestehenden Modelle übertrifft. Die Analyse zeigt, dass die Informationsauswahl bei hohen Text-Bild-Relevanzwerten und die Informationsergänzung bei niedrigen Relevanzwerten besonders vorteilhaft sind. Insgesamt bietet die Methode eine signifikante Verbesserung bei der multimodalen Beziehungsextraktion.</sample>
    <sample id="18">Das Beispiel für die Präferenz für kürzere linke Konjunktionen ist "salt and pepper" anstelle von "pepper and salt".</sample>
    <sample id="19">Der Beitrag "A Survey for Efficient Open Domain Question Answering" von Zhang Qin, einem Masterstudenten der Shenzhen University, präsentiert eine Übersicht über die Herausforderungen und Lösungen im Bereich des effizienten offenen Fragestellungsbeantwortens (Open Domain Question Answering, ODQA). Die Arbeit konzentriert sich auf die Optimierung von ODQA-Systemen, um geringere Speicheranforderungen, schnellere Inferenzzeiten und vergleichbare Leistung zu erreichen. Die traditionelle Zwei-Stufen-Methode, bestehend aus einem Retrieval- und einem Reader-Modul, wird diskutiert, wobei die Herausforderungen durch die große Größe des Wikipedia-Korpus und die damit verbundenen Indexierungs- und Suchprobleme hervorgehoben werden. Der Beitrag untersucht alternative Ansätze wie One-Stage-Systeme, einschließlich Retrieval-only- und Generator-only-Systemen, und diskutiert Techniken zur Beschleunigung der Beweissuche, zur Reduzierung der Indexgröße und zur Modellverkleinerung. Die Analyse zeigt, dass Retrieval- und Reader-Systeme einen guten Kompromiss zwischen Geschwindigkeit, Speicherbedarf und Leistung bieten, während Retrieval-only-Systeme schnelle Inferenzen und Generator-only-Systeme keine Indexierung, aber größere Modelle aufweisen. Schlussfolgerungen umfassen Strategien zur Ressourcenoptimierung, wie die Verkleinerung von Indexen oder Modellen durch Techniken wie Wissensverdichtung oder die Entwicklung von One-Stage-Modellen. Zukünftige Arbeiten könnten sich auf die Implementierung von ODQA-Systemen auf Geräten mit geringer Leistung und die Berücksichtigung zusätzlicher Evaluationsmetriken konzentrieren.</sample>
    <sample id="20">Ja, die Modelle sind frei verfügbar auf Hugging Face unter der MIT-Lizenz, und die Trainings-Skripte sind auf dem GitHub-Repository des Teams verfügbar. Sie können sie für Ihre Forschung verwenden.</sample>
    <sample id="21">DEPLAIN-apa enthält hauptsächlich nachrichtenbasierte Texte.</sample>
    <sample id="22">Für eine gute Generalisierung sind drei Hauptfaktoren erforderlich: die Modellarchitektur (insbesondere Transformer-Modelle), die Modellgröße (größere Modelle führen zu besserer Generalisierung) und die Anzahl der Feinabstimmungsbeispiele (mehr Beispiele führen zu besserer Generalisierung).</sample>
    <sample id="23">Die Forschung von Dan Garrette konzentriert sich auf die Verbesserung der Fähigkeit von Text-Bild-Modellen, visuellen Text darzustellen. Obwohl Modelle wie Imagen hochwertige Bilder generieren können, haben sie Schwierigkeiten, Text korrekt darzustellen. Dies liegt an der Verwendung von T5-XXL-Encodern mit SentencePiece-Tokenisierung, die Subwort-IDs anstelle von einzelnen Buchstaben verwendet. Dies führt zu einer schlechten Schreibfähigkeit, selbst bei größeren Modellen. Im Vergleich dazu schneiden PaLM-Modelle besser ab, sind jedoch aufgrund ihrer Größe und des benötigten Datenvolumens weniger praktikabel. ByT5, das einzelne Bytes verwendet, zeigt eine hohe Schreibgenauigkeit, da es direkten Zugriff auf Buchstabeninformationen hat. Die Forschung zeigt, dass häufige Wörter für T5 schwieriger zu schreiben sind, da sie durch wenige Subwörter repräsentiert werden. Durch die Erweiterung des Imagen-Modells mit einer zusätzlichen Textrepräsentation aus ByT5-small verbessert sich die Schreibfähigkeit und die Bildgenerierung. Die Hauptergebnisse umfassen die Einführung der WikiSpell- und DrawText-Benchmarks sowie eine effiziente Strategie zur Verbesserung der Schreibfähigkeit durch die Verwendung von charakterbewussten Modellen.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde in der Studie durch die Messung der Länge in Zeichen, Silben und Wörtern gemessen. Der Fokus lag dabei auf der Messung in Wörtern.</sample>
    <sample id="25">Die Experimente wurden gestaltet, indem Statistiken über Koordinationen aus der erweiterten Version des Penn Treebanks extrahiert wurden. Es wurde beobachtet, dass die Tendenz, dass der linke Konjunkt kürzer ist, nur auftritt, wenn der Begrenzer links oder abwesend ist. Diese Beobachtung wurde durch die Messung der Länge in Zeichen, Silben und Wörtern bestätigt. Es wurde festgestellt, dass diese Tendenz mit der absoluten Längendifferenz in Wörtern wächst, wenn der Begrenzer links oder abwesend ist, aber verschwindet, wenn der Begrenzer rechts ist.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, insbesondere mit nur 43 Beispielen für die Dissonanzklasse, performt nicht viel besser als zufällig.</sample>
    <sample id="27">Der Inhalt gibt keine spezifische Anzahl von Autoren an, die an der Arbeit beteiligt sind.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostische Modelle bei den Diskursphänomenen Formulierungsstil und lexikalische Kohäsion.</sample>
    <sample id="30">Der Artikel stellt "LLM-Blender" vor, ein Ensemble-Lernframework für große Sprachmodelle, entwickelt von einem Team von AI2 und USC. Die Methode basiert auf paarweiser Rangfolge und generativer Fusion, um die Leistung von Sprachmodellen zu verbessern. Obwohl einige Modelle wie Vicuna in Bezug auf die durchschnittliche Gesamtleistung führend sind, zeigt die Forschung, dass die optimale Modellauswahl je nach Eingabebeispiel variiert. LLM-Blender verwendet ein zweistufiges Framework: Zuerst generieren n verschiedene Modelle Ausgaben für eine Eingabe X, die dann von einem PairRanker verglichen werden, um eine Rangfolge zu erstellen. Dieser Prozess nutzt eine Cross-Attention-Modul, um die Unterschiede zwischen Kandidatenpaaren zu analysieren. Die besten Kandidaten werden dann von einem sequenz-zu-sequenz-Modell für die finale Ausgabe fusioniert. Die Forschung zeigt, dass LLM-Blender die Leistung bestehender Modelle wie Open Assistant und Vicuna übertrifft, indem es in 68% bzw. 76% der Fälle bessere Ergebnisse liefert. Das MixInstruct-Dataset wurde entwickelt, um die Leistung von Ensemble-Lernframeworks zu bewerten, und die Ergebnisse unterstreichen die Effektivität von LLM-Blender. Der Artikel betont die Einfachheit und Effektivität von LLM-Blender und stellt eine einheitliche Codebasis für zukünftige Forschung bereit.</sample>
    <sample id="31">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="33">Das Framework NLPositionality quantifiziert die Positionalität, indem es Datensätze mit diversen Annotatoren neu annotiert und die Annotierungen nach demografischen Merkmalen vergleicht. Es verwendet den Pearson's R Korrelationskoeffizienten, um die Übereinstimmung zwischen den Annotierungen der Anwender und den Vorhersagen sowie den Labels der Modelle und Datensätze zu messen. Dadurch wird die Positionalität der Modelle und Datensätze im Vergleich zu den Endnutzern analysiert.</sample>
    <sample id="34">Marcos Treviso präsentiert "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation", eine Zusammenarbeit mit Alexis Ross, Nuno Guerreiro und André Martins. CREST kombiniert selektive Rationalisierung und Gegenfaktengenerierung, um die Stärken beider Methoden zu nutzen. Die Rationalisierung hebt bedeutungsvolle Tokens hervor, während die Gegenfaktengenerierung durch gezielte Bearbeitung des Inputs menschliches kausales Denken nachahmt. CREST generiert Gegenfaktoren, indem es rationale Teile des Inputs maskiert und mit einem Goldlabel voranstellt, um sie einem Editor (einem Masked Language Model) zu übergeben, der neue Tokens einfügt. Die Qualität der Gegenfaktoren wird durch menschliche Bewertungen auf Validität und Natürlichkeit überprüft, wobei CREST gegenüber MiCE überlegen ist. CREST wird auch für Datenanreicherung und Rationalisierung mit Fakten und Gegenfakten eingesetzt, wobei ein Regularisierungsterm die Ähnlichkeit der neuen Rationalisierungen zu den ursprünglichen fördert. Experimente zeigen, dass CREST-Rationalisierung auf IMDB und in kontrastiven und out-of-domain Datensätzen überlegen ist. Die Rationalisierungen von CREST sind plausibel und simulierbar, insbesondere in Bezug auf Gegenfaktensimulierbarkeit, was ihre Fähigkeit zeigt, die Entscheidungen eines Klassifikators durch kontrastive Bearbeitungen zu ändern. CREST liefert somit valide, flüssige und vielfältige Gegenfaktoren, die zu plausiblen Erklärungen führen, die sich auf die kontrastierenden Teile des Inputs konzentrieren.</sample>
    <sample id="36">Dieses Papier präsentiert "Learning Language-Specific Layers for Multilingual Machine Translation", eine Zusammenarbeit, die sich mit der Herausforderung der begrenzten Kapazität pro Sprache in multilingualen Übersetzungsmodellen befasst. Die Autoren schlagen Language-Specific Layers (LSLs) vor, um die Kapazität gezielt zu erhöhen, ohne die Inferenzkosten zu steigern. Jede Sprache verfügt über einen eigenen Transformer-Sublayer, der während der Inferenz aktiviert wird, um die Effizienz zu gewährleisten. Die Platzierung der LSLs erfolgt durch ein Training mit geteilten, Quell- und Zielsprachengewichten, um die optimale Architektur zu ermitteln. Die Ergebnisse zeigen, dass die LSLs die Leistung signifikant verbessern, insbesondere bei niedrig-resourcigen Sprachen, und die Inferenzgeschwindigkeit erhöhen. Die Experimente basieren auf WMT21-Daten und zeigen Verbesserungen gegenüber Baseline-Modellen und Sprachadaptern. Die Ergebnisse sind statistisch signifikant für die meisten Sprachrichtungen.</sample>
    <sample id="37">Die vorherige Studie, bei der menschlichen Teilnehmenden die gleichen Persona-Prompts gegeben wurden, zeigte, dass auch menschliche Teilnehmende rassistische Stereotype aufdecken konnten. Dies ermöglichte einen direkten Vergleich zwischen den von den Modellen generierten Personas und den von Menschen verfassten Antworten.</sample>
    <sample id="38">Die Studie verwendete Daten aus der erweiterten Version des Penn Treebank.</sample>
    <sample id="39">Die Arbeit wird von einem Autor, Adam Przepiórkowski, präsentiert.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die "Debatte" (ein Thema-unabhängiger Dissonanz-Standpunkt-Klassifikationsaufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder im Widerspruch stehen) und die binäre Klassifizierung der "CE"-Klassen (Expansion und Vergleich) des PDTB, da diese eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind.</sample>
    <sample id="41">Dieses Papier präsentiert PeaCoK, ein Persona-grounded Commonsense Knowledge Graph, entwickelt von der Natural Language Processing Lab an der EPFL University in Zusammenarbeit mit Sony Group Corporation. PeaCoK zielt darauf ab, kohärente und ansprechende Erzählungen zu unterstützen, indem es reichhaltiges Wissen über Persönlichkeiten und deren Verbindungen darstellt. Der Graph umfasst etwa 3.800 Persönlichkeiten mit 40.000 einzigartigen Attributen, die 100.000 persönliche Schlussfolgerungen oder Fakten bilden. Die Persönlichkeiten sind durch etwa 9.200 Attribute miteinander verbunden, was zu einer reichen Vernetzung führt. PeaCoK wurde in drei Schritten entwickelt: Auswahl von Persönlichkeiten aus bestehenden Wissensgraphen, Induktion von Attributen aus Wissensgraphen und Sprachmodellen sowie Crowdsourcing von Relationen mit einem AI-gestützten Mehrheitsabstimmungssystem. Die Qualität der Relationen erreicht eine durchschnittliche F1-Genauigkeit von 87%. PeaCoK wurde verwendet, um ein BART-basiertes Wissensgenerierungsmodell zu trainieren, das in automatischen und menschlichen Bewertungen besser abschneidet als große Sprachmodelle wie GPT-3. In einem Dialoggenerierungsexperiment auf der ConvAI2 PersonaChat-Datenbank verbessert PeaCoK die Dialogqualität in Bezug auf Flüssigkeit, Konsistenz und Engagement. Die Ergebnisse zeigen, dass PeaCoKs persona-zentriertes Wissen die Erzählmodellierung effektiver unterstützt als allgemeines soziales Wissen. PeaCoK dient als zuverlässige Wissensbasis für die Generierung von Persönlichkeitswissen und die Verbesserung der Erzählkonsistenz und -ansprechbarkeit.</sample>
    <sample id="42">Der Inhalt gibt keine Informationen über die Anzahl der Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="43">Der Inhalt gibt keine spezifische Anzahl von Autoren an.</sample>
    <sample id="44">Das vorgestellte Framework NLPositionality unterscheidet sich von bisherigen Arbeiten dadurch, dass es die Annotationsdaten von realen Nutzern mit bestehenden Datensätzen und Modellen vergleicht, anstatt sich nur auf Annotator-Abstimmungen oder die Modellierung von Annotator-Verteilungen zu konzentrieren. Es verwendet Pearson's R Korrelationskoeffizienten, um die Übereinstimmung zwischen den Annotationsdaten nach demografischen Merkmalen und den Modellen/Datensätzen zu bewerten. Im Gegensatz zur Literatur über Annotator-Abweichungen vergleicht es Endnutzer mit Modellen und Datensätzen, indem es Vorhersagen und Labels analysiert. Zudem wird es durch die Verwendung von Lab in the Wild ermöglicht, um eine vielfältige Gruppe von Annotatoren zu rekrutieren, was eine breitere demografische Abdeckung als Plattformen wie Mechanical Turk bietet.</sample>
    <sample id="45">Die generierten Personas enthalten mehr Überschneidungen mit dem Lexikon der Stereotypen als die von Menschen verfassten Personas.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit "Von Vortraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Die Spuren politischer Voreingenommenheit verfolgen, die zu unfairen NLP-Modellen führen". Sprachmodelle werden mit groß angelegten Web-Crawl-Daten trainiert. Politische Nachrichtenmedien sind gut in diesen Vortraining-Daten abgedeckt. Laut einer Umfrage des C4-Korpus sind die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. gut in den Sprachmodell-Trainingsdaten vertreten. Dies hat eine Art zweischneidiges Schwert für Sprachmodell-Anwendungen geschaffen. Einerseits konnten sie von vielfältigen Perspektiven lernen, was Demokratie und Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen inhärent sozial voreingenommen und könnten potenzielle Fairnessprobleme in Downstream-Aufgabenanwendungen verursachen. Dazu schlagen wir vor, den Propagationsprozess politischer Voreingenommenheit von Vortraining-Daten zu Sprachmodellen zu Downstream-Aufgaben zu untersuchen, indem wir folgende Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielen Vortraining-Daten bei solchen politischen Voreingenommenheiten? Zweitens, wie schneiden Sprachmodelle mit unterschiedlichen politischen Ausrichtungen tatsächlich in Downstream-Aufgaben ab und könnten dadurch Fairnessprobleme in NLP-Anwendungen entstehen? Speziell schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu verwenden, die politische Fragebögen wie den politischen Konferenztest nutzen. Dies ermöglicht eine automatische Bewertung, die gut in der politikwissenschaftlichen Literatur verankert ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten auf dem politischen Campus. Wir sehen auch, dass GPT-4 das liberalste Sprachmodell von ihnen allen ist, und die GPT-Serie ist im Allgemeinen sozial liberaler als die BART-Serie und ihre Varianten. Zweitens zielen wir darauf ab, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir können ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedenen parteiischen Korpora vortrainieren, die in Nachrichten und soziale Medien unterteilt sind, weiter unterteilt nach ihrer politischen Ausrichtung. Durch das Weitertrainieren von Sprachmodellen auf solchen parteiischen Korpora sehen wir, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschoben werden. Zum Beispiel zeigt RoBERTa, das weiter auf dem linken Reddit-Korpus trainiert wurde, einen erheblichen liberalen Verschiebung in Bezug auf seine politischen Voreingenommenheiten. Wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung aufnehmen können, die in unserer modernen Gesellschaft vorherrscht. Wir teilen die Vortraining-Korpora in die Zeit vor und nach dem 45. Präsidenten der Vereinigten Staaten auf. Wir trainieren Sprachmodelle separat auf den beiden unterschiedlichen zeitlichen Korpora. Wir sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung haben, die weiter vom Zentrum entfernt ist, nach 2017. Dies zeigt, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Schließlich bewerten wir Sprachmodelle mit unterschiedlichen politischen Ausrichtungen bei der Erkennung von Hassrede und Falschmeldungen, NLP-Anwendungen, die oft Sprachmodelle einbeziehen und erhebliche Auswirkungen haben können. Wir sehen, dass, wenn wir die Leistung pro Kategorie untersuchen, also die Leistung in verschiedene Demografien oder politische Ausrichtungen von Nachrichtenmedien aufteilen, ein Muster erkennbar ist. Zum Beispiel sind bei der Erkennung von Hassrede linkslastige Sprachmodelle besser darin, Hassrede gegen sozial benachteiligte Gruppen zu erkennen, sind jedoch schlechter darin, Hassrede gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Umgekehrt sind rechtslastige Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu erkennen, jedoch schlechter darin, Hassrede gegen Schwarze, LGBTQ+-Personen und andere Minderheitengruppen zu erkennen. Ähnliche Trends treten auch bei der Erkennung von Falschmeldungen auf, wo wir sehen, dass linkslastige Sprachmodelle besser darin sind, Fehlinformationen von ihrer gegensätzlichen politischen Ausrichtung zu erkennen und umgekehrt. Wir zeigen viele qualitative Beispiele, um zu sehen, dass Sprachmodelle mit unterschiedlichen politischen Ausrichtungen unterschiedliche Vorhersagen zu Hassrede- und Fehlinformationsbeispielen auf der Grundlage ihrer sozialen Kategorien treffen. Es gibt viele weitere Beispiele im Anhang, um dies weiter zu unterstreichen, was darauf hinweist, dass es ein sehr dringendes Fairnessproblem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen gibt. Zum Beispiel, wenn rechtslastige Sprachmodelle für die Erkennung von Hassrede oder Fehlinformationen oder was auch immer weiter trainiert und auf einer beliebten sozialen Medienplattform eingesetzt würden, würde dies bedeuten, dass Menschen mit gegensätzlichen politischen Ansichten marginalisiert werden könnten und Hassrede gegen Minderheitengruppen einfach unkontrolliert laufen könnte. Dies hat bei uns Alarm geschlagen, um die Fairnessprobleme anzuerkennen und zu bewältigen, die durch die politischen Ausrichtungen von Sprachmodellen entstehen. Ein wenig Diskussion. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf politische Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Skylla und Charybdis. Wenn wir politische Meinungen in den Sprachmodell-Trainingsdaten nicht bereinigen, würde die Voreingenommenheit von den Vortraining-Daten zu Sprachmodellen zu Downstream-Aufgaben propagieren und letztendlich Fairnessprobleme verursachen. Wenn wir versuchen, sie auf irgendeine Weise zu bereinigen, riskieren wir auch Zensur oder Ausschluss. Und es ist äußerst schwierig zu bestimmen, was tatsächlich neutral ist und in den Sprachmonitoring-Daten beibehalten werden sollte. Es ist so etwas wie das Elektrische Trolley-Problem. Ok, großartig. Ich denke, das ist im Grunde alles, was ich heute zu sagen habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">Die Anzahl der Autoren wird im Text nicht spezifiziert.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu einer Kontextlänge von 1024 Token durchgeführt.</sample>
    <sample id="50">Die Präsentation stellt DEPLAIN vor, ein neues Korpus für die Identifizierung und Vereinfachung deutscher Texte auf Dokument- und Satzebene. Regina Stodden erläutert, dass Textvereinfachung darauf abzielt, Texte für spezifische Zielgruppen wie Menschen mit Lesebehinderungen oder Nicht-Muttersprachler verständlicher zu machen. Dazu sind parallele Textpaare erforderlich, die durch Techniken wie lexikalische Substitution, Satzverkürzung oder Wortinsertion vereinfacht werden. DEPLAIN, bestehend aus den Subkorpora DEPLAIN-apa (basierend auf Nachrichtentexten) und DEPLAIN-web (verschiedene Domänen), bietet 13.000 und 30.450 parallele Satzpaare, die manuell und teilweise automatisch ausgerichtet wurden. Die Analyse zeigt eine hohe Vielfalt an Vereinfachungstransformationen. Omar präsentiert Anwendungsfälle: die Bewertung automatischer Ausrichtungsmethoden, wobei MASSalign als beste Methode für die deutsche Textvereinfachung identifiziert wurde. Zudem wird die automatische Textvereinfachung durch das Feinabstimmen von Sprachmodellen wie long-mBART und mBART demonstriert, wobei die Ergebnisse als Basisbenchmark für zukünftige Forschungen dienen.</sample>
    <sample id="51">Die Domains, die in den Datensatz aufgenommen wurden, sind Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Dieses Papier präsentiert "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge", das die Erkennung von kognitiver Dissonanz in Sprache untersucht. Kognitive Dissonanz, definiert als Inkonsistenz zwischen Überzeugungen oder Handlungen, ist selten in sprachlichen Daten, stellt jedoch ein wichtiges Forschungsgebiet dar, um Entscheidungsprozesse, gesellschaftliche Trends und psychische Gesundheit zu verstehen. Die Autoren haben eine große Anzahl von Tweets annotiert, wobei nur 3,5% der Paare Dissonanz aufwiesen. Aufgrund der Seltenheit von Dissonanzdaten entwickelten sie eine Methode, die Transfer- und aktives Lernen kombiniert, um die Erkennung zu verbessern. Anfänglich übertrugen sie Gewichte von verwandten Aufgaben, was die Leistung des Modells deutlich verbesserte. Die Strategie "Cumulative" erwies sich als überlegen gegenüber "Iterative" bei der Modellaktualisierung. Die Einführung der "Probability-of-Rare-Class" (PRC)-Strategie führte zu einer signifikanten Verbesserung der Erkennung von Dissonanz, wobei die AUC auf 0,75 stieg. Die Studie zeigt, dass PRC effektiv für die Erkennung seltener Klassen ist, obwohl die Annotatoren die Beispiele als schwierig empfanden. Die Ergebnisse unterstreichen die Bedeutung von Transferlernen und aktiven Lernstrategien zur Verbesserung der Erkennung seltener Klassen in sprachlichen Daten.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, indem es die Aufmerksamkeitsmechanismen des Modells nutzt, ohne es neu zu trainieren oder eine spezifische Architektur für SimulST zu verwenden.</sample>
    <sample id="56">Der Inhalt enthält keine spezifische Information über die Anzahl der Autoren, die an der Arbeit beteiligt sind.</sample>
    <sample id="57">Das getestete Modell funktioniert in der Testsuite nicht gut, ohne spezifische Training auf KITMUS. Mit Training auf KITMUS verbessern sich die Leistungen der Modelle erheblich, aber sie haben immer noch Schwierigkeiten, Wissen zuverlässig zu integrieren, das nur bei der Inferenzzeit bereitgestellt wird.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. **Background-Pretrain**: Hintergrundwissen ist bei der Vortrainierung verfügbar.
2. **Background-Both**: Hintergrundwissen ist sowohl bei der Vortrainierung als auch bei der Inferenzzeit verfügbar.
3. **Background-Inference**: Beide Wissensarten sind nur bei der Inferenzzeit verfügbar.</sample>
    <sample id="59">This presentation introduces DrBERT, the first French biomedical pre-trained model, based on RoBERTa and trained on NACHOS, a dataset of medical web-crawled data. DrBERT addresses the scarcity of specialized models in French for biomedical and clinical domains. The study compares DrBERT with ChuBERT, a model trained on anonymized clinical data from Nantes University Hospital, and evaluates various pre-training strategies using different data volumes. Seven models, including DrBERT and ChuBERT, are compared against six baseline models on 11 French biomedical and clinical tasks. Results show that models perform best on tasks with similar training data, with heterogeneous data sources offering versatility. From-scratch pre-training generally yields higher performance, though control pre-training with CamemBERT weights shows comparable results to DrBERT with limited data. DrBERT outperforms generic models like CamemBERT on nine tasks. The study concludes that specialized data enhances performance but does not scale well. All models and scripts are available on Hugging Face and GitHub under the MIT license.</sample>
    <sample id="60">Die Autoren gehören der Cornell University an.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: Sollten wir die sauberen Proben nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen?</sample>
    <sample id="62">Dieses ACL-Papier, "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training", untersucht die Komprimierung von großen Sprachmodellen für die natürliche Sprachgenerierung (NLG), um die Leistung zu bewahren. Die Autoren, Nitay Calderon, Amir, Subhabrata und Roi, konzentrieren sich auf die Anwendung von Wissensverdünnung, um kleinere Modelle effizienter zu gestalten. Sie untersuchen zwei Hauptmethoden: Wortniveau- und Sequenzniveau-Verdünnung, wobei letztere die Verwendung von Pseudo-Zielen durch den Lehrer umfasst. Im Gegensatz zu früheren Arbeiten, die sich auf Klassifikations- oder NLU-Aufgaben konzentrieren, untersucht diese Studie spezifische NLG-Aufgaben wie Zusammenfassung, Fragegenerierung, allgemeines Wissen und Stiltransfer in realistischen, industriegetriebenen Szenarien. Die Studie verwendet mittelgroße, kommerziell verfügbare Modelle und berücksichtigt die Effizienz der Inferenzzeit. Die Autoren präsentieren acht Stufen der Untersuchung, einschließlich der Erkundung von Architekturentscheidungen und der Auswirkungen von Pruning. Die Hauptbeiträge umfassen die Erweiterung der Pseudo-Zielverwendung, die Bedeutung von ungelabelten Daten und die Einführung einer neuen Technik namens Joint-Teaching, die das Lernen aus Fehlern und die Korrektur von Verzerrungen beim Schüler adressiert. Die Studie zeigt, dass die Generierung mehrerer, vielfältiger Pseudo-Ziele die Leistung des Schülers verbessert.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, konsistente Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine schlechtere Leistung des Modells, da sie anzeigt, dass das Modell inkonsistente Ausgaben für die gleiche Aufgabe produziert, abhängig von leichten Variationen in der Wortwahl der Anweisung. Eine niedrigere Sensitivität ist wünschenswert, da sie auf konsistentere Leistung hinweist.</sample>
    <sample id="66">Dieses Papier untersucht die Anwendung von Deep Learning auf mathematische Problemlösung und Beweisführung, ein zentrales Element der künstlichen Intelligenz und natürlichen Sprachverarbeitung. Es diskutiert die Entwicklung von Methoden, die mathematische Probleme sowohl in textbasierten als auch in multimodalen Kontexten (wie Bildern und Tabellen) lösen können. Zwei Hauptkategorien werden betrachtet: visuelle und tabellarische Kontexte. Geometrische Probleme werden als neuro-symbolische Probleme formalisiert, die Diagramme, Theoreme und Solver umfassen. Automatisierte Beweisführung wird ebenfalls behandelt, wobei die Herausforderung darin besteht, die Wahrheit mathematischer Behauptungen durch sequenzielle Argumentation zu demonstrieren. Verschiedene neuronale Netzwerkarchitekturen, wie sequenz-zu-sequenz- und sequenz-zu-baum-Modelle, werden vorgestellt, um mathematische Ausdrücke zu verarbeiten. Die Leistung von großen Sprachmodellen (LLMs) auf mathematischen Problemen wird ebenfalls untersucht, wobei die Kette des Denkens als Methode zur Verbesserung der Problemlösungsfähigkeit genutzt wird. Trotz ihrer Fortschritte zeigen LLMs Einschränkungen in der präzisen mathematischen Problemlösung, die durch Strategien wie Selbstkonsistenz verbessert werden können. Das Papier hebt auch die Notwendigkeit hervor, mathematische Problemlösung in ressourcenarmen und nicht-englischen Kontexten zu erforschen, und betont die Herausforderungen der Generalisierung und Robustheit in bestehenden Modellen.</sample>
    <sample id="67">Dieses Papier untersucht Interferenzen in multilingualen Übersetzungsmodellen, insbesondere die Faktoren, die zu Interferenzen oder Synergien führen. Es wird festgestellt, dass Interferenzen hauptsächlich bei sehr kleinen Modellen im Vergleich zur Datenmenge auftreten. Die Studie zeigt, dass die Anpassung der Stichproben-Temperatur entscheidend für eine starke Leistung ist. Während die Sprachähnlichkeit und die Anzahl der Sprachen angenommen wurden, um die Leistung zu beeinflussen, zeigt die Forschung, dass diese Faktoren nur geringe Auswirkungen haben. Experimente mit vier Varianten der Transformer-Architektur und 15 Sprachen aus WMT zeigen, dass Interferenzen bei kleinen Modellen und bei geringen Datenmengen für eine Sprache auftreten, aber mit zunehmender Skalierung abnehmen. Die Studie empfiehlt die Verwendung von temperaturgesteuerter Stichprobenziehung, um die Interferenzen zu minimieren, wobei eine kalibrierte Temperatur für optimale Ergebnisse entscheidend ist. Insgesamt wird festgestellt, dass Modell- und Datengröße die Interferenzniveaus beeinflussen, während andere Faktoren wie Sprachähnlichkeit weniger Einfluss haben. Modeste Skalierung und kalibrierte Temperatur können das Problem erheblich reduzieren, ohne auf spezialisierte Methoden zurückgreifen zu müssen.</sample>
    <sample id="68">Der englische Inhalt gibt keine spezifischen Details darüber, welchen linguistischen Kontext die Modelle während des Pre-Trainings erhalten. Er konzentriert sich auf die Auswirkungen von Kontextlängen und -arten auf die Akzeptabilitätsurteile von Sprachmodellen, insbesondere im Rahmen des minimalen Paarparadigmas (MPP).</sample>
    <sample id="69">Typischerweise werden etwa 20 saubere Validierungsbeispiele pro Klasse benötigt, um eine gute Leistung bei der WSL zu erzielen.</sample>
    <sample id="70">Die Autoren gehören der Stanford University an.</sample>
    <sample id="71">In "Resolving Indirect Referring Expressions for Entity Selection," Javad Hosseini and colleagues introduce the AltEntities Corpus to address the challenge of understanding indirect references in conversational systems. The corpus, comprising 6,000 alternative questions across music, books, and recipes, aims to benchmark language models' entity understanding. Using a cartoon completion setup, annotators provide indirect referring expressions to select between two entities, with background knowledge provided to aid comprehension. The study evaluates the T5 XL model's performance, showing high accuracy (92-95%) when given identical background knowledge to annotators, and moderate accuracy (82-87%) with partially overlapping knowledge. However, accuracy drops to 60% with only entity names. The research highlights the importance of context and background information in improving entity selection and demonstrates domain-generalizability of the models.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die politischen Vorurteile in Sprachmodellen zu bewerten und zu verstehen, wie diese Vorurteile von den Trainingsdaten auf die Modelle und schließlich auf die Downstream-Aufgaben übertragen werden. Dies hilft, potenzielle Fairnessprobleme in NLP-Anwendungen zu identifizieren und zu adressieren, insbesondere bei Aufgaben wie der Erkennung von Hassrede und Falschmeldungen, wo unterschiedliche politische Vorurteile zu unterschiedlichen Ergebnissen führen können, die bestimmte Gruppen benachteiligen.</sample>
    <sample id="73">Servin.</sample>
    <sample id="74">Dieses Papier präsentiert Dense-ATOMIC, eine dicht verbundene Erweiterung des ATOMIC-Knowledgespeichers, um die Abdeckung von Wissensinhalten und die Anzahl von Mehrfachpfaden zu erhöhen. ATOMIC, ein groß angelegter Speicher für alltägliches Wissen, enthält hauptsächlich B-to-A-Verbindungen, was zu einer unzureichenden Abdeckung und wenigen Mehrfachpfaden führt. Dense-ATOMIC ergänzt fehlende B-to-B, A-to-B und A-to-A-Verbindungen und ermöglicht Mehrfachpfade, wie z.B. "X fragt Y um die Hand, Y sagt ja, und X lächelt". Der Aufbau von Dense-ATOMIC umfasst die Normalisierung von Ereignissen, das Training eines Beziehungs-Vorhersagemodells und die Konstruktion des erweiterten Graphen. Das vorgeschlagene Rel-CSKGC-Modell nutzt RoBERTa zur Kodierung von Ereignissen und MaxPooling zur Beziehungs-Vorhersage, um die Probleme der spärlichen Graphenstruktur und der unzureichenden Nutzung semantischer Informationen zu überwinden. Ein Intra- und Inter-Cluster-Vervollständigungsstrategie wird eingesetzt, um die Effizienz zu steigern. Evaluierungen zeigen, dass Dense-ATOMIC eine höhere Wissensabdeckung und verbesserte Leistung von COMET bietet, insbesondere bei der Generierung diversifizierter Ergebnisse und der Bewertung von Mehrfachpfaden. Das Papier demonstriert die Vorteile von Dense-ATOMIC für die Wissensabdeckung und die Potenziale für das alltägliche Schlussfolgern.</sample>
    <sample id="75">Der Beitrag stellt Jointprop vor, ein von Zheng Yandan, Hao Anran und Luu Anh Tuan entwickeltes Framework für die semi-supervised joint Entity Recognition (NER) und Relation Extraction (RE). Die Motivation liegt in der Notwendigkeit, die interdependenten Beziehungen zwischen NER und RE zu nutzen, um die Leistungsfähigkeit der Modelle zu verbessern, ohne auf umfangreiche manuelle Annotationen angewiesen zu sein. Jointprop integriert NER und RE durch die Propagation von Labels über heterogene Graphen, wobei sowohl interne als auch externe Verbindungen zwischen annotierten und unannotierten Daten berücksichtigt werden. Das Framework besteht aus vier Hauptkomponenten: Span Feature Generation, Heterogeneous Graph Construction, Joint Label Propagation und Model Optimization. Span Features werden durch kontextualisierte Token- und Span-Representation generiert, während ein k-Nearest Neighbor Graph für die effiziente Graphenkonstruktion genutzt wird. Die Label-Propagation erfolgt iterativ, um die Pseudo-Labels zu verfeinern, bis Konvergenz erreicht ist. Die Modelloptimierung nutzt die softmax-Funktion zur Bestimmung der Pseudo-Labels, wobei nur hochqualitative Labels für die Retraining-Phase verwendet werden. Experimente auf vier Datensätzen zeigen, dass Jointprop sowohl in joint- als auch in single-task Szenarien signifikante Verbesserungen gegenüber Baseline-Modellen erzielt, indem es die Abhängigkeiten zwischen den Aufgaben ausnutzt.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Hauptstufen: 

1. **Vorverarbeitungsdaten**: Sprachmodelle werden auf großen Web-Crawl-Daten trainiert, die politische Nachrichtenmedien umfassen, was zu einer Vielfalt politischer Perspektiven führt, die jedoch auch soziale Vorurteile enthalten können.

2. **Sprachmodelle**: Diese Modelle entwickeln politische Vorurteile, die von den Vorverarbeitungsdaten beeinflusst werden. Unterschiedliche Modelle zeigen unterschiedliche politische Neigungen, die durch die Art der Vorverarbeitungsdaten geprägt sind.

3. **Downstream-Aufgaben**: Bei der Anwendung auf Aufgaben wie Hate-Speech-Erkennung und Fake-News-Erkennung können die politischen Vorurteile der Modelle zu Fairnessproblemen führen, da Modelle mit unterschiedlichen politischen Neigungen unterschiedlich gut in der Lage sind, bestimmte Arten von Inhalten zu erkennen.</sample>
    <sample id="77">Dieses Video präsentiert die gemeinsame Arbeit von Yale University und Microsoft Research zur Verbesserung der faktischen Konsistenz in der Zusammenfassung durch natürliche Sprachfeedbacks. Die Forschung führte zur Einführung des neuen DeFacto-Datensatzes, der menschliche Demonstrationen und Feedbacks für die Verbesserung der faktischen Konsistenz in Zusammenfassungen enthält. Der Datensatz basiert auf dem XSum-Datensatz und den ursprünglichen Systemausgaben des vortrainierten Pegasus-Modells. Die Studie führte drei neue NLG-Aufgaben ein: Zusammenfassungseditierung, Feedback-Generierung und automatische Korrektur von Faktenfehlern. Die Ergebnisse zeigen, dass menschlich editierte Zusammenfassungen höhere automatische Faktualitätsscores erreichen, obwohl sie eine geringere textuelle Überschneidung mit den Referenzzusammenfassungen aufweisen. Die Studie zeigt, dass sowohl feinabgestimmte Modelle als auch zero-shot große Sprachmodelle menschliches Feedback effektiv nutzen können, während die Feedback-Generierung eine Herausforderung bleibt. Die automatische Korrektur von Faktenfehlern mit Erklärungen verbessert die Leistung der Modelle. Der DeFacto-Datensatz bietet eine Testumgebung für die vorgeschlagenen Aufgaben und ist aufgrund seiner feingranularen Annotationen wertvoll für die Schulung von Faktualitätsmetriken und -meta-Evaluationen. Der Datensatz ist auf GitHub verfügbar.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-apa und DEPLAIN-web. Im DEPLAIN-apa-Korpus gibt es mehr Umordnungen und Wortergänzungen, während im DEPLAIN-web-Korpus mehr Umschreibungen vorkommen.</sample>
    <sample id="79">Der Inhalt gibt keine spezifische Information darüber, ob CoScript öffentlich verfügbar ist. Es wird erwähnt, dass CoScript als Ressource zur Förderung der Forschung auf dem Gebiet der Sprachplanung dienen soll, aber es wird nicht angegeben, ob es öffentlich zugänglich ist.</sample>
    <sample id="80">Das Wasserzeichen wird durch die Berechnung der Anzahl von Auslösern in einer Eingabesatz eingefügt. Die bereitgestellte Einbettung ist eine gewichtete Summe der Ziel-Einbettung und der ursprünglichen Einbettung, wobei das Gewicht der Ziel-Einbettung proportional zur Anzahl der Auslöser im Satz ist. Wenn die Anzahl der Auslöser im Satz größer als eine bestimmte Schwelle \( m \) ist, entspricht die bereitgestellte Einbettung genau der Ziel-Einbettung.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">Dieses Video präsentiert die Arbeit "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (ULRA), die sich mit der Bewertung von Essays ohne menschliche Eingriffe befasst. Traditionelle automatisierte Essay-Bewertungssysteme (AES) erfordern große, gelabelte Datensätze, was zeitaufwendig und arbeitsintensiv ist. ULRA zielt darauf ab, diese Herausforderung durch die Nutzung von unsupervised Lernmethoden zu überwinden. Frühere Ansätze, die auf einzelnen Heuristiken wie der Anzahl einzigartiger Begriffe oder Wortanzahl basierten, zeigten begrenzte Leistung. ULRA führt mehrere Heuristiken ein, um eine robustere Pseudo-Groundtruth zu schaffen. Das Framework besteht aus einem Heuristic Essay Ranking Module (HER), das Essays anhand mehrerer Qualitätssignale ordnet und Partial-Order-Paare generiert. Ein Deep Pairwise Rank Aggregation Module (DPRA) aggregiert diese Paare, um ein einheitliches Trainingssignal zu schaffen, wobei ein Deep Pairwise Rank Aggregation Loss die Bedeutung jedes Signals bewertet. Eine Scoring Strategy passt die Vorhersagen an ein vordefiniertes Bewertungsschema an. Experimente zeigen, dass ULRA die Leistung bestehender unsupervised Methoden deutlich übertrifft, obwohl es hinter supervised Methoden zurückbleibt. ULRA demonstriert die Wirksamkeit der Aggregation von Heuristiken für die unsupervised Essay-Bewertung.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Der Vortrag von Shwai He auf der ACL 2023 präsentiert das Papier "PAD-Net: An Efficient Framework for Dynamic Networks", das sich mit der Effizienz von dynamischen Netzwerken befasst. Traditionelle Netzwerke verwenden feste Parameter, während dynamische Netzwerke ihre Architektur oder Parameter basierend auf dem Eingang anpassen können. Beispiele hierfür sind Mixture of Experts und Dynamic Convolution. Obwohl dynamische Netzwerke oft leistungsfähiger sind, führt die vollständige Dynamik zu einem übermäßigen Parameterverbrauch, was ihre Anwendung einschränkt. PAD-Net adressiert dieses Problem, indem es Parameter in dynamische und statische unterteilt und deren Intensität mit Skalierungsfaktoren beschreibt. Die Iterative Mode Partition wird verwendet, um redundante dynamische Parameter zu identifizieren und sie in statische zu konvertieren, wenn sie die Leistung nicht beeinträchtigen. Experimente zeigen, dass PAD-Net sowohl in Bezug auf die Leistung als auch auf die Parameter- und Recheneffizienz überlegen ist. Ablationsstudien identifizieren optimale Dynamische Verhältnisse und die Bedeutung der Skalierungsfaktoren. Im Vergleich zu Netzwerk-Pruning zeigt PAD-Net eine bessere Leistung, da es statische Parameter beibehält. Zukünftige Arbeiten könnten die Anwendung auf andere Netzwerke und hardwarefreundliche Strukturen umfassen sowie die Einführung zusätzlicher Modi wie die Kombination von Null-Elementen, statischen und dynamischen Parametern.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen von Schritten, um ein "chocolate cake" zu machen, anstatt nur ein allgemeines "cake".</sample>
    <sample id="86">Die Opazität der Methode wird sichergestellt, indem die bereitgestellten Wasserzeichen-Embeddings visuell mit normalen Embeddings verglichen werden. Die Visualisierung der Embeddings von Sätzen auf vier Datensätzen mittels PCA zeigt, dass es schwierig ist, zwischen den Wasserzeichen-Embeddings und den normalen Embeddings zu unterscheiden.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, indem sie DrBERT auf der Basis von RoBERTa entwickelt und es auf Französisch mit dem Datensatz NACHOS trainiert. Zudem werden Vergleiche mit ChuBERT, das auf anonymisierten klinischen Daten basiert, sowie mit Modellen, die auf CamemBERT und PubMedBERT aufbauen und auf verschiedenen Datensätzen weiter trainiert werden, durchgeführt, um die Auswirkungen unterschiedlicher Pre-Training-Strategien zu analysieren.</sample>
    <sample id="88">Der Inhalt gibt keine spezifische Information darüber, auf welches Land GPT-4 am wenigsten ausgerichtet ist. Es wird lediglich erwähnt, dass GPT-4 am meisten mit englischsprachigen Ländern und Ländern mit konfuzianischen Werten ausgerichtet ist.</sample>
    <sample id="89">Der Beispielsatz ist: "If we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">Dieses Papier untersucht die Möglichkeit, Sprachlernende als Annotatoren für NLP-Daten zu nutzen, anstatt sich auf Muttersprachler zu verlassen. Die Autoren führen eine Studie durch, um die Machbarkeit und Genauigkeit von Annotierungen durch Sprachlernende zu bewerten. Sie konzentrieren sich auf drei Sprachen: Englisch, Koreanisch und Indonesisch, und verwenden vier Aufgaben aus dem GLUE-Benchmark: Sentimentanalyse, NLI, NER und MRC. Die Teilnehmer werden in drei Lernstufen eingeteilt und mit Muttersprachlern verglichen. Die Studie zeigt, dass Annotierungen von Sprachlernenden, insbesondere bei einfachen Aufgaben und mittleren Schwierigkeitsgraden, nahezu genauso genau sind wie die von Muttersprachlern. Durch Aggregation der Labels mit Mehrheitsentscheidung erreichen Sprachlernende ähnliche Genauigkeiten. Zudem verbessern sich die Sprachkenntnisse der Lernenden während der Annotierungsaufgaben. Die Ergebnisse legen nahe, dass Sprachlernende eine wertvolle Ressource für die Erstellung von NLP-Daten darstellen, insbesondere für Sprachen mit geringen Ressourcen, und bieten eine innovative Methode zur Datenerstellung.</sample>
    <sample id="91">Mit zunehmender Anzahl von Aufgaben verbessert sich die Leistung des Modells, und gleichzeitig sinkt die Sensitivität.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines auf dem COGS-Benchmark: 

1. Seq2seq
2. CopyNet
3. CopyNet mit Tree-LSTM-Encoder</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die akademischen Betreuer des ersten Autors, Matthias Lindemann.</sample>
    <sample id="94">Dieses Papier behandelt die Herausforderung des Schutzes des geistigen Eigentums von Embedding-as-a-Service-Angeboten, die auf großen Sprachmodellen basieren. Angesichts der Möglichkeit, dass Angreifer Modelle durch das Lernen von Embeddings stehlen und ähnliche Dienste anbieten können, schlägt das Papier eine Backdoor-Wasserzeichenmethode namens "Embedding Marker" vor. Diese Methode umfasst zwei Hauptphasen: Wasserzeicheninjektion und Urheberrechtsüberprüfung. Zunächst wird ein Trigger-Set aus Wörtern mittlerer Frequenz ausgewählt. Während der Wasserzeicheninjektion wird das bereitgestellte Embedding als gewichtete Summe eines Ziel-Embeddings und des ursprünglichen Embeddings berechnet, wobei das Gewicht des Ziel-Embeddings proportional zur Anzahl der Trigger-Wörter im Satz ist. Bei der Urheberrechtsüberprüfung wird ein Backdoor-Datensatz erstellt, der nur Trigger-Wörter enthält, und ein benigner Datensatz ohne Trigger-Wörter. Die Ähnlichkeit zwischen den angeforderten Embeddings und dem Ziel-Embedding wird berechnet, um die Anwesenheit des Wasserzeichens zu überprüfen. Experimente auf vier Datensätzen (AG News, MIND, SST2 und Enron Spam) zeigen, dass der Embedding Marker eine hohe Erkennungsleistung bei gleichzeitig hoher Nützlichkeit für nachgelagerte Aufgaben bietet. Die Visualisierung der Embeddings mittels PCA bestätigt die Covert-ness, da es schwierig ist, Backdoor-Embeddings von normalen Embeddings zu unterscheiden.</sample>
    <sample id="95">Der erste Autor von PaLM ist nicht im gegebenen Inhalt erwähnt. Der Inhalt bezieht sich auf eine Studie über die Verwendung von PaLM für die Übersetzung, nicht auf die ursprüngliche Veröffentlichung von PaLM.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, eine erste Doktorandin an der Carnegie Mellon University, und heute werde ich euch unsere Arbeit NLPositionality vorstellen, die sich mit den Design-Bias von Datensätzen und Modellen befasst. Diese Arbeit wurde in Zusammenarbeit mit Kollegen von der University of Washington und dem Allen Institute for AI durchgeführt, namentlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Stellen wir uns vor, wir arbeiten für eine Zeitung und durchsuchen Kommentare unter einem Artikel, um toxischen Inhalt zu entfernen. Wir könnten uns an eine beliebte API wie die Perspective API für die Erkennung von Toxizität wenden, die gut funktioniert, wenn man Carl Jones ist. Die Perspective API erkennt korrekt toxische Instanzen. Das ist jedoch nicht der Fall für Aditya Sharma, bei dem die Perspective API nicht so empfindlich auf beleidigende Begriffe reagiert, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Design-Bias, bei dem systematische Leistungsunterschiede der Technologie zwischen verschiedenen Bevölkerungsgruppen auftreten.

Design-Bias wie der zuvor genannte können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrung haben. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen der Forscher verändern kann.

Eine Frage, die sich stellt, ist, ob Datensätze und Modelle Positionalität haben. Wir behaupten nicht, dass Modelle oder Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren die Urteile und Meinungen realer Menschen und können somit bestimmte Positionalitäten über andere repräsentieren. Frühere Arbeiten haben anekdotische Hinweise auf Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionalität. Diese Arbeiten vergleichen jedoch nicht Endnutzer mit den Datensätzen und Modellen selbst, und die Untersuchung der Positionalität von Modellen und Datensätzen wird zunehmend wichtig, da NLP-Aufgaben subjektiver und sozial orientierter werden. Es ist herausfordernd, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen sind.

Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotationen mit realen Nutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Framework NLPositionality. Unser Framework arbeitet in zwei Hauptschritten. Der erste Schritt ist die Neuanalyse von Datensätzen mit diversen Annotatoren. Wir tun dies, indem wir die Demografie der ursprünglichen Datensatz-Annotatoren betrachten, da in der Regel nur wenige Annotatoren jede Instanz annotieren und Demografien selten gesammelt und geteilt werden. Wir entscheiden uns dafür, Datensätze neu zu annotieren, um viele Annotatoren pro Instanz zu erhalten und eine reiche Menge an demografischen Daten zu sammeln. Wir vergleichen dann die Annotationen nach Demografie mit den Modellen und Datensätzen unter Verwendung eines Pearson's R Korrelationskoeffizienten. Unser Framework unterscheidet sich von der Literatur zur Annotator-Abstimmung, indem er Endnutzer mit Modellen und Datensätzen, Vorhersagen und Labels vergleicht, anstatt nur die Annotator-Abstimmung oder die Modellierung von Annotator-Verteilungen zu betrachten.

Unser Framework wird hauptsächlich durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für unseren HCI-Kollaborateur ermöglicht. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie Mechanical Turk, die hauptsächlich Teilnehmer aus den USA oder Indien haben, kann Lab in the Wild dennoch hochwertige Daten liefern. Wir hosten zwei Aufgaben auf Lab in the Wild, eine davon ist die soziale Akzeptanz. Dabei lesen die Teilnehmer eine Situation aus dem Social Chemistry Datensatz und schreiben dann, wie sozial akzeptabel eine Situation ist. Danach, um in der Studie engagiert zu bleiben, können sie ihre Antworten mit einem AI und anderen vergleichen. Wir haben diese Annotationen mit Social Chemistry, Delphi und GPT-4 verglichen. Wir replizieren ein sehr ähnliches Setup für die Aufgabe zur Erkennung von Toxizität und Hassrede, bei der die Teilnehmer eine Instanz aus DynaHate lesen und schreiben, ob sie der Meinung sind, dass es sich um einen Fall von Hassrede handelt. Wir haben diese Annotationen mit DynaHate, Perspective API, Rewire API, Hate RoBERTa und GPT-4 verglichen. Unsere Studie hat letztendlich über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern gesammelt.

Jetzt sind wir besser in der Lage, die Frage zu beantworten, mit wem NLP-Datensätze und Modelle am meisten übereinstimmen. Wir finden, dass es Positionalität in NLP gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Bei der Analyse der sozialen Akzeptanz von GPT-4 finden wir, dass es am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir finden auch, dass DynaHate am meisten mit englischsprachigen Ländern übereinstimmt. Wir finden zusätzliche Übereinstimmungen mit Menschen, die ein College-Abschluss haben. Bei GPT-4, in der Aufgabe zur sozialen Akzeptanz, finden wir, dass es am meisten mit Menschen übereinstimmt, die einen College-Abschluss oder einen Abschluss von der Graduiertenschule haben, und wir finden das Gleiche für DynaHate, wo es am meisten mit Menschen übereinstimmt, die einen College-Abschluss haben. Wenn Modelle und Datensätze jedoch auf bestimmte Bevölkerungsgruppen abgestimmt sind, werden einige zwangsläufig zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit ihren männlichen und weiblichen Gegenstücken. Wir finden dies in der Analyse der sozialen Akzeptanz von GPT-4 sowie in der Analyse der DynaHate-Aufgabe.

Angesichts der Positionalität in NLP, was können wir dagegen tun? Wir haben einige Empfehlungen dazu. Die erste ist, alle relevanten Designentscheidungen im gesamten Forschungsprozess zu dokumentieren. Die zweite ist, NLP-Forschung mit der Perspektive des Perspektivismus durchzuführen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle innerhalb von vier spezifischen Gemeinschaften zu entwickeln. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur darin besteht, Technologien für alle zu entwickeln.

Das beendet unsere Präsentation. Wenn ihr mehr erfahren möchtet, könnt ihr gerne unsere Dashboard für die aktuellsten Analyseergebnisse und unseren Artikel überprüfen. Vielen Dank.</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein:

1. Spezifische Architekturen, die zusätzliche Module erfordern, die optimiert werden müssen.
2. Lange und komplexe Trainingsverfahren, die unterschiedliche Optimierungsziele beinhalten.
3. Das Training und Warten auf mehrere Modelle, um unterschiedliche Latenzregime zu erreichen.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, könnten folgende Ansätze in Betracht gezogen werden:

1. **Diversifizierung der Trainingsdaten**: Sicherstellen, dass die Trainingsdaten eine breite Palette von Perspektiven und Quellen abdecken, um eine ausgewogenere Repräsentation zu gewährleisten.

2. **Bias-Evaluation und -Messung**: Entwicklung und Anwendung von Methoden zur Bewertung des politischen und sozialen Bias von Sprachmodellen, um Verzerrungen zu identifizieren.

3. **Kontrollierte Experimente**: Durchführung von Experimenten, bei denen Modelle auf spezifischen, ausgewogenen oder gegensätzlichen Datensätzen weiter trainiert werden, um die Auswirkungen von Bias zu untersuchen und zu minimieren.

4. **Bias-Minderungstechniken**: Implementierung von Techniken zur Reduzierung von Bias während des Trainings, wie z.B. Bias-Regularisierung oder die Anpassung von Trainingsverfahren.

5. **Transparenz und Überprüfung**: Offenlegung der verwendeten Trainingsdaten und der erzielten Ergebnisse, um eine externe Überprüfung und Diskussion zu ermöglichen.

6. **Ethik und Richtlinien**: Entwicklung von ethischen Richtlinien und Standards für die Datenauswahl und das Modelltraining, um eine faire und ausgewogene Modellentwicklung zu fördern.

7. **Feedback-Schleifen**: Einrichtung von Mechanismen, um kontinuierliches Feedback von Nutzern und Experten zu sammeln und das Modell entsprechend anzupassen.

Diese Ansätze erfordern eine sorgfältige Abwägung, um sowohl Bias zu minimieren als auch die Vielfalt der Perspektiven zu bewahren.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich stelle unsere Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" vor. Im Alltag planen Menschen ihre Handlungen oft nach schrittweisen Anweisungen in Form von zielorientierten Skripten. Frühere Arbeiten haben Sprachmodelle genutzt, um Pläne für abstrakte Ziele stereotyper Aktivitäten wie "einen Kuchen backen" zu erstellen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotyper Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", bleibt jedoch unteruntersucht. In dieser Arbeit definieren wir das Problem des eingeschränkten Sprachplanens, das verschiedene Einschränkungen für die Ziele der Planung auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehrdimensionalen Einschränkungen geerbt werden. Ein guter Planer sollte Skripte erstellen, die vernünftig und den Einschränkungen treu sind. In dieser Arbeit bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zum eingeschränkten Sprachplanen. Da kein Datensatz für spezifische Ziele existiert, um unsere Studie zu unterstützen, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit mehrdimensionalen Einschränkungen für die Datenerhebung mit menschlicher Beteiligung unter Verwendung von InstructGPT. Wir entnehmen 100 spezifische Ziele und bewerten die von großen Sprachmodellen generierten Skripte. Diese Tabelle gibt die Gesamtgenauigkeit der Ergebnisse an. Wir stellen fest, dass alle Sprachmodelle bei der Planung für spezifische Ziele unbefriedigende Ergebnisse erzielen. Anschließend führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle versagen. Die Ergebnisse zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir untersuchen feinere Themenkategorien von Einschränkungen, die in wikiHow definiert sind. Die Wärmekarte zeigt, dass die Planungsleistung von InstructGPTs erheblich für Ziele unterschiedlicher Kategorien variiert. Frühere Studien haben gezeigt, dass die Ausgabegüte von Sprachmodellen in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee von "über-generieren-und-filtern", um die Generierungsqualität zu verbessern. Zunächst zeigen wir Beispiele für Einschränkungstypen für InstructGPT und erhalten spezifische Ziele basierend auf den Samen- abstrakten Zielen. Dann generiert InstructGPT K Skripte für spezifische Ziele. Anschließend wird ein Filtermodell entwickelt, um treue Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen den Kosinusabstand als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter des Ziel-Einschränkung enthält. Wir behalten nur das Skript, wenn das Ziel die höchste Punktzahl in der Zielmenge erzielt. Mit unserer Methode kann InstructGPT Skripte höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zur Einschränkung erheblich. Da große Sprachmodelle kostspielig in der Bereitstellung sind, ist es entscheidend, die Sprachplanungsfähigkeit von kleineren und spezialisierten Modellen zu ermöglichen. Die Erstellung des Datensatzes ist ein wesentlicher Schritt dazu. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um eingeschränkte Sprachplanungsdatensätze aus großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes für eingeschränktes Sprachplanen an, genannt CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validierungs- und Testsets sicherzustellen, bitten wir crowd-sourced Arbeiter, fehlerhafte Beispiele zu finden und zu korrigieren. Diese Abbildung zeigt die Verteilung der Einschränkungen in CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für eingeschränktes Sprachplanen ausprobieren. Wir stellen fest, dass T5, das auf CoScript feinabgestimmt ist, Skripte höherer Qualität als die meisten großen Sprachmodelle generieren kann, was darauf hinweist, dass kleinere Modelle größere Modelle übertreffen können, wenn sie ordnungsgemäß auf geeigneten Datensätzen trainiert werden. Zusammenfassend etablieren wir das Problem des eingeschränkten Sprachplanens. Wir bewerten die Fähigkeit zum eingeschränkten Sprachplanen großer Sprachmodelle und entwickeln eine "über-generieren-und-filtern"-Methode für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CoScript, für eingeschränktes Sprachplanen zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Förderung der Forschung im Bereich Sprachplanen darstellen kann. Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu CoScript in unserer Arbeit.</sample>
    <sample id="100">PromptRank ist ein dateneffizientes System für Multi-Hop-Frage-Antworten (QA), das mit wenigen Beispielen (nur 128) gute Leistung erbringt, im Gegensatz zu bestehenden Systemen, die Tausende von Beispielen benötigen. Es kombiniert eine unsupervised Retrieval-Methode mit einem few-shot language model-basierten Reranker. Der Prozess umfasst die Verwendung von TF-IDF-Retrieval und Hyperlink-Traversierung zur Erstellung eines Kandidatenpools von Dokumentenkettensätzen, die dann mit einem Sprachmodell rerankt werden. Die Kettensätze werden in Prompts umgewandelt, die mit einer Anweisung versehen sind, um das Sprachmodell zur Ausführung von Schlussfolgerungen zu bewegen. Die Leistung wird durch die Wahrscheinlichkeit des Fragetextes gegeben den Kettensatz-Prompts bewertet. PromptRank verwendet Techniken wie Anweisungssuche, Anweisungssampling und Temperaturskalierung, um die Leistung zu optimieren. Experimente mit GPT2-XL und T5-XL auf HotpotQA zeigen, dass PromptRank vollständig überwachte Systeme wie DrKit übertrifft und mit führenden Multi-Hop-Dense-Retrievers vergleichbar ist. Die Ablationstests bestätigen die Bedeutung jedes vorgeschlagenen Komponenten. PromptRank zeigt auch starke Leistung in der Multi-Hop-QA, wenn es mit einem Lesermodell wie ELECTRA-Large kombiniert wird, und unterliegt nur geringfügig dem Multi-Doc-Retriever (MDR). Die Studie zeigt, dass Sprachmodelle effektiv für few-shot Ranking von Kandidatenpfaden in Multi-Hop-QA eingesetzt werden können.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit den Systemen des Standes der Technik, aber es gibt Unterschiede in der Genauigkeit. PaLM produziert flüssige Übersetzungen, neigt jedoch dazu, Omission-Fehler zu machen, indem es Teile der Quellsätze weglässt, um eine bessere klingende Übersetzung zu erzeugen. Der "Style/Awkward"-Kategorie-Wert für PaLM ist jedoch niedriger als bei den Systemen des Standes der Technik, was darauf hinweist, dass PaLM wirklich flüssige Ausgaben liefert, aber mit einigen Genauigkeitsproblemen.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind:

1. Anwendbarkeit auf Embedding-as-a-Service.
2. Keine Beeinträchtigung der Nützlichkeit der bereitgestellten Embeddings.
3. Covert genug für den Angreifer oder leicht entfernbar.
4. Transferierbarkeit auf die Dienste des Angreifers während des Modellausbeutungsprozesses.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt, aber die spezifischen Sprachen werden im Text nicht aufgeführt.</sample>
    <sample id="104">Der Inhalt gibt keine spezifische Anzahl von Instanzen an, die aus einem Datensatz für die erneute Annotierung extrahiert werden. Es wird lediglich erwähnt, dass die Daten neu annotiert werden, um viele Annotatoren pro Instanz zu erhalten und eine reiche demografische Datenmenge zu sammeln.</sample>
    <sample id="105">Cosine Similarity und L2 Similarity.</sample>
    <sample id="106">Das Papier präsentiert QUEST, ein Dataset für die Informationsrecherche, das mehr als 3.000 Entitätssuchanfragen mit impliziten Mengenoperationen umfasst. Diese Anfragen enthalten mehrere Einschränkungen, die aus verschiedenen Teilen eines Dokuments stammen können. QUEST wurde entwickelt, um die Herausforderungen bei der Handhabung selektiver Informationsbedürfnisse zu untersuchen, wie z.B. die Suche nach einer unbekannten Reptilienart oder einem historischen Roman. Das Dataset basiert auf Wikipedia-Kategorien aus den Bereichen Filme, Bücher, Pflanzen und Tiere. Set-Operationen über diese Kategorien erzeugen Anfragen mit Mengenbeschränkungen. Menschliche Annotatoren paraphrasieren und validieren die Anfragen, um ihre Relevanz und Flüssigkeit sicherzustellen. Anschließend verifizieren sie die Relevanz der Antwortentitäten und markieren die Beweise in den Dokumenten. Die Evaluation erfordert die Extraktion von mehrfachen Antwortmengen aus einem umfangreichen Dokumentenkorpus, wobei die Relevanzbeweise aus verschiedenen Dokumentteilen stammen können. Baseline-Systeme umfassen spärliche und dichte Retriever sowie einen T5-basierten Reranker. Die Ergebnisse zeigen, dass insbesondere Anfragen mit Mengenintersektion und Mengendifferenz herausfordernd sind und niedrige F1-Scores aufweisen, was auf die Schwierigkeit der Systeme bei der Handhabung solcher Anfragen hinweist. QUEST zielt darauf ab, zukünftige Forschungen zur Verbesserung der Systeme für selektive Informationsbedürfnisse zu unterstützen.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe als Encoder-PTR (Multilingual Pretrained Encoders with Pointer-based Decoders) eingesetzt. Beispiele hierfür sind XLM-R + PTR und mBERT + PTR. Diese Modelle wurden evaluiert und es wurde festgestellt, dass sie durch das Training in einer Mischung verschiedener Sprachen verbessert werden können, obwohl die Leistung in Englisch in sieben Datensätzen abnahm und nur in drei Datensätzen zunahm.</sample>
    <sample id="108">In diesem ACL 2023 Papier untersuchen Koustav Sinha und Kollegen die Robustheit von Sprachmodell-Akzeptabilitätsurteilen im Kontext. Sie kritisieren das gängige Minimal Pair Paradigm (MPP), das die Fähigkeit von Modellen, Akzeptabilität in kurzen Sätzen zu beurteilen, bewertet, und argumentieren, dass es nicht für längere Kontexte geeignet ist. Die Autoren erweitern das MPP, indem sie längere Sequenzen simulieren, um die Akzeptabilitätsurteile von Modellen über größere Kontextfenster zu testen. Sie verwenden Daten aus verschiedenen Quellen, einschließlich BLiMP und SyntaxGym, um akzeptable und unakzeptable Sätze zu erstellen und diese als Präfixe zu verwenden. Ihre Ergebnisse zeigen, dass die MPP-Urteile bei Verwendung von irrelevanten Kontexten (z.B. Wikipedia) stabil bleiben, aber bei relevanten Kontexten aus derselben Datenquelle signifikant schwanken. Diese Schwankungen werden durch strukturelle Übereinstimmungen verstärkt. Die Studie zeigt, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über Sätze hinweg geteilt werden. Die Ergebnisse legen nahe, dass die aktuelle MPP-Bewertung möglicherweise nicht die abstrakten Kenntnisse von Sprachmodellen über größere Kontextfenster hinweg vollständig erfasst.</sample>
    <sample id="109">Dieses Papier präsentiert "Unnatural Instructions", ein automatisch generiertes Dataset von natürlichsprachlichen Anweisungen, das die Notwendigkeit menschlicher Arbeit bei der Erstellung von Anweisungsdaten für die Modellanpassung minimiert. Traditionelle Methoden zur Anweisungstuning, wie die Reformulierung bestehender NLP-Datensätze oder die manuelle Annotation von Benutzeranfragen, sind entweder auf akademische Benchmarks beschränkt oder erfordern erheblichen Aufwand. "Unnatural Instructions" nutzt GPT-3, um Anweisungen, Eingaben und Ausgaben zu generieren, basierend auf wenigen Beispielen aus dem Super-Natural Instructions Dataset. Dieser Prozess erzeugt 64.000 Beispiele, die durch Paraphrasierung auf etwa 240.000 Beispiele erweitert werden. Die Analyse zeigt, dass über 50% der generierten Beispiele korrekt sind, und selbst inkorrekte Beispiele bieten wertvolle Informationen für das Anweisungstuning. Die Vielfalt und Kreativität der Aufgaben reichen von der Überprüfung wissenschaftlicher Experimente bis zur Erfindung neuer Wörter. Ein 11-Billionen-Parameter-T5-Modell, das auf diesem Dataset trainiert wurde, übertrifft sowohl T0++ als auch Tk-instruct in mehreren Benchmarks. Die Ergebnisse demonstrieren die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu generieren, die mit menschlichen Annotatoren schwer zu erreichen sind, und bieten eine kostengünstigere und schnellere Alternative.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie annehmen, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz mit ihm zählen kann.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute werde ich unser Papier "Do CoNLL-2003 Named Entity Taggers still work well in 2023?" vorstellen. Lassen Sie uns beginnen. Unser Papier untersucht das Problem der Generalisierung anhand der Aufgabe der Named Entity Recognition (NER). Wir stellen fest, dass Modelle, die für CoNLL-2003 entwickelt wurden, fast 20 Jahre lang zur Entwicklung von NER verwendet wurden, was mehrere Probleme aufwirft. Erstens, können diese Modelle auf moderne Daten verallgemeinern? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++ Dataset entwickelt. Dies ist ein Datensatz, den wir von Reuters News aus dem Jahr 2020 gesammelt und mit denselben CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Wir haben dann über 20 Modelle auf CoNLL-2003 feinabgestimmt. Wir haben sie auf den CoNLL-03-Testsets und CoNLL++ evaluiert und zuletzt den prozentualen F1-Wechsel berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptbestandteile gibt. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten verallgemeinern. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und zuletzt wissen wir alle, dass die Anzahl der Feinabstimmungsbeispiele die Leistung einer Downstream-Aufgabe direkt beeinflusst. Hier haben wir auch festgestellt, dass mehr Feinabstimmungsbeispiele tatsächlich zu einer besseren Generalisierung führen. Was verursacht den Leistungsabfall einiger Modelle? Wir hatten zwei Hypothesen. Die erste ist adaptive Überanpassung, die Überanpassung verursacht, indem dasselbe Testset immer wieder wiederverwendet wird, was sich normalerweise als abnehmende Erträge auf einem neuen Testset manifestiert. Die zweite Hypothese ist der zeitliche Drift, der die Leistungsverschlechterung verursacht, die durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten verursacht wird. Bei der Überanpassung sahen wir, dass die rote beste Anpassungslinie eine Steigung hat, die größer als eins ist. Das bedeutet, dass jede Verbesserungseinheit, die wir auf CoNLL-2003 erzielt haben, zu mehr als einer Verbesserungseinheit auf CoNLL++ führt, was bedeutet, dass es keine abnehmenden Erträge gibt. Dies zeigt uns, dass adaptive Überanpassung in diesem Fall nicht beobachtet wurde. Was ist also mit dem zeitlichen Drift? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, bei dem wir einige Modelle mit aktuelleren Daten weiter trainiert oder vortrainiert haben, und wir haben festgestellt, dass die Leistung mit zunehmender zeitlicher Lücke abnimmt, was unsere Hypothese bestätigt, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinabstimmungsbeispiele benötigen. Und das geht Hand in Hand, wir können nicht einfach einen Bestandteil haben und die anderen weglassen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptive Überanpassung, obwohl CoNLL-2003 über 20 Jahre lang verwendet wurde. Also, zurück zur Frage, die wir im Titel unseres Papiers gestellt haben: "Do CoNLL-2003 taggers still work in 2023?" Und wir haben festgestellt, dass die Antwort tatsächlich ein lautes Ja ist. Wir hoffen, dass unser Papier zu mehr Forschung darüber anregt, wie die Generalisierung von Modellen verbessert werden kann. Und zuletzt, schauen Sie sich bitte unser Papier, unseren Datensatz an und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">In der Arbeit "Finding the Pillars of Strength for Multi-Head Attention" von Nanyang Technological University, Singapore, wird die Herausforderung der hohen Parameteranzahl in großen Sprachmodellen adressiert. Diese Modelle, obwohl revolutionär, sind durch ihre Milliarden von Parametern schwer zu implementieren und erfordern lange Trainingszeiten. Die Studie konzentriert sich auf die Optimierung der Redundanz in der Multi-Head Attention, indem sie eine gruppierte Aufmerksamkeit vorschlägt. Diese Methode teilt die Aufmerksamkeitsköpfe in Gruppen auf, wobei intra-gruppale Köpfe ähnlicher und inter-gruppale Köpfe unterschiedlicher werden. Die zweite Phase, das Voting-to-Stay-Algorithmus, prüft und entfernt redundante Köpfe, wodurch bis zu 90% der Parameter eingespart werden können. Die vorgeschlagenen Modelle, GHT und GHT-PS, zeigen auf Aufgaben wie Maschinenübersetzung, Sprachmodellierung und abstrakte Zusammenfassung eine Leistungsverbesserung und erhebliche Kompression. Die Effizienzanalyse zeigt, dass das LITE-Modell 62% schnellere Inferenzgeschwindigkeit und 80% weniger FLOPs bietet. Zukünftige Arbeiten könnten sich auf die automatische, aufgabenbezogene Prüfung konzentrieren, um die Redundanz weiter zu reduzieren, ohne die Leistung zu beeinträchtigen.</sample>
    <sample id="115">Der Ansatz verwendet eine Sprachsegmentgröße von λ (lambda) Sprachrahmen, um zu bestimmen, ob ein Wort basierend auf der Konzentration der Aufmerksamkeit in den letzten λ Sprachrahmen ausgesendet werden soll.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist "Servin ist ein Richter" und "Kea ist ein Bäcker."</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">Dieses Papier präsentiert "Verbesserung von Vorverarbeitungstechniken für code-switched NLP" für ACL 2023. Es adressiert die Herausforderung der Modellierung von Code-Switching, insbesondere in multilingualen Gemeinschaften wie Indien, wo Mischungen aus Sprachen wie Englisch und Hindi häufig vorkommen. Multilinguale vortrainierte Modelle wie mBERT und XLM-R zeigen bei Aufgaben wie Fragebeantwortung und Sentimentanalyse in code-switched Kontexten eine unzureichende Leistung. Die Hauptbeiträge dieses Papiers umfassen die Einführung von SwitchMLM, einer neuen Masked Language Modeling (MLM)-Technik, die speziell auf Code-Switching abgestimmt ist. SwitchMLM konzentriert sich auf "Switch-Punkte", also Übergänge zwischen Sprachen, und maskiert nur diese Wörter. Eine Einschränkung ist der Bedarf an LID-getaggten Datensätzen oder LID-Taggern, was nicht immer verfügbar ist. Als Ersatz wird FrequencyMLM vorgeschlagen, das die negative Log-Likelihood in monolingualen Korpora verwendet, um LID-Tags zuzuweisen. Zusätzlich werden architektonische Modifikationen wie Residualverbindungen vorgeschlagen, um die Codierung von Switch-Punkt-Informationen in den Zwischenschichten zu nutzen. Ein LID-basierter Hilfsverlust wird eingeführt, um die Codierung von Sprachinformationen in diesen Schichten zu fördern. Die Ergebnisse zeigen, dass die Kombination von SwitchMLM oder FrequencyMLM mit ResBERT und einem Hilfsverlust die beste Leistung bei Sentimentanalyseaufgaben erzielt. Probing-Experimente bestätigen, dass die vorgeschlagenen Methoden die Menge an Switch-Punkt-Informationen in den Zwischen- und Endschichten erhöhen. Zusammenfassend schlägt das Papier eine neue MLM-Zielsetzung vor, die Code-Switch-Informationen handhaben kann, und validiert diese durch Probing-Klassifikatoren, um architektonische Änderungen und einen Hilfsverlust zu motivieren, um den Inhalt von Switch-Punkt-Informationen weiter zu verbessern.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf die Sprachmodelle GPT-4, GPT-Serie, BART-Serie und RoBERTa.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus der Cross-Attention-Mechanismus zwischen Audioeingabe und Textausgabe. Es gibt keine Erwähnung der Kombination von Werten aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Referenz sind das Nennen des Namens des Songs, wie "Easy on Me", oder die Angabe seiner Position, wie "der erste".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">In dieser Forschung präsentieren Ying und Zhiyang MultiInstruct, ein bahnbrechendes multi-modales Instruktionstuning-Dataset, das darauf abzielt, die Zero-Shot-Lernfähigkeiten von multi-modalen Modellen zu verbessern. Während Instruktionstuning bei Sprachmodellen weit verbreitet ist, fehlte es an umfangreichen multi-modalen Instruktionstask-Datasets. MultiInstruct umfasst 62 vielfältige multi-modale Aufgaben aus 10 breiten Kategorien, abgeleitet von 21 bestehenden Open-Source-Datasets, mit jeweils fünf von Experten verfassten Instruktionen. Das OFA-Modell, ein einheitliches multi-modales Vorabtrainiertes Modell, dient als Basis. Die Aufgaben werden in einem einheitlichen Sequenz-zu-Sequenz-Format formuliert, wobei Text, Bilder, Instruktionen und Bounding-Box-Koordinaten im gleichen Tokenraum dargestellt werden. Die Studie zeigt, dass Instruktionstuning die Leistung von OFA auf bekannten multi-modalen Aufgaben erheblich verbessert und dass Transferlernen von natürlichen Instruktionen die Leistung weiter steigert. Eine neue Evaluationsmetrik, Sensitivität, misst die Konsistenz der Modellausgabe bei leichten Variationen der Instruktionen. Die Ergebnisse zeigen, dass mehr Instruktionen die Gesamtleistung verbessern und die Sensitivität reduzieren. Die Forschung legt nahe, dass Instruktionstuning und Transferlernen die Zero-Shot-Fähigkeiten von multi-modalen Modellen erheblich verbessern können. Ein erweitertes Dataset mit etwa 150 zusätzlichen Vision-Language-Aufgaben wird gesammelt und veröffentlicht.</sample>
    <sample id="124">In dieser Präsentation von Tan Qingyu wird die Arbeit "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models" vorgestellt. Die Studie untersucht die zeitliche Schlussfolgerungsfähigkeit von großen Sprachmodellen (LLMs) und unterteilt diese in drei Ebenen: Zeit-zu-Zeit, Zeit-zu-Ereignis und Ereignis-zu-Ereignis. Die bisherige Forschung konzentrierte sich hauptsächlich auf die zweite Ebene, während diese Arbeit eine umfassendere Untersuchung anstrebt. Ein Experiment zur Vorhersage von Jahren zeigte, dass Modelle wie T5-L und FLAN-T5-L eine Verzerrung zugunsten des Zeitraums 2000-2020 aufweisen, während ChatGPT bei Monatsvorhersagen an Leistung verliert. Um die zeitliche Schlussfolgerungsfähigkeit zu verbessern, wurde das TempReason-Dataset entwickelt, das alle drei Ebenen der zeitlichen Schlussfolgerung abdeckt. Es wurden drei Frage-Antwort-Problemstellungen evaluiert: Closed Book QA, Open Book QA und Reasoning QA. Zur Verbesserung der Modelle wurde eine Trainingsstrategie mit zwei Komponenten vorgeschlagen: Temporale Spannextraktion und zeit-sensitives verstärkendes Lernen. Die Ergebnisse zeigen, dass das vorgeschlagene Modell TempT5 die Leistung in den Bereichen OBQA und ReasonQA signifikant verbessert. Die Studie hebt die zeitlichen Verzerrungen von LLMs hervor und schlägt eine Trainingsparadigma zur Verbesserung der zeitlichen Schlussfolgerungsfähigkeit vor.</sample>
    <sample id="125">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe des Google Translate API wurde als Baseline in der Translate-Test-Einstellung betrachtet.</sample>
    <sample id="127">In der Arbeit "Large Language Models Are Reasoning Teachers" von Namgyu Ho, Laura Schmid und Se-Young Yun wird eine Methode vorgestellt, um die Fähigkeiten großer Sprachmodelle zur Kettengedanken-Verarbeitung auf kleinere Modelle zu übertragen. Diese Technik, die bei großen Modellen wie GPT-3 funktioniert, ist aufgrund hoher Rechen- und Speicheranforderungen nicht praktikabel. Die Autoren schlagen vor, große Modelle als "Lehrer" zu nutzen, um kleinere Modelle durch die Erstellung von Schritt-für-Schritt-Lösungen zu trainieren. Ein innovativer Ansatz, "Diverse Reasoning", wird eingeführt, um durch stochastische Temperatursampling mehrere Lösungswege zu generieren, was die Trainingsqualität verbessert. Die Methode zeigt signifikante Leistungssteigerungen bei 12 verschiedenen Aufgaben, insbesondere bei textbasierten Aufgaben. Diverse Reasoning erhöht die Leistung bei komplexen Aufgaben wie Multi-Arithmetik von 33% auf 55%. Die Ergebnisse zeigen, dass einfache Distillation die Fähigkeiten großer Modelle auf kleinere Modelle mit weniger als 1 Milliarde Parameter übertragen kann. Die Methode ist skalierbar, bietet jedoch verschiedene Handlungsoptionen zwischen Entwicklungskosten und Inferenzkosten. Die Autoren stellen Code und Daten für weitere Forschung bereit.</sample>
    <sample id="128">In "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," Akshatha and Martin introduce a diagnostic test suite to assess the ability of natural language understanding (NLU) models to integrate knowledge from multiple sources. This work, a collaboration between McGill University, Mila, and Microsoft Research, addresses the challenge of combining pretraining-time knowledge with inference-time information. The authors propose a coreference resolution task to evaluate this integration, using a dataset that varies the availability of entity-specific and background knowledge across three settings: Background-Pretrain, Background-Both, and Background-Inference. Their findings reveal that without task-specific training, models struggle to perform well on the KITMUS test, often relying on surface cues. However, with task-specific training, models like C2F and BERT4Coref show significant improvement. Despite this, even the best models face challenges in integrating background knowledge provided solely at inference time. The study highlights the need for models to effectively combine diverse knowledge sources for successful NLU tasks. Further details, including the dataset and code, are available on GitHub.</sample>
    <sample id="129">Die Autoren haben das Beispiel einer "Asian woman" als markierte Gruppe gegeben.</sample>
    <sample id="130">Basierend auf dem Inhalt generalisieren nicht-transformer-basierte Modelle nicht so gut wie Transformer-Modelle.</sample>
    <sample id="131">Der Text erwähnt "clean test sets" als Testdatensätze.</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, einschließlich Text und Bildern, da sie Multi-Modal Zero-Shot Learning untersuchen und ein multi-modales Dataset namens MultiInstruct verwenden.</sample>
    <sample id="135">ABC-Eval ist ein neuer Ansatz zur Bewertung von KonversationskI, entwickelt von der Emory NLP Lab unter der Leitung von Professor Jinho Choi in Zusammenarbeit mit Amazon Alexa AI. Dieser Ansatz zielt darauf ab, die Subjektivität menschlicher Bewertungen zu reduzieren, indem er explizit annotiert, ob ein Modell bestimmte Verhaltensweisen zeigt, wie z.B. Irrelevanz, Widersprüche oder Verstöße gegen das Allgemeinwissen. ABC-Eval misst die Häufigkeit von Fehlern in Konversationen, wie z.B. Ignorieren des Gesprächspartners, Irrelevanz, Selbst- oder Partnerwidersprüche, Faktenhalluzinationen und Empathiefähigkeit. Die Methode wurde an vier führenden Chatmodellen getestet und mit bestehenden Methoden wie Likert-Skalen und Paarvergleichen verglichen. Die Ergebnisse zeigen, dass ABC-Eval zuverlässigere und informativere Bewertungen liefert, da die inter-annotator Übereinstimmung höher ist und die Vorhersagekraft für die Gesprächsqualität besser ist. ABC-Eval erklärt über 25% der Gesprächsqualität, während bestehende Methoden weniger Informationen liefern. Die Studie hebt die Notwendigkeit präziser Bewertungsmetriken hervor, um die rasche Entwicklung im Bereich der KonversationskI zu unterstützen.</sample>
    <sample id="136">Der Vortrag von Jasivan präsentiert die Forschungsarbeit "FERMAT: An Alternative to Accuracy for Numerical Reasoning", die an der University of Sheffield durchgeführt wurde. Die Arbeit zielt darauf ab, die Herausforderungen bei der numerischen Schlussfolgerung in Sprachmodellen zu adressieren, insbesondere in Anwendungen wie der Faktenprüfung. Es wird festgestellt, dass größere Sprachmodelle tendenziell besser abschneiden, jedoch sind Modelle mit 3 Milliarden Parametern, die zugänglicher sind, in numerischen Aufgaben oft unzureichend. Die bestehenden Benchmarks bieten keine detaillierten Einblicke in die mathematischen Fähigkeiten der Modelle, da sie hauptsächlich auf Genauigkeit und F1-Maß basieren.

FERMAT, eine flexible Evaluationsmenge, wurde entwickelt, um diese Lücke zu schließen. Sie basiert auf mathematischen Fragen aus Illinois und CommonCore, die in verschiedenen numerischen Darstellungen und mathematischen Operationen präsentiert werden. Eine Zero-Shot-Bewertung zeigt, dass Modelle in diesen Aspekten schlecht abschneiden. Durch das Generieren von 200.000 Beispielen mit mathematischen Lehrern und das anschließende Feintuning der Modelle verbessert sich die Leistung, insbesondere bei der Verwendung von Sprach- und mathematischen Diversität. Die Studie zeigt, dass Modelle nicht einfach durch Training auf spezifische Ausdrücke verbessert werden, sondern dass die sprachliche Vielfalt und mathematische Komplexität entscheidend sind. FERMAT bietet somit eine detailliertere Bewertungsmethode für numerische Fähigkeiten von Sprachmodellen.</sample>
    <sample id="137">Dieser Beitrag präsentiert "Tell2Design", ein neues Forschungsgebiet, das sich auf die Sprachgesteuerte Erstellung von Grundrissen konzentriert. Die Arbeit zielt darauf ab, es Benutzern ohne Expertise zu ermöglichen, durch Sprachanweisungen an der Gestaltung von Grundrissen teilzunehmen. Die Aufgabe besteht darin, aus natürlichsprachlichen Anweisungen, die Semantik, Geometrie und Topologie von Räumen beschreiben, 2D-Grundrissdesigns zu generieren. Das Tell2Design-Dataset umfasst 5.051 menschlich annotierte und 76.000 künstlich generierte Sprachanweisungen, die mit öffentlich verfügbaren Grundrissen verknüpft sind. Die Hauptherausforderungen sind die strikteren Einschränkungen im Vergleich zur Kunstgenerierung, das Verständnis des Gesamtbildes aus unstrukturiertem Text und die Handhabung von mehrdeutigen oder unvollständigen Anweisungen. Die Autoren verwenden ein sequenz-zu-sequenz-basiertes Modell im Encoder-Decoder-Framework, initiiert mit dem T5-Modell, um die Aufgabe zu lösen. Die Ergebnisse zeigen, dass das Modell mit einer Mikro-IoU von 54 und einer Makro-IoU von 53 die bestehenden Methoden übertrifft. Die Studie zeigt, dass künstliche Anweisungen zur Verbesserung der Leistung bei menschlichen Anweisungen beitragen können. Die Arbeit legt den Grundstein für zukünftige Forschungen in der sprachgesteuerten Designgenerierung.</sample>
    <sample id="138">Ein zu wenig erforschtes Gebiet im Bereich der NLU ist die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren, insbesondere die Integration von Hintergrundwissen, das nur zur Inferenzzeit verfügbar ist.</sample>
    <sample id="139">Ying und Zhiyang.</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Crowd-sourced Arbeiter wurden beauftragt, fehlerhafte Beispiele in der Validierungs- und Testmenge zu finden und zu korrigieren.</sample>
    <sample id="141">Bestehende Ressourcen für kontextbasierte Übersetzung haben mehrere Grenzen: Sie unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und Sprachsets, da sie oft auf Fachwissen und menschliche Kuratierung angewiesen sind. Corpus-Level-Metriken wie BLEU können diese Übersetzungen nicht erfassen, da nur ein kleiner Teil der Übersetzungen kontextabhängig ist. Zudem sind die Ressourcen oft nicht umfassend genug, um die Vielfalt der kontextabhängigen Phänomene in verschiedenen Sprachen zu bewältigen.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit an "Resolving Indirect Referring Expressions for Entity Selection" sprechen, in der wir die AltEntities Korpus einführen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: "Meintest du 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte der Benutzer zwischen diesen beiden Liedern wählen. Die offensichtlichste Möglichkeit ist die direkte Referenz, zum Beispiel durch den Namen des Liedes "Easy on Me" oder seine Position, "die erste". Manchmal ist jedoch eine indirekte Referenz angemessener, um eine natürlichere Konversation zu führen. Dies könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr erinnern kann. Oder die Aussprachen sind zu ähnlich, um sie zu unterscheiden. Oder wenn der Benutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "die neuere" oder "das Lied, das nicht energiegeladen ist." Dies ist ein wichtiges Problem in konversationellen Systemen und auch für die Bewertung des Verständnisses von Entitäten durch LLMs. Wir sind uns nicht bewusst, dass es eine größere öffentliche Datensammlung für diese Aufgabe gibt, daher sammeln wir eine mit Hilfe von Crowd-Annotation. Unsere Datensammlung umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensammlungsmethodik betont die Informalität mit einem Cartoon-Setup. Der Cartoon hat drei Sprachblasen. In der ersten Sprachblase sagt Bob: "Erinnerst du dich an das Lied, das wir gestern gehört haben?" Und mit dieser Aussage setzt Bob den Dialogkontext. In der zweiten Sprachblase sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Dies ist die alternative Frage. Und in der dritten Sprachblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel "die neuere". Wir liefern die ersten und zweiten Sprachblasen automatisch, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprachblase wird aus einigen manuellen Prompts pro Domäne ausgewählt. Die zweite, die alternative Frage, wird wie folgt generiert. Wir verwenden immer ein einfaches Template: Meinst du A oder B? Wo A und B Beispiele aus Wikipedia sind. Hier sind die verschiedenen Stichprobenmethoden, die wir verwendet haben. Wenn wir höher in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Unterscheidung zu treffen. Die erste ist zufällig einheitlich. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxen oder Attribute auf Wikipedia haben. Zum Beispiel, das gleiche Genre oder der gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, kennen sie die Namen dieser Entitäten, aber sie kennen die Entitäten nicht unbedingt. Was wir also tun, ist, dass wir den Annotatoren einige Hintergrundinformationen über die beiden Entitäten zeigen. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten die Annotatoren, zumindest ein wenig von jedem Lied anzuhören und über jedes Lied zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied "Easy on Me". Für die Domänen Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, ebenfalls aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Referenzausdrücken zu beschreiben. Zum Beispiel "die mit dem Klavierspiel". Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "die ohne Worte", "nicht die mit dem 12-jährigen Jungen" oder "die fiktive" oder "kommt aus Aserbaidschan" und so weiter. Die AltEntities Korpus umfasst 6.000 alternative Fragen in drei Domänen und 42.000 indirekte Referenzausdrücke. Die Ergebnisse mit dem T5 XL Modell sind unten zusammengefasst. Wenn die Sprachmodelle Zugang zu denselben Hintergrundinformationen haben wie die Annotatoren, ist die Genauigkeit sehr hoch, etwa 92 bis 95%. Aber das ist nicht realistisch. Wenn die Sprachmodelle Zugang zu teilweise überlappenden Hintergrundinformationen haben, liegt die Genauigkeit zwischen 82 und 87%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur Zugang zu den Namen der Entitäten hat, liegt die Genauigkeit nur bei 60%, also gibt es viel Raum für Verbesserung. Wir haben auch gezeigt, dass die Modelle domänenübergreifend einsetzbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">Der Ansatz wird mit den bestehenden SimulST-Richtlinien Wait-k und Local Agreement verglichen, die auf Offline-Modellen angewendet werden, sowie mit dem state-of-the-art Architektur, die speziell für simultane Vorübersetzung entwickelt wurde.</sample>
    <sample id="144">Nantes University.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Dieser Vortrag behandelt die Analyse von Auslassungen in der Dialogzusammenfassung, einem Subbereich der Textzusammenfassung, der darauf abzielt, wichtige Informationen aus Dialogen prägnant darzustellen. Trotz Fortschritten durch große vortrainierte Sprachmodelle leiden Zusammenfassungen oft unter Fehlern, insbesondere Auslassungen, die zu unvollständigen Zusammenfassungen führen. Die Studie zeigt, dass etwa 70% der generierten Zusammenfassungen Auslassungen aufweisen, was die Schwere des Problems unterstreicht. Die Analyse zeigt, dass ausgelassene Informationen zufällig im Dialog verteilt sind, was die Identifizierung von Schlüsselinformationen erschwert. Um dieses Problem zu adressieren, wurde das OLDS-Dataset entwickelt, das hochwertige Auslassungsetiketten für Dialogzusammenfassungen bereitstellt. Es basiert auf fünf bestehenden Benchmarks und umfasst verschiedene Modelle und Decodierungsstrategien. Drei Baseline-Modelle wurden untersucht, um die Auslassungserkennung zu bewerten, wobei die Ergebnisse eine F1-Score von etwa 50% zeigten, was die Herausforderung des Problems verdeutlicht. Eine Post-Editing-Methode zur Zusammenfassungsverbesserung, die ausgelassene Inhalte integriert, zeigte eine deutliche Leistungssteigerung, was die Bedeutung der Auslassungserkennung und -korrektur für die Qualität der Dialogzusammenfassung hervorhebt.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler, und ich werde kurz das Papier "Attention as a Guide for Simultaneous Speech Translation" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Simultane Sprachübersetzung, oder SimulST, ist der Prozess der Echtzeit-Übersetzung gesprochener Sprache in Text in einer anderen Sprache, um die zwischenmenschliche Kommunikation über Sprachgrenzen hinweg zu ermöglichen. Und was sind die Probleme der aktuellen SimulST-Modelle? Spezifische Architekturen werden in der Regel trainiert, wobei zusätzliche Module optimiert werden müssen. Lange und komplexe Trainingsverfahren, zum Beispiel Training mit unterschiedlichen Optimierungszielen. Und das Training und die Wartung mehrerer Modelle, um unterschiedliche Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Also, was ist unsere Lösung? Zuerst, bestehende Offline-ST-Modelle ohne Neutrainieren oder die Annahme spezifischer Architekturen für SimulST zu verwenden. Nur ein Modell für jedes Latenzregime zu verwenden und die Latenz durch spezifische Parameter zu handhaben. Und das Wissen zu nutzen, das das Modell durch die Aufmerksamkeitsmechanismen zwischen Audioeingabe und Textausgabe erworben hat. Das ist der Querverweismechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung ist es, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob eine teilweise Übersetzung ausgegeben werden soll oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, wenn ihre Summe unter einem bestimmten Schwellenwert alpha für die letzten lambda Sprachframes liegt, was bedeutet, dass die empfangenen Informationen stabil genug sind. Zum Beispiel, wenn wir einen Sprachchunk erhalten, der "Ich werde über..." enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Querverweisgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachframes zeigen, während das letzte Wort auf die letzten empfangenen Sprachframes zeigt, als lambda Sprachframes. Das bedeutet, dass die ersten beiden Wörter ausgegeben werden, während das letzte Wort, da die Summe der Querverweis-Aufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, nicht ausgegeben wird und wir auf einen weiteren Sprachchunk warten. Wenn wir fortfahren und einen weiteren Sprachchunk erhalten, und unser Modell drei weitere Wörter vorhersagt, und wir uns die Querverweisgewichte dieser Wörter ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachframes zeigt. Das bedeutet, dass diese drei Wörter ausgegeben werden. Wenn wir uns die Hauptergebnisse von EDAtt ansehen, werden wir die simultanen Sprachübersetzungsergebnisse in Diagrammen darstellen, in denen wir auf der einen Seite den BLEU-Wert haben, der die Übersetzungsqualität misst, und die durchschnittliche Verzögerung, die das Latenzmaß ist, und wir berücksichtigen auch die rechenzeitbewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage der Ausgabe berücksichtigt. Wir möchten also, dass unsere Kurven so hoch wie möglich in diesem Diagramm sind. Wir möchten auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf Offline-Modellen angewendet werden, wie die Wait-k-Strategie und die Lokale Übereinstimmung. Und wir vergleichen auch mit dem aktuellen Stand der Technik in Architekturen, die speziell für simultane Vorübersetzung entwickelt wurden. Das sind alle Ergebnisse der simultanen Sprachübersetzungstrategie auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächlich verstrichene Zeit oder die rechenzeitbewusste Zeit berücksichtigen, dass dies die schnellste Strategie ist. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier. Und wir haben auch den Quellcode und die Modelle sowie die simultane Ausgabe offen veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Der Text gibt keine Informationen darüber, ob der CoNLL++ Datensatz öffentlich zugänglich ist.</sample>
    <sample id="150">Dieses Papier präsentiert MeetingQA, ein neues extraktives Frage-Antwort-Dataset, das auf Fragen basiert, die in Meeting-Transkripten gestellt werden, und die entsprechenden Antwortabsätze. MeetingQA adressiert die Lücke in der NLP-Forschung, die sich bisher hauptsächlich auf die Zusammenfassung und die Extraktion von Aufgabenpunkten konzentriert hat, und nutzt die inhärenten QA-Elemente in Meetings. Das Dataset basiert auf dem AMI-Korpus mit fast 100 Stunden transkribierter mehrseitiger Meetings. Es enthält 7.7K Fragen, wobei 30% unbeantwortbar sind, 40% mehrere Antwortabsätze haben und 48% mehrere Sprecher beinhalten. Die Fragen sind oft rhetorisch (20%) und erfordern Diskussionen, wobei 70% der mehrseitigen Antworten Meinungsverschiedenheiten aufweisen. Die Fragen und Antworten sind durchschnittlich 12 bzw. 35 Wörter lang. Die menschliche Leistung auf dem Testset erreicht eine F1 von 84.6. Verschiedene Modelle wurden getestet, darunter kurze Kontextmodelle mit Kontext-Retrieval und mehrere Spannmodelle. Die Ergebnisse zeigen eine signifikante Lücke zwischen Modell- und menschlicher Leistung, insbesondere in der Zero-Shot-Einstellung. Die Analyse der Fehler zeigt Schwierigkeiten bei der Identifizierung rhetorischer Fragen und der Zuordnung von Antworten zu Sprechern. MeetingQA stellt eine neue Herausforderung für bestehende QA-Modelle dar und bietet ein reichhaltiges Potenzial für zukünftige Forschung.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang wird mit mir unsere Forschung zu MultiInstruct vorstellen, die Multi-Modale Zero-Shot-Lernen durch Anweisungsschulung verbessert. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen erforscht, um vortrainierte Sprachmodelle für verschiedene Downstream-Aufgaben auf eine parametrische und dateneffiziente Weise zu wiederverwenden. Kürzlich haben viele Studien gezeigt, dass Anweisungsschulung große Sprachmodelle befähigt, auf unerkannte Aufgaben in einem Zero-Shot-Modus zu arbeiten, indem sie natürliche Anweisungen befolgen. Dennoch konzentrierten sich die meisten vorherigen Arbeiten zur Anweisungsschulung darauf, die Zero-Shot-Leistung bei sprachbasierten Aufgaben zu verbessern, während die Computer Vision und die Multi-Modale Aufgaben vernachlässigt wurden. Daher möchten wir in dieser Arbeit untersuchen, ob die Anweisungsschulung eines multi-modalen vortrainierten Modells tatsächlich die Generalisierung auf unerkannte multi-modale Aufgaben verbessern kann. Darüber hinaus stellten wir bei unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Anweisungsdatensätzen zwischen NLP und Multi-Modal fest. Es gibt mehr als 1600 sprachbasierte Anweisungsaufgaben. Es gibt jedoch keinen großen, öffentlich verfügbaren multi-modalen Anweisungsaufgabe. Daher motiviert uns dies, einen multi-modalen Anweisungsschulungsdatensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multi-modalen Anweisungsschulungsbenchmark-Datensatz, der 62 vielfältige multi-modale Aufgaben umfasst, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet. Um die multi-modale Anweisungsschulung auf unserem vorgeschlagenen Datensatz zu untersuchen, verwenden wir OFA, ein einheitliches multi-modales vortrainiertes Modell, als Basismodell. OFA verwendet eine einheitliche Vokabel für Sprache, Bild-Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatenarten zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben im einheitlichen Sequenz-zu-Sequenz-Format. Dabei werden die Eingabetexte, Bilder, Anweisungen und Begrenzungsrahmen im gleichen Token-Raum dargestellt. Nun werde ich über die multi-modale Anweisungsschulung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und entnehmen 10.000 Instanzen pro Aufgabe. Für das Testen reservieren wir die gesamte Gruppe für allgemeines Wissen für das Testen und wählen zusätzlich 5 Aufgaben aus den VQ- und Miscellaneous-Gruppen aus. Wir verwenden alle Instanzen im Testsplit für jede Aufgabe. Zusätzlich entnehmen wir zufällig 20 Aufgaben aus dem Testsplit der natürlichen Anweisungen als unerkannte Aufgabe für NLP. Wir verwenden das vortrainierte große OFA-Modell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir über die Mindest- und Höchstleistung sowie die Standardabweichung der Leistung über alle 5 Experimente. Wenn die Aufgabe eine multi-modale Klassifizierungsaufgabe ist, berichten wir über die Genauigkeit. Wenn es sich um eine multi-modale Generierungsaufgabe handelt, berichten wir über Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls über Rouge-L. Wir führen auch ein zusätzliches Evaluationsmaß ein, das Sensitivität genannt wird. Dies misst die Fähigkeit des Modells, konsistent die gleichen Ausgaben für die gleiche Aufgabe zu produzieren, unabhängig von leichten Variationen in der Wortwahl der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Anweisungsschulung die Leistung von OFA bei erkannten multi-modalen Aufgaben erheblich verbessern. Auch das Transferlernen von natürlichen Anweisungsdatensätzen kann der Anweisungsschulung zugutekommen. Wie wir sehen können, verbessert sich die Leistung des Modells, je mehr Aufgaben es gibt, und gleichzeitig sinkt die Sensitivität. Wir führten auch ein Experiment durch. Wir verwenden eine Anweisung gegenüber fünf Anweisungen. Wie wir sehen können, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Modellsensitivität. Wie wir sehen können, kann das Transferlernen von natürlichen Anweisungsdatensätzen dem Modell eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell verleihen. Wir können auch sehen, dass das Transferlernen von natürlichen Anweisungsdatensätzen OFA dabei hilft, eine viel bessere Leistung auf dem natürlichen Anweisungsdatensatz zu erreichen. Insgesamt schlagen wir den ersten großen multi-modalen Anweisungsschulungsdatensatz vor, der die kurzfristige Fähigkeit von OFA erheblich verbessert, und wir erforschen verschiedene Transferlern-Techniken und zeigen deren Vorteile. Wir entwerfen ein neues Maß, das Sensitivität genannt wird. Noch etwas, wir sammeln einen viel größeren multi-modalen Anweisungsschulungsdatensatz mit rund 150 zusätzlichen visuellen Sprachaufgaben und werden sie veröffentlichen. Hier ist ein QR-Code für unsere Daten und unser Modell. Vielen Dank.</sample>
    <sample id="152">In der Präsentation "Exploring Large Language Models for Classical Philology" stellt Frederick Riemenschneider die Entwicklung neuer Sprachmodelle für die klassische Philologie vor. Trotz der Einführung von monolingualen Modellen wie Latin BERT und Ancient Greek BERT besteht weiterhin Bedarf an Modellen, die sowohl Altgriechisch als auch Latein verarbeiten können. Die vorgestellten Modelle, GreBERTa und GreTa, sind monolingual für Altgriechisch und basieren auf RoBERTa bzw. T5-Architekturen. PhilBERTa und PhilTa sind multilinguale Modelle, die Altgriechisch, Latein und Englisch verarbeiten. Die Modelle wurden mit einem neuen, qualitativ hochwertigen Korpus aus dem Internet Archive trainiert, das durch die Identifizierung von falsch transkribierten griechischen Stoppwörtern erstellt wurde. Die Modelle wurden anhand von Aufgaben wie POS-Tagging, Abhängigkeitsanalyse und Lemmatisierung evaluiert und übertrafen die bestehende State-of-the-Art. Besonders die encoder-decoder Modelle zeigten Stärken in der Lemmatisierung. Untersuchungen zur semantischen und weltlichen Wissensfähigkeit der Modelle zeigten, dass die multilingualen Modelle die monolingualen nicht signifikant übertreffen. Die Präsentation betont die Bedeutung der Entwicklung spezialisierter Modelle für die klassische Philologie und bietet einen Überblick über die Methoden und Ergebnisse.</sample>
    <sample id="153">In der Arbeit "Resolving Ambiguities in Text-to-Image Generative Models" untersucht Ninareh Mehrabi Ambiguitäten in Text-zu-Bild-Modellen. Die Studie konzentriert sich darauf, wie unklare Anweisungen zu unterschiedlichen Interpretationen führen können, was die Erzeugung von Bildern, die den Benutzerabsichten entsprechen, erschwert. Ein Beispiel ist der Satz "The girl enters the room with flowers", der unklar lässt, ob die Blumen mit dem Mädchen oder im Raum sein sollen. Um diese Ambiguitäten zu adressieren, wird ein Rahmenwerk vorgeschlagen, das sowohl zur Minderung als auch zur Bewertung dieser Unklarheiten dient. Ein modifiziertes LAVA-Korpus dient als Benchmark-Dataset, das verschiedene Ambiguitätstypen abdeckt. Ein Sprachmodell generiert entweder klärende Fragen oder mögliche visuelle Interpretationen, um die Anweisungen zu präzisieren. Die disambiguierten Anweisungen werden dann in Text-zu-Bild-Modelle eingegeben, um die Bildgenerierung zu bewerten. Ein automatisiertes Bewertungssystem, das ein VQA-Modell verwendet, überprüft, ob die generierten Bilder den Benutzerabsichten entsprechen. Die Ergebnisse zeigen, dass die vorgeschlagene Methode die Erzeugung von Bildern, die den Benutzerabsichten entsprechen, verbessert und dass das automatisierte Bewertungssystem mit menschlichen Bewertungen übereinstimmt. Die Arbeit schließt mit der Feststellung, dass die vorgeschlagenen Rahmenwerke effektiv zur Minderung und Bewertung von Ambiguitäten in Text-zu-Bild-Modellen beitragen.</sample>
    <sample id="154">Die Autoren gehören der Universität von Trient an.</sample>
    <sample id="155">Der/die Referent*in ist Bob.</sample>
    <sample id="157">Das Papier "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" von Shen Gao und Kollegen präsentiert ein Modell zur Zusammenfassung von Dialogen, das statische und dynamische Graphstrukturen kombiniert. Ziel ist es, wichtige Informationen aus komplexen, mehrteiligen Dialogen effizient zu extrahieren. Traditionelle Methoden nutzen vorab berechnete statische Graphstrukturen, die jedoch von der Zuverlässigkeit externer linguistischer Werkzeuge abhängen und nicht dynamisch anpassbar sind. Das vorgestellte SDDS-Modell umfasst vier Hauptkomponenten: einen Utterance Encoder, eine statische Graphkonstruktion, einen Static-Dynamic Graph-Modul und einen Summary Generator. Der Static-Dynamic Graph-Modul kombiniert mehrere statische Graphen und nutzt einen dynamischen Graphen, um semantische Beziehungen basierend auf tiefen Vektorrepräsentationen zu erfassen. Vier heuristische Methoden werden zur Modellierung der statischen Dialogstruktur verwendet, darunter ein Diskurs-Parsing-Graph und eine Methode zur Modellierung der Sprecherinteraktion. Ein dynamischer Graph wird mittels eines multi-head attention Modells berechnet. Die Integration von statischen und dynamischen Graphen erfolgt durch eine Fusion der Beziehungs- und Adjazenzmatrix. Ein dualer Cross-Attention-Mechanismus wird verwendet, um die Graphrepräsentation in den Generierungsprozess zu integrieren. Das Modell zeigt, wie die Kombination von statischen und dynamischen Strukturen die Zusammenfassung von Dialogen verbessern kann. Code und Daten sind auf GitHub verfügbar.</sample>
    <sample id="158">Der Vortrag von Qipeng Guo von AWS stellt die Arbeit "Dual Cache for Long Document Neural Coreference Resolution" vor. Die Aufgabe der Koreferenzauflösung besteht darin, Erwähnungen von Entitäten in einem Dokument zu identifizieren und sie zu gruppieren, wenn sie dieselbe Entität bezeichnen. Traditionelle Methoden erfordern die Paarbildung aller Erwähnungen, was zu quadratischer Komplexität führt. Kürzlich vorgeschlagene cachebasierte Methoden reduzieren dies auf lineare Komplexität durch die Verwendung eines festen Caches mit einem LRU-Evictionspolitik. In langen Dokumenten führt jedoch die häufige Themenwechsel dazu, dass LRU zu hohen Cache-Miss-Raten führt, insbesondere bei häufig erwähnten globalen Entitäten.

Um dieses Problem zu lösen, schlagen die Autoren einen Dual Cache vor, der einen lokalen und einen globalen Cache kombiniert. Der lokale Cache verwendet LRU für lokale Entitäten, während der globale Cache LFU für globale Entitäten verwendet. Der Dual Cache klassifiziert neue Erwähnungen und fügt sie basierend auf ihrer Häufigkeit in den entsprechenden Cache ein. Die Evaluation auf vier öffentlichen Benchmarks zeigt, dass der Dual Cache die Baseline-Methoden übertrifft, insbesondere bei langen Dokumenten, und die Cache-Miss-Raten signifikant reduziert. Der Dual Cache bietet das beste Verhältnis von Leistung zu Kosten im Vergleich zu Einzel-Cache-Methoden.</sample>
    <sample id="159">Hallo, alle zusammen. Ich bin Koustav Sinha, und ich freue mich, Sie zu unserer Diskussion über unser ACL 2023 Papier willkommen zu heißen. Sprachmodellakzeptabilitätsurteile sind nicht immer robust gegenüber dem Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit setzen wir uns mit den minimalen Paar-Paradigmen auseinander. Das minimale Paar-Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen, die auch Grammatikalität wie bei BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotype wie bei CrowS Pairs umfassen können. Im typischen minimalen Paar-Paradigma wird ein Sprachmodell einem akzeptablen oder grammatikalischen Satz und einem akzeptablen oder ungrammatikalischen Satz gezeigt. Die Hoffnung ist, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweist. Der aktuelle MPP-Pipeline erlaubt es uns nicht, die Akzeptanz eines Modells gegenüber längeren Sätzen zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern. Es ist daher entscheidend, dass wir die Modelle hinsichtlich ihrer Akzeptanz über das gesamte Kontextfenster bewerten, und das ist unser Ziel. Wir versuchen, die MPP-Pipeline zu überarbeiten, indem wir das Modell auffordern, die Akzeptanz über längere und längere Sequenzen zu bewerten. Dazu simulieren wir längere Sequenzen, indem wir die Datensätze selbst überarbeiten und Sätze neu erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir ein typisches Paar aus dem BLiMP-Datensatz im Fall der Adjunct Island ausgewählt. Um längere akzeptable Sequenzen mit der gleichen grammatischen Struktur zu erstellen, extrahieren wir grammatikalische Sätze aus Adjunct Island und fügen sie als Präfix zu beiden akzeptablen und unakzeptablen Anfragen hinzu. Das Gleiche können wir tun, indem wir unakzeptable Sätze aus der gleichen Struktur auswählen, um die Akzeptanz der Modelle zu testen. Wir können auch Sätze aus einem anderen Subset oder einem anderen Datensatz wählen. Das nennen wir die Mismatch-Situation. Die Sätze stammen immer noch aus relevanten Datensätzen, aber nicht aus demselben Datensatz, den wir bewerten. Das Gleiche können wir für den Unakzeptabilitätsfall tun. Schließlich können wir Sätze aus einem völlig unverwandten Bereich wie Wikipedia wählen. Das zeigt uns, ob die Akzeptanzurteile der Modelle tatsächlich durch den Kontext beeinflusst werden, ob der Kontext aus einem anderen Subset des Datensatzes stammt oder völlig irrelevant für den aktuellen Satz ist. Wie schneidet das Modell ab? Zuerst schauen wir uns die Wikipedia-Sätze an, die völlig irrelevant für das aktuelle Anfragepaar sind, und finden heraus, dass die MPP-Urteile bei zufälliger Kontextlänge größtenteils robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT-2-Modelle auszureizen. Dort sehen wir, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz wählen? Hier erstellen wir Sätze aus akzeptablen und unakzeptablen Domänen aus demselben BLiMP- oder SyntaxGym-Datensatz. Dort sehen wir, dass die MPP-Urteile erheblich steigen oder fallen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Wenn wir die Struktur abgleichen, also Sätze aus demselben Phänomen in BLiMP oder SyntaxGym wählen, sehen wir einen massiven Anstieg oder Abfall der MPP-Urteile für das Modell, je nachdem, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist sehr groß und nimmt mit zunehmender Kontextlänge zu, was wahrscheinlich neue Sprachmodelle mit großen Kontextfenstern beeinflussen wird. Warum beeinflusst das abgestimmte Präfix das Urteil des Sprachmodells so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir die Eingabesätze durch Hinzufügen von Rauschen zu den relevanten Strukturen verändert haben. Nach mehreren solcher Veränderungen stellten wir fest, dass keines dieser Rauschen das Modell dazu brachte, seine MPP-Urteile zu ändern. Wir fanden heraus, dass die Modelle empfindlich auf die veränderten Sätze in ähnlicher Weise reagieren. Wenn wir Sätze aus dem akzeptablen Bereich verändern, sehen wir einen ähnlichen Anstieg bei allen Veränderungen, und wenn wir Sätze aus dem unakzeptablen Bereich verändern, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die sich über Sätze erstrecken. Die aktuelle MPP-Bewertung mit kurzen und einzelnen Satzeingaben mag das abstrakte Wissen der Sprachmodelle über das gesamte Kontextfenster nicht vollständig erfassen. Bitte lesen Sie unser Papier für weitere Details zu unseren Experimenten. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem unsortierten Multiset von Tokens zugeordnet, die im Output erscheinen werden.</sample>
    <sample id="161">55,000 Skripte sind in CoScript vertreten.</sample>
    <sample id="163">Die beste automatische Ausrichtungsmethode für DEPLAIN ist die Methode von MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen (WSL) besteht darin, dass es die Kosten für die manuelle Annotation von Daten reduziert, indem es auf schwache Labelquellen wie einfache Heuristiken, Wissensbasen oder niedrigwertige Crowdsourcing-Daten zurückgreift. Dies macht die Annotationen viel günstiger, obwohl sie auch fehleranfällig sind. WSL-Methoden zielen darauf ab, robuste neuronale Netzwerke unter solchen Labelrauschen zu trainieren, um eine gute Generalisierung zu erreichen.</sample>
    <sample id="165">In der Arbeit "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations" präsentieren Wenting Zhao und Kollegen eine neue Methode zur abduktiven Schlussfolgerung, die auf unsupervised Lernen setzt. Die Studie adressiert die Herausforderung, plausible Erklärungen für gegebene Kontexte und Ergebnisse zu identifizieren, ohne auf annotierte Daten angewiesen zu sein. Die vorgestellte Methode, LiPoR (Likelihood Learning with Posterior Regularization), behandelt Erklärungen als latente Variablen und maximiert die marginale Wahrscheinlichkeit des Ergebnisses, gegeben den Kontext, ohne explizit plausible Erklärungen zu kennen. Um plausible Erklärungen zu bevorzugen, wird eine Regularisierung eingeführt, die auf der gegenseitigen Ausschließlichkeit von Erklärungen basiert. LiPoR übertrifft bestehende zero-shot Modelle und den bisher besten unsupervised Ansatz auf dem AlphaNLI-Datensatz um über 4 absolute Punkte in der Genauigkeit, einschließlich eines starken GPT-3 Baselines. Diese Arbeit zeigt, dass es möglich ist, abduktive Schlussfolgerung ohne Supervision zu lernen, indem sie die inhärente Struktur von Erklärungen nutzt.</sample>
    <sample id="166">Dieses Papier präsentiert einen neuartigen Ansatz, den Neural Divide-and-Conquer Reasoning Framework (NDCR), zur Verbesserung der Bildabfrage aus sprachlich komplexen Texten. Die Herausforderung besteht darin, dass typische visuelle Sprachmodelle bei komplexen Texten an Leistung verlieren, da sie sich hauptsächlich auf analoges Denken (System 1) konzentrieren. Der NDCR nutzt die Divide-and-Conquer-Strategie und die Dual-Process-Theorie, um sowohl analoges als auch logisches Denken (System 2) zu integrieren. Der Ansatz umfasst drei Hauptkomponenten: den Proposition Generator, der komplexe Texte in einfache Propositionen zerlegt; den Visual-Linguistic Interactor, der visuelle und propositionale Informationen analog verarbeitet; und den Neural-Symbolic Reasoner, der logische Operationen wie Negation und Konjunktion durchführt, um die endgültige Lösung zu erzielen. Experimentelle Ergebnisse zeigen, dass der NDCR andere Baseline-Methoden übertrifft und die Wirksamkeit jedes Moduls durch Abolitionsexperimente bestätigt wird. Der Ansatz demonstriert interoperables Verarbeiten und bietet Einblicke in die Verbesserung der kompositionalen Schlussfolgerung und Planung großer Sprachmodelle durch die Integration von Dual-Process-Theorie und Divide-and-Conquer-Strategien.</sample>
    <sample id="167">In DEPLAIN-web wurden 750 Dokumente sowohl manuell als auch mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem Daten von Reuters News aus dem Jahr 2020 gesammelt und mit den gleichen CoNLL-2003-Annotationrichtlinien annotiert wurden.</sample>
    <sample id="169">Dieses Papier präsentiert die erste systematische Studie zur Verwendung von Sprachmodell-Prompting für die maschinelle Übersetzung, insbesondere mit dem PaLM-Modell. Die Autoren bewerten die Übersetzungsfähigkeit von PaLM unter Verwendung der besten Praktiken der Übersetzungsgemeinschaft, einschließlich der Verwendung aktueller Testsets und Vergleichen mit den besten Systemen der WMT-Bewertung. Die Studie zeigt, dass die Wahl der Prompting-Strategie einen erheblichen Einfluss auf die Leistung hat, wobei ein 5-Schuss-Prompting mit markierten Sprachen die besten Ergebnisse liefert. Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Quelltext. Obwohl spezialisierte Systeme immer noch einen Vorteil haben, erreicht PaLM eine vergleichbare Flüssigkeit, leidet jedoch unter Genauigkeitsproblemen, insbesondere bei Auslassungsfehlern. Die Ergebnisse legen nahe, dass PaLM nahe an kommerziellen Systemen wie Google Translate liegt, jedoch mit Herausforderungen in der Übersetzungspräzision.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute präsentiere ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". Semantische Parsing ist die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen. Wir verwenden neuronale Modelle, um Anfragen in mehreren natürlichen Sprachen in SQL, Lambda oder FunQL und andere zu übersetzen. Bestehende cross-linguale semantische Parsing-Modelle wurden separat für Datensätze mit begrenzten Aufgaben und Anwendungen vorgeschlagen und evaluiert. Zum Beispiel gibt es viele Abdeckungen für bestimmte natürliche Sprachen, aber Chinesisch fehlt und es gibt eine unzureichende Abdeckung für bestimmte Bedeutungsrepräsentationen. Lambda-Kalkül fehlt oder sie wurden nur für bestimmte neuronale Modelle evaluiert. Zum Beispiel wurde nur ein einzelnes Modell zur Bewertung verwendet. Daher schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für cross-linguales semantisches Parsing in mehreren natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unsere Benchmark besser zu bewerten, berücksichtigen wir sechs Trainings- und Evaluierungsszenarien. Das erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quellsprache in die Zielsprache zu übersetzen, und verwenden dann ein monolinguales Modell zum Training und zur Bewertung. Zum Beispiel trainieren wir das englische Modell mit englischen Anfragen und übersetzen während der Inferenz die deutsche Anfrage mit der API ins Englische, um die SQL-Vorhersage mit dem trainierten Modell zu treffen. Wir testen auch das Monolinguale Modell, bei dem die Quellsprache mit der Zielsprache übereinstimmt, z. B. Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch das Monolinguale Few-shot-Szenario, indem wir monolinguale Modelle mit nur 10 % des Trainingsdatensatzes trainieren. Wir testen das Multilinguale Modell, bei dem wir ein multilinguales Modell für alle Sprachen trainieren. Zum Beispiel trainieren wir mit deutschen, englischen und chinesischen Anfragen ein multilinguales Modell. Während der Inferenz können wir dieses Modell verwenden, um deutsche Anfragen oder chinesische Anfragen zu übersetzen, usw. Wir berücksichtigen auch Cross-linguale Zero-shot und Few-shot-Übertragungen. Wir trainieren auf einer Quellsprache und übertragen auf eine andere Sprache. Während des Trainings trainieren wir es mit englischen Anfragen oder einer Kombination aus englischen und deutschen Few-shot-Anfragen, um ein multilinguales Modell zu trainieren, das die SQL-Ausgabe vorhersagt. Wir finden auch viele interessante Ergebnisse. Bei der Analyse von monolingualen Modellen bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, was Multilinguale Vorabtrainierte Encoder mit Zeigerbasierten Decodern bedeutet, wie XLM-R + PTR und mBERT + PTR. Wir bewerten auch Encoder-Decoder-Modelle, die Multilinguale Vorabtrainierte Encoder-Decoder-Modelle sind, wie mBART und mT5. Wir fanden heraus, dass Encoder-Decoder auf allen neun Datensätzen die beste Leistung erbringt. Wir bewerten mT5 und XLM-R + PTR im multilingualen Szenario. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung verschiedener Sprachen verbessert werden kann. Wir fanden heraus, dass dies daran liegt, dass die meisten der wichtigsten natürlichen Sprachen Leistungsgewinne erzielen, außer dass die Leistung des Englischen in sieben Datensätzen abnimmt und nur in drei Datensätzen zunimmt. Ich denke, dies ist als "Fluch der Multilingualität" bekannt. Wir vergleichen auch die cross-linguale Leistungsunterschiede. In dieser Abbildung ist die blaue Linie die Cross-linguale Few-shot-Übertragung. Die orangefarbene Linie ist die Cross-linguale Zero-shot-Übertragung. Während die grüne Linie die Monolinguale Einstellung ist. Wir fanden heraus, dass, indem wir die grüne und orangefarbene Linie vergleichen, der Zero-shot-Übertragungsleistungsunterschied signifikant ist, und dann, indem wir die blaue und orangefarbene Linie vergleichen, der Übertragungsunterschied im Few-shot-Szenario schnell verkürzt wird. Wir finden auch andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vortraining auf Englisch als natürliche Sprache kann die Leistung von Few-shot auf Ziel-Natürliche Sprachen erheblich steigern, und wir fanden heraus, dass multilinguale Sprachmodelle wie Codex und BLOOM immer noch unzureichend für cross-linguale semantische Parsing-Aufgaben sind. Zusammenfassend haben wir XSemPLR aufgebaut, eine einheitliche Benchmark für cross-linguales semantisches Parsing mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. Wir führen eine umfassende Benchmark-Studie auf drei repräsentativen Arten von multilingualen Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Und wir laden Sie ein, unsere Arbeit und den Code zu besuchen. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Bisherige Arbeiten zur Schutz des Urheberrechts von Embedding-as-a-Service umfassen Methoden zur Einbettung von Wasserzeichen, die jedoch entweder nicht auf Embedding-as-a-Service anwendbar sind oder an Übertragbarkeit mangeln. Diese bestehenden Methoden werden in vier Kategorien eingeteilt.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex oder Bloom sind für Cross-Lingual Semantic Parsing (CLSP) noch nicht ausreichend.</sample>
    <sample id="174">Das Papier "ArgAnalysis35K: A Large-Scale Dataset for Argument Quality Analysis" stellt ein einzigartiges Datenset vor, das Argumentqualität bewertet. Es enthält 35.000 Argument-Analyse-Paare, die größtenteils aus hochwertigen Quellen wie Debattenturnieren und Experten stammen. Im Gegensatz zu bestehenden Datensätzen, die oft auf Crowdsourcing basieren und eine begrenzte Vielfalt aufweisen, bietet ArgAnalysis35K eine breite Palette von 24 Themen, die aus verschiedenen Quellen stammen. Ein innovatives Merkmal ist die Einführung von "Analysis", einer Kombination aus Behauptungen und Prämissen, die Argumente besser erklärt. Das Datenset nutzt eine instanzbasierte Annotator-Reliabilität, um menschliche Voreingenommenheiten zu berücksichtigen, ohne die gesamte Arbeit eines Annotators zu verwerfen. Zudem wird ein Relevanzmodell eingeführt, das Argumente auf einer Skala von 0 bis 1 bewertet, um ihre Relevanz für verschiedene Themen zu bestimmen. Diese Merkmale machen ArgAnalysis35K zu einem vielseitigen und zuverlässigen Ressourcenwerkzeug für die Analyse von Argumentqualität.</sample>
    <sample id="175">Die Methode adressiert die Mehrdeutigkeit der Permutationen, indem sie eine GPU-freundliche kontinuierliche Relaxation verwendet, die es ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen. Dies hilft, das Problem der NP-Schwierigkeit, das mit der Suche nach der höchstbewerteten Permutation verbunden ist, zu umgehen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert durch die Fähigkeit des Modells, unabhängig von politischen Vorurteilen oder sozialen Kategorien konsistente und gerechte Ergebnisse zu liefern. Dies beinhaltet, dass das Modell nicht systematisch bestimmte Gruppen oder politische Meinungen bevorzugt oder benachteiligt, insbesondere bei Aufgaben wie der Erkennung von Hassrede oder Falschmeldungen. Die Fairness wird durch die Analyse der Leistung des Modells in verschiedenen demografischen oder politischen Kategorien bewertet, um sicherzustellen, dass es keine Vorurteile gegenüber bestimmten Gruppen gibt.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Dieser Vortrag von Melanie Sclar präsentiert "SymbolicToM", eine Methode zur Verbesserung der Theory of Mind (ToM) Fähigkeiten von großen Sprachmodellen (LLMs) durch explizite grafische Darstellungen. ToM bezieht sich auf die Fähigkeit, die mentalen Zustände anderer zu verstehen, oft durch falsche Glaubensfragen getestet. Große LLMs wie ChatGPT und GPT-3 zeigen Schwächen bei solchen Aufgaben. SymbolicToM verwendet mehrere grafische Darstellungen, um die mentalen Zustände von Charakteren in Geschichten zu modellieren, und ermöglicht es, Fragen effizient zu beantworten, indem es diese Graphen nutzt. Die Methode wurde mit verschiedenen LLMs getestet und zeigte signifikante Leistungssteigerungen, insbesondere bei zweiter Ordnung falscher Glaubensfragen. Experimente mit den ToMi-Daten und neuen, herausfordernden Datensätzen (D₁, D₂, D₃ und ParaphrasedToMi) zeigten, dass SymbolicToM die Leistung von LLMs verbessert, insbesondere bei der Generalisierung auf neue Strukturen und linguistische Variationen. Im Vergleich zu überwachten Baselines übertraf SymbolicToM diese deutlich, insbesondere bei der Bewältigung von außerhalb des Domänenverständnisses. Die Methode bietet interpretierbare Schlussfolgerungen und vermeidet das Risiko des Overfittings. Weitere Details sind im zugehörigen Papier zu finden.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">Dieses Papier stellt das Problem des eingeschränkten Sprachplanens vor, bei dem spezifische Ziele mit vielfältigen Einschränkungen berücksichtigt werden. Während große Sprachmodelle (LLMs) abstrakte Ziele in Schritte zerlegen können, ist ihre Leistung bei spezifischen, eingeschränkten Zielen unzureichend. Um dies zu adressieren, entwickeln die Autoren eine Methode zur Erstellung eines Datensatzes namens CoScript, der spezifische Ziele mit Skripten umfasst. Sie verwenden InstructGPT, um spezifische Ziele zu generieren und ein "über-generieren-und-filtern"-Verfahren anzuwenden, um die Qualität der Skripte zu verbessern. Dies beinhaltet die Generierung mehrerer Skripte und die Auswahl derjenigen, die den Einschränkungen am besten entsprechen. Der CoScript-Datensatz, der 55.000 spezifische Ziele umfasst, zeigt eine hohe Vielfalt und ermöglicht die Schulung kleinerer, spezialisierter Modelle wie T5, die in der Qualität der generierten Skripte größere LLMs übertreffen können. Die Studie zielt darauf ab, die Forschung im Bereich des Sprachplanens voranzutreiben, indem sie eine wertvolle Ressource in Form des CoScript-Datensatzes bereitstellt.</sample>
    <sample id="182">Im Zusammenhang mit dieser Arbeit bezieht sich Tropikalismus auf ein Stereotyp, das mit lateinamerikanischen Frauen verbunden ist, indem sie als "vibrant" und "curvaceous" beschrieben werden. Dieses Stereotyp verbindet lateinamerikanische Frauen mit tropischen und exotischen Bildern, was zu einer vereinfachten und essentialisierenden Darstellung ihrer Identität führt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie ähnliche Prompts wie die für die Sprachmodelle verwendeten Prompts an menschliche Teilnehmer weitergaben. Diese Prompts waren inspiriert von einem Studiendesign, das feststellte, dass solche Prompts bei menschlichen Teilnehmern ebenfalls rassistische Stereotype hervorrufen können.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextual Mutual Information) verwendet, um die Kontextnutzung zu messen. Es wurde erweitert zu Pointwise CXMI (P-CXMI), um die Kontextnutzung auf Satz- oder Wortebene zu messen.</sample>
    <sample id="185">DrBERT ist ein auf RoBERTa basierendes Modell, das auf NACHOS, einem Datensatz mit medizinisch gecrawlten Daten aus dem Web, trainiert wurde. ChuBERT hingegen basiert auf anonymisierten Daten aus dem Datenlager des Nantes University Hospital und umfasst sowohl klinische Notizen als auch Daten aus NACHOS. DrBERT konzentriert sich auf allgemeine medizinische Daten, während ChuBERT spezifischer auf klinische Daten ausrichtet.</sample>
    <sample id="187">Zwei Autoren, Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen beinhaltet das schrittweise Feinabstimmen eines Modells auf mehreren Aufgaben. Im Kontext des Papiers beinhaltet es das anfängliche Übertragen von Gewichten von eng verwandten Aufgaben (wie der Klassifizierung von Stimmungen in Debatten und der binären Klassifizierung von Erweiterungs- und Vergleichsklassen) und das anschließende schrittweise Feinabstimmen auf diesen Aufgaben, um die Leistung des Modells zu verbessern, bevor es für die aktive Lernphase verwendet wird.</sample>
    <sample id="189">Das Ziel des AltEntities Corpus ist es, das Verständnis von indirekten Bezugsausdrücken in der Sprache von Benutzern zu verbessern, wenn sie eine Auswahl zwischen Entitäten treffen möchten, insbesondere in konversationellen Systemen und zur Bewertung des Verständnisses von Entitäten durch große Sprachmodelle (LLMs).</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen Embedding as a Service (EaaS) extrahieren, indem er das EaaS lernt und ähnliche Dienste bereitstellt. Dies geschieht durch das Extrahieren von Informationen aus den bereitgestellten Embeddings, um ein ähnliches Modell zu erstellen.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">Yang Luo präsentiert "CAME: Confidence-guided Adaptive Memory Efficient Optimization", ein Optimierungsverfahren für die robuste Schulung großer Sprachmodelle. Traditionelle adaptive Methoden wie Adam erfordern dreifachen Speicher für Momentenschätzungen, während memory-effiziente Optimierer wie Adafactor Speicher sparen, jedoch mit Leistungseinbußen. CAME zielt darauf ab, schnelle Konvergenz und geringen Speicherverbrauch zu kombinieren. Inspiriert von der nicht-negativen Matrixfaktorisierung (NMF) und den Fehlern in Adafactor, schlägt CAME eine Methode vor, die die Instabilität in den Momenten berücksichtigt, um die Optimierungsschritte zu verbessern. Experimente auf BookCorpus und English Wikipedia zeigen, dass CAME im Vergleich zu Adam und Adafactor eine signifikante Leistungssteigerung bei der Schulung von BERT, GPT-2 und T5 bietet, mit einer Erhöhung der Validierungsgenauigkeit um etwa 3,4% bei gleicher Anzahl von Trainingsstufen. CAME reduziert den Speicherverbrauch erheblich, insbesondere bei großen Batch-Größen, und zeigt eine vergleichbare Leistung zu Baseline-Modellen bei geringerem Speicherverbrauch. CAME übertrifft Adam und Adafactor in der Schulung von BERT-Large und zeigt seine Effektivität in großen Batch-Trainingsumgebungen.</sample>
    <sample id="193">Der ursprüngliche Datensatz wurde von 10 Annotatoren erstellt.</sample>
    <sample id="194">Die Autoren gehören zu Carnegie Mellon University, der University of Washington und dem Allen Institute for AI.</sample>
    <sample id="195">Dieses Papier präsentiert RoHT, ein neuartiges Framework für erklärbares Frage-Antworten (XQA), das die Herausforderungen der Fragezerlegung und der Integration von Wissensquellen angeht. XQA zielt darauf ab, Fragen zu beantworten und Erklärungen für die ausgewählten Antworten zu liefern. Traditionelle Ansätze, wie neuro-symbolische Methoden und dekompositions-basierte Methoden, haben Einschränkungen, da sie entweder auf strukturierte Wissensbasen (KBs) angewiesen sind oder mit der Vielfalt natürlicher Sprache in freitextbasierten Korpora kämpfen. RoHT adressiert diese Probleme durch die Erstellung eines Hierarchischen Fragezerlegungsbaums (HQDT), der die strukturelle Komposition einer komplexen Frage darstellt. Der Baum beginnt mit der ursprünglichen Frage als Wurzel und zerlegt sie in Subfragen und atomare Fragen. RoHT führt probabilistische Schlussfolgerungen über den HQDT durch, um Wissen aus KBs und Textkorpora zu integrieren. Der Prozess umfasst die Auswahl geeigneter Wissensquellen, die Ausführung von Antworten mit Wahrscheinlichkeiten und die Aggregation von Kandidatenantworten. Die Evaluation auf den KQA Pro- und Musique-Datensätzen zeigt, dass RoHT bestehende Methoden übertrifft, indem es die Vorteile der expliziten Zerlegung und der Nutzung von Wissen aus verschiedenen Quellen demonstriert. RoHT verbessert die Leistung sowohl mit unvollständigen KBs als auch mit ergänzenden Textkorpora erheblich.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa."</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme umfasst die Verwendung von menschlichen Bewertungen, um die Qualität von Dialogen zu bewerten, wie z.B. durch Likert-Skalen oder Paarvergleiche. Diese Methoden bewerten die Gesamtqualität von Dialogen, aber es gibt ein wachsendes Interesse an der Bewertung mehrerer Dimensionen der Dialogqualität, um die Stärken und Schwächen von Modellen auf einer feineren Ebene zu verstehen. ABC-Eval ist eine neue Methode, die entwickelt wurde, um die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit annotiert, ob ein Modell bestimmte Verhaltensweisen zeigt, wie z.B. irrelevante Informationen oder Widersprüche. Diese Methode hat sich als zuverlässiger und informativer erwiesen als bestehende Methoden, indem sie einzigartige Aspekte der Dialogqualität erfasst und eine höhere Auflösung bei der Bewertung von Conversational AI bietet.</sample>
    <sample id="198">Es ist wichtig, die Akzeptanz der Modelle über das gesamte Kontextfenster zu bewerten, da moderne große Sprachmodelle längere Kontextfenster haben. Die aktuelle MPP-Pipeline erlaubt es nicht, die Akzeptanz von Modellen für längere Sätze zu bewerten. Durch die Bewertung über das gesamte Kontextfenster können wir sicherstellen, dass die Modelle die Akzeptanz in längeren und komplexeren Kontexten korrekt beurteilen.</sample>
    <sample id="199">Ja, das mehrsprachige Training führte zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell in sieben der neun Datensätze, wobei nur in drei Datensätzen eine Leistungssteigerung erzielt wurde. Dies wird als "Curse of Multilinguality" bezeichnet.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus, da sie den Namen der Entitäten in der alternativen Frage sehen und zusätzliche Hintergrundinformationen über die Entitäten erhalten.</sample>
    <sample id="201">State-of-the-art neural MT metrics and expert-based human evaluation results were used for the assessment.</sample>
    <sample id="202">Der englische Inhalt gibt keine spezifischen Informationen darüber, ob die Regression bei der Generalisierung auf bestimmte NER-Typen auswirkt. Die Diskussion konzentriert sich auf allgemeine Faktoren wie Modellarchitektur, Modellgröße und Anzahl der Feinabstimmungsbeispiele sowie auf die Ursachen für Leistungsabfälle, insbesondere temporale Drift, ohne spezifische NER-Typen zu erwähnen.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven und Entscheidungen von Forschern und Entwicklern beeinflusst, was zu systematischen Leistungsunterschieden von Technologien zwischen verschiedenen Bevölkerungsgruppen führen kann. Da NLP-Aufgaben zunehmend subjektiv und sozial orientiert sind, ist es entscheidend, wie diese Positionalitäten in Datensätzen und Modellen repräsentiert werden, um sicherzustellen, dass sie nicht bestimmte Gruppen benachteiligen.</sample>
    <sample id="204">Der Inhalt gibt keine spezifischen Informationen darüber, ob mehrsprachige LLMs wie BLOOM durch Adapter oder eine vollständige Feinabstimmung angepasst wurden. Es wird lediglich erwähnt, dass solche Modelle für die Aufgabe des cross-lingualen semantischen Parsings als unzureichend befunden wurden.</sample>
    <sample id="205">Dieses Papier untersucht die Verbreitung politischer Voreingenommenheiten von der Trainingsdatenbasis über Sprachmodelle bis hin zu Downstream-Aufgaben. Es zeigt auf, dass Sprachmodelle, die auf umfangreichen Web-Crawl-Daten trainiert werden, in denen politische Nachrichtenmedien gut vertreten sind, sowohl von der Vielfalt der Perspektiven profitieren als auch potenzielle Fairnessprobleme aufgrund sozialer Voreingenommenheiten aufweisen. Die Studie evaluiert die politische Ausrichtung von Sprachmodellen und deren Einfluss auf die Leistung bei Downstream-Aufgaben wie der Erkennung von Hassrede und Falschmeldungen. Durch die Verwendung von politischen Fragebögen zur Bewertung der politischen Ausrichtung von Sprachmodellen zeigt die Forschung, dass Modelle unterschiedliche politische Neigungen aufweisen, die sich in ihren Vorhersagen widerspiegeln. Experimente mit weiterem Training auf parteiischen Korpora zeigen, dass die politische Ausrichtung der Modelle sich entsprechend verschiebt. Die Ergebnisse deuten darauf hin, dass politische Voreingenommenheiten in Sprachmodellen zu Fairnessproblemen führen können, insbesondere bei der Erkennung von Hassrede und Falschmeldungen, wobei Modelle tendenziell besser bei der Erkennung von Hassrede und Falschmeldungen aus der entgegengesetzten politischen Ausrichtung abschneiden. Die Studie hebt die Herausforderung hervor, politische Voreingenommenheiten zu entschärfen, ohne in Zensur zu verfallen, und betont die Notwendigkeit, Fairnessprobleme in Sprachmodellen anzugehen.</sample>
    <sample id="206">Das Modell für das Transferlernen wird durch das Transferieren von Gewichten von zwei verschiedenen Aufgaben gestartet: der Klassifizierung des Standpunkts in Debatten (Debate) und der binären Klassifizierung der Klassen Expansion und Vergleich (CE) aus dem PDTB. Die beste Leistung wird erzielt, indem zuerst CE feinabgestimmt und dann weiter auf Debate feinabgestimmt wird.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die neuesten Testsets der WMT-Evaluation, um eine Überschneidung mit den Trainingsdaten des Sprachmodells zu vermeiden.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode verbessert die Planungsfähigkeit sowohl in Bezug auf die semantische Vollständigkeit als auch die Treue zu den Einschränkungen erheblich. T5, feinabgestimmt auf CoScript, kann Skripte von höherer Qualität als die meisten großen Sprachmodelle generieren.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz DEPLAIN können als Benchmark für die Probleme der automatischen Textvereinfachung verwendet werden.</sample>
    <sample id="212">Die Arbeit experimentiert mit einem kleineren Modell, dem T5.</sample>
    <sample id="213">OFA (Unified Multi-Modal Pre-Trained Model) wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="215">Der Vortrag von Adam Przepiórkowski behandelt die Abhängigkeitsstruktur der Koordination und vergleicht verschiedene theoretische Ansätze. In der Universal Dependencies und Igor Mel'čuks Meaning Text Theory wird die Koordination asymmetrisch behandelt, wobei der erste Konjunkt als Kopf fungiert. Im Gegensatz dazu wird in der Prager Schule die Koordination durch die Konjunktion selbst geleitet, während Hudsons Word Grammar eine multi-köpfige Struktur annimmt, bei der alle Konjunkte als Köpfe fungieren. Przepiórkowski argumentiert für symmetrische Strukturen der Koordination, basierend auf dem Prinzip der Minimierung der Abhängigkeitslänge. Er zeigt, dass in der englischen Sprache direkte Objekte typischerweise nahe am Verb stehen, während Adjunkte weiter entfernt sein können. Diese Regel kann jedoch durch die Länge des direkten Objekts beeinflusst werden, wobei längere direkte Objekte nach Adjunkten platziert werden können, um die Abhängigkeitslänge zu minimieren. Statistische Analysen der erweiterten Version des Penn Treebanks bestätigen, dass kürzere Konjunkte tendenziell links stehen, insbesondere wenn der Regent links oder abwesend ist. Diese Beobachtung unterstützt symmetrische Koordinationsstrukturen und stellt asymmetrische Ansätze in Frage. Weitere Details und Argumente finden sich im vollständigen Papier.</sample>
    <sample id="217">In "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation," Weihao Zeng, Lulu Zhao, and Keqing He address the limitations of existing dialogue generation models that focus on single attributes. They propose a novel Disentangled Controllable Generation (DCG) model to enhance multi-attribute dialogue generation. DCG learns attribute concepts from seen values and employs a disentanglement loss to separate different attribute combinations. The model uses a compositional prompt module based on DialoGPT, incorporating attribute-oriented and task-oriented prompts to guide dialogue generation. A unified reference-free evaluation framework, MAE, is introduced to assess various attribute granularities without requiring extensive labeled data. The authors establish benchmarks and demonstrate DCG's superior performance in attribute controllability and text equality, particularly for unseen attribute combinations. Experiments show that attribute-oriented prompts focus on specific information, while task-oriented prompts enhance text equality, and disentanglement learning improves compositional generalization. The study confirms the effectiveness of DCG in transforming seen attributes to unseen combinations, outperforming existing methods in both controllability and text equality. The MAE metric, validated against human judgments, shows high correlation with quality assessments, underscoring the model's robustness and generalizability.</sample>
    <sample id="218">Die Autoren gehören Google an.</sample>
    <sample id="219">Dieses Papier präsentiert eine vergleichende Multistufen-Pipeline zur Entdeckung finanzieller Signale in Finanzberichten, insbesondere Form 10-K, die von der SEC verlangt werden. Die Autoren, Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, und ihre Berater Professoren Che Lin und Chuan-Ju Wang, beobachteten, dass die Berichte jährlich ähnlich sind, wobei etwa 80% der Tokens gleich sind. Um nützliche Informationen zu extrahieren, schlagen sie eine Hervorhebungsaufgabe vor, die die Unterschiede zwischen Berichten aufeinanderfolgender Jahre analysiert. Die Pipeline umfasst mehrere Stufen: Relationserkennung, die Berichtspaare in drei Typen klassifiziert (höchste syntaktische und semantische Ähnlichkeit, überarbeitete Paare und nicht übereinstimmende Paare), und zwei Stufen der Modellanpassung (out-of-domain und in-domain). Die Modellanpassung nutzt das eSNLI-Dataset und pseudo-positive Labels für die Überarbeitung. Die Leistung wird mit Präzision und PCC bewertet. Die Ergebnisse zeigen, dass das domänenspezifische Hervorhebungsmodell die beste Leistung auf dem FINAL-Dataset erzielt und die Generalisierungsfähigkeit auf eSNLI beibehält. Zukünftige Arbeiten könnten die Effektivität verbessern oder zusätzliche Merkmale und Techniken aus dem Bereich der Informationsretrieval integrieren.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Die Arbeit untersuchte das Sprachpaar Deutsch-Englisch.</sample>
    <sample id="222">Dieses Papier untersucht die Herausforderungen und Interventionen für die Domänenanpassung in der offenen Frage-Antwort-Verarbeitung (Open-Domain QA). Es wird festgestellt, dass allgemeine Modelle, die auf allgemeinen Korpora wie Wikipedia trainiert sind, Schwierigkeiten haben, spezialisierte Fragen, z.B. aus dem Biomedizinischen Bereich, zu beantworten. Die Studie untersucht verschiedene Dateninterventionen, um die Generalisierungsfähigkeit dieser Modelle zu verbessern. Zwei Hauptmethoden werden betrachtet: Zero-Shot und Few-Shot. Few-Shot-Methoden verwenden wenige Beispiele aus dem Zielbereich, um große Sprachmodelle zur Generierung weiterer Beispiele zu nutzen. Zero-Shot-Techniken kontrollieren die Interaktionen zwischen Frage, Antwort und Kontext, indem sie zwei Variablen fixieren und die dritte variieren. Die Studie identifiziert auch die Art des Datenshifts, den ein neuer Bereich aufweist, und misst die Kompatibilität von Quell- und Zielmodellen. Es wird festgestellt, dass Few-Shot-Anpassungen für alle Zielsets effektiv sind, während Zero-Shot-Anpassungen bei Konzept- und Kovariatenverschiebungen nützlich sind. Die Experimente zeigen, dass durch verschiedene Dateninterventionen die Leserleistung um bis zu 24% verbessert werden kann, wobei die Effektivität der Interventionen vom Typ des Datenshifts abhängt.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Während der Experimente wurden die Modelle MASSalign und long-mBART untersucht. MASSalign wurde für die automatische Textausrichtung verwendet, und long-mBART wurde für die Erzeugung von Dokumentenebene-Simplifikationen feinabgestimmt. Außerdem wurde das normale Basis mBART für die Erzeugung von Satzebene-Simplifikationen feinabgestimmt.</sample>
    <sample id="225">Für das Training werden 53 Aufgaben aus 9 Gruppen verwendet. Für das Testen werden die gesamte Gruppe für Common Sense Reasoning und zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Der Artikel diskutiert die Herausforderungen der verankerten Sprachverständnisforschung und schlägt eine innovative Lösung vor. Verankertes Sprachverständnis bezieht sich auf die Zuordnung natürlicher Sprachausdrücke zu ausführbaren Plänen oder Programmen in spezifischen Umgebungen, was für Anwendungen wie Sprachassistenten, semantische Suche und Robotik entscheidend ist. Die Hauptproblematik liegt in der fehlenden Verankerung während der Vorab-Training-Phase von Sprachmodellen, die typischerweise auf textbasierten Korpora trainiert werden. Dies führt zu Schwierigkeiten bei der Generierung von gültigen und grammatikalischen Plänen. Der vorgeschlagene Rahmen, Pangu, trennt die Generierung von der Bewertung, indem ein symbolischer Agent Kandidatenpläne vorschlägt und ein Sprachmodell diese bewertet und rangiert. Dieser Ansatz zeigt sich in Experimenten mit verschiedenen Modellen wie BERT, T5 und Codex als effektiv und effizient, insbesondere bei der in-situ-Lernung. Pangu übertrifft bestehende Modelle in Bezug auf Genauigkeit und Stichprobeneffizienz und zeigt Robustheit unter nicht-i.i.d. Bedingungen. Der Schlüssel zur Verbesserung des verankerten Sprachverständnisses liegt in der Diskriminierung anstelle der Generierung.</sample>
    <sample id="228">Die Autoren haben Experimente an den Datensätzen AG News, MIND, SST2 und Enron Spam durchgeführt.</sample>
    <sample id="229">Dieses Papier präsentiert die Arbeit von Gabriella Skitalinskaya und Henning Wachsmuth zur Erkennung von verbesserungsfähigen Behauptungen in argumentativen Texten. Textrevision ist entscheidend für die Erreichung optimaler Formulierungen, insbesondere in der argumentativen Schreibweise, da sie die Wirkung auf das Publikum beeinflusst. Die Studie konzentriert sich auf zwei Aufgaben: die Erkennung suboptimaler Behauptungen und die Vorschläge zur Verbesserung von Behauptungen. Anstatt explizit zu definieren, was eine gute oder schlechte Behauptung ausmacht, untersucht das Papier, wie implizite Muster von Revisionen in Online-Debattierplattformen wie Kialo genutzt werden können, um die Qualität argumentativer Texte zu modellieren. Es werden vier Herausforderungen identifiziert: Repräsentativität und Zuverlässigkeit, Modellkomplexität und -architektur, kontextuelle Abhängigkeiten und thematische sowie benutzerspezifische Verzerrungen. Die Studie zeigt, dass revisionbasierte Daten effektiv für die Aufgaben eingesetzt werden können, wobei die Modellierung der Distanz zwischen Behauptungsversionen zur Erkennung suboptimaler Behauptungen beiträgt. Der Einfluss kontextueller Informationen variiert je nach Aufgabe und den spezifischen Qualitätsproblemen des Textes. Weitere Details und Ergebnisse sind im vollständigen Papier zu finden.</sample>
    <sample id="231">NACHOS ist ein Datensatz medizinischer gecrawlter Daten aus dem Web, der zur Entwicklung des DrBERT-Modells verwendet wurde.</sample>
    <sample id="232">David Vilar</sample>
    <sample id="233">Der Artikel "Attention as a Guide for Simultaneous Speech Translation" von Sara Papi, Matteo Negri und Marco Turchi beschreibt eine innovative Lösung für die Herausforderungen der Simultansprachübersetzung (SimulST). SimulST ermöglicht die Echtzeit-Übersetzung gesprochener Sprache in Text einer anderen Sprache, was die interlinguale Kommunikation erleichtert. Aktuelle SimulST-Modelle leiden unter komplexen Architekturen, langwierigen Trainingsprozessen und der Notwendigkeit, mehrere Modelle für unterschiedliche Latenzregime zu trainieren und zu warten. Die vorgeschlagene Lösung, EDAtt (Encoder-Decoder Attention), nutzt bestehende Offline-Übersetzungsmodule ohne Neutrainieren oder spezielle Architekturen für SimulST. Sie verwendet eine einzige Modellinstanz für jedes Latenzregime und steuert die Latenz über spezifische Parameter. Die Entscheidung, ob eine partielle Übersetzung ausgegeben wird, basiert auf der Verteilung der Aufmerksamkeitsmechanismen zwischen Audioeingabe und Textausgabe. Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht auf den letzten λ Sprachframes konzentriert ist, was auf ausreichend stabile Informationen hinweist. Die Ergebnisse zeigen, dass EDAtt die Qualität der Übersetzung (gemessen an BLEU) und die Latenz (gemessen an durchschnittlicher Verzögerung und computeraufmerksamer Verzögerung) verbessert und die bestehenden Strategien übertrifft. Der Code und die Modelle sind Open Source verfügbar, um die Reproduzierbarkeit zu erleichtern.</sample>
    <sample id="234">Die Prompt-Strategie hat einen erheblichen Einfluss auf die Leistung von großen Sprachmodellen bei der Übersetzung. Ein einfaches Experiment zeigte, dass die Verwendung unterschiedlicher Prompts für dieselbe Satzstruktur zu einer signifikanten Leistungsänderung von mehr als einem BLEURT-Punkt führen kann, in extremen Fällen sogar bis zu 40 BLEURT-Punkten. Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Quellsatz, und bei mehreren Prompts (wie 5-Shot-Prompting) ist die Form des Prompts weniger entscheidend als die Qualität der Beispiele.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Der englische Inhalt enthält keine spezifischen Details zu den fünf Anweisungen der Expert*innen. Es wird lediggegeben, dass jede Aufgabe mit fünf Experten geschriebenen Anweisungen ausgestattet ist, aber die genauen Anweisungen werden nicht aufgeführt.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle zur Nutzung von Informationen aus mehreren Quellen mit einem diagnostischen Testset namens KITMUS zu testen. Dieses Testset umfasst eine Aufgabe zur Koreferenzauflösung, die darauf abzielt, die Fähigkeit der Modelle zu bewerten, auf Wissen aus verschiedenen Quellen zuzugreifen. Sie definieren drei Einstellungen: "Background-Pretrain", "Background-Both" und "Background-Inference", um die Verfügbarkeit von Hintergrund- und entitätsspezifischem Wissen zu variieren.</sample>
    <sample id="238">In diesem Video stellt Yebowen Hu von der University of Central Florida die neue Benchmark-Datenbank MeetingBank vor, die darauf abzielt, die Entwicklung von Zusammenfassungstechnologien für Meetings zu unterstützen. MeetingBank umfasst 1.366 Stadtratssitzungen mit fast 7.000 Instanzen, einschließlich Transkripten, Referenzzusammenfassungen und weiteren Ressourcen. Die Daten wurden mithilfe der Speechmatics-API aus Audio in Transkripte umgewandelt und mit Referenzzusammenfassungen aus den Sitzungsprotokollen abgeglichen. Die Analyse der Abstraktionsebene in den Zusammenfassungen zeigt, dass die meisten zwischen 0,7 und 0,9 liegen, was auf eine hohe Verwendung von wörtlichen Punkten hinweist. Die Dichtebewertung zeigt, dass Seattle und Boston die höchsten und Denver die niedrigsten Werte aufweisen. Bei der Modellbewertung wurden sowohl extraktive als auch abstraktive Systeme getestet, wobei DialogLM bei den abstraktiven Modellen und Extr-Oracle bei den extraktiven Modellen die besten ROUGE-2-Scores erzielten. GPT-3 zeigte in automatischen Metriken schlechte Leistungen, übertraf jedoch in menschlichen Bewertungen, insbesondere in Bezug auf Flüssigkeit und Kohärenz, obwohl es in Bezug auf Informationsgehalt und Faktizität weniger überzeugend war. MeetingBank bietet wertvolle Einblicke in die Entscheidungsprozesse von Stadträten und dient als wertvolles Werkzeug für die Forschung.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar, und ich werde eine kurze Besprechung des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein großes Sprachmodell mit 540 Milliarden Parametern, das letztes Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Tokens umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den State-of-the-Art in hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Verwendung von Sprachmodell-Prompting für die maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle mit den Best Practices der MT-Community evaluiert. Dies beinhaltet die Verwendung der neuesten Testsets, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir verglichen uns mit den bestehenden Systemen, also dem besten System, der WMT-Evaluation. Wir verwendeten die neuesten, neuralen MT-Metriken und zeigten zusätzlich Ergebnisse von Experten-basierten menschlichen Bewertungen. Schließlich geben wir einige Empfehlungen für Prompt-Auswahlstrategien. Das Prompting hat einen großen Einfluss auf die Leistung der LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir One-Shot-Prompting verwendeten und für jede Zeile zwei unterschiedliche Prompts bereitstellten. Die Mehrheit der Sätze, 516 von 1.000, zeigte einen Unterschied von mehr als einem BLEURT-Punkt. In extremen Fällen kann dies bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Prompting-Strategie auszuwählen. In unseren Experimenten entschieden wir uns für eine 5-Shot-Prompting-Strategie, bei der wir jede an das System übergebene Zeile mit der Sprache markierten, in der sie steht. In diesem Beispiel, bei der Übersetzung von Deutsch ins Englische, sind die deutschen Quellzeilen mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir stellten fest, dass die tatsächliche Form des Promptings bei mehreren kurzen Promptings keinen großen Einfluss hat. Es ist entscheidend für Zero- und One-Shot-Prompting. Und wenn wir, wie in unserem Fall, zu Five-Shot-Prompting gehen, gibt es fast keinen Unterschied zur tatsächlichen Form des Promptings. Es sind die Beispiele, die den größten Einfluss haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zur Quellzeile. Es ist wichtig, Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Prompts aus den Trainingsdaten für die WMT-Evaluierungen auf den Dev-Daten. Die Dev-Daten sind viel sorgfältiger kuratiert und von höherer Qualität als die Trainingsdaten, die lauter sind. Ihre Ergebnisse zeigen eine bessere Leistung, wenn die Dev-Daten verwendet werden. Dennoch haben spezialisierte State-of-the-Art-Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir uns entschieden, mit Google Translate zu evaluieren. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir mit dem MQM-Rahmenwerk durchgeführt haben, besagen, dass die Flüssigkeit von PaLM vergleichbar mit State-of-the-Art-Systemen ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere sind die häufigsten Fehler Auslassungsfehler. Es scheint also, dass PaLM manchmal Teile der Quellzeile weglässt, um eine bessere klingende Übersetzung zu produzieren. Allerdings ist die "Style/Awkward"-Kategorie für PaLM niedriger als für die State-of-the-Art-Systeme, was ein zusätzliches Signal ist, dass PaLM wirklich flüssige Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diese wirklich kurze Besprechung. Für mehr Details, besuchen Sie bitte die vollständige Präsentation des Papiers. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Promotionsstudent an der Saarland-Universität in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in die schwache Überwachung und das schwach überwachte Lernen beginnen. Bei der schwachen Überwachung werden die Daten nicht manuell beschriftet. Stattdessen beschriften wir die Daten mit schwachen Beschriftungsquellen, wie einfache Heuristiken, Wissensbasen oder niedrigwertige Crowdsourcing, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwächeren Annotationen viel günstiger, jedoch auch fehlerhaft, was bedeutet, dass ein bestimmter Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt mit schwach beschrifteten Daten trainieren, neigen die neuronalen Netze dazu, das Label-Rauschen zu memorieren und nicht zu verallgemeinern. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solchem Label-Rauschen zu trainieren, sodass die trainierten Modelle dennoch gut verallgemeinern. In jüngsten Arbeiten im Bereich WSL, also WSL steht für Weakly Supervised Learning, wird häufig behauptet, dass Modelle nur mit schwach beschrifteten Daten trainiert und dennoch hohe Leistungen auf sauberen Testsets erzielt werden. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken: Es wird angenommen, dass ein zusätzliches sauberes Validierungsset für die Modellauswahl verfügbar ist. Wir können nicht bei diesem Problemsetting stehen bleiben, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Aber wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Zweifel führen zu drei Forschungsfragen. Erstens, ist sauberes Validierungsdaten für WSL notwendig oder können wir vielleicht ein rauschendes Validierungsset verwenden? Zweitens, wenn saubere Daten erforderlich sind, oder wenn saubere Daten für WSL unerlässlich sind, dann wie viele saubere Beispiele benötigen wir? Schließlich, sollten wir die sauberen Beispiele nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit adressiert und unsere Erkenntnisse sind wie folgt. Erstens finden wir interessanterweise, dass aktuelle WSL-Methoden tatsächlich saubere Validierungsbeispiele benötigen, um ordnungsgemäß zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, wenn es keine sauberen Validierungsbeispiele gibt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies zeigt, dass WSL-Ansätze tatsächlich sauber beschriftete Daten benötigen, um ordnungsgemäß zu funktionieren, und die Annotierungskosten für die Beschaffung sauberer Validierungsbeispiele sollten nicht übersehen werden. Unsere zweite Erkenntnis ist, dass die Erhöhung der Anzahl sauberer Validierungsbeispiele WSL-Ansätzen hilft, eine bessere Leistung zu erzielen, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Beispiele pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir entscheiden, saubere Beispiele zu verwenden, wird das direkte Training auf ihnen noch bessere Leistungen erzielen. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf sauberen Daten angewendet werden, und WSL-Ansätzen, die saubere Daten nur für die Validierung verwenden. Wie wir sehen können, beginnt die direkte Feinabstimmung, wenn wir 10 Beispiele pro Klasse haben, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem erlaubt wird, die Feinabstimmung auf den sauberen Validierungsbeispielen fortzusetzen. Wie wir aus den Abbildungen sehen, unterperformt das Standardmodell, das als FTw bezeichnet wird, zunächst komplexere WSL-Methoden wie COSINE. Wenn wir jedoch erlauben, die Feinabstimmung auf den sauberen Beispielen fortzusetzen, dann erzielt FTw eine Leistung, die ebenso gut ist wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass aktuelle WSL-Ansätze saubere, manuell annotierte Beispiele benötigen, um ordnungsgemäß zu funktionieren. Ihre Leistungssteigerung und Praktikabilität sind stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie über die Modellauswahlkriterien. Zum Beispiel, berichten Sie, ob die Modellauswahl über saubere Validierungsbeispiele erfolgt. Zweitens sollten WSL-Ansätze mit wenige-Schuss-Lern-Baselines verglichen werden, da beide auf sauberen Beispielen arbeiten. Drittens ist die kontinuierliche Feinabstimmung ein einfacher, aber starker Baseline, der in zukünftigen Arbeiten im Bereich WSL berücksichtigt werden sollte. Schließlich haben wir unseren Code freigegeben. Sie können ihn über den QR-Code auf dieser Folie finden. Bitte schauen Sie es sich an. Vielen Dank und genießen Sie die Konferenz.</sample>
    <sample id="241">Dieses Papier präsentiert einen human-in-the-loop Ansatz zur frühzeitigen Erkennung von Fehlinformationen, speziell im Kontext von COVID-19-Behandlungen. Die Autoren kritisieren bestehende automatisierte Erkennungssysteme für Fehlinformationen auf sozialen Medien, die oft unrealistisch evaluiert werden und menschliche Interaktionen vernachlässigen. Sie schlagen ein neues Evaluationsrahmenwerk vor, das menschliches Feedback in verschiedenen Stadien integriert und von der Rohdatenverarbeitung bis zu handlungsorientierten Ausgaben reicht. Das System besteht aus zwei Hauptkomponenten: der Erkennung irreführender Behauptungen und der Verifizierung von Verstößen gegen die Richtlinien. Die erste Komponente verwendet ein T5-Modell zur Extraktion von Behauptungen aus Tweets, die dann nach Trendigkeit sortiert und von Menschen verifiziert werden. Die zweite Komponente nutzt ein BERT-basiertes Modell zur Klassifizierung der Haltung der Autoren zu ungenehmigten Behandlungen. Die Evaluation zeigt, dass das System Behauptungen vor ihrer öffentlichen Widerlegung erkennen kann und eine 65%ige Genauigkeit bei der Erkennung von Verstößen gegen die Richtlinien aufweist. Es bestätigt 124,2 Verstöße pro Arbeitsstunde. Das Papier betont die Bedeutung eines realistischen, menschenzentrierten Ansatzes zur Fehlinformationsdetektion und motiviert die Entwicklung zukünftiger Systeme.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind:

1. Human evaluation durch Auswahl der besseren von zwei Konversationen.
2. Bewertung von Konversationen mit einem Likert-Skala.
3. Likert-Bewertungen auf Turn-Ebene.
4. Likert-Bewertungen auf Dialog-Ebene.
5. Dialog-Level-Paarvergleiche.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt: Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap und Jenny.</sample>
    <sample id="244">Das Hintergrundwissen, das im Beispiel mit Servin und Kea benötigt wird, ist "Richter entscheiden Fälle in Gerichten."</sample>
    <sample id="245">Dieses Papier präsentiert eine Methode zur Identifizierung von hochübereinstimmenden Amazon Mechanical Turk (MTurk) Arbeitern für die Zusammenfassungsaufgaben. Die Studie adressiert die Probleme automatischer Bewertungsmetriken und unzureichend verstandene Best Practices bei der Rekrutierung auf MTurk. Die vorgestellte Methode umfasst zwei Qualifikationsstufen: eine Qualifikationsaufgabe, die die Fähigkeit der Annotatoren testet, mehrere Dimensionen korrekt zu bewerten, und eine Ausdauerprüfung, die die Kapazität der Annotatoren für umfangreiche Arbeitslasten bewertet. Die Qualifikationsaufgabe resultiert in 26 qualifizierten Arbeitern (8 Gold, 18 Silber), während die Ausdauerprüfung 12 (4 Gold, 8 Silber) auswählt. Diese hochübereinstimmenden Arbeiter zeigen eine bessere Inter-Annotator-Übereinstimmung (IAA) als Experten. Die Studie vergleicht auch die Leistung dieser Arbeiter mit Baseline-MTurk-Arbeitern, die durch statistische Filter wie MACE ausgewählt wurden, und CloudResearch-Arbeitern, die für ihre hohe Qualität bekannt sind. Die Ergebnisse zeigen, dass die Pipeline-Arbeiter eine hohe Übereinstimmung und ähnliche Qualität wie CloudResearch-Arbeiter erreichen, jedoch zu geringeren Kosten. Die Studie schließt mit der Feststellung, dass die Methode effizient Ressourcen spart und als Best Practice für hochübereinstimmende Annotationen auf großen Skalen dient. Zukünftige Arbeiten werden sich auf die Rekrutierung von hochwertigen Arbeitern in verschiedenen Anwendungen konzentrieren. Einschränkungen umfassen die Begrenzung auf englische Zusammenfassungen und die Notwendigkeit, die Korrektheit der Ausbildung zu gewährleisten.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Sie können ihn auf GitHub finden.</sample>
    <sample id="247">Das Papier "FACTKG: Fact Verification via Reasoning on Knowledge Graphs" von Jiho Kim et al. von KAIST AI stellt ein neues Dataset und eine Aufgabe für die Faktenermittlung vor, die auf Wissensgraphen basiert. Im Gegensatz zu bestehenden Datensätzen wie FEVER und VitaminC, die auf Wikipedia-Texten basieren, oder TabFact und InfoTabs, die Tabellen als Beweise verwenden, nutzt FACTKG Wissensgraphen, um die Zuverlässigkeit und Praktikabilität der Faktenermittlung zu verbessern. Das Dataset verwendet DBpedia als Wissensgraph und umfasst Ansprüche in beiden, schriftlicher und umgangssprachlicher Form. Die Aufgabe besteht darin, Beweise aus DBpedia zu extrahieren und Ansprüche mit fünf Arten von Schlussfolgerungen zu verifizieren: one-hop, Konjunktion, Existenz, multi-hop und Negation. FACTKG enthält Ansprüche in beiden Stilen, um praktische Anwendungen zu ermöglichen, und nutzt einen Stiltransfermodell sowie Präsuppositionsvorlagen zur Generierung umgangssprachlicher Ansprüche. Baseline-Methoden, einschließlich eines Ansatzes, der nur Ansprüche verwendet, und des GEAR-Modells, das graphische Beweise nutzt, zeigen, dass die Verwendung von Wissensgraphen die Leistung verbessert. Das Dataset ist öffentlich zugänglich und bietet eine neue Möglichkeit, die Konsistenz zwischen natürlicher Sprache und Wissensgraphen zu überprüfen.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Die Studie sammelte über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern, was auf eine breite, aber nicht notwendigerweise ausgewogene demographische Repräsentation hinweist.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durch Hinzufügen von Rauschen zu den Eingabesätzen durcheinander gebracht, während die relevanten Strukturen erhalten blieben. Diese Störungen führten zu einem ähnlichen Anstieg der MPP-Bewertungen, unabhängig von der Art der Störung.</sample>
    <sample id="250">Eine dimensionale Bewertung bezieht sich auf die Bewertung mehrerer Aspekte oder Dimensionen der Dialogqualität, um die Stärken und Schwächen eines Modells auf einer feineren Ebene zu verstehen. Im Kontext von ABC-Eval umfasst dies das Annotieren spezifischer Verhaltensweisen in Chats, wie z.B. Irrelevanz, Selbstwidersprüche oder Empathie, um die Qualität des Dialogs umfassender zu bewerten.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China an.</sample>
    <sample id="252">Dieses Papier präsentiert "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion", eine Methode zur Verbesserung der Aufgabe der Prior Case Retrieval (PCR) in der Rechtswissenschaft. Die Autoren, Sai Kiran Tanikella, Abhinav Joshi, Akshat Sharma und Ashutosh Modi, stellen zwei Hauptbeiträge vor: das IL-PCR-Dataset und den U-CREAT-Pipeline. Das IL-PCR-Dataset, ein neues Benchmark-Dataset für PCR-Aufgaben, umfasst 7.070 indische Rechtsfälle mit durchschnittlich 6,775 Zitaten pro Dokument. Es bietet eine umfassende Plattform zur Bewertung von PCR-Algorithmen und übertrifft das bestehende COLIEE’21-Dataset in Bezug auf die Anzahl der Fälle, die Länge der Dokumente, den Wortschatz und die Anzahl der Zitate. Der U-CREAT-Pipeline nutzt Techniken des unüberwachten Lernens und eine ereignisbasierte Herangehensweise, um die Effizienz der Rückgewinnung zu steigern, die Inferenzzeit zu senken und die Generalisierung über indische und kanadische Rechtssysteme zu ermöglichen, ohne spezifische Anpassungen vorzunehmen. Die ereignisbasierte Methode extrahiert Ereignisse aus Dokumenten mithilfe von Abhängigkeitsparsen und bildet Subjekt-Verb-Objekt-Tripletts. Diese Ereignisse werden verwendet, um eine Interaktionsmatrix zu erstellen, die zur Rangfolge von Kandidaten in verschiedenen Modellen dient. Experimente zeigen, dass ereignisbasierte Modelle, insbesondere das Event Filtered Documents-Modell, die Leistung der Basismethoden erheblich übertreffen. U-CREAT übertrifft bestehende Ansätze, einschließlich des MTFT-BERT-Teams, und stellt den aktuellen State-of-the-Art für die COLIEE’21-Dokumentenabrufaufgabe dar.</sample>
    <sample id="253">Der Vortrag von Mario Ezra Aragón präsentiert "DisorBERT", ein Modell zur Erkennung von Anzeichen psychischer Störungen in sozialen Medien durch doppelte Domänenanpassung. Psychische Störungen beeinträchtigen Denken, Gefühle, Stimmung und Verhalten. Soziale Medien bieten eine Plattform, auf der Menschen ihre Erfahrungen teilen, was die Forschung in diesem Bereich erleichtert. DisorBERT zielt darauf ab, automatisch Beiträge zu analysieren, um psychische Gesundheitsprobleme frühzeitig zu erkennen. Die Domänenanpassung wird genutzt, um die Leistung des Modells zu verbessern, indem Wissen aus verwandten Bereichen übertragen wird. DisorBERT beginnt mit einem allgemeinen Sprachmodell und integriert spezifische Informationen aus Reddit und der psychischen Gesundheit, ergänzt durch ein Lexikon zur Fokussierung auf wichtige Wörter. Die Ergebnisse zeigen, dass DisorBERT im Vergleich zu anderen Modellen eine bessere Balance zwischen Präzision und Rückruf aufweist. Die Analyse zeigt, dass DisorBERT Wörter mit negativer psychologischer Bedeutung bevorzugt, was seine Eignung zur Erkennung psychischer Störungen unterstreicht. Die Visualisierung der Aufmerksamkeitsscores hebt relevante Themen wie "ängstlich" und "Medikation" hervor. DisorBERT übertrifft MentalBERT und zeigt ein solides Gleichgewicht zwischen der Identifizierung und korrekten Klassifizierung von Nutzern. Zukünftige Arbeiten sollen die Anwendung verschiedener lexikalischer Ressourcen und klinischer Daten erforschen.</sample>
    <sample id="254">Dieses Papier präsentiert einen Rahmen für die Entfernung von Lärm in der Dokumenten-Ebene der entfernten Beziehungsextraktion (DocRE) unter Verwendung von unsicherheitsgeleiteter Label-Denoising. Die Arbeit zielt darauf ab, die Qualität der Labels in distanzüberwachten (DS) Daten zu verbessern, die oft mit Fehlern behaftet sind. Der vorgeschlagene Rahmen nutzt eine Vor-Denoising DocRE-Modellierung mit DS- und menschlich annotierten Daten, um Pseudo-Labels zu generieren. Um die Zuverlässigkeit der Vorhersagen zu bewerten, wird eine Instanz-Level-Unsicherheitsschätzung eingeführt, die Monte Carlo Dropout verwendet, um die Unsicherheit in den Modellvorhersagen zu erfassen. Dies ist besonders wichtig für das Problem der überlappenden Beziehungen, bei dem mehrere Beziehungen zwischen einem Entitätspaar bestehen können. Dynamische Klassen-Unsicherheitsschwellen werden vorgeschlagen, um Pseudo-Labels mit hoher Unsicherheit zu filtern, indem sie durch solche mit niedrigerer Unsicherheit ersetzt werden. Ein multi-phasischer Trainingsansatz wird entwickelt, um die DS-Daten iterativ neu zu beschriften und die Leistung des DocRE-Modells weiter zu steigern. Experimente zeigen, dass der Rahmen die Leistung auf öffentlichen Datensätzen im Vergleich zu starken Baselines verbessert. Die Hauptbeiträge umfassen die Verbesserung der Labelqualität durch unsicherheitsgeleitete Label-Denoising, eine Instanz-Level-Unsicherheitsschätzung für überlappende Beziehungen, eine iterative Re-Label-Strategie mit dynamischen Klassen-Unsicherheitsschwellen und signifikante Leistungsverbesserungen.</sample>
    <sample id="255">Die Form des Prompts ist besonders wichtig bei Zero- und One-Shot-Prompting. Bei mehreren Short-Promptings, wie im Fall von Five-Shot-Prompting, hat die tatsächliche Form des Prompts kaum Einfluss.</sample>
    <sample id="257">Die Autoren evaluierten vier state-of-the-art Chatmodelle.</sample>
    <sample id="258">In diesem Video stellt Chiang Cheng-Han die Arbeit "Can Large Language Models Be an Alternative to Human Evaluation?" vor. Die Studie untersucht, ob große Sprachmodelle (LLMs) die Qualität von Texten in der natürlichen Sprachverarbeitung (NLP) bewerten können, indem sie Anweisungen erhalten und darauf basierend Bewertungen abgeben. Obwohl ähnliche Ansätze wie G-Eval existieren, war die Idee, LLMs zur Bewertung zu nutzen, zum Zeitpunkt der Einreichung beim ACL neuartig. Die Motivation besteht darin, die Instabilität und Reproduzierbarkeitsprobleme menschlicher Bewertungen zu umgehen. Die Forschung vergleicht die Bewertungen von LLMs mit menschlichen Bewertungen, die von Englischlehrern durchgeführt wurden, um die Qualität von Geschichten, die von GPT-2 oder Menschen verfasst wurden, zu bewerten. Die Bewertungskriterien umfassen Grammatik, Kohärenz, Sympathie und Relevanz. Die Ergebnisse zeigen, dass einige LLMs, insbesondere Davinci und ChatGPT, menschliche Bewertungen in ihrer Präferenz für menschlich verfasste Texte widerspiegeln. Kleinere LLMs zeigen jedoch keine klare Präferenz. Die Studie beantwortet Fragen zur Übereinstimmung zwischen LLMs und menschlichen Bewertungen, zur Auswirkung von Anweisungen und Stichprobenmethoden sowie zu den Vor- und Nachteilen der Verwendung von LLMs im Vergleich zu menschlichen Bewertungen. Weitere Details finden sich in der vollständigen Studie.</sample>
    <sample id="259">Dieses Papier präsentiert XSemPLR, eine einheitliche Benchmark für die cross-linguale semantische Parsing-Aufgabe, die mehrere natürliche Sprachen und Bedeutungsrepräsentationen umfasst. XSemPLR adressiert die Lücken in bestehenden Modellen, die oft auf bestimmte Sprachen und Bedeutungsrepräsentationen beschränkt sind, indem es eine umfassende Datensammlung mit 9 Datensätzen, 5 Parsing-Aufgaben, 8 Bedeutungsrepräsentationen und 22 Sprachen in 15 Sprachfamilien bietet. Die Studie evaluiert sechs Trainings- und Evaluierungsszenarien: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot und Few-shot Transfer. Die Ergebnisse zeigen, dass Encoder-Decoder-Modelle, insbesondere mT5, über alle Datensätze hinweg die besten Leistungen erbringen. Die Studie hebt auch die "Curse of Multilinguality" hervor, bei der die Leistung in den meisten Sprachen steigt, während die englische Leistung in sieben Datensätzen abnimmt. Die Analyse zeigt, dass die Leistungslücke bei Zero-shot-Übertragungen signifikant ist, sich jedoch bei Few-shot-Übertragungen schnell verringert. Darüber hinaus zeigt die Studie, dass die Vorabtraining auf Englisch die Leistung von Few-shot-Übertragungen auf Ziel-Sprachen erheblich verbessert, während bestehende multilinguale Modelle wie Codex und BLOOM für diese Aufgaben noch unzureichend sind. Insgesamt bietet XSemPLR eine umfassende Plattform zur Bewertung und Verbesserung von cross-lingualen semantischen Parsing-Modellen.</sample>
    <sample id="260">Der Inhalt gibt keine spezifische Anzahl von Autoren an, die an der Arbeit beteiligt sind.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die sowohl vernünftig als auch den spezifischen Einschränkungen treu sind.</sample>
    <sample id="262">Die Anzahl der Autoren wird im bereitgestellten Inhalt nicht erwähnt.</sample>
    <sample id="263">Dieses Papier untersucht die Problematik von Label-Bias in der In-Context-Learning-Paradigma mit großen Sprachmodellen, insbesondere bei Textklassifikationsaufgaben. Es identifiziert drei Arten von Label-Bias: Vanilla-Label-Bias, Context-Label-Bias und die neu entdeckte Domain-Label-Bias. Letztere zeigt, dass die Sichtung von zufälligen in-domain Wörtern aus dem Aufgabencorpus die Vorhersagen der Modelle erheblich beeinflussen kann, während zufällige englische Wörter dies nicht tun. Um diese Bias-Probleme zu adressieren, schlagen die Autoren eine innovative Domänen-Kontext-Kalibrierungsmethode vor. Diese Methode nutzt zufällige in-domain Wörter als nahezu inhaltsfreie Texte, um die Vorhersagen der Modelle zu kalibrieren und somit alle Arten von Label-Bias zu mindern. Experimente zeigen, dass diese Methode die Leistung von In-Context-Learning signifikant verbessert, insbesondere bei Aufgaben mit hohem Domain-Label-Bias. Die Studie bestätigt, dass die Verwendung von mehreren zufälligen Wörtern und spezifisch in-domain Wörtern die Kalibrierungseffizienz erhöht. Die Ergebnisse deuten darauf hin, dass die Domänen-Kontext-Kalibrierung eine umfassende Lösung für die Herausforderungen von Label-Bias in In-Context-Learning bietet.</sample>
    <sample id="264">Der Vortrag von Lin Wang präsentiert das Papier "TAVT: Towards Transferable Audio-Visual Text Generation", das sich mit der Herausforderung der multimodalen Textgenerierung befasst, insbesondere bei audio-visuellen Aufgaben. Während uni-modale Textgenerierungsaufgaben wie maschinelle Übersetzung und Bildbeschriftung durch groß angelegte Vorabtrainings und große Modellkapazitäten vorangeschritten sind, sind multimodale Aufgaben wie die audio-visuelle Textgenerierung aufgrund der aufwendigen und teuren Datenannotation und der Leistungseinbußen in verschiedenen Domänen herausfordernd. Um diese Einschränkungen zu überwinden, schlägt das Papier eine neue Aufgabe vor: Transferable Audio-Visual Text Generation. Die Hauptproblematik sind multimodale Domänenverschiebungen wie visuelle Stile und Audio-Energie. Es wird festgestellt, dass visuelle Inhalte bei stilistischen Änderungen und Kamerawinkeln erheblich variieren, während Änderungen im Audioinhalt wie Rhythmus und Energie die Ereignisverständnis kaum beeinflussen. Daher wird ein einheitlicher audiosemantischer Raum vorgeschlagen, um visuelle Konzepte über Domänen hinweg zu alignieren. Das vorgestellte Framework besteht aus drei Komponenten: einem audio-visuellen Meta-Mapper-Netzwerk, einem audio-visuellen Encoder und Sprachmodell-Generator sowie einem counterfactual contrastive learning. Der Meta-Mapper mappt visuelle Konzepte in einen einheitlichen audiosemantischen Raum, während der Transformer-basierte Encoder und Generator die Beiträge verschiedener Modi bewerten. Ein Dual Counterfactual Contrastive Learning (DCLL) wird eingeführt, um die visuell-audio Alignment-Scores direkt zu optimieren. Die Meta-Trainingsdetails ähneln MAML, wobei Domänen für Support- und Query-Sets ausgewählt werden. Experimente auf den MSVD- und MSR-VTT-Benchmarks zeigen, dass TAVT bestehende Modelle in allen Metriken übertrifft, insbesondere in ressourcenarmen Domänen.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Der Inhalt enthält keine Informationen über die Universität, der die Autoren angehören.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, bei denen Teile der Quellsätze in der Übersetzung weggelassen werden, um eine flüssigere Übersetzung zu erzielen.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute erzählen wir Ihnen alles über ABC-Eval, einen neuen mehrdimensionalen Ansatz zur Bewertung von konversationeller KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Angenommen, Sie haben gerade ein Dialogmodell entwickelt und möchten sehen, wie es im Vergleich zum aktuellen Stand der Technik abschneidet. Die gängige Praxis ist die Verwendung von menschlichen Bewertungen, z. B. indem menschliche Richter gebeten werden, zwischen zwei Gesprächen zu wählen oder Gespräche anhand einer Likert-Skala zu bewerten. Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der Gesprächsqualität zu liefern, aber die Gesprächsqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chat-Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit besteht darin, menschlichen Richtern zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, z. B. die Relevanz der Modellantworten mit bestehenden vergleichenden oder Likert-Skalenmethoden. Wir glauben jedoch, dass es einen präziseren und zuverlässigeren Ansatz für die mehrdimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem explizit annotiert wird, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z. B. das Reagieren mit irrelevanten Informationen oder das Widersprechen sich selbst. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um umfassend Verhaltensweisen von Chat-Modellen abzudecken, die in der jüngsten Literatur als Einflussfaktoren auf die Chat-Qualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chat-Modelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Gesprächsrunden, in denen ein Chat-Modell seinen Partner ignoriert oder etwas Irrelevantes, Widersprüchliches oder Halluziniertes sagt oder allgemeines Wissen verletzt, und ob das Modell erfolgreich oder erfolglos Empathie zeigt. Um festzustellen, welche Art von Bewertung am effektivsten ist, haben wir vier state-of-the-art Chat-Modelle ausgewählt und diese anhand von 100 menschlich-bot-Gesprächen pro Modell mit ABC-Eval bewertet. Zum Vergleich haben wir diese Gespräche auch anhand von drei bestehenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarvergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekte des Dialogs gesammelt, da dies der Standard für die Bewertung von Chat-Modellen entlang mehrerer Dimensionen ist. Aus unserer Analyse der Bewertungsergebnisse stellten wir fest, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die mit bestehenden Methoden gesammelten Labels, gemessen an der Inter-Annotator-Übereinstimmung bei 100 doppelt beschrifteten Gesprächen. Darüber hinaus sind ABC-Eval-Labels prädiktiver für die Gesamtqualität des Gesprächs im Vergleich zu Metriken, die von bestehenden Methoden produziert werden, wie durch diese einfache lineare Regressionsanalyse gezeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Runden mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzwerte nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chat-Qualität erfasst, indem wir eine schrittweise lineare Regression durchgeführt haben. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken mehr als 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, verlieren die meisten von ihnen eine beträchtliche Menge an Informationen über die Qualität. Im Gegensatz dazu erklärt die Kombination aller turn-basierten Likert-Metriken viel weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und einzigartigen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung als bisherige Methoden zu bewerten. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und präzise quantifiziert wurden. Zum Beispiel haben die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das Allgemeinwissen, sie produzieren in etwa 15 % der Antworten irrelevante Informationen und sie widersprechen sich selbst oder ihrem Partner etwa 10 % der Zeit. Mit dem rasanten Fortschritt im Bereich könnten viele dieser Fehlerquoten in neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, nach zuverlässigen und präzisen Bewertungsmetriken für den Vergleich von Modellen zu suchen. Wir hoffen, dass ABC-Eval von anderen im Bereich als bedeutender Schritt in diese Richtung genutzt werden kann. Und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank, dass Sie zugeschaut haben.</sample>
    <sample id="270">Emory University.</sample>
    <sample id="271">In dieser Arbeit steht CFT für "Continuous Fine-Tuning".</sample>
    <sample id="272">Sieben Autoren sind an der Arbeit beteiligt: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel "When Does Translation Require Context? A Data-driven, Multilingual Exploration" vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie übersetzen wir "mole" in diesem Satz? Wenn der vorherige Satz lautet "Things could start to get dangerous if the ministers find out", bezieht sich "mole" auf einen Spion. Wenn der vorherige Satz jedoch "Could it be anything serious, doctor?" lautet, bezieht sich "mole" auf einen Muttermal. Abhängig vom Kontext ändert sich die Bedeutung des Wortes und somit auch seine Übersetzung. Allerdings ist es schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was es korpusweiten Metriken wie BLEU unmöglich macht, diese Übersetzungen zu erfassen. Einige haben vorgeschlagen, gezielte Bewertungen für kontextabhängige Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und Sprachsets, da sie in der Regel auf Fachwissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir damit begonnen, zu messen, wie sehr ein Wort vom Kontext während der Übersetzung abhängt. In der vorherigen Arbeit haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungsmuster eingeführt. Dies wird durch Messung der Menge an Informationen gemessen, die der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann CXMI als den Informationsgewinn betrachten, der durch die Bereitstellung von Kontext für das Modell entsteht. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satz- oder Wortebene messen kann. Man kann Wörter mit hohem P-CXMI als solche betrachten, die für die Übersetzung Kontext benötigen. Wir analysieren Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripten von TED-Vorträgen durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst schauen wir uns die Wortarten mit hohem durchschnittlichem P-CXMI an. Dies ermöglicht es uns, zum Beispiel, Dualpronomen im Arabischen zu finden, die relativ hohes P-CXMI aufweisen. Dies kann dadurch erklärt werden, dass das Englische keine Dualpronomen hat, sodass man den Kontext benötigt, um festzustellen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Ebenso finden wir, dass bestimmte Sprachen auch Kontext benötigen, um die geeignete Verbform zu wählen. Dann schauen wir uns Vokabeln an, die hohes P-CXMI über alle ihre verschiedenen Vorkommen hinweg aufweisen. Dies hilft uns, Fälle wie diesen zu identifizieren, bei denen man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man die gleiche Übersetzung im gesamten Dokument verwendet. Ebenso finden wir, dass Kontext wichtig ist, um die richtige Formalität zu übersetzen. Schließlich schauen wir uns einzelne Tokens mit hohem P-CXMI an. Dies ermöglicht es uns, Phänomene zu identifizieren, die nicht wirklich durch das Wort selbst erfasst werden können, sondern vielmehr in der Satzstruktur zum Ausdruck kommen, wie z.B. die Auflösung von Ellipsen. Nun verwenden wir unsere Erkenntnisse aus der Analyse, um einen Benchmark für die Übersetzung auf Dokumentenebene zu entwerfen. Für jede der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die zu dem Phänomen gehören. Wir nennen unseren Tagger den Multilingual Discourse-Aware, oder MuDA-Tagger. Wir können dann auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene aufweisen. Dann verwenden wir den MuDA-Tagger, indem wir den Tagger auf ein paralleles Korpus anwenden, das wir für die Bewertung verwenden möchten, und wenden unsere Übersetzungsmetriken der Wahl auf die vom MuDA-Tagger identifizierten kontextabhängigen Beispiele an. Schließlich verwenden wir unseren Benchmark sowie andere Metriken, um Modelle für die maschinelle Übersetzung auf Dokumentenebene zu bewerten. Zunächst einmal, wenn wir korpusweite Metriken verwenden: für BLEU finden wir, dass kontext-agnostische Modelle die beste Leistung erbringen. Aber wenn wir COMET verwenden, erbringen kontextbewusste Modelle die beste Leistung. Und wenn wir die Wort-F-Messung verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistung. Dies zeigt erneut, dass es schwierig ist, das beste Übersetzungssystem auf Dokumentenebene zu bestimmen, wenn wir nur korpusweite Metriken verwenden. Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten, und finden, dass kontextbewusste Modelle signifikant genauere Ergebnisse als Modelle liefern, die keinen Kontext für bestimmte Diskursphänomene wie Formalität und lexikalische Kohäsion verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipsen, Pronomen und Verbform verwenden. Dies deutet darauf hin, wo wir mehr Fortschritte für die Übersetzung auf Dokumentenebene sehen müssen. Wir vergleichen auch verschiedene kommerzielle Systeme und unser Benchmark zeigt, dass DeepL in der Regel genauer als Google Translate für die Übersetzung auf Dokumentenebene ist. Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und verwenden unsere Erkenntnisse, um einen Benchmark für die maschinelle Übersetzung auf Dokumentenebene zu erstellen, der uns dabei hilft, zu identifizieren, welche Diskursphänomene Modelle gut oder schlecht bewältigen können, und welche Übersetzungssysteme gut für die Übersetzung auf Dokumentenebene sind. Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">Dieses Papier präsentiert "IndicMT Eval", ein Datensatz zur Meta-Evaluation von maschinellen Übersetzungsmetriken für indische Sprachen. Es adressiert die Lücke in der Bewertung von Übersetzungen in Richtung indischer Sprachen, die oft die englisch-zentrierten Metriken übernehmen, obwohl diese die einzigartigen linguistischen Merkmale indischer Sprachen nicht berücksichtigen. Die Studie konzentriert sich auf fünf indische Sprachen: Tamil und Malayalam (Dravidisch) sowie Hindi, Marathi und Gujarati (Indo-Arisch). Aus dem Flores-Datensatz wurden 200 Sätze zufällig ausgewählt und mit sieben verschiedenen Übersetzungssystemen übersetzt, was 7.000 Kandidatensätze ergibt. Bilinguale Experten annotierten diese Sätze detailliert, um Fehlerarten und -schweregrade zu identifizieren, basierend auf dem MQM-Rahmenwerk. Die Analyse zeigt, dass chrF die höchste Korrelation unter den overlap-basierten Metriken aufweist, während LabSE-Embeddings und BERTscore mit multilingualen Modellen bessere Korrelationen bieten. COMET-Metriken zeigen die höchste Korrelation insgesamt. Die Studie zeigt, dass die meisten Metriken eine eingeschränkte Skalennutzung aufweisen, was die Interpretation erschwert. Durch die Anpassung von COMET an den MQM-Datensatz entstand IndicCOMET, das in drei von fünf Sprachen die COMET-Baselines übertrifft und eine bessere Korrelation in allen Sprachen zeigt. IndicCOMET MQM zeigt auch eine höhere Robustheit im ACES Translation Accuracy Challenge Set. Der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="277">Die neue Methode hat keinen spezifischen Namen im bereitgestellten Inhalt.</sample>
    <sample id="278">Die Methode der „markierten Wörter“ basiert auf dem soziolinguistischen Konzept der „Markiertheit“, bei dem es eine unmarkierte Standardgruppe gibt und jede Gruppe, die von dieser abweicht, linguistisch markiert ist. Die Methode vergleicht die generierten Personas, indem sie die „Fightin’ Words“-Methode verwendet, die gewichtete Log-Odds-Verhältnisse nutzt, um die charakteristischen Wörter für jede markierte Gruppe zu identifizieren. Dabei werden die markierten Gruppen mit den entsprechenden unmarkierten Gruppen verglichen, um spezifische Stereotype und Muster zu erkennen.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">Dieses Papier präsentiert MultiEMO, ein auf Aufmerksamkeit basierendes, korrelationsbewusstes multimodales Fusionsframework zur Emotionserkennung in Gesprächen (ERC). Ziel der ERC ist es, die Emotion jedes Gesprächsbeitrags vorherzusagen, wobei jeder Beitrag über textuelle, auditive und visuelle Modalitäten verfügt. Die meisten bestehenden ERC-Methoden konzentrieren sich auf die Modellierung von Sprecher- und Kontextinformationen, vernachlässigen jedoch die Komplementarität multimodaler Informationen und haben Schwierigkeiten bei der Klassifizierung von Minderheits- und semantisch ähnlichen Emotionen. MultiEMO adressiert diese Herausforderungen durch vier Hauptkomponenten: unimodale Merkmalsextraktion, Kontextmodellierung, multimodale Fusion und Emotionsklassifizierung. Die Hauptbeiträge umfassen die Einführung von VisExtNet, einem visuellen Merkmalsextraktor, der redundante Szeneninformationen vermeidet, und MultiAttn, einem multimodalen Fusionsmodell, das bidirektionale multi-head cross-attention Schichten verwendet, um Modalitäten effektiv zu integrieren. Darüber hinaus wird der Sample-Weighted Focal Contrastive Loss vorgeschlagen, um die Klassifizierung von Minderheits- und semantisch ähnlichen Emotionen zu verbessern. Experimente auf den MELD- und IEMOCAP-Datensätzen zeigen, dass MultiEMO die bestehenden Leistungen übertrifft, insbesondere bei Minderheits- und semantisch ähnlichen Emotionen, obwohl es Einschränkungen wie die Unterscheidung zwischen Sprechern und irrelevanten Personen und die Anforderung großer Batch-Größen gibt.</sample>
    <sample id="281">Dieses Papier untersucht, wann Übersetzungen Kontext erfordern, und bewertet, wie gut Modelle mit diesen Fällen umgehen. Die Autoren entwickeln Pointwise Contextual Mutual Information (P-CXMI), um den Kontextbedarf von Wörtern zu messen, und analysieren TED-Transkripte, die in 14 Sprachen übersetzt wurden. Sie identifizieren fünf Diskursphänomene, die Kontext erfordern: Dualpronomen, Verbformen, Eigennamen, Formalität und Ellipsenauflösung. Mit dem Multilingual Discourse-Aware (MuDA) Tagger erstellen sie ein Benchmark-Set zur Bewertung von Übersetzungssystemen auf Dokumentenebene. Die Ergebnisse zeigen, dass kontextbewusste Modelle bei bestimmten Phänomenen wie Formalität und lexikalischer Kohäsion besser abschneiden, während sie bei anderen wie Ellipsen und Pronomen nicht signifikant besser sind. Die Analyse zeigt, dass DeepL im Allgemeinen genauer als Google Translate bei der Dokumentenübersetzung ist. Diese Arbeit bietet Einblicke, wo Fortschritte in der dokumentenbasierten Übersetzung erforderlich sind.</sample>
    <sample id="282">In "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing," Xuekai Zhu introduces a novel approach to non-parallel text style transfer at the story and discourse levels, addressing the challenge of imitating an author's linguistic preferences in long texts. Traditional methods focus on token or sentence levels, but StoryTrans advances this by learning discourse representations and combining them with style embeddings to generate texts in target styles. The model tackles two main challenges: capturing author-specific discourse structures and transferring style-specific content. StoryTrans employs a two-stage generation process, first masking style-specific content keywords and then incorporating them explicitly. The training framework includes self-reconstruction, disentanglement, sentence order, and style classifier losses to enhance content preservation and style control. Extensive experiments on new Chinese and English datasets demonstrate StoryTrans's superiority over baselines in style control and content preservation, confirmed by both automatic and manual evaluations. Style visualization shows alignment with target styles, and StoryTrans effectively supplements storylines while maintaining source semantics.</sample>
    <sample id="283">Hudson's Word Grammar</sample>
    <sample id="284">In diesem Papier wird ein neuer Mechanismus namens FSUIE (Fuzzy Span Universal Information Extraction) vorgestellt, der die Herausforderungen der aktuellen span-basierten UIE-Modelle adressiert, die stark von den exakten Grenzen annotierter Spannen abhängen. Diese Modelle leiden unter der Ambiguität der goldenen Spanngrenzen, da verschiedene Annotationen als plausibel angesehen werden können. FSUIE schlägt vor, dass die Spanngrenzen unscharf anstatt präzise sein sollten, um diese Ambiguität zu berücksichtigen. Darüber hinaus wird eine Anpassung der Aufmerksamkeitsmechanismen vorgeschlagen, um die Diskrepanz zwischen der globalen Merkmalsextraktion von Basistransformatoren und der begrenzten Länge von Spannen zu überwinden. Der Mechanismus modelliert die wahrscheinlichste Spanngrenze als kontinuierliche Verteilung und wandelt diese in diskrete Werte um, um einen unscharfen Spannverlust zu berechnen. Dieser Verlust kombiniert den binären Kreuzentropieverlust mit der Kullback-Leibler-Divergenz. Ein unscharfer Spann-Aufmerksamkeitsmechanismus wird als Maskenfunktion eingeführt, um die Aufmerksamkeitsverteilung dynamisch anzupassen. Experimente auf drei Hauptinformationsextraktionsaufgaben zeigen, dass FSUIE signifikante Leistungsverbesserungen erzielt, insbesondere bei kleineren Datensätzen, und neue State-of-the-Art-Ergebnisse auf Beziehungsextraktions- und Aspekt-Sentiment-Triplet-Extraktionsaufgaben erreicht. Die Ergebnisse der Ablationsstudie bestätigen, dass die vorgeschlagenen Mechanismen die Konvergenzgeschwindigkeit verbessern und die Informationsextraktionsfähigkeit erhöhen. Insgesamt zeigt FSUIE eine überlegene Leistung in einer Vielzahl von IE-Aufgaben.</sample>
    <sample id="285">Der Beitrag von Mingqi Gao von Peking University präsentiert die Arbeit "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". Diese Arbeit adressiert die Herausforderung von Faktenfehlern in Dialogzusammenfassungen, die durch Modelle generiert werden. Zwei Hauptansätze zur Lösung werden diskutiert: die Integration von Faktualitätszielen in den Trainingsprozess und die Entwicklung eines unabhängigen Factual Error Correction (FEC) Modells. Die Studie hebt Mängel in der Bewertung bestehender FEC-Modelle hervor, insbesondere die Verwendung vager Faktualitätsmetriken wie FactCC und DAE, die die Unterscheidung zwischen den beiden Lösungsansätzen verwischen. Um diese Probleme zu beheben, schlägt die Arbeit die Einführung manuell annotierter Referenzkorrekturen vor, um eine präzisere Bewertung zu ermöglichen. Eine neue Taxonomie von Faktenfehlern wird vorgeschlagen, die in inhaltsbasierte und formbasierte Kategorien unterteilt ist. Das vorgeschlagene Evaluationsframework basiert auf ERRANT und umfasst die Schritte Ausrichtung, Klassifizierung und Vergleich. Experimente zeigen, dass die Verwendung von Referenzzusammenfassungen aus Dialogzusammenfassungsdatensätzen die besten Ergebnisse liefert und die Notwendigkeit einer Änderung der Bewertungsmethoden für FEC-Modelle unterstreicht. Die Kombination von menschlich annotierten Daten mit synthetischen Daten wird als vielversprechender Ansatz betrachtet, während aktuelle FEC-Modelle Schwierigkeiten haben, bestimmte Faktenfehler wie Additionen und Attribute zu korrigieren.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP und SyntaxGym.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind nicht im bereitgestellten Inhalt aufgeführt. Der Text erwähnt nur zwei Methoden: FTw und COSINE.</sample>
    <sample id="291">Das Modell wird anhand von Aufgaben wie Named Entity Recognition, Klassifikation, Part-of-Speech-Tagging und Fragebeantwortung evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit dem französischen Teil des OSCAR-Datensatzes trainiert, der 138 GB umfasst.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert eine Studie zur Ironieerkennung in natürlicher Sprache, die von der Universität Turin und Amazon Alexa durchgeführt wurde. Die Forschung hinterfragt die Annahme einer einzigen Wahrheit (Ground Truth) in der manuellen Annotation und konzentriert sich auf die Erkennung von Ironie, ein komplexes Phänomen in der natürlichen Sprache. Das Team entwickelte das EPIC-Korpus (English Perspectivist Irony Corpus), bestehend aus 300 kurzen Konversationen aus sozialen Medien, gesammelt über 1½ Jahre. Die Daten wurden von 74 Annotatoren über Prolific annotiert, wobei jede Konversation durchschnittlich fünf Annotationen erhielt. Die Studie untersuchte Unterschiede in der Inter-Annotator-Übereinstimmung entlang verschiedener Dimensionen wie Geschlecht, Alter, Nationalität und fand heraus, dass die Übereinstimmung je nach Gruppe variierte. Durch die Entwicklung von perspektivenbewussten Modellen, die auf verschiedenen Annotatoren-Splits trainiert wurden, zeigten diese Modelle eine höhere Zuversicht in ihren Vorhersagen im Vergleich zu Modellen, die auf einer aggregierten Goldstandard-Annotation basierten. Besonders auffällig war, dass Altersgruppen, die sich näher stehen, häufiger in ihrer Wahrnehmung von Ironie uneinig waren, und dass die größten Unterschiede in den Antworten zwischen Annotatoren aus dem Vereinigten Königreich und Irland bestanden.</sample>
    <sample id="297">Das Projekt "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models" untersucht die Verwendung von Dogwhistles in der politischen Rhetorik, um verdeckte, oft kontroverse Botschaften zu kommunizieren. Dogwhistles sind Begriffe, die für eine In-Gruppe eine zweite, oft tabuisierte Bedeutung haben, während sie für eine Out-Gruppe harmlos erscheinen. Ein Beispiel ist der Begriff "cosmopolitan", der in bestimmten Kontexten als anti-semitisch interpretiert werden kann. Das Projekt entwickelt eine Typologie und ein Glossar mit über 340 Begriffen, die hauptsächlich rassistische, transphobe und anti-semitische Dogwhistles umfassen. Eine Analyse historischer US-amerikanischer politischer Reden zeigt, dass die Verwendung von Dogwhistles seit der Bürgerrechtsbewegung zugenommen hat, insbesondere im Zusammenhang mit der Republikanischen Südstaatenstrategie. Experimente mit dem Sprachmodell GPT-3 zeigen, dass es in der Lage ist, viele Dogwhistles zu identifizieren, insbesondere formelle, jedoch weniger erfolgreich bei informellen und transphoben Begriffen. Die Studie zeigt auch, dass Dogwhistles Inhaltsmoderationssysteme umgehen können, indem sie die Wahrnehmung von Toxizität in Sätzen verringern, wenn standardmäßige Gruppenbezeichnungen durch Dogwhistles ersetzt werden. Das Projekt trägt somit zum Verständnis der Rolle von Dogwhistles in der politischen Kommunikation und deren Auswirkungen auf die Inhaltsmoderation bei.</sample>
    <sample id="298">Die Schlussfolgerung, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war, basierte auf einem Experiment, bei dem einige Modelle mit aktuelleren Daten weiter trainiert oder vortrainiert wurden. Es wurde festgestellt, dass die Leistung mit zunehmendem zeitlichen Abstand zwischen Trainings- und Testdaten abnahm, was die Hypothese der zeitlichen Verzögerung bestätigte.</sample>
    <sample id="299">Dieses Papier präsentiert eine neue Methode zur Verbesserung der Robustheit von Natural Language Inference (NLI) Modellen durch Minimax-Training, um die Abhängigkeit von Kurzschlüssen zu reduzieren. NLI-Modelle neigen dazu, Kurzschlüsse zu nutzen, die spurious Korrelationen zwischen Eingabeeigenschaften und Labels darstellen, was zu schlechter Leistung bei Out-of-Distribution-Daten führt. Die vorgeschlagene Methode zielt darauf ab, die Gewichtung von unterrepräsentierten „schwierigen“ Beispielen zu erhöhen, die die Kurzschlüsse in den dominanten „leichten“ Beispielen konterkarieren. Ein Lernmodell minimiert den Verlust der NLI-Aufgabe, während ein Hilfsmodell den Verlust maximiert, indem es Beispielgewichte generiert, die den Lerner dazu anregen, sich auf Bereiche des Eingaberaums zu konzentrieren, in denen hohe Verluste auftreten. Beide Modelle werden abwechselnd optimiert. Die Methode verwendet ein Feed-Forward-Netzwerk für das Hilfsmodell und erfordert keine Annahmen über die Art der Kurzschlüsse im Datensatz. Die Evaluation zeigt, dass das Minimax-Training die Out-of-Distribution-Leistung im Vergleich zu herkömmlichen Methoden verbessert, während die In-Distribution-Akzeptanz hoch bleibt. Die Studie untersucht auch die Auswirkungen der Vorab-Training des Lerners, die Größe des Hilfsmodells und führt eine qualitative Bewertung der gelernten Beispielgewichtverteilung durch.</sample>
    <sample id="300">Belinda präsentiert die Einführung des interaktiven Diktierens, einer neuen Aufgabe, die es Benutzern ermöglicht, Dokumente durch natürliche Sprache zu diktieren und zu bearbeiten. Diese Arbeit wurde von Semantic Machines in Zusammenarbeit mit Jason Eisner, Adam Pauls und Sam Thomson durchgeführt. Interaktives Diktieren ermöglicht es Benutzern, während der Diktation Korrekturen vorzunehmen und Befehle zu erteilen, ohne auf festgelegte Triggerwörter angewiesen zu sein. Die Aufgabe umfasst vier Schritte: ASR-Transkription, Segmentierung in Diktation und Befehle, Normalisierung und Ausführung der Befehle. Die Forschung umfasst die Entwicklung einer Datenbankschnittstelle und eines Datensatzes sowie eines Basissystems, das T5- und GPT-3-Modelle verwendet. Die Ergebnisse zeigen, dass GPT-3-Modelle genauere, aber langsamere Vorhersagen liefern, während T5-Modelle eine effizientere Programmierung ermöglichen. Die Forschung eröffnet Möglichkeiten für zukünftige Verbesserungen und bietet Code und weitere Details in der begleitenden Publikation.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, weil nach dem ersten Schritt, in dem jedem Eingangstoken ein unsortiertes Multiset von Tokens zugeordnet wird, die richtigen Tokens vorhanden sind, aber nicht in der richtigen Reihenfolge. Der zweite Schritt verwendet ein Modell, um eine Permutation vorherzusagen, um die Tokens in die richtige Reihenfolge zu bringen.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um zu verstehen, ob positive Stereotype und essentialisierende Erzählungen auf übermäßige Wertanpassungen oder andere Anti-Stereotypisierungsmethoden zurückzuführen sind. Ohne Transparenz können keine fundierten Annahmen getroffen oder weiterführende Studien durchgeführt werden.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind Sätze, die aus einem ungrammatischen oder inakzeptablen Satz bestehen, der mit einem akzeptablen oder grammatikalischen Satz verglichen wird. In der Minimalpaarparadigma (MPP) wird erwartet, dass Sprachmodelle dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuweisen. Inakzeptable Eingaben können durch das Hinzufügen von unakzeptablen Präfixen zu akzeptablen Sätzen oder durch das Verwenden von Sätzen aus unterschiedlichen Phänomenen oder irrelevanten Domänen wie Wikipedia erstellt werden.</sample>
    <sample id="305">In der Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" untersuchen Dawei und Kollegen die Anforderungen an saubere Validierungsdaten in der schwach überwachten Lernmethode (WSL). WSL nutzt schwache Labelquellen wie Heuristiken oder Crowdsourcing, um Daten zu annotieren, was kostengünstig, aber fehleranfällig ist. Die Studie zeigt, dass aktuelle WSL-Methoden saubere Validierungsdaten benötigen, um korrekt zu funktionieren, da ohne diese die Modelle nicht generalisieren können. Es wird festgestellt, dass bereits 20 saubere Proben pro Klasse ausreichen, um hohe Leistung zu erzielen. Interessanterweise übertrifft direktes Feintuning mit sauberen Daten die WSL-Methoden, die diese nur zur Validierung nutzen. Die Forschung empfiehlt, die Abhängigkeit von sauberen Validierungsdaten offenzulegen, WSL-Methoden mit wenigschichtigen Lernansätzen zu vergleichen und kontinuierliches Feintuning als Baseline zu berücksichtigen. Die Ergebnisse deuten darauf hin, dass die Leistungssteigerungen von WSL-Methoden überschätzt sind und die Notwendigkeit sauberer Daten oft übersehen wird.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim untersuchen die Fähigkeit von Sprachmodellen zur Entitätenverfolgung in Diskursen. Sie argumentieren, dass das Verständnis von Entitäten und deren Zustandsänderungen entscheidend für das Verständnis längerer Diskurse ist, wie z.B. in Rezepten. Die Herausforderung besteht darin, dass Sprachmodelle möglicherweise aufgrund von Mustern in der Trainingsdaten oder einfachen Wortassoziationen korrekte Vorhersagen treffen, ohne tatsächlich Entitäten zu verfolgen. Um dies zu bewerten, entwickelten sie eine Aufgabe, die das Verfolgen von Objekten in Boxen umfasst, wobei mehrere Operationen wie das Bewegen oder Hinzufügen von Objekten durchgeführt werden. Die Modelle Flan-T5 und GPT-3/-3.5 wurden mit 2-shot in-context Lernen getestet. Die Ergebnisse zeigen, dass nur GPT-3.5-Modelle, die auf umfangreichen Code trainiert wurden, nicht-triviale Entitätenverfolgungsfähigkeiten aufweisen. Kleinere Modelle wie T5-base können durch direkte Feinabstimmung lernen, Entitäten zu verfolgen, während zufällig initialisierte Modelle dies nicht können. Die Autoren betonen, dass weitere Untersuchungen erforderlich sind, um die Allgemeingültigkeit dieser Fähigkeiten zu bestimmen. Weitere Ergebnisse und Analysen, einschließlich Experimenten mit GPT-4, sind in ihrer Veröffentlichung auf arXiv verfügbar.</sample>
    <sample id="307">Die Autoren haben die Modelle anhand von Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech-Tagging und Fragebeantwortung bewertet. Es wird jedoch nicht spezifisch auf die verwendeten Bewertungsmetriken eingegangen.</sample>
    <sample id="308">Die Präsentation von Jenny, einer ersten Jahres-PhD-Studentin an der Carnegie Mellon University, untersucht die Design-Bias in NLP-Daten und Modellen durch die Einführung von NLPositionality. Diese Arbeit, in Zusammenarbeit mit Forschern der University of Washington und des Allen Institute for AI, beleuchtet, wie die Positionalität von NLP-Forschern und Modellentwicklern zu systematischen Leistungsunterschieden zwischen verschiedenen Bevölkerungsgruppen führen kann. Positionalität, beeinflusst durch Demografie, Identität und Lebenserfahrung, kann die Forschungsergebnisse verzerren. Die Studie hinterfragt, ob Daten und Modelle Positionalität aufweisen, indem sie die Aggregation von Urteilen und Meinungen realer Menschen betrachtet. Durch die Re-Annotierung von Datensätzen mit diversen Annotatoren und den Vergleich dieser mit bestehenden Modellen und Datensätzen mittels Pearson's R Korrelationskoeffizienten, zeigt die Studie, dass NLP-Modelle und -Datensätze hauptsächlich mit englischsprachigen Ländern und Personen mit Hochschulbildung übereinstimmen. Dies führt dazu, dass bestimmte Gruppen, wie nicht-binäre Personen, weniger abgedeckt sind. Empfehlungen umfassen die Dokumentation relevanter Designentscheidungen, die Forschung mit einem perspektivischen Ansatz und die Entwicklung spezialisierter Datensätze und Modelle für bestimmte Gemeinschaften, wie das Masakhani-Initiative. Weitere Informationen sind auf der Projekt-Dashboard und im Papier verfügbar.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Die Autoren gehören der Universität Duisburg-Essen an.</sample>
    <sample id="312">MultiInstruct ist der erste große Multi-Modal Instruction Tuning Benchmark, der 62 diverse Multi-Modal Aufgaben in 10 breiten Kategorien umfasst. Diese Aufgaben stammen aus 21 bestehenden Open-Source-Datensätzen, wobei jede Aufgabe mit fünf von Experten verfassten Anweisungen ausgestattet ist. Im Gegensatz zu bestehenden Benchmarks, die sich hauptsächlich auf Sprachaufgaben konzentrieren, adressiert MultiInstruct die Lücke in der Verfügbarkeit von Anweisungsdatensätzen für Multi-Modal-Aufgaben.</sample>
    <sample id="313">Die Arbeit wurde von der Emory NLP Lab unter der Leitung von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Es wird nicht spezifiziert, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="314">Die Definition der binären Koordination im Kontext des Vortrags bezieht sich auf die Struktur, bei der zwei Elemente (Konjunkte) miteinander verbunden werden. Die Diskussion umfasst verschiedene theoretische Ansätze zur Strukturierung dieser Koordination, einschließlich asymmetrischer Ansätze, bei denen ein Konjunkt als Kopf fungiert (z.B. Universal Dependencies und Igor Mel'čuks Meaning-Text Theory), und symmetrischer Ansätze, bei denen alle Konjunkte als gleichwertig betrachtet werden (z.B. Hudsons Word Grammar). Der Vortrag argumentiert für symmetrische Strukturen basierend auf dem Prinzip der Minimierung der Abhängigkeitslänge.</sample>
    <sample id="315">Der Inhalt gibt keine spezifische Information über die durchschnittliche Länge der in der Studie verwendeten Prompts.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, das auf dem CoScript-Datensatz feinabgestimmt wurde, in der Lage ist, Skripte von höherer Qualität zu generieren als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere, spezialisierte Modelle die Leistung größerer Modelle übertreffen können, wenn sie auf geeigneten Datensätzen trainiert werden.</sample>
    <sample id="317">Dieses Papier präsentiert "CodeIE", eine Methode, die große Code-Generierungsmodelle für die effiziente Few-Shot-Informationsextraktion nutzt. Traditionelle Ansätze zur Informationsextraktion, wie Named Entity Recognition (NER) und Relation Extraction (RE), verwenden vorab trainierte Sprachmodelle wie T5 und GPT-3, die während der Vorverarbeitung in einem Text-zu-Text-Modus arbeiten. Allerdings wird die strukturierte Ausgabe während der Inferenz linearisiert, was zu einer Diskrepanz zwischen den Trainings- und Inferenzformaten führt und spezielle Strategien erfordert. CodeIE transformiert die Aufgabe in eine Struktur-zu-Struktur-Code-Generierungsaufgabe, indem es Code-LLMs wie Codex verwendet, um die Ausgabestruktur zu erhalten. Durch die Verwendung von Code-Prompts, die die Eingabe in strukturierte Formate umwandeln, wird die Ausgabe besser ausgerichtet. Die Methode wurde auf drei NER-Datensätzen und vier RE-Datensätzen evaluiert, wobei Modelle wie T5, UIE, GPT-3 und Codex verglichen wurden. Die Ergebnisse zeigen, dass CodeIE mit Code-Prompts in One-to-Few-Shot-Szenarien die traditionellen Modelle deutlich übertrifft, mit geringeren strukturellen Fehlern und besseren Recall-Werten. Die Analyse zeigt, dass Code-LLMs besser mit der Informationsextraktionsaufgabe übereinstimmen und weniger strukturelle Fehler aufweisen. Insgesamt bietet CodeIE eine effektive Lösung für die Herausforderungen der Informationsextraktion.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu "DrBERT: Ein robuster vortrainierter Modell für die französischsprachige Biomedizin- und Klinikumgebung" vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Dann stellen wir die Hauptbeiträge unseres Artikels vor. Wir stellen das erste biomedizinische Modell in Französisch namens DrBERT vor, das auf RoBERTa basiert und auf NACHOS trainiert wurde, einem Datensatz medizinischer aus dem Web gecrawlter Daten. Wir haben auch einen Vergleich von Modellen mit mehreren Vortrainingseinstellungen und Datenquellen vorgestellt. Anschließend präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich fassen wir die Experimente zusammen und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einer der effektivsten Methoden geworden, um Aufgaben der natürlichen Sprachverarbeitung zu lösen, und bietet enorme Leistungssteigerungen im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder mehr. Seitdem wurde dieses Modell in viele andere Sprachen, wie Französisch mit CamemBERT, und auch in Domänen wie Biomedizin mit PubMedBERT und BioBERT sowie auf klinischer Ebene mit ClinicalBERT, aber hauptsächlich in Englisch, angepasst. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vortraining aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bislang kein Open-Source-Modell für die Biomedizin. Daher fragten wir uns, welche die geeignetsten Datenquellen für eine breite Anwendung sind und ob diese gecrawlten Daten eine gute Substitution für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager des Universitätsklinikums Nantes stammen. Anschließend fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier von-Null-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB eines Teils von NACHOS; eine erste Version von ChuBERT, ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen; und eine finale Version von ChuBERT mit einer Mischung aus 4 GB eines Teils von NACHOS und 4 GB von klinischen Notizen. Zusätzlich zu diesem Vergleich haben wir drei Modelle eingeführt, die auf kontinuierlichem Vortraining basieren, um die Auswirkungen der Vortrainingsstrategie zu analysieren. Eines basiert auf den Gewichten von CamemBERT und wird auf einem 4 GB großen Teil von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wird jedoch dieses Mal auf 4 GB von klinischen Notizen trainiert, und schließlich eines, das auf dem englischsprachigen biomedizinischen Modell PubMedBERT basiert und auf 4 GB eines Teils von NACHOS trainiert wird. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammeln wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech-Tagging und Fragebeantwortung. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Bewertung zeigt, dass die Modelle am besten auf Aufgaben mit Daten abschneiden, die der Natur der Daten entsprechen, auf denen das Modell trainiert wurde. Wir beobachten jedoch, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir stellen auch fest, dass die Verwendung mehrerer Daten zu einer besseren Leistung führt. Insgesamt scheint das von-Null-Vortraining auf den meisten Aufgaben eine höhere Leistung zu erzielen. Unsere Experimente zum kontinuierlichen Vortraining unter Verwendung der Gewichte und Tokenisierung von CamemBERT, trainiert auf dem 4 GB großen Teil von NACHOS, zeigten vergleichbare Ergebnisse zu denen, die mit DrBERT 4 GB von-Null erzielt wurden. Dies ist nicht der Fall für das Modell, das auf den Gewichten und dem Tokenizer von CamemBERT basiert, das mit Stabilitätsproblemen zu kämpfen hat. Abschließend bietet unser spezielles System eine bessere Leistung bei neun der 11 Downstream-Aufgaben und übertrifft insgesamt die Ergebnisse des generischen Modells, hier CamemBERT. Wir beobachten auch, dass spezialisierte Daten besser sind, aber nicht gut skalieren. Alle vortrainierten Modelle, die aus NACHOS stammen, sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz, und alle Trainings-Skripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns darauf, im Poster-Seminar in Toronto auszutauschen.</sample>
    <sample id="319">Die Arbeit untersucht folgende Lernstrategien:

1. Von-Null-Anfangs-Training (from-scratch) mit verschiedenen Datenmengen (4 GB und 7 GB) von NACHOS.
2. Von-Null-Anfangs-Training mit anonymisierten klinischen Daten (4 GB) für ChuBERT.
3. Kontinuierliches Training (continual pre-training) mit verschiedenen Basismodellen:
   - CamemBERT, trainiert auf 4 GB von NACHOS.
   - CamemBERT, trainiert auf 4 GB klinischer Notizen.
   - PubMedBERT, trainiert auf 4 GB von NACHOS.</sample>
    <sample id="320">Adaptive overfitting, speziell auf die Wiederverwendung von Tests zurückzuführend, wurde in diesem Fall nicht beobachtet. Dies wurde durch die Analyse gezeigt, dass die Steigung der besten Anpassungslinie größer als eins war, was darauf hinweist, dass es keine abnehmenden Erträge auf einem neuen Testset gab.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch die Analyse der Art der Vereinfachung in den Satzpaaren bewertet, einschließlich lexikalischer Vereinfachung, struktureller Vereinfachung und des Gesamtniveaus der Vereinfachung. Es wurde festgestellt, dass die Bibeltexte stärker vereinfacht wurden als die Nachrichtentexte oder die Texte für Sprachlerner. Zudem wurde die Vielfalt der verschiedenen Vereinfachungstransformationen im DEPLAIN-Korpus hervorgehoben.</sample>
    <sample id="322">Der Vortrag von Enrico bei ACL 23 untersucht, was Textklassifikatoren über Moral lernen. Er betont, dass menschliche Moral subjektiv ist und verschiedene Menschen dieselben Konzepte unterschiedlich bewerten. Die Moral Foundation Theory, die fünf moralische Grundlagen vorschlägt, wird als Rahmen verwendet, um zu verstehen, wie Menschen Moral wahrnehmen. Diese Theorie wird in der NLP-Forschung angewendet, um Moral in Texten zu klassifizieren. Enrico präsentiert eine Studie, die erklärbare KI-Techniken verwendet, um zu untersuchen, wie Textklassifikatoren Moral in verschiedenen Domänen erkennen. Die Studie nutzt das Moral Foundation Twitter Corpus, das 35.000 Tweets aus sieben Domänen umfasst, darunter #AllLivesMatter und #BlackLivesMatter. Die Ergebnisse zeigen, dass Klassifikatoren feine Unterschiede in der moralischen Ausdrucksweise erkennen, wie z.B. die unterschiedliche Wahrnehmung von Subversion in ALM und BLM. Die Studie warnt davor, dass die Verwendung eines einzigen Modells für verschiedene Domänen zu Missverständnissen führen kann.</sample>
    <sample id="323">Dieses Papier präsentiert DHLK, eine Methode zur Verbesserung der Leistung bei der Beantwortung von Fragen zum Allgemeinwissen (Commonsense QA) durch dynamische heterogene Graphen- und Sprachmodellverarbeitung. Commonsense QA erfordert die Integration von Wissen aus Sprachmodellen und Wissensbasen, um relevante Informationen zu extrahieren und zu verarbeiten. Bestehende Ansätze leiden unter der Einführung von Rauschentitäten und begrenzter Interaktion zwischen Text und Graphen. DHLK adressiert diese Probleme durch den Aufbau eines heterogenen Wissensgraphen (HKG) mit einem zweistufigen Pruning- und Wissensrepräsentationslernprozess. Es entfernt irrelevante Subwörter und verbindet Paraphrasen von Schlüsselentitäten, um den HKG zu erweitern. Die Methode verwendet RoBERTa und Mask Self-Attention zur Kodierung und Fusion von QA-Kontexten und Entitäten, wobei dynamisch irrelevante Entitäten entfernt werden. TransE optimiert die Einbettungen von Entitäten und Beziehungen, während Relation Mask Self-Attention (RMSA) zur Modellierung des HKG eingesetzt wird. Die endgültige Antwortvorhersage erfolgt durch Eingabe der Graph- und Kontexteinbettungen in eine MLP. Experimente auf CommonsenseQA und OpenBookQA zeigen, dass DHLK im Vergleich zu anderen Methoden überlegene Ergebnisse erzielt.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Sie zeigen politische Neigungen, die sich in verschiedenen Quadranten des politischen Spektrums befinden, und diese Neigungen können durch die Art der Trainingsdaten beeinflusst werden. Unterschiedliche Sprachmodelle, wie die GPT- und BART-Serien, zeigen unterschiedliche politische Ausrichtungen, wobei die GPT-Serie tendenziell sozial liberaler ist. Diese politischen Vorurteile können sich auf die Leistung der Modelle bei Aufgaben wie der Erkennung von Hassrede und Falschmeldungen auswirken, was zu Fairnessproblemen führen kann.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute möchte ich Ihnen eine kurze Einführung in unsere Arbeit mit dem Titel "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, tiefere Rekursion und bisher nicht gesehene Zusammensetzungen von Phrasen zu handhaben, die individuell während des Trainings gesehen wurden. Im Kontext der semantischen Parsing könnte ein Test für kompositionelle Generalisierung so aussehen. Wie üblich haben wir eine Trainingsmenge von Äußerungen. In diesem Fall: "The girl slept." und "Mary knew that the girl slept." Diese Äußerungen sind mit logischen Formen gepaart, die zentrale Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung stammt die Testmenge nicht aus derselben Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings eine flache Rekursion gesehen und wird auf ein Beispiel mit tieferer Rekursion getestet. Naive Seq2Seq-Modelle haben Schwierigkeiten mit dieser Art von Out-of-Distribution-Verallgemeinerung und produzieren oft Ausgaben, die vom Eingang abgekoppelt sind. Insbesondere versagen sie oft darin, die systematischen Entsprechungen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in dem Beispiel farblich hervorgehoben sind. Ein beliebter Ansatz, um dieses Problem zu lösen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositionellen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Dies funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen auf irgendeine Weise ermittelt werden. Dies kann kompliziert und manchmal ein rechenintensiver Prozess sein. Typischerweise beinhaltet dies eine erhebliche formalismusspezifische Vorverarbeitung der logischen Formen, z. B. zur Handhabung von Variablenzeichen. Die Ermittlung von Bäumen kann auch spezialisierte Grammatik-Induktionsverfahren beinhalten. In dieser Arbeit verwenden wir keine Bäume und stellen ein neuronales Seq2Seq-Modell vor, das die Entsprechungen zwischen Fragmenten der Eingabe und Fragmenten der Ausgabe direkt modelliert. Erstmals zeigen wir eine starke Generalisierung zu tieferer Rekursion ohne die Verwendung von Bäumen. Unser Ansatz prognostiziert die Ausgabe aus der Eingabe in zwei Schritten. Zunächst kennzeichnen wir jedes Eingabetoken mit einer unsortierten Menge von Tokens, die in der Ausgabe erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Tokens, aber sie sind nicht sortiert. Deshalb verwenden wir im zweiten Schritt ein anderes Modell, um eine Permutation vorherzusagen, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Dies macht unseren Ansatz sehr flexibel und ausdrucksstark. Konzeptuell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über die Ausgabe und bestimmen, welches Menge-Token in jede Position gesetzt wird. Für die erste Ausgabeposition wählen wir einfach eines aus, wie in Rot hervorgehoben. Dann springen wir zum nächsten Menge-Token, um das zweite Token in der Ausgabe zu bestimmen. Wir bestimmen das dritte Token in der Ausgabe auf ähnliche Weise, indem wir zu einem anderen Menge-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen baumlosen Modellen auf der COGS-Benchmark. Unser Modell übertrifft die anderen bei der Generalisierung zu tieferer Rekursion um einen großen Abstand. Einige andere Arten struktureller Generalisierung bleiben jedoch sehr herausfordernd. In unserer Arbeit lösen wir einige interessante technische Herausforderungen. Zunächst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe im Trainingsdatensatz nicht gegeben. Daher wissen wir für ein gegebenes Token nicht, aus welcher Menge es stammt, was eine Herausforderung für das Training darstellt. Außerdem gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt jedoch die Herausforderung mit sich, dass die Suche nach der höchstbewerteten Permutation NP-schwer ist. Dies liegt daran, dass dies mit dem "Reisenden Händler"-Problem zusammenhängt. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Entspannung, die es uns auch ermöglicht, durch die Lösung zurückzupropagieren und die linguistisch plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen, erfahren möchten, schauen Sie bitte in unsere Arbeit oder besuchen Sie unser Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist das Phänomen, bei dem zwei Überzeugungen oder Handlungen inkonsistent sind, wie zum Beispiel, wenn jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann "Ich habe nach dem Meeting ein paar Zigaretten geraucht". Diese Überzeugung und Handlung sind inkonsistent und in Dissonanz.</sample>
    <sample id="327">Dieses Papier präsentiert ManagerTower, eine innovative Architektur für das Vision-Language (VL) Lernen, die darauf abzielt, die Einsichten mehrerer unimodaler Experten zu aggregieren, um die Repräsentationslernfähigkeit zu verbessern. Im Gegensatz zu bestehenden zwei-Turm-Architekturen, die nur die letzte Schicht unimodaler Repräsentationen verwenden, nutzt ManagerTower mehrere Schichten unimodaler Repräsentationen durch Manager in jedem cross-modalen Layer. Diese Manager aggregieren adaptiv die Einsichten der unimodalen Experten, um eine umfassendere cross-modale Ausrichtung und Fusion zu ermöglichen. Im Vergleich zu BridgeTower, das eine feste Zuordnung von unimodalen Schichten zu cross-modalen Layern verwendet, ermöglicht ManagerTower eine flexiblere und effektivere Nutzung der unimodalen semantischen Kenntnisse. Mit RoBERTa und CLIP-ViT als unimodalen Encodern erreicht ManagerTower herausragende Leistungen auf verschiedenen Downstream-Aufgaben, einschließlich einer 39,15%igen Genauigkeit auf dem Wikivideo-Teststandard, und übertrifft Modelle, die mit mehr Daten oder Parametern trainiert wurden. Die Visualisierung der Aggregationsgewichte zeigt, dass adaptive Manager unterschiedliche Gewichtsverteilungen in verschiedenen cross-modalen Layern aufweisen, was ihre Fähigkeit zur adaptiven Nutzung unimodaler semantischer Kenntnisse unterstreicht. Das Papier, der Code und die Modelle sind auf Archive und GitHub verfügbar.</sample>
    <sample id="328">GPT-4 steht am meisten links.</sample>
    <sample id="329">In diesem Papier präsentieren Minghang Zheng und Kollegen eine Methode zur zero-shot Video-Satz-Lokalisierung, die strukturierte Pseudo-Labels zur Reduzierung von Label-Rauschen verwendet. Die Aufgabe besteht darin, relevante Videoabschnitte basierend auf einer natürlichen Sprachabfrage zu identifizieren, ohne manuelle Annotationen. Bestehende Methoden generieren einfache Pseudo-Abfragen und -Ereignisse, was zu einer Diskrepanz zwischen realen Abfragen und unzureichender Relevanz außerhalb der Ereignisse führt. Die vorgeschlagene Methode verwendet ein vortrainiertes Bild-Beschreibung-Modell, um komplexe Pseudo-Abfragen zu generieren, und ein vortrainiertes Modell zur Messung der Relevanz zwischen Einzelbildern und Pseudo-Abfragen, um Ereignisse mit hoher Relevanz zu generieren. Die Methode reduziert das Rauschen durch Gewichtung von Stichproben und Verfeinerung von Labels. Experimente auf den Datensätzen ActivityNet Captions und Charades-STA zeigen, dass die Methode die bestehenden zero-shot Methoden in den meisten Metriken übertrifft. Die Methode erreicht die beste zero-shot Leistung auf zwei Datensätzen.</sample>
    <sample id="330">Kumulatives Training ist gleichwertig oder besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED Talks, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">In this work, "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation," we address the challenge of non-smooth representation spaces in neural machine translation (NMT) models, which hinder generalization due to the sparse dispersion of low-frequency tokens. We propose the INK framework to inject kNN knowledge into NMT, enhancing generalization and performance. INK utilizes a training loop with two steps: extracting kNN knowledge from a datastore to adjust representations and asynchronously updating the datastore with refined representations. This loop continues until convergence. We align contextualized representations with token embeddings and kNN token embeddings to maintain and enrich semantic meanings, addressing sparsity issues. Experiments on the WMT’19 German-English news translation task demonstrate that INK outperforms the state-of-the-art kNN-MT system, achieving an average gain of 1.99 COMET and 1.0 BLEU scores. The INK system achieves higher BLEU scores with less memory space and faster inference speed, showing that smoother representation spaces significantly improve translation performance.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf das Training eines Modells in einer Quellsprache und das Übertragen des trainierten Modells auf eine andere Zielsprache. Es gibt zwei spezifische Einstellungen: Cross-lingual Zero-shot Transfer, bei dem das Modell ohne jegliche Anpassung an die Zielsprache übertragen wird, und Cross-lingual Few-shot Transfer, bei dem das Modell mit einer kleinen Menge an Daten der Zielsprache angepasst wird.</sample>
    <sample id="337">Dieses Papier präsentiert eine innovative Methode zur Bedeutungsinferenz von Out-of-vocabulary (OOV) Wörtern durch Graph-basiertes Relation Mining. OOV-Wörter stellen eine Herausforderung für die Leistung von embedding-basierten Modellen dar. Die vorgeschlagene Methode nutzt Wortbildung und -assoziation, um die Bedeutung von OOV-Wörtern zu inferieren. Ein Word Relationship Graph, der lexikalische Regeln nachahmt, wird eingeführt. OOV-Wörter werden in Wortstücke zerlegt und mit relevanten Wörtern assoziiert, um eine zweistufige Graphenstruktur zu bilden. Jedes Wort oder Wortstück ist ein Knoten mit einem entsprechenden Wort-Embedding als Attribut. Ein Graph Attention Network (GAT) wird verwendet, um die wichtigsten Informationen zu extrahieren und die Auswirkungen von Rauschneigbarn zu reduzieren. Eine Readout-Block-Schicht fasst die gesamte Grapheninformation zusammen. Die Verwendung von kontrastivem Lernen im Verlustfunktion mit NT-XENT-positiven Proben fördert die Nähe zwischen relevanten Wörtern. Experimente zeigen, dass das Modell die Leistung von Baseline-Modellen in intrinsischen und extrinsischen Aufgaben übertrifft. Es verbessert sowohl statische als auch kontextuelle Modelle in nachgelagerten Aufgaben. Die Anwendbarkeit auf andere Sprachen hängt von der Rationalität der Wortzerlegung ab, wobei agglutinierende Sprachen besser geeignet sind.</sample>
    <sample id="338">Der Vortrag von Bingsheng präsentiert die Forschungsarbeit "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations", eine Zusammenarbeit von Rensselaer Polytechnic Institute, Northeastern University und IBM Research. Die Studie untersucht die Qualität und Nützlichkeit von menschlichen Erklärungen in maschinellen Lernmodellen. Traditionelle Bewertungsmethoden wie BLEU und ROUGE konzentrieren sich auf Wortähnlichkeit und behandeln menschliche Annotationen als Goldstandard, was die subjektive und aufgabenabhängige Natur von Erklärungen ignoriert. Die Forschung schlägt eine einheitliche Datenstruktur vor, die verschiedene Aufgaben in ein einheitliches Multiple-Choice-Format umwandelt, und führt Vor- und Nachteile von Erklärungen in Modellen durch. Die Studie führt ein neues Bewertungsmetrik, TREU, ein, das die Nützlichkeit von Erklärungen während des Feinabstimmungsprozesses bewertet und traditionelle Metriken wie den Simulatabilitätsscore übertrifft. Die Analyse zeigt, dass menschliche Erklärungen, selbst wenn sie als niedrigwertig angesehen werden, dennoch Modellvorhersagen verbessern können. Die Ergebnisse betonen die Aufgaben- und Formatabhängigkeit von Erklärungen und legen nahe, dass zukünftige Forschungen ähnliche Qualitätsprüfungen durchführen sollten.</sample>
    <sample id="339">Die Autoren gehören der Saarland University in Deutschland an.</sample>
    <sample id="340">In diesem Papier präsentieren wir "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation", eine Zusammenarbeit von Kuan-Hao Huang und Kollegen. Das Ziel ist es, ein großes, syntaktisch vielfältiges Paraphrasen-Dataset zu erstellen, um die Leistung von NLP-Anwendungen zu verbessern. Traditionelle Methoden zur Paraphrasenerzeugung, wie Back-Translation, erzeugen zwar große Mengen an Daten, bieten jedoch oft nicht genügend syntaktische Vielfalt. Wir nutzen AMR (Abstract Meaning Representations), um diese Lücke zu schließen. Durch das Ändern des Fokus in AMR-Graphen und die anschließende Textgenerierung erzeugen wir paraphrasierte Sätze, die semantisch ähnlich, aber syntaktisch unterschiedlich sind. Das resultierende Dataset, ParaAMR, enthält etwa 15 Millionen Quellsätze mit durchschnittlich 6,9 Paraphrasen pro Satz. Quantitative Analysen zeigen, dass ParaAMR ähnliche semantische Ähnlichkeitswerte wie andere Back-Translation-Datasets aufweist, jedoch höhere syntaktische Diversität. Anwendungen wie das Erlernen von Satzeinbettungen, syntaktische Kontrolle bei der Paraphrasenerzeugung und Datenverstärkung für Few-Shot-Lernen profitieren von ParaAMR. Das Dataset ist öffentlich zugänglich und zeigt, dass es bestehende Paraphrasen-Datasets in mehreren NLP-Anwendungen übertrifft.</sample>
    <sample id="341">Die Autoren verwenden zwei Latenzmessungen: die durchschnittliche Verzögerung (average lagging) und die computeraufmerksame durchschnittliche Verzögerung (computational-aware average lagging), die die Rechenzeiten des Modells zur Vorhersage des Ausgangs berücksichtigt.</sample>
    <sample id="342">Dieses Papier präsentiert LiveChat, ein groß angelegtes, personalisiertes Dialog-Dataset, das automatisch aus Live-Streaming-Videos erstellt wurde. Die Autoren, Gao Jingsheng, Lian Yixin, Zhou Ziyi, Fu Yuzhuo und Wang Baoyuan, stellen die Bedeutung von Open-Domain-Dialogen hervor, die eine breite Palette von Themen abdecken und keine spezifischen Ziele haben. Sie betonen die Notwendigkeit eines video-basierten Dialog-Datasets, das realistischere gesprochene Konversationen widerspiegelt, im Gegensatz zu den hauptsächlich textbasierten bestehenden Datensätzen. LiveChat adressiert die Herausforderungen bestehender Datensätze, wie die begrenzte Skalierung und die Schwierigkeiten bei der Darstellung von Persönlichkeitsinformationen und mehrseitigen Dialogen. Das Dataset wird in drei Schritten erstellt: Extraktion von Streaming-Videos, Transkription von Audio in Dialoge und Sammlung von Persönlichkeitsinformationen. LiveChat übertrifft bestehende Datensätze in Bezug auf Skalierung und Personalisierung. Experimente zeigen, dass LiveChat die Leistung von Modellen in Aufgaben wie Response Modeling und Addressee Recognition verbessert. Die Ergebnisse bestätigen die Einzigartigkeit von LiveChat und seine Nützlichkeit für die Entwicklung von Anwendungen wie virtuellen Streamern und Mitarbeitern. Zukünftige Arbeiten werden sich auf effiziente Transferlernmethoden für LiveChat konzentrieren.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natürliche Sprachverstehensmodelle stützen sich auf eine Vielzahl von Wissensquellen, wie Wissen, das in ihren Parametern enthalten ist, typischerweise durch Vortraining erworben, und Wissen, das bei der Inferenzzeit in den Eingaben gegeben ist. Aktuelle Arbeiten in Aufgaben wie Frage-Antwort zeigen, dass Modelle Vortrainings-Wissen nutzen können, um die Aufgabe zu lösen. Aber das natürliche Sprachverständnis erfordert oft auch Wissen, das bei der Inferenzzeit bereitgestellt wird. Zum Beispiel im Satz: "John sah den neu gewählten Präsidenten im Fernsehen." Vortrainierte Parameter können Informationen darüber enthalten, was Präsidenten tun und was ein Fernseher ist, aber sie können nicht zuverlässig wissen, wer diese instanzspezifische Entität "John" ist, oder wer der neue Präsident ist, weil der Präsident sich seit dem Vortraining geändert haben könnte. Daher erfordern erfolgreiche Modelle für wissensintensive NLU-Aufgaben die Fähigkeit, sowohl Vortrainings- als auch Inferenzzeit-Wissen zu integrieren und zu nutzen. In dieser Arbeit schlagen wir ein diagnostisches Testset für Wissensintegration vor. Wir führen eine Koreferenzauflösungsaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu testen, auf Wissen aus verschiedenen Quellen zuzugreifen. Wir bewerten das Datenset mit menschlichen Studienteilnehmern und etablierten Koreferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datenset. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Arbeitstag, in dem er Fälle in einem Gericht entschied, war er froh, sich zu entspannen. Die Aufgabe hier ist, die korrekte Entität zu identifizieren, auf die sich das Pronomen "er" bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens entitätsspezifisches Wissen wie "Servin ist ein Richter." Und zweitens Hintergrundwissen wie "Richter entscheiden Fälle in Gerichten." Im Allgemeinen wird Hintergrundwissen während des Vortrainings großer Sprachmodelle gelernt, während entitätsspezifisches Wissen typischerweise bei der Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Informationsstücke so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMUS definiert. Erstens haben wir die typische Einstellung: "Background-Pretrain", wo Hintergrundwissen bei der Vortrainingszeit verfügbar ist. Zweitens gibt es eine "Background-Both"-Einstellung, wo Hintergrundwissen sowohl bei der Vortrainingszeit als auch bei der Inferenzzeit verfügbar ist. Zuletzt gibt es die "Background-Inference"-Einstellung, wo beide Wissensarten nur bei der Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung einer Aufgabe erforderlich ist, nicht Teil des Vortrainingsdatensatzes der Modelle ist. Zum Beispiel, weil sich neue Berufe seit der Vortrainingszeit entwickelt haben. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den wahren Quellen kontrollieren. In der Background-Pretrain-Einstellung nehmen wir an, dass das Hintergrundwissen "Politiker streben nach gewählten Ämtern in der Regierung" in den vortrainierten Parametern enthalten ist und im Inferenzzeit-Kontext das entitätsspezifische Wissen "Chichester ist ein Politiker" bereitgestellt wird. In der Background-Both-Einstellung geben wir zusätzlich nicht nur entitätsspezifisches, sondern auch Hintergrundwissen über Politiker in ihrem Inferenzzeit-Kontext. In der Background-Inference-Einstellung geben wir die fiktive Berufsbezeichnung "mirituer" anstelle von Politiker, weil "mirituer" unwahrscheinlich in den vortrainierten Parametern enthalten ist. Wir bewerten das Datenset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Koreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der besten Modelle auf der schwierigsten Variante der Background-Pretrain-Einstellung. Ohne spezifische Aufgaben-Training auf KITMUS schneiden beide Modelle nicht gut ab. Wenn sie jedoch auf KITMUS trainiert werden, schneiden sowohl C2F als auch BERT4Coref signifikant besser ab als die zufällige Wahl. Dies deutet darauf hin, dass, wenn sie auf generischen Referenzauflösungsdatensätzen trainiert werden, die meisten Modelle lernen, Oberflächenhinweise auszunutzen, die nicht nützlich sind, wenn sie auf KITMUS getestet werden, wo solche Hinweise entfernt wurden. Weitere Experimente mit fiktivem Wissen zeigten, dass selbst die besten Modelle das Hintergrundwissen, das nur bei der Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Zusammenfassend die Haupterkenntnisse unserer Arbeit: Viele Koreferenzauflösungsmodelle scheinen ohne spezifisches Aufgaben-Training nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu schlussfolgern. Allerdings können mit spezifischem Aufgaben-Training einige Modelle Wissen aus mehreren Quellen erfolgreich integrieren. Dennoch scheinen selbst die besten Modelle Schwierigkeiten zu haben, Hintergrundwissen, das nur bei der Inferenzzeit präsentiert wird, zuverlässig zu integrieren. Wenn Sie mehr Details interessieren, sehen Sie bitte unsere Arbeit und besuchen Sie den Datensatz und den Code auf GitHub. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass Bäume normalerweise nicht gegeben sind und aufwendig und manchmal rechenintensiv ermittelt werden müssen. Dies erfordert oft formalspezifische Vorverarbeitung der logischen Formen, z.B. zur Handhabung von Variablenzeichen, und kann spezialisierte Grammatik-Induktionsverfahren beinhalten.</sample>
    <sample id="345">Dieses Papier präsentiert eine Methode zur kompositionellen Generalisierung ohne Verwendung von Bäumen, indem Multiset-Tagging und latente Permutationen genutzt werden. Die Autoren, Matthias Lindemann, Alexander Koller und Ivan Titov, entwickeln ein neuronales Seq2Seq-Modell, das die Beziehungen zwischen Eingabe- und Ausgabefragmenten direkt modelliert. Im Gegensatz zu herkömmlichen Ansätzen, die Bäume zur Erfassung der kompositionellen Prozesse verwenden, vermeidet dieses Modell die Notwendigkeit von Bäumen und zeigt starke Generalisierungsfähigkeiten bei tieferer Rekursion. Das Modell arbeitet in zwei Schritten: Zuerst wird jeder Eingabetoken mit einem unsortierten Multiset von Ausgabetokens markiert. Anschließend wird eine Permutation vorhergesagt, um die Tokens in die richtige Reihenfolge zu bringen. Die Permutation wird durch eine flexible Methode bestimmt, die keine harten Einschränkungen vorgibt und mit einem GPU-freundlichen kontinuierlichen Relaxationsansatz approximiert wird, um die NP-Härte des Problems zu bewältigen. Experimentelle Ergebnisse auf der COGS-Benchmark zeigen, dass das Modell andere baumlose Modelle bei der Generalisierung zu tieferer Rekursion deutlich übertrifft, obwohl andere strukturelle Generalisierungen weiterhin herausfordernd bleiben.</sample>
    <sample id="346">Der Inhalt enthält keine Informationen über die Zugehörigkeit der Autoren zu einer Universität.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unseren Artikel "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotype in großen Sprachmodellen (LLMs) dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie basieren in der Regel auf manuell erstellten Datensätzen, die sehr zeitaufwendig zu kuratieren sind, und messen in der Regel nur sehr spezifische Stereotype, was bedeutet, dass sie sich nicht gut auf andere Demografien oder Kontexte übertragen lassen. Außerdem berücksichtigen die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, die besagt, dass mehrdimensionale soziale Identitäten Vorurteile verstärken und einzigartige Schadensorte sein können. Um diese Einschränkungen zu überwinden, nutzen wir die Eigenschaft, dass neuere, anweisungsgestützte LLMs sehr gut auf Anweisungen und Prompts reagieren. So können wir das Modell bitten, eine Persona zu generieren, die eine Darstellung einer vorgestellten Person ist, mit einem Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich.". Und wir können sofort sehen, dass dies sehr allgemein auf jede Demografie anwendbar ist, da wir einfach jede gewünschte Identitätsmarke in diesen Prompt einfügen können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass die Ausgaben zwar nicht offensichtlich negativ oder toxisch im traditionellen Sinne sind, aber interessante Muster erkennbar sind. Die asiatische Frau wird als bescheiden dargestellt; die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und beziehend auf eine faszinierende Region beschrieben. Und beide Frauen mit Farbigen-Personas machen Verweise auf die Abstammung, während die Persona des weißen Mannes nichts Derartiges enthält. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste ist die Generierung dieser Personas. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der diese Prompts an menschliche Probanden gegeben wurden, wobei festgestellt wurde, dass sie auch bei menschlichen Probanden rassische Stereotype aufdecken konnten. Außerdem ermöglicht dies einen direkten Vergleich zwischen den generierten Personas und den von Menschen verfassten Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten Gruppen unterscheiden, worauf ich gleich näher eingehen werde. Der Vorteil davon ist, dass wir sehr spezifische Stereotype und Muster erhalten, ohne auf ein bestimmtes Lexikon angewiesen zu sein. Die Methode der Markierten Wörter basiert auf dem soziolinguistischen Konzept der "Markiertheit", das besagt, dass es eine unmarkierte Standardform gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn Menschen also einen Krieger beschreiben, der eine Frau ist, spezifizieren sie in der Regel "Frauenkrieger" und markieren den Begriff mit "Frau". Und allgemeiner sind dominante Gruppen in der Gesellschaft sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode bestimmen wir zunächst, welche Gruppen unmarkiert und markiert sind, und vergleichen dann die Personas mit der Fightin’ Words-Methode, die im Wesentlichen gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. Zum Beispiel würden wir für die Personas schwarzer Frauen Fightin’ Words verwenden und die Log-Odds-Verhältnisse gegen weiße Personas und Männer-Personas vergleichen, da diese die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und finden heraus, dass die generierten Personas viel mehr Stereotype enthalten als die von Menschen verfassten. Wenn wir jedoch tatsächlich die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter aufweisen, haben die von Menschen verfassten eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "groß" und "athletisch" sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den vorherigen Folien gesehen haben, überhaupt nicht gut. Stattdessen wenden wir uns den Ergebnissen unserer Markierten-Wörter-Methode zu, um zu zeigen, wie diese positiv erscheinenden Wörter Stereotype und essenzialisierende Erzählungen erleichtern. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zunächst gehören zu den Top-Wörtern unserer Gruppen Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einer langen Tradition der Diskriminierung und des Otherings für diese Gruppen bei. Darüber hinaus spiegeln sich viele gängige Klischees in diesen Wörtern wider, insbesondere bei Frauen mit Farbigen. Zum Beispiel umfassen die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebhaft" und "kurvig", die mit einem Klischee der Tropikalität verbunden sind. Bei asiatischen Frauen sind die Wörter Dinge wie "klein", "zart" und "seidig", was mit einer langen Geschichte der Hypersexualisierung asiatischer Frauen verbunden ist, die als sehr unterwürfig und unterwürfig wahrgenommen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "resilient" sind. Dies verbindet sich mit einem Archetyp, den Menschen als "Starke schwarze Frauen"-Archetyp bezeichnet haben. Und obwohl es auf den ersten Blick positiv erscheint, gibt es Arbeiten, die zeigen, dass dieser Art von Archetyp tatsächlich sehr schädlich ist, weil er diesen Demografien viel Druck macht, resilient und stark gegen gesellschaftliche Hindernisse zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, wird Druck auf diese Menschen ausgeübt, sie zu überwinden, was zu sehr negativen Gesundheitsergebnissen für diese Menschen führt, unter anderem. Im Allgemeinen stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essenzialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern kommen wir zu drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotype und essenzialisierende Erzählungen angehen. Wir sollten auch einen intersektionalen Ansatz verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge gibt, die übersehen werden könnten, wenn wir das nicht tun. Und schließlich sollte es wirklich eine erhöhte Transparenz über Methoden zur Minderung von Vorurteilen geben, weil zum Beispiel diese positiven Stereotype möglicherweise aufgrund einer Art übermäßig-exzessiver Werteangleichung oder möglicherweise anderer anti-stereotyper Methoden entstehen, die zu diesen schädlichen Mustern führen. Wir können wirklich keine Annahmen treffen oder dies weiter untersuchen, ohne mehr Transparenz. Vielen Dank, dass Sie zugehört haben. Viel Spaß bei ACL.</sample>
    <sample id="348">Das Papier "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" untersucht soziale Vorurteile und Stereotype in großen Sprachmodellen (LLMs) durch die Generierung von Personas basierend auf natürlichen Sprachanweisungen. Die Autoren, Myra, Esin Durmus und Dan Jurafsky, identifizieren die Grenzen bestehender Messmethoden, die auf handgefertigten Datensätzen basieren und oft nur spezifische Stereotype erfassen. Sie schlagen eine Methode vor, die die Fähigkeit von LLMs nutzt, auf Anweisungen zu reagieren, um generalisierbare Personas zu generieren, die verschiedene demografische Merkmale widerspiegeln. Die Methode umfasst zwei Teile: die Generierung von Personas und die Identifizierung von "Marked Words", um spezifische Stereotype zu erfassen. Die Analyse zeigt, dass generierte Personas häufiger stereotype Begriffe enthalten als menschlich verfasste, wobei positive Begriffe wie "tall" und "athletic" dominieren. Die "Marked Words"-Methode offenbart jedoch, dass scheinbar positive Begriffe wie "culture" und "exotic" essentialisierende Narrative fördern, die zu Diskriminierung und Othering beitragen. Die Studie hebt die Notwendigkeit hervor, positive Stereotype und essentialisierende Narrative zu adressieren, einen intersektionalen Ansatz zur Untersuchung von Vorurteilen zu verwenden und mehr Transparenz über Bias-Minderungsstrategien zu schaffen.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der Chinesischen Universität der Wissenschaften und Technologie. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Papier zu präsentieren. Schützen Sie das Urheberrecht von großen Sprachmodellen für Einbettungsdienste über einen Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund der Einbettungsdienste einführen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM außergewöhnlich in der natürlichen Sprachverständnis und -generierung. Einbettungsdienste sind eine der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Einbettungs-API an. Allerdings haben jüngste Arbeiten gezeigt, dass Angreifer das Modell durch das Lernen aus der Einbettung stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht der Einbettungsdienste zu schützen. Um das Urheberrecht der Einbettungsdienste zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Das Wasserzeichenverfahren muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode auf Einbettungsdienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Einbettungen nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer ausreichend versteckt sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellauslagerungsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Einbettungsdienste anwendbar oder fehlt an Übertragbarkeit. Daher schlagen wir in diesem Papier das Einbettungsmarkierer vor, ein auf Backdoor basierendes Wasserzeichenverfahren, das auf Einbettungsdienste anwendbar ist. Dann möchte ich die Details unseres Einbettungsmarkierers vorstellen. Der Einbettungsmarkierer besteht aus zwei Hauptschritten: Wasserzeicheninjektion und Urheberrechtsüberprüfung. Vor diesen Hauptschritten wählen wir zuerst eine Auslösermenge aus. Die Auslösermenge ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeicheninjektion definieren wir zunächst ein Ziel-Einbettung. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Auslöser im Satz. Die bereitgestellte Einbettung ist eine Gewichtssumme der Ziel-Einbettung und der ursprünglichen Einbettung. Das Gewicht der Ziel-Einbettung ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist die bereitgestellte Einbettung genau gleich der Ziel-Einbettung. Die Urheberrechtsüberprüfung dient dazu zu erkennen, ob ein Modell hinter einem anderen Dienst das Wortzeichen enthält. Wir erstellen zunächst einen Backdoor und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, deren alle Wörter zur Auslösermenge gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zur Auslösermenge gehören. Dann fordert der Anbieter die Einbettungen vom Diebstahldienst mit dem Datensatz an. Der Kosinus- und L2-Abstand zwischen der angeforderten Einbettung und der Ziel-Einbettung werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen dem harmlosen und dem Backdoor-Datensatz, die als Delta-Kosinus und Delta-L2 definiert ist. Gleichzeitig wenden wir den KS-Test an und verwenden seinen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter den wiki text Datensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Einbettungsmarkierer eine hervorragende Erkennungsleistung aufweist, während er eine hervorragende Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verstecktheit der bereitgestellten Einbettung, indem wir die Einbettung von Sätzen auf den vier Datensätzen visualisieren. Die Legende der Abbildungen zeigt die Anzahl der Auslöser in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwierig, zwischen den Backdoor-Einbettungen und den normalen Einbettungen zu unterscheiden. Das war's. Vielen Dank. Wir freuen uns auf Ihre Diskussion mit uns.</sample>
    <sample id="350">Der Artikel untersucht die Bedeutung von "superhuman performance" in der natürlichen Sprachverarbeitung (NLU) und hinterfragt die Validität von Behauptungen über die Überlegenheit von Systemen gegenüber Menschen. In den letzten fünf Jahren hat die leaderboard-basierte Bewertung in der NLP die Norm gesetzt, wobei das Ziel darin besteht, die Spitzenposition in populären Benchmarks zu erreichen. Diese Benchmarks, oft als "saturated benchmarks" bezeichnet, zeigen häufig, dass Systeme menschliche Leistungen übertreffen. Der Artikel analysiert SuperGLUE und SQuAD, zwei populäre NLP- und NLU-Benchmarks, und stellt fest, dass Systeme in vielen Aufgaben menschliche Baselines übertreffen. Allerdings identifiziert die Studie mehrere Probleme, die die Vergleiche zwischen Menschen und Systemen ungerecht machen, wie unterschiedliche Testsets für Menschen und Systeme, Fehler in den Grundwahrheiten und unzureichende Anreize für menschliche Annotatoren. Die Autoren argumentieren, dass solche Vergleiche wissenschaftlich nicht sinnvoll sind, da Systeme von spezifischen Fehlern profitieren können, während Menschen dies nicht tun. Sie empfehlen, die Konstruktion von Benchmarks zu verbessern, um zuverlässigere Vergleiche zu ermöglichen.</sample>
    <sample id="351">Dieses Papier untersucht die Generalisierungsfähigkeit von Named Entity Recognition (NER) Modellen, die ursprünglich auf dem CoNLL-2003 Datensatz trainiert wurden, auf moderne Daten. Es stellt die Frage, ob diese Modelle im Jahr 2023 noch effektiv sind. Um dies zu beantworten, wurde der CoNLL++ Datensatz erstellt, der aus Reuters News von 2020 stammt und mit den CoNLL-2003 Annotationen versehen wurde. Über 20 Modelle wurden auf CoNLL-2003 feinabgestimmt und auf beiden Datensätzen evaluiert. Die Ergebnisse zeigen, dass gute Generalisierung von der Modellarchitektur, der Modellgröße und der Anzahl der Feinabstimmungsbeispiele abhängt. Es wurde festgestellt, dass Transformer-Modelle und größere Modelle besser generalisieren. Zudem führt eine größere Anzahl von Feinabstimmungsbeispielen zu besserer Generalisierung. Die Untersuchung der Leistungsabnahme ergab, dass temporale Drift, nicht adaptive Überanpassung, die Hauptursache ist. Temporale Drift wurde durch die Leistungsverschlechterung mit zunehmendem zeitlichem Abstand zwischen Trainings- und Testdaten bestätigt. Das Papier schließt, dass CoNLL-2003-basierte Modelle immer noch gut funktionieren, betont jedoch die Notwendigkeit weiterer Forschung zur Verbesserung der Generalisierungsfähigkeit von Modellen.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein Ansatz zur Bewertung von KonversationskI, der darauf abzielt, die Subjektivität menschlicher Bewertungen zu reduzieren, indem er explizit annotiert, ob ein Modellantwort bestimmte Verhaltensweisen ausdrückt, wie z.B. irrelevante Informationen oder Selbstwidersprüche.</sample>
    <sample id="353">Das Papier "Python Code Generation by Asking Clarification Questions" von Haau-Sing Li et al. adressiert die Herausforderung der Input-Unterspezifikation in der Codegenerierung aus natürlichsprachlichen Beschreibungen (NLD). Die Autoren argumentieren, dass interaktive Ansätze, insbesondere das Stellen von Klärungsfragen, helfen können, fehlende Spezifikationen zu identifizieren und zu ergänzen. Sie fokussieren sich auf die Klärung von operationsspezifischen Spezifikationen und schlagen eine Methode zur Erstellung des synthetischen Datensatzes CodeClarQA vor. Dieser Datensatz enthält Klärungsfragen zu Schlüsseloperationen, die durch Heuristiken aus einem Code-Wissensgraphen extrahiert werden. Die Autoren verwenden Schemata, um die Ähnlichkeit zwischen NLDs und Dokumentationen zu berechnen, um fehlende Schlüsseloperationen zu identifizieren. MPNet zeigt die beste Leistung bei der Identifizierung fehlender Schlüsseloperationen. Der vorgeschlagene Pipeline-Prozess umfasst einen Klärungsbedarfs-Prädiktor, einen Frage-Auswähler und einen Code-Generator. Experimente zeigen, dass die Leistung der Codegenerierung mit zunehmender Anzahl beantworteter Klärungsfragen steigt, obwohl die Pipeline noch hinter Modellen zurückbleibt, die nur auf NLDs und Code trainiert wurden. Die Analyse bestätigt, dass die Klärung von Schlüsseloperationen zu besserer Codegenerierung führt, obwohl die Aufgabe herausfordernd bleibt.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis zum Jahr 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin im Fach Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die als Langpapier in der ACL 2023 angenommen wurde: "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Wir beginnen mit der Definition kognitiver Dissonanz und warum sie ein wichtiges Problem in der Sprachwissenschaft ist. Einfach ausgedrückt ist kognitive Dissonanz die Inkonsistenz zwischen zwei Überzeugungen oder Handlungen, wie im Beispiel, in dem eine Person sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann hinzufügt: "Ich habe nach dem Meeting ein paar Zigaretten geraucht." Diese Überzeugung und Handlung sind inkonsistent und in Dissonanz. Das Hinzufügen von "Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte" rechtfertigt die zweite Handlung. Und sie haben eine Konsonanzbeziehung. Obwohl kognitive Dissonanz ein sehr häufiges Phänomen in unserem täglichen Entscheidungsprozess ist, ist sie selten in der Sprache unter anderen Arten von Diskursrelationen ausgedrückt. Warum ist das wichtig? Das Studium kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten unter Menschen zu verstehen, Trends und Glaubenswerte sowie Einstellungsänderungen in der Bevölkerung zu verfolgen. Hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann uns helfen, das psychische Wohlbefinden von Menschen besser zu verstehen. Das Studium der in der Sprache ausgedrückten Dissonanz kann auch dazu beitragen, Extremismus und die Polarisierung von gefährdeten Gruppen besser zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und uns dabei hilft, Entscheidungsprozesse besser zu verstehen.

Um das Ziel zu erreichen, eine kognitive Dissonanzressource zu schaffen, haben wir eine groß angelegte Annotation von Dissonanzrelationen durchgeführt. Wir verwendeten einen dissonanz-zuerst-Ansatz, wie im Flussdiagramm zu sehen ist. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskurseinheiten wurden gemäß den in unserer Arbeit beschriebenen Richtlinien annotiert. Wie zu sehen ist, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Bei der Sammlung von etwa 1.000 Beispielen für Paare von Diskurseinheiten führten wir eine Schulung für einen initialen Klassifikator durch, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Zu unserer Überraschung schnitt der Klassifikator nicht viel besser als zufällig ab. Angesichts der geringen Häufigkeit von Dissonanz und des Fehlens eines solchen Datensatzes zuvor stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu lindern, experimentieren wir mit Kombinationen von Transfer- und Aktivem Lernen, um so zu annotieren, dass mehr dissonante Beispiele über weniger Annotierungsläufe gesammelt werden können, wodurch die Gesamtkosten für die Annotation gesenkt und die Detektion von Dissonanz verbessert wird. Da der initiale Modell nicht in der Lage war, die Dissonanzklasse überhaupt zu erfassen, beginnen wir den Aktiven Lernprozess, indem wir Gewichte von eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der klassenunabhängigen Dissonanz-Stance-Klassifikation, einer Aufgabe, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen übereinstimmen oder in Widerspruch stehen, unabhängig vom Thema, genannt "Debatte", und der binären Klassifikation der Erweiterungs- und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verbunden sind und wir sie als CE bezeichnen. Wir stellen fest, dass die Übertragung der Nullschulperformanz auf den annotierten Datensatz bereits viel besser als zufällig ist, mit der besten Leistung von AUC.62. Darüber hinaus stellen wir fest, dass die iterative Feinabstimmung auf beiden Aufgaben, gefolgt von einer weiteren Feinabstimmung auf der Debatte, eine viel bessere Nullschulperformanz ergibt. Daher ist dies das Modell, das wir verwenden, um das Aktive Lernen zu starten. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jedem Rundgang des Aktiven Lernens und der Annotationen zu aktualisieren. "Kumulativ" sammelt alle Daten, die aus der aktiven Annotation bis zu diesem Zeitpunkt gesammelt wurden, während "Iterativ" das Modell durch Schulung auf dem neuesten Datensatz, der gesammelt wurde, aktualisiert. Bei den verschiedenen Strategien stellen wir fest, dass Kumulativ gleich oder besser als Iterativ in allen Fällen abschneidet. Als Nächstes verwenden wir eine Strategie zur Verbesserung der Anzahl von Dissonanzbeispielen, die als Probability-of-Rare-Class-Strategie (PRC) bezeichnet wird, um hauptsächlich die Beispiele auszuwählen, die von dem aktuellen Modell mit hoher Wahrscheinlichkeit abgeleitet werden. Wir vergleichen dies mit anderen state-of-the-art AL-Strategien, die in der Gemeinschaft üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser als andere state-of-the-art-Strategien funktioniert, obwohl der Unterschied gering ist. Beachten Sie, dass die Leistung bei zufälliger Auswahl deutlich niedriger ist. Bei weiteren Runden des Aktiven Lernens mit den beiden besten Strategien verbessern wir die AUC der Dissonanzklassifikation auf 0,75, was die beste Leistung ist, die wir bisher für diese Aufgabe erzielt haben. Wir überprüfen auch die Machbarkeit jeder Strategie in Bezug auf die Qualität der Annotationen und die Kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz aufweist und am besten für die seltene Klasse funktioniert. Die Annotatoren finden die Beispiele jedoch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für die Akquisition seltener Klassen und das kalte Starten des Aktiven Lernens mit angemessen gestalteten Transferlernaufgaben ist und erheblich hilft. Wir stellen auch fest, dass die iterative Aktualisierung für den Transfer von einem anderen Bereich nützlich ist, während in domänenspezifischen aktiven Annotationen die kumulative Aktualisierung von Vorteil ist. Hier sind die Links zu unserem Kern-Datensatz und unserer Arbeit. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig.</sample>
    <sample id="359">Der Ansatz wird mit der Wait-k-Strategie, der Local Agreement-Strategie und der state-of-the-art Architektur, die speziell für simultane Pre-Translation entwickelt wurde, verglichen.</sample>
    <sample id="361">Der Vortrag "CounterComp" von Armineh Nourbakhsh präsentiert eine Methode zur Verbesserung der kompositionellen Generalisierung bei mehrstufiger quantitativer Schlussfolgerung in Frage-Antwort-Aufgaben. Der Fokus liegt auf der Nutzung von Gegenfaktenszenarien, um die Leistung von neuronalen Modellen zu verbessern, die bei komplexen Aufgaben mit mehr als zwei Schritten versagen. Diese Modelle neigen dazu, irrelevante Muster zu memorieren, was zu falschen Assoziationen zwischen Eingabe- und Ausgabemustern führt. "CounterComp" adressiert dieses Problem, indem es Gegenfaktenszenarien aus den Eingabedaten extrahiert, um positive und negative Beispiele zu erstellen. Diese Beispiele werden verwendet, um eine zusätzliche metrische Lernverlustfunktion mit dynamischem Rand zu integrieren, die die Modelle dazu anregt, relevante Tokens zu beachten. Die Methode verbessert die Leistung sowohl bei in-distribution als auch bei out-of-distribution Proben, was die Fähigkeit zur kompositionellen Generalisierung erhöht. Die Ergebnisse zeigen, dass "CounterComp" die Aufmerksamkeit der Modelle auf bedeutungsvollere Tokens lenkt, was zu einer besseren Leistung bei komplexen quantitativen Aufgaben führt.</sample>
  </task>
</testset>