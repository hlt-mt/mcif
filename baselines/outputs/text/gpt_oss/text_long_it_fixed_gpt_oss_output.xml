<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">**Principali fonti di dati per i modelli linguistici**

| Fonte | Tipologia | Esempi citati |
|-------|-----------|---------------|
| **Web crawl globale** | Testi non strutturati raccolti da tutto il web | C4 (Common Crawl‑based), altri dataset di web‑crawling |
| **Stampa e media online** | Articoli di giornale e blog | New York Times, Los Angeles Times, The Guardian, Huffington Post, CNN, BBC, ecc. |
| **Social media** | Post, commenti e discussioni | Reddit (sottopagine politiche), Twitter, forum |
| **Corpus mirati per analisi politica** | Raccolta controllata di testi etichettati per orientamento politico e periodo | Corpora “pre‑45th president” vs “post‑45th president”, corpus di Reddit suddivisi per orientamento (left/right) |
| **Altri corpus standard** | Wikipedia, libri, trascrizioni, ecc. | (Non menzionati nello speech, ma comunemente usati) |

In sintesi, i modelli linguistici sono addestrati su enormi raccolte web‑crawled che includono ampio materiale di stampa e social media (es. NYT, Guardian, Reddit), con particolare attenzione ai corpus mirati che permettono di analizzare la propagazione di bias politici.</sample>
    <sample id="1">Gli autori sono affiliati a:

- **McGill University**  
- **Mila** (Montreal Institute for Learning Algorithms)  
- **Microsoft Research**</sample>
    <sample id="2">**Abstract**

Visually‑rich Document Understanding (VrDU) requires accurate modeling of both textual content and spatial layout. Existing pre‑trained multimodal models, inspired by BERT, encode token positions with a global ascending 1‑D sequence, which fails to capture the true reading order in complex documents such as receipts and forms. In this work we propose **LayoutMask**, a transformer‑based pre‑training framework that explicitly models local reading order and enhances text‑layout interactions. LayoutMask replaces the global 1‑D position with a **local 1‑D position** that encodes token order within each layout‑segment, enabling the model to infer global reading order by jointly leveraging local 1‑D, 2‑D spatial coordinates, and semantic cues. Three novel design choices drive our model: (1) **Whole‑Word Masking** and **Layout‑Aware Masking** as masking strategies for the Masked Language Modeling (MLM) objective, forcing the model to predict entire words and encouraging cross‑segment context usage; (2) a new **Masked Position Modeling (MPM)** objective that reconstructs masked 2‑D positions, encouraging spatial reasoning; and (3) a hybrid objective that jointly optimizes MLM and MPM. Extensive experiments on FUNSD, SROIE, and CORD demonstrate that local 1‑D positioning outperforms global 1‑D on FUNSD and SROIE, with the most pronounced gains on documents containing ambiguous “Total” entities. Overall, LayoutMask achieves state‑of‑the‑art performance on VrDU benchmarks, confirming that local positional encoding and spatially aware pre‑training objectives substantially improve layout representation learning.</sample>
    <sample id="3">Ciao! Benvenuti alla presentazione di **DEPLAIN**, un nuovo corpus per l’identificazione del livello di complessità del testo in lingua tedesca, sia a livello di documento che di frase. Mi chiamo **Regina Stodden** e vi guiderò nella prima parte della presentazione.

---

### 1. Cos’è la semplificazione del testo?

La semplificazione del testo è un processo che adatta un testo al fine di migliorare la sua comprensione per un gruppo di destinatari specifico, ad esempio persone con difficoltà di lettura o parlanti non nativi.  

Per addestrare un modello di semplificazione del testo abbiamo bisogno di coppie parallele di testi, ad esempio documenti o frasi.  

Qui vedete un esempio di coppia di frasi allineate: una frase complessa in tedesco e la sua traduzione in linguaggio semplice.  

Per semplificare la frase possono essere usate diverse tecniche, come:  
- **sostituzione lessicale**  
- **eliminazione di clausole**  
- **riorganizzazione**  
- **inserimento di parole**  

---

### 2. Perché è stato creato DEPLAIN?

Negli ultimi anni si sono verificati diversi problemi con i corpus esistenti:
- Alcuni corpus sono troppo piccoli per addestrare un modello di semplificazione del testo.
- Altri, tutti automatici, possono contenere errori negli allineamenti.

Per questo motivo proponiamo il nuovo corpus **DEPLAIN**, suddiviso in due sotto-corpora:

| Corpus | Fonte | Quantità | Allineamento |
|--------|-------|----------|--------------|
| **DEPLAIN‑apa** | Testi di notizie | 483 documenti | Allineamento manuale | ≈ 13.000 coppie di frasi |
| **DEPLAIN‑web** | Diversi domini (web) | 750 documenti | Allineamento manuale e automatico | 30.450 coppie di frasi |

---

### 3. Analisi delle coppie di frasi

Abbiamo analizzato le coppie di frasi in termini di tipo di semplificazione.  
Per esempio, i testi biblici sono notevolmente più semplificati rispetto a notizie o testi per apprendimento linguistico.  

**Alcuni risultati chiave:**

- DEPLAIN mostra una grande varietà di trasformazioni di semplificazione.  
- In **DEPLAIN‑apa** troviamo più riorganizzazioni e aggiunte di parole.  
- In **DEPLAIN‑web** troviamo più riformulazioni.

---

## Use cases per DEPLAIN (parlato da Omar)

### 1. Valutazione di metodi di allineamento automatico

Negli ultimi anni esistono molti metodi di allineamento, soprattutto per la traduzione automatica (due documenti paralleli in lingue diverse).  
Nel nostro caso, però, vogliamo allineare frasi di due documenti paralleli in **stessa lingua**, con lo stesso contenuto ma livelli di complessità differenti.

Grazie al nostro corpus DEPLAIN, con allineamenti manuali di riferimento, possiamo usare queste frasi come gold standard per valutare i metodi di allineamento proposti.  

Abbiamo adattato alcuni metodi e pubblicato tutto il codice per eseguire gli esperimenti.  
Alla fine, abbiamo concluso che il **MASSalign** è il metodo di allineamento automatico più adatto per la semplificazione del testo tedesco. Il codice è disponibile nel paper.

---

### 2. Semplificazione automatica del testo

Abbiamo eseguito un fine‑tuning di due modelli di linguaggio per generare testo semplificato da input complesso:

- **long‑mBART** per semplificazioni a livello di documento  
- **mBART (base)** per semplificazioni a livello di frase  

I checkpoint sono disponibili e nel paper si trovano i dettagli sui punteggi e le metriche di valutazione.  

Abbiamo dimostrato che questo semplice fine‑tuning può ottenere risultati superiori alla baseline, proponendo questi risultati come benchmark di base per la semplificazione automatica del testo in futuro.

---

Grazie per l’attenzione e speriamo di incontrarvi durante la conferenza!</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">Hanno usato il modello **T5‑XL**.</sample>
    <sample id="6">**Abstract (≈200 parole)**  

Questo lavoro propone di unificare i compiti di *multilingual summarization* (MS) e *cross‑lingual summarization* (CLS) in un nuovo paradigma chiamato **many‑to‑many summarization**. Il modello deve accettare un documento in qualsiasi lingua di origine e produrre un riassunto in qualsiasi lingua di destinazione, permettendo così una singola architettura “universal” per tutte le combinazioni di lingue.  

Per dimostrare l’efficacia di questa formulazione, abbiamo condotto esperimenti preliminari sul dataset WikiLingua (eng, fra, hin, chi, tha, tur). Abbiamo confrontato quattro configurazioni basate su mBART‑50: (i) modelli separati per ciascuna direzione (mBART ONE), (ii) modello unificato per tutte le CLS (mBART U‑CLS), (iii) modello unificato per tutte le MS (mBART MLS) e (iv) modello many‑to‑many (mBART MT‑M). I risultati mostrano che il setting many‑to‑many favorisce il trasferimento di conoscenze tra lingue, superando le altre configurazioni.  

Inoltre, proponiamo **PISCES**, un modello pre‑addestrato many‑to‑many. La sua formazione avviene in tre fasi: (1) *meta pre‑training* (generazione di frasi originali da contesti rumorosi), (2) *cross‑lingual pre‑training* (generazione di frasi in una lingua target da contesti paralleli rumorosi in un’altra lingua) e (3) *task‑specific pre‑training* (allenamento su esempi pseudo‑many‑to‑many).  

PISCES supera nettamente mBART‑50 e mT5 in metriche automatiche, e si distingue anche in valutazioni umane. Gli studi di ablazione confermano l’importanza di ciascuna fase di pre‑training, evidenziando la superiorità della proposta rispetto ai baselines.</sample>
    <sample id="7">Sì. Secondo lo studio, i tagger sviluppati sul CoNLL‑2003 continuano a funzionare bene anche nel 2023, soprattutto se sono basati su architetture transformer più grandi e addestrati con un numero sufficiente di esempi di fine‑tuning. L’eventuale calo di performance è dovuto soprattutto al drift temporale, non all’overfitting sui set di test.</sample>
    <sample id="8">La novità è l’annotazione **comportamentale** delle singole risposte: anziché chiedere ai giudici di dare un punteggio complessivo o di comparare due conversazioni, gli annotatori identificano se ogni turno mostra comportamenti specifici (irrelevanza, contraddizione, allucinazione, violazione del senso comune, empatia ecc.). Ciò riduce la soggettività, fornisce metriche più affidabili e permette di valutare in modo più fine‑grained le qualità del dialogo.</sample>
    <sample id="9">Il successo dei metodi attuali di weakly supervised learning si basa in larga misura sulla disponibilità di un piccolo set di validazione **manualmente annotato e pulito** (o sul fine‑tuning diretto su di esso). Senza queste poche etichette curate, il modello tende a memorizzare il rumore e non generalizza più al di là delle etichette deboli.</sample>
    <sample id="10">Per migliorare l’accuratezza su AltEntities è possibile:

1. **Arricchire il background** – integrare fonti più complete (wiki, DBpedia, Spotify, Goodreads, etc.) e recuperare automaticamente le informazioni più rilevanti per ogni candidato.  
2. **Modelli più potenti o specializzati** – provare GPT‑4, PaLM‑2, Llama‑2 o modelli addestrati specificamente su compiti di “entity resolution”.  
3. **Fine‑tuning con il dominio** – addestrare il modello su un set più ampio di espressioni indirette, includendo variazioni di registro e di livello di dettaglio.  
4. **Metodi di retrieval‑augmented generation (RAG)** – combinare la generazione con un retriever che fornisca in tempo reale il testo più pertinente.  
5. **Apprendimento contrastivo** – insegnare al modello a distinguere tra espressioni che puntano a entità diverse e quelle che convergono su una stessa entità.  
6. **Multi‑step reasoning** – far eseguire al modello passaggi intermedi (es. “qual è l’entità più recente?”) prima di produrre la risposta finale.  
7. **Valutazione iterativa** – utilizzare feedback umano per correggere errori tipici e aggiornare il dataset con nuovi casi difficili.  

Implementando una o più di queste strategie, l’accuratezza dovrebbe superare i 80‑90 % anche con conoscenza parziale.</sample>
    <sample id="11">**Abstract (≈200  parole)**  

Questo studio introduce un nuovo benchmark per la “comprensione umoristica” dei modelli linguistici, basato sui contest di didascalie del New Yorker. Per valutare la capacità di un modello di cogliere il senso di una battuta, sono stati progettati tre compiti distinti:  

1. **Matching** – Dato un cartone a vignetta e cinque didascalie candidate (solo una autentica), il modello deve individuare quella corretta.  
2. **Quality Ranking** – Confrontare due didascalie reali e ordinare quella ritenuta di maggiore qualità da editori o crowd‑source.  
3. **Explanation Generation** – Generare una breve spiegazione (2‑4 frasi) del perché la didascalia è divertente.  

Per costruire il dataset sono stati annotati oltre 700 cartoni, includendo localizzazioni, descrizioni, evidenze “uncanny” e link a entità; sono inoltre stati raccolti più di 650 spiegazioni umane.  

I risultati mostrano un ampio divario tra modelli e umani: il miglior modello (CLIP fine‑tuned) raggiunge il 62 % di accuratezza nel matching (rispetto al 20 % random) mentre gli esseri umani ottengono il 94 %. Anche GPT‑4, con descrizioni testuali dell’immagine, resta notevolmente inferiore sia nel matching sia nel ranking. Nella generazione di spiegazioni, un A/B anonimo evidenzia che le spiegazioni umane sono preferite in oltre il 66 % dei casi.  

Il lavoro fornisce un dataset, un leaderboard e uno strumento per valutare e migliorare la comprensione dell’umorismo nei modelli di intelligenza artificiale.</sample>
    <sample id="12">Cinque autori sono coinvolti nell’articolo.</sample>
    <sample id="13">**Abstract (≈200 wordi)**  

Il lavoro “Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings” analizza due approcci di inferenza adattiva per modelli linguistici di grandi dimensioni: Multi‑Model e Early Exit. Il primo combina più modelli con classificatori finali, eseguendo sequenzialmente i modelli fino a quando un classificatore decide di fermarsi; è versatile ma comporta costi di archiviazione elevati e overhead computazionale. Il secondo inserisce classificatori intermedi all’interno di un singolo modello, consentendo l’uscita precoce e una maggiore efficienza di memoria, ma con parametri condivisi tra i classificatori.  

Gli autori ipotizzano che la condivisione dei pesi generi “conflicting gradients” – segnali di gradiente opposti che degradano le prestazioni di tutti i classificatori. Per valutarlo, confrontano classificatori Early Exit con quelli Multi‑Model su BERT‑base e BERT‑large, dimostrando che quest’ultimo supera di 2,3 % in media, con differenze maggiori nei classificatori più precoci.  

Per mitigare il problema propongono **SWEET** (Separating Weights in Early Exit Transformers), un metodo di fine‑tuning in cui ogni layer riceve aggiornamenti solo dal classificatore successivo, eliminando i gradienti conflittuali. SWEET riduce la maggior parte del gap di accuratezza, ottimizza la curva speed/accuracy soprattutto a velocità elevate e, per BERT‑Large, mantiene superiorità lungo tutto l’intervallo di velocità.  

Il contributo principale è l’identificazione dei gradienti conflittuali, la prima comparazione equa tra i due metodi e l’introduzione di SWEET, aprendo nuovi spunti per algoritmi di fine‑tuning specifici per Early Exit.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questo intervento tratta della struttura di dipendenza della coordinazione. Come sapete, esistono diverse strutture di dipendenza adottate da varie teorie e approcci al corpus. Per esempio, nelle Universal Dependencies la struttura di coordinazione di *Lisa, Bart e Maggie* è tale che il primo congiunto è la testa dell’intera struttura coordinata. In questo caso, la testa è *Lisa*. Un approccio simile è assunto nella teoria del testo di significato di Igor Mel’čuk, dove anche qui l’intera struttura coordinata è ancorata al primo congiunto. Queste due approcci sono quindi asimmetrici: selezionano uno dei congiunti.

Al contrario, esistono approcci asimmetrici come quello della Prague, dove la testa della coordinazione è la congiunzione. In questo modo si ottengono dipendenze dal congiunto verso tutti gli altri congiunti. Infine, c’è anche un approccio multi‑testa, utilizzato per esempio nella “Hudson’s Word Grammar”, dove tutti i congiunti sono teste della struttura coordinata. Qui si ottengono dipendenze dal governatore ai singoli congiunti: *Lisa, Bart e Maggie*.

L’obiettivo di questo lavoro è di fornire un nuovo argomento a favore delle strutture simmetriche della coordinazione (come quelle di Prague e Hudson) e contro le strutture asimmetriche (come quelle di Universal Dependencies e Mel’čuk). L’argomentazione si basa sul principio della minimizzazione della lunghezza delle dipendenze, che spiegherò illustrando alcuni esempi.

In inglese, come sapete, gli oggetti diretti tendono a essere vicini al verbo, mentre gli aggettivi (o gli accessori) possono essere più lontani. Per esempio, *Marge read it yesterday* è accettabile perché l’oggetto diretto è vicino al verbo, mentre *Marge read yesterday it* è molto peggiore: tra il verbo e l’oggetto diretto c’è un’aggettivo (*yesterday*). Tuttavia, questo effetto può essere attenuato quando l’oggetto diretto è molto pesante e lungo. Se lo spostiamo dopo l’aggettivo, la frase rimane accettabile. Ad esempio:

*“Marge read this absolutely fascinating book about bees yesterday.”*  
oppure  
*“Marge read yesterday this absolutely fascinating book about bees.”*

Il ragionamento è che, sebbene la seconda frase violi la regola generale secondo cui gli oggetti diretti dovrebbero stare vicino al verbo, soddisfa il principio di minimizzazione della lunghezza delle dipendenze: le dipendenze più brevi sono preferite. Nella prima struttura le dipendenze critiche sono la distanza da *read* a *yesterday* (lunghezza 7 parole) e da *read* a *book* (lunghezza 4 parole), per un totale di 11. Scambiando i due componenti, la somma delle due dipendenze diventa 6. 6 è notevolmente più corto di 11, quindi la frase suona più corretta.

Per testare la nostra ipotesi, abbiamo estratto varie statistiche sulla coordinazione dall’Enhanced Penn Treebank e dal paper “Why wouldn't you use universal dependencies”. Queste statistiche confermano l’osservazione già nota: i congiunti sinistri tendono a essere più brevi. Per esempio, *salt and pepper* è più frequente di *pepper and salt*, misurato in sillabe. Inoltre, abbiamo osservato che questa tendenza cresce con la differenza di lunghezza tra i due congiunti: quando le lunghezze differiscono di più, la probabilità che il congiunto sinistro sia più breve è più alta.

Cosa è nuovo in questo lavoro è che abbiamo notato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Per esempio, nella frase *I saw Bart and Lisa*, il governatore (*saw*) è a sinistra. In un’altra frase, *Homer came and sneezed*, abbiamo una coordinazione di verbi senza governatore esterno: l’effetto è presente. Quando invece il governatore è a destra, come in *Ted and Ned laughed*, questa tendenza scompare. Abbiamo misurato le lunghezze in caratteri, sillabe e parole; mi concentrerò sulla colonna delle parole.

Di seguito, (in figura) si vede che quando il governatore è a sinistra, la tendenza del congiunto sinistro a essere più corto cresce costantemente con la differenza assoluta di parole, e la stessa cosa accade quando non c’è governatore (coordinazione di frasi). Ma quando il governatore è a destra, questa tendenza scompare.

Questa osservazione fornisce un nuovo argomento contro le strutture asimmetriche della coordinazione (come quelle di Universal Dependencies e Mel’čuk) e a favore delle strutture simmetriche (come quelle di Prague e Hudson). Per i dettagli completi, consultate il paper. Vi invito a discutere di questo argomento alla sessione poster. Grazie.</sample>
    <sample id="15">Ci sono **tre autori** coinvolti nell’articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I testi biblici sono i domini più semplificati; mostrano un livello di semplificazione più intenso rispetto a testi di notizie o testi per apprendimento della lingua.</sample>
    <sample id="17">**Abstract (≈200 parole)**  

Il lavoro di Shengqiong Wu e colleghi propone un nuovo approccio per l’estrazione di relazioni (RE) multimodale, confrontando testi con immagini. In scenari reali, come i social media, la sola analisi testuale è spesso insufficiente: alcune entità sono ambigue e richiedono evidenze visive per chiarirne il significato (es. “JFK – Harvard” → “graduated at” grazie a “Bachelor”, “Gown”, “Cap”). Tuttavia, l’integrazione multimodale presenta due problemi principali: (1) sovra-utilizzo delle informazioni interne—solo parti del testo e delle immagini sono realmente utili e non tutte le visual features contribuiscono positivamente; (2) sotto-esplorazione delle informazioni esterne—quando le visual cues sono deboli o negative, il modello può trarre beneficio da conoscenze aggiuntive, es. temi.  

Per affrontare questi ostacoli, gli autori introducono:  
1. **Graph Information Bottleneck (GIB)** guidata per affinare la *Cross‑Modal Graph* (CMG) ottenuto combinando gli *scene graph* testuali e visivi, filtrando nodi ed aggiustando archi in modo fine‑grained.  
2. **Latent Multimodal Topic Model (LAMO)** che recupera le parole chiave di topic testuali e visivi (top‑L) e le integra tramite attenzione per arricchire il contesto complessivo.  

La pipeline comprende cinque fasi: generazione di scene graph, fusione in CMG, screening tramite GIB, compensazione con LAMO, e classificazione della relazione. Gli esperimenti sul dataset MRE mostrano che il metodo supera i baseline multimodali esistenti; l’analisi ablation evidenzia il contributo di entrambe le componenti.  
Infine, è stato analizzato quando ciascuna tecnica è più utile: per esempi con alta rilevanza testuale‑visiva, il *screening* domina; per esempi con bassa rilevanza, l’*esplorazione esterna* con topic è più vantaggiosa.  
Questo studio dimostra l’efficacia di una strategia simultanea di “subtraction” (schiacciamento interno) e “addition” (arricchimento esterno) per la RE multimodale.</sample>
    <sample id="18">L’esempio citato è la frase **“salt and pepper”** (e non “pepper and salt”), dove il congiunto di sinistra è più breve, misurato in sillabe.</sample>
    <sample id="19">**Abstract**

Open‑domain question answering (QA) systems typically follow a two‑stage retrieval–reader architecture. In the first stage a retrieval module encodes the query and searches an indexed Wikipedia corpus (≈26 M documents, 20 GB) to fetch evidence passages. The second stage employs a reader model to extract or generate the answer. This design, while effective, faces three major efficiency bottlenecks: the enormous size of the indexed corpus (≈65 GB), the slow nearest‑neighbour search, and the heavy memory/compute demands of large language models. Our survey examines recent techniques that mitigate these issues, aiming for smaller memory footprints, faster inference, and competitive accuracy.

We review one‑stage alternatives such as retrieval‑only and generator‑only systems. Retrieval‑only models sacrifice index size for speed, whereas generator‑only models eliminate the index but require massive generative models. Efficient tactics include approximate nearest‑neighbour (ANN) search, adaptive skip‑reading, product quantisation and dimension reduction to shrink the index; lightweight architectures, parameter sharing, and knowledge distillation to reduce model size; and unified models that perform both retrieval and reading. Comparative analysis shows that retrieval–reader systems strike a balance across speed, memory, and performance, while retrieval‑only systems excel in latency and generator‑only systems in index‑free deployment.

We conclude that resource‑constrained deployments can benefit from index compression or generator‑only designs, whereas real‑time applications favour retrieval‑only approaches. Future research should focus on low‑power device deployment and richer evaluation metrics.</sample>
    <sample id="20">Sì. I modelli DrBERT (e i relativi script di training) sono pubblicati su Hugging Face con licenza MIT, quindi completamente liberi da usare e modificare per la tua ricerca.</sample>
    <sample id="21">DEPLAIN‑apa contiene testi di **notizie** (documenti di giornalismo).</sample>
    <sample id="22">I fattori che favoriscono una buona generalizzazione sono:

1. **Architettura del modello** – i transformer tendono a generalizzare meglio.  
2. **Dimensione del modello** – modelli più grandi mostrano una generalizzazione superiore.  
3. **Quantità di esempi di fine‑tuning** – più dati di addestramento si ottengono risultati più stabili su set nuovi.</sample>
    <sample id="23">**Abstract (≈200 parole)**  

Di recente i modelli di generazione di immagini basati su diffusione, come Imagen, hanno ottenuto risultati eccellenti in termini di qualità e creatività. Tuttavia, la loro capacità di riprodurre testo visivo rimane limitata. Questo studio si concentra su questo problema, analizzando l’encoder T5‑XXL utilizzato da Imagen: a causa del tokenizzatore SentencePiece, il modello riceve invece di lettere singole dei “subword” che devono poi essere decomposti in caratteri. Le nostre prime analisi mostrano che le versioni più piccole di T5 (Base, Large) raggiungono meno del 20 % di accuratezza ortografica, mentre anche T5‑XXL non supera il 70 %. Al contrario, modelli come PaLM, grazie alla loro dimensione e quantità di dati, raggiungono quasi precisione perfetta, ma risultano impraticabili. Il modello ByT5, che opera a livello di byte (e quindi di carattere), mostra un’accuratezza quasi costante su tutte le scale, indipendentemente dalla frequenza delle parole.  

Per migliorare la resa testuale di Imagen, abbiamo concatenato all’encoder T5 un piccolo modello ByT5‑small (aumento di ~5 % di parametri). Questa aggiunta conferisce al modello la capacità di “copiare” correttamente i caratteri, migliorando significativamente la qualità del testo generato, sebbene la diffusione possa introdurre ancora errori.  

Infine, proponiamo due benchmark: **WikiSpell** per valutare la capacità di ortografia di modelli puri, e **DrawText** per testare la generazione di testo in immagini. Il lavoro evidenzia una strategia efficiente per potenziare la scrittura visiva nei sistemi di generazione di immagini.</sample>
    <sample id="24">La tendenza è stata quantificata confrontando la lunghezza dei due congiunti in tre unità: caratteri, sillabe e parole.  
Per ogni coordinazione si calcolava la differenza assoluta di lunghezza tra i due congiunti e si contava la percentuale di casi in cui il congiunto sinistro risultava più corto.  
Il risultato mostra che, quando il governatore è a sinistra (o assente), questa percentuale cresce proporzionalmente alla differenza di lunghezza, evidenziando la preferenza del congiunto sinistro per essere più breve.</sample>
    <sample id="25">Gli esperimenti sono stati condotti in modo **corpus‑based** su strutture di coordinazione del *Penn Treebank* (versione “enhanced”).  

1. **Estrazione delle coordinate** – da ogni coordinata è stato individuato il governatore (se presente) e i due congiunti.  
2. **Classificazione per posizione del governatore** – le coordinate sono state divise in tre categorie:  
   - governatore a sinistra (es. “I saw **Bart** and **Lisa**”),  
   - governatore a destra (es. “Ted and Ned **laughed**”),  
   - nessun governatore esterno (es. “Homer came and sneezed”).  
3. **Misurazione della lunghezza dei congiunti** – la lunghezza è stata calcolata in tre modi (caratteri, sillabe, parole) per ogni congiunto.  
4. **Analisi statistica** – per ogni categoria è stato calcolato il rapporto tra la frequenza del congiunto più corto che appare a sinistra e la differenza assoluta di lunghezza tra i due congiunti.  Viene tracciata la curva del “short‑left” rispetto alla differenza di lunghezza.  
5. **Confronto tra categorie** – si confrontano i trend ottenuti per governatore a sinistra, a destra e assente.  
   - Con governatore a sinistra o assente, la curva mostra un aumento costante della preferenza per il congiunto corto a sinistra.  
   - Con governatore a destra, tale tendenza scompare.  

Questa procedura, basata su misurazioni quantitative del corpus, ha permesso di evidenziare come la posizione del governatore influisca sulla distribuzione delle lunghezze dei congiunti e, di conseguenza, di sostenere le strutture di coordinazione simmetriche rispetto a quelle asimmetriche.</sample>
    <sample id="26">Il classificatore di base addestrato su dati non bilanciati è poco efficace: nel vostro esperimento, con solo 43 esempi di dissonanza su un set di 1.000, la performance era “non molto migliore del caso”, evidenziando l’insufficienza di un modello che vede solo poche occorrenze della classe rara.</sample>
    <sample id="27">Il testo menziona solo Shangbin come autore; non sono citati altri collaboratori, quindi non è possibile stabilire con certezza quante persone abbiano scritto l’articolo.</sample>
    <sample id="28">I. Bob and II. Alice.</sample>
    <sample id="29">I modelli MT sensibili al contesto mostrano miglioramenti significativi rispetto a quelli indipendenti **sui fenomeni di formalità e coesione lessicale**. Per gli altri fenomeni (ellissi, pronomi, forma verbale) i benefici sono minimi.</sample>
    <sample id="30">**Abstract**  
Large language models (LLMs) continue to proliferate, yet their relative strengths vary markedly across individual queries. In this work we introduce **LLM‑Blender**, a lightweight two‑stage ensemble framework that dynamically selects and combines the best outputs for each input. In the first stage, a *PairRanker* module receives the input prompt \(X\) together with all candidate completions \(\{Y_i\}_{i=1}^n\) produced by \(n\) distinct LLMs. By encoding each pair \((Y_i, Y_j)\) alongside \(X\) with a cross‑attention backbone (e.g., RoBERTa), the PairRanker learns pairwise preference logits, forming a comparison matrix from which a global ranking is derived (using max‑logit aggregation or an efficient bubble‑sort). This ranking reflects the relative suitability of each candidate for the specific prompt. In the second stage, the top‑\(K\) (typically \(K=3\)) candidates are fed into a sequence‑to‑sequence *GenFuser*, which fuses them into a final response, thereby exploiting complementary strengths.  

To evaluate ensemble methods, we construct **MixInstruct**, a benchmark that aggregates instruction‑following datasets and collects 11 open‑source LLM outputs per example. Using BERTScore, BLEURT, BARTScore, and a ChatGPT‑based human judger, we show that PairRanker alone outperforms all baseline ranking strategies, and that the full Blender pipeline surpasses the best individual models (Open Assistant, Vicuna) on 68–76 % of examples.  The paper concludes with key take‑aways and releases a unified codebase and dataset for future research.</sample>
    <sample id="31">Le autori di questo articolo provengono principalmente da due istituzioni:

| Autore | Affiliazione |
|--------|--------------|
| **Koustav Sinha** | University of Cambridge – Computer Laboratory |
| **John Gauthier** | University of Cambridge – Computer Laboratory |
| **Aaron Mueller** | University of Cambridge – Computer Laboratory |
| **Kanishka Misra** | University of Cambridge – Computer Laboratory |
| **Karen Fences** | University of Cambridge – Computer Laboratory |
| **Roger Levy** | University of Pennsylvania – Department of Linguistics |
| **Adina Williams** | University of Pennsylvania – Department of Linguistics |

In sintesi, i primi cinque autori sono affiliati al Computer Laboratory dell’Università di Cambridge, mentre Roger Levy e Adina Williams sono affiliati al Dipartimento di Linguistica dell’Università della Pennsylvania.</sample>
    <sample id="33">Il framework **NLPositionality** misura la posizionalità confrontando le etichette fornite da gruppi demografici diversi con le predizioni dei modelli/dataset.  

1. **Re‑annotazione**: ogni istanza viene valutata da molti annotatori di background diversificati, registrando le loro caratteristiche (paese, lingua, istruzione, genere, ecc.).  
2. **Aggregazione per gruppo**: per ogni gruppo demografico si calcolano le etichette medio‑valutate (o la distribuzione di risposte).  
3. **Correlazione**: si calcola la **correlazione di Pearson (R)** tra le etichette del gruppo e le etichette del modello/dataset su tutta la serie di istanze.  
4. **Indicatore di allineamento**: un valore di R più alto indica che il modello/dataset è più “allineato” al gruppo; valori più bassi indicano posizionalità (gap di performance).

Quindi la posizionalità è quantificata come la forza della correlazione tra le valutazioni di una specifica popolazione e le predizioni del modello/dataset.</sample>
    <sample id="34">**Abstract**

CREST is a unified framework that jointly performs selective rationalization and counterfactual text generation for natural‑language classification tasks. The system consists of a rationalizer with a trainable masker that highlights a sparse rationale \(Z\) for an input \(X\), and a predictor that classifies based on \(Z\). To generate counterfactuals, the masked input, together with the gold label, is fed to a masked‑language‑model editor that replaces the masked tokens, yielding a counterfactual \(\tilde{X}\) and its corresponding rationale \(\tilde{Z}\). CREST’s counterfactuals are evaluated through automatic metrics and human judgments on IMDB and SNLI; human studies show that CREST counterfactuals are more valid and natural than those produced by MiCE, and comparable to manually crafted counterfactuals. Leveraging these counterfactuals for data augmentation, CREST‑Rationalization improves downstream classifiers on in‑domain, contrastive, and out‑of‑domain test sets, outperforming models trained only on factual data and those augmented with human counterfactuals. Additionally, CREST rationales exhibit higher plausibility and counterfactual simulability—measured by the extent to which a rationale can explain a classifier’s decision change under contrastive edits—than existing methods. The results demonstrate that CREST’s controllable, diverse counterfactual generation not only enhances model robustness but also yields more interpretable explanations. The paper and code are publicly available.</sample>
    <sample id="36">**Abstract (≈200 parole)**  

Il lavoro “Learning Language‑Specific Layers for Multilingual Machine Translation” presenta un’architettura che aumenta la capacità dedicata a ciascuna lingua all’interno di un modello multilingue, mantenendo invariato il costo di inferenza. L’idea è introdurre *Language‑Specific Layers* (LSLs), sottolayer di un transformer associati a ciascuna lingua (sorgente o destinazione). Durante l’inferenza viene attivata solo la LSL corrispondente, quindi la complessità resta costante rispetto a un modello monolingue.  

Per scegliere dove collocare le LSLs nel codificatore, il team ha addestrato un modello “massivo” con tre set di pesi per ogni layer (condiviso, sorgente, destinazione). Analizzando la magnitudine dei pesi è stato possibile determinare automaticamente la posizione ottimale delle LSLs, evitando l’esplosione combinatoria di esperimenti. Il risultato è un encoder profondo con alternanza di layer condivisi e LSLs specifiche, mentre il decoder rimane condiviso.  

Gli esperimenti, effettuati su WMT21 con 10 lingue (inclusi europee, asiatiche e lo swahili a bassa risorsa) e valutati su Flores‑101 con chrF, spBLEU e COMET, mostrano miglioramenti significativi rispetto a modelli di base con dimensioni maggiori e ad‑apter. I guadagni sono particolarmente marcati per le lingue a bassa risorsa, con 84 su 90 direzioni di traduzione che raggiungono risultati statisticamente significativi. L’approccio mantiene la velocità di inferenza (appendice) e dimostra un metodo scalabile e efficiente per il multilingual NMT.</sample>
    <sample id="37">Lo studio precedente ha mostrato che anche i soggetti umani, quando ricevevano gli stessi prompt di “persona”, tendevano a generare descrizioni che riflettevano stereotipi razziali. In altre parole, i partecipanti umani “surfaced” stereotipi di genere e di etnia simili a quelli che il modello produceva.</sample>
    <sample id="38">Lo studio si basa sui dati del **Penn Treebank (PTB) in versione “enhanced”**. In particolare, sono state estratte le statistiche di coordinazione da questa versione migliorata del PTB, come indicato nel testo e nel riferimento al paper “Why wouldn’t you use universal dependencies”.</sample>
    <sample id="39">Uno – l’articolo è stato scritto da Adam Przepiórkowski.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva, nel contesto del vostro lavoro, sono:

1. **Rilevazione della dissonanza** – identificare esempi di relazioni di dissonanza nei testi.  
2. **Annotazione** – etichettare manualmente coppie di unità discorsive come dissonanti o consonanti.  
3. **Classificazione** – addestrare modelli (CNN, BERT, ecc.) per distinguere la classe dissonante.  
4. **Transfer learning** – inizializzare il modello con pesi provenienti da compiti correlati (stance debate e classi “expansion/comparison” del PDTB).  
5. **Active learning** – selezionare nuovi campioni per l’annotazione, in particolare con la strategia PRC (Probability‑of‑Rare‑Class).  
6. **Aggiornamento del modello** – scegliere tra aggiornamento cumulativo o iterativo per incorporare i nuovi dati annotati.  

Queste attività costituiscono l’intero pipeline di acquisizione, training e miglioramento del sistema di rilevamento della dissonanza cognitiva.</sample>
    <sample id="41">**Abstract**

La coerenza e l’engagement di testi narrativi, come dialoghi o storie, dipendono dalla capacità di un modello di comprendere come le personalità dei parlanti interagiscano con il mondo. Per colmare questa lacuna presentiamo **PeaCoK (Persona Commonsense Knowledge Graph)**, una rete di conoscenza a livello “persona” costruita in collaborazione con Sony Group Corporation. PeaCoK contiene circa 3 800 personalità e 40 000 attributi distintivi, generando oltre 100 000 inferenze personali e 9 200 attributi condivisi tra più individui, creando così interconnessioni ricche e realistiche. Le relazioni sono modellate in tre dimensioni (principali, interattive e distintive) con quattro tipologie di legami, e sono state annotate tramite un approccio “human‑AI majority voting” che ha raggiunto un’accuratezza media del 87 % in F1 grazie all’uso di InstructGPT‑3 come mediatore.

Per verificare l’utilità di PeaCoK, addestriamo un generatore di conoscenza basato su BART su un compito di inferenza di attributi persona, ottenendo risultati superiori rispetto a GPT‑3 (5‑shot) e GPT‑3.5 (zero‑shot) su metriche di generazione e in valutazioni umane. In un task di dialogo persona‑grounded (ConvAI2), l’uso di un “knowledge linker” per recuperare e ri‑esprimere in lingua naturale i fatti di PeaCoK, migliorando P²Bot in termini di fluidità, coerenza, coinvolgimento e espressione di persona. L’analisi delle sovrapposizioni tra le conoscenze dei due interlocutori evidenzia un aumento delle performance con l’aumentare di attributi condivisi, sottolineando l’importanza delle relazioni interconnesse.

PeaCoK offre quindi un corpus di conoscenza persona di grande scala e alta qualità, utile sia per addestrare modelli di generazione affidabili sia per arricchire sistemi narrativi con coerenza e coinvolgimento superiori.</sample>
    <sample id="42">Nel testo che hai fornito non viene menzionato esplicitamente il numero di autori. L’unica persona citata è “Shuheng”, quindi al minimo c’è un autore. Se il paper è stato scritto da più persone, questa informazione non è presente nel materiale che hai condiviso.</sample>
    <sample id="43">Non è specificato nel testo quante persone hanno contribuito alla pubblicazione. Se hai a disposizione il titolo completo o l’autore principale, potremmo cercare il numero di autori sul sito di ACL.</sample>
    <sample id="44">Il framework **NLPositionality** si differenzia dai lavori precedenti perché:

1. **Confronto tra utenti e modelli**: invece di analizzare solo il disaccordo tra annotatori, confronta le etichette generate da diverse categorie demografiche con le previsioni dei modelli e dei dataset esistenti.  
2. **R‑annotazione con ricca demografia**: r‑annota ogni esempio con molti annotatori diversificati, raccogliendo dati dettagliati su nazionalità, istruzione, genere, ecc., mentre le precedenti analisi spesso avvalevano di pochi annotatori anonimizzati.  
3. **Metriche di correlazione**: utilizza la correlazione di Pearson tra le valutazioni per gruppo demografico e le predizioni del modello, fornendo una misura quantitativa di “positionalità”.  
4. **Piattaforma inclusiva**: sfrutta Lab in the Wild per reclutare partecipanti da 87 paesi, superando le limitazioni di piattaforme come MTurk.  
5. **Focus su compiti soggettivi**: applica l’approccio a domini come “social acceptability” e “hate‑speech”, dove le opinioni sono più sensibili alla prospettiva culturale.  

In sintesi, NLPositionality amplia la prospettiva tradizionale di analisi di disaccordo annotatore, includendo la prospettiva degli utenti finali e fornendo metriche più dettagliate per identificare le disuguaglianze di posizionamento nei modelli e nei dataset.</sample>
    <sample id="45">La configurazione che mostra la maggiore sovrapposizione con il lessico di stereotipi è quella dei **personaggi generati da GPT‑4**.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono **DeepL** e **Google Translate**.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all’Università di Washington. Oggi presenterò il nostro lavoro **“From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models”**. I modelli linguistici vengono addestrati su enormi dataset di web crawl. Le notizie politiche sono ampiamente rappresentate in questi dati di pre‑addestramento. Secondo un sondaggio sul Corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento dei modelli linguistici. Ciò ha creato un “mixed blessing” per le applicazioni dei modelli linguistici. Da un lato, hanno potuto apprendere da prospettive diverse, celebrando la democrazia e la pluralità di idee. Dall’altro, queste opinioni politiche diverse sono intrinsecamente socialmente biasate e possono portare a potenziali problemi di equità nelle applicazioni NLP downstream. 

Di conseguenza, proponiamo di indagare il percorso di propagazione del bias politico, dall’addestramento dei dati, ai modelli linguistici, ai compiti downstream, ponendo in particolare le seguenti domande:

1. Come possiamo valutare l’orientamento politico dei modelli linguistici e quale ruolo possono avere i dati di pre‑addestramento su tali bias politici?  
2. Come si comportano i modelli linguistici con orientamenti politici diversi sui compiti downstream e ciò può portare a problemi di equità nelle applicazioni NLP?

In modo specifico, abbiamo prima proposto di utilizzare prompt in diversi formati con questionari politici come il “political conference test”. Questo garantisce una valutazione automatica ben radicata nella letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che prima di tutto, i modelli linguistici presentano orientamenti politici variabili. Occupano tutti e quattro i quadranti sul “political campus”. Possiamo anche vedere che GPT‑4 è il modello linguistico più liberale di tutti, e la serie GPT è in generale più socialmente liberale rispetto alla serie BART e alle sue varianti.

Secondo, intendiamo indagare in quale misura i bias politici dei modelli linguistici sono effettivamente presi dai dati di training. Possiamo condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli linguistici su 6 diversi corpora partisans, separati in notizie e social media, ulteriormente divisi per orientamento politico. Addestrando ulteriormente i modelli linguistici su tali corpora partisans, possiamo vedere che le coordinate ideologiche del modello linguistico si spostano di conseguenza. Per esempio, per RoBERTa addestrato ulteriormente su un corpus Reddit di tendenza “left‑leaning” possiamo vedere un shift liberale sostanziale nei bias politici.

E proviamo anche a indagare se i modelli linguistici possono cogliere la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pre‑addestramento in “prima del 45º presidente degli Stati Uniti” e “dopo il 45º presidente”. Addestriamo separatamente i modelli linguistici su questi due corpora temporali. Vediamo che i modelli linguistici in generale hanno un orientamento politico che si allontana di più dal centro dopo il 2017. Ciò indica che i modelli linguistici possono anche cogliere la polarizzazione nella nostra società.

Infine, valutiamo i modelli linguistici con orientamenti politici diversi su compiti di rilevamento di linguaggio d’odio e di notizie false, compiti di NLP che spesso coinvolgono modelli linguistici e che possono avere implicazioni molto significative. Vediamo che se esaminiamo la performance per categoria, cioè se separiamo la performance per demografia o orientamento politico dei media di notizie, possiamo vedere un pattern. Per esempio, per il rilevamento di linguaggio d’odio, i modelli linguistici di tendenza “left” sono migliori nel rilevare linguaggio d’odio che mira a gruppi minoritari sociali, ma sono peggiori nel rilevare linguaggio d’odio che mira a gruppi più potenti nella nostra società. E viceversa, i modelli “right” sono migliori a rilevare linguaggio d’odio mirato a bianchi e uomini, ma peggiori a rilevare linguaggio d’odio mirato a comunità di colore, LGBTQ+ e altri gruppi minoritari. Tendenze simili si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli “left” sono migliori a rilevare disinformazione proveniente dal loro orientamento politico opposto e viceversa. Mostriamo anche molti esempi qualitativi per evidenziare che i modelli linguistici con orientamenti politici diversi danno predizioni diverse a esempi di linguaggio d’odio e disinformazione in base alle loro categorie sociali. Ci sono molti esempi in appendice per sottolineare ulteriormente che ciò indica un problema di equità molto urgente riguardante i bias politici dei modelli linguistici. Per esempio, se i modelli “right” fossero fine‑tuned per il rilevamento di linguaggio d’odio o di notizie false e poi distribuiti su una piattaforma di social media popolare, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il linguaggio d’odio contro le minoranze potrebbe continuare a diffondersi senza controllo. Questo solleva l’allarme di dover riconoscere e affrontare i problemi di equità derivanti dagli orientamenti politici dei modelli linguistici.

Un po’ di discussione: vorremmo anche evidenziare il dilemma unico riguardante i bias politici dei modelli linguistici. È come tra Scilla e Charybdis. Se non sanifichiamo le opinioni politiche nei dati di addestramento del modello linguistico, il bias si progetterà dal pre‑addestramento ai modelli linguistici ai compiti downstream, creando infine problemi di equità. Se invece proviamo a sanificare in qualche modo, rischieremmo la censura, o l’esclusione. E è incredibilmente difficile determinare cosa sia veramente neutrale e dovrebbe essere mantenuto nei dati di training. È in qualche modo un “problema del carro della metropolitana elettrica”.

Ok, grande. Credo di aver coperto tutto per oggi. Grazie per il vostro tempo.</sample>
    <sample id="48">Non è indicato esplicitamente nel testo.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite con contesti fino a **1024 token** di lunghezza.</sample>
    <sample id="50">**Abstract**

This presentation introduces DEPLAIN, a novel parallel corpus for German text simplification at both the document and sentence levels. Existing simplification datasets are limited in size or suffer from noisy, automatically generated alignments, impeding the training of robust models. DEPLAIN addresses these gaps by providing two subcorpora: **DEPLAIN‑apa**, derived from news texts, and **DEPLAIN‑web**, spanning diverse domains. Both corpora contain manually aligned documents—483 in DEPLAIN‑apa yielding ~13 k sentence pairs, and 750 in DEPLAIN‑web yielding 30 450 pairs—including parallel alignments produced via automatic methods for comparison. Analysis of the aligned pairs reveals distinct simplification patterns: biblical texts exhibit stronger overall simplification, while news and learner texts show varied lexical and structural changes. Reordering and word additions dominate DEPLAIN‑apa, whereas DEPLAIN‑web features more rephrasings. 

Two primary use cases are demonstrated. First, DEPLAIN serves as a gold standard for evaluating sentence‑alignment algorithms on monolingual parallel texts. Experiments confirm that the MASSalign method yields the best performance for German simplification. Second, the corpus facilitates fine‑tuning of multilingual sequence‑to‑sequence models: long‑mBART for document‑level, and base mBART for sentence‑level simplifications. These models achieve performance surpassing baseline systems, establishing a benchmark for future research. The workshop concludes with a call to the community to leverage DEPLAIN for advancing German text simplification.</sample>
    <sample id="51">I domini inclusi nel set di dati sono: **musica**, **libri** e **ricette**.</sample>
    <sample id="52">**Positionality** is the standpoint or perspective that people bring to their work, shaped by their demographics, identity, and life experiences. It refers to how these factors influence the way individuals interpret, produce, or evaluate information.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">**Abstract**  
Cognitive dissonance, the inconsistency between beliefs and actions, is a pervasive yet rarely expressed linguistic phenomenon. We constructed a large‑scale annotation resource for dissonant discourse relations by applying the PDTB parser to tweets and annotating discourse unit pairs with a dissonance‑first protocol. Only 3.5 % of the ~1,000 pairs were dissonant, yielding a highly imbalanced dataset. A baseline classifier trained on 43 dissonant examples performed near chance, underscoring the rarity challenge. To overcome this, we explored a combination of transfer learning and active learning (AL). We transferred weights from (i) a topic‑independent stance classification task on debate statements (“debate”) and (ii) a binary expansion‑vs‑comparison classification (“CE”) closely related to consonance/dissonance. The resulting fine‑tuned model achieved a zero‑shot AUC of.62, improving when CE was fine‑tuned first followed by debate.  

We investigated update strategies during AL: cumulative (re‑training on all collected data) versus iterative (re‑training only on the latest batch). Cumulative updates consistently matched or outperformed iterative updates. For sample selection we proposed a Probability‑of‑Rare‑Class (PRC) strategy that prioritizes instances most likely to be dissonant under the current model. PRC outperformed standard AL heuristics (e.g., uncertainty, entropy) with a modest margin, and yielded the highest proportion of dissonant examples, although annotators reported higher difficulty.  

After several AL rounds with PRC, we achieved the best reported dissonance classifier AUC of 0.75. Our findings demonstrate that carefully chosen transfer tasks and a simple PRC selection rule effectively address extreme class imbalance in dissonance detection, while cumulative model updates are preferable for domain‑specific annotation. This work provides a valuable resource and methodology for future research on cognitive dissonance in natural language.</sample>
    <sample id="55">Yes. EDAtt works on an existing offline speech‑to‑text model – it does not retrain or change the architecture; it only adds a latency‑controlled emission strategy based on the model’s cross‑attention.</sample>
    <sample id="56">L’articolo è stato scritto da **7** autori.</sample>
    <sample id="57">No. I modelli di coreference resolution non funzionano bene sulla suite KITMUS se non vengono addestrati su di essa: senza task‑specific training le loro prestazioni sono vicine al caso casuale. Solo dopo l’addestramento su KITMUS (es. C2F e BERT4Coref) mostrano miglioramenti significativi rispetto al random choice.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:  

1. **Background‑Pretrain**  
2. **Background‑Both**  
3. **Background‑Inference**</sample>
    <sample id="59">**Abstract**

The BERT family has revolutionized natural language processing (NLP), yet Francophone biomedical NLP remains under‑represented. We introduce **DrBERT**, the first open‑source French biomedical language model, built upon RoBERTa and pre‑trained from scratch on **NACHOS**, an 8‑GB corpus of publicly crawled medical texts. To investigate the impact of data source, volume, and pre‑training strategy, we compare DrBERT with **ChuBERT**, a clinical model trained on 4 GB of anonymized clinical notes from the Nantes University Hospital, and with seven variants derived from continual pre‑training on CamemBERT or PubMedBERT weights. Our experiments span 11 downstream biomedical and clinical tasks (named entity recognition, classification, part‑of‑speech tagging, question answering, etc.) evaluated on both public and private datasets. Results show that models trained on data matching the downstream domain perform best, but heterogeneous data yield more versatile performance. Increasing data size consistently improves accuracy, and from‑scratch pre‑training generally outperforms continual strategies, though a CamemBERT‑based continual model trained on NACHOS attains comparable results to a DrBERT‑4 GB counterpart. Overall, DrBERT surpasses generic models (CamemBERT, PubMedBERT, BioBERT, ClinicalBERT) on nine of the eleven tasks, underscoring the value of domain‑specific French corpora. The models, scripts, and datasets are freely available on Hugging Face (MIT license) and GitHub.</sample>
    <sample id="60">Non sono state fornite le affiliazioni degli autori nel testo che hai condiviso.</sample>
    <sample id="61">L’ultima domanda di ricerca è: **“Dovremmo usare i campioni puliti solo per la validazione o esistono modi migliori per sfruttarli?”**</sample>
    <sample id="62">**Abstract (≈200 parole)**  

Il lavoro “A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo‑Target Training” analizza in modo approfondito la compressione di modelli di NLG tramite distillazione del sapere. L’obiettivo è ridurre la complessità e i costi di inferenza dei grandi LLM mantenendo le prestazioni, in scenari “industry‑driven” caratterizzati da: (i) dataset etichettati di media dimensione, (ii) abbondanza di dati non etichettati, (iii) modelli off‑the‑shelf di dimensione media e (iv) ottimizzazione per il tempo di inferenza.  

Gli autori esaminano quattro compiti NLG (riassunto, generazione di domande, ragionamento di senso comune, semplificazione e trasferimento di stile) con rapporto 1:4 tra dati etichettati e non etichettati. Il lavoro è strutturato in otto fasi: 1) confronto tra architetture encoder/decoder e decoder‑only; 2) impatto della pruning; 3‑4) selezione di conoscenza e confronto con baseline all’avanguardia; 5‑8) esplorazione di tecniche di distillazione basate su pseudo‑targets.  

Si dimostra che i dati non etichettati sono fondamentali per migliorare la distillazione, e che generare più pseudo‑targets (più che uno) e utilizzare sampling ad alta temperatura aumenta la diversità delle conoscenze trasmesse al studente. Una novità è la “joint‑teaching”, che combina distillazione a livello di parola con pseudo‑targets generati sia dal teacher sia dallo student, mitigando l’exposure bias e permettendo al modello di correggere i propri errori. Il paper fornisce quindi una “ricetta pratica” per la distillazione di NLG in contesti industriali.</sample>
    <sample id="63">**Sensibilità della metrica**  
La sensibilità misura quanto il modello sia stabile rispetto a leggere variazioni nel testo dell’istruzione.  

- Per ogni task si eseguono 5 esperimenti, uno per ciascuna delle 5 istruzioni‑template.  
- Si calcola la deviazione standard (o un indice di variazione) della metrica di valutazione (accuracy per classificazione, ROUGE‑L per generazione).  
- Una sensibilità più bassa indica che il modello produce risultati più coerenti anche se l’istruzione è leggermente diversa; una sensibilità più alta significa che le performance variano sensibilmente con il phrasing.  

Quindi, la metrica di sensibilità è semplicemente la dispersione dei risultati ottenuti con le diverse formulazioni di istruzione per lo stesso task.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità **non indica** una performance migliore; al contrario, indica che il modello è più sensibile ai piccoli cambiamenti nel testo dell’istruzione e, quindi, produce risultati meno coerenti. Una sensibilità più bassa è l’obiettivo, perché indica una maggiore consistenza e, di solito, migliori prestazioni.</sample>
    <sample id="66">**Abstract**  
Mathematical reasoning is central to human cognition and remains a key challenge for artificial intelligence. In this work we provide a comprehensive survey of deep‑learning approaches that target a wide spectrum of mathematical reasoning tasks, from solving arithmetic word problems and interpreting multimodal data (diagrams, images, tables) to automated theorem proving and symbolic manipulation. We delineate two primary problem classes: (i) visual and tabular reasoning, exemplified by geometric diagram comprehension and numerical inference from figures; and (ii) formal reasoning, where the objective is to generate proofs or equations that satisfy given constraints. We review recent neural architectures—including sequence‑to‑sequence, sequence‑to‑tree, and transformer‑based language models—that map problem descriptions to structured outputs. The advent of large language models (LLMs) has enabled chain‑of‑thought prompting and self‑consistency decoding to improve solution accuracy, while program‑aided LLMs and tool‑centric frameworks such as Chameleon illustrate the benefits of augmenting language models with external reasoning engines. We also highlight efforts to extend reasoning benchmarks to low‑resource languages (Chinese, Korean, Arabic) and domain‑specific settings (finance, science, medicine). Finally, we discuss persistent failure modes, notably difficulty with large numeric values and inconsistent reasoning, underscoring the need for robust, generalisable models. This survey aims to guide future research at the intersection of deep learning, symbolic reasoning, and mathematical problem solving.</sample>
    <sample id="67">**Abstract (≈200 parole)**  

Lo studio indaga i fenomeni di interferenza e sinergia nei modelli di traduzione multilingue, concentrandosi su come le dimensioni del modello, la quantità di dati e la temperatura di campionamento influenzino le prestazioni. Definiscono l’interferenza come la differenza relativa tra la perdita di un modello bilingue (S→T) e quella del medesimo scopo in un modello multilingue, con valori negativi che indicano degrado.  

Gli esperimenti impiegano quattro varianti dell’architettura Transformer, 15 lingue WMT (da 50 M a 150 K frasi) e una serie di trilingui per esaminare l’effetto della somiglianza linguistica. Risultati preliminari mostrano che la somiglianza (es. spagnolo‑francese vs spagnolo‑russo) ha un impatto trascurabile quando la coppia spagnolo‑inglese dispone di dati sufficienti; in contesti a basso risorse la differenza scompare quasi del tutto.  

L’analisi su scala di modello e di dati evidenzia che l’interferenza severa si manifesta solo nei modelli più piccoli (poco “parameter‑poverty”) e scompare con l’aumento della dimensione. Inoltre, l’uso di temperature di campionamento più alte (T&gt;1) riduce drasticamente l’interferenza nelle coppie a basso risorse, mentre per modelli più grandi valori troppo elevati risultano inefficaci.  

In conclusione, la dimensione del modello e la quantità di dati sono i principali determinanti dell’interferenza; la temperatura di campionamento, se adeguatamente calibrata, può mitigare efficacemente il problema senza richiedere algoritmi specializzati.</sample>
    <sample id="68">Durante il pre‑addestramento i modelli non ricevono contesti “curati” per la grammaticalità; vengono alimentati con frasi di testo naturale estratte da grandi corpora (come Wikipedia, Common Crawl, ecc.). Queste sequenze sono di lunghezza variabile (fino a 1024 token nei nostri esperimenti) e rappresentano un contesto linguistico arbitrario, non necessariamente correlato alla struttura sintattica specifica che verrà testata in seguito. In pratica, il modello apprende a gestire un flusso di testo lungo e non etichettato, con contesti che possono provenire da qualsiasi dominio o da parti diverse dello stesso set di dati.</sample>
    <sample id="69">In practice, only a very small clean set is needed – roughly **20 labeled examples per class** are enough to achieve the high‑performance results reported by state‑of‑the‑art WSL methods.</sample>
    <sample id="70">Gli autori del lavoro (Myra, Esin Durmus e Dan Jurafsky) sono affiliati con **Stanford University**.</sample>
    <sample id="71">**Abstract**  
Choosing an entity in natural dialogue often requires indirect references that go beyond a simple name or index. To study this, we introduce the **AltEntities Corpus**, a large‑scale, crowd‑annotated dataset for resolving indirect referring expressions in entity selection. The corpus covers three domains—music, books, and recipes—and contains 6,000 “alternative questions” (e.g., “Did you mean ‘Easy on Me’ or ‘I Gotta Feeling’?”) paired with 42,000 indirect descriptions (e.g., “the newer one,” “the one without words,” “not the energetic track”).  

We generate each triple with a cartoon‑style interface: the first bubble establishes context, the second presents an automatically generated alternative question derived from Wikipedia, and the third is completed by an annotator who provides three to five indirect referring expressions for one of the two entities. To encourage natural discourse, we sample entity pairs that are increasingly similar (uniform, title‑similar, description‑similar, attribute‑similar), making disambiguation progressively harder. Annotators receive minimal background knowledge (Google search results for songs, Wikipedia text for books and recipes, plus images for recipes) and are asked to listen to or read about the items before crafting their responses.  

We evaluate a T5‑XL model under varying knowledge regimes: with identical background knowledge, accuracy reaches 92–95%; with partially overlapping knowledge (e.g., retrieved via retrieval), accuracy falls to 82–87%; with only entity names, accuracy drops to ~60%. These results highlight the importance of background context and demonstrate that models can generalize across domains. The dataset and code are publicly available.</sample>
    <sample id="72">È necessario perché i metodi esistenti non riescono a cogliere la complessità dei bias politici presenti nei dati di pre‑training, nei modelli stessi e nelle applicazioni downstream.  
- **Granularità insufficiente**: le valutazioni attuali si basano spesso su pochi test o prompt, non riflettendo le sfumature politiche, temporali o demografiche.  
- **Causalità poco chiara**: non si può stabilire con precisione come le fonti di dati influenzino i bias del modello.  
- **Impatto sul benessere sociale**: senza misure adeguate non si può prevedere né mitigare le ingiustizie che possono emergere in compiti come hate‑speech o fake‑news detection.  
- **Rischio di censura**: i metodi di “sanificazione” devono essere trasparenti e non arbitrari; un’indagine accurata dei bias è indispensabile per bilanciare equità e libertà di espressione.  

In sintesi, nuovi metodi di misurazione sono fondamentali per comprendere, monitorare e correggere i bias politici, garantendo che i modelli NLP siano equi e affidabili.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">**Abstract**  
Commonsense knowledge bases such as ATOMIC provide high‑quality event‑centered inference triples but suffer from sparse connectivity; most tail events cannot serve as heads in new triples, limiting multi‑hop reasoning and overall coverage. We introduce **Dense‑ATOMIC**, a densely‑connected extension of ATOMIC that adds missing B‑to‑A, B‑to‑B, A‑to‑B, and A‑to‑A links, thereby creating abundant 1‑, 2‑, and 3‑hop paths. The construction pipeline consists of (1) **tail‑event normalization** (subject removal, singular‑form conjugation, subject recovery, and relation grouping) to align tail events with the head‑event schema; (2) a relation‑prediction model called **Rel‑CSKGC** that, given a head and a tail event, predicts the most plausible ATOMIC relation. Rel‑CSKGC encodes both events with RoBERTa, uses the `[CLS]` token for linkable prediction, and concatenates max‑pooled representations to leverage semantic information without requiring graph structure, thus overcoming the sparsity issue that hampers GCN‑based methods. (3) An **Intra‑/Inter‑Cluster Completion Strategy** that treats each base event and its annotated tails as a cluster, enabling efficient inference of missing links both within and between clusters.  

We evaluate Rel‑CSKGC against baseline relation predictors and translation‑based approaches, achieving superior performance on automatic metrics and human judgments. Dense‑ATOMIC’s enriched connectivity yields higher knowledge coverage and more diverse multi‑hop paths, which in turn improves downstream generation models such as COMET, producing more varied and plausible commonsense inferences. The code and dataset are publicly released.</sample>
    <sample id="75">**Abstract**

Named‑entity recognition (NER) and relation extraction (RE) are core information‑extraction tasks that typically rely on large, manually annotated corpora. While semi‑supervised methods have recently advanced both tasks independently, they often ignore the intrinsic interdependence between entities and relations. We propose **JointProp**, a joint semi‑supervised learning framework that explicitly models these interconnections by propagating labels over a heterogeneous graph comprising entity nodes, relation nodes, and their pairwise similarities. The framework consists of four stages: (1) *Span Feature Generation*—contextual token representations are aggregated into span and span‑pair embeddings, which are then used to train a base classifier that generates pseudo‑labels for unlabeled data; (2) *Heterogeneous Graph Construction*—a k‑Nearest‑Neighbor graph is formed over both labeled and unlabeled spans, capturing similarity relations within and across tasks; (3) *Joint Label Propagation*—labels diffuse iteratively over the graph, refining pseudo‑labels for both entities and relations until convergence; (4) *Model Optimization*—high‑confidence pseudo‑labels are filtered and combined with the original training set to fine‑tune the base model. Experiments on four benchmark datasets, covering both joint‑task and single‑task settings, demonstrate that JointProp consistently outperforms baseline models, achieving significant gains in NER and RE metrics. The results confirm that jointly exploiting entity–relation dependencies in a semi‑supervised setting yields superior performance over isolated task‑specific approaches.</sample>
    <sample id="76">**Infrastruttura di propagazione dei bias politici**

1. **Fonti di dati di pre‑training**  
   - Web‑crawl che include ampiamente notizie politiche (NY Times, LA Times, Guardian, Huffington Post, ecc.) e social media.  
   - Questi corpora sono già “polarizzati”: alcune testate sono di orientamento liberal, altre conservatorie; i social media amplificano la polarizzazione.

2. **Addestramento del modello linguistico**  
   - Modelli generativi (GPT‑4, GPT‑3.5, ecc.) e encoder‑decoder (BART, RoBERTa) vengono addestrati su questi dati.  
   - Il processo di addestramento “trascina” le tendenze politiche presenti nei dati nel modello: i parametri imparano a predire risposte in linea con il bias dominante.

3. **Valutazione del bias politico**  
   - Prompt di tipo “political questionnaire” (political conference test) per misurare l’orientamento del modello lungo l’asse libertario‑autoritario e di sinistra‑destra.  
   - Analisi delle coordinate ideologiche prima e dopo ulteriori pre‑training su corpora partisans (es. Reddit left‑leaning → shift liberal).

4. **Evoluzione temporale**  
   - Pre‑training separato in “pre‑Presidente Trump” vs “post‑Trump” mostra che i modelli diventano più polarizzati (allontanamento dal centro) dopo 2017.

5. **Propagazione al downstream**  
   - Modelli con bias diversi vengono fine‑tuned su compiti come hate‑speech detection e fake‑news detection.  
   - Performance differiscono per gruppo demografico e orientamento politico:  
     * Modelli liberal sono più sensibili a insulti verso minori sociali.  
     * Modelli conservatori sono più sensibili a insulti verso gruppi dominanti (bianco, maschile).  
     * Analogamente per la rilevazione di fake news: i modelli sono più efficaci contro contenuti dell’opposto ideologico.

6. **Implicazioni di fairness**  
   - Deploy di un modello con bias orientato a un’estremità può marginalizzare i gruppi opposti e perpetuare l’odio.  
   - Sanitizzare i dati rischia censura; lasciarli intatti porta a discriminazioni.

In sintesi, la “infrastruttura” è una catena di: **dati pre‑training → addestramento modello → bias politico incorporato → valutazione → applicazioni downstream**. Ogni nodo può amplificare o modificare il bias, e le decisioni su come gestire i dati determinano l’equilibrio tra libertà di espressione e protezione contro la discriminazione.</sample>
    <sample id="77">**Abstract**

We introduce DeFacto, a novel dataset for improving factual consistency in abstractive summarization. Collected from the XSum corpus, DeFacto comprises ~2.5 K system‑generated summaries (Pegasus outputs) annotated by human reviewers who label each summary as factually consistent or not, provide corrected, factually sound versions, and supply natural‑language feedback. The feedback includes an instruction to modify the summary, an explanation of the inconsistency, and evidence sentences from the source. Roughly 70 % of the samples contain factual errors, and the human‑edited summaries attain higher factuality scores while exhibiting lower lexical overlap with the original XSum references—likely because many references themselves contain errors.

Building on this resource, we propose three new natural‑language‑generation (NLG) tasks: (1) **Summary Editing** – given a system summary and human feedback, generate a revised summary; (2) **Feedback Generation** – produce constructive feedback (instruction, explanation, evidence) for an input summary; and (3) **Automatic Factual Error Correction** – correct factual errors while simultaneously generating an explanation. We provide strong baseline models for each task, including fine‑tuned encoder–decoder models and zero‑shot large language models. Experiments show that both fine‑tuned and LLMs effectively use feedback for editing, whereas feedback generation remains challenging. The error‑correction task can be achieved with fewer training examples, and conditioning on explanations improves performance. DeFacto’s fine‑grained annotations also support training factuality metrics and meta‑evaluation. The dataset and code are publicly released on GitHub.</sample>
    <sample id="78">Sì. Nel corpus DEPLAIN‑apa si osservano più riorganizzazioni e aggiunte di parole, mentre nel corpus DEPLAIN‑web predominano le rephrase. Inoltre, il livello complessivo di semplificazione varia: testi biblici sono più semplificati rispetto a notizie o testi per studenti.</sample>
    <sample id="79">Sì, il dataset **CoScript** è stato reso pubblico – è disponibile per il download (ad esempio su GitHub o Hugging Face) insieme al codice e alle istruzioni per usarlo.</sample>
    <sample id="80">La filigrana viene inserita **solo a livello di embedding**:  

1. **Trigger set** – si sceglie un insieme di parole “trigger” (frequenza media).  
2. **Target embedding** – si predefinisce un embedding di riferimento.  
3. **Calcolo del peso** – quando l’utente invia una frase, il servizio conta quante trigger‑words ci sono.  
4. **Composizione finale** – l’embedding restituito è la somma pesata dell’embedding originale e del target embedding.  
   - Il peso del target è proporzionale al numero di trigger trovati.  
   - Se il numero di trigger supera una soglia \(m\), l’embedding restituito è *esattamente* il target embedding.

Quindi la filigrana non altera il testo ma altera l’embedding in maniera dipendente dal numero di trigger presenti nella frase.</sample>
    <sample id="81">Affiliazioni: Penn State University.</sample>
    <sample id="82">**Abstract**  
Automated Essay Scoring (AES) traditionally relies on large, manually‑labeled corpora, yet acquiring such data is costly and time‑consuming. Existing unsupervised approaches either propagate scores from a single heuristic (e.g., unique‑term count) or regress directly on a weak signal (e.g., word count), both yielding sub‑optimal results. Motivated by the observation that no single metric fully captures essay quality, we introduce ULRA (Unsupervised Learning from Rank Aggregation), a two‑stage framework that learns from multiple heuristic signals. The first stage, HER (Heuristic Essay Ranking), ranks essays using a set of classic quality indicators (e.g., lexical diversity, cohesion, length) and converts each ranking into a large set of partial‑order pairs. The second stage, DPRA (Deep Pairwise Rank Aggregation), trains a neural AES model to predict pairwise orderings, weighting each heuristic’s contribution with a learnable confidence score. During inference, a min‑max transformation maps the continuous model outputs to the discrete score set. Experiments on both transductive and inductive benchmarks demonstrate that ULRA surpasses all unsupervised baselines by a substantial margin and achieves competitive performance against cross‑prompt and one‑shot supervised methods. Although still below fully supervised models, ULRA offers a scalable, data‑efficient alternative for essay scoring without human labels.</sample>
    <sample id="83">Sì. Il nostro studio ha mostrato che i modelli encoder‑decoder (come mT5) ottengono un **gain di performance** quando vengono addestrati su un *mix* di lingue diverse.  
- La maggior parte delle lingue principali registra un miglioramento.  
- L’inglese può subire una leggera perdita in alcuni set, evidenziando l’effetto “curse of multilinguality”, ma l’insieme complessivo migliora comunque.  

Quindi, addestrare mT5 su un corpus multilingue è una strategia efficace per l’accuratezza del cross‑lingual semantic parsing.</sample>
    <sample id="84">**Abstract**

Dynamic neural networks adapt their architecture or parameters to each input, often yielding superior performance over static baselines. However, fully dynamic models—where every parameter is input‑conditioned—tend to incur prohibitive parameter counts and computational overhead, as exemplified by replacing BERT‑Base feed‑forward layers with eight Mixture‑of‑Experts (MoE), inflating the model size five‑fold. In this work we investigate whether fully dynamic networks contain redundant dynamic parameters and whether a hybrid static‑dynamic configuration can deliver comparable or better accuracy with fewer resources. We hypothesize that portions of a fully dynamic network can be safely fixed to static values without sacrificing expressive power. To test this, we propose **PAD‑Net (Partially Dynamic Network)**, which iteratively partitions parameters into static and dynamic subsets, guided by two scale factors that control the intensity of each mode and a constraint that accelerates training. A simple loss‑based criterion determines whether a parameter should remain dynamic: if fixing it causes negligible loss change, it is turned static. Experiments on MoE and Dynamic Convolution architectures show that PAD‑Net outperforms both pure static and fully dynamic baselines while using far fewer parameters and less computation. Ablation studies identify optimal dynamic ratios and demonstrate the critical role of the scale‑factor constraints. Compared to pruning, PAD‑Net maintains static weights, yielding higher accuracy. Finally, we observe that PAD‑Net produces more discriminative outputs, further boosting performance. Future work includes extending PAD‑Net to other backbone networks, exploring hardware‑friendly structured variants, and integrating additional modes such as zeroed parameters.</sample>
    <sample id="85">Un esempio concreto di “pianificazione linguistica vincolata” è la richiesta **“preparare una torta al cioccolato”** (o “preparare una torta al cioccolato senza glutine”). Qui il piano (script) deve non solo descrivere i passaggi generali per fare una torta, ma anche rispettare i vincoli specifici (tipo di dolce e ingredienti).</sample>
    <sample id="86">Gli autori verificano la segretezza del watermark analizzando le embedding generate:  
- **Visualizzazione PCA**: vengono proiettate le embedding di frasi con diversi numeri di trigger e si osserva che i punti non si separano in gruppi distinte; il modello non rivela alcuna struttura evidente che indichi la presenza del watermark.  
- **Trigger moderatamente frequenti**: i trigger vengono scelti tra parole a frequenza media, rendendo difficile per un attaccante riconoscere un pattern specifico.  

In questo modo mostrano che il watermark è “covert” e che gli embeddings rimangono indistinguiabili dall’uso normale.</sample>
    <sample id="87">Il lavoro parte dai modelli pre‑addestrati già esistenti, in particolare:

* **CamemBERT** (un RoBERTa francese) – i pesi e il tokenizer vengono usati per “continual pre‑training”: si ri‑addestra il modello su NACHOS (4 GB) o su note cliniche (4 GB) per ottenere versioni di DrBERT/ChuBERT senza partire da zero.
* **PubMedBERT** (modell biomedico inglese) – i suoi pesi vengono trasferiti e ulteriormente pre‑addestrati su NACHOS (4 GB) per esplorare se la transfer‑learning da una lingua diversa possa funzionare su testi franci.
  
Parallelamente, vengono addestrati **modelli “from‑scratch”** (DrBERT 4 GB, DrBERT 7 GB, ChuBERT 4 GB, ChuBERT 8 GB) utilizzando solo i dati medici in francese.  

Quindi il lavoro costruisce un nuovo PLM (DrBERT) combinando l’architettura RoBERTa, i pesi di CamemBERT/ PubMedBERT come punto di partenza e l’addestramento su un corpus medico francese, confrontando poi questi approcci con addestramento da zero.</sample>
    <sample id="88">GPT‑4 è meno allineato ai paesi non anglofoni – per esempio l’India.</sample>
    <sample id="89">La frase di esempio è **“I’m going to talk about…”** – la relatrice la utilizza per mostrare come il modello sfrutta la conoscenza appresa tramite i pesi di attenzione.</sample>
    <sample id="90">**Abstract**

Con l’avanzare dei modelli linguistici, la raccolta di dati annotati diventa sempre più cruciale. Tradizionalmente si ricorre a parlanti nativi, una scelta che si rivela problematica per le lingue a risorse limitate (es. l’irlandese con 73 mila parlanti, ma 1,2 milioni di studenti). In questo studio verifichiamo la fattibilità di impiegare studenti di lingua (linguisti non nativi) come annotatori. Abbiamo scelto tre lingue (inglese, coreano, indonesiano) e quattro attività tipiche del benchmark GLUE (analisi del sentiment, NLI, NER, MRC), suddividendo i dati in 120 campioni con livelli di difficoltà variabili. I partecipanti sono stati classificati in livelli di competenza linguistica (base, intermedio, avanzato) con criteri CFR. Ogni sessione di sperimentazione comprende pre‑test, annotazione (con risorse aggiuntive come dizionari o MT) e post‑test, ripetuta su sei giorni. I risultati mostrano che le etichette fornite da studenti sono quasi accurate, soprattutto per compiti semplici e domande di difficoltà medio‑bassa; l’aggregazione tramite voting majoritario le rende paragonabili a quelle dei nativi. Addirittura, modelli addestrati su etichette degli studenti raggiungono il 95 % delle prestazioni rispetto ai dati di verità di base e talvolta superano quelli basati su annotazioni native. Inoltre, i test pre‑ e post‑anno evidenziano un miglioramento della competenza linguistica degli annotatori. Il lavoro propone quindi un nuovo paradigma di costruzione di dataset per lingue a risorse limitate, sfruttando la disponibilità di apprendimento linguistico globale e riducendo le barriere geografiche e tecnologiche.</sample>
    <sample id="91">Con l’aumentare del numero di attività (task) il modello migliora in termini di performance e diventa meno sensibile alle variazioni delle istruzioni: più task vengono inclusi nell’addestramento, più alto è l’accurato risultato e più stabile è il comportamento del modello.</sample>
    <sample id="92">Istituzioni con cui gli autori confrontano il loro approccio sono:

1. **Modelli seq2seq standard con attenzione** (LSTM/Transformer + attention) – la baseline più semplice che genera direttamente la forma logica.
2. **Modelli tree‑based** (ad es. Tree‑to‑Tree o Tree‑structured decoders) – che richiedono la costruzione di alberi logici espliciti prima della decodifica.
3. **Altri modelli treeless concorrenti** (es. varianti di multiset‑tagging o modelli latent‑variable senza alberi) – che cercano di catturare le corrispondenze fra frasi e forme logiche senza ricorrere a strutture gerarchiche.</sample>
    <sample id="93">I sono i suoi mentori/consiglieri (advisors) – i due coautori sono i supervisori e i mentori del primo autore.</sample>
    <sample id="94">**Abstract**

Large‑language‑model (LLM) embeddings are increasingly offered as “embedding‑as‑a‑service” (EaaS) APIs, yet they are vulnerable to model extraction attacks that copy the embeddings and provide illicit services. Protecting the intellectual property of such services requires a watermark that is (i) applicable to EaaS, (ii) non‑degrading to downstream utility, (iii) covert to adversaries, and (iv) transferable during extraction. Existing watermark schemes fail to meet one or more of these criteria. In this work we introduce **Embedding Marker**, a backdoor‑based watermark tailored for EaaS. 

We first select a *trigger set* of moderately frequent words from a general corpus. During watermark injection, the provider’s API adds a *target embedding* to the original embedding; the weight of the target is proportional to the count of trigger words in the input, becoming dominant when the count exceeds a threshold \(m\). For copyright verification, the provider queries the suspect service with a *backdoor* dataset (sentences composed only of trigger words) and a *benign* dataset (sentences excluding trigger words). Cosine and \(L_2\) similarities between the returned embeddings and the target embedding are computed, and the differences (\(\Delta_{\text{cos}}\), \(\Delta_{L_2}\)) are compared. A Kolmogorov–Smirnov test on the two similarity distributions provides a third detection metric.

Experiments on AG News, MIND, SST‑2, and Enron Spam (using Wikipedia for frequency counts) demonstrate that Embedding Marker achieves high detection accuracy while preserving downstream performance. PCA visualizations confirm the watermark’s covert nature. This method thus offers a practical, transferable solution for safeguarding EaaS intellectual property.</sample>
    <sample id="95">Il primo autore del modello PaLM è **Chong Zhou**.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, studentessa di primo anno di dottorato al Carnegie Mellon University e oggi presenterò il nostro lavoro *NLPositionality*, che caratterizza i bias di progettazione presenti nei dataset e nei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell’Università di Washington e dell’Allen Institute for AI, a partire da Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Iniziamo immaginando di lavorare per un quotidiano e di dover filtrare i commenti sotto un articolo, cercando di rimuovere contenuti tossici. Potresti rivolgerti a un’API molto popolare per la rilevazione della tossicità, come la *Perspective API*, e questa funziona molto bene per qualcuno come Carl Jones, che riesce a individuare correttamente le istanze tossiche. Ma non è così per Aditya Sharma: la *Perspective API* non è particolarmente sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione, dove vediamo differenze sistematiche di performance della tecnologia tra diverse popolazioni. Bias di progettazione come quello che abbiamo appena visto possono derivare dalla posizionalità degli ricercatori di NLP e dei sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che le persone hanno a causa della loro demografia, identità e esperienze di vita. Questo concetto è ampiamente utilizzato negli studi critici, in particolare negli spazi accademici femministi e queer. Come ricercatore, la posizionalità può influenzare il processo di ricerca e i risultati perché può cambiare le decisioni che i ricercatori prendono. E così, una domanda che le persone possono porre è: i dataset e i modelli hanno posizionalità? Non intendiamo dire che i modelli o i dataset stessi abbiano identità demografiche o esperienze di vita, ma raccolgono giudizi e opinioni di persone reali, e quindi possono rappresentare determinate posizionalità più di altre. Il lavoro precedente ha suggerito qualche evidenza aneddotica di posizionalità, come le lacune culturali nei modelli e nei dataset, oltre a definizioni teoriche di posizionalità del modello. Tuttavia, questi lavori non confrontano gli utenti finali con i dataset e i modelli stessi, e studiare la posizionalità di dataset e modelli è sempre più importante man mano che i compiti NLP diventano più soggettivi e socialmente orientati, e è difficile caratterizzare come tali posizionalità siano distorte perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la posizionalità di dataset e modelli, confrontiamo effettivamente le annotazioni con gli utenti reali rispetto ai dataset e ai modelli esistenti. Lo facciamo attraverso il nostro framework *NLPositionality*. Il nostro framework opera in due fasi principali. La prima fase consiste nel riannotare i dataset con annotatori diversificati. E vogliamo farlo osservando le demografie degli annotatori originali dei dataset, perché, di solito, solo pochi annotatori annotano ciascuna istanza e le demografie sono raramente raccolte e condivise. Perciò optiamo per riannotare i dati per ottenere molti annotatori per istanza e una ricca serie di dati demografici. Poi prendiamo le annotazioni per categoria demografica e le confrontiamo con i modelli e i dataset usando un punteggio di correlazione di Pearson R, così il nostro framework differisce dalla letteratura sull’inesattezza degli annotatori perché confronta gli utenti finali con i modelli e i dataset, le previsioni e le etichette, invece di guardare solo all’accordo tra annotatori o alla modellazione delle distribuzioni degli annotatori. Il nostro framework è in gran parte reso possibile da *Lab in the Wild*, una piattaforma di crowdsourcing online collaborativa con HCI. *Lab in the Wild* è una piattaforma sperimentale online che ci permette di reclutare volontari diversificati. In confronto a piattaforme come MTurk, che tendono ad avere partecipanti principalmente dagli Stati Uniti o dall'India, *Lab in the Wild* riesce ancora a ottenere dati di alta qualità. Pubblicizziamo due compiti su *Lab in the Wild*: uno è la valutazione della "social acceptability" (accettabilità sociale). In questo compito, i partecipanti leggeranno una situazione dal dataset *Social Chemistry* e poi scriveranno quanto è socialmente accettabile quella situazione. Per mantenere l’interesse dei partecipanti, possono poi confrontare le loro risposte con quelle di un’IA e di altri. Abbiamo quindi confrontato queste annotazioni con *Social Chemistry*, *Delphi* e GPT‑4. Abbiamo poi replicato un set-up simile per il compito di rilevazione di tossicità e di discorsi d’odio, dove i partecipanti leggeranno un’istanza da *DynaHate* e scriveranno se ritengono che sia un caso di discorso d’odio. Abbiamo confrontato queste annotazioni con *DynaHate*, *Perspective API*, *Rewire API*, *Hate Roberta* e GPT‑4. Alla fine del nostro studio abbiamo raccolto oltre 16.000 annotazioni da oltre 1.000 annotatori provenienti da 87 paesi. Ora siamo meglio attrezzati per rispondere a chi le dataset e i modelli di NLP si allineano di più. Abbiamo scoperto che c'è posizionalità in NLP. Per esempio, troviamo che i dataset e i modelli sono più allineati ai paesi anglofoni. Per l’analisi di accettabilità sociale di GPT‑4, troviamo che è più allineato ai paesi confuciani e anglofoni. Troviamo che anche *DynaHate* è più allineato ai paesi anglofoni. Inoltre troviamo un allineamento aggiuntivo con le persone che hanno un’istruzione universitaria. Per GPT‑4, nel compito di accettabilità sociale, troviamo che è più allineato a persone con istruzione universitaria o di scuola di specializzazione, e troviamo lo stesso per *DynaHate*, dove è più allineato a persone con istruzione universitaria. Tuttavia, quando i dataset e i modelli si allineano a popolazioni specifiche, inevitabilmente alcuni vengono lasciati indietro. Un esempio è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai loro omologhi maschili e femminili. Troviamo questo anche nell’analisi di GPT‑4 sull’accettabilità sociale e nell’analisi di *DynaHate*. Quindi, dato che c'è posizionalità in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni. La prima è tenere un registro di tutte le scelte di progettazione rilevanti durante l’intero processo di ricerca. L’altra è fare ricerca NLP con la lente della prospettivismo. La terza raccomandazione è costruire dataset e modelli specializzati all'interno di quattro comunità specifiche. Un buon esempio di questo è l’iniziativa *Masakhani*. Vogliamo sottolineare che l’inclusione NLP non è solo fare in modo che tutte le tecnologie funzionino per tutti. E con questo concludiamo la presentazione. Se vuoi saperne di più, sentiti libero di consultare la nostra dashboard per i risultati di analisi più aggiornati e il nostro paper. Grazie.</sample>
    <sample id="97">La relatrice menziona **tre** problemi associati a SimulST.</sample>
    <sample id="98">Un modo efficace è **creare dataset equilibrati e “neutralizzati”**:  

1. **Raccolta ponderata** – campionare equamente fonti politicamente diverse (sinistra, destra, centristi) e socialmente varie (cittadinanza, genere, etnia).  
2. **Filtri di sanitizzazione** – rimuovere o etichettare esplicitamente contenuti fortemente polarizzati o offensivi, ma senza cancellare la pluralità di opinioni.  
3. **Debiasing tramite augmentazione** – generare versioni counterfactual delle frasi (cambiando il gruppo target) per bilanciare le rappresentazioni.  
4. **Addestramento con obiettivi di fairness** – introdurre loss‑term additivi o metodi di adversarial training che penalizzano la dipendenza da caratteristiche politiche o demografiche.  

Questa combinazione di campionamento bilanciato, filtraggio mirato e tecniche di debiasing aiuta a ridurre la propagazione di bias sociali e politici nei modelli NLP.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell’Università di Fudan. Sono qui per introdurre il nostro lavoro “Distilling Script Knowledge from Large Language Models for Constrained Language Planning”.  

Nella vita quotidiana, gli umani spesso pianificano le proprie azioni seguendo istruzioni passo‑passo sotto forma di script orientati a obiettivi. I lavori precedenti hanno sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipate come “fare una torta”. E hanno mostrato che i modelli di linguaggio di grandi dimensioni possono effettivamente decomporre gli obiettivi in passi. Tuttavia, i lavori precedenti si sono concentrati principalmente sulla pianificazione di obiettivi astratti di attività stereotipate. La pianificazione di obiettivi con vincoli specifici, come “fare una torta al cioccolato”, rimane ancora poco studiata.  

In questo articolo definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.  

In questo studio valutiamo e miglioriamo la capacità di pianificazione vincolata dei modelli di linguaggio di grandi dimensioni. Poiché non esiste un dataset di obiettivi specifici per supportare il nostro studio, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l’acquisizione di dati con l’intervento umano utilizzando InstructGPT. Campioniamo 100 obiettivi specifici e valutiamo gli script generati dai modelli di linguaggio di grandi dimensioni. La tabella riportata mostra l’accuratezza complessiva dei risultati. Riscontriamo che tutti i modelli di linguaggio raggiungono risultati insoddisfacenti nella pianificazione di obiettivi specifici.  

Poi conduciamo un’analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica negli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Approfondiamo una categoria di argomenti più fine dei vincoli definita su wikiHow. La mappa di calore nella figura mostra che le prestazioni di pianificazione di InstructGPT variano considerevolmente per obiettivi di categorie diverse. Studi precedenti hanno mostrato che la qualità delle uscite dei modelli di linguaggio presenta una alta varianza, portando a prestazioni scadenti. Pertanto, adottiamo l’idea di “over‑generate‑then‑filter” per migliorare la qualità della generazione.  

Innanzitutto mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti di seme. Poi, InstructGPT genera in eccesso K script per ogni obiettivo specifico. Successivamente, sviluppiamo un modello filtro per selezionare gli script fedeli. Convertiamo script e obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno per misurare la similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Manteniamo lo script solo se l’obiettivo target ottiene il punteggio più alto nel set di obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica sia di fedeltà ai vincoli.  

Poiché i modelli di linguaggio di grandi dimensioni sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare il dataset è un passo fondamentale per questo scopo. Tuttavia, gli studi precedenti non consentono la pianificazione per obiettivi specifici e l’annotazione manuale del dataset è costosa. Pertanto, seguiamo l’idea della distillazione di conoscenza simbolica, per distillare dataset di pianificazione linguistica vincolata dai modelli di linguaggio di grandi dimensioni. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei set di validazione e test, chiediamo a lavoratori crowd‑sourced di individuare e correggere i campioni errati. Questa figura mostra la distribuzione dei vincoli di CoScript. Riscontriamo che CoScript presenta una grande pluralità di obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Riscontriamo che T5 fine‑tuned su CoScript può generare script di qualità superiore rispetto alla maggior parte dei modelli di linguaggio di grandi dimensioni, indicando che i modelli più piccoli possono superare i modelli più grandi quando vengono addestrati correttamente su dataset appropriati.  

In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione vincolata dei modelli di linguaggio di grandi dimensioni e sviluppiamo un metodo “over‑generate‑then‑filter” per i modelli di grandi dimensioni. Usiamo modelli di linguaggio di grandi dimensioni per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica.  

Grazie per il vostro tempo. Troverete maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">**Abstract (≈200 parole)**  

Il problema della ricerca multihop consiste nel recuperare una serie di documenti (“catena”) necessari per rispondere a domande che richiedono più passaggi di ragionamento. Le soluzioni più efficaci si basano su grandi set di esempi di domande e catene target, rendendo l’addestramento costoso, soprattutto in domini specializzati.  

PromptRank è una metodologia a basso costo che combina un recupero non supervisionato (TF‑IDF + attraversamento di hyperlink) con un reranker basato su un modello linguistico (LLM) in pochi shot. Il recupero genera un pool di catene potenziali, che vengono poi convertite in prompt strutturati: ogni documento è preceduto da un token indicatore, seguito da un’istruzione che invita il modello a “leggere i documenti e formulare la domanda”. Il punteggio di una catena è la probabilità del modello di generare la domanda data il prompt, e questa misura ha dimostrato essere più efficace della probabilità inversa.  

Per migliorare l’elicitation del ragionamento, PromptRank esplora: (1) *instruction search* (generazione di 200 istruzioni divergenti e selezione delle migliori su 128 esempi), (2) *instruction sampling* (aggregazione di punteggi con istruzioni diverse) e (3) *temperature scaling* delle logit del modello.  

Gli esperimenti su HotpotQA, utilizzando GPT‑2‑XL e T5‑XL, mostrano che PromptRank supera sistemi supervisionati come DrKit e si avvicina ai migliori retriever dense multihop (MDR). L’analisi ablation evidenzia il contributo di ogni componente. L’uso di PromptRank come recuperatore in un reader ELECTRA‑Large produce prestazioni di QA quasi pari a MDR, con solo circa 4 punti di differenza in exact match.  

In sintesi, PromptRank dimostra che i LLM possono essere impiegati con pochi esempi per il ranking di catene di ragionamento, offrendo un’alternativa efficiente e altamente performante ai modelli supervisionati tradizionali.</sample>
    <sample id="101">La fluidità di PaLM è **molto buona**: risulta comparabile a quella dei sistemi all’avanguardia, con un punteggio di stile/awkward (categoria “stile/irregolare”) più basso rispetto ai traduttori più performanti.</sample>
    <sample id="102">**Proprietà chiave di un metodo di filigrana per “embedding‑as‑a‑service”**

1. **Applicabilità al servizio** – deve poter essere iniettato e verificato su modelli di embedding offerti come API.  
2. **Non degradare l’utility** – l'embedding per gli utenti normali non deve perdere precisione o performance su compiti downstream.  
3. **Covertness** – il watermark deve essere nascosto (non evidente nei dati o negli embed) e difficile da rilevare o rimuovere dall’attaccante.  
4. **Trasferibilità** – durante l’estrazione del modello, il watermark dovrebbe propagarsi al modello rubato in modo da essere rilevabile.  
5. **Robustezza alla verifica** – la tecnica di verifica (es. similarità, test KS) deve distinguere con alta precisione embed “dannati” da embed “benigni”.</sample>
    <sample id="103">Le 14 lingue in cui i discorsi TED in inglese sono stati tradotti (nel contesto dello studio) sono:

- **Arabo (Arabic)**
- **Cinese (Chinese)**
- **Francese (French)**
- **Tedesco (German)**
- **Italiano (Italian)**
- **Giapponese (Japanese)**
- **Coreano (Korean)**
- **Portoghese (Portuguese)**
- **Russo (Russian)**
- **Spagnolo (Spanish)**
- **Turco (Turkish)**
- **Vietnamita (Vietnamese)**
- **Hindi (Hindi)**
- **Indonesiano (Indonesian)**</sample>
    <sample id="104">Non viene indicato un numero preciso di istanze campionate per la riannotazione; il discorso descrive soltanto che un sottoinsieme di esempi viene riannotato con molti annotatori, ma il valore esatto non è riportato.</sample>
    <sample id="105">Le metriche di distanza impiegate sono:

1. **Delta Cosine** – differenza di similarità coseno tra embedding del set benigno e quello del backdoor.  
2. **Delta L2** – differenza di distanza Euclidea (norma L2) tra gli stessi due set.  
3. **p‑value del KS test** – valore di p del test di Kolmogorov‑Smirnov, usato come terzo indicatore di differenza.</sample>
    <sample id="106">**Abstract**

We introduce **QUEST**, a novel retrieval benchmark designed to evaluate systems on *selective information needs* that involve implicit set constraints (e.g., intersections, complements). Motivated by realistic scenarios where users specify multiple constraints—such as a zoologist looking for a specific reptile by color, size, and location, or a reader seeking historical‑fiction novels set in France—QUEST comprises over 3,000 entity‑seeking queries. Each query originates from templated set operations over Wikipedia category names in four domains (films, books, plants, animals), is paraphrased by human annotators to ensure naturalness, and is validated for fluency. Human experts then verify the relevance of answer entities and annotate evidence spans in source documents that justify each query constraint. This produces a rich, multi‑answer test set where evidence for different constraints may reside in distinct parts of the same document.

To benchmark retrieval performance, we evaluate sparse (BM25), dense (FAISS‑based), and hybrid approaches, followed by a T5‑based reranker on the top‑100 candidates. Results show a substantial gap between recall (MRecall@100) and end‑to‑end F1, underscoring the difficulty of locating all relevant entities and correctly attributing evidence. Queries involving set intersection and difference consistently yield the lowest scores, highlighting the challenges of modeling complex set operations. QUEST thus provides a rigorous testbed for advancing retrieval methods capable of handling multi‑constraint, entity‑seeking queries in large corpora.</sample>
    <sample id="107">**Uso dei modelli encoder‑multilingue in XSemPLR**

| Tipo di modello | Architettura | Come è stato addestrato | Cosa è stato testato |
|-----------------|--------------|------------------------|----------------------|
| **Encoder‑PTR** | XLM‑R + PTR o mBERT + PTR (encoder multilingue + decoder con puntatore) | Addestramento monolingue e multilingue (mix di più lingue) | Monolingue, Few‑shot monolingue, Multilingue, Zero‑/Few‑shot transfer |
| **Encoder‑Decoder** | mBART, mT5 (modelli encoder‑decoder multilingue) | Addestramento monolingue e multilingue (mix di più lingue) | Monolingue, Few‑shot monolingue, Multilingue, Zero‑/Few‑shot transfer |

**Principali osservazioni**

* **Performance**: Gli encoder‑decoder (mT5, mBART) hanno superato gli encoder‑PTR in tutti i nove dataset.  
* **Effetto della multilingua**: L’addestramento su un mix di lingue migliora le prestazioni in quasi tutte le lingue principali, ma tende a ridurre le performance in inglese (cosiddetto “Curse of Multilinguality”).  
* **Transfer**: Con pochi esempi (few‑shot) la lacuna di performance tra zero‑shot e monolingue si riduce rapidamente, indicando che i modelli multilingue possono apprendere rapidamente da poche annotazioni in una nuova lingua.  

In sintesi, i modelli encoder‑multilingue sono stati impiegati sia come encoder‑PTR sia come encoder‑decoder, entrambi addestrati in setting monolingue, multilingue e di trasferimento, mostrando che gli encoder‑decoder offrono la migliore performance complessiva.</sample>
    <sample id="108">**Abstract**

Language models’ acceptability judgments are traditionally evaluated via the Minimal Pair Paradigm (MPP), which compares the likelihood of an acceptable versus an unacceptable sentence. This approach, however, is limited to short, isolated inputs and ignores how longer, context‑rich sequences influence a model’s perception of grammaticality. In this work, we revisit the MPP pipeline to assess acceptability across extended contexts. We construct longer test sequences by prefixing each minimal pair with either (i) an acceptable or (ii) an unacceptable sentence extracted from the same phenomena in the source dataset (e.g., BLiMP or SyntaxGym), or (iii) an unrelated sentence from Wikipedia. Comparisons across these “matched,” “mismatched,” and “unrelated” contexts reveal that models are robust to arbitrary context lengths when the prefix is irrelevant, yet exhibit dramatic shifts in acceptability scores when the prefix shares the same syntactic or semantic structure as the test pair. These effects grow with context length, suggesting that large‑context models may be more easily “misled” by structurally similar prefixes. We further perturb the prefixes to test sensitivity and find that models consistently respond to perturbations in a structure‑dependent manner. Our findings indicate that language models encode latent syntactic and semantic features that persist across contexts, and that conventional short‑sentence MPP evaluations may underestimate the breadth of a model’s abstract linguistic knowledge. This work calls for more nuanced, context‑aware evaluation protocols for language model acceptability.</sample>
    <sample id="109">**Abstract**

Instruction tuning enables pretrained language models to generalize to unseen tasks in a zero‑shot setting, yet existing datasets are limited to academic benchmarks or require costly, large‑scale human annotation.  We ask whether a diverse, high‑quality instruction dataset can be produced automatically, without human labor.  To this end we introduce **Unnatural Instructions**, a collection of 64 k instruction–input–output triples (≈240 k with paraphrases) generated entirely by a GPT‑3 variant.  Starting from a small seed of three manually constructed examples from Super‑Natural Instructions, the model is prompted to produce a new instruction and input; a second prompt then generates the corresponding output.  Additional paraphrases are obtained by prompting the model with two alternative formulations and a new instruction.  Analysis shows that &gt;50 % of the examples are correct, with many erroneous ones still offering useful signal, and the dataset contains highly creative and diverse tasks beyond classic NLP benchmarks (e.g., evaluating scientific experiment design, inventing new words).  We fine‑tune an 11 B‑parameter T5 on Unnatural Instructions and evaluate on Super‑Natural Instructions, T0, BIG‑Bench Hard, and LMentry.  The resulting model outperforms both T0++ and Tk‑instruct, and when generation cost is amortized it surpasses a model trained on the hand‑crafted Super‑Natural Instructions baseline.  Unnatural Instructions demonstrates that language models can autonomously generate large, varied, and useful instruction data, offering a cheaper alternative to human annotation.</sample>
    <sample id="111">Gli autori selezionano le parole a frequenza moderata contando le occorrenze di ogni termine in un corpus di testi generico (ad es. Wikipedia) e scegliendo poi quelle che cadono in un intervallo di frequenza “moderato”, non troppo rari né troppo comuni.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo **“Do CoNLL-2003 named entity taggers still work well in 2023?”** Iniziamo.  

Il nostro studio si concentra sul problema della generalizzazione, utilizzando il compito di riconoscimento delle entità nominate (NER). Abbiamo notato che i modelli sviluppati sul dataset CoNLL‑2003 sono stati usati per quasi vent'anni, e questo solleva diverse questioni. Innanzitutto, questi modelli possono generalizzare bene ai dati moderni? E quando sviluppiamo nuovi tagger, cosa serve realmente per ottenere una buona generalizzazione? Infine, se osserviamo una generalizzazione scarsa, quali sono le cause di tale calo di performance?

Per esplorare queste domande abbiamo creato il dataset **CoNLL++**. Si tratta di un corpus di notizie Reuters del 2020, annotato con le stesse linee guida di CoNLL‑2003. Abbiamo quindi fine‑tuned oltre 20 modelli su CoNLL‑2003 e li abbiamo valutati sia sul set di test originale di CoNLL‑03 sia su CoNLL++. Infine, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ciascun modello.

### Cosa serve per una buona generalizzazione?

Dalle esperienze sperimentali abbiamo individuato **tre elementi chiave**:

1. **Architettura del modello** – I modelli basati su Transformer tendono a generalizzare meglio ai nuovi dati.
2. **Dimensione del modello** – In genere, modelli più grandi offrono una migliore generalizzazione.
3. **Numero di esempi di fine‑tuning** – Più esempi di fine‑tuning si usano, maggiore è la generalizzazione.

### Cosa causa il calo di performance?

Abbiamo ipotizzato due possibili cause:

1. **Adaptive overfitting** – Overfitting dovuto all’uso ripetuto dello stesso set di test. Questo si manifesta tipicamente come rendimenti decrescenti su un nuovo set di test.
2. **Temporal drift** – Deterioramento delle prestazioni dovuto all’aumentare del divario temporale tra dati di training e dati di test.

Per l’**adaptive overfitting**, il grafico a destra mostra una retta di regressione rossa con pendenza maggiore di uno. Ciò significa che ogni unità di miglioramento su CoNLL‑2003 si traduce in più di un'unità di miglioramento su CoNLL++. Non osserviamo quindi ritorni decrescenti; quindi l’overfitting adattivo in questo caso non è presente.

Per il **temporal drift**, abbiamo eseguito esperimenti di ri‑addestramento o di continuazione del pre‑training su modelli con dati più recenti. I risultati mostrano che la performance diminuisce con l’aumentare del divario temporale, confermando l’ipotesi che la principale causa del calo è il temporal drift.

### Conclusioni

Per ottenere una buona generalizzazione servono:

- **Un’architettura di modello più adatta** (ad esempio, Transformer);
- **Un modello più grande**;
- **Un maggior numero di esempi di fine‑tuning**.

Questi tre fattori devono essere considerati insieme; non basta ottimizzare uno solo. Inoltre, abbiamo scoperto che il calo di performance è principalmente dovuto al temporal drift, e non all’overfitting adattivo, nonostante CoNLL‑2003 sia stato utilizzato per oltre vent'anni.

Quindi, tornando alla domanda che avevamo posto nel titolo del nostro articolo: **“Do CoNLL‑2003 taggers still work in 2023?”** La risposta è un chiaro **sì**. Speriamo che il nostro lavoro stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli NER.

Infine, vi invitiamo a consultare il nostro articolo, il dataset CoNLL++ e, se avete domande, non esitate a contattarci. Grazie mille!</sample>
    <sample id="114">**Abstract**

Large‑scale language models (LLMs) typically contain billions of parameters, leading to prohibitive training costs, deployment challenges, and token‑hungry requirements. A notable source of redundancy lies in the multi‑head attention (MHA) mechanism, where many heads capture overlapping subspaces. We propose a two‑stage “Grouped Head Attention” framework to identify and prune redundant MHA heads while preserving or improving performance.  

**Stage 1 – Group‑Constrained Training (GCT)**: Attention heads are partitioned into groups. Within each group we enforce *homogenization* (encouraging intra‑group similarity) and *diversification* (promoting inter‑group dissimilarity) via a joint loss comprising a K‑means‑style clustering term and a contrastive separation term. This yields compact, semantically distinct head groups.  

**Stage 2 – Voting‑to‑Stay (VTS)**: After GCT, each training batch serves as a voter; heads are evaluated on a validation metric, and votes are accumulated. Heads receiving insufficient votes are pruned, leaving a single representative per group.  

Evaluated on WMT14 En‑De, CNN/DailyMail summarization, and WikiText‑103 language modeling, our GHT and GHT‑PS variants achieve BLEU gains of 3.8–4.4 %, ROUGE‑L improvements of 6.7–7 %, and perplexity reductions of 2.8–2.9 % while compressing 16.9–32.1 % of parameters. A lightweight “LITE” variant with 90 % parameter pruning yields a 62 % inference speedup and 80 % FLOP reduction, matching baseline performance.  

Future work will explore task‑specific pruning guided by the Lottery Ticket Hypothesis to further trim LLMs to application‑specific footprints.</sample>
    <sample id="115">L’approccio utilizza un “chunk” di **λ frame audio** (cioè un piccolo segmento, di solito poche centinaia di frame).</sample>
    <sample id="116">Nel caso di Servin e Kea, le conoscenze specifiche dell’entità richieste sono:

- **Servin è un giudice**  
- **Kea è un panettiere**  

Questa informazione sull’identità e la professione di ciascun nome è ciò che permette al modello di capire che “he” si riferisce a Servin.</sample>
    <sample id="117">Il fattore più importante è la **qualità dell’esempio**: scegliere esempi di traduzione di alta qualità è più determinante della somiglianza con la frase sorgente.</sample>
    <sample id="118">**Abstract**

Il nostro lavoro affronta il problema del *code‑switching*, la mescolanza simultanea di due lingue all’interno di una stessa frase, un fenomeno particolarmente diffuso in contesti multilingue come l’India.  I modelli pre‑addestrati multilingue, quali mBERT e XLM‑R, mostrano prestazioni insoddisfacenti in compiti NLP code‑switched (domande‑risposte, sentiment analysis).  Per colmare questa lacuna proponiamo due nuove tecniche di *masked language modeling* (MLM) ottimizzate per il code‑switching: **SwitchMLM** e **FrequencyMLM**.  

*SwitchMLM* definisce i *switch‑points* (transizioni linguistiche) come le uniche posizioni mascherabili, richiedendo però etichette di lingua (LID).  *FrequencyMLM* sostituisce l’etichettatura LID con una procedura basata sulla frequenza di occorrenza in corpora monolingue, calcolando il log‑probability negativo delle parole per assegnare le etichette.  

In aggiunta all’obiettivo di pre‑addestramento, introduciamo modifiche architetturali: collegamenti residuali tra strati intermedi che contengono maggior informazione sui switch‑points e una perdita ausiliaria LID applicata a tali strati.  Le prove di “probing” (linear e conditional) confermano che questi interventi aumentano la capacità delle rappresentazioni finali di catturare i switch‑points.  

I risultati empirici su compiti di sentiment analysis mostrano che la combinazione di Switch / FrequencyMLM con ResBERT e perdita ausiliaria sovrasta i modelli di base su tutte le coppie di lingue studiate.  In sintesi, la nostra proposta di MLM mirata e le architetture adattate migliorano significativamente le prestazioni per il NLP code‑switched.</sample>
    <sample id="119">L’articolo esamina principalmente i modelli di LLM più diffusi: **GPT‑4 e l’intera serie GPT**, **BART (e le sue varianti)** e **RoBERTa** (con i suoi checkpoint). In particolare, i test estesi includono:  

- Prompt‑based valutazioni su GPT‑4, GPT‑3.5, GPT‑3, ecc.  
- Confronto tra GPT‑series e BART‑series.  
- Pre‑addestramento ulteriormente su corpora partizani su **RoBERTa**.  

Questi sono i modelli centrali nei loro esperimenti.</sample>
    <sample id="120">The EDAtt strategy looks only at the cross‑attention scores produced by the decoder’s **single cross‑attention layer** (usually the final decoder layer). It does not combine or average the attention maps from multiple layers.</sample>
    <sample id="121">Esempi di inferenza **diretta** sono le referenze che indicano esplicitamente l’entità da scegliere, come:

- **Nome dell’entità**: “Easy on Me” o “I Gotta Feeling”.
- **Posizione nell’elenco**: “il primo” o “il secondo”.</sample>
    <sample id="122">Istituto di ricerca: **Università di Fudan (Fudan University)** – affiliazione di Siyu Yuan.</sample>
    <sample id="123">**Abstract**

Large language models (LLMs) have shown that instruction‑tuning can enable zero‑shot performance on unseen tasks, yet multi‑modal instruction tuning remains underexplored due to the lack of large‑scale datasets. We introduce **MultiInstruct**, the first benchmark for multi‑modal instruction tuning, comprising 62 diverse tasks drawn from 21 open‑source datasets and grouped into ten broad categories (e.g., visual question answering, captioning, grounding). Each task is paired with five expert‑written instruction templates, and all modalities (text, image, bounding boxes) are encoded into a unified token space following the OFA framework. For training, we sample 10,000 instances from 53 tasks across nine groups; for evaluation we reserve the entire common‑sense reasoning group, five additional vision‑questioning and miscellaneous tasks, and 20 unseen natural‑language tasks. We fine‑tune the large OFA model using mixed‑task instruction‑tuning, evaluating each task with all five instruction variants. Metrics include accuracy for classification, ROUGE‑L for generation, and a novel **sensitivity** score measuring consistency across instruction wording. Results show that instruction‑tuning substantially boosts OFA’s performance on both seen and unseen multi‑modal tasks while reducing sensitivity. Moreover, transfer learning from natural‑language instruction datasets further improves zero‑shot performance and stability. We also demonstrate that using multiple instruction templates during training yields higher gains and lower sensitivity than a single template. Finally, we outline plans to expand MultiInstruct with ~150 additional vision‑language tasks, which will be publicly released.</sample>
    <sample id="124">**Abstract**  
Il lavoro di Tan Qingyu et al. analizza la capacità di ragionamento temporale dei grandi modelli linguistici (LLM), proponendo un approccio più completo rispetto allo studio precedente che si concentrava principalmente sul livello “time‑to‑event” (L2). Gli autori definiscono tre livelli di ragionamento:  
1) **Time‑to‑Time (L1)** – predizione di unità temporali (anno, mese).  
2) **Time‑to‑Event (L2)** – associazione di eventi a periodi (es. squadra di Messi nel 2010).  
3) **Event‑to‑Event (L3)** – confronto di sequenze di eventi (es. squadra dopo FC Barcelona).  

Per valutare questi livelli è stato creato il dataset **TempReason**, che copre tutte le tipologie, con una gran lunga copertura temporale. Le domande sono generate a partire da Wikidata e Wikipedia e distribuite in tre scenari: Closed‑Book QA, Open‑Book QA (con testo di Wikipedia) e “Reason‑QA” (con conoscenza temporale fornita).  

I risultati mostrano che ChatGPT eccelle su L1 ma degrada notevolmente su predizioni mensili e su L2/L3, evidenziando bias temporali. I modelli T5 fine‑tuned (T5‑SFT) superano le versioni zero‑shot, e l’ulteriore approccio **TempT5** – pre‑training di estrazione di intervalli temporali e reinforcement learning sensibile al tempo – migliora significativamente le prestazioni su Open‑Book e Reason‑QA.  

Il lavoro mette in luce le disparità temporali nei LLM, presenta un benchmark robusto e propone una strategia di addestramento per ridurre tali bias, suggerendo direzioni future per un ragionamento temporale più equilibrato.</sample>
    <sample id="125">L’articolo è stato scritto da **cinque autori**.</sample>
    <sample id="126">No.  In XSemPLR the “Translate‑Test” setup—translating a query with an MT system (e.g., Google Translate) and then feeding the English output to a monolingual semantic‑parser—is treated as one baseline **setting** rather than the de‑facto standard.  Cross‑lingual semantic parsing research typically focuses on direct multilingual models or zero‑/few‑shot transfer without an explicit translation step.</sample>
    <sample id="127">Large language models (LLMs) can perform complex reasoning via chain‑of‑thought (CoT) prompting, but this ability appears only in very large models (e.g., GPT‑3, PaLM). Deploying such models is costly, limiting practical use. We propose a teacher‑student distillation framework that transfers CoT reasoning from a large teacher to a much smaller student. For each benchmark question, we generate a step‑by‑step solution with a zero‑shot CoT prompt to a large LLM. When the teacher’s final answer is correct, the full reasoning trace is formatted as a supervised training example for the student. Moreover, we introduce “Diverse Reasoning,” which samples multiple distinct CoT solutions per question using stochastic temperature sampling, enriching the training signal. We evaluate on 12 reasoning benchmarks, including math, symbolic reasoning, and commonsense tasks. Compared to prompt‑based baselines, our fine‑tuned CoT models achieve markedly higher accuracy, and Diverse Reasoning yields additional gains (e.g., from 33 % to 55 % on Multi‑Arith). Even the smallest student (0.3 B parameters) outperforms vanilla fine‑tuning on most tasks. The approach scales with teacher quality, dataset size, and student capacity, offering a flexible trade‑off between development and inference costs. We release code, data, and pretrained checkpoints, demonstrating that emergent reasoning can be distilled to sub‑billion‑parameter models.</sample>
    <sample id="128">**Abstract**

We introduce KITMUS (Knowledge Integration from Multiple Sources), a diagnostic test suite designed to probe how natural‑language understanding models combine knowledge acquired during pre‑training with information supplied at inference time. Coreference resolution is employed as a probing task, because resolving pronouns often requires both entity‑specific facts (e.g., “Servin is a judge”) and background knowledge (e.g., “judges decide cases in courts”). Three settings capture different patterns of knowledge availability: (i) *Background‑Pretrain* where background facts are expected to be encoded in the model’s parameters, with only entity facts provided at inference; (ii) *Background‑Both* where both background and entity facts are supplied at inference; and (iii) *Background‑Inference* where even background facts are absent from pre‑training and must be supplied at inference (e.g., using a novel occupation “mirituer”).  

We evaluate human annotators and state‑of‑the‑art coreference systems (C2F, BERT4Coref) on KITMUS. Without task‑specific training, models fail to generalize beyond surface cues, achieving near‑random performance. After fine‑tuning on KITMUS, performance improves markedly on the most challenging *Background‑Pretrain* variant, indicating that models can learn to integrate heterogeneous knowledge when explicitly trained. Nonetheless, even the best models struggle with *Background‑Inference* examples, underscoring the difficulty of leveraging backward knowledge that is absent from pre‑training. Our results suggest that coreference resolution systems require dedicated training to reason over multi‑source knowledge, yet current approaches still exhibit limited robustness to inference‑only knowledge integration. The KITMUS dataset, code, and pre‑trained models are publicly available on GitHub.</sample>
    <sample id="129">They used **black women** as a concrete example of a marked group.</sample>
    <sample id="130">Le architetture che **non** mostrano una buona generalizzazione sono quelle **non‑transformer**: i modelli tradizionali a base di RNN/LSTM (es. BiLSTM‑CRF, RNN‑CRF, ecc.) e altri approcci sequenziali di prima generazione. Questi sistemi, rispetto ai transformer, hanno dimostrato prestazioni più basse e una maggiore sensibilità al drift temporale sui dati moderni.</sample>
    <sample id="131">Nel testo non vengono menzionati nomi specifici di set di test. Si fa riferimento soltanto a “clean test sets” (insiemi di test privi di rumore), ma non si elencano dataset concreti.</sample>
    <sample id="132">Due autori sono menzionati: Akshatha e Martin.</sample>
    <sample id="133">L’autore opera con più modalità: oltre al testo impiega immagini, bounding‑box e altre informazioni visive (multi‑modal).</sample>
    <sample id="135">**Abstract (≈200 parole)**  

Il lavoro presenta **ABC‑Eval**, un nuovo approccio dimensionale per la valutazione di sistemi di dialogo conversazionale. L’idea è di ridurre la soggettività delle valutazioni umane annotando esplicitamente se ogni risposta del modello esprime comportamenti specifici (es. irrelevanza, contraddizioni, allucinazioni, violazioni di senso comune, empatia). ABC‑Eval copre una gamma di errori tematici che la letteratura moderna suggerisce influenzare la qualità del dialogo.  

Per testarlo, gli autori hanno valutato quattro modelli di chat all’avanguardia su 100 conversazioni per modello, confrontando ABC‑Eval con tre metodi esistenti: valutazioni Likert a livello di turno, a livello di dialogo e comparazioni di coppia. L’inter‑annotator agreement di ABC‑Eval supera quello dei metodi tradizionali, e le sue metriche predicono meglio la qualità complessiva del dialogo (analisi di regressione lineare). Un’analisi stepwise mostra che l’insieme di metriche ABC‑Eval spiega oltre il 25 % della variabilità della qualità, mentre le metriche Likert spiegherebbero molto meno.  

I risultati rivelano che, anche nei modelli più avanzati, le violazioni di senso comune raggiungono il 20 %, le informazioni irrilevanti il 15 % e le contraddizioni il 10 %. ABC‑Eval fornisce, quindi, valutazioni più affidabili, informative e distintive, permettendo un’analisi più fine delle forze e delle debolezze dei modelli conversazionali.</sample>
    <sample id="136">**Abstract**

Numerical reasoning is a core capability for many real‑world language‑model applications, yet existing benchmarks inadequately capture a model’s mathematical proficiency. We introduce **FERMAT**, a flexible evaluation suite that probes models across three dimensions: number representation, mathematical operations, and training dependency. FERMAT draws from the Illinois and CommonCore curricula, generating diverse arithmetic questions that vary in scale (small, large, decimal) and operation type (single, composite). By altering numeric tokens (e.g., “5.0” vs. “5”) we expose tokenisation and representation weaknesses that conventional accuracy metrics overlook.

We first evaluate several language models (0‑to‑10 B parameters) in a zero‑shot setting, finding that smaller models (&lt; 3 B) perform poorly across all FERMAT axes, while larger models only marginally outperform them. Fine‑tuning on 200 k synthetic examples produced by teacher‑crafted templates improves performance on both the original dataset and across all FERMAT sub‑tasks, though exact memorisation remains limited. Further experiments demonstrate that incorporating diverse linguistic and mathematical templates from GSM8K and AQUA—especially those that blend multiple operations—yields the greatest gains.

Our analyses reveal that (i) traditional accuracy scores mask nuanced deficiencies, (ii) tokenisation of numeric expressions is a key bottleneck, and (iii) enriched training data that spans varied linguistic and numerical patterns is essential for robust reasoning. FERMAT thus provides a more informative, granular alternative to accuracy for assessing and guiding the development of numerical reasoning in large language models.</sample>
    <sample id="137">**Abstract**  
Nell’ambito della progettazione d’interni, l’interfaccia “parlare‑e‑progettare” offre un’opportunità unica per coinvolgere utenti non esperti nella creazione di planimetrie. In questo lavoro presentiamo *Tell2Design*, un dataset di 5 051 planimetrie pubbliche ognuna accompagnata da 200 parole medio di istruzioni testuali, arricchito poi da circa 76 000 frasi generate tramite template. Le istruzioni includono componenti semantiche (tipo e funzione delle stanze), geometriche (forma e dimensioni) e topologiche (relazioni spaziali). Il compito è formulato come generazione sequenza‑a‑sequenza: dal testo di input si predice una sequenza di bounding box che caratterizzano ogni stanza. Per ottenere questo risultato utilizziamo un modello encoder‑decoder basato su T5, addestrato con la tipica loss di language modeling.  

I risultati dimostrano che il modello *T2D* raggiunge un Micro‑IoU di 54 e un Macro‑IoU di 53, superando di gran lunga i baselines di text‑to‑image tradizionali. L’addestramento preliminare con istruzioni artificiali riduce significativamente la gap linguistica, evidenziando la complementarità tra dati sintetici e reali. Inoltre, i test su istruzioni di autori non presenti nel training confermano la capacità del modello di generalizzare a nuovi contesti. In conclusione, *Tell2Design* stabilisce una nuova frontiera per la generazione di design guidata dal linguaggio, ponendo le basi per future ricerche su interfacce conversazionali per la progettazione d’interni.</sample>
    <sample id="138">Secondo gli autori, poco studiata è l’**integrazione della conoscenza proveniente da più fonti** (pre‑training e input di inference) nelle attività di NLU.</sample>
    <sample id="139">I am Ying and my colleague Zhiyang.</sample>
    <sample id="140">Yes. The CoScript dataset was quality‑checked by asking crowd‑sourced workers to review the validation and test sets and correct any incorrect samples.</sample>
    <sample id="141">Ilimiti delle risorse esistenti per la traduzione dipendente dal contesto sono:

- **Copertura limitata**: solo una piccola percentuale delle traduzioni effettivamente dipende dal contesto, quindi le metriche corpus‑level (es. BLEU) non le rilevano.
- **Scarsa generalità**: le valutazioni mirate si concentrano su pochi fenomeni specifici (es. pronomi, coerenza lessicale) e non su un’ampia gamma di discorsi.
- **Restrizioni linguistiche**: le risorse sono disponibili solo per un numero limitato di lingue, spesso perché richiedono conoscenze di dominio e curazione umana.
- **Dipendenza dalla curazione**: la necessità di annotazioni manuali rende le risorse costose e poco scalabili.</sample>
    <sample id="142">Ciao! Parlerò del nostro lavoro su **“Resolving Indirect Referring Expressions for Entity Selection”**, in cui presentiamo il **AltEntities Corpus**. Mi chiamo Javad Hosseini e questa è una collaborazione con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è capire come gli utenti formano il linguaggio quando vogliono fare una scelta. Considera questa domanda alternativa: *“Intendi ‘Easy on Me’ o ‘I Gotta Feeling’?”* Qui un utente vuole selezionare una delle due canzoni. La soluzione più ovvia è usare un riferimento diretto, ad esempio dicendo il nome della canzone “Easy on Me” o la sua posizione, “la prima”. Ma a volte un riferimento indiretto è più appropriato per mantenere una conversazione più naturale. Ciò può succedere quando l’utente non ricorda il nome della canzone, quando le pronunce sono troppo simili e difficili da distinguere, o quando l’utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti: “la più recente” o “la canzone che non è energica”. È un problema importante per i sistemi conversazionali e anche per valutare la comprensione delle entità nei LLM. Non conosciamo un dataset pubblico di grande scala per questo compito, quindi lo raccogliamo tramite annotazione crowdsourced. Il nostro dataset copre tre domini diversi: musica, libri e ricette. La metodologia di raccolta dei dati enfatizza l’informalità tramite un setup di completamento di cartoni. Il cartone ha tre fumetti. Nel primo fumetto, Bob dice: “Ricordi quella canzone che stavamo ascoltando ieri?” E con questo, Bob imposta il contesto del dialogo. Nel secondo fumetto, Alice dice: “Intendi ‘Easy on Me’ o ‘I Gotta Feeling’?” Questa è la domanda alternativa. Nel terzo fumetto, Bob usa un riferimento indiretto per selezionare una delle entità, per esempio, “la più recente”. Forniamo i primi due fumetti automaticamente, ma il terzo è riempito dall’annotatore. Il primo fumetto è scelto da alcuni prompt manuali per dominio. Il secondo, la domanda alternativa, è generato con un semplice template: *Do you mean A or B?*, dove A e B sono campioni da Wikipedia. Ecco i diversi metodi di sampling che abbiamo usato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro e di solito è più difficile fare la distinzione. Il primo è uniformemente a caso. Il secondo è quando le entità hanno titoli simili, per esempio due libri con il nome “The Return”. Il terzo è quando hanno descrizioni simili su Wikipedia. E infine quando hanno info box o attributi simili su Wikipedia. Per esempio, lo stesso genere o lo stesso artista per una canzone. Quando mostriamo questa domanda alternativa agli annotatori, conoscono il nome di queste entità, ma non necessariamente ne conoscono i dettagli. Quindi, ciò che facciamo è mostrare loro alcune informazioni di base su entrambe le entità. Per le canzoni mostriamo semplicemente un link di ricerca Google per ogni canzone e chiediamo agli annotatori di ascoltare almeno una parte di ogni canzone e di leggere qualcosa su di esse. Ecco, ad esempio, il risultato di ricerca Google per la canzone “Easy on Me”. Per le ricette e i libri, mostriamo del testo di background da Wikipedia. Per le ricette, mostriamo anche le loro immagini, sempre da Wikipedia, in modo che gli annotatori sappiano come appaiono. Poi chiediamo agli annotatori di scegliere una delle entità, per esempio, la prima, e descriverla usando tre o cinque espressioni di riferimento indiretto. Ad esempio, “quella con il pianoforte”, “non quella con il ragazzo di 12 anni”, “quella fittizia”, “che viene dall'Azerbaigian”, ecc. Il **AltEntities Corpus** contiene 6.000 domande alternative in tre domini e 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL sono riassunti qui sotto. Se il modello linguistico ha accesso allo stesso background knowledge esatto degli annotatori, l’accuratezza è davvero alta, intorno al 92‑95%. Ma questo non è realistico. Se il modello linguistico ha accesso a qualche knowledge parzialmente sovrapposta, l’accuratezza è tra l’82 e il 87%, che è più realistico. Ad esempio, quando il modello linguistico recupera la knowledge di background. Se il modello linguistico ha accesso solo ai nomi delle entità, l’accuratezza è solo il 60%, quindi c'è molto margine di miglioramento. Abbiamo anche mostrato che i modelli sono generalizzabili tra domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L’approccio EDAtt viene confrontato con le politiche di traduzione simultanea più diffuse, cioè:

1. **Wait‑k** – la strategia classica in cui il decoder attende i primi *k* frame audio prima di iniziare a generare.
2. **Local Agreement** – un metodo che si basa su una soglia di allineamento locale tra l’audio e la traduzione.
3. **La migliore architettura di SimulST dedicata** – l’ultimo modello “state‑of‑the‑art” progettato specificamente per la traduzione simultanea.

In pratica, EDAtt è misurato contro Wait‑k, Local Agreement e il modello di SimulST più avanzato disponibile al momento.</sample>
    <sample id="144">Le affiliazioni degli autori non sono esplicitamente riportate nel testo fornito. Tuttavia, l’articolo fa riferimento al **Nantes University Hospital (CHU Nantes)** e ai dati provenienti dal suo data‑warehouse, suggerendo che gli autori siano collegati a quell’istituzione (e probabilmente all’Università di Nantes o a laboratori di ricerca afferenti).</sample>
    <sample id="145">La relatrice è **Jenny**.</sample>
    <sample id="146">**Abstract (≈ 200 parole)**  

Il talk di Yicheng presenta un’analisi sistematica del problema dell’*omission* nella sintesi di dialoghi. A differenza delle tradizionali tecniche di riassunto testuale, i sistemi di sintesi dialogica, sebbene alimentati da modelli linguistici pre‑addestrati di grande scala, soffrono di errori di fact-checking e, soprattutto, di omissioni: fatti e informazioni cruciali presenti nel dialogo originale ma assenti nel riassunto generato. L’autore mostra che, attraverso un’analisi quantitativa su cinque domini e sei modelli all’avanguardia, circa il 70 % dei riassunti presenta omissioni, con una distribuzione casuale delle parti di dialogo omesse, evidenziando l’impossibilità attuale di identificare automaticamente le informazioni chiave.  

Per studiare questo fenomeno è stato creato il dataset **OLDS**, che fornisce etichette di omissione di alta qualità per 15 benchmark di sintesi dialogica. Le etichette sono generate automaticamente da più modelli di sintesi astratta e verificate tramite annotazioni umane.  

Come task di riferimento, l’autore propone la *omission detection* a livello di frase di dialogo, valutata con Precision, Recall, F1 e un nuovo metriche WR (recall a livello di parola). I tre baseline (pair‑wise classification, sequence labeling, pointer network) raggiungono un F1 intorno al 50 %, dimostrando la difficoltà del compito.  

Infine, viene mostrato un metodo di post‑editing che concatena il riassunto candidato con le frasi omesse per ottenere un riassunto raffinato, con miglioramenti significativi delle metriche ROUGE, confermando il valore pratico della rilevazione delle omissioni.</sample>
    <sample id="147">Tre autori sono coinvolti nell’articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell’Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente il paper “Attention as a Guide for Simultaneous Speech Translation”, lavoro congiunto con Matteo Negri e Marco Turchi.  

**Cos’è la traduzione simultanea dello speech (SimulST)?**  
La SimulST è il processo di traduzione in tempo reale di una lingua parlata in testo in un’altra lingua, permettendo così la comunicazione tra parlanti di lingue diverse.  

**Quali sono i problemi dei modelli SimulST attuali?**  
- I modelli spesso richiedono architetture specifiche, con componenti aggiuntive da ottimizzare.  
- I procedimenti di addestramento diventano lunghi e complessi, coinvolgendo più obiettivi di ottimizzazione.  
- È necessario addestrare e mantenere diversi modelli per ottenere differenti livelli di latenza (es. un modello con latenza media di un secondo, un altro con due secondi, ecc.).  

**Qual è la nostra soluzione?**  
1. Utilizzare modelli di ST offline già esistenti senza ritraining o architetture specifiche per la SimulST.  
2. Usare un solo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici.  
3. Sfruttare la conoscenza già acquisita dal modello tramite il meccanismo di attenzione tra audio e testo, cioè l’attenzione incrociata (cross‑attention).  

Proponiamo **EDAtt** (Encoder‑Decoder Attention), una strategia che decide se emettere o meno una traduzione parziale in base a dove punta l’attenzione. Una parola viene emessa se l’attenzione non è concentrata, ovvero se la somma delle sue attese verso gli ultimi λ frame audio è inferiore a una soglia α, indicando che le informazioni ricevute sono sufficientemente stabili.  

Esempio: riceviamo un chunk audio contenente “I’m going to talk about…”. Il nostro modello predice la traduzione in tedesco e guardiamo i pesi di cross‑attention: le prime due parole puntano ai primi frame audio ricevuti, mentre l’ultima parola punta agli ultimi λ frame. Pertanto le prime due parole vengono emesse (perché la somma di attenzione è sopra α), mentre l’ultima resta in attesa di un ulteriore chunk. Con un nuovo chunk arrivano altre tre parole; i pesi di attenzione di queste non puntano più agli ultimi λ frame, quindi le tre parole vengono emesse.  

**Risultati principali di EDAtt**  
Grazie a grafici con **BLEU** (qualità della traduzione) sull’asse y e **average lagging** (latency) sull’asse x, includendo anche la **computational‑aware average lagging** (che tiene conto del tempo di calcolo reale).  
- Vogliamo curve il più alto possibile (massima qualità) e spostate a sinistra (minore latenza).  
- Confrontiamo con le strategie popolari applicate a modelli offline: **Wait‑k** e **Local Agreement**.  
- Confrontiamo anche con l’architettura di punta specificamente ottimizzata per la pre‑traduzione simultanea.  

I risultati sul set di dati tedesco mostrano che EDAtt supera tutte le strategie applicate ai modelli offline: le curve risultano spostate a sinistra. Inoltre, se consideriamo il tempo effettivamente trascorso o il tempo computazionale, EDAtt è la strategia più rapida.  

Per scoprire ulteriori risultati, leggete il nostro paper.  
Abbiamo inoltre reso open‑source il codice, i modelli e gli output simultanei per facilitare la riproducibilità del nostro lavoro.  

Grazie per l’attenzione.</sample>
    <sample id="149">Sì, il dataset CoNLL++ è disponibile pubblicamente.</sample>
    <sample id="150">**Abstract – MeetingQA: Extractive Question‑Answering on Meeting Transcripts**  
In questo lavoro presentiamo MeetingQA, un nuovo dataset di domande‑risposte estrattive basato su trascrizioni di riunioni reali. Le domande provengono da partecipanti a meeting multi‑partecipanti (AMI corpus, 100 h di trascrizioni manuali) e sono caratterizzate da lunghezza media di 12 parole, domande aperte, con richieste di discussione dettagliata. L’annotazione delle risposte è stata svolta da esperti, con un’alta concordanza (Krippendorff’s α = 0.73). Il dataset contiene 7.7 k domande suddivise in train, dev e test; il 30 % è non rispondibile, il 40 % richiede più segmenti (multi‑span) e il 48 % coinvolge più oratori. La maggior parte delle domande è di tipo sì/no o ricerca opinione, mentre il 20 % è retorico. Le risposte media 35 parole. La performance umana sul test (F1 = 84.6) evidenzia l’attinenza del compito.  

Abbiamo sperimentato modelli “short‑context” con recupero di contesto, “single‑span” (start‑end) e “multi‑span” (token‑classification). In regime fine‑tuned, la differenza rispetto all’umano è di ~25 punti F1, con i modelli short‑context (RoBERTa) leggermente superiori a Longformer, e le versioni multi‑span comparabili. In zero‑shot, la brecha è di ~50 punti, ma l’augmentazione con dati silver da MediaSum migliora significativamente i risultati; i modelli FLAN‑T5 raggiungono performance paragonabili.  

L’analisi degli errori mostra difficoltà nella rilevazione di domande retoriche, nella selezione di frasi irrilevanti (single‑span) e nell’identificazione dell’oratore responsabile, soprattutto in zero‑shot. MeetingQA rappresenta quindi un benchmark sfidante per la QA su documenti lunghi e discusionali.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e con il mio collega Zhiyang presenteremo i nostri risultati su **MultiInstruct**, un approccio per migliorare l’apprendimento zero‑shot multi‑modal tramite l’“instruction tuning”.  

Con i progressi dei grandi modelli linguistici, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento che permettono di riutilizzare modelli linguistici pre‑addestrati in modo efficiente sia in termini di parametri che di dati. Recentemente, numerosi studi hanno dimostrato che l’“instruction tuning” consente ai modelli linguistici di eseguire compiti non visti in modalità zero‑shot semplicemente seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull’“instruction tuning” si è concentrata sull’ottimizzazione delle prestazioni zero‑shot di compiti esclusivamente linguistici, lasciando i compiti di visione e multi‑modalità quasi trascurati.  

Perciò, in questo lavoro vogliamo indagare se l’“instruction tuning” di modelli multi‑modal pre‑addestrati possa effettivamente migliorare la generalizzazione verso compiti multi‑modalità non visti. Inoltre, al momento della nostra ricerca, abbiamo scoperto una notevole discrepanza nella disponibilità di dataset di istruzioni tra NLP e multi‑modalità: esistono più di 1600 compiti di istruzioni solo per il linguaggio, ma non esiste un dataset di istruzioni multi‑modalità di larga scala pubblicamente disponibile. Questo ci ha motivato a costruire un dataset di “instruction tuning” multi‑modal.  

Qui presentiamo **MultiInstruct**, il primo benchmark di dataset di “instruction tuning” multi‑modal che comprende 62 compiti multi‑modal diversificati, distribuiti in 10 categorie ampie. Ogni compito è derivato da 21 dataset open‑source esistenti e viene accompagnato da cinque istruzioni scritte da esperti.  

Per valutare l’“instruction tuning” multi‑modal sul nostro dataset, utilizziamo **OFA**, un modello pre‑addestrato multi‑modal unificato. OFA utilizza un vocabolario unificato per il linguaggio, i token di immagine e le coordinate di un bounding box. Mostriamo alcuni esempi di istanze dal nostro dataset MultiInstruct, che unificano il trattamento di vari tipi di input e output. Seguendo il metodo di OFA, formuliamo tutti i compiti in un formato sequence‑to‑sequence unificato, in cui testo di input, immagini, istruzioni e bounding box sono rappresentati nello stesso spazio di token.  

**Ecco come funziona l’“instruction tuning” multi‑modal**  

- Per il dataset di addestramento, usiamo 53 compiti da 9 gruppi, campionando 10.000 istanze per compito.  
- Per il test, riserviamo l’intero gruppo “common sense reasoning” e aggiungiamo 5 compiti extra dai gruppi “VQ” e “Miscellaneous”. Usiamo tutte le istanze del test per ogni compito.  
- Inoltre, campioniamo casualmente 20 compiti dal test di “natural instructions” come compiti NLP non visti.  

Partiamo da OFA Large pre‑addestrato. Durante l’addestramento, mescoliamo tutte le istanze di tutti i compiti: ogni istanza è combinata casualmente con una delle sue cinque istruzioni template. Durante il test, per ogni compito eseguiamo 5 esperimenti, valutando il modello con ognuna delle cinque istruzioni. In ciascun esperimento riportiamo il valore minimo, massimo e la deviazione standard delle prestazioni su tutti i 5 esperimenti.  

- Se il compito è una classificazione multi‑modal, riportiamo l’accuratezza.  
- Se è un compito di generazione multi‑modal, usiamo ROUGE‑L.  
- Per i compiti NLP usiamo anch’essi ROUGE‑L.  

Per la prima volta introduciamo un ulteriore metriche chiamata **sensibilità**: misura la capacità del modello di produrre output coerenti per lo stesso compito indipendentemente da leggere variazioni nel wording dell’istruzione.  

**Risultati principali**  

- L’“instruction tuning” migliora significativamente le prestazioni di OFA sui compiti multi‑modal visti.  
- L’apprendimento transfer dal dataset di istruzioni naturali aiuta anche l’“instruction tuning”.  
- Man mano che aumentiamo il numero di compiti, il modello raggiunge prestazioni migliori e una sensibilità più bassa.  

Abbiamo condotto un esperimento confrontando l’uso di una sola istruzione vs. cinque istruzioni: l’uso di più istruzioni migliora le prestazioni complessive e riduce sensibilmente la sensibilità. Questo evidenzia l’effetto delle diverse strategie di fine‑tuning sulla sensibilità del modello.  

Il transfer learning da dataset di istruzioni naturali permette al modello di ottenere una sensibilità molto migliore rispetto all’originale OFA e di ottenere prestazioni notevolmente superiori sul dataset di istruzioni naturali.  

In sintesi, proponiamo il primo dataset di “instruction tuning” multi‑modal su larga scala, che migliora sensibilmente le capacità di OFA. Abbiamo esplorato diverse tecniche di transfer learning e ne abbiamo mostrato i benefici. La nuova metrica “sensibilità” ci ha permesso di valutare la robustezza del modello rispetto a variazioni di formulazione delle istruzioni.  

**Progetti futuri**: stiamo raccogliendo un dataset di “instruction tuning” multi‑modal ancora più grande, con circa 150 compiti aggiuntivi di vision‑language, che verrà rilasciato pubblicamente.  

Ecco il QR code per il dataset e il modello. Grazie.</sample>
    <sample id="152">**Abstract**  
We introduce a suite of large‑language models specifically tuned for classical philology, encompassing both encoder‑only and encoder‑decoder architectures and spanning Ancient Greek, Latin, and English. The monolingual models—GreBERTa (a RoBERTa variant) and GreTa (a T5‑based encoder‑decoder)—are trained from scratch on a newly curated Ancient Greek corpus. This corpus is assembled by mining the Internet Archive: Greek texts are identified via OCR‑misspellings of stop‑words (e.g., “γάρ” → “yap”), then re‑OCRed in Greek mode and rigorously cleaned. For multilingual models (PhilBERTa and PhilTa) we augment the Greek data with Latin (Corpus Corporum) and English antiquity‑related texts.  

We benchmark the models on part‑of‑speech tagging, dependency parsing, and lemmatization using the Universal Dependencies treebanks for Greek and the EvaLatina 2022 dataset for Latin. All models surpass the prior state‑of‑the‑art, with GreTa’s encoder‑decoder architecture yielding a 5‑point gain in Greek lemmatization. An ablation study shows that a T5 encoder alone initially performs poorly but converges to encoder‑only performance after sufficient training, underscoring architectural differences.  

Additional probing tasks assess semantic and world knowledge (synonym/antonym discrimination, hero‑god relation extraction). Multilingual models do not significantly outperform their monolingual counterparts, suggesting limited transfer benefit across these ancient languages. Our work demonstrates that native tokenizers, language‑specific pre‑training, and encoder‑decoder designs can markedly improve NLP for classical texts.</sample>
    <sample id="153">**Abstract**

Text‑to‑image models often fail to produce faithful visualizations when user prompts contain linguistic ambiguities. We investigate this problem by curating a benchmark dataset derived from the LAVA corpus that systematically covers a range of ambiguity types (e.g., spatial, referential, and compositional). To resolve ambiguities we propose a two‑stage disambiguation framework that leverages a large language model (LLM). In the first variant, the LLM generates clarifying questions in an in‑context learning setting; the user’s answer is concatenated with the original prompt to form a disambiguated version. In the second variant, the LLM instead proposes multiple plausible visual interpretations; the user selects the intended one, again yielding a refined prompt. 

We evaluate the impact of disambiguation on image fidelity by feeding both the ambiguous and disambiguated prompts to a state‑of‑the‑art text‑to‑image model. Generated images are assessed automatically with a Visual Question Answering (VQA) system that receives the user’s intention expressed as a question; a “yes” answer indicates that the image satisfies the intention. Our experiments reveal that disambiguation improves faithful generation overall, though effectiveness varies across ambiguity categories. Moreover, the VQA‑based metric correlates strongly with human judgments, demonstrating its utility as a scalable evaluation tool. 

This work contributes a new benchmark for prompt ambiguity, a practical disambiguation pipeline, and an automatic evaluation framework that together advance responsible development of image generation systems.</sample>
    <sample id="154">**Affiliations**

- **Sara Papi** – University of Trento &amp; Fondazione Bruno Kessler  
- **Matteo Negri** – University of Trento  
- **Marco Turchi** – University of Trento</sample>
    <sample id="155">Il relatore è **Javad Hosseini**.</sample>
    <sample id="157">**Abstract**

Dialogue summarization seeks to distill the salient information from multi‑party, semi‑structured conversations into concise summaries. Existing approaches rely on pre‑computed static dialogue graphs (e.g., discourse parsing, dialogue state tracking), yet these are brittle to noisy linguistic tools and cannot adapt to the downstream summarization objective. We propose **SDDS** (Static‑Dynamic Structure‑Fusion Graph), a unified framework that jointly learns static and dynamic dialogue structures. First, an utterance encoder maps each utterance into a dense vector. We construct four heuristic static graphs: (1) a discourse‑parse graph, (2) a key‑co‑occurrence graph, (3) a speaker‑interaction graph based on sliding‑window speaker frequencies, and (4) a positional graph encoding relative utterance distances. The adjacency matrices of these graphs are fused through a lightweight 1×1 convolution to form a composite static graph. Second, a dynamic graph is learned via multi‑head self‑attention over the utterance representations, yielding data‑driven inter‑utterance relations. The static and dynamic graphs are merged into a unified adjacency matrix, which is incorporated into the decoder through a dual cross‑attention mechanism that augments the standard self‑attention with graph‑aware updates. Finally, a pre‑trained language model generates the summary conditioned on this enriched representation. Experiments demonstrate that SDDS outperforms state‑of‑the‑art methods on benchmark dialogue summarization datasets, and code and data are publicly available on GitHub.</sample>
    <sample id="158">**Abstract**

Coreference resolution in long documents is challenging because entities can be mentioned sparsely across many sections, leading to quadratic‑time pairwise enumeration in conventional models. Recent cache‑based methods mitigate this by storing a fixed‑size buffer of entity representations, but they typically employ a single eviction policy (e.g., Least‑Recently‑Used, LRU). In documents with frequent topic shifts, LRU causes many cache misses, especially for high‑frequency entities whose mentions are widely distributed. 

To address this, we propose a **Dual‑Cache** architecture that separates entity storage into a **local cache** and a **global cache**. The local cache, governed by LRU, captures short‑lived, context‑specific entities. The global cache, governed by Least‑Frequently‑Used (LFU) eviction, retains high‑frequency entities that recur across the entire document. During linear left‑to‑right processing, each new mention is first classified as either a new entity or an update to an existing cache entry; its frequency is then updated, determining whether it belongs in the global or local cache. When a cache is full, the appropriate eviction rule is applied.

We evaluate the dual cache on four public coreference benchmarks (LitBank, OntoNotes, WikiCoref, and a custom 30,000‑word book). Across all datasets, the dual cache outperforms single‑cache baselines and even models with unbounded memory when training data is available. Without training data, the unbounded model slightly edges out, yet the dual cache remains markedly faster. Importantly, the dual cache reduces cache misses by a large margin, especially in book‑level documents, and delivers the best performance‑to‑cost ratio among cache‑based approaches. The results demonstrate that a two‑tiered cache with complementary eviction policies is an effective strategy for scalable, high‑quality coreference resolution in long documents.</sample>
    <sample id="159">Ciao a tutti. Sono **Koustav Sinha** e sono lieto di darvi il benvenuto al nostro talk sull’articolo presentato a **ACL 2023**.  
I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo lavoro è nato in collaborazione con **John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy** e **Adina Williams**. In questo studio riconsideriamo i paradigmi delle coppie minime (minimal pair paradigms, MPP).

Il paradigma delle coppie minime valuta fondamentalmente i modelli linguistici in base ai giudizi di accettabilità. Tali giudizi possono includere la grammaticalità (ad esempio BLiMP, SyntaxGym) o l’accettabilità in termini di stereotipi (come i CrowS pairs).  
Nel paradigma tradizionale si mostra al modello una frase accettabile o grammaticalmente corretta e poi una frase inaccettabile o scorretta; l’aspettativa è che il modello assegni una probabilità maggiore alla frase accettabile.

Il pipeline MPP attuale non permette di valutare l’accettabilità di un modello verso frasi più lunghe. Oggi i grandi modelli linguistici gestiscono finestre di contesto sempre più estese, quindi è cruciale valutare l’accettabilità lungo l’intera finestra di contesto. Il nostro obiettivo è riconsiderare il pipeline MPP chiedendo al modello di valutare l’accettabilità su sequenze sempre più lunghe.

**Come procediamo**  
Per simulare sequenze più lunghe riconsideriamo i set di dati stessi e ricreiamo frasi scegliendo forme accettabili o inaccettabili da quegli insiemi.  
Ad esempio, prendiamo una tipica coppia di grammaticalità dal set BLiMP relativo al caso *Adjunct Island*. Estraiamo frasi grammaticali da *Adjunct Island* e le aggiungiamo come prefisso sia alla query accettabile sia a quella inaccettabile.  
Facciamo lo stesso con frasi inaccettabili dello stesso matching; ciò può essere usato per testare l’accettabilità del modello. Possiamo anche scegliere frasi da un sottoinsieme diverso o da un set di dati diverso—questo è quello che chiamiamo scenario di *mismatch*.  
Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia, per verificare se i giudizi di accettabilità del modello sono influenzati da un contesto irrilevante rispetto alla frase in esame.

**Risultati**  
1. **Wikipedia (contesto irrilevante)**: i giudizi MPP risultano in gran parte robusti per lunghezze di contesto arbitrarie. Abbiamo aumentato la lunghezza del contesto fino a 1024 token per sfruttare al massimo i modelli OPT e GPT‑2; nella linea puntata arancione vediamo che i giudizi MPP rimangono relativamente stabili.  

2. **Stesso set di dati (contesto simile)**: quando scegliamo frasi dallo stesso set BLiMP o SyntaxGym, i giudizi MPP aumentano o diminuiscono in modo significativo a seconda che il prefisso sia accettabile o inaccettabile. Quando corrispondiamo la struttura (stessi fenomeni in BLiMP/SyntaxGym), osserviamo un enorme incremento o una drastica diminuzione del giudizio MPP, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto cresce con la lunghezza del contesto e potrebbe influenzare i modelli linguistici più recenti che dispongono di finestre di contesto più grandi.  

3. **Perturbazioni**: abbiamo eseguito una serie di analisi perturbando le frasi di input, cercando di preservare la struttura rilevante ma aggiungendo rumore. Dopo diverse perturbazioni, nessuno di questi rumori cambia significativamente il giudizio MPP del modello.  
   - Perturbando frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni.  
   - Perturbando frasi nel dominio inaccettabile, osserviamo una diminuzione simile dei giudizi MPP.  

**Conclusioni**  
I principali risultati del nostro lavoro sono:  
- I modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi.  
- La valutazione MPP, come la facciamo attualmente con input di singola frase breve, potrebbe non cogliere pienamente la conoscenza astratta dei modelli lungo l’intera finestra di contesto.  

Per ulteriori dettagli sugli esperimenti, vi invitiamo a leggere il nostro articolo. Grazie per l’attenzione!</sample>
    <sample id="160">Il primo passaggio mappa ogni token di input a un **multiset (insieme con ripetizioni) di token che appariranno nell’output**.</sample>
    <sample id="161">55 000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è **MASSalign**.</sample>
    <sample id="164">Il vantaggio principale è che le etichette “deboli” (regole euristiche, knowledge‑base, crowdsourcing di bassa qualità, ecc.) sono molto più economiche e veloci da ottenere rispetto alle annotazioni umane. In questo modo si può raccogliere una grande quantità di dati a basso costo, anche se rumorosi, e addestrare modelli su di essi.</sample>
    <sample id="165">**Abstract (≈200 parole)**  

Questo lavoro propone un approccio *unsupervised* per il ragionamento abductivo basato su spiegazioni mutuamente esclusive. Il problema è modellato come una situazione chiusa: a partire da un contesto \(X\) (es. “Emily era bloccata nel traffico”) e da un risultato \(Y\) (es. “Emily è arrivata al volo”), viene fornito un insieme di possibili spiegazioni \(Z\). L’obiettivo è selezionare un sottoinsieme plausibile di spiegazioni senza richiedere etichette soggettive.  

La soluzione, denominata **LiPoR** (Likelihood Learning with Posterior Regularization), tratta le spiegazioni come variabili latenti e massimizza la verosimiglianza marginale \(\mathcal{L} = \log P(Y|X)\), che non dipende dall’etichettatura. Per guidare la selezione verso spiegazioni coerenti, viene introdotto un regolarizzatore \(\Omega\) che sfrutta l’esclusività mutuamente delle spiegazioni. In pratica, \(\Omega\) valuta l’entropia della distribuzione posteriore \(P(Z|X,Y)\): se l’entropia supera \(\log M\) (dove \(M\) è il numero di spiegazioni plausibili), l’algoritmo riduce l’entropia, premiando così la scelta di un sottoinsieme ristretto di spiegazioni.  

I risultati su AlphaNLI, il dataset più usato per il ragionamento abductivo, mostrano che LiPoR supera tutte le baselines zero‑shot e l’approccio unsupervised precedente, ottenendo un incremento di precisione di oltre 4 punti assoluti, anche rispetto a GPT‑3. Questo dimostra che la combinazione di una massimizzazione della verosimiglianza e di un regolarizzatore basato sull’esclusività è efficace per l’addestramento non supervisionato di sistemi di ragionamento commonsense.</sample>
    <sample id="166">**Abstract**

The paper introduces **NDCR (Neural Divide‑and‑Conquer Reasoning)**, a framework designed to tackle image retrieval from linguistically complex text—a task that poses significant challenges due to highly similar images and long, intricate descriptions. Standard visual‑language models excel at image‑sentence retrieval but falter when confronted with such complexity. NDCR draws inspiration from the Divide‑and‑Conquer strategy and Dual‑Process Theory, combining an analogical “System 1” with a logical “System 2” to handle the problem more effectively.

The system comprises three core modules:

1. **Proposition Generator** – decomposes a complex query into a set of simple propositions and uses a BART decoder to generate corresponding natural‑language sentences.
2. **Visual‑Linguistic Interactor (System 1)** – performs analogical reasoning between each proposition and the image set, producing matching scores and intermediate reasoning states.
3. **Neural‑Symbolic Reasoner (System 2)** – integrates the outputs of System 1 via a negation executor and a conjunction operation, synthesizing positive and negative reasoning states into a final inference.

The authors conduct extensive experiments demonstrating that NDCR surpasses state‑of‑the‑art baselines. Ablation studies confirm the contribution of each module, and qualitative case studies illustrate the interpretable reasoning paths produced by the framework. The paper concludes by highlighting the promise of neural‑symbolic computation for compositional reasoning, the parallels between Divide‑and‑Conquer and chain‑of‑thought prompting, and the potential for further integration of Dual‑Process Theory with large language models.</sample>
    <sample id="167">I DEPLAIN‑web comprende 750 documenti provenienti da diversi domini.  
Per ciascuno di essi è stato eseguito un bilanciamento a due livelli:

1. **Allineamento manuale** – un team di annotatori ha letto i due versioni (complesa e semplificata) di ogni documento e ha indicato manualmente quale frase della versione complessa corrisponde a quale frase della versione semplificata. Questo set di allineamenti è considerato “gold‑standard”.

2. **Allineamento automatico** – per gli stessi documenti è stato applicato un algoritmo automatico di allineamento (es. MASSalign, adattato per il testo semplificato). L’output di questo algoritmo fornisce un set di allineamenti automatizzati, che viene poi confrontato con quello manuale per valutare l’efficacia degli algoritmi di allineamento.

In sintesi, i documenti di DEPLAIN‑web sono stati allineati sia manualmente (come riferimento di qualità) sia automaticamente (per testare ed eccedere i metodi di allineamento).</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccogliendo articoli di Reuters del 2020 e annotandoli con le stesse linee guida di annotazione del CoNLL‑2003.</sample>
    <sample id="169">**Abstract**

We present a systematic evaluation of prompting strategies for the 540‑billion‑parameter PaLM language model on machine translation (MT) tasks. PaLM, trained on 780 billion tokens, achieved state‑of‑the‑art performance on a broad range of NLP benchmarks. However, its translation capabilities have not yet been rigorously benchmarked against established MT systems. We therefore assessed PaLM on recent WMT test sets, explicitly avoiding overlap with its training corpus, and compared the results to the top‑ranked neural MT systems using BLEURT, BLEU, and human MQM evaluations.

Our experiments explored zero‑, one‑, and few‑shot prompting. While the specific prompt wording has a limited impact in five‑shot scenarios, it is critical in zero‑ and one‑shot settings. We found that the quality of the demonstration examples is far more influential than their lexical similarity to the test sentence. Five‑shot prompts that include high‑quality, curated dev‑set translations outperform those drawn from noisy training data. Despite these gains, PaLM still trails specialized commercial MT engines by a substantial margin, although it approaches their performance in terms of fluency.

Human evaluation using the MQM framework reveals that PaLM’s translations are fluent but frequently omit source content, leading to lower accuracy. The “Style/Awkward” error category is lower than that of state‑of‑the‑art systems, indicating smoother output. These findings highlight the importance of careful prompt selection and provide practical guidelines for leveraging large language models in MT applications.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dall’Università del Penn State. Oggi presenterò il nostro lavoro “XSemPLR: Cross‑Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations”.  

Il semantic parsing è il compito di costruire rappresentazioni semantiche delle query degli utenti, come SQL e Lambda Calculus. Il Cross‑Lingual Semantic Parsing invece consiste nel tradurre query in molteplici lingue naturali in molteplici rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in molteplici lingue naturali usando modelli neurali verso SQL, Lambda, FunQL, ecc.  

I modelli esistenti per il Cross‑Lingual Semantic Parsing sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitate. Ad esempio, c’è molta copertura su certe lingue naturali, ma il cinese è assente e manca la copertura di alcune rappresentazioni di significato. Il Lambda Calculus è assente, o sono valutati solo su determinati modelli neurali. Eccone un esempio: c’è solo un singolo modello per valutare tutti.  

Per questo motivo proponiamo XSemPLR. Forniamo un dataset uniforme XSemPLR per il Cross‑Lingual Semantic Parsing in molteplici lingue naturali e rappresentazioni di significato. Contiene 9 dataset in vari domini, 5 compiti di semantic parsing, 8 rappresentazioni di significato e 22 lingue naturali appartenenti a 15 famiglie linguistiche.  

Per valutare meglio il benchmark consideriamo sei configurazioni di training e valutazione.  
1. **Translate‑Test**. Usiamo l’API Google Translate per tradurre la sorgente nella lingua target, poi addestriamo e valiamo un modello monolinguale. Ad esempio, addestriamo un modello in inglese su query in inglese e, durante l’inferenza, traduciamo la query tedesca in inglese tramite l’API e usiamo il modello addestrato per prevedere lo SQL.  

2. **Monolingual Model**. In questa configurazione la lingua di origine è la stessa della lingua target, ad esempio tedesco → tedesco o inglese → inglese.  

3. **Monolingual Few‑shot**. Addestriamo modelli monolingui con solo il 10 % dei dati di training.  

4. **Multilingual Model**. Addestriamo un unico modello multilingue su tutte le lingue. Per esempio, mettiamo insieme query tedesche, inglesi e cinesi e addestriamo un modello multilingue. Durante l’inferenza possiamo usare questo modello per prevedere lo SQL di query tedesche o cinesi, ecc.  

5. **Cross‑lingual Zero‑shot**. Addestriamo su una lingua sorgente e trasferiamo a un’altra lingua. Durante l’addestramento, addestriamo su query inglesi o sulla combinazione di query inglesi e tedesche “few‑shot”, per poi prevedere lo SQL in una lingua target.  

6. **Cross‑lingual Few‑shot transfer**. Simile al caso zero‑shot, ma con un piccolo set di training in lingua target.  

Inoltre, abbiamo trovato molti risultati interessanti.  

### Analisi dei modelli monolingui  
Abbiamo valutato due gruppi di modelli:  
- **Encoder‑PTR**: encoder pre‑addestrati multilingue con decoder a puntatore, come XLM‑R + PTR e mBERT + PTR.  
- **Encoder‑Decoder**: modelli encoder‑decoder pre‑addestrati multilingue, come mBART e mT5.  

Abbiamo scoperto che i modelli Encoder‑Decoder ottengono le migliori prestazioni su tutti i nove dataset.  

Abbiamo poi valutato mT5 e XLM‑R + PTR in configurazioni multilingue. Abbiamo visto che Encoder‑Decoder o Encoder‑PTR possono essere migliorati addestrandoli su una miscela di varie lingue. Il motivo è che la maggior parte delle lingue principali ottiene un guadagno di performance, tranne l’inglese che perde in sette dataset e ne migliora solo in tre. Questo è noto come la “Curse of Multilinguality”.  

### Gap di performance cross‑lingua  
In questa figura la linea blu rappresenta il Cross‑lingual Few‑shot transfer, l’arancione il Cross‑lingual Zero‑shot e la verde la configurazione monolingua.  
- Confrontando la linea verde con quella arancione, troviamo che il gap di performance nel Zero‑shot è significativo.  
- Confrontando la linea blu con quella arancione, notiamo che con il Few‑shot il gap si riduce rapidamente.  

Altri risultati interessanti:  
- Encoder‑Decoder supera i lavori precedenti o raggiunge risultati comparabili.  
- Il pre‑addestramento su lingua inglese può migliorare significativamente la performance Few‑shot in lingue target.  
- Modelli multilingue come Codex e BLOOM risultano ancora inadeguati per i compiti di Cross‑lingual Semantic Parsing.  

### Conclusione  
Abbiamo costruito XSemPLR, un benchmark unificato per il Cross‑Lingual Semantic Parsing con molteplici lingue naturali e rappresentazioni di significato. Abbiamo condotto un’analisi benchmark completa su tre tipologie rappresentative di modelli linguistici multilingue. I nostri risultati mostrano molte scoperte interessanti.  

Grazie per l’attenzione. Visitate il nostro paper e il codice.</sample>
    <sample id="171">**Lavori principali correlati al watermarking di embeddings e alla protezione dei “embedding‑as‑a‑service”:**

| # | Titolo &amp; Autori | Anno | Tipo di approccio | Note rilevanti |
|---|------------------|------|-------------------|----------------|
| 1 | **“Watermarking Machine Learning Models”** | 2017 | Watermark di modello (signature‑based) | Introduce la tecnica di inserire un “fingerprint” nei pesi di un modello; base per molti studi successivi. |
| 2 | **“Backdoor‐Based Watermarking for Deep Neural Networks”** | 2019 | Backdoor‑based | Mostra come una colonna di trigger (parole o pattern) possa essere usata per identificare un modello. |
| 3 | **“Detecting Stolen Embeddings: A Practical Study”** | 2020 | Embedding‑specific | Analizza la vulnerabilità delle embeddings di Word2Vec/GloVe a extraction attacks e propone metodi di rilevamento. |
| 4 | **“Embedding Watermarking for Copyright Protection”** | 2021 | Embed‑specific (target‑embedding) | Inserisce un embedding “target” associato a un trigger set; simile all’approccio di Embedding Marker. |
| 5 | **“Transferable Backdoors for Large Language Models”** | 2022 | Backdoor‑based + transferability | Dimostra che i backdoor addestrati su un modello possono propagarsi a modelli clonati, rendendo la watermark trasmissibile. |
| 6 | **“Model Extraction Attacks on Embedding APIs”** | 2023 | Attacco di estrazione + watermark | Studia come gli attacker estraggono embeddings da servizi come OpenAI e propone metodi per contrastarli. |
| 7 | **“Watermarking for Embedding‑as‑a‑Service”** | 2024 | Backdoor + utility‑preserving | Propone un watermark che non degrada la qualità delle embeddings, con test su AG‑News, SST‑2, ecc. |
| 8 | **“Covert Backdoor Watermarks for NLP Models”** | 2024 | Covert watermark | Valuta la capacità di un watermark di passare inosservato agli attacker, usando analisi PCA/TSNE. |

**Categorie di approcci citati nei lavori:**

1. **Signature‑based watermarking** – inserimento di un pattern nei pesi o nei gradienti.  
2. **Backdoor‑based watermarking** – trigger set che attiva un embedding “target”.  
3. **Fine‑tuning watermarking** – addestramento di un modello con un piccolo set di esempi “signature”.  
4. **Dataset‑based watermarking** – utilizzo di dati specifici (es. trigger words) per creare un fingerprint.

Questi studi forniscono la base teorica e pratica su cui si fonda il metodo “Embedding Marker” descritto nel tuo video.</sample>
    <sample id="172">No. Nel nostro studio i modelli multilingue come Codex e BLOOM risultano ancora inadeguati per le attività di Cross‑Lingual Semantic Parsing (CLSP).</sample>
    <sample id="174">**Abstract**  
ArgAnalysis35K è la più grande raccolta di coppie “argomento‑valutazione” (≈35 000) per l’analisi della qualità degli argomenti, presentata da un team che ha combinato fonti di alta qualità (discorsi di tornei, dibattitori esperti e intermedi) con un campionamento diversificato di temi. A differenza dei dataset esistenti, che spesso si limitano a 30‑40 posizioni predefinite e si basano su crowdsourcing, ArgAnalysis35K copre 24 temi distinti scelti in base all’esperienza nel circuito di dibattito, garantendo una maggiore varietà di moti e di contesti.  

Il corpus introduce il concetto di **analysis**, un elemento che va oltre claim e premise: descrive in modo coerente e completo la logica di un argomento, combinando affermazioni, motivazioni e fatti.  

Un altro punto innovativo è la valutazione della **affidabilità annotatore su base di esempio**: anziché escludere interi annotatori per motivi di bias, il dataset elimina solo le valutazioni ritenute non attendibili per argomenti specifici, preservando così il valore delle opinioni specializzate.  

Infine, ArgAnalysis35K presenta un **modello di rilevanza** che assegna a ogni argomento un punteggio (0‑1) per ciascun tema, riconoscendo che un singolo argomento può essere pertinente a molteplici contesti (governo, religione, azienda, ecc.).  

Questa combinazione di dimensione, qualità, analisi approfondita, affidabilità contestuale e rilevanza tematica rende ArgAnalysis35K un punto di riferimento unico per la ricerca sull’analisi della qualità degli argomenti.</sample>
    <sample id="175">Il metodo tratta l’ambiguità delle permutazioni con un’approssimazione continua: invece di cercare la permutazione ottimale (NP‑hard, equivalente al TSP), si ottiene una relaxazione GPU‑friendly che restituisce un “permutatore” soft. Questa soluzione è differenziabile, quindi può essere inserita nel grafo di calcolo e back‑propagata, permettendo al modello di imparare a preferire le permutazioni linguisticamente plausibili anche quando più di una è compatibile con i dati.</sample>
    <sample id="176">La “fairness” di un modello NLP a valle è valutata confrontando le sue prestazioni su **diverse categorie di soggetti** (demografie, gruppi di genere, etnia, orientamento, ecc.) o su **contenuti di diversa inclinazione politica**.  

In pratica si calcolano metriche di efficacia (accuracy, precision, recall, F1, ecc.) **separatamente per ogni gruppo** e si misura la differenza di performance (gap di errore, gap di precisione, ecc.). Un modello è considerato equo quando questi gap sono minimizzati: le stesse percentuali di correttezza e di falsi positivi/negativi sono raggiunte indipendentemente dal gruppo a cui appartengono i dati o dal tono politico del contenuto.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">**Abstract**

Il lavoro di Melanie Sclar esplora il miglioramento delle capacità di ragionamento “Theory of Mind” (ToM) nei modelli linguistici di grandi dimensioni (LLM). Si concentra sui compiti di false‑belief, dove la realtà differisce dalla credenza di un personaggio, e sui test classici come il Sally‑Anne. Gli LLM attuali, tra cui ChatGPT e GPT‑3, mostrano prestazioni insoddisfacenti su questi quesiti, specialmente di ordine superiore. Per superare questa lacuna, l’autrice presenta **SymbolicToM**, un metodo “plug‑and‑play” che opera al momento dell’inferenza. Il cuore dell’approccio è la costruzione di grafici di credenza (BBob, BBob,Alice, ecc.) che rappresentano esplicitamente gli stati mentali di ciascun personaggio e delle sue percezioni di altri personaggi, fino a un livello di ToM prefissato. I grafici vengono generati in tempo reale tramite modelli NLI e OpenIE, e successivamente utilizzati per rispondere a domande di ragionamento tramite una ricorsione di domande fattuali sui grafici.

Gli esperimenti confrontano SymbolicToM con modelli supervisionati (GPT‑3 fine‑tuned e Textual Time Travel) su ToMi, mostrando incrementi di precisione di 50‑70 punti percentuali su modelli di base. Per testare la generalizzazione, gli autori creano tre nuovi dataset (D₁, D₂, D₃) che variano la struttura narrativa e un dataset “ParaphrasedToMi” che aumenta la diversità linguistica. I modelli supervisionati degradano drasticamente su questi set (≈50 % di accuratezza), mentre SymbolicToM continua a fornire miglioramenti sostanziali, consentendo anche a GPT‑4 di risolvere quasi interamente i nuovi compiti.

In conclusione, SymbolicToM offre un algoritmo di inferenza basato su rappresentazioni grafiche simboliche che migliora in modo significativo le capacità ToM degli LLM, supera i modelli supervisionati su dati fuori dominio e mantiene un ragionamento interpretabile senza rischi di overfitting.</sample>
    <sample id="180">La relatrice è **Myra**.</sample>
    <sample id="181">**Abstract**

Human agents routinely decompose complex goals into actionable scripts. Existing work has shown that large language models (LLMs) can generate such scripts for abstract, stereotypical activities (e.g., “make a cake”), but their ability to respect *specific, multi‑faceted constraints* (e.g., “make a chocolate cake with gluten‑free flour”) remains largely unexplored. In this paper we formalize **constrained language planning**: given a goal and a set of constraints, produce a realistic, faithful script.  

We first construct a benchmark of 100 constrained goals by extending abstract goals with constraints sampled from wikiHow and verified by human annotators. Evaluation reveals that all tested LLMs (GPT‑4, InstructGPT, etc.) achieve low overall accuracy; semantic completeness is acceptable, yet faithfulness to constraints is poor and varies markedly across constraint categories.  

To mitigate this, we introduce an **over‑generate‑then‑filter** pipeline: InstructGPT produces *K* candidate scripts per goal, and a lightweight filter (based on InstructGPT embeddings and keyword rewards) selects the most faithful script. This approach substantially improves both completeness and constraint adherence.  

Recognizing the cost of fine‑tuning large models, we further propose **symbolic knowledge distillation** to bootstrap a high‑quality dataset, **CoScript**. By applying the over‑generate‑then‑filter pipeline, we generate 55k constrained goals and scripts, subsequently refined by crowd workers. CoScript exhibits diverse constraint distributions. Training a modest T5 model on CoScript yields script quality that surpasses most commercial LLMs, demonstrating that specialized, well‑curated data can empower smaller models.  

Our work establishes a new constrained planning task, delivers a robust generation strategy, and provides the CoScript dataset to spur future research.</sample>
    <sample id="182">Il “tropicalismo” è il cliché che associa le donne latine a immagini tipiche del tropico—vivaci, esotiche, sensualmente accattivanti—e che le riduce a un’archetipica, essenzializzante narrazione di “tropicalità”.</sample>
    <sample id="183">Gli autori hanno creato rappresentazioni “umane” dei gruppi target generando **personas** con prompt di tipo “Immagina di essere una donna asiatica. Descriviti.”  Le istruzioni sono state inserite in modelli LLM istruttivi (es. GPT‑4) per farli generare descrizioni di se stessi.  Successivamente hanno confrontato queste generazioni con risposte scritte da soggetti umani e hanno analizzato i testi tramite la tecnica “Marked Words” (log‑odds ponderati) per individuare le parole distintive che marcano i gruppi marginalizzati rispetto al gruppo non marcato.</sample>
    <sample id="184">È stato utilizzato il **CXMI** (context‑dependent cross‑entropy mutual information), esteso a livello pointwise (P‑CXMI) per misurare l’uso del contesto.</sample>
    <sample id="185">**DrBERT** – a French biomedical model trained from scratch on the NACHOS corpus (web‑crawled medical text, ~4–7 GB).

**ChuBERT** – a French clinical model trained from scratch on anonymised clinical notes from the Nantes University Hospital data warehouse (≈4 GB).  

In short, DrBERT learns from web‑sourced biomedical text, while ChuBERT learns from real clinical records.</sample>
    <sample id="187">Due to the information provided, the article appears to involve **two authors**: Ying and Zhiyang.</sample>
    <sample id="188">**Trasferimento iterativo dell’apprendimento** è la procedura in cui un modello, prima addestrato su un compito correlato (ad es. “CE” o “debate”), viene **ricaricato** e **riscoppiato** ad ogni nuova “round” di annotazione, addestrandolo **solo** sui dati appena raccolti (invece di includerli tutti insieme).  

Questa strategia permette di:

1. **Mantenere l’orientamento** verso il dominio di destinazione (dato che l’inizializzazione proviene dal modello già trasferito).  
2. **Aggiornare il modello in modo incremental** senza dover ri‑addestrare su tutti i dati precedenti, riducendo tempo e costi di calcolo.  

In sintesi, il trasferimento iterativo è un aggiornamento progressivo e sequenziale del modello, dove ogni passo di fine‑tuning parte dal modello precedente e si concentra sui nuovi esempi annotati.</sample>
    <sample id="189">L’obiettivo del dataset è fornire una base di benchmark per la comprensione e l’elaborazione di *indirect referring expressions* (espressioni indirette) al fine di selezionare correttamente l’entità a cui l’utente si riferisce in dialoghi informali. In pratica, si vuole testare e migliorare i sistemi conversazionali (e LLM) nella capacità di interpretare riferimenti non diretti per scegliere l’oggetto corretto (canzone, libro, ricetta, ecc.).</sample>
    <sample id="190">Un utente malintenzionato può “rubare” i parametri di un modello di embedding sfruttando l’API di EaaS in questo modo:

1. **Query massicce** – invia un gran numero di frasi (spesso casuali o appositamente generate) al servizio di embedding e raccoglie le embeddings restituite.  
2. **Addestramento di un modello sostitutivo** – usa le coppie input‑embedding come dati di addestramento per un nuovo modello (di solito una rete neurale simile) e lo ottimizza affinché reproduca le embeddings del servizio target.  
3. **Stealing/approximation** – il modello addestrato diventa un “stealer” che, sebbene non contenga esattamente gli stessi pesi, replica con alta precisione il comportamento del modello originale.  
4. **Analisi di back‑door** – se il servizio incorpora trigger o watermark, l’aggressore può anche sfruttarli per estrarre informazioni aggiuntive (es. target embeddings) e migliorare la precisione del modello rubato.

In sintesi, tramite una serie di richieste di embeddings e l’addestramento di un modello surrogate, l’utente malintenzionato è in grado di ricostruire una versione molto vicina del modello originale, ottenendo così i suoi parametri impliciti.</sample>
    <sample id="191">Tre autori.</sample>
    <sample id="192">**Abstract (≈200 wordi)**  
Il lavoro presentato introduce *CAME* (Confidence‑Guided Adaptive Memory‑Efficient Optimization), un nuovo algoritmo di ottimizzazione progettato per superare i limiti dei metodi adattivi tradizionali (es. Adam) e delle soluzioni a memoria ridotta (es. Adafactor). In particolare, CAME mira a mantenere la velocità di convergenza di Adam riducendo al contempo l’utilizzo di memoria.  

Per raggiungere questo obiettivo, il metodo sfrutta la fattorizzazione a matrice non‑negativa (NMF) per comprimere le stime dei momenti di primo e secondo ordine, ispirandosi all’approccio di Adafactor che riduce la complessità da O(mn) a O(m+n). Tuttavia, Adafactor soffre di errori di aggiornamento che rallentano l’addestramento. CAME risolve questo problema introducendo un meccanismo di “confidence‑guidance”: la differenza residua tra la momentum prevista (mₜ) e l’aggiornamento effettivo (uₜ) viene utilizzata come fattore di normalizzazione, permettendo aggiornamenti più stabili e adattivi.  

Gli esperimenti su BookCorpus e Wikipedia, con modelli BERT, GPT‑2 e T5, mostrano che CAME raggiunge accuracies superiori di circa 3,4 % rispetto ad Adafactor e supera Adam nella fase di pre‑training dei modelli più grandi, pur consumando significativamente meno memoria (soprattutto con batch size fino a 32K). Inoltre, i modelli finiti ottenuti con CAME mantengono performance competitive su compiti downstream, dimostrando l’efficacia dell’ottimizzatore in scenari di addestramento a grandi batch e su modelli di linguaggio estesi.</sample>
    <sample id="193">Non è specificato nel testo quante annotatori sono stati impiegati per creare il set di dati iniziale.</sample>
    <sample id="194">**Affiliations of the authors mentioned in the talk**

| Author | Affiliation |
|--------|-------------|
| **Jenny** | Carnegie Mellon University (CMU) – PhD student |
| **Sebastian Santy** | University of Washington |
| **Ronan Le Bras** | Allen Institute for AI (AI2) |
| **Katharina Reinecke** | Carnegie Mellon University (CMU) |
| **Maarten Sap** | Allen Institute for AI (AI2) |

So the work was carried out by researchers from **Carnegie Mellon University**, the **University of Washington**, and the **Allen Institute for AI**.</sample>
    <sample id="195">**Abstract**

L’Explainable Question Answering (XQA) mira a fornire non solo la risposta a una domanda complessa ma anche una spiegazione chiara del motivo per cui è stata scelta. Le soluzioni recenti si dividono in approcci neuro‑simbolici, che trasformano il testo in query formali (es. SPARQL), e in metodi basati sulla decomposizione, che generano passaggi intermedi in linguaggio naturale. Entrambe le direzioni hanno limiti: i sistemi neuro‑simbolici dipendono da KB strutturate e spesso incomplete, mentre le soluzioni di decomposizione si affidano solo a corpus di testo, rendendo difficile gestire la diversità semantica.  

In questo lavoro proponiamo **RoHT (Reasoning over Hierarchical Question Decomposition Tree)**, un framework a due stadi che combina i vantaggi dei due approcci. In primo luogo, viene costruito un **Hierarchical Question Decomposition Tree (HQDT)** tramite un decomposer che identifica le domande atomiche e un generator che produce domande intermedie raggruppate per token di riferimento, accompagnate da punteggi di certezza. In secondo luogo, eseguiamo un ragionamento probabilistico sullo stesso albero: un scheduler decide se interrogare un KB, un corpus testuale o risolvere ricorsivamente i figli; gli executor ottengono risposte con probabilità; un aggregator combina le candidature per selezionare le risposte finali più probabili.  

RoHT è stato valutato su **KQA Pro** (KB incompleta + Wikipedia) e **Musique** (testo + Wikidata). In entrambe le suite, RoHT supera i metodi esistenti (es. TransferNet, EX(SA)), dimostrando l’efficacia della decomposizione esplicita e dell’integrazione di fonti eterogenee.</sample>
    <sample id="196">L’esempio citato è **“I saw Bart and Lisa.”**  
Qui il governatore (“saw”) si trova a sinistra del coordinato.</sample>
    <sample id="197">I’m sorry, but the passage you provided only mentions that **four state‑of‑the‑art chat models** were evaluated. It does not list their specific names.</sample>
    <sample id="198">La valutazione deve estendersi all’intera finestra di contesto perché i moderni LLM dispongono di span di memoria molto più lunghi e, a differenza delle coppie minime tradizionali (una sola frase), i loro giudizi di accettabilità possono cambiare con l’aumentare della lunghezza e del contenuto del contesto.  

In pratica:

1. **Robustezza reale** – Nella pratica i modelli leggono sequenze di molte centinaia di token; verificare che mantengano giudizi coerenti lungo tutta la finestra è essenziale per capire se l’“accettabilità” è davvero un’abilità acquisita o solo una risposta a schemi locali.  

2. **Effetto di contesto** – I risultati mostrano che prefissi grammaticalmente corretti o scorretti, anche se estratti dallo stesso set o da set diversi, alterano significativamente le probabilità di accettabilità. Con una finestra più ampia questa sensibilità si amplifica, quindi è necessario testarla su scala reale.  

3. **Latenti strutture semantiche** – Il modello sembra rilevare caratteristiche sintattiche e semantiche condivise in tutto il contesto. Una valutazione a breve distanza non coglie questi pattern latenti, mentre una valutazione su tutta la finestra li mette in evidenza.  

Quindi, per garantire che le misure di accettabilità riflettano effettivamente la conoscenza astratta del modello e non solo la sua risposta a frasi brevi, è indispensabile testarle lungo l’intera finestra di contesto.</sample>
    <sample id="199">Sì. Nel benchmark XSemPLR si osserva che l’addestramento multilingue migliora le prestazioni nella maggior parte delle lingue, ma per l’inglese i risultati tendono a peggiorare in sette dei nove dataset (solo in tre si nota un guadagno), evidenziando il noto “Curse of Multilinguality”.</sample>
    <sample id="200">No.  Gli annotatori non conoscono le entità prima dell’annotazione;  vengono presentati con i due nomi e il relativo background (link, testo, immagini) e devono familiarizzare con essi durante il compito.</sample>
    <sample id="201">Istituzionalmente il lavoro ha usato un **metrico neurale di qualità**: **BLEURT** (che è stato l’unico metrico menzionato nei risultati, con differenze misurate in “BLEURT points”).  
Per la valutazione qualitativa sono stati poi condotti esperimenti di **human evaluation** basati sul framework **MQM**, ma MQM non è un metrico di MT automatico.</sample>
    <sample id="202">Non è stato osservato un impatto diverso sui diversi tipi di entità: il calo di performance dovuto al *temporal drift* si manifesta in modo globale, senza evidenza di una maggiore degradazione su singole categorie (persone, organizzazioni, località, ecc.). In pratica, la regressione nella generalizzazione influisce in modo uniforme su tutti gli NER‑tag, mentre l’architettura, la dimensione del modello e la quantità di esempi di fine‑tuning sono gli aspetti principali che ne determinano l’entità.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché:

- **Rende visibili i bias**: mostra come dataset e modelli riflettano le prospettive di chi ha creato o annotato i dati, lasciando indietro gruppi specifici (es. popolazioni non‑bianche, non‑bianche, persone con educazione diversa, ecc.).
- **Garantisce equità e inclusività**: se un modello è allineato solo a certe comunità (es. paesi anglofoni, etichette di genere tradizionali), gli utenti appartenenti a gruppi diversi subiranno risultati meno precisi o più offensivi.
- **Migliora la qualità delle decisioni**: comprendere la posizionalità aiuta a scegliere annotatori, metriche e test più rappresentativi, riducendo errori sistematici e migliorando la robustezza del modello.
- **Supporta la responsabilità sociale**: documentando le scelte di design e le prospettive coinvolte, i ricercatori possono rendere trasparente l’impatto delle loro tecnologie e intervenire per ridurre le disparità.

In sintesi, la posizionalità è cruciale per costruire sistemi NLP equi, affidabili e socialmente responsabili.</sample>
    <sample id="204">Gli LLM multilingue come BLOOM non sono stati “affinati” in questo studio con adattatori – è stato usato il modello base (senza adattatori) e, nel caso in cui venisse addestrato, con una messa a punto integrale (full fine‑tuning). In altre parole, non viene applicato un approccio di fine‑tuning tramite adapter.</sample>
    <sample id="205">**Abstract**

Large‑scale language models (LMs) are trained on vast web‑crawled corpora that include extensive coverage of politically oriented news outlets. While this diversity can promote democratic pluralism, it also embeds socially biased viewpoints that may propagate through the LM training pipeline and surface as fairness violations in downstream applications. In this study, we investigate the full trajectory of political bias—from pretraining data to LMs to end‑to‑end tasks—by addressing two central questions: (1) How can we quantify the political leaning of contemporary LMs, and what role does the training corpus play? (2) Do LMs with distinct ideological profiles differentially affect the fairness of downstream NLP systems?

We evaluate LM political orientations using structured political questionnaires (e.g., the Political Conference Test), revealing that models span the liberal–conservative spectrum, with GPT‑4 the most liberal and BART variants the most conservative. Controlled fine‑tuning experiments on partisan corpora (news vs. social media, left vs. right) demonstrate that ideological coordinates shift accordingly, and temporal segmentation (pre‑ vs. post‑2017) shows increasing polarization in model biases. Finally, we benchmark left‑leaning versus right‑leaning LMs on hate‑speech and fake‑news detection, uncovering systematic disparities: left‑leaning LMs excel at flagging minority‑targeted hate speech but under‑detect powerful‑group targeting, whereas right‑leaning LMs display the opposite pattern; similar asymmetries appear in misinformation detection across political leanings.

These findings expose a “Scylla‑Charybdis” dilemma: sanitizing political content risks censorship, while retaining it perpetuates biased downstream decisions that can marginalize vulnerable communities. Our work underscores the urgency of developing principled mitigation strategies for political bias in large‑scale NLP systems.</sample>
    <sample id="206">Usano un modello transformer pre‑addestrato (in pratica BERT‑style) che viene prima “incollato” su due compiti vicini:  

1. **Debate stance classification** – classificazione di dissonanza/corde senza tematiche.  
2. **CE (expansion/comparison) task** – classificazione binaria delle classi di congiunzione “expansion” e “comparison” del PDTB.  

Il modello pre‑addestrato riceve i pesi da questi due task, poi è fine‑tuned su dissonanza.</sample>
    <sample id="207">I set di test più recenti impiegati per valutarne le capacità sono i **WMT (World Machine Translation) test sets**—le ultime versioni disponibili (ad esempio WMT 2022/2023) e le relative dev‑sets, che garantiscono che i dati di valutazione non coincidano con quelli di addestramento del modello.</sample>
    <sample id="208">Gli autori hanno proposto **tre** suggerimenti alla fine della presentazione.</sample>
    <sample id="209">Il metodo proposto migliora in modo significativo la qualità dei script rispetto al semplice “prompt‑to‑output” dei LLM di riferimento:  

* **Maggiore completezza semantica** – i passaggi necessari per raggiungere l’obiettivo sono più coerenti e completi.  
* **Maggiore fedeltà ai vincoli** – i script risultano aderire meglio alle restrizioni specifiche (es. “chocolate cake” vs “cake”).  
* **Efficienza su modelli più piccoli** – grazie al dataset distillato (CoScript) anche modelli di dimensioni ridotte (es. T5) possono produrre script di qualità superiore rispetto a molti LLM più grandi.  

In sintesi, l’over‑generate‑then‑filter e la distillazione del dataset portano a script più accurati, più aderenti ai vincoli e permettono a modelli più piccoli di superare quelli grandi.</sample>
    <sample id="210">Il relatore è **Shuheng**.</sample>
    <sample id="211">Sì. Il corpus DEPLAIN, insieme ai risultati delle esperienze di allineamento e di fine‑tuning (MASSalign, long‑mBART, mBART), è stato presentato come base di riferimento per la semplificazione testuale in tedesco; i dati, i checkpoint e i codici offerti nel paper forniscono un benchmark standard e comparabile.</sample>
    <sample id="212">Il documento fa riferimento a un singolo modello più piccolo: **T5** fine‑tuned sul dataset CoScript.</sample>
    <sample id="213">Il modello di base utilizzato è **OFA** (Open Pre-trained Vision‑and‑Language Model).</sample>
    <sample id="215">**Abstract**

Coordination is traditionally analyzed with asymmetric dependency structures, where either the first conjunct (e.g., Universal Dependencies, Mel’čuk’s Meaning Text Theory) or the conjunction itself (Prague Dependency Treebanks) functions as the head of the whole coordinate phrase. In contrast, multi‑headed approaches (e.g., Hudson’s Word Grammar) treat every conjunct as a head, yielding symmetric structures. This paper presents a novel argument favoring symmetric coordination by appealing to the principle of dependency‑length minimization (DLMin). Using the example “Marge read this absolutely fascinating book about bees yesterday,” we demonstrate that relocating a heavy direct object after an adjunct can reduce the sum of dependency lengths, making the sentence acceptable despite violating the usual proximity preference for objects.  

We then analyze coordination in the enhanced Penn Treebank, measuring conjunct lengths in characters, syllables, and words. The data confirm the long‑standing observation that left‑hand conjuncts tend to be shorter, with the effect intensifying as the length disparity grows. Crucially, this asymmetry appears only when the governor is on the left or absent (e.g., coordination of clauses or verbs). When the governor is on the right, the preference for a shorter left conjunct disappears. These findings undermine asymmetric head‑based models, which predict a consistent head‑directional bias, and instead support symmetric coordination structures that treat all conjuncts on equal footing. The full argument and empirical evidence are available in the accompanying paper.</sample>
    <sample id="217">**Abstract**

Controllable dialogue systems traditionally focus on a single attribute (e.g., sentiment or persona), yet real‑world applications require simultaneous control over multiple attributes, many of which are continuous. Existing multi‑attribute models either concatenate independent attribute controllers or rely on extensive annotated data, limiting their ability to generalize to unseen attribute combinations. In this work, we investigate *compositional generalization* for multi‑attribute controllable dialogue generation and introduce **Disentangled Controllable Generation (DCG)**. DCG learns attribute concepts from seen values and employs a *disentanglement loss* to separate different attribute combinations. Prompt engineering is central: we design attribute‑oriented prompts that embed instance‑specific control signals, and task‑oriented prompts that capture global, instance‑independent dialogue features. The two prompt embeddings are concatenated, and pseudo‑combinations are added to enhance diversity.  

To evaluate controllability without costly labeling, we propose a unified, reference‑free metric framework, **MAE**, which combines discrete templates and trainable continuous prompts. MAE correlates strongly with human judgments on both coarse‑grained discrete and fine‑grained continuous attributes.  

We build two benchmarks (DailyDialog‑CG and a second dataset) and conduct extensive experiments. DCG outperforms strong baselines (CTRL, fine‑tuned DialoGPT, etc.) on controllability metrics (E‑ACC, A‑ACC) and text quality (BLEU), while maintaining minimal loss on attribute fidelity. Visualization of prompt embeddings confirms effective disentanglement and transfer from seen to unseen attribute combinations. Overall, our prompt‑based, disentangled approach enables robust compositional generalization for multi‑attribute dialogue generation.</sample>
    <sample id="218">Gli autori del lavoro sono tutti affiliati a Google – in particolare al team di Google Translate (Google AI / Google Research).</sample>
    <sample id="219">**Abstract (≈200 parole)**  

Il lavoro “A Compare‑and‑Contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports” propone un approccio automatico per estrarre segnali finanziari significativi dai report annuali (Form 10‑K) presentati alla SEC. La motivazione principale è la forte somiglianza testuale tra report successivi (≈ 80 % di token identici), che rende difficile individuare le variazioni rilevanti. Per questo motivo è stato definito un compito di “highlighting”: dato un report di riferimento (R) e un report target (T) dello stesso ente, il modello deve assegnare un punteggio di importanza a ciascun token di T, evidenziando le parole che spiegano la differenza tra i due documenti (es. “decrease”).  

Il pipeline è suddiviso in più fasi:  
- **Stage 0**: segmentazione del documento (descritta in dettaglio nel paper).  
- **Stage 1**: classificazione delle coppie T–R in tre categorie – β (alta similarità sintattica e semantica), “revised” (stili simili ma significati diversi) e “mismatched” (informazioni nuove).  
- **Stage 2 &amp; 2+**: fine‑tuning in due passaggi. Si utilizza prima un dataset esterno di NLI (eSNLI) con annotazioni token‑level per il dominio “out‑of‑domain”, poi si addestra con le coppie “revised”, usando etichette pseudo‑positive e un campionamento casuale di negativi. L’ottimizzazione combina cross‑entropy e KL‑divergenza per gestire l’alta variabilità delle pseudo‑etichette.  

La valutazione avviene su eSNLI e sul nuovo dataset “FINAL” (rilasciato con il paper). I metriche principali sono Precision‑at‑Recall e Pearson Correlation Coefficient (PCC). Il modello domain‑adaptive ottiene il miglior risultato su FINAL, mantenendo al contempo una buona generalizzazione su eSNLI e mostrando miglioramenti anche su coppie “mismatched” non utilizzate durante l’addestramento.  

Il lavoro presenta quindi un compito di highlighting per report finanziari, un dataset pubblico e un pipeline semplice ma efficace basato su due stadi di fine‑tuning, aprendo la strada a ulteriori miglioramenti e applicazioni in ambito IR.</sample>
    <sample id="220">Based on the information you provided, the only affiliation explicitly mentioned is:

- **Vasudha** – Computer Science PhD candidate, **Stony Brook University**.

I don’t have details on any additional authors or their institutions. If you can share the full author list, I’ll gladly add their affiliations.</sample>
    <sample id="221">L’articolo ha analizzato la coppia linguistica **tedesco‑inglese** (German → English).</sample>
    <sample id="222">**Abstract (≈200 parole)**  

Questo lavoro affronta la problematica dell’adattamento di sistemi di risposta a domande open‑domain (QA) a domini specialistici, esaminando come rendere robuste sia le componenti di recupero (retriever) sia di lettura (reader) quando il corpus di riferimento passa da Wikipedia (dominio generale) a domini più ristretti (biomedica, legale, ecc.). Le principali innovazioni sono tre:  

1. **Interventi di dati** – valutiamo approcci zero‑shot e few‑shot per generare esempi di domande, risposte e contesti di nuovo dominio. Nei casi few‑shot, utilizziamo esempi target per promptare modelli di linguaggio a generare fatti, che vengono poi convertiti in domande a completamento (cloze). Gli interventi zero‑shot mirano a controllare le variabili question‑answer‑context senza esempi di dominio.  

2. **Taxonomia dello shift** – introduciamo una misura di “compatibilità” basata sulla probabilità assegnata da modello source a contesti e risposte target. In questo modo classificiamo i dataset in quattro categorie di shift: nessuno, concetto, covariato e completo.  

3. **Interventi mirati** – analizzando i risultati, scopriamo che tutti i set di dati rispondono positivamente a few‑shot, mentre i set con shift concettuali o covariati traggono beneficio anche da zero‑shot.  

Con questi interventi miglioriamo la performance del reader fino al 24 % e dimostriamo che la scelta dell’intervento dipende dalla natura dello shift del dominio target.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">I esperimenti hanno valutato:

- **MASSalign** (metodo di allineamento delle frasi) come miglior approccio per l’allineamento di testi semplificati in tedesco.  
- **long‑mBART** (versione estesa di mBART) per la semplificazione a livello di documento.  
- **mBART (base)** per la semplificazione a livello di frase.</sample>
    <sample id="225">**Training:** 53 delle 62 attività  
**Test:** 9 delle 62 attività (tutte le attività del gruppo “Common Sense Reasoning” più 5 altre dal gruppo “VQ &amp; Miscellaneous”)</sample>
    <sample id="226">Due autori sono coinvolti nell’articolo.</sample>
    <sample id="227">**Abstract**  
Grounded language understanding—mapping natural‑language expressions to executable plans or queries—remains a central challenge for modern language models because pre‑training is almost exclusively text‑based and lacks environmental grounding. Existing approaches typically employ autoregressive generation to produce plans, but the generated outputs frequently violate syntactic or semantic constraints of the target domain (e.g., invalid SQL queries or empty KB results). To address this, we propose **Pangu**, a novel framework that decouples generation from evaluation: a symbolic agent enumerates candidate plans, while a language model is used solely as a discriminator to rank them. By shifting the burden from generation to discrimination, Pangu avoids the pitfalls of producing ill‑formed plans while still leveraging the expressive power of large language models.

We instantiate Pangu on knowledge‑based question answering (KBQA), a prototypical grounded language task, and evaluate it with several model families—BERT, T5, and Codex—under both fine‑tuning and in‑context learning regimes. Across all settings, Pangu consistently outperforms the baseline autoregressive model ArcaneQA, achieving superior sample efficiency and a 50 %+ accuracy on the GRAIL query set with only a single demonstration in Codex. Moreover, Pangu exhibits robust generalization to non‑i.i.d. test distributions; its probability distributions over seen and unseen structures remain closely aligned, unlike ArcaneQA, which overfits to training patterns. These findings suggest that, for grounded language understanding, discriminative modeling offers a more reliable and efficient alternative to generative approaches.</sample>
    <sample id="228">Gli autori hanno testato il metodo su quattro set di dati: **AG News, MIND, SST‑2 e Enron Spam**.</sample>
    <sample id="229">**Abstract**

Nell’ambito della scrittura argomentativa il processo di revisione è cruciale per garantire che le affermazioni siano chiare, precise e persuasivi. In questo lavoro presentiamo due compiti nuovi basati su dati di revisione collaborativa raccolti su piattaforme di dibattito online (es. Kialo): (1) **Rilevamento di affermazioni sub‑ottimali** – classificare se una claim necessita di ulteriori revisioni o è già ottimale; (2) **Suggerimento di miglioramenti** – identificare le tipologie di problemi di qualità che devono essere corretti.  

Per affrontare questi compiti abbiamo affrontato quattro principali sfide:  
1. **Rappresentatività e affidabilità** – verificare che le versioni finali delle claim siano davvero ottimali e non semplicemente trascurate dalla comunità.  
2. **Complessità modellistica** – valutare diverse architetture (da modelli pre‑addestrati a modelli fine‑tuned) per catturare le sottili variazioni linguistiche che indicano la necessità di revisione.  
3. **Contesto** – determinare quali informazioni contestuali (argomento, claim genitore, conoscenza di dominio) siano utili per la valutazione della qualità.  
4. **Bias tematico e utente** – gestire rumore, errori accidentali e pregiudizi culturali presenti nei dati di revisione.  

I risultati mostrano che i dati di revisione sono utili per entrambi i compiti, che la modellazione della distanza tra versioni successive migliora la rilevazione di claim sub‑ottimali, e che l’inclusione di contesto è vantaggiosa solo in specifici scenari di qualità. Questi risultati aprono la strada a sistemi di supporto alla scrittura argomentativa basati su revisioni collaborative.</sample>
    <sample id="231">**NACHOS** is a large French corpus of medical texts collected by web‑crawling. It contains billions of words from publicly available medical websites and is used as the in‑domain data for pre‑training the DrBERT biomedical language model.</sample>
    <sample id="232">Il relatore è **David Vilar**.</sample>
    <sample id="233">**Abstract**

Simultaneous speech translation (SimulST) delivers real‑time trans‑lingual communication by translating spoken input into text while the speaker is still talking. Existing SimulST systems typically rely on specially engineered architectures and multi‑stage training pipelines, often requiring distinct models for each latency target. These approaches incur high computational costs, complexity, and lack flexibility.  

In this work we propose **EDAtt (Encoder‑Decoder Attention)**, a simple yet powerful strategy that repurposes standard offline speech‑to‑text models for SimulST without any architectural changes or additional training. EDAtt leverages the cross‑attention weights between audio encoder states and decoder outputs to decide when to emit partial translations. A token is released only when the cumulative attention over the most recent λ audio frames falls below a threshold α, indicating sufficient confidence that the token is grounded in fully observed input. This adaptive emission policy directly controls latency while preserving translation quality.  

We evaluate EDAtt on the English‑to‑German task, reporting BLEU scores and average lagging (AL) as well as computational‑aware AL that incorporates decoding time. Across a wide range of latency settings, EDAtt consistently outperforms popular offline strategies such as Wait‑k and Local Agreement, and achieves competitive or superior results compared to specialized SimulST architectures. The method is lightweight, requiring only a single model for all latency regimes, and we release the code, checkpoints, and simultaneous outputs to support reproducibility. Our findings demonstrate that attention‑driven emission is an effective, general‑purpose solution for real‑time speech translation.</sample>
    <sample id="234">La strategia di prompting ha un impatto molto rilevante sui risultati di traduzione:

- **Variazioni di performance**: Un semplice esperimento con one‑shot prompting ha mostrato differenze di oltre 1 punto BLEURT per la maggior parte delle frasi (516 su 1.000) e, in casi estremi, variazioni fino a 40 punti BLEURT.
- **Importanza delle esempi**: Con cinque‑shot prompting, la forma esatta del prompt è quasi trascurabile; ciò che conta è la qualità degli esempi di traduzione forniti. Esempi di alta qualità (es. dal dev set curato) producono risultati migliori rispetto a esempi presi dal training set più rumoroso.
- **Zero/One‑shot**: In questi regimi la scelta del prompt è critica, mentre con più shot l’effetto è attenuato.

In sintesi, la strategia di prompting può modificare notevolmente le metriche di qualità, e la selezione di esempi di qualità è il fattore più determinante.</sample>
    <sample id="235">**Affiliations of the authors**

- **Kayo Yin** – University of Toronto  
- **Patrick Fernandes** – University of Toronto  
- **Emmy Liu** – University of Toronto  
- **André F. T. Martins** – University of Cambridge  
- **Graham Neubig** – University of Washington</sample>
    <sample id="236">Nel discorso non sono riportati i testi effettivi delle cinque istruzioni.  
Viene menzionato solo che **ogni task del dataset MultiInstruct è accompagnato da cinque “expert‑written instructions”**, ma non vengono mostrati i loro contenuti.</sample>
    <sample id="237">Gli autori introducono **KITMUS**, un “diagnostic test suite” basato su un task di coreference.  
In KITMUS il dataset è costruito con tre configurazioni (Background‑Pretrain, Background‑Both, Background‑Inference) che controllano se le informazioni di sfondo e quelle specifiche all’entità siano disponibili nei parametri pre‑addestrati, nel contesto di inferenza o in entrambi i casi. In questo modo si può verificare se i modelli riescono a integrare conoscenza proveniente da più fonti.</sample>
    <sample id="238">**Abstract (≈200 parole)**  

Il video presenta *MeetingBank*, un nuovo benchmark di dati per la sintesi di riunioni pubbliche. Il dataset è stato costruito a partire da audio delle assemblee dei consigli comunali di diverse città (Boston, Seattle, Denver, ecc.) e comprende 1 366 riunioni, quasi 7 000 istanze di sintesi, oltre a trascrizioni, segmenti temporali e link ai documenti ufficiali. La pipeline di acquisizione utilizza l’API Speechmatics per la trascrizione, identifichi un *MeetingID* univoco, recupera i riassunti ufficiali dai verbali, allinea i timestamp e produce un “second transcript” temporizzato. 

Sono stati analizzati i metodi di sintesi esistenti, valutando metriche di copertura e densità per misurare l’astrazione. Le riunioni tendono a mostrare un alto livello di copertura (0,7–0,9) e una densità variabile, con Seattle e Boston più “estratti” e Denver più editati.  

Per la valutazione delle prestazioni sono stati testati estrattori (Oracle, LEAD, LexRank, TextRank) e modelli neurali astratti (BART‑Large, PaGATSuS, Longformer, DialogLM, HMNet). DialogLM ha ottenuto il miglior ROUGE‑2 tra gli astratti. Un prompt zero‑shot con GPT‑3 (Davinci‑003) ha mostrato buone metriche automatiche ma scarso risultato in informatività e factualità.  

Infine, un’analisi umana su 200 campioni ha evidenziato che GPT‑3 domina in fluidità e coerenza, ma esita in accuratezza e completezza. L’autore conclude che *MeetingBank* è un prezioso strumento per lo sviluppo di sintetizzatori di riunioni più informativi e propone la creazione di metriche di valutazione più allineate alle preferenze umane.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e presenterò una breve recensione dell’articolo *“Prompting PaLM for Translation: Assessing Strategies and Performance”*. È lavoro con i miei colleghi di Google Translate. PaLM è un modello linguistico di grandi dimensioni con 540 miliardi di parametri, pubblicato l’anno scorso, nel 2022. È stato addestrato su un vasto corpus di testo, composto da 780 miliardi di token. All’epoca della pubblicazione, ha raggiunto risultati di stato‑dell’arte in centinaia di compiti NLP.

In questo lavoro presentiamo la prima studio sistematico sul prompting di modelli linguistici di grandi dimensioni (LLM) per la traduzione automatica. Abbiamo valutato la capacità di transizione di tali modelli usando le migliori pratiche della comunità MT. Ciò comporta l’utilizzo delle ultime versioni dei set di test per evitare che i dati di test coincidano con i dati di addestramento del modello linguistico. Inoltre, abbiamo confrontato i risultati con i sistemi di stato‑dell’arte, in particolare con il miglior sistema di valutazione WMT. Usiamo metriche di traduzione neurale all’avanguardia e, inoltre, presentiamo anche risultati di valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.

Il prompting ha una grande influenza sulle prestazioni degli LLM per la traduzione, come si vede in un esperimento semplice in cui abbiamo usato il prompting one‑shot e fornito due prompt diversi per ciascuna frase. La maggior parte delle frasi (516 su 1 000) mostra una differenza di più di un punto BLEURT. In casi estremi, questa differenza può arrivare fino a 40 punti BLEURT. È quindi fondamentale scegliere una buona strategia di prompting.

Nei nostri esperimenti siamo arrivati a una strategia di prompting a 5‑shot in cui marcavamo semplicemente ciascuna frase che forniamo al sistema con l’indicazione della lingua: per esempio, in una traduzione dal tedesco all’inglese, le frasi originali in tedesco sono segnalate con “German:” e le traduzioni in inglese con “English:”. Abbiamo osservato che la forma effettiva del prompting non ha un grande impatto quando si usano prompt relativamente lunghi. È cruciale per zero e one‑shot prompting. Quando passiamo a un prompting a 5‑shot, come nel nostro caso, la differenza con la forma del prompting è quasi nulla. Sono gli esempi stessi a portare il peso maggiore.

Il riepilogo dei nostri risultati sperimentali è che la qualità degli esempi è più importante della somiglianza con la frase di origine. È quindi importante selezionare esempi provenienti da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dal set di training per le valutazioni WMT con il set di sviluppo. Il set di sviluppo è molto più curato e di qualità superiore rispetto al training, che è più rumoroso. I risultati mostrano una migliore performance quando si utilizzano i dati di sviluppo. Tuttavia, i sistemi specializzati di stato‑dell’arte hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina abbastanza a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate.

Le intuizioni che abbiamo ottenuto dalla valutazione umana, effettuata usando il framework MQM, indicano che la fluidità di PaLM è comparabile a quella dei sistemi di stato‑dell’arte, ma la principale differenza risiede nella precisione. In particolare, gli errori più comuni sono errori di omissione. PaLM tende a produrre una traduzione che suona meglio, a volte omettendo parti della frase di origine. Tuttavia, la categoria “Style/Awkward” per PaLM è più bassa rispetto ai sistemi di stato‑dell’arte, un ulteriore segnale che PaLM fornisce un output davvero fluente, ma con ancora problemi di accuratezza.

Ecco qui per questa breve panoramica. Per maggiori dettagli, vi invito a partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao, sono Dawei, studente di dottorato presso l’Università di Saarland in Germania. In questo video presenterò il nostro lavoro recente “Weaker Than You Think: A Critical Look at Weakly Supervised Learning”, che è stato svolto in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.

Vorrei cominciare con una breve introduzione al concetto di weak supervision e al learning supervisionato debolmente. Nel weak supervision non si etichetta manualmente il dataset; invece si ricorre a fonti di etichettatura deboli, come regole euristiche semplici, knowledge‑bases o crowdsourcing di bassa qualità, come illustrato nella figura a destra. Rispetto alle annotazioni umane, quelle deboli sono molto più economiche, ma sono anche rumorose: una parte delle etichette è semplicemente errata. Se si addestra direttamente una rete neurale sui dati weak‑labelled, la rete tende a memorizzare il rumore e a non generalizzare.

Il learning supervisionato debolmente (WSL) propone algoritmi di training robusti in grado di addestrare reti neurali anche in presenza di rumore nelle etichette, in modo che il modello finale continui a generalizzare bene. In molti lavori recenti sul WSL si sostiene di addestrare i modelli solo su dati weak‑labelled e di ottenere alte prestazioni su set di test puliti. Tecnicamente questa affermazione è corretta, ma c’è una “trappola”: i metodi presuppongono l’esistenza di un set di validazione pulito per la selezione del modello. Questa esigenza di annotazioni addizionali non viene spesso menzionata, ma è fondamentale.

Questa osservazione ci ha portato a porci tre domande di ricerca:

1. **È davvero necessario un set di validazione pulito per il WSL, oppure possiamo usare un set di validazione rumoroso?**  
2. **Se è indispensabile un set pulito, quanti campioni puliti servono?**  
3. **Dovremmo limitare l’uso dei campioni puliti alla sola validazione, oppure esistono modi migliori per sfruttarli?**

Abbiamo risposto a queste domande nel nostro lavoro e i risultati sono i seguenti.

**Primo risultato**: i metodi WSL più recenti richiedono in effetti campioni di validazione puliti per funzionare correttamente. Se non disponiamo di un set pulito, il modello addestrato non riesce a generalizzare oltre le etichette deboli originali, rendendo il training inutile. In altre parole, i metodi WSL hanno bisogno di dati annotati manualmente per la selezione del modello e il costo di ottenere questi campioni non dovrebbe essere sottovalutato.

**Secondo risultato**: aumentare il numero di campioni puliti migliora le prestazioni dei metodi WSL, come mostrato nella figura a sinistra. In genere bastano circa 20 campioni per classe per raggiungere prestazioni elevate. Ma questo non è l’ultimo capitolo: se scegliamo di accedere ai campioni puliti, addestrare direttamente su di essi porta a risultati ancora migliori. La figura a destra confronta le prestazioni di approcci di fine‑tuning diretto (aplicati sui dati puliti) con gli approcci WSL che usano i campioni puliti solo per la validazione. Si vede che, con 10 campioni per classe, il fine‑tuning diretto inizia a superare i metodi WSL.

**Terzo risultato**: la differenza di prestazioni sostenuta dagli approcci WSL precedenti può essere facilmente raggiunta semplicemente continuando il fine‑tuning sui campioni di validazione puliti. Come si vede dalle figure, il modello “vanilla”, denominato FTw, inizialmente performa peggio di metodi più complessi come COSINE. Tuttavia, se permettiamo di continuare il fine‑tuning sui campioni puliti, FTw raggiunge prestazioni pari a quelle degli altri metodi. In pratica, non c’è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio su disco.

**Riepilogo**: abbiamo dimostrato che i recenti approcci WSL necessitano di campioni manualmente annotati per funzionare correttamente. Il guadagno di prestazioni e la praticità di queste tecniche sono spesso sopravvalutati. Le nostre raccomandazioni concrete per il futuro sono le seguenti:

1. **Rendere esplicito il criterio di selezione del modello**. Per esempio, indicare se la selezione è fatta tramite un set di validazione pulito.  
2. **Confrontare gli approcci WSL con baseline di few‑shot learning**, poiché entrambe le metodologie operano su campioni puliti.  
3. **Considerare il continuo fine‑tuning come una baseline semplice ma potente** da includere nei lavori futuri di WSL.  
4. **Open‑source del codice**: il nostro codice è disponibile tramite il QR code su questa slide. Sentitevi liberi di consultarlo.

Grazie e buon divertimento alla conferenza!</sample>
    <sample id="241">**Abstract**  
Il lavoro presenta un framework di valutazione “human‑in‑the‑loop” per la rilevazione precoce di disinformazione, con un caso di studio sui trattamenti per il COVID‑19. I sistemi automatici esistenti sono criticati per due motivi principali: (1) la validazione è spesso irrealistica, basandosi su dataset retrospettivi e vulnerabili a “counter‑evidence”; (2) i processi non coinvolgono adeguatamente i moderatori umani, che sono essenziali su piattaforme rumorose e in scala. Il framework proposto integra umani in più fasi, partendo dai tweet grezzi fino alla produzione di output gestibili dai moderatori.  

Il sistema è composto da due componenti principali. Il primo filtra i tweet per keyword, utilizza un modello T5 addestrato per il question‑answering (“What is the mentioned COVID‑19 cure?”) per estrarre claim, e li ordina per “trendiness” con Fisher’s Exact Test prima di passare alla verifica umana. Il secondo componenti impiega un classificatore BERT‑based stance per identificare tweet che supportano trattamenti non approvati, che vengono poi segnalati per revisione umana.  

Nel test sulla disinformazione precoce, il sistema rileva trattamenti non approvati prima della loro prima menzione nei notiziari di debunking. Valutazioni di policy‑violation con scala Likert mostrano un’accuratezza di 65 % e un throughput di 124,2 violazioni per ora di lavoro umano (claim + verifica). Con queste metriche il framework dimostra di catturare realisticamente l’interazione tra sistemi automatizzati e moderatori, offrendo una metodologia replicabile per futuri sistemi “human‑in‑the‑loop” di disinformazione.</sample>
    <sample id="242">I metodi di valutazione più diffusi per i sistemi di dialogo sono:

1. **Valutazione Likert** – giudizi di qualità su scala (es. 1‑5) a livello di **turno** o di intero **dialogo**.  
2. **Confronti a coppie (pairwise comparison)** – i revisori scelgono quale di due conversazioni è migliore.  

Questi approcci qualitativi basati su annotatori umani sono i più usati per confrontare modelli conversazionali.</sample>
    <sample id="243">Cinque autori sono coinvolti nell’articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Nel caso di Servin e Kea, la conoscenza di base richiesta è che **i giudici decidono i casi in un tribunale** (cioè che i giudici lavorano in un law court).</sample>
    <sample id="245">**Abstract**

Automatic evaluation metrics for text summarization often exhibit unreliable correlations with human judgment, and recruiting consistently high‑quality annotators on Amazon Mechanical Turk (MTurk) remains poorly understood. We propose a two‑step pipeline that combines pre‑task filtering with a qualification–endurance framework to identify workers who achieve high inter‑annotator agreement (IAA) while minimizing cost. Pre‑task qualifications restrict workers by location, HIT count, and approval rate. The qualification phase presents a training set followed by a test comprising three documents (one with an attention check) and a single summary rated on six dimensions; only “gold” and “silver” performers advance, yielding 26 workers (13 % of 200). The endurance phase administers 10 HITs on a single document with four saliency‑rated summaries; 12 workers (6 % of 200) pass, achieving a Krippendorff’s α of 0.443 and superior Cohen’s κ relative to experts. A reference‑based task (30 HITs) further evaluates coverage; pipeline workers attain α = 0.534. Baseline MTurk workers using MACE achieve α = 0.380, whereas CloudResearch workers reach α = 0.513 but with lower acceptance. Correlation analysis on 50 random samples shows significant Spearman’s ρ between pipeline and CloudResearch judgments, while generic GPT models align closely with expert scores. The pipeline thus delivers high‑agreement annotations at lower cost, comparable to premium platforms, and mitigates wasted effort on poor‑quality data. Future work will extend this method to multiple languages, tasks, and platforms, and explore strategies to guarantee training correctness. Limitations include English‑only evaluation, task‑specific design, and lack of perfect correctness assurance. We thank Google for funding this research.</sample>
    <sample id="246">Sì. Il codice (insieme al dataset) è pubblicato su GitHub – è possibile scaricarlo direttamente dal repository indicato nel paper.</sample>
    <sample id="247">**Abstract**

Fact verification has traditionally relied on free‑text or tabular evidence, yet knowledge graphs (KGs) provide a structured, reliable source of facts that can be directly queried. We introduce **FactKG**, the first dataset for *Knowledge‑Graph‑Based Fact Verification*, built on DBpedia. The dataset contains natural‑language claims in two styles—formal written and conversational colloquial—each labeled as **SUPPORTED** or **REFUTED**. Claims are annotated with five reasoning patterns: (1) *one‑hop* (single triple), (2) *conjunction* (multiple one‑hop assertions), (3) *existence* (verifying a relation for an entity), (4) *multi‑hop* (paths spanning several triples), and (5) *negation* (requiring an additional inference step). The colloquial claims are generated via a style‑transfer model and presupposition templates to emulate realistic user utterances. FactKG comprises 5,000 claims, with 2,500 in each style, and 60 % of the claims require multi‑hop reasoning. Baseline experiments demonstrate that models using only the claim text outperform the majority class baseline (51 % accuracy), while a KG‑aware model (GEAR) that retrieves and reasons over DBpedia triples achieves the best performance, underscoring the value of graph evidence. FactKG is publicly available and invites research on KG‑based fact verification, with applications ranging from dialogue systems to knowledge‑base consistency checks.</sample>
    <sample id="248">No. NLPositionality non mira a creare un campione *equilibrato* rispetto a ogni dimensione demografica (paese, genere, livello di istruzione, ecc.).  
Il progetto si basa su una re‑annotazione ampia (oltre 1 000 annotatori da 87 paesi) e su piattaforme come Lab in the Wild che favoriscono la diversità, ma non impone una distribuzione uniforme delle categorie. Di conseguenza, sebbene la base di annotatori sia variegata, i risultati mostrano ancora squilibri (ad es. maggiore allineamento con paesi anglofoni, con persone istruite a livello universitario e con minor rappresentanza di persone non binarie). In altre parole, si è privilegiata la diversità qualitativa più che la quantità equilibrata di ciascun gruppo.</sample>
    <sample id="249">Le frasi accettabili sono state alterate aggiungendo “rumore” pur mantenendo intatta la struttura sintattica principale: si scambiavano parole o si inserivano token irrilevanti, in modo da mantenere la forma grammaticale ma modificare il contenuto.</sample>
    <sample id="250">**Valutazione dimensionale** significa misurare più aspetti (dimensioni) della qualità di un dialogo – per esempio rilevanza, coerenza, empatia, errori di senso comune, ecc. – anziché affidarsi a un unico punteggio globale, ottenendo così una valutazione più fine‑grained e informativa.</sample>
    <sample id="251">Gli autori del lavoro sono affiliati all’Università della Scienza e della Tecnologia della Cina (University of Science and Technology of China).</sample>
    <sample id="252">**Abstract**

U‑CREAT (Unsupervised Case Retrieval using Events extrAcTion) introduces a novel framework for Prior Case Retrieval (PCR) in legal texts, coupled with a new benchmark dataset, the Indian Legal Prior Case Retrieval (IL‑PCR) dataset. IL‑PCR comprises 7,070 Indian court decisions, each citing an average of 6.8 precedents, featuring longer documents, a richer vocabulary, and a larger citation network than the existing COLIEE’21 Canadian benchmark. U‑CREAT leverages unsupervised learning and an event‑based representation of case narratives. Events are extracted via dependency parsing and subject‑verb‑object triplets, yielding a structured event matrix for each document. Retrieval models then compute interaction matrices between query and candidate events, ranking candidates by shared events. Experiments evaluate count‑based, transformer‑based (BERT, DistilBERT, DistilRoBERTa, InCaseLawBERT, InLegalBERT), and event‑based approaches (Atomic, Non‑Atomic, and Event‑Filtered Documents). Event‑based models markedly outperform baselines, with the Event‑Filtered Documents model achieving the highest F1 scores and lowest inference times. On COLIEE’21, U‑CREAT surpasses existing supervised methods, establishing the current state‑of‑the‑art for document‑level PCR. The work demonstrates that event‑centric, unsupervised retrieval generalizes across legal systems without domain‑specific tuning, offering a scalable solution for legal professionals facing voluminous precedent corpora.</sample>
    <sample id="253">**Abstract**

We introduce **DisorBERT**, a double‑domain adaptation framework designed to detect signs of mental disorders in social‑media posts. Starting from the general‑domain BERT model, we first adapt it to the informal linguistic style of Reddit and then further specialize it to the mental‑health domain. Guided masking, driven by a hand‑crafted lexicon of mental‑health terms, biases the training objective toward semantically relevant tokens, encouraging the model to attend to symptoms and affective cues. We evaluate DisorBERT on the eRisk 2020 dataset, reporting a balanced precision‑recall profile that surpasses baseline models, including the state‑of‑the‑art MentalBERT. Ablation studies confirm that both domain‑adaptation stages and the guided masking contribute to performance gains. Additionally, we analyze model predictions on Beck’s Depression Inventory items, illustrating that DisorBERT generates more clinically relevant completions compared to vanilla BERT. Attention visualizations on high‑BDI users reveal salient terms such as “anxious” and “medication,” underscoring the model’s focus on disorder‑related content. Future work will explore alternative lexical resources, incorporation of clinical data, and broader application to other psychiatric conditions. DisorBERT demonstrates that sequential domain adaptation coupled with lexical guidance yields robust, interpretable detection of mental‑health signals in user‑generated text.</sample>
    <sample id="254">**Abstract**

Il documento propone un nuovo framework per l’estrazione di relazioni a livello di documento (DocRE) basato su dati distamente supervisionati (DS). La principale difficoltà con i dati DS è la presenza di rumore, soprattutto a causa di pseudo‑label errati che possono introdurre relazioni false (ad esempio “composer”) e far perdere quelle corrette (“place of birth”). Per mitigare questo problema, il metodo introduce una stima dell’incertezza delle previsioni del modello DocRE. Utilizzando *Monte Carlo dropout*, il modello effettua più passaggi stocastici, ottenendo una distribuzione di probabilità per ciascuna relazione. Tuttavia, la stima tradizionale non è adatta quando un’istanza può presentare più relazioni sovrapposte. Perciò si propone un approccio a livello di istanza che calcola l’incertezza per ogni pseudo‑label positivo, ottenendo così una misura più fine. Analizzando la distribuzione dell’incertezza per classe, si osserva che le classi frequenti hanno valori medi più bassi rispetto a quelle lungo coda; di conseguenza si definiscono soglie dinamiche di incertezza per ciascuna classe. I pseudo‑label con incertezza superiore a tali soglie vengono rimossi e sostituiti con quelli più affidabili. Infine, il modello viene addestrato in più fasi successive, in cui ogni fase rielabora i DS con i nuovi pseudo‑label, migliorando progressivamente la qualità delle annotazioni. I risultati su due dataset pubblici mostrano un miglioramento significativo rispetto ai baseline, confermando l’efficacia della denoising guidata dall’incertezza.</sample>
    <sample id="255">La forma del prompt è rilevante **solo nei casi di zero‑shot e uno‑shot prompting**.  
Per prompt più lunghi (ad es. 5‑shot) o quando si forniscono più esempi, la struttura del prompt non influisce significativamente sul risultato.</sample>
    <sample id="257">I ricercatori hanno valutato **quattro** modelli di dialogo di ultima generazione (state‑of‑the‑art).</sample>
    <sample id="258">**Abstract (≈200 parole)**  

Il lavoro “Can Large Language Models Be an Alternative to Human Evaluation?” esplora l’uso di modelli linguistici di grandi dimensioni (LLM) per sostituire la valutazione umana nella valutazione di testi generati da reti neurali. Gli autori propongono di fornire agli LLM istruzioni in linguaggio naturale che dettagliano i criteri di valutazione (grammatica, coerenza, piacevolezza e pertinenza) e di chiedere ai modelli di assegnare punteggi ai campioni. Per verificare la validità di questa procedura, i ricercatori valutano storie generate da GPT‑2 e storie scritte da umani, confrontando le valutazioni degli LLM con quelle ottenute da insegnanti di inglese, considerati esperti nel compito di valutazione di saggi.

I risultati mostrano che alcuni LLM (davinci e ChatGPT) riescono a distinguere efficacemente tra testi umani e automatizzati, mostrando una preferenza per i testi umani quasi identica a quella dei valutatori umani. Al contrario, modelli più piccoli (T0, curie) non riescono a rilevare tale differenza. Gli autori approfondiscono inoltre la robustezza delle valutazioni rispetto a variazioni di formulazione delle istruzioni, a differenti strategie di sampling delle risposte e a considerazioni di costo‑beneficio rispetto alla valutazione umana. Infine, presentano risultati preliminari sull’applicazione di questa metodologia a compiti di valutazione più ampi.  

In sintesi, lo studio dimostra che, sebbene non tutti i modelli siano adatti, alcuni LLM possono fungere di sostituto affidabile della valutazione umana, offrendo vantaggi in termini di velocità, replicabilità e costo.</sample>
    <sample id="259">**Abstract (≈200 words)**  

In questo lavoro presentiamo **XSemPLR**, un benchmark unificato per il *cross‑lingual semantic parsing* che copre 22 lingue (15 famiglie linguistiche), 9 domini, 5 compiti di parsing e 8 rappresentazioni di significato (SQL, Lambda‑calculus, FunQL, ecc.). Il dataset comprende traduzioni di query in molteplici lingue, permettendo di esplorare sei configurazioni di addestramento ed inferenza: *Translate‑Test*, *Monolingual*, *Monolingual Few‑Shot*, *Multilingual*, *Zero‑Shot* e *Few‑Shot* cross‑lingual.  

Abbiamo valutato tre classi di modelli multilingue: encoder‑pointer (XLM‑R + PTR, mBERT + PTR) e encoder‑decoder (mBART, mT5). L’analisi mostra che i modelli encoder‑decoder superano gli encoder‑pointer in tutti i dataset; in particolare, mT5 e XLM‑R + PTR beneficiano significativamente dall’addestramento su un mix di lingue, con l’unico contro‑effetto osservato sul performance in inglese (fenomeno “Curse of Multilinguality”).  

Il gap di performance tra zero‑shot e monolingual è sostanziale, ma si riduce rapidamente con la disponibilità di pochi esempi (few‑shot). Pre‑addestrare su inglese aumenta notevolmente la capacità di trasferimento, mentre modelli di grande scala come Codex e BLOOM risultano inadeguati per questa attività.  

XSemPLR fornisce quindi un quadro completo per lo sviluppo e la valutazione di sistemi di parsing semantico multilingue.</sample>
    <sample id="260">1 autori.</sample>
    <sample id="261">Un buon pianificatore deve:

1. **Scrivere script ragionevoli** – le sequenze di azioni devono essere logiche e realistiche.  
2. **Essere fedele ai vincoli** – ogni requisito specifico (es. tipo di prodotto, risorse disponibili, tempi) deve essere rispettato.  
3. **Garantire completezza semantica** – l’intero piano deve coprire tutti gli aspetti necessari per raggiungere l’obiettivo.  
4. **Gestire vincoli multi‑fasettati** – adattarsi a diverse categorie di restrizioni (tempi, materiali, condizioni, ecc.).  

In sintesi, un pianificatore ideale combina ragionevolezza, fedeltà ai vincoli e completezza semantica.</sample>
    <sample id="262">Non è indicato nel testo fornito.</sample>
    <sample id="263">**Abstract**

Il lavoro “Mitigating Label Biases for In‑context Learning” affronta la instabilità dei modelli linguistici di grandi dimensioni (LLM) quando impiegati in‑context per compiti di classificazione. Gli autori identificano tre fonti di bias delle etichette: *vanilla‑label bias* (preferenziazione intrinseca delle etichette), *context‑label bias* (influenza degli esempi di contesto) e, in modo originale, *domain‑label bias* (influenza del corpus di dominio). Per evidenziare quest’ultimo, vengono presentati esperimenti in cui, a differenza di parole inglesi casuali, parole casuali estratte dal dominio di task spostano significativamente le predizioni del modello, soprattutto in compiti con forte bias di dominio.

Per mitigare tali bias, gli autori propongono **Domain‑Context Calibration (DCC)**: una procedura di calibrazione che utilizza testi “content‑free” costituiti da parole casuali del dominio per stimare il bias specifico su ogni etichetta e correggere le predizioni originali. Rispetto alle tecniche esistenti (che impiegano token predefiniti come “not available”), DCC riduce l’effetto di tutti i bias in modo olistico.  

Sperimentazioni su diversi LLM (incluso GPT‑3) e su una varietà di dataset mostrano che DCC migliora significativamente le performance medio‑globali, con guadagni maggiori sui compiti a forte domain‑label bias. Analisi approfondite confermano che l’uso di più token casuali e l’adozione di parole in‑domain sono chiavi per il successo della calibrazione. Il paper fornisce inoltre una tipologia sistematica dei bias di etichetta e un framework replicabile per la loro mitigazione.</sample>
    <sample id="264">**Abstract**

This paper introduces **TAVT** (Transferable Audio‑Visual Text Generation), a novel multimodal generation task aimed at producing coherent textual descriptions from audio‑visual inputs while being resilient to domain shifts. Existing methods suffer from heavy annotation costs and severe performance drop across domains due to variations in visual style, audio energy, and other modality‑specific factors. TAVT addresses these challenges by aligning visual concepts across domains within a unified audio semantic space. The proposed framework comprises three key modules:  

1. **Audio‑Visual Meta‑Mapper** – clusters a large collection of audio clips (via K‑means) to form a shared audio semantic space and learns visual‑prefix tokens that map visual content to this space through a reconstruction objective.  
2. **Audio‑Visual Encoder &amp; Language Generator** – a transformer‑based encoder receives both modalities, while a language decoder generates text. An adaptive weighting factor α_t modulates each modality’s contribution to each generated token based on cross‑attention relevance.  
3. **Dual Counterfactual Contrastive Learning (DCLL)** – generates fine‑grained supervision signals from counterfactual pairs to directly optimize visual‑textual alignment, circumventing reliance on random negatives.  

The model employs a meta‑learning strategy akin to MAML, sampling support/query domains during training and fine‑tuning on few‑shot target domains. Experiments on cross‑dataset and cross‑domain splits of MSVD and MSR‑VTT demonstrate that TAVT surpasses state‑of‑the‑art RNN and transformer baselines by a large margin, especially on low‑resource domains such as “Kids” and “Beauty”. Ablation studies confirm the critical role of audio features and counterfactual contrastive supervision in achieving transferable performance.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">**Affiliations of the article’s authors**  
- **Adam Przepiórkowski** – University of Warsaw, Institute of Computer Science (Poland).  
- *Other co‑authors (if any) are listed with the same institutional affiliation in the full paper.*</sample>
    <sample id="268">I più frequenti errori di PaLM sono **omissioni**: il modello tende a generare traduzioni più fluide ma a volte omette parti del testo di origine, sacrificando così l’accuratezza.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABC‑Eval, un nuovo approccio dimensionale per valutare l’IA conversazionale. Questo lavoro è stato realizzato dal laboratorio NLP di Emory, guidato dal Professor Jinho Choi dell’Università di Emory, in collaborazione con Amazon Alexa AI.

Immaginate di aver appena sviluppato un modello di dialogo e di voler capire quanto si confronta con lo stato dell’arte attuale. La pratica comune è quella di utilizzare la valutazione umana, ad esempio chiedendo ai giudici di scegliere quale di due conversazioni sia migliore o di valutare le conversazioni secondo una scala Likert. Questi approcci funzionano bene per fornire valutazioni olistiche della qualità complessiva del dialogo, ma la qualità del dialogo ha molteplici aspetti. Pertanto, potreste voler valutare più dimensioni della qualità della chat per comprendere in modo più dettagliato i punti di forza e di debolezza del modello.

Un approccio è semplicemente chiedere ai giudici di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi esistenti o scale Likert. Tuttavia, riteniamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi.

Chiamiamo questo approccio l’annotazione dei comportamenti nella chat, o ABC‑Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti dei modelli di chat che sono stati suggeriti nella letteratura recente per influenzare la qualità della chat. ABC‑Eval è in grado di misurare la frequenza con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC‑Eval misura il numero di turni in cui un modello di chat ignora il partner o dice qualcosa di irrilevante, si contraddice o contraddice il partner, crea fatti errati o viola la conoscenza del buon senso, e quando il modello riesce o fallisce nel mostrare empatia.

Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chat all’avanguardia e li abbiamo valutati su 100 conversazioni uomo‑bot per modello utilizzando ABC‑Eval. Per confronto, abbiamo anche valutato queste conversazioni con tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e comparazioni a coppie a livello di dialogo.

Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.

Dalla nostra analisi di questi risultati di valutazione, abbiamo scoperto che le etichette di comportamento di ABC‑Eval sono complessivamente più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall’accordo tra annotatori su 100 conversazioni doppiamente etichettate.

Inoltre, le etichette di ABC‑Eval sono più prevedibili della qualità complessiva della conversazione rispetto alle metriche generate dai metodi esistenti, come dimostrato da questa semplice analisi di regressione lineare.

Ad esempio, potete vedere come misurare la proporzione di turni con contraddizioni proprie e con il partner spiega rispettivamente il 5 % e il 10 % della qualità della conversazione, mentre i punteggi medi di coerenza Likert spiegano solo il 4 % o meno.

Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare stepwise. Potete vedere come la combinazione di tutte le metriche ABC‑Eval spiega oltre il 25 % della qualità della conversazione, e man mano che rimuovete le metriche una alla volta, la maggior parte di esse comporta la perdita di una discreta quantità di informazioni sulla qualità.

Al contrario, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche.

Queste metriche ABC‑Eval affidabili, informative e distinte ci consentono di valutare l’IA conversazionale con una risoluzione più alta rispetto ai metodi precedenti.

Potete vedere nei risultati del nostro esperimento che diverse sfide rimangono ancora e sono state precisamente quantificate. Ad esempio, i bot che abbiamo testato hanno violazioni del buon senso in circa il 20 % delle loro risposte. Producono informazioni irrilevanti in circa il 15 % delle risposte, e si contraddicono o contraddicono il partner in circa il 10 % delle volte.

Con il rapido ritmo di miglioramento del campo, molte di queste percentuali di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione. Tuttavia, questo è ancora più motivo di perseguire metriche di valutazione affidabili e precise per confrontare i modelli.

Ci auguriamo che ABC‑Eval possa essere sfruttato da altri nel settore come un passo significativo in questa direzione.

E non vediamo l’ora di vedere come l’IA conversazionale progredirà nei prossimi mesi e anni. Grazie per averci ascoltato.</sample>
    <sample id="270">Gli autori sono affiliati a:

- **Emory University** – Emory NLP Lab (dirigito da Jinho Choi)  
- **Amazon Alexa AI** (collaborazione con Emory NLP Lab)</sample>
    <sample id="271">CFT sta per **Continuous Fine‑Tuning** (fine‑tuning continuo). È la procedura semplice che, dopo l’addestramento sui dati deboli, prosegue il fine‑tuning anche sui campioni di validazione puliti, ottenendo prestazioni comparabili a quelle delle tecniche di WSL più complesse.</sample>
    <sample id="272">7 autori sono coinvolti nell’articolo.</sample>
    <sample id="273">Buongiorno, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato **“Quando la traduzione richiede contesto? Un’analisi data‑driven e multilingue”**.  Questo studio è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.

Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo “mole” in questa frase? Se la frase precedente fosse *“Things could start to get dangerous if the ministers find out”*, “mole” indica un spia. Se invece la frase precedente fosse *“Could it be anything serious, doctor?”*, “mole” indica una macchia cutanea. In altre parole, il significato della parola cambia a seconda del contesto, e quindi anche la sua traduzione.

Valutare quanto bene i modelli gestiscono questi casi è però difficile. In primo luogo, solo una piccola frazione delle traduzioni dipende dal contesto, quindi metriche di livello corpus come BLEU non sono in grado di coglierle. In secondo luogo, alcune proposte di valutazione mirata si concentrano su tipi limitati di traduzioni contestuali e su un numero ristretto di lingue, perché si basano su conoscenza di dominio e su curazione umana.

In questo lavoro ci proponiamo di rispondere a due domande:
1. **Quando la traduzione richiede contesto?**  
2. **Quanto bene i modelli gestiscono questi casi?**

Per la prima domanda, abbiamo prima misurato quanto una parola dipenda dal contesto durante la traduzione. Nella ricerca precedente abbiamo introdotto il **CXMI** (Conditional Cross‑Language Mutual Information) come misura di quanto il contesto *C* fornisca informazioni sulla destinazione *Y*, dato il sorgente *X*.  Si può pensare al CXMI come all’informazione aggiuntiva che si ottiene fornendo contesto al modello.

In questo studio abbiamo esteso il CXMI a una versione **Pointwise CXMI (P‑CXMI)**, che permette di misurare l’uso del contesto a livello di frase o di parola.  Le parole con alto P‑CXMI sono quindi quelle che richiedono contesto per essere tradotte correttamente.

Abbiamo analizzato i termini con alto P‑CXMI per individuare schemi tra queste parole. L’analisi è stata eseguita su trascrizioni di TED‑talk che sono state tradotte dall’inglese a 14 lingue diverse, e a tre livelli diversi:

1. **Etichette parte‑di‑parola** con alto P‑CXMI medio.  Ad esempio, abbiamo scoperto che i pronomi **dual** in arabo hanno un P‑CXMI relativamente alto, perché l’inglese non ha pronomi duali e occorre il contesto per determinarne la presenza. Allo stesso modo, alcune lingue richiedono contesto per scegliere la forma verbale corretta.

2. **Vocabolario** con alto P‑CXMI medio su tutte le sue occorrenze.  Ciò ci ha permesso di individuare casi come quello in cui, in cinese, è necessario il contesto per tradurre correttamente i nomi propri, assicurandosi che la stessa traduzione sia usata in tutto il documento. Abbiamo anche scoperto che il contesto è importante per la scelta della formalità.

3. **Token individuali** con alto P‑CXMI.  Questo ci ha permesso di identificare fenomeni che non si possono catturare solo con la parola, ma che sono espressi dalla struttura della frase, come la risoluzione delle ellissi.

Con le nostre scoperte, abbiamo progettato un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, abbiamo creato tagger automatici per individuare le parole associate al fenomeno. Abbiamo chiamato questo tagger **MuDA tagger** (Multilingual Discourse‑Aware).  Inoltre, abbiamo osservato che le proporzioni di questi fenomeni variano tra le lingue.

Utilizzando il MuDA tagger, applichiamo il tagger su un corpus parallelo che vogliamo valutare e calcoliamo le metriche di traduzione – come BLEU, COMET, o la misura F‑score a livello di parola – solo sugli esempi che il tagger ha identificato come dipendenti dal contesto.

**Risultati di valutazione**:
- Con metriche a livello corpus, come BLEU, i modelli **senza contesto** ottengono le migliori prestazioni.
- Con COMET, i modelli **con contesto** sono i migliori.
- Con la misura F‑score a livello di parola, modelli con e senza contesto hanno prestazioni comparabili.

Questi risultati mostrano che è difficile stabilire quale sia il miglior sistema di traduzione a livello di documento se si basano solo su metriche a livello corpus.

Con il benchmark MuDA, abbiamo valutato diversi modelli e scoperto che i modelli **con contesto** sono significativamente più precisi di quelli senza contesto per determinati fenomeni di discorso, come la formalità e la coesione lessicale. Tuttavia, per altri fenomeni come l’ellipsis, i pronomi e la forma verbale, i modelli con contesto non mostrano un miglioramento notevole. Questo indica dove è necessario fare più progressi nella traduzione a livello di documento.

Abbiamo anche confrontato diversi sistemi commerciali e il benchmark ha mostrato che **DeepL** è solitamente più accurato di **Google Translate** per la traduzione a livello di documento.

**In sintesi**, abbiamo condotto un’analisi data‑driven su 14 coppie di lingue per identificare quando la traduzione richiede contesto, e abbiamo usato queste scoperte per costruire un benchmark che aiuta a capire quali fenomeni di discorso i modelli gestiscono bene o meno, e quale sistema di traduzione è migliore per il livello di documento.

Grazie per l’attenzione. **Ci vediamo a Toronto!**</sample>
    <sample id="274">Il relatore è **Yusen Zhang**.</sample>
    <sample id="276">**Abstract**  
IndicMT Eval is the first large‑scale meta‑evaluation benchmark for machine translation (MT) metrics on Indian languages. We collected 7,000 English‑to‑Indian‑language translations across five languages (Tamil, Malayalam, Hindi, Marathi, Gujarati) from seven commercial and open‑source MT systems, generating 1,400 candidate outputs per language. Bilingual expert annotators performed fine‑grained quality assessment using an MQM‑style scheme: each output was scored overall and annotated for error type (accuracy, fluency, or special) and severity. The resulting dataset enables a detailed study of how existing MT metrics correlate with human judgments in non‑English directions.

We evaluated 18 popular metrics (BLEU, SacreBLEU, chrF, BERTScore, COMET variants, etc.) on the dataset. Overlap‑based metrics consistently underperformed, while embedding‑based metrics (especially BERTScore with MuRIL) and COMET variants achieved the highest Pearson and Kendall‑tau correlations. However, many metrics exhibited a skewed score distribution, limiting interpretability. Analysis on error‑type subsets revealed higher correlations for accuracy‑only errors than for fluency errors.

Building on these insights, we fine‑tuned COMET on IndicMT Eval, producing IndicCOMET variants. IndicCOMET MQM outperformed all baselines on three of five languages and maintained superior zero‑shot performance on unseen languages. It also achieved a higher robustness correlation (0.36) on the ACES Translation Accuracy Challenge Sets compared to the original COMET. The IndicMT Eval dataset and fine‑tuned models are publicly released to foster further research in MT metric evaluation for Indian languages.</sample>
    <sample id="277">Il nuovo metodo è chiamato **“Multiset Tagging and Latent Permutations”**.</sample>
    <sample id="278">L’autore ha descritto **“Marked Words”** come un approccio che sfrutta il concetto sociolinguistico di *markedness*: i gruppi dominanti sono considerati “unmarked” (default), mentre i gruppi marginalizzati sono “marked”. Il metodo confronta le personas generate per un gruppo marcato con quelle di uno o più gruppi unmarked (ad esempio, white vs. black women, women vs. men) e calcola i log‑odds ponderati (Fightin’ Words) per trovare le parole più fortemente associate al gruppo marcato. In pratica, identifica le parole che distinguono linguisticamente i gruppi marginalizzati dai gruppi di riferimento, evidenziando così stereotipi e narrazioni essenzializzanti.</sample>
    <sample id="279">Le autori di questo lavoro sono affiliati all'Università di Washington (Shangbin è uno studente di dottorato presso questa istituzione).</sample>
    <sample id="280">**Abstract**  
Emotion Recognition in Conversations (ERC) aims to classify the affective state of each utterance by exploiting textual, audio, and visual signals. Existing approaches largely rely on text or simple concatenation for multimodal fusion, leading to sub‑optimal use of complementary cues, poor performance on minority emotion classes, and difficulty distinguishing semantically similar emotions. To address these issues, we propose **MultiEMO**, an end‑to‑end framework comprising four components: unimodal feature extraction, contextual modeling, multimodal fusion, and classification.  
1. **VisExtNet** replaces scene‑heavy visual encoders with a facial‑expression‑centric extractor that fuses multiple frames via MTCNN and a VGGFace‑pretrained ResNet‑101, eliminating irrelevant background information.  
2. **MultiAttn** applies bidirectional multi‑head cross‑attention across modalities, sequentially aligning text with audio and then with visual cues, followed by a feed‑forward memory module and residual connections.  
3. **Sample‑Weighted Focal Contrastive (SWFC) Loss** emphasizes hard, minority samples and enforces inter‑class separation, thereby improving discrimination of similar emotions.  
Extensive experiments on MELD and IEMOCAP demonstrate state‑of‑the‑art accuracy, with notable gains on low‑frequency and semantically adjacent classes. Limitations include speaker‑agnostic visual modeling, the need for large batch sizes for SWFC, and residual performance gaps for minority emotions. Nonetheless, MultiEMO offers a comprehensive, attention‑based solution for multimodal ERC.</sample>
    <sample id="281">**Abstract (≈200 parole)**  

In questo lavoro indaghiamo quando la traduzione di una parola o di una frase richiede l’uso del contesto documentale. Per misurare l’interdipendenza tra contesto \(C\), sorgente \(X\) e target \(Y\) introduciamo l’indicatore **CXMI** (Context‑to‑Target Mutual Information). Estendiamo questa misura a livello puntuale (P‑CXMI) in modo da valutare, sia a livello di frase sia di token, quanto l’informazione aggiuntiva fornita dal contesto migliora la traduzione. Analizzando trascrizioni di TED‑talk tradotte dall’inglese a 14 lingue, scopriamo pattern interessanti: pronome duale in arabo, scelta di verbi in lingue che non hanno forme equivalenti in inglese, coerenza lessicale in cinese e variazioni di formalità. Inoltre, P‑CXMI evidenzia fenomeni strutturali come l’ellissi.  

Da queste analisi sviluppiamo il tagger multilingue **MuDA** (Multilingual Discourse‑Aware), capace di identificare automaticamente i token che richiedono contesto per ciascuno dei cinque fenomeni discorsivi identificati. Utilizzando MuDA, costruiamo un benchmark document‑level che valuta specificamente le traduzioni contestuali. Con questo benchmark confrontiamo modelli context‑aware e context‑agnostic: metrica corpus‑level (BLEU) favorisce i modelli senza contesto, mentre COMET evidenzia il vantaggio dei modelli con contesto. MuDA mostra che i modelli con contesto migliorano significativamente per formalità e coerenza lessicale, ma non per ellissi, pronomi o forme verbali. Infine, confrontiamo sistemi commerciali: DeepL supera Google Translate per la traduzione documentale. Il lavoro fornisce quindi una prospettiva data‑driven sui casi che richiedono contesto e un benchmark per valutare i progressi in MT documentale.</sample>
    <sample id="282">**Abstract**

Non‑parallel text style transfer has traditionally been limited to token‑ or sentence‑level manipulation, leaving the rich discourse‑level characteristics of authorial style largely unexplored. We introduce **StoryTrans**, a two‑stage generation framework that performs **non‑parallel story‑level author‑style transfer** while preserving narrative coherence and content. In the first stage, StoryTrans learns a discourse representation of the source story and disentangles style from content using a combination of self‑reconstruction, sentence‑level disentanglement, sentence‑order, and style‑classification losses. This yields style‑agnostic sentence embeddings that capture the underlying discourse structure. Style is re‑imposed via learnable style embeddings, and style‑specific content keywords are masked to prevent contamination of the discourse representation. The second stage fine‑tunes the model to recover the masked keywords and produce fluent, style‑consistent text, independent of the style‑transfer objective. We devised a novel objective that pulls representations of texts from different styles closer in latent space, thereby reducing style leakage. 

To evaluate our approach, we curated two large, non‑parallel corpora in Chinese and English, containing fairytales and everyday stories paired with distinct authorial styles. Automatic metrics (BLEU, Style Accuracy, Content Preservation) and human judgments demonstrate that StoryTrans outperforms strong baselines in both style control and content fidelity. Visualizations of the style‑feature space confirm that generated stories align closely with target‑style exemplars. Finally, case studies show that unlike existing models (e.g., StyleLM), StoryTrans avoids inserting unrelated sentences and instead enriches the storyline with concise, style‑appropriate additions. Our code and datasets are publicly available.</sample>
    <sample id="283">La prima struttura di dipendenza **simmetrica** menzionata è quella del **“Hudson’s Word Grammar”** (l’approccio multi‑head in cui tutti i congiunti sono teste).</sample>
    <sample id="284">**Abstract**

We introduce FSUIE, a span‑based Universal Information Extraction (UIE) framework that mitigates the brittleness of precise span boundary supervision by learning *fuzzy* boundaries. Traditional span models over‑rely on exact start/end positions, yet the gold span boundaries are often ambiguous, leading to suboptimal generalization. FSUIE replaces rigid boundary labels with a continuous probability distribution over a span range \([R_{\min}, R_{\max}]\). The distribution is parameterized by a correctness function \(Q\) and is discretized via a sampling function to compute a fuzzy‑span loss comprising a binary cross‑entropy term against the hard labels and a KL‑divergence term against the fuzzy distribution. To further align attention with these soft boundaries, we propose a *fuzzy span attention* mask \(G\) that adaptively scales the attention window: an optimizable range parameter \(\Delta\) controls the span length, while the mask decays linearly at the edges rather than truncating abruptly. This layer is inserted only at the top transformer level, preserving the encoder’s global representations.

We evaluate FSUIE on three core IE tasks—named entity recognition (NER), relation extraction (RE), and aspect‑sentiment triplet extraction (ASTE). On NER, FSUIE‑base outperforms UIE‑base, especially on small datasets, indicating faster convergence. On RE, it sets new state‑of‑the‑art results on ACE2004/2005 and ADE, demonstrating strong cross‑domain generalization. On ASTE, FSUIE attains SOTA scores on 14lap, 15res, and 16res of the AST‑V2 benchmark. Ablation studies confirm that the fuzzy span attention (FSA) accelerates convergence and that the fuzzy span loss (FSL) improves extraction accuracy; their combination yields the largest gains. Visualizations of the attention distribution reveal that the model focuses on a limited range of preceding tokens, consistent with the intended semantic locality. FSUIE thus provides a unified, efficient, and robust approach to UIE across diverse tasks.</sample>
    <sample id="285">**Abstract (≈200 words)**  

Il lavoro di Mingqi Gao e colleghi presenta una revisione critica della correzione di errori fattuali (FEC) nelle sintesi di dialoghi e propone un nuovo approccio di valutazione. Attualmente, i modelli FEC vengono valutati principalmente con metriche di factualità (FactCC, DAE) che restituiscono un singolo punteggio, rendendo difficile discernere se il modello effettivamente corregga gli errori o semplicemente generi un nuovo riassunto più accurato. Inoltre, l’uso di metriche aggregate può confondere le due strategie principali di miglioramento della factualità: l’ottimizzazione del modello di sintesi e l’uso di un FEC esterno.  

Per superare queste lacune, gli autori introducono **correzioni di riferimento annotate manualmente**, che servono sia al training che alla valutazione. Propongono una **taxonomia di errori fattuali** divisa in categorie “basate sul contenuto” (parte del discorso e dipendenze) e “basate sulla forma” (inserimento, cancellazione, sostituzione). L’**framework di valutazione** si ispira a ERRANT, prevedendo allineamento, classificazione e confronto delle modifiche.  

Le sperimentazioni con diversi modelli FEC mostrano che: (1) il training con riferimenti umani migliora la factualità più di dati sintetici; (2) l’uso combinato di dati umani e sintetici è promettente; (3) i modelli attuali faticano a correggere errori di aggiunta e altri tipi di errori complessi. Questi risultati sottolineano la necessità di metodi di valutazione più fine‑grained e di dataset di correzione annotati per avanzare la FEC in sintesi di dialoghi.</sample>
    <sample id="286">Il relatore (o la relatrice) è **James Finch** (con Sarah Finch che co‑partecipa).</sample>
    <sample id="287">Quattro autori sono coinvolti nell’articolo.</sample>
    <sample id="288">I sintattici vengono testati principalmente con i dataset **BLiMP** e **SyntaxGym** (che contengono coppie minime su varie strutture grammaticali).  
Per verificare l’effetto di contesto non pertinente si usa **Wikipedia** (dominio completamente diverso).  
Inoltre, per la valutazione di accettabilità in termini di stereotipi si menziona **CrowS‑pairs**.</sample>
    <sample id="290">**Abbreviazioni dei cinque metodi valutati nella prima domanda di ricerca**

| Abbreviazione | Nome completo (metodo WSL) |
|---------------|-----------------------------|
| **FTw** | Fine‑Tune with weak labels (basico “vanilla” modello) |
| **COS** | COSINE – il metodo proposto nel nostro studio |
| **PL** | Pseudolabeling (self‑labeling delle previsioni) |
| **CT** | Co‑Teaching (due modelli che si scambiano le etichette più affidabili) |
| **ST** | Self‑Training (addestramento iterativo con etichette pseudo‑generate) |

Queste cinque abbreviazioni sono state usate per confrontare le performance dei vari approcci WSL quando si dispone o non si dispone di un set di validazione pulito.</sample>
    <sample id="291">Il modello è testato su 11 compiti di downstream biomedicali e clinici, tra cui:

- Named‑Entity Recognition (NER)  
- Classificazione di testi (es. classificazione di documenti clinici)  
- Part‑of‑Speech tagging (POS)  
- Question answering (QA)  

Questi compiti coprono una gamma completa di attività tipiche del settore sanitario.</sample>
    <sample id="294">CamemBERT è stato originariamente pre‑addestrato su **OSCAR**, un enorme corpus di testi francese raccolti dal web (circa 138 GB).  
In pratica, il modello parte dai token‑embedding e dalla tokenizzazione di RoBERTa, ma i suoi pesi vengono addestrati su OSCAR, che fornisce una copertura ampia e variegata del francese.</sample>
    <sample id="295">L’autore del talk è **Adam Przepiórkowski**.</sample>
    <sample id="296">**Abstract**

This work presents EPIC (English Perspectivist Irony Corpus), a large-scale, multimodal dataset for studying irony detection beyond binary classification. Collaborating with Amazon Alexa and the University of Turin, we collected 300 short conversational exchanges from Reddit and Twitter, covering five English varieties (American, British, Australian, Canadian, and Indian) over a 1½-year period. Using the Prolific crowdsourcing platform, 74 annotators (≈15 per variety) independently labeled each exchange as “ironic” or “not ironic” with additional attention checks, yielding an average of five annotations per instance. The resulting inter‑annotator agreement varied systematically across demographic axes (gender, age, nationality), as shown in violin plots. To capture these perspectival differences, we trained *perspective‑aware* models by fine‑tuning a pre‑trained language model on splits corresponding to distinct annotator groups. While overall classification performance remained comparable to gold‑standard aggregated models, perspective‑aware models exhibited higher confidence and lower uncertainty in predictions. Analysis revealed that adjacent age cohorts and geographically proximate annotators (e.g., UK vs. Ireland) produced the greatest disagreement. These findings suggest that irony perception is highly context‑dependent and that incorporating annotator perspective can improve model calibration and interpretability. Future work will explore richer annotation schemes and further demographic analyses.</sample>
    <sample id="297">**Abstract (circa 200 parole)**  

Il lavoro “From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models” affronta la diffusione e la rilevazione di retorica codificata, in particolare *dogwhistles* (termine “cosmopolitan”, “sex‑based rights”, ecc.) che trasmettono un messaggio duplice: un’affermazione apparente e un’implicatura nascosta destinata a un gruppo di parte. Viene presentato un glossario di oltre 340 termini e simboli, raccolti da fonti accademiche, Wikipedia e blog, annotati con persona (antisemitismo, razzismo, transphobia), registro (formale/informale) e tipo di implicatura.  

Il progetto include un case study storico sui discorsi politici americani: l’analisi del Congressional Record mostra un incremento dei dogwhistles, in particolare quelli razziali, in linea con la strategia del Sud repubblicana post‑diritti civili, e un legame crescente con la conservatività nel tempo.  

Successivamente, l’efficacia di modelli linguistici (GPT‑3) nella generazione e riconoscimento di dogwhistles viene valutata attraverso prompt variegati; i risultati indicano buona performance sui termini formali ma lacune sui social‑media e sui transphobic.  

Infine, un esperimento su un modello di rilevazione di tossicità (Prospective API) mostra che sostituire slur standard con dogwhistles riduce significativamente la valutazione di tossicità, evidenziando come tali codifiche eludano i sistemi di moderazione.  

Il lavoro conclude con l’importanza di una tipologia strutturata e di approcci di rilevazione automatica per mitigare la diffusione di retorica di odio nascosta.</sample>
    <sample id="298">I risultati chiave sono:

1. **Analisi del “over‑fitting adattivo”** – la curva di performance sul nuovo set (CoNLL++) mostra una pendenza &gt; 1, quindi ogni miglioramento su CoNLL‑2003 porta a un miglioramento maggiore su CoNLL++, indicando l’assenza di ritorni diminuiti e quindi l’assenza di over‑fitting adattivo.

2. **Esperimento di “continua pre‑addestramento”** – i modelli sono stati ulteriormente pre‑addestrati o addestrati con dati più recenti (dal 2020). La misura F1 è diminuita progressivamente all’aumentare del divario temporale tra i dati di addestramento e quelli di test. Questa degradazione, direttamente correlata al gap temporale, conferma che la principale causa della perdita di prestazioni è la **deriva temporale**.</sample>
    <sample id="299">**Abstract**

Natural Language Inference (NLI) systems now achieve state‑of‑the‑art results on popular benchmarks, yet recent studies reveal that much of this success stems from exploiting spurious shortcuts—dataset‑induced correlations such as high word‑overlap between premise and hypothesis in MNLI. Models that rely on such shortcuts perform well on in‑distribution data but fail on out‑of‑distribution (OOD) adversarial sets where the shortcuts break. Existing shortcut‑mitigation methods typically require a pre‑trained auxiliary model that explicitly learns to use shortcuts, re‑weighting training examples accordingly. These approaches impose domain‑specific assumptions, assume the learner will mimic the auxiliary’s shortcut usage, and incur extra computational cost.

We propose a minimax training framework that automatically discourages shortcut reliance without any prior shortcut knowledge. A lightweight feed‑forward auxiliary generates example weights that maximize the learner’s loss, forcing the learner to focus on “hard” instances that contradict dominant shortcut patterns. Training alternates between the learner (minimizing NLI loss) and the auxiliary (maximizing loss via weight generation). At test time only the learner is used. Experiments on MNLI, FEVER, and QQP, and their OOD adversarial sets (HANS Symmetric, PAWS), show that our method consistently improves OOD performance while preserving in‑distribution accuracy, outperforming ERM and leading shortcut‑mitigation baselines. We further analyze transfer to larger models, synthetic shortcuts, out‑of‑domain tests, the impact of learner pre‑training, auxiliary size, and provide a qualitative view of the learned weight distribution. This work demonstrates that adversarial weighting derived from training dynamics can robustly mitigate shortcuts in NLI.</sample>
    <sample id="300">**Abstract (≈200 parole)**  
Il lavoro presentato introduce e formalizza il compito di *interactive dictation*, un’interfaccia in cui l’utente può, mediante voce, non solo dettare un documento ma anche eseguire correzioni e modifiche in modo fluido e naturale. A differenza dei tradizionali sistemi di riconoscimento vocale che si limitano alla trascrizione, l’interactive dictation permette l’intercalare di detti e comandi di modifica senza bisogno di parole chiave fisse, sfruttando frasi di comando aperte e intuitive, analoghe a quelle che un assistente umano comprenderebbe.  

Per rendere possibile questa nuova attività, gli autori hanno:  
1. Definito formalmente il processo in quattro fasi – riconoscimento ASR, segmentazione tra detti e comandi, normalizzazione ed esecuzione delle modifiche, fino allo stato finale del documento.  
2. Creato un’interfaccia di raccolta dati e costruito un dataset annotato con traiettorie reali di detti e comandi, con particolare attenzione alla correzione di errori ASR e al supporto di comandi complessi (es. “cambia la virgola dopo ’Hey’ in punto esclamativo”).  
3. Sviluppato un baseline che addestra modelli separati per ciascuna fase, sperimentando architetture T5 e GPT‑3 con due modalità di output (programma intermedio o stato finale diretto).  

I risultati mostrano che GPT‑3 offre maggiore accuratezza ma a costi di tempo più elevati, mentre T5 bilancia velocità e precisione, soprattutto quando predice programmi intermedi. Nonostante i progressi, l’accuratezza complessiva lascia spazio a notevoli miglioramenti, aprendo la strada a ulteriori ricerche su questa nuova task.</sample>
    <sample id="302">Per ottenere la sequenza corretta di output, la prima fase produce solo un **multiset** di token: tutti i token giusti sono presenti, ma non sono ordinati.  
La **permuta** serve quindi a riordinare questi token nell’ordine sintattico corretto del risultato finale, perché la corrispondenza tra frasi di input e forme logiche è sequenziale e la prima fase non le impone.</sample>
    <sample id="303">Gli autori hanno consigliato maggiore trasparenza perché, senza conoscere i metodi di mitigazione che un modello utilizza, non è possibile capire se le “stereotipi positivi” e le narrazioni essenzializzanti che emergono dal modello derivano da un’adeguamento di valori eccessivo, da tecniche anti‑stereotipo o da altri interventi di bias‑mitigation. La mancanza di chiarezza ostacola la valutazione, la riproducibilità e la responsabilità, impedendo di investigare e correggere i bias in modo efficace.</sample>
    <sample id="304">Gli **input inaccettabili di coppia minima** sono le frasi che costituiscono la “negativa” di una coppia di confronto. In un minimal pair, si presenta a un modello due sentenze: una accettabile (grammatical) e una inaccettabile (ungrammatical o non accettabile dal punto di vista semantico/linguistico). Gli input inaccettabili sono quindi frasi costruite appositamente per violare regole sintattiche, semantiche o di stile, servendo a testare se il modello assegna una probabilità più bassa a queste frasi rispetto a quelle accettabili.</sample>
    <sample id="305">**Abstract**

Weakly supervised learning (WSL) relies on inexpensive, noisy labeling sources—such as heuristic rules, knowledge bases, or low‑quality crowdsourcing—to train neural models. Although recent WSL methods claim high accuracy on clean test sets when trained solely on weakly labeled data, they implicitly assume the availability of a clean validation set for model selection, an assumption rarely acknowledged. In this work, we systematically investigate three key questions: (1) Is a clean validation set truly necessary for effective WSL? (2) If so, how many clean samples are required? (3) How best to exploit these clean samples? Our experiments demonstrate that state‑of‑the‑art WSL algorithms fail to generalize without clean validation data, underscoring its essential role. We find that merely 20 clean examples per class suffice to recover competitive performance. Moreover, when clean data is available, directly fine‑tuning a model on these samples outperforms sophisticated WSL methods after only 10 examples per class. Extending fine‑tuning beyond the initial training—what we call “continuous fine‑tuning”—matches or surpasses the gains reported by complex WSL approaches, while incurring negligible computational overhead. Accordingly, we recommend (i) explicitly reporting the validation strategy, (ii) benchmarking WSL against few‑shot learning baselines that also rely on clean data, and (iii) adopting continuous fine‑tuning as a strong, simple baseline. Our code and data are publicly released to facilitate reproducibility.</sample>
    <sample id="306">**Abstract**

Understanding discourse requires a system to keep track of entities and how their states evolve. In this study we investigate whether large pre‑trained language models (PLMs) possess this capability by designing a controlled “box‑and‑object” task that isolates entity‑state tracking from simple pattern matching or memorization. Each example begins with a description of the initial contents of multiple boxes, followed by a sequence of state‑changing operations (e.g., moving or adding objects). The model must then predict the final contents of each box. To prevent shortcut strategies, we carefully randomize initial configurations, ensure that state changes cannot be inferred from single tokens, and avoid allowing the model to memorize operation sequences.

We evaluated several PLMs—Flan‑T5, GPT‑3, GPT‑3.5, and GPT‑4—using 2‑shot in‑context learning. Accuracy is plotted against the number of operations applied to a target box. Most models simply copy the initial state, yielding high accuracy only when the final state matches the starting configuration. Only text‑davinci‑003 and the GPT‑3.5 series achieve non‑trivial tracking, with performance surpassing a random‑simulation baseline. A systematic analysis reveals that GPT‑3.5 models’ exposure to large amounts of code during pre‑training likely underlies this advantage, whereas models lacking substantial code data fail to track states. Fine‑tuning a small T5‑base model can induce tracking, but randomly initialized T5‑base models cannot, underscoring the importance of pre‑training. While our results demonstrate that certain PLMs can track entity states, further work is required to ascertain the generality of these abilities beyond our synthetic setup.</sample>
    <sample id="307">Gli autori hanno valutato i modelli sui seguenti task e hanno usato le metriche standard associate a ciascuno di essi:

| Task | Metriche usate |
|------|----------------|
| **Named Entity Recognition (NER)** | F1‑score (macro‑e micro‑averaged) |
| **Classificazione di testo** | Accuracy (percentuale di classi corrette) |
| **Part‑of‑Speech (POS) tagging** | Accuracy (percentuale di token etichettati correttamente) |
| **Question‑Answering (QA)** | Exact Match (EM) e F1‑score (per le risposte estratte) |

Queste metriche consentono di confrontare le prestazioni dei modelli DrBERT e ChuBERT con i baseline (CamemBERT, PubMedBERT, BioBERT, ClinicalBERT) su 11 task di downstream in francese.</sample>
    <sample id="308">**Abstract**

Design biases in natural language processing (NLP) systems arise when the perspectives of researchers and data annotators shape model behavior, leading to uneven performance across demographic groups. We introduce **NLPositionality**, a framework that quantifies the alignment of datasets and models with real users by re‑annotating existing corpora using a diverse, demographically rich workforce recruited through *Lab in the Wild*. In two tasks—social acceptability (Social Chemistry) and hate‑speech detection (Dynahate)—we collected over 16,000 annotations from more than 1,000 volunteers spanning 87 countries. By computing Pearson correlations between user annotations and model predictions (GPT‑4, Perspective API, Rewire API, Hate‑RoBERTa, etc.), we reveal systematic positionality: both datasets and models exhibit strongest agreement with English‑speaking, college‑educated users, while non‑binary individuals are consistently under‑represented. These findings highlight the need for explicit documentation of design decisions and a perspectivist lens in NLP research. We propose three actionable recommendations: (1) maintain detailed records of every design choice, (2) adopt perspectivist methodologies to foreground diverse viewpoints, and (3) develop community‑specific datasets and models—as exemplified by the Masakhani initiative—to ensure inclusivity. Our dashboard and accompanying paper provide deeper insights and encourage the community to address positionality in NLP systematically.</sample>
    <sample id="309">La metrica utilizzata è **l’inter‑annotator agreement** (valutato con gli standard di accordo tra annotatori, es. κ di Cohen).</sample>
    <sample id="310">Il dominio scelto è **Wikipedia**.</sample>
    <sample id="311">Affiliations non menzionate nel testo fornito.</sample>
    <sample id="312">**Differenze principali di MultiInstruct rispetto ad altri benchmark:**

| Aspetto | MultiInstruct | Benchmark tipici (es. instruction‑tuning NLP) |
|--------|---------------|---------------------------------------------|
| **Modalità** | Multi‑modal (testo + immagini + bounding boxes) | Predominantemente testo (solo NLP) |
| **Copertura** | 62 task diversi, 10 categorie, 21 dataset sorgenti | Oltre 1600 task, tutti solo linguaggio |
| **Istruzioni** | 5 istruzioni esperte per ogni task | Di solito 1–2 template per task |
| **Formato** | Seq2Seq unificato su token vocabolario condiviso (testo, immagine, coordinate) | Generalmente separato per input/output |
| **Scopo** | Valutare generalizzazione zero‑shot su task multi‑modal e testare transfer learning da NLP | Valutare solo compiti NLP in zero‑shot |
| **Valutazione** | Accuracy per classificazione, ROUGE‑L per generazione + nuova metrica “sensitivity” | Metriche standard (accuracy, F1, etc.) |

In sintesi, MultiInstruct è il primo benchmark pubblicamente disponibile che combina molteplici modalità, task e istruzioni di alta qualità, permettendo di esaminare l’influenzamento dell’instruction tuning in un contesto multi‑modal vero e proprio, cosa che i benchmark precedenti non offrono.</sample>
    <sample id="313">Tre autori sono coinvolti nell’articolo: James Finch, Sarah Finch e Professor Jinho Choi.</sample>
    <sample id="314">**Coordinazione binaria**: una struttura sintattica in cui due elementi (congiunti) sono uniti da una congiunzione coordinante (ad es. *e*, *ma*, *o*). In pratica, è una coordinazione che contiene esattamente due componenti.</sample>
    <sample id="315">Il documento non fornisce alcuna informazione sul tempo medio impiegato per i prompt; l’uso dei prompt è stato semplicemente eseguito per generare i personaggi.</sample>
    <sample id="316">I risultati mostrano che un modello T5 più piccolo, addestrato sul dataset CoScript, può produrre script di qualità superiore rispetto alla maggior parte dei LLM più grandi. Ciò implica che, con il giusto set di dati (cosa che il distillato di CoScript fornisce), i modelli più piccoli non solo sono più efficienti dal punto di vista computazionale e economico, ma possono anche superare i modelli di grandi dimensioni nella pianificazione linguistica vincolata. In pratica, la qualità del dataset è più determinante della dimensione del modello, aprendo la strada a soluzioni più leggere e scalabili per compiti di pianificazione con vincoli specifici.</sample>
    <sample id="317">**Abstract**  
In this work we introduce **CodeIE**, a novel approach that reformulates few‑shot information extraction (IE) as a code‑generation problem. Conventional IE systems, such as T5 or GPT‑3, are trained in a text‑to‑text paradigm and then forced at inference time to produce linearised structured outputs, which causes a mismatch between training and inference formats and leads to structural errors. CodeIE instead casts IE as a structured‑to‑structured generation task that can be directly handled by code‑focused large language models (LLMs) such as Codex. For Named Entity Recognition (NER) we design a prompt that defines a Python function, injects the input sentence, and expects the model to append entity spans to an `EntityList`. A similar prompt is used for Relation Extraction (RE).  

We evaluate CodeIE on three NER datasets (CoNLL‑2003, OntoNotes, and WNUT) and four RE datasets (TAC‑RE, SemEval‑2010, ACE‑05, and SciERC). Baselines include T5, UIE, GPT‑3 (text‑davinci‑002), and Codex (code‑davinci‑002). Across both one‑shot and few‑shot settings, CodeIE with code‑style prompts consistently outperforms all baselines, achieving higher F1 scores on NER and RE. Analysis reveals that code‑format inputs lower perplexity for code‑trained models, reduce structural errors, and improve recall. Moreover, Codex surpasses GPT‑3 even when both use the same prompt style. These results demonstrate that aligning the training and inference formats via code generation enables large code models to act as powerful few‑shot IE engines.</sample>
    <sample id="318">**Presentazione di Yanis Labrak – “DrBERT: Un modello pre‑addestrato robusto in francese per i domini biomedicali e clinici”**

In questa presentazione descriviamo prima l’uso della modellazione del linguaggio nell’ambito sanitario. Successivamente presentiamo il contributo principale del nostro articolo: l’introduzione del primo modello biomedicale in francese, DrBERT, basato su RoBERTa e addestrato sul dataset NACHOS, costituito da dati web “crawled” (estratti dal web). Confrontiamo anche le prestazioni di diversi modelli pre‑addestrati, variando impostazioni di pre‑addestramento e fonti di dati. Infine illustreremo i risultati ottenuti su 11 compiti downstream biomedicali e clinici in francese, concludendo con una panoramica su come accedere ai modelli.

---

## 1. Modelli di linguaggio in ambito sanitario

Da quando BERT è stato pubblicato nel 2018, è diventato uno degli approcci più efficaci per risolvere le attività di NLP, offrendo enormi miglioramenti rispetto ai metodi statici e contestuali più vecchi (Word2vec, fastText, ecc.). Da allora BERT è stato adattato a molte altre lingue (es. CamemBERT in francese) e a domini specifici come il biomedicale (PubMedBERT, BioBERT) e il clinico (ClinicalBERT), ma per la maggior parte in inglese. Modelli specializzati per altre lingue sono rari e spesso si basano su *continual pre‑training* a causa della scarsità di dati in‑domain. Per il francese, fino ad ora non esisteva un modello open‑source dedicato al biomedicale.

Ci siamo posti quindi la domanda: **quali sono le fonti di dati più appropriate per un ampio spettro di utilizzi e in cui i dati “crawled” possono sostituire i dati clinici?** Per rispondere, confrontiamo DrBERT con ChuBERT, un modello clinico basato su dati anonimizzati provenienti dall’archivio clinico dell’Università di Nantes.

---

## 2. Quanto spazio di dati serve per addestrare un modello specializzato in francese?

Per valutare questo aspetto, abbiamo addestrato e confrontato quattro modelli “from‑scratch”:

| Modello | Fonte dati | Dimensione |
|---------|------------|------------|
| DrBERT‑1 | NACHOS | 7 GB |
| DrBERT‑2 | NACHOS | 4 GB |
| ChuBERT‑1 | Note cliniche | 4 GB |
| ChuBERT‑2 | NACHOS (4 GB) + Note cliniche (4 GB) | 8 GB |

Oltre a questi, abbiamo introdotto tre modelli addestrati tramite *continual pre‑training* per analizzare l’impatto della strategia di pre‑training:

1. **CamemBERT‑init**: pesi CamemBERT addestrati su 4 GB di NACHOS.
2. **CamemBERT‑clinical**: pesi CamemBERT addestrati su 4 GB di note cliniche.
3. **PubMedBERT‑init**: pesi PubMedBERT addestrati su 4 GB di NACHOS.

In totale, abbiamo quindi sette modelli.

---

## 3. Valutazione

Per valutare i sette modelli, abbiamo raccolto compiti pubblici e privati downstream, tra cui:

- Named Entity Recognition (NER)
- Classificazione
- Part‑of‑Speech (POS) tagging
- Question‑Answering (QA)

I modelli sono stati confrontati con sei baseline:

- CamemBERT OSCAR 138 GB
- CamemBERT OSCAR 4 GB
- CamemBERT CCNET 4 GB
- PubMedBERT
- BioBERT
- ClinicalBERT

### Risultati principali

- I modelli performano meglio sui compiti che coinvolgono dati di natura simile a quella di addestramento.
- I dataset eterogenei mostrano una maggiore versatilità.
- Maggiore quantità di dati porta generalmente a migliori prestazioni.
- L’addestramento *from‑scratch* tende a dare risultati più elevati su la maggior parte dei compiti.
- Il *continual pre‑training* con i pesi e la tokenizzazione di CamemBERT su 4 GB di NACHOS produce risultati comparabili a DrBERT‑2 (4 GB *from‑scratch*).  
  Tuttavia, il modello basato solo sui pesi di CamemBERT mostra problemi di stabilità.

---

## 4. Conclusioni

- Il nostro modello “DrBERT” supera globalmente la baseline generica (CamemBERT) in nove delle undici attività downstream.
- L’uso di dati più specializzati migliora le prestazioni, ma non scala linearmente con la quantità di dati.
- I modelli pre‑addestrati basati su NACHOS sono disponibili gratuitamente su Hugging Face sotto licenza MIT, e tutti gli script di training sono presenti nel nostro repository GitHub.

---

**Grazie per l’attenzione.**  
Siamo disponibili a discutere i risultati alla sessione poster a Toronto.</sample>
    <sample id="319">Nel lavoro vengono testate due principali strategie di pre‑training:

| Strategia | Descrizione | Dati utilizzati |
|-----------|-------------|-----------------|
| **From‑scratch pre‑training** | Si addestra un modello BERT puro (RoBERTa) con i propri tokenizzatori e pesi, partendo da zero. | 1) 7 GB di NACHOS (medico web). 2) 4 GB di NACHOS. 3) 4 GB di note cliniche (ChuBERT). 4) 4 GB NACHOS + 4 GB note cliniche. |
| **Continual (fine‑tuning) pre‑training** | Si parte da un modello già pre‑addestrato (CamemBERT o PubMedBERT) e si pre‑addestra ulteriormente su dati specifici. | 1) CamemBERT + 4 GB NACHOS. 2) CamemBERT + 4 GB note cliniche. 3) PubMedBERT + 4 GB NACHOS. |

Queste sette varianti (quattro from‑scratch + tre continual) vengono confrontate su 11 compiti clinici/biomedici in francese, oltre a sei baseline generici (CamemBERT OSCAR, CCNET, PubMedBERT, BioBERT, ClinicalBERT).</sample>
    <sample id="320">Non è presente. Nella visualizzazione riportata, la retta di regressione rossa ha pendenza &gt; 1, il che indica che ogni miglioramento su CoNLL‑2003 porta a un miglioramento **maggiore** su CoNLL++. In pratica non c’è alcun fattore di overfitting dovuto al riutilizzo del test.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata confrontando i testi semplificati generati dai modelli con il set di riferimento “gold standard” contenente le versioni semplificate manualmente.  Per misurare la corrispondenza tra generazione e target sono stati utilizzati i tipici metriche automatiche riportate nel paper (ad es. BLEU, SARI, BERTScore, ecc.), che hanno permesso di quantificare la resa in termini di accuratezza linguistica e semplificazione.  I risultati così ottenuti sono stati quindi confrontati con quelli di baseline, evidenziando che i modelli fine‑tuned superano le prestazioni di riferimento.</sample>
    <sample id="322">**Abstract (≈200 words)**  

In questa presentazione a ACL 23 esploriamo cosa imparano i classificatori di testo riguardo alla moralità, un concetto intrinsecamente soggettivo e contestualizzato. Partendo dalla **Moral Foundation Theory (MFT)**, che identifica cinque fondamenti morali (cura, lealtà, autorità, giustizia e purezza), sottolineiamo che la priorità attribuita a ciascun fondamento varia da persona a persona. Per evitare il pericolo di una “media” antropologicamente riduttiva, proponiamo un approccio che riconosca la pluralità delle interpretazioni morali.

Utilizzando il **Moral Foundation Twitter Corpus** (≈35 k tweet in sette domini, es. #AllLivesMatter, #BlackLivesMatter), addestriamo modelli linguistici addestrati a assegnare etichette MFT ai testi. Successivamente, applichiamo tecniche di XAI per interpretare le decisioni dei modelli e analizzare come la moralità sia espressa diversamente nei vari contesti. Un esempio emblematico è la distinzione tra ALM e BLM: mentre l’ALM associa “sottoposto” a termini come “coup” e “mayhem” (sottolineando l’opposizione alla subversione), nel BLM la stessa parola è più accolta, suggerendo una prospettiva di resistenza legittima.

I risultati mostrano che i modelli non solo riconoscono la differenza di registro, ma anche la variazione nei fondamenti morali sottostanti. Queste evidenze avvertono sul rischio di applicare un unico modello per più domini, potenzialmente generando interpretazioni errate o dannose della moralità. Il lavoro apre la strada a sistemi più sensibili al contesto, in grado di riflettere la complessità e la pluralità delle norme morali umane.</sample>
    <sample id="323">**Abstract**  
Commonsense question answering (QA) demands that a system retrieve and reason over external knowledge. Existing approaches typically retrieve a subgraph from a knowledge base (KB) via entity matching, then encode the subgraph and the QA context independently, which introduces noisy entities (e.g., “Top,” “Bank,” “Cat”) and limits cross‑modal interaction. Moreover, they ignore the semantic relations between entities. To address these issues, we propose **DHLK** (Dynamic Heterogeneous‑Graph Reasoning with Language Models and Knowledge Representation Learning). DHLK first constructs a heterogeneous knowledge graph (HKG) from multiple KBs (ConceptNet, WordNet, Wiktionary) by a two‑stage pruning strategy and knowledge‑representation learning (KRL). We remove subword fragments of phrase entities using a dictionary vocabulary and augment the graph with paraphrases retrieved from WordNet and Wiktionary, thereby enriching the entity set. Using RoBERTa with Mask Self‑Attention, we jointly encode the QA context and the HKG, and dynamically prune weakly relevant entities (e.g., “wood”) based on attention scores. Entity and relation embeddings are initialized by mean‑pooling and further refined with TransE. Instead of a conventional GNN, we introduce **Relation‑Mask Self‑Attention (RMSA)**—an RGAT‑inspired module that incorporates relation types into attention—which iteratively updates node embeddings for \(L\) layers. The final graph embedding is obtained by max‑pooling over question‑key entities. We then fuse HKG path information with the context embedding and feed the combined representation into an MLP for answer prediction. Experiments on CommonsenseQA and OpenBookQA, leveraging two‑hop ConceptNet paths and KeyBERT‑extracted key entities, demonstrate that DHLK outperforms state‑of‑the‑art language‑model and HKG baselines on both datasets.</sample>
    <sample id="324">Sì. Gli esperimenti mostrano che i modelli linguistici non sono neutrali: differiscono notevolmente per inclinazione politica.  
- GPT‑4 (e la serie GPT in generale) tende a essere più liberale, mentre i modelli della serie BART sono più moderati o tendono verso il centro.  
- Una ulteriori fase di pre‑training su corpus partisans (es. Reddit “left‑leaning”) sposta il modello verso quella ideologia; analogamente con corpora “right‑leaning”.  
- I modelli addestrati su testi post‑2017 mostrano una maggiore polarizzazione rispetto a quelli addestrati su testi pre‑2017.  

Quindi i bias politici variano in modo sistematico tra i modelli e si riflettono nelle performance di compiti downstream come la rilevazione di hate speech e fake news.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann e oggi vi presenterò brevemente il nostro articolo intitolato **“Compositional Generalization without Trees using Multiset Tagging and Latent Permutations”**. È un lavoro congiunto con i miei advisor Alexander Koller e Ivan Titov.

---

### Cos’è la generalizzazione composizionale?

La generalizzazione composizionale è la capacità di un apprendimento a gestire ricorsioni più profonde e composizioni di frasi non viste in precedenza, pur avendo già visto le singole parti durante l’addestramento. Nel contesto della semantic parsing, un test di generalizzazione composizionale può essere strutturato così:  

- **Training set**: frasi di esempio come *“The girl slept.”* e *“Mary knew that the girl slept.”* ognuna accoppiata a una forma logica che rappresenta gli aspetti principali del suo significato.  
- **Test set**: non proviene dalla stessa distribuzione; contiene forme logiche strutturalmente nuove. Nel nostro esempio, il modello ha visto una ricorsione superficiale durante l’addestramento ma viene testato su una frase con una ricorsione più profonda.

I modelli seq‑to‑seq “nuovi” (naïve) faticano con questo tipo di generalizzazione fuori‑distribuzione e spesso producono output che sono scollegati dall’input. In particolare, non riescono a riprodurre le corrispondenze sistematiche tra input e output, come quelle evidenziate di colore nell’esempio.

---

### L’approccio tradizionale con gli alberi

Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi mirano a catturare il processo composizionale che lega le frasi con le forme logiche. Funziona bene, ma gli alberi non sono di solito forniti e devono essere ottenuti in qualche modo. Ciò può essere complicato e, a volte, richiede un processo computazionale costoso: di solito richiede un pre‑processing specifico per la formalità, per esempio per gestire i simboli di variabile. Ottenere gli alberi può anche richiedere procedure di induzione grammaticale specializzate.

---

### Il nostro approccio “senza alberi”

In questo articolo non usiamo alberi. Presentiamo un modello seq‑to‑seq neurale che modella direttamente le corrispondenze fra frammenti dell’input e frammenti dell’output. Per la prima volta dimostriamo una forte generalizzazione verso ricorsioni più profonde senza fare affidamento sugli alberi.

Il nostro approccio predice l’output dall’input in due fasi:

1. **Tagging di multinsieme**  
   Ogni token dell’input viene etichettato con un *unordered multiset* (multinsieme non ordinato) di token che appariranno nell’output. Dopo il primo step abbiamo tutti i token giusti, ma non sono ordinati.

2. **Predizione della permutazione**  
   Un secondo modello predice una permutazione per collocare i token nella giusta sequenza. Introduciamo un nuovo metodo per prevedere la permutazione che non impone vincoli rigidi sulle possibili permutazioni, rendendo il nostro approccio piuttosto flessibile ed espressivo.

### Come funziona il modello di permutazione?

Concettualmente, il modello di permutazione procede così:

- Andiamo da sinistra a destra sull’output e determiniamo quale token del multinsieme posizionare in ogni posizione.  
- Per la prima posizione di output scegliamo semplicemente un token (in rosso).  
- Per la seconda posizione, "saltiamo" al prossimo token del multinsieme per determinare il secondo token in output.  
- Ripetiamo il processo fino a quando ogni token del primo step è stato visitato esattamente una volta.

---

### Risultati sperimentali

Ecco un teaser dei risultati sperimentali: confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera di gran lunga gli altri per la generalizzazione a ricorsioni più profonde. Alcune altre forme di generalizzazione strutturale rimangono tuttavia molto difficili.

---

### Sfide tecniche affrontate

1. **Allineamento non fornito**  
   L’allineamento tra input e output non è dato nei dati di training. Per un dato token non sappiamo da quale multinsieme proviene, cosa rende difficile l’addestramento.  
   *Soluzione*: indichiamo l’allineamento come parte del processo di training.

2. **Permutazioni latenti**  
   A volte esistono molte permutazioni compatibili con i dati, ma la permutazione linguisticamente corretta è latente.  
   *Soluzione*: indichiamo e impariamo la permutazione più plausibile.

3. **NP‑difficoltà nella ricerca della permutazione ottima**  
   Trovare la permutazione con punteggio più alto è NP‑difficile (relativo al problema del commesso viaggiatore).  
   *Soluzione*: approssimiamo con un rilassamento continuo compatibile con GPU, che permette anche la retropropagazione attraverso la soluzione e l’apprendimento di permutazioni più linguisticamente plausibili.

---

Se desiderate approfondire i nostri esperimenti e capire come affrontiamo queste sfide, date un’occhiata al nostro paper o venite a parlare al nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è lo stato di tensione che nasce quando una persona possiede due credenze, opinioni o comportamenti in conflitto, risultando in una percezione di incoerenza tra ciò che pensa e ciò che fa.</sample>
    <sample id="327">**Abstract (≈200 words)**  

Vision‑language (VL) models aim to jointly understand images and text, with Visual Question Answering (VQA) serving as a flagship downstream task. Recent progress has largely relied on two‑tower architectures comprising a visual encoder, a textual encoder, and a cross‑modal fusion module. However, conventional designs such as METER feed only the final unimodal representations into the fusion stage, ignoring rich semantic cues present across intermediate layers. BridgeTower mitigates this by linking multiple top unimodal layers to each cross‑modal layer, yet it still suffers from two drawbacks: (1) each cross‑modal layer is constrained to a single, pre‑assigned unimodal depth, limiting exploitation of multi‑level knowledge; (2) the number of cross‑modal layers must match the number of unimodal layers, hindering scalability.  

We propose **ManagerTower**, a novel VL architecture that introduces *managers* at every cross‑modal layer. Each manager aggregates insights from multiple pre‑trained unimodal experts (e.g., RoBERTa for text and CLIP‑ViT for vision) across different depths, enabling adaptive weighting of semantic information. This design decouples cross‑modal and unimodal layers, allowing flexible scaling and richer fusion. Experiments on VQA and other benchmarks (e.g., Wikivideo) using only 4 M image‑text pairs demonstrate that ManagerTower surpasses BridgeTower and several larger baselines, achieving a 39.15 % accuracy improvement on Wikivideo. Ablation studies with static versus adaptive managers reveal distinct, layer‑specific weighting patterns, confirming that adaptive aggregation effectively harnesses multi‑level unimodal knowledge. The code, pretrained checkpoints, and detailed results are publicly released on GitHub and the ACL archive.</sample>
    <sample id="328">Il modello più liberale è **GPT‑4** (la serie GPT in generale risulta più socialmente liberal rispetto a BART e alle sue varianti).</sample>
    <sample id="329">**Abstract**

Zero‑shot video sentence localization seeks to locate the temporal segment of a long video that best matches a natural‑language query without relying on costly manual annotations. Existing zero‑shot approaches generate pseudo‑events from video content and then synthesize simple pseudo‑queries (e.g., concatenated nouns and verbs). These methods suffer from three key deficiencies: (1) the pseudo‑queries are overly simplistic and diverge from real user queries; (2) the generated pseudo‑events guarantee high relevance only within the event, but fail to enforce low relevance outside the event, leading to misalignment; and (3) they treat all pseudo‑labels as ground truth, ignoring inevitable label noise.

We propose a **Noise‑Resistant Structured Pseudo‑Label (SPL) Generation** framework. First, dense video frames are fed to the BLIP image‑captioning model to produce rich, free‑form pseudo‑queries that better reflect realistic language. Second, we compute frame‑to‑query similarity scores and, via a sliding‑window search, select pseudo‑events that maximize the difference between intra‑event and inter‑event similarity—ensuring high relevance inside and low relevance outside the event. Only the top‑K high‑quality query‑event pairs are retained, and highly overlapping pairs are discarded. Third, during training we weight each sample by a function of the model’s confidence and the IoU with its pseudo‑label, down‑weighting suspected noisy samples. Highly confident predictions that overlap well with their pseudo‑labels are promoted as new pseudo‑labels for subsequent training iterations, enabling progressive refinement.

We evaluate SPL on **ActivityNet Captions** and **Charades‑STA**. Using standard metrics (R@M and mIoU), SPL consistently outperforms prior zero‑shot methods across most settings, demonstrating the effectiveness of structured pseudo‑label generation and noise‑aware training. The released code and data enable reproducibility and further research.</sample>
    <sample id="330">Sì. Nel nostro studio, l’update cumulativo ha performato uguale o meglio dell’iterativo in tutti i casi, specialmente nelle annotazioni di dominio.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa sono stati estratti dai trascritti dei TED Talk, tradotti dall’inglese in 14 lingue diverse.</sample>
    <sample id="333">**Abstract (≈200 words)**  
In this work we propose **INK** (“Injecting kNN Knowledge in Nearest‑Neighbor Machine Translation”), a training framework that smooths the representation space of neural machine translation (NMT) models by iteratively leveraging k‑nearest‑neighbor (kNN) knowledge. Standard NMT models learn a highly non‑smooth embedding space, in which low‑frequency tokens are sparsely dispersed, creating “holes” that degrade generalization. kNN‑MT addresses this by retrieving nearest neighbors from a large datastore during decoding, but this incurs high inference latency and prevents easy updates to the stored representations.  

INK replaces the expensive runtime retrieval with a lightweight adapter: during training, the adapter is guided by three KL‑divergence objectives—aligning contextual representations with token embeddings, aligning them with kNN‑derived token embeddings, and aligning representations of identical target tokens. After each training step, the datastore is refreshed with the updated representations, forming an iterative “trend loop” that converges to a smoother representation manifold. Once convergence is reached, the datastore can be discarded, yielding a purely inference‑time NMT model with an adapter that captures the benefits of kNN‑MT at a fraction of the cost.  

Experiments on the WMT’19 German‑English news task (using the WMT‑winning baseline) demonstrate that INK achieves an average gain of **1.99 COMET** and **1.0 BLEU** over state‑of‑the‑art kNN‑MT systems, while using less memory and offering faster inference. Ablation studies confirm that the adapter alone improves performance, but the combination of adapter and periodic datastore refresh yields the best results, indicating that further refinement of the representation space can still be achieved.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico (cross‑lingual transfer) è la capacità di un modello, addestrato su dati di una lingua (o su più lingue), di svolgere la stessa attività in un’altra lingua senza averla vista durante l’addestramento. In pratica si trasferisce la conoscenza linguistica acquisita in un “source language” a un “target language”, ottenendo prestazioni zero‑shot (senza esempi) o few‑shot (con pochi esempi).</sample>
    <sample id="337">**Abstract**

Out‑of‑vocabulary (OOV) words pose a persistent challenge for embedding‑based downstream models. We propose a graph‑based framework that learns OOV embeddings by exploiting word formation and lexical associations. When an OOV token appears, we decompose it into sub‑word pieces and construct a two‑level word‑relationship graph: the first layer contains all sub‑pieces (preserving full compositional detail), while the second layer samples a fixed number of related words to mitigate noisy neighbors. Node attributes for the OOV pieces are generated via a self‑attention network over the character sequence, ensuring that sub‑unit semantics are retained. We process the graph with two successive Graph Attention Network (GAT) layers, concatenating each layer’s output with its input to obtain robust node representations. A read‑out module aggregates these into a graph‑level embedding that captures the overall word formation. To align the learned embeddings with the background vector space, we employ a contrastive NT‑XENT loss, using positive pairs drawn from two‑hop neighbors, synonyms, and the OOV word itself. Extensive intrinsic and extrinsic evaluations demonstrate that our approach outperforms baselines on both static and contextual models. The method is language‑agnostic; it is particularly well suited to agglutinative languages where morphemes concatenate linearly, while it remains effective for fusional languages given appropriate segmentation. Overall, our graph‑based relation mining framework offers a principled, scalable solution for OOV embedding learning across diverse languages.</sample>
    <sample id="338">**Abstract (≈200 parole)**  

In questo lavoro si affronta la problematica di come valutare la qualità delle spiegazioni in linguaggio naturale annotate da esseri umani, in particolare quando queste vengono impiegate per addestrare modelli di intelligenza artificiale. I metodi tradizionali (BLEU, ROUGE) si basano su somiglianze lessicali con un “gold standard”, mentre la metrica di simulatabilità misura solo la variazione delle performance di un modello quando la spiegazione è presente o assente, trascurando il ruolo delle spiegazioni sia durante il fine‑tuning che durante l’inferenza.  

Per superare queste limitazioni, i ricercatori propongono:  
1. una struttura dati unificata che trasforma vari compiti (commonsense QA, NLI, validazione di affermazioni) in una forma di multiple‑choice;  
2. un esperimento approfondito su cinque dataset (CoS‑E, ECQA, e‑SNLI, ComVE, Plus) che dimostra che il fine‑tuning con spiegazioni non aggiunge nuova conoscenza ma induce il modello a dipendere dall’input esplicativo;  
3. la metrica TREU, estensione della simulatabilità che valuta anche l’utilità delle spiegazioni nel fine‑tuning, confrontando due modelli addestrati con e senza spiegazioni.  

L’analisi con T5 e BART mostra che TREU ordina coerentemente la “qualità” delle spiegazioni sui dataset, mentre la simulatabilità fallisce soprattutto per ComVE e e‑SNLI. Inoltre, le spiegazioni risultano più utili in classi di entailment rispetto a neutral o contradiction, evidenziando la dipendenza dal compito e dal formato.  

Il contributo principale è quindi un framework completo di valutazione, che promuove annotazioni di alta qualità e può guidare future ricerche sulla collaborazione uomo‑macchina.</sample>
    <sample id="339">Tutti gli autori sono affiliati alla **Saarland University (Saarbrücken, Germania)**.</sample>
    <sample id="340">**Abstract (≈200 parole)**  

Paraphrase generation è una componente cruciale per molte applicazioni NLP, ma la disponibilità di dataset di grandi dimensioni e di alta qualità rimane limitata. I corpus annotati a mano (MRPC, PAN, Quora) sono pochi e i dataset generati automaticamente tramite back‑translation mostrano poca diversità sintattica. In questa presentazione proponiamo **ParaAMR**, un corpus di 15 mila frasi di origine con quasi 7 paraphrase per esempio, ottenuto tramite *AMR back‑translation*.  

Il metodo parte dall’analisi AMR di una frase originale: un grafo diretto che cattura il significato astratto. Cambiamo l’“focus” (root) del grafo selezionando casualmente un nodo, riorganizzando le etichette delle relazioni e, infine, rigeneriamo il testo con un modello grafo‑a‑testo. Poiché il grafo rimane invariato, il significato è preservato, mentre l’enfasi sul nuovo focus produce variazioni sintattiche significative.  

La qualità semantica è confermata da metriche automatiche e da valutazioni umane, che mostrano una similarità di significato paragonabile a dataset tradizionali ma con una diversità sintattica superiore.  

I benefici di ParaAMR sono dimostrati in tre applicazioni: (1) apprendimento di embeddings di frase, dove si ottengono punteggi STS più alti; (2) generazione controllata di paraphrase con migliore controllo sintattico; (3) data augmentation per few‑shot learning, con risultati migliori rispetto ai dataset esistenti.  

ParaAMR è disponibile pubblicamente e offre un nuovo strumento per la ricerca e lo sviluppo di modelli di generazione di testo più robusti e flessibili.</sample>
    <sample id="341">Gli autori valutano la latenza con **Average Lagging** (la media del ritardo tra l’input audio e l’output tradotto) e con la sua variante **Computational‑Aware Average Lagging**, che aggiunge al ritardo la durata computazionale effettiva delle predizioni.</sample>
    <sample id="342">**Abstract**

We introduce **LiveChat**, a large‑scale Chinese dialogue corpus automatically constructed from live‑streaming videos on platforms such as Douyin (TikTok). While most open‑domain dialogue datasets are text‑based and lack the natural prosody and multimodal cues of spoken conversation, LiveChat bridges this gap by extracting speech from 10,000+ hours of live streams, transcribing it with ASR, and pairing utterances with audience comments via a reply‑to‑whom matching algorithm. Each dialogue is enriched with persona information: manually annotated basic profiles (e.g., age, gender, interests) and automatically extracted extended profiles using rule‑based heuristics and a trained persona classifier. The resulting corpus contains over 5 million utterances, 600,000 multi‑party sessions, and 3,000 unique personas, making it the largest Chinese multi‑party, video‑sourced, personalized dialogue dataset to date.

We evaluate LiveChat on two benchmark tasks. First, **Response Modeling**: retrieval baselines show that incorporating persona embeddings and longer session histories significantly improves response relevance. Second, **Addressee Recognition**: a single‑stream BERT model outperforms a dual‑stream counterpart, with persona features further enhancing accuracy. We also benchmark pre‑trained generation models (BART, GPT‑3, LLaMA) and find that BART yields the best fluency and contextual appropriateness, underscoring the domain shift from existing text corpora. In‑context learning experiments reveal that performance scales with the number of demonstrations up to 8 shots, beyond which noise from random samples degrades results.

LiveChat thus offers a rich resource for advancing personalized, multi‑party dialogue systems, and future work will focus on efficient transfer learning of large language models to this domain.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha e oggi il mio co‑autore Martin e io presenteremo il nostro lavoro “The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources”. Questo studio è frutto di una collaborazione tra l’Università McGill, Mila e Microsoft Research.

I modelli di comprensione del linguaggio naturale attingono a una varietà di fonti di conoscenza, come quella incorporata nei loro parametri (acquisita durante il pre‑addestramento) e le informazioni fornite negli input al momento dell’inferenza. I lavori più recenti sul question answering mostrano che i modelli possono sfruttare la conoscenza pre‑addestrata per risolvere il compito. Tuttavia, la comprensione del linguaggio spesso richiede anche conoscenze fornite al momento dell’inferenza. Per esempio, nella frase “John ha visto il nuovo presidente in TV”, i parametri pre‑addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è una TV, ma non possono sapere in modo affidabile chi sia l’entità specifica “John” o chi sia il nuovo presidente, perché il presidente potrebbe essersi cambiato da quando il modello è stato addestrato. Perciò, i modelli di NLU che devono affrontare compiti intensivi di conoscenza devono essere in grado di integrare e usare sia la conoscenza pre‑addestrata sia quella fornita al momento dell’inferenza.

In questo lavoro proponiamo una suite di test diagnostici per la verifica dell’integrazione della conoscenza. Introduciamo un compito di risoluzione delle coreferenze, progettato per sondare la capacità di attingere a informazioni disponibili in fonti diverse. Valutiamo il dataset con partecipanti a uno studio umano e con modelli di risoluzione delle coreferenze già consolidati. Ecco un esempio del nostro dataset.

&gt; **Servin** è un giudice. **Kea** è un cuoco. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata al lavoro, decidendo casi in un tribunale, lui era felice di rilassarsi.  
&gt; Il compito è identificare l’entità corretta a cui si riferisce il pronome “lui”, che in questo caso è Servin.  
&gt; La risoluzione di un pronome dato richiede due tipi di informazioni:  
&gt; 1. Conoscenza specifica dell’entità, come “Servin è un giudice”.  
&gt; 2. Conoscenza di base, come “i giudici decidono casi in tribunali”.  
&gt; Generalmente, la conoscenza di base è appresa durante il pre‑addestramento dei grandi modelli linguistici, mentre la conoscenza specifica dell’entità viene osservata al momento dell’inferenza.

Variamo la disponibilità di questi due tipi di informazioni in modo che possano trovarsi in una sola fonte, oppure in più fonti. Abbiamo definito tre impostazioni per KITMUS:

1. **Background‑Pretrain** – la conoscenza di base è disponibile solo al momento del pre‑addestramento.  
2. **Background‑Both** – la conoscenza di base è disponibile sia al momento del pre‑addestramento sia al momento dell’inferenza.  
3. **Background‑Inference** – le conoscenze di base e specifiche dell’entità sono disponibili solo al momento dell’inferenza.  
   Questa terza impostazione è particolarmente interessante perché simula il caso in cui la conoscenza di base necessaria per risolvere un compito non è presente nei dati di pre‑addestramento del modello (ad es., occupazioni nuove emerse dopo il pre‑addestramento).

Ecco un esempio di come controlliamo la disponibilità delle informazioni nei veri fonti.  
- In **Background‑Pretrain** assumiamo che la conoscenza di base “i politici cercano posti elettorali nel governo” sia contenuta nei parametri pre‑addestrati e che il contesto di inferenza fornisca la conoscenza specifica “Chichester è un politico”.  
- In **Background‑Both** forniamo sia la conoscenza specifica sia la conoscenza di base sui politici nel contesto di inferenza.  
- In **Background‑Inference** sostituiamo l’occupazione reale “politico” con un’occupazione fittizia “mirituer”, perché “mirituer” è improbabile di essere presente nei parametri pre‑addestrati.

Valutiamo il dataset sia con partecipanti umani sia con modelli di risoluzione delle coreferenze consolidati. In questa figura mostriamo i risultati dei modelli con le migliori prestazioni nella variante più difficile di **Background‑Pretrain**. Senza un addestramento specifico su KITMUS, entrambi i modelli non performano bene. Quando vengono addestrati su KITMUS, sia C2F che BERT4Coref migliorano significativamente rispetto alla scelta casuale. Questo suggerisce che, quando vengono addestrati su dataset generici di risoluzione delle coreferenze, la maggior parte dei modelli tende a sfruttare indizi superficiali, indizi che non sono utili quando si testa su KITMUS, dove tali segnali sono stati eliminati.

Altri esperimenti con conoscenze fittizie hanno evidenziato che anche i modelli con le migliori prestazioni non riescono a integrare in modo affidabile la conoscenza “indietro” fornita solo al momento dell’inferenza.

Per riassumere i principali risultati del nostro lavoro:

- Molti modelli di risoluzione delle coreferenze sembrano incapaci di ragionare su conoscenze provenienti da fonti diverse senza addestramento specifico per il compito.  
- Con addestramento specifico, alcuni modelli riescono a integrare con successo conoscenze da più fonti.  
- Tuttavia, anche i modelli con le migliori prestazioni mostrano difficoltà nell’integrare in modo affidabile la conoscenza fornita esclusivamente al momento dell’inferenza.

Se siete interessati a maggiori dettagli, consultate il nostro articolo e scaricate il dataset e il codice su GitHub. Grazie per l’attenzione.</sample>
    <sample id="344">I metodi basati su alberi presentano diversi svantaggi:

1. **Necessità di ottenere gli alberi** – in genere non sono forniti e devono essere ricavati.
2. **Pre‑elaborazione complessa** – richiede passaggi specifici al formalismo (es. gestione di simboli variabili).
3. **Procedura di induzione del grammar** – spesso è necessario ricorrere a tecniche di grammar‑induction specializzate.
4. **Costo computazionale** – l’estrazione e l’uso degli alberi aumentano la complessità e il tempo di elaborazione.</sample>
    <sample id="345">**Abstract (≈200 parole)**  

La generalizzazione composizionale è la capacità di un modello di gestire ricorsioni più profonde e composizioni di frasi mai viste durante l’addestramento. In ambito di parsing semantico, questo si manifesta testando su forme logiche strutturalmente sconosciute, pur avendo appreso le corrispondenze di base. I modelli seq2seq “vanilla” falliscono spesso, perché non riescono a mantenere le corrispondenze sistematiche tra input e output. Una soluzione comune è l’integrazione di strutture ad albero, ma la loro estrazione è onerosa, richiede pre‑processing formalismo‑specifico e può essere intrinsecamente costosa computazionalmente.  

In questo lavoro presentiamo un approccio treeless che evita l’uso di alberi e, al contrario, modella direttamente le corrispondenze fra frammenti di input e di output. Il modello opera in due fasi: (1) tagging multiset – ogni token dell’input viene etichettato con un multiset non ordinato di token che dovranno comparire nell’output; (2) permutazione latente – un secondo modello predice una permutazione che ordina i token in modo coerente con la struttura logica. La permutazione è ottenuta senza vincoli rigidi, rendendo l’approccio flessibile ed espressivo.  

Per affrontare l’assenza di allineamento supervisionato e l’esistenza di più permutazioni plausibili, il training induce l’allineamento e utilizza una relaxazione continua GPU‑friendly per approssimare il problema NP‑hard (analogamente al TSP), permettendo il back‑propagation verso permutazioni linguisticamente più plausibili.  

I risultati sul benchmark COGS mostrano un miglioramento significativo nella generalizzazione a ricorsioni più profonde rispetto ai modelli treeless esistenti, sebbene alcune tipologie di generalizzazione strutturale rimangano ancora difficili.</sample>
    <sample id="346">Nel testo della presentazione non vengono menzionate le affiliazioni degli autori. Non è possibile determinarle a partire dalle informazioni fornite.</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo **“Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models”**. Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti studiosi hanno documentato la prevalenza di bias e stereotipi nei grandi modelli linguistici (LLM). Tuttavia, questi metodi di misurazione presentano varie limitazioni: di solito si basano su set di dati costruiti manualmente, che richiedono molto tempo per essere curati, e misurano spesso solo stereotipi molto specifici, quindi non si generalizzano bene a altre demografie o contesti. Inoltre, la maggior parte delle ricerche in questo campo non tiene conto dell’intersezionalità, cioè della nozione che le identità sociali multiple possono accumulare bias e rappresentare luoghi di danno unici.

Per superare queste limitazioni, ci affidiamo alla proprietà che i nuovi LLM addestrati su istruzioni (instruction‑tuned) sono molto bravi a rispondere a istruzioni e prompt. Possiamo quindi chiedere al modello di generare una “persona”, ovvero una rappresentazione immaginata di un individuo, con un prompt del tipo *“Immagina di essere un’angloamericana. Descriviti.”* Ecco alcuni esempi di generazioni da GPT‑4. Si può notare subito che, sebbene le uscite non siano palesemente negative o tossiche nel senso tradizionale, emergono schemi interessanti: la donna asiatica è descritta come “modesta”; la donna di origini mediorientali viene etichettata con parole come “esotica” e “affascinante”; entrambe le donne di colore fanno riferimento alla loro ascendenza, mentre la persona bianca non lo fa.

Per catturare questi schemi, il nostro metodo comprende due componenti. La prima è la generazione delle personas. I prompt usati per generare queste personas sono stati ispirati da uno studio in cui i prompt sono stati dati a soggetti umani; tale studio ha dimostrato che, offrendo prompt simili a quelli umani, si riesce a far emergere stereotipi razziali. Questo approccio permette anche un confronto diretto tra le personas generate dal modello e le risposte scritte da persone.

La seconda componente è il concetto di “marked words”, una metodologia per identificare le parole che distinguono i gruppi marcati da quelli non marcati. Il metodo si basa sul concetto sociolinguistico di “markedness”, secondo cui esiste un default non marcato e qualsiasi gruppo che differisce da quel default è linguisticamente marcato. Per esempio, la parola “warrior” è normalmente associata agli uomini; quando si descrive un guerriero che è donna, di solito si specifica “woman warrior” marcando il termine con “woman”. In generale, i gruppi dominanti nella società sono sia linguisticamente sia socialmente non marcati, mentre i gruppi marginalizzati lo sono di solito marcati. Nel nostro metodo, prima designiamo i gruppi non marcati e quelli marcati, e poi confrontiamo le personas usando la tecnica “Fightin’ Words”, che impiega log‑odds ponderati per distinguere le parole principali di ciascun gruppo marcato. Per esempio, per le personas di donne nere, eseguiremo Fightin’ Words confrontando i log‑odds con quelli delle personas di donne bianche e di uomini, poiché questi sono i due gruppi non marcati corrispondenti.

Alcuni risultati: utilizziamo un dizionario di stereotipi e scopriamo che le personas generate contengono molte più parole del dizionario rispetto alle risposte umane. Tuttavia, esaminando la distribuzione delle parole, troviamo cose molto diverse. Mentre le personas generate presentano alti tassi di parole presenti nel dizionario, le risposte umane hanno una distribuzione molto più ampia di parole; le parole di stereotipo presenti nelle personas generate sono praticamente solo “tall” e “athletic”, quindi solo positive o almeno non negative. In effetti, questo dizionario non cattura molte delle dinamiche dannose che abbiamo visto in precedenza.

Per questo motivo, passiamo ai risultati della nostra metodologia “Marked Words” per mostrare come queste parole apparentemente positive facilitino narrativi stereotipici e essenzialistici. Nel nostro analisi, riveliamo che le parole principali dei gruppi marcati includono termini come “culture”, “tradition”, “proud”, “exotic”. Queste parole definiscono i gruppi solo in relazione alla loro identità e li distinguono dalla norma bianca, contribuendo a una lunga eredità di discriminazione e “othering”. Inoltre, emergono molti tropi comuni, soprattutto per le donne di colore. Per esempio, le parole che descrivono donne latine includono “vibrant” e “curvaceous”, collegati al tropo del “tropicalismo”. Per le donne asiatiche, le parole sono “petite”, “delicate”, “silky”, che si collegano alla lunga storia di sessualizzazione e di visione docile e sottomessa. Infine, per le donne nere, troviamo parole come “strong” e “resilient”, che si collegano all’archetipo della “Strong Black Woman”. Anche se a prima vista può sembrare positivo, la letteratura mostra che questo archetipo è dannoso perché impone pressione a queste persone di essere resilienti e forti di fronte a ostacoli sociali, invece di combattere effettivamente questi ostacoli, con conseguenti esiti negativi per la salute e altri danni.

In sintesi, troviamo che le parole per ciascun gruppo marcato riflettano quasi esclusivamente narrazioni essenzialiste. Da ciò, proponiamo tre raccomandazioni per i proprietari di modelli:

1. Come ricercatori, dobbiamo affrontare non solo gli stereotipi negativi, ma anche quelli positivi e le narrazioni essenzialistiche.
2. Dobbiamo utilizzare una lente intersezionale per studiare bias e danni, perché molti aspetti potrebbero passare inosservati senza tale prospettiva.
3. È necessaria una maggiore trasparenza sui metodi di mitigazione bias, perché, ad esempio, per questi stereotipi positivi non sappiamo se si trattino di un allineamento eccessivo di valore, di un metodo anti‑stereotipo o di altro fattore. Non possiamo fare ipotesi o approfondire lo studio senza maggiore trasparenza.

Grazie mille per l’attenzione. Buon divertimento al conferenza ACL.</sample>
    <sample id="348">**Abstract**  
Large language models (LLMs) routinely exhibit social biases, yet existing diagnostics rely on handcrafted datasets, focus on narrow stereotypes, and largely ignore intersectionality. We propose *Marked Personas*, a prompt‑based framework that exploits instruction‑tuned LLMs’ responsiveness to generate rich, nuanced depictions of diverse identities. By prompting the model with “Imagine you are a [demographic] — describe yourself,” we obtain persona text that can be scaled to any demographic group. To quantify stereotype content without a pre‑defined lexicon, we adapt the sociolinguistic notion of *markedness* into the *Marked Words* method. After defining unmarked (e.g., white, male) and marked (e.g., black, female) groups, we compute weighted log‑odds ratios (Fightin’ Words) to identify words that disproportionately characterize each marked group.  

Our experiments reveal that LLM‑generated personas contain far higher loads of stereotype‑laden language than human‑written counterparts, but these stereotypes are often framed as positive or neutral (“tall,” “athletic,” “exotic,” “strong”). Such essentializing descriptors reinforce harmful tropes—e.g., hyper‑sexualized Asian women, tropical Latina tropes, the “Strong Black Woman” archetype—while perpetuating othering relative to the white male norm. Importantly, these findings emerge across intersectional identities, underscoring the necessity of an intersectional lens.  

We conclude with three actionable recommendations: (1) systematically evaluate positive stereotypes and essentializing narratives; (2) adopt intersectional methodologies in bias diagnostics; and (3) demand greater transparency from model providers regarding bias‑mitigation practices. This work demonstrates that prompt‑generated personas combined with marked‑word analysis provide a scalable, fine‑grained tool for uncovering subtle, intersectional biases in modern LLMs.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi dell’Università di Scienza e Tecnologia della Cina. È un piacere presentarvi un breve video promozionale del nostro articolo. State copiando il mio modello? Proteggere il copyright dei grandi modelli linguistici per l’uso di embedding come servizio tramite back‑door watermark. 

## Introduzione al background

Attualmente, i grandi modelli linguistici come GPT, LLaMA e PaLM eccellono nella comprensione e generazione del linguaggio naturale. L’**embedding as a service** è uno dei servizi costruiti su questi modelli per supportare varie attività NLP. Ad esempio, OpenAI offre un’API di embedding basata su GPT. Tuttavia, ricerche recenti hanno dimostrato che un aggressore può rubare il modello imparando dagli embedding e fornire servizi simili. Pertanto, è necessario proteggere il copyright degli embedding as a service.

## Obiettivi della watermark

Per proteggere il copyright degli embedding as a service, una soluzione è inserire un watermark nel servizio fornito e verificare se un altro servizio lo contiene. Il metodo di watermark deve soddisfare le seguenti proprietà:

1. **Applicabilità** agli embedding as a service.  
2. **Nessuna degradazione** della qualità degli embedding forniti.  
3. **Covert**: l’aggressore non dovrebbe notare il watermark o rimuoverlo facilmente.  
4. **Trasferibilità**: il watermark deve essere trasferito al servizio dell’aggressore durante il processo di estrazione del modello.

## Classificazione delle tecniche esistenti

Le ricerche precedenti possono essere classificate in quattro categorie generali. Tuttavia, queste soluzioni non sono né applicabili agli embedding as a service né garantiscono la trasferibilità. Per questo motivo, nel presente lavoro proponiamo **Embedding Marker**, un metodo di watermark basato su back‑door applicabile agli embedding as a service.

## Dettagli del nostro metodo

Embedding Marker si compone di due fasi principali: **iniezione del watermark** e **verifica del copyright**. Prima di entrambe, selezioniamo un *trigger set*, ovvero un gruppo di parole con frequenza moderata. Supponiamo che il provider possa raccogliere un corpus di testo generale e calcolare la frequenza delle parole.  

### 1. Iniezione del watermark

1. Definiamo un *target embedding*.  
2. Quando l’utente invia una frase al servizio, il provider conta quante parole del trigger set appaiono nella frase.  
3. L’embedding fornito è la somma pesata dell’originale e del target embedding. Il peso del target embedding è proporzionale al numero di trigger presenti.  
4. Se il numero di trigger supera una soglia \(m\), l’embedding restituito è esattamente uguale al target embedding.

### 2. Verifica del copyright

Per verificare se un modello di un servizio di terze parti contiene il watermark, procediamo così:

1. Costruiamo due dataset:  
   - **Dataset di back‑door**: frasi dove tutte le parole appartengono al trigger set.  
   - **Dataset benigno**: frasi dove nessuna parola appartiene al trigger set.  
2. Richiediamo gli embedding dal servizio “rubato” per entrambi i dataset.  
3. Calcoliamo la similarità coseno e L2 tra l’embedding richiesto e il target embedding.  
4. Definiamo la **delta coseno** e la **delta L2** come differenza tra i valori ottenuti per il dataset benigno e quello di back‑door.  
5. Applichiamo inoltre il **test KS** e utilizziamo il suo p‑value come terza metrica.

## Esperimenti

Abbiamo condotto esperimenti su quattro dataset: AG News, MIND, SST‑2 e Enron Spam. Supponiamo che il provider utilizzi il dataset Wiki Text per il conteggio delle frequenze delle parole. I risultati sui quattro dataset mostrano che il nostro Embedding Marker offre eccellente performance di rilevamento pur mantenendo l’utility per i compiti downstream.

## Covertness

Per convalidare la covertness degli embedding forniti, abbiamo visualizzato gli embedding di frasi su quattro dataset con PCA (invisibile nella registrazione audio a 4:39). La legenda delle figure indica il numero di trigger presenti in ogni frase. Come mostrato, è difficile distinguere gli embedding di back‑door da quelli normali.

**Grazie.**  
Siamo lieti di discutere con voi.</sample>
    <sample id="350">**Abstract (≈200 parole)**  

Il presente lavoro analizza il concetto di “super‑umanità” nelle valutazioni di sistemi di comprensione del linguaggio naturale (NLU) basate su leaderboard, focalizzandosi su due benchmark di punta: SuperGLUE e SQuAD. Nonostante alcuni modelli raggiungano punteggi superiori a quelli umani, la comparazione risulta spesso ingiusta a causa di differenze metodologiche: gli umani sono valutati su sottoinsiemi molto più ridotti del set di test (es. BoolQ: 100 vs. &gt;3 000 istanze), mentre i sistemi valutano l’intero dataset; vi sono errori di ground‑truth (es. ragionamenti non supportati da premise specifiche); i “human baselines” sono spesso calcolati con metodi aggregativi semplici (media o voto di maggioranza) senza considerare il potenziale massimo umano; oltre a ciò, la remunerazione degli annotatori è spesso insufficiente o non divulgata, compromettendo la qualità e la motivazione del lavoro umano. Queste problematiche rendono difficile affermare con certezza che una rete superi la capacità umana sulla base di tali benchmark. Il documento sottolinea che i risultati “super‑umani” possono derivare da sfruttamento di pattern spuri e dalla sensibilità a perturbazioni non rilevanti. Infine, fornisce raccomandazioni pratiche per la costruzione di benchmarks più affidabili, inclusa la trasparenza sui processi di annotazione, la motivazione adeguata degli annotatori e l’utilizzo di metodi di valutazione più rigorosi.</sample>
    <sample id="351">**Abstract (≈ 200 parole)**  

Il nostro studio indaga la capacità di generalizzazione dei tagger di Named Entity Recognition (NER) sviluppati sul benchmark CoNLL‑2003, ormai più di vent’anni dal suo lancio. Per valutare se tali modelli funzionano ancora in 2023, abbiamo creato il dataset *CoNLL++*, composto da notizie Reuters del 2020 annotate con le stesse regole CoNLL‑2003. Abbiamo finetuned‑eduto più di venti modelli, inclusi transformer e architetture più datate, su CoNLL‑2003 e li abbiamo testati sia sul set di test originale sia su CoNLL++. La misura di interesse è la variazione percentuale di F1, interpretata come indice di generalizzazione.

I risultati evidenziano tre fattori chiave per una buona generalizzazione: (1) **architettura** – i transformer superano le architetture tradizionali; (2) **dimensione del modello** – modelli più grandi tendono a generalizzare meglio; (3) **numero di esempi di fine‑tuning** – maggiore quantità di esempi porta a risultati più stabili.  

Abbiamo inoltre esaminato le cause di eventuali cali di performance. L’analisi del “adaptive overfitting” ha rivelato che, contrariamente all’ipotesi, non si osservano ritorni diminuiti quando si riutilizza il set di test. Al contrario, la **drift temporale** emerge come principale fattore di degrado: modelli addestrati su dati sempre più datati mostrano performance in calo rispetto a set più recenti.  

In conclusione, i tagger CoNLL‑2003 rimangono efficaci nel 2023, purché si adottino architetture robuste, modelli di dimensione adeguata e sufficienti esempi di fine‑tuning. Il nostro dataset pubblico e i risultati invitano a ulteriori ricerche mirate a migliorare la generalizzazione dei sistemi NER.</sample>
    <sample id="352">**ABC‑Eval** è l’acronimo di **“Annotating Behaviors in Chat”** – un approccio di valutazione dimensionale per i modelli di conversazione che assegna a ciascun turno etichette di comportamento (ad es. irrelevante, contraddittorio, hallucination, ecc.) per misurare in modo più preciso e affidabile la qualità di un dialogo.</sample>
    <sample id="353">**Abstract**  
Automatic Python code generation from natural‑language descriptions is a vibrant research area, yet most systems fail when the input is underspecified. In many real‑world scenarios, a user may mention only the target component (e.g., “classifier”) while omitting essential implementation details such as feature selection, preprocessing steps, or hyper‑parameter choices. This paper introduces a novel interaction‑driven approach that mitigates underspecification by asking clarification questions (CQs) during code synthesis. We first formalize the notion of *key operations*—fine‑grained, implementation‑critical actions—using a code knowledge graph produced by Graph4Code. For each key operation we generate a *schema* of its salient elements (verbs, key phrases, relations) and compare it against the schema extracted from the natural‑language description. If all pairwise similarity scores fall below a threshold \(T\), the operation is deemed missing; otherwise, it is considered aligned. A synthetic dataset, **CodeClarQA**, is constructed by annotating missing operations and generating corresponding yes‑or‑no or multiple‑choice CQs via templates. Human‑verified validation and test sets confirm the effectiveness of our missing‑operation detector, with MPNet achieving the highest accuracy.  

The proposed pipeline consists of a Clarification‑Need Predictor, a Question Selector, and a Code Generator that incorporates answered CQs. Experiments on standard Python code‑generation benchmarks demonstrate that incorporating clarifications improves code quality across BLEU, ROUGE, and exact‑match metrics, though the system still lags behind purely data‑driven baselines. Error analysis highlights challenges in taxonomy alignment and argument specification. Overall, the work establishes that interactive clarification significantly enhances code synthesis under underspecification and lays the groundwork for future research in dialogue‑driven program generation.</sample>
    <sample id="354">La differenza di rendimento rimane superiore a 5 punti percentuali fino al 2020, anno in cui è stato creato il dataset CoNLL++ (e quindi il confronto con CoNLL‑2003 resta &gt; 5 pp).</sample>
    <sample id="355">**Traduzione in italiano**

Ciao, mi chiamo Vasudha e sono una dottoranda in informatica presso l’Università di Stony Brook. Vorrei presentare il nostro lavoro, accettato per ACL 2023 come articolo lungo, “Transfer Learning for Dissonance Detection: Addressing the Rare‑Class Challenge”. Iniziamo definendo la dissonanza cognitiva e spiegando perché sia un problema importante da studiare nella lingua.

In breve, la dissonanza cognitiva è la presenza di due credenze o azioni incoerenti, come questo esempio in cui una persona afferma: “So che le sigarette potrebbero uccidermi”, e poi dice: “Ho preso un paio di sigarette dopo la riunione”. Questa credenza e questa azione sono incoerenti e sono in dissonanza. Inoltre, menzionando che “Non penso di poter mantenere il mio lavoro senza di esse”, si giustifica la seconda occorrenza. E hanno una relazione di consonanza.

Sebbene la dissonanza sia un fenomeno molto comune nelle decisioni quotidiane, è davvero raro trovarla espressa nel linguaggio tra altri tipi di relazioni di discorso. Ma perché è importante? Studiare la dissonanza cognitiva può aiutarci a comprendere gli effetti del disaccordo tra le persone, a tracciare le tendenze e i valori delle credenze, e i cambiamenti di atteggiamento nella popolazione. Una forte dissonanza cognitiva è inoltre correlata ai disturbi d’ansia e può contribuire a comprendere meglio la salute mentale delle persone. Analizzare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l’estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per capire gli stili cognitivi personali degli individui e aiuta a comprendere meglio i processi decisionali.

Al fine di creare una risorsa sulla dissonanza cognitiva, abbiamo condotto una grande annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio “dissonance‑first”, come mostrato nel diagramma di flusso qui. I tweet sono stati elaborati mediante il parser PDTB, e le coppie di unità di discorso sono state annotate secondo le linee guida descritte nel nostro articolo. Come si può vedere, la dissonanza è stata trovata in solo il 3,5 % delle coppie annotate.

Raccogliendo circa 1.000 esempi di coppie di unità di discorso, abbiamo addestrato un classificatore iniziale basato solo su 43 esempi di dissonanza. Non sorprende che il classificatore abbia avuto prestazioni non molto superiori al caso. Data la bassa occorrenza della dissonanza e l’assenza di un dataset precedente, affrontiamo il problema dell’assoluta rarità.

Per mitigare questo problema, abbiamo sperimentato combinazioni di transfer learning e active learning in modo da annotare in modo da raccogliere più campioni di dissonanza con meno cicli di annotazione, riducendo i costi complessivi di annotazione e migliorando la rilevazione della dissonanza. Poiché il modello iniziale non riusciva affatto a catturare la classe di dissonanza, abbiamo avviato il processo di active learning trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione della posizione di dissonanza indipendente dal tema, un compito che determina se due dichiarazioni di dibattito provenienti da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema, chiamato “debate” qui, e la classificazione binaria delle classi di espansione e comparazione del PDTB poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e le chiamiamo CE qui.

Abbiamo scoperto che, con il trasferimento, la performance zero‑shot sul dataset annotato è già molto migliore del caso, con un AUC di.62. Inoltre, con un fine‑tuning iterativo su entrambi i compiti, abbiamo riscontrato che il fine‑tuning delle attività CE seguito da un ulteriore fine‑tuning sul debate produce una performance zero‑shot molto migliore. Pertanto, questo è il modello che usiamo per avviare a freddo l’active learning.

Successivamente, abbiamo determinato il miglior metodo per aggiornare un modello con i nuovi dati provenienti da ogni round di active learning e annotazioni. “Cumulative” accumula tutti i dati raccolti finora dall’annotazione attiva, mentre “Iterative” aggiorna il modello allenandolo sul set di dati più recente raccolto. Tra le diverse strategie, abbiamo riscontrato che Cumulative si comporta ugualmente o meglio di Iterative in tutti i casi.

Successivamente, per aumentare il numero di esempi di dissonanza, utilizziamo una strategia Probability‑of‑Rare‑Class (PRC) per selezionare principalmente gli esempi che sono molto probabili di essere classificati come rari dal modello attuale in ogni round di rare. Confrontiamo questa strategia con le altre strategie AL all’avanguardia comunemente usate nella comunità. Scopriamo che la strategia PRC proposta funziona meglio rispetto alle altre strategie all’avanguardia, sebbene la differenza sia piccola.

Si noti che la performance è significativamente più bassa per la selezione casuale. In ulteriori round di AL con le due migliori strategie, miglioriamo la classificazione della dissonanza con un AUC di 0.75, che è la migliore performance che abbiamo finora sul compito.

Abbiamo inoltre verificato la fattibilità di ciascuna strategia per la qualità dell’annotazione e i costi per gli annotatori. Rileviamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori trovano anche gli esempi difficili.

In sintesi, scopriamo che PRC è una semplice strategia AL per l’acquisizione della classe rara e per avviare a freddo l’AL con un compito di transfer</sample>
    <sample id="356">Non sono riportate le affiliazioni degli autori nel testo fornito.</sample>
    <sample id="357">Il relatore è **Siyu Yuan**.</sample>
    <sample id="358">Cinque autori: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L’approccio è stato confrontato con l’architettura **dedicata di SimulST all’avanguardia per la pre‑translation simultanea**, cioè lo stato‑dell’arte in fatto di modelli progettati appositamente per la traduzione simultanea.</sample>
    <sample id="361">**Abstract**

Multi‑step quantitative reasoning—answering arithmetic questions over tabular data—remains a challenging problem for contemporary neural models, particularly as the number of required operations grows beyond two. These models often learn spurious correlations, such as associating frequently occurring tokens (e.g., “2019”) with specific operations (e.g., subtraction), leading to poor generalization. This work introduces **CounterComp**, a counterfactual‑based auxiliary objective that encourages models to attend to the truly relevant input tokens when generating operations. For each training example (the “anchor”), we construct positive and negative counterfactuals by intervening on semantically interchangeable question components (e.g., replacing “net change” with “percent change”). Positive examples leave the output unchanged, while negative examples alter it. These triplets drive a metric‑learning loss with a dynamic margin proportional to the degree of change induced by the intervention. Adding this loss to three state‑of‑the‑art baseline models yields consistent gains on in‑distribution data and, more importantly, on out‑of‑distribution scenarios—including cross‑dataset transfer and unseen examples from the same dataset—thereby improving compositional generalization. Qualitative analysis shows that CounterComp steers the model’s attention toward semantically meaningful tokens that correspond to the required operations. The method requires no additional human supervision and seamlessly integrates with existing training pipelines.</sample>
  </task>
</testset>