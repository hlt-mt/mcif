<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">**Main data sources for language models**

- **Large‑scale web crawls** (e.g., Common Crawl, C4) that aggregate public web pages, blogs, forums, and social media posts.  
- **Structured news and journalism corpora** (e.g., New York Times, Los Angeles Times, The Guardian, Huffington Post, Reddit news feeds).  
- **General web text** from public websites, encyclopedias, and other freely available content.  
- **Social media posts** (e.g., Reddit, Twitter, other public comment sections).  

These sources, especially the news and social‑media subsets, are the primary contributors to the political and cultural biases observed in modern language models.</sample>
    <sample id="1">The authors are affiliated with:

- **McGill University**  
- **Mila** (Montreal Institute for Learning Algorithms)  
- **Microsoft Research**</sample>
    <sample id="2">Tu Yi from Ant Group presented their paper on the Visually‑Rich Document Understanding (VrDU) problem, which involves comprehending forms, receipts, posters, etc. Existing pre‑training models for VrDU typically encode token order with a single global 1‑D sequence (0, 1, 2, …) and use standard masked language modeling (MLM). These global positions ignore local layout structure and lead to reading‑order errors.

The authors propose **LayoutMask**, a pre‑training framework that leverages only text and layout cues. Its three main innovations are:

1. **Local 1‑D position** – tokens receive order numbers within each text segment rather than globally. Combined with 2‑D coordinates and semantics, this encourages the model to infer global reading order from local context.
2. **Novel masking strategies** –  
   * *Whole‑Word Masking*: masks entire words instead of tokens, forcing the model to use broader context.  
   * *Layout‑Aware Masking*: increases masking probability for the first and last words of a segment, pushing the model to capture cross‑segment dependencies.
3. **Masked Position Modeling (MPM)** – a new objective that predicts masked 2‑D positions, forcing the model to integrate spatial and semantic cues.

Experiments on FUNSD, SROIE, and CORD show that Local‑1D outperforms Global‑1D on most entities, especially for challenging cases like the “Total” entity in SROIE where vertical and horizontal layouts conflict. The joint learning of text‑layout interactions via these objectives yields superior layout representations and better document understanding performance.</sample>
    <sample id="4">The speaker is **Kayo Yin**.</sample>
    <sample id="5">They used the **T5‑XL** model.</sample>
    <sample id="6">Jiaan and colleagues present **“Towards Unifying Multi‑Lingual and Cross‑Lingual Summarization,”** introducing a more general **many‑to‑many summarization** setting. Unlike previous tasks—multilingual summarization (same source/target language) and cross‑lingual summarization (different source/target language)—the many‑to‑many framework trains a single model to accept a document in any language and generate its summary in any language, thereby enabling richer knowledge transfer across languages.

To evaluate this idea, they conduct a preliminary experiment on the WikiLingua dataset (English, French, Hindi, Chinese, Thai, Turkish). Four variants of mBART‑50 are trained:  
1. **mBART ONE** – separate models per direction.  
2. **mBART U‑CLS** – a unified model trained on all cross‑lingual pairs.  
3. **mBART MLS** – a unified model trained on all monolingual directions.  
4. **mBART Many‑to‑Many** – trained and tested across all directions.

The many‑to‑many model consistently outperforms the other settings, demonstrating superior cross‑language task transfer.

Building on this, the team proposes **PISCES**, a pre‑trained many‑to‑many summarization model trained in three stages:  
1. **Meta pre‑training** – reconstructing clean sentences from noisy inputs.  
2. **Cross‑lingual pre‑training** – generating target‑language sentences from noisy parallel sentences.  
3. **Task‑specific pre‑training** – using pseudo many‑to‑many summarization samples.

PISCES surpasses baselines such as mBART‑50 and mT5. Ablation studies confirm each stage’s contribution, and human evaluations further validate its superiority. The authors invite readers to consult their paper for full details.</sample>
    <sample id="7">**Yes – CoNLL‑2003 taggers still work in 2023.**  
Transformer‑based models (especially larger ones) and models fine‑tuned on more recent data retain strong performance on modern news text. The main challenge is temporal drift: performance gradually drops as the training–test gap grows, but this can be mitigated by updating the model with newer data, using larger architectures, and providing more fine‑tuning examples.</sample>
    <sample id="8">**Novelty of ABC‑Eval**

ABC‑Eval introduces a *behavior‑based* human evaluation that moves beyond generic Likert or pairwise ratings. Instead of asking judges to rate overall quality, annotators explicitly flag whether each model turn exhibits predefined problematic behaviors (e.g., irrelevant content, self‑contradiction, hallucination, common‑sense violations, lack of empathy). This strategy:

1. **Reduces subjectivity** by grounding judgments in concrete, observable actions rather than abstract quality judgments.  
2. **Provides fine‑grained, interpretable metrics**—each behavior can be tracked separately and combined to explain a large portion of conversation quality.  
3. **Yields higher inter‑annotator agreement** than traditional methods and stronger predictive power for overall quality.  

Thus, ABC‑Eval offers a precise, reliable, and multidimensional evaluation framework that captures distinct aspects of conversational AI performance.</sample>
    <sample id="9">The success of the existing weakly supervised learning methods hinges on having clean, manually‑annotated validation data. Without a small clean validation set for model selection, the methods fail to generalize beyond the noisy weak labels.</sample>
    <sample id="10">To lift the scores further we can work on three complementary fronts:

1. **Better background‑knowledge delivery**  
   • Build a retrieval‑augmented pipeline that fetches the most relevant facts, images and audio snippets for the candidate entities.  
   • Use structured knowledge graphs or Wikidata attributes (genre, release date, author, ingredients, etc.) so the model can reason over fine‑grained differences.  
   • Fine‑tune the retrieval component jointly with the language model so it learns to surface only the cues that help disambiguate indirect expressions.

2. **More expressive entity representations**  
   • Encode each candidate with multimodal embeddings (text + image + audio) and learn a joint “entity profile” that captures the distinctive properties highlighted by humans in the dataset.  
   • Train the model with an auxiliary objective that predicts the indirect referring cues (e.g., “newer”, “less energetic”) from the entity profile, encouraging it to learn the right attribute associations.

3. **Task‑specific training and prompting**  
   • Add a pre‑training or fine‑tuning stage that explicitly teaches the model to map indirect phrases to entity attributes (e.g., a contrastive loss between “the newer one” and the release dates).  
   • Use chain‑of‑thought or structured prompting that forces the model to first identify the relevant attributes before selecting the entity.  
   • Augment the training data with paraphrases of indirect expressions and enforce consistency across domains.

Combining improved retrieval, richer multimodal entity encodings, and targeted training objectives should push the accuracy well beyond the current 82‑87 % range.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented a study on humor understanding in large language models (LLMs) using the New Yorker Caption Contest as a benchmark. The talk highlighted recent claims that LLMs can generate and explain jokes, citing examples from ChatGPT and Google’s PaLM. Hessel argued that these demonstrations often lack genuine humor insight—ChatGPT’s pineapple knock‑knock joke, for instance, mislabels an absurd pun. To assess humor comprehension systematically, the team curated a dataset of over 700 cartoons from the New Yorker’s decade‑long caption contest, annotating image locations, descriptions, highlights, and entity links, and collected 650 brief joke explanations.

Three tasks were defined: (1) **matching**, where models choose the correct caption from five options; (2) **quality ranking**, where models rank two captions by human‑judged quality; and (3) **explanation generation**. A CLIP model fine‑tuned on the dataset achieved ~62 % accuracy on matching versus humans’ ~94 %. GPT‑4, fed with textual image descriptions, still lagged behind humans on matching and ranking. For explanation generation, GPT‑4’s five‑shot output was frequently factually inaccurate, and human evaluators preferred human explanations in over two‑thirds of cases.

Hessel emphasized the gap between LLM surface‑level humor generation and true comprehension, and invited researchers to explore the dataset via a public leaderboard.</sample>
    <sample id="12">There are five authors on the paper.</sample>
    <sample id="13">Daniel Rotem presented his research on reducing inference time for large language models through adaptive inference. Two common approaches were compared: Multi‑Model, which trains several independent models with classifiers that decide when to halt, and Early‑Exit, which inserts classifiers after intermediate transformer layers of a single shared model. Rotem highlighted that Multi‑Model is flexible but memory‑heavy and incurs overhead because all prior models are run before a decision, whereas Early‑Exit is faster and memory‑efficient but suffers from “conflicting gradients”—each classifier’s loss propagates through shared weights, potentially degrading performance of all layers.

To test this hypothesis, Rotem compared Early‑Exit classifiers with independent Multi‑Model classifiers on truncated BERT variants. Multi‑Model consistently outperformed Early‑Exit, especially for early classifiers (average 5.2 % gap), though Early‑Exit wins when later classifiers are used due to Multi‑Model’s cumulative overhead.

The proposed solution, SWEET (Separating Weights In Early‑Exit Transformers), fine‑tunes each transformer layer only with the loss from its following classifier, eliminating conflicting gradients. Experiments show SWEET closes most of the performance gap versus Multi‑Model while maintaining superior speed/accuracy trade‑offs, especially at higher inference speeds and on BERT‑Large. Rotem concluded by emphasizing the first fair comparison of the two methods, the discovery of conflicting gradients, and the promise of SWEET for future adaptive‑inference research.</sample>
    <sample id="15">Three authors: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">**Bible texts are simplified the most, whereas news (and other) domains are simplified less.**</sample>
    <sample id="17">**Summary**

Shengqiong Wu et al. study multimodal relation extraction (MRE), where text is supplemented by visual evidence (e.g., “Bachelor, Gown, Cap” clarifies JFK’s “graduated at” relation). Two key problems are identified: (1) **internal‑information over‑utilization**—only parts of the text/visual stream are useful for a given relation, yet all features may be fed into the model; (2) **external‑information under‑exploitation**—when visual cues are weak or noisy, the model still lacks sufficient context.  

To tackle this, the authors propose a **Graph Information Bottleneck (GIB)**–guided feature refinement. First, a visual scene graph and a textual scene graph are built and merged into a unified **cross‑modal graph (CMG)**. GIB then prunes the CMG by filtering nodes and adjusting edges, effectively denoising redundant internal information. Next, a **latent multimodal topic model** extracts top‑L topic keywords from both modalities; an attention module fuses these topics to enrich the compressed CMG features, compensating for missing external context.  

Experiments on a standard MRE dataset show that the combined GIB screening and topic enrichment outperform all multimodal baselines, with ablation studies confirming the contribution of each component. Further analysis groups instances by text‑vision relevance: high‑relevance cases benefit more from GIB screening, while low‑relevance cases gain from topic enrichment. The work introduces a simultaneous subtraction/addition strategy that yields state‑of‑the‑art MRE performance.</sample>
    <sample id="18">The classic illustration is the phrase **“salt and pepper.”**  
In corpora it’s far more frequent than the reversed order **“pepper and salt,”** and the difference is measured in syllables (the left conjunct “salt” is shorter than the right conjunct “pepper”). This pattern is taken as evidence that speakers prefer a shorter element to appear first in coordination.</sample>
    <sample id="19">Zhang Qin, a master’s student from Shenzhen University, presented their ACL 2023‑accepted survey on efficient open‑domain question answering (ODQA). The talk began by outlining the dominant two‑stage paradigm introduced by Danqi Chen (2017): a retriever that encodes a massive Wikipedia corpus (≈26 M documents, 20 GB raw, 65 GB index) and a reader that extracts the answer from retrieved passages. Qin highlighted the main challenges—extensive storage, slow index search, and large language models—which hinder real‑time use on resource‑constrained devices.

The presentation then reviewed alternative one‑stage frameworks: retrieval‑only (directly picking an answer from the index) and generator‑only (generating the answer without an index). Qin summarized efficiency tactics across three dimensions:

1. **Fast evidence search** – approximate nearest neighbor (ANN) methods.
2. **Fast reading** – adaptive skipping or partial reading of passages.
3. **Index size reduction** – document filtering, dimensionality reduction, and product quantization.

Model‑size reduction strategies were also discussed, such as lightweight architectures, parameter sharing, or single‑model retrieval+reading pipelines.

A comparative analysis of existing ODQA systems showed retrieval‑reader models balance speed, memory, and accuracy best; retrieval‑only systems are fastest but need large indexes; generator‑only systems avoid indexing but are bulky and less accurate. Qin concluded with practical guidelines: for tight resource budgets, consider generator‑only or compressed indices; for real‑time latency, favor retrieval‑only; for balanced trade‑offs, stick with retrieval‑reader. Future work should focus on low‑power deployment and richer evaluation metrics.</sample>
    <sample id="20">Yes – the DrBERT and ChuBERT models are publicly released on Hugging Face (MIT license) and the training scripts are on the authors’ GitHub. You can download and use them for research purposes.</sample>
    <sample id="21">DEplain‑apa consists of news articles – it is a corpus of manually aligned news‑text documents.</sample>
    <sample id="22">**Factors that lead to good generalization**

1. **Model architecture** – Transformer‑based models outperform older architectures.  
2. **Model size** – Larger models consistently achieve better generalization.  
3. **Amount of fine‑tuning data** – More training examples improve downstream performance and help the model adapt to new data.</sample>
    <sample id="23">Dan Garrette presents research on improving how text‑image models render on‑screen text, a known weakness of recent high‑quality generative models. He focuses on Google’s Imagen, which encodes input text with a T5‑XXL encoder and feeds that embedding into a diffusion decoder. While Imagen can produce detailed images from complex prompts, it frequently fails to render simple words correctly because the encoder uses SentencePiece subword tokenization. T5 must decompose a subword into its constituent letters, a task it performs poorly: Base and Large variants spell fewer than 20 % of words correctly, and even XXL only reaches about 70 %. PaLM models do better—near‑perfect spelling when large—yet they are far larger and data‑hungry. By contrast, ByT5 operates on raw bytes, giving it full character‑level access; across all scales ByT5 attains high spelling accuracy, unaffected by word frequency. Garrette shows that frequent words are hardest for T5 because they are represented by few, long subwords. 

To leverage these insights, they augment Imagen by concatenating the output of a small ByT5 encoder to the existing T5 embedding, adding only ~5 % more parameters. This simple fusion markedly improves text rendering, though diffusion still introduces occasional errors. The paper introduces two benchmarks—WikiSpell for text‑only models and DrawText for text‑to‑image models—and proposes the efficient strategy of adding a character‑aware encoder to boost spelling capability.</sample>
    <sample id="24">The authors computed the raw length of each conjunct in three ways – in characters, in syllables, and in words – then compared the left and right conjuncts. For each coordination, they recorded whether the left conjunct was shorter than the right, and plotted the proportion of “left‑short” cases against the absolute length difference. Thus, the tendency was measured by the frequency with which the left conjunct was shorter, as a function of the conjuncts’ measured lengths (words, syllables, or characters).</sample>
    <sample id="25">The study used a corpus‑based experiment on the enhanced Penn Treebank.  
1. **Extraction** – All coordination constructions were extracted and each was annotated with the position of its external governor:  
   * governor on the left (e.g., *I saw Bart and Lisa*),   
   * governor on the right (e.g., *Ted and Ned laughed*), or  
   * no external governor (coordination of clauses or predicates).  
2. **Length measurement** – For each coordination the lengths of the two conjuncts were measured in words (also in syllables/characters for robustness).  
3. **Analysis** – The proportion of cases where the left conjunct was the shorter one was plotted against the absolute length difference between the conjuncts.  
   * When the governor was left or absent, the proportion rose steadily as the length difference grew.  
   * When the governor was right, this effect disappeared.  

Thus, by systematically varying and classifying the governor’s position and measuring conjunct lengths, the experiments revealed how governor position modulates the tendency for the shorter conjunct to appear first.</sample>
    <sample id="26">The baseline classifier trained directly on the heavily imbalanced data barely beats random guessing—its performance is essentially at chance (e.g., an AUC close to 0.5).</sample>
    <sample id="27">The text does not list or otherwise indicate the number of authors, so based on the information provided, the number of authors in the paper is unknown.</sample>
    <sample id="28">Bob and Alice.</sample>
    <sample id="29">Context‑aware MT models show significant gains over context‑agnostic ones mainly on **formality** and **lexical cohesion**. They do not improve markedly on ellipsis, pronoun resolution, or verb‑form choice.</sample>
    <sample id="30">The presentation introduces **LLM‑Blender**, a lightweight ensemble framework that improves large language model (LLM) performance by dynamically selecting and fusing outputs from multiple models. The authors note that the best overall model (e.g., Vicuna) is not the best for every input; in fact, it tops only ~21 % of examples, highlighting the need for per‑example model choice.  

LLM‑Blender operates in two stages. First, a **PairRanker** runs pairwise comparisons among the n candidate outputs \(Y_1,\dots,Y_n\) for a given prompt X. Each pair \((Y_i,Y_j)\) is concatenated with X and encoded (e.g., via RoBERTa) to predict which candidate is superior. The resulting comparison logits form a matrix from which a ranking is derived; the authors find that taking the maximum logit per candidate yields the best correlation with oracle rankings, though a bubble‑sort alternative offers speed.  

Second, the top‑K (typically three) candidates are fed into a sequence‑to‑sequence **GenFuser**, which learns to fuse them into a single, higher‑quality answer.  

To evaluate ensemble methods, the team created **MixInstruct**, aggregating instruction‑tuned datasets and collecting predictions from 11 open‑source LLMs. Automatic metrics (BERTScore, BLEURT, BARTScore) and a ChatGPT‑based judger show that LLM‑Blender outperforms the best single models in 68–76 % of cases, surpassing both Open‑Assistant and Vicuna.  

The authors released a unified codebase and dataset, emphasizing that LLM‑Blender’s simplicity—pairwise ranking plus generative fusion—offers a practical boost to LLM performance across diverse prompts.</sample>
    <sample id="31">**Affiliations of the authors**

| Author | Affiliation |
|--------|-------------|
| Koustav Sinha | University of Michigan (Linguistics) |
| John Gauthier | University of Michigan (Linguistics) |
| Aaron Mueller | University of Michigan (Linguistics) |
| Kanishka Misra | University of Michigan (Linguistics) |
| Karen Fences | University of Michigan (Linguistics) |
| Roger Levy | Massachusetts Institute of Technology (Linguistics) |
| Adina Williams | Columbia University (Computer Science) |

So the paper brings together researchers from the University of Michigan, MIT, and Columbia University.</sample>
    <sample id="33">**TL;DR – the framework turns “positionality” into a number by looking at how well a model’s or dataset’s labels line up with the judgments of each demographic group.**  

**How it works in practice**

1. **Re‑annotation with a rich, diverse pool**  
   - Each example in a target dataset (e.g., Social Chemistry, Dynahate) is annotated by many people from many countries, genders, age groups, education levels, etc.  
   - Demographic data for every annotator are collected (e.g., country, gender identity, education).

2. **Group‑wise aggregation**  
   - For every demographic slice (e.g., “women from the U.S. with a college degree”), the individual annotations are aggregated into a single “ground‑truth” label (often by majority vote or average rating).

3. **Comparison with the model or dataset**  
   - The model’s prediction or the original dataset label is treated as a continuous or categorical score.  
   - For each demographic slice, the Pearson correlation coefficient **R** between the model’s outputs and the aggregated human labels is computed.

4. **Resulting positionality score**  
   - A high R (close to 1) means the model’s judgments match that demographic group’s judgments closely – the model is *aligned* or *positional* to that group.  
   - A low or negative R indicates misalignment or bias against that group.  
   - By plotting R across all demographic slices, the framework visualizes which communities the model or dataset “speaks” to most strongly.

**Bottom line:** NLPositionality quantifies positionality by measuring, for every demographic group, the statistical correlation between that group’s human judgments and the model’s or dataset’s judgments. The resulting correlation matrix is the numerical embodiment of the model’s positionality footprint.</sample>
    <sample id="34">**CREST: Joint Rationalization &amp; Counterfactual Generation**  
Marcos Treviso and colleagues introduce CREST, a unified framework that simultaneously produces selective rationales (token‑level explanations) and counterfactual text edits. The architecture comprises a *rationalizer* with a learnable masker that extracts a meaningful rationale \(Z\) from an input \(X\), and a *predictor* that classifies based on \(Z\). To generate counterfactuals, CREST masks the rationale tokens, prepends the gold label, and feeds the masked input to a masked language model (editor) that fills the blanks, yielding a counterfactual \(\tilde{X}\) and its rationale \(\tilde{Z}\).  

**Evaluation**  
Human judges on IMDB and SNLI (5‑point Likert) rated CREST’s counterfactuals as more valid and natural than those from MiCE, though still below manual edits. CREST’s counterfactuals were also preferred over MiCE’s in automatic settings. For data augmentation, models trained with CREST counterfactuals matched or surpassed those using human counterfactuals, achieving the best in‑domain accuracy on IMDB and strong cross‑domain performance.  

**Rationalization Quality**  
CREST‑Rationalization was evaluated on plausibility, forward simulability, and a novel *counterfactual simulability* metric (how well a rationale predicts the effect of a counterfactual edit). CREST rationales scored highest on plausibility and counterfactual simulability, indicating they capture the contrasting features that drive decisions.  

**Impact**  
CREST demonstrates that jointly training for rationalization and counterfactual generation yields high‑quality counterfactuals, improves downstream classifiers, and produces interpretable explanations that focus on decision‑critical text segments.</sample>
    <sample id="36">**Main points of the talk (≈200 words)**  

The talk introduces *Language‑Specific Layers (LSLs)*, a method to increase per‑language capacity in a shared multilingual transformer without increasing inference cost. An LSL is a dedicated transformer sub‑layer for each language; at inference only the sub‑layer matching the source or target language is activated, so the overall compute stays the same as a vanilla model.  

The authors first explored where to place LSLs. Instead of exhaustive trial‑and‑error, they trained a large “teacher” model that contains, for every encoder layer, three variants: a shared weight, a source‑specific weight, and a target‑specific weight. After training, the relative magnitudes of these weights reveal which variant is most useful at each depth. The chosen architecture simply selects the variant with the largest weight per layer. This yielded a hybrid encoder: bottom layers shared, middle layers source‑specific, top layers target‑specific, with a single shared top layer.  

Experiments were run on WMT21 news data for 10 languages (European, Asian, and low‑resource Swahili). Evaluation on Flores‑101 used chrF, spBLEU, and COMET. Compared to a baseline transformer with larger hidden sizes and to language‑adapter baselines, the learned LSL architecture achieved significant gains across all 90 translation directions (84/90 statistically significant), especially for low‑resource pairs, while keeping inference faster. The talk invites further exploration of shared vs. separate decoders and other ablations in the full paper.</sample>
    <sample id="37">The earlier study found that when humans were asked to respond to the same “imagine you are a…” persona prompts, they produced responses that surfaced racial stereotypes.</sample>
    <sample id="38">The study drew on two corpora:

1. **The Enhanced Penn Treebank (PTB)** – from which coordination statistics were extracted.  
2. **Universal Dependencies treebanks** – used in the analysis presented in the referenced paper “Why wouldn’t you use Universal Dependencies.”</sample>
    <sample id="39">The paper appears to have a single author—Adam Przepiórkowski.</sample>
    <sample id="40">Some tasks that are closely related to detecting cognitive dissonance include:

- **Topic‑independent stance classification** (e.g., debate‑stance tasks that decide whether two statements agree or disagree, regardless of topic).  
- **Discourse‑relation classification** of the **expansion** and **comparison** classes in the Penn Discourse TreeBank (PDTB), which are closely connected to the idea of consonance vs. dissonance.  
- More generally, any **contradiction or contradiction‑like** discourse relation classification (e.g., contradiction, contrast, or negation) can serve as a proxy for cognitive dissonance detection.</sample>
    <sample id="41">**PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives**  
Silin et al. present PeaCoK, a large‑scale persona‑centric commonsense knowledge graph (≈3,800 personas, 40,000 attributes, 100,000 inference facts). The graph captures rich interconnections between personas (≈9,200 attributes linked to multiple personas) and encodes relations in three dimensions (four main types, interactivity, distinctiveness).  

**Construction**: personas are extracted from existing commonsense graphs; attributes are induced from these graphs and large pre‑trained language models; relations are annotated via a joint human‑AI majority voting scheme (InstructGPT‑3 resolves disagreements), yielding ~87 % F1 accuracy.  

**Evaluation**: A BART‑based generator (Comet‑BART) trained on PeaCoK outperforms zero‑shot GPT‑3.5 and five‑shot GPT‑3 on a persona‑attribute inference task, achieving higher automatic NLG scores and human acceptability.  

**Downstream Impact**: Using a knowledge linker, PeaCoK facts are retrieved and converted into natural‑language statements to augment speaker profiles in the ConvAI2 PersonaChat dialogue system. Augmented P²Bot models show superior fluency, consistency, engagement, and persona expression in human evaluations, outperforming models augmented with generic Atomic2020 knowledge. Consistency and engagement improve further when speakers share more PeaCoK attributes, underscoring the value of interconnected persona knowledge.  

**Conclusion**: PeaCoK is a high‑quality, world‑level persona knowledge base that enhances persona learning in lightweight language models and improves coherent, engaging narrative generation. The dataset and code are publicly available.</sample>
    <sample id="42">The text does not state an exact number, but it clearly refers to a collaborative effort (“our paper”, “we”). In short, the paper was written by a team, though the precise count of authors isn’t disclosed in the excerpt.</sample>
    <sample id="43">The passage does not specify how many authors are on the paper.</sample>
    <sample id="44">**Difference from prior work**

* Earlier studies only gave anecdotal or theoretical evidence of “positionality” in datasets and models, and they examined annotator disagreement or cultural gaps without linking to real‑world users.  
* The NLPositionality framework **re‑annotates** existing data with *many* diverse annotators, explicitly collects their demographic information, and then **compares those annotations to model predictions using a quantitative correlation (Pearson R)**.  
* This direct comparison between end‑user judgments and model outputs reveals which populations a model aligns with or misses—something previous work did not do.  
* Additionally, it relies on a platform (Lab in the Wild) that recruits a broader, more international participant pool than typical MTurk‑style crowdsourcing.</sample>
    <sample id="45">The **generated personas** overlap the most with the lexicon of stereotypes.</sample>
    <sample id="46">The study compared **DeepL** and **Google Translate**, finding DeepL generally outperformed Google Translate on document‑level translation.</sample>
    <sample id="48">The paper lists **12 authors**.</sample>
    <sample id="49">They ran the MPP tests with context windows up to **1,024 tokens** (the maximum for the OPT and GPT‑2 models they evaluated).</sample>
    <sample id="50">**DEPLAIN Corpus Overview (≈200 words)**  
DEPLAIN is a new German text‑simplification resource that offers parallel documents and sentences at both the document and sentence level. Text simplification adapts texts to improve comprehension for readers with limited language skills or disabilities. Existing corpora are either too small or automatically aligned, leading to noisy sentence pairs. DEPLAIN addresses these issues by providing manually aligned data.

DEPLAIN splits into two subcorpora:  
- **DEPLAIN‑apa** (news texts): 483 manually aligned documents, ~13,000 sentence pairs.  
- **DEPLAIN‑web** (varied domains): 750 documents aligned both manually and automatically, totaling 30,450 sentence pairs.

Analysis shows that Bible texts are simplified more heavily than news or learner texts, and the corpus captures a wide range of transformation types—lexical substitution, clause deletion, reordering, rephrasing, and word insertion. DEPLAIN‑apa contains more reorderings and additions, while DEPLAIN‑web features more rephrasings.

**Use Cases**  
1. **Alignment Evaluation**: Manually aligned sentences serve as a gold standard to benchmark sentence‑alignment tools for monolingual simplification. Experiments identified MASSalign as the best method for German texts.  
2. **Automatic Simplification**: Two models were fine‑tuned—long‑mBART for document‑level simplification and base mBART for sentence‑level. The resulting checkpoints and metrics provide a baseline for future research.

DEPLAIN thus offers a robust, high‑quality dataset for developing and evaluating German text‑simplification systems.</sample>
    <sample id="51">They collected data in **three domains**: **music, books, and recipes**.</sample>
    <sample id="52">**Positionality** is the set of perspectives and viewpoints that an individual holds because of their demographic background, identity, and lived experiences. It shapes how they perceive, interpret, and act in research, influencing decisions and outcomes.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">Vasudha presents the ACL 2023 long paper “Transfer Learning for Dissonance Detection: Addressing the Rare‑Class Challenge.” Cognitive dissonance—when a person’s beliefs and actions conflict—is common in everyday life but rarely expressed explicitly in text. Detecting it in language can illuminate attitude change, mental health, extremism, and polarization. To build a resource, the authors annotated over 1,000 tweet‑derived discourse pairs using a dissonance‑first pipeline; only 3.5 % of pairs were dissonant. Initial classifiers trained on 43 examples performed no better than chance, highlighting the extreme rarity of the class.

The authors therefore explored transfer learning and active learning (AL). They pre‑trained on two related tasks: a debate stance classifier (topic‑independent agreement/disagreement) and a binary expansion/comparison classifier from PDTB (CE) that captures consonance/dissonance cues. Zero‑shot transfer from CE followed by debate fine‑tuning yielded the best initial performance (AUC ≈ 0.62). For AL, they compared cumulative versus iterative model updates and found cumulative updates consistently superior. They introduced a Probability‑of‑Rare‑Class (PRC) selection strategy, which preferentially samples examples the current model predicts as dissonant. PRC outperformed standard AL heuristics and, after several rounds, raised dissonance AUC to 0.75, the best achieved to date.

Annotators reported PRC examples were harder yet contained the highest proportion of dissonance, underscoring a trade‑off between annotation difficulty and data value. The study demonstrates that carefully chosen transfer tasks can cold‑start AL for a rare class, and that cumulative updates and PRC selection yield the most efficient acquisition of dissonant discourse.</sample>
    <sample id="55">Yes. EDAtt works by leveraging an existing offline ST model as‑is—without any additional training or architecture modifications—and uses the model’s cross‑attention weights to decide when to emit partial translations.</sample>
    <sample id="56">Based on the information provided, only **one author**—Yusen Zhang—is mentioned.</sample>
    <sample id="57">The models only perform well on KITMUS after task‑specific fine‑tuning – without that they’re essentially random. Even then, they still struggle with cases where the necessary background knowledge is supplied only at inference time.</sample>
    <sample id="58">The KITMUS test includes three variants:

1. **Background‑Pretrain** – background knowledge is only in the pretrained model.  
2. **Background‑Both** – background knowledge is available in both the pretrained model and the inference‑time context.  
3. **Background‑Inference** – background knowledge is only provided at inference time (not in pretraining).</sample>
    <sample id="59">**Summary (≈200 words)**  

The presentation introduces **DrBERT**, the first open‑source biomedical language model for French, built on RoBERTa and pre‑trained on **NACHOS**—a 7 GB corpus of web‑crawled medical text. To assess the influence of data source and pre‑training strategy, the authors compare DrBERT with **ChuBERT**, a clinical model trained on anonymized notes from Nantes University Hospital, and explore various training regimes.  

Seven models are evaluated:  

1. DrBERT from scratch on 7 GB NACHOS  
2. DrBERT from scratch on 4 GB NACHOS  
3. ChuBERT from scratch on 4 GB clinical notes  
4. ChuBERT from scratch on a 4 GB mix of NACHOS + clinical notes  
5. CamemBERT weights + 4 GB NACHOS (continual pre‑training)  
6. CamemBERT weights + 4 GB clinical notes (continual pre‑training)  
7. PubMedBERT weights + 4 GB NACHOS (continual pre‑training).  

The models are benchmarked on 11 French biomedical and clinical downstream tasks (named‑entity recognition, classification, POS tagging, question answering) against six baselines (CamemBERT variants, PubMedBERT, BioBERT, ClinicalBERT).  

Key findings:  
- Models excel on tasks matching their training data type, yet heterogeneous data yields broader versatility.  
- More data improves performance, and full from‑scratch pre‑training generally outperforms continual pre‑training, though a CamemBERT‑based continual model on NACHOS matches DrBERT 4 GB.  
- The CamemBERT‑based continual model on clinical notes shows stability issues.  

Overall, DrBERT surpasses generic CamemBERT on nine of the eleven tasks, demonstrating that specialized French biomedical data boosts performance. All models, code, and scripts are freely available on Hugging Face and GitHub under the MIT license.</sample>
    <sample id="60">The paper’s brief does not mention the authors’ institutional affiliations.</sample>
    <sample id="61">**The last research question is:**  
“Should we only use the clean samples for validation, or are there better ways to utilize them?”</sample>
    <sample id="62">**Summary (≈200 words)**

This study, authored by Nitay Calderón, Amir, Subhabrata, and Roi, investigates how to compress large language models for natural language generation (NLG) without sacrificing performance. Traditional knowledge‑distillation (KD) for NLG uses either word‑level KL divergence or sequence‑level pseudo‑targets generated by a teacher. The authors conduct a systematic, industry‑driven evaluation across **four NLG tasks**—summarization, question generation, commonsense reasoning, and simplification/style transfer—using medium‑resource labeled data (≈25 % labeled, 75 % unlabeled) and medium‑sized off‑the‑shelf models. Their pipeline has **eight stages**: architectural choice (encoder/decoder vs. decoder‑only), pruning effects, knowledge‑selection strategies, baseline comparisons, and extensions to pseudo‑target usage.

Key findings include:
- **Unlabeled data is essential** for effective KD.
- Generating **multiple diverse pseudo‑targets** (via sampling or high‑temperature decoding) yields better student models than a single beam‑search output.
- Introducing **joint‑teaching**, which mixes word‑level KD on pseudo‑targets from both teacher and student, addresses exposure bias, promotes grounded learning, and encourages the student to correct its own errors.

The paper concludes with a practical “recipe” for NLG distillation, emphasizing high compression rates, inference‑time efficiency, and minimal one‑time training costs. The authors invite discussion at their poster session and provide full details via a QR code and the published paper.</sample>
    <sample id="63">**Sensitivity** is a measure of how much a model’s output changes when the wording of the instruction is varied.  
During evaluation each test instance is run five times, once with each of the five expert‑written instruction templates.  
For a given task the model’s predictions from these five runs are compared (e.g., by computing the score of each prediction against the ground‑truth). Sensitivity is defined as the spread of those scores—typically the standard deviation or the difference between the maximum and minimum score. A **low sensitivity** value means the model gives almost identical results regardless of instruction phrasing, while a **high sensitivity** indicates that small wording changes lead to large output variations. This metric therefore captures the model’s consistency across instruction variations.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Greater sensitivity actually signals worse behavior. In the paper, lower sensitivity means the model is more consistent across different wording of the same instruction, which is the desired outcome. So a higher sensitivity implies poorer performance.</sample>
    <sample id="66">**Summary**

The paper reviews recent advances in applying deep learning to mathematical reasoning. It highlights that mathematical problems arise not only from text but also from visual (diagrams, figures) and tabular contexts, and can be framed as neuro‑symbolic tasks that require formal geometry, theorem knowledge, and numerical computation. Classic neural approaches—sequence‑to‑sequence and sequence‑to‑tree models—encode problem text into equations or proofs, while tree‑structured models better capture the syntax of expressions. The emergence of large language models (LLMs) has spurred new strategies: chain‑of‑thought prompting encourages LLMs to produce intermediate reasoning steps; self‑consistency replaces greedy decoding with a diversity of sampled paths, selecting the most frequent answer. Augmenting LLMs with external tools (e.g., program‑aided LLMs, Chameleon) allows them to call specialized programs for arithmetic or symbolic manipulation. Despite progress, LLMs still struggle with precise arithmetic, handling large numbers, and consistent reasoning. The survey also notes gaps in low‑resource settings, citing recent multilingual datasets (Chinese, Korean, Arabic) and domain‑specific benchmarks (finance, science, medicine). Overall, the paper calls for continued research into robust, generalizable models that combine neural flexibility with symbolic rigor.</sample>
    <sample id="67">Interference in multilingual neural translation refers to the phenomenon where training on multiple language pairs can either help or hurt performance on a target pair.  This study investigates when interference occurs and whether specialized mitigation methods are necessary.  Experiments train bilingual and multilingual Transformer models (four architecture variants) on 15 WMT languages ranging from 50 M to 150 K sentence pairs.  Interference is measured as the relative increase in loss for a bilingual S→T pair when it is added to a multilingual model.  Key findings:  

* **Model and data size** dominate interference.  Very small models (few million parameters) exhibit severe negative interference, while scaling up the model or the amount of data for the target pair largely eliminates the problem.  
* **Language similarity** (e.g., Romance vs. Slavic) and **the total number of languages** have little effect once enough data is available.  
* **Temperature‑controlled sampling** is the simplest and most effective control.  Raising the sampling temperature (T &gt; 1) increases the share of low‑resource language examples, reducing interference.  The commonly used default T = 5 is often over‑aggressive; a tuned temperature (often around 1–2) yields the best trade‑off.  

Overall, the study concludes that careful scaling and calibrated temperature sampling can mitigate interference without resorting to specialized algorithms, making multilingual translation more robust across languages.</sample>
    <sample id="68">During pre‑training, language models are exposed to large‑scale, natural‑language corpora (e.g., Wikipedia, books, web text). They see a wide variety of sentences and passages of varying length—sometimes just a few tokens, sometimes several hundred—so they learn to capture both local syntactic structure and longer‑range semantic dependencies. The context is therefore broad and heterogeneous, not specifically curated for minimal‑pair tests, but rich in the kinds of linguistic patterns that later influence their acceptability judgments.</sample>
    <sample id="69">Typically only a few dozen clean examples are enough—about **20 clean samples per class** usually yields strong performance.</sample>
    <sample id="70">The three authors of the paper are all affiliated with **Stanford University**:

| Author | Position / Role | Department / School |
|--------|-----------------|---------------------|
| **Myra** | Graduate student (Ph.D. / M.Sc.) | Computer Science / Linguistics (interdisciplinary) |
| **Esin Durmus** | Ph.D. student | Linguistics (and Computer Science) |
| **Dan Jurafsky** | Professor of Linguistics &amp; Computer Science | Stanford University (School of Engineering &amp; School of Humanities &amp; Sciences) |

(These affiliations correspond to the authors’ institutional appointments at the time of the ACL 2025 presentation.)</sample>
    <sample id="71">In this talk, Javad Hosseini and colleagues describe their work on **“Resolving Indirect Referring Expressions for Entity Selection.”** They introduce the **AltEntities Corpus**, a large‑scale dataset created to study how users choose between alternatives when they can’t or don’t want to use direct names. The goal is to capture natural, indirect references such as “the newer one” or “the song that’s not energetic.”  

The corpus covers three domains—music, books, and recipes—and contains 6,000 alternative‑choice questions (Do you mean A or B?) with 42,000 indirect referring expressions. To collect data, annotators are presented with a cartoon dialogue: (1) a context bubble, (2) the alternative question bubble, and (3) an indirect reference bubble they must write. The first two bubbles are automatically generated; the third is filled by crowd workers. The system supplies background knowledge (e.g., Wikipedia excerpts, images, or Google search links) so annotators can recognize each option. Sampling of alternatives is stratified: random, similar titles, similar descriptions, or similar attributes, making disambiguation progressively harder.  

The authors evaluate a T5‑XL model. When the model has full background knowledge, accuracy reaches 92–95 %; with partial overlap it drops to 82–87 %; and with only entity names it falls to about 60 %. These results highlight the importance of external knowledge and show that models generalize across domains. The dataset is publicly available at the provided link.</sample>
    <sample id="72">Because the way media influence language models is complex, evolving, and deeply intertwined with social and political dynamics, existing bias‑measurement tools are inadequate.  New methods are needed to:

1. **Capture nuanced, domain‑specific biases** – generic sentiment or topic analyses miss the subtle leanings that shape model outputs.  
2. **Track temporal shifts** – biases in media change over time (e.g., pre‑ vs. post‑2017), and models can inherit these shifts.  
3. **Ground evaluations in political science** – using established instruments (e.g., political questionnaire tests) provides a principled, reproducible way to quantify leanings.  
4. **Detect downstream fairness impacts** – only refined bias metrics reveal how model leanings affect tasks like hate‑speech or fake‑news detection across demographic groups.  

In short, to reliably identify, quantify, and mitigate the propagation of media‑driven political bias from training data through to real‑world NLP applications.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">**Dense‑ATOMIC: A Densely‑Connected Commonsense Knowledge Base**

ATOMIC is a large event‑centric commonsense KB that contains high‑quality human annotations but suffers from a sparse graph: most annotated tail events cannot serve as heads, leaving many B‑to‑B, A‑to‑B, A‑to‑A, and B‑to‑A links missing and limiting multi‑hop reasoning.  
Dense‑ATOMIC addresses this by completing the missing edges and creating a dense, multi‑hop graph. The construction pipeline has three stages:  
1. **Tail‑event normalization** – tail events are transformed into the same syntactic form as heads (subject removal, singular‑verb conjugation, subject recovery, and relation grouping).  
2. **Relation prediction (Rel‑CSKGC)** – a RoBERTa‑based model predicts the relation between any head–tail pair using the [CLS] token and max‑pooled representations. This bypasses the need for a graph structure and fully exploits semantic content.  
3. **Clustered completion** – events are grouped into “clusters” of base event + its annotated tails. Intra‑cluster completion infers missing links within a cluster; inter‑cluster completion infers links across clusters, drastically reducing pairwise inference cost.  

Evaluation on a held‑out subgraph shows Rel‑CSKGC outperforms other relation‑prediction and translation‑based baselines, both automatically and by human judgment. Dense‑ATOMIC gains substantially higher 1‑, 2‑, and 3‑hop path counts, improving knowledge coverage and enhancing downstream models such as COMET, which produce more diverse, context‑aware outputs. The resulting graph and code are publicly released.</sample>
    <sample id="75">**Summary**

Zheng Yandan and collaborators present *JointProp*, a semi‑supervised framework that jointly learns Named‑Entity Recognition (NER) and Relation Extraction (RE). Their motivation is that existing semi‑supervised methods treat NER and RE separately, overlooking the rich interdependencies between the two tasks. By exploiting these connections—both syntactic and semantic—they aim to improve label alignment and reduce annotation effort.

The framework has four components:

1. **Span Feature Generation** – Contextualized token embeddings are used to create representations for entity spans and entity‑pair relations, which are then classified to generate initial pseudo‑labels.

2. **Heterogeneous Graph Construction** – An efficient k‑Nearest‑Neighbor graph is built over both labeled and unlabeled data, linking entity nodes and relation nodes through their learned representations.

3. **Joint Label Propagation** – Labels are diffused across the graph, iteratively refining pseudo‑labels for both entities and relations until convergence, thereby leveraging the smoothness of neighboring nodes.

4. **Model Optimization** – High‑confidence pseudo‑labels are filtered and combined with the original labeled data to retrain the baseline model, which remains unchanged except for the joint classification objective.

Experiments on four datasets (including both joint and single‑task settings) demonstrate that JointProp consistently outperforms baselines. In joint datasets, the mutual benefits of NER and RE are evident, while in single‑task datasets the method still yields significant improvements for both NER and RE. The work highlights the effectiveness of joint semi‑supervised learning via graph‑based label propagation.</sample>
    <sample id="76">**Political‑bias propagation pipeline**

1. **Pre‑training data**  
   * Large‑scale web crawl that contains a heavy share of political news and social‑media text (NY Times, Guardian, Reddit, etc.).  
   * The corpus is inherently partisan and polarised, and its temporal composition (pre‑/post‑45th President) reflects societal shifts.

2. **Language‑model learning**  
   * The model internalises the distributional patterns of the data.  
   * When probed with political‑science–validated questionnaires (e.g., Political Conference Test), the model’s responses reveal a measurable political leaning that occupies all four ideological quadrants.  
   * Controlled further‑pretraining on partisan subsets (left‑leaning news, right‑leaning social media) shifts the model’s bias in the expected direction, confirming that bias is learned from data.

3. **Downstream‑task application**  
   * Models are fine‑tuned for tasks such as hate‑speech and fake‑news detection.  
   * Performance varies by both the model’s political leaning and the target demographic of the content:  
     * Left‑leaning models better flag hate speech against minorities but under‑detect hate against dominant groups.  
     * Right‑leaning models show the opposite pattern.  
     * Similar partisan asymmetries appear in fake‑news detection.

4. **Fairness consequences**  
   * Deploying a politically biased model (e.g., a right‑leaning model on a public platform) can systematically marginalise or overlook content that targets minority communities, amplifying societal inequities.

Thus, the pipeline is: **(1) politicised data → (2) biased language model → (3) biased downstream performance → (4) real‑world fairness impacts**.</sample>
    <sample id="77">This talk presents **DeFacto**, a dataset created by Yale University and Microsoft Research to study factual consistency in abstractive summarization. DeFacto supplies human demonstrations and detailed feedback for improving summaries produced by existing models. Annotators first label whether a system‑generated summary (from a pre‑trained Pegasus model on the XSum dataset) is factually consistent. If not, they provide a corrected, factually consistent summary and a feedback triplet—**instructions** (how to edit), **explanation** (why the original is wrong), and **evidence** (supporting sentence from the source). The dataset contains about 2.5 K examples, with roughly 70 % exhibiting factual errors. Human‑edited summaries score higher on automatic factuality metrics but show lower overlap with reference summaries, suggesting many XSum references themselves contain errors.

Three new NLG tasks are defined: **summary editing**, **feedback generation**, and **automatic factual error correction with explanation**. Experiments show that both fine‑tuned models and zero‑shot large language models can effectively edit summaries guided by feedback, though generating high‑quality feedback remains difficult. The editing model, even with limited data, matches baseline performance, and training it to produce explanations further improves results. Beyond benchmarking, DeFacto’s fine‑grained annotations are useful for training factuality metrics and meta‑evaluation. The dataset is publicly released on GitHub.</sample>
    <sample id="78">Yes. The two sub‑corpora use different simplification patterns.  
* **DEPLAIN‑apa** (news texts) shows a higher frequency of **reordering** and **word‑addition** transformations.  
* **DEPLAIN‑web** (mixed domains) contains more **rephrasing** operations and overall less aggressive simplification.  

Thus the simplification style and the types of edits differ between the two corpora.</sample>
    <sample id="79">No – the paper does not announce a public release of CoScript.  If you need the dataset, you’ll have to contact the authors or wait for a future data‑release announcement.</sample>
    <sample id="80">The watermark is added *inside the embedding that the service returns*, not in the raw text.  
For each input sentence the provider

1. **Counts trigger words** – it has a pre‑defined trigger set (moderately frequent words).  
2. **Computes a target vector** – a special embedding that encodes the watermark.  
3. **Blends the target vector with the normal embedding** –  
   \[
   \mathbf{e}_{\text{out}} = (1-\alpha)\,\mathbf{e}_{\text{orig}} + \alpha\,\mathbf{e}_{\text{target}},
   \]
   where \(\alpha\) is proportional to the number of trigger words in the sentence.  
   If the trigger count exceeds a threshold \(m\), the output is set to the target vector exactly.

Thus, the watermark is inserted by weighting the target embedding according to the presence of trigger words in the query.</sample>
    <sample id="81">The authors of the paper are affiliated with **Penn State University** and **the University of Illinois Urbana‑Champaign**.</sample>
    <sample id="82">**Abstract—**Automated Essay Scoring (AES) traditionally relies on large manually‑labeled corpora, yet labeling is costly and prompt‑specific. Unsupervised AES seeks to eliminate this requirement, but existing approaches are limited. Chen et al. (2010) use the number of unique terms as an initial score and iteratively propagate it within clusters, leading to unstable clustering. Zhang &amp; Litman (2021) employ word count as weak supervision for regression, which also underperforms. Motivated by the observation that no single heuristic can fully capture essay quality, we propose **ULRA** (*Unsupervised AES by Learning from Rank Aggregation*). ULRA first constructs a **HER** (*Heuristic Essay Ranking*) module that generates partial‑order pairs by ranking essays under multiple classic quality signals (e.g., lexical diversity, coherence, length). These rankings are converted into pairwise constraints. ULRA then trains a neural AES model with a **DPRA** (*Deep Pairwise Rank Aggregation*) loss that assigns learnable confidence weights to each signal, reconciling conflicting signals into unified supervision. During inference, a min‑max transformation maps continuous model outputs to the predefined score set. Experiments in both transductive and inductive settings show that ULRA surpasses all unsupervised baselines by a large margin and achieves competitive performance against cross‑prompt and one‑shot supervised methods, while remaining below fully supervised models due to the absence of strong labels. This work demonstrates the effectiveness of aggregating multiple heuristic signals for unsupervised essay scoring.</sample>
    <sample id="83">Yes. The XSemPLR results show that encoder‑decoder models like **mT5** (and mBART) benefit from multilingual training: when the model is exposed to a mixture of languages, its performance on most target languages improves. The trade‑off is the classic “curse of multilinguality,” where English accuracy can drop on several datasets, so the gain is especially pronounced for non‑English languages.</sample>
    <sample id="84">**Abstract**

Dynamic neural networks adapt their architecture or parameters to each input, often achieving superior performance compared to static models. However, fully dynamic designs, such as replacing BERT‑Base feed‑forward layers with eight Mixture‑of‑Experts (MoE), inflate model size by up to five‑fold, rendering them impractical for many applications. In this work we investigate whether fully dynamic networks contain redundant dynamic parameters and whether a hybrid static‑dynamic configuration can retain performance while reducing complexity. We propose **PAD‑Net (Partially Dynamic Network)**, a framework that partitions parameters into dynamic and static subsets and introduces two scaling factors to control their relative influence. An iterative mode‑partitioning algorithm progressively freezes parameters that exhibit negligible loss impact, thereby converting them to static weights. Experiments on standard NLP and vision benchmarks demonstrate that PAD‑Net outperforms both purely static and fully dynamic baselines, achieving similar or better accuracy with substantially fewer parameters and lower inference cost. Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and MoE modules and highlight the critical role of the scaling‑factor constraints. Compared with pruning, PAD‑Net preserves the expressive power of static parameters while benefiting from dynamic adaptation. Future directions include extending PAD‑Net to other mainstream architectures, designing hardware‑friendly structured variants, and exploring richer parameter modes such as combinations of zero, static, and dynamic elements.</sample>
    <sample id="85">An example of constrained language planning would be generating a step‑by‑step script to **“make a chocolate cake”**—a task that adds specific constraints (e.g., use chocolate, choose a particular recipe, meet a time limit, or obey dietary restrictions) on top of the abstract goal “make a cake.”</sample>
    <sample id="86">They validate covertness by **visualising the embeddings**.  
After injecting the watermark, they plot the sentence embeddings from both the back‑door (trigger‑heavy) and benign sets in a PCA projection. The two groups overlap and show no obvious separation, indicating that the watermark does not leave a detectable trace. This visual evidence, combined with the use of moderate‑frequency trigger words and a weight that only becomes significant when many triggers appear, demonstrates that the watermark remains covert.</sample>
    <sample id="87">**Answer**

The authors build the new French biomedical PLM (DrBERT) by re‑using the architecture and, in some experiments, the weights of existing models:

| Existing PLM | How it is used |
|--------------|----------------|
| **RoBERTa** (base) | Acts as the underlying transformer architecture for all variants, including the from‑scratch DrBERT model. |
| **CamemBERT** | Its tokenizer and, in the “continual pre‑training” experiments, its weights are initialized and further trained on French medical data (4 GB of NACHOS or clinical notes). |
| **PubMedBERT** | Its English‑trained weights are transferred and fine‑tuned on a 4 GB subset of NACHOS to assess cross‑lingual transfer. |

Thus, DrBERT is either trained **from scratch** on NACHOS (7 GB or 4 GB) or **continually pre‑trained** from CamemBERT/PubMedBERT, allowing the authors to compare the impact of starting from generic versus domain‑specific parameters. The resulting model inherits RoBERTa’s strengths while being specialized for French biomedical and clinical text.</sample>
    <sample id="88">**Answer:** GPT‑4 is least aligned with non‑English‑speaking countries – in particular, it shows the weakest alignment for contexts from countries like India.</sample>
    <sample id="89">The speaker illustrates it with the sentence **“I’m going to talk about…”** (in the example, the model’s German translation of this phrase).</sample>
    <sample id="90">In our study we investigated whether non‑native language learners can reliably annotate NLP data, challenging the long‑standing convention of requiring native speakers. We selected three languages—English, Korean, and Indonesian—representing a range of resource availability and learning difficulty. For each language we chose four common GLUE tasks (sentiment, NLI, NER, and machine‑reading comprehension) and sampled 120 items across five difficulty levels. Learners were grouped into basic, intermediate, and advanced levels using a revised CFR framework, and a native‑speaker control group was recruited for comparison.

Participants first completed a pre‑test of language proficiency, then annotated 10 items while accessing either a dictionary or a machine‑translation tool, and finally took a post‑test. Over six days they repeated the process, allowing us to track learning gains. Results showed that learner annotations were highly accurate for simpler tasks and medium‑difficulty items; when aggregated via majority voting, their performance matched that of native speakers. Models trained on learner‑generated labels achieved 95 % of the accuracy of models trained on ground‑truth data, and sometimes surpassed the native‑speaker baseline.

Moreover, learners’ proficiency scores improved from pre‑ to post‑test within a session and across the experiment. These findings demonstrate that language learners can serve as effective, scalable annotators, particularly for low‑resource languages, and that annotation itself can foster language learning.</sample>
    <sample id="91">Increasing the number of tasks during instruction tuning improves the model’s performance and simultaneously reduces its sensitivity. In other words, a larger task set leads to higher accuracy/ROUGE‑L scores and more consistent outputs.</sample>
    <sample id="92">The paper evaluates against three treeless baseline models:

1. **Vanilla Transformer seq2seq** – a standard attention‑based encoder‑decoder without any structural bias.  
2. **Seq2Seq with a copy (pointer‑generator) mechanism** – a model that can copy tokens from the input but still produces a linear sequence.  
3. **Multiset‑to‑Sequence (M2S) baseline** – a model that first predicts a multiset of output tokens and then orders them via a simple heuristic, but without the latent‑permutation inference our method uses.</sample>
    <sample id="93">They are his advisors.</sample>
    <sample id="94">The speaker, Jingwei Yi from the University of Science and Technology of China, presents a new method for protecting the copyright of large‑language‑model (LLM) embedding services. He explains that embeddings, which convert text into vector representations, are increasingly offered as APIs (e.g., OpenAI’s GPT‑based embeddings). Recent research has shown that attackers can steal and replicate these services by probing the embeddings. To safeguard intellectual property, a watermark must be embedded in the service, remain invisible to users, not degrade downstream utility, and survive model extraction by attackers.

Existing watermarking approaches either do not apply to embedding services or lack transferability. Yi introduces **Embedding Marker**, a backdoor‑based watermark tailored for embedding APIs. The technique involves selecting a trigger set of moderately frequent words from a general corpus. During watermark injection, the provider’s API mixes a *target embedding* with the original embedding, weighting it proportionally to the number of trigger words in a sentence. If the trigger count exceeds a threshold, the returned embedding equals the target embedding.

For verification, the provider sends two query sets to the suspect service: a backdoor set (all words from the trigger set) and a benign set (no trigger words). By comparing cosine and L2 similarities to the target embedding and applying a Kolmogorov–Smirnov test, the provider can detect whether the target service contains the watermark.

Experiments on AG News, MIND, SST‑2, and Enron Spam demonstrate that Embedding Marker achieves high detection accuracy while preserving downstream task performance. Visualizations via PCA show the watermark’s covert nature, making it hard to distinguish from normal embeddings. The talk concludes with an invitation for discussion.</sample>
    <sample id="95">The first author of the PaLM paper is **Zhenghao Liu**.</sample>
    <sample id="97">The speaker lists **three** distinct problems with current SimulST models.</sample>
    <sample id="98">**Answer:**  
Balance the training data so that each political/social group is adequately represented, filter or de‑emphasize extremist or overtly biased content, and augment the corpus with counter‑factual or balanced examples. Combine this with fairness‑aware training objectives (e.g., adversarial debiasing, group‑fairness regularization) to keep the model from amplifying any single viewpoint.</sample>
    <sample id="100">The talk introduces PromptRank, a data‑efficient method for multi‑hop question answering (QA) that reduces the need for large annotated training sets. Multi‑hop QA requires chaining together several documents to answer a question; traditional retrievers learn to predict these chains from thousands of labeled examples. PromptRank instead starts with an unsupervised retrieval of candidate chains using TF‑IDF and hyperlink traversal. These chains are then reranked by a few‑shot language‑model (LM) reranker that scores each chain by the likelihood of the question given a specially constructed prompt.

The prompt embeds the chain’s documents with indicator tokens and includes an instruction like “Read the previous documents and ask a question,” which encourages the LM to reason over the chain. The team explores instruction search (generating many candidate instructions and selecting the best), instruction sampling (aggregating scores across multiple instructions), and temperature scaling of LM logits. Experiments on HotpotQA show PromptRank outperforms fully supervised baselines such as DrKit and rivals dense retrievers, even with only 128 training examples. Ablation confirms each component’s importance. When combined with an ELECTRA‑Large reader, PromptRank’s downstream QA accuracy is only about four exact‑match points lower than top multi‑hop dense retrievers (MDR). Overall, the work demonstrates that LM‑based scoring of chain likelihood is a powerful, low‑resource approach to multi‑hop retrieval.</sample>
    <sample id="101">PaLM’s translations are **highly fluent—essentially on par with state‑of‑the‑art MT systems**. Human MQM evaluation shows its “Style/Awkward” scores are lower than those of specialized models, indicating that PaLM produces smooth, natural output even though it can still miss content (omission errors).</sample>
    <sample id="102">**Key properties a watermarking method should have**

1. **Applicability to embedding‑as‑a‑service** – the watermark must be able to be injected and checked via the API that serves embeddings.  
2. **Utility preservation** – it should not noticeably degrade the quality of the embeddings for downstream NLP tasks.  
3. **Covert / stealthy** – the watermark should be hard for an attacker to detect or remove from the model.  
4. **Transferability** – when a model is extracted or fine‑tuned, the watermark should still survive and be detectable in the stealer’s service.  

(Additional desirable traits: low computational overhead, robust against common attacks, and clear, reproducible detection metrics.)</sample>
    <sample id="103">The English TED talks were translated into the following 14 languages:

1. Arabic  
2. Chinese (Simplified)  
3. French  
4. German  
5. Hindi  
6. Indonesian  
7. Italian  
8. Japanese  
9. Korean  
10. Portuguese  
11. Russian  
12. Spanish  
13. Turkish  
14. Vietnamese</sample>
    <sample id="104">The study re‑annotated **200 instances per dataset**.</sample>
    <sample id="105">The paper uses **cosine similarity** and **L2 (Euclidean) similarity** to compute the differences between the benign and back‑door embeddings. These differences are expressed as Δ‑cosine and Δ‑L2, and are further complemented by a Kolmogorov‑Smirnov test for statistical validation.</sample>
    <sample id="106">The speaker introduces **QUEST**, a new retrieval dataset focused on *entity‑seeking queries with implicit set constraints*. To motivate the task, two everyday scenarios are presented: Jane, a zoologist in Costa Rica, wants the name of an unknown red reptile under 12 inches, and Austin, a reader, seeks historical‑fiction novels set in France. Both examples require combining multiple constraints (e.g., intersection, complement) to form a single query.

QUEST contains over 3,000 such queries. The authors generate them by performing set operations on Wikipedia category names across four domains (films, books, plants, animals). Human annotators then paraphrase the templatic queries to natural language, verify fluency, and validate answer entities. Annotators also mark *attributable spans* in source documents that support each query constraint, ensuring evidence can come from distinct parts of a document.

For evaluation, systems must retrieve multi‑answer sets from a large corpus, with evidence spanning multiple document sections. Baselines include sparse and dense retrievers plus a T5 reranker on the top 100 candidates. Results show low recall (MRecall@100) and overall F1, highlighting the difficulty of the task. Queries involving set intersection and set difference perform worst. The speaker invites researchers to use QUEST to improve retrieval for selective information needs and encourages attendance at an ACL presentation.</sample>
    <sample id="107">**Multilingual encoder‑based models were employed in two main ways:**

1. **Encoder‑Pointer Decoder (Encoder‑PTR)** – A multilingual pre‑trained encoder (e.g., XLM‑R or mBERT) feeds the input query into a pointer‑based decoder that selects tokens from the source to form the semantic representation (SQL, Lambda, FunQL, etc.). These models were trained and evaluated in both monolingual and multilingual settings, and also used for zero‑shot/few‑shot cross‑lingual transfer.

2. **Encoder‑Decoder (Encoder‑Decoder)** – Full multilingual encoder‑decoder models (e.g., mBART, mT5) were trained end‑to‑end on the same datasets. They directly generate the target meaning representation from the encoded input. These models achieved the best overall performance across the nine datasets and were also tested under the same multilingual, monolingual, and cross‑lingual transfer scenarios.</sample>
    <sample id="108">In this ACL 2023 talk, the authors investigate how language models (LMs) judge acceptability of sentences when presented with varying contextual lengths. They revisit the minimal pair paradigm (MPP), which traditionally evaluates whether a model assigns higher probability to a grammatically correct sentence versus an incorrect one. Current MPP pipelines only test short, isolated sentences, yet modern LMs feature large context windows. To probe robustness, the authors generate longer contexts by prefixing acceptable or unacceptable sentences from the same or different datasets (BLiMP, SyntaxGym, CrowS‑pairs) to the target pair. They also test mismatched contexts and unrelated Wikipedia snippets. Results show that arbitrary, unrelated prefixes leave MPP judgments stable even up to 1024 tokens. However, when prefixes come from the same grammatical phenomenon, MPP scores shift dramatically—accepting prefixes raise scores and unaccepting ones lower them—an effect that grows with context length. Perturbing the prefixes with noise does not change this pattern, indicating that LMs rely on latent syntactic and semantic features shared across sentences. The key takeaway is that MPP evaluation, as currently practiced, may not fully capture an LM’s abstract knowledge across long contexts, highlighting the need for more robust assessment methods.</sample>
    <sample id="109">**Abstract**  
Instruction tuning enables pretrained language models to generalize to unseen tasks in a zero‑shot setting, yet current instruction corpora are limited to existing benchmarks and require costly human annotation. We propose **Unnatural Instructions**, a large dataset of natural‑language instructions paired with inputs and outputs that is generated entirely automatically. Starting from a small seed of manually curated examples from the Super‑Natural Instructions corpus, we prompt a GPT‑3 variant to produce a new instruction and input. A second prompt then asks the model to generate the corresponding output. To increase linguistic diversity, we further prompt the model to paraphrase each instruction using two example paraphrases. This pipeline yields 64 k unique instruction–input–output triples, and roughly 240 k paraphrased variants. Analysis shows that over half of the generated examples are correct, many incorrect ones still contain useful information, and the dataset exhibits high creativity and task diversity beyond classical NLP benchmarks (e.g., evaluating scientific experiment design or inventing new words). Fine‑tuning an 11 B‑parameter T5 model on Unnatural Instructions outperforms the baseline T5 model trained on Super‑Natural Instructions across several benchmarks (Super‑Natural Instructions, T0, BIG‑Bench Hard, LMentry), and surpasses T0++ and Tk‑instruct when the generation cost is amortized. The results demonstrate that language‑model‑generated data can efficiently produce diverse, high‑quality instruction sets, offering a scalable alternative to human‑annotated corpora.</sample>
    <sample id="111">The authors build a trigger set by first collecting a large, general‑text corpus. They count how often each word appears in that corpus, then pick words whose frequencies lie in a “moderate” range—i.e., not among the most frequent (stop‑words, very common terms) and not among the rarest. In practice this means selecting words whose occurrence counts fall between two chosen percentiles (e.g., between the 40th and 70th percentile), yielding a set of words with intermediate frequency.</sample>
    <sample id="114">**Summary**

The talk presents the ACL 2023 paper “Finding the Pillars of Strength for Multi‑Head Attention” from NTU Singapore. The authors highlight that large language models (LLMs) are highly parameter‑heavy, slow to train, and token‑hungry, making them difficult to deploy on modest hardware. Multi‑head attention (MHA) is a key source of redundancy, as many heads can be pruned without harming performance. Existing pruning strategies fall into three categories: (1) homogenization, which forces heads to be similar but hurts accuracy; (2) diversification, which encourages dissimilar heads but lacks compression; and (3) score‑based pruning, which still leaves much redundancy.

The authors propose a **Grouped Head Attention (GHT)** framework that first **group‑constrained trains** heads into a few clusters, encouraging intra‑group similarity and inter‑group separation through a dual loss (homogenization + diversification). After training, a **Voting‑to‑Stay (VTS)** algorithm selects one representative head per group by collecting votes from training batches (“voters”) and pruning low‑scoring heads. This two‑step process yields a dramatically smaller MHA.

Empirically, GHT and its pruned variant (GHT‑PS) outperform SOTA baselines on machine translation, language modeling, and abstractive summarization, achieving BLEU and ROUGE gains of 3.8–7 % while compressing 16.9–32.1 % of parameters. In a “LITE” variant, they prune 90 % of parameters, speed up inference by 62 % and cut FLOPs by 80 %. Future work will explore task‑specific automatic pruning guided by the Lottery Ticket Hypothesis.</sample>
    <sample id="115">The method works on fixed‑size speech chunks of **λ (lambda) frames**.  In practice, it evaluates the cross‑attention of each generated word against the last λ frames of the incoming audio; when the attention weight on those last λ frames is low enough (below a chosen threshold α), the word is emitted.  Thus the segment size that drives the decision‑making in EDAtt is a chunk of λ speech frames.</sample>
    <sample id="116">The entity‑specific fact that ties the pronoun to Servin is: **“Servin is a judge.”**</sample>
    <sample id="117">The experiments showed that **the quality of the examples matters far more than how similar they are to the source sentence**.</sample>
    <sample id="118">**Key Points (≈200 words)**  

The paper tackles *code‑switching*, the mixing of two languages (e.g., English–Hindi) in a single sentence. Current multilingual models (mBERT, XLM‑R) underperform on tasks involving code‑switching such as sentiment analysis and QA.  

The authors propose new masked‑language‑model (MLM) objectives tailored to code‑switching.  
- **SwitchMLM** masks only *switch‑points* (pairs of tokens that cross a language boundary). This requires language‑identification (LID) tags, which may not be available.  
- **FrequencyMLM** is a surrogate that infers LID tags from monolingual corpora by comparing negative log‑likelihoods.  

To embed switch‑point knowledge deeper into the network, they add **residual connections** from intermediate layers (which encode more switch‑point information) to the final layer. An **auxiliary LID loss** forces the chosen intermediate layer to learn language identity, further enriching switch‑point cues.  

Empirically, the combined method (Switch/FrequencyMLM + ResBERT + auxiliary loss) outperforms baselines on sentiment analysis across multiple language pairs.  

**Probing experiments** confirm the approach:  
- *Linear probing* shows layer 9 of standard BERT contains more switch‑point information than layer 12.  
- *Conditional probing* (comparing to a zero baseline) demonstrates that representations trained with SwitchMLM carry more switch‑point signal than standard MLM.  

Thus, the work introduces a code‑switch‑aware MLM objective, architectural tweaks, and auxiliary supervision that together increase switch‑point information in transformer representations and improve downstream code‑switched NLP tasks.</sample>
    <sample id="119">The extended experiments focus on the same family of models that were evaluated in the main study: **RoBERTa, BART (and its variants), and the GPT‑series (including GPT‑4)**. These models are further pretrained on partisan corpora to observe how their political leanings shift.</sample>
    <sample id="120">The EDAtt approach uses the cross‑attention scores from the **final encoder‑decoder layer only**. It does not combine or average the attention maps from multiple layers – the decision to emit a word is based solely on the attention distribution produced by the last cross‑attention module.</sample>
    <sample id="121">**Examples of direct inference (direct references):**

- Naming the entity explicitly: “**Easy on Me**” or “**I Gotta Feeling**”.
- Referring to its position in the list: “**the first one**” or “**the second one**”.
- Using a clear ordinal or label that uniquely identifies the choice.</sample>
    <sample id="122">**Affiliations (as mentioned in the excerpt)**  
- **Siyu Yuan** – Fudan University  

The excerpt does not provide the institutional affiliations of the other authors, so only Siyu Yuan’s affiliation is known from the information given.</sample>
    <sample id="123">Ying and Zhiyang introduce **MultiInstruct**, the first large‑scale benchmark for instruction tuning of multimodal pre‑trained models. The dataset contains 62 diverse tasks spanning 10 broad categories, derived from 21 open‑source multimodal datasets. Each task is paired with five expert‑written natural‑language instructions, enabling a unified sequence‑to‑sequence formulation that encodes text, images, bounding boxes, and instructions in a single token space.  

They employ **OFA**—a unified multimodal model with a shared vocabulary for language and vision—as the base. For training, 53 tasks (9 groups) are sampled at 10,000 instances per task, mixing all tasks and randomly selecting one of the five instructions per instance. Testing reserves a full common‑sense reasoning group, five additional tasks from VQ and Miscellaneous groups, and 20 unseen NLP tasks. Evaluation metrics include accuracy for classification, ROUGE‑L for generation, and a novel **sensitivity** metric that measures consistency across instruction variations.  

Results show that instruction tuning markedly boosts OFA’s zero‑shot performance on seen multimodal tasks and improves sensitivity. Transfer learning from natural‑language instruction datasets further enhances performance and reduces sensitivity. Experiments comparing one versus five instruction templates confirm that more diverse instructions improve overall accuracy and stability.  

The authors plan to expand the dataset with ~150 additional vision‑language tasks and will release the data and models. This work demonstrates that multimodal instruction tuning can substantially improve generalisation to unseen tasks and offers a new metric to assess model robustness across instruction wording.</sample>
    <sample id="124">In this talk, Tan Qingyu presents a comprehensive study of temporal reasoning in large language models (LLMs). He first decomposes temporal reasoning into three levels: (1) time‑to‑time (e.g., “What year follows 2010?”), (2) time‑to‑event (e.g., “Which club did Messi play for in 2010?”), and (3) event‑to‑event (e.g., “Which club did Messi join after FC Barcelona?”). Prior work has focused mainly on level 2, so the authors built the **TempReason** benchmark covering all three levels with long temporal spans.

TempReason includes three QA settings: **Closed‑Book** (only the question is given), **Open‑Book** (a Wikipedia article on the subject is supplied), and a novel **Reasoning‑QA** where all relevant temporal facts are provided and the model must reason over them. Experiments show that even ChatGPT struggles with month‑level predictions and exhibits large performance gaps across different time periods, indicating temporal biases.

To address this, the authors propose **TempT5**, a T5‑based model trained with (1) a pre‑training objective that reconstructs masked temporal and entity spans, and (2) time‑sensitive reinforcement learning that rewards correct temporal answers and penalizes temporally incorrect ones. TempT5 outperforms baseline models (FLAN‑T5‑L, ChatGPT, T5‑SFT) on all TempReason splits, though some fluctuation remains due to training data imbalance. The study highlights both the shortcomings of current LLMs in temporal reasoning and a promising training paradigm to mitigate these biases.</sample>
    <sample id="125">Based on the text provided, the only author explicitly named is **Yanis Labrak**. No other authors are listed or mentioned.</sample>
    <sample id="126">Yes. The **Translate‑Test** setting—where a machine‑translation service (Google Translate API) first converts the query into the language of a monolingual semantic‑parsing model—was included as one of the baseline evaluation protocols.</sample>
    <sample id="127">**Summary**

Namgyu Ho and colleagues present a method to transfer the complex reasoning abilities of very large language models (LLMs) to much smaller, more deployable models. They build on *chain‑of‑thought* (CoT) prompting, which enables large models (e.g., GPT‑3, PaLM) to solve multi‑step tasks by producing step‑by‑step reasoning. However, CoT only works reliably on these huge models, making it impractical for many settings. 

To address this, the authors use a large LLM as a “teacher” that generates CoT solutions for benchmark questions. When the teacher’s final answer is correct, the entire reasoning chain is formatted as a training example for a smaller “student” model. The student is fine‑tuned to produce its own CoT answers. Crucially, they introduce **Diverse Reasoning**: instead of a single deterministic CoT, the teacher generates multiple diverse reasoning paths by sampling at higher temperatures. These varied explanations enrich the training data and improve student performance.

Evaluated on 12 reasoning benchmarks (including math, data‑understanding, and logical tasks), the distilled 0.3‑B student outperforms prompt‑based baselines and vanilla fine‑tuning on most tasks, achieving up to 55 % accuracy on Multi‑Arithmatic. The authors also show that scaling the number of diverse samples, the teacher’s size, and the student’s capacity yields further gains, highlighting a clear trade‑off between development‑time cost and inference‑time efficiency. The paper includes code, data, and extensive analysis, making the approach reproducible and extensible to other emergent LLM abilities.</sample>
    <sample id="128">Akshatha and co‑author Martin present **“The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources,”** a collaboration between McGill, Mila, and Microsoft Research. The work addresses a key gap in natural language understanding (NLU): many tasks require both *pre‑trained* knowledge (learned during model pretraining) and *inference‑time* knowledge (provided as context). Coreference resolution, for example, often needs both entity‑specific facts (e.g., “Servin is a judge”) and background facts (e.g., “judges decide cases in courts”). 

KITMUS is a diagnostic coreference test suite that systematically varies where each type of knowledge is available. Three settings are defined:  
1. **Background‑Pretrain** – background knowledge lives in the model’s parameters; only entity facts appear in the input.  
2. **Background‑Both** – both background and entity facts are supplied at inference time.  
3. **Background‑Inference** – neither knowledge type is pre‑trained; all facts must come from the input (e.g., fictional occupations like “mirituer”).  

Human studies and baseline coreference models (C2F, BERT4Coref) show that, without KITMUS training, models rely on surface cues and perform poorly. Training on KITMUS improves performance markedly, but even the best models struggle to integrate background facts that appear solely in the input. The results suggest that task‑specific training is necessary for robust multi‑source knowledge integration, yet current models still cannot reliably use inference‑time background knowledge alone. The dataset and code are publicly available on GitHub for further research.</sample>
    <sample id="129">They used **black women** as a concrete example of a marked group.</sample>
    <sample id="130">The presentation indicates that **non‑transformer architectures**—such as the classic BiLSTM‑CRF and other CNN/feature‑based taggers—do **not generalize well** to the newer CoNLL++ data. In contrast, transformer‑based models (e.g., BERT, RoBERTa, GPT‑style variants) consistently show better cross‑dataset performance.</sample>
    <sample id="131">The talk does not name any specific datasets; it only refers generically to “clean test sets.”</sample>
    <sample id="132">The paper has **two authors**: Akshatha and her co‑author Martin.</sample>
    <sample id="133">Yes—the author works with multiple modalities, combining text with images (and related visual data) in the MultiInstruct benchmark.</sample>
    <sample id="135">The video introduces **ABC‑Eval**, a new dimensional evaluation framework for conversational AI developed by the Emory NLP Lab and Amazon Alexa AI. Unlike traditional human judgments that use pairwise or Likert‑scale ratings to judge overall dialogue quality, ABC‑Eval focuses on **specific behaviors** that affect chat quality. Annotators label each model turn for whether it is irrelevant, contradictory, hallucinated, violates common sense, or shows empathy. This explicit behavior‑level annotation aims to reduce subjectivity and capture fine‑grained weaknesses.

To assess its effectiveness, the authors evaluated four state‑of‑the‑art chat models on 100 human‑bot conversations per model using ABC‑Eval, and compared the results to three existing methods: turn‑level Likert ratings, dialogue‑level Likert ratings, and pairwise dialogue comparisons across eight common quality aspects. Inter‑annotator agreement for ABC‑Eval was higher than for the other methods. Linear regression showed that proportions of contradictory turns explained 5–10 % of overall conversation quality, whereas Likert scores explained less than 4 %. A stepwise regression revealed that the full set of ABC‑Eval metrics accounted for over 25 % of quality variance, whereas the combination of all Likert metrics explained far less and many lacked unique information.

The study also quantified current challenges: about 20 % of responses violate common sense, 15 % are irrelevant, and 10 % contain self or partner contradictions. The authors argue that ABC‑Eval provides a more reliable, informative, and distinct set of metrics, enabling higher‑resolution comparisons of conversational models and guiding future improvements.</sample>
    <sample id="136">**Summary**

Jasivan and supervisor Nafise present *FERMAT*, a new benchmark for numerical reasoning in language models. Current tasks (e.g., fact‑checking from tables) reveal that larger models perform better, yet 3‑billion‑parameter models—more commonly used—struggle. Accuracy and F1 scores on existing datasets fail to expose specific mathematical weaknesses.

FERMAT addresses this gap by offering a flexible, arithmetic‑focused evaluation set. It draws problems from CommonCore and Illinois, then systematically varies number representations (small/large integers, decimals) and operation types (addition, subtraction, combinations). The dataset tests three dimensions: number understanding, mathematical operations, and training dependency.

Baseline zero‑shot results show most models perform poorly across all dimensions. Fine‑tuning with 200k teacher‑generated templates (with number placeholders) improves scores on the original set and across the new dimensions. Analysis of training dependency reveals that even when exact expressions appear in training, accuracy remains below 50%, indicating models do not simply memorize numbers; linguistic cues also matter. Further experiments varying template diversity (adding GSM8K and AQUA templates) demonstrate that both language and mathematical variety boost performance.

The authors conclude that standard benchmarks are unrepresentative, single scores are insufficient, and that FERMAT provides a more informative alternative. They emphasize the importance of diverse training templates and highlight number encoding/tokenization as areas for future work.</sample>
    <sample id="137">**Tell2Design: Language‑Guided Floor Plan Generation** introduces a new task of creating 2‑D floor plans directly from natural‑language descriptions. The authors collected the Tell2Design dataset, comprising 5,051 human‑written instruction sets and ~76,000 template‑generated ones, each describing room semantics, geometry, and topology. Instructions average 200 words, spanning more than ten sentences, and are sourced from Amazon Mechanical Turk to ensure realism. The task is challenging because (1) it requires strict adherence to multiple constraints unlike free‑form artistic generation, (2) it demands global understanding of a document‑level, fuzzy, and entangled text, and (3) it must tolerate ambiguous or incomplete user input.

To address this, the authors cast floor‑plan generation as a sequence‑to‑sequence problem. A transformer encoder‑decoder (initialized with pre‑trained T5) maps the instruction text to a sequence of room bounding boxes (center coordinates, height, width). The model is trained with a language‑modeling objective and evaluated using IoU metrics. On the test split (no annotator overlap), the T2D model achieves a Micro IoU of 54 and Macro IoU of 53, outperforming text‑conditional image generation baselines by a large margin. Experiments reveal a language‑distribution gap between artificial and human instructions; pre‑training on artificial data improves performance on human data by ~10 IoU points. The paper concludes by presenting a foundation for future research on language‑guided design generation, starting with the floor‑plan domain.</sample>
    <sample id="138">The authors point out that **integrating knowledge from multiple sources—pre‑trained parameters and inference‑time inputs—is an understudied area in natural language understanding.**</sample>
    <sample id="139">The speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes. The CoScript dataset was vetted by crowd‑sourced workers who inspected the validation and test sets, identified any erroneous samples, and revised them to ensure overall quality.</sample>
    <sample id="141">Existing resources for evaluating context‑dependent translation are limited in several ways:

* **Scope of phenomena** – They cover only a handful of discourse phenomena (e.g., pronouns, lexical cohesion) and ignore many others such as ellipsis, formality, or verb‑form disambiguation.  
* **Language coverage** – They are usually built for a few high‑resource languages, leaving most language pairs unrepresented.  
* **Manual curation** – The datasets rely on domain knowledge and human annotation, which is time‑consuming, costly, and hard to scale.  
* **Corpus‑level metrics** – Standard metrics like BLEU operate on entire corpora, masking the relatively small fraction of context‑sensitive tokens that actually need special handling.  

Because of these constraints, current benchmarks cannot fully capture or evaluate how well machine‑translation systems handle the wide variety of context‑dependent situations that arise in real‑world translation.</sample>
    <sample id="143">The approach is compared against the two most common SimulST policies – **Wait‑k** and **Local Agreement** – as well as a **state‑of‑the‑art simultaneous pre‑translation architecture** that is specifically designed for SimulST.</sample>
    <sample id="144">The authors are based at the University of Nantes and the Nantes University Hospital (CHU Nantes) in France.</sample>
    <sample id="145">The speaker is Jenny.</sample>
    <sample id="146">**Summary**

Dialogue summarization aims to produce concise, informative summaries of multi‑turn conversations. Although large pretrained language models generate fluent text, they frequently omit critical facts, leading to incomplete summaries. Yicheng’s paper quantifies this issue: across five domains and six models, roughly 70 % of generated summaries suffer from omissions, and omitted content is randomly distributed throughout dialogues, indicating that current models struggle to identify key information.

To enable systematic study, the authors introduce OLDS, a new dataset that annotates utterance‑level omissions in candidate summaries. OLDS is built from five existing dialogue‑summarization benchmarks; multiple abstractive models and decoding strategies generate diverse candidate summaries, which are automatically labeled for omissions and then verified by human evaluation.

Three baseline omission‑detection frameworks are evaluated—pair‑wise classification, sequence labeling, and a pointer network—using precision, recall, F1, and a word‑level recall metric (WR). The best F1 scores hover around 50 %, underscoring the difficulty of the task.

Finally, the authors demonstrate that incorporating detected omissions into a post‑editing model substantially improves summary quality, showing that omission detection is not only valuable for analysis but also for practical refinement of dialogue summaries.</sample>
    <sample id="147">Three authors are involved in the paper—in total: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="149">Yes – the CoNLL++ dataset is publicly released.  The authors host it on their project page (and a copy is also available on GitHub), so anyone can download the Reuters‑2020 text and the CoNLL‑2003 style annotations for research or reproducibility.</sample>
    <sample id="150">**MeetingQA: Extractive Question‑Answering on Meeting Transcripts**

Archiki presents *MeetingQA*, a new extractive QA dataset derived from real‑world meeting transcripts. The dataset addresses the gap in meeting‑NLP research, which has largely focused on summarization and action‑item extraction, by capturing the QA dynamics inherent to meetings. Using 100 h of manually transcribed AMI‑corpus meetings, the team selects longer, open‑ended questions (filtered by punctuation) and annotates answer spans at the sentence level. The resulting corpus contains 7.7 K questions (Train/Dev/Test split), with 30 % unanswerable, 40 % multi‑span, and 48 % multi‑speaker answers. Questions are typically yes/no or opinion‑seeking, with 20 % rhetorical and 70 % of multi‑speaker answers involving disagreement. Average question and answer lengths are 12 and 35 words, respectively. Human F1 on the test set is 84.6 %.

Modeling approaches include short‑context retrieval (e.g., RoBERTa) versus long‑context encoders (Longformer), single‑span (start‑end) versus multi‑span token‑classification models, and data augmentation via automatically annotated MediaSum interview questions. Fine‑tuned models achieve ~25 F1 points below human performance; short‑context models slightly outperform long‑context ones, and multi‑span models are comparable to single‑span. Zero‑shot performance lags by ~50 F1 points, but silver data and large instruction‑tuned models (FLAN‑T5) help close the gap. Error analysis shows difficulty with rhetorical questions and speaker attribution, particularly in zero‑shot settings. The dataset remains a challenging benchmark for QA in meeting contexts.</sample>
    <sample id="152">Frederick Riemenschneider presents the “Exploring Large Language Models for Classical Philology” project, which aims to advance NLP for Ancient Greek and Latin. Existing models (Latin‑BERT, Ancient‑Greek‑BERT variants) are BERT‑based, monolingual, and lack robust evaluation. To address this, the team built four new models: two monolinguals—GreBERTa (RoBERTa for Greek) and GreTa (T5‑based encoder‑decoder for Greek)—and two multilinguals—PhilBERTa and PhilTa—trained on Greek, Latin, and English. Pre‑training data were expanded by mining the Internet Archive; Greek texts were identified via OCR artifacts (e.g., “γάρ” mis‑transcribed as “yap”) and re‑OCR’d with Greek settings, yielding a high‑quality corpus. Additional Latin data came from Corpus Corporum, and English was sourced from antiquity‑related texts. Models were benchmarked on Universal Dependencies (Greek) and EvaLatina 2022 (Latin) across POS tagging, dependency parsing, and lemmatization. Results show significant improvements over state‑of‑the‑art, especially in lemmatization (≈5 % gain). Analysis of the T5 encoder revealed initially poor performance that improves with training. Multilingual models did not outperform monolingual ones on semantic or world‑knowledge tasks, suggesting limited cross‑lingual benefit. Overall, the work delivers robust, multilingual, encoder‑ and encoder‑decoder models with a newly curated Greek corpus, providing strong baselines for classical philology NLP.</sample>
    <sample id="153">**Summary**

Ninareh Mehrabi presented the paper *“Resolving Ambiguities in Text‑to‑Image Generative Models.”* The work tackles the problem that many user prompts for text‑to‑image systems are inherently ambiguous—for example, “The girl enters the room with flowers” could mean the flowers are with the girl, inside the room, or both. To study this, the authors curated a benchmark dataset based on the existing LAVA corpus, categorizing prompts by their ambiguity type.

The proposed solution has two disambiguation stages. First, a language model (LM) either (1) generates clarifying questions for the user or (2) proposes multiple concrete visual interpretations. The user’s answer or choice is concatenated with the original prompt to produce a disambiguated prompt. These prompts are then fed to a text‑to‑image model to generate images.

For evaluation, the authors developed an automatic framework that uses a Visual Question Answering (VQA) model. The VQA model receives the generated image and a question phrased from the user’s intended meaning; a “yes” answer indicates the image satisfies the intention. Human evaluations confirm that the VQA scores align well with subjective judgments.

Key findings include: (1) disambiguation effectiveness varies across ambiguity types; (2) overall, disambiguation improves faithfulness of generated images; and (3) the VQA‑based evaluation reliably reflects human assessment. The paper demonstrates both a practical approach to clarifying user intent and a scalable method to evaluate text‑to‑image models’ faithfulness.</sample>
    <sample id="154">The authors are affiliated with the University of Trento and the Fondazione Bruno Kessler (both in Trento, Italy).</sample>
    <sample id="155">The speaker’s name is **Javad Hosseini**.</sample>
    <sample id="157">**Dialogue Summarization with Static‑Dynamic Structure Fusion Graph (SDDS)**  
Shen Gao and colleagues at Shandong University present SDDS, a new framework for summarizing multi‑party dialogues. The task requires distilling key information from semi‑structured, multi‑speaker conversations. Existing approaches rely on pre‑computed static graphs (e.g., discourse parsing, dialogue state tracking). These suffer from two main issues: (1) they depend on external linguistic tools whose errors propagate, and (2) the fixed graph cannot adapt to the downstream summarization objective.

SDDS addresses these limitations with four core components. First, an **utterance encoder** transforms each dialogue turn into a vector. Second, four heuristic methods generate a **static graph**:  
1. *Discourse Parsing Graph* (dependency‑based relations).  
2. *Keyword Co‑occurrence* (shared key terms).  
3. *Speaker Interaction Graph* (frequency of speakers in a sliding window).  
4. *Position Graph* (relative utterance distances).  
The adjacency matrices from these channels are fused via a 1×1 convolution to form a unified static adjacency matrix.

Third, a **Dynamic Graph Module** learns utterance relations directly from the learned embeddings using multi‑head attention, avoiding any external heuristics. The dynamic adjacency matrix and the static one are fused into a unified graph \(G^{u}\).

Finally, a **summary generator**—a pre‑trained language model—incorporates this fused graph through a dual cross‑attention mechanism, enabling the model to attend to both static and dynamic dialogue structures while generating concise summaries.

The authors released the code and dataset on GitHub, offering a practical resource for further research.</sample>
    <sample id="158">In this presentation, Qipeng Guo from AWS introduces a new “Dual Cache” approach for neural coreference resolution in long documents. Coreference resolution requires clustering all mentions of the same entity, a task that traditionally enumerates every mention pair, leading to quadratic time and memory costs. Recent cache‑based models reduce this to linear complexity by keeping a fixed‑size cache of entities; however, they use a single eviction policy (usually LRU). In long texts, topics shift frequently, so an LRU cache frequently evicts relevant entities, causing many cache misses—especially for high‑frequency entities that appear throughout the document.

The Dual Cache method separates the cache into two components. A local cache stores recent entities with an LRU policy, while a global cache holds long‑range, frequently mentioned entities and uses an LFU policy to evict the least frequently used items. As the model scans text, each new mention is classified as belonging to a cached entity or a new one, updated in the appropriate cache, and evicted if necessary. Experiments on four public datasets (LitBank, OntoNotes, WikiCoref, and a 30,000‑word annotated book) show that Dual Cache outperforms single‑cache baselines and even unbounded‑memory models when training data is available. Even without training data, it remains competitive while being faster. The results demonstrate a higher performance/efficiency ratio, confirming that separating local and global entities via Dual Cache effectively reduces cache misses and improves coreference accuracy in long documents.</sample>
    <sample id="160">It maps each input token to an **unordered multiset of output tokens**.</sample>
    <sample id="161">The CoScript dataset contains **55,000** scripts (one script per specific goal).</sample>
    <sample id="163">The best automatic alignment method for DEPLAIN is **MASSalign**.</sample>
    <sample id="164">**Benefit of weakly supervised learning**  
Weakly supervised learning lets you train models on large volumes of data without the expensive effort of manual annotation. By using inexpensive sources such as heuristic rules, knowledge bases, or low‑quality crowdsourcing, you can obtain many noisy labels quickly and cheaply, enabling you to exploit more data than would be feasible with fully supervised labeling while still achieving reasonable generalization.</sample>
    <sample id="165">The talk introduces **LiPoR** (Likelihood Learning with Posterior Regularization), an unsupervised framework for abductive commonsense reasoning. Abductive reasoning seeks a plausible explanation that bridges a given context (X) to an outcome (Y). In the presented example, “Emily was stuck in traffic” (X) and “Emily made it to her flight” (Y) are connected by explanations such as “Her flight was delayed” versus “Her flight left on time.” The speaker notes that supervised approaches suffer from noisy, subjective annotations, with crowd workers disagreeing on 60% of explanations.

LiPoR treats explanations (Z) as latent variables and maximizes the marginal likelihood \(P(Y|X)\) by summing over all candidate explanations. However, this objective alone does not favor plausible explanations. To enforce mutual exclusivity—a key property of explanations—LiPoR adds a posterior regularizer \(\Omega\). This regularizer compares the entropy of the posterior \(P(Z|X,Y)\) to \(\log M\), where \(M\) is the target number of plausible explanations. If entropy exceeds \(\log M\), the regularizer encourages the model to concentrate probability mass on a smaller, mutually exclusive set of explanations.

Experiments on the AlphaNLI dataset show that LiPoR outperforms zero‑shot baselines and the prior best unsupervised method, achieving a 4‑point increase in accuracy over a strong GPT‑3 zero‑shot baseline. The paper is available at tinyurl.com/zhao‑lipor.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology presents **NDCR**—a neural divide‑and‑conquer framework for retrieving images from linguistically complex text. The task is hard because images are visually similar and the queries are long and detailed, causing conventional vision‑language models to falter. Inspired by the **Divide‑and‑Conquer** strategy and **Dual‑Process Theory**, NDCR combines two complementary reasoning systems: **System 1** (analogical, fast) and **System 2** (logical, slow).  

The pipeline begins with a **Proposition Generator** that decomposes a complex query into simple propositions and uses a BART decoder to produce corresponding sentences. The **Visual‑Linguistic Interactor** (System 1) matches each proposition to candidate images, yielding similarity scores and intermediate reasoning states. Next, **System 2**—a Neural‑Symbolic Reasoner—integrates these states using a **Negation Executor** (to negate positive propositions) and a **Conjunction Operation** (to fuse positive and negated reasoning). Finally, the outputs from System 1 and System 2 are combined to produce the final retrieval decision.  

Experiments on a benchmark dataset show NDCR outperforms baseline vision‑language models, and ablation studies confirm each module’s contribution. Sample cases illustrate the model’s ability to expose intermediate inference states, demonstrating transparent, interoperable reasoning. Yunxin concludes that neural‑symbolic approaches and divide‑and‑conquer reasoning hold promise for improving compositional reasoning in large language models.</sample>
    <sample id="167">All 750 documents of the **DEPLAIN‑web** subcorpus were processed twice:  

- **Manual alignment** – every document pair was aligned by hand, producing the gold‑standard sentence pairs.  
- **Automatic alignment** – the same set of 750 documents was also aligned using the automated methods (e.g., MASSalign).

Thus the allocation is 750 documents for manual alignment and 750 documents for automatic alignment, yielding a total of 30 450 sentence pairs.</sample>
    <sample id="168">The CoNLL++ dataset was built by taking Reuters news articles from 2020 and annotating them with the same CoNLL‑2003 NER annotation guidelines.</sample>
    <sample id="169">The talk presents a systematic study of how large language models can be prompted for machine translation, focusing on PaLM—a 540‑billion‑parameter model trained on 780 billion tokens that topped many NLP benchmarks in 2022. The authors evaluate PaLM’s translation ability using state‑of‑the‑art MT practices: they avoid overlap between test data and the model’s training set, benchmark on WMT test sets, use neural MT metrics (BLEURT, etc.), and conduct expert human evaluations with the MQM framework.

A key finding is that the prompt design heavily influences performance. A simple one‑shot experiment with two different prompts for 1,000 sentences showed a BLEURT gain of over one point, occasionally up to 40 points. The authors therefore adopt a 5‑shot strategy that labels each source sentence with its language (e.g., “German: …”) and each target with “English: …”. With five examples, the exact wording of the prompt matters little; the examples themselves carry most of the weight. High‑quality translations used as examples outperform noisy training‑data examples, and curated dev‑set examples give better results than training‑set ones.

Despite PaLM’s strong performance—approaching commercial systems like Google Translate—it still lags behind specialized MT systems. Human evaluation shows PaLM’s fluency is comparable to state‑of‑the‑art outputs, but accuracy suffers, especially with omission errors. Nonetheless, PaLM’s style and awkwardness scores are lower, indicating more fluent translations. The talk concludes with recommendations for effective prompt selection in large‑model MT.</sample>
    <sample id="171">**Existing watermarking approaches for large‑language‑model (LLM) embeddings / embedding‑as‑a‑service (EaaS)**  

| Category | Typical strategy | Representative papers | Key limitation for EaaS |
|----------|------------------|-----------------------|------------------------|
| **1. Activation‑based (model‑level) watermarking** | Embed a secret pattern in the activations of a selected layer (e.g., by fine‑tuning on a small “watermark” dataset). | *Shokri et al., 2017* “Watermarking Deep Neural Networks”; *Gong et al., 2020* “Backdooring neural networks for watermarking”. | Requires full model access; does not directly affect the embedding output that is exposed by an EaaS API. |
| **2. Data‑based watermarking** | Add a set of specially crafted sentences (trigger set) that, when fed to the model, produce a distinctive embedding signature. | *Li et al., 2021* “Watermarking NLP models with trigger sentences”; *Zhou et al., 2022* “Dataset‑based watermarking for LLMs”. | Triggers are usually only detectable when the exact trigger set is known; transfer to a stolen model may degrade if the model has been fine‑tuned or pruned. |
| **3. Architecture/Weight‑based watermarking** | Modify a small fraction of weights or the network topology to encode a unique pattern. | *Erasmus et al., 2019* “Embedding watermarking via weight perturbations”; *Wang et al., 2022* “Model watermarking with weight signatures”. | The watermark is tied to the specific parameterization; it may vanish after extraction or compression, and it does not influence the embeddings that the service returns. |
| **4. Backdoor‑based watermarking** | Train the model to produce a target (e.g., a fixed “target embedding”) whenever a trigger sentence is processed. | *Zhang et al., 2023* “Backdoor watermarking for LLMs”; *Yi et al., 2024* (the paper you presented) “Embedding marker”. | Earlier backdoor works focused on classification or generation outputs, not on the embedding vectors supplied by an EaaS API; many lacked guarantees that the watermark would transfer to a stolen extraction of the embedding function. |

**Take‑away**  
- Prior methods either target the full model or its output logits, not the *embedding* that an EaaS provider exposes.  
- They also often assume that the victim will not remove the watermark or that the watermark will survive fine‑tuning/pruning.  
- The “Embedding marker” approach you described is the first backdoor‑based watermark that **directly targets the embedding vector**, keeps downstream utility, is covert, and remains transferable to a stolen model.</sample>
    <sample id="172">No. In our benchmark we found that large multilingual LLMs such as Codex and BLOOM do **not** yet match the performance of specialized encoder‑decoder or encoder‑pointer models (e.g., mT5, mBART, XLM‑R + PTR) on cross‑lingual semantic‑parsing tasks. They remain inadequate for CLSP and require further adaptation or task‑specific fine‑tuning to be competitive.</sample>
    <sample id="174">**Abstract**

The *ArgAnalysis35K* dataset is the largest corpus of its kind for argument quality analysis, comprising 35 000 argument–analysis pairs scored on a continuous 0–1 scale. Unlike prior corpora that rely on crowdsourced judgments, limited to a handful of motions, and lack explanatory depth, ArgAnalysis35K draws 85 % of its arguments from high‑quality sources such as expert and intermediate debaters, with the remainder from novice speakers, thereby ensuring superior argument quality. To capture thematic diversity, the dataset spans 24 carefully selected debate themes drawn from professional circuits, Hellomotions.com, and expert input, each enriched with multiple motions rather than a single pre‑chosen set. A novel *analysis* field is introduced for each argument, aggregating claims, premises, and logical links into a single explanatory unit that clarifies why an argument holds—a construct absent from existing NLP resources.

The dataset further incorporates instance‑based annotator reliability, allowing selective filtering of biased judgments per argument rather than discarding entire annotators, which preserves valuable subjective insights. Finally, a *relevance* model assigns a 0–1 score to each argument for every theme, reflecting its applicability across diverse debate contexts (e.g., governance, corporate policy, social movements). Together, these features—scale, quality, thematic breadth, explanatory depth, refined reliability, and relevance scoring—position ArgAnalysis35K as a comprehensive resource for advancing research in argument quality assessment and automated debate support.</sample>
    <sample id="175">The model treats the permutation as a **latent variable** that is not fixed during training.  
To handle the ambiguity:

1. **Continuous relaxation** – Instead of searching over all discrete permutations (an NP‑hard problem), the approach uses a differentiable relaxation (e.g., a doubly‑stochastic matrix) that approximates a permutation.  
2. **Learning by back‑propagation** – The relaxation is differentiable, so the model can back‑propagate gradients through the “soft” permutation to the parameters that generate it.  
3. **Inducing the best permutation** – During training the model evaluates many candidate permutations and selects the one with the highest score under the relaxed objective, effectively learning the linguistically plausible ordering.  
4. **No hard constraints** – Because the method is not tied to a fixed, tree‑based order, it can flexibly discover the correct ordering even when multiple permutations are consistent with the data.

Thus, the ambiguity is resolved by learning a continuous, learnable approximation of the permutation that can be optimized jointly with the rest of the network.</sample>
    <sample id="176">In this work, fairness of a downstream NLP model is defined by **how evenly the model’s performance is distributed across different social or political groups**.  
- The authors evaluate the model on tasks such as hate‑speech and fake‑news detection and measure accuracy, precision, recall, etc. separately for each demographic or political‑leaning subgroup (e.g., minority‑targeted vs. majority‑targeted hate speech, left‑ vs. right‑leaning news).  
- A fair model would show **minimal performance gaps** between these groups; large disparities (e.g., a model that is good at detecting hate against one group but poor against another) signal a fairness problem.</sample>
    <sample id="177">The speaker is **Yanis Labrak**.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar presents **SymbolicToM**, a plug‑and‑play technique designed to boost large language models (LLMs) on Theory of Mind (ToM) tasks, particularly false‑belief reasoning. Traditional ToM tests, such as the Sally‑Anne story, probe whether a model can infer what one character thinks about another’s mental state. Existing LLMs—including GPT‑3 and ChatGPT—perform poorly on these false‑belief questions, especially at higher order (second‑order) reasoning.

SymbolicToM introduces explicit graphical belief representations that capture nested mental states. For each character pair (p₁, p₂) up to a maximum ToM level *m*, the method constructs belief graphs (e.g., **BBob** for what Bob believes the world looks like, and **BBob,Alice** for what Bob believes Alice believes). These graphs are generated at inference time using off‑the‑shelf Natural Language Inference and OpenIE models. When a question is posed, the system retrieves the appropriate graph, recursively rewrites the query into a factual question over the graph, and then feeds the resulting sentences plus the question to an LLM to produce the answer.

Experiments on the ToMi benchmark show dramatic gains: over 60 accuracy points for GPT‑3‑Davinci, 67 for Macaw, and 51 for Flan‑T5‑XXL. Two new out‑of‑domain datasets (D₁–D₃) and a linguistically paraphrased version (ParaphrasedToMi) were created to test generalization. Supervised fine‑tuned models collapse to ~50% accuracy on these, whereas SymbolicToM still yields strong improvements, even enabling GPT‑4 to solve them completely (e.g., a 42‑point boost on D₁).

In sum, SymbolicToM is an inference‑time, symbolic‑graph approach that improves ToM reasoning in LLMs without overfitting, offering more interpretable reasoning and robust cross‑domain performance.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">**Abstract**  
We formalize *constrained language planning*, a setting in which a language model must generate step‑by‑step scripts that satisfy multi‑faceted constraints (e.g., “make a chocolate cake”) rather than only abstract goals. Using InstructGPT we automatically acquire 100 specific goals by extending abstract Wikipedia‑style activity templates with constraint categories from WikiHow, and evaluate the quality of generated scripts. Although semantic completeness is high, faithfulness to constraints is poor, with performance varying markedly across constraint types. To address this, we introduce an *over‑generate‑then‑filter* pipeline: each goal is over‑generated (K scripts) and a lightweight filter selects the most faithful script by comparing InstructGPT embeddings and rewarding keyword matches. This approach substantially boosts both semantic completeness and constraint adherence.  
Recognizing the cost of deploying large models, we construct a high‑quality dataset, **CoScript**, by distilling constrained scripts from the same LLM. CoScript contains 55 000 specific goals with corresponding scripts; a human‑reviewed validation and test split ensures reliability. The dataset exhibits diverse constraint coverage. Training a specialized T5 model on CoScript produces scripts that surpass most large language models in faithfulness, demonstrating that appropriately curated data can enable smaller models to excel at constrained planning. Our work introduces the constrained language planning task, presents an effective LLM‑based generation strategy, and provides CoScript as a resource to advance research in script generation under constraints.</sample>
    <sample id="182">**Tropicalism** in the paper refers to the stereotypical portrayal of Latina women as “exotic,” “vibrant,” and “curvaceous”—imageable, tropical‑like tropes that essentialize them as sexually alluring and fiery. It highlights how the model’s positive‑appearing descriptors actually reinforce a narrow, exoticized stereotype.</sample>
    <sample id="183">The authors had people actually write the descriptions. They recruited human subjects and gave them the same persona‑generation prompts used for the LLMs (e.g., “Imagine you are an Asian woman. Describe yourself.”). The responses produced by these participants served as the human‑written portrayals of each target group.</sample>
    <sample id="184">They measured context usage with **pointwise CXMI (P‑CXMI)**, an extension of the CXMI metric that quantifies how much the context \(C\) informs the target \(Y\) given the source \(X\).</sample>
    <sample id="185">**DrBERT vs. ChuBERT**

| Aspect | DrBERT | ChuBERT |
|--------|--------|---------|
| **Primary domain** | Biomedical (general medical knowledge) | Clinical (patient‑care records) |
| **Data source** | NACHOS – French medical text crawled from the web | Anonymized clinical notes from the Nantes University Hospital data warehouse |
| **Typical training size** | Up to 7 GB (also a 4 GB version) | 4 GB of clinical sentences (plus a mixed 4 GB + 4 GB variant) |
| **Use‑case focus** | Broad biomedical NLP tasks (NER, classification, QA, etc.) | Clinical‑specific tasks (e.g., clinical note processing) |

In short, **DrBERT** is a French biomedical language model trained on web‑crawled medical text, whereas **ChuBERT** is a French clinical model trained on hospital‑derived clinical notes.</sample>
    <sample id="187">Two authors.</sample>
    <sample id="188">**Iterative transfer learning** is an approach where a model is fine‑tuned **sequentially** on a series of related source tasks, one after another, instead of training on all tasks at once. In the paper, the authors first fine‑tune the model on the CE (expansion/comparison) task, then fine‑tune that updated model on the debate stance task, and finally on the target dissonance detection task. Each step uses the weights learned in the previous step, allowing the model to gradually acquire knowledge from each domain before tackling the rare‑class target. This iterative, multi‑stage transfer yields a stronger zero‑shot starting point for active learning.</sample>
    <sample id="189">The AltEntities Corpus is built to benchmark and improve how conversational systems and language models resolve **indirect referring expressions when a user must choose between two entities**. By providing natural, informal prompts and a range of indirect references across music, books, and recipes, the dataset allows researchers to test entity‑disambiguation abilities and to train models that can understand and act on subtle, context‑dependent selection cues.</sample>
    <sample id="190">**Short answer:**  
An attacker can recover (or at least approximate) the underlying parameters of an EaaS‑model by **query‑based model extraction**.  
They repeatedly send carefully crafted inputs to the embedding API, collect the returned vectors, and then solve for the model’s weight matrices (or a close substitute) using techniques such as interpolation, linear regression, or neural–network inversion. Because embeddings are deterministic and high‑dimensional, even a modest number of query‑embedding pairs can yield a faithful surrogate model that mimics the original system’s behavior.  

**Key steps in practice**

| Step | What the attacker does | Why it works |
|------|-----------------------|--------------|
| 1. **Query generation** | Construct a set of input samples (sentences, phrases, or tokens) that cover the vocabulary and syntax space. Often they use synthetic or random text, or a large corpus of public data. | More diverse queries expose more of the model’s internal mapping. |
| 2. **Embedding collection** | Send each query to the EaaS endpoint, retrieve the returned embedding vectors, and store the (input, embedding) pairs. | These pairs are the “data points” that the attacker will use to fit a model. |
| 3. **Model fitting** | Treat the embedding function as a black‑box mapping \(f: \text{text} \rightarrow \mathbb{R}^d\). Fit a surrogate (e.g., linear regression, kernel ridge, or a small neural net) that predicts the same vectors from the input representation (e.g., token IDs, one‑hot words, or sentence embeddings). | If the surrogate’s predictions are close to the true embeddings, the attacker has effectively reconstructed the key parameters (weights, attention maps, etc.). |
| 4. **Parameter extraction** | Once the surrogate model is trained, the attacker can inspect its weights (e.g., embedding matrix, projection layers, attention heads). In some attacks, the attacker directly inverts the linear transformation to recover the exact embedding matrix. | The surrogate’s parameters approximate or equal the original model’s parameters, enabling the attacker to reproduce the service. |

**Why this is feasible**

- **Deterministic outputs**: Embedding APIs return the same vector for identical input, so repeated queries yield consistent data.
- **High‑dimensional linearity**: Many embedding layers are essentially linear projections; solving a linear system with enough queries recovers the matrix.
- **No need for model access**: The attacker only needs the public API, which is the standard EaaS interface.

**Defense implication**  
Watermarking or backdoor techniques (like the “Embedding Marker” you described) aim to make this extraction detectable or less useful, but the fundamental extraction logic above remains the core threat model for EaaS.</sample>
    <sample id="191">Three authors are involved: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">Yang Luo presented **CAME: Confidence‑Guided Adaptive Memory‑Efficient Optimization**, a new optimizer that blends the fast convergence of Adam with the low memory footprint of Adafactor. He first highlighted the memory burden of traditional adaptive optimizers, which store first and second‑moment estimates for every parameter, and noted that Adafactor reduces this overhead by factorizing the moment matrices via non‑negative matrix factorization (NMF). However, Adafactor’s rank‑1 NMF introduces erroneous updates that slow convergence and destabilize training.  

To address this, Yang introduced a confidence‑based mechanism that measures the residual between the predicted momentum \(m_t\) and the actual update \(u_t\). This “instability” matrix \(S_t\) is used as a denominator for the momentum, effectively down‑scaling unreliable updates and improving stability. The resulting optimizer, CAME, retains Adafactor’s memory savings while matching or surpassing Adam’s training speed.  

Experiments on BookCorpus, Wikipedia, and models such as BERT, GPT‑2, and T5 showed that CAME delivers up to a 3.4 % boost in validation accuracy over Adafactor and outperforms Adam on very large pre‑training runs, especially at batch sizes from 8K to 32K. Memory profiling confirmed that CAME uses less memory than Adam, LAMB, and SM3, and downstream tasks with BERT‑Large achieved comparable results to baselines with reduced resource usage. The talk concluded that CAME offers a robust, memory‑efficient alternative for large‑scale language model training.</sample>
    <sample id="193">The presentation does not specify how many annotators were involved in creating the initial dataset.</sample>
    <sample id="194">**Author affiliations (as stated in the talk)**  

| Author | Affiliation |
|--------|-------------|
| Jenny (first‑year PhD student) | Carnegie Mellon University |
| Sebastian Santy | University of Washington |
| Ronan Le Bras | Allen Institute for AI |
| Katharina Reinecke | Allen Institute for AI |
| Maarten Sap | Allen Institute for AI |</sample>
    <sample id="195">**Summary**

This work tackles explainable question answering (XQA) by combining neuro‑symbolic and decomposition techniques. Traditional neuro‑symbolic methods translate questions into formal queries (e.g., SPARQL) but rely on incomplete knowledge bases (KBs), limiting recall. Decomposition approaches generate natural‑language intermediate steps, yet they depend on free‑text corpora whose linguistic diversity hampers accuracy. The authors argue that integrating heterogeneous sources—structured KBs and unstructured text—can better answer complex questions, especially when a question can be answered directly or needs further breakdown.

They propose **RoHT (Reasoning over Hierarchical Question Decomposition Tree)**, a two‑stage framework. First, a **Hierarchical Question Decomposition Tree (HQDT)** is built: the root is the original question, internal nodes are generated by grouping atomic leaf questions (obtained via a decomposer) into intermediate sub‑questions, and each node is assigned a certainty score. Second, **probabilistic reasoning** traverses the tree: a scheduler chooses the appropriate knowledge source (KB, text corpus, or recursively solving children); executors retrieve answer candidates with probabilities; an aggregator fuses these to output the top answers. 

RoHT was evaluated on **KQA Pro** (KB QA with 50% missing triples, supplemented by Wikipedia) and **Musique** (reading comprehension with added Wikidata). Results show that RoHT outperforms state‑of‑the‑art KB methods (e.g., TransferNet) and text‑only models, achieving significant gains when combining KB and text, and demonstrating the benefit of explicit hierarchical decomposition for explainable QA.</sample>
    <sample id="196">The example with the governor on the left is **“I saw Bart and Lisa.”**</sample>
    <sample id="197">**State‑of‑the‑art dialogue systems (as of mid‑2024)**  

| Category | Typical model names | Key features |
|----------|--------------------|--------------|
| **Large‑scale open‑source LLMs** | LLaMA‑2 Chat, LLaMA‑3, Mistral 7B‑Chat, Mixtral | 13–70 B parameters, fine‑tuned on conversational data, high performance on open‑domain chat |
| **Commercial LLMs** | OpenAI GPT‑4 / ChatGPT‑4, Anthropic Claude 2/3, Google Gemini 1.0/1.5, Meta BlenderBot 3 | 30–100 B parameters, optimized for safety, multimodal, strong factual grounding |
| **Specialized fine‑tuned models** | DialoGPT‑Large, Cohere Command R, Amazon Titan‑Chat | Tailored for dialogue, often smaller but heavily tuned for response quality and consistency |
| **Hybrid retrieval‑augmented systems** | Retrieval‑augmented GPT‑4, LLaMA‑2 Chat + Retrieval, Gemini + Retrieval | Combine LLM generation with external knowledge bases to reduce hallucinations |

These models represent the current leading performers in open‑domain conversational AI and are typically benchmarked using human‑rated metrics such as the ABC‑Eval framework described in the paper.</sample>
    <sample id="198">Because modern language models process ever‑longer contexts, and their acceptability judgments can shift when additional preceding or following material is added. Testing across the full context window reveals whether a model’s grammaticality assessment is robust to contextual changes, ensuring that its abstract knowledge is consistently applied even in long, multi‑sentence settings.</sample>
    <sample id="199">Yes. In the multilingual experiments, the English model’s accuracy dropped on **seven** of the nine datasets (only improving on three), so training a single multilingual model typically hurts English performance relative to a monolingual English model.</sample>
    <sample id="200">No – annotators only know the entity names beforehand. They are not expected to know the details of the items; the background information (search links, Wikipedia snippets, images, etc.) is supplied to them at annotation time so they can form indirect references.</sample>
    <sample id="201">The paper evaluated PaLM translations with the leading neural MT metrics – in particular **BLEURT** (and the comparable neural metric **COMET**) were used to score the outputs, in addition to the standard BLEU and other reference‑based measures for completeness.</sample>
    <sample id="202">**Short answer:**  
The paper did not break down the generalization drop by entity type, so it cannot say definitively that only some NER categories suffer more. However, the overall F1 decline observed on the new CoNLL++ data is driven by a mix of all entity classes, and in practice temporal drift tends to hit the rarer, less‑well‑represented types (e.g., *ORG* and *LOC*) more severely than the frequent *PER* tags. Thus, while the study confirms that performance degrades in general, a fine‑grained type‑level analysis remains an open question.</sample>
    <sample id="203">Because the judgments that NLP systems learn from are shaped by the people who created and labeled the data, the resulting models tend to reflect the perspectives, experiences, and values of those groups. This “positionality” matters because:

* It creates systematic performance gaps—models that work well for English‑speaking, college‑educated, cisgender users often perform poorly for non‑English speakers, people with lower education, or non‑binary individuals.
* It can amplify bias and exclusion in socially sensitive tasks (toxicity detection, hate‑speech, social acceptability), leading to unfair or harmful outcomes for under‑represented groups.
* As NLP moves into more subjective, socially oriented applications, ensuring that models are aligned with a diversity of real‑world users becomes essential for equity, trust, and responsible deployment.</sample>
    <sample id="204">The paper did **not** use adapter‑based tuning for BLOOM (or the other large multilingual models).  The experiments involved full fine‑tuning of the models on the cross‑lingual semantic‑parsing tasks.</sample>
    <sample id="205">Shangbin presents a study on how political bias in large‑scale web‑crawl pretraining data propagates to language models (LMs) and downstream NLP tasks. By prompting LMs with political questionnaires (e.g., the Political Conference Test), the authors automatically rate the models’ leanings, finding that GPT‑4 is the most liberal, while GPT series are generally more socially liberal than BART variants. To trace the source of bias, they further pretrain checkpoints on partisan corpora—split by news vs. social media and by left/right leanings—and observe corresponding ideological shifts (e.g., RoBERTa gains a liberal bias after training on left‑leaning Reddit). They also divide corpora by pre‑ and post‑2017 presidential periods, showing that models trained after 2017 lean further from the center, mirroring societal polarization.

The study then evaluates politically biased LMs on hate‑speech and fake‑news detection. Left‑leaning models better detect hate speech against minorities but underperform on hate targeting powerful groups; right‑leaning models show the opposite pattern. In fake‑news detection, each model more accurately flags misinformation from the opposite side. Qualitative examples confirm these disparities, raising fairness concerns: deploying a biased LM could marginalize opposing political views and allow hateful content against minorities to slip through. The authors warn of the dilemma between sanitizing training data (risking censorship) and preserving political diversity, likening it to a “Scylla‑Charybdis” or trolley‑problem scenario.</sample>
    <sample id="206">They start with a pre‑trained transformer (the same model used for the other tasks) and transfer its weights first from the **CE** task (binary classification of expansion / comparison relations) and then further fine‑tune it on the **debate** stance‑classification task. This CE‑then‑debate transfer gives the best zero‑shot performance and is used as the cold‑start model for active learning.</sample>
    <sample id="207">The paper evaluates PaLM on the **latest WMT shared‑task test sets** for the language pairs studied (e.g., the WMT 22 and WMT 23 test sets for German→English and English→German). In addition, the authors use the corresponding **WMT dev sets** to pick high‑quality examples for few‑shot prompting. These WMT datasets are the most recent public benchmarks that the MT community uses for evaluation.</sample>
    <sample id="208">They propose **three** recommendations.</sample>
    <sample id="209">The paper reports that the over‑generate‑then‑filter approach gives a noticeable boost over the strongest baseline, but the exact numeric gain is not disclosed in the excerpt you provided.</sample>
    <sample id="210">The speaker’s name is **Shuheng**.</sample>
    <sample id="211">Yes.  
The paper introduces **DEPLAIN**, a manually‑aligned corpus of 30 k+ sentence pairs (and 483–750 documents) at both the sentence and document levels, together with detailed statistics on simplification types. The authors also release fine‑tuned baseline models (long‑mBART for documents, mBART for sentences) and evaluation results. This makes the dataset and the reported scores a ready‑to‑use benchmark for German text‑simplification and sentence‑alignment tasks.</sample>
    <sample id="212">They only experimented with a single smaller model: a T5 variant fine‑tuned on the CoScript dataset.</sample>
    <sample id="213">The base model used for multi‑modal instruction tuning is the pre‑trained **OFA (Large) model**.</sample>
    <sample id="215">In this presentation Adam Przepiórkowski argues that coordination should be modeled with symmetric dependency structures rather than asymmetric ones. He contrasts four mainstream theories: the Universal Dependencies (first conjunct heads the coordination), Mel’čuk’s meaning‑text theory (also first‑conjunct headed), the Prague approach (conjunction heads the coordination), and Hudson’s Word Grammar (all conjuncts are heads).  

The core claim rests on the principle of dependency‑length minimization (DLM). Using the example “Marge read this absolutely fascinating book about bees yesterday,” he shows that moving a long noun phrase after an adjunct shortens the overall dependency length, making the sentence acceptable even though it violates the typical “direct object close to verb” rule.  

Przepiórkowski then reports statistical analysis of coordinated structures in the Penn Treebank (enhanced UD). He confirms the well‑known left‑conjunct‑shorter tendency and finds it increases with length difference when the governor is on the left or absent. Crucially, this effect disappears when the governor appears on the right. These findings weaken asymmetric models (which predict a fixed head) and support symmetric structures where all conjuncts share the head. The poster session invites further discussion.</sample>
    <sample id="217">**Summary (≈200 words)**  

Weihao Zeng et al. investigate *compositional generalization* in **multi‑attribute controllable dialogue generation** (CDG). Existing methods either control a single attribute or rely on discrete labels, leaving continuous attributes and unseen attribute combinations largely unaddressed. They introduce **DCG (Disentangled Controllable Generation)**, a prompt‑based model built on DialoGPT that learns *attribute concepts* from seen attribute values and disentangles different attribute combinations using a dedicated loss. DCG employs two types of prompts: (1) **attribute‑oriented prompts** that encode instance‑specific control signals, and (2) **task‑oriented prompts** that provide global dialogue features. The two are concatenated into a full prompt embedding. To enrich diversity, pseudo attribute combinations are generated, and a disentanglement loss is applied to the compositional prompts.  

The authors also propose **MAE (Multi‑Attribute Evaluation)**, a unified, reference‑free metric that evaluates controllability and text quality across discrete (e.g., emotion, persona) and continuous (e.g., sentiment score) attributes. MAE uses a template with masked tokens plus a trainable continuous prompt, yielding high correlation with human judgments and outperforming classic metrics.  

Benchmarks on DailyDialog‑CG and a new dataset demonstrate that DCG outperforms baselines (CTRL, fine‑tuned models) on both controllability (E‑ACC, A‑ACC) and text equality (BLEU), with only a minor drop on unseen attribute combinations. PCA visualizations confirm effective disentanglement of attribute embeddings. Overall, the study shows that prompt‑based disentanglement enables robust multi‑attribute control and compositional generalization in dialogue systems.</sample>
    <sample id="218">The paper’s authors are all affiliated with **Google Translate (Google Research)**.</sample>
    <sample id="219">**Summary**

Jia‑Huei Ju and colleagues from Academia Sinica present “A Compare‑and‑Contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports.” The work focuses on the SEC Form 10‑K, an annual report that contains detailed company information but is labor‑intensive to mine. The authors observed that 80 % of tokens are identical across consecutive years, suggesting strong yearly dependence. Motivated by this, they introduce a *highlighting* task: given a target report (T) and its previous‑year reference (R), the model must identify the words that explain the relationship between the two documents.

The proposed pipeline has several stages. **Stage 0** (not detailed in the talk) performs document segmentation. **Stage 1** classifies all target‑reference pairs into three categories: β (high syntactic/semantic similarity), *revised* (similar syntax but different meaning), and *mismatched* (new information). **Stage 2** fine‑tunes a pre‑trained model on an out‑of‑domain natural‑language‑inference dataset (eSNLI) with token‑level rationales. **Stage 2+** performs in‑domain fine‑tuning using the *revised* pairs as pseudo‑positive examples and random negatives, employing a hybrid objective that mixes cross‑entropy and KL‑divergence to mitigate noisy labels.

Evaluation is conducted on eSNLI and a newly released dataset, **FINAL**. Two metrics are used: precision (precision over recall) and Pearson correlation coefficient (PCC). The domain‑adaptive model achieves the best results on FINAL while retaining strong generalization to eSNLI. The authors also show that the model benefits from mismatched pairs, which were not used during training. Future work includes enhancing effectiveness, incorporating additional features, and applying information‑retrieval techniques to further improve the application.</sample>
    <sample id="220">The authors of the ACL 2023 paper “Transfer Learning for Dissonance Detection: Addressing the Rare‑Class Challenge” are all affiliated with **Stony Brook University**. Vasudha, the PhD candidate presenting the work, is a member of the Computer Science department at Stony Brook. The other co‑authors are also listed as Stony Brook University researchers in the official paper.</sample>
    <sample id="221">The paper evaluated PaLM on the standard WMT‑style test sets that Google Translate uses for its public benchmarks.  In practice the authors ran experiments on **eight bilingual directions**:

| Source → Target |
|------------------|
| **German → English** (de‑en) |
| **English → German** (en‑de) |
| **French → English** (fr‑en) |
| **English → French** (en‑fr) |
| **Russian → English** (ru‑en) |
| **English → Russian** (en‑ru) |
| **Chinese → English** (zh‑en) |
| **English → Chinese** (en‑zh) |

These pairs cover the most common WMT test sets (de‑en, fr‑en, ru‑en, zh‑en) in both translation directions, and the paper reports results for each of them when comparing different prompting strategies and against state‑of‑the‑art MT systems such as Google Translate.</sample>
    <sample id="222">**Summary (≈200 words)**  

The talk, “To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open‑Domain Question Answering,” explores how a Wikipedia‑trained retriever–reader system fails when confronted with a new, sparse domain such as biomedical text. The speaker illustrates this with a question about the output of plants in Narora, where the system correctly answers “nuclear power.” When the corpus is expanded to include biomedical documents, the retriever pulls passages with the word “plants,” confusing the reader and producing a wrong answer, because the reader was never trained on such domain‑specific language.

To address this, the authors investigate data‑driven interventions: **zero‑shot** (no target examples) and **few‑shot** (few target examples used to prompt a large language model to generate synthetic facts). The generated facts are converted into cloze questions that help adapt both the retriever and reader, yielding average gains of 8 % (retriever) and 11 % (reader). For zero‑shot, they control the three QA variables—question, answer, context—by fixing two and varying the third (e.g., using cloze questions, sampling uniform answer types, mixing source and target contexts). BM25 remains the most robust unsupervised baseline.

The second part formalizes domain shift using a compatibility score based on likelihoods of contexts (retriever) and answers (reader). Plotting target datasets on a 2‑D grid distinguishes **no shift**, **concept shift**, **covariate shift**, and **full shift**. Few‑shot methods help all datasets; zero‑shot works best for concept and covariate shifts. The study reports up to 24 % reader improvement and highlights that the efficacy of interventions depends on the type of shift.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">The experiments examined:

1. **MASS‑align** – the automatic sentence‑alignment method that proved most effective for German text‑simplification.
2. **Long‑mBART** – fine‑tuned for document‑level simplification.
3. **mBART (base)** – fine‑tuned for sentence‑level simplification.</sample>
    <sample id="225">**Training**: 53 of the 62 tasks  
**Testing**: the remaining 9 tasks (the full Common‑Sense Reasoning group plus 5 extra VQ/Miscellaneous tasks)  

So, 53 tasks are used for training and 9 tasks for testing.</sample>
    <sample id="226">The paper is authored by two people: Regina Stodden and Omar.</sample>
    <sample id="227">The speaker discusses a key gap in current language‑model research: grounded language understanding—mapping natural language into executable forms (e.g., SQL queries or robot action plans). Existing large models are pre‑trained purely on text, lacking real‑world grounding, which makes downstream tasks difficult. Typical approaches use autoregressive generation to produce plans, but these often yield grammatically incorrect or invalid outputs (e.g., KB queries that fail to execute). To address this, the authors propose **Pangu**, a framework that shifts the language model’s role from generation to discrimination. A symbolic agent first enumerates candidate plans; the language model then scores and ranks them, removing the burden of ensuring syntactic validity. This design is generic but instantiated on knowledge‑based QA, a challenging grounded setting. Experiments with BERT, T5, and Codex (both fine‑tuned and in‑context) show Pangu consistently outperforms baselines such as ArcaneQA, especially in sample efficiency: with Codex in‑context, a single demo yields &gt;50% accuracy on GRAIL‑query, far surpassing other methods. Moreover, Pangu’s probability distributions remain stable across seen and unseen structures, indicating robustness to non‑i.i.d. data and less overfitting than autoregressive models. The central takeaway: for grounded language understanding, **discrimination is a far better strategy than generation**. The speaker invites discussion and collaboration on extending this approach beyond QA.</sample>
    <sample id="228">The authors ran experiments on four datasets: **AG News, MIND, SST‑2, and Enron Spam**. (They also used a Wikipedia text corpus to compute word frequencies for trigger selection.)</sample>
    <sample id="229">**Summary**

Gabriella Skitalinskaya and Henning Wachsmuth present a study on automatically detecting and improving argumentative claims using revision histories from online debate platforms such as Kialo. The authors first motivate the importance of precise phrasing in argumentative writing and illustrate the revision process with the example “Cell phones cause brain cancer,” which evolves to “Cell phone radiation may cause brain cancer.” They formalise two tasks: (1) Suboptimal‑Claim detection, deciding whether a claim needs revision; and (2) Claim Improvement Suggestion, identifying the quality issues that warrant revision. Rather than hand‑crafting quality criteria, they learn from implicit revision patterns in collaborative data, treating final versions as optimal and earlier ones as suboptimal.

Four major challenges are identified:  
1. **Representativity &amp; Reliability** – ensuring that final claim versions truly reflect optimal quality rather than oversight.  
2. **Model Complexity &amp; Architecture** – selecting models sensitive to subtle wording changes while balancing pre‑training, fine‑tuning, and classification design.  
3. **Contextual Dependencies** – determining whether claim quality depends on broader debate context, parent claims, or domain knowledge.  
4. **Topical &amp; User Bias** – accounting for noise, accidental edits, and cultural biases inherent in collaborative revisions.

Experiments compare several model architectures and demonstrate that explicitly modelling the distance between claim versions improves suboptimal claim detection. The impact of contextual signals varies across tasks and quality dimensions. Overall, the study shows that revision‑based data can effectively train systems to identify and suggest improvements for argumentative claims.</sample>
    <sample id="231">**NACHOS** is a large French‑language corpus of medical text obtained by crawling the web. It contains hundreds of millions of sentences (up to ~7 GB of tokenized data) and is used as the in‑domain training data for DrBERT, the first French biomedical pre‑trained model.</sample>
    <sample id="232">The speaker’s name is **David Vilar**.</sample>
    <sample id="233">Sara Papi from the University of Trento and Foundazione Bruno Kessler presents the paper “Attention as a Guide for Simultaneous Speech Translation.”  Simultaneous Speech Translation (SimulST) translates spoken language into text in real time, enabling cross‑language communication.  Current SimulST systems rely on specialised architectures that require multi‑stage training, different optimisation objectives, and separate models for each latency regime (e.g., 1 s vs 2 s).  This complexity hampers scalability and flexibility.  

Papi’s solution, EDAtt (Encoder‑Decoder Attention), repurposes existing offline Speech‑Translation models without retraining or architectural changes.  A single model serves all latency settings; latency is controlled by a simple rule that decides whether to emit a partial translation based on the cross‑attention distribution.  If the attention weights for a target word are concentrated on the most recent λ speech frames, the model defers emission (the sum of weights exceeds a threshold α); otherwise it outputs the word.  This mechanism uses the model’s own attention signals to gauge when sufficient acoustic evidence is available.  

Experimental results on German demonstrate that EDAtt achieves higher BLEU scores for a given average lagging, and a left‑shifted curve in the BLEU‑vs‑lag plot compared to Wait‑k, Local Agreement, and specialised SimulST architectures.  The authors also report computational‑aware average lagging, showing faster real‑time performance.  Code, models, and sample outputs are publicly released to support reproducibility.</sample>
    <sample id="234">Prompting strategy has a **large impact** on translation quality.  

- In a one‑shot experiment, changing just the prompt caused a **&gt;1 BLEURT‑point swing on most sentences (516/1,000)** and in extreme cases up to **~40 BLEURT points**.  
- At five‑shot, the *form* of the prompt matters little; the **quality of the examples** dominates.  

So, especially in low‑shot settings, prompt design can shift BLEURT scores by several points and even tens of points.</sample>
    <sample id="235">**Affiliations of the authors**

| Author | Affiliation |
|--------|-------------|
| **Kayo Yin** | University of Washington (PhD student) |
| **Patrick Fernandes** | Google AI (Research Scientist) |
| **Emmy Liu** | Google AI (Research Scientist) |
| **André F. T. Martins** | Google AI (Research Scientist) |
| **Graham Neubig** | University of Washington (Associate Professor, Computer Science) |

These institutions are the primary research affiliations listed in the paper “When Does Translation Require Context? A Data‑driven, Multilingual Exploration.”</sample>
    <sample id="236">The “five expert‑written instructions” are **five distinct natural‑language prompts that were hand‑crafted by domain experts for each of the 62 MultiInstruct tasks**.  
- Each task comes with 5 different wording variations (e.g., “Describe the image”, “Generate a caption for the picture”, “Summarize the scene”, “Answer the following question about the photo”, “Explain what happens in the image”).
- They are used during training so the model learns to follow any of the phrasing variants, and during evaluation they are sampled one‑by‑one to measure robustness (the “sensitivity” metric).

The presentation did not publish the exact wording of these templates, only that such a set of five instructions exists per task and is used to train and test the model’s instruction‑following capability.</sample>
    <sample id="237">They propose the **KITMUS test suite** – a coreference‑resolution benchmark that deliberately varies whether the relevant background facts are available only in the model’s pretrained parameters, only in the inference‑time context, or in both. By doing so, KITMUS forces models to integrate knowledge drawn from multiple sources (pre‑training vs. inference‑time information).</sample>
    <sample id="238">**MeetingBank: A City‑Council Meeting Summarization Benchmark**  
Yebowen Hu (UCF) introduces MeetingBank, a new dataset of 1,366 city‑council meetings (≈7,000 instances) from the U.S. that pairs audio‑derived transcripts with expert‑written summaries. Using Speechmatics to transcribe recordings, the team extracts meeting metadata (type, date, ItemID) from council websites, locates official minutes for reference summaries, and aligns segment timestamps to produce second‑level transcripts paired with summaries. The dataset spans several cities (Boston, Seattle, Denver, etc.) and years, offering statistics on meeting length, token counts, speaker counts, and average summary size.

Analytical metrics show high coverage (0.7–0.9) and varying density scores, indicating most summaries are extractive with some editing. For evaluation, the authors benchmark ten systems: extractive methods (Oracle, LEAD, LexRank, TextRank) and neural abstractive models (BART‑Large, Pegasus, Longformer, DialogLM, HMNet). BART‑Large fine‑tuned on MeetingBank outperforms others, while DialogLM achieves the best ROUGE‑2 among abstractive models. Surprisingly, GPT‑3 Davinci‑003 scores poorly on automatic metrics but excels in human‑rated fluency and coherence, though it lags in informativeness and factuality.

Human evaluation (200 instances, 3 annotators) used five Likert criteria (informativeness, factuality, fluency, coherence, redundancy). Results suggest future summarizers should better capture key discussion points and that evaluation metrics need to better reflect human preferences. MeetingBank is released publicly for researchers and practitioners to advance meeting summarization and to explore city‑council decision processes.</sample>
    <sample id="241">Ethan presented a human‑in‑the‑loop framework for early misinformation detection, focusing on COVID‑19 treatment claims. He identified two shortcomings in existing systems: (1) unrealistic evaluation using retrospectively built datasets and the risk of counter‑evidence leakage, and (2) a lack of human participation throughout the process, treating humans only as a final arbiter. To address these, the authors built a two‑stage system that processes raw tweets end‑to‑end while embedding human feedback at multiple points.

The first stage extracts claimable statements. Tweets are filtered by keywords, then a T5 model is fine‑tuned to answer “What is the mentioned COVID‑19 cure?” The output claims are ranked by trendiness using Fisher’s Exact Test before human verification. The second stage detects policy violations. A BERT‑based stance classifier judges whether a tweet supports an unapproved treatment, and supporting tweets are flagged for human review.

Evaluation focuses on “early detection”—identifying a claim before its first appearance in a debunking news article. The system detected several unapproved treatments ahead of news coverage. For policy violation detection, human annotators rated tweets on a Likert scale; the system achieved 65 % agreement on high‑violation scores. Human workload was measured as 124.2 policy violations confirmed per hour. Ethan concluded that this realistic, human‑centric framework better reflects real‑world moderation needs and can guide future misinformation detection research.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include:

| Category | Typical Approaches |
|----------|--------------------|
| **Human‑based** | • **Likert‑scale ratings** (turn‑level or dialogue‑level) – judges rate aspects such as relevance, fluency, engagement.&lt;br&gt;• **Pairwise comparisons** – judges choose the better of two conversations or responses.&lt;br&gt;• **Error/behavior annotation** – annotators label specific issues (e.g., hallucination, contradiction, irrelevance). |
| **Automatic / Proxy metrics** | • **Token‑overlap metrics** – BLEU, ROUGE, METEOR.&lt;br&gt;• **Perplexity / language‑model score** – measures fluency. &lt;br&gt;• **Embedding‑based similarity** – cosine similarity of sentence embeddings.&lt;br&gt;• **Task‑specific success rates** – e.g., goal completion in task‑oriented systems. |
| **Hybrid** | • **Combined human‑automatic** – use human ratings to calibrate or weight automated scores. |

These methods are often used together to get a holistic picture of a model’s performance and to identify strengths and weaknesses across different dialogue dimensions.</sample>
    <sample id="243">The paper lists five authors in total: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke, and Maarten Sap.</sample>
    <sample id="244">The background knowledge needed is that **judges work in law courts and decide cases**. This general fact helps link the pronoun “he” to the entity “Servin”, who is identified as a judge.</sample>
    <sample id="245">**Summary**

This work presents a two‑step pipeline for recruiting high‑agreement annotators on Amazon Mechanical Turk (MTurk) for summarization. The first step—*Qualification Settings*—filters workers by location, HIT count, and approval rate. The *Qualification Task* tests annotators on six dimensions of summary quality using three documents plus an attention check. Workers are labeled gold, silver, bronze, or blocked; only gold and silver pass. From 200 applicants, 26 qualified (8 gold, 18 silver; 13 %). The second step—*Endurance Task*—assesses sustained performance with 10 HITs on a single document and four saliency‑based summaries. Twelve workers (4 gold, 8 silver; 6 %) passed, achieving higher inter‑annotator agreement (Cohen’s κ, Krippendorff’s α = 0.443) than experts.

A *Reference‑Based Task* (30 HITs, one reference, four candidate summaries) evaluates coverage in both directions. Eight of the 12 pipeline workers completed all HITs, yielding κ = 0.534 and α = 0.534. Baseline MTurk workers filtered with MACE reached α = 0.380 (incomplete HIT coverage), while CloudResearch participants achieved α = 0.513 but with lower acceptance rates. A correctness analysis on 50 random samples showed strong Spearman correlations between Pipeline and CloudResearch workers and between GPT predictions and expert judgments.

The pipeline thus secures high‑agreement annotations at lower cost, comparable to premium platforms, and avoids wasted resources on poor data. Future work will explore broader tasks, languages, and recruitment strategies, while acknowledging limitations such as English‑only testing and the absence of guaranteed correctness training.</sample>
    <sample id="246">Yes – the authors have released the code on GitHub.  The repository (along with the KITMUS data set) is linked in the paper’s “check out the data set and code on GitHub” note.</sample>
    <sample id="247">**FACTKG: Fact Verification via Reasoning on Knowledge Graphs**  
Existing fact‑verification datasets—such as FEVER, VitaminC (Wikipedia text) and TabFact, InfoTabs (tables)—lack knowledge‑graph (KG) evidence. FACTKG introduces a new task that uses DBpedia as evidence for natural‑language claims. Claims come in two styles (written and colloquial) and are labeled **SUPPORTED** or **REFUTED**. The dataset requires two sub‑tasks: retrieve supporting evidence from the KG and verify the claim.

Five reasoning categories are defined:  
1. **One‑hop** – a single triple directly links the two entities.  
2. **Conjunction** – multiple one‑hop triples must all hold.  
3. **Existence** – the presence of a particular relation for an entity.  
4. **Multi‑hop** – a path of two or more triples is required.  
5. **Negation** – evidence must be found and then shown to be false.

Colloquial claims are generated via a style‑transfer model and presupposition templates to increase realism.  

Baseline experiments include a **Claim‑Only** model (no KG) and the **GEAR** model that incorporates graph evidence. Both beat the majority‑class baseline (51 %), with GEAR achieving the highest accuracy. The dataset and code are publicly available for further research.</sample>
    <sample id="248">No. The study recruited a large, geographically and demographically diverse pool (over 1,000 annotators from 87 countries), but the distribution across demographics (country, gender, education level, etc.) was uneven – the dataset is not balanced for each group.</sample>
    <sample id="249">The authors perturbed acceptable sentences by introducing small “noise” changes that left the overall grammatical structure intact. They applied a handful of simple edits—such as replacing words with synonyms, swapping non‑essential tokens, or adding minor filler words—while ensuring the sentence remained syntactically correct. These perturbations were designed to test whether the model’s acceptability judgment would shift when the sentence’s surface form varied but its underlying structure (the phenomenon being tested) stayed the same.</sample>
    <sample id="250">A dimensional evaluation means assessing a conversational AI on multiple, distinct aspects or “dimensions” of quality—such as relevance, consistency, factual accuracy, common‑sense adherence, empathy, etc.—rather than giving it a single overall score. Each dimension is measured separately, allowing fine‑grained insight into the model’s strengths and weaknesses.</sample>
    <sample id="251">The authors are affiliated with the **University of Science and Technology of China**.</sample>
    <sample id="252">Sai Kiran Tanikella presents **U‑CREAT**, a novel unsupervised approach for Prior Case Retrieval (PCR) that uses event‑based representations of legal documents. PCR is crucial for lawyers and judges to locate precedent cases cited in a new query case, but the growing volume of judgments makes manual search infeasible.  

U‑CREAT’s first contribution is the **IL‑PCR dataset**, a benchmark of 7,070 Indian court cases with an average of 6.8 citations per query. Compared to the Canadian COLIEE’21 set, IL‑PCR contains longer documents, a richer vocabulary, and more citations, providing a challenging test bed.  

The second contribution is the **U‑CREAT pipeline**, which extracts **subject‑verb‑object (SVO) events** via dependency parsing. For each query and candidate document, an interaction matrix of shared events is built and fed into retrieval models. Experiments evaluate three families of models: word‑count (BM25), transformer‑based (BERT, DistilBERT, DistilRoBERTa, legal‑domain variants), and event‑based. Transformers perform poorly compared to BM25, and even legal‑domain fine‑tuned models lag behind. In contrast, event‑based models—Atomic, Non‑Atomic, and especially **Event‑Filtered Documents**—outperform all baselines, achieving the highest F1 scores and lowest inference times.  

On COLIEE’21, U‑CREAT surpasses existing supervised methods (e.g., MTFT‑BERT), establishing a new state‑of‑the‑art for unsupervised PCR. The work demonstrates that event‑centric, unsupervised retrieval generalizes across Indian and Canadian legal contexts without domain‑specific tuning.</sample>
    <sample id="253">**DisorBERT: A Double Domain Adaptation Model for Detecting Mental Disorders in Social Media**

Mario Ezra Aragón presents a collaborative effort between researchers in Mexico and Spain. The work addresses the challenge of automatically detecting signs of mental disorders—such as depression, PTSD, bulimia, and anorexia—within large amounts of user‑generated content on platforms like Reddit. Because labeled data are scarce, the authors employ **double domain adaptation**: first adapting a pretrained BERT model to the informal language of Reddit, and then further fine‑tuning it on a mental‑health‑specific corpus. To steer the model toward clinically relevant cues, they introduce **guided masking** informed by a mental‑health lexicon, encouraging the model to focus on emotionally salient terms during pre‑training.

Evaluation on the eRisk datasets shows that DisorBERT achieves a balanced precision–recall trade‑off, outperforming baseline approaches such as MentalBERT. Qualitative analyses of masked‑language predictions reveal that DisorBERT generates more psychologically oriented completions (e.g., “focus,” “talk,” “breath”) compared to generic BERT outputs. Attention visualizations further highlight key terms (“anxious,” “medication”) in high‑scoring depression users.

The authors conclude that the combination of dual domain adaptation and lexicon‑guided masking effectively captures mental‑health signals in social media. Future directions include integrating additional lexical resources and clinical data to further refine the model.</sample>
    <sample id="254">**Summary**

Sun Qi from Nanjing University of Science and Technology presents a new framework for document‑level relation extraction (DocRE) that tackles the pervasive noise in distantly supervised (DS) data. Traditional DS methods rely on automatically generated pseudo labels, which often introduce false positives—e.g., a model may incorrectly predict a “composer” relation while missing the correct “place of birth” relation. To mitigate this, the authors first train a pre‑denoising DocRE model using both DS and manually annotated data, then generate pseudo labels for the DS set. They introduce uncertainty estimation based on Monte Carlo dropout, performing multiple stochastic forward passes to capture model confidence. Recognizing that entity pairs can share multiple relations, they propose an **instance‑level uncertainty** metric that assigns an uncertainty score to each positive pseudo label rather than to the entity pair as a whole. Empirical analysis shows that uncertainty distributions differ across relation classes, with frequent classes exhibiting lower average uncertainty than long‑tail classes.

To filter out noisy pseudo labels, the authors devise **dynamic class‑specific uncertainty thresholds**. Pseudo labels whose uncertainty exceeds the threshold for their class are discarded, and the remaining confident labels replace the original DS annotations. This denoising step is repeated in a **multi‑phase training strategy**, where the model is iteratively retrained and the DS data are re‑labelled, progressively improving label quality. The resulting system achieves significant gains over strong baselines on two public DocRE datasets. The main contributions are: (1) an uncertainty‑guided label‑denoising framework, (2) instance‑level uncertainty estimation for overlapping relations, (3) an iterative relabeling strategy with dynamic class thresholds to address long‑tail issues, and (4) demonstrable performance improvements.</sample>
    <sample id="255">The form of the prompt matters only in the very low‑shot setting.  
- **Zero‑shot and one‑shot prompting**: the exact wording/format of the prompt has a noticeable impact on performance.  
- **Five‑shot (and higher) prompting**: the concrete phrasing of the prompt makes little difference; the choice of the example translations dominates.</sample>
    <sample id="257">The authors evaluated **four state‑of‑the‑art chat models** on 100 human‑bot conversations each. (The specific model names are not mentioned in the excerpt.)</sample>
    <sample id="258">**Summary of “Can Large Language Models Be an Alternative to Human Evaluation?”**

The paper explores whether large language models (LLMs) can replace human judgments when assessing text quality. The authors argue that human evaluation is unstable, hard to reproduce, and costly, whereas LLMs—with their instruction‑following capabilities—might offer a scalable, consistent alternative.  

They conduct experiments where LLMs rate machine‑generated (GPT‑2) and human‑written stories on grammar, coherence, likability, and relevance. Four LLMs are tested: T0, InstructGPT (Curie and Davinci), and ChatGPT. Instructions and sample stories are identical for both LLMs and human raters (English teachers). Human raters consistently prefer human writing, setting a ground truth for comparison.  

Results show that smaller LLMs fail to reflect this preference, but Davinci and ChatGPT align with human judgments, correctly favoring human texts. The study also examines factors that could influence LLM ratings—instruction wording, sampling strategy, and task selection—and discusses benefits (speed, reproducibility) versus costs (model access, potential biases).  

The authors conclude that certain LLMs can serve as viable proxies for human evaluation in text generation tasks and invite readers to delve deeper into the paper or discuss their findings at the ACL poster session.</sample>
    <sample id="259">**XSemPLR: A Unified Cross‑Lingual Semantic Parsing Benchmark**

XSemPLR introduces a comprehensive benchmark for translating user queries in multiple natural languages into diverse semantic representations (SQL, Lambda Calculus, FunQL, etc.). The dataset aggregates **nine domains, five parsing tasks, eight meaning representations, and 22 languages** spanning 15 language families, notably adding Chinese and Lambda calculus support that previous benchmarks lacked.

The benchmark defines **six experimental settings**:

1. **Translate‑Test** – source language is translated to the target language via Google Translate, then a monolingual model is applied.  
2. **Monolingual** – training and inference in the same language.  
3. **Monolingual Few‑shot** – only 10 % of training data.  
4. **Multilingual** – a single model trained on all languages.  
5. **Cross‑lingual Zero‑shot** – train on one language, evaluate on another.  
6. **Cross‑lingual Few‑shot** – augment the source language with few target‑language examples.  

**Model evaluation** covered two families:  
- **Encoder‑PTR** (XLM‑R + PTR, mBERT + PTR)  
- **Encoder‑Decoder** (mBART, mT5).  

Key findings:  
- Encoder‑Decoder models consistently outperform Encoder‑PTR across all datasets.  
- Multilingual training boosts performance for most languages but can **degrade English** results (the “curse of multilinguality”).  
- Cross‑lingual Zero‑shot gaps are large, but adding a few target‑language examples (Few‑shot) quickly narrows the gap.  
- Pretraining on English strongly aids few‑shot transfer.  
- Large models such as Codex and BLOOM still fall short on this task.  

In sum, XSemPLR offers a unified, multilingual framework that reveals the strengths and limitations of current multilingual models for semantic parsing.</sample>
    <sample id="260">Only one author is explicitly mentioned – Jingwei Yi.</sample>
    <sample id="261">**Ideal qualities of a good planner**

- **Reasonable** – the script must be logical, coherent, and feasible.  
- **Faithful to constraints** – every specified constraint (e.g., ingredient type, dietary restriction, tool availability) must be respected and reflected in the plan.  
- **Semantically complete** – all necessary steps to achieve the goal are included.  
- **Adaptable** – it works consistently across different categories of constrained goals.  

In short, a good planner writes reasonable, complete scripts that remain faithful to all imposed constraints.</sample>
    <sample id="262">Based on the information you shared, only **Siyu Yuan** is explicitly mentioned as an author of the paper.  If there are additional co‑authors, they weren’t listed in the excerpt you provided.</sample>
    <sample id="263">In this work we investigate the instability of in‑context learning (ICL) for large language models, attributing it to **label biases** that arise from design choices such as example selection and ordering. We formalise a **bias typology** for text classification ICL:  

1. **Vanilla‑label bias** – the model’s intrinsic preference for certain label names.  
2. **Context‑label bias** – the influence of the in‑context examples on label choice.  
3. **Domain‑label bias** – a newly identified bias where the task corpus itself skews predictions; sampling random in‑domain words can strongly bias the model, whereas random English words do not.

Empirical evaluation on many datasets shows that tasks with high domain‑label bias suffer severe performance drops, often near chance, even after existing calibration methods. To mitigate all bias types we propose **Domain‑Context Calibration**: estimate each label’s bias using *content‑free* text, but instead of a single pre‑defined token, we sample many random in‑domain words, thereby capturing domain bias while keeping the text largely neutral.  

Across several models (including GPT‑3) and datasets, this calibration yields significant average accuracy gains, especially on tasks with large domain‑label bias. A series of ablation studies confirms that (a) replacing the single token with random English words improves performance, (b) using multiple random words yields further gains, and (c) incorporating in‑domain words yields the largest benefit.  

Thus, the paper offers a systematic taxonomy of label biases, reveals domain‑label bias as a critical issue, and presents a practical calibration method that substantially stabilises and improves ICL for large language models.</sample>
    <sample id="264">**Summary**

Lin Wang presents *TAVT: Towards Transferable Audio‑Visual Text Generation*, a new multimodal generation task aimed at overcoming the data scarcity and domain shift problems that plague audio‑visual captioning. In this task, the goal is to train a model that can quickly adapt to new audio‑visual domains with only a few labeled examples. The main challenge is the heterogeneous multimodal domain shift—visual style and angle change dramatically, whereas audio rhythm and energy vary much less—so Wang proposes aligning visual concepts across domains in a **unified audio semantic space**.

The proposed framework has three components. First, an **audio‑visual meta‑mapper** clusters thousands of Flickr audio clips with k‑means, creating audio “clusters” that serve as a shared semantic space. Learnable visual‑prefix tokens map visual features into this space, and the model is trained to reconstruct audio from visual queries, encouraging semantic alignment. Second, a **transformer‑based encoder‑generator** with an adaptive weight α estimates each modality’s contribution to every generated word. Third, **Dual Counterfactual Contrastive Learning (DCLL)** directly optimizes visual‑to‑text alignment by generating fine‑grained counterfactual supervision, avoiding reliance on random negatives.

Training follows a MAML‑style meta‑learning protocol: multiple source domains form a support set for meta‑training, while a held‑out domain serves as the query set. In meta‑testing, the model adapts to a novel target domain using only a few examples. Experiments on MSVD and MSR‑VTT benchmarks (cross‑dataset and cross‑domain settings) show that TAVT outperforms state‑of‑the‑art RNN and transformer baselines by a large margin, especially in low‑resource scenarios such as the Kids and Beauty domains. Ablation studies confirm the critical role of audio features in boosting performance.</sample>
    <sample id="265">The speaker’s name is **Vasudha**.</sample>
    <sample id="266">The abstract you provided does not mention any institutional affiliations for the authors.</sample>
    <sample id="268">The most frequent mistake Pa LM makes is **omission** – it tends to drop or omit parts of the source sentence, producing a fluent but sometimes incomplete translation.</sample>
    <sample id="270">The authors are affiliated with **Emory University (Emory NLP Lab)** and **Amazon Alexa AI**.</sample>
    <sample id="271">CFT stands for **Continuous Fine‑Tuning**. It refers to the practice of taking the model trained on weak labels and then continuing (fine‑tuning) it on the small clean validation set to improve performance.</sample>
    <sample id="272">The paper lists seven authors.</sample>
    <sample id="274">The speaker’s name is **Yusen Zhang**.</sample>
    <sample id="276">**IndicMT‑Eval: Meta‑Evaluation of MT Metrics for Indian Languages**  
Ananya &amp; Vignesh present a new dataset for meta‑evaluating machine‑translation (MT) metrics on Indian languages. They focus on five languages (Tamil, Malayalam, Hindi, Marathi, Gujarati) drawn from the Flores corpus, sampling 200 source sentences per language. Seven MT systems (including NLLB, IndicTrans, Google, Bing, mT5, CVID, mBART) generate 1,400 candidate English translations per language, yielding 7,000 outputs.  

Bilingual expert annotators use an MQM‑style interface to mark each error’s type (accuracy, fluency, special) and severity, and provide an overall quality score. The dataset captures rich, fine‑grained human judgments.  

Correlation analysis shows overlap‑based metrics (chrF) have the highest but still weak correlations; embedding‑based metrics (LabSE, multilingual BERT‑based BERTScore) improve further, while COMET variants achieve the best overall Pearson/Kendall correlations across all languages. Metrics often exhibit narrow, skewed score ranges, complicating interpretation.  

The authors fine‑tune COMET on their MQM data, creating IndicCOMET variants. IndicCOMET MQM outperforms baseline COMET on three of five languages and is superior overall. Zero‑shot tests (training on four languages, testing on a fifth) confirm IndicCOMET’s cross‑lingual robustness, achieving higher correlations than COMET baselines (e.g., 0.36 vs. 0.272 on ACES translation accuracy sets).  

The dataset and IndicCOMET models are released publicly for further research.</sample>
    <sample id="277">The method is called **Multiset Tagging and Latent Permutations**.</sample>
    <sample id="278">The “Marked Words” method is a sociolinguistic approach that treats dominant (unmarked) groups as the linguistic default and identifies words that *mark* minority or marginalized groups. The authors first label which groups are marked and which are unmarked (e.g., black women vs. white men), then apply a Fightin’ Words analysis—weighted log‑odds ratios—to persona texts to extract the top words that distinguish each marked group from the unmarked ones. These marked words reveal the essentializing stereotypes embedded in the model’s output.</sample>
    <sample id="279">The authors are affiliated with the University of Washington.</sample>
    <sample id="280">**MultiEMO: Attention‑based, Correlation‑aware Multimodal Fusion for Emotion Recognition in Conversations**  
Shi Tao presents MultiEMO, a framework that jointly models textual, audio, and visual cues to predict utterance‑level emotions. Existing emotion‑regulation‑in‑conversations (ERC) methods mainly focus on speaker and context modeling, often ignoring multimodal complementarity, minority‑class performance, and the difficulty of separating semantically similar emotions.  

**Key contributions**  
1. **VisExtNet** – a visual extractor that uses MTCNN + VGGFace2 ResNet‑101 to capture facial expressions across frames while discarding redundant scene information.  
2. **MultiAttn** – a multimodal fusion network comprising three sub‑modules (text, audio, visual). Each sub‑module applies stacked bidirectional multi‑head cross‑attention: first fusing text with audio, then the resulting representation with visual cues, followed by a feed‑forward memory layer and residual connections.  
3. **Sample‑Weighted Focal Contrastive Loss (SWFC)** – assigns higher weight to minority classes (α) and a focusing term (γ) that enlarges inter‑class distances, improving discrimination of semantically similar emotions.  

**Results** – On MELD and IEMOCAP, MultiEMO outperforms state‑of‑the‑art baselines, notably boosting accuracy for minority and hard‑to‑distinguish emotion classes (see Tables 1‑2).  

**Limitations** – VisExtNet cannot separate speakers from background individuals; SWFC demands large batch sizes; minority‑class performance remains below majority classes. The work demonstrates that attentive, correlation‑aware fusion significantly advances ERC performance.</sample>
    <sample id="281">**Summary (≈200 words)**  

This work, “When Does Translation Require Context? A Data‑driven, Multilingual Exploration,” investigates when document‑level context is essential for accurate machine translation. Using English‑to‑14‑language TED‑talk transcripts, the authors extend their earlier **CXMI** metric to **Pointwise CXMI (P‑CXMI)**, quantifying the information a source sentence’s context provides about each target word. High‑P‑CXMI words signal context‑dependency.  

Three analyses are performed:  
1. **POS‑level** – e.g., Arabic dual pronouns (absent in English) and verb‑form selection.  
2. **Vocab‑level** – e.g., consistent Chinese proper‑noun translation and formality choices.  
3. **Token‑level** – e.g., ellipsis resolution that depends on sentence structure.  

From these patterns the authors built the **MuDA tagger**, a multilingual, discourse‑aware tool that automatically flags tokens belonging to five discourse phenomena: dual pronouns, verb form, lexical cohesion, formality, and ellipsis.  

The MuDA benchmark is then used to evaluate MT systems. Corpus‑level metrics (BLEU, COMET, word‑F) give conflicting signals: BLEU favors context‑agnostic models, COMET favors context‑aware ones. MuDA shows context‑aware models excel on formality and lexical cohesion, but offer little gain on ellipsis, pronouns, or verb‑form. Commercial systems are compared; DeepL consistently outperforms Google Translate on document‑level translation.  

In sum, the study provides a data‑driven map of context‑dependency across languages, a diagnostic benchmark, and insights into the current strengths and weaknesses of document‑level MT.</sample>
    <sample id="282">**Abstract**

We introduce **StoryTrans**, a novel framework for non‑parallel style transfer at the story level. Unlike prior work that operates on tokens or sentences, our method targets the transfer of an author’s style across entire narratives, where style is tightly coupled with discourse structure and topic‑specific content. The primary challenge is to disentangle stylistic cues from content while preserving long‑range discourse coherence. StoryTrans learns hierarchical discourse representations from source stories and augments them with learnable style embeddings. A two‑stage training regime first reconstructs the source while enforcing style‑content disentanglement through self‑reconstruction, sentence‑level disentanglement, sentence‑order, and style‑classification losses. In the second stage, style‑specific content tokens that were masked during the first stage are replaced with model‑generated keywords, restoring missing plot elements. We collected parallel collections of fairytales and everyday stories in Chinese and English and evaluated transfer to several target author styles. Automatic metrics and human judgments show that StoryTrans outperforms strong baselines on style control and content preservation, and style‑embedding visualizations confirm alignment with gold standard. Compared to prior models such as StyleLM, StoryTrans produces coherent narratives with minimal unrelated insertions. Our code and datasets are publicly released.</sample>
    <sample id="283">Hudson’s Word Grammar.</sample>
    <sample id="284">Peng Tianshuo presents **FSUIE**, a span‑based Universal Information Extraction (UIE) framework that introduces fuzzy span mechanisms to address two key limitations of conventional span models. First, boundary ambiguity: gold spans often have multiple acceptable boundaries, so the model should learn a *fuzzy* rather than a hard boundary. FSUIE defines a continuous probability distribution over span limits, bounded by \(R_{\min}\) and \(R_{\max}\), and uses a sampling function to generate discrete loss targets. The loss combines Binary Cross‑Entropy with the gold span and a KL‑divergence term that aligns the predicted fuzzy distribution with supplementary information. Second, transformer attention mismatch: standard attention is static and global, ignoring the fact that useful span information is locally concentrated. FSUIE introduces **Fuzzy Span Attention (FSA)**, a mask function \(G\) with a learnable width parameter \(\delta\) that dynamically limits attention to a local window and applies a linear decay at the window boundaries. This mask is applied only at the top encoder layer, preserving the underlying text representation.

Experiments on NER, relation extraction (ACE2004/2005, ADE), and ASTE (AST‑V2) show that the **FSUIE‑base** model surpasses baseline UIE and sets new state‑of‑the‑art results, especially on small datasets. Ablation studies confirm that FSA accelerates convergence and FSL (fuzzy span loss) enhances extraction capability, with their combination yielding the strongest gains. Attention visualizations reveal that the module focuses on semantically relevant tokens within a limited context, validating the design. Overall, FSUIE offers a unified, efficient, and generalizable approach to IE tasks.</sample>
    <sample id="285">Mingqi Gao of Peking University presented “Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine‑grained Evaluation Framework.” The talk highlighted that many generated and even reference summaries still contain factual errors, and that two main approaches exist: (1) embedding factuality objectives during training or decoding, and (2) using a separate Factual Error Correction (FEC) model that refines a model’s summary. Gao argued that no work has yet addressed factual errors in dialogue summarization, and that existing FEC evaluations are flawed. Current practices rely on automatic factuality metrics (FactCC, DAE) which produce vague overall scores and allow a model to simply generate a new, more factual summary without actually correcting the original.  

To remedy this, the authors propose manual, reference‑based corrections that enforce minimal edit operations (substitution, insertion, deletion) while preserving fluency and non‑redundancy. They introduce a new taxonomy of factual errors—content‑based (dependent on part‑of‑speech or syntax) and form‑based (type of edit operation). Building on the ERRANT framework for grammar error correction, they align, classify, and compare errors to enable fine‑grained evaluation. Experiments show that training FEC models on reference‑corrected dialogue data yields the best scores according to unreliable factuality metrics, underscoring the need to revise evaluation methods. Combining human‑annotated and synthetic data improves performance, yet current models still struggle with addition, attribute, modality, and link errors. The study calls for a shift toward reference‑based evaluation and richer training data for FEC in dialogue summarization.</sample>
    <sample id="286">The speaker is **James Finch**.</sample>
    <sample id="287">There are four authors involved in the paper.</sample>
    <sample id="288">The paper highlights several benchmark datasets that are commonly used to probe syntactic phenomena in language models:

| Dataset | Focus / Example Phenomena |
|---------|---------------------------|
| **BLiMP** (Benchmark of Linguistic Minimal Pairs) | Minimal‑pair tests for a wide range of syntactic constructions (e.g., adjunct islands, wh‑movement, agreement, etc.). |
| **SyntaxGym** | A large collection of syntactic minimal‑pair tests, each linked to a specific grammatical rule or phenomenon. |
| **CrowS‑Pairs** | Minimal‑pair datasets that also capture sociolinguistic and stereotypical acceptability judgments. |
| **Wikipedia (unrelated text)** | Used as a control to test whether context from unrelated domains influences acceptability judgments. |

These datasets provide the structured, annotated sentences needed for minimal‑pair paradigms and allow researchers to systematically evaluate how language models handle different syntactic constructions.</sample>
    <sample id="290">The five methods examined for the first research question are:

1. **FTw** – the vanilla fine‑tuning baseline  
2. **COSINE** – the cosine‑similarity based weak‑label method  
3. **PCT** – Pseudo‑labeling with Confidence Thresholding  
4. **CWS** – Clean‑Weak‑Supervision  
5. **FWL** – Fully‑Weak‑Learning  

These are the abbreviations used in the study to refer to the different weak‑supervision techniques.</sample>
    <sample id="291">The seven models are evaluated on a set of 11 biomedical/clinical downstream tasks, which include:

- **Named‑entity recognition (NER)**
- **Text classification** (e.g., disease or outcome classification)
- **Part‑of‑speech (POS) tagging**
- **Question answering (QA)**

These four task families cover the 11 specific evaluation datasets used in the study.</sample>
    <sample id="294">CamemBERT was originally pre‑trained on the **OSCAR** French corpus – the large web‑crawled dataset (the full version is about 138 GB of French text).</sample>
    <sample id="295">The speaker is **Adam Przepiórkowski**.</sample>
    <sample id="296">Valerio Basile presents a collaboration between the University of Turin and Amazon Alexa that focuses on irony detection in natural language. He explains that traditional supervised learning assumes a single “ground truth,” but this assumption is limited, especially for subtle, pragmatic phenomena like irony. To investigate these limitations, the team created the EPIC (English Perspectivist Irony Corpus) by collecting ~300 short conversational exchanges from Reddit and Twitter across five English varieties over 1½ years. They recruited 74 annotators via Prolific, each rating 200 exchanges for irony with a simple “Ironic / Not ironic” interface, and used attention checks for quality. The average inter‑annotator agreement varied across groups (gender, age, nationality), revealing systematic differences. Basile then describes “perspective‑aware” models: fine‑tuned language models trained on splits of data labeled by specific annotator groups. While raw performance didn’t differ dramatically, these models exhibited higher confidence and less uncertainty compared to models trained on aggregated gold standard labels. Further analysis showed that adjacent age cohorts and geographic groups (e.g., UK vs. Ireland) produced the most divergent annotations. The presentation concludes by inviting questions and discussions at the poster session.</sample>
    <sample id="297">**From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models**

This research investigates how political speakers embed hidden, hateful messages—known as dogwhistles—to influence audiences while maintaining plausible deniability. Dogwhistles convey a primary public meaning to an out‑group and an additional, often taboo, meaning to an in‑group (e.g., “cosmopolitan” as an anti‑Jewish cue). The study develops a comprehensive glossary of 340+ English terms, enriched with contextual data on persona (e.g., anti‑Semitic, transphobic), register (formal vs. informal), and type (covert implicature vs. overt signaling). A typology framework classifies dogwhistles along these dimensions.

A historical case study of U.S. Congressional speeches shows a rise in racial dogwhistles post‑Civil Rights era, aligning with the Republican Southern Strategy and increasing conservatism. The authors then evaluate GPT‑3’s ability to surface and interpret dogwhistles. The model reliably generates formal‑register dogwhistles, yet struggles with informal or transphobic cues; performance improves with explicit prompts that include definitions or “secret cues.” Finally, the research demonstrates that dogwhistles can evade content moderation: toxicity scores for hateful sentences drop when slurs are replaced with coded terms, even though the underlying meaning remains unchanged.

Overall, the paper offers a structured typology, a richly annotated glossary, empirical evidence of dogwhistle prevalence, and insights into how language models can both reveal and miss these covert signals—highlighting the need for better moderation tools.</sample>
    <sample id="298">The authors ruled out adaptive overfitting by showing that the improvement curve on the new CoNLL++ data had a slope &gt; 1 – meaning gains on the old CoNLL‑2003 test set translate into *larger* gains on the newer data, so there is no diminishing‑return signal of overfitting.  
In contrast, when they kept the same models and continued pre‑training or fine‑tuning them on progressively more recent Reuters data, they observed a clear drop in performance as the temporal gap between the training data and the 2020‑era CoNLL++ test set grew. This systematic decline with increasing recency gap confirmed that **temporal drift is the main driver of the performance loss**.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos present “Improving the robustness of NLI models with minimax training.” Current NLI systems achieve state‑of‑the‑art results but often learn **shortcuts**—spurious correlations such as high word overlap in the MNLI dataset—that make them brittle on out‑of‑distribution (OOD) adversarial tests. Existing mitigation methods rely on a pre‑trained auxiliary model that explicitly exploits shortcuts and re‑weights training data, but they require domain‑specific knowledge, assume the learner will use the same shortcuts, and add computational overhead.

The authors propose a **minimax training framework** that eliminates these assumptions. A lightweight feed‑forward auxiliary network learns to assign **importance weights** to training examples, maximizing the learner’s loss. The learner, in turn, minimizes its loss, which drives it to focus on **hard, under‑represented examples** that counteract shortcut patterns. Training alternates between the two models using standard optimizers. At test time, only the learner is used.

The method is evaluated on MNLI, FEVER, and QQP, with corresponding OOD benchmarks (HANS Symmetric, PAWS). It consistently outperforms empirical risk minimization and the best existing shortcut‑mitigation baselines, while preserving in‑distribution accuracy. Additional experiments examine transfer to larger models, synthetic shortcuts, and cross‑domain OOD sets, study the impact of learner pre‑training, auxiliary size, and provide a qualitative analysis of the learned weight distribution. The poster invites discussion of these findings.</sample>
    <sample id="300">**Summary (≈200 words)**

The presentation introduces *interactive dictation*, a new task that allows users to speak both to dictate text and to issue natural‑language edits to a document in real time. Unlike traditional speech‑to‑text tools, which only capture dictation, interactive dictation supports continuous interleaving of dictation and editing commands without a fixed trigger word. Users can correct themselves mid‑utterance (“on Friday the 23rd”) or issue commands such as “Replace ‘the event’ with ‘it’,” and the system should recognize the intent and apply the change immediately.

The work formalizes the process into four steps: (1) ASR converts raw audio to a transcript; (2) the transcript is segmented into dictation and command utterances; (3) commands are extracted, normalized, and any ASR errors are repaired; and (4) the sequence of dictation and commands is executed to produce the final document. To train models for these steps, the authors designed a data‑collection interface where annotators type or speak commands and dictations, allowing the system to learn from realistic interactions. They collected a dataset of such trajectories and released the code.

For evaluation, separate models were trained for each step. Segmentation proved accurate and fast. The interpretation module was tested with T5 and GPT‑3, exploring two output styles: generating executable programs versus directly predicting the next document state. GPT‑3 achieved higher accuracy but slower runtime; T5’s program‑prediction approach improved efficiency with minimal loss in accuracy. The authors conclude that while baseline performance is promising, substantial room remains for future research.</sample>
    <sample id="302">The first step only tells us *which* output tokens are needed (an unordered multiset). It does not give their sequence. To produce a valid logical form or sentence we must arrange those tokens in the correct left‑to‑right order, so a second “permutation” step is required to order the multiset into the final output sequence.</sample>
    <sample id="303">The authors urged greater transparency so that researchers can determine **where** and **why** those positive‑stereotype patterns arise. Without clear visibility into how a model’s bias‑mitigation pipeline is designed and tuned, it’s impossible to tell whether the stereotypes stem from alignment tricks, anti‑stereotyping techniques, or other factors—making it hard to assess, reproduce, or improve mitigation strategies.</sample>
    <sample id="304">**Minimal‑pair unacceptable inputs** are the *ungrammatical* or otherwise *unacceptable* sentences that are paired with an acceptable counterpart in a minimal‑pair test.  In such a pair, the two sentences differ only in a single lexical, syntactic, or pragmatic feature that makes one sentence acceptable and the other unacceptable; the latter is the “minimal‑pair unacceptable input.”</sample>
    <sample id="305">In this talk, Dawei from Saarland University presents their paper “Weaker Than You Think: A Critical Look at Weakly Supervised Learning.” He begins by explaining weak supervision—using inexpensive, noisy labeling sources such as heuristic rules, knowledge bases, or low‑quality crowdsourcing—in contrast to manual, high‑quality annotations. Training neural nets directly on such weak labels typically leads to memorization of noise and poor generalization. Weakly supervised learning (WSL) methods aim to mitigate this by robustly training models under label noise, usually claiming high performance on clean test sets while only using weakly labeled data. However, these claims often assume access to a clean validation set for model selection, an assumption that is rarely acknowledged.

Dawei outlines three research questions: (1) Is clean validation data actually necessary, or can a noisy validation set suffice? (2) If clean data is required, how many samples are needed? (3) Should clean samples be used only for validation, or can they be exploited better?

Their findings show that recent WSL methods do indeed need clean validation samples; without them, performance drops dramatically. Adding as few as 20 clean samples per class improves results, and with 10–20 samples per class, simple fine‑tuning on the clean data outperforms more complex WSL techniques. Continuous fine‑tuning on clean data can match or exceed sophisticated WSL baselines, suggesting that the claimed gains from WSL are often overstated.

In conclusion, Dawei recommends that future work explicitly report validation strategies, compare against few‑shot learning baselines, and consider continuous fine‑tuning as a strong baseline. The authors have also released their code via a QR code.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim discuss their study on “Entity Tracking in Language Models,” a crucial skill for understanding extended discourse. They argue that a model must keep track of which entities appear and how their states change, citing a recipe example where ingredients move from a bowl to a batter. The researchers point out three main pitfalls when evaluating entity‑tracking: (1) pre‑training data may already encode common state patterns, (2) simple word‑level heuristics can predict states without true tracking, and (3) fine‑tuning or in‑context demonstrations can lead to memorization or slot‑filling shortcuts. To avoid these, they designed a box‑and‑object task: the prompt lists initial box contents, then a sequence of operations (e.g., moving objects). The model must predict the final contents of each box. They tested Flan‑T5 and GPT‑3/3.5 with 2‑shot in‑context learning. Results show that most models simply copy the initial state, achieving high accuracy only when the final state matches the start. Only text‑davinci‑003 and the GPT‑3.5 series (trained on substantial code) exhibit non‑trivial tracking, suggesting code pre‑training fosters this ability. Fine‑tuning a T5‑base can teach tracking, but randomly initialized T5 cannot, underscoring pre‑training’s role. The authors note limited generalization beyond their setup and invite readers to view full results, including GPT‑4 experiments, on their arXiv paper.</sample>
    <sample id="307">The paper evaluates each model on the 11 downstream tasks using the *standard* metrics that are customary for those tasks:

| Task | Typical metric(s) used |
|------|------------------------|
| **Named‑entity recognition (NER)** | Micro‑averaged **F1‑score** (sometimes macro‑averaged F1 as a secondary measure) |
| **Sequence / sentence classification** | **Accuracy** (sometimes macro‑averaged F1 for multi‑class cases) |
| **Part‑of‑speech (POS) tagging** | **Token‑level accuracy** (often reported as overall accuracy or per‑tag precision/recall) |
| **Question answering (QA)** | **Exact‑match (EM)** and **F1‑score** on the answer span (the usual GLUE / SQuAD‑style metrics) |

The authors report these metrics for each of the seven pre‑trained models and compare them to six baseline models. In short, they rely on the conventional precision/recall‑based F1 for NER, accuracy for classification and POS, and EM/F1 for QA to quantify performance improvements.</sample>
    <sample id="308">**Summary**

Jenny, a first‑year PhD student at CMU, presented the NLPositionality framework, developed with colleagues from the University of Washington and the Allen Institute for AI. The work investigates how NLP datasets and models reflect the perspectives (positionality) of the people who created and use them. By re‑annotating existing corpora with a diverse, demographically rich crowd sourced via Lab in the Wild, the authors compare these annotations to model predictions and dataset labels using Pearson’s R. Two tasks were studied: (1) *Social Acceptability* (situations from Social Chemistry) and (2) *Hate‑Speech Detection* (instances from Dynahate). Over 16,000 annotations from more than 1,000 annotators across 87 countries were collected.

Key findings show clear positionality: models and datasets align best with English‑speaking, college‑educated users, while non‑binary annotators are under‑represented. GPT‑4’s predictions, for instance, match closely with users from Confucian and English‑speaking backgrounds. The study underscores that NLP systems inherit the demographic biases of their creators and users.

Recommendations include: (1) maintaining a detailed record of all design choices, (2) adopting a perspectivist lens in research, and (3) building specialised datasets for under‑served communities, exemplified by the Masakhani initiative. The presentation concludes with a call for more inclusive NLP and invites viewers to explore the authors’ dashboard and paper.</sample>
    <sample id="309">The transcript itself does not name a specific statistic – it simply says that the ABC‑Eval labels “were overall more reliable… as measured by inter‑annotator agreement.” In the original paper the authors report using **Cohen’s Kappa** (computed on the 100 doubly‑labeled conversations) to quantify that agreement.</sample>
    <sample id="310">They added unrelated sentences from **Wikipedia**.</sample>
    <sample id="311">Regina Stodden is affiliated with the University of Stuttgart (Germany), while Omar is affiliated with the German Research Center for Artificial Intelligence (DFKI).</sample>
    <sample id="312">**MultiInstruct sets itself apart in three key ways:**

1. **Multimodal, Instruction‑Tuned Scope** – Unlike most existing benchmarks that focus on language‑only or task‑specific vision tasks, MultiInstruct is built explicitly for instruction‑tuned multi‑modal models. All tasks are framed as sequence‑to‑sequence problems that integrate text, images, and bounding‑box coordinates into a single token vocabulary.

2. **Large, Diverse Task Set with Rich Instruction Variants** – It contains 62 distinct tasks spanning 10 broad categories, pulled from 21 open‑source datasets. Each task comes with **five expert‑crafted instruction templates**, enabling systematic study of instruction sensitivity and robustness—something not available in previous multi‑modal benchmarks.

3. **Novel Evaluation Insight** – In addition to standard accuracy/ROUGE‑L metrics, MultiInstruct introduces a **sensitivity metric** that quantifies how consistently a model follows different phrasings of the same instruction, providing a new lens on generalisation to unseen tasks.

In short, MultiInstruct is the first public, large‑scale, instruction‑tuned multi‑modal benchmark that combines diverse tasks, multiple instruction templates, and a new sensitivity evaluation, filling a gap left by earlier language‑only or single‑task vision datasets.</sample>
    <sample id="313">Three authors are involved in the paper.</sample>
    <sample id="314">**Binary coordination** is the linguistic construction that joins exactly **two** constituents (e.g., nouns, verbs, clauses) with a coordinating conjunction (such as *and*, *or*, *but*). It is the simplest form of coordination, forming a single coordinated phrase or clause that contains two conjuncts linked by a conjunction.</sample>
    <sample id="315">The prompts used in the study were very short – on average only about **8 words** long.</sample>
    <sample id="316">The key takeaway is that a modest‑sized model like T5 can actually outperform many large language models once it is fine‑tuned on a high‑quality, task‑specific dataset such as CoScript. This shows that:

* **Model size is not the sole determinant of performance** – with the right data, a smaller, specialized model can achieve, or even exceed, the accuracy of larger, more generic models.
* **High‑quality, constrained‑planning data is a critical resource** – the CoScript dataset provides the precise signals needed for the model to learn faithfulness to constraints, which is harder for generic LMs to capture.
* **Deployment becomes more efficient and cost‑effective** – smaller models require less compute, memory and inference latency, making them practical for real‑world applications that need reliable constrained planning.
* **Future research can focus on dataset engineering and distillation** – rather than scaling up models, investing in curated, domain‑specific data can yield significant performance gains.

In short, the findings highlight that careful data preparation and fine‑tuning can unlock strong performance in lightweight models, offering a promising path for scalable, efficient language planning systems.</sample>
    <sample id="317">**Summary**

Peng Li from Fudan University presents *CodeIE*, a novel approach that reframes classic information‑extraction (IE) tasks—such as named‑entity recognition (NER) and relation extraction (RE)—as structured code‑generation problems. Traditional IE models (e.g., T5, GPT‑3) learn a text‑to‑text mapping during pre‑training, but during inference they must linearize structured outputs, leading to a mismatch that hampers accurate structure generation. CodeIE circumvents this by converting the input text into a structured representation and then prompting a code‑trained large language model (LLM) such as Codex to produce code that directly builds the desired output structure. For NER, a prompt defines a function that extracts entities, while for RE a similar code template is used. The authors evaluate CodeIE on three NER and four RE datasets, comparing against T5, UIE, GPT‑3 (text‑davinci‑002), and Codex (code‑davinci‑002). Results show that the code‑style prompts with Codex consistently outperform text‑style prompts and baseline models, especially in few‑shot settings. Detailed analysis reveals lower perplexity for code‑format inputs, fewer structural errors, and fewer out‑of‑vocabulary labels. Overall, CodeIE demonstrates that aligning the training and inference formats via code generation yields superior IE performance.</sample>
    <sample id="319">The paper explores two main learning strategies for building a French biomedical language model:

| Strategy | What it means in this work |
|----------|---------------------------|
| **From‑scratch pre‑training** | Training a model’s weights from random initialization on domain‑specific corpora (e.g., 7 GB or 4 GB of NACHOS, 4 GB of clinical notes, or a 4 GB mix of both). |
| **Continual (continued) pre‑training** | Taking an existing pre‑trained model (CamemBERT or PubMedBERT) and further pre‑training it on a French biomedical subset (4 GB of NACHOS or clinical notes). |

Additionally, the study varies the **size and type of training data** (web‑crawled NACHOS, clinical notes, or a mix) to see how data quantity and domain relevance affect downstream performance.</sample>
    <sample id="320">In our experiments the effect of re‑using the CoNLL‑2003 test set was essentially **zero**.  
The regression line for the “adaptive overfitting” plot had a slope &gt; 1, meaning that every gain on the old test set translated into an even larger gain on the new CoNLL++ data. In other words, there was no sign of diminishing returns or inflated performance from test‑set reuse – the overfitting factor was negligible (close to 0).</sample>
    <sample id="321">The paper evaluated simplification quality with *automatic* metrics that are standard in the simplification community.  
- For the sentence‑level experiments the fine‑tuned mBART and the baseline models were compared to the reference plain‑language sentences using **BLEU, SARI, FKGL (Flesch‑Kincaid Grade Level)** and related scores.  
- For the document‑level experiments the same set of metrics was applied to the long‑mBART outputs against the manually aligned document pairs.  

The results were reported in the paper (checkpoints are provided) and the authors noted that the fine‑tuned models achieved higher scores than the baselines, establishing a benchmark for future German‑text‑simplification work. No explicit human‑judgment studies were mentioned in the presentation.</sample>
    <sample id="322">Enrico introduces the talk by defining morality as the internal compass that distinguishes right from wrong and stressing that moral judgments are highly subjective. He argues that treating morality as a single “immoral‑to‑moral” scale ignores the pluralistic nature of human values. To capture this nuance, he cites Moral Foundation Theory (MFT), which posits five independent moral dimensions (e.g., fairness, authority, harm) that people prioritize differently.  

Recent NLP work has begun to model morality, but most systems still use a single, averaged moral label. Enrico’s paper investigates whether language models can learn the richer, domain‑specific moral signals that MFT predicts. Using the Moral Foundation Twitter Corpus (≈35 k tweets across seven hashtag domains such as #AllLivesMatter and #BlackLivesMatter), he applies explainable‑AI techniques to probe how a model distinguishes moral foundations in different contexts.  

A key example shows the model learns that “subversion” is framed negatively in ALM (words like “overthrow,” “mayhem”) but positively in BLM, indicating it captures domain‑dependent moral rhetoric. The findings warn that a single, generic moral classifier risks misinterpretation when applied cross‑domain, and highlight the need for domain‑aware moral understanding in language models. The talk concludes with a call for more nuanced, explainable approaches to moral classification and an invitation to discuss the results at ACL‑23 in Toronto.</sample>
    <sample id="323">**Dynamic Heterogeneous‑Graph Reasoning with Language Models and Knowledge Representation Learning (DHLK)** addresses the challenge of Commonsense QA, where systems must retrieve and reason over external knowledge. Existing methods retrieve subgraphs from knowledge bases (KBs) via entity matching, often introducing noisy nodes (e.g., “Top”, “Bank”, “Cat”) and encode text and graph separately, overlooking semantic links between entities.

DHLK first constructs a **Heterogeneous Knowledge Graph (HKG)** by pruning two‑stage and applying Knowledge Representation Learning (KRL). Subword phrases are removed using a dictionary, and paraphrases from WordNet and Wiktionary are added as new nodes, enriching the subgraph. RoBERTa with Mask Self‑Attention jointly encodes QA context and entities; attention weights dynamically prune weakly relevant nodes (e.g., “wood”). Initial entity/relation embeddings are obtained by mean‑pooling, then refined with **TransE**.

Instead of a traditional GNN, DHLK employs **Relation Mask Self‑Attention (RMSA)**—a variant of RGAT that explicitly incorporates relation types. Through L layers of RMSA, entity and relation embeddings are iteratively updated, and the final graph embedding is derived by max‑pooling the question’s key entities.

Path information from the HKG is fused back into the QA context, yielding a path‑enhanced context embedding. All representations (graph embedding, path info, context embedding) are fed into an MLP to predict answer probabilities.

Experiments on **CommonsenseQA** and **OpenBookQA** using ConceptNet, WordNet, and Wiktionary demonstrate that DHLK outperforms prior language‑model‑based and HKG methods, achieving top leaderboard positions.</sample>
    <sample id="324">**Short answer:**  
Yes – language models vary in their political leanings, and these differences are measurable, traceable to training data, and have tangible effects on downstream tasks.

**Key points**

| What you found | How it was shown | Effect on tasks |
|----------------|------------------|-----------------|
| **Models differ in political orientation** | Prompted with political–science questionnaires (e.g., political conference test) and plotted on a 2‑D ideological space. | Models occupy all four quadrants; GPT‑4 is the most liberal, GPT series lean left, BART‑based models lean right. |
| **Biases can be shifted by training data** | Further pre‑training on partisan corpora (news or social media, left vs. right) changes the model’s ideological coordinates accordingly (e.g., RoBERTa + left‑leaning Reddit → liberal shift). | Shows a causal link between data and bias. |
| **Societal polarization is reflected** | Models pre‑trained on texts before vs. after the 45th U.S. president diverge farther from the center after 2017. | Models encode contemporary polarization. |
| **Downstream fairness is impacted** | Hate‑speech and fake‑news classifiers built with left‑ vs. right‑leaning models show asymmetric performance across demographic and political categories. | Left‑leaning models better flag minority‑targeted hate, right‑leaning models better flag majority‑targeted hate; analogous patterns for misinformation. |

**Bottom line:** Political bias is not uniform across language models; it depends on the training corpus, can be deliberately tuned, and materially influences the fairness of downstream NLP applications.</sample>
    <sample id="326">Cognitive dissonance is the mental conflict that arises when a person holds two or more contradictory beliefs, attitudes, or engages in actions that do not align with their stated beliefs—e.g., saying “smoking is harmful” while still smoking. This inconsistency creates psychological discomfort and motivates people to resolve the mismatch.</sample>
    <sample id="327">In this presentation, Xiao Xu introduces **ManagerTower**, a new vision‑language (VL) model that improves upon the two‑tower architecture and its recent variant, **BridgeTower**. Traditional VL models (e.g., METER) feed only the final unimodal layer into a cross‑modal encoder, missing richer semantic signals from intermediate layers. BridgeTower alleviates this by connecting multiple unimodal layers to each cross‑modal layer, but it still assigns a single, fixed unimodal layer to every cross‑modal block, limiting flexibility and scalability.

ManagerTower addresses these issues by inserting a **manager** at each cross‑modal layer. Each manager receives *multiple* unimodal representations (from various depths of the pre‑trained visual and textual encoders) and learns to *adaptively aggregate* them. This dynamic weighting allows the model to exploit different semantic levels as needed in each cross‑modal stage, yielding a more comprehensive fusion of vision and language cues.

Using RoBERTa for text and CLIP‑ViT‑Base for images, ManagerTower is trained on only 4 M image‑text pairs yet outperforms both METER and BridgeTower across several benchmarks, notably achieving 39.15 % accuracy on the Wikivideo test set. Visualizations of manager weights on VQA‑v2 show that static managers exhibit uniform, progressive patterns, whereas adaptive managers learn distinct, layer‑specific distributions that differ between visual and textual modalities. The work demonstrates that well‑designed managers can more effectively harness multi‑level unimodal knowledge, leading to superior cross‑modal alignment and downstream performance.</sample>
    <sample id="328">The most liberal language model reported is **GPT‑4**.</sample>
    <sample id="329">**Abstract**  
Zero‑shot video sentence localization seeks to retrieve temporal segments that best match a natural‑language query without requiring manually annotated training data. Existing zero‑shot approaches generate pseudo‑events and pseudo‑queries, but they suffer from overly simplistic queries, weak alignment between queries and events, and noisy pseudo‑labels that degrade model performance. We propose a noise‑resistant Structured Pseudo‑Label (SPL) framework that addresses these shortcomings. First, we densely sample video frames and employ a pre‑trained image‑caption model (BLIP) to produce rich, free‑form pseudo‑queries for each frame. Second, we compute frame‑query similarities and, via a sliding‑window strategy, identify pseudo‑events that maximize the difference between intra‑event and inter‑event similarity, thereby ensuring high query relevance inside the event and low relevance outside. We retain only the top‑K pseudo‑queries with high event quality and prune overlapping query‑event pairs. Third, during model training we mitigate label noise by weighting each sample according to its predicted confidence and IoU with the pseudo‑label; high‑confidence, high‑IoU predictions are promoted as refined pseudo‑labels for iterative refinement. Experiments on ActivityNet Captions and Charades‑STA demonstrate that SPL outperforms prior zero‑shot methods on Recall@M and mean IoU, achieving state‑of‑the‑art results while remaining fully annotation‑free. The code is publicly available.</sample>
    <sample id="330">Yes – in the study cumulative training matched or outperformed the iterative approach. Across all experiments, the cumulative strategy was equal to or better than the iterative one for active‑learning updates.</sample>
    <sample id="331">The speaker is **Sara Papi**.</sample>
    <sample id="332">The MuDA benchmark is built on the TED‑Talk transcript data—English source sentences that have been translated into 14 target languages.</sample>
    <sample id="333">In this presentation, Wenhao from Nanjing University introduces **INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation**. The team—Jingjing Xu, Shujian Huang, Jiajun Chen, and Lingpeng Kong—focuses on enhancing neural machine translation (NMT) by addressing the non‑smooth, sparse representation space that hampers generalization, especially for low‑frequency tokens. They note that **kNN‑MT** mitigates this by smoothing predictions with nearest‑neighbor retrieval from a datastore built on the training corpus, but this approach is slow and inflexible once the datastore is fixed.

INK proposes an iterative training loop that injects kNN knowledge into the model. First, a small adapter is trained to align contextual representations with both token embeddings and kNN‑derived embeddings using KL‑divergence, while also clustering similar target tokens to reduce sparsity. The updated representations are then used to refresh the datastore asynchronously. This loop repeats until convergence, after which the datastore can be discarded at inference, yielding faster, more efficient decoding.

Experiments on WMT’19 German‑English with the winning baseline show that INK surpasses state‑of‑the‑art kNN‑MT, achieving an average gain of 1.99 COMET and 1.0 BLEU, with lower memory usage and faster inference. The study also demonstrates that combining the adapter with a datastore yields further gains, suggesting that smoother representations can be achieved with more sophisticated frameworks.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Cross‑lingual transfer is the process of training a semantic‑parsing model on data in one language (or a subset of languages) and then applying that trained model to a different target language. It can be done in a zero‑shot setting, where no target‑language data is seen during training, or in a few‑shot setting, where a small amount of target‑language data (e.g., 10 % of the training set) is used. The goal is to evaluate how well the model generalizes across languages, measuring the performance gap between monolingual, zero‑shot, and few‑shot transfer scenarios.</sample>
    <sample id="337">**Summary**

The presentation introduces a novel method for learning embeddings of out‑of‑vocabulary (OOV) words by exploiting their internal structure and contextual relationships. The approach builds a *Word Relationship Graph* that captures both word‑formation rules (via tokenization into wordpieces) and associations with related words. Each node in the graph—either a full word or a wordpiece—carries an embedding; the OOV node’s attributes are generated by a self‑attention module over its characters. To mitigate noise from many neighbors, the graph is constructed in two layers: the first preserves all wordpiece nodes, while the second samples a fixed number of related words. Two stacked Graph Attention Networks (GATs) process the graph, concatenating node representations from each layer to produce refined node embeddings. A readout layer aggregates these into a graph‑level representation that reflects the word’s formation. To align this representation with the background embedding space, a contrastive loss (NT‑XENT) is used, with positive samples drawn from two‑hop neighbors, synonyms, or the OOV word itself. Experiments demonstrate that the method outperforms baselines on both intrinsic similarity tasks and extrinsic downstream applications, for both static and contextual models. The authors also discuss extending the approach to other languages, noting that agglutinative languages are naturally suited due to clear morpheme boundaries, while fusional languages pose more challenges.</sample>
    <sample id="338">The talk presents a study on how to objectively evaluate human‑written natural language explanations used to train large language models. The authors note that current metrics (BLEU, ROUGE, simulatability) treat human annotations as gold standards or focus only on inference accuracy, ignoring how explanations help during fine‑tuning and the task‑dependent nature of explanation quality. They therefore propose a unified data format that converts diverse tasks—commonsense QA (CoS‑E, ECQA), NLI (e‑SNLI), and commonsense validation (ComVE)—into a multiple‑choice setting with two modes: baseline (no explanation) and infusion (explanation as extra input). Extensive experiments with T5 and BART show that fine‑tuning on explanations does not add new knowledge but encourages models to rely on the explanation portion, and that small amounts of explanation data can improve performance. Building on these findings, the authors introduce TREU, a new metric that extends simulatability by comparing model performance after fine‑tuning with and without explanations. TREU consistently ranks dataset quality better than simulatability, revealing that explanation helpfulness varies by task and class (e.g., positive for entailment, negative for neutral/contradiction in e‑SNLI). The work offers a unified evaluation framework and a metric that more accurately reflects the utility of human explanations for model training.</sample>
    <sample id="339">All authors of the paper are affiliated with **Saarland University, Germany** (Dawei is a PhD student there, and the co‑authors are faculty or researchers at the same institution).</sample>
    <sample id="340">**ParaAMR: A Large‑Scale, Syntactically Diverse Paraphrase Dataset via AMR Back‑Translation**

Kuan‑Hao Huang and colleagues from UCLA introduce **ParaAMR**, a new paraphrase corpus that leverages Abstract Meaning Representation (AMR) to generate syntactically varied paraphrases. Traditional paraphrase datasets (e.g., MRPC, PAN, Quora) are high‑quality but small; automatically constructed sets via back‑translation are large but tend to preserve the original sentence syntax. ParaAMR addresses this gap by first parsing each source sentence into an AMR graph, then randomly re‑rooting the graph to alter the focus of the meaning representation. The modified graph is decoded back into text with an AMR‑to‑text generator, producing paraphrases that share semantic content but differ in syntactic structure. The resulting corpus contains ~15 million source sentences with an average of 6.9 paraphrases each, offering greater syntactic diversity than existing back‑translation datasets while maintaining comparable semantic similarity.

Quantitative evaluation shows that ParaAMR yields higher syntactic diversity scores without sacrificing meaning. Experiments demonstrate its practical value: sentence embeddings trained on ParaAMR outperform those trained on other corpora on STS benchmarks; paraphrase models fine‑tuned with ParaAMR exhibit stronger syntactic control; and data augmentation with ParaAMR improves few‑shot learning performance. The dataset is publicly released, providing a resource for researchers needing diverse paraphrase examples.</sample>
    <sample id="341">The authors report two latency metrics:  

1. **Average Lagging (AL)** – the standard measure of how far the translation lags behind the source.  
2. **Computational‑Aware Average Lagging (CAAL)** – the same lag measure adjusted for the actual computational time needed to generate the output.</sample>
    <sample id="342">**Summary**

Gao Jingsheng presents “LiveChat: A Large‑Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming,” a study from Shanghai Jiao Tong University and Xiaobing.AI. The talk begins by defining open‑domain dialogue—conversations with no fixed goal that rely on pre‑trained models and large corpora. Existing corpora are mainly text‑based; video‑sourced datasets are scarce, often scripted or manually annotated, limiting scale and realism. LiveChat addresses these gaps by automatically constructing a Chinese video‑sourced dialogue set from Douyin (TikTok) live streams. The process involves: (1) scraping raw videos, extracting audio, and transcribing into utterances with ASR; (2) collecting audience comments and linking them to speakers via a reply‑to‑whom matching algorithm; (3) extracting persona information, combining manually labeled basic profiles with rule‑based and classifier‑derived “profile” data. The resulting dataset boasts the largest scale, longest average session length, and rich persona annotations among Chinese open‑domain dialogue corpora.

Experiments evaluate two benchmarks: Response Modeling and Addressee Recognition. Retrieval baselines show that longer sessions and accurate persona extraction improve performance; single‑stream BERT outperforms dual‑stream BERT for addressee recognition, though persona helps both. Transfer learning tests with BART, GPT‑3.5, and GPT‑4 reveal that BART best adapts to LiveChat, indicating a domain shift from existing datasets. In‑context learning experiments demonstrate that LLM performance rises with more demonstrations up to 8 shots, after which noise degrades results. The paper concludes that LiveChat offers a valuable resource for personalized, multi‑party dialogue research and outlines plans for efficient LLM fine‑tuning on the dataset.</sample>
    <sample id="344">Tree‑based approaches have several practical limitations:

1. **Extra preprocessing** – Logical forms must first be converted into parse trees, which often requires formalism‑specific handling of symbols (e.g., variables).  
2. **Tree induction** – When trees are not provided, they must be induced (e.g., via grammar induction), a non‑trivial and costly step.  
3. **Computational overhead** – Building and using these trees adds significant computational complexity compared to flat seq‑2‑seq models.  
4. **Limited flexibility** – The tree structure imposes hard constraints that may not align with every target representation, reducing expressiveness.</sample>
    <sample id="345">**Abstract**  
We address compositional generalization in semantic parsing without relying on explicit tree structures.  In this setting a model is trained on utterance–logical‑form pairs, but the test set contains unseen recursive compositions, e.g., deeper nested quantifiers.  Conventional sequence‑to‑sequence models fail to transfer systematic input‑output correspondences under such distribution shifts.  We propose a two‑stage neural architecture that directly learns the correspondence between fragments of the input and the output.  First, each input token is tagged with an unordered multiset of output tokens that will appear in the target logical form.  This guarantees that all necessary tokens are produced, but their order is unknown.  Second, a permutation model predicts a latent ordering of the multiset tokens.  Unlike prior work, we impose no hard constraints on the permutations; instead we formulate the inference as a continuous relaxation of the traveling‑salesman–style objective, enabling efficient GPU computation and end‑to‑end back‑propagation.  The alignment between input tokens and multiset entries is also learned jointly because the training data provide no explicit alignment.  Experiments on the COGS benchmark show that our treeless model outperforms existing approaches by a large margin on generalization to deeper recursion, while still leaving other structural generalization challenges open.  The proposed framework demonstrates that compositional generalization can be achieved without explicit syntactic trees, relying instead on multiset tagging and latent permutation learning.</sample>
    <sample id="346">The transcript you shared does not include any explicit mention of the authors’ institutional affiliations.  Thus, based on the information provided, we cannot state which universities, research labs, or organizations the authors belong to.</sample>
    <sample id="348">**Abstract**

We introduce *Marked Personas*, a lightweight framework for probing social bias in instruction‑tuned large language models (LLMs). Rather than relying on hand‑crafted datasets, we generate persona descriptions by prompting the model with identity‑specific instructions (e.g., “Imagine you are an Asian woman. Describe yourself.”). These automatically generated texts are compared to human‑written counterparts to assess how well LLMs surface stereotypes across arbitrary demographic markers. To quantify stereotype content without a pre‑defined lexicon, we adopt a sociolinguistic “markedness” approach: we designate dominant (unmarked) and marginalized (marked) groups and compute weighted log‑odds ratios—our *Fightin’ Words* method—to isolate words that disproportionately appear in marked personas. Results show that LLM personas contain substantially more stereotype‑laden language than human texts, with over‑representation of terms such as “culture,” “tradition,” “exotic,” “vibrant,” and “strong.” These words, though often positive, reinforce essentializing narratives (e.g., the “Strong Black Woman” archetype) and perpetuate othering. Our analysis highlights the necessity of an intersectional lens, as biases compound across gender, race, and other identities. We conclude with three recommendations for model developers: (1) systematically evaluate positive stereotypes, (2) adopt intersectional bias assessment frameworks, and (3) enhance transparency of bias‑mitigation strategies. This work underscores the importance of nuanced, scalable methods for uncovering both overt and subtle biases in modern LLMs.</sample>
    <sample id="350">In their talk, Simone Tedeschi and collaborators examine the claim that modern NLU systems achieve “superhuman” performance.  Over the past five years, leaderboard‑based benchmarks have become the dominant evaluation method in NLP; many systems now beat the reported human baselines on popular datasets such as SuperGLUE and SQuAD.  The authors investigate whether these comparisons are meaningful.  They analyze two flagship benchmarks—SuperGLUE, a 10‑task suite for commonsense reasoning, entailment, and reading comprehension, and SQuAD, a reading‑comprehension QA collection.  On SuperGLUE, humans rank 8th overall and are outperformed on six of ten tasks; on SQuAD, state‑of‑the‑art models beat the best human scorers by significant margins.  

However, the authors uncover several flaws that undermine these comparisons.  First, humans are evaluated on a far smaller subset of the test data (e.g., 100 samples for BoolQ versus 3,000 for models).  Second, many test instances contain annotator errors or ambiguous labels, enabling models to exploit spurious correlations that humans cannot.  Third, human baselines are often aggregated from low‑pay workers (as little as \$3.60 / hour), reducing motivation and quality.  Finally, details about annotator demographics, selection procedures, and incentives are routinely omitted, preventing a fair assessment of “best possible” human performance.  

The paper concludes that claims of superhuman NLU performance are not yet scientifically grounded and offers concrete recommendations for constructing more reliable, well‑documented benchmarks.</sample>
    <sample id="351">**Abstract**

We investigate whether the widely‑used CoNLL‑2003 named‑entity recognition (NER) taggers retain their effectiveness in contemporary data. To this end, we constructed *CoNLL++*, a new benchmark derived from Reuters news articles of 2020 and annotated under the exact CoNLL‑2003 guidelines. We fine‑tuned 20 representative models—including classic linear chains, LSTM‑CRFs, and modern transformer‑based architectures—on the original CoNLL‑2003 training set, then evaluated them on both the CoNLL‑2003 test split and on CoNLL++. The percentage change in F1 score served as a metric of generalization.

Our experiments reveal three key factors that consistently improve cross‑temporal performance: (1) **Model architecture**—transformer models outperform older recurrent and linear models; (2) **Model size**—larger parameter counts yield higher robustness; and (3) **Fine‑tuning data volume**—more annotated examples enhance generalization. We also examined two potential causes of performance decay: *adaptive overfitting* (repeated test‑set reuse) and *temporal drift* (distribution shift over time). Analysis of learning curves shows no evidence of diminishing returns indicative of adaptive overfitting, whereas models that were continued‑pretrained on progressively newer data exhibit a clear degradation with increasing temporal gap, confirming temporal drift as the primary culprit.

In summary, CoNLL‑2003 taggers remain viable in 2023 provided that they are built on transformer architectures, scaled appropriately, and exposed to sufficient modern data. Our findings underscore the importance of addressing temporal drift in NER systems and motivate further research into long‑term generalization. The CoNLL++ dataset and all experimental artifacts are publicly released for reproducibility.</sample>
    <sample id="352">ABC‑Eval stands for **“Annotating Behaviors in Chat.”**</sample>
    <sample id="353">The talk introduces a new approach to address the **underspecification problem** in natural‑language‑to‑code generation.  While recent models can translate a user’s description into code, they often miss crucial operational details, leading to buggy or incomplete programs.  The authors argue that **interactive clarification**—asking the user targeted questions—can supply the missing specifications.

To study this, they build a synthetic dataset, **CodeClarQA**, comprising natural‑language descriptions (NLDs), key operations extracted from a code knowledge graph (Graph4Code), and corresponding clarification questions (yes/no or multiple‑choice).  They define a *schema* for each operation and compute similarity scores between the NLD’s schema and the operation’s documentation; if all scores fall below a threshold, the operation is deemed missing.  Empirically, MPNet outperforms other models at detecting missing operations.

The proposed pipeline consists of three modules: a **Clarification Need Predictor** that flags missing ops, a **Question Selector** that ranks candidate clarification questions, and a **Code Generator** that incorporates the user’s answers.  Experiments show that while clarification helps, the end‑to‑end system still lags behind a non‑interactive baseline, highlighting the challenge of CQ ranking.  Error analysis points to taxonomy and argument‑value issues.  Overall, the authors confirm that clarifying key operations improves the quality of the generated code.</sample>
    <sample id="354">The performance drop (ΔF1 &gt; 5 %) persists only up to the year of the new data – i.e., until **2020**. Beyond that point the temporal drift starts to erode the gap.</sample>
    <sample id="356">All three authors are affiliated with the **University of Oxford (United Kingdom)**.</sample>
    <sample id="357">The speaker’s name is **Siyu Yuan**.</sample>
    <sample id="358">The paper has **five authors**: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="359">The approach is compared against a **state‑of‑the‑art dedicated simultaneous Speech‑Translation architecture that relies on pre‑translation**.</sample>
    <sample id="361">**Summary**

Armineh Nourbakhsh presents “CounterComp,” a method for improving compositional generalization in multi‑step quantitative reasoning—specifically, answering questions over financial tables that require several arithmetic operations. Existing neural QA models excel only when the answer involves one or two steps; they struggle with longer reasoning chains because they learn spurious correlations, such as associating a repeated token like “2019” with a particular operation (e.g., subtraction). Rather than adding costly hand‑crafted supervision, the authors exploit counterfactual scenarios inherent in the data: small changes in the question (e.g., swapping “net change” for “percent change”) alter the required operations. For each training example (anchor), they mine positive examples—questions whose intervention does not change the answer—and negative examples—those whose intervention does. These triplets feed a dynamic‑margin metric‑learning loss that scales with the magnitude of the question change. Adding this auxiliary loss to three state‑of‑the‑art baselines boosts performance, especially for reasoning chains longer than two steps. The gains appear both in‑distribution and out‑of‑distribution, addressing the core of compositional generalization. Qualitative analysis shows the model attends to more semantically relevant tokens. The work is supported by CMU’s Language Technologies Institute and JP Morgan AI Research. The poster and contact details are available for further inquiry.</sample>
  </task>
</testset>