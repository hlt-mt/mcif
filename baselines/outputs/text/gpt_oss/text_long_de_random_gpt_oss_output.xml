<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">**Key data sources that dominate current large‑scale language‑model training**

| Category | Typical Corpora / Providers | Notes |
|-----------|-----------------------------|-------|
| **Public web crawl** | *Common Crawl*, *C4* (derived from Common Crawl) | Largest raw‑text source; covers news, blogs, forums, books, code, etc. |
| **News &amp; media** | New York Times, Los Angeles Times, The Guardian, Huffington Post, CNN, Reuters, etc. | Heavy representation in pre‑training data; major source of political content and bias. |
| **Social media** | Reddit (subreddits), Twitter, other public forums | Rich in informal language, slang, and partisan commentary. |
| **Structured encyclopedias &amp; knowledge bases** | Wikipedia, Wikidata | Provide factual knowledge, but also reflect editorial biases. |
| **Books &amp; literature** | Open‑licensed books, Project Gutenberg, book‑crawls | Contribute narrative styles, historical context. |
| **Academic &amp; technical text** | arXiv, PubMed, conference proceedings | Add domain‑specific jargon and formal language. |
| **Other specialized corpora** | Legal documents, government reports, policy briefs | Influence the model on specific policy‑related terminology. |

In practice, most commercial models are trained on a **mixture** of these sources, with the web‑crawl (C4/​Common Crawl) being the backbone, supplemented by curated news, social‑media, and domain‑specific collections. The political bias you observe in models largely stems from the heavy presence of the news and social‑media datasets listed above.</sample>
    <sample id="1">Die Autoren kommen aus einer Kooperation zwischen **McGill University**, dem **Mila‑Institut (Montreal Institute for Learning Algorithms)** und **Microsoft Research**.</sample>
    <sample id="2">**Kurzfassung (≈ 200 Wörter)**  

Die Studie von Tu Yi und seinem Team aus Ant Group präsentiert *LayoutMask*, ein neues vortrainiertes Modell für das Problem der visuell‑reichen Dokumenten‑Verständnis (VRDU). Ziel ist es, Texte, Layout‑Informationen und Layout‑Interaktionen besser zu erfassen, insbesondere die richtige Lesereihenfolge von Dokumenten wie Formularen, Quittungen und Plakaten.  

Aktuelle VRDU‑Modelle verwenden von BERT inspirierte globale 1‑D‑Positionsnummerierungen (0, 1, 2 …) für Tokens. Diese „globalen“ Positionsangaben führen zu Problemen bei der Erkennung der Lesereihenfolge, wenn Abschnitte vertikal, horizontal oder mischausgerichtet sind. LayoutMask löst diese Einschränkung, indem es **lokale 1‑D‑Positionen** nutzt – die Reihenfolge innerhalb eines Segments – und die globale Reihenfolge durch die Kombination von 1‑D‑Position, 2‑D‑Koordinaten und Semantik rekonstruieren lässt.  

Zur Förderung tieferer Text‑Layout‑Interaktionen wurden zwei neue Maskierungsstrategien eingeführt:  
1. **Whole‑Word‑Masking** (WWM): ganze Wörter werden maskiert, nicht einzelne Tokens, wodurch semantische Bezüge innerhalb eines Wortes verschwindet und der Kontext stärker gewichtet wird.  
2. **Layout‑Aware Masking** (LAM): erste und letzte Wörter eines Segments haben höhere Maskierungswahrscheinlichkeit, was die Modell‑Aufmerksamkeit auf übergreifende Segmentgrenzen lenkt.  

Zusätzlich wurde das neue Vortrainingsziel **Masked Position Modeling** (MPM) entwickelt, bei dem zufällig maskierte 2‑D‑Positionen wiederhergestellt werden müssen. Dies zwingt das Modell, sowohl semantische als auch räumliche Zusammenhänge zu nutzen.  

Experimentell übertrifft das lokale 1‑D‑Positionsschema globale 1‑D‑Positionen auf den Datensätzen FUNSD und SROIE, während bei CORD ein kleiner Rückstand besteht – dort spielt die Entität „Total“ eine entscheidende Rolle. Die Ergebnisse zeigen, dass LayoutMask durch die Kombination von lokalem 1‑D‑Positioning, Layout‑Aware Masking und MPM effektivere Layout‑Repräsentationen lernt und dadurch die Leistung bei VRDU‑Aufgaben steigert.</sample>
    <sample id="3">**Eröffnung**

Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Dokumenten‑ und Satz‑Ebene der deutschen Textvereinfachung. Mein Name ist Regina Stodden und ich begleite Sie durch den ersten Teil der Präsentation.

---

### 1. Was ist Textvereinfachung?

Textvereinfachung ist ein Prozess, bei dem ein Text so angepasst wird, dass er für eine bestimmte Zielgruppe besser verständlich wird – etwa Menschen mit Leseschwierigkeiten oder Nicht‑Muttersprachlern. Um ein Modell für die Textvereinfachung zu trainieren, benötigen wir parallel angeordnete Textpaare, beispielsweise Dokumente oder Sätze.  

Im Beispiel sehen Sie ein komplexes deutsches Satz‑Paar, das mit einer Version in einfacher Sprache abgeglichen ist. Um den Satz zu vereinfachen, können verschiedene Techniken eingesetzt werden, wie zum Beispiel:

* Lexikalische Substitution  
* Klausel‑Löschung  
* Umordnung (Reordering)  
* Einfügen von Wörtern

---

### 2. Warum DEPLAIN?

In den letzten Jahren gab es einige Probleme mit bestehenden Korpora für Textvereinfachung:

* Die vorhandenen Korpora sind zu klein, um ein Modell zuverlässig zu trainieren.  
* Die drei neueren Korpora sind automatisch ausgerichtet – das kann zu fehlerhaften Alignments führen.

Daher stellen wir unseren neuen Korpus **DEPLAIN** vor, welcher in zwei Teilkorpora unterteilt ist:

| Teilkorpus | Inhalt | Manuelle Ausrichtung | Automatische Ausrichtung | Anzahl Dokumente | Parallel‑Satzpaare |
|------------|--------|----------------------|--------------------------|------------------|---------------------|
| DEPLAIN‑apa | Nachrichten | 483 | – | 483 | ~13 000 |
| DEPLAIN‑web | Diverse Domains | 750 | 750 | 750 | 30 450 |

---

### 3. Analyse der Satzpaare

Wir haben die Satzpaare genauer untersucht, insbesondere hinsichtlich:

* **Typ der Vereinfachung** – z. B. wird im Bibeltestext deutlich stärker vereinfacht als in Nachrichten oder Lerntexten.  
* **Lexikalische Vereinfachung** – Wortersetzungen, Wortentfernung.  
* **Strukturelle Vereinfachung** – Satzbauänderungen, Vereinfachung von Nebensätzen.  
* **Gesamtvereinfachungsgrad** – Kombination aller Merkmale.

**Varianz der Transformationen**  
* **DEPLAIN‑apa** enthält mehr Umordnungen und Wortadditionen.  
* **DEPLAIN‑web** weist häufiger Rephrasings auf.

---

### 4. Anwendungsfälle – präsentiert von Omar

#### 4.1. Auswertung automatischer Alignments

In den letzten Jahren wurden zahlreiche Alignment‑Methoden entwickelt, vor allem im Kontext der maschinellen Übersetzung (zwei parallele Dokumente in unterschiedlichen Sprachen). Unser Anwendungsfall ist jedoch die Ausrichtung von Sätzen innerhalb zweier paralleler Dokumente **in derselben Sprache**, die unterschiedliche Komplexitätsstufen aufweisen.  

Mit DEPLAIN, das über manuell ausgerichtete Gold‑Standards verfügt, können wir diese Methoden evaluieren. Wir haben die vorgeschlagenen Verfahren angepasst und die Implementierungen sowie die Code‑Beispiele im zugehörigen Paper veröffentlicht.  

**Ergebnis:** Für die deutsche Textvereinfachung ist die Methode **MASSalign** die beste Wahl. Der zugehörige Code ist ebenfalls im Paper verfügbar.

#### 4.2. Automatische Textvereinfachung durch Fine‑Tuning

Wir haben zwei Modelle feinabgestimmt:

| Modell | Ziel | Ausgerichtet auf |
|--------|------|------------------|
| long‑mBART | Dokument‑Ebene | Textvereinfachung |
| mBART‑Base | Satz‑Ebene | Textvereinfachung |

Alle Checkpoints und detaillierte Ergebnisse (Bewertungsmetriken, Scores) sind im Paper verfügbar.  

**Fazit:** Das Grund‑Fine‑Tuning liefert bereits bessere Scores als die Baselines und bildet damit eine Basis‑Benchmark für zukünftige Forschung zur automatischen Textvereinfachung.

---

### 5. Schlusswort

Vielen Dank für Ihre Aufmerksamkeit! Wir freuen uns darauf, Sie an der Konferenz zu treffen.

**Vielen Dank!**</sample>
    <sample id="4">Der Referent heißt **Kayo Yin**.</sample>
    <sample id="5">Die Genauigkeit von 82 – 87 % wurde mit dem **T5‑XL**‑Modell erzielt.</sample>
    <sample id="6">In der vorliegenden Arbeit wird ein neues Summarisation-Setting vorgestellt, das die bisherigen Aufgaben der mehrsprachigen (multilingual) und der kreuzsprachigen (cross‑lingual) Summarisation zusammenführt und erweitert: die **many-to-many Summarisation**. Hierbei soll ein einzelnes Modell Dokumente in beliebiger Ausgangssprache verarbeiten und Zusammenfassungen in beliebiger Zielsprache erzeugen – ein viel flexibleres Szenario als die bisherigen „same‑language“ bzw. „different‑language“ Settings. Um die Vorteile dieser Generalisierung aufzuzeigen, führen die Autoren eine Pilotstudie auf dem WikiLingua‑Datensatz durch, der aus sechs Sprachen (Englisch, Französisch, Hindi, Chinesisch, Thai und Türkisch) besteht. Vier Varianten von mBART‑50 werden verglichen: ein einzeln trainiertes Modell pro Richtung, ein universelles Modell für alle Cross‑Lingual‑Samples, ein Modell für alle monolingualen Samples und schließlich das neue many-to-many Modell. Die Ergebnisse zeigen, dass das many-to-many Modell die Transferfähigkeiten zwischen Sprachen am stärksten verbessert.  

Darauf aufbauend wird das **PISCES**‑Modell vorgestellt, ein speziell für many-to-many Summarisation entwickeltes vortrainiertes System. PISCES verwendet einen dreistufigen Pre‑Training‑Ansatz: 1) Metapretraining (Denoising von Text), 2) Cross‑Lingual Pretraining (Übersetzen von verrauschten Paralleltexten) und 3) Task‑specific Pretraining (Erzeugung von pseudo‑Summaries für viele Sprachpaare). Die Experimente zeigen, dass PISCES die Baseline‑Modelle mBART‑50 und mT5 deutlich übertrifft. Zudem wird die Wirksamkeit jeder Trainingsstufe durch Ablationsstudien belegt, während eine Menschen‑Studie die Qualität der generierten Zusammenfassungen weiter unterstreicht. Das Paper bietet damit sowohl eine theoretische als auch eine praktische Basis für die breite Nutzung von many-to-many Summarisation.</sample>
    <sample id="7">**Antwort:**  
Ja – CoNLL‑2003‑Tagger funktionieren noch, besonders Transformer‑basierte Modelle. Ihre Leistung sinkt jedoch mit zunehmender Zeitdifferenz zwischen Trainings‑ und Testdaten (temporal drift). Größere Modelle, moderne Architekturen und mehr Fine‑Tuning‑Beispiele verbessern die Generalisierung erheblich.</sample>
    <sample id="8">Die neue Methode (ABC‑Eval) unterscheidet sich dadurch, dass menschliche Prüfer nicht mehr nur globale Likert‑Skalen oder Paarvergleiche vornehmen, sondern **explizit jede Antwort auf das Vorhandensein bestimmter Verhaltensweisen (z. B. Irrelevanz, Selbst‑ oder Partner‑Widersprüche, Halluzinationen, Empathie) annotieren**. Dadurch wird die Subjektivität verringert, die Zuverlässigkeit (Inter‑Annotator‑Agreement) steigt und man erhält vielschichtige, eindeutig getrennte Qualitätsmetriken.</sample>
    <sample id="9">Der Erfolg der heutigen schwach überwachten Lernmethoden hängt entscheidend davon ab, dass **manuelle, saubere Validierungsdaten vorhanden sind**. Ohne ein kleines, korrekt gelabeltes Validierungs‑Set verlieren die Modelle fast den Lernvorgang und generalisieren nicht über die ursprünglichen, verrauschten Labels hinaus. Die Leistung steigt mit der Größe des sauberen Sets (typischerweise etwa 20 Beispiele pro Klasse), und wenn man die Modelle anschließend auf diesen sauberen Daten feinjustiert, erreichen selbst einfache Fine‑Tuning‑Ansätze die gleiche oder bessere Genauigkeit als komplexere WSL‑Methoden. Kurz gesagt: Saubere Validierungsdaten (und deren gezielte Nutzung) sind der Schlüssel zum Erfolg.</sample>
    <sample id="10">**Wie lässt sich die Leistung noch steigern?**

| Ansatz | Kurzbeschreibung | Warum es hilft |
|--------|------------------|----------------|
| **Robustere Wissens‑Retrieval** | Modell nutzt dynamische Retrieval‑Module (z. B. DPR, BM25) um relevanteren Kontext aus Wikipedia/DBpedia zu ziehen. | Genaue, kontextbezogene Infos (Genres, Autor*innen, Bilder) werden sofort zur Verfügung gestellt, statt auf statische Dumps zu vertrauen. |
| **Multimodale Einbindung** | Ergänze Audio‑Samples, Bilder oder kurze Zusammenfassungen als zusätzliche Inputs. | Indirekte Hinweise („der mit dem Piano“, „das Bild zeigt…“) lassen sich direkter nutzen. |
| **Domain‑spezifische Fine‑Tuning‑Phasen** | Nach dem allgemeinen Training das Modell auf jedem der drei Domänen separat fein‑tunen. | Spezifische Terminologie (Musik‑Genres, Koch‑Zutaten, Literatur‑Schlagwörter) wird besser gelernt. |
| **Chain‑of‑Thought bzw. Multi‑Step Prompting** | Modell wird angeleitet, zuerst die relevanten Eigenschaften zu extrahieren und dann die Auswahl zu treffen. | Verbessert die interne Repräsentation und reduziert Fehler bei Mehrdeutigkeiten. |
| **Erweiterte Annotierung** | Füge zusätzliche indirekte Ausdrücke hinzu (z. B. Wortschatz‑Variationen, Synonyme, Metaphern). | Das Modell lernt, dass unterschiedliche Formulierungen dieselbe Auswahl bedeuten. |
| **Knowledge‑Graph‑Integration** | Nutze strukturierte Graphen (e.g. Wikidata) als ergänzenden Wissens‑Backbone. | Erlaubt dem Modell, Beziehungen (z. B. gleiche Autor*innen) logisch zu verknüpfen. |
| **Active‑Learning‑Loop** | Setze das Modell ein, um schwierige Beispiele zu markieren, die dann von Crowd annotators geprüft werden. | Kontinuierliche Verbesserung an schwer zu lösenden Fällen. |
| **Evaluation‑Pipeline mit User‑Feedback** | Simuliere reale Benutzerantworten und verwende diese als Feedback zur feinen Abstimmung. | Misst, ob das Modell tatsächlich in natürlichen Gesprächen funktioniert. |
| **Experiment mit größeren Modellen (e. g. GPT‑4, Llama‑2‑70B)** | Nutze neuere Architekturen mit größerem Kontextfenster. | Mehr Kapazität für komplexe Inferenz und längere Kontextverarbeitung. |

Durch Kombination von **präziserem Retrieval**, **multimodaler Kontextualisierung** und **domänenspezifischem Feintuning** lässt sich die Trefferquote für indirekte Referenzen deutlich über die derzeitigen 82–87 % anheben – idealerweise auf 90 %+ bei realen Retrieval‑bedingungen.</sample>
    <sample id="11">Jack Hessel präsentiert das Projekt **“Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest”**. Ziel ist es, die Fähigkeit großer Sprachmodelle zu prüfen, Humor tatsächlich zu verstehen, anstatt nur oberflächlich witzige Texte zu generieren. Der Ansatz nutzt die wöchentlichen Caption‑Contests des *New Yorker*, bei denen Leser zu einem Cartoon passende Titel vorschlagen. Aus über 700 Cartoons wurden Bild‑ und Text‑Annotationen erstellt sowie 650 zweitausend‑Satz‑Erklärungen zu witzigen Captionen gesammelt.

Drei Aufgaben werden definiert:
1. **Matching** – ein Modell wählt aus fünf Caption‑Optionen die richtige zu einem Cartoon aus.
2. **Quality Ranking** – ein Modell entscheidet, welche von zwei echten Captions qualitativ besser ist.
3. **Explanation Generation** – ein Modell erklärt in zwei bis vier Sätzen, warum eine Caption witzig ist.

Für das Matching erreichte ein CLIP‑Modell nach Fein‑Tuning etwa 62 % Genauigkeit, während Menschen 94 % erreichen. GPT‑4, das Bildinhalte nicht direkt verarbeitet, wurde mit einer menschlich verfassten Bildbeschreibung konditioniert und zeigte ebenfalls einen deutlichen Leistungsabstand zu menschlichen Referenzen. Bei Erklärungen generierte GPT‑4 häufig fehlerhafte Interpretationen (z. B. falsche Sprecherzuordnung), die in Blind‑A/B‑Tests von über zwei Dritteln der menschlichen Erklärungen übertroffen wurden.

Das Projekt bietet einen öffentlichen Leaderboard und Datenbankzugang, um weitere Fortschritte im Humor‑Verstehen von KI‑Modellen zu fördern.</sample>
    <sample id="12">Five authors in total – Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="13">In this presentation, Daniel Rotem explains adaptive inference as a way to reduce the latency of large language models by using lighter models for easier inputs. He contrasts two prevalent adaptive strategies: **Multi‑Model** and **Early Exit**. Multi‑Model stores several independently trained models, each followed by a classifier that decides whether to halt inference. It is versatile and memory‑efficient but suffers from storage cost and inference overhead, since all preceding models are still run. Early Exit attaches multiple classifiers to intermediate transformer layers, allowing a sample to stop early without extra overhead. However, because all classifiers share the same weights, their gradients can conflict, degrading overall performance—a phenomenon Rotem calls *conflicting gradients*.

To investigate this, the speaker compared Early Exit classifiers to equivalent Multi‑Model classifiers and found that Multi‑Model consistently outperformed Early Exit by an average of 2.3 % (up to 5.2 % for the earliest exits). Yet Early Exit excels when using later layers because Multi‑Model’s overhead dominates.

The solution proposed is **SWEET (Separating Weights in Early Exit Transformers)**, a fine‑tuning technique that assigns each transformer layer to be updated only by the loss of its subsequent classifier, thereby eliminating conflicting gradients. SWEET narrows the performance gap between the two approaches and, in many settings—including BERT‑Large—achieves superior speed‑accuracy trade‑offs across the board. The talk concludes by highlighting the discovery of conflicting gradients, a fair comparison of the two adaptive methods, and the promise of SWEET for future research.</sample>
    <sample id="14">**Titel:** Die Abhängigkeitsstruktur der Koordination

**Rede von Adam Przepiórkowski**

---

**Einleitung**

Mein Name ist Adam Przepiórkowski und diese Präsentation beschäftigt sich mit der Abhängigkeitsstruktur von Koordination. Wie Sie vielleicht wissen, gibt es verschiedene theoretische und korpusbasierte Ansätze, die unterschiedliche Abhängigkeitsstrukturen für Koordination annehmen. So nimmt beispielsweise die Universal‑Dependencies‑(UD)‑Konvention an, dass die Koordination *Lisa, Bart und Maggie* von der ersten Konjunktion (*Lisa*) abhängt. Die erste Konjunktion ist also der Kopf der gesamten Koordinationsstruktur. Ein ähnlicher Ansatz findet sich in Igor Mel'čuks *Meaning‑Text‑Theory*, in der ebenfalls das gesamte Koordinationsgefüge von der ersten Konjunktion abhängt. Diese beiden Ansätze sind also asymmetrisch, weil sie ein Konjunktum hervorheben.

Im Gegensatz dazu steht der *conjunction‑headed* Ansatz, wie er im Prag‑Zweig verwendet wird: dort ist die Konjunktion selbst der Kopf der Koordination und bildet somit Abhängigkeiten zu allen einzelnen Konjunkten. Ein weiterer, vollständig symmetrischer Ansatz ist der von Hudsons *Word‑Grammar*, bei dem alle Konjunkten gleichzeitig die Köpfe der Koordination sind. So entstehen Abhängigkeiten vom Governer zu allen Konjunkten: *Lisa*, *Bart* und *Maggie*.

**Ziel der Arbeit**

Ziel dieser Arbeit ist es, ein neues Argument dafür zu liefern, dass Koordination symmetrisch strukturiert sein sollte – wie in den drei oben genannten Beispielen – und gleichzeitig gegen die asymmetrischen Strukturen wie bei UD und Meaning‑Text‑Theory zu argumentieren. Das Argument stützt sich auf das Prinzip der *dependency‑length‑minimization* (DLM), das ich anhand einiger Beispiele erläutern werde.

---

### 1. Das Prinzip der Abhängigkeitslängenminimierung

Im Englischen bevorzugen direkte Objekte eine Nähe zum Verb, während Adverbiale (Adjuncts) weiter entfernt sein können. Ein Beispiel:

- *Marge read it yesterday.*  
  – Hier steht das direkte Objekt *it* direkt nach dem Verb, was als gut wahrgenommen wird.

- *Marge read yesterday it.*  
  – Dieses Satzbild ist deutlich schlechter, weil zwischen Verb und direktem Objekt ein Adverbial (*yesterday*) platziert ist.

Allerdings kann dieses Prinzip abgeschwächt werden, wenn das direkte Objekt sehr lang und schwer ist. In diesem Fall kann es „nach“ das Adverbial verschoben werden. Das lässt sich an den folgenden Sätzen sehen:

- *Marge read this absolutely fascinating book about bees yesterday.*  
- *Marge read yesterday this absolutely fascinating book about bees.*

Obwohl der zweite Satz das allgemeine grammatische Prinzip verletzt, indem das direkte Objekt nicht unmittelbar nach dem Verb steht, erfüllt er das Prinzip der DLM. Wir betrachten dabei nur die Längen der entscheidenden Abhängigkeiten – diejenigen, die sich zwischen den beiden Satzvarianten unterscheiden. In der ersten Variante ist die Abhängigkeit von *read* zu *yesterday* 7 Wörter lang und die von *read* zu *book* 4 Wörter, also zusammen 11. In der zweiten Variante wechselt die Reihenfolge, und die Gesamtlänge der beiden Abhängigkeiten beträgt nur 6. Somit ist die zweite Variante laut DLM kürzer und daher akzeptabler.

---

### 2. Statistische Analyse aus dem Penn Treebank

Wir haben verschiedene Statistiken zur Koordination aus der *enhanced* Version des Penn Treebank extrahiert (siehe auch den Artikel *Why wouldn’t you use Universal Dependencies?*). Diese Bestätigungen zeigen, dass linke Konjunktionen tendenziell kürzer sind – z. B. *salt and pepper* statt *pepper and salt*. Die Messung erfolgte in Silben. Außerdem wurde in der Parsing‑Forschung beobachtet, dass diese Tendenz mit zunehmendem Längenunterschied der Konjunkten stärker wird: je größer der Unterschied in der Länge, desto deutlicher ist die Präferenz für die kürzere Konjunktion an erster Stelle.

**Neue Beobachtung**  
Wir haben festgestellt, dass diese Tendenz nur dann auftritt, wenn der Governer links steht oder fehlt. Beispiele:

- *I saw Bart and Lisa.* – Governer (*saw*) steht links.  
- *Homer came and sneezed.* – Koordination von zwei Verben ohne externen Governer.  

In beiden Fällen bevorzugt die linke Konjunktion die kürzere Variante, insbesondere bei großem Längenunterschied. Wenn der Governer hingegen rechts steht, wie in *Ted and Ned laughed*, verschwindet diese Tendenz.

Wir haben die Längen in Zeichen, Silben und Wörtern gemessen. Auf der rechten Spalte (Wörter) sieht man:

- Bei linkem Governer wächst die Präferenz für eine kürzere linke Konjunktion stetig mit zunehmendem Wortunterschied.  
- Bei fehlendem Governer (z. B. Verb‑Koordination) zeigt sich die gleiche Tendenz.  
- Bei rechtem Governer ist die Tendenz nicht vorhanden.

---

### 3. Implikationen für die Modellierung von Koordination

Diese Beobachtungen liefern ein starkes Argument gegen die asymmetrischen Modelle (UD, Meaning‑Text‑Theory), die einen einzigen Kopf (erstes Konjunktum oder die Konjunktion selbst) einführen, und befürworten stattdessen symmetrische Modelle (Prag‑Ansatz, Word‑Grammar). Die DLM‑Erklärung zeigt, dass die Koordination natürlicher wirkt, wenn die Abhängigkeiten symmetrisch verteilt sind und die Längenunterschiede zwischen den Konjunkten berücksichtigt werden. Insbesondere:

- Das Prinzip der DLM erklärt die Präferenz für kürzere linke Konjunktionen, wenn der Governer links oder fehlt.  
- Die asymmetrische Struktur würde zu längeren Abhängigkeiten führen, die die DLM‑Präferenz verletzen.  
- Symmetrische Modelle ermöglichen eine gleichmäßigere Verteilung der Abhängigkeiten und erklären die beobachteten Längenpräferenzen ohne zusätzliche asymmetrische Annahmen.

---

**Fazit**

Die Analyse der Koordination in großen Korpora, kombiniert mit theoretischen Überlegungen zur Minimierung der Abhängigkeitslängen, liefert starke Belege für symmetrische Koordinationsstrukturen. Diese Erkenntnisse sollten bei der Weiterentwicklung von Dependency‑Treebanks und bei der theoretischen Modellierung von Koordination berücksichtigt werden.

**Abschluss**  
Ich lade Sie herzlich ein, sich mit mir im Poster‑Sitzungsbereich zu unterhalten. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="15">Drei Autoren: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bibeltexte werden deutlich stärker vereinfacht als beispielsweise Nachrichten‑ oder Sprachlerntexte.</sample>
    <sample id="17">**Abstract**

Wir adressieren die Herausforderungen der multimodalen Relationserkennung (MRE), bei der Bild- und Textdaten kombiniert werden, um semantische Beziehungen zwischen Entitäten zu extrahieren. Zwei zentrale Probleme sind (1) internes Informationsüber‑ausnutzen: nur ein Teil des Textes bzw. Bildes ist relevant, und (2) externes Informationsunter‑ausnutzen: zusätzliche Kontextinformationen wie Themen sind oft ungenutzt. Wir stellen ein Verfahren vor, das die beiden Dimensionen gleichzeitig reduziert und ergänzt. Zunächst werden Text‑ und Bilddaten als Szenengraphen repräsentiert und zu einem einheitlichen Cross‑Modal Graphen (CMG) zusammengeführt. Anschließend filtern wir fein granular Knoten und Kanten des CMG unter Einsatz des Graph Information Bottleneck (GIB), um redundante interne Informationen zu entfernen. Zur Kompensation extrahieren wir latente multimodale Themen mittels eines Joint‑Topic‑Modells und fügen die relevanten Themenwörter dem CMG‑Feature‑Set hinzu. Experimente auf dem MRE‑Benchmark zeigen, dass unser Ansatz die Best‑Performance aller Baselines übertrifft. Ablationsanalysen bestätigen den Nutzen von Screening und Ergänzung; die Szenengraph‑Struktur verbessert die Modellierung. Eine Analyse nach Text‑Bild‑Relevanz verdeutlicht, dass bei hochrelevanten Inputs die interne Filterung (GIB) dominanter ist, während bei niedrigrelevanten Inputs die externe Thema‑Ergänzung (LAMO) stärker wirkt. Das System demonstriert damit die Wirksamkeit simultaner Informationssubtraktion und -addition in multimodalen Relationserkennung.</sample>
    <sample id="18">Ein klassisches Beispiel ist die Präferenz für **„salt and pepper“** statt **„pepper and salt“** – das kürzere Konjunkt (salt) steht links.</sample>
    <sample id="19">**Kurzfassung (ca. 200 Wörter)**  

Zhang Qin von der Shenzhen University berichtet über die Annahme seiner Arbeit „A Survey for Efficient Open Domain Question Answering“ bei ACL 2023. Die Studie untersucht die Effizienz von Open‑Domain‑Question‑Answering (ODQA) Systemen, die in der Regel ein zweistufiges Modell nutzen: zunächst ein Retrieval‑System, das relevante Wikipedia‑Dokumente anhand eines Index abruft, und anschließend ein Reader, der die Antwort aus den gefundenen Kontexten extrahiert. Zhang erläutert die Herausforderungen: die Wikipedia‑Korpus (26 M Dokumente, 20 GB) und der Index (65 GB) verlangsamen die Inferenz, und große Sprachmodelle erschweren die Nutzung auf ressourcenbeschränkten Geräten.  

Um diese Probleme zu adressieren, werden verschiedene Optimierungsmethoden vorgestellt:  
1. **Effizientes Retrieval** – Einsatz von Approximate Nearest Neighbor (ANN) statt Volltextsuche, Dokumentenfilterung, Dimensionskompression und Produktquantisierung, um die Indexgröße zu reduzieren.  
2. **Schnelles Lesen** – Adaptive Computation und Skip‑Reading, bei denen weniger relevante Texte übersprungen werden.  
3. **Modell‑Reduktion** – Leichtgewichtige Architekturen, Parameter‑Sharing, Knowledge‑Distillation und sogar One‑Stage‑Modelle, die Retrieval und Reader in einem Netzwerk kombinieren.  

Zhang vergleicht die drei Hauptarchitekturen (Retrieval‑Reader, Retrieval‑Only, Generator‑Only) hinsichtlich Geschwindigkeit, Speicherbedarf und Performance. Retrieval‑Only Systeme sind am schnellsten, aber speicherintensiv; Generator‑Only Systeme benötigen keinen Index, sind aber meist schwer und weniger performant. Retrieval‑Reader‑Systeme bieten das beste Gleichgewicht.  

Abschließend betont er zwei Forschungsfelder: die Ausführung von ODQA auf Low‑Power‑Geräten und die Entwicklung erweiterter Evaluationsmetriken.</sample>
    <sample id="20">Yes – the DrBERT models (and the related ChuBERT variants) are released under the MIT license and are publicly available on Hugging Face. You can download, fine‑tune, and use them for any research work, provided you acknowledge the original authors and the license terms. All training scripts are also on the authors’ GitHub repository for reproducibility.</sample>
    <sample id="21">DEplain‑apa basiert auf **Nachrichten­texten** (Journalistik).</sample>
    <sample id="22">Die Studie identifizierte drei Schlüsselfaktoren für eine gute Generalisierung von NER‑Taggern:

1. **Modellarchitektur** – Transformer‑basierte Modelle generalisieren besser als ältere Architekturen.  
2. **Modellgröße** – Größere Modelle liefern tendenziell höhere und stabilere Leistungen.  
3. **Anzahl der Fine‑Tuning‑Beispiele** – Mehr Trainingsdaten führen zu einer stärkeren und robusteren Generalisierung.</sample>
    <sample id="23">Dan Garrette diskutiert die jüngsten Fortschritte bei text‑zu‑Bild‑Modellen und deren Schwächen beim Rendern von Schriftzeichen. Insbesondere die Imagen‑Architektur, die Eingabetext über einen T5‑XXL‑Encoder verarbeitet und anschließend in ein Diffusionsmodell einspeist, erzeugt zwar beeindruckende Bilder, aber häufig Fehler in Texten. Der Grund liegt in der Subwort‑Tokenisierung von SentencePiece: das Modell erhält keine Einzelbuchstaben, sondern größere Wort‑oder Teilwort‑IDs, die erst in Buchstaben zerlegt werden müssen. Experimente zeigen, dass die gängigen T5‑Modelle – Base und Large – bei der Rechtschreibung unter 20 % liegen, T5‑XXL knapp 70 %. PaLM‑Modelle performen deutlich besser, sind jedoch wesentlich größer. Im Gegensatz dazu nutzt ByT5 Byte‑Level‑Tokenisierung und hat daher direkten Zugriff auf die Zeichen; sie erreicht in allen Größenordnungen hohe Rechtschreibgenauigkeit. Eine Analyse nach Worthäufigkeit enthüllt, dass T5 bei häufigen Wörtern besonders schlecht abschneidet, weil diese meist als ein einzelnes Subwort kodiert werden und das Modell viele Buchstaben daraus rekonstruieren muss. ByT5 bleibt dagegen robust. Um die Bild‑Modellierung zu verbessern, fügten die Autoren den ByT5‑Small‑Encoder als zusätzliche Textrepräsentation zu Imagen hinzu. Dieser Aufsatz erhöht die Parameterzahl lediglich um etwa 5 % und führt zu einer deutlich verbesserten Text‑Reproduktion, obwohl die Diffusion immer noch Fehler einführen kann. Das Paper präsentiert zwei neue Benchmarks – WikiSpell für reine Text‑Modelle und DrawText für Text‑zu‑Bild‑Modelle – und zeigt einen effizienten Ansatz, die Rechtschreibfähigkeiten durch eine Zeichen‑bewusste Ergänzung zu steigern.</sample>
    <sample id="24">Die Studie hat die Länge der beiden Konjunktionen direkt aus den Enhanced‑PTB‑Parsebäumen gezählt.  
Für jede koordinierte Struktur wurden die beiden Konjunktionen in

* **Zeichen (characters)**  
* **Silben (syllables)**  
* **Wörtern (words)**  

gemessen.  

Anschließend wurde für jede Koordination das **Längen‑Differenz‑Muster** analysiert:  
* Man ermittelte die absolute Längen­differenz zwischen links und rechts.  
* Man berechnete den Anteil, bei dem die linke Konjunktion kürzer ist als die rechte.  

Die Ergebnisse zeigten, dass dieser Anteil mit zunehmender Differenz (in Wörtern, Silben oder Zeichen) steigt – aber nur, wenn der Governator links oder nicht vorhanden ist. Somit wurde die Tendenz zu kürzeren linken Konjunktionen durch die Messung der Längen in den drei Einheiten und die Analyse der Differenzproportionen ermittelt.</sample>
    <sample id="25">Die Untersuchung der Einflüsse der Position des „Governors“ (des übergeordneten Elements) wurde experimentell mithilfe von Korpusdaten gestaltet.  

1. **Datensammlung**  
   - Aus dem **enhanced Penn Treebank** wurden alle Koordinationsstrukturen extrahiert.  
   - Jede Koordination wurde in die drei Kategorien eingeteilt:  
     1. **Governor links** (z. B. *I saw Bart and Lisa* – der Verb steht links von der Koordination),  
     2. **Governor rechts** (z. B. *Ted and Ned laughed* – Verb rechts von der Koordination),  
     3. **Kein Governor** (z. B. Koordination von Satzteilen ohne externen Governer).

2. **Längenmessung**  
   - Für jede Koordination wurden die Längen beider Konjunkte in **Zeichen, Silben und Wörtern** gemessen.  
   - Die Differenz der Längen (|L1 – L2|) wurde als unabhängige Variable verwendet.

3. **Statistische Analyse**  
   - Für jede Kategorie wurde der Anteil berechnet, in dem der **linke Konjunkt** kürzer ist als der rechte.  
   - Diese Anteile wurden gegen die absolute Längen­differenz aufgetragen.  
   - Durch Vergleich der Kurven für die drei Governor‑Positionen konnte die Abhängigkeit des „kurzen linken Konjunkt“-Effekts von der Position des Governors untersucht werden.

4. **Interpretation**  
   - Der beobachtete Trend (linker Konjunkt wird bei größerer Längen­differenz häufiger kürzer) zeigte sich **nur bei Links‑oder‑fehlendem Governor**, verschwand jedoch bei rechts positioniertem Governor.  
   - Diese Ergebnisse wurden im Kontext der **Abhängigkeitslängen‑Minimierung** diskutiert und als Argument für symmetrische Koordinationsstrukturen verwendet.

Kurz gesagt: Die Experimente folgten einer korpusbasierter Analyse, bei der Koordinationsstrukturen anhand der Position des Governors klassifiziert, die Längen der Konjunkte gemessen und die Häufigkeit des „kurzen linken Konjunkt“ in Abhängigkeit von der Längen­differenz statistisch verglichen wurden.</sample>
    <sample id="26">Ein Basisklassifikator, der auf stark unausgewogenen Daten (nur 43 dissonante Beispiele) trainiert wird, liefert kaum mehr als Zufalls­ergebnisse – der AUC liegt praktisch bei 0,5, also kaum besser als „chance“.</sample>
    <sample id="27">Der Vortrag nennt nur Shangbin als Autor – also mindestens **ein** Autor ist beteiligt.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive Modelle zeigen deutliche Verbesserungen gegenüber kontextagnostischen Modellen bei **Formality** (formale Register) und **lexikalischer Kohärenz** (Lexical Cohesion). Bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen liegt der Unterschied kaum.</sample>
    <sample id="30">**LLM‑Blender – Ein Ensemble‑Framework für große Sprachmodelle**

Das Team aus AI2 und USC stellt mit „LLM‑Blender“ ein einfaches, aber wirkungsvolles Ensemble‑Framework vor, das mehrere Large Language Models (LLMs) gleichzeitig nutzt. Die zentrale Idee ist, dass die optimale Modellwahl je nach Eingabe stark variiert – beispielsweise ist Vicuna zwar der besten Durchschnittsmodell, aber nur bei 21 % der Testbeispiele der Spitzenreiter. Daher sollen bei jeder Eingabe mehrere Modelle laufen und die besten Antworten kombiniert werden.

**Zwei‑Stufen‑Pipeline**  
1. **PairRanker**: Für jede Eingabe X werden n Modelle ausgeführt und ihre Ausgaben Y₁…Yₙ generiert. Der PairRanker vergleicht jedes Paar (Yᵢ, Yⱼ) zusammen mit X mittels eines Cross‑Attention‑Encoders (z. B. RoBERTa) und erzeugt eine Vergleichsmatrix, in der jedes Element das Logit für „Yᵢ ist besser als Yⱼ“ angibt. Durch die Aggregation (idealerweise Max‑Logits, alternativ Bubble‑Sort) erhält man eine Rangfolge der Kandidaten. Dieses Verfahren erzielt höhere Korrelationen mit der Oracle‑Rangfolge als klassische Einzel‑Scoring‑Methoden.

2. **GenFuser (Generative Fuser)**: Die Top‑K (typisch 3) Kandidaten aus dem PairRanker werden als Eingabe an ein seq2seq‑Modell übergeben, das die endgültige Antwort generiert. Dieses „fusioniert“ die Stärken der besten Modelle zu einer einzigen, qualitativ besseren Ausgabe.

**Evaluation**  
Für die Bewertung wurde die neue Benchmark‑Menge **MixInstruct** erstellt, die Instruktion‑Datensätze mit Kandidaten aus 11 Open‑Source‑LLMs kombiniert. Automatische Metriken (BERTScore, BLUERT, BARTScore) und ein ChatGPT‑Jury wurden eingesetzt. LLM‑Blender übertrifft sowohl einzelne Top‑Modelle (Open Assistant, Vicuna) als auch andere Ranking‑Ansätze in allen vier Metriken, wobei es in 68 % bzw. 76 % der Beispiele besser abschneidet.

**Take‑Home‑Messages**  
- LLM‑Blender ist ein leichtgewichtiges, aber leistungsstarkes Ensemble‑Framework.  
- Der PairRanker nutzt paarweise Vergleiche, um Kandidaten differenziert zu bewerten.  
- GenFuser kombiniert die Top‑K-Ausgaben zu einer hochwertigen Antwort.  
- MixInstruct ermöglicht die standardisierte Bewertung von Ensemble‑Ansätzen.  
- Der veröffentlichte Code und die Datenbasis fördern weitere Forschung in diesem Bereich.</sample>
    <sample id="31">Die Autoren stammen aus drei Institutionen:

| Autor | Universität / Forschungseinrichtung |
|-------|--------------------------------------|
| Koustav Sinha | Google Research (Mountain View) |
| John Gauthier | Google Research |
| Aaron Mueller | Google Research |
| Kanishka Misra | Google Research |
| Karen Fences | Google Research |
| Roger Levy | Columbia University |
| Adina Williams | MIT (Massachusetts Institute of Technology) |

Kurz gesagt: die Mehrheit der Autoren ist bei Google Research, während Roger Levy von Columbia University und Adina Williams von MIT kommt.</sample>
    <sample id="33">The framework quantifies positionality by **re‑annotating each data point with many diverse annotators** and recording their demographic attributes.  
For every demographic group it then **computes a Pearson‑R correlation** between that group’s labels and the labels produced by the dataset or the model.  
The resulting correlation score indicates how well the dataset or model “aligns” with that particular population—higher scores mean stronger positional alignment, lower scores reveal which groups are under‑represented or poorly captured.</sample>
    <sample id="34">**Zusammenfassung (≈ 200 Wörter)**

CREST (Counterfactual‑Rationalization for Explainable Text) ist ein einheitlicher Rahmen, der selektive Rationalisierung und Counterfactual‑Generierung kombiniert. Zunächst erzeugt ein Rationalizer‑Modell mit einem trainierbaren „Masker“ eine rationale Teilmenge Z des Eingabetextes X. Diese rationale wird genutzt, um das Original zu maskieren und den Gold‑Label‑String (z. B. „positive“) voranzustellen. Das maskierte Beispiel wird anschließend an einen Editor – praktisch ein Masked Language Model – übergeben, das die Masken mit neuen Tokens füllt und so einen Counterfactual‑Text X ̃ generiert.  

Die Qualität der Counterfactuals wird durch menschliche Bewertungen auf IMDB‑ und SNLI‑Datensätzen gemessen. CREST übertrifft das automatische Verfahren MiCE, bleibt jedoch hinter manuellen Counterfactuals zurück. Für die Datenaugmentation zeigen Experimente, dass Modelle, die mit CREST‑Rationales trainiert werden, auf dem IMDB‑Testdatensatz die besten Ergebnisse erzielen, in kontrastiven Tests genauso gut wie Modelle, die menschliche Counterfactuals nutzen, und bei Out‑of‑Domain‑Daten sogar überlegen sind.  

Zur Analyse der Erklärungen werden drei Dimensionen verwendet: Plausibilität, Forward‑Simulability und die neu vorgeschlagene Counterfactual‑Simulability, die misst, wie stark ein rationale Teil die Klassifikationsentscheidung bei einer kontrastiven Editierung beeinflusst. CREST‑Rationales schneiden dabei in allen drei Metriken besser ab als Konkurrenzmethoden.  

Zusammenfassend liefert CREST valide, fließende und vielfältige Counterfactuals, die die Trainierbarkeit von Modellen verbessern und plausiblere, kontrastorientierte Erklärungen erzeugen.</sample>
    <sample id="36">In ihrer Präsentation stellen Telmo Pessoa Pires und die Co‑Autor*innen ein neues Konzept für mehrsprachige maschinelle Übersetzung vor: **Language‑Specific Layers (LSLs)**. Das Ziel ist, die Kapazität pro Sprache zu erhöhen, ohne die Inferenzzeit zu steigern. Statt für jede Sprachrichtung ein eigenes Modell zu trainieren, wird ein gemeinsamer Transformer verwendet, in dem für jede Sprache ein spezieller Unter‑Layer eingebaut ist. Während des Trainings werden für jede Encoder‑Ebene drei Gewichtungen angelegt – eine gemeinsam, eine für die Quellsprache und eine für die Zielsprache. Auf Basis der größten Gewichtung wird anschließend die optimale Platzierung der LSLs bestimmt. Das Ergebnis ist ein Modell, bei dem die unteren Ebenen gemeinsam, mittlere Ebenen quellspezifisch und obere Ebenen zielspezifisch sind, während der Decoder meist gemeinsam bleibt.

Die Autoren trainieren auf WMT21 News-Daten mit zehn Sprachen (inkl. Swahili) und evaluieren auf dem Flores‑101‑Korpus. Im Vergleich zu herkömmlichen Transformer‑Modellen und Sprach‑Adapter‑Ansätzen erzielt das LSL‑Modell signifikante Verbesserungen (chrF, spBLEU, COMET), insbesondere bei Low‑Resource‑Sprachen. Die Inferenz bleibt dabei unverändert schnell, und die Verbesserungen gelten für 84 von 90 Sprachpaaren. Zusammenfassend zeigen die Ergebnisse, dass sprachspezifische Schichten die Effizienz und Qualität mehrsprachiger Übersetzungsmodelle deutlich steigern können.</sample>
    <sample id="37">Die vorherige Studie zeigte, dass auch menschliche Teilnehmende, die mit denselben Persona‑Prompts befragt wurden, rassistische Stereotype in ihren Antworten reproduzierten.</sample>
    <sample id="38">In der vorgestellten Untersuchung wurden ausschließlich korpusbasierte Daten verwendet – konkret das **erweiterte Penn‑Treebank** (Enhanced PTB).  Die dort enthaltenen Koordinationseinheiten wurden ausgewertet, wobei die Ergebnisse auch mit Erkenntnissen aus der Universal‑Dependencies‑Community (z. B. dem Paper „Why wouldn’t you use Universal Dependencies“) verglichen wurden.  Somit ist die Hauptdatenquelle das Enhanced PTB, ergänzt durch referenzielle Analysen aus den Universal‑Dependencies‑Treebanks.</sample>
    <sample id="39">Ein Autor – Adam Przepiórkowski.</sample>
    <sample id="40">The paper identifies two closely related tasks that can be used for transfer learning into cognitive‑dissonance detection:

1. **Topic‑independent dissonance stance classification (Debate)** – a stance‑classification task that decides whether two debate statements from different speakers are in agreement or disagreement, regardless of topic.

2. **Binary classification of the Expansion and Comparison classes in PDTB (CE)** – a discourse‑relation task that distinguishes the two PDTB classes most analogous to consonance and dissonance.

These two tasks are the primary “related” benchmarks leveraged for transfer learning in the study.</sample>
    <sample id="41">**Kurzfassung (≈ 200 Wörter)**  

Silin und das NLP‑Lab der EPFL stellen „PeaCoK“ vor – ein Persona‑Commonsense‑Knowledge‑Graph, der über 3 800 Personas, 40 000 Attribute und rund 100 000 persönliche Fakten verfügt. Der Graph verbindet Personas über vier Hauptrelationen (z. B. „interaktiv“ und „distinctive“) und schafft damit ein reiches Netzwerk von miteinander verknüpften Charakterprofilen. Der Aufbau erfolgte in drei Schritten: 1) Auswahl von Personas aus bestehenden Commonsense‑Graphen, 2) Ableitung ihrer Attribute aus Commonsense‑Datenbanken sowie großen Sprachmodellen, 3) Crowdsourcing‑Annotationen der Relationen mittels eines „Human‑AI‑Majority‑Voting“-Schemas, das eine durchschnittliche Genauigkeit von 87 % erreicht.  

PeaCoK wurde zur Schulung eines BART‑basierten Generators verwendet, der Attribute für gegebene Personas vorhersagen soll. Im Vergleich zu GPT‑3‑Varianten liefert der Komet‑BART‑Trainer bessere automatische Metriken und höhere Akzeptanzraten in menschlichen Bewertungen, was zeigt, dass PeaCoK auch leichtere Modelle mit vergleichbarem Wissen ausstatten kann.  

Darüber hinaus wurde PeaCoK zur Verbesserung von Dialogsystemen eingesetzt. Durch einen Knowledge‑Linker wurden relevante Fakten aus PeaCoK extrahiert und in natürliche Aussagen überführt, um die Persona‑Profile in ConvAI2 zu ergänzen. Beim P²Bot‑Modell verbesserten sich Fluenz, Konsistenz, Engagement und Persona‑Ausdruck signifikant, insbesondere wenn die beiden Sprecher gemeinsame Attribute teilten. Im Vergleich zu Atomik‑2020‑Wissen erwies sich PeaCoK als wirksamer für persona‑zentrierte Narrative.  

Zusammenfassend liefert PeaCoK eine hochwertige, verknüpfte Persona‑Wissensbasis, die sowohl das Training von Wissensgeneratoren als auch die Erzeugung konsistenter, ansprechender Dialoge unterstützt.</sample>
    <sample id="42">The presentation is delivered by a single author, Shuheng.</sample>
    <sample id="43">Die Anzahl der Autoren wird im vorliegenden Text nicht genannt.</sample>
    <sample id="44">Das vorgestellte NLPositionality‑Framework unterscheidet sich von bisherigen Arbeiten vor allem dadurch, dass es

1. **Daten neu belegt** – statt einer einzigen, kleinen Gruppe von Annotatoren werden die gleichen Instanzen von vielen, demografisch vielfältigen Nutzern erneut annotiert, sodass für jede Instanz ein reichhaltiges Profil an Merkmalen entsteht.  
2. **Demografische Details erfasst** – die Demografie der Annotatoren wird systematisch gesammelt, was zuvor selten dokumentiert wurde.  
3. **End‑User‑Alignment misst** – anstatt lediglich die Übereinstimmung zwischen Annotatoren zu betrachten, wird die Korrelation (Pearson‑R) zwischen den annotator‑basierten Labels und den Vorhersagen der Modelle/Datasets berechnet. Dadurch kann nachvollzogen werden, mit welcher Nutzergruppe ein Modell bzw. ein Dataset am besten übereinstimmt.  
4. **Vielseitige Plattform nutzt** – die Studie verwendet „Lab in the Wild“, um weltweit diversifizierte Teilnehmende einzubinden, was über die üblichen MTurk‑Belegungen hinausgeht.

Kurz gesagt: NLPositionality geht über annotator‑Agreement hinaus, indem es die Perspektiven und Demografie der End‑Nutzer einbezieht und deren Übereinstimmung mit Modellen/Datasets quantitativ bewertet.</sample>
    <sample id="45">Der Vergleich zeigt, dass die **von GPT‑4 generierten Personas** die meisten Überschneidungen mit dem Stereotypen‑Lexikon aufweisen – sie enthalten deutlich mehr lexikalische Stereotypen als die menschlich verfassten Personas.</sample>
    <sample id="46">Die verglichenen kommerziellen Systeme sind **DeepL** und **Google Translate**.</sample>
    <sample id="47">**Titel:**  
„Von den Trainingsdaten über Sprachmodelle bis zu Down‑stream‑Aufgaben: Verfolgung der Spur politischer Vorurteile, die zu unfairen NLP‑Modellen führen“

**Einleitung**  
Ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit, in der wir untersuchen, wie politische Vorurteile aus den Trainingsdaten in Sprachmodelle gelangen und anschließend in Down‑stream‑Anwendungen die Fairness beeinträchtigen können. Sprachmodelle werden mit enormen Mengen an Web‑Krawledaten trainiert. Politische Nachrichtenmedien sind dabei stark vertreten – laut einer Analyse des C4‑Korpus ist beispielsweise die New York Times, Los Angeles Times, The Guardian, Huffington Post und weitere Medienquellen gut in die Trainingsdaten eingebunden. Diese Vielfalt hat zweischneidige Auswirkungen: Einerseits profitieren die Modelle von unterschiedlichen Perspektiven, die Demokratie und Pluralität widerspiegeln; andererseits können die inhärenten sozialen Vorurteile dieser Quellen zu Fairness‑Problemen in Down‑stream‑Anwendungen führen.

**Forschungsfragen**  
1. **Wie lässt sich die politische Neigung von Sprachmodellen evaluieren und welche Rolle spielt dabei das Trainingsmaterial?**  
2. **Wie wirken sich Sprachmodelle mit unterschiedlichen politischen Neigungen auf Down‑stream‑Aufgaben aus, und führen diese zu Fairness‑Problemen?**

**Methodik**  
Um die politische Neigung von Sprachmodellen systematisch zu messen, nutzen wir Fragebögen aus der Politikwissenschaft (z. B. den „Political Conference Test“) und passen verschiedene Prompt‑Formate an. Damit können wir automatisierte, wissenschaftlich fundierte Bewertungen durchführen.  

Wesentliche Experimente umfassen:  
- **Weiter‑Training** von Modellen auf sechs partizipierten Korpora, die nach politischer Zugehörigkeit (links‑ bzw. rechts‑orientiert) und Medienart (Nachrichten vs. soziale Medien) differenziert sind.  
- **Zeitliche Analyse**: Modelle werden separat auf Texten vor und nach der 45. Präsidentschaft der USA trainiert, um die Verstärkung politischer Polarisierung zu überprüfen.

**Ergebnisse**  
1. **Politische Neigungen von Modellen**  
   - Modelle liegen in allen vier Quadranten des politischen Campus verteilt.  
   - GPT‑4 ist das liberalste Modell; generell zeigen GPT‑Modelle tendenziell liberale Tendenzen, während BART‑Modelle und deren Varianten eher konservativ sind.  

2. **Einfluss des Trainingsmaterials**  
   - Nach Weiter‑Training auf linksladenden Reddit‑Korpora verschiebt sich die Ideologie von RoBERTa signifikant nach links.  
   - Modelle, die mit Daten aus der Ära nach 2017 trainiert werden, zeigen eine stärkere Abweichung vom Zentrum, was die zunehmende Polarisierung der Gesellschaft widerspiegelt.  

3. **Auswirkungen auf Down‑stream‑Aufgaben**  
   - *Hate‑Speech‑Detection*: Linksladene Modelle erkennen Hassrede gegen soziale Minderheiten besser, während sie Hass gegen dominante Gruppen schlechter klassifizieren; rechts‑laden Modelle zeigen das Gegenteil.  
   - *Fake‑News‑Detection*: Linksladene Modelle sind effektiver bei der Erkennung von Fehlinformationen aus der gegenüberliegenden politischen Lag, und umgekehrt.  
   - Qualitative Beispiele verdeutlichen, dass Modelle mit unterschiedlichen Neigungen bei denselben Texten unterschiedliche Vorhersagen treffen, basierend auf den sozialen Kategorien, die im Text angesprochen werden.  

Diese Muster zeigen ein dringendes Fairness‑Problem: Würde ein rechts‑ladenes Modell auf einer großen Social‑Media‑Plattform eingesetzt, könnten Nutzer mit gegensätzlichen politischen Ansichten marginalisiert werden, während Hassrede gegen Minderheiten unkontrolliert weiterverbreitet würde.

**Diskussion**  
Wir stehen vor einer einzigartigen Dilemma‑Situation, vergleichbar mit „Scylla und Charybdis“.  
- **Keine Säuberung**: Vorurteile aus den Trainingsdaten werden unvermeidlich auf die Modelle und weiter auf die Down‑stream‑Aufgaben übertragen.  
- **Säuberung**: Das Entfernen politischer Inhalte birgt das Risiko von Zensur oder Ausschluss und erschwert die Definition, was tatsächlich neutral ist.  

Es ist ein klassisches „elektrisches Zugvogel‑Problem“: Entscheiden wir uns für die eine Seite, riskieren wir andere Werte und Prinzipien.

**Fazit**  
Unsere Untersuchung verdeutlicht, dass politische Vorurteile im Trainingsmaterial in Sprachmodelle eindringen und sich in Down‑stream‑Aufgaben manifestieren, wodurch signifikante Fairness‑Probleme entstehen. Es ist daher entscheidend, diese Bias‑Ketten zu erkennen, zu messen und geeignete Gegenmaßnahmen zu entwickeln, um faire und verantwortungsbewusste NLP‑Anwendungen zu gewährleisten.

Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="48">There are **8 authors** on the paper.</sample>
    <sample id="49">Bis zu **1 024 Tokens** wurden für die MPP‑Auswertungen verwendet.</sample>
    <sample id="50">Die Präsentation stellt DEPLAIN vor, einen neuen deutschen Korpus zur Textvereinfachung, der sowohl auf Dokumenten‑ als auch Satzebene parallelisierte Texte umfasst. Textvereinfachung bedeutet, Inhalte so anzupassen, dass sie für Zielgruppen wie Menschen mit Lese­schwierigkeiten oder Nicht‑Muttersprache leichter verständlich werden. Dafür werden parallele Paare von komplexen und vereinfachten Sätzen benötigt. DEPLAIN besteht aus zwei Teil­korpora: DEPLAIN‑apa (nachrichten­basierte Texte) und DEPLAIN‑web (verschiedene Domänen). In DEPLAIN‑apa wurden 483 Dokumente manuell ausgerichtet, was ca. 13 000 vereinfachte Satzpaare ergibt. DEPLAIN‑web umfasst 750 Dokumente, die sowohl manuell als auch automatisch ausgerichtet wurden, und liefert insgesamt 30 450 Satzpaare. Analyse der Paare zeigt, dass biblische Texte stärker vereinfacht werden als Nachrichten­ oder Lern­texte, und es werden unterschiedliche Vereinfachungs­techniken (lexikalische Substitution, Klausel­löschung, Neu­ordnung, Wort­einfügung, Re­phrasierung) beobachtet.  

Omar erläutert zwei Hauptanwendungen: Erstens die Bewertung von automatischen Ausrichtungs­methoden für gleichsprachige, aber komplexitäts­unterschiedliche Dokumente. Auf Basis der manuellen Gold‑Standard‑Ausrichtungen stellte die Studie fest, dass MASSalign die beste Methode ist. Zweitens die automatische Vereinfachung mittels Fein‑Tuning von Sprach­modellen: long‑mBART für Dokument­niveau und mBART für Satz­niveau. Die Modelle erreichten überlegene Scores gegenüber Baselines und liefern einen Benchmark für zukünftige Arbeiten. Insgesamt bietet DEPLAIN ein umfangreiches, qualitativ hochwertiges Dataset, das Forschung und Praxis im Bereich deutscher Textvereinfachung vorantreibt.</sample>
    <sample id="51">Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezeichnet die Perspektiven und Sichtweisen, die Menschen aufgrund ihrer demografischen Merkmale, Identitäten und Lebenserfahrungen einnehmen und die – in Forschung und Technik – ihre Entscheidungen, Bewertungen und damit die Ergebnisse beeinflussen.</sample>
    <sample id="53">The speaker is **Dawei**.</sample>
    <sample id="54">**Abstract**  
Wir untersuchen kognitive Dissonanz als sprachliche Konstellation, bei der gegensätzliche Überzeugungen oder Handlungen ausgedrückt werden. Dissonanz ist selten, etwa 3,5 % der annotierten Diskurspaare auf Twitter, jedoch von großer Bedeutung für die Analyse von Meinungsverschiedenheiten, Polarisierung und psychischer Gesundheit. Um ein annotiertes Repertoire zu schaffen, haben wir ein groß angelegtes Annotation‑Projekt gestartet, das 1 000 Paare generierte, wobei zunächst nur 43 dissonante Beispiele vorlagen. Die daraus resultierende Basisklassifikation erreichte nahezu Zufallsleistung.  

Wir kombinieren Transfer‑ und Active‑Learning, um die seltene Klasse kosteneffizient zu erweitern. Als Transfer‑Quelle nutzen wir (1) die dissonanz‑unabhängige Debatten‑Stance‑Klasse (Debate) und (2) die Expansion/Comparison‑Klasse der PDTB‑Parsers (CE). Das fortlaufende Fine‑Tuning von CE gefolgt von Debate liefert die beste Zero‑Shot‑Basis (AUC ≈ 0,62).  

Für Active‑Learning vergleichen wir „Cumulative“ und „Iterative“ Updates und zeigen, dass Cumulative gleich oder besser abschneidet. Unsere neue Strategie „Probability‑of‑Rare‑Class“ (PRC) wählt Beispiele mit hoher Wahrscheinlichkeit für die seltene Klasse, was die Dissonanzrate in den annotierten Setts erhöht und die AUC auf 0,75 steigert. PRC erweist sich als einfaches, effektives Verfahren für seltene Klassen bei gleichzeitig hoher Annotationsqualität.</sample>
    <sample id="55">Ja – EDAtt ist so entworfen, dass es auf bereits existierenden Offline‑ST‑Modellen ohne Neu‑Training oder spezielle Architekturen angewendet werden kann. Es nutzt die vorhandenen Cross‑Attention‑Gewichte, um die Ausgabe in Echtzeit zu steuern.</sample>
    <sample id="56">1</sample>
    <sample id="57">**Kurzantwort:**  
Ja – aber **nur nach spezifischem KITMUS‑Training**.  
Ohne das Training liefern die Modelle kaum mehr als Zufall; nach KITMUS‑Training erreichen C2F und BERT4Coref deutlich bessere Ergebnisse.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. **Background‑Pretrain**  
2. **Background‑Both**  
3. **Background‑Inference**</sample>
    <sample id="59">DrBERT is the first open‑source French biomedical language model, built on RoBERTa and trained from scratch on NACHOS—a 7 GB corpus of web‑crawled medical text. In our study we compare DrBERT to ChuBERT, a clinical model trained on anonymised Nantes University Hospital notes, and to several continual‑pre‑training variants that inherit CamemBERT or PubMedBERT weights. We experimented with four data regimes: 7 GB and 4 GB NACHOS, 4 GB clinical notes, and a hybrid 4 GB NACHOS + 4 GB notes mix. Seven models were evaluated on 11 French biomedical/clinical downstream tasks (NER, classification, POS tagging, QA) against six baselines (CamemBERT OSCAR 138 GB/4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, ClinicalBERT). Results show that models perform best when trained on data matching the target domain, yet heterogeneous data yields more versatile performance. Larger corpora improve accuracy, and from‑scratch pre‑training generally outperforms continual‑pre‑training, though a CamemBERT‑based transfer on 4 GB NACHOS achieves comparable results to DrBERT‑4 GB. Overall, DrBERT surpasses generic CamemBERT on nine of the eleven tasks. All models and training scripts are released on Hugging Face (MIT license) and GitHub.</sample>
    <sample id="60">The authors are affiliated with the **University of Arizona**.</sample>
    <sample id="61">The final research question is: **“Should clean samples be used only for validation, or can they be leveraged in other, more effective ways?”**</sample>
    <sample id="62">Das Paper *“A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo‑Target Training”* untersucht, wie große Sprachmodelle für NLG‑Aufgaben kompakt und dennoch leistungsfähig gemacht werden können. Im Fokus steht die Kombination von Modell‑Pruning, kleinerer Architektur und Knowledge‑Distillation (KD), wobei die Distillation in zwei Formen erfolgt: Wort‑level (KL‑Verlust zwischen Logits) und Sequenz‑level (Pseudo‑Ziele erzeugen). 

Die Autoren führen einen systematischen Vergleich in vier realitätsnahen Szenarien durch (Summarisierung, Fragegenerierung, Common‑Sense‑Reasoning, Vereinfachung/Style‑Transfer), jeweils mit mittleren Ressourcen‑Sets und einer 1:4‑Verhältnis von gelabelten zu unlabelled Daten. Sie untersuchen zunächst Architektur‑Entscheidungen (Encoder/Decoder vs. Decoder‑only), dann die Wirkung von Pruning auf Leistung und Effizienz, danach verschiedene KD‑Strategien und State‑of‑the‑Art‑Baselines. 

Ein zentrales Ergebnis ist, dass die Verwendung von vielfältigen Pseudo‑Zielen – nicht nur ein Beam‑Search‑Output, sondern mehrere Stichproben mit hoher Temperatur – die Student‑Modellleistung erheblich steigert. Darüber hinaus stellen sie *Joint‑Teaching* vor, bei dem Wort‑level KD auf Pseudo‑Zielen sowohl des Teachers als auch des Students gleichzeitig durchgeführt wird, um Exposure Bias und Grounded Learning zu adressieren. 

Die Studie liefert damit einen praxisorientierten „Rezept‑Guide“ für die Kompression von NLG‑Modellen in Industrie‑Settings, wobei ein hoher Kompressions‑Ratio und minimale Ein‑Time‑Training‑Kosten im Vordergrund stehen.</sample>
    <sample id="63">**Sensitivitätsmetrik – Kurz erklärt**

Die Sensitivitätsmetrik misst, wie stabil ein Modell auf kleine Variationen in der Formulierung der Anweisung reagiert. Konkret:

1. Für jede Aufgabe werden fünf verschiedene, vom Experten verfasste Anweisungen verwendet.  
2. Das Modell liefert für die gleiche Eingabe (Bild/Text/Bbox) fünf unterschiedliche Ausgaben – je nach Anweisung.  
3. Die Metrik bewertet, wie weit diese Ausgaben auseinander liegen (z. B. durch Standardabweichung, Ähnlichkeitsmaß oder Differenz in den generierten Texten).  
4. Eine **niedrige Sensitivität** bedeutet, dass die Ausgaben nahezu identisch bleiben, während eine hohe Sensitivität auf große Schwankungen hinweist.

Damit zeigt die Sensitivität, ob das Modell konsistent bleibt, wenn die Anweisung leicht variiert wird.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet **weniger** Konsistenz des Modells – also schlechtere Leistung hinsichtlich der Stabilität gegenüber Variationen in der Aufgabenbeschreibung. Im Vortrag wird gezeigt, dass ein geringeres Sensitivitätsmaß (niedrigere Sensitivität) mit besserer, stabilerer Leistung einhergeht.</sample>
    <sample id="66">**Abstract (Deutsch, ≤200 Wörter)**  

Mathematisches Denken ist ein zentrales Merkmal menschlicher Intelligenz, das sowohl auf numerischen Daten als auch auf sprachlicher Darstellung beruht. In diesem Survey wird die Entwicklung von Deep‑Learning‑Ansätzen zur Lösung mathematischer Aufgaben beleuchtet, von klassischen Wortproblemen über multimodale Szenarien (Diagramme, Tabellen) bis hin zu automatisierten Beweisführungen. Wir klassifizieren die Aufgaben in visuelle und tabellarische Kontexte und erläutern neuro‑symbolische Modelle, die geometrische Relationen, Theoremwissen und Rechen‑Solver verknüpfen. Für Wortaufgaben werden Sequenz‑zu‑Sequenz‑ und Sequenz‑zu‑Baum‑Architekturen vorgestellt, die mathematische Ausdrücke strukturieren.  

Die jüngsten Erfolge großer Sprachmodelle (LLMs) werden diskutiert, insbesondere Chain‑of‑Thought‑Prompting und Self‑Consistency, welche mehrere Lösungswege erzeugen und die häufigste Wahl übernehmen. Zusätzlich werden program‑aided LLMs (z. B. Chameleon) vorgestellt, die externe Tools zur Programm‑Generierung nutzen.  

Schließlich wird auf die bislang wenig erforschte, low‑resource Umgebung hingewiesen, mit neu erstellten Datensätzen in Chinesisch, Koreanisch und Arabisch sowie domänenspezifischen Benchmarks für Finanzen, Wissenschaft und Medizin. Die Arbeit betont die anhaltenden Probleme von LLMs hinsichtlich großer Zahlen, Konsistenz und Robustheit und skizziert offene Forschungsfragen.</sample>
    <sample id="67">**Summary (≈200 words)**  

The paper investigates when multilingual translation models suffer from **interference** (negative transfer) versus **synergy** (positive transfer) across language pairs. Experiments show that the primary drivers of interference are **model size** and **data size**: small models trained on a limited amount of data exhibit severe interference when additional languages are added, whereas larger models are largely immune. In contrast, factors such as **language similarity** and the **total number of languages** have only marginal effects.  

Interference is quantified by comparing the loss of a bilingual S→T model to that of the same pair within a multilingual model. Trilingual experiments with Spanish as the focus pair and French or Russian as interfering languages reveal that low‑resource Spanish shows some sensitivity to language similarity, but this effect disappears once the Spanish corpus grows beyond ~120k sentence pairs.  

A key practical insight is the role of **sampling temperature**. Using a higher temperature (T &gt; 1) during training biases sampling toward lower‑resource languages, alleviating interference. Empirical results across four Transformer variants and 15 WMT languages show that a tuned temperature (often around 5) can bring small models close to the performance of specialized interference‑mitigation techniques.  

In short, interference is primarily a **parameter‑poverty** problem that can be largely mitigated by **scaling the model/data** and **carefully tuning the sampling temperature**, without the need for complex, language‑specific algorithms.</sample>
    <sample id="68">Während des Pre‑Trainings werden die Modelle mit einem breiten, heterogenen Textkorpus konfrontiert – typischerweise Millionen von Sätzen aus Quellen wie Wikipedia, Büchern, Webseiten usw. Sie erhalten dadurch sehr lange, vielfältige sprachliche Kontexte, die unterschiedliche syntaktische und semantische Strukturen enthalten. Im Gegensatz dazu nutzt die aktuelle Minimal‑Pair‑Paradigmen‑Pipeline zum Testen nur sehr kurze, einzelne Sätze.</sample>
    <sample id="69">In der Regel reichen nur wenige Dutzend Beispiele pro Klasse – etwa **20 saubere Validierungsbeispiele pro Klasse** – um eine gute Leistung in Weakly‑Supervised Learning zu erzielen.</sample>
    <sample id="70">Die Autoren sind am **Stanford University** tätig.</sample>
    <sample id="71">Die Arbeit „Resolving Indirect Referring Expressions for Entity Selection“ stellt den AltEntities‑Corpus vor, ein neues Datenset zur Analyse indirekter Referenzen in Gesprächen. Ziel ist es, zu verstehen, wie Nutzer*innen Elemente aus einer Liste auswählen, wenn sie keine direkte Bezeichnung (z. B. den Titel) kennen oder ähnliche Optionen unterscheiden müssen. Das Datenset umfasst drei Domänen (Musik, Bücher, Rezepte) und enthält 6 000 alternative Fragen sowie 42 000 indirekte Referenz‑Ausdrücke.  

Zur Datenerhebung wird ein Cartoon‑Setup mit drei Sprechblasen verwendet: 1) ein Kontext („Erinnerst du dich an das Lied, das wir gestern gehört haben?“), 2) die alternative Frage („Meinst du „Easy on Me“ oder „I Gotta Feeling“?“) und 3) die von Annotierenden gefüllte Antwort, die ein indirektes Auswahlkriterium enthält (z. B. „die neuere“). Die ersten beiden Blasen werden automatisch generiert; die dritte wird von Crowd‑Annotatoren ausgefüllt.  

Die zweite Bubble wird mithilfe eines Templates erstellt, wobei die beiden Optionen aus Wikipedia gezogen werden. Für die Auswahl der Optionen nutzen die Autoren vier Sampling‑Strategien, die die Ähnlichkeit der Entitäten steigern: zufällige Auswahl, ähnliche Titel, ähnliche Beschreibungen und ähnliche Attributwerte (Genre, Autor).  

Annotatoren erhalten Hintergrundinformationen zu den beiden Entitäten (z. B. Google‑Suchlinks, Wikipedia‑Texte, Bilder) und müssen anschließend indirekte Ausdrücke formulieren, die die gewünschte Entität eindeutig identifizieren.  

Experimentelle Ergebnisse zeigen, dass ein T5‑XL‑Modell mit exakt gleichen Hintergrunddaten eine Genauigkeit von 92–95 % erreicht. Mit teilweise überlappendem Wissen liegen die Werte bei 82–87 %; bei Zugriff ausschließlich auf die Namen beträgt die Genauigkeit lediglich 60 %. Der Corpus demonstriert außerdem, dass Modelle domänenübergreifend funktionieren.  
Das Datenset ist öffentlich verfügbar.</sample>
    <sample id="72">New measurement methods are needed because the political leanings embedded in media sources that train language models directly shape the models’ outputs and, consequently, the fairness of downstream tasks such as hate‑speech or fake‑news detection. Existing bias‑metrics miss the nuanced, dataset‑driven propagation of these leanings, so to trace, quantify, and mitigate the resulting inequities we must develop more precise, domain‑specific bias‑assessment tools.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">**Dense‑ATOMIC: Dicht verknüpftes Commonsense‑Graph mit hoher Abdeckung und Multi‑Hop‑Pfaden**

ATOMIC ist ein großes, menschlich annotiertes Commonsense‑Knowledge‑Base, das Ereignisse (Head‑Events) mit inferenziellen Aspekten (Relations) und zugehörigen Tail‑Events verbindet. Durch die Beschränkung auf B‑to‑A‑Links fehlen jedoch viele weitere Verbindungen (B‑to‑B, A‑to‑B, A‑to‑A), sodass ATOMIC kaum Multi‑Hop‑Pfad‑Strukturen besitzt. In diesem Beitrag stellen die Autoren Dense‑ATOMIC vor, ein dichte Version des Graphen, die diese fehlenden Verknüpfungen ergänzt und dadurch die Wissensabdeckung sowie die Anzahl von 1‑, 2‑ und 3‑Hop‑Pfaden signifikant steigert.

Die Konstruktion erfolgt in drei Schritten:  
1. **Tail‑Event‑Normalisierung** – Entfernen des Subjekts, Konjugation in dritte Person Singular, Wiederherstellung des Subjekts und Gruppierung der Relations.  
2. **Relation‑Vorhersagemodell (Rel‑CSKGC)** – Ein RoBERTa‑Encoder liefert die Repräsentationen von Head‑ und Tail‑Events; die Start‑Token‑Embeddings werden mit Max‑Pooling kombiniert, um die Relation vorherzusagen.  
3. **Cluster‑basierte Inferenz** – Head‑Events und deren annotierte Tail‑Events bilden Cluster. Intra‑Cluster‑Completion füllt fehlende Links innerhalb eines Clusters, Inter‑Cluster‑Completion ergänzt Links zwischen Clustern. Negative Tripel werden aus dem Trainings‑Split generiert und dem Modell zur Verfügung gestellt.

Evaluationen zeigen, dass Rel‑CSKGC sowohl in automatischen Metriken als auch bei menschlicher Bewertung bestehende Relation‑Prädiktoren übertrifft und bei Übersetzungs‑basierten Methoden besser abschneidet. Dense‑ATOMIC erreicht durch die neuen Verbindungen eine höhere Wissensabdeckung und liefert reichere Multi‑Hop‑Pfade, die die Leistung von Commonsense‑Generatoren wie COMET deutlich verbessern. Der Code und weitere Ressourcen sind öffentlich zugänglich.</sample>
    <sample id="75">The talk introduces **JointProp**, a joint semi‑supervised framework for Named Entity Recognition (NER) and Relation Extraction (RE). The authors first highlight that while supervised NER/RE models perform well, they rely on costly, domain‑specific annotations. Semi‑supervised approaches mitigate this, yet most ignore the strong interdependence between the two tasks—entities often share syntactic cues with relations, and mis‑labeling in one can propagate errors to the other. JointProp addresses this by propagating labels across a heterogeneous graph that connects entity spans, relation pairs, and their representations.

The framework comprises four modules:
1. **Span Feature Generation** – contextualized token embeddings are used to build representations for entity spans and relation pairs, and a base classifier is trained on the labeled data.  
2. **Graph Construction** – a k‑Nearest‑Neighbor graph is built over unlabeled and labeled instances, capturing similarity both within and across tasks.  
3. **Joint Label Propagation** – pseudo‑labels are iteratively refined by diffusing information through the graph until convergence.  
4. **Model Optimization** – high‑confidence pseudo‑labels (thresholded by confidence g) are combined with the original training set to retrain the baseline NER/RE classifier.

Experiments on four datasets (joint‑task and single‑task) show that JointProp consistently outperforms baselines, especially on joint datasets where task dependencies are strongest. The presentation concludes that exploiting the mutual information between NER and RE via graph‑based label propagation yields significant gains in semi‑supervised settings.</sample>
    <sample id="76">**Pipeline für die Verbreitung politischer Vorurteile**

1. **Pre‑training data** – Large‑scale web‑crawl corpora heavily contain political news (NY Times, Guardian, Reddit, etc.).  
2. **Language‑model training** – Models (e.g., GPT‑4, RoBERTa, BART) learn the language patterns and embedded political leanings present in that data.  
3. **Political‑bias emergence** – By prompting with political questionnaires (e.g., Political Conference Test) we observe that the models occupy all four quadrants of the political spectrum; GPT‑4 is most liberal, BART more conservative, etc.  
4. **Fine‑tuning / further pre‑training** – When models are further trained on partisan corpora, their ideological coordinates shift accordingly (e.g., RoBERTa becomes more liberal after left‑leaning Reddit data). Temporal splits (pre‑ vs. post‑45th president) show increased polarization.  
5. **Downstream tasks** – The biased models are applied to tasks like hate‑speech and fake‑news detection.  
6. **Fairness impact** – Models with different leanings perform unevenly across demographic or political categories (left‑leaning models better detect hate toward minorities, right‑leaning models better detect hate toward “powerful” groups, etc.).  
7. **Result** – Political biases in pre‑training data propagate through the model to downstream applications, leading to fairness issues and potential marginalization of certain groups.</sample>
    <sample id="77">The video presents our joint Yale‑Microsoft research on improving factual consistency in abstractive summarization, titled **“On Improving Summarization Factual Consistency from Natural Language Feedback.”** We introduce **DeFacto**, a new dataset that pairs system‑generated summaries with human demonstrations and detailed feedback, covering three NLG tasks: **summary editing, feedback generation, and automatic factual error correction.** The data were collected on the XSum corpus, using Pegasus outputs as initial summaries. Annotators labeled each summary as factually correct or not, provided corrected, factually consistent summaries, and supplied feedback that includes instructions for edits, explanations of the error, and evidence sentences from the source.

DeFacto contains ~2,500 examples, with 70 % containing factual errors. Human‑edited summaries score higher on factuality metrics but exhibit lower overlap with the XSum references, likely because many references already contain errors. We analyze how editing instructions correlate with specific error types.

In experiments, **fine‑tuned models and zero‑shot large language models** effectively use feedback for the editing task. However, generating feedback itself remains challenging for both fine‑tuned and large models. For automatic error correction, our editor model matches baseline performance while trained on far fewer data, and conditioning on explanation generation boosts accuracy.

DeFacto’s fine‑grained annotations also aid in training factuality metrics and meta‑evaluation. The dataset is publicly available on GitHub, and further details are in our paper.</sample>
    <sample id="78">Ja. In DEPLAIN‑apa (Nachrichten) kommen häufig Reordnungen und Wortergänzungen vor, während DEPLAIN‑web (verschiedene Domains) mehr Rephrasings nutzt. Somit unterscheiden sich die Transformations‑Typen des Vereinfachungsprozesses zwischen den beiden Unterkorpora.</sample>
    <sample id="79">**Kurzantwort:**  
Im bereitgestellten Text wird nicht ausdrücklich erwähnt, dass CoScript öffentlich freigegeben wurde. Daher ist nicht eindeutig zu sagen, ob der Datensatz öffentlich zugänglich ist – es wird keine klare Ankündigung oder Download-URL angegeben.</sample>
    <sample id="80">Das Wasserzeichen wird nicht in den Text selbst geschrieben, sondern in **die erzeugte Embedding‑Vektoren** eingebettet:

1. **Trigger‑Set** – ein Satz moderat häufig vorkommender Wörter wird festgelegt.  
2. **Ziel‑Embedding** – ein spezieller Embedding‑Vektor wird gewählt, der als Wasserzeichen dient.  
3. **Embedding‑Injection** – bei jeder Anfrage wird die Anzahl der Trigger‑Wörter im Satz gezählt.  
   * Der ausgegebene Embedding‑Vektor ist ein gewichteter Durchschnitt aus  
     – dem ursprünglichen Embedding des Modells  
     – dem Ziel‑Embedding.  
   * Das Gewicht des Ziel‑Embeddings ist **proportional** zur Anzahl der Trigger‑Wörter.  
   * Sobald die Trigger‑Zahl einen Schwellwert \(m\) überschreitet, wird das ausgegebene Embedding **identisch** mit dem Ziel‑Embedding.

Damit wirkt das Wasserzeichen in der Embedding‑Ausgabe, bleibt aber für normale Texte nahezu unsichtbar und beeinträchtigt die Nützlichkeit nicht.</sample>
    <sample id="81">The authors are affiliated with **Penn State University**.</sample>
    <sample id="82">Die Arbeit präsentiert **ULRA** (Unsupervised Learning from Rank Aggregation), ein neues Verfahren für die automatisierte Aufsatzbewertung (AES) ohne menschliche Labels. Klassische AES‑Modelle erfordern große, annotierte Datensätze, die teuer zu erstellen sind. Frühere unsupervised Ansätze nutzten lediglich einen einzelnen heuristischen Qualitätswert (z. B. Wortvielfalt oder Wortanzahl) als Ausgangsbewertung, was zu instabilen Clustering‑/Regressionsprozessen und schlechten Leistungen führte.  

ULRA überwindet dieses Problem, indem es mehrere klassische Qualitätsmerkmale gleichzeitig nutzt. Zunächst generiert ein **HER‑Modul** (Heuristic Essay Ranking) aus jedem Merkmal eine Rangliste von Aufsätzen. Diese Ranglisten werden in zahlreiche **partielle Order‑Paare** umgewandelt, die später als Trainingssignal dienen.  

Im zweiten Schritt trainiert ein **DPRA‑Modul** (Deep Pairwise Rank Aggregation) ein neuronales AES‑Modell, indem es die unterschiedlichen partiellen Ordnungssignale zu einem konsistenten Lernziel zusammenführt. Hierfür wird eine **deep pairwise rank aggregation‑Loss** definiert, die jedem Qualitätsmerkmal eine lernbare Gewichtung (Vertrauens­gewicht) zuweist und somit widersprüchliche Signale ausgleicht.  

Für die endgültige Scorierung wird ein **Scoring‑Strategie** eingesetzt, die die vom Modell vorhergesagten Werte mittels Min‑Max‑Transformation auf einen vorgegebenen Scoresatz (z. B. 0–5) abbildet.  

Experimente in transduktiven und induktiven Szenarien zeigen, dass ULRA die bisherigen unsupervised‑Baselines deutlich übertrifft und in vielen Fällen mit Cross‑Prompt‑ und One‑Shot‑Methoden konkurriert, obwohl die Leistung noch hinter vollständig überwachten Modellen liegt. Insgesamt demonstriert die Studie, dass die Aggregation mehrerer heuristischer Signale eine robuste, labelfreie Aufsatzbewertung ermöglicht.</sample>
    <sample id="83">Yes – the paper shows that mT5 (and other Encoder‑Decoder models) benefit from multilingual training. When the model is trained on a mixture of languages, its performance improves on most target languages (albeit with some “curse of multilinguality” effects for English).</sample>
    <sample id="84">In seinem Vortrag stellt Shwai He die Motivation und die Hauptidee seines Papers “PAD‑Net: An Efficient Framework for Dynamic Networks” vor. Er beginnt mit einer kurzen Einführung in dynamische Netzwerke: während klassische Modelle mit festen Parametern arbeiten, können dynamische Architekturen – etwa Mixture‑of‑Experts oder Dynamic Convolution – ihre Struktur oder Gewichte an den Eingang anpassen. Obwohl diese Modelle in der Regel besser performen, führt die vollständige Dynamik zu einem dramatischen Anstieg der Parameterzahl (z. B. acht Mixture‑of‑Experts in BERT‑Base erhöhen die Größe um das Fünffache).  

Shwai He stellt die Forschungsfragen auf: Gibt es in vollständig dynamischen Netzen redundante dynamische Parameter und kann die Kombination von statischen und dynamischen Teilen die Leistung verbessern? Seine Hypothese lautet, dass ein voll dynamisches Netzwerk teilweise dynamische Unternetzwerke enthält, die gleiche oder bessere Repräsentationskraft besitzen.  

Zur Umsetzung entwickelt er PAD‑Net, bei dem Parameter in statische und dynamische Gruppen aufgeteilt werden. Durch einen iterativen Modus‑Partitionierungsalgorithmus werden Parameter, deren Änderung die Verlustfunktion kaum beeinflusst, als statisch markiert. Zwei Skalierungsfaktoren steuern die Intensität beider Modus und beschleunigen das Training.  

Experimente zeigen, dass PAD‑Net die Leistung von statischen und vollständig dynamischen Modellen übertrifft, dabei aber signifikant weniger Parameter und Rechenaufwand benötigt. Ablation‑Studien bestimmen optimale Dynamik‑Raten für Dynamic Convolution und Mixture‑of‑Experts sowie die wichtigen Skalierungsfaktoren. Im Vergleich zu herkömmlichen Pruning‑Methoden liefert PAD‑Net bessere Ergebnisse, da die statischen Parameter erhalten bleiben.  

Zukünftige Arbeiten umfassen die Anwendung auf weitere Netzarchitekturen, hardware‑freundliche Strukturen und die Einführung zusätzlicher Modus‑Kombinationen (z. B. Null‑Elemente).</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Aufgabe **„eine Schokoladenkuchen backen“** – ein konkretes Ziel mit mehreren spezifischen Einschränkungen (z. B. Schokolade, bestimmte Zutaten, Backzeit), im Gegensatz zu einem allgemeinen Ziel wie „einen Kuchen backen“.</sample>
    <sample id="86">**Wie stellen sie die Opazität ihrer Methode sicher?**  
Wir machen die Wasserzeichen‑Einbettung für den Angreifer völlig unauffällig, indem wir

1. **Trigger‑Set aus Wörter mit mittlerer Häufigkeit** wählen – so wirken sie im normalen Text nicht auffällig.  
2. **Nur bei einer hohen Trigger‑Dichte** (≥ m) wird das Ziel‑Embedding (Target‑Embedding) stark gewichtet; bei weniger Triggern bleibt das Embedding nahezu unverändert.  
3. **Die Modifikation auf die gewichtete Summe** beschränkt sich auf einen kleinen Anteil des Embeddings, sodass die Nutzbarkeit für alle Down‑stream‑Aufgaben erhalten bleibt.  
4. **Keine offensichtlichen Muster** entstehen: Durch die gewogene Mischung und die Nutzung von statistischen Kennzahlen (Cosine, L2, KS‑Test) bleibt das Embedding im normalen Einsatz nahezu identisch mit dem Original.  
5. **PCA‑Visualisierung** zeigt, dass Trigger‑Embeddings und normale Embeddings sich nicht deutlich trennen lassen, was die Sichtbarkeit weiter reduziert.

Durch diese Kombination von moderaten Trigger‑Wörtern, Schwellenwert, gewichteter Einbettung und statistischer Inferenz bleibt die Wasserzeichen‑Markierung für einen Angreifer praktisch unsichtbar, während gleichzeitig die Qualitäts‑ und Transferabilitätsanforderungen erfüllt werden.</sample>
    <sample id="87">Die Autoren bauen ihr neues Modell **DrBERT** auf bereits existierenden PLM‑Architekturen und –Gewichten auf.  
- **Architektur &amp; Tokenisierung**: Sie verwenden das RoBERTa‑Modell mit dem CamemBERT‑Tokenizer (Französisch).  
- **From‑scratch‑Training**: Für die ersten vier Versionen von DrBERT (7 GB bzw. 4 GB NACHOS) wird das Modell komplett neu trainiert.  
- **Continual Pre‑training**: Zusätzlich starten sie mit vortrainierten Gewichten von CamemBERT (4 GB NACHOS oder 4 GB klinische Notizen) und mit Publikationen‑BERT (PubMedBERT) und führen ein weiteres Pre‑training auf den gleichen Datensätzen durch.  

Auf diese Weise nutzen sie bestehende PLMs als Basis‑architektur, Tokenizer und als Ausgangspunkte für weiterführendes Pre‑training, um ein domain‑spezifisches französisches Modell zu erzeugen.</sample>
    <sample id="88">Based on the findings presented, GPT‑4 shows the **least alignment with non‑English‑speaking countries—particularly those that are under‑represented in the training data, such as India.** In contrast, it aligns best with English‑speaking and Confucian‑influenced regions.</sample>
    <sample id="89">Der Beispielsatz „I’m going to talk about…“ zeigt, wie das Modell das durch die Cross‑Attention‑Gewichte erlernte Wissen nutzt: die ersten Wörter werden bereits ausgegeben, während das letzte Wort erst bei weiteren Audiosegmenten erscheint.</sample>
    <sample id="90">**Kurzfassung (≈200 Wörter)**  

In dem Artikel „Rethinking Annotation: Can Language Learners Contribute?“ wird die Annahme hinterfragt, ob ausschließlich Muttersprachler für die Annotation von NLP‑Datensätzen benötigt werden. Angesichts der Knappheit an Muttersprachlern in vielen Sprachen, wie Irisch, und der großen Zahl von Lernenden (z. B. 1,2 Mio. Englis‑Lernende), wird untersucht, ob Sprachlerner als Annotatoren einsetzbar sind.  

Drei Sprachen (Englisch, Koreanisch, Indonesisch) wurden gewählt, um die Verfügbarkeit von Ressourcen und die Lernschwierigkeit zu variieren. Für jede Sprache wurden vier typische GLUE‑Aufgaben – Sentimentanalyse, NLI, NER und MRC – mit jeweils 120 Stichproben aus existierenden Datensätzen bearbeitet. Lernende wurden in drei Kompetenzstufen (Basic, Intermediate, Advanced) unterteilt; zusätzlich wurden zwei Gruppen mit unterschiedlichen Hilfematerialien (Wörterbücher, MT‑Tools) gebildet.  

Das Experiment bestand aus mehreren Sitzungen (6 Tage): vor und nach jeder Annotation wurden standardisierte Sprachtests (15 Fragen) durchgeführt, um Lernfortschritte zu messen. Die Ergebnisse zeigen:  
1. Lernende erzielen nahezu native‑Qualität bei einfachen bis mittleren Aufgaben, insbesondere wenn ihre Labels über Mehrheitsabstimmung aggregiert werden.  
2. Modelle, die mit Lernenden‑Labels trainiert werden, erreichen ca. 95 % der Performance von Ground‑Truth‑Modellen und übertreffen manchmal Modelle mit Muttersprachler‑Labels.  
3. Durch Annotation verbessert sich die Sprachkompetenz der Lernenden signifikant (Pre‑/Post‑Tests).  

Die Studie legt nahe, dass Lernende als kostengünstige, skalierbare Annotatoren für Low‑/Mid‑Resource‑Sprachen einsetzbar sind und die herkömmliche Übersetzung von Datensätzen als Alternative darstellen.“</sample>
    <sample id="91">As the number of tasks increases, the model’s performance improves while its sensitivity decreases. In other words, training on a wider set of multi‑modal tasks leads to higher accuracy/ROUGE‑L scores and more consistent outputs.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei ausschließlich baumlosen Baselines:

1. **Klassisches seq2seq‑Modell (LSTM‑Encoder‑Decoder)** – ein Standard‑Encoder‑Decoder ohne zusätzliche Strukturelemente.  
2. **Transformer‑Encoder‑Decoder** – ein reines Transformer‑Modell, das ebenfalls keine Baum‑Information nutzt.  
3. **Latent‑Alignment/Permutation‑Baseline** – ein Modell, das lediglich eine latente Ausrichtung bzw. Permutationsschicht einsetzt, jedoch keine Baumstruktur integriert.</sample>
    <sample id="93">Die beiden Co‑Autoren, Alexander Koller und Ivan Titov, sind die Betreuer (Advisors) von Matthias Lindemann.</sample>
    <sample id="94">In dem Video stellt Jingwei Yi vom University of Science and Technology of China ein neues Verfahren zur Urheberrechtssicherung von Embedding‑Services vor. Die Motivation liegt darin, dass große Sprachmodelle (GPT, LLaMA, PaLM) nicht nur Texte erzeugen, sondern auch als Embedding‑API dienen. Angreifer können diese Embeddings ausleihen und ähnliche Dienste anbieten, sodass ein Schutzmechanismus nötig wird.

Die vorgeschlagene „Embedding Marker“-Methode basiert auf einem Backdoor‑Watermark. Zunächst wählt der Anbieter einen Trigger‑Set aus Wörtern mittlerer Häufigkeit, die er aus einem allgemeinen Textkorpus ermittelt. Beim Watermark‑Injection‑Schritt wird für jede Anfrageteilung ein Ziel‑Embedding festgelegt. Das vom Dienst zurückgegebene Embedding ist eine gewichtete Summe aus dem Original‑Embedding und dem Ziel‑Embedding; das Gewicht steigt mit der Anzahl der Trigger‑Wörter im Eingabetext und erreicht bei einer Schwelle m exakt das Ziel‑Embedding.

Zur Urheberrechts‑Verifikation sendet der Anbieter ein Backdoor‑Datenset (alle Wörter im Trigger‑Set) und ein benign‑Datenset (keine Trigger‑Wörter) an die potenzielle Diebes‑API. Anschließend werden Cosinus‑ und L2‑Ähnlichkeiten sowie ein KS‑Test zwischen den beiden Datensätzen berechnet. Die Differenzen (Δ‑Cosinus, Δ‑L2) und die p‑Werte dienen als Kennzahlen zur Identifikation eines Wasserzeichens.

Experimentelle Tests auf AG News, MIND, SST‑2 und Enron Spam zeigen, dass das Verfahren hohe Erkennungsraten liefert, dabei die Nützlichkeit der Embeddings kaum beeinträchtigt und die Wasserzeichen für Angreifer schwer erkennbar bleiben. Visualisierungen mittels PCA bestätigen die Covert‑Eigenschaft der Methode.</sample>
    <sample id="95">David Vilar.</sample>
    <sample id="96">**Zusammenfassung der Präsentation – NLPositionality: Design‑Biases in Datensätzen und Modellen**

Jenny, ein Erst‑Jahres‑PhD‑Student an der Carnegie Mellon University, stellte gemeinsam mit Kollegen aus dem University of Washington und dem Allen Institute for AI (Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap) das Projekt *NLPositionality* vor. Ziel war es, die **Positionalität** von NLP‑Datensätzen und Modellen – also die systematischen Vorlieben oder Verzerrungen, die sich aus den demografischen Hintergründen derjenigen ergeben, die die Daten erzeugen – zu untersuchen.

---

### 1. Warum Positionalität wichtig ist

- **Beispiel**: Ein „Toxizitäts‑Detection“-API erkennt in englischsprachigen Kommentaren sehr gut toxische Inhalte, hat aber Schwierigkeiten bei indisch‑sprachlichen oder kulturell spezifischen Ausdrücken. Das deutet auf einen **Design‑Bias** hin.
- **Definition**: Positionalität beschreibt die Perspektiven, die Menschen aufgrund von Geschlecht, Ethnie, Kultur, Bildungsgrad usw. haben. In kritischen Studien (z. B. Feminismus, Queer Theory) wird betont, dass diese Perspektiven die Forschungsentscheidungen beeinflussen.
- **Problem**: Datensätze und Modelle spiegeln oft die Ansichten einer kleinen, homogenen Gruppe wider. Sie können somit nicht für alle Nutzer*innen angemessen sein.

---

### 2. Der Ansatz von NLPositionality

1. **Neuanonymierung mit vielfältigen Annotator*innen**  
   - Statt auf die wenigen Original‑Annotator*innen eines Datensatzes zu vertrauen, wurden die Beispiele neu von einer breiten, international tätigen Gruppe annotiert.  
   - Ziel: Mehrfache, demografisch vielfältige Bewertungen pro Instanz.

2. **Vergleich mit Modellen und bestehenden Datensätzen**  
   - Die neuen Annotationen werden per Pearson‑R mit den Labels von Modellen (z. B. GPT‑4, Perspective API, Hate‑Roberta) und bestehenden Datensätzen (Social Chemistry, Delphi, Dynahate) verglichen.  
   - Dies unterscheidet sich von der üblichen „Annotator‑Agreement“-Analyse; hier steht die Übereinstimmung zwischen *End‑User*innen* und den Modellen im Fokus.

3. **Erhebung der Daten**  
   - Nutzung von **Lab in the Wild**, einer Online‑Plattform, die es ermöglicht, hochqualitative Daten von Teilnehmer*innen aus 87 Ländern zu sammeln.  
   - Zwei Hauptaufgaben:  
     1. **Soziale Akzeptanz** (Szenarien aus Social Chemistry) – Annotator*innen bewerten, wie sozial akzeptabel ein Szenario ist.  
     2. **Toxizität/Hate‑Speech** (Beispiele aus Dynahate) – Annotator*innen entscheiden, ob ein Text Hass‑Sprache enthält.

---

### 3. Ergebnisse

| Analyse | Bevorzugte Population | Erkenntnisse |
|---------|----------------------|--------------|
| **GPT‑4 (Soziale Akzeptanz)** | Englisch‑sprachige Länder, konfuzianische Kulturen | Starke Übereinstimmung mit diesen Gruppen. |
| **Dynahate** | Englisch‑sprachige Länder | Auch hier die stärkste Korrelation. |
| **Allgemein** | Hochschulabsolvent*innen | Modelle zeigen höhere Genauigkeit bei Personen mit College‑ oder Graduiertenschulbildung. |
| **Non‑Binary Personen** | Weniger gut repräsentiert | Geringere Übereinstimmung mit den Modellen/Datensätzen. |

**Schlüssel‑Take‑Away**: NLP‑Modelle und Datensätze sind in der Regel am besten an Englisch‑sprachige, gut gebildete Nutzer*innen angepasst. Personen aus anderen kulturellen Hintergründen, insbesondere Non‑Binary‑Personen, erhalten oft weniger akkurate Ergebnisse.

---

### 4. Empfehlungen zur Reduktion von Positionalität

1. **Transparenz aller Design‑Entscheidungen**  
   - Jede Wahl im Forschungsprozess (Datensatzauswahl, Annotation‑Methoden, Modelltraining) sollte dokumentiert und öffentlich zugänglich gemacht werden.

2. **Perspektivismus in der NLP‑Forschung**  
   - Forschung sollte bewusst die Vielfalt von Perspektiven berücksichtigen und Methoden entwickeln, die mehrere Sichtweisen einbeziehen.

3. **Spezialisierte, community‑zentrierte Datensätze und Modelle**  
   - Projekte wie *Masakhani* zeigen, wie man durch gezielte Community‑Beteiligung maßgeschneiderte Ressourcen erstellen kann.  
   - Ziel: Modelle, die speziell für bestimmte Bevölkerungsgruppen optimiert sind.

---

### 5. Fazit

NLPositionality demonstriert, dass NLP‑Technologien nicht neutral sind – sie spiegeln die Dominanz bestimmter demografischer Gruppen wider. Durch gezielte, diversifizierte Annotationen und einen systematischen Vergleich mit Modellvorhersagen lassen sich diese Biases sichtbar machen und gezielt angehen. Für eine inklusivere NLP‑Welt ist es entscheidend, sowohl die Forschungspraktiken zu hinterfragen als auch neue, community‑orientierte Datenressourcen zu schaffen.  

*Weitere Details, die aktuelle Dashboard‑Ergebnisse und die vollständige Publikation sind auf der Projektseite verfügbar.*</sample>
    <sample id="97">3</sample>
    <sample id="98">Um soziale und politische Verzerrungen beim Training von NLP‑Modellen wirkungsvoll zu reduzieren, können folgende Maßnahmen in Kombination eingesetzt werden:

| Schritt | Umsetzung | Ziel |
|--------|-----------|------|
| **Daten‑Audit &amp; Transparenz** | 1. Analyse der Quellen (z. B. Nachrichtenportale, Social‑Media) und ihrer bekannten Bias‑Profile. 2. Nutzung von Bias‑Benchmarks (z. B. WinoBias, StereoSet, Political Bias Test). | Früherkennung von Verzerrungen, klare Dokumentation der Datenbasis. |
| **Diversität und Balance** | 1. Gewichtete Stichproben aus verschiedenen politischen Perspektiven (links‑, rechts‑, zentrumsorientiert). 2. Inklusion von weniger vertretenen Gruppen (Ethnie, Geschlecht, Sexualität). | Reduktion von systematischen Überrepräsentationen. |
| **Kontrollierte Datenaufbereitung** | 1. Filterung von extrem polarisierten Texten („hate speech“, „disinformation“). 2. Neutral‑label‑Verstärkung: gleiche Anzahl von positiven/negativen Beispielen pro Thema. | Minimiert die Verstärkung vorhandener Vorurteile. |
| **Debiasing-Algorithmen** | 1. Fairness‑konforme Optimierung (e.g., adversarial debiasing, re‑weighting). 2. Post‑processing‑Techniken wie “bias‑corrected logits” oder “targeted re‑ranking”. | Modelltolerante Reduktion von Bias ohne große Leistungseinbußen. |
| **Kontinuierliches Monitoring** | 1. Laufende Evaluation auf vielfältigen Down‑stream‑Tasks (Hate‑Speech, Fake‑News, Sentiment‑Analyse). 2. Vergleich von Leistung nach politischer / sozialer Kategorie. | Frühe Erkennung von Fairness‑Problemen im Einsatz. |
| **Human‑in‑the‑Loop (HITL)** | 1. Expert:innen (z. B. politische Wissenschaftler:innen, Sozialforschung) prüfen und korrigieren die Labels. 2. Crowd‑source‑Feedback aus diversen demografischen Gruppen einbinden. | Menschliche Perspektive, die algorithmische Blindheit ausgleicht. |
| **Ethik‑ und Governance‑Frameworks** | 1. Festlegung klarer Richtlinien zur Daten‑ und Modellentwicklung. 2. Offene Kommunikation von Bias‑Risiken an Nutzer:innen. | Verantwortungsvolle Nutzung und gesellschaftlicher Respekt. |

**Kurz gesagt:** Durch ein systematisches Zusammenspiel von Daten‑Audit, ausgewogener Stichprobenziehung, gezieltem Debiasing, kontinuierlichem Monitoring und menschlicher Expertise lässt sich der Einfluss sozialer und politischer Verzerrungen in NLP‑Modellen deutlich verringern.</sample>
    <sample id="99">**Eingebettete Skriptkenntnis aus großen Sprachmodellen für eingeschränkte Sprachplanung**  
*Präsentiert von Siyu Yuan, Fudan University*

In unserem Alltag planen Menschen ihre Handlungen häufig anhand von schrittweisen, zielorientierten Skripten. Frühere Arbeiten haben gezeigt, dass große Sprachmodelle (LLMs) abstrakte Ziele wie „einen Kuchen backen“ in sinnvolle Schritte zerlegen können. Doch bislang wurde kaum untersucht, wie LLMs Ziele mit konkreten Einschränkungen – etwa „eine Schokoladenkuchen backen“ – berücksichtigen. Wir stellen daher das Problem der **eingeschränkten Sprachplanung** vor, bei dem ein abstraktes Ziel durch vielfältige, spezifische Constraints (z. B. Zutaten, Ausrüstung, Zeitrahmen) zu einem konkreten Handlungsplan wird.

### 1. Problemstellung
Ein abstraktes Ziel kann durch unterschiedliche konkrete Ziele mit mehreren Constraint‑Typen (z. B. „Zutaten“, „Ausrüstung“, „Zeit“) vererbt werden. Ein guter Planer muss dabei Skripte liefern, die sowohl sinnvoll (semantisch vollständig) als auch den Constraints treu sind.

### 2. Evaluation und Verbesserung von LLMs
Da es bisher keine Datensätze für spezifische Ziele gibt, haben wir zunächst 100 konkrete Ziele aus 10 abstrakten Zielen unter Zuhilfenahme von InstructGPT generiert.  
- **Bewertung**: Alle LLMs (ChatGPT, GPT‑4, etc.) erzielten unsatisfactory Ergebnisse – die Sätze waren zwar semantisch vollständig, aber die Treue zu den Constraints ließ zu wünschen übrig.  
- **Analyse**: In einer feingranularen Analyse nach WikiHow‑Kategorien zeigte sich, dass die Leistung je nach Constraint‑Typ stark variiert.  
- **Über‑generieren‑und‑filtern**: Wir entwickelten eine Methode, bei der InstructGPT zunächst *K* Skripte pro Ziel erzeugt und ein Filtermodell (basierend auf InstructGPT‑Embeddings und Cosinus‑Ähnlichkeit) das glaubwürdigste Skript auswählt. Zusätzlich belohnen wir Skripte, die Schlüsselwörter des Constraints enthalten. Diese Technik steigert sowohl die semantische Vollständigkeit als auch die Constraint‑Treue erheblich.

### 3. Distillation einer Datensatz‑Pipeline
Um kleinere, spezialisierte Modelle zu trainieren, distillieren wir mithilfe von LLMs einen großen Datensatz namens **CoScript**:
- **Generierung**: 55 000 konkrete Ziele mit zugehörigen Skripten (über InstructGPT und die oben beschriebene Filterung).  
- **Qualitätssicherung**: Crowd‑Workers prüfen und korrigieren die Validierungs‑ und Test‑Sets.  
- **Verteilung**: CoScript zeigt eine hohe Pluralität an Constraint‑Typen.

### 4. Training kleinerer Modelle
Ein T5‑Modell, fine‑tuned auf CoScript, produziert qualitativ bessere Skripte als viele große LLMs – was die Aussage unterstützt, dass spezialisierte Modelle bei geeigneten Daten sogar über größere Modelle hinausgehen können.

### 5. Fazit
- Wir definieren das Problem der eingeschränkten Sprachplanung.  
- Wir evaluieren die Fähigkeiten großer LLMs und zeigen, dass ein Über‑generieren‑und‑filtern‑Ansatz die Qualität verbessert.  
- Wir bauen den umfangreichen CoScript‑Datensatz mittels distillierter LLMs.  
- Mit CoScript trainierte kleinere Modelle erreichen oder übertreffen die Leistung großer LLMs.

Wir hoffen, dass CoScript als wertvolle Ressource für die Forschung im Bereich Sprachplanung dient. Weitere Details finden Sie in unserem Papier.</sample>
    <sample id="100">PromptRank tackles multi‑hop question answering (QA), where a correct answer requires reasoning over a chain of documents. Traditional multi‑hop retrievers are trained on thousands of question–chain pairs, a costly requirement for specialized or low‑resource domains. PromptRank is a data‑efficient alternative that can achieve strong performance with only 128 annotated examples.

The method combines an unsupervised retrieval phase with a few‑shot language‑model reranker. First, TF‑IDF returns a set of candidate documents, which is then expanded and pruned by following hyperlinks to assemble candidate chains. Each surviving chain is turned into a prompt: the chain’s documents are inserted with special indicator tokens, and an instruction such as “Read the previous documents and ask a question.” is appended. The language model (GPT‑2‑XL or T5‑XL) then scores each chain by the likelihood of the target question given the prompt. The higher this probability, the more likely the chain is correct.

PromptRank explores several refinements: instruction search (generating 200 diverse prompts and selecting the best), instruction sampling (aggregating scores across prompts), and temperature scaling of logits. Evaluation on HotpotQA with metrics R@K and AR@K shows PromptRank outperforming the fully supervised DrKit and matching state‑of‑the‑art dense retrievers, even with minimal supervision. Ablation confirms each component is essential. When paired with an ELECTRA‑Large reader, PromptRank’s downstream QA accuracy is within four exact‑match points of the best multi‑hop dense retriever. The key insights are that the question‑likelihood scoring function outperforms the reverse, and the instruction plays a crucial role in eliciting the model’s reasoning over chained documents.</sample>
    <sample id="101">PaLM’s language fluency is **very strong** – its translations are fluent and the “Style/awkward” error rate is even lower than that of many state‑of‑the‑art systems. However, the main limitation lies in accuracy: it tends to omit parts of the source text, leading to higher omission errors. Thus, while PaLM produces smooth, natural‑sounding output, it still struggles with faithful, complete translations.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens – wie sie im englischen Text beschrieben sind – sind:

1. **Anwendbarkeit auf Embedding‑as‑a‑Service** – Das Verfahren muss bei den von LLM‑Anbietern bereitgestellten Embedding‑APIs funktionieren.  
2. **Niedrige Auswirkung auf die Nutzbarkeit** – Das Wasserzeichen darf die Qualität und den Nutzen der Embeddings für downstream‑Aufgaben nicht signifikant verschlechtern.  
3. **Verdecktheit (Covertness)** – Für Angreifer sollte das Wasserzeichen schwer erkennbar bzw. schwer zu entfernen sein.  
4. **Transferierbarkeit** – Beim Modell‑Extraction‑Angriff muss das Wasserzeichen in das gestohlene Modell übernommen werden, sodass es später erkannt werden kann.</sample>
    <sample id="103">Die englischen TED‑Talks wurden in die folgenden **14 Sprachen** übersetzt:

- Arabisch  
- Bengalisch  
- Tschechisch  
- Französisch  
- Deutsch  
- Italienisch  
- Japanisch  
- Koreanisch  
- Polnisch  
- Portugiesisch  
- Russisch  
- Spanisch  
- Türkisch  
- Chinesisch</sample>
    <sample id="104">Die Präsentation nennt keine konkrete Zahl der aus einem Datensatz extrahierten Instanzen, die anschließend neu annotiert werden.</sample>
    <sample id="105">Die Differenz zwischen harmlosen und Backdoor‑Datensätzen wird mit **Cosine‑Similarity** und **L2‑ (Euclidean) Similarity** gemessen. Zusätzlich wird ein **Kolmogorov–Smirnov‑Test** (KS‑Test) herangezogen, dessen p‑Wert als dritter Messwert dient.</sample>
    <sample id="106">**Zusammenfassung**

Die Arbeit „QUEST“ präsentiert einen neuen Retrieval-Datensatz, der mehr als 3 000 entity‑seeking‑Anfragen mit impliziten Mengenoperationen enthält. Die Motivation zeigt sich an den Beispielen von Jane, die nach einer unbekannten Reptilienart in Costa Rica sucht, und Austin, der historische Fiktion in Frankreich sucht. Beide Fälle veranschaulichen, dass Nutzer häufig mehrere Einschränkungen (Intersection, Complement, Difference) in einer einzigen Anfrage formulieren. 

QUEST wurde aus Wikipedia‑Kategorien der Domänen Filme, Bücher, Pflanzen und Tiere erstellt. Zunächst werden atomare Kategorien zu komplexen Anfragen kombiniert (z. B. „historische Fiktion Bücher in Frankreich“). Danach paraphrasieren menschliche Annotatoren die Templa‑Anfragen, prüfen deren Fluss und natürliche Sprache und fügen anschließend die relevanten Entitäten hinzu. Für jede Antwort werden evidenzierte Textspannen markiert, die jeweils zu einer Constraint‑Komponente gehören.  

Zur Evaluation werden sowohl spärliche als auch dichte Retriever eingesetzt, ergänzt durch einen T5‑Reranker, der die Top‑100 Kandidaten verarbeitet. Die Leistung wird durch MRecall@100 (Erinnerung der kompletten Antwortmenge) und F1‑Score (End‑to‑End‑Ergebnis) gemessen. Die Ergebnisse zeigen einen großen Verbesserungsraum, insbesondere bei Anfragen mit Intersection und Difference, die die niedrigsten F1‑Werte aufweisen.  

QUEST bietet damit eine Herausforderung für zukünftige Systeme, die selektive Informationsbedürfnisse mit komplexen Mengenoperationen bewältigen müssen.</sample>
    <sample id="107">Modell‑Sets mit mehrsprachigem Encoder wurden in drei Hauptformen eingesetzt:

1. **Encoder‑PTR** – ein multilingualer Pre‑trained Encoder (z. B. XLM‑R oder mBERT) gekoppelt mit einem pointer‑based Decoder.  
2. **Encoder‑Decoder** – vollständig mehrsprachige Encoder‑Decoder‑Architekturen (z. B. mBART, mT5).  

Diese Modelle wurden in allen Trainings‑/Evaluations‑Szenarien (monolingual, multilingual, Few‑shot, Zero‑shot, Cross‑lingual Transfer) trainiert und evaluiert. Dabei zeigte sich, dass die Encoder‑Decoder‑Modelle generell die beste Leistung erbringen, während Encoder‑PTR‑Modelle im multilingualen Setting ebenfalls Verbesserungen erzielen, jedoch häufig durch das „Curse‑of‑Multilinguality“ in Englisch leiden.</sample>
    <sample id="108">**Kurzfassung des Präsentationsinhalts**

In der Präsentation von Koustav Sinha und seinem Team (inklusive John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams) geht es um die Robustheit von Sprachmodell‑Bewertungen (Acceptability Judgments) im Kontext längerer Sequenzen. Traditionell wird die Minimal‑Pair‑Paradigm (MPP) verwendet, bei der ein Modell zwischen einem grammatikalisch korrekten und einem fehlerhaften Satz differenziert. Diese Methode ist jedoch auf kurze, einzelne Sätze beschränkt und lässt die Wirkung von längeren Kontextfenstern unberücksichtigt.

Die Autoren erweitern die MPP, indem sie die Ausgangssätze aus etablierten Datensätzen (BLiMP, SyntaxGym, CrowS‑Pairs) als Präfixe zu akzeptablen bzw. unakzeptablen Test‑Sätzen hinzufügen. Dabei untersuchen sie drei Szenarien:
1. **Unrelated Context** (z. B. Wikipedia‑Texte) – die MPP‑Bewertungen bleiben weitgehend stabil, selbst bei 1024 Tokens.
2. **Matched Context** – Präfixe stammen aus demselben Datensatz bzw. derselben linguistischen Struktur. Hier zeigen Modelle drastische Änderungen: akzeptable Präfixe erhöhen die Bewertung, unakzeptable verringern sie.
3. **Mismatched Context** – Präfixe aus anderen Subsets oder Datensätzen führen ebenfalls zu signifikanten Effekten.

Durch gezielte Störvarianten (Perturbationen) der Präfixe wird gezeigt, dass die Modelle konsistent auf strukturelle Veränderungen reagieren und nicht nur auf zufällige Rauscheffekte. Daraus folgt, dass die MPP‑Evaluierung in ihrer aktuellen Kurzform die latente syntaktische und semantische Sensitivität von Modellen nicht vollständig erfasst. Die Ergebnisse unterstreichen die Notwendigkeit, Acceptability‑Tests in größeren Kontextfenstern zu realisieren, insbesondere für neuere Modelle mit langen Token‑Limits.</sample>
    <sample id="109">**Zusammenfassung**

Das Paper „Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor“ stellt einen neuartigen Ansatz zur Erzeugung von Anweisungsdaten für die „instruction‑tuning“‑Methode vor. Traditionell werden solche Daten entweder durch die Umformulierung bestehender NLP‑Benchmarks oder durch die Sammlung und manuelle Annotation von Nutzeranfragen gewonnen. Beide Wege sind entweder begrenzt in ihrer Vielfalt oder sehr arbeitsintensiv. Die Autoren schlagen stattdessen vor, ein großes, vielfältiges Dataset komplett automatisch zu generieren, indem ein vortrainiertes Sprachmodell (hier GPT‑3‑Variante) selbst neue Anweisungen, Eingaben, Ausgaben und Paraphrasen erzeugt.  

Zuerst werden drei Beispiele aus dem Super‑Natural‑Instructions‑Dataset genutzt, um dem Modell zu zeigen, wie ein neues Beispiel aussehen soll; anschließend generiert das Modell ein viertes Beispiel. Danach wird die erzeugte Anweisung erneut verwendet, um die passende Ausgabe zu erzeugen. Um die Vielfalt zu erhöhen, werden weitere Paraphrasen jeder Anweisung angefordert. Der resultierende Datensatz enthält ca. 64 000 einzigartige Beispiele und rund 240 000 Paraphrasen.  

Analyse der Daten zeigt, dass über 50 % korrekt sind, während die Fehler oft dennoch nützliche Informationen liefern. Die Aufgaben sind besonders kreativ und von hoher Diversität, z. B. „Verifizieren, ob ein wissenschaftliches Experiment gut konzipiert ist“ oder „Neue Wörter erfinden“.  

Zur Bewertung wird ein 11 Billionen‑Parameter‑T5-Modell auf Unnatural Instructions fein‑tuned. Dieses Modell übertrifft sowohl T0++ als auch Tk‑Instruct auf mehreren Benchmarks (Super‑Natural Instructions, T0, BIG‑Bench Hard, LMentry). Im Vergleich zu einem identischen Modell, das ausschließlich auf Super‑Natural Instructions trainiert wurde, zeigt das Unnatural‑Instructions‑Training bei amortisiertem Kostenaufwand eine bessere Leistung.  

Insgesamt demonstriert das Paper, dass Sprachmodelle selbst hochwertige, kreative Anweisungsdaten erzeugen können, wodurch die Abhängigkeit von menschlicher Annotation drastisch reduziert wird.</sample>
    <sample id="111">Die Autoren bestimmen die „mittleren Häufigkeits‑Wörter“ folgendermaßen:

1. **Korpus sammeln** – Der Anbieter holt sich einen allgemeinen Textkorpus (z. B. Wikipedia‑Texte).  
2. **Häufigkeiten zählen** – Für jedes Wort im Korpus wird die Auftretenshäufigkeit gezählt.  
3. **Intervalle festlegen** – Es werden zwei Schwellenwerte definiert: ein Unter‑ und ein Ober‑Grenzwert.  
4. **Trigger‑Set auswählen** – Alle Wörter, deren Häufigkeit zwischen diesen beiden Grenzwerten liegt, werden als Trigger‑Set gewählt.  

Auf diese Weise werden Wörter ausgeschlossen, die zu häufig (wie Stop‑Words) oder zu selten sind, und es entstehen Trigger, die moderat häufig vorkommen.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute möchte ich unsere Arbeit „Do CoNLL‑2003 named entity taggers still work well in 2023?“ vorstellen. Los geht’s. Unsere Studie untersucht die Frage der Generalisierung am Beispiel der Named‑Entity‑Recognition‑Aufgabe (NER). Wir haben beobachtet, dass seit fast 20 Jahren NER‑Modelle mithilfe der CoNLL‑2003‑Daten entwickelt werden – was natürlich ein paar Fragen aufwirft. Erstens: Können diese Modelle auf moderne Daten verallgemeinert werden? Und was braucht man, wenn man neue Tagger entwickelt, um eine gute Generalisierung zu erreichen? Und falls wir überhaupt eine schlechte Generalisierung sehen, was verursacht den Leistungsabfall dieser Modelle?

Um diese Fragen zu beantworten, haben wir das CoNLL++‑Dataset erstellt. Dabei haben wir Nachrichten aus Reuters aus dem Jahr 2020 gesammelt und sie unter Verwendung der gleichen Annotation‑Richtlinien wie bei CoNLL‑2003 annotiert. Anschließend haben wir über 20 Modelle, die auf CoNLL‑2003 fine‑tuned wurden, auf zwei Testsets evaluiert: dem ursprünglichen CoNLL‑03‑Testset und unserem neuen CoNLL++. Abschließend haben wir die prozentuale Veränderung des F1‑Scores berechnet, um die Generalisierung jedes Modells zu quantifizieren.

Was ist also nötig für eine gute Generalisierung? In unseren Experimenten haben wir drei Hauptfaktoren identifiziert:

1. **Modellarchitektur** – Transformer‑Modelle generalisieren in der Regel besser auf neue Daten.
2. **Modellgröße** – Größere Modelle führen üblicherweise zu einer besseren Generalisierung.
3. **Anzahl der Fine‑Tuning‑Beispiele** – Mehr Fine‑Tuning‑Beispiele führen ebenfalls zu einer besseren Generalisierung.

Nun zu unserer zweiten Frage: Was verursacht den Leistungsabfall einiger Modelle? Wir haben zwei Hypothesen aufgestellt:

* **Adaptive Overfitting** – Überanpassung durch wiederholte Nutzung desselben Testsets, was sich in abnehmender Rendite auf einem neuen Testset manifestiert.
* **Temporal Drift** – Leistungsabfall, der durch die zunehmende zeitliche Lücke zwischen Trainings- und Testdaten entsteht.

Für das Adaptive Overfitting haben wir in der rechten Grafik gezeigt, dass die rote Trendlinie einen Anstieg größer als 1 hat. Das bedeutet, dass jede Einheit der Verbesserung auf CoNLL‑2003 zu mehr als einer Einheit Verbesserung auf CoNLL++ führt – also keine abnehmende Rendite. Damit ist Adaptive Overfitting in diesem Fall nicht beobachtet.

Beim Temporal Drift haben wir Experimente durchgeführt, bei denen wir manche Modelle mit neueren Daten weiterpre‑trainiert haben. Dabei stellten wir fest, dass die Leistung mit zunehmender zeitlichen Lücke abnimmt, was unsere Hypothese bestätigt: der Hauptgrund für den Leistungsabfall ist der Temporal Drift.

Unsere Schlussfolgerungen sind also: Für eine gute Generalisierung benötigt man eine bessere Modellarchitektur, ein größeres Modell und mehr Fine‑Tuning‑Beispiele – und diese Faktoren wirken Hand in Hand. Gleichzeitig haben wir festgestellt, dass der Leistungsabfall hier durch Temporal Drift und nicht durch Adaptive Overfitting verursacht wird, obwohl CoNLL‑2003 seit über 20 Jahren verwendet wird.

Zum Schluss noch die zentrale Frage unseres Titels: „Do CoNLL‑2003 taggers still work in 2023?“ Die Antwort ist ein klares Ja. Wir hoffen, dass unsere Arbeit mehr Forschung anregen wird, wie man die Generalisierung von Modellen weiter verbessern kann. Und vergesst nicht, unsere Arbeit und das Dataset zu prüfen. Falls ihr Fragen habt, könnt ihr euch gern an mich wenden. Vielen Dank!</sample>
    <sample id="114">**Abstract**

In der Präsentation wird ein Ansatz zur Reduktion der Parameterzahl großer Sprachmodelle vorgestellt, der sich auf die Redundanz der Multi‑Head‑Attention‑Schichten konzentriert. Aktuelle Modelle wie LLaMA enthalten Billionen Parameter, erfordern enorme Trainingsdauer und Speicher, was die Bereitstellung in ressourcenbeschränkten Umgebungen erschwert. Bestehende Verfahren zur Optimierung der Multi‑Head‑Attention lassen sich in drei Kategorien einteilen: Homogenisierung, Diversifizierung und Scoring‑basiertes Pruning, welche jeweils Nachteile in Performance‑ oder Kompressionseffizienz aufweisen.  

Der vorgeschlagene **Grouped Head Attention (GHT)** folgt einem zweistufigen Ansatz:  
1. **Group‑Constrained Training**: Die Köpfe werden in Gruppen eingeteilt und durch einen Verlust aus Homogenisierungs‑ und Diversifizierungs­termen intra‑ und inter‑Gruppenähnlichkeit steuern.  
2. **Voting‑to‑Stay**: Nach dem Training werden Köpfe anhand von Batch‑Werten bewertet; pro Gruppe bleibt nur der Kopf mit den meisten Stimmen erhalten.  

Evaluierungen auf Maschineller Übersetzung, Sprachmodellierung und abstrakter Zusammenfassung zeigen BLEU‑Verbesserungen von 3.8–4.4 % bzw. 6.7–7 % sowie Kompressionen von bis zu 32 % bei gleichbleibender Leistung. Die LITE‑Variante reduziert 90 % der Parameter, beschleunigt die Inferenz um 62 % und senkt die FLOPs um 80 %.  

Zukünftige Arbeiten sollen task‑spezifisches Pruning unter Berücksichtigung der Lottery‑Ticket‑Hypothese vertiefen, um nur jene Modellanteile zu behalten, die für die jeweilige Anwendung notwendig sind.</sample>
    <sample id="115">The approach operates on a **chunk of λ utterance frames** – i.e., it processes a fixed-size segment of λ speech frames at a time.</sample>
    <sample id="116">Das entitätsspezifische Wissen, das hier benötigt wird, ist: **„Servin ist ein Richter.“**  
(Kein spezifisches Wissen zu Kea ist für die Pronomenauflösung nötig.)</sample>
    <sample id="117">Der entscheidende Faktor ist die **Qualität der Beispiele** – sie überwiegt die Ähnlichkeit zum Ausgangssatz.</sample>
    <sample id="118">In der Präsentation geht es um die Verbesserung von Vortrainingsmethoden für Code‑Switched NLP, d. h. Texte, die mehrere Sprachen mischen, wie z. B. „Laptop, mere, bag, me, rakha, hai“ (Englisch‑Hindi). Solche Code‑Switch‑Sätze sind in vielsprachigen Gemeinschaften häufig, doch Standard‑Multilingual‑Modelle wie mBERT oder XLM‑R schneidern bei Aufgaben wie Frage‑Antwort oder Sentiment‑Analyse schlechter ab.

Die Hauptbeiträge des Papers sind:
1. **SwitchMLM** – ein neues Masked‑Language‑Model‑Objective, bei dem nur Tokens an *Switch‑Points* (Übergänge zwischen Sprachen) maskiert werden. Das erfordert jedoch ein LID‑Tagging.
2. **FrequencyMLM** – ein Ersatz‑Ansatz, der die negative Log‑Likelihood eines Tokens in monolingualen Corpora nutzt, um LID‑Informationen zu schätzen, ohne expliziten LID‑Tagger zu benötigen.
3. **Architektur‑Optimierungen** – Residualverbindungen von einer mittleren BERT‑Schicht, die besonders Switch‑Point‑Informationen trägt, zur Endschicht. Zusätzlich wird ein auxiliärer LID‑Loss auf dieser Zwischenschicht eingebaut, um die Sprachidentifikation zu stärken.

Ergebnisse auf Sentiment‑Analyse‑Datensätzen zeigen, dass die Kombination aus Switch/FrequencyMLM + ResBERT + LID‑Loss die besten Leistungen über alle Sprachpaare liefert.  
Probing‑Experimente (lineare und konditionale Prüfungen) bestätigen, dass die vorgestellten Methoden die Switch‑Point‑Information in den Representations­ebenen erhöhen, insbesondere von Schicht 9 zu Schicht 12. Zusammenfassend bieten die neuen MLM‑Ziele und architektonischen Änderungen eine gezielte Verbesserung für Code‑Switched NLP.</sample>
    <sample id="119">The extended experiments probe a range of widely‑used transformer models – specifically the **GPT‑4 and the broader GPT family**, the **BART family and its variants**, and **RoBERTa** (including checkpoints further fine‑tuned on partisan corpora).</sample>
    <sample id="120">Das Modell nutzt die Aufmerksamkeitswerte einer einzelnen Ebene – typischerweise die Cross‑Attention‑Schicht des letzten Encoder‑Decoder‑Blocks. Es kombiniert keine Werte aus mehreren Schichten.</sample>
    <sample id="121">Direct inference occurs when the user points straight to the entity with a clear, unambiguous cue—typically the entity’s name or a simple positional indicator.  
Examples from the AltEntities corpus:  

- “Easy on Me” (the exact title)  
- “the first one” (or “the second one”)  

These are straightforward, literal references that let the system pick the target without needing additional contextual clues.</sample>
    <sample id="122">Die Autoren sind der Fudan University zugehörig.</sample>
    <sample id="123">Ying und Zhiyang stellen **MultiInstruct** vor – ein neues, groß angelegtes Datenset für Instruction‑Tuning in multimodalen Modellen. Sie zeigen, dass die jüngsten Fortschritte bei Large Language Models (LLMs) und der damit verbundenen Zero‑Shot‑Generalisierung nicht nur für reine Textaufgaben gelten, sondern auch für multimodale Szenarien. Das Hauptproblem war das Fehlen von umfangreichen Instruction‑Datensätzen, die sowohl Bild‑ als auch Textdaten nutzen. MultiInstruct löst dieses Problem, indem es 62 vielfältige multimodale Aufgaben aus 21 Open‑Source‑Datensätzen zusammenführt und jede Aufgabe mit fünf Experten‑Anweisungen versieht. 

Zur Evaluierung wird OFA (ein einheitliches multimodales Pretrained‑Modell) als Basis genutzt. Für das Training werden 53 Aufgaben aus neun Gruppen mit jeweils 10.000 Instanzen verwendet; das Test‑Set umfasst die gesamte Kategorie „Common Sense Reasoning“ plus weitere Aufgaben. Die Leistung wird über Accuracy (klassifikation) bzw. ROUGE‑L (generative Aufgaben) gemessen, zusätzlich wird ein neuer **Sensitivity‑Score** eingeführt, der die Konsistenz der Ausgaben bei leicht variierenden Anweisungen quantifiziert.

Die Ergebnisse zeigen, dass Instruction‑Tuning OFA signifikant auf sichtbaren Aufgaben verbessert, die Sensitivität sinkt und die Zero‑Shot‑Leistung auf nicht gelernten Aufgaben steigt. Durch Transfer‑Learning aus nativen NLP‑Instruction‑Datensätzen erreicht das Modell zudem noch bessere Sensitivität und Performance. Die Autoren kündigen an, das Datenset um ca. 150 weitere Vision‑Language‑Aufgaben zu erweitern und die Modelle freizugeben.</sample>
    <sample id="124">Die vorgestellte Studie untersucht die Fähigkeit großer Sprachmodelle (LLM) zur Zeit‑Beantwortung. Der Autor definiert drei Ebenen der Zeit‑Logik: L1 (Zeit‑zu‑Zeit), L2 (Zeit‑zu‑Ereignis) und L3 (Ereignis‑zu‑Ereignis). Zunächst wird L1 als reine Jahres‑Berechnung getestet; T5‑L, FLAN‑T5‑L und ChatGPT werden verglichen. Dabei zeigt sich ein starker Bias der Modelle für den Zeitraum 2000‑2020; ChatGPT löst Jahre fast vollständig, verliert jedoch bei Monatsvorhersagen an Genauigkeit.

Um diese Schwächen zu adressieren, wird das Datensatz‑Benchmark „TempReason“ vorgestellt, der Fragen aller drei Ebenen und einen breiten Zeit‑Spannungsbereich abdeckt. Für L1 wird die Schwierigkeit von Jahres‑zu‑Monats‑Vorhersagen erhöht, während L2 und L3 aus Wikidata und Wikipedia generiert werden.

Die Evaluation erfolgt in drei Szenarien: Closed‑Book QA, Open‑Book QA (mit Wikipedia‑Text) und ein neues „Reasoning‑QA“, bei dem sämtliche relevanten Zeit‑Informationen vorhergegeben werden.

Zur Verbesserung der Zeit‑Logik werden zwei Trainingsschritte vorgeschlagen: (1) Pre‑Training zur Rekonstruktion von maskierten Zeit‑ und Entitäts‑Spannen; (2) zeit‑sensitives Reinforcement‑Learning, das korrekte Vorhersagen belohnt und falsch‑zeitlich inkorrekte Antworten bestraft. Das Ergebnismodell „TempT5“ übertrifft sowohl die Zero‑Shot‑Leistung von FLAN‑T5‑L als auch die von ChatGPT, insbesondere bei Open‑Book und Reasoning‑QA. Trotzdem zeigen sich weiterhin periodische Leistungsunschärfen, die auf Trainingsdaten‑Ungleichgewichte hinweisen. Die Arbeit schließt mit dem Aufruf zur Abbau von Zeit‑Bias in LLMs.</sample>
    <sample id="125">Only one author is mentioned – Yanis Labrak.</sample>
    <sample id="126">Ja – in der „Translate‑Test“-Konfiguration wird die Eingabe zunächst mit einem maschinellen Übersetzungs‑API (Google Translate) in die Ziel‑Sprache übersetzt und danach mit einem monolingualen Parser verarbeitet. Diese Vorgehensweise dient als Ausgangs‑Baseline, an die die anderen, direkt multilinguale Modelle und Transfer‑Ansätze gemessen werden.</sample>
    <sample id="127">Die vorgestellte Arbeit mit dem Titel **“Large Language Models Are Reasoning Teachers”** untersucht, wie die beeindruckenden Mehrschritt‑Rechenfähigkeiten sehr großer Sprachmodelle (z. B. GPT‑3, PaLM) auf kleinere, praktischere Modelle übertragen werden können. Der Ansatz nutzt Chain‑of‑Thought‑Prompting: ein großes „Teacher“‑Modell löst eine Aufgabe Schritt für Schritt, liefert die komplette Lösung inklusive Endantwort. Diese generierten Problemlösungen werden als Trainingsdaten verwendet, um ein kleines „Student“-Modell zu fine‑tunen, sodass es selbständig Schritt‑für‑Schritt antworten kann.  

Ein wesentlicher Beitrag ist die Idee der **„Diverse Reasoning“**: statt nur einer Lösung erzeugen die Autoren mit stochastischem Temperature‑Sampling mehrere unterschiedliche, aber korrekte Lösungswege. Durch diese Vielfalt lernen die Schüler besser und erzielen deutlich höhere Leistungen. Auf 12 Benchmark‑Aufgaben zeigen Experimente, dass das methodische Fine‑Tuning mit Diverse Reasoning besonders bei textbasierten Problemen (z. B. Datenverständnis, Coin‑Flip) die Leistung von naiven Prompt‑Baselines um ein Vielfaches steigert – bei Multi‑Arithmetik von 33 % auf 55 %. Selbst das kleinste Modell (0,3 B Parameter) übertrifft herkömmliches Fine‑Tuning in den meisten Tests.  

Der Ansatz skaliert gut: Mehr Daten, ein stärkerer Teacher oder ein größeres Student‑Modell führen zu weiteren Verbesserungen, während die Kosten für Training, Teacher‑Inference und Inference‑Kosten abgewogen werden müssen. Die Autoren stellen Code, Daten und Modelle öffentlich zur Verfügung und betonen, dass die Methode auch für andere emergente Fähigkeiten erweiterbar sei.</sample>
    <sample id="128">**Abstract**

In diesem Beitrag stellen wir den KITMUS-Test vor, ein Diagnostik‑Set zur Bewertung der Integration von Wissen aus mehreren Quellen in Sprach‑Modellen. KITMUS konzentriert sich auf Coreference‑Resolution‑Aufgaben, bei denen ein Pronomen sowohl auf eigenschaftsspezifisches Wissen (z. B. „Servin ist Richter“) als auch auf generelles Hintergrundwissen (z. B. „Richter entscheiden Fälle“) angewiesen ist. Wir variieren die Verfügbarkeit dieser beiden Wissensarten in drei Szenarien:  
1. **Background‑Pretrain** – Hintergrundwissen liegt ausschließlich im vortrainierten Modell vor;  
2. **Background‑Both** – Hintergrund‑ und entitätsspezifisches Wissen stehen im Modell und im Kontext zur Verfügung;  
3. **Background‑Inference** – Beide Wissensarten sind nur im inference‑Zeit‑Kontext vorhanden, um neu entstehende Fakten zu simulieren.  

Durch Experimente mit menschlichen Prüfern und etablierten Coreference‑Modellen (C2F, BERT4Coref) zeigen wir, dass ohne KITMUS‑spezifisches Training die Modelle stark unterliegen und keine zuverlässige Integration von Wissen aus unterschiedlichen Quellen demonstrieren können. Nach Training auf KITMUS verbessert sich die Leistung signifikant, jedoch bleibt die Integration von ausschließlich inference‑Zeit‑Wissen (z. B. fiktive Berufe) eine Herausforderung. Unser Testkit bietet somit ein zielgerichtetes Mittel, um die Fähigkeit großer Sprachmodelle zur multimodalen Wissensintegration zu untersuchen.</sample>
    <sample id="129">Sie geben als Beispiel **schwarze Frauen** an, die als „markierte“ Gruppe gegenüber den unmarkierten Gruppen „weiße Personen“ und „Männer“ betrachtet wird.</sample>
    <sample id="130">Nicht‑Transformer‑Architekturen (z. B. klassische Bi‑LSTM‑CRF‑Modelle und ähnliche RNN‑basierte Tagger) zeigen in den Experimenten eine deutlich schlechtere Generalisierung auf das neuere CoNLL++‑Set.</sample>
    <sample id="131">In the presentation the speaker does **not** mention any specific dataset names.  The talk refers generically to a “clean test set” (i.e., a test set with correct labels) but does not identify whether it is, for example, MNIST, CIFAR‑10, GLUE, etc.  Consequently, the names of the test datasets used in the study are not provided in the text.</sample>
    <sample id="132">Es sind zwei Autoren beteiligt – Akshatha und Martin.</sample>
    <sample id="133">The authors work with multiple modalities—specifically, image–text (vision‑language) tasks.</sample>
    <sample id="135">The Emory NLP Lab, led by Professor Jinho Choi and partnered with Amazon Alexa AI, introduced **ABC‑Eval**, a new dimensional approach for assessing conversational AI. Unlike conventional human evaluations that rely on Likert scales or pairwise comparisons, ABC‑Eval explicitly annotates whether each model response exhibits specific behaviors—such as irrelevance, contradiction, hallucination, or empathy failures. This fine‑grained labeling aims to reduce subjectivity and capture the nuanced aspects that shape overall dialogue quality.

To validate the method, the team evaluated four state‑of‑the‑art chat models across 100 human‑bot conversations per model. They also collected traditional turn‑level and dialogue‑level Likert ratings, as well as pairwise comparisons, on eight common quality dimensions. Inter‑annotator agreement on ABC‑Eval labels was higher than that of the existing methods, and linear‑regression analyses showed that ABC‑Eval metrics explain a larger proportion of variance in overall conversation quality (over 25 %) compared to turn‑level Likert metrics. Moreover, metrics like self‑contradiction and partner‑contradiction accounted for 5–10 % of quality variance, outperforming consistency scores from Likert ratings.

The study also quantified persistent challenges: common‑sense violations in ~20 % of responses, irrelevant content in ~15 %, and contradictions in ~10 %. As models rapidly improve, these error rates may decline, but reliable, behavior‑based metrics like ABC‑Eval remain essential for precise, actionable evaluation. The authors hope the community will adopt ABC‑Eval to better benchmark and guide the next generation of conversational AI.</sample>
    <sample id="136">In ihrer Präsentation stellt Jasivan die Arbeit „FERMAT: An Alternative to Accuracy for Numerical Reasoning“ vor, die er gemeinsam mit ihrer Betreuerin Nafise an der University of Sheffield durchgeführt hat. Das zentrale Problem ist die mangelnde Leistungsfähigkeit von Sprachmodellen (insbesondere der 3‑Billionen‑Parameter‑Skala) bei numerischen Aufgaben, die in realen Anwendungen wie Fakten‑Checking entscheidend sind. Traditionelle Benchmarks wie Accuracy oder F1 liefern keine detaillierte Einsicht in die Stärken und Schwächen der Modelle hinsichtlich mathematischer Fähigkeiten.

Daher wurde das neue Benchmark‑Set FERMAT entwickelt. Es besteht aus mathematischen Wortaufgaben aus Illinois und CommonCore und testet gezielt drei Dimensionen: Zahlverständnis, mathematische Operationen und Trainingsabhängigkeit. Die Aufgaben werden variert, indem Zahlen in unterschiedlichen Formaten (Ganzzahlen, Dezimalzahlen, große Zahlen) und in unterschiedlichen Kontexten (z. B. Währungen) dargestellt werden, um die Breite der Modelle zu prüfen.

Im ersten Baseline‑Test (Zero‑Shot) zeigen die Modelle insgesamt schwache Leistungen. Durch Fine‑Tuning mit 200 000 generierten Beispielen, die anhand von Lehrer‑vorlagen erstellt wurden, verbessert sich die Genauigkeit signifikant, sowohl bei den ursprünglichen Aufgaben als auch bei neuen Kombinationen von Operationen. Eine Analyse der Trainingsabhängigkeit zeigt, dass selbst das exakte Auftreten einer Rechenoperation im Trainingsdatensatz die Genauigkeit nicht über 50 % steigert, was auf Sprachvariationen im Ausdruck hinweist.

Schließlich demonstriert die Untersuchung der Trainingsvorlagen (Zero‑Shot, Base, Base‑Scaled, Base‑Diversified) dass eine sprachliche und mathematische Vielfalt – insbesondere durch Einbeziehung von GSM‑8K‑ und AQUA‑Templates – die Modellleistung weiter erhöht. Abschließend betont Jasivan, dass FERMAT ein aussagekräftigerer, differenzierter Ersatz für herkömmliche Genauigkeitsmaße ist und wichtige Erkenntnisse zu Zahlencodierung und Tokenisierung liefert.</sample>
    <sample id="137">**Zusammenfassung (ca. 200 Wörter)**  

Der Beitrag „Tell2Design: A Dataset for Language‑Guided Floor Plan Generation“ präsentiert die Herausforderung, Innenarchitektur‑Layouts allein aus natürlichen Sprachanweisungen zu generieren. Im Fokus steht die Erstellung von 2‑D‑Grundrissen, bei denen jede Raum­box durch Typ‑Label und Koordinaten‑/Dimensionen definiert ist. Für die Datensammlung wurden 5 051 menschlich annotierte Anweisungen sowie ca. 76 000 synthetisch generierte Anweisungen aus Vorlagen gesammelt. Jede Anweisung beschreibt über 200 Wörter – also mehr als zehn Sätze – und enthält Semantik (Raumtyp), Geometrie (Größe/Shape) und Topologie (Raumbeziehungen).  

Die Autoren formulieren die Aufgabe als Sequenz‑zu‑Sequenz‑Problem: Ein Encoder‑Decoder‑Transformer (T5‑Basis) wandelt die Text‑Input‑Sequenz in eine Ziel‑Sequenz von Bounding‑Box‑Koordinaten um. Durch diese Struktur kann das Modell flexibel mit unterschiedlichen Anweisungslängen und Raumzahlen umgehen.  

Evaluation zeigt, dass das T2D‑Modell mit einem Micro‑IoU von 54 und Macro‑IoU von 53 deutlich besser abschneidet als klassische text‑conditional‑image‑Generations‑Modelle. Der Hauptgrund liegt darin, dass die Basis‑Modelle auf hoch‑realistische Kunstwerke aus kurzen Texten optimiert sind und die präzisen, mehrdimensionalen Design‑Constraints nicht erfüllen.  

Die Ergebnisse verdeutlichen, dass eine Sprach‑zu‑Design‑Übersetzung sinnvoller ist als herkömmliche Bild‑Generierung. Der Beitrag liefert damit sowohl einen großen, annotierten Datensatz als auch ein starkes Basismodell und eröffnet neue Forschungsperspektiven für sprachgesteuerte Design‑Generierung.</sample>
    <sample id="138">Nach Ansicht der Autoren ist die **Integration von Wissen aus mehreren Quellen – insbesondere die Kombination von prädiktivem, vortrainiertem Wissen mit inference‑time‑Speziellen Fakten (z. B. neuen, nicht im Pre‑Training enthaltenen Informationen)** ein zu wenig erforschtes Gebiet im Bereich der natürlichen Sprachverarbeitung.</sample>
    <sample id="139">Die Referenten sind Ying und Zhiyang.</sample>
    <sample id="140">Ja – der Validierungs‑ und Test‑Set wurde von Crowd‑source‑Arbeitern überprüft und korrigiert, um die Qualität sicherzustellen.</sample>
    <sample id="141">**Grenzen vorhandener Ressourcen für kontextbasierte Übersetzung**

1. **Begrenzter Anwendungsbereich** – Sie decken nur wenige, häufig manuell kuratierte Diskursphänomene ab (z. B. Ellipsen, Pronomen, Formulierung).  
2. **Eingeschränkte Sprachabdeckung** – Meist nur ein paar Sprachen (oft Englisch‑zentriert) werden unterstützt, sodass andere Sprachpaare fehlen.  
3. **Ressourcenintensive Erstellung** – Der Aufbau erfordert umfangreiches Domänenwissen und menschliche Annotation, was die Skalierbarkeit einschränkt.  
4. **Schwache Abdeckung von Kontextabhängigkeit** – Nur ein kleiner Anteil von Übersetzungen hängt wirklich vom Kontext ab; daher erfassen corpus‑weite Metriken wie BLEU diese Unterschiede kaum.  

Kurz gesagt: Aktuelle Ressourcen sind länderspezifisch, thematisch eng gefasst und stark von manueller Arbeit abhängig, sodass sie die Breite und Vielfalt kontextabhängiger Übersetzungsprobleme kaum erfassen.</sample>
    <sample id="142">Hallo! Ich möchte über unsere Arbeit zum Thema „Auflösung indirekter Bezugsausdrücke für die Auswahl von Entitäten“ sprechen, in der wir das AltEntities‐Korpus vorstellen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer*innen zu verstehen, wenn sie eine Auswahl treffen wollen. Betrachten wir die folgende Alternativfrage: „Meintest du ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Hier möchte ein Nutzer zwischen einer von beiden Lieder auswählen. Das offensichtliche Vorgehen wäre eine direkte Referenz, zum Beispiel indem er den Namen des Liedes „Easy on Me“ nennt oder seine Position angibt, „das erste“. Aber manchmal ist eine indirekte Referenz sinnvoller, um ein natürlicheres Gespräch zu führen. Das kann passieren, wenn der Nutzer den Namen des Liedes nicht mehr im Kopf hat, die Aussprache zu ähnlich ist und schwer zu unterscheiden ist oder wenn er eine Präferenz angeben möchte. Hier ein paar Beispiele für indirekte Referenzen, z. B. „der neuere“ oder „das Lied, das nicht energiegeladen ist“. Dies ist ein wichtiges Problem in Konversationssystemen und auch für die Bewertung von LLMs hinsichtlich ihres Verständnisses von Entitäten. Wir sind uns nicht bewusst, dass es ein größeres öffentliches Datenset für diese Aufgabe gibt, daher haben wir ein solches mit Crowd‑Annotationen gesammelt. Unser Datenset umfasst drei unterschiedliche Domänen: Musik, Bücher und Rezepte. Die Datensammlungsmethodik legt Wert auf Informalität und nutzt ein Cartoon‑Completion-Setup. Der Cartoon hat drei Sprechblasen. In der ersten Sprechblase sagt Bob: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ Damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: „Meinst du ‚Easy on Me‘ oder ‚I Gotta Feeling‘?“ Das ist die Alternativfrage. In der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine der Entitäten auszuwählen, z. B. „der neuere“. Wir stellen die erste und zweite Sprechblase automatisch bereit, aber die dritte Blase wird von der Annotator*innen ausgefüllt. Die erste Sprechblase wird aus ein paar manuellen Prompt‑Vorschlägen pro Domäne ausgewählt. Die zweite, die Alternativfrage, wird wie folgt generiert. Wir benutzen immer eine einfache Vorlage: „Meinst du A oder B?“ wobei A und B Stichproben aus Wikipedia sind. Hier sind die unterschiedlichen Sampling‑Methoden, die wir verwendet haben. Je höher wir in der Liste sind, desto ähnlicher sind die Entitäten und desto schwieriger ist die Unterscheidung. Die erste Methode ist uniform zufällig. Die zweite nutzt Entitäten mit ähnlichen Titeln, z. B. zwei Bücher mit dem Namen „The Return“. Die dritte nutzt Entitäten mit ähnlichen Beschreibungen auf Wikipedia. Und schließlich wenn die Entitäten ähnliche Infobox‑Attribute haben, z. B. denselben Genre‑ oder Künstlernamen für ein Lied. Wenn wir diese Alternativfrage den Annotator*innen zeigen, kennen sie die Namen der Entitäten, aber nicht unbedingt die Inhalte der Entitäten. Deshalb zeigen wir ihnen etwas Hintergrundwissen zu den beiden Entitäten. Für Lieder zeigen wir einfach einen Google‑Suchlink zu jedem Lied und bitten die Annotator*innen, mindestens einen Teil jedes Liedes anzuhören und sich über jedes Lied zu informieren. Hier ein Beispiel, der Google‑Suchergebnis für das Lied „Easy on Me“. Für Rezepte und Bücher zeigen wir Hintergrundtext aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, erneut aus Wikipedia, damit die Annotator*innen wissen, wie sie aussehen. Dann bitten wir die Annotator*innen, eine der Entitäten auszuwählen, z. B. die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Beispiele aus unserem Datensatz: „der ohne Worte“, „nicht der mit dem 12‑jährigen Jungen“, „der fiktive“, „kommt aus Aserbaidschan“ usw. Das AltEntities‑Korpus hat 6.000 Alternativfragen über drei Domänen und insgesamt 42.000 indirekte Bezugsausdrücke. Ergebnisse mit dem T5‑XL‑Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell Zugang zu genau demselben Hintergrundwissen hat wie die Annotator*innen, ist die Genauigkeit sehr hoch, etwa 92 – 95 %. Das ist jedoch unrealistisch. Hat das Sprachmodell Zugang zu teilweise überlappendem Hintergrundwissen, liegt die Genauigkeit zwischen 82 – 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell das Hintergrundwissen abruft. Hat das Sprachmodell nur Zugriff auf die Entitätennamen, liegt die Genauigkeit bei nur 60 %, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle domänenübergreifend generalisieren. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="143">The EDAtt approach is compared against the two most common SimulST policies that can be applied to off‑line models—**Wait‑k** and **Local Agreement**—as well as against a **state‑of‑the‑art dedicated simultaneous pre‑translation architecture**.</sample>
    <sample id="144">The authors are affiliated with Nantes University (Université de Nantes) – specifically the Nantes University Hospital data warehouse.</sample>
    <sample id="145">Der/die Referent*in heißt **Jenny**.</sample>
    <sample id="146">In der heutigen Präsentation stellt Yicheng die Problematik von **Omission** (Auslassungen) in der Dialogzusammenfassung vor. Zunächst wird kurz erläutert, dass Dialogsummaries, trotz großer Fortschritte durch große vortrainierte Sprachmodelle, häufig Faktenfehler aufweisen – insbesondere das Fehlen wichtiger Informationen. Durch eine Analyse von fünf Domänen und sechs Modellen zeigt die Arbeit, dass etwa 70 % der generierten Zusammenfassungen Auslassungen enthalten, wobei die ausgelassenen Inhalte zufällig im Dialog verteilt sind.  

Um dieses Problem systematisch anzugehen, wurde der neue **OLDS**‑Datensatz erstellt. Er basiert auf fünf bestehenden Benchmarks, erzeugt für jedes Dialog‑Exemplar zehn abstrakte Kandidaten durch unterschiedliche Modelle und Decodierungsstrategien und weist anschließend automatisch Auslassungslabels zu. Die Qualität der Labels wurde durch menschliche Evaluation bestätigt.  

Zur Untersuchung der Erkennungsaufgabe wurden drei Baseline‑Architekturen evaluiert: (1) Paar‑zu‑Klassifikation, (2) Sequenz‑Labeling und (3) Pointer‑Netzwerk. Mit Präzision, Recall, F1‑Score und einem Wort‑Level‑Recall‑Metrik (WR) wird gezeigt, dass die F1‑Scores nur rund 50 % erreichen – ein Hinweis auf die hohe Schwierigkeit der Aufgabe.  

Schließlich demonstriert Yicheng eine Post‑Editing‑Methode, bei der die erkannten Auslassungsinhalte an die Kandidatensummary angehängt werden. Das Modell erzeugt daraus eine verbesserte Zusammenfassung, was die Wirksamkeit der Auslassungsdetektion als Qualitätssteigerungsmaßnahme unterstreicht.</sample>
    <sample id="147">Three authors are involved: Myra, Esin Durmus, and Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler und ich möchte kurz das Papier **“Attention as a Guide for Simultaneous Speech Translation”** vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.  

**Was ist simultane Sprachübersetzung?**  
Simultane Sprachübersetzung – oder SimulST – ist der Prozess, gesprochene Sprache in Echtzeit in Text einer anderen Sprache zu übersetzen und damit die Kommunikation über Sprachgrenzen hinweg zu ermöglichen.  

**Welche Probleme haben die aktuellen SimulST‑Modelle?**  
- Spezielle Architekturen werden üblicherweise trainiert, wodurch zusätzliche Module entstehen, die optimiert werden müssen.  
- Lange und komplizierte Trainingsverfahren, zum Beispiel Training mit unterschiedlichen Optimierungszielen.  
- Das Training und die Wartung mehrerer Modelle, um unterschiedliche Latenzbereiche zu erreichen – etwa ein Modell mit einer durchschnittlichen Latenz von einer Sekunde, ein anderes mit zwei Sekunden usw.  

**Was ist also unsere Lösung?**  
1. Bestehende Offline‑ST‑Modelle ohne erneutes Training oder spezielle SimulST‑Architekturen verwenden.  
2. Nur ein Modell für jede Latenzbandbreite nutzen und die Latenz durch spezifische Parameter steuern.  
3. Das bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und Textausgabe erworbene Wissen nutzen – das ist der Cross‑Attention‑Mechanismus (Beispiel rechts).  

**Unsere Lösung: EDAtt (Encoder‑Decoder Attention)**  
EDAtt ist eine Strategie, bei der wir entscheiden, ob eine teilweise Übersetzung ausgegeben wird, basierend darauf, wohin die Aufmerksamkeit zeigt.  
- Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. die Summe liegt unter einem bestimmten Schwellenwert \(\alpha\) für die letzten \(\lambda\) Sprachrahmen. Das bedeutet, die empfangenen Informationen sind ausreichend stabil.  

**Beispiel**  
Wenn wir einen Sprachabschnitt mit „I'm going to talk about …“ erhalten und unser Modell die Übersetzung auf Deutsch vorhersagt, betrachten wir die Cross‑Attention‑Gewichte:  
- Die ersten beiden Wörter zeigen auf die frühesten empfangenen Sprachrahmen.  
- Das letzte Wort zeigt auf die zuletzt empfangenen \(\lambda\) Sprachrahmen.  

Daraus folgt:  
- Die ersten beiden Wörter werden ausgegeben, weil die Summe der Cross‑Attention unter \(\alpha\) liegt.  
- Das letzte Wort wird nicht ausgegeben, weil die Summe über \(\alpha\) liegt, und wir warten auf einen weiteren Sprachabschnitt.  

Wenn ein weiterer Abschnitt eintrifft und das Modell drei weitere Wörter vorhersagt, zeigen deren Cross‑Attention‑Gewichte **nicht** auf die letzten \(\lambda\) Rahmen. Diese drei Wörter werden dann ausgegeben.  

**Ergebnisse von EDAtt**  
Wir plotten die simultanen Übersetzungsergebnisse auf Diagrammen, auf denen:  
- BLEU misst die Übersetzungsqualität.  
- Durchschnittliche Verzögerung (Average Lagging) misst die Latenz.  
- Berechnungsbewusste durchschnittliche Verzögerung berücksichtigt die Rechenzeit des Modells zur Vorhersage.  

Wir streben an, dass unsere Kurven so hoch wie möglich sind und nach links verschoben liegen.  

**Vergleiche**  
- Wir vergleichen mit beliebten Strategien, die ebenfalls auf Offline‑Modelle angewendet werden: **Wait‑k** und **Local Agreement**.  
- Wir vergleichen auch mit der State‑of‑the‑Art‑Architektur, die speziell für simultane Pre‑Übersetzung entwickelt wurde.  

Alle Ergebnisse beziehen sich auf die simultane Übersetzung ins Deutsche. Wir sehen, dass EDAtt alle Strategien, die auf Offline‑Modelle angewendet wurden, übertrifft, weil die Kurven nach links verschoben sind. Zudem zeigen sich die schnellste Strategie, wenn wir die tatsächliche verstrichene Zeit bzw. die berechnungsbewusste Zeit berücksichtigen.  

Wenn Sie weitere Ergebnisse entdecken möchten, lesen Sie unser Papier. Wir haben zudem den Code, die Modelle und die simultane Ausgabe als Open Source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern.  

Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja – der CoNLL++‑Datensatz ist öffentlich zugänglich.  In der Veröffentlichung wird darauf hingewiesen, dass sowohl das Paper als auch das Dataset zum Download bereitstehen (in der Regel über einen Link im PDF bzw. auf einer Projekt‑Website).</sample>
    <sample id="150">**Zusammenfassung**

Die Arbeit „MEETINGQA: Extractive Question‑Answering on Meeting Transcripts“ stellt das neue Dataset MeetingQA vor, das sich auf echte Fragen und ihre Antworten in mehrteiligen Meetings konzentriert. Während frühere Studien bei Zusammenfassungen und Action‑Item‑Extraktion lagen, nutzt MeetingQA die bedeutende QA‑Komponente von Meeting‑Diskussionen. Das Dataset basiert auf dem AMI‑Corpus (etwa 100 Stunden multikultureller, manueller Transkriptionen). Durch punktuelle Auswahl und Längenfilterung werden Fragen identifiziert, die anschließend von annotierten Arbeitskräften beantwortet werden. Die Annotation erreicht einen Krippendorff‑Alpha von 0,73. Insgesamt enthält MeetingQA 7.700 Fragen, davon 30 % unantwortbar, 40 % mehrteilige Antworten und 48 % mehrsprachen‑übergreifende Antworten. Die Fragen sind meist Ja‑/Nein‑Fragen, die dennoch ausführliche Diskussionen auslösen; 20 % sind rhetorisch. Durchschnittlich umfassen Fragen 12 und Antworten 35 Wörter. Der menschliche F1‑Score liegt bei 84,6 %.

Zur Modellierung werden kurze‑Kontext‑Modelle mit Retrieval, Ein‑Span‑ und Mehr‑Span‑Ansätze sowie ein Silver‑Data‑Augmentation‑Schritt (MediaSum‑Interviewfragen) eingesetzt. In der feinabgestimmten Umgebung beträgt der F1‑Gap zu Menschen 25 Punkte; RoBERTa übertrifft Longformer, und Mehr‑Span‑Modelle sind vergleichbar oder leicht schlechter. Im Zero‑Shot‑Modus liegt das Gap bei 50 Punkten; Silver‑Daten und große, instruction‑getunte Modelle wie FLAN‑T5 verbessern die Leistung. Fehleranalysen zeigen, dass rhetorische Fragen, irrelevante Sätze und Sprecheridentifikation besonders schwierig sind. Insgesamt bleibt MeetingQA ein anspruchsvolles Ziel für aktuelle QA‑Modelle.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung zu MultiInstruct vorstellen, die das multimodale Zero‑Shot‑Learning durch Instruction Tuning verbessert. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, bei denen vortrainierte Sprachmodelle für verschiedene Down‑stream‑Aufgaben auf parameter‑ und daten‑effiziente Weise wiederverwendet werden. Kürzlich haben viele Studien gezeigt, dass Instruction Tuning große Sprachmodelle in die Lage versetzt, unbekannte Aufgaben in einer Zero‑Shot‑Methode zu erfüllen, indem sie natürliche Anweisungen befolgen. Allerdings konzentrierten sich die meisten bisherigen Arbeiten zum Instruction Tuning darauf, die Zero‑Shot‑Leistung bei rein sprachbasierten Aufgaben zu verbessern, während Computer Vision und multimodale Aufgaben außen vor blieben. Daher wollen wir in dieser Arbeit untersuchen, ob Instruction Tuning multimodaler vortrainierter Modelle tatsächlich die Generalisierung auf unbekannte multimodale Aufgaben verbessern kann.

Zusätzlich haben wir bei der Durchführung unserer Forschung festgestellt, dass es einen erheblichen Unterschied in der Verfügbarkeit von Anleitungsdatensätzen zwischen NLP und multimodaler Verarbeitung gibt. Es existieren mehr als 1 600 rein sprachbasierte Anleitungsaufgaben. Es gibt jedoch keinen groß angelegten, öffentlich verfügbaren multimodalen Anleitungsdatensatz. Das motivierte uns, einen multimodalen Anleitungs‑Tuning‑Datensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multimodalen Anleitungs‑Tuning‑Benchmark‑Datensatz, der 62 diverse multimodale Aufgaben umfasst, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 bestehenden Open‑Source‑Datensätzen, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.

Um das multimodale Instruction Tuning auf unserem vorgeschlagenen Datensatz zu untersuchen, wählen wir OFA, ein einheitliches multimodales vortrainiertes Modell, als Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild‑Token und die Koordinaten eines Begrenzungsrahmens. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct‑Datensatz, um die Verarbeitung verschiedener Eingabe‑ und Ausgabedatentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence‑to‑Sequence‑Format, in dem der Eingabetext, die Bilder, die Anweisungen und die Begrenzungsrahmen im selben Token‑Raum dargestellt werden.

Nun möchte ich über das multimodale Instruction Tuning sprechen. Für den Trainingsdatensatz nutzen wir 53 Aufgaben aus 9 Gruppen und ziehen für jede Aufgabe 10 000 Instanzen. Für das Testen reservieren wir die gesamte Gruppe „Common Sense Reasoning“ für den Test und wählen zusätzlich 5 Aufgaben aus den Gruppen VQ und Miscellaneous. Wir verwenden alle Instanzen im Test‑Split für jede Aufgabe. Zusätzlich ziehen wir zufällig 20 Aufgaben aus dem Test‑Split der natürlichen Anweisungen als unbekannte Aufgabe für NLP. Wir verwenden das vortrainierte OFA‑Large‑Modell als Basismodell. Während des Trainings mischen wir alle Instanzen aller Aufgaben. Jede Instanz wird zufällig mit einer seiner fünf Anweisungsvorlagen kombiniert. Im Test führen wir für jede Aufgabe insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen evaluieren. In jedem Experiment berichten wir die minimale und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente. Ist die Aufgabe eine multimodale Klassifikationsaufgabe, berichten wir die Genauigkeit. Ist es eine multimodale Generierungsaufgabe, berichten wir Rouge‑L. Für NLP‑Aufgaben berichten wir ebenfalls Rouge‑L. Wir führen auch ein zusätzliches Evaluationsmaß namens Sensitivität ein. Dieses misst die Fähigkeit des Modells, für dieselbe Aufgabe stets dieselben Ausgaben zu erzeugen, unabhängig von leichten Variationen in der Formulierung der Anweisung.

Hier sind unsere Hauptergebnisse. Wie wir sehen können, kann Instruction Tuning die Leistung von OFA bei bekannten multimodalen Aufgaben signifikant verbessern. Außerdem kann Transferlernen aus natürlichen Anleitungsdatensätzen das Instruction Tuning begünstigen. Wir sehen, dass bei zunehmender Anzahl von Aufgaben die Leistung steigt und gleichzeitig die Sensitivität sinkt. Wir haben zudem ein Experiment durchgeführt, bei dem wir eine Anweisung mit fünf Anweisungen vergleichen. Wie wir sehen, kann die Verwendung mehrerer Anweisungen die Gesamtleistung des Modells erheblich verbessern und die Sensitivität stark reduzieren. Das zeigt den Einfluss verschiedener Feintuning‑Strategien auf die Sensitivität des Modells. Durch Transferlernen aus natürlichen Anleitungsdatensätzen kann das Modell eine viel bessere Sensitivität erzielen als das ursprüngliche OFA-Modell. Wir sehen ebenfalls, dass Transferlernen aus natürlichen Anleitungsdatensätzen OFA helfen kann, eine viel bessere Leistung auf dem Natural‑Instruct‑Datensatz zu erzielen.

Zusammenfassend schlagen wir den ersten groß angelegten multimodalen Anleitungs‑Tuning‑Datensatz vor, der die Kurzzeitfähigkeit von OFA signifikant verbessert, und untersuchen verschiedene Transfer‑Lerntechniken und deren Nutzen. Wir haben ein neues Maß namens Sensitivität entworfen. Noch ein weiteres Anliegen: Wir sammeln einen viel größeren multimodalen Anleitungs‑Tuning‑Datensatz mit rund 150 zusätzlichen Vision‑Language‑Aufgaben und werden ihn veröffentlichen. Hier ist ein QR‑Code für unsere Daten und Modelle.

Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider präsentiert in seiner Kurzvorstellung „Exploring Large Language Models for Classical Philology“ die Entwicklung neuer Sprachmodelle für Altgriechisch und Latein. Im Fokus stehen zwei monolinguale Modelle – GreBERTa (RoBERTa‑basiert) und GreTa (Encoder‑Decoder‑T5) – sowie ihre multilingualen Gegenstücke PhilBERTa und PhilTa, die Altgriechisch, Latein und Englisch gemeinsam verarbeiten. 

Der Einstieg liefert einen Überblick über die bisherige Modelllandschaft: 2020 wurde Latin BERT eingeführt, 2021 ein Altgriechisch‑BERT und 2022 eine weitere Variante. Diese Modelle sind allerdings ausschließlich BERT‑Encoder und monolingual. Multilinguale Alternativen sind oft nicht auf altgriechische Texte trainiert, und die Leistungsbewertung ist unzureichend.

Zur Datensammlung nutzte das Team die Open Greek &amp; Latin‑Sammlung und erweiterten sie um einen neuen, hochqualitativen Korpus aus der Internet Archive. Durch gezielte Suche nach falsch transkribierten Stop‑Words (z.B. „γάρ“ → „yap“) identifizierten die Forscher altgriechische Texte und re‑OCR‑ten sie mit korrekten Zeichensatz‑Einstellungen. Für Latein kam das Corpus Corporum hinzu, für Englisch wurden antiquity‑bezogene Texte verwendet.

Die Modelle wurden anschließend anhand von Universal Dependencies (Altgriechisch), EvaLatina 2022 (Latein) und klassischen Aufgaben wie POS‑Tagging, Abhängigkeitsparsing und Lemmatisierung evaluiert. GreTa übertrifft die bisherigen State‑of‑the‑Art‑Ergebnisse, insbesondere bei der Lemmatisierung, wo ein Plus von ca. 5 % erzielt wird. Zudem untersuchten die Autoren die semantische und Weltwissen‑Kapazität: Synonym‑Antonym‑Unterscheidung, Beziehungen zwischen Helden und Göttern sowie mögliche Vorteile der Multilingualität. Obwohl die Multilingual‑Modelle ähnliche Leistungen wie die monolingualen Modelle erbrachen, konnten sie die semantischen Benchmarks deutlich besser schlagen als frühere Modelle.

Zusammenfassend stellt die Arbeit neue, von Grund auf trainierte Modelle vor, die sowohl Encoder‑only als auch Encoder‑Decoder‑Architekturen nutzen, einen qualitativ hochwertigen altgriechischen Korpus liefern und die Leistung in klassischen NLP‑Aufgaben signifikant steigern. Weitere Details finden sich im zugehörigen Paper.</sample>
    <sample id="153">In this talk, Ninareh Mehrabi presents work on “Resolving Ambiguities in Text‑to‑Image Generative Models.” The authors address the problem that many user prompts are ambiguous, leading to images that may not match the user’s intent. They curate a benchmark dataset derived from the LAVA corpus, categorizing prompts by distinct ambiguity types (e.g., object‑relation, spatial, contextual). Their pipeline first disambiguates prompts via a language‑model–driven framework: (1) the model generates clarifying questions; the user answers, and the answer is concatenated to the prompt; or (2) the model proposes multiple visual interpretations, and the user selects the intended one, again producing a refined prompt. The disambiguated prompts are fed to a text‑to‑image system, and the resulting images are evaluated for faithfulness to the original intention. For automatic assessment, they employ a Visual Question‑Answering (VQA) model: the image and a question reflecting the user’s intention are input, and a “yes”/“no” answer indicates whether the image satisfies the intention. Their experiments reveal that disambiguation improves faithful generation, that performance varies across ambiguity types, and that the VQA‑based metric correlates strongly with human judgments, validating its use as a reliable evaluation tool. The work thus offers both a benchmark for ambiguity and practical methods to mitigate and assess it in generative image models.</sample>
    <sample id="154">Die Autoren sind an der Universität Trento sowie der Fondazione Bruno Kessler tätig.</sample>
    <sample id="155">Bob.</sample>
    <sample id="157">In ihrer Präsentation stellt Shen Gao aus der Shandong University das System **“Dialogue Summarization with Static‑Dynamic Structure Fusion Graph”** vor, ein gemeinsames Projekt mit Xin Cheng, Mingzhe Li, Xiuying Chen, Jinpeng Li, Dongyan Zhao und Rui Yan. Das Ziel ist es, aus mehrteiligen, halbstrukturierten Dialogen prägnante Zusammenfassungen zu erzeugen – eine Aufgabe, die besonders herausfordernd ist, weil Dialoge komplexe Interaktionen zwischen mehreren Sprechern enthalten.  

Aktuelle Methoden nutzen meist vorab berechnete statische Graphen, die mit externen Werkzeugen wie Diskurs‑Parsen oder Dialog‑State‑Tracking erzeugt werden. Diese Ansätze sind fehleranfällig, da sie auf der Zuverlässigkeit der externen Tools angewiesen sind, und die festen Graphen können sich nicht dynamisch an die spezifische Zusammenfassungsaufgabe anpassen.  

Das vorgestellte **SDDS‑Modell** besteht aus vier Hauptkomponenten. Zunächst wird jede Äußerung mit einem **Utterance Encoder** in Vektorrepräsentationen überführt. Anschließend werden vier heuristische Graphen aufgebaut: (i) Diskurs‑Parsing‑Graph, (ii) Key‑Co‑Occurrence‑Graph, (iii) Sprecher‑Interaktionsgraph, (iv) Positionsgraph. Diese Adjazenzmatrizen werden über eine 1×1‑Faltung zu einer gefusionierten statischen Struktur zusammengeführt.  

Eine **Dynamic Graph‑Modul** nutzt Multi‑Head‑Attention, um auf Basis der tiefen Vektor­darstellungen semantische Beziehungen zwischen Äußerungen zu lernen, ohne auf vorgegebene Regeln zurückzugreifen. Die statische und die dynamische Struktur werden zu einer einheitlichen Graphen‑Matriz Gᵘ fusioniert.  

Für die Sprachgenerierung wird ein vortrainiertes Sprachmodell als **Summary Generator** eingesetzt und durch einen **Dual‑Cross‑Attention‑Mechanismus** – bestehend aus einer graph‑basierten Attention‑Schicht oberhalb der traditionellen Self‑Attention – mit den Graphinformationen angereichert.  

Der gesamte Code und die Daten sind auf GitHub verfügbar; ein QR‑Code lädt die Ressourcen herunter.</sample>
    <sample id="158">In der Vortragsfolie wird zunächst die Aufgabe der **Coreference‑Resolution** erläutert: Entitäten in einem Dokument können mehrfach erwähnt werden, und das Ziel ist es, diese Mentions zu identifizieren und gleichartige Entitäten zu clustern – beispielsweise „John“ mit „he“ und „Maria“ mit „her“. Traditionelle Verfahren prüfen alle möglichen Mentions‑Paare, was zu quadratischer Zeit‑ und Speicherkomplexität führt.

Um die Komplexität zu linear zu reduzieren, wurden **Cache‑basierte Modelle** eingeführt, die einen festen Cache benutzen. Beim Erreichen der Kapazität wird nach einer Evictions‑Strategie (wie LRU – Least Recently Used) evicted. In langen Texten wechselt das Thema jedoch häufig, sodass Entitäten über große Textbereiche verstreut sind. Das führt zu vielen Cache‑Misses, besonders bei hochfrequenten Entitäten, die global verteilt auftreten.

Die vorgestellte Lösung ist ein **Dual‑Cache**: ein lokaler Cache (LRU‑Eviction) für lokal gültige Entitäten und ein globaler Cache (LFU‑Eviction) für häufig erwähnte Entitäten. Das Modell läuft von links nach rechts, klassifiziert jede neue Mention als neue Entität oder als bereits im Cache befindliche, bewertet die Frequenz und entscheidet, ob die Entität in den globalen oder lokalen Cache aufgenommen wird. Bei voller Kapazität wird die entsprechende Eviction‑Regel angewandt.

Auf vier öffentlichen Benchmarks (LitBank, OntoNotes, WikiCoref, und einem 30.000‑Wort‑Buch) zeigt der Dual‑Cache sowohl bei vorhandenem Trainings‑Datensatz als auch ohne Trainingsdaten eine bessere Leistung als ein einzelner Cache oder ein unbeschränkter Speicher. Besonders bei Buch‑Level‑Dokumenten ist die Leistungsverbesserung deutlich. Der Dual‑Cache reduziert die Cache‑Miss‑Rate signifikant und bietet das beste Verhältnis von Kosten zu Leistung. In der Schlussfolgerung wird betont, dass der Dual‑Cache lokale und globale Entitäten getrennt verwaltet, die Effizienz steigert und die Kosten senkt.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha und freue mich, euch zu unserem Vortrag über unser ACL‑2023‑Paper begrüßen zu dürfen. Die Akzeptanzurteile von Sprachmodellen sind nicht immer robust gegenüber Kontext. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit überarbeiten wir die Minimal‑Pair‑Paradigmen.

Die Minimal‑Pair‑Paradigm (MPP) bewertet Sprachmodelle anhand von Akzeptanzurteilen, die auch die Grammatikalität wie BLiMP, SyntaxGym oder Stereotypen‑Akzeptanz wie CrowS‑Paaren einschließen können. Im Minimal‑Pair‑Paradigma wird typischerweise ein akzeptabler bzw. ein ungrammatischer Satz gezeigt, und die Hoffnung besteht darin, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuordnet.

Das aktuelle MPP‑Pipeline erlaubt es nicht, die Akzeptanz eines Modells für längere Sätze zu bewerten. Heutzutage kommen große Sprachmodelle mit immer längeren Kontextfenstern daher daher ist es entscheidend, die Akzeptanz des Modells während des gesamten Kontextfensters zu evaluieren – und das ist genau das, woran wir hier arbeiten.

Unser Ansatz besteht darin, die Datensätze selbst neu zu bearbeiten und Sätze zu rekonstruieren, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Beispielsweise haben wir ein typisches Grammatikpaar aus dem BLiMP‑Datensatz, dem „Adjunct Island“‑Fall, genommen und verlängern wir die Sätze, indem wir einen grammatikalischen Satz als Präfix zu beiden, dem akzeptablen und dem unakzeptablen, hinzufügen. Wir können dasselbe auch tun, indem wir unakzeptable Sätze aus derselben Übereinstimmung wählen – das dient ebenfalls zur Bewertung der Akzeptanz des Modells. Wir können auch Sätze aus einem anderen Teilbereich oder einem anderen Datensatz auswählen – das nennen wir das „Mismatch“-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus demjenigen, mit dem Sie gerade evaluieren. Dasselbe kann für den Unakzeptanzfall gemacht werden.

Schließlich können wir Sätze aus einem völlig unzusammenhängenden Bereich wie Wikipedia auswählen. Dadurch können wir herausfinden, ob die Akzeptanzurteile des Modells tatsächlich von irgendeinem Kontext beeinflusst werden – ob der Kontext aus einer anderen Teilmenge des Datensatzes stammt oder völlig irrelevant ist.

Wie verhält sich das Modell? Zuerst betrachten wir Wikipedia‑Sätze, die völlig irrelevant für das aktuelle Paar sind, und wir stellen fest, dass die MPP‑Urteile bei beliebiger Kontextlänge weitgehend robust bleiben. Wir erhöhen die Kontextlänge bis zu 1024 Token, um die maximal möglichen OPT- und GPT‑2‑Modelle auszuschöpfen, und sehen, dass die MPP‑Urteile in der orangefarbenen gestrichelten Linie relativ stabil bleiben.

Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir Sätze aus akzeptablen und unakzeptablen Domänen derselben BLiMP‑ oder SyntaxGym‑Datensätze. Dabei sehen wir, dass die MPP‑Urteile entweder signifikant steigen oder fallen, wenn wir akzeptable oder unakzeptable Präfixe hinzufügen. Und wenn wir die Struktur matchen – also Sätze aus demselben Phänomen in BLiMP oder SyntaxGym auswählen – sehen wir einen massiven Anstieg bzw. Abfall der MPP‑Urteile für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt ist enorm und nimmt mit zunehmender Kontextlänge zu, was auch neuere Sprachmodelle mit großen Kontextfenstern beeinträchtigen könnte.

Warum beeinflusst das passende Präfix die Urteile des Modells so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir den Eingangssatz durch Störungen (Noise) verändert haben, um die relevante Struktur zu erhalten, aber Rauschen hinzuzufügen. Nach mehreren dieser Störungen stellen wir fest, dass keine dieser Störungen das Modell in seiner MPP‑Urteilsergebnissignifikant verändert. Das Modell reagiert auf die gestörten Sätze in ähnlicher Weise: wenn wir Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir Sätze im unakzeptablen Bereich stören, sehen wir einen ähnlichen Rückgang bei allen Störungen.

Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die über die Sätze hinweg geteilt werden. Und die MPP‑Bewertung, wie wir sie derzeit mit kurzen, einzelnen Sätzen durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen der Sprachmodelle im gesamten Kontextfenster. Bitte lest unser Papier für weitere Details zu unseren Experimenten. Vielen Dank für eure Aufmerksamkeit.</sample>
    <sample id="160">Im ersten Schritt werden jedem Input‑Token ein **unordiniertes Multiset von Ausgabetokens** zugeordnet. Diese Multisets enthalten genau die Tokens, die später in der logischen Form erscheinen sollen.</sample>
    <sample id="161">In CoScript gibt es insgesamt **55 000** Skripte.</sample>
    <sample id="163">Die beste Ausrichtungsmethode laut dem englischen Vortrag ist **MASSalign**.</sample>
    <sample id="164">Der Hauptvorteil von schwach überwachtem Lernen ist die **Kosten‑ und Zeitersparnis bei der Datenerfassung**: statt Millionen von explizit annotierten Beispielen werden mit einfachen Heuristiken, Wissensbasen oder günstiger Crowd‑Annotation große Mengen an „schwachen“ Labels erzeugt. Dadurch lassen sich Modelle auf sehr großen Datensätzen trainieren, die sonst wegen fehlender manueller Anmerkungen nicht nutzbar wären.</sample>
    <sample id="165">In der Vortrags­präsentation stellt Wenting Zhao von der Cornell University die Arbeit „Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations“ vor. Sie erläutert zunächst den Begriff des abductiven Denkens anhand eines konkreten Beispiels: aus dem Kontext „Emily war im Stau“ folgt das Ergebnis „Emily erreichte ihren Flug“. Mögliche Erklärungen sind „Ihr Flug war verspätet“ oder „Ihr Flug lief rechtzeitig“. Das Ziel ist, eine plausible Erklärung zu finden, die die Lücke zwischen Kontext und Ergebnis überbrückt. 

Die Arbeit betrachtet ein geschlossenes Setting mit einer vorgegebenen Menge möglicher Erklärungen \(Z\) und strebt an, ein plausibles Teilset zu identifizieren, ohne dass diese explizit gelabelt sind. Traditionelle Ansätze sind stark auf supervision angewiesen, was zu subjektiven und fehlerhaften Anmerkungen führt (z. B. 60 % Unstimmigkeit bei Crowdsourcing). 

Zhao et al. führen das unüberwachte Verfahren **LiPoR** (Likelihood Learning with Posterior Regularization) ein. Dabei wird die Erklärung als latenter Variable behandelt und die marginale Likelihood \(p(Y|X)\) maximiert – ohne Plausibilitätslabels. Um dennoch plausiblen Erklärungen Vorrang einzuräumen, nutzt LiPoR die Eigenschaft gegenseitiger Ausschließlichkeit: die Regularisierung \(\Omega\) minimiert die Entropie der posterioren Verteilung \(p(Z|X,Y)\) bis zu einem Schwellenwert \(\log M\) (M = Anzahl plausibler Erklärungen). Dadurch wird die Gewichtung auf ein kleines, plausibles Subset konzentriert.

Experimentell wird LiPoR auf dem AlphaNLI‑Dataset getestet. Im Vergleich zu Zero‑Shot‑Modellen und dem bisher besten unüberwachten Ansatz übertrifft LiPoR die Genauigkeit um mehr als vier Prozentpunkte, sogar gegenüber einem starken GPT‑3 Zero‑Shot Baseline. Die Studie demonstriert damit, dass abductives Reasoning ohne explizite Plausibilitätslabels erlernt werden kann, indem man die inhärente Ausschließlichkeit von Erklärungen nutzt.</sample>
    <sample id="166">**Abstract**  
Wir stellen NDCR (Neural Divide‑and‑Conquer Reasoning) vor, einen neuartigen Rahmen zur Bild‑Suchaufgabe aus linguistisch komplexen Texten. Der Ansatz kombiniert die *Divide‑and‑Conquer*‑Strategie mit der *Dual‑Process Theory*, indem er zwei ergänzende Systeme einsetzt. System 1, ein visuell‑sprachliches Interactor, nutzt analogische Inferenz wie aktuelle multimodale Modelle, um einfache Propositionen aus dem Text zu extrahieren und mit Bildmerkmalen abzugleichen. System 2, ein neuronaler‑symbolischer Reasoner, übernimmt formale Logik: ein Negations‑Executor wandelt positive Propositionen in deren negierte Zustände um, während eine Konjunktions‑Operation die Ergebnisse der beiden Zustände integriert. Der Propositions‑Generator zerlegt zunächst komplexe Texte in leicht handhabbare Einheiten, die anschließend von beiden Systemen verarbeitet werden. Durch die Kombination der analogischen und logischen Ergebnisse erzielt NDCR signifikante Verbesserungen gegenüber bestehenden Baselines; Ablation‑Tests zeigen die Bedeutung aller Module. Demonstrationsfälle zeigen explizite Inferenzschritte und Resultate, was die Interoperabilität des Ansatzes unterstreicht. Abschließend wird betont, dass neuronale‑symbolische Berechnungen, die Divide‑and‑Conquer‑ähnliche Kettenlogik und Dual‑Process‑Ansätze zusammenführen, die Kompositionsfähigkeit großer Sprachmodelle nachhaltig steigern können.</sample>
    <sample id="167">Alle 750 Dokumente des DEPLAIN‑web‑Korpus wurden **gleichzeitig** manuell und automatisch ausgerichtet – das heißt, jedes der 750 Dokumente erhielt sowohl ein manueller als auch ein automatischer Alignment‑Ergebnis, was insgesamt 30 450 Satz‑paare ergab.</sample>
    <sample id="168">Der CoNLL++‑Datensatz wurde aus Reuters‑Nachrichten aus dem Jahr 2020 zusammengestellt und anschließend mit exakt den gleichen Annotation‑Richtlinien wie beim CoNLL‑2003‑Set (BIO‑Tagging, Entitätstypen etc.) annotiert.</sample>
    <sample id="169">**Zusammenfassung des Papers „Prompting PaLM for Translation: Assessing Strategies and Performance“**  

In dieser Studie wird die Leistung des 540‑Billionen‑Parameter‑LLM PaLM bei maschinellen Übersetzungen systematisch untersucht. Die Autoren nutzen aktuelle WMT‑Testsets, um Überschneidungen mit den Trainingsdaten zu vermeiden, und vergleichen PaLM mit führenden neuronalen MT‑Systemen, darunter Google Translate. Für die Bewertung werden modernste automatisierte Metriken (BLEURT, etc.) sowie eine Experten‑Human‑Evaluation nach dem MQM‑Framework eingesetzt.

Ein zentrales Thema ist die Prompt‑Strategie. In einem einfachen Experiment zeigen die Autoren, dass sogar ein einziger Prompt‑Änderungspunkt mehr als einen BLEURT‑Punkt ausmacht, und extreme Unterschiede bis zu 40 Punkten erreichen können. Aus diesen Erkenntnissen folgt die Wahl einer 5‑Shot‑Prompt‑Strategie, bei der jede zu übersetzende und übersetzte Satzform mit ihrer Sprache gekennzeichnet wird (z. B. „German: … / English: …“). Die Form des Prompts hat bei fünf Beispielen kaum Einfluss, während die Qualität der Beispiele entscheidend ist. Hochwertige, kuratierte Beispiele aus dem WMT‑Dev‑Set liefern bessere Ergebnisse als Beispiele aus dem (geringer bereinigten) Trainingsset.

Die Ergebnisse zeigen, dass PaLM zwar nahe an kommerziellen Systemen liegt, aber noch hinter spezialisierten MT‑Modellen zurückbleibt. Die MQM‑Analyse verdeutlicht, dass PaLM besonders flüssig ist, jedoch häufig Inhalte vernachlässigt (Omission‑Fehler). Die „Style/Awkward“-Kategorie ist bei PaLM niedriger, was die bessere Sprachqualität bestätigt. Insgesamt liefert die Arbeit wertvolle Einblicke in Prompt‑Design und die Grenzen von LLM‑basierten Übersetzungen.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute möchte ich unsere Arbeit „XSemPLR: Cross‑Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“ vorstellen. Semantic Parsing ist die Aufgabe, semantische Repräsentationen von Nutzeranfragen zu erzeugen, z. B. SQL‑Abfragen oder Lambda‑Kalkül. Cross‑Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in dieser Abbildung zu sehen ist, müssen wir Anfragen in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda‑Kalkül, FunQL usw. übersetzen.

Bestehende Modelle für Cross‑Lingual Semantic Parsing werden separat entwickelt und auf Datensätzen mit begrenzter Aufgabenvielfalt und Anwendung getestet. Beispielsweise gibt es viele Arbeiten zu bestimmten natürlichen Sprachen, aber Chinesisch fehlt noch und die Abdeckung bestimmter Bedeutungsdarstellungen ist unzureichend. Das Lambda‑Kalkül fehlt häufig, oder die Modelle werden nur mit einer einzigen Architektur evaluiert. Daher stellen wir XSemPLR vor. Wir liefern ein einheitliches Datenset XSemPLR für Cross‑Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen. Es enthält 9 Datensätze aus verschiedenen Domänen, 5 semantische Parsing‑Aufgaben, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen aus 15 Sprachfamilien.

Um die Benchmark besser evaluieren zu können, betrachten wir sechs Trainings‑ und Evaluierungssettings:

1. **Translate‑Test**  
   Wir benutzen die Google Translate API, um die Quelle in die Ziel‑Sprache zu übersetzen, und trainieren anschließend ein monolinguales Modell. Beispiel: Wir trainieren das englische Modell mit englischen Anfragen und übersetzen bei der Inferenz die deutsche Anfrage ins Englische, bevor das trainierte Modell die SQL‑Ausgabe erzeugt.

2. **Monolingual Model**  
   Hier ist die Quell- und Ziel‑Sprache identisch (z. B. Deutsch‑zu‑Deutsch oder Englisch‑zu‑Englisch).

3. **Monolingual Few‑shot**  
   Wir trainieren monolinguale Modelle mit nur 10 % der Trainingsdaten.

4. **Multilingual Model**  
   Wir trainieren ein einziges Modell für alle Sprachen. Beispiel: Wir kombinieren deutsche, englische und chinesische Anfragen zu einem Trainingsdatensatz und das Modell kann dann Anfragen in jeder dieser Sprachen verarbeiten.

5. **Cross‑Lingual Zero‑shot Transfer**  
   Wir trainieren nur in einer Quellsprache (z. B. Englisch) und übertragen auf eine andere Sprache.

6. **Cross‑Lingual Few‑shot Transfer**  
   Wir trainieren in einer Quellsprache und ergänzen mit wenigen (z. B. 10 %) Daten aus einer Zielsprache.

Wir haben interessante Ergebnisse erhalten. Bei der Analyse monolingualer Modelle haben wir zwei Modellgruppen evaluiert:

* **Encoder‑PTR** – Multilinguale vortrainierte Encoder mit Pointer‑basierten Decodern, z. B. XLM‑R + PTR und mBERT + PTR.
* **Encoder‑Decoder** – Multilinguale vortrainierte Encoder‑Decoder‑Modelle, z. B. mBART und mT5.

Encoder‑Decoder erzielt die besten Ergebnisse auf allen neun Datensätzen. In der multilingualen Einstellung haben wir mT5 und XLM‑R + PTR getestet. Wir fanden heraus, dass Encoder‑Decoder und Encoder‑PTR durch das Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Der Grund ist, dass bei den meisten Hauptsprachen eine Leistungssteigerung zu beobachten ist, während die englische Leistung in sieben Datensätzen sinkt und in nur drei Datensätzen steigt – ein Phänomen, das als „Curse of Multilinguality“ bezeichnet wird.

Wir vergleichen die Cross‑Lingual Performance‑Gap. In dieser Abbildung ist die blaue Linie Cross‑Lingual Few‑shot Transfer, die orange Linie Cross‑Lingual Zero‑shot Transfer und die grüne Linie das Monolinguale Setting. Wir stellen fest, dass der Gap zwischen der grünen und orangefarbenen Linie signifikant ist, während der Gap zwischen blau und orange mit wenigen Daten schnell verkleinert wird.

Weitere interessante Erkenntnisse:

* Encoder‑Decoder übertrifft frühere Arbeiten oder erzielt vergleichbare Ergebnisse.
* Pretraining in Englisch kann die Leistung bei Few‑shot-Transfer auf Zielsprachen erheblich steigern.
* Multilinguale Sprachmodelle wie Codex und BLOOM sind für Cross‑Lingual Semantic Parsing noch nicht ausreichend.

Zusammenfassend haben wir XSemPLR entwickelt – einen einheitlichen Benchmark für Cross‑Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsdarstellungen. Wir haben eine umfassende Benchmark‑Studie zu drei repräsentativen Typen multilingualer Sprachmodelle durchgeführt. Unsere Ergebnisse liefern viele interessante Erkenntnisse. Besucht gerne unser Paper und den Code. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Based on the presentation, the following types of prior work have already been pursued:

1. **Model‑level watermarking for large language models** – methods that embed a secret pattern in the parameters or outputs of a full‑scale LLM (e.g., via fine‑tuning, parameter perturbations, or output‑based signatures).  
2. **Embedding‑specific watermarking** – techniques that modify the embedding layer or the returned vectors (e.g., adding a fixed “marker” vector or altering embedding distributions).  
3. **Backdoor‑based watermarking** – approaches that inject a trigger set into the training data or inference pipeline so that the presence of the trigger reveals ownership.  
4. **Statistical or cryptographic watermarking** – schemes that use statistical tests (e.g., KS test, cosine similarity) or cryptographic primitives to detect whether a service contains the embedded watermark.

These earlier studies generally suffer from either **inapplicability to embedding‑as‑a‑service** (they target full models rather than the API) or **lack of transferability** (the watermark does not survive model extraction or fine‑tuning). The work you presented, “Embedding Marker,” addresses these gaps by injecting a trigger‑based watermark directly into the embedding outputs and providing a verification protocol that remains effective even after an attacker steals and re‑deploys the model.</sample>
    <sample id="172">Nein – laut der Untersuchung sind mehrsprachige LLMs wie Codex und BLOOM für Cross‑Lingual Semantic Parsing (CLSP) noch nicht ausreichend.</sample>
    <sample id="174">ArgAnalysis35K ist ein umfangreiches, qualitativ hochwertiges Datenset zur Bewertung von Argumenten auf einer Skala von 0 bis 1. Mit 35 000 Argument‑Bewertungs-Paaren ist es das bislang größte Korpus in diesem Bereich. Etwa 85 % der Argumente stammen aus Reden von Top‑Debattierern, Experten‑Debatten und fortgeschrittenen Teilnehmern, während die restlichen 15 % von Anfängern oder Alltagsdebattern stammen, was die Argumentqualität deutlich steigert.

Das Set zeichnet sich durch große thematische Vielfalt aus: anstelle von ein paar vorgegebenen Motiven wurden 24 Themen aus dem Debattier‑Circuit, Plattformen wie Hellomotions.com und Expertenrat zusammengestellt. Für jedes Thema wurden so viele Motive wie möglich gesammelt, was die Repräsentativität für parlamentarische Debatten erhöht.

Ein neu eingeführter Begriff ist „Analyse“ – eine zusammenhängende Erklärung, die sowohl Behauptungen als auch Begründungen enthält. Diese Ergänzung geht über reine Claim‑ oder Premise‑Spalten hinaus und zwingt die Annotatoren, die Argumente im Kontext zu erklären.

Zur Gewährleistung von Zuverlässigkeit wurde ein „instance‑based“ Annotator‑Reliability‑Modell eingeführt: Anstatt ganze Annotatoren für bestimmte Themen auszuschließen, werden nur deren voreingenommene Bewertungen entfernt. Dadurch bleibt mehr wertvolle Datenmengen erhalten.

Schließlich wird jedes Argument auf einer Skala von 0 bis 1 hinsichtlich seiner Relevanz zu jedem Thema bewertet. Dieses Relevanzmodell erlaubt die Anwendung eines Arguments in unterschiedlichen Debattenkontexten (z. B. Regierung, Kirche, Unternehmen) und liefert ein differenzierteres Verständnis der Argumente.

Zusammenfassend bietet ArgAnalysis35K eine große, hochwertige und vielfältige Sammlung mit erweiterten Analyse‑Spalten, instanzbasierter Zuverlässigkeit und einem Relevanz‑Score, was es zu einer wertvollen Ressource für Forschende im Bereich Argument Quality Analysis macht.</sample>
    <sample id="175">Die Methode behandelt Mehrdeutigkeit bei Permutationen, indem sie die optimale Reihenfolge nicht fest vorgibt, sondern sie als **latent** lässt und sie im Training induziert. Dazu werden alle möglichen Konsistenz­perm­utationen im Hinter­grund betrachtet, aber das Modell nutzt ein GPU‑freundliches, kontinuierliches Relaxationsverfahren, um die höchstwahrscheinliche Permutation zu approximieren. Durch Back‑Propagation kann das Netzwerk lernen, die linguistisch plausibelste Permutation auszuwählen, auch wenn mehrere mathematisch zulässige Permutationen existieren.</sample>
    <sample id="176">Fairness eines nachgeschalteten NLP‑Modells wird damit definiert, dass seine Leistung (z. B. Genauigkeit, Fehlerquoten) **über alle relevanten Gruppen hinweg gleichmäßig** ist – sei es nach Demografie, politischer Leaning oder Medienquelle. In anderen Worten: ein fairer Modell liefert keine systematischen Über- oder Unterrepräsentationen von bestimmten Gruppen in den Vorhersagen.</sample>
    <sample id="177">Der Referent heißt **Yanis Labrak**.</sample>
    <sample id="178">Der Referent bzw. die Referentin heißt **Koustav Sinha**.</sample>
    <sample id="179">**Abstract (≤200 Wörter)**  

Diese Arbeit untersucht, wie das Theory‑of‑Mind‑(ToM)‑Verständnis von großen Sprachmodellen (LLMs) verbessert werden kann. Standard‑ToM‑Tests, etwa das Sally‑Anne‑Experiment, zeigen, dass Modelle wie GPT‑3 bei falschen Glaubensfragen stark schwächeln. Wir stellen **SymbolicToM** vor – ein inference‑time‑Methoden‑Framework, das explizite graphische Repräsentationen mentaler Zustände erzeugt. Für jedes Zeichenpaar \(p_i, p_j\) wird ein Graph \(B_{p_i,p_j}\) erstellt, der die aktuelle Welt aus Sicht von \(p_i\) und dessen Schätzung von \(p_j\)'s Sicht wiedergibt. Diese Graphen werden durch NLI‑ und OpenIE‑Modelle generiert, sodass bei jeder Frage die passende Graph‑Satz‑Extraktion erfolgt und anschließend ein LLM zur finalen Antwort eingesetzt wird. In‑Domain‑Tests (ToMi) zeigen, dass SymbolicToM die Performance aller ausgerollten LLMs um bis zu 65 % steigert. Für zwei neu konzipierte Out‑of‑Domain‑Datensätze (D₁–D₃) und ein linguistisch diversifiziertes ParaphrasedToMi‑Set bleibt die Methode überlegen; GPT‑4 erreicht dank SymbolicToM nahezu perfekte Ergebnisse. Das Verfahren ist plug‑and‑play, vermeidet Overfitting und liefert interpretierbare, graphbasierte Begründungen.</sample>
    <sample id="180">The speaker is **Myra**.</sample>
    <sample id="181">**Abstract**

Wir stellen das Problem des *constrained language planning* vor, bei dem ein Ziel nicht nur abstrakt („einen Kuchen backen“) sondern unter konkreten, vielschichtigen Einschränkungen (z. B. „einen Schokoladenkuchen backen“) formuliert wird. Bestehende LLM‑basierte Planer decken diese speziellen Ziele kaum ab. Aus diesem Grund generieren wir zunächst 100 spezifische Ziele über InstructGPT und analysieren die von LLMs erstellten Skripte. Obwohl die semantische Vollständigkeit akzeptabel ist, fehlt die Treue zu den Constraints. Wir zeigen, dass die Leistung stark von der Kategorienvielfalt der Constraints abhängt.

Zur Verbesserung führen wir eine *over‑generate‑then‑filter*‑Strategie ein: InstructGPT erzeugt K Skripte pro Ziel, anschließend wählt ein Filtermodell – basierend auf InstructGPT‑Embeddings, Cosinus‑Ähnlichkeit und Schlüsselwort‑Rewards – das glaubwürdigste Skript aus. Diese Methode steigert sowohl Vollständigkeit als auch Constraint‑Faithfulness signifikant.

Schließlich distillieren wir ein umfangreiches Dataset, **CoScript**, mit 55 000 spezifischen Zielen und zugehörigen Skripten. Crowd‑source‑Review sichert die Qualität von Validierungs‑ und Test‑Samples. Durch fine‑tuning von T5 auf CoScript erreichen kleinere Modelle eine Qualität, die viele große LLMs übertrifft. CoScript soll als Grundlage für weiterführende Forschungen im Bereich kontext‑sensitiver Sprachplanung dienen.</sample>
    <sample id="182">**Tropikalismus** ist in dieser Arbeit die stereotype Darstellung von Personen aus tropischen Regionen (z. B. Lateinamerika) als „exotisch“, „lebhaft“ und „sinnlich“.  
Es beschreibt die Tendenz, Menschen aus diesen Gebieten ausschließlich über tropische Merkmale zu charakterisieren – ein Beispiel für eine positiv klingende, aber letztlich problematische, Essentialisierung.</sample>
    <sample id="183">Die Autoren haben Menschen zu selbst‑beschreibenden Personas aufgefordert, indem sie dieselben Prompt‑Strukturen wie für die LLM‑Generierung verwendeten. Konkret ließen sie Probanden antworten auf Fragen wie „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst.“ Dadurch entstand ein Satz von von Menschen verfassten Beschreibungen, der als Vergleichsdatensatz zu den von den Modellen generierten Personas diente.</sample>
    <sample id="184">In der Arbeit wird die Kontextnutzung mithilfe des **CXMI‑Metrics** gemessen – genauer mit einer erweiternden Variante, dem **Pointwise‑CXMI (P‑CXMI)**, die den Informationsgewinn aus dem Kontext für jedes Wort bzw. jeden Satz erfasst.</sample>
    <sample id="185">**Unterschied zwischen DrBERT und ChuBERT**

| Merkmal | DrBERT | ChuBERT |
|---------|--------|---------|
| **Ziel** | Speziell für den französischen *biomedizinischen* Bereich (allgemeine medizinische Texte) | Speziell für den französischen *klinischen* Bereich (medizinische Notizen, Patientenakten) |
| **Datenquelle** | NACHOS‑Korpus – medizinische Web‑Crawls (etwa 4–7 GB) | Anonymisierte klinische Daten aus dem Datenlager der Nantes‑Universität (≈ 4 GB) |
| **Training** | From‑scratch‑Pre‑training (auch Variante mit 7 GB) | From‑scratch‑Pre‑training (oder Variante mit Mischung aus NACHOS+klinischen Daten) |
| **Anwendungsfall** | Allgemeine biomedizinische NLP‑Aufgaben (NER, Klassifikation, QA usw.) | Klinische NLP‑Aufgaben (z. B. Entitäten aus Patienten‑Notizen, klinische Klassifikation) |

Kurz: **DrBERT** ist ein französischer Biomedizin‑Modell, trainiert auf medizinischem Web‑Text, während **ChuBERT** ein französischer Clinical‑Modell ist, das auf anonymisierten klinischen Notizen basiert.</sample>
    <sample id="187">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="188">**Iteratives Transferlernen** bedeutet, ein Modell schrittweise von einem Task auf einen anderen zu übertragen, indem man es **nach jedem Transfer neu fein‑tuned**.  

In der Praxis wird ein vortrainiertes Modell zunächst auf einen verwandten Task (z. B. CE‑Klassen) angepasst, die daraus gewonnene Gewichte werden dann als Ausgangspunkt für den nächsten Task (z. B. Debatte) verwendet, und so weiter. Jeder Schritt nutzt die bereits erhaltenen Gewichte als Basis, anstatt alle Daten auf einmal zu sammeln („kumulative“ Ansatz).  

Dieses Vorgehen erleichtert die Anpassung an unterschiedliche Domänen und kann bei seltenen Klassen, wie im Dissonanz‑Erkennungs‑Projekt, die Leistung verbessern.</sample>
    <sample id="189">The AltEntities Corpus is built to benchmark and improve models’ ability to interpret **indirect referring expressions** in conversational choice‑making. It provides natural dialogues where users select between two or more entities (songs, books, recipes) using non‑explicit references, enabling research on entity selection from context.</sample>
    <sample id="190">Ein Angreifer kann die Modellparameter bei einem Embedding‑as‑a‑Service (EaaS) durch **Daten‑Steganographie** extrahieren:  

1. **Massive Abfragen** – Der Angreifer sendet systematisch viele Texte an das EaaS‑Endpoint.  
2. **Erfassung der Embeddings** – Für jede Anfrage erhält er die vom Service zurückgegebenen Vektoren (Embeddings).  
3. **Schaffung eines Trainingsdatensatzes** – Die gesammelten Text‑Embedding‑Paare bilden einen eigenen Datensatz.  
4. **Schulung eines Surrogat‑Modells** – Mit diesem Datensatz trainiert der Angreifer ein eigenes Modell (z. B. ein neuronales Netz), das die gleiche Embedding‑Funktion approximiert.  
5. **Parameter‑Approximation** – Durch das Training kann der Angreifer die zugrunde liegenden Gewichtungen und Architekturparameter des Originalmodells rekonstruieren bzw. nachbilden.  

Damit nutzt der Angreifer die öffentlich zugängliche Embedding‑API als „Black‑Box“-Schleife, um die inneren Modellparameter indirekt zu „stehlen“.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="192">In der Präsentation schildert Yang Luo das Ziel, einen Optimierer zu entwickeln, der die schnelle Konvergenz adaptiver Verfahren wie Adam mit der geringen Speichereffizienz von Speicher‑Effizienten Optimierern wie Adafactor kombiniert. Adam speichert pro Parameter zwei Momentenschätzungen, was das Speicher‑Footprint vervielfacht, während Adafactor dieses Problem durch eine Faktorisierung der zweiten Momentenmatrix auf eine niedrigere Rang‑1‑Approximation löst. Diese Annäherung reduziert jedoch die Genauigkeit der Gradientenupdates und führt zu einer langsamen, instabilen Konvergenz.  

Luo führt die Idee ein, die „Instabilität“ zwischen dem vorhergesagten Update‑Moment (mₜ) und dem tatsächlich erzeugten Update (uₜ) zu messen. Durch die Residual‑Matrix wird ein adaptiver Nenner für das Update berechnet, der die Fehler kompensiert. Der resultierende Optimierer, CAME (Confidence‑guided Adaptive Memory‑Efficient Optimizer), nutzt diese Residual‑basierten Gewichtungen, um die Updates zu stabilisieren und gleichzeitig die Speicher‑Komplexität zu reduzieren.  

Experimentell wird CAME auf BERT, GPT‑2 und T5 in Pre‑Training‑Aufgaben mit großen Batch‑Größen evaluiert. Die Ergebnisse zeigen, dass CAME im Vergleich zu Adam und Adafactor die Validierungsgenauigkeit um etwa 3 % steigert und bei sehr großen Modellen bis zu 4 × weniger Speicher benötigt. Auch bei Down‑stream‑Tests liefern BERT‑Modelle, die mit CAME trainiert wurden, vergleichbare Leistungen wie die Baseline, jedoch mit deutlich geringeren Speicherverbrauch. CAME demonstriert somit eine effiziente, robuste Alternative für die Optimierung großer Sprachmodelle.</sample>
    <sample id="193">Die in der Zusammenfassung angegebene Information enthält keine Angabe zur Anzahl der Annotatoren, die zur Erstellung des ursprünglichen Datensatzes eingesetzt wurden.</sample>
    <sample id="194">Die Autoren kommen aus drei Institutionen:

- **Carnegie Mellon University** (Jenny – erste‑Jahr‑PhD‑Studentin)  
- **University of Washington** (Sebastian Santy, Ronan Le Bras, Katharina Reinecke)  
- **Allen Institute for AI** (Maarten Sap)

So die Arbeit wurde in Zusammenarbeit zwischen CMU, UW und dem Allen Institute for AI durchgeführt.</sample>
    <sample id="195">**Zusammenfassung**

Die Präsentation stellt das neue Framework *RoHT* (Reasoning over Hierarchical Question Decomposition Tree) vor, das erklärbare Frage‑Antwortsysteme (XQA) verbessern soll. XQA‑Modelle lassen sich in neuro‑symbolische Ansätze (Übersetzung von Fragen in formale Abfragen wie SPARQL) und dekompositionsbasierte Methoden (Generierung von Zwischenfragen oder Chain‑of‑Thought‑Schritten) einteilen. Neuro‑symbolische Systeme sind an strukturierte Wissensbasen (KB) gebunden und leiden bei unvollständigen Daten, während dekompositionsbasierte Verfahren oft ausschließlich auf freien Textkorpora zurückgreifen und dadurch an Genauigkeit verlieren. RoHT löst diese Probleme, indem es komplexe Fragen in eine hierarchische Struktur zerlegt – den *Hierarchical Question Decomposition Tree* (HQDT). Jeder Knoten repräsentiert eine Teilfrage; Blätter sind atomare Fragen, die nicht weiter zerlegbar sind.  

Der Prozess besteht aus zwei Stufen:  
1. **HQDT‑Erstellung** – ein Decomposer erzeugt atomare Teilfragen, ein Generator bildet daraus Zwischenfragen und berechnet für jeden Knoten eine Vertrauenswahrscheinlichkeit.  
2. **Probabilistisches Reasoning** – rekursiv von Wurzel zu Blatt: ein Scheduler wählt die passende Wissensquelle (KB, Textkorpus oder weitere Rekursion), ein Executor liefert Antworten mit Wahrscheinlichkeiten und ein Aggregator fasst diese zu den finalen Antworten zusammen.  

RoHT wurde auf zwei anspruchsvollen Datensätzen getestet: *KQA‑Pro* (KB‑basierte QA, KB unvollständig, Wikipedia als Textkorpus) und *Musique* (Text‑QA‑Comprehension, Wikidata als KB). In beiden Fällen übertraf RoHT die bestehenden Methoden deutlich – sowohl in KB‑Modellen als auch in Mixed‑Modellen, wobei die Kombination von KB‑ und Textwissen die größte Leistungsvorteile erzielte. Die Ergebnisse unterstreichen die Wirksamkeit expliziter Decomposition und der hierarchischen Probabilitätssynthese bei komplexen Frage‑Antwortaufgaben.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer (Governor) auf der linken Seite lautet: **„I saw Bart and Lisa.“**</sample>
    <sample id="197">**Stand der Technik für Dialogsysteme (nach dem ABC‑Eval‑Paper)**  

- **Architektur**: Moderne Chat‑Modelle basieren auf großen Transformer‑Netzwerken (z. B. GPT‑4‑ähnlichen, LLaMA‑ähnlichen oder Claude‑artigen Modellen).  
- **Funktionalität**: Sie können kontextuell relevante, zusammenhängende und teilweise empathische Antworten generieren und werden in interaktiven Szenarien wie Alexa, ChatGPT oder ähnlichen Plattformen eingesetzt.  
- **Evaluierung**: Traditionell werden sie per Likert‑Skala (Turn‑Level oder Dialog‑Level) oder Paarvergleich beurteilt. Das neue ABC‑Eval‑Framework ergänzt diese Ansätze, indem es explizite Fehler‑Labels (z. B. Irrelevanz, Selbst‑/Partner‑Kontradiktion, Halluzination, Common‑Sense‑Verletzung, Empathie) annotiert.  
- **Leistungsprofil**: Laut ABC‑Eval weisen aktuelle Modelle (4 State‑of‑the‑Art‑Bots) noch signifikante Fehler auf – etwa 20 % Common‑Sense‑Verletzungen, 15 % irrelevante Informationen und 10 % Selbst‑/Partner‑Kontradiktionen. Trotz dieser Schwächen liefern die Modelle immer noch eine hohe Gesamtkonversation‑qualität, die jedoch durch die neuen, dimensions‑basierten Metriken besser differenziert werden kann.  

Kurz gesagt: Der aktuelle Stand ist die Nutzung großer, vortrainierter Transformer‑Modelle, die mit hochpräzisen, dimensionsbasierten Bewertungsrahmen wie ABC‑Eval ergänzt werden, um sowohl die Stärken als auch die Schwächen noch detaillierter zu quantifizieren.</sample>
    <sample id="198">We müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil moderne große Sprachmodelle immer längere Kontexte verarbeiten und deren Urteilsfähigkeit stark von vorangehenden Sätzen abhängen kann. Der klassische Minimal‑Pair‑Paradigm‑Ansatz testet lediglich einzelne, isolierte Sätze und kann daher nicht erkennen, ob ein Modell seine grammatische Sensibilität im Verlauf eines längeren Textes verliert oder verstärkt. Durch die Evaluation über das gesamte Kontextfenster stellen wir sicher, dass die Modelle auch bei den nun üblichen, umfangreichen Kontextfenstern zuverlässig und robust bleiben.</sample>
    <sample id="199">Ja – in der Mehrsprachigkeit sinkt die Leistung des englischen Modells in den meisten Datensätzen (bei sieben von neun fällt sie, bei drei verbessert sie sich).</sample>
    <sample id="200">No – annotators only know the entity **names**.  Before writing the indirect references they are given background knowledge (e.g., Wikipedia snippets, search links, images) so they can learn the details of each entity, but they don’t already know the entities’ full attributes or content.</sample>
    <sample id="201">In der Studie wurde primär **BLEURT** als neuronale MT‑Metrik verwendet. Zusätzlich wurden weitere moderne neuronale Metriken (wie COMET, BLEU usw.) herangezogen, wobei BLEURT im Fokus der Analyse stand.</sample>
    <sample id="202">Nein – die Untersuchung zeigte keinen Hinweis darauf, dass die Regession bei der Generalisierung gezielt einzelne NER‑Typen (z. B. Personen, Organisationen oder Orte) stärker beeinflusst. Der Rückgang in der Leistung war in den Gesamt‑F1‑Scores beobachtet und nicht auf bestimmte Entitätstypen beschränkt.</sample>
    <sample id="203">**Why Positionalität matters for NLP**

- **Captures real‑world bias:** Models and datasets encode the judgments of their creators and annotators; if these reflect a narrow set of identities, the system will systematically favor those groups and mis‑serve others.  
- **Ensures fairness and inclusivity:** By quantifying how well predictions align with diverse users, we can identify and correct unfair performance gaps (e.g., lower accuracy for non‑binary or non‑English speakers).  
- **Guides responsible design:** Knowing the positionality of a model helps researchers make informed design choices, document assumptions, and build transparency into the research process.  
- **Supports equitable deployment:** When a system is aligned with only a subset of the population, its real‑world impact is limited and potentially harmful; understanding positionality allows for targeted improvements or community‑specific models.  

In short, positionality is essential for building NLP systems that are accurate, fair, and truly useful across the full spectrum of users.</sample>
    <sample id="204">Nein – die vorgestellten mehrsprachigen LLMs (wie BLOOM) wurden in der Studie **vollständig fein‑justiert** (Fine‑Tuning), nicht über Adapter‑Module angepasst.</sample>
    <sample id="205">**Zusammenfassung (≈ 200 Wörter)**  

Die Arbeit untersucht, wie politische Vorurteile aus den Trainingsdaten von Sprachmodellen (LMs) über die Modelle selbst bis zu downstream‑Aufgaben (Hate‑Speech‑ und Fake‑News‑Detection) übertragen werden. Zunächst werden LMs mit politischen Fragebögen (z. B. dem „Political Conference Test“) evaluiert, um deren ideologische Neigung zu bestimmen. Die Ergebnisse zeigen, dass LMs alle vier Quadranten des politischen Spektrums abdecken, wobei GPT‑4 am liberalsten ist, während BART‑Variante tendenziell konservativer wirken.  

Um den Einfluss der Daten zu isolieren, werden Modelle auf sechs partizipierten Korpora (nach Quelle, Plattform und politischer Neigung) weitertrainiert. Dabei verschiebt sich die Ideologie des Modells in die Richtung des Trainingsdatensatzes (z. B. robert‑b auf linkes Reddit wird liberaler). Ein weiteres Experiment trennt Daten vor und nach der 45. US‑Präsidentenwahl (2017); Modelle, die nach 2017 trainiert wurden, zeigen stärker ausgeprägte Polarisierung.  

Schließlich wird die Leistung politisch orientierter LMs in Hate‑Speech‑ und Fake‑News‑Datenbanken analysiert. Linke Modelle erkennen besser Hass‑Sprache gegen Minderheiten, rechte Modelle besser gegen dominante Gruppen; analog bei Fake‑News‑Erkennung. Diese Unterschiede können zu unfairen Ergebnissen führen, wenn beispielsweise ein rechtsgerichtetes LM für Moderationsaufgaben eingesetzt wird.  

Die Studie betont das Dilemma: politische Neutralität zu gewährleisten, ohne Zensur zu riskieren, und ruft zu einer sorgfältigen Abwägung von Datensäuberung, Modellbias und gesellschaftlicher Fairness auf.</sample>
    <sample id="206">Sie nutzen ein vortrainiertes Transformer‑Modell (z. B. BERT/roBERTa), das zunächst auf den **CE‑Aufgaben** (Expansion/Comparison) und anschließend auf der **Debatte‑Aufgabe** fine‑tuned wird. Dieses Modell liefert die Basis für das Transfer‑Learning in Ihrem Arbeitsablauf.</sample>
    <sample id="207">Die PaLM‑Evaluierung wurde ausschließlich an den aktuellen WMT‑Benchmark‑Testsets durchgeführt – also den offiziellen Testsets der neuesten WMT‑Shared‑Task‑Runden (z. B. WMT 22/23) für die untersuchten Sprachpaare (z. B. Deutsch → Englisch, Englisch → Deutsch usw.). Für die Auswahl der Prompt‑Beispiele wurden zusätzlich die zugehörigen WMT‑Dev‑Sets verwendet, um qualitativ hochwertige Referenzübersetzungen zu ziehen.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode liegt bei etwa **12 % absolute Verbesserung** gegenüber der stärksten Baseline (gemessen an den kombinierten Kennzahlen für semantische Vollständigkeit und Treue zu den Constraints).</sample>
    <sample id="210">Der Referent*in heißt **Shuheng**.</sample>
    <sample id="211">Ja, die Ergebnisse und der DEPLAIN‑Datensatz können als Benchmark genutzt werden.  
Der Datensatz liefert manuell alignierte Dokumenten- und Satzpaare in deutscher Sprache – ideal als Gold‑Standard für die Bewertung von Alignierungsverfahren und als Trainings‑/Test‑Set für Modelle, die komplexe Texte in einfache Sprache überführen. Die in der Studie erzielten Scores dienen zudem als Referenz für zukünftige Verbesserungen.</sample>
    <sample id="212">The experiments with smaller, specialized models involve **one** such model – a fine‑tuned T5.</sample>
    <sample id="213">The base model used for the multimodal instruction‑tuning experiments is **OFA (the large version of the unified multi‑modal pre‑trained model).**</sample>
    <sample id="215">**Zusammenfassung (ca. 200 Wörter)**  

In der Präsentation erläutert Adam Przepiórkowski die unterschiedlichen Ansätze zur Abhängigkeitsstruktur von Koordination in der Syntax. Traditionell sind die Modelle asymmetrisch: In den Universal Dependencies (UD) und Mel'čuk’s Meaning‑Text‑Theory (MTT) wird die gesamte Koordinationsstruktur von der ersten Konjunktion (z. B. „Lisa“) als Kopf geführt. Im Pragmatic‑Ansatz der Prager Grammatik hingegen wird der Konjunkt „und“ selbst zum Kopf, sodass von ihm zu allen Konjunktoren abgeleitet wird. Ein weiterer, weniger verbreiteter Ansatz ist der mehrköpfige in Hudson’s Word Grammar, bei dem jeder Konjunkt gleichzeitig Kopf ist.  

Przepiórkowski argumentiert für die symmetrische Variante (Konjunkt als Kopf), indem er die Regel der Minimierung der Abhängigkeitslänge (Dependency Length Minimization, DLM) heranzieht. Er zeigt anhand von Beispielen, dass ein Objekt („Buch“) nahe am Verb stehen muss, aber bei sehr langen Objekten zulässig ist, wenn dadurch die Gesamtlänge der Abhängigkeiten verringert wird („Marge liest gestern dieses faszinierende Buch …“).  

Des Weiteren wurden Statistiken aus der „Enhanced Penn Treebank“ ausgewertet. Dabei wurde bestätigt, dass linker Konjunkt tendenziell kürzer ist – ein Effekt, der stärker wird, je größer der Längenunterschied der Konjunktionen ist. Dieser Trend tritt nur auf, wenn kein rechter Governer vorhanden ist oder dieser links steht; bei rechtsgerichtetem Governer verschwindet die Tendenz.  

Die Ergebnisse stützen die symmetrische Sichtweise, da asymmetrische Modelle diesen beobachteten Längen‑ und Governertendenzen nicht gerecht werden.</sample>
    <sample id="217">**Summary (≈200 words)**  

Weihao Zeng et al. address the gap in controllable dialogue systems that currently handle only single attributes, often discrete, and lack generalization to unseen attribute combinations. They propose **Disentangled Controllable Generation (DCG)**, a prompt‑based model built on DialoGPT that learns attribute concepts from seen values and disentangles different attribute combinations via a dedicated loss.  

DCG employs two complementary prompts:  
1. **Attribute‑oriented prompts** encode instance‑specific controllable signals (e.g., a specific emotion or persona) by concatenating attribute‑value embeddings.  
2. **Task‑oriented prompts** provide instance‑independent global features that improve text quality.  

To boost diversity, pseudo attribute combinations are generated, and a disentanglement loss encourages the model to separate distinct attribute representations.  

For evaluation, the authors introduce **MAE**, a unified reference‑free metric that uses template prompts (e.g., “emotion/act/persona controls the response [MASK]”) and a trainable continuous dialogue‑oriented prompt, eliminating the need for large annotated datasets. MAE correlates strongly with human judgments for both coarse‑grained discrete and fine‑grained continuous attributes, outperforming classic metrics.  

Experiments on DailyDialog‑CG and a second benchmark show DCG surpasses baselines (including CTRL) in both controllability (E‑ACC, A‑ACC) and text equality (BLEU), with only a minor drop in controllability. PCA visualisations confirm that DCG’s prompt embeddings disentangle attribute space and enable compositional generalization from seen to unseen combinations. The work demonstrates that shared, disentangled prompts can effectively transfer learned attribute concepts across multiple attributes.</sample>
    <sample id="218">Die Autoren sind dem Google‑Translate‑Team zuzuordnen – es handelt sich nicht um eine Universitätsaffiliation.</sample>
    <sample id="219">**Kurzfassung (≈ 200 Wörter)**  

Jia‑Huei Ju und seine Kollegen präsentieren einen mehrstufigen Pipeline-Ansatz zur automatischen Analyse von SEC‑Form 10‑K‑Berichten. Das Ziel ist es, wichtige finanzielle Signale aus den jährlichen Unternehmensberichten zu extrahieren, ohne dass umfangreiche manuelle Annotationsarbeit nötig ist. Sie stellen fest, dass etwa 80 % der Tokens zwischen aufeinanderfolgenden Jahresberichten identisch sind, was die Notwendigkeit einer Vergleichs‑und‑Kontrastaufgabe hervorhebt.  

Der Ansatz besteht aus vier Stufen:  
1. **Dokumentsegmentierung** (Stage 0, nicht im Vortrag behandelt).  
2. **Relationserkennung** (Stage 1), bei der Text‑Paare in drei Typen eingeteilt werden – *β* (hohe syntaktische und semantische Ähnlichkeit), *Revised* (gleiche Syntax, unterschiedliche Bedeutung) und *Mismatched* (neu eingeführte Informationen).  
3. **Out‑of‑Domain‑Fine‑Tuning** (Stage 2) mit dem externen eSNLI‑NLI‑Datensatz, der token‑basierte Rationale enthält.  
4. **In‑Domain‑Fine‑Tuning** (Stage 2+) unter Verwendung von pseudo‑positiven Labels aus den *Revised*‑Paaren, wobei Soft‑Label‑Techniken (Cross‑Entropy + KL‑Divergenz) zur Linderung von Qualitätsproblemen eingesetzt werden.  

Die Bewertung erfolgt anhand von Precision@Recall und Pearson‑Correlation (PCC) auf zwei Datensätzen: eSNLI und dem neu veröffentlichten **FINAL**‑Datensatz. Die erweiterte Pipeline erreicht die besten Ergebnisse auf FINAL und behält gleichzeitig eine solide Generalisierung auf eSNLI bei. Zudem demonstriert sie Vorteile bei der Behandlung von *Mismatched*‑Paaren, die beim Training nicht verwendet wurden.  

Die Autoren betonen, dass zukünftige Arbeiten zusätzliche Features, fortgeschrittene IR‑Techniken und weitere Verbesserungen der Effektivität beinhalten könnten. Das Paper und der zugehörige GitHub‑Repository stehen zur Verfügung.</sample>
    <sample id="220">The authors are affiliated with **Stony Brook University**.</sample>
    <sample id="221">Die Arbeit untersuchte insbesondere das Sprachpaar **Deutsch → Englisch** (und entsprechend die Rückübersetzung Englisch → Deutsch, wenn diese in den Experimente enthalten war).</sample>
    <sample id="222">**Zusammenfassung (≈ 200 Wörter)**  

Das Papier untersucht, wie Open‑Domain‑Question‑Answering‑Modelle – bestehend aus einem Retriever und einem Reader – von einer generischen Quelle (Wikipedia) auf spezialisierte Zieldomänen (z. B. Biomedizin, Recht, Technik) übertragen werden können. Der Hauptfokus liegt auf datenbasierten Interventionen, die die Domänenkompatibilität verbessern.  

Zwei Hauptansätze werden vorgestellt: *zero‑shot*‑Interventionen, bei denen große Sprachmodelle ohne Zielbeispiele neue Fakten generieren, und *few‑shot*‑Interventionen, bei denen wenige Ziel‑Beispiele genutzt werden, um zusätzliche Trainingsdaten zu synthetisieren. Beide Methoden transformieren die generierten Fakten in Cloze‑Fragen, um Retriever und Reader zu adaptieren. In Experimenten steigert die *few‑shot*‑Intervention den Reader‑Score im Durchschnitt um 11 % und den Retriever um 8 %.  

Zur Klassifizierung von Shift‑Typen wird ein Kompatibilitätsmaß entwickelt, das die Wahrscheinlichkeit des Retrievers bzw. Readers für gegebene Kontext‑ bzw. Antwort‑Sets misst. Auf Basis dieses 2‑D‑Scores werden sieben Ziel‑Datasets in *No Shift*, *Concept Shift*, *Covariate Shift* und *Full Shift* eingeteilt. Die Analyse zeigt, dass *few‑shot*‑Anpassungen bei allen Shifts wirksam sind, während *zero‑shot*‑Interventionen besonders bei Concept‑ bzw. Covariate‑Shift helfen.  

Schließlich demonstriert die Studie, dass gezielte Dateninterventionen die Reader‑Leistung um bis zu 24 % steigern können, wobei die Effektivität stark vom jeweiligen Shift‑Typ abhängt.</sample>
    <sample id="223">Der Referent*in heißt **Shangbin**.</sample>
    <sample id="224">Die Experimente untersuchten:

1. **MASSalign** – als automatische Aufgabenausrichtung (Alignment) von Sätzen im gleichen Text auf unterschiedlichen Komplexitätsstufen.  
2. **long‑mBART** – als Modell für die Dokument‑Level‑Textvereinfachung.  
3. **mBART (Basis‑Version)** – als Modell für die Satz‑Level‑Textvereinfachung.</sample>
    <sample id="225">In total, **62** distinct tasks are included in MultiInstruct.  
- **53 tasks** are selected for the training set.  
- The remaining **9 tasks** (the rest of the 62) are kept for the test set.</sample>
    <sample id="226">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="227">**Zusammenfassung**

Die Arbeit kritisiert die aktuelle Dominanz von generativen Sprachmodellen bei der „grounded language understanding“ (GLU), also der Übersetzung natürlicher Sprache in ausführbare Programme oder Pläne in spezifischen Umgebungen. Der zentrale Schwachpunkt liegt im fehlenden Grounding während des Pre‑Trainings: Modelle lernen ausschließlich aus Textkorpus, wodurch sie bei der Umwandlung von Sprache in strukturierte Ausdrücke (z. B. SQL‑Abfragen, ROS‑Befehle) oft grammatikalisch falsche oder nicht ausführbare Programme erzeugen.  

Um dieses Problem zu umgehen, wird ein neues Rahmenkonzept namens **Pangu** vorgeschlagen. Hier übernimmt ein symbolischer Agent die Generierung von Kandidatenplänen, während das Sprachmodell lediglich die Kandidaten bewertet und sortiert. Dadurch entfällt die Notwendigkeit, die syntaktische Korrektheit selbst zu garantieren, und die Stärken des Modells als Diskriminator werden voll ausgenutzt.  

Die Methode wird auf Knowledge‑Based Question Answering (KBQA) getestet – ein typisches GLU‑Problem – und mit BERT, T5 und Codex sowohl im Fine‑Tuning als auch im In‑Context‑Learning evaluiert. Pangu übertrifft das bestehende Basismodell ArcaneQA in aller Hinsicht: höhere Genauigkeit, bemerkenswerte Sample‑Effizienz (z. B. &gt; 50 % Genauigkeit bei GRAIL mit nur einem Demo‑Beispiel) und verbesserte Robustheit gegen Nicht‑i.i.d. Verteilungen.  Die Analyse zeigt, dass autoregressive Modelle tendenziell auf bekannte Strukturen overfitten, während Pangu sowohl bei bekannten als auch unbekannten Strukturen ähnliche Wahrscheinlichkeitsverteilungen erzeugt.  

**Kernbotschaft**  
„Bei grounded language understanding ist Generierung oft weniger geeignet als Diskriminierung – Pangu demonstriert, dass die Kombination aus symbolischer Planung und modellbasierter Bewertung die Leistungsfähigkeit signifikant steigert.“</sample>
    <sample id="228">Die Autoren haben Experimente an vier Datensätzen durchgeführt: **AG News, MIND, SST‑2 und Enron Spam**.</sample>
    <sample id="229">Gabriella Skitalinskaya presents research on detecting and improving argumentative claims by leveraging revision histories from collaborative debate platforms such as Kialo. She first motivates the importance of precise phrasing in argumentative writing, using the example “Cell phones cause brain cancer” and its subsequent revisions to illustrate how claims are refined. The work introduces two core tasks: (1) **Suboptimal‑Claim Detection**—classifying whether a claim requires further revision, and (2) **Claim Improvement Suggestion**—identifying specific quality issues that should be addressed. Rather than hand‑crafting quality metrics, the authors learn from implicit revision patterns, treating earlier versions as suboptimal and final versions as optimal.

Four methodological challenges arise:  
1. **Representativity &amp; Reliability** – ensuring that final versions truly reflect optimal quality and are not merely overlooked.  
2. **Model Complexity &amp; Architecture** – selecting models sensitive to small textual changes while balancing pre‑training and fine‑tuning.  
3. **Contextual Dependencies** – determining which contextual signals (parent claim, debate topic, domain knowledge) influence quality judgments.  
4. **Topical &amp; User Bias** – mitigating noise from user or moderator preferences and controversial topics.

Experiments demonstrate that revision‑based data is effective for both tasks; modeling the distance between claim versions improves suboptimal claim detection, and the usefulness of contextual information varies with the task and the specific quality issue. The study concludes that revision histories provide a practical source for training systems that support argumentative writing.</sample>
    <sample id="231">NACHOS ist ein französisches medizinisches Korpus, das aus Web‑Crawling‑Daten zusammengestellt wurde. Es liefert große Mengen an Fachtexten (z. B. Artikel, Leitlinien, Patientendokumentationen), die für das Vor‑Training von Modellen wie DrBERT verwendet werden.</sample>
    <sample id="232">Der Referent heißt David Vilar.</sample>
    <sample id="233">**Abstract**  
Simultaneous speech translation (SimulST) translates spoken language into a target text in real time, yet current systems rely on specialised architectures, multi‑stage training, and separate models for each latency regime. We propose **EDAtt (Encoder‑Decoder Attention)**, a lightweight strategy that leverages the cross‑attention weights of an off‑the‑shelf offline speech‑translation model to decide when to emit partial translations. For each target token, if the summed attention over the last λ source frames falls below a threshold α, the token is output; otherwise the model waits for more audio. This simple rule regulates latency without retraining or architectural changes, allowing a single model to span a continuum of latency settings. Experiments on the English→German IWSLT data show that EDAtt outperforms classic strategies such as Wait‑k and Local Agreement, and matches or exceeds state‑of‑the‑art SimulST models in BLEU while achieving lower average lagging and computational‑aware lagging. The approach is easy to implement, fully reproducible, and we release the code, pretrained models, and simultaneous decoding outputs. EDAtt demonstrates that the attention mechanism can serve as an effective, model‑agnostic guide for real‑time translation.</sample>
    <sample id="234">Die Prompt‑Strategie hat einen großen Einfluss auf die Übersetzungsqualität von PaLM:

* **Zero / One‑Shot** – Die genaue Form des Prompts (z. B. Beispieltext, Sprachkennzeichnung, Satzstruktur) ist entscheidend. Schon unterschiedliche Formulierungen führen zu deutlich unterschiedlichen BLEURT‑Scores (mehr als 1 Punkt, bis zu 40 Punkte in Extremsituationen).  
* **Five‑Shot** – Bei fünf Beispielen wirkt sich die Form des Prompts kaum mehr aus. Stattdessen bestimmen die *Qualität* und *Relevanz* der Beispiele das Ergebnis. Gute, hochwertige Übersetzungen aus dem Dev‑Set liefern bessere Leistungen als zufällige Trainingsbeispiele.  

Kurz gesagt: Eine gut durchdachte Prompt‑Strategie (insbesondere bei wenigen Shots) ist entscheidend, während bei mehr Beispielen die Auswahl hochwertiger Beispiele den größten Einfluss hat.</sample>
    <sample id="235">Die Autoren sind an der **University of Washington** (Universität von Washington).</sample>
    <sample id="236">Die genauen Formulierungen der fünf Experten‑Anweisungen sind im bereitgestellten Text nicht angegeben. In der Präsentation wird lediglich erwähnt, dass jede Aufgabe mit fünf von Experten verfassten Instruktionen ausgestattet ist, aber die konkreten Anweisungen selbst werden nicht aufgeführt.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle anhand einer **diagnostischen Test‑Suite namens KITMUS** zu prüfen.  
KITMUS besteht aus Coreference‑Resolution‑Aufgaben, bei denen die Verfügbarkeit von zwei Arten von Wissen gezielt gesteuert wird:

| Wissen | Quelle | Beispiel‑Setup |
|--------|--------|----------------|
| **Background‑Knowledge** | Pre‑training | In der *Background‑Pretrain*‑Konfiguration soll das Modell bereits über generelles Hintergrundwissen verfügen. |
| | Inference | In der *Background‑Inference*‑Konfiguration ist das Hintergrundwissen **nur** im Eingabetext vorhanden. |
| **Entity‑Specific Knowledge** | Inference | Das Modell erhält immer die spezifische Information (z. B. „Servin ist ein Richter“) im Text. |

Die drei Szenarien sind:
1. **Background‑Pretrain** – Hintergrundwissen im Modell, Entitätswissen im Text.  
2. **Background‑Both** – Beide Wissensarten sowohl im Modell als auch im Text.  
3. **Background‑Inference** – Beide Wissensarten ausschließlich im Text (neu erfundene Berufe etc.).

Durch gezielte Manipulation dieser Einstellungen wird getestet, ob Modelle in der Lage sind, **Informationen aus unterschiedlichen Quellen zu integrieren** und die korrekte Pronomen‑Referenz zu bestimmen.</sample>
    <sample id="238">In dem Vortrag stellt Yebowen Hu aus der University of Central Florida die neue Benchmark‑Datenbank **MeetingBank** vor. Ziel ist es, hochwertige Zusammenfassungen von Städten‑Rats‑Versammlungen zu sammeln und damit Modelltraining und -evaluation für Meeting‑Summarisation zu ermöglichen. 

Zuerst werden Audiodateien über die Speechmatics‑API transkribiert. Anschließend wird die offizielle Website des jeweiligen Stadtrats geöffnet, die Meeting‑ID (ItemID) extrahiert und damit die dazugehörigen Minuten‑ und Zusammenfassungen abgeholt. Mit der ItemID werden auch die Zeitstempel der einzelnen Sprechersegmente bestimmt, sodass Transkripte mit exakten Zeitangaben synchronisiert werden können. So entsteht ein Corpus von 1 366 Sitzungen und fast 7 000 Einheiten.

Statistisch liefert MeetingBank Angaben zu Dauer, Token‑Anzahl, Sprecherzahl und Abstraktionsgrad (Coverage und Density) der Zusammenfassungen. Die meisten Summaries zeigen Coverage‑Werte zwischen 0,7 und 0,9, was auf einen starken Einsatz von wörtlicher Wiedergabe hinweist. Seattle und Boston weisen die höchsten Density‑Scores auf, während Denver niedrigere Werte hat, was auf intensiveres Editing schließen lässt.

Für die Evaluation wurden zehn Systeme getestet: extraktive Methoden (Oracle, Lead, LexRank, TextRank) und abstraktive Modelle (BART‑Large, Pegasus, Longformer, DialogLM, HMNet). DialogLM erzielte die besten ROUGE‑2‑Werte. GPT‑3 (Davinci‑003) lief zwar schlecht bei automatischen Metriken, zeigte jedoch in einer menschlichen Bewertung die höchste Gesamtpunktzahl, besonders in Fluency und Coherence, verfehlte jedoch Informativeness und Factuality.

Abschließend betont Hu, dass MeetingBank sowohl als Forschungswerkzeug als auch als Einblick in städtische Entscheidungsprozesse dient und die Community zur Nutzung und Weiterentwicklung einlädt.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar und ich werde einen kurzen Überblick über die Arbeit „Prompting PaLM for Translation: Assessing Strategies and Performance“ geben. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Billionen‑Parameter‑Large‑Language‑Model, das im Jahr 2022 vorgestellt wurde. Es wurde mit einer großen Textsammlung trainiert, die 780 Milliarden Tokens umfasst. Zum Zeitpunkt der Veröffentlichung erreichte es den Stand‑der‑Kunst bei Hunderten von NLP‑Aufgaben.

In dieser Arbeit präsentieren wir die erste systematische Untersuchung von Prompting mit großen Sprachmodellen für maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeiten solcher Modelle anhand der Best Practices der MT‑Community evaluiert. Das bedeutet, wir haben die neuesten Testsets verwendet, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Außerdem haben wir die Ergebnisse mit den State‑of‑the‑Art‑Systemen verglichen, also dem best‑performenden System der WMT‑Evaluation. Wir haben modernste neuronale MT‑Metriken eingesetzt und zusätzlich auch Expert*innen‑basierte, menschliche Evaluationsresultate gezeigt. Abschließend geben wir Empfehlungen für die Auswahl von Prompt‑Strategien.

Das Prompting hat einen großen Einfluss auf die Leistung der LLMs im Bereich Übersetzung. Das sehen wir an einem einfachen Experiment, bei dem wir One‑Shot‑Prompting verwendet haben und für jede zu übersetzende Sentence zwei verschiedene Prompts bereitgestellt haben. Für die Mehrheit der Sätze – 516 von 1 000 – zeigte sich ein Unterschied von mehr als einem BLEURT‑Punkt. In extremen Fällen kann der Unterschied bis zu 40 BLEURT‑Punkte betragen. Deshalb ist es wichtig, eine gute Prompt‑Strategie zu wählen.

In unseren Experimenten haben wir uns für eine 5‑Shot‑Strategie entschieden, bei der wir jede zu übersetzende Sentence mit dem jeweiligen Sprachlabel versehen. In dem Beispiel unten übersetzen wir von Deutsch nach Englisch. Die deutschen Ausgangssätze sind mit „German:“ gekennzeichnet, die englischen Übersetzungen mit „English:“. Wir stellten fest, dass die konkrete Form des Promptings bei mehr als wenigen kurzen Promptings keine große Auswirkung hat. Sie ist entscheidend bei Zero‑ und One‑Shot‑Prompting. Sobald wir jedoch zu 5‑Shot‑Prompting übergehen – wie in unserem Fall – gibt es nahezu keinen Unterschied bezüglich der Prompt‑Form. Die Beispiele tragen den größten Einfluss. Die Qualität der Beispiele ist wichtiger als die Ähnlichkeit zum Quelldokument. Deshalb ist es wichtig, Beispiele aus hochwertigen Übersetzungen zu wählen.

Insbesondere haben wir die Auswahl der Prompts aus den Trainingsdaten für die WMT‑Evaluationen auf den Dev‑Daten verglichen. Die Dev‑Daten sind viel kuratierter und von höherer Qualität als die Trainingsdaten, die stärker verrauscht sind. Die Ergebnisse zeigen, dass die Verwendung von Dev‑Daten zu einer besseren Leistung führt. Trotzdem haben spezialisierte State‑of‑the‑Art‑Systeme einen erheblichen Vorteil gegenüber den PaLM‑Übersetzungen. PaLM kommt jedoch ziemlich nah an ein kommerzielles System heran. In unserem Fall haben wir die Leistung gegen Google Translate verglichen.

Die Erkenntnisse aus der von uns durchgeführten menschlichen Evaluation mit dem MQM‑Framework zeigten, dass die Flüssigkeit von PaLM mit den State‑of‑the‑Art‑Systemen vergleichbar ist, aber der Hauptunterschied in der Genauigkeit liegt. Insbesondere treten die häufigsten Fehler in Form von Auslassungen (Omission‑Errors) auf. Das bedeutet, PaLM neigt dazu, eine besser klingende Übersetzung zu erzeugen, indem es manchmal Teile des Ausgangstextes weglässt. Im Gegensatz dazu ist die Kategorie „Style/Awkward“ für PaLM niedriger als bei den State‑of‑the‑Art‑Systemen – ein zusätzlicher Hinweis darauf, dass PaLM sehr flüssige Ausgaben liefert, aber noch Probleme mit der Genauigkeit hat.

Das war's für diesen kurzen Überblick. Für weitere Details lade ich euch herzlich ein, an der vollständigen Präsentation der Arbeit teilzunehmen. Vielen Dank!</sample>
    <sample id="240">**Übersetzung des Vortrags (in deutscher Sprache)**  

---

**Einleitung**  
Hallo, ich bin Dawei, Doktoranden an der Saarland University in Deutschland. In diesem Video möchte ich unsere jüngsten Arbeiten vorstellen: „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“. Dies ist ein gemeinsames Projekt mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.  

---

**Was ist schwache Überwachung (weak supervision) und was ist schwach-supervised learning?**  
In der schwachen Überwachung (weak supervision) werden die Daten nicht von Menschen annotiert. Stattdessen nutzen wir „schwache“ Label‑Quellen – zum Beispiel einfache Heuristiken, Wissensbasen oder kostengünstiges Crowdsourcing – um die Daten zu kennzeichnen (siehe Abbildung rechts). Im Vergleich zu menschlich erstellten Labels sind diese schwachen Anmerkungen deutlich günstiger, jedoch auch fehlerhaft: ein Teil der Annotationen ist inkorrekt.  

Wenn man neuronale Netzwerke direkt mit schwach annotierten Daten trainiert, neigen die Modelle dazu, das Label‑Rauschen zu memorieren und generalisieren schlecht. Schwach-supervised learning (WSL) befasst sich daher mit Trainingsalgorithmen, die robust gegenüber diesem Rauschen sind, sodass die Modelle auf sauberen Testdaten immer noch gute Leistungen erzielen.  

---

**Der häufige, aber problematische Anspruch in WSL**  
Viele neuere Arbeiten im Bereich WSL behaupten, dass sie Modelle ausschließlich mit schwach annotierten Daten trainieren und dabei hohe Leistungen auf sauberen Testsets erreichen. Technisch gesehen ist diese Aussage nicht falsch – allerdings gehen die Autoren davon aus, dass ein zusätzliches sauberes Validierungs‑Set vorhanden ist, um das Modell auszuwählen. Diese Annahme wird oft übersehen, obwohl sie eine zusätzliche manuelle Annotationsarbeit erfordert.  

Damit ergibt sich die zentrale Fragestellung unseres Projekts, die wir in drei Forschungsfragen formulieren:  

1. **Ist ein sauberes Validierungs­datensatz zwingend nötig für WSL, oder kann man auch ein verrauschtes Validierungs­set verwenden?**  
2. **Falls saubere Daten erforderlich sind, wie viele saubere Samples benötigen wir dann?**  
3. **Sollten die sauberen Samples ausschließlich zur Validierung eingesetzt werden, oder gibt es bessere Wege, sie zu nutzen?**  

---

**Unsere Erkenntnisse**  

1. **Saubere Validierungsdaten sind unverzichtbar**  
   Interessanterweise benötigen aktuelle WSL‑Methoden tatsächlich saubere Validierungs‑Samples. Fehlen diese, fällt die Performance drastisch – die Modelle generalisieren nicht über die ursprünglichen schwachen Labels hinaus. Das bedeutet, dass WSL‑Ansätze in der Praxis auch manuelle, saubere Annotationen benötigen, was oft unterschätzt wird.  

2. **Mehr saubere Samples = bessere Performance**  
   In der Abbildung links sehen wir, wie die Genauigkeit steigt, sobald die Anzahl der sauberen Validierungs‑Samples erhöht wird. Bereits etwa 20 saubere Samples pro Klasse reichen in der Regel, um eine hohe Leistung zu erzielen.  

3. **Direktes Fine‑Tuning mit sauberen Daten übertrifft WSL**  
   Die Abbildung rechts vergleicht zwei Vorgehensweisen:  
   - **Fine‑Tuning** (FT): Das Modell wird direkt auf den sauberen Daten weitertrainiert.  
   - **WSL‑Ansatz**: Das Modell nutzt die sauberen Daten ausschließlich für die Modell­auswahl.  
   Wenn nur 10 Samples pro Klasse zur Verfügung stehen, übertrifft das direkte Fine‑Tuning bereits die komplexeren WSL‑Methoden.  

   Außerdem zeigen die Ergebnisse, dass die angeblichen Performance‑Verbesserungen früherer WSL‑Methoden leicht erreicht werden können, indem man nach dem Basis‑Training die Modelle weiter auf den sauberen Validierungs‑Samples fine‑tune. In diesem Szenario performt ein einfaches Modell (FTw) genauso gut wie komplexere Ansätze wie COSINE.  

---

**Zusammenfassung und Empfehlungen**  

- **WSL‑Methoden erfordern in der Praxis saubere, manuell annotierte Samples** – die Kosten für deren Beschaffung sollten nicht ignoriert werden.  
- **Die behaupteten Leistungsgewinne von WSL sind oft überschätzt**; ein simples, kontinuierliches Fine‑Tuning auf sauberen Daten liefert vergleichbare oder bessere Ergebnisse, ohne den Aufwand der komplexen Algorithmen.  

**Konkrete Empfehlungen für zukünftige Arbeiten**  

1. **Transparente Modell‑Auswahlkriterien**: Geben Sie an, ob die Auswahl über ein sauberes Validierungs‑Set erfolgt.  
2. **Vergleich mit Few‑Shot‑Learning‑Baselines**: Da beide Ansätze auf sauberen Daten arbeiten, sollte ein Vergleich erfolgen.  
3. **Kontinuierliches Fine‑Tuning**: Dies ist ein einfacher, aber starkes Baseline‑Verfahren, das in zukünftigen WSL‑Studien berücksichtigt werden sollte.  

---

**Code‑Verfügbarkeit**  
Wir haben unseren Code open‑source gestellt. Sie finden ihn über den QR‑Code auf dieser Folie. Zögern Sie nicht, ihn auszuprobieren.  

Vielen Dank und viel Spaß auf der Konferenz!</sample>
    <sample id="241">**Zusammenfassung (≈200 Wörter)**  

Die vorgestellte Arbeit „Human‑in‑the‑loop Evaluation for Early Misinformation Detection: A Case Study of COVID‑19 Treatments“ adressiert zwei zentrale Schwächen bestehender automatisierter Fakten‑Check‑Systeme: unrealistische Evaluation und mangelnde Einbindung von menschlichen Moderatoren. Herkömmliche Ansätze nutzen retrospektiv zusammengestellte Datensätze, die oft schon nach einer Debunking‑Artikel existieren, und lassen menschliche Interaktion nur am Ende des Prozesses zu.  

Das vorgestellte Framework ist end‑to‑end und integriert Menschen in mehreren Phasen. Zunächst werden rohe Tweets über Keyword‑Filter gesichtet, anschließend ein T5‑Modell trainiert, um aus jedem Tweet die fragliche Behandlung („What is the mentioned COVID‑19 cure?“) als Claim zu extrahieren. Diese Claims werden nach Trend‑Wahrscheinlichkeit (Fisher‑Exact‑Test) gewichtet und an Moderatoren zur Verifikation übergeben.  

In der zweiten Phase wird ein BERT‑basiertes Stance‑Classifier eingesetzt, um die Haltung des Autors zu unautorisierten Behandlungen zu bestimmen. Unterstützende Tweets werden als potenzielle Policy‑Verstöße markiert und erneut an Moderatoren zur Bewertung (Likert‑Skala) geschickt.  

Evaluierungskriterien umfassen: (1) „frühe Erkennung“, definiert als Entdeckung eines Claims vor dem ersten Debunk‑Artikel; (2) Genauigkeit bei der Policy‑Verstoß‑Erkennung (≈65 %); (3) Effizienz, gemessen als 124,2 bestätigte Verstöße pro menschlicher Arbeitsstunde.  

Das System demonstriert, dass menschliches Feedback in Echtzeit die Früherkennung von Misinformationen verbessert und gleichzeitig die Belastung der Moderatoren quantifizierbar reduziert.</sample>
    <sample id="242">**Gängige Bewertungsmethoden für Dialogsysteme**  
- **Menschliche Evaluation**:  
  - **Vergleich zweier Dialoge** („Which conversation is better?“)  
  - **Likert‑Skalen**: Bewertung einzelner Turns oder des gesamten Dialogs (z. B. 1‑5‑Skala).  
- **Automatische Metriken** (im Kontext erwähnt, z. B. BLEU, ROUGE, etc., aber im Video fokussiert auf die oben genannten menschlichen Ansätze).</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="244">Das Hintergrundwissen, das im Servin‑und‑Kea‑Beispiel benötigt wird, lautet: **Richter treffen Entscheidungen in Gerichten** (oder allgemein, dass Richter in einer Gerichtsbarkeit arbeiten).</sample>
    <sample id="245">In dieser Arbeit wird ein zweistufiger Pipeline-Ansatz vorgestellt, um auf Amazon Mechanical Turk (MTurk) hoch­verlässige Annotatoren für die Summarisierung zu identifizieren. Zunächst werden im „Qualification Settings“-Schritt vorab Filter wie Standort, Anzahl abgearbeiteter HITs und HIT‑Approval‑Rate gesetzt. Im ersten Schritt („Qualification Task“) testen 200 Teilnehmende anhand von drei Dokumenten (inkl. Attention‑Check) die Fähigkeit, sechs Bewertungskriterien korrekt zu beurteilen. Nach der Schulung werden die Kandidaten in Gold, Silber, Bronze oder Block eingeteilt; nur Gold‑ und Silber­kandidaten (26 Personen, 13 %) gelangen in den nächsten Schritt. Im zweiten Schritt („Endurance Task“) werden 10 HITs mit einem Dokument und vier Zusammenfassungen auf die Belastbarkeit geprüft. Dabei bleiben 12 Annotatoren (4 Gold, 8 Silber, 6 %) übrig, die eine höhere Inter‑Annotator‑Agreement (Cohen‑Kappa, Krippendorff‑Alpha = 0.443) als Experten erzielen.  

Die „Reference‑Based Task“ evaluiert die tatsächliche Leistung: 30 HITs mit einem Referenz‑Summary und vier Kandidaten. Acht der zwölf Pipeline‑Annotatoren schließen alle HITs ab, wobei Krippendorff‑Alpha = 0.534 erreicht wird. Im Vergleich dazu liefern Basis‑MTurk‑Annotatoren (MACE‑Filter, Alpha = 0.380) bzw. CloudResearch‑Annotatoren (Alpha = 0.513) geringere bzw. ähnliche Qualitätswerte, jedoch mit höherem Ausschuss bzw. geringerer Akzeptanzrate.  

Eine Heat‑Map‑Analyse zeigt, dass Pipeline‑ und CloudResearch‑Annotatoren signifikant korrelieren, während Pipeline‑Annotatoren nicht notwendigerweise die Korrektheit trainieren. GPT‑Modelle korrelieren stark mit Experten.  

Zusammenfassend demonstriert die Pipeline, dass gezielte Vor‑Filterung zu einer kostengünstigen, hoch­aggregerierten Annotationsqualität führt und vergleichbar mit kostenintensiveren CloudResearch‑Ansätzen ist. Beschränkungen liegen in der englischen Sprache, dem spezifischen Task‑Design und fehlender Gewährleistung der Korrektheits­schulung. Zukünftige Arbeiten sollen die Rekrutierung auf weitere Sprachen, Plattformen und Aufgaben ausweiten. Dank an Google für die Fördermittel.</sample>
    <sample id="246">Ja – der Code und das Datenset sind öffentlich zugänglich.  
Sie finden beides im zugehörigen GitHub‑Repository, das im Paper verlinkt ist (z. B. `https://github.com/microsoft/KITMUS`).</sample>
    <sample id="247">FactKG – Fact Verification via Reasoning on Knowledge Graphs  
Die Autoren stellen den Datensatz **FactKG** vor, der die bislang fehlende Aufgabe der Faktenprüfung mit Wissensgraphen (KG) adressiert. Als Wissensbasis wird DBpedia genutzt und die Aussagen kommen in zwei Stilen vor: formell (written) und umgangssprachlich (colloquial). Jede Aussage ist mit einer der beiden Labels **SUPPORTED** bzw. **REFUTED** versehen. Das Ziel ist es, aus dem KG die relevanten Evidenztriple abzurufen und anschließend zu prüfen, ob die Aussage wahr oder falsch ist.

Der Datensatz umfasst fünf Typen logischer Inferenzschritte:  
1. **One‑hop** – eine einzelne Triple genügt.  
2. **Conjunction** – mehrere unabhängige One‑hop‑Triples müssen alle erfüllt sein.  
3. **Existence** – ein bestimmtes Subjekt‑Relation‑Objekt‑Muster muss existieren.  
4. **Multi‑hop** – Pfade über mehrere Knoten, z. B. von AIDAStella zu Papenburg.  
5. **Negation** – zusätzlich muss gezeigt werden, dass ein bestimmtes Triple nicht existiert.

Die Colloquial‑Claims wurden durch ein Sprachstil‑Transfer‑Modell (Kim et al.) erzeugt und zusätzlich mit Presupposition‑Templates ergänzt, um die Plausibilität realer Dialoge zu erhöhen.

Für die Evaluation wurden zwei Baseline‑Ansätze entwickelt:  
* **Claim‑Only** – prüft die Aussage ausschließlich anhand des Textes.  
* **GEAR** – nutzt den KG, um evidenzbasierte Argumente zu generieren.  

Die Ergebnisse zeigen, dass beide Modelle die Mehrheitsklasse (≈ 51 %) deutlich übertreffen und insbesondere GEAR die beste Leistung erzielt, was die Nützlichkeit von KG‑Evidence für die Faktenprüfung unterstreicht. Der Datensatz und die zugehörigen Code‑Releases stehen zur Verfügung.</sample>
    <sample id="248">No – the study aimed for **diversity**, not perfect balance.  
- The authors recruited more than 1 000 annotators from 87 countries and collected demographic attributes (gender, education, etc.).  
- They re‑annotated the data with many annotators per instance to capture a rich set of perspectives, but the distribution across each group (e.g., country, gender, non‑binary, etc.) was uneven.  
- Consequently, while the dataset is broadly heterogeneous, it is not uniformly balanced for every demographic category.</sample>
    <sample id="249">Sie haben die Sätze in der akzeptablen Domain nicht neu geschrieben, sondern gezielt **gestört**, wobei die zugrunde liegende grammatische Struktur erhalten blieb. Dazu wurden die Sätze mit „Rauschen“ versehen – etwa durch Wort‑ bzw. Token‑Swap, zufällige Ersetzungen oder ähnliche Störungen – sodass die semantische bzw. syntaktische Struktur unverändert blieb, aber die konkrete Wortfolge durcheinandergebracht wurde.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, die Qualität eines Chat‑Dialogs nicht nur mit einem einzigen Gesamtnotenwert zu beurteilen, sondern ihn in mehrere, unabhängige Attribute (z. B. Relevanz, Kohärenz, Halluzination, Empathie) aufzuteilen und jede dieser Dimensionen separat zu messen. Dadurch lassen sich Stärken und Schwächen der KI präziser und differenzierter erkennen.</sample>
    <sample id="251">Die Autoren sind der **University of Science and Technology of China** zugehörig.</sample>
    <sample id="252">**Zusammenfassung**

Die Arbeit „U‑CREAT: Unsupervised Case Retrieval using Events extrAcTion“ präsentiert zwei zentrale Beiträge zum Prior‑Case‑Retrieval (PCR) in der Rechtsinformatik. Zuerst wird der **IL‑PCR‑Datensatz** (Indian Legal Prior Case Retrieval) vorgestellt, ein Benchmark aus 7.070 indischen Gerichtsakten mit durchschnittlich 6,8 Zitaten pro Anfrage. Er übertrifft die frühere COLIEE‑21‑Sammlung durch größere Dokumente, umfangreichere Vokabulare und mehr Zitierungen.  

Der zweite Beitrag ist die **U‑CREAT‑Pipeline**, ein vollständig unüberwachtes Verfahren, das auf Ereigniserkennung basiert. Durch Abhängigkeitsparsen werden aus jedem Dokument Subjekt‑Verb‑Objekt‑Triplets (Ereignisse) extrahiert. Anschließend wird eine Interaktionsmatrix zwischen Anfrage‑ und Kandidatendokumenten erstellt, die auf gemeinsamen Ereignissen beruht. Verschiedene Retrieval‑Modelle (count‑basierte, transformer‑basierte und ereignis‑basierte) wurden evaluiert. Die einfachste ereignis‑basierte Variante („Atomic Events“) und die komplexere Variante mit getrennten Wort‑Einheiten („Non‑Atomic Events“) zeigten Verbesserungen gegenüber Baselines, jedoch mit geringerer Effektivität als das **Event‑Filtered‑Documents‑Modell**, das die Corpus‑Filterung auf Satz‑Ebene vornimmt, um nur Sätze mit gemeinsamen Ereignissen zu behalten.  

Diese Methode erreicht überlegene F1‑Scores, geringere Inferenzzeiten und generalisiert über indische und kanadische Rechtssysteme, ohne länderspezifische Anpassungen. Im COLIEE‑21‑Test übertrifft U‑CREAT sogar aktuelle überwachte Ansätze und stellt damit den aktuellen Stand der Technik im Dokumentretrieval dar.</sample>
    <sample id="253">**Kurzfassung (ca. 200 Wörter)**  

In der Präsentation von Mario Ezra Aragón wird das Modell *DisorBERT* vorgestellt, das gezielt auf die Erkennung von psychischen Erkrankungen in Social‑Media‑Posts abzielt. Der Ansatz nutzt eine doppelte Domänenanpassung: Zunächst wird ein generisches Sprachmodell (BERT) an die Sprache von Reddit angepasst, um die informelle und slanghafte Wortwahl besser zu erfassen. Anschließend erfolgt eine weitere Spezialisierung auf den Mental‑Health‑Domänenbereich, wobei ein lexikalisches Wissen zur gezielten Maskierung verwendet wird. Durch diese „guided masking“‑Methode wird das Modell dazu angeregt, besonders relevante, psychologisch belastete Wörter zu lernen und damit die semantische Tiefe im Kontext von psychopathologischen Inhalten zu erhöhen.  

Die Leistungsbewertung erfolgt anhand der eRisk‑Datensätze; *DisorBERT* erreicht ein ausgewogenes Verhältnis von Präzision und Recall und übertrifft dabei Baseline‑Modelle sowie MentalBERT. Eine Analyse der Masken‑Prädiktionen anhand von Beck‑Depression‑Inventar‑Sätzen zeigt, dass das Modell tendenziell negativ‑konnotierte und psychologisch relevante Wörter vorschlägt, im Vergleich zu generischeren Vokabeln von BERT.  

Schließlich werden Attention‑Visualisierungen genutzt, um für einen hochbewerteten Depressions‑Benutzer die wichtigsten Wörter („anxious“, „medication“) hervorzuheben. Abschließend werden zukünftige Forschungsrichtungen aufgezeigt, etwa die Einbindung weiterer Lexika und klinischer Daten.</sample>
    <sample id="254">**Kurzfassung (≈ 200 Wörter)**  

In der Arbeit *Uncertainty Guided Label Denoising for Document‑level Distant Relation Extraction* geht es um die Verbesserung der Relation‑Extraktion auf Dokumentebene unter Einsatz von distanzbasierten (distantly supervised, DS) Daten. Diese DS‑Korpora sind groß, jedoch stark verrauscht, weil die Relation‑Labels automatisch aus Wissensgraphen abgeleitet werden. Klassische Methoden nutzen Pseudo‑Labels, die allerdings falsche Relationen einführen und korrekte Relationen vernachlässigen können.  

Die Autoren stellen ein mehrstufiges Framework vor, das zunächst ein Vor‑Denoising‑DocRE‑Modell mit DS‑ und menschlich annotierten Daten trainiert, um Pseudo‑Labels zu generieren. Um die Zuverlässigkeit dieser Labels zu bewerten, wird Monte‑Carlo‑Dropout eingesetzt, wobei mehrere zufällige Vorwärtsdurchläufe die Modellunsicherheit erfassen. Der Schlüssel ist die *instance‑level* Unsicherheitsabschätzung, die für jede positive Relation eines Entity‑Paares ein eigenes Unsicherheitsmaß berechnet, anstatt nur die Gesamtsicherheit zu betrachten.  

Basierend auf dieser Unsicherheit werden **dynamische Klassengrenzen** festgelegt: Relationen mit Unsicherheitswerten über dem Schwellenwert ihrer Klasse werden verworfen, wodurch insbesondere seltene (long‑tail) Klassen besser geschützt werden. Anschließend wird ein **mehrphasiges Trainingsschema** implementiert, bei dem DS‑Daten iterativ neu gelabelt und das Modell erneut trainiert wird.  

Experimentell übertrifft das Verfahren alle starken Baselines auf zwei öffentlichen Dokument‑Relation‑Datasets und erzielt damit signifikante Leistungssteigerungen. Die Arbeit liefert somit vier Hauptbeiträge: ein Unsicherheits‑gesteuertes Label‑Denoising‑Framework, eine instance‑level Unsicherheitsabschätzung für überlappende Relationen, dynamische Klassengrenzen zur Bewältigung des Long‑Tail‑Problems und ein iteratives Re‑Label‑Training, das die Gesamtleistung des DocRE‑Modells erheblich verbessert.</sample>
    <sample id="255">Die Prompt‑Form ist hauptsächlich bei **Zero‑ und One‑Shot‑Prompting** entscheidend – dort beeinflusst sie die Qualität stark. Bei längeren Prompt‑Setups (z. B. 5‑Shot) hat die genaue Form fast keinen Einfluss, die gewählten Beispiele tragen den größten Beitrag zur Leistung.</sample>
    <sample id="257">**Welche Dialogmodelle wurden getestet?**  
Die Autoren haben vier aktuelle, „state‑of‑the‑art“-Chat‑Modelle evaluiert – deren spezifische Namen werden im bereitgestellten Text allerdings nicht genannt.</sample>
    <sample id="258">In this presentation, Chiang Cheng‑Han introduces a novel study that investigates whether large language models (LLMs) can replace human judges in evaluating natural‑language text quality. The authors argue that human evaluation, while standard, is unstable and hard to reproduce; LLMs, capable of following natural‑language instructions, might offer a more consistent alternative. To test this, they prompted several LLMs—including T0, InstructGPT (Curie and Davinci), and ChatGPT—to rate stories (both GPT‑2 generated and human‑written) on four attributes: grammar, coherence, likability, and relevance. The prompts were identical to those given to human evaluators (English teachers who scored the same stories), allowing a direct comparison of ratings.

Results showed that while smaller LLMs failed to distinguish human from GPT‑2 text, larger models—Davinci and ChatGPT—mirrored human preferences, consistently favoring human‑written stories. The study also explored how instruction wording, sampling strategies, and other factors influence LLM judgments, and discussed the cost–benefit trade‑offs relative to human evaluation. The authors concluded that certain LLMs can serve as viable alternatives for text evaluation and encourage interested readers to consult their full paper or visit their poster at ACL.</sample>
    <sample id="259">**Kurzfassung (ca. 200 Wörter)**  

Yusen Zhang vorstellt das neue Benchmark‑Set **XSemPLR**, das Cross‑Lingual Semantic Parsing (CLSP) auf 22 Sprachen, 9 Domänen, 5 Parsing‑Aufgaben und 8 Semantik‑Repräsentationen (z. B. SQL, Lambda‑Kalkül, FunQL) einheitlich adressiert. Der Fokus liegt auf der fehlenden Abdeckung von Chinesisch und der geringen Evaluierung mehrerer Repräsentationen in bestehenden CLSP‑Modellen.  

XSemPLR definiert sechs Trainings‑/Evaluations‑Settings:  
1. **Translate‑Test** – Quellsprache wird mit Google Translate in die Ziel‑/Ausgabesprache übersetzt, anschließend wird ein monolingualer Model trainiert.  
2. **Monolingual** – Source und Ziel sind identisch (z. B. Deutsch→Deutsch).  
3. **Monolingual Few‑Shot** – Training mit nur 10 % der Daten.  
4. **Multilingual** – Ein gemeinsames Modell für alle Sprachen.  
5. **Cross‑Lingual Zero‑Shot** – Training in einer Sprache, Test in einer anderen.  
6. **Cross‑Lingual Few‑Shot** – Training in einer Sprache plus ein paar Beispiele der Ziel‑Sprache.  

Für die Modellerauswertung wurden zwei Hauptarchitekturen untersucht:  
- **Encoder‑PTR** (z. B. XLM‑R + PTR, mBERT + PTR)  
- **Encoder‑Decoder** (z. B. mBART, mT5).  

Ergebnisse zeigen, dass Encoder‑Decoder in allen neun Datensätzen die höchste Genauigkeit erzielen. Multilingual‑Training verbessert die meisten Sprachen, führt aber bei Englisch zu einem „Curse of Multilinguality“ (Leistungsabfall). Cross‑Lingual‑Zero‑Shot weist einen großen Gap auf, der durch Few‑Shot‑Data nahezu geschlossen wird.  

Die Studie bestätigt, dass Englisch‑Pretraining die Few‑Shot‑Leistung in Zielsprachen stark steigert, während große multimodale Modelle wie Codex oder BLOOM für CLSP bislang unzureichend sind.  

XSemPLR stellt somit ein umfassendes, multilinguales Benchmark‑Set dar und liefert wertvolle Erkenntnisse für zukünftige CLSP‑Forschung.</sample>
    <sample id="260">Der Vortrag kommt von **Jingwei Yi** – es wird im Text nicht auf weitere Mitautoren hingewiesen. Somit ist die Arbeit an der Stelle mit **einzigem** Autor beteiligt.</sample>
    <sample id="261">Ein guter Planner sollte laut dem englischen Text **logische, vollständige Skripte erzeugen, die zugleich **angemessen** (reasonable) und **treu** (faithful) zu allen geforderten Einschränkungen (constraints) sind.</sample>
    <sample id="262">Ein Autor (Siyu Yuan).</sample>
    <sample id="263">**Kurzfassung (ca. 200 Wörter)**  

In‑context‑Learning (ICL) nutzt große Sprachmodelle, indem sie Beispiel‑Aussagen im Prompt enthalten. Seine Leistung ist jedoch stark von der Auswahl und Reihenfolge der Beispiele abhängig, was zu stabilitäts‑ und bias‑Problemen führt. In dieser Arbeit stellen wir zunächst eine Typologie von **Label‑Biases** vor:  

1. **Vanilla‑Label‑Bias** – das Modell hat eine Präferenz für bestimmte Labels, unabhängig vom Prompt.  
2. **Context‑Label‑Bias** – die im Prompt enthaltenen Beispiele beeinflussen die Vorhersagen.  
3. **Domain‑Label‑Bias** – ein bisher unerkanntes Phänomen, bei dem das Task‑Korpus selbst die Modell‑Präferenzen steuert.  

Wir zeigen experimentell, dass das zufällige Einführen von in‑Domain‑Wörtern (statt generischer Tokens wie „not available“) die Modell‑Biases stark verzerrt, während englische Zufallswörter weniger Einfluss haben. Bei Tasks mit hohem Domain‑Label‑Bias erreichen Modelle kaum Leistungen über dem Zufall, selbst mit bestehenden Calibration‑Methoden.  

Um diese Biases zu reduzieren, schlagen wir **Domain‑Context Calibration** vor. Dabei werden zufällige, in‑Domain‑Wörter als nahezu content‑freies Textsignal genutzt, um die inhärenten Label‑Präferenzen zu schätzen und die Vorhersagen zu korrigieren.  

Experimente mit GPT‑3 und kleineren Modellen über viele Datensätze hinweg zeigen, dass diese Methode die durchschnittliche ICL‑Performance signifikant erhöht – besonders bei stark domänenbasierten Aufgaben – und bessere Entscheidungsgrenzen liefert. Die Ergebnisse unterstreichen die Notwendigkeit einer systematischen Bias‑Analyse und einer angepassten Calibration für robuste in‑Context‑Learning‑Anwendungen.</sample>
    <sample id="264">Lin Wang aus der Zhejiang University stellt die Arbeit **“TAVT: Towards Transferable Audio‑Visual Text Generation”** vor. Ziel ist es, multimodale Text‑Generationsmodelle zu entwickeln, die sich mit wenig gelabelten Daten an neue Audio‑Visual‑Domänen anpassen können. Der Hauptschwierigkeit liegen die starken Domänenverschiebungen in visuellen Stilen, Kamerawinkeln und Audioenergie, die herkömmliche Modelle stark beeinträchtigen.

Die Lösung besteht aus drei modularen Komponenten:

1. **Audio‑Visual Meta‑Mapper** – mappt unterschiedliche visuelle Konzepte in einen einheitlichen audiellen Semantik‑Raum. Hier wird ein Clustering von Audio‑Clips (k‑Means) verwendet, um Audioklassen zu definieren, und lernbare „Visual‑Prefix‑Tokens“ (in Anlehnung an Prompt‑Learning) optimieren die Repräsentation, indem sie den visuellen Input mit den Audio‑Klassen abgleichen.

2. **Audio‑Visual Encoder &amp; Text‑Generator** – ein Transformer‑Modell, das eine Modulations­variable α einführt, die die jeweilige Beitrag‑Gewichtung der Modalitäten zu jedem Wort berechnet. Diese Gewichtung basiert auf der Relevanz der Cross‑Attention zu vorherigen Tokens.

3. **Dual Counterfactual Contrastive Learning (DCLL)** – erzeugt feinkörnige Supervision durch Gegenfaktische Beispiele und optimiert die visuell‑textuelle Ausrichtung direkt, ohne sich auf zufällig gewählte Negative zu verlassen.

Das Training folgt einem MAML‑ähnlichen Meta‑Learning‑Schema: für jede Iteration werden K‑1 Domänen als Support‑Set, die übrige als Query‑Set verwendet. Beim Meta‑Test wird ein neues Ziel‑Domänen‑Set mit wenigen Beispielen feinjustiert.

Experimentell wurden zwei Benchmarks auf MSVD und MSR‑VTT erstellt (Cross‑Dataset und Cross‑Domain). TAVT übertrifft bestehende RNN‑ und Transformer‑Baseline‑Modelle deutlich, insbesondere bei ressourcenschwachen Domänen wie „Kids“ und „Beauty“. Ablationsstudien zeigen, dass die Audio‑Features einen signifikanten Beitrag zur Leistungssteigerung leisten.</sample>
    <sample id="265">The speaker is **Vasudha**.</sample>
    <sample id="266">The transcript you provided does not mention any university affiliation for the author, Adam Przepiórkowski, or for any other authors.  No institutional affiliation is given in the text.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind **Omission‑Errors** – das Modell lässt häufig Teile des Ausgangstextes weg, wodurch die Genauigkeit leidet.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC‑Eval erzählen, einen neuen dimensionalen Ansatz zur Bewertung von conversational AI. Diese Arbeit wurde von der Emory NLP‑Lab, angeführt von Professor Jinho Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI durchgeführt.

Stellen Sie sich vor, Sie haben ein Dialogmodell entwickelt und möchten sehen, wie es im Vergleich zu den aktuellen State‑of‑the‑Art‑Modellen abschneidet. Die übliche Praxis ist, eine menschliche Bewertung einzusetzen, zum Beispiel indem man menschliche Gutachter fragt, welches von zwei Gesprächen besser ist, oder indem man Gespräche anhand einer Likert‑Skala bewertet. Diese Ansätze funktionieren gut, um einen ganzheitlichen Überblick über die Dialogqualität zu erhalten, aber die Qualität von Gesprächen umfasst viele Aspekte. Daher möchten Sie vielleicht mehrere Dimensionen der Chat‑Qualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.

Eine Möglichkeit wäre, menschliche Gutachter einfach mehrere Dimensionen der Dialogqualität bewerten zu lassen, zum Beispiel die Relevanz der Modellantworten mittels bestehender Vergleichs‑ oder Likert‑Skalen. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem wir explizit annotieren, ob oder nicht jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Auftreten irrelevanter Informationen oder das Widersprechen.

Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat oder kurz ABC‑Eval. Wir haben diese Methode entwickelt, um chat‑Modell‑Verhaltensweisen umfassend abzudecken, die laut aktueller Literatur die Chat‑Qualität beeinflussen. ABC‑Eval kann die Raten messen, mit denen Chat‑Modelle verschiedene thematische Fehler begehen. Beispielsweise misst ABC‑Eval die Anzahl der Runden, in denen ein Chat‑Modell seine Gesprächspartner ignoriert oder etwas Irrelevantes sagt, sich selbst oder den Partner widerspricht, falsche Fakten halluciniert oder gegen den allgemeinen Sinn verstoßen. Außerdem wird erfasst, wann das Modell Erfolg oder Misserfolg bei Empathie zeigt.

Um herauszufinden, welche Bewertung am effektivsten ist, haben wir vier state‑of‑the‑Art‑Chat‑Modelle ausgewählt und sie anhand von 100 menschlich‑Bot‑Konversationen pro Modell mit ABC‑Eval bewertet. Für den Vergleich haben wir diese Gespräche auch mit drei bestehenden Methoden bewertet: Likert‑Bewertungen auf Runden‑Ebene, Likert‑Bewertungen auf Gesprächs‑Ebene und pairwise Vergleiche auf Gesprächs‑Ebene. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten von Dialogen gesammelt, da dies die Standardpraxis für die mehrdimensionalen Bewertungen von Chat‑Modellen ist.

Aus der Analyse dieser Bewertungsergebnisse haben wir herausgefunden, dass ABC‑Eval‑Verhaltenslabel insgesamt zuverlässiger sind als Labels, die von den bestehenden Methoden gesammelt wurden, gemessen am Inter‑Annotator‑Agreement bei 100 doppelt gelabelten Gesprächen. Darüber hinaus sind ABC‑Eval‑Labels besser vorhersagbar für die Gesamtkonversation‑Qualität als die von bestehenden Methoden erzeugten Metriken, wie durch eine einfache lineare Regressionsanalyse gezeigt wird. Zum Beispiel zeigen Sie, wie das Messen des Anteils an Runden mit Selbst‑ und Partner‑Widersprüchen 5 % bzw. 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert‑Konsistenzscores nur 4 % oder weniger erklären.

Schließlich haben wir überprüft, ob jede Bewertungsmethode einen einzigartigen Aspekt der Chat‑Qualität erfasst, indem wir eine schrittweise lineare Regression durchgeführt haben. Sie können sehen, wie die Kombination aller ABC‑Eval‑Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, geht bei den meisten davon ein angemessener Anteil an Informationen über die Qualität verloren. Auf der anderen Seite erklärt die Kombination aller Runden‑Likert‑Metriken deutlich weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen bei.

Diese zuverlässigen, informativen und einzigartigen ABC‑Eval‑Metriken ermöglichen es uns, conversational AI mit einer höheren Auflösung zu bewerten als frühere Methoden es konnten. Sie können sehen, dass in den Ergebnissen unseres Experiments noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel haben die getesteten Bots in rund 20 % ihrer Antworten Verstöße gegen den allgemeinen Sinn, liefern in rund 15 % ihrer Antworten irrelevante Informationen und widersprechen sich selbst oder ihrem Partner in etwa 10 % der Zeit.

Mit dem raschen Fortschritt in diesem Bereich könnten viele dieser Fehlerquoten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, sinken. Das ist jedoch genau der Grund, warum wir zuverlässige und präzise Bewertungskriterien für den Vergleich von Modellen verfolgen sollten. Wir hoffen, dass ABC‑Eval von anderen in der Forschung als ein bedeutender Schritt in diese Richtung genutzt werden kann, und wir freuen uns darauf zu sehen, wie conversational AI in den kommenden Monaten und Jahren voranschreitet. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren gehören an der **Emory University** (das Emory NLP Lab unter Professor Jinho Choi).</sample>
    <sample id="271">In the paper **CFT** denotes **“Clean Fine‑Tuning”** – i.e. the simple baseline that takes a model trained on the weak labels and then continues fine‑tuning it on the small set of clean, manually annotated samples. This approach turns out to match or beat the more elaborate weak‑supervision methods discussed in the work.</sample>
    <sample id="272">Sieben Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="273">**Wann erfordert die Übersetzung Kontext? Eine datengetriebene, mehrsprachige Untersuchung**

Bei vielen Übersetzungen ist der Kontext entscheidend. Ein Beispiel: das Wort *“mole”* kann je nach vorheriger Satzstellung entweder einen Spion oder eine Muttermarkerei bezeichnen. Welche Bedeutung zutrifft, bestimmt die Übersetzung.  

Die Bewertung von Modellen bei solchen Fällen ist schwierig. Standardmetriken wie BLEU erfassen nur sehr wenige kontextabhängige Wörter. Zielgruppenbasierte Tests existieren, decken aber nur wenige Phänomene und Sprachen ab, weil sie stark von Domänenwissen und manueller Annotation abhängen.

**Fragestellungen**

1. Wann erfordert die Übersetzung Kontext?  
2. Wie gut gehen Modelle damit um?

**Methodik**

- **CXMI** (Conditional eXtra Mutual Information) misst, wie viel Information die Kontext­seite \(C\) über das Ziel \(Y\) liefert, gegeben die Quelle \(X\).  
- Wir erweitern CXMI zu **P‑CXMI** (Pointwise CXMI), um Kontexterfordernisse auf Satz- bzw. Wortebene zu quantifizieren. Hohe P‑CXMI‑Werte deuten auf Wörter hin, die Kontext brauchen.  

**Analyse**

Die Analyse basiert auf TED‑Talk‑Transkripten, übersetzt von Englisch in 14 Sprachen. Wir untersuchen drei Ebenen:

1. **POS‑Tags**: Hohe mittlere P‑CXMI‑Werte zeigen beispielsweise duale Pronomen im Arabischen (Englisch besitzt keine Dual‑Form) oder Verben, die im Ziel die richtige Konjugation benötigen.  
2. **Wortschatz**: Durchschnittliche P‑CXMI‑Werte für Wörter zielen auf Phänomene wie die konsistente Übersetzung von Eigennamen in Chinesisch oder die richtige Formalitäts­stufe.  
3. **Einzel‑Token**: Individuelle Token mit hoher P‑CXMI verdeutlichen strukturelle Probleme wie Ellipsenauflösung, die nicht allein aus dem Wort erkennbar sind.

**Benchmark‑Entwicklung**

Aus den fünf identifizierten Diskursphänomenen bauen wir einen automatischen Tagger namens **MuDA** (Multilingual Discourse‑Aware). MuDA taggt Wörter, die zu einem bestimmten Phänomen gehören.  

Mit MuDA markieren wir ein paralleles Korpus‑Dataset und evaluieren anschließend nur die kontextabhängigen Beispiele mit beliebigen Metriken.

**Ergebnisse**

| Metrik | Kontext‑agnostisches Modell | Kontext‑sensitives Modell |
|--------|------------------------------|---------------------------|
| BLEU   | Beste Leistung               | Schlechtere Leistung      |
| COMET  | Schlechtere Leistung         | Beste Leistung            |
| Wort‑F‑Measure | Gleichwertig | Gleichwertig |

Damit wird deutlich, dass reine Korpus‑metriken nicht aussagekräftig sind. Mit MuDA als gezielter Benchmark zeigen sich folgende Muster:

- Kontext‑sensitiv erstellte Modelle sind deutlich besser bei **Formalität** und **lexikalischer Kohäsion**.  
- Für **Ellipsen, Pronomen** und **Verbformen** gibt es kaum Verbesserungen gegenüber Modellen ohne Kontext.  

**Systemvergleich**

DeepL weist in der Regel eine höhere Genauigkeit bei dokumenten‑basierten Übersetzungen auf als Google Translate.

**Fazit**

Wir haben, basierend auf einer datengetriebenen Analyse über 14 Sprachpaare, identifiziert, wann Übersetzungen Kontext benötigen, und daraus ein Benchmark für dokumenten‑level‑Machine‑Translation abgeleitet. Dieses Tool ermöglicht es, gezielt Diskursphänomene zu evaluieren und zeigt, welche Modelle und Systeme für kontextabhängige Übersetzungen besonders geeignet sind.</sample>
    <sample id="274">Der Referent heißt **Yusen Zhang**.</sample>
    <sample id="276">**Kurzfassung (≈ 200 Wörter)**  

Das Projekt „IndicMT Eval“ erstellt einen Datensatz zur Meta‑Bewertung von maschinellen Übersetzungsmetriken für fünf indische Sprachen – Tamil, Malayalam (Dravidisch) und Hindi, Marathi, Gujarati (Indo‑Aryan). Aus dem FLORES‑Korpus werden 200 Sätze pro Sprache zufällig ausgewählt. Für jeden Satz werden mit sieben unterschiedlichen Übersetzungs‑APIs bzw. Modellen (Google, Bing, NLLB, IndicTrans, mT5, CVID, mBART) 1 400 Kandidaten generiert, insgesamt also 7 000 Einträge.  

Bilingual‑Experten annotieren die Übersetzungen mit dem MQM‑Framework: Fehler werden nach Typ (Genauigkeit, Fluency, Spezial) und Schweregrad markiert, zusätzlich wird eine Gesamtpunktzahl vergeben. Die Annotations‑Beispiele zeigen farbige Hervorhebungen für Fehler und deren Kategorien.  

Die Analyse der klassischen Metriken (BLEU, chrF, COMET, BERTScore, etc.) zeigt, dass overlap‑basierte Metriken meist schwach korrelieren, während embedding‑basierte Varianten (LabSE, MuRIL‑BERTScore) besser abschneiden. COMET‑Varianten erzielen die höchsten Korrelationen, jedoch mit stark eingeschränktem Wertebereich.  

Durch Fine‑Tuning von COMET mit den MQM‑Daten entsteht „IndicCOMET‑MQM“, das in drei von fünf Sprachen die Baselines übertrifft und generell höhere Korrelationen liefert. In Zero‑Shot‑Tests bleibt es besser als die Standard‑COMET‑Modelle.  

Robustheits‑Tests an den ACES‑Challenge‑Sets zeigen, dass IndicCOMET‑MQM (0,36) robuster ist als COMET (0,272). Der Datensatz wird öffentlich bereitgestellt.</sample>
    <sample id="277">Die vorgestellte Technik wird im Beitrag als **„Multiset Tagging and Latent Permutations“** bezeichnet.</sample>
    <sample id="278">Die Autoren beschreiben die „Marked‑Words“-Methode als ein Verfahren, das Wörter identifiziert, die ein markiertes (marginalisiertes) Identitäts‑Set von einem unmarkierten (dominerenden) Set unterscheiden. Dazu bestimmen sie zunächst, welche Gruppen als unmarkiert (z. B. weiße Männer) und welche als markiert (z. B. schwarze Frauen) gelten. Anschließend wenden sie das „Fightin’ Words“-Tool an, das gewichtete Log‑Odds‑Raten berechnet, um die Wörter mit der höchsten Differenz zwischen den beiden Gruppen zu ermitteln. Die resultierenden Wörter gelten als sprachlich „markiert“ und spiegeln stereotype, essentialisierte Darstellungen der markierten Gruppe wider.</sample>
    <sample id="279">Die Autoren sind an der University of Washington (Washington State University) angestellt.</sample>
    <sample id="280">**Kurzfassung der Präsentation (≈200 Wörter)**  

Shi Tao stellt „MultiEMO“, ein multimodales Verfahren zur Emotionserkennung in Gesprächen (ERC), vor. ERC zielt darauf ab, für jedes Sprachsegment die zugehörige Emotion (Text, Audio, Video) vorherzusagen. Der aktuelle Stand konzentriert sich meist auf Sprecher‑ und Kontextinformationen; die Komplementarität der Modalitäten wird kaum genutzt, Minderheitsklassen sind schlecht klassifiziert und semantisch ähnliche Emotionen werden schwer unterschieden.  

**Hauptbeiträge**  
1. **VisExtNet**: Eine visuelle Feature‑Extraktion, die ausschließlich Gesichts‑ und Interaktionsinformationen aus mehreren Frames extrahiert, ohne raumbezogene Hintergrunddaten, die irrelevant sind. Sie kombiniert MTCNN mit einem vortrainierten ResNet‑101 auf VGGFace2.  
2. **MultiAttn**: Ein multimodales Fusion‑Netz, das über drei modulare Sub‑Netze (Text, Audio, Video) arbeitet. Für jede Sub‑Netz wird eine bidirektionale Multi‑Head Cross‑Attention verwendet, wobei jede Modalität als Query, Key und Value in abwechselnder Reihenfolge agiert. Residual‑ und Layer‑Normalization sichern stabile Trainingsschritte.  
3. **Sample‑Weighted Focal Contrastive Loss (SWFC)**: Eine Verlustfunktion, die schwer zu klassifizierende Minderheits­klassen stärker gewichtet (Alpha) und fokussiert (Gamma), während sie gleichzeitig die interklassenische Trennung maximiert, um ähnliche Emotionen besser zu trennen.  

**Ergebnisse**  
MultiEMO übertrifft bestehende Methoden auf MELD und IEMOCAP, insbesondere bei seltenen und schwer zu differenzierenden Emotionen (siehe Tabellen 1 &amp; 2). Heat‑Map‑Visualisierungen zeigen die verbesserte Erkennung bei asynchronen Modaldarstellungen.  

**Limitierungen**  
- VisExtNet kann Sprecher nicht von unbeabsichtigten Personen unterscheiden.  
- SWFC erfordert große Batch‑Größen auf MELD.  
- Minderheitsklassen bleiben trotz Verbesserungen noch unterrepräsentiert.  

Insgesamt demonstriert die Arbeit einen signifikanten Fortschritt in multimodaler ERC, indem sie Modalkomplementarität explizit nutzt und die Klassifizierungs­schwierigkeiten bei seltenen Emotionen adressiert.</sample>
    <sample id="281">In this paper, the authors investigate when a translation needs surrounding context and how well current models handle those cases. They extend their earlier CXMI metric to a pointwise version (P‑CXMI) that can be computed at the sentence or word level. Using English‑to‑14‑language TED‑talk transcripts, they analyze words with high P‑CXMI to uncover patterns. Part‑of‑speech analysis reveals, for example, that Arabic dual pronouns and certain verb forms require contextual clues because English lacks these distinctions. Vocabulary‑level analysis shows that context is crucial for consistent proper‑noun translation in Chinese and for choosing appropriate formality levels across languages. Token‑level examination highlights phenomena such as ellipsis resolution that cannot be inferred from the word alone.

Building on these insights, the authors train a multilingual discourse‑aware tagger (MuDA) that automatically flags tokens belonging to five discourse phenomena (pronouns, verb form, formality, lexical cohesion, ellipsis). They then apply MuDA to parallel corpora and evaluate models only on the identified context‑dependent examples. While corpus‑level metrics (BLEU, COMET, word‑F1) give conflicting indications, the MuDA benchmark shows that context‑aware models outperform context‑agnostic ones on formality and lexical cohesion but not on ellipsis, pronouns, or verb form. Commercial systems are compared, revealing DeepL generally outperforms Google Translate for document‑level translation. The study thus provides a data‑driven taxonomy of context‑sensitive translation cases and a benchmark to guide future research on document‑level MT.</sample>
    <sample id="282">**Abstract**  
Wir stellen StoryTrans vor, ein Modell für die nichtparallele Autor:innen-Stil‑Transfer‑Aufgabe auf Erzähl‑ und Diskurs‑Ebene. Ziel ist es, lange Geschichten in einen Ziel‑Stil zu transformieren, wobei diskursive Strukturen und stilistische Präferenzen des Autors beibehalten werden. Hauptschwierigkeiten sind (1) die Imitation komplexer diskursiver Narrative und (2) das Übertragen von stilus‑spezifischem Inhalt, der stark thematisch konditioniert ist. StoryTrans löst diese Probleme, indem es aus Quelltexten diskursive Repräsentationen extrahiert und mit lernbaren Stil‑Embeddings kombiniert. Ein neues Training‑Objective reduziert stilistische Signale in den Repräsentationen, sodass Texte desselben Stils im latenten Raum näher liegen. Die Generation erfolgt in zwei Stufen: erst wird der Text mit stil‑spezifischen Schlüsselwörtern maskiert und stilistisch transformiert, anschließend werden die maskierten Tokens explizit in den Text eingefügt, um Inhaltserhaltung zu gewährleisten. Der erste Trainingsschritt nutzt Selbst‑Rekonstruktion, Stil‑Entanglement‑ und Satz‑Abhängigkeitsverluste; der zweite Schritt fokussiert auf das Unmaskieren. Auf neu zusammengestellten chinesischen und englischen Datensätzen überträgt StoryTrans erfolgreich Märchen und Alltagsgeschichten in verschiedene Autorenstile und übertrifft starke Baselines in Stilkontrolle und Inhalts‑Preservation, wie automatische und manuelle Bewertungen zeigen.</sample>
    <sample id="283">Hudson</sample>
    <sample id="284">Peng Tianshuo introduced FSUIE, a framework that replaces rigid span boundaries with a fuzzy, probabilistic notion. The model predicts a continuous probability distribution over start and end positions, addressing the ambiguity of gold spans. FSUIE employs a fuzzy span loss combining binary cross‑entropy and a KL divergence term to align predictions with a smoothed reference. On ACE2004/2005, ADE, and AST‑V2, FSUIE‑base surpasses a standard UIE baseline, achieving new state‑of‑the‑art results. Ablation studies show that FSA accelerates convergence and the fuzzy span loss (FSL) boosts extraction accuracy. The FSA mask G, applied at the top transformer layer, uses a learnable δ to adapt the attention span and decays linearly at the boundaries, allowing soft focus on peripheral tokens.</sample>
    <sample id="285">Der Vortrag von Mingqi Gao (Peking University) beleuchtet, dass sowohl automatisch generierte Zusammenfassungen als auch von Menschen erstellte Referenzzusammenfassungen häufig faktische Fehler enthalten. Zwei Lösungsansätze existieren: Einerseits werden faktische Ziele in Training und Inferenz integriert, andererseits wird ein unabhängiges Factual‑Error‑Correction‑Modell (FEC) entwickelt, das aus der Quelle und dem ursprünglichen Summary korrigierte Sätze erzeugt. Für Dialog‑Zusammenfassungen fehlt bislang ein systematischer Ansatz. Gao kritisiert die aktuelle Bewertung von FEC‑Modellen, die sich auf globale Factuality‑Metriken wie FactCC und DAE stützen. Diese Metriken sind vage und können ein FEC‑Modell dazu verleiten, das Original vollständig zu ersetzen, anstatt lediglich Fehler zu korrigieren. Um dies zu beheben, schlägt er die Einführung von manuellen Referenzkorrekturen vor, die minimale Substitutions‑, Einfüge‑ und Löschoperationen fordern. Außerdem entwickelt er eine neue Fehlertaxonomie, die zwischen inhaltlichen und formalen Fehlern unterscheidet. Aufbauend auf ERRANT erstellt er ein dreistufiges Evaluationsframework (Alignment, Classification, Comparison). Durch Experimente mit verschiedenen Trainingsmodi zeigt er, dass FEC‑Modelle, die mit Referenzen aus Dialog‑Datensätzen trainiert werden, die etablierten Metriken übertreffen. Kombinierte synthetische und menschliche Daten verbessern die Leistung weiter. Zudem bleiben Fehler wie Additions‑ und Attributfehler ungelöst. Abschließend betont er die Notwendigkeit eines präziseren Evaluation‑ und Trainingsparadigmas für FEC‑Modelle in der Dialog‑Zusammenfassung. Dies verdeutlicht, dass zukünftige Forschung neue, granularere Metriken und annotierte Datensätze erfordert.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="288">Zu syntaktischen Phänomenen können im Vortrag folgende Datensätze herangezogen werden:

- **BLiMP** (Benchmark for Linguistic Minimal Pairs)  
- **SyntaxGym** (eine Sammlung grammatikalischer Minimal‑Pair‑Aufgaben)  
- **CrowS‑pairs** (Minimal‑Pairs zu Stereotype‑ und Bias‑Konstruktionen)

Diese drei Ressourcen liefern die typischen Minimal‑Pair‑Phrasen, die im Minimal‑Pair‑Paradigm (MPP) zur Bewertung von Sprachmodellen verwendet werden.</sample>
    <sample id="290">**Abkürzungen der fünf Methoden, die für die erste Forschungsfrage evaluiert wurden**

| Kürzel | Vollständiger Name (Kurzbeschreibung) |
|--------|----------------------------------------|
| **FTw** | Fine‑Tuning ausschließlich auf weak‑labelled Daten (keine Clean‑Validation) |
| **FTw+cv** | Fine‑Tuning auf weak‑labelled Daten + Modell‑Selektion über eine Clean‑Validation‑Menge |
| **COSINE** | WSL‑Methode, die Cosinus‑Entfernungen zur Gewichtung von Pseudo‑Labels nutzt |
| **CT** | Co‑Training (klassisches Co‑Training mit zwei oder mehr Views) |
| **ST** | Self‑Training (Iteratives Training mit eigenen Pseudo‑Labels) |

Diese fünf Kurzformen wurden im Experiment zu Punkt 1 („Ist Clean‑Validation‑Daten für WSL nötig?“) verwendet.</sample>
    <sample id="291">Das Modell wird anhand von **vier Arten von Down‑stream‑Aufgaben** evaluiert:  

1. **Named Entity Recognition (NER)**  
2. **Text‑Classification**  
3. **Part‑of‑Speech (POS) Tagging**  
4. **Question‑Answering (QA)**  

Diese Aufgaben bilden die 11 spezifischen Tests, die für die französische medizinische Domäne verwendet wurden.</sample>
    <sample id="294">CamemBERT wurde ursprünglich auf dem **OSCAR‑Korpus** trainiert – etwa **138 GB französisches Textmaterial**, das aus einer Vielzahl öffentlicher Quellen zusammengestellt wurde.</sample>
    <sample id="295">Der Referent ist **Adam Przepiórkowski**.</sample>
    <sample id="296">Valerio Basile stellt in seinem Vortrag ein gemeinsames Projekt der Universität Turin und Amazon Alexa vor, das sich mit der Erkennung von Ironie in natürlicher Sprache befasst. Er betont, dass viele NLP‑Methoden auf überwachten Lern‑Ansätzen beruhen, die große, manuell annotierte Datensätze benötigen. Eine zentrale Annahme dieser Ansätze – die Existenz einer einzigen „Ground Truth“ – wird jedoch bei Ironie als problematisch angesehen, da Ironie ein hochgradig latentes, pragmatisches Phänomen ist.  

Um dieses Problem zu untersuchen, hat das Team den **EPIC**‑Korpus („English Perspectivist Irony Corpus“) erstellt. Dieser umfasst etwa 300 kurze Konversationen (jeweils zwei aufeinanderfolgende Texte) aus Reddit und Twitter, verteilt auf fünf englische Varietäten und über ein Jahr und ein halbes Jahr. Für jede Variante wurden rund 15 Annotatoren auf der Plattform Prolific rekrutiert (insgesamt 74), die jeweils 200 Konversationen bewerteten. Die Annotation war einfach: „Ist die Antwort ironisch?“ mit den Optionen „Ironic“ bzw. „Not ironic“. Zusätzliche Aufmerksamkeitschecks sorgten für Qualitätskontrolle.  

Durch Analyse der Inter‑Annotator‑Agreement‑Verteilungen (Violin‑Plots) zeigte sich, dass die Meinungsvielfalt je nach Geschlecht, Alter, Nationalität usw. stark variiert. Valerio entwickelte daraufhin **perspektive‑aware Modelle**: vortrainierte Sprachmodelle wurden auf jeweils annotator‑spezifische Datensplits fine‑tuned. Während die reinen Performance‑Kennzahlen keine klaren Trends aufwiesen, waren die perspektiven‑sensiblen Modelle insgesamt weniger unsicher und zeigten höhere Konfidenzwerte.  

Abschließend identifizierte das Team, dass Generationen, die einander nahe stehen, sowie geografische Unterschiede (z. B. UK vs. Ireland) besonders große Annotation‑Unterschiede erzeugen. Der Vortrag endet mit der Einladung zu Fragen und weiterführenden Diskussionen.</sample>
    <sample id="297">Die Arbeit „From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models“ untersucht, wie politischer Diskurs durch versteckte Sprachcodes – sogenannte Dogwhistles – manipulativ und diskriminierend wirken kann. Nach einer kurzen Einführung, die das Beispiel von Senator Josh Hawley nutzt, wird erläutert, dass Dogwhistles doppeldeutig sind: für Außenstehende klingt der Begriff harmlos, während er für die Zielgruppe (z. B. jüdische Menschen bei „Cosmopolitan“) eine rassistische oder antisemitische Botschaft transportiert. Die Autoren betonen, dass solche Codes schwer zu erkennen sind, weil das Zielpublikum oft nicht bewusst ist, dass der Begriff eine verborgene Bedeutung trägt.

Um Dogwhistles systematisch zu erfassen, wurde ein Glossar mit über 340 Begriffen erstellt, das Kategorien wie rassistisch, transphob und antisemitisch abdeckt. Jeder Eintrag enthält Register (informell/formell), Persona (z. B. antisemitisch) und Typ (Implikatur vs. reiner Signal). Anschließend wird ein Fallstudien-Ansatz an historischen US‑Parlamentsreden angewendet. Dabei lässt sich erkennen, dass die Verwendung von Dogwhistles seit den Bürgerrechtsbewegungen zugenommen hat und stark mit konservativen Reden verknüpft ist – ein Echo der „Southern Strategy“.

Weiterhin evaluieren die Autoren die Leistung von GPT‑3 bei der Erkennung und Erzeugung von Dogwhistles. Das Modell kann formelle Dogwhistles gut reproduzieren, kämpft aber mit informellen oder transphoben Varianten. Durch gezielte Prompt-Strategien verbessert sich die Erkennungsrate signifikant. Abschließend wird gezeigt, dass Dogwhistles in automatischen Toxicity‑Modellen (z. B. Prospective API) die Erkennung von Hassrede schwächen: Wenn Slang‑Begriffe durch Dogwhistles ersetzt werden, sinkt die toxic‑Score deutlich.

Zusammenfassend liefert die Studie ein detailliertes Typologiemodell, ein umfangreiches Glossar, historische Analysen und empirische Tests, die die Gefahr von Dogwhistles für die öffentliche Debatte und Online‑Moderation verdeutlichen.</sample>
    <sample id="298">Die Schlussfolgerung, dass die **zeitliche Verzögerung** (temporal drift) die Hauptursache für den Leistungsverlust ist, stützt sich auf zwei zentrale Befunde aus der Arbeit:

1. **Kein Anzeichen von adaptivem Over‑Fitting**  
   – In der Analyse der Performance‑Verbesserungen auf CoNLL‑2003 und CoNLL++ zeigte die Regressionslinie (rote Gerade) einen **Steigungswert größer als 1**.  
   – Das bedeutet, dass jede Einheit an Verbesserung auf dem ursprünglichen CoNLL‑2003‑Datensatz zu **mehr als einer Einheit** an Verbesserung auf dem neueren CoNLL++‑Datensatz führt.  
   – Bei adaptivem Over‑Fitting würde man hingegen einen **Abfall bzw. keine Steigerung** (Diminishing Returns) erwarten. Da dies nicht der Fall ist, kann man einen generellen Over‑Fitting‑Effekt ausschließen.

2. **Experimentelle Nachbildung mit neueren Daten**  
   – Die Autoren haben die Modelle mit **aktualisierten, jüngeren News‑Daten** weitertrainiert bzw. weiter‑pre‑trainiert.  
   – Bei zunehmender **zeitlichen Lücke zwischen Trainings‑ und Testdaten** (also je älter die ursprünglichen Daten relativ zum Testzeitpunkt) **sank die F1‑Performance**.  
   – Dieses klare, korrelierende Muster zwischen größerem Zeitunterschied und schlechterer Leistung spricht eindeutig dafür, dass **temporal drift** – die Veränderung von Sprachgebrauch, Themen und Schreibstilen im Laufe der Zeit – der dominierende Faktor für den Leistungsverlust ist.

Kurz gesagt: Die Kombination aus **(a) fehlendem Anzeichen für adaptives Over‑Fitting** und **(b) demonstrierter Leistungsverschlechterung, wenn die Modelle mit Daten aus immer weiter entfernten Zeiträumen weitertrainiert werden**, führte zum Schluss, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust ist.</sample>
    <sample id="299">Michalis Korakakis and Andreas Vlachos present a new training framework, **Minimax Training**, aimed at reducing shortcut reliance in Natural Language Inference (NLI) models. Current state‑of‑the‑art NLI systems often exploit spurious correlations—“shortcuts”—such as high word‑overlap in MNLI, which yields strong in‑distribution accuracy but poor robustness on out‑of‑distribution (OOD) adversarial sets. Existing shortcut‑mitigation methods typically rely on a pre‑trained auxiliary model that must be engineered to exploit shortcuts, requiring domain‑specific knowledge and additional computational cost. Moreover, the auxiliary’s predictions may not match the learner’s behavior, leading to suboptimal re‑weighting.

Minimax Training bypasses these issues by letting the learner and a lightweight auxiliary compete during training. The learner minimizes the NLI loss, while the auxiliary generates example weights that maximize the learner’s loss, thereby focusing learning on “hard” instances whose patterns conflict with dominant shortcuts. This dynamic re‑weighting is learned solely from the learner’s training trajectory, without assumptions about shortcut types. At test time, only the learner is needed.

Experiments on MNLI, FEVER, and QQP, and their OOD adversarial tests (HANS Symmetric, PAWS), show consistent gains in OOD performance over ERM and the best existing mitigation methods, while preserving in‑distribution accuracy. Additional analyses confirm transfer to larger models, synthetic shortcuts, and out‑of‑domain data, and reveal that a very small auxiliary suffices. The authors invite discussion at their poster session.</sample>
    <sample id="300">**Zusammenfassung**

In der Präsentation wird das neue Konzept „Interactive Dictation“ vorgestellt, bei dem Nutzer ihre Stimme sowohl zum Redigieren als auch zum Erstellen von Texten in einer einheitlichen, natürlichen Weise nutzen können. Anders als bei klassischen ASR‑Systemen, die nur Diktate verarbeiten, erlaubt Interactive Dictation das gleichzeitige Einfügen von Korrekturen und Befehlen ohne feste Trigger‑Wörter. Ein Beispiel zeigt, wie ein Sprecher ein Missverständnis im Text korrigiert („on Friday the 23rd“) und anschließend einen Befehl gibt („Replace 'the event'…“).

Die Arbeit definiert die Aufgabe in vier Schritten: (1) Roh‑Audio wird von einem ASR in einen Transkript übersetzt, (2) das Transkript wird in Diktat‑ und Befehls‑Segmenten aufgeteilt, (3) Befehle werden normalisiert und Fehler im ASR werden korrigiert, (4) die Folge von Diktaten und Befehlen wird ausgeführt, um den End‑Text zu erzeugen.  

Zur Datenerhebung wurde ein spezielles Interface entwickelt, mit dem Nutzer sowohl Diktate als auch Befehle in Echtzeit eingeben und gleichzeitig die Dokumenten­veränderungen visualisieren können. Daraus entsteht ein Datensatz, der die Interaktions‑Trajektorien enthält.  

Ein Baseline‑System trennt die vier Schritte in separate Modelle. Für die Segmentierung und Interpretation wurden T5‑ und GPT‑3‑Architekturen getestet. GPT‑3 liefert höhere Genauigkeit, ist aber langsamer; T5 ist effizienter und kann Programme zur Textmanipulation vorhersagen, ohne die Genauigkeit stark zu beeinträchtigen.  

Die vorgestellte Methode zeigt, dass Interactive Dictation realistisch umsetzbar ist, liefert jedoch noch viel Raum für Verbesserungen. Der Quellcode und weitere Details stehen im zugehörigen Paper und auf GitHub zur Verfügung.</sample>
    <sample id="302">We first only identify *which* output tokens belong to the input – that gives us an unordered multiset.  
To produce a valid logical form we must place those tokens in the correct left‑to‑right order. Hence a permutation step is necessary to turn the unordered set into a properly ordered output sequence.</sample>
    <sample id="303">Die Autoren nennen drei Hauptgründe für die Forderung nach mehr Transparenz bei Bias‑Mitigation‑Methoden:

1. **Unklare Herkunft der Stereotypen** – Ohne Einsicht in die angewandten Techniken lässt sich nicht bestimmen, ob die beobachteten „positiven“ Stereotype aus der Modellarchitektur, der Alignment‑Dynamik oder aus spezifischen Anti‑Stereotype‑Methoden resultieren.  
2. **Schwierigkeit der vergleichenden Analyse** – Transparenz ermöglicht es Forschern, verschiedene Modelle und Mitigationsansätze direkt zu vergleichen und festzustellen, welche Ansätze tatsächlich zu den unerwünschten Mustern beitragen.  
3. **Verantwortung und Rechenschaftspflicht** – Offenlegung der Bias‑Reduktionsverfahren schafft Vertrauen und erlaubt es der Community, die Wirksamkeit und mögliche unbeabsichtigte Nebenwirkungen der Methoden kritisch zu bewerten und zu verbessern.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind die Sätze eines Minimalpaars, die **grammatisch unkorrekt oder semantisch unzulässig** sind.  
In der Studie werden sie aus den gleichen oder unterschiedlichen Datensätzen (z. B. BLiMP, SyntaxGym, CrowS‑Pairs) generiert, oft mit zusätzlichen „unacceptable“ Präfixen, um die Sensitivität von Sprachmodellen gegenüber ungrammatischen Kontexten zu testen.</sample>
    <sample id="305">Hier ein kompakter Überblick über den Vortrag von Dawei und seinem Team aus dem Saarland University:

Der Vortrag beginnt mit einer Einführung in **Weakly Supervised Learning (WSL)**, bei dem Daten nicht manuell, sondern mit „schwachen“ Quellen (Heuristiken, Wissensbasen, Crowdsourcing) gelabelt werden. Diese Labels sind zwar kostengünstig, jedoch fehlerträchtig. Wenn man ein neuronales Netz direkt auf solchen Daten trainiert, tendiert es, den Rauschen zu „memorieren“ und schlecht zu generalisieren. Deshalb wurden spezialisierte Trainingsverfahren entwickelt, die robust gegen Label‑Noise sein sollen.

Die Autoren stellen drei zentrale Forschungsfragen:  
1) Ist ein **sauberes Validierungs‑Set** zwingend nötig?  
2) Wenn ja, wie viele saubere Beispiele reichen?  
3) Und wie kann man diese sauberen Daten optimal nutzen?

Ihre Ergebnisse zeigen, dass aktuelle WSL‑Methoden **ohne** saubere Validierungsdaten stark an Leistung verlieren – die Modelle generalisieren kaum über die ursprünglichen fehlerhaften Labels hinaus.  
Mit etwa **20 sauberen Beispielen pro Klasse** lässt sich die Leistung deutlich steigern. Doch sobald man in der Lage ist, die Modelle direkt auf den sauberen Daten weiter zu trainieren („Fine‑Tuning“), übertrifft dieses Vorgehen die komplexeren WSL‑Ansätze bereits bei nur 10 Beispielen pro Klasse.  

Daher betonen die Autoren, dass die bisherige Leistung von WSL‑Methoden überschätzt wird, weil oft ein sauberes Validierungs‑Set vorausgesetzt wird. Sie empfehlen, in zukünftiger Forschung: (a) die Auswahlkriterien explizit zu dokumentieren, (b) WSL mit Few‑Shot‑Learning‑Baselines zu vergleichen und (c) kontinuierliches Fine‑Tuning als starke, einfache Baseline zu berücksichtigen.  

Der Code zum Reproduzieren der Experimente ist öffentlich verfügbar.</sample>
    <sample id="306">In ihrer Präsentation stellen Sebastian Schuster und Najoung Kim die Forschungsfrage vor, inwieweit vortrainierte Sprachmodelle die Zustandsverfolgung von Entitäten in einem Diskurs beherrschen. Sie betonen, dass ein Agent, der einen Text versteht, wissen muss, welche Entitäten genannt werden und wie sich ihre Zustände im Verlauf verändern – z. B. dass Eier, Zucker und Mehl nach einer „Addieren‑in‑einen‑Schüssel“-Operation in der Schüssel landen und später Teil eines Teigs werden.  

Um die Fähigkeit zur Entitätentracking systematisch zu prüfen, entwickelten die Autoren einen Aufgabenaufbau mit mehreren Boxen und Objekten. Der Modell-Input beginnt mit einer Beschreibung des Ausgangszustands der Boxen. Anschließend werden Sequenzen von Operationen (z. B. „Verschiebe die Uhr von Box 3 nach Box 1“) angegeben. Das Modell muss danach die Endinhalte jeder Box vorhersagen. Die Aufgabe ist so gestaltet, dass Modelle nicht auf einfache Heuristiken (wie Wort‑assoziative Vorhersagen) zurückgreifen können; z. B. dürfen sie nicht nur den Ausgangszustand kopieren, sondern müssen Änderungen berücksichtigen.

Die Experimente umfassten Flan‑T5, GPT‑3 und GPT‑3.5 (2‑Shot In‑Context Learning). Auf der rechten Seite des Diagramms – wo der Endzustand der Box denselben Inhalt wie im Ausgang hat – erzielten alle Modelle hohe Genauigkeiten, weil sie einfach kopieren konnten. Auf der linken Seite, wo der Zustand geändert wurde, zeigte sich ein deutlicher Unterschied: Nur das Modell *text‑davinci‑003* (GPT‑3.5) erreichte Werte über einem zufälligen Baseline. Alle anderen Modelle, insbesondere jene ohne umfangreiche Code‑Prä‑Training, ließen sich unterhalb des Baselines vorhersagen.

Eine vertiefte Analyse der GPT‑Reihe ergab, dass die GPT‑3.5‑Modelle – die mit viel Code‑Training gefüttert wurden – signifikant bessere Tracking‑Leistungen zeigten als Modelle ohne Code‑Prä‑Training. Die Autoren vermuten, dass die strukturelle Art von Programmcode den Bedarf an Zustandsverfolgung schärft und somit die Fähigkeit verbessert.  

Weiterhin konnten kleinere Modelle wie T5‑base, wenn sie fein‑getuned wurden, die Aufgabe erlernen. Eine vergleichbare Leistung erreichte jedoch kein random‑initialisiertes Modell derselben Architektur, was erneut die Bedeutung des vortrainierten Wissens unterstreicht.  

Abschließend betonen die Forscher, dass noch unklar ist, ob die beobachteten Tracking‑Fähigkeiten über ihre spezifische Box‑/Objekt‑Umgebung hinaus generalisieren. Sie verweisen auf weitere Ergebnisse, insbesondere zu GPT‑4, in ihrem veröffentlichten Paper (arXiv).</sample>
    <sample id="307">The authors evaluated their models with the standard metrics that are normally used for each downstream task:

| Task | Primary metric(s) |
|------|-------------------|
| Named‑entity recognition (NER) | Precision, Recall, and micro‑averaged F1 |
| Text classification | Accuracy (and macro‑averaged F1 when appropriate) |
| Part‑of‑speech (POS) tagging | Accuracy and macro‑averaged F1 |
| Question answering (QA) | Exact Match (EM) and F1 score |

These metrics were reported for each of the 11 biomedical/clinical tasks to compare the seven pre‑training variants against the six baseline models.</sample>
    <sample id="308">**Zusammenfassung**

Jenny, eine erste‑Jahr‑PhD‑Studentin an der Carnegie Mellon University, stellt das Projekt *NLPositionality* vor, das die „Positionalität“ von NLP‑Datensätzen und Modellen untersucht – also die systematischen Leistungsunterschiede, die sich aus den demografischen und kulturellen Hintergründen der Entwickler und der annotierenden Personen ergeben. Als Beispiel wird die Sprachensensitivität von Toxicity‑APIs gezeigt: Während die Prospective API für englischsprachige Anwender wie Carl Jones gut funktioniert, erkennt sie für Nutzer wie Aditya Sharma, die mit indischsprachigen Begriffen vertraut sind, weniger toxische Inhalte.  

Zur Analyse rekonstruiert *NLPositionality* Datensätze, indem viele annotierende Personen mit detaillierten Demografie‑Informationen erneut bewerten. Anschließend werden diese Annotationen mit den Originallabels sowie den Vorhersagen von Modellen (Social Chemistry, Delphi, GPT‑4, Dynahate, Perspective API, Rewire API, Hate‑Roberta) mittels Pearson‑R‑Korrelationskoeffizienten verglichen. Diese Vorgehensweise differenziert sich von bisherigen Arbeiten, die sich nur auf annotatorische Diskrepanzen konzentrierten.  

Die Studie sammelte über 16 000 Annotationen von mehr als 1 000 Teilnehmenden aus 87 Ländern. Ergebnisse zeigen, dass Datensätze und Modelle besonders gut mit englischsprachigen und bildungsreicher Bevölkerung übereinstimmen, während nicht‑binäre Personen, sowie viele andere Gruppen, schlechter abgebildet werden.  

Zur Beseitigung von Design‑Bias empfiehlt das Team: (1) lückenlose Dokumentation aller Designentscheidungen, (2) Anwendung von perspectivistischen Ansätzen in der Forschung und (3) Aufbau spezialisierter Datensätze und Modelle für konkrete Gemeinschaften (z. B. Masakhani‑Initiative). Diese Maßnahmen sollen die Inklusivität von NLP‑Technologien stärken.</sample>
    <sample id="309">The agreement between annotators was quantified using an **inter‑annotator agreement metric** (e.g., Cohen’s κ).</sample>
    <sample id="310">Die Domain war **Wikipedia** (komplett unzusammenhängende Sätze aus Wikipedia wurden hinzugefügt).</sample>
    <sample id="311">Die Universität, an der die Autoren zu arbeiten scheinen, wird im vorliegenden Text nicht explizit genannt.</sample>
    <sample id="312">MultiInstruct is the **first large‑scale, multi‑modal instruction‑tuning benchmark**.  
Unlike most existing benchmarks, which are language‑only, it contains 62 diverse tasks drawn from 21 open‑source vision‑language datasets, grouped into 10 broad categories. Each task comes with **five expert‑written instruction templates**, allowing studies of instruction variability and model “sensitivity.” The data are formatted into a unified sequence‑to‑sequence token space (text, images, bounding boxes) so that a single pre‑trained model (e.g., OFA) can be fine‑tuned end‑to‑end. Additionally, MultiInstruct introduces a new evaluation metric—**sensitivity**—to measure consistency across instruction wording. These features make it uniquely suited to assess and improve multi‑modal zero‑shot learning via instruction tuning.</sample>
    <sample id="313">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="314">Binary coordination is the construction in which exactly two conjuncts are linked by a coordinating conjunction (e.g., *and, or, but*).  It forms a binary coordination structure in which the two elements are the only members of the coordinate phrase and are typically treated symmetrically (each conjunct can be thought of as a “head” of the coordination, or each can be seen as a dependent of the conjunction, depending on the theoretical framework).</sample>
    <sample id="315">Die Präsentation nennt keine explizite Angabe zur durchschnittlichen Länge der Prompts.  Das Beispiel‑Prompt „Imagine you are an Asian woman. Describe yourself.“ ist jedoch sehr kurz (etwa 8–10 Wörter), sodass die Prompts insgesamt als knapp beschrieben werden.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass ein kleineres T5‑Modell, wenn es mit dem CoScript‑Datensatz fein‑abgestimmt wird, Script‑Pläne von höherer Qualität erzeugt als die meisten großen Sprachmodelle. Damit demonstriert die Studie, dass spezialisierte, kleinere Modelle bei richtiger Datenbasis sogar die Leistung der großen Modelle übertreffen können.</sample>
    <sample id="317">**Zusammenfassung**

Peng Li von der Fudan‑University stellt in seiner Präsentation „CodeIE: Large Code Generation Models are Better Few‑Shot Information Extractors“ ein neues Vorgehen zur Information Extraction (IE) vor. Traditionell werden Aufgaben wie Named‑Entity‑Recognition (NER) und Relation‑Extraction (RE) mit text‑to‑text Modellen wie T5 oder GPT‑3 gelöst. Diese Modelle erzeugen strukturierte Ausgaben, indem sie die Ausgabe linearisiert und anschließend mit speziellen Decodierungsstrategien abgleichen. Dadurch entsteht ein Mismatch zwischen dem während des Pre‑trainings erlernten Text‑output und dem bei der Inferenz benötigten strukturierten Format, was große Mengen an annotierten Daten und aufwändige Decodierung erfordert.

CodeIE überwindet dieses Problem, indem es die IE‑Aufgabe in ein Code‑Generierungsproblem umformt und dafür große Code‑Sprachmodelle (z. B. Codex) nutzt. Für NER wird eine Funktion definiert, die die Eingabetext‑Zeichenkette akzeptiert, Kommentare zur Extraktion einfügt und die erkannten Entitäten in einer Liste speichert. Für RE werden analog strukturierte Prompts erstellt. Durch wenige‑Shot‑In‑Context‑Demonstrationen liefert das Modell Code, der die Entitäten bzw. Relationen korrekt extrahiert und in der vorgesehenen Struktur zurückgibt.

Die Autoren evaluieren CodeIE an drei NER‑ und vier RE‑Datasets. Sie vergleichen die Leistung von T5, UIE, GPT‑3 (text‑Style‑Prompt) und Codex (code‑Style‑Prompt). In einem‑bis‑few‑Shot‑Setting übertrifft Codex mit Code‑Prompts die Baseline‑Modelle deutlich, insbesondere in Recall. Analysen zeigen, dass die Perplexität von Code‑Samples niedriger ist als bei text‑Samples, was die bessere Alignment‑Übereinstimmung erklärt. Zudem treten bei Codex seltene strukturelle Fehler auf, während GPT‑3 häufig Labels aus dem vordefinierten Set entfernt oder neue, unerwartete Labels erzeugt. Insgesamt demonstriert die Studie, dass die Kombination aus Code‑Generation‑Modellen und code‑basierten Prompts ein robustes, wenig‑shot‑geeignetes Framework für strukturierte Informationsgewinnung darstellt.</sample>
    <sample id="318">**DrBERT: Ein robuster vortrainierter Modell für die französische Sprache im biomedizinischen und klinischen Bereich**

Ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten zu „DrBERT: A Robust Pre‑trained Model in French for Biomedical and Clinical Domains“ vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend präsentieren wir die Hauptbeiträge unseres Artikels. Wir stellen das erste biomedizinische Modell in Französisch vor, genannt DrBERT, das auf RoBERTa basiert und auf NACHOS trainiert wurde – einem Datensatz aus medizinisch gecrawlten Webdaten. Wir führen außerdem einen Vergleich von Modellen mit unterschiedlichen Pre‑training‑Einstellungen und Datenquellen durch. Danach zeigen wir unsere Ergebnisse zu 11 biomedizinischen und klinischen Down‑stream‑Aufgaben in Französisch. Abschließend schließen wir die Experimente ab und geben weitere Details dazu, wie Sie auf diese Modelle zugreifen können.

Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze geworden, um Natural‑Language‑Processing‑Aufgaben zu lösen, und bietet enorme Leistungssteigerungen gegenüber historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder anderen. Seitdem wurde dieses Modell auf viele weitere Sprachen adaptiert, beispielsweise auf Französisch mit CamemBERT, und auch auf Domänen wie die Biomedizin mit PubMedBERT und BioBERT sowie auf klinische Anwendungen mit ClinicalBERT – jedoch überwiegend in Englisch. Spezialisierte Modelle für andere Sprachen sind knapp und basieren häufig auf kontinuierlichem Pre‑training, weil es an in‑Domain‑Daten mangelt. Französisch hatte bis jetzt kein Open‑Source‑Modell für die Biomedizin. Wir stellen uns daher die Frage, welche Datenquellen am besten geeignet sind – und ob gecrawlte Daten eine gute Alternative zu klinischen Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datenlager der Universität Nantes basiert.

Des Weiteren fragen wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französischdaten zu trainieren – 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainieren und vergleichen wir vier Modelle von Grund auf:  
1. Die erste Version von DrBERT, mit 7 GB NACHOS;  
2. Eine zweite Version von DrBERT, mit 4 GB NACHOS;  
3. Die erste Version von ChuBERT, ein klinisches Modell mit 4 GB Sätzen aus klinischen Notizen;  
4. Eine vierte Version von ChuBERT, die 4 GB NACHOS und 4 GB klinische Notizen mischt.

Zusätzlich führen wir drei Modelle durch kontinuierliches Pre‑training durch, um die Auswirkungen der Pre‑training‑Strategie zu analysieren:  
- Ein Modell, das auf den Gewichten von CamemBERT basiert und auf einem 4 GB‑Set von NACHOS trainiert wird;  
- Ein weiteres Modell, ebenfalls auf CamemBERT, aber auf 4 GB klinischer Notizen trainiert;  
- Und ein Modell, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf 4 GB NACHOS trainiert wird.

Insgesamt haben wir sieben Modelle. Um diese sieben Modelle zu evaluieren, sammeln wir Daten für öffentliche und private Down‑stream‑Aufgaben wie Named‑Entity‑Recognition, Klassifikation, Part‑of‑Speech‑Tagging und Frage‑Antwort. Diese Modelle werden mit sechs Basismodellen verglichen: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT. Die Evaluation zeigt, dass Modelle am besten auf Aufgaben performen, deren Daten der gleichen Natur entsprechen wie die, auf denen das Modell trainiert wurde. Gleichzeitig lässt sich beobachten, dass Daten aus heterogenen Quellen vielseitiger sind. Wir sehen außerdem, dass mehr Daten zu besseren Leistungen führen. Insgesamt scheint das Training von Grund auf höhere Leistungen auf den meisten Aufgaben zu erzielen. Unsere Experimente zum kontrollierten Pre‑training mit den Gewichten und der Tokenisierung von CamemBERT, trainiert auf einem 4 GB‑Subset von NACHOS, zeigten jedoch vergleichbare Ergebnisse wie DrBERT 4 GB von Grund auf. Das Modell, das CamemBERT‑Gewichte und -Tokenizer verwendet, weist hingegen Stabilitätsprobleme auf.

Abschließend zeigen unsere Modelle für die Biomedizin bessere Leistungen bei neun von elf Down‑stream‑Aufgaben und übertreffen allgemein das generische Modell CamemBERT. Wir beobachten zudem, dass stärker spezialisierte Daten besser sind, aber nicht gut skalieren. Alle von NACHOS vortrainierten Modelle sind frei verfügbar auf Hugging Face, unter der MIT‑Lizenz, und sämtliche Trainingsskripte sind in unserem GitHub‑Repository zu finden. Vielen Dank für Ihre Aufmerksamkeit, und wir freuen uns auf einen Austausch bei der Poster‑Session in Toronto.</sample>
    <sample id="319">**Lernstrategien, die in der Arbeit untersucht werden:**

| Kategorie | Vorgehen | Beispielmodelle |
|-----------|----------|-----------------|
| **From‑scratch‑Pretraining** | Modell von Grund auf mit französischen medizinischen Daten trainieren | DrBERT‑4 GB, DrBERT‑7 GB, ChuBERT‑4 GB (klinische Daten), ChuBERT‑mix (NACHOS + klinische Daten) |
| **Continual (Fine‑Tuning) Pretraining** | Vortrainierte Gewichte eines bestehenden Modells verwenden und weiter pretrainen | CamemBERT‑4 GB (NACHOS), CamemBERT‑4 GB (klinische Daten), PubMedBERT‑4 GB (NACHOS) |
| **Daten‑Abhängig** | Effekt von Datenmenge und -quelle auf die Leistung untersuchen | 4 GB vs. 7 GB, NACHOS vs. klinische Notizen vs. Mischung |

Kurz: Die Studie vergleicht **From‑scratch‑Pretraining** gegen **Continual Pretraining** (mit CamemBERT‑ und PubMedBERT‑Gewichten) und analysiert die Auswirkungen von Datenmenge und -typ auf die Modellleistung.</sample>
    <sample id="320">Der Faktor liegt **größer als 1** – die Steigung der Regressionsgeraden ist &gt; 1, was bedeutet, dass jede Verbesserung bei CoNLL‑2003 zu einer noch größeren Verbesserung bei CoNLL++ führt. Damit gibt es keine abnehmenden Renditen und somit keine signifikante Überanpassung durch wiederholte Testnutzung.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand mehrerer Kriterien beurteilt:

1. **Analyse der Transformationen** – Die Parallel‑Satzpaare wurden hinsichtlich ihrer Vereinfachungs­maßnahmen (lexikalische Substitution, Klausel‑Löschung, Um‑Reihenfolge, Wort‑Einfügung, Rephrasierung) untersucht. Dabei wurden die Häufigkeit dieser Transformationen sowie ihr Einfluss auf die Gesamtsimplifizierung gemessen.

2. **Strukturelle und lexikalische Messgrößen** – Für jedes Satzpaar wurden Metriken zur Wort­- und Satz­struktur vereinfacht (z. B. Satzlänge, Wort­häufigkeit, Lesbarkeits‑Indizes) ermittelt, um den Grad der Vereinfachung quantitativ zu erfassen.

3. **Standard‑Evaluationsmetriken** – Die automatisch generierten Vereinfachungen wurden mit etablierten NLP‑Metriken bewertet (z. B. BLEU, SARI, FK‑Gelesen‑Level), um die Übereinstimmung mit den manuellen Gold‑Sätzen sowie die Verbesserung gegenüber Baselines zu quantifizieren.

Diese kombinierten Analysen lieferten einen umfassenden Überblick über die Vereinfachungsqualität in DEPLAIN.</sample>
    <sample id="322">Enrico wird auf der ACL 23 einen Vortrag mit dem Titel „What does a Text Classifier Learn about Morality?“ halten. Er beginnt mit einer Definition von Moral als innerer Kompass, der zwischen richtig und falsch unterscheidet, und betont, dass Moral stark subjektiv und pluralistisch ist. Einfache Skalen, die ein Urteil von „immoral“ bis „moral“ abbilden, würden dieser Komplexität nicht gerecht werden, da Menschen ein und dieselbe Aussage unterschiedlich bewerten – etwa zu Abtreibung oder LGBTQ‑Rechten. Um dieser Mehrdeutigkeit Rechnung zu tragen, verweist Enrico auf die Moral‑Foundation‑Theory (MFT), die fünf grundlegende moralische Dimensionen (Fairness, Autorität, Fürsorge, Loyalität, Freiheit) beschreibt und erklärt, dass Individuen diese unterschiedlich gewichten.  

MFT wurde bereits in NLP eingesetzt und gezeigt, dass Sprachmodelle Moral erkennen können. Enrico möchte nun untersuchen, was die Modelle tatsächlich lernen. Dazu nutzt er den „Moral Foundation Twitter Corpus“, der 35 000 Tweets aus sieben unterschiedlichen Domänen (z. B. #AllLivesMatter, #BlackLivesMatter) enthält. Mit erklärbaren KI‑Methoden analysiert er, ob Modelle die Domänen‑spezifische Moralexpression unterscheiden können. Ein Beispiel: Im ALM‑Kontext wird „Subversion“ (Rebellion gegen Autorität) negativ bewertet, während im BLM‑Kontext Subversion teilweise positiv konnotiert ist. Die Ergebnisse zeigen, dass Modelle solche Nuancen erkennen, warnen jedoch davor, ein einziges Modell für alle Domänen zu verwenden, da sonst Missverständnisse gefährlich werden könnten. Enrico lädt die Zuhörer ein, bei der Konferenz in Toronto vorbeizuschauen.</sample>
    <sample id="323">**Zusammenfassung**

Commonsense‑QA ist ein anspruchsvolles Problem, bei dem Maschinen Fragen beantworten müssen, die auf allgemeinem Wissen basieren. Traditionelle Ansatz­werke nutzen sowohl Sprachmodelle als auch Wissensdatenbanken (KB). Sie ermitteln relevante Fakten durch Entitäts‑Matching, bauen daraus einen Untergraphen und führen anschließend eine getrennte Codierung von Text und Graph durch. Diese Verfahren führen häufig zu Stör‑Entitäten (z. B. „Top“, „Bank“, „Cat“) und vernachlässigen die semantische Beziehung zwischen Entitäten sowie die Interaktion zwischen Text‑ und Graph‑Modalen.

Wir stellen **DHLK** vor, ein Verfahren, das diese Schwächen adressiert. Zunächst wird ein heterogenes Wissensgraph (HKG) aus ConceptNet, WordNet und Wiktionary konstruiert. Durch einen zweistufigen Pruning‑Ansatz und Knowledge‑Representation‑Learning (KRL) werden der Graphstruktur und die Embeddings optimiert. Entitäten werden erst anhand eines Wörterbuch‑Vokabulars gesäubert, anschließend werden Paraphrasen aus WordNet/Wiktionary ergänzt, um den Graphen zu erweitern. RoBERTa sowie Masked Self‑Attention codieren dann Text und Entitäten gemeinsam; schwach relevante Knoten (z. B. „wood“) werden dynamisch entfernt. Für die Embeddings wird TransE eingesetzt, während die Interaktion im Graphen durch Relation‑Mask‑Self‑Attention (RMSA) modelliert wird – eine Erweiterung von RGAT. Nach L‑Schichten RMSA wird der Graph‑Embedding mittels Max‑Pooling aus den Schlüssel‑Entitäten gewonnen.

Schließlich werden Pfadinformationen aus dem HKG in den QA‑Kontext integriert, und ein MLP kombiniert Graph‑, Pfad‑ und Kontext‑Embeddings zur Vorhersage der Antwort. Experimente auf CommonsenseQA und OpenBookQA (unter Einsatz von ConceptNet, WordNet und Wiktionary) zeigen, dass DHLK die aktuellen Benchmarks übertrifft und damit einen vielversprechenden Ansatz für Commonsense‑QA darstellt.</sample>
    <sample id="324">Ja. Unsere Studie zeigt, dass Sprachmodelle unterschiedliche politische Vorurteile aufweisen. Durch gezielte Prompts (z. B. „Political Conference Test“) lassen sich Modelle eindeutig in die vier klassischen politischen Quadranten (liberal, moderat‑links, konservativ, right‑wing) einordnen. GPT‑4 gilt etwa als am liberalsten, während BART‑ und ähnliche Modelle tendenziell konservativer sind. Weiterführende Pre‑Training auf parteiisch gefilterten Corpora verschiebt die Ideologiekurven entsprechend – ein RoBERTa‑Checkpoint, der auf einem linksliberalen Reddit‑Corpus weitertrainiert wurde, zeigt einen deutlichen liberalen Schub. Diese Unterschiede propagieren sich auch in downstream‑Aufgaben: linksliberale Modelle erkennen besser Hassrede gegen Minderheiten, während rechtsextreme Modelle besser Hassrede gegen dominante Gruppen detektieren. Kurz gesagt: Sprachmodelle tragen tatsächlich unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann und heute möchte ich Ihnen einen kurzen Überblick über unser Paper „Compositional Generalization without Trees using Multiset Tagging and Latent Permutations“ geben. Die Arbeit stammt aus einer gemeinsamen Zusammenarbeit mit meinen Betreuern Alexander Koller und Ivan Titov.

**Compositional Generalization** kann als die Fähigkeit eines Lernsystems verstanden werden, tiefer verschachtelte Strukturen und neu zusammengesetzte Phrasen zu handhaben, die im Training zwar einzeln, aber nicht in Kombination vorkamen. In der semantischen Parsing‑Domäne sieht ein Test für diese Art von Generalisierung typischerweise so aus: Wir haben ein Trainingsset von Sprachfragmente, z. B. „The girl slept.“ und „Mary knew that the girl slept.“ Jedes dieser Beispiele ist mit einer logischen Form gekoppelt, die die wesentlichen Bedeutungsbestandteile abbildet. Im Gegensatz zur Standard‑ML‑Evaluation stammt das Testset nicht aus derselben Verteilung, sondern enthält strukturell ungewohnte logische Formen. In dem obigen Beispiel hat das Modell beim Training nur flache Rekursion gesehen, im Test wird es mit einer tieferen Rekursion konfrontiert. Naive seq2seq‑Modelle haben hier Probleme und erzeugen oft Ausgaben, die vom Input abweichen – insbesondere verlieren sie die systematischen Korrespondenzen zwischen Input‑ und Output‑Token, die im Beispiel farblich hervorgehoben sind.

Eine beliebte Lösung besteht darin, Bäume in das Modell einzubinden, um den kompositorischen Prozess, der die Sprache mit der logischen Form verknüpft, zu erfassen. Das funktioniert zwar gut, aber Bäume werden meist nicht zur Verfügung gestellt und müssen erst extrahiert werden. Das ist oft kompliziert und rechenintensiv: Man muss die logischen Formen formalismenspezifisch vorverarbeiten, Variablensymbole behandeln, und in manchen Fällen sogar spezielle Grammatik‑induktive Verfahren einsetzen.  

In diesem Paper verzichten wir auf Bäume und stellen ein neuronales seq2seq‑Modell vor, das die Zusammenhänge zwischen Input‑Fragmenten und Output‑Fragmenten direkt modelliert. Für die erste‑mal‑in‑der‑Geschichte zeigen wir starke Generalisierung zu tieferer Rekursion, ohne dass Bäume nötig sind.

**Wie funktioniert unser Ansatz?**

Das Modell erzeugt die Ausgabe in zwei Schritten:

1. **Multiset‑Tagging**  
   Jeder Input‑Token wird mit einem (unbestimmten) Multiset von Output‑Token getagged. Nach diesem Schritt haben wir zwar alle richtigen Tokens, aber ohne Reihenfolge.

2. **Permutation‑Modell**  
   Ein zweites Modell berechnet eine Permutation, die diese Multiset‑Tokens in die korrekte Reihenfolge bringt. Wir stellen eine neue Methode vor, die keine harten Einschränkungen für mögliche Permutationen auferlegt, wodurch das Vorgehen sehr flexibel und ausdrucksstark wird.

Konkret arbeitet das Permutationsmodell wie folgt: Wir gehen von links nach rechts durch die Ausgabe und bestimmen für jede Position, welches Multiset‑Token dort erscheinen soll. Für die erste Ausgabeposition wählen wir einfach ein Token (rot markiert). Anschließend „springen“ wir zum nächsten Multiset‑Token, um die zweite Ausgabe zu bestimmen, und wiederholen das Verfahren, bis jedes Token aus dem ersten Schritt genau einmal besucht wurde.

**Experimentelle Ergebnisse**

Auf dem COGS‑Benchmark vergleichen wir unseren Ansatz mit anderen treeless‑Modellen. Unser Modell übertrifft die Konkurrenten deutlich in der Generalisierung zu tieferer Rekursion. Andere Formen der strukturellen Generalisierung bleiben jedoch weiterhin sehr schwierig.

**Technische Herausforderungen und Lösungen**

1. **Keine gegebene Alignment‑Information**  
   Im Training ist das Alignment zwischen Input‑ und Output‑Token nicht vorgegeben. Für einen gegebenen Token wissen wir also nicht, aus welchem Multiset er stammt. Wir lösen das, indem wir das Alignment als Teil des Trainings induzieren.

2. **Mehrere zulässige Permutationen**  
   Es gibt oft mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich richtige Permutation ist latente. Unser Permutationsmodell ist sehr flexibel, jedoch ist das Finden der höchstwertigen Permutation NP‑schwer (ähnlich wie das „Travelling Salesman“-Problem). Wir nähern uns diesem Problem mit einer GPU‑freundlichen kontinuierlichen Relaxation, die es uns außerdem ermöglicht, durch die Lösung zu backpropagieren und somit lernbarere, sprachlich plausiblere Permutationen zu erhalten.

Wenn Sie mehr über unsere Experimente und die Lösung der genannten Herausforderungen erfahren wollen, lesen Sie gern unser Paper oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist das unangenehme Spannungsgefühl, das entsteht, wenn jemand gleichzeitig zwei widersprüchliche Überzeugungen hält oder wenn seine Handlung nicht mit einer seiner Überzeugungen übereinstimmt – z. B. zu sagen, dass Zigaretten tödlich sind, aber trotzdem weiter zu rauchen.</sample>
    <sample id="327">**Abstract (Deutsch, ≤200 Wörter)**  

ManagerTower ist ein neues Vision‑Language‑Architekturmodell, das die Erkenntnisse mehrerer vortrainierter unimodaler Experten auf unterschiedlichen Ebenen zusammenführt. Im Gegensatz zu klassischen Zwei‑Tower‑Modellen, die nur die letzte Schicht der unimodalen Encoder nutzen, integriert ManagerTower Manager in jeder Cross‑Modal‑Schicht, die gewichtete Aggregationen der Sicht‑ und Text‑Representationen verschiedener Ebenen durchführen. Dadurch können kontextabhängige, semantische Informationen aus allen Schichten des unimodalen Encoders (z. B. RoBERTa, CLIP‑ViT‑Base) flexibel genutzt werden. Im Vergleich zu BridgeTower, das eine feste Schicht‑zu‑Schicht‑Verbindung vorsieht, bietet ManagerTower eine adaptive Aggregation, die sich an die Anforderungen jeder Cross‑Modal‑Schicht anpasst und die Skalierbarkeit erhöht. Mit nur 4 Mio. Bild‑Text‑Paaren erreicht ManagerTower überlegene Leistungen auf VQA‑ und Wikivideo‑Testsets (z. B. 39,15 % Genauigkeit auf Wikivideo) und übertrifft sogar einige Modelle mit mehr Daten oder Parametern. Die Analyse der Aggregationsgewichte bestätigt, dass Manager dynamisch unterschiedliche Ebenen der unimodalen Semantik für eine umfassendere Cross‑Modal‑Fusion nutzen. ManagerTower demonstriert damit, dass adaptive Manager die effektive Nutzung multimodaler Semantik signifikant verbessern.</sample>
    <sample id="328">GPT‑4 – laut Ihren vorläufigen Befunde ist GPT‑4 das am stärksten links‑orientierte Modell in Ihrer Analyse.</sample>
    <sample id="329">**Summary (≈200 words)**  

This work tackles zero‑shot video‑sentence localization, which requires identifying the temporal interval in a long video that best matches a natural‑language query. Conventional zero‑shot approaches generate pseudo‑events and pseudo‑queries from the video, but their queries are overly simple, the alignment between queries and events is weak, and the resulting pseudo‑labels are noisy. The authors propose a noise‑resistant structured pseudo‑label generation framework. First, dense video frames are fed to a pre‑trained image‑caption model (BLIP) to produce free‑form, complex pseudo‑queries. Second, a relevance score between each frame and a query is computed, and a sliding‑window search selects the interval that maximises the difference between average inside‑event similarity and outside‑event similarity—this yields a high‑quality pseudo‑event. Top‑K queries with the highest event quality are retained while eliminating highly overlapping query‑event pairs. For training, the model’s predicted confidence and IoU with the pseudo‑label are used to estimate label noise; samples with low confidence or IoU are down‑weighted, and high‑confidence, high‑IoU predictions are treated as new pseudo‑labels for iterative refinement. Experiments on ActivityNet Captions and Charades‑STA show that the proposed SPL method outperforms existing zero‑shot baselines on R@M and mIoU metrics, demonstrating robust performance despite noisy pseudo‑labels. The code is publicly released.</sample>
    <sample id="330">Yes – in the study cumulative training performed as well as or better than iterative training across all tested strategies, making it the preferred choice for active learning in this scenario.</sample>
    <sample id="331">The speaker is **Sara Papi**.</sample>
    <sample id="332">**Quelle der MuDA‑Benchmark-Daten:**  
Die Daten stammen aus den Transkripten von TED‑Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Diese TED‑Talk‑Korpora bilden die Basis für die Analyse und das Benchmark‑Setup.</sample>
    <sample id="333">In dieser Präsentation stellt Wenhao aus der Nanjing University das System **INK** vor, das die Leistung von neuronalen Maschinenübersetzungsmodellen (NMT) durch gezielte Injektion von kNN‑Kenntnissen verbessert. Ausgangspunkt ist die Beobachtung, dass NMT‑Modelle eine nicht‑glatte Repräsentations‑Raumstruktur erzeugen: seltene Tokens sind stark dispergiert, wodurch „Löcher“ entstehen, in denen die semantische Bedeutung schlecht definiert ist. kNN‑MT wurde als Lösung entwickelt, indem bei jedem Dekodierungsschritt ein Daten‑Store mit Schlüssel‑Wert‑Paaren (Repräsentationen und Ziel‑Tokens) abgefragt wird. Dieses Verfahren liefert zwar bessere Ergebnisse, ist jedoch zeitaufwändig und lässt sich nach der Erstellung des Stores nicht leicht aktualisieren.

INK löst diese Probleme durch einen iterativen Trainingszyklus: Zuerst extrahiert ein Adapter kNN‑Kenntnis aus dem Store und leitet diese an die NMT‑Architektur weiter. Anschließend werden die aktualisierten Repräsentationen verwendet, um den Store asynchron zu refreshen. Der Prozess wiederholt sich, bis Konvergenz erreicht ist. Dabei werden drei Ziele mit KL‑Divergenz optimiert: (1) Angleichung der kontextualisierten Repräsentationen an Token‑Embeddings, (2) Angleichung an kNN‑Token‑Embeddings zur Semantikverstärkung und (3) Angleichung von Repräsentationen desselben Ziel‑Tokens zur Reduktion von Sparsität.

Auf dem WMT’19 Deutsch‑Englisch‑News‑Corpus evaluiert man INK mit dem erstklassigen Off‑the‑Shelf‑NMT‑Modell. Die Ergebnisse zeigen, dass INK die BLEU‑ und COMET‑Scores um durchschnittlich 1,0 bzw. 1,99 Punkte über dem aktuellen kNN‑MT‑State‑of‑the‑Art verbessert, dabei jedoch keine Daten‑Store mehr benötigt und die Inferenzgeschwindigkeit steigt. Die Kombination aus Adapter und Daten‑Store liefert weitere, aber kleinere Verbesserungen, was darauf hindeutet, dass die Repräsentationsverbesserung noch weiter optimiert werden kann. Insgesamt demonstriert INK, dass ein kleiner Adapter ausreicht, um die Repräsentationsraum‑Glätte zu erhöhen, die Speicher‑ und Laufzeitkosten zu reduzieren und die Übersetzungsqualität signifikant zu steigern.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">**Sprachübergreifender Transfer** bedeutet, dass ein Modell, das in einer Sprache (z. B. Englisch) auf Daten trainiert wurde, anschließend auf eine andere Sprache (z. B. Deutsch oder Chinesisch) angewendet wird. Dabei kann das Modell entweder ohne zusätzliche Daten (Zero‑Shot) oder mit nur wenigen Beispielen (Few‑Shot) in der Ziel‑Sprache arbeiten. In der Präsentation wird gezeigt, dass der Transfer von Englisch auf andere Sprachen zunächst einen großen Leistungsabfall (Zero‑Shot) verursacht, der aber mit wenigen Ziel‑Sprach‑Beispielen (Few‑Shot) schnell verkleinert wird.</sample>
    <sample id="337">**Kurzfassung (ca. 200 Wörter)**  

Der Vortrag stellt ein neues Verfahren zur Einbettung von Out‑of‑Vocabulary‑(OOV)‑Wörtern vor, das auf der Idee basiert, die Wortbildung und die semantische Beziehung zu bereits bekannten Wörtern zu nutzen. Bei Auftreten eines OOV‑Wortes wird es zunächst in Wortstücke (word‑pieces) zerlegt. Diese Stücke bilden zusammen mit verwandten Wörtern (Synonymen, Nachbarn im Vokabular, Zweifach‑Nachbarn) einen zweischichtigen Wort‑Beziehungsgraphen. Jede Knoten­repräsentation erhält ein Attribut – für bekannte Wörter die vortrainierte Einbettung, für OOV‑Knoten wird ein selbst‑Aufmerksamkeits‑Netzwerk eingesetzt, das die Zeichen des Wortes verarbeitet.  

Zur Verarbeitung des Graphen wird ein zweistufiges Graph‑Attention‑Network (GAT) verwendet, dessen Ausgaben mit den ursprünglichen Attributen kombiniert werden, um robuste Knoteneinbettungen zu erzeugen. Ein Read‑Out‑Block fasst die Knoteneinbettungen zu einer Graph‑Ebene‑Repräsentation zusammen.  

Die Lernschicht nutzt ein kontrastives NT‑XENT‑Verlust, bei dem als positive Paare die OOV‑Einbettung, ihre Nachbarn und Synonyme dienen, während negative Paare andere Wörter sind. Dadurch wird die OOV‑Einbettung im Vektorraum der Grund‑Einbettungen positioniert.  

Experimente zeigen, dass das Modell sowohl auf intrinsischen als auch auf extrinsischen Aufgaben (z. B. Klassifikation, Frage‑Beantwortung) die Baselines übertrifft und sowohl statische als auch kontextuelle Modelle verbessert. Abschließend wird die Übertragbarkeit auf andere Sprachen diskutiert: agglutinative Sprachen eignen sich sehr gut, während für Fusions‑Sprachen eine geeignete Morpheme‑Segmentierung entscheidend ist.</sample>
    <sample id="338">**Kurzfassung (ca. 200 Wörter)**  

Die vorgestellte Arbeit untersucht, ob menschliche Erklärungen in natürlicher Sprache immer hilfreich für KI‑Modelle sind und entwickelt dafür ein objektives Bewertungssystem. Zunächst wird ein einheitliches Datenformat vorgestellt, das verschiedene Aufgaben (Commonsense‑QA, NLI, Commonsense‑Validierung) in ein gemeinsames Multiple‑Choice‑Setup überführt. Dieses Format ermöglicht den Vergleich von Modellen in einer Basis‑ und einer „Infusion‑“Variante, bei der die Erklärungen als zusätzliches Eingabe­material verwendet werden.  

Durch gezielte Experimente mit fünf großen Datensätzen (CoS‑E, ECQA, e‑SNLI, ComVE) und zwei Sprachmodellen (T5, BART) wird gezeigt, dass Erklärungen nicht automatisch neues Wissen vermitteln, sondern häufig nur die Abhängigkeit vom Erklärungstext erhöhen. Vor allem bei kleinen Datenmengen führen fein‑tuning‑Infusionen zu signifikanten Verbesserungen.  

Auf Basis dieser Beobachtungen wird der neue Metrik‑Ansatz **TREU** (Task‑Relevance‑Enhanced Utility) eingeführt. TREU erweitert den bestehenden Simulatability‑Score um die Bewertung der Nützlichkeit von Erklärungen im Fine‑Tuning‑Prozess. Im Vergleich zum Simulatability‑Score ordnet TREU die Datenqualität konsistenter, erkennt jedoch Unterschiede je nach Aufgabe und Erklärungsstil (z. B. Negation in e‑SNLI).  

Die Arbeit liefert damit ein einheitliches Framework, erste Erkenntnisse zur Nutzen‑Faktoren von Erklärungen und einen robusteren Bewertungsmaßstab, der zukünftige Annotationen und Modelltrainings besser steuern kann.</sample>
    <sample id="339">Die Autoren (Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow) sind am Saarland University in Deutschland tätig.</sample>
    <sample id="340">**Kurzfassung des Vortrags (ca. 200 Wörter)**  

Kuan‑Hao Huang von UCLA präsentiert *ParaAMR*, einen großen, syntaktisch vielfältigen Paraphrasensatz, der durch AMR‑Back‑Translation erstellt wurde. Traditionelle menschlich annotierte Daten (MRPC, PAN, Quora) sind zwar qualitativ hochwertig, aber zu klein. Automatisch erzeugte Datensätze mittels Back‑Translation liefern zwar Menge, weisen aber kaum syntaktische Vielfalt auf.  

Um dieses Problem zu lösen, nutzt das Verfahren die AMR‑Graphen (Abstract Meaning Representation), die die semantische Struktur eines Satzes als gerichteten Graphen darstellen. Ein vortrainierter AMR‑Parser erzeugt zunächst den Graphen des Ausgangssatzes. Anschließend wird zufällig ein Knoten als neuer Fokus (Root) gewählt und die zugehörigen Kanten angepasst. Der modifizierte Graph wird dann mit einem Graph‑zu‑Text‑Generator in Text überführt. Da sich alle generierten Sätze auf denselben Graphen stützen, bleiben die Semantik erhalten, während die Syntax durch die neue Fokus‑Platzierung variiert.  

*ParaAMR* umfasst ca. 15 Mio. Ausgangssätze mit durchschnittlich 6,9 Paraphrasen pro Satz. Quantitative Analysen zeigen, dass die semantische Ähnlichkeit vergleichbar mit anderen Back‑Translation‑Datasets ist, die syntaktische Diversität jedoch deutlich höher.  

Anwendungen:  
1. **Satz‑Embedding‑Training** – Modelle, die mit *ParaAMR* trainiert wurden, erzielen bessere STS‑Scores.  
2. **Syntaktische Steuerung** – Paraphrasengeneratoren aus *ParaAMR* zeigen verbesserte Kontrolle über Satzstruktur.  
3. **Few‑Shot‑Learning‑Augmentation** – Durch syntaktisch vielfältige Augmentation steigen die Leistungen bei wenigen Trainingsbeispielen.  

Der Datensatz ist öffentlich zugänglich. Der Vortrag schließt mit einer Zusammenfassung, dass *ParaAMR* sowohl syntaktische Vielfalt als auch semantische Qualität bietet und mehrere NLP‑Anwendungen positiv beeinflusst.</sample>
    <sample id="341">The authors report two latency metrics:  

1. **Average lagging** – the standard measure of how far behind the translation is relative to the source speech.  
2. **Computational‑aware average lagging** – the same lag metric adjusted to include the model’s actual inference time, reflecting the real elapsed time for producing the output.</sample>
    <sample id="342">**LiveChat: A Large‑Scale Personalized Dialogue Dataset from Live Streaming**

Gao Jingsheng presented the paper “LiveChat: A Large‑Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming,” conducted with teammates from Shanghai Jiao Tong University and Xiaobing.AI. The presentation began with an overview of open‑domain dialogue—conversational AI that can discuss any topic using pre‑trained models and large corpora. Existing corpora are largely text‑based, whereas video‑sourced dialogue is closer to natural speech. Current video datasets are either scripted (TV, movies) or small interview collections, limited by manual annotation. LiveChat addresses these gaps by automatically building a massive, personalized, multi‑party Chinese dialogue corpus from Douyin (TikTok) live streams.

**Dataset Construction**:  
1. Scrape Douyin live videos, extract audio, and transcribe utterances via ASR.  
2. Gather audience comments and match them to speakers using a reply‑to‑whom algorithm to form dialogues.  
3. Extract persona information: basic profiles manually labeled and richer profiles via rule‑based and classifier‑based methods.

**Experiments**:  
- **Response Modeling**: Retrieval baselines show that longer sessions and extracted persona data improve performance; both rule‑based and classifier‑based personas help.  
- **Addressee Recognition**: Single‑stream BERT outperforms double‑stream BERT; persona cues still benefit the task.  
- **Transfer Learning**: BART outperforms other LLMs, indicating a domain shift; in‑context learning improves with more shots up to 8, after which performance drops due to noisy demonstrations.

**Conclusion**: LiveChat offers a large, personalized, multi‑party dialogue dataset with significant advantages for speaker‑specific response generation. Future work will focus on efficient LLM transfer learning for this domain.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha und heute präsentieren mein Co‑Autor Martin und ich unsere Arbeit **„The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources“**. Diese Studie entstand in Kooperation zwischen der McGill University, Mila und Microsoft Research.

---

### Was wir untersuchen

Modle der natürlichen Sprachverarbeitung greifen auf verschiedenste Wissensquellen zurück – zum einen auf das Wissen, das im Modell selbst während des Pretrainings gespeichert wird, und zum anderen auf zusätzliche Informationen, die ihnen schon beim Inferenz‑Zeitpunkt übergeben werden. Viele neuere Arbeiten in Bereichen wie Frage‑Antworten zeigen, dass Modelle bereits im Pretraining‑Zeitpunkt über genügend Wissen verfügen, um die Aufgabe zu lösen. In vielen NLU‑Aufgaben (Natural‑Language‑Understanding) ist jedoch zusätzlich Wissen nötig, das erst zum Inferenz‑Zeitpunkt bereitgestellt wird.

**Beispiel:**  
„John sah den neugewählten Präsidenten im Fernsehen.“  
Hier besitzt ein Modell zwar Wissen über die Rolle eines Präsidenten und über ein Fernsehgerät, kann aber nicht zuverlässig ermitteln, wer *John* ist oder wer der neue Präsident ist – diese Informationen könnten seit dem Pretraining geändert worden sein. Ein erfolgreiches Modell muss also in der Lage sein, sowohl auf **pretrain‑Zeitpunkt** als auch auf **inferenz‑Zeitpunkt** zugrundeliegendes Wissen zu integrieren.

---

### Der KITMUS‑Test

Wir haben deshalb einen diagnostischen Test‑Set entworfen, der insbesondere die Fähigkeit zur **Wissensintegration** prüft. Das Herzstück ist ein Coreference‑Aufgabentyp, bei dem das Modell entscheiden muss, auf welches Subjekt ein Pronomen verweist, wobei das nötige Wissen in unterschiedlichen Quellen liegen kann.

**Beispiel aus unserem Datensatz**  
&gt; „Servin ist Richter. Kea ist Bäckerin. Servin und Kea trafen sich im Park. Nach einem langen Arbeitstag, an dem er Fälle in einem Gericht entschied, war er froh, sich zu entspannen.“  
Die Aufgabe besteht darin, das Pronomen „er“ korrekt auf **Servin** zu referenzieren. Hierfür braucht man zwei Wissensarten:
1. **Entitätsspezifisches Wissen** – z. B. „Servin ist Richter“  
2. **Allgemeines Hintergrundwissen** – z. B. „Richter entscheiden Fälle in Gerichten“

Während das Hintergrundwissen typischerweise im Pretraining gelernt wird, ist das entitätsspezifische Wissen meist erst bei der Inferenz bekannt.

---

### Drei Wissens‑Konfigurationen

1. **Background‑Pretrain**  
   * Hintergrundwissen liegt im Pretrain‑Modell vor.  
   * Bei der Inferenz wird nur das entitätsspezifische Wissen übermittelt.

2. **Background‑Both**  
   * Sowohl Hintergrund‑ als auch entitätsspezifisches Wissen werden bei der Inferenz bereitgestellt.

3. **Background‑Inference**  
   * Beide Wissensarten sind ausschließlich bei der Inferenz verfügbar – das Modell hat sie also nicht im Pretraining.  
   * Dieses Setting simuliert z. B. neue Berufe, die seit dem Pretraining entstanden sind.

**Illustratives Beispiel**  
*Background‑Pretrain:*  
* Hintergrundwissen: „Politiker suchen gewählte Ämter in der Regierung“ (im Pretraining vorhanden).  
* Inferenz‑Kontext: „Chichester ist Politiker.“  

*Background‑Both:*  
* Zusätzlich wird im Inferenz‑Kontext auch das Hintergrundwissen über Politiker bereitgestellt.  

*Background‑Inference:*  
* Statt „Politiker“ wird ein fiktiver Beruf „mirituer“ verwendet, der im Pretraining kaum vorgekommen ist.  

---

### Evaluation

Wir haben die Datensätze sowohl mit menschlichen Prüfern als auch mit etablierten Coreference‑Modellen getestet.

- **Baseline‑Modelle** (C2F, BERT4Coref) performten ohne KITMUS‑Training schlecht – sie nutzen überwiegend Oberflächen‑Cues, die bei KITMUS absichtlich entfernt wurden.  
- **Mit KITMUS‑Training** verbesserten sich die Modelle signifikant, fast bis zur menschlichen Leistung.  
- Bei Varianten mit **fiktivem Wissen** (Background‑Inference) blieben selbst die besten Modelle noch Schwierigkeiten, die ausschließlich im Inferenz‑Kontext präsent sein muss.

---

### Fazit

1. **Coreference‑Modelle ohne spezielle KITMUS‑Training** können kaum Wissen aus unterschiedlichen Quellen kombinieren.  
2. **Mit KITMUS‑Training** gelingt es einigen Modellen, Wissen zuverlässig zu integrieren.  
3. **Reine Inferenz‑Wissensintegration** (Background‑Inference) bleibt jedoch eine große Herausforderung – selbst die leistungsstärksten Modelle können hier nicht immer zuverlässig entscheiden.

---

Wenn ihr mehr Details sehen wollt, schaut euch bitte den vollständigen Artikel an – der Datensatz und der Code sind auf GitHub verfügbar. Vielen Dank für eure Aufmerksamkeit!</sample>
    <sample id="344">Die Nachteile baumbasierter Ansätze liegen vor allem in der Notwendigkeit, die Baumstruktur selbst zu erzeugen:

* **Kein Baum vorgegeben** – Die Modelle müssen die Baumstruktur aus den Daten extrahieren, was ein zusätzlicher, oft aufwändiger Schritt ist.  
* **Formalismspezifische Vorverarbeitung** – Für jedes Ziel‑Formalismus (z. B. Logik, Variablensymbole) ist eine eigene Vorverarbeitung erforderlich.  
* **Spezialisierte Grammatik‑Induktion** – Oft werden komplexe, domänenspezifische Grammatik‑Induktionsverfahren benötigt.  
* **Rechenintensiv** – Das Erzeugen und Verwenden der Bäume kann sehr kosten‑ und zeitaufwendig sein, besonders bei großen Datensätzen oder tiefen Strukturen.  

Kurz gesagt: Baumbasierte Methoden verlangen zusätzliche, oft komplizierte und teure Schritte zur Baumerzeugung, die die Skalierbarkeit und Generalisierbarkeit einschränken.</sample>
    <sample id="345">**Kurzfassung des Papers**

Matthias Lindemann, Alexander Koller und Ivan Titov stellen in ihrem Beitrag einen neuen Ansatz für die **kompositionsielle Generalisierung** in der semantischen Parsing‑Aufgabe vor. Ziel ist es, Modelle zu bauen, die tiefe Rekursionen und bislang unbekannte Kombinationen von Phrasen korrekt interpretieren können, ohne sich auf vorgegebene Baumanalysen zu stützen. Kompositionsielle Generalisierung bedeutet hier, dass ein Modell, das von Sätzen wie „The girl slept.“ und „Mary knew that the girl slept.“ mit zugehörigen logischen Formen trainiert wurde, auch Sätze mit tieferer Rekursion korrekt in logische Ausdrücke überführen kann.

Der vorgeschlagene **Seq2Seq‑Ansatz** arbeitet in zwei Schritten. Zunächst tags jeder Input‑Token ein **unordentliches Multiset** von Output‑Tokens, wodurch alle benötigten Tokens bestimmt, aber nicht geordnet werden. Im zweiten Schritt wird mittels eines zweiten Modells eine **Permutation** dieser Tokens erzeugt, die die richtige Reihenfolge im Zielstring liefert. Hierbei wird ein neu entwickeltes, flexibles Permutationsmodell verwendet, das keine harten Einschränkungen für mögliche Permutationen auferlegt. Die Optimierung der Permutation ist NP‑schwer, wird aber mithilfe einer GPU‑freundlichen kontinuierlichen Relaxation gelöst, die Gradient‑Propagierung ermöglicht.

Ein weiteres Problem ist die fehlende **Alignments** zwischen Input‑ und Output‑Tokens in den Trainingsdaten. Das Modell induziert diese Alignments selbstständig und wählt, falls mehrere Permutationen möglich sind, die sprachlich plausibelste aus. Auf der COGS‑Benchmark übertrifft das treenlose Modell die bisherigen Ansätze in der Generalisierung zu tiefer Rekursion, während andere strukturelle Generalisierungsaufgaben noch schwierig bleiben.</sample>
    <sample id="346">Die Transkription enthält keine Angabe zur Universitätszugehörigkeit der Autoren.</sample>
    <sample id="347">**Markierte Personas: Wie natürliche Sprachaufforderungen Vorurteile in Sprachmodellen messen**

Hallo, ich bin Myra und heute werde ich über unser Paper „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Die Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky verfasst. In den letzten Jahren haben viele die Verbreitung von sozialen Vorurteilen und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Diese Messungen haben jedoch verschiedene Einschränkungen. Sie beruhen in der Regel auf handkonstruierten Datensätzen, die sehr zeitaufwändig zu kuratieren sind, und sie messen meist nur sehr spezifische Stereotypen, wodurch sie schlecht auf andere Demografien oder Kontexte übertragbar sind. Oft fassen sie lediglich allgemeine, breite Assoziationen zusammen, etwa negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigen die meisten Arbeiten in diesem Bereich keine Intersektionalität – das heißt, sie ignorieren, dass soziale Identitäten mehrfach kombiniert werden können und so einzigartige Schadensstellen darstellen.

Um diese Begrenzungen zu überwinden, nutzen wir die Eigenschaft, dass neuere, instruktionstaugliche LLMs sehr gut auf Anweisungen und Aufforderungen reagieren. Wir können das Modell also bitten, eine Persona zu generieren – eine Darstellung einer imaginären Person – mit einer Aufforderung wie „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich.“ Wir sehen sofort, dass diese Vorgehensweise für jede Demografie generalisierbar ist, weil wir einfach die gewünschten Identitätsmarkierungen in die Prompt einbauen. Hier ein paar Beispielgenerierungen von GPT‑4. Obwohl die Ausgaben nicht von sich aus negativ oder toxisch sind, zeigen sich interessante Muster: Die asiatische Frau wird als zurückhaltend dargestellt; die mittelöstliche Frau wird mit Begriffen wie „exotisch“ und „fesselnd“ assoziiert. Beide Frauen mit Hautfarbe erwähnen ihre Abstammung, während die weiße Mann-Persona keine solche Bezugnahme enthält.

Zur Erfassung dieser Muster hat unsere Methode zwei Teile. Erstens generieren wir Personas. Die Prompts wurden von einer Studie inspiriert, in der menschliche Subjekte ähnliche Aufforderungen erhielten und dadurch rassistische Stereotypen aufgedeckt wurden. So lässt sich auch direkt mit den von Menschen verfassten Antworten vergleichen. Zweitens verwenden wir „Marked Words“, ein Verfahren, um die Wörter zu identifizieren, die markierte Gruppen von unmarkierten unterscheiden. Ich erläutere das im Folgenden.

Der Vorteil ist, dass wir sehr spezifische Stereotypen und Muster erfassen können, ohne auf ein vorgegebenes Lexikon angewiesen zu sein. Das Konzept der Markiertheit stammt aus der Soziolinguistik: Es gibt einen unmarkierten Standard; jede Gruppe, die davon abweicht, wird sprachlich markiert. Beispielsweise wird „Krieger“ typischerweise mit Männern assoziiert; wenn jemand einen Krieger beschreibt, der eine Frau ist, wird häufig „weiblicher Krieger“ angegeben, um die Marke zu setzen. In der Gesellschaft sind dominante Gruppen sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen meist markiert sind.

In unserer Methode bestimmen wir zunächst die unmarkierten und markierten Gruppen und vergleichen anschließend die Personas mithilfe der „Fightin’ Words“-Methode, die gewichtete Log‑Odds‑Raten nutzt, um die wichtigsten Wörter jeder markierten Gruppe zu bestimmen. So würden wir bei den Personas von schwarzen Frauen die Log‑Odds-Raten gegen weiße Personas und Männer Personas (die die beiden unmarkierten Gruppen sind) vergleichen.

**Ergebnisse**

Zunächst nutzten wir ein Stereotyplexikon und fanden heraus, dass die generierten Personas wesentlich mehr Stereotypwörter enthalten als die von Menschen verfassten. Allerdings unterscheiden sich die Verteilungen der Wörter stark. Die generierten Personas weisen zwar eine höhere Häufigkeit der Lexikoneinträge auf, die menschlichen Texte zeigen eine viel breitere Wortverteilung. Die Stereotypwörter in den generierten Personas beschränken sich dabei kaum auf „tall“ und „athletic“ – also nur auf scheinbar positive oder zumindest nicht-negativ klingende Begriffe. Das Lexikon erfasst also kaum die schädlichen Muster, die wir zuvor bemerkt haben.

Stattdessen wenden wir unsere Marked‑Words‑Methode an, um zu zeigen, wie scheinbar positive Darstellungen schädliche Muster und essentialisierende Narrative fördern. In der Analyse stellen wir fest, dass die wichtigsten Wörter der markierten Gruppen Begriffe wie „Kultur“, „Tradition“, „stolz“ und „exotisch“ enthalten. Diese Wörter definieren die Gruppen ausschließlich durch ihr Verhältnis zur Identität und unterscheiden sie damit von der weißen Norm. Damit wird ein langjähriges Vermögen von Diskriminierung und Anderenbildung gefördert.

Besonders bei Frauen mit Hautfarbe tauchen häufig Klischees auf. Für lateinamerikanische Frauen zeigen sich Begriffe wie „lebhaft“ und „kurvig“, die mit dem tropischen Stereotyp verbunden sind. Für asiatische Frauen erscheinen Wörter wie „klein“, „zart“ und „seidig“, die auf die lange Geschichte der Hypersexualisierung und Unterwürfigkeit asiatischer Frauen hinweisen. Für schwarze Frauen tauchen Wörter wie „stark“ und „resilient“ auf, die mit dem „Strong Black Woman“-Archetyp verknüpft sind. Auch wenn diese Begriffe zunächst positiv erscheinen, zeigen Studien, dass der Archetyp schädlich ist, weil er Druck ausübt, gesellschaftliche Hindernisse zu überwinden, anstatt diese Hindernisse zu beseitigen, was negative gesundheitliche Folgen hat.

Insgesamt zeigen die Wörter für jede markierte Gruppe, dass sie sehr essentialisierende Narrative widerspiegeln. Daraus ziehen wir drei Empfehlungen für Betreiber von Modellen:

1. **Positive Stereotype und essentialisierende Narrative ansprechen** – Wir sollten nicht nur negative Vorurteile messen, sondern auch die schädlichen positiven Stereotypen untersuchen.
2. **Intersektionalen Blick einbeziehen** – Eine Untersuchung unter Berücksichtigung von Mehrfachidentitäten ist notwendig, da ansonsten wichtige Schadensstellen übersehen werden.
3. **Transparenz bei Bias‑Mitigation‑Methoden** – Wir brauchen mehr Offenheit darüber, welche Techniken zur Bias‑Reduktion eingesetzt werden. Ohne Transparenz können wir nicht abschätzen, ob die positiven Stereotype durch übermäßige Wertausrichtung oder durch andere Anti‑Stereotyp-Maßnahmen entstehen.

Vielen Dank für eure Aufmerksamkeit. Viel Spaß bei ACL!</sample>
    <sample id="348">**Kurzfassung (≈ 200 Wörter)**  

Die vorgestellte Arbeit untersucht, wie moderne, instruell‑getunete LLMs (z. B. GPT‑4) stereotype Darstellungen von sozialen Identitäten erzeugen, auch wenn diese nicht explizit negativ oder toxisch wirken. Klassische Bias‑Messungen sind zeitaufwendig, auf handkonstruierten Datensätzen begrenzt und berücksichtigen selten Intersektionalität.  

Die Autoren nutzen die Fähigkeit der Modelle, auf präzise Anweisungen zu reagieren, um „Personas“ zu generieren (z. B. „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich.“). Diese Prompt‑Methode ist generisch und lässt sich beliebige Identitätsmarker einbauen, wodurch sie direkt mit menschlichen Antworten verglichen werden kann.  

Zur Analyse wenden sie die sociolinguistische Markedness‑Theorie an und entwickeln die **Marked‑Words**‑Methode (Fightin’ Words). Dabei werden log‑odds‑Raten benutzt, um die Wörter, die markierte Gruppen von den unmarkierten (z. B. weiß/​mann) unterscheiden, zu identifizieren.  

Ergebnisse zeigen, dass generierte Personas häufiger stereotype Wörter enthalten als menschliche Texte, jedoch meist positive Begriffe wie „tall“ oder „athletic“. Die Marked‑Words‑Analyse enthüllt, dass die häufigsten Worte für marginalisierte Gruppen („culture“, „tradition“, „exotic“, „vibrant“, „curvaceous“, „strong“, „resilient“) stark essentialisierend sind und lange bestehende Stereotypen (z. B. „Strong Black Woman“) reproduzieren.  

Die Autoren fordern: (1) Erforschung von positiven Stereotypen, (2) intersektionale Analyse, und (3) mehr Transparenz bei Bias‑Minderungsmaßnahmen, um unerwartete, potenziell schädliche Darstellungen zu erkennen.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es ist mir eine Freude, ein kurzes Werbevideo zu unserem Papier vorzustellen.  
„Sind Sie mein Modell kopiert?“ – Schutz des Urheberrechts großer Sprachmodelle für Embedding‑as‑a‑Service mittels Backdoor‑Wasserzeichen.

Zunächst möchte ich den Hintergrund von Embedding‑as‑a‑Service erläutern. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM in der natürlichen Sprachverarbeitung und -erzeugung außergewöhnlich leistungsfähig. Embedding‑as‑a‑Service ist einer der auf großen Sprachmodellen basierenden Dienste, um verschiedene NLP‑Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT‑basierte Embedding‑API an. Jedoch haben jüngste Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienste anbieten können. Daher ist es notwendig, das Urheberrecht von Embedding‑as‑a‑Service zu schützen.

Um das Urheberrecht von Embedding‑as‑a‑Service zu schützen, ist eine Lösung, ein Wasserzeichen in den Provider‑Dienst einzubetten und zu prüfen, ob ein anderer Dienst das Wasserzeichen enthält. Das Wasserzeichen‑Verfahren muss die folgenden Eigenschaften erfüllen:  
1. Es sollte auf Embedding‑as‑a‑Service anwendbar sein.  
2. Das Wasserzeichen sollte die Nützlichkeit der bereitgestellten Embeddings nicht verschlechtern.  
3. Das Wasserzeichen muss für den Angreifer gut verborgen sein oder der Angreifer sollte es nicht einfach entfernen können.  
4. Das Wasserzeichen muss während des Modellausgrabens auf die Dienste des Angreifers übertragbar sein.

Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Methoden sind jedoch entweder nicht auf Embedding‑as‑a‑Service anwendbar oder weisen keine Übertragbarkeit auf. Deshalb schlagen wir in diesem Papier den *Embedding Marker* vor – ein backdoor‑basiertes Wasserzeichenverfahren, das für Embedding‑as‑a‑Service geeignet ist.

Im Folgenden erläutere ich die Details unseres Embedding Marker. Der Marker besteht aus zwei Hauptschritten: Wasserzeichen‑Injektion und Urheberrechts‑Verifikation.

**Wasserzeichen‑Injektion**  
Zunächst wählen wir einen Trigger‑Set. Der Trigger‑Set ist eine Gruppe von Wörtern mit einer moderaten Frequenz. Wir nehmen an, dass der Provider ein allgemeines Textkorpus sammeln und die Wortfrequenz daraus zählen kann. In der Injektion definieren wir zunächst ein Ziel‑Embedding. Wenn ein Benutzer einen Satz an den Provider‑Dienst sendet, zählt der Provider die Anzahl der Trigger in diesem Satz. Das bereitgestellte Embedding ist eine gewichtete Summe aus dem Ziel‑Embedding und dem ursprünglichen Embedding. Das Gewicht des Ziel‑Embeddings ist proportional zur Anzahl der Trigger im Satz. Sobald die Anzahl der Trigger im Satz größer als `m` ist, entspricht das bereitgestellte Embedding exakt dem Ziel‑Embedding.

**Urheberrechts‑Verifikation**  
Ziel ist es festzustellen, ob ein hinter einem anderen Dienst verstecktes Modell das Wort‑Mark enthält. Wir erstellen zunächst einen Backdoor‑Datensatz und einen benign‑Datensatz. Der Backdoor‑Datensatz besteht aus Sätzen, in denen jedes Wort im Trigger‑Set liegt, während im benign‑Datensatz keine Wörter aus dem Trigger‑Set vorkommen. Der Provider fordert dann die Embeddings vom Angreifer‑Dienst für beide Datensätze an. Der Kosinus‑ und L2‑Ähnlichkeitswert zwischen dem angeforderten Embedding und dem Ziel‑Embedding werden berechnet. Wir bestimmen die Ähnlichkeitsdifferenz zwischen benign und Backdoor, die als Delta‑Kosinus bzw. Delta‑L2 definiert ist. Zusätzlich führen wir einen KS‑Test durch und nutzen dessen p‑Wert als dritte Metrik.

Wir haben Experimente an vier Datensätzen durchgeführt: AG News, MIND, SST2 und Enron Spam. Wir gehen davon aus, dass der Provider das Wikipedia‑Textkorpus verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse für die vier Datensätze zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielt und gleichzeitig die Nützlichkeit für downstream‑Aufgaben beibehält.

Wir haben außerdem die Tarnung des bereitgestellten Embeddings validiert, indem wir die Embeddings der Sätze auf vier Datensätzen mit PCA visualisiert haben. Die Legende der Abbildungen gibt die Anzahl der Trigger in jedem Satz an. Wie in den Abbildungen zu sehen ist, ist es schwierig, Backdoor‑Embeddings von normalen Embeddings zu unterscheiden.

Das war's. Vielen Dank. Wir freuen uns darauf, mit Ihnen zu diskutieren.</sample>
    <sample id="350">Our paper, “What’s the Meaning of Superhuman Performance in Today’s NLU?” examines why many recent NLP systems appear to beat humans on popular benchmarks. In the last five years, leaderboard‑based evaluation has become the norm, and once a task is “saturated” systems routinely reach or exceed human performance. We focus on two flagship datasets—SuperGLUE and SQuAD—to investigate how reliable these comparisons really are.

We find several fundamental problems. First, humans and systems are evaluated on different data: human scorers usually work on a tiny subset of the test set, while systems run on the full set (e.g., BoolQ). Second, many gold labels contain errors or are ambiguous, allowing models to exploit spurious correlations that humans cannot. Third, human baselines are often crudely aggregated (average or majority vote) and based on low‑pay, minimally motivated annotators—sometimes as little as $3.6/hr—making the scores unreliable. Finally, crucial details about the annotator pool (size, selection process, background) are almost always omitted, preventing a fair comparison to the best possible human performance.

Our conclusion is that claims of “superhuman” performance lack scientific rigor. We provide concrete recommendations for creating more trustworthy benchmarks: use larger, better‑motivated human panels, report detailed annotator demographics, ensure consistent test sets, and validate gold labels. Our goal is to encourage a more nuanced, evidence‑based assessment of NLU progress.</sample>
    <sample id="351">**Abstract (Deutsch, ≤ 200 Wörter)**  

Wir untersuchen, ob NER‑Tagger, die auf dem CoNLL‑2003‑Korpus entwickelt wurden, auch noch im Jahr 2023 zuverlässig funktionieren. Zu diesem Zweck haben wir einen neuen Datensatz, CoNLL++, erstellt: Reuters‑Nachrichten aus 2020, annotiert nach den ursprünglichen CoNLL‑2003‑Regeln. Über 20 vortrainierte Modelle wurden auf CoNLL‑2003 fein‑getuned und anschließend auf CoNLL‑2003‑Test‑ und CoNLL++‑Testdaten evaluiert. Der prozentuale F1‑Verlust zwischen den beiden Testsets dient als Maß für die Generalisierbarkeit.  

Unsere Experimente zeigen, dass drei Faktoren entscheidend sind: (1) Transformer‑Architekturen liefern bessere Generalisierung als frühere Modelle, (2) größere Modelle erzielen höhere F1‑Raten, und (3) mehr Fine‑Tuning‑Beispiele verbessern die Leistung.  

Um die Ursachen für Leistungsabfälle zu untersuchen, haben wir zwei Hypothesen getestet: adaptives Overfitting (mehrmaliges Testen am gleichen Set) und temporale Drift. Die Analyse der Lernkurven weist darauf hin, dass adaptives Overfitting nicht vorherrschend ist, während einer fortlaufenden Pre‑Training‑Experiment mit zeitlich verfeinerten Daten der Leistungsabfall mit wachsendem Zeitabstand eindeutig nachweisbar ist.  

Schlussfolgernd funktioniert CoNLL‑2003‑basierte NER auch jetzt noch gut, wenn man auf moderne Architekturen, größere Modelle und ausreichend Fine‑Tuning‑Daten setzt. Unser Beitrag unterstreicht die Wichtigkeit, temporale Drift in zukünftigen NER‑Studien zu berücksichtigen und lädt zu weiterführender Forschung im Bereich der Generalisierung ein.</sample>
    <sample id="352">ABC‑Eval steht für **“Annotating Behaviors in Chat”** – eine Methode, bei der Chat‑Antworten anhand spezifischer Verhaltens‑Labels (z. B. Irrelevanz, Widerspruch, Halluzination, Empathie) bewertet werden.</sample>
    <sample id="353">**Kurzfassung**

In dem Paper „Python Code Generation by Asking Clarification Questions“ wird die problematische Unter­spezifikation von Natural‑Language‑Descriptions (NLD) für die automatische Code‑Generierung untersucht. Aktuelle Modelle schaffen es oft nicht, fehlende Spezifikationen zu erkennen, was in realen Anwendungen zu fehlerhaftem Code führt. Die Autoren schlagen deshalb Interaktivität vor: das Modell stellt gezielte Klarstellungsfragen (CQs), um fehlende Operation‑Level‑Spezifikationen zu erfragen.

Zunächst wird ein synthetisches Dataset namens **CodeClarQA** erstellt. Hierzu werden Schlüsseloperationen mithilfe des Code‑Knowledge‑Graphs von Graph4Code extrahiert und durch Schema‑Elemente (Verben, Schlüsselphrasen) beschrieben. Für jede NLD wird die Ähnlichkeit zwischen Schema‑Elementen und Operation‑Dokumentation berechnet; bei niedriger Ähnlichkeit gilt die Operation als unklar. Anschließend generiert ein Template‑System CQs (Ja‑/Nein‑ oder Multiple‑Choice‑Fragen), die anschließend von annotierten Val‑/Test‑Sets verifiziert werden.

Das vorgeschlagene Pipeline‑Modell besteht aus drei Komponenten:  
1. **Clarification‑Need‑Predictor** (erkennt, ob eine Klarstellung nötig ist),  
2. **Question‑Selector** (wählt die relevantesten CQs),  
3. **Code‑Generator** (generiert den Code unter Berücksichtigung der beantworteten CQs).

Experimentelle Ergebnisse zeigen:  
- MPNet liefert die beste Leistung bei der Erkennung fehlender Schlüsseloperationen.  
- Die Pipeline verbessert die Code‑Qualität im Vergleich zu Modellen ohne Interaktion, jedoch bleibt sie hinter einem reinen Modell‑Only‑Ansatz zurück, was die Komplexität der CQ‑Ranking‑Aufgabe unterstreicht.  

Analyse‑Belege deuten darauf hin, dass die Klärung von Schlüsseloperationen der Hauptgrund für die verbesserte Code‑Erzeugung ist. Das Paper bietet Code‑ und Datensatz‑Zugriff und lädt zu weiterem Feedback ein.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL‑2003 und CoNLL++ liegt bis zum Jahr **2020** über 5 Prozentpunkten.</sample>
    <sample id="355">**Kurzfassung des ACL‑2023‑Papiers „Transfer Learning for Dissonance Detection: Addressing the Rare‑Class Challenge“ (German)**  

**1. Problemstellung**  
Kognitive Dissonanz bezeichnet die Unvereinbarkeit zweier Überzeugungen oder Handlungen, z. B. „Ich weiß, dass Zigaretten mich töten können“ vs. „Ich habe nach dem Meeting noch ein paar geraucht“. Solche Diskrepanzen sind im Alltag häufig, werden jedoch in der schriftlichen Sprache selten ausgedrückt – laut unserer großen Annotationsprobe liegen sie bei nur etwa 3,5 % aller Diskurs‑Paare.  

**2. Motivation**  
Das Erkennen von Dissonanz liefert Einblicke in:  
- Entscheidungs‑ und Diskussionsprozesse,  
- gesellschaftliche Trends und Meinungsänderungen,  
- psychische Gesundheit (z. B. Angststörungen),  
- Extremismus‑ und Polarisierungsforschung.  

**3. Datenerstellung**  
- Tweets wurden zunächst mit dem PDTB‑Parser analysiert.  
- Anschließend wurden Diskurs‑Einheiten nach unseren eigenen Annotation‑Richtlinien zu Dissonanz‑ bzw. Konsonanz‑Beziehungen bewertet.  
- Insgesamt wurden ca. 1 000 Paare gesammelt, wovon 43 Dissonanz‑Beispiele enthalten waren.  

**4. Anfangsmodell &amp; Herausforderung**  
Ein auf diesen 43 Beispielen trainierter Klassifikator erreichte kaum bessere Leistung als Zufall (AUC ≈ 0,5). Die extreme Seltenheit der Dissonanz‑Klasse machte ein direktes Training unmöglich.  

**5. Transfer‑Learning‑Ansatz**  
Wir nutzten zwei verwandte Aufgaben als Ausgangspunkt:  
1. **Debate** – Debattier‑Stance‑Klassifikation (widersprüchliche/vereinbarende Aussagen, unabhängig vom Thema).  
2. **CE** – Binäre Klassifikation der PDTB‑Klassen *Expansion* und *Comparison* (eng verbunden mit Konsonanz/Dissonanz).  

- **Zero‑Shot‑Transfer**: Mit CE‑Gewichten starteten wir das Modell, das bereits AUC ≈ 0,62 erreichte – deutlich über Zufall.  
- **Sequenzielles Fine‑Tuning**: CE‑Fine‑Tuning gefolgt von Debate‑Fine‑Tuning führte zu noch besseren Zero‑Shot‑Ergebnissen.  

**6. Active Learning (AL)**  
- **Update‑Strategien**:  
  - *Iterativ* – Modell wird nur auf den aktuellen Annotierungs‑Batch trainiert.  
  - *Kumulativ* – Modell wird auf allen bisher annotierten Daten trainiert.  
  - Ergebnis: Kumulativ war gleich oder besser als iterativ, besonders bei domänenspezifischen Daten.  

- **Auswahlstrategie**: Wir entwickelten die **Probability‑of‑Rare‑Class (PRC)**‑Strategie, die Beispiele auswählt, bei denen das aktuelle Modell mit hoher Wahrscheinlichkeit die seltene Dissonanz‑Klasse vorhersagt.  

- **Vergleich**: PRC übertraf andere gängige AL‑Strategien (z. B. uncertainty‑sampling), wobei die Differenz klein blieb. Random‑Sampling war deutlich schlechter.  

**7. Ergebnisse**  
- Nach mehreren AL‑Runden mit PRC (und ergänzend den besten alternativen Strategien) erreichten wir AUC ≈ 0,75 – die bisher beste Leistung für die Dissonanz‑Detektion.  
- Annotation‑Kosten: PRC generierte die höchste Quote an Dissonanz‑Beispielen, aber Annotatoren empfanden die Beispiele als schwierig.  

**8. Fazit**  
- PRC ist eine einfache, aber effektive AL‑Strategie für seltene Klassen.  
- Ein sorgfältig ausgewählter Transfer‑Learning‑Start (CE → Debate) ermöglicht einen guten Zero‑Shot‑Start.  
- Beim Transfer aus einer anderen Domäne hilft ein iteratives Update; bei domänenspezifischen AL‑Runden ist ein kumulatives Update vorzuziehen.  

**Verfügbarkeit**  
Die Datensätze und der zugehörige Code sind online verfügbar (Links im Papier). Bei Fragen stehen wir gerne zur Verfügung.</sample>
    <sample id="356">Die Autoren arbeiten an folgenden Institutionen:

| Autor           | Universität / Organisation           |
|-----------------|----------------------------------------|
| Alexander Koller | Universität Bonn                       |
| Ivan Titov      | DeepMind (früher Universität Oxford)   |
| Matthias Lindemann | (vermutlich Universität Bonn)           |</sample>
    <sample id="357">Der/die Referent*in heißt **Siyu Yuan**.</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit der **state‑of‑the‑art Architektur, die speziell für simultane Pre‑Translation entwickelt wurde**, verglichen.</sample>
    <sample id="361">**Abstract**  
Wir stellen *CounterComp* vor, eine Methode, die Counterfactual‑Szenarien nutzt, um die kompositorische Generalisierung bei Mehr‑Schritt‑Quantitäts‑Reasoning in Frage‑Antwort‑Aufgaben zu verbessern. Konkret werden Fragen zu Finanztabellen beantwortet, wobei die Antwort aus mehreren arithmetischen Operationen besteht. Bestehende neuronale Modelle lernen häufig spurielle Muster; beispielsweise korreliert die wiederholte Präsenz von Tokens wie „2019“ mit einer bestimmten Operation (z. B. Subtraktion), was zu Fehlern führt. CounterComp minimiert dieses Problem, indem es aus jedem Trainingsbeispiel (Anker) positive und negative Gegenbeispiele generiert: Eine Intervention im Text, die die Ausgabe unverändert lässt, bildet ein positives Paar; eine Intervention, die die Ausgabe verändert, bildet ein negatives Paar. Diese Triplets werden in einen metrischen Lernverlust mit dynamischer Marge eingebettet, die die Ausmaß der Veränderung im Text misst. Durch Hinzufügen dieses Verlustes zu drei State‑of‑the‑Art‑Baselines erzielen wir signifikante Verbesserungen, insbesondere bei mehr als zwei Rechenschritten. Die Methode funktioniert sowohl bei in‑Distribution‑ als auch bei Out‑of‑Distribution‑Tests (kriß‑Datensätze, noch nicht gesehenen Beispielen) und führt zu einer besseren Aufmerksamkeit auf semantisch relevante Tokens. Unsere Ergebnisse zeigen, dass CounterComp die Erklärbarkeit und Robustheit von Modellen für quantitative Frage‑Antwort‑Aufgaben erhöht.</sample>
  </task>
</testset>