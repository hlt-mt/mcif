<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono i **grandi dataset di web‑crawl**.  
- Il più citato è il **C4 Corpus**, un vasto raccolto di contenuti web che comprende grande parte delle **notizie di media tradizionali** (New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc.) e di **social media** (Reddit, Twitter, ecc.).  
- Questi corpora di news e social media costituiscono la base di addestramento di molti modelli come GPT‑4, RoBERTa, BART, ecc., e sono quindi le principali fonti di informazioni politicamente orientate.</sample>
    <sample id="1">Gli autori sono affiliati a McGill University, Mila (l’istituto di ricerca di Montréal) e Microsoft Research.</sample>
    <sample id="2">The presentation introduces **LayoutMask**, a new pre‑trained model for Visually‑Rich Document Understanding (VrDU). Traditional document pre‑training models use a global 1‑D token order (0, 1, 2, …) that does not capture the true reading order of complex layouts. LayoutMask instead employs **local 1‑D positions**—the order of tokens within each text segment—combined with 2‑D spatial coordinates and semantic cues, enabling the model to infer global reading order from local interactions.

Three innovations distinguish LayoutMask:

1. **Masking strategy**:  
   - **Whole‑Word Masking** replaces entire words rather than individual tokens, forcing the model to rely on broader context.  
   - **Layout‑Aware Masking** biases masking toward the first and last words of each segment, encouraging cross‑segment reasoning and better handling of ambiguous layouts.

2. **Pre‑training objectives**:  
   - Standard **Masked Language Modeling (MLM)** augmented by the two masking strategies.  
   - A new **Masked Position Modeling (MPM)** task, where the model predicts the original 2‑D positions of masked words, mirroring a cloze test that blends semantic and spatial inference.

3. **Position encoding**: Local 1‑D positions outperform Global 1‑D on FUNSD and SROIE, and are competitive on CORD. The advantage is most evident for entities like “Total” in receipts, where vertical and horizontal layouts coexist and misleading numbers appear.

Empirical results confirm that local ordering, coupled with layout‑aware masking and MPM, improves text‑layout interactions and yields better document understanding performance. The paper and poster provide further experimental detail.</sample>
    <sample id="3">Ciao! Benvenuti alla presentazione di **DEPLAIN**, un nuovo corpus per l’identificazione del livello di complessità di testi tedeschi sia a livello di documento sia a livello di frase. Mi chiamo Regina Stodden e vi guiderò nella prima parte della presentazione.

---

## Che cos’è la semplificazione del testo?

La **semplificazione del testo** è un processo di adattamento di un testo per migliorarne la comprensione da parte di un gruppo target specifico, ad esempio persone con difficoltà di lettura o non‑madri lingue.  
Per addestrare un modello di semplificazione del testo abbiamo bisogno di coppie parallele di frasi (o documenti), cioè una frase complessa e la sua versione semplificata.

Nell’esempio che vedete, una frase complessa in tedesco è allineata con la sua traduzione in linguaggio semplice. Per semplificarla possono essere usate diverse tecniche, come:

- **sostituzione lessicale** (parole più semplici)
- **eliminazione di clausole** (rimozione di componenti non essenziali)
- **riorganizzazione** (cambiamento dell’ordine delle frasi)
- **inserzione di parole** (aggiunta di termini esplicativi)

---

## Perché abbiamo creato DEPLAIN?

Negli ultimi anni abbiamo riscontrato diversi problemi nei corpora esistenti:

- **Dimensioni ridotte**: i corpora sono troppo piccoli per addestrare modelli di semplificazione robusti.
- **Allineamenti automatici**: i tre corpora più recenti sono stati allineati automaticamente, il che può introdurre errori di allineamento.

Per questo motivo abbiamo creato **DEPLAIN**, diviso in due sotto‑corpora:

| Sotto‑corpus | Origine | Documenti allineati manualmente | Coppie di frasi parallele |
|--------------|---------|----------------------------------|----------------------------|
| **DEPLAIN‑apa** | Testi di notizie | 483 | ~13 000 |
| **DEPLAIN‑web** | Diversi domini | 750 | 30 450 |

*DEPLAIN‑apa* è basato su testi di notizie e comprende 483 documenti allineati manualmente, per un totale di circa 13 000 coppie di frasi.  
*DEPLAIN‑web* comprende documenti di vari domini; abbiamo allineato manualmente 750 documenti e, in aggiunta, abbiamo applicato metodi di allineamento automatico per ottenere 30 450 coppie di frasi.

---

## Analisi delle coppie di frasi

Abbiamo analizzato le coppie di frasi in termini di tipo di semplificazione:

- I testi biblici sono **molto** più semplificati rispetto, ad esempio, ai testi di notizie o ai testi per apprendenti di lingua.
- A livello di **semplificazione lessicale**, **strutturale** e **generale** vediamo differenze marcate.
- **DEPLAIN** presenta un’ampia varietà di trasformazioni di semplificazione: ad esempio, *DEPLAIN‑apa* presenta più riorganizzazioni e aggiunte di parole, mentre *DEPLAIN‑web* mostra più riformulazioni.

---

## Casi d’uso per DEPLAIN

### 1. Valutazione di metodi di allineamento automatico

Negli ultimi anni sono stati sviluppati diversi metodi di allineamento automatico, soprattutto nel contesto delle traduzioni automatiche.  
Nel nostro caso, però, vogliamo allineare frasi di due documenti paralleli **stesso idioma** ma con livelli di complessità differenti.  
Grazie a DEPLAIN, con le sue allineamenti manuali come gold standard, possiamo valutare i metodi di allineamento proposti.

Dopo aver adattato alcuni metodi, abbiamo pubblicato i codici e i risultati nel nostro articolo.  
**Conclusione**: il metodo **MASSalign** è il più performante per l’allineamento di testi tedeschi da semplificare.

---

### 2. Semplificazione automatica del testo

Abbiamo addestrato due modelli di linguaggio:

1. **Long‑mBART** per la semplificazione a livello di documento.
2. **mBART (base)** per la semplificazione a livello di frase.

I checkpoint risultanti sono disponibili e l’articolo fornisce dettagli sui punteggi e sulle metriche di valutazione.  
**Risultato**: la semplice fine‑tuning di questi modelli produce punteggi superiori alla baseline, fornendo un benchmark di riferimento per la semplificazione automatica del testo in futuro.

---

Grazie mille per la vostra attenzione. Speriamo di incontrarvi tutti al convegno!</sample>
    <sample id="4">**Kayo Yin**.</sample>
    <sample id="5">Hanno usato il modello **T5‑XL**.</sample>
    <sample id="6">In questo intervento Jiaan presenta il lavoro “Towards Unifying Multi‑Lingual and Cross‑Lingual Summarization”, realizzato con Fandong, Duo, Yunlong, Zhixu, Jianfeng e Jie. Il contributo principale è la definizione di **many‑to‑many summarization**, un paradigma che unifica la sintesi multilingue (dove testo e sommario sono nella stessa lingua) e la sintesi cross‑lingua (testo in una lingua, sommario in un’altra). In this setting, un unico modello è in grado di ricevere un documento in qualsiasi lingua di origine e generare un sommario in qualunque lingua di destinazione, sfruttando così una trasferibilità del sapere più ampia.

Per dimostrare il valore di questo approccio, è stato condotto un esperimento preliminare sul dataset WikiLingua (lingue: English, French, Hindi, Chinese, Thai, Turkish). Sono stati addestrati quattro modelli basati su mBART‑50: (1) “mBART ONE” – modelli separati per ciascuna direzione; (2) “mBART U‑CLS” – modello unificato con tutti i campioni cross‑lingua; (3) “mBART MLS” – modello unificato con campioni monolingui; (4) “mBART Many‑to‑Many” – modello addestrato e testato in tutte le direzioni. I risultati mostrano che il modello many‑to‑many trasferisce meglio le competenze di sintesi tra lingue rispetto alle altre configurazioni.

Inoltre, è stato introdotto **PISCES**, un modello pre‑addestrato many‑to‑many che utilizza un piano di tre fasi: (i) meta‑pre‑training (generazione di frasi originali da versioni rumorose), (ii) pre‑training cross‑lingua (generazione di frasi target da coppie parallele rumorose), e (iii) pre‑training specifico alla task (creazione di campioni pseudo many‑to‑many). PISCES supera i baseline come mBART‑50 e mT5, e i risultati sono confermati anche da studi di ablation e valutazioni umane.</sample>
    <sample id="7">I tagger basati sul dataset CoNLL‑2003 continuano a funzionare bene nel 2023, ma il loro successo dipende da tre fattori chiave:  
1. **Architettura** – i modelli Transformer (es. BERT, RoBERTa, XLM‑R) mostrano la migliore generalizzazione.  
2. **Dimensione del modello** – versioni più grandi tendono a mantenere una precisione più alta sui dati moderni.  
3. **Numero di esempi di fine‑tuning** – più esempi di addestramento con annotazioni recenti aumentano la robustezza.

Il principale motivo di calo di performance nei vecchi tagger è lo **spostamento temporale** tra i dati di addestramento (anni 2000‑2003) e i testi correnti (2020‑2023). L’overfitting adattivo, spesso temuto, non si è dimostrato un fattore significativo.  

In sintesi: **sì, i tagger CoNLL‑2003 «vanno ancora bene» se si utilizza un’architettura Transformer di dimensioni adeguate e si fine‑tune con esempi recenti; altrimenti la loro efficacia decresce a causa del drift temporale.**</sample>
    <sample id="8">The novelty is that instead of asking judges to give overall ratings or pairwise comparisons, ABC‑Eval asks them to **explicitly annotate each turn for concrete behaviors** (e.g., irrelevance, self‑contradiction, hallucination, commons‑sense violations, empathy). This turn‑level behavior labeling reduces subjectivity, yields higher inter‑annotator agreement, and better predicts overall conversation quality than traditional Likert or pairwise methods.</sample>
    <sample id="9">Il successo dei metodi di weakly‑supervised learning attuali dipende in gran parte dalla disponibilità di **dati di validazione puliti**.  
Questi approcci si affidano a campioni con etichette corrette per la selezione del modello, il tuning degli iperparametri e, in molti casi, anche per un fine‑tuning finale. Senza un set di validazione “clean”, le performance calano drasticamente, e il vantaggio dichiarato dai metodi complessi si riduce a quello di un semplice fine‑tuning su pochi esempi annotati.</sample>
    <sample id="10">Per aumentare l’accuratezza nella risoluzione delle espressioni indirette si possono:

1. **Migliorare il recupero delle informazioni**: integrare sistemi di retrieval più precisi (es. embedding‑based search) per fornire al modello dati più pertinenti e completi sul contesto.

2. **Arricchire il modello con conoscenza strutturata**: usare grafi di conoscenza (DBpedia, Wikidata) o embeddings semantici per collegare le entità a proprietà rilevanti (genere, anno, protagonisti).

3. **Addestramento multi‑modal**: combinare testo con immagini (per ricette) o clip audio (per canzoni) per aiutare il modello a distinguere entità con caratteristiche visive o sonore.

4. **Prompting contestuale avanzato**: disporre prompt che includano “prompt chain” o “meta‑prompt” che guidino il modello a fare inferenze basate su attributi specifici.

5. **Fine‑tuning su sotto‑task mirati**: addestrare modelli su sotto‑domini (musica, libri, ricette) con task di distinzione basati su attributi specifici (es. “il più recente”, “non energico”).

6. **Espansione e diversificazione del dataset**: aumentare la varietà di riferimenti indiretti, includere più domini e scenari conversazionali complessi per ridurre l’overfitting.

7. **Validazione iterativa**: utilizzare il feedback umano per correggere errori comuni, migliorare le annotazioni e raffinare i modelli.

Implementando queste strategie si dovrebbe notare un aumento sostanziale delle prestazioni oltre i 90 % di accuratezza.</sample>
    <sample id="11">Lo studio di Jack Hessel e colleghi di AI2, UofU, Cornell, UW, Air Mail e OpenAI presenta un nuovo benchmark per valutare la comprensione dell’umorismo da parte dei modelli di linguaggio. Il corpus si basa sul “New Yorker Caption Contest”, che raccoglie migliaia di cartoon senza didascalie e le relative caption votate dai lettori e dagli editori. I ricercatori hanno annotato oltre 700 disegni con localizzazioni, descrizioni, highlight e link a entità, e raccolto 650 spiegazioni di 2‑4 frasi per i migliori caption.

Il benchmark comprende tre task:
1. **Matching** – il modello deve scegliere la caption corretta tra cinque opzioni. Il miglior modello CLIP fine‑tuned ottiene ~62 % (baseline random 20 %), mentre gli umani raggiungono ~94 %.  
2. **Quality Ranking** – confrontare due caption reali e indicare quella di qualità superiore. Anche qui si osserva un grande divario a favore degli umani.  
3. **Explanation Generation** – generare una breve spiegazione del perché il caption è divertente. GPT‑4, pur mostrando capacità di spiegare, commette errori frequenti e viene superato dagli umani in più di due terzi delle valutazioni.

I risultati evidenziano la notevole lacuna tra modelli di linguaggio e comprensione umana dell’umorismo. Il dataset, le annotazioni e una leaderboard sono disponibili online, invitando la comunità a testare nuovi approcci.</sample>
    <sample id="12">Cinque autori sono coinvolti nell’articolo.</sample>
    <sample id="13">Daniel Rotem presented “Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low‑Resource Settings,” a study from Professor Roy Schwartz’s lab at Hebrew University. Adaptive inference reduces inference latency by exploiting input difficulty: easier samples are handled by smaller models, while harder ones use larger ones. Two mainstream approaches exist: **Multi‑Model** (separate models with a final classifier deciding when to stop) and **Early‑Exit** (a single model with auxiliary classifiers after intermediate layers). Rotem highlighted pros and cons: Multi‑Model is versatile but memory‑heavy and incurs overhead because all preceding models run regardless of the stopping point; Early‑Exit is memory‑efficient and incurs no overhead but shares parameters across classifiers, potentially harming performance.

The team hypothesized that **conflicting gradients**—different classifiers pushing model weights in opposing directions—degrade Early‑Exit performance. Experiments on BERT‑Base and BERT‑Large confirmed that Multi‑Model classifiers consistently outperformed Early‑Exit ones, especially for the earliest exits (up to 5.2% accuracy gap). Speed‑accuracy trade‑offs showed Multi‑Model dominating at high speeds, but Early‑Exit catching up with deeper classifiers due to lower overhead.

To mitigate gradient conflict, the authors proposed **SWEET** (Separating Weights in Early‑Exit Transformers). SWEET trains each transformer layer only with gradient signals from the downstream classifier, thus isolating updates. Results demonstrated that SWEET closes most of the accuracy gap with Multi‑Model, sometimes even outperforming it across the entire speed‑accuracy curve, particularly for BERT‑Large. The work establishes the existence of gradient conflict, offers a fair comparative benchmark, and introduces a promising fine‑tuning strategy for adaptive inference.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questa presentazione riguarda la struttura di dipendenza della coordinazione. Come forse sapete, esistono diverse strutture di dipendenza assunte da varie teorie e approcci basati sui corpora. Per esempio, nella dipendenza universale (Universal Dependencies) la struttura di coordinazione *Lisa, Bart e Maggie* è così che il primo congiunto è la testa dell’intera struttura coordinata. In questo caso la testa è **Lisa**. Un approccio simile è adottato dalla teoria del testo di significato di Igor Mel’čuk, dove di nuovo l’intera struttura coordinata è guidata dal primo congiunto. Quindi queste due teorie sono **asimmetriche**: singolano uno dei congiunti.

Al contrario, la prospettiva di Prag (Prague) assume la **congiunzione** come testa della struttura coordinata. In questo modello si ottengono dipendenze che vanno dall’inizio alla fine della coordinazione. Infine, esiste un **approccio multi‑testa**, come quello usato nella *Hudson’s Word Grammar*, dove tutti i congiunti sono teste della struttura coordinata. In questo caso le dipendenze vanno dal governatore a ciascun congiunto (per esempio *loves* → *Lisa*, *loves* → *Bart*, *loves* → *Maggie*).

L’obiettivo di questo lavoro è di presentare un nuovo argomento a favore delle strutture **simmetriche** di coordinazione (come le prime due) e contro le strutture **asimmetriche** (come le ultime due). L’argomento si basa sul principio della **minimizzazione della lunghezza delle dipendenze** (dependency length minimization, DLM), che spiegherò con alcuni esempi.

---

### 1. Minore lunghezza delle dipendenze

In inglese, per esempio, gli oggetti diretti tendono a essere vicini al verbo, mentre gli avverbiali (ad esempio avverbi di tempo) possono essere più lontani:

- **“Marge read it yesterday.”**  
  E' accettabile perché l’oggetto diretto (*it*) è vicino al verbo (*read*).

- **“Marge read yesterday it.”**  
  È molto peggiore, perché tra verbo e oggetto diretto c’è l’avverbio di tempo (*yesterday*).

Tuttavia, l’effetto può essere attenuato quando l’oggetto è molto lungo e pesante. In tal caso può essere spostato dopo l’avverbio:

- **“Marge read this absolutely fascinating book about bees yesterday.”**  
- **“Marge read yesterday this absolutely fascinating book about bees.”**

Entrambe le versioni sono accettabili. Il motivo è che, pur violando la regola generale “oggetto diretto vicino al verbo”, la seconda struttura soddisfa la DLM: le dipendenze risultano più brevi.

Per chiarire, consideriamo due alberi di dipendenza:

|  | “Marge read it yesterday.” | “Marge read yesterday it.” |
|---|---|---|
| Dipendenza *read → yesterday* | 7 parole | 1 parola |
| Dipendenza *read → it* | 1 parola | 4 parole |
| Lunghezza totale | 8 | 5 |

La somma delle lunghezze è 8 in prima frase e 5 nella seconda, quindi la seconda è più “leggera” dal punto di vista DLM.

---

### 2. Statistiche sulla coordinazione nel Penn Treebank

Abbiamo estratto varie statistiche sulla coordinazione dal *Penn Treebank* (versione enhanced) e le abbiamo confrontate con i risultati di “Why wouldn’t you use universal dependencies”. Queste statistiche confermano l’osservazione nota da tempo: **i congiunti sinistri tendono ad essere più brevi**. Per esempio, si preferisce *salt and pepper* a *pepper and salt* (misurato in sillabe). Inoltre, questa tendenza cresce con la differenza di lunghezza tra i due congiunti: più grande è la differenza, più forte è la preferenza per un congiunto sinistro più corto.

**Novità del nostro studio**: abbiamo scoperto che questa tendenza si verifica **solo quando il governatore si trova a sinistra o è assente**.

- **Governatore a sinistra**: *I saw Bart and Lisa* (governatore *saw* a sinistra).  
  Qui il congiunto sinistro tende a essere più corto.

- **Governatore assente**: *Homer came and sneezed.* (coordinazione di due verbi senza governatore esterno).  
  Ancora una volta, il congiunto sinistro è più corto.

- **Governatore a destra**: *Ted and Ned laughed.* (governatore *laughed* a destra).  
  In questo caso la tendenza scompare.

Abbiamo misurato la lunghezza in caratteri, sillabe e parole (concentriamoci sull’ultima). Il grafico (non mostrato qui) mostra che quando il governatore è a sinistra, la differenza assoluta di lunghezza tra i congiunti è in aumento, così cresce la preferenza per il congiunto sinistro più corto. Quando il governatore è a destra, questa preferenza scompare.

---

### 3. Implicazioni per la teoria della coordinazione

Questi risultati forniscono un forte **argomento a favore delle strutture simmetriche** (dove i congiunti non hanno un “testa” privilegiata) e un **argomento contro le strutture asimmetriche** (dove un congiunto è la testa). In particolare, la minimizzazione della lunghezza delle dipendenze spiega perché la struttura con il governatore a sinistra (o assente) favorisca un congiunto sinistro più corto: la dipendenza tra governatore e congiunto è più breve se il congiunto è vicino al governatore.

---

### 4. Conclusioni

In sintesi, l’analisi basata sulla DLM e le statistiche empiriche suggeriscono che la struttura coordinata è più coerente con un modello **simmetrico**. Vi invito a leggere il paper per i dettagli completi e a discutere con noi alla sessione poster. Grazie!</sample>
    <sample id="15">Tre autori: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">Istituzioni bibliche: i testi sacri, in particolare le versioni semplificate della Bibbia, risultano i più semplificati rispetto a altri domini come le notizie (DEPLAIN‑apa) o i materiali per apprendimento della lingua (DEPLAIN‑web).</sample>
    <sample id="17">Il lavoro di Shengqiong Wu e colleghi si occupa di *multimodal relation extraction* (MRE), ossia l’identificazione delle relazioni semantiche tra entità quando il contesto testuale è arricchito da informazioni visive tipiche dei social media. L’idea principale è che, oltre ai modelli testuali tradizionali, l’integrazione di elementi visivi può fornire prove concrete (es. “Bachelor”, “Gown”, “Cap” per dedurre che JFK è “graduated at” Harvard). Tuttavia, l’uso indiscriminato delle informazioni interne (tutto il testo e l’immagine) può portare a sovra-utilizzo di dettagli non rilevanti, mentre l’uso limitato delle informazioni esterne può far perdere contesto utile. Per questo si propone un approccio in due fasi:

1. **Pruning interno**: costruzione di un *Unified Cross‑Modal Graph* (CMG) combinando scene graph testuali e visivi, poi filtraggio fine‑grained di nodi e archi guidato dal principio del *Graph Information Bottleneck* (GIB), in modo da mantenere solo le componenti più informative e ridurre rumore.

2. **Compensazione esterna**: modellazione di un *Latent Multimodal Topic Model* (LAMO) che genera parole chiave tematiche sia testuali sia visive (top‑L). Un’attenzione integra queste parole nel CMG, arricchendo il contesto.

Gli esperimenti su un dataset MRE dimostrano che il metodo supera i baselines multimodali e l’analisi di ablation evidenzia l’importanza sia del pruning che della compensazione. Inoltre, l’analisi basata sulla rilevanza testo‑visione mostra che il pruning è più utile quando la correlazione è alta, mentre la compensazione è più preziosa in presenza di bassa rilevanza. In sintesi, il lavoro introduce un nuovo paradigma di “sottrazione interna + aggiunta esterna” per migliorare l’extraction di relazioni in contesti multimodali.</sample>
    <sample id="18">L’esempio più citato è la coppia **“salt and pepper”** (dove il congiunto sinistro “salt” è più corto) rispetto a **“pepper and salt”**. Un altro esempio tipico è **“I saw Bart and Lisa”**: il congiunto sinistro “Bart” è più breve di “Lisa”. In entrambi i casi si osserva la preferenza per congiunti più brevi a sinistra quando non vi è un governatore a destra.</sample>
    <sample id="19">**Summary (≈200 words)**  

Zhang Qin presents a survey on efficient open‑domain question answering (ODQA), focusing on the dominant two‑stage framework introduced by Danqi Chen (2017). In this paradigm, a retrieval module first fetches evidence snippets from a massive Wikipedia corpus (≈26 M documents, 20 GB) by encoding the query and scanning a pre‑indexed document embedding file (≈65 GB). A reader then parses the retrieved passages to extract or generate the answer. Qin highlights three major bottlenecks: the sheer size of the corpus and index, the slow search latency of the index, and the gigantic size of contemporary language models (millions of parameters), all of which hinder real‑time deployment on resource‑constrained devices.

The talk surveys techniques that reduce memory, speed up inference, and maintain competitive accuracy. One‑stage models (retrieval‑only and generator‑only) eliminate either the reader or the index. Efficient tactics include approximate nearest‑neighbor (ANN) search, adaptive skip reading, document filtering, dimensionality reduction, and product quantization to shrink the index. Model‑side optimisations involve lightweight backbones, parameter sharing, and knowledge distillation to reduce LM size, even enabling a single model to handle both retrieval and reading.

Comparative analysis shows retrieval‑reader systems strike a balance among speed, memory, and performance; retrieval‑only systems are fastest but require large indexes; generator‑only systems avoid indexes but suffer from large models and lower accuracy. Qin concludes that resource‑tight scenarios may favor generator‑only or compressed retrieval, while real‑time use cases benefit from retrieval‑only. Future work should target low‑power deployment and richer evaluation metrics.</sample>
    <sample id="20">Sì, i modelli sono **liberamente disponibili** su Hugging Face con licenza MIT, quindi puoi usarli per la tua ricerca (basta rispettare i termini della licenza MIT).</sample>
    <sample id="21">DEplain‑apa contiene **testi di notizie (news texts)**.</sample>
    <sample id="22">I fattori chiave per una buona generalizzazione, secondo lo studio, sono:

1. **Architettura del modello** – i transformer tendono a generalizzare meglio.  
2. **Dimensione del modello** – modelli più grandi di solito offrono prestazioni più stabili su dati nuovi.  
3. **Numero di esempi di fine‑tuning** – più dati di addestramento fine‑tuning si traducono in una maggiore capacità di generalizzare.</sample>
    <sample id="23">Dan Garrette presenta i risultati di un lavoro volto a migliorare la capacità dei modelli di generazione di immagini di riprodurre testo visivo. I modelli recenti, come Imagen, combinano un encoder T5‑XXL con un modello di diffusione: l’encoder trasforma il testo in un vettore che poi guida la generazione dell’immagine. Tuttavia, questi sistemi falliscono spesso a produrre parole corrette. Il problema principale è che T5 utilizza la tokenizzazione SentencePiece, che suddivide il testo in sub‑parole. Quando un modello deve “scrivere” una parola, deve quindi decomporre il token sub‑parola in singole lettere, un compito che T5 fatica a eseguire. I test mostrano che le versioni Base e Large di T5 hanno meno del 20 % di accuratezza nella trascrizione, mentre la versione XXL rimane sotto il 70 %. Anche i modelli PaLM, più grandi e addestrati su più dati, raggiungono quasi il 100 % di accuratezza, ma sono impraticabili per molte applicazioni.

Confrontando con ByT5, che riceve i singoli byte anziché sub‑parole, si osserva una performance notevolmente superiore: tutti i modelli ByT5, indipendentemente dalla scala, mantengono una buona accuratezza di spelling, poiché non devono fare delle ricombinazioni complesse. Analizzando per frequenza delle parole, si nota che i termini più frequenti sono i più difficili per T5, mentre ByT5 è immune a questa dipendenza.

Per sfruttare queste intuizioni, l’équipe ha aggiunto o concatenato all’encoder di Imagen un ulteriore rappresentazione di testo proveniente da un modello ByT5‑small (solo +5 % di parametri). Questo piccolo accento consente al modello di “sapere” l’ortografia corretta, e porta a una miglior resa delle parole nell’immagine, sebbene la diffusione possa comunque introdurre errori.

I principali contributi del lavoro sono: 1) il benchmark WikiSpell per valutare la sola capacità di spelling dei modelli; 2) il benchmark DrawText per testare la generazione di testo nelle immagini; 3) la nuova strategia efficiente di concatenazione di un modello carattere‑consapevole (ByT5) per migliorare la resa testuale senza aumentare eccessivamente la complessità.</sample>
    <sample id="24">La tendenza è stata valutata analizzando il Penn Treebank Enhanced: si è misurata la lunghezza di ciascun congiunto in caratteri, sillabe e parole, calcolato la differenza tra le lunghezze e poi contato la percentuale di casi in cui il congiunto di sinistra è più corto. L’analisi è stata condotta in funzione della differenza di lunghezza e della posizione del governatore (sinistra, destra o assente).</sample>
    <sample id="25">Gli esperimenti sono stati realizzati con un approccio empirico basato sul corpus enhanced Penn Treebank.  
Per ogni coordinazione presente nel corpus è stato:

1. **Identificato il governatore** dell’intera struttura coordinata (se presente).  
   - *Governatore a sinistra*: il nodo che governa la coordinazione si trova prima delle congiunzioni.  
   - *Governatore a destra*: il nodo che governa la coordinazione si trova dopo le congiunzioni.  
   - *Governatore assente*: la coordinazione è indipendente da un nodo esterno (es. coordinazione di frasi o di verbi).

2. **Misurata la lunghezza dei due congiunti** in tre modalità:  
   - numero di parole,  
   - numero di sillabe,  
   - numero di caratteri.

3. **Calcolata la differenza di lunghezza** tra i due congiunti.

4. **Contabilizzata la frequenza con cui il congiunto più corto appare a sinistra** per ciascuna categoria di governatore e per ciascuna misura di lunghezza.

5. **Tracciati grafici** che mostrano come la proporzione di congiunto corto a sinistra varia con l’ampiezza della differenza di lunghezza, confrontando i tre casi di governatore.

In sintesi, l’esperimento ha confrontato l’ordine dei congiunti in funzione della posizione del governatore, utilizzando statistiche di lunghezza derivate dal corpus enhanced Penn Treebank. Questa analisi ha permesso di evidenziare che l’effetto “congiunto corto a sinistra” è presente solo quando il governatore è a sinistra o assente, ma scompare quando il governatore è a destra.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati è quasi a caso: nel vostro esperimento, con solo 43 esempi di dissonanza, la performance non superava il livello di chance.</sample>
    <sample id="27">Il testo menziona solo un autore: Shangbin.  

**Numero di autori coinvolti: 1**</sample>
    <sample id="28">Bob e Alice.</sample>
    <sample id="29">I modelli di MT sensibili al contesto mostrano un miglioramento significativo rispetto a quelli senza contesto soprattutto per i fenomeni di **formalità** e **cohesione lessicale** (cioè la scelta di traduzioni coerenti all’interno dello stesso documento). Per gli altri fenomeni – ellissi, pronomi e forma verbale – la differenza di performance è marginale.</sample>
    <sample id="30">Il lavoro “LLM‑Blender” presenta un framework semplice ma efficace per l’ensembling di modelli di linguaggio di grandi dimensioni (LLM). Il problema principale è che, pur essendo alcuni LLM i migliori in media, la loro efficacia varia notevolmente a seconda del singolo input: ad esempio, Vicuna, pur ottenendo la migliore media su 11 modelli, è il più performante in solo il 21 % dei casi.  
Il framework prevede due fasi. Nella prima, per un dato input \(X\) si eseguono \(n\) LLM diversi che generano output \(Y_1,\dots,Y_n\). Un modulo chiamato **PairRanker** confronta tutte le coppie di candidati \(Y_i, Y_j\) concatenandoli con \(X\) e utilizzando un encoder (es. RoBERTa) con cross‑attention per decidere quale candidato è migliore. Il risultato è una matrice di logit di confronto; la classifica finale si ottiene aggregando i logit (la strategia “max logits” funziona meglio, ma è possibile usare anche bubble‑sort per efficienza).  
Nella seconda fase, i primi \(K\) (di solito 3) candidati selezionati vengono passati a un modello seq2seq di fusione (GenFuser) che produce l’output finale, fondendo le migliori risposte.  
Per valutare l’approccio è stato creato il dataset **MixInstruct**, che raccoglie istruzioni provenienti da diversi set e le risposte di 11 LLM open‑source. Le metriche automatiche (BERTScore, BLUERT, BARTScore) e il giudizio di ChatGPT mostrano che PairRanker e Blender superano le singole migliori LLM (Open Assistant e Vicuna) in oltre il 70 % dei casi.  
In sintesi, LLM‑Blender combina un ranking pairwise accurato con una fusione generativa per migliorare significativamente la qualità delle risposte, fornendo al contempo un nuovo dataset di valutazione.</sample>
    <sample id="31">Non sono state menzionate le affiliazioni istituzionali degli autori nel testo fornito.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando **le etichette fornite da gruppi demografici diversi** con le etichette (o le previsioni) dei dataset e dei modelli stessi.  

1. **Re‑annotazione**: ogni istanza è annotata da un gran numero di annotatori con dati demografici completi (lingua, nazionalità, genere, istruzione, ecc.).  
2. **Aggregazione per gruppo**: per ogni gruppo demografico si calcola la media delle sue etichette.  
3. **Correlazione**: si misura la correlazione di Pearson \(R\) tra questa media e le etichette del dataset/ del modello.  
4. **Interpretazione**: un valore alto indica che il dataset o il modello è “allineato” con quel gruppo; un valore basso indica una minore rappresentazione di quel gruppo.

Così la posizionalità è quantificata come la serie di coefficienti di correlazione che mostrano quanto ciascun modello o dataset rispecchi le opinioni di ciascun gruppo demografico.</sample>
    <sample id="34">Il lavoro presentato, “CREST: A Joint Framework for Rationalization and Counterfactual Text Generation”, combina due approcci interpretativi del NLP: la **rationalizzazione selettiva** e la **generazione di counterfactuals**.  

Il modello CREST è composto da due moduli principali.  
1. **Rationalizer**: un masker addestrato a produrre una **rationale** (Z) rilevante per la decisione del classificatore.  
2. **Editor**: un modello di linguaggio mascherato (MLM) che, avendo la rationale come input e un’etichetta “gold” preceduta, riempie i token mascherati per creare un **counterfactual** (X̃) e una nuova rationale (Z̃).  

Per valutarlo, gli autori hanno confrontato CREST con metodi esistenti (MiCE, manual counterfactuals) su IMDB e SNLI. Gli esperimenti di valutazione umana, basati su scala Likert a 5 punti, hanno mostrato che i counterfactuals di CREST sono più validi e naturali di MiCE, anche se leggermente inferiori a quelli generati a mano.  

CREST è ulteriormente sfruttato per l’**augmentazione dei dati** e per il **rationalizzazione con dati factuali e counterfactual**. In quest’ultimo, due flussi (factual e counterfactual) condividono lo stesso rationalizer, e una regolarizzazione incoraggia le rationale a corrispondere a quelle generate da CREST‑Generation.  

I risultati su IMDB dimostrano che CREST‑Rationalization ottiene la migliore accuratezza su dati in‑domain, competere con l’augmentazione con counterfactuals umani sui contrastive, e superare gli altri metodi su set out‑of‑domain.  

Infine, è stato introdotto il nuovo metric **counterfactual simulability**, che misura la capacità di una rationale di cambiare la decisione del classificatore quando viene applicata un edit guidato da essa. CREST‑Rationalization mostra la più alta plausibilità e simulabilità counterfactual, confermando la sua interpretabilità.  

Il paper e il codice sono disponibili per approfondimenti.</sample>
    <sample id="36">**Abstract**

La traduzione multilingue rappresenta una soluzione scalabile e veloce rispetto ai modelli monolingua, ma comporta una capacità limitata per ogni lingua e può richiedere modelli di dimensioni crescenti, con conseguente rallentamento delle prestazioni di inferenza. In questo lavoro proponiamo le **Language‑Specific Layers (LSLs)**, un’architettura che aumenta la capacità dedicata a ciascuna lingua senza aumentare i costi di inferenza. Ogni layer del modello assume la forma di tre componenti: condivisa, specifica al linguaggio sorgente e specifica al linguaggio di destinazione. Durante l’addestramento, tutte e tre le componenti sono attivate, consentendo al modello di apprendere quale strato è più rilevante per ogni livello dell’encoder. Successivamente, analizziamo i pesi risultanti e, per ogni livello, selezioniamo la componente con il peso massimo; questa procedura determina in modo automatico la posizione ottimale delle LSLs. 

I nostri esperimenti su WMT21 News, con 10 lingue (incluso lo Swahili, una lingua a risorse limitate), mostrano che l’architettura in verde (con LSLs) supera sia il modello di base (simile a un traduttore tradizionale) sia i linguaggi‑adapters, ottenendo miglioramenti medi di 2‑3 punti in chrF, spBLEU e COMET su 90 direzioni di traduzione. L’efficacia è particolarmente marcata per le lingue a bassa risorsa, con risultati statisticamente significativi in 84 delle 90 direzioni testate. L’inferenza rimane comparabile a quella del modello di base, confermando la scalabilità della proposta.</sample>
    <sample id="37">The prior study found that when human participants were given the same persona prompts, they too produced stereotypical language, revealing racial stereotypes in their responses.</sample>
    <sample id="38">Il lavoro si è basato principalmente sul **Penn Treebank (PTB) con le annotazioni “enhanced”**.  
In particolare, sono stati estratti e analizzati i dati di coordinazione presenti nella versione “enhanced” del PTB (ad esempio PTB‑3 con le dipendenze arricchite) per calcolare le lunghezze, le frequenze e le preferenze sintattiche dei congiunti.  

Eventuali riferimenti a “Why wouldn’t you use Universal Dependencies” indicano solo la comparazione con le annotazioni UD, ma i dati empirici provengono esclusivamente dal PTB enhanced.</sample>
    <sample id="39">Un autore (Adam Przepiórkowski).</sample>
    <sample id="40">**Attività strettamente correlate alla dissonanza cognitiva**

| Attività | Descrizione |
|----------|-------------|
| **Annotazione di relazioni di dissonanza** | Selezione di coppie di unità di discorso (ad es. tweet) che esprimono credenze e azioni incoerenti, usando il parser PDTB e linee guida specifiche. |
| **Classificazione di stance (debate)** | Determinazione di accordo/disaccordo tra due dichiarazioni di persone diverse, indipendente dal tema – task “debate” che è correlato alla dissonanza. |
| **Classificazione di relazioni di espansione/comparazione (PDTB CE)** | Estrazione delle classi “expansion” e “comparison” del PDTB, che riflettono concetti di consonanza/dissonanza. |
| **Transfer Learning dai task correlati** | Uso di pesi pre‑addestrati da “debate” e “CE” per migliorare la zero‑shot performance sul compito di dissonanza. |
| **Active Learning con PRC (Probability‑of‑Rare‑Class)** | Selezione di esempi con alta probabilità di appartenere alla classe rara (dissonanza) per ridurre i costi di annotazione e aumentare l’efficacia. |
| **Aggiornamento cumulativo vs iterativo** | Strategie di fine‑tuning: “cumulative” (allenamento su tutti i dati raccolti finora) o “iterative” (allenamento solo sull’ultimo batch). |
| **Valutazione di costo/qualità dell’annotazione** | Analisi dell’efficacia delle strategie di acquisizione in termini di percentuale di dissonanza, difficoltà per gli annotatori e costi. |

Queste attività costituiscono l’intero pipeline di ricerca: dal rilevamento di esempi di dissonanza nel testo, alla costruzione di un dataset, al training di modelli tramite transfer learning e active learning, fino alla valutazione pratica delle strategie di acquisizione di dati.</sample>
    <sample id="41">PeaCoK è un grafo di conoscenza comune basato sulla persona, sviluppato in collaborazione con Sony per migliorare la coerenza e l’engagement delle narrazioni generate automaticamente. Il grafo contiene circa 3 800 persone, 40 000 attributi distintivi e 100 000 inferenze, con più di 9 200 attributi collegati a più di due persone, creando un ricco intreccio di relazioni. Le relazioni sono modellate su tre dimensioni (tipo, interattività, distintività) e quattro tipi principali di legami. Per costruirlo, sono stati: 1) estratti personaggi da grafi di conoscenza esistenti; 2) indotti attributi da fonti di conoscenza comune e modelli linguistici pre‑addestrati; 3) validati tramite una procedura di voto di maggioranza con supporto IA (InstructGPT‑3), ottenendo un F1 medio del 87 %.  

PeaCoK è stato impiegato per addestrare un generatore BART (Comet‑BART) su un compito di inferenza di attributi di persona, superando modelli di grandi dimensioni (GPT‑3 5‑shot e GPT‑3.5 zero‑shot) sia in metriche automatiche sia in valutazioni umane. Inoltre, è stato usato come knowledge linker in una task di dialogo persona‑grounded (ConvAI2) con il modello P²Bot. L’augmentazione con PeaCoK ha portato a dialoghi più fluidi, coerenti, coinvolgenti e aderenti alle personalità, superando l’augmentazione con Atomic2020. L’efficacia migliora con l’aumentare degli attributi condivisi tra i parlanti, evidenziando l’importanza delle connessioni inter‑persona. In sintesi, PeaCoK fornisce una base di conoscenza persona di alto livello che potenzia l’apprendimento di modelli leggeri e la generazione narrativa più coerente e coinvolgente.</sample>
    <sample id="42">I’m sorry, but the presentation text you provided doesn’t list the authors of the paper, so I can’t determine how many authors are involved.</sample>
    <sample id="43">Il numero esatto di autori non è indicato nel testo fornito; si parla di “il nostro lavoro” e di “un team”, ma il conteggio preciso non viene specificato.</sample>
    <sample id="44">Il framework NLPositionality si differenzia dai lavori precedenti in due modi chiave:

1. **Confronto tra utenti reali e modelli/dataset** – invece di limitarsi all’analisi delle discordanze tra annotatori o alla definizione teorica di “positionalità”, il framework confronta direttamente le etichette elaborate da annotatori diversificati (diversi per nazionalità, istruzione, genere, ecc.) con le previsioni dei dataset e dei modelli esistenti.

2. **Annotazioni rielaborate con ampia demografia** – i dati originali sono rielaborati con un gran numero di annotatori globali (oltre 1000 persone da 87 paesi) tramite la piattaforma Lab in the Wild, raccogliendo simultaneamente informazioni demografiche. Successivamente si calcola la correlazione di Pearson tra queste annotazioni demografiche e le risposte dei modelli, fornendo così una misura quantitativa di “positionalità” che non è stata esplorata in precedenti studi.</sample>
    <sample id="45">The **generated personas** (the GPT‑4–created descriptions) overlap the most with the stereotype lexicon – they contain significantly more lexicon words than the human‑written personas.</sample>
    <sample id="46">I sistemi commerciali comparati sono **DeepL** e **Google Translate**.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all’Università di Washington. Oggi presenterò il nostro lavoro “From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models”. I modelli di linguaggio vengono addestrati su enormi dataset web‑crawl. I media di notizie politiche sono coperti in maniera consistente nel loro corpus di pre‑addestramento. Secondo un’indagine sul C4 Corpus, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ampiamente rappresentati nei dati di addestramento dei modelli di linguaggio. Questo ha creato una doppia faccia della moneta per le applicazioni dei modelli di linguaggio. Da un lato, hanno avuto la possibilità di apprendere da prospettive diverse, celebrando così la democrazia e la pluralità di idee. Dall’altro, queste diverse opinioni politiche sono intrinsecamente socialmente distorte e potrebbero portare a potenziali problemi di equità nelle applicazioni di NLP. A questo proposito, proponiamo di indagare il pipeline di propagazione dei bias politici dal pre‑addestramento dei dati ai modelli di linguaggio e ai compiti downstream, ponendo specificamente le seguenti domande: innanzitutto, come possiamo valutare l’inclinazione politica dei modelli di linguaggio e quale ruolo può giocare il pre‑addestramento nel generare tali bias politici? In secondo luogo, come si comportano i modelli di linguaggio con diverse inclinazioni politiche effettivamente sui compiti downstream e se ciò possa comportare problemi di equità nelle applicazioni NLP? In particolare, abbiamo proposto di stimolare i modelli di linguaggio con diverse forme di prompt usando questionari politici come il “political conference test”. Questo garantisce una valutazione automatica ben radicata nella letteratura di scienze politiche. Alcuni risultati preliminari dimostrano che, in primo luogo, i modelli di linguaggio presentano inclinazioni politiche variabili. Occupano tutti e quattro i quadranti dello spettro politico. Si può anche osservare che GPT‑4 è il modello di linguaggio più liberale di tutti, e le serie GPT sono generalmente più socialmente liberali rispetto alle serie BART e alle loro varianti. In secondo luogo, ci proponiamo di indagare in che misura i bias politici dei modelli di linguaggio vengono effettivamente assorbiti dai dati di addestramento. Possiamo quindi condurre un esperimento controllato addestrando ulteriormente i checkpoint dei modelli di linguaggio su sei diversi corpora partizani divisi in notizie e social media, a loro volta separati per inclinazione politica. Addestrando ulteriormente i modelli di linguaggio su tali corpora partizani, possiamo vedere che le coordinate ideologiche del modello di linguaggio si spostano conseguentemente. Per esempio, per RoBERTa addestrato ulteriormente su un corpus Reddit a sinistra si osserva un notevole spostamento verso il liberalismo in termini di bias politici. Inoltre, proviamo a indagare se i modelli di linguaggio possono cogliere la polarizzazione prevalente nella nostra società moderna. Dividiamo i corpora di pre‑addestramento in due periodi: prima del 45° presidente degli Stati Uniti e dopo il 45° presidente. Addestriamo separatamente i modelli di linguaggio su questi due diversi corpora temporali. Si osserva che i modelli di linguaggio tendono ad avere un’inclinazione politica più distante dal centro dopo il 2017. Ciò indica che i modelli di linguaggio possono anche cogliere la polarizzazione nella nostra società. Infine, valutiamo i modelli di linguaggio con diverse inclinazioni politiche su compiti di rilevamento di linguaggio di odio e di notizie false, compiti NLP che spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni significative. Vogliamo vedere che se indaghiamo la performance per categoria, ovvero se separiamo la performance in base a demografie o inclinazione politica dei media di notizie, possiamo osservare un pattern. Per esempio, per il rilevamento di linguaggio di odio, i modelli di linguaggio a sinistra sono migliori nel rilevare linguaggio di odio rivolto a gruppi minoritari sociali, ma peggiori nel rilevare linguaggio di odio rivolto a gruppi più potenti della società. E viceversa, i modelli di linguaggio a destra sono migliori nel rilevare linguaggio di odio rivolto a bianchi e uomini, ma peggiori nel rilevare linguaggio di odio rivolto a comunità di colore, LGBTQ+ e altre minoranze. Stessi trend si verificano anche per il rilevamento di notizie false, dove vediamo che i modelli di linguaggio a sinistra sono migliori nel rilevare informazioni errate provenienti dalla loro inclinazione politica opposta e viceversa. Mostriamo anche molti esempi qualitativi per vedere che i modelli di linguaggio con inclinazioni politiche diverse danno previsioni diverse su esempi di linguaggio di odio e di disinformazione in base alle loro categorie sociali. Ci sono molti altri esempi nell’appendice per evidenziare ulteriormente che questo indica un problema di equità molto urgente riguardante i bias politici dei modelli di linguaggio. Per esempio, se i modelli di linguaggio a destra fossero finemente addestrati su linguaggio di odio o disinformazione e poi distribuiti su una piattaforma di social media popolare, ciò significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il linguaggio di odio verso le minoranze potrebbe continuare senza alcun controllo. Questo solleva l’allarme per noi di riconoscere e affrontare i problemi di equità derivanti dalle inclinazioni politiche dei modelli di linguaggio. Un breve commento finale: vorremmo anche evidenziare il dilemma unico riguardante i bias politici dei modelli di linguaggio. È come stare tra Scilla e Cariddi. Se non saniteremo le opinioni politiche nei dati di addestramento dei modelli di linguaggio, il bias si propagerà dal pre‑addestramento ai modelli di linguaggio ai compiti downstream, creando infine problemi di equità. Se cerchiamo di sanificare in qualche modo, rischiamo anche la censura o l’esclusione. Ed è incredibilmente difficile determinare cosa sia realmente neutro e dovrebbe essere mantenuto nei dati di monitoraggio del linguaggio. È quasi come il dilemma del treno elettrico. Bene, grazie per il vostro tempo.</sample>
    <sample id="48">Non è stato indicato esplicitamente nel testo quante persone hanno scritto l’articolo.</sample>
    <sample id="49">Sono state eseguite valutazioni MPP con contesti fino a **1024 token** di lunghezza.</sample>
    <sample id="50">**Riassunto della presentazione DEPLAIN**

L’evento ha introdotto DEPLAIN, un nuovo corpus di testi tedeschi per la semplificazione a livello di documento e frase. Dopo una breve definizione di “text simplification” – l’adattamento di un testo per migliorarne la comprensibilità da parte di lettori con difficoltà o di non‑nativi – è stato illustrato come la formazione di modelli di semplificazione richieda coppie parallele di frasi o documenti.  

DEPLAIN è suddiviso in due sotto‑corpora:  
- **DEPLAIN‑apa** (testi di notizie): 483 documenti allineati manualmente, equivalenti a circa 13 000 coppie frasi‑frase.  
- **DEPLAIN‑web** (domini vari): 750 documenti allineati sia manualmente sia con metodi automatici, per un totale di 30 450 coppie.

La qualità manuale supera i precedenti set, che erano troppo piccoli o totalmente automatici e quindi soggetti a errori di allineamento. Un’analisi delle coppie ha evidenziato differenze tra domini: testi biblici sono più semplificati rispetto a notizie e testi per apprendimento della lingua. Inoltre, la frequenza di trasformazioni di semplificazione (ristrutturazione, aggiunta, rimozione, riformulazione) varia tra DEPLAIN‑apa e DEPLAIN‑web.

Gli usi principali dimostrati sono:  

1. **Valutazione di metodi di allineamento**: utilizzando le coppie manuali come gold standard, si è confrontato e adattato diversi approcci, con il risultato che **MASSalign** si è rivelato il più efficace per l’allineamento di testi semplificati tedeschi.  
2. **Semplificazione automatica**: si è fine‑tuned due modelli mBART (base per frasi, long‑mBART per documenti), ottenendo risultati superiori alla baseline e fornendo checkpoint e metriche come benchmark per il futuro.

In conclusione, DEPLAIN fornisce un set di dati robusto, diversificato e di alta qualità, indispensabile per la ricerca e lo sviluppo di sistemi di semplificazione del testo tedesco.</sample>
    <sample id="51">Istituiti tre domini: **musica, libri e ricette**.</sample>
    <sample id="52">Posizionalità è la prospettiva che una persona detiene a causa della propria demografia, identità e esperienze di vita.</sample>
    <sample id="53">Il relatore è **Dawei**.</sample>
    <sample id="54">In this ACL 2023 long paper, Vasudha et al. tackle the “rare‑class” problem of detecting cognitive dissonance in text—situations where a speaker’s beliefs and actions are inconsistent. Dissonance is cognitively and socially important: it signals anxiety, polarization, and changing attitudes, yet it appears in only ~3.5 % of discourse unit pairs. The authors built a large annotation set (~1 000 pairs) using a dissonance‑first pipeline with the PDTB parser. Training an initial classifier on just 43 dissonant examples yielded near‑chance performance, motivating a two‑stage active‑learning strategy. First, they pre‑train a model on two related tasks: (1) “debate” stance classification (agreement vs. disagreement, topic‑agnostic) and (2) binary expansion/comparison classification from PDTB (CE), which captures consonance/dissonance patterns. The resulting model shows a zero‑shot AUC of.62, improving after fine‑tuning CE then debate. Next, they compare cumulative versus iterative updates during active learning, finding cumulative updates consistently superior. They propose a “Probability‑of‑Rare‑Class” (PRC) sampling strategy that selects examples most likely to be dissonant under the current model. PRC outperforms conventional AL methods (e.g., uncertainty, diversity) by a small margin, raising dissonance AUC to 0.75 after several rounds. Although annotators report difficulty, PRC yields the highest proportion of dissonant instances, making it a cost‑effective strategy for rare‑class acquisition. The work demonstrates that thoughtful transfer learning plus PRC‑based active learning can substantially improve detection of cognitively rare phenomena.</sample>
    <sample id="55">Sì, **EDAtt** utilizza un modello di ST offline già esistente. Non richiede ri‑addestramento né architetture specifiche per la simultaneità: si impiega un unico modello per tutte le impostazioni di latenza, gestendo la latenza tramite parametri e il meccanismo di attenzione.</sample>
    <sample id="56">L’articolo è stato scritto da **8 autori**.</sample>
    <sample id="57">No.  Senza addestramento specifico su KITMUS i modelli di coreference (C2F, BERT4Coref) non riescono a risolvere correttamente i casi della suite; solo dopo averli addestrati su KITMUS mostrano un miglioramento significativo, ma continuano a faticare soprattutto con le conoscenze che sono disponibili solo a tempo di inferenza.</sample>
    <sample id="58">Le tre varianti di KITMUS sono:

1. **Background‑Pretrain** – la conoscenza di base è presente nei parametri pre‑addestrati.  
2. **Background‑Both** – la conoscenza di base è disponibile sia nei parametri pre‑addestrati sia nel contesto di inferenza.  
3. **Background‑Inference** – la conoscenza di base è disponibile solo nel contesto di inferenza (non è presente nei parametri pre‑addestrati).</sample>
    <sample id="59">**Summary (≈200 words)**  

The presentation introduces *DrBERT*, the first French biomedical pre‑trained language model, built on RoBERTa and trained from scratch on *NACHOS*, a large corpus of medical texts crawled from the web. The authors compare DrBERT to *ChuBERT*, a clinical model derived from anonymized Nantes University Hospital notes, and to several continuation‑pre‑training variants that fine‑tune CamemBERT or PubMedBERT on subsets of NACHOS or clinical data. In total, seven models are evaluated across 11 French biomedical and clinical downstream tasks—named entity recognition, classification, POS tagging, and question answering—alongside six baselines (CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, ClinicalBERT). Results show that models excel on tasks matching their training domain, but heterogeneous data yields broader versatility. More data generally improves performance, and from‑scratch pre‑training outperforms most continual‑pre‑training setups, except a CamemBERT‑based model on 4 GB NACHOS that matches DrBERT 4 GB. Overall, DrBERT surpasses generic models on nine of the eleven tasks, demonstrating the value of domain‑specific French biomedical data. All models and training scripts are openly available on Hugging Face (MIT license) and GitHub.</sample>
    <sample id="60">Non sono state fornite le affiliazioni degli autori nel testo che hai condiviso. Se hai a disposizione l’intestazione dell’articolo o un link alla pubblicazione, potrei recuperarle da lì.</sample>
    <sample id="61">The final research question is: **“Should we only use the clean samples for validation, or are there better ways to utilize them?”**</sample>
    <sample id="62">Il lavoro di Nitay Calderon e colleghi si concentra sulla compattazione di grandi modelli di generazione del linguaggio naturale (NLG) mantenendo le prestazioni. L’obiettivo è ridurre la dimensione e la latenza dei modelli, con un impatto economico più che inferenziale, e per questo si propone un approccio di distillazione del sapere (Knowledge Distillation, KD).  

Nella NLG esistono due principali tecniche di KD:  
* **Distillazione a livello di parola** – minimizzare la divergenza KL tra i log‑probabili del teacher e del student;  
* **Distillazione a livello di sequenza** – generare “pseudo‑target” con il teacher e addestrare il student su di essi.  

Il contributo principale è un’analisi sistematica su quattro compiti reali (riassunto, generazione di domande, ragionamento sul senso comune, semplificazione e stile), con le seguenti caratteristiche “realistiche”: set di dati etichettati di media dimensione, grande quantità di dati non etichettati, modelli off‑the‑shelf di dimensioni intermedie e focus su efficienza di inference.  

Il paper esplora otto fasi: scelta di architettura (encoder‑decoder vs decoder‑only), pruning, selezione del sapere, baseline di stato dell’arte, e infine le estensioni di pseudo‑target.  
Concretamente si dimostra che:  

1. L’uso di dati non etichettati è cruciale per migliorare la KD;  
2. Generare **più** pseudo‑target invece di uno singolo, e farlo con sampling ad alta temperatura, aumenta la diversità e la qualità del student;  
3. Si propone **Joint‑Teaching**, che combina distillazione a livello di parola su pseudo‑target generati sia dal teacher sia dallo student, per ridurre l’“exposure bias” e insegnare al student a correggere i propri errori.  

Il risultato è un “ricettario” pratico per la compressione di NLG, utile sia per ricerca che per applicazioni industriali.</sample>
    <sample id="63">**SENSITIVITY METRIC**  
- **What it measures:** How consistently a model produces the same output when the same task is described with slightly different natural‑language instructions.  
- **How it’s computed:** For each task, the model is evaluated 5 times—once with each of the 5 instruction templates.  
  - Compute the performance metric (e.g., accuracy or Rouge‑L) for each run.  
  - Calculate the **standard deviation** (or range: max – min) of those 5 scores.  
- **Interpretation:**  
  - **Low sensitivity** (small std./range) → the model’s predictions are stable across wording changes.  
  - **High sensitivity** (large std./range) → the model is easily influenced by minor instruction variations.  

Thus, a lower sensitivity score indicates a more robust, instruction‑invariant model.</sample>
    <sample id="64">Il relatore è **Jingwei Yi**.</sample>
    <sample id="65">Una maggiore sensibilità **non** indica una performance migliore; al contrario, indica che il modello è più sensibile alle piccole variazioni nella formulazione dell’istruzione, cioè produce risultati meno coerenti. Un valore di sensibilità più basso è quindi desiderabile.</sample>
    <sample id="66">The paper “Deep Learning for Mathematical Reasoning” surveys recent advances in AI systems that tackle mathematical tasks. Mathematical reasoning is essential for understanding numerical data and language, and has long been a goal of NLP and AI. The survey focuses on two main domains: text‑based math word problems and multimodal problems that involve images, diagrams, or tables. For example, solving geometric problems requires extracting relations from a diagram, applying theorems, and computing numeric answers, which can be framed as neuro‑symbolic reasoning over diagrams and solvers. Automated theorem proving is another core line of work, where models generate sequences of logical steps to prove a claim.

The paper reviews neural architectures such as sequence‑to‑sequence and sequence‑to‑tree models that generate equations or proofs, and highlights the rise of large language models (LLMs). LLMs can solve word problems using chain‑of‑thought prompts, but they struggle with precise arithmetic and large numbers. Techniques like self‑consistency (sampling multiple reasoning paths) and program‑aided LLMs (integrating external tools) improve performance. Recent work, such as Chameleon, generates natural‑language programs to compose tools for complex reasoning. The survey also notes gaps in low‑resource settings, citing new non‑English datasets in Chinese, Korean, and Arabic, as well as domain‑specific benchmarks in finance, science, and medicine. Despite progress, models still exhibit generalization and robustness failures, especially with large numeric values and inconsistent reasoning.</sample>
    <sample id="67">Il lavoro analizza quando e perché le traduzioni multilingue subiscono interferenza o sinergia. Si confronta la perdita di un modello bilingue S→T con quella di un modello multilingue che traduce S→T, definendo l’interferenza come la differenza relativa. I risultati mostrano che l’interferenza più severa si verifica solo quando il modello è molto piccolo rispetto alla quantità di dati disponibili; con modelli di dimensioni maggiori o con un volume di dati sufficiente l’effetto svanisce. Un altro fattore importante è la temperatura di campionamento: valori di T&gt;1 favoriscono il campionamento di esempi da lingue a basso risorse, migliorando la performance. I valori comunemente usati (come T=5) spesso non sono calibrati e possono peggiorare le prestazioni in modelli più grandi. La ricerca dimostra che, al contrario di quanto ipotizzato, la similarità linguistica tra le lingue di interferenza e il numero totale di lingue non influiscono significativamente sull’interferenza. In pratica, l’interferenza è guidata principalmente dalla scarsità di parametri e dal non‑calibrato valore di temperatura. Pertanto, la soluzione più semplice e efficace è l’uso di una temperatura di campionamento adeguatamente tarata, senza necessità di algoritmi specializzati. In sintesi, dimensione del modello, quantità di dati e temperatura di campionamento sono i principali determinanti dell’interferenza nelle traduzioni multilingue.</sample>
    <sample id="68">I am sorry, but I cannot find any information in the provided speech that describes the specific type of linguistic context that is given to language models during pre‑training.</sample>
    <sample id="69">In genere basta **circa 20 campioni puliti per classe** per raggiungere buone prestazioni in WSL.</sample>
    <sample id="70">Gli autori sono tutti affiliati all’Università di Stanford (dipartimento di Computer Science e Linguistics; Dan Jurafsky è professore di CS e Linguistics presso Stanford).</sample>
    <sample id="71">Il lavoro “Resolving Indirect Referring Expressions for Entity Selection” presenta il *AltEntities Corpus*, un nuovo dataset di 6 000 domande alternative (Do you mean A or B?) e 42 000 espressioni indirette di riferimento, raccolte in tre domini: musica, libri e ricette.  

Il metodo di crowdsourcing si basa su una completazione di cartone animato con tre bulle:  
1) un contesto (“Ricordi quella canzone che ascoltavamo ieri?”),  
2) la domanda alternativa con due entità estratte da Wikipedia, e  
3) una frase di risposta scritta dall’annotatore che utilizza espressioni indirette per selezionare una delle due entità.  

La seconda bulle è generata con un template semplice e le coppie di entità sono scelte in base a diversi livelli di similarità (casuale, titoli simili, descrizioni simili, attributi simili). Per aiutare l’annotatore, vengono forniti link di ricerca Google (per canzoni), estratti di Wikipedia e immagini (per ricette).  

Il corpus è stato annotato con 3‑5 espressioni indirette per risposta, come “la più recente”, “il brano senza parole”, “quello con l’immagine del ragazzo di 12 anni”, ecc.  

I risultati con un modello T5‑XL mostrano che, con conoscenza esplicita (stessa che l’annotatore), la precisione è 92‑95 %. Con conoscenza parzialmente sovrapposta (ricerca del modello) la precisione scende a 82‑87 %, mentre con solo i nomi delle entità è circa 60 %. Il modello dimostra anche una buona generalizzabilità tra domini.  
Link al dataset: &lt;URL&gt;.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell’informazione perché i modelli linguistici, addestrati su grandi corpus di notizie e social media, assorbono e amplificano i pregiudizi politici presenti nei dati. Questi bias si traducono in comportamenti diversificati nei compiti downstream (come rilevazione di hate‑speech o fake news), con conseguenze di ingiustizia verso determinate categorie politiche o demografiche. Le tecniche tradizionali di valutazione non riescono a catturare la complessità dei posizionamenti ideologici né la polarizzazione crescente del discorso pubblico; pertanto occorre approcci più specifici—ad esempio test di orientamento politico ispirati alla scienza politica—per identificare, quantificare e mitigare in modo mirato i bias che emergono dai dati di pre‑training e che si propagano nei modelli.</sample>
    <sample id="73">La relatrice è **Akshatha**.</sample>
    <sample id="74">**Dense‑ATOMIC: a densely‑connected commonsense knowledge graph**  
ATOMIC is a high‑quality, event‑centric commonsense KB, but its sparse B‑to‑A links leave many potential connections (B‑to‑B, A‑to‑B, A‑to‑A) and multi‑hop paths missing. Dense‑ATOMIC augments ATOMIC by recovering these absent links, thereby increasing 1‑, 2‑, and 3‑hop path coverage and enabling richer commonsense reasoning.

The construction pipeline has three stages. First, *tail‑event normalization* rewrites tail events into the same syntactic form as head events (subject removal, singular conjugation, subject recovery, and relation grouping). Second, a relation‑prediction model, **Rel‑CSKGC**, is trained. It encodes head and tail events with RoBERTa, uses the [CLS] token for linkable prediction, applies max‑pooling, and concatenates head and tail representations to predict the relation. This approach bypasses sparse graph structure and directly exploits event semantics. To avoid iterating over all event pairs, events are clustered by base event: intra‑cluster completion infers missing links within a cluster, inter‑cluster completion handles cross‑cluster links. Negative samples are drawn from the training split to form the training set.

Third, the predicted links are added to ATOMIC to form Dense‑ATOMIC. Evaluation shows that Rel‑CSKGC outperforms both traditional link‑prediction and translation‑based baselines on automatic metrics and human judgment. Dense‑ATOMIC yields higher knowledge coverage and improves downstream models such as COMET, producing more diverse and accurate commonsense inferences. The full code and dataset are publicly available.</sample>
    <sample id="75">Zheng Yandan presented **Jointprop**, a joint semi‑supervised framework for Named Entity Recognition (NER) and Relation Extraction (RE). The work addresses the heavy annotation cost of fully supervised models and the limited benefit of existing semi‑supervised methods, which often treat NER and RE independently. Jointprop exploits the strong interdependence between the two tasks—e.g., syntactic similarities such as “used to” vs. “use in” and shared entity types—to propagate labels across a heterogeneous graph that connects both labeled and unlabeled data.

The framework has four stages:

1. **Span Feature Generation** – contextualized token embeddings are aggregated into span and span‑pair representations; a base classifier is trained on labeled data and used to generate initial predictions for unlabeled spans.
2. **Heterogeneous Graph Construction** – a k‑nearest‑neighbor graph is built, linking entity nodes and relation nodes via their representations, thereby capturing similarity among unlabeled samples and between labeled samples.
3. **Joint Label Propagation** – labels diffuse through the graph iteratively. Pseudo‑labels for both entities and relations are refined until convergence, leveraging high‑density regions of the unlabeled data.
4. **Model Optimization** – confident pseudo‑labels (above a threshold \(g\)) are combined with ground truth to retrain the base classifier, which remains unchanged.

Experiments on four datasets (joint‑task and single‑task) show that Jointprop outperforms baselines on both NER and RE, particularly benefiting from task co‑dependency in joint settings. The presentation concluded with a call for further exploration of label propagation in multi‑task semi‑supervised learning.</sample>
    <sample id="76">**L’infrastruttura di propagazione dei bias politici è una catena di cinque fasi principali:**

1. **Raccolta e marcatura del corpus**  
   - I dati di pre‑training sono estratti da grandi web‑crawl (C4, Reddit, ecc.) e taggati per orientamento politico (sinistra, destra) e per fonte (news, social media).  
   - Vengono inoltre suddivisi temporalmente (prima e dopo la 45ª presidenza degli USA) per catturare la polarizzazione.

2. **Pre‑training del modello**  
   - Modelli base (RoBERTa, GPT, BART, ecc.) vengono pre‑addestrati sui corpus “politicamente neutrali”.  
   - Successivamente si eseguono **further‑pretraining** su corpora partisans (es. solo “left‑leaning Reddit”) per osservare il movimento ideologico.

3. **Valutazione del posizionamento politico**  
   - Si applicano questionari politici (Political Conference Test) con prompt diversi per misurare l’ideologia del modello.  
   - Si tracciano le coordinate ideologiche su un piano “sinistra‑destra” per confrontare modelli diversi (GPT‑4 è il più liberale, BART più conservatore, ecc.).

4. **Fine‑tuning e testing su compiti downstream**  
   - Modelli con diversi bias vengono fine‑tuned su compiti come hate‑speech detection e fake‑news detection.  
   - Si valuta la performance per categorie demografiche e per orientamento politico delle fonti di test.

5. **Analisi qualitativa e monitoraggio**  
   - Si esaminano gli errori qualitativi per evidenziare come i bias influenzano le decisioni del modello su gruppi specifici.  
   - Si documentano le implicazioni di fairness e di potenziale censura.

Questa infrastruttura mostra chiaramente come i bias politici presenti nei dati di pre‑training si propagano al modello e infine si manifestano nelle applicazioni downstream, creando potenziali problemi di equità e di marginalizzazione delle opinioni opposte.</sample>
    <sample id="77">**Summary (≈200 words)**  

This work, a collaboration between Yale University and Microsoft Research, introduces **DeFacto**, a new dataset designed to improve factual consistency in abstractive summarization. DeFacto contains human demonstrations and detailed feedback for summarization models, collected on the XSum dataset using outputs from the pre‑trained Pegasus model. Annotators first label each system‑generated summary as factually consistent or not. If inconsistent, they provide a corrected, factually accurate summary, along with **feedback** that includes explicit **instructions** for editing, an **explanation** of the error, and an **evidence** sentence from the source document. The dataset comprises roughly 2.5 k examples, with about 70 % containing factual errors. Edited summaries achieve higher factuality scores than the originals but exhibit lower overlap with reference summaries, likely because many reference summaries themselves contain errors.

The authors propose and evaluate **three new NLG tasks**:  

1. **Summary Editing** – the model follows human feedback to edit an initial summary. Fine‑tuned models and zero‑shot large language models (LLMs) effectively use the feedback.  
2. **Feedback Generation** – a critic model generates useful editing feedback. This remains challenging for both fine‑tuned models and LLMs.  
3. **Automatic Factual Error Correction with Explanation** – the editor model corrects errors and produces an explanation. It matches baseline performance while training on far fewer examples, and explanation generation further improves outcomes.

DeFacto’s fine‑grained annotations also support training factuality metrics and meta‑evaluation. The dataset is publicly released on GitHub.</sample>
    <sample id="78">Sì. Nel corpus DEPLAIN‑apa le semplificazioni tendono a privilegiare **riformulazioni, riorganizzazioni e aggiunte di parole**, mentre nel corpus DEPLAIN‑web le trasformazioni più frequenti sono le **rephrasings**. Inoltre, i testi biblici (in entrambi i sotto‑corpora) sono semplificati in misura più marcata rispetto a notizie o testi per apprendimento della lingua.</sample>
    <sample id="79">Yes – the authors have released **CoScript** as a public dataset that anyone can download and use for research on constrained language planning.</sample>
    <sample id="80">La filigrana viene inserita **modificando l’embedding restituito**:  

1. **Scegli un “target embedding”** (vector di riferimento).  
2. Per ogni frase ricevuta, il provider conta quante parole appartengono al *trigger set*.  
3. L’embedding restituito è una **somma pesata** tra l’embedding originale e il target:  

   \[
   \text{embedding}_{out}= (1-\alpha)\,\text{embedding}_{orig}+\alpha\,\text{target}
   \]

   dove \(\alpha\) è proporzionale al numero di trigger (es. \(\alpha = \frac{\#\text{trigger}}{m}\)).  
4. Se il numero di trigger supera una soglia \(m\), l’embedding diventa **esattamente il target**.

Così la filigrana è incorporata tramite l’aggiunta di un componente predeterminato ai vettori di embedding.</sample>
    <sample id="81">Gli autori del lavoro sono affiliati alla **Penn State University** (ad esempio, Yusen Zhang).</sample>
    <sample id="82">**Riassunto**

Il video presenta il progetto “Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring” (ULRA), che mira a valutare automaticamente la qualità degli elaborati senza etichette umane. L’Automated Essay Scoring (AES) è un’area fondamentale dell’elaborazione del linguaggio naturale applicata all’educazione, ma l’addestramento supervisionato richiede enormi corpora etichettati, costosi e laboriosi da produrre. Per superare questa limitazione, gli autori esaminano due approcci precedenti: (1) un metodo di clustering non supervisionato basato sul numero di termini unici (Chen et al., 2010) e (2) un modello neurale addestrato con la lunghezza delle frasi come segnale debole (Zhang &amp; Litman, 2021). Entrambi presentano risultati insoddisfacenti, evidenziando la necessità di più segnali di qualità.

ULRA introduce un nuovo framework che sfrutta più “quality signals” (ad es. frequenza di parole, complessità sintattica, coerenza logica) per generare ranking parziali di saggi. Questi ranking vengono trasformati in coppie di ordine parziale, che forniscono supervisione pseudo‑ground‑truth. Il modulo **Deep Pairwise Rank Aggregation (DPRA)** combina queste coppie mediante una perdita di aggregazione di rango, assegnando ad ogni segnale un peso di fiducia apprendibile che misura la sua importanza relativa. Alla fase di inferenza, i punteggi neurali vengono normalizzati tramite una trasformazione min‑max per adattarsi al set di punteggi predefinito.

Gli esperimenti, condotti sia in scenari transductivi che induttivi, mostrano che ULRA supera i baselines non supervisionati e si avvicina alle prestazioni dei metodi cross‑prompt e one‑shot, sebbene rimanga leggermente indietro rispetto ai modelli supervisionati. In sintesi, ULRA dimostra che aggregare segnali di qualità multipli tramite un approccio di ranking può fornire una supervisione robusta per l’AES senza etichette umane.</sample>
    <sample id="83">Sì. Nella nostra analisi abbiamo osservato che i modelli encoder‑decoder (come **mT5**) mostrano miglioramenti quando vengono addestrati su un mix di più lingue. La variabilità linguistica fornisce un contesto più ricco e permette al modello di apprendere rappresentazioni più generali, il che si traduce in performance più alte su quasi tutte le lingue incluse nel dataset. Tuttavia, questo beneficio è accompagnato da una leggera perdita di accuratezza in inglese in alcuni set di dati, un effetto noto come “Curse of Multilinguality”. In sintesi, l’addestramento multilingue è vantaggioso per la maggior parte delle lingue, ma può comportare un trade‑off con l’inglese in alcuni casi.</sample>
    <sample id="84">In this presentation, Shwai He introduces **PAD‑Net (Partially Dynamic Network)**, a framework designed to improve the efficiency of dynamic neural networks. Traditional dynamic models—such as Mixture of Experts and Dynamic Convolution—replace entire network layers with input‑dependent modules, often leading to excessive parameter counts (e.g., an eight‑expert BERT‑Base expands five‑fold). He argues that many of these dynamic parameters are redundant and that a fully dynamic network may contain sub‑networks that can be made static without sacrificing performance.

PAD‑Net addresses this by **partitioning parameters into static and dynamic sets** and applying two scale factors that control the intensity of each mode. An **Iterative Mode Partition** procedure gradually converts low‑impact dynamic parameters into static ones, guided by their influence on loss. Experiments show that PAD‑Net outperforms both purely static and fully dynamic baselines while using far fewer parameters and less computation. Ablation studies identify optimal dynamic ratios for Dynamic Convolution and Mixture of Experts, and demonstrate the importance of the scale‑factor constraints. Compared to pruning, PAD‑Net retains static weights that preserve discriminative power.

Future directions include extending PAD‑Net to other mainstream architectures, exploring hardware‑friendly structured variants, and incorporating additional modes (e.g., zero elements). Overall, PAD‑Net offers a principled way to balance dynamism and efficiency in neural networks.</sample>
    <sample id="85">Un esempio di **pianificazione linguistica vincolata** è la scrittura di uno script per “fare una torta al cioccolato” dove si specificano vincoli come il tipo di cioccolato (bianco, fondente), la presenza di allergeni (glutine, latte) o la modalità di cottura (in forno, al microonde). In questo caso, il piano deve rispettare chiaramente tutti i vincoli dati.</sample>
    <sample id="86">Istituiscono la “segretezza” del watermark verificando che le embedding generate per frasi contenenti i trigger non siano distinguibili da quelle normali.  
Per farlo:

1. **Trigger a frequenza moderata** – scelgono parole che non siano né troppo comuni né troppo rare, in modo che il loro uso nelle frasi non sia evidente.  
2. **Visualizzazione PCA** – proiettano le embedding di frasi con e senza trigger in uno spazio a due dimensioni tramite PCA e mostrano che i punti non si separano visivamente.  
3. **Metriche di similarità** – confrontano cosine, L₂ e KS‑test tra embedding di trigger e benigni, ottenendo valori che indicano che il watermark non altera l’utility né è facilmente individuabile.

Questa combinazione di trigger “discreti” e analisi visiva/matriciale conferma che il metodo rimane covert e difficile da rilevare o rimuovere.</sample>
    <sample id="87">Il lavoro si basa su modelli PLM già esistenti sfruttandone sia l’architettura sia i pesi di partenza:

* **Architettura**: utilizza la variante RoBERTa di BERT, già adottata da CamemBERT per il francese.
* **Tokenizer e pesi di base**: parte dal tokenizer e dai pesi di CamemBERT (OSCAR 4 GB) per il “continual pre‑training”, oltre a provare i pesi di PubMedBERT (biomedical English) per comparare l’effetto di un transfer learning multilingue.
* **Pre‑training da zero**: costruisce nuove versioni “from‑scratch” (DrBERT e ChuBERT) addestrandole direttamente su grandi corpus di testo medico francese (NACHOS, 4–7 GB) o su dati clinici anonimizzati, mantenendo la stessa architettura RoBERTa.

In questo modo, il progetto combina l’infrastruttura di modelli pre‑addestrati (CamemBERT, PubMedBERT) con un nuovo ciclo di pre‑training su dati specifici del dominio francese, ottenendo DrBERT, il primo modello biomedicale open‑source in francese.</sample>
    <sample id="88">GPT‑4 is least aligned with countries that are neither English‑speaking nor Confucian‑influenced – in practice, many non‑English‑speaking, non‑Confucian nations.</sample>
    <sample id="89">Nel suo intervento la relatrice illustra il funzionamento del modello usando come esempio la frase **“I’m going to talk about…”** (che il modello traduce in tedesco e per cui si esaminano i pesi di cross‑attenzione).</sample>
    <sample id="90">**Abstract**  
The increasing sophistication of language models has amplified the need for high‑quality annotated corpora, yet recruiting native speakers remains a bottleneck for many languages. This study investigates whether non‑native language learners can serve as reliable annotators. We recruited participants in English, Korean, and Indonesian and stratified them into basic, intermediate, and advanced levels using a revised CFR framework. Each participant completed a three‑step session (pre‑test, annotation, post‑test) over six days, annotating 10 items drawn from four GLUE‑style tasks—sentiment analysis, natural‑language inference, named‑entity recognition, and machine‑reading comprehension—while optionally consulting dictionaries or machine‑translation tools. Pre‑ and post‑tests measured language proficiency and vocabulary gains.  

Results show that learner‑generated labels are nearly as accurate as those from native speakers, especially on simpler items and with moderate difficulty. Aggregating learner annotations via majority voting yields performance on par with native annotations. In downstream training simulations, models trained on learner labels achieved ~95 % of the accuracy obtained with gold labels and in some cases surpassed models trained on native‑speaker data. Moreover, learners exhibited measurable gains in test scores across sessions, indicating concurrent language learning. These findings challenge the prevailing reliance on native speakers for data annotation and demonstrate that language learners can efficiently contribute high‑quality annotations, thereby accelerating resource development for low‑ to mid‑resource languages.</sample>
    <sample id="91">Più attività vengono incluse nell’addestramento, il modello di solito migliora: aumentano le metriche di accuratezza/ROUGE‑L e, contemporaneamente, diminuisce la sensibilità (la variabilità dell’output rispetto a variazioni di istruzioni). In altre parole, espandere il set di task porta a prestazioni più elevate e a risultati più stabili.</sample>
    <sample id="92">I autori confrontano il loro approccio con **tre categorie di baseline**:

1. **Modelli basati su alberi (tree‑based)** – sistemi di semantic parsing che impiegano strutture sintattiche esplicite (es. tree‑to‑tree neural networks).  
2. **Modelli seq2seq “standard” senza struttura** – reti LSTM/Transformer che trascrivono direttamente l’input in output senza alcuna informazione strutturale.  
3. **Altri approcci treeless** – modelli già pubblicati sul benchmark COGS che prevedono l’output senza alberi (ad es. multiset‑tagging puro o sistemi di decodifica con permutazioni latenti).</sample>
    <sample id="93">I coautori, Alexander Koller e Ivan Titov, sono i supervisori (advisors) di Matthias Lindemann.</sample>
    <sample id="94">**Riassunto (≈200 parole)**  

Lo studio di Jingwei Yi e colleghi propone “Embedding Marker”, un metodo di watermarking back‑door per servizi di embedding basati su LLM (GPT, LLaMA, PaLM). L’obiettivo è proteggere il copyright dei servizi di embedding senza degradare la loro utilità, mantenendo il watermark covert e trasferibile a eventuali servizi di estensione/estrazione del modello.

1. **Trigger set**: una lista di parole a frequenza media, ottenuta da un corpus generale (es. Wikipedia).  
2. **Iniezione del watermark**: quando l’utente invia una frase, il provider conta i trigger presenti. L’embedding restituito è una somma ponderata tra l’embedding originale e un “target embedding” predefinito; il peso del target è proporzionale al numero di trigger (se superiore a \(m\), l’embedding è identico al target).  
3. **Verifica del copyright**: si costruiscono due set di test – “back‑door” (frasi composte solo da trigger) e “benign” (senza trigger). Si richiedono gli embedding al servizio sospetto, si calcolano le similarità coseno e L2 con il target, si confrontano le differenze (\(\Delta\)coseno, \(\Delta\)L2) e si esegue il test KS per ottenere un p‑value.  

Gli esperimenti su AG News, MIND, SST‑2 e Enron Spam (frequenze basate su Wikipedia) mostrano che il metodo garantisce alta precisione di rilevamento mentre mantiene l’utility per compiti downstream. L’analisi PCA evidenzia la copertura del watermark: embedding back‑door e normali sono indistinguibili visivamente. Lo studio conclude che “Embedding Marker” è un approccio pratico e robusto per proteggere embedding‑as‑a‑service.</sample>
    <sample id="95">Il primo autore del paper PaLM è **Andrew M. Dai**.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, studentessa di dottorato al primo anno al Carnegie Mellon University e oggi vi presenterò il nostro lavoro **NLPositionality**, che caratterizza i bias di progettazione presenti nei dataset e nei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell’Università di Washington e dell’Allen Institute for AI, tra cui Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Partiamo immaginando di lavorare per un quotidiano e di dover filtrare i commenti sotto un articolo per rimuovere contenuti tossici. Potreste rivolgervi a un’API molto popolare, come la **Perspective API** per la rilevazione della tossicità, e questa funziona molto bene se siete **Carl Jones**. Dove l’API è in grado di rilevare correttamente gli episodi tossici. Ma questo non è vero per **Aditya Sharma**, dove la Perspective API non è particolarmente sensibile ai termini offensivi più comuni nei contesti indiani. Questo è un esempio di bias di progettazione, dove vediamo differenze di prestazioni sistematiche tra tecnologie e popolazioni diverse.

I bias di progettazione, come quello che abbiamo appena citato, possono derivare dalla **positionalità** degli ricercatori NLP e degli sviluppatori di modelli. La positionalità è semplicemente la prospettiva che le persone mantengono a causa della loro demografia, identità e esperienze di vita. Questo concetto è ampiamente usato negli studi critici, in particolare negli spazi accademici femminili e queer. Come ricercatori, la positionalità può influenzare il processo di ricerca e i suoi risultati, perché può cambiare le decisioni che prendiamo.

Quindi una domanda che molte persone potrebbero chiedersi è: **i dataset e i modelli hanno positionalità?** Non intendiamo dire che i modelli o i dataset possiedano identità demografiche e esperienze di vita, ma raccolgono giudizi e opinioni di persone reali e possono quindi rappresentare certe positionalità più di altre. Il lavoro precedente ha suggerito qualche evidenza aneddotica di positionalità, come differenze culturali nei dataset e nei modelli, e definizioni teoriche di positionalità del modello. Tuttavia, questi lavori non confrontano gli utenti finali con i dataset e i modelli stessi, e studiare la positionalità di dataset e modelli è sempre più importante man mano che le attività NLP diventano più soggettive e orientate alla società. È difficile caratterizzare in che misura queste positionalità sono distorte, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API.

Per studiare la positionalità di dataset e modelli, confrontiamo le annotazioni con gli utenti reali e i dataset e modelli esistenti. Per questo utilizziamo il nostro framework **NLPositionality**. Il framework si compone di due fasi principali:

1. **Riannota i dataset con annotatori diversificati.** Solitamente, solo pochi annotatori lavorano su ciascuna istanza e le demografie degli annotatori originali sono rare o non condivise. Quindi rianotiamo i dati per ottenere un gran numero di annotazioni per ogni istanza e una ricca serie di dati demografici.

2. **Confronta le annotazioni per demografia con i modelli e dataset.** Usiamo la correlazione di Pearson R per confrontare le annotazioni con le previsioni dei modelli e le etichette dei dataset. Il nostro approccio differisce dalla letteratura sull’accordo tra annotatori, perché confrontiamo gli utenti finali con i modelli e i dataset, non solo l’accordo tra annotatori o la modellizzazione delle distribuzioni degli annotatori.

Il nostro framework è reso possibile da **Lab in the Wild**, una piattaforma di sperimentazione online che permette di reclutare volontari diversificati. Rispetto a piattaforme come MTurk, che hanno principalmente partecipanti dagli USA o dall’India, Lab in the Wild riesce a ottenere dati di alta qualità da utenti più variegati.

Abbiamo ospitato due compiti su Lab in the Wild:

- **Social Acceptability**: i partecipanti leggono una situazione dal dataset *Social Chemistry* e scrivono quanto è socialmente accettabile la situazione. Per mantenere l’engagement, possono poi confrontare le loro risposte con quelle di un AI e di altri partecipanti.
  
- **Toxicity &amp; Hate Speech Detection**: i partecipanti leggono un esempio dal dataset *DynaHate* e indicano se pensano che sia un caso di hate speech. Poi confrontano le loro annotazioni con *DynaHate*, *Perspective API*, *Rewire API*, *HateBERT* e *GPT‑4*.

Alla fine abbiamo raccolto oltre 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi. Con questi dati possiamo rispondere a: **con chi si allineano maggiormente i dataset e i modelli NLP?**

Ecco i risultati principali:

- **Positionalità nei dataset e nei modelli**: i dataset e i modelli sono più allineati ai paesi anglofoni. Per esempio, GPT‑4 sull’analisi della social acceptability è più allineato ai paesi confuciani e anglofoni. *DynaHate* è anch’esso più allineato ai paesi anglofoni.

- **Allineamento con l’istruzione**: c’è un più forte allineamento con persone con istruzione universitaria (college o graduate school) sia per GPT‑4 che per *DynaHate*.

- **Rappresentazione di gruppi minoritari**: i dataset e i modelli sono meno allineati ai non binari rispetto ai sessi maschile e femminile. Questo si osserva sia nel compito di social acceptability con GPT‑4 sia nel compito di hate speech con *DynaHate*.

Quindi, dato che esiste la positionalità in NLP, cosa possiamo fare al riguardo? Abbiamo tre raccomandazioni:

1. **Tieni un registro di tutte le scelte di progettazione** durante tutto il processo di ricerca.
2. Conduci la ricerca NLP con la prospettiva di **perspettivismo**.
3. Costruisci dataset e modelli specializzati per **quattro comunità specifiche**. Un buon esempio è l’iniziativa **Masakhani**.

Vogliamo sottolineare che l’**NLP inclusiva** non significa semplicemente che tutte le tecnologie funzionino per tutti, ma che siano progettate per funzionare per tutti.

Concludo qui la presentazione. Se volete approfondire, potete consultare il nostro dashboard per i risultati più recenti e il nostro articolo. Grazie!</sample>
    <sample id="97">La relatrice elenca **tre** problemi associati ai modelli SimulST attuali.</sample>
    <sample id="98">Un modo efficace è **l’“augmentazione e bilanciamento contrastivo”**:  
1. **Raccolta/Selezione** – raccogliere testi da fonti politicamente diverse in proporzioni quasi‑equilibrate.  
2. **Rimozione/Neutralizzazione** – filtrare termini e frasi fortemente polarizzanti con tecniche di NLP (es. TF‑IDF, embedding‑based saliency).  
3. **Data‑augmentation** – generare versioni “counterfactual” (ad es. sostituire pronome di genere, termini ideologici) con un modello controllato.  
4. **Bilanciamento** – normalizzare le frequenze di temi e vocaboli tra le classi politiche prima del training.  

Questa pipeline riduce la sovra‑rappresentazione di una prospettiva, mantenendo al contempo un contesto informativo variegato e quindi mitigando i bias sociali e politici nei modelli di NLP.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dell’Università di Fudan. Sono qui per introdurre il nostro lavoro “Distilling Script Knowledge from Large Language Models for Constrained Language Planning” (Raffinamento della conoscenza di script da modelli linguistici di grandi dimensioni per la pianificazione linguistica vincolata).  

Nel quotidiano, gli esseri umani pianificano le loro azioni seguendo istruzioni passo‑passo sotto forma di script orientati a obiettivi. Studi precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate, come “fare una torta”, e hanno dimostrato che i grandi modelli linguistici possono effettivamente decomporre gli obiettivi in passaggi. Tuttavia, la maggior parte delle ricerche si è concentrata su obiettivi astratti di attività tipiche. La pianificazione di obiettivi con vincoli specifici, ad esempio “fare una torta al cioccolato”, rimane ancora poco esplorata.  

In questo articolo definiamo il problema della pianificazione linguistica vincolata, che impone diversi vincoli sugli obiettivi da pianificare. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multifaccettati. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli.  

In primo luogo, valutiamo e miglioriamo la capacità di pianificazione vincolata dei grandi modelli linguistici. Poiché non esiste un dataset di obiettivi specifici per supportare la nostra analisi, dobbiamo prima acquisire questi obiettivi. Come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multifaccettati per l’acquisizione di dati con l’intervento umano utilizzando InstructGPT. Campioniamo 100 obiettivi specifici ed valutiamo gli script generati dai grandi modelli linguistici. La tabella riportata mostra l’accuratezza complessiva dei risultati. Scopriamo che tutti i modelli linguistici ottengono risultati insoddisfacenti nella pianificazione di obiettivi specifici.  

Successivamente, conduciamo un’analisi dettagliata per capire perché i modelli di apprendimento falliscono. I risultati del grafico mostrano che la completezza semantica degli script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Analizziamo una categoria di vincoli più fine, definita da wikiHow. La mappa di calore nel grafico mostra che la performance di pianificazione di InstructGPT varia notevolmente per obiettivi di categorie differenti. Studi precedenti hanno dimostrato che la qualità dell’output dei modelli linguistici presenta una grande varianza, che porta a prestazioni scadenti.  

Perciò adottiamo l’idea di “over‑generate‑then‑filter” per migliorare la qualità della generazione. Prima mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sugli obiettivi astratti semplificati. Poi InstructGPT genera in eccesso K script per ogni obiettivo specifico. Successivamente, sviluppiamo un modello filtro per selezionare gli script fedeli. Convertiamo script e obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno come punteggio di similarità semantica. Inoltre, premiamo lo script che contiene le parole chiave del vincolo target. Tenere solo lo script se l’obiettivo target ottiene il punteggio più alto nel set di obiettivi. Con questo metodo, InstructGPT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in termini di completezza semantica sia di fedeltà al vincolo.  

Poiché i grandi modelli linguistici sono costosi da distribuire, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare il dataset è un passo fondamentale. Tuttavia, studi precedenti non permettono la pianificazione di obiettivi specifici e l’annotazione manuale è costosa. Pertanto, seguiamo l’idea della “symbolic knowledge distillation”, per distillare dataset di pianificazione linguistica vincolata dai grandi modelli linguistici. Applichiamo il nostro metodo per costruire un dataset di pianificazione linguistica vincolata, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità del set di validazione e di test, chiediamo a lavoratori crowd‑source di individuare e correggere gli esempi errati. Il grafico mostra la distribuzione dei vincoli di CoScript. Scopriamo che CoScript mostra un’alta pluralità di obiettivi specifici generati. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione linguistica vincolata. Rileviamo che T5 fine‑tuned su CoScript può generare script di qualità superiore rispetto alla maggior parte dei grandi modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando vengono addestrati su dataset adeguati.  

In sintesi, abbiamo stabilito il problema della pianificazione linguistica vincolata. Valutiamo la capacità di pianificazione vincolata dei grandi modelli linguistici e sviluppiamo un metodo “over‑generate‑then‑filter” per i grandi modelli linguistici. Usiamo i grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere una risorsa preziosa per far progredire la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Troverete maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank è una tecnica di recupero per domande “multi‑hop” (che richiedono più passi di ragionamento, ognuno associato a un documento). Invece di addestrare un recuperatore con migliaia di esempi di domande e percorsi corretti, PromptRank combina un recupero non supervisionato (TF‑IDF + attraversamento di hyperlink) con un reranker basato su un modello linguistico a pochi shot.  

Il pipeline è divisa in due fasi:  
1. **Recupero** – si ottengono una pool di documenti candidati tramite TF‑IDF, si espandono e si tronca la catena attraversando i link, ottenendo così possibili “percorso” o “chain”.  
2. **Reranking** – per ogni chain si costruisce un prompt che inserisce i documenti con token indicativi e un’istruzione tipo “Leggi i documenti precedenti e rispondi alla domanda”. Il modello linguistico (GPT‑2‑XL o T5‑XL) calcola la probabilità della domanda data la chain; questa probabilità è il punteggio di ranking.  

PromptRank introduce tecniche addizionali: ricerca di istruzioni ottimali, campionamento di istruzioni diverse (aggregando punteggi), e scaling della temperatura per i logits.  

Valutazioni su HotpotQA con metriche R@K e AR@K mostrano che PromptRank supera sistemi supervisionati (es. DrKit) e si avvicina a recuperatori dense di ultima generazione. Con un lettore ELECTRA‑Large, il sistema raggiunge performance di QA quasi pari a MDR, con un margine di circa 4 punti di esattezza.  

In sintesi, PromptRank dimostra che la probabilità del modello sulla domanda è un efficace scoratore per percorsi, e che le istruzioni sono cruciali per sfruttare la capacità di ragionamento del modello.</sample>
    <sample id="101">La fluidità di PaLM è molto buona: è comparabile ai sistemi di traduzione di stato‑dell’arte e presenta una percentuale di errori “Style/Awkward” più bassa, il che indica output più scorrevole e naturale.</sample>
    <sample id="102">Le proprietà chiave di un metodo di filigrana per “embedding‑as‑a‑service” sono:

1. **Applicabilità** – Deve funzionare con servizi basati su embedding e non solo su modelli di generazione.  
2. **Non‑degradazione della utilità** – L’inserimento della filigrana non deve influire negativamente sulle prestazioni di downstream NLP.  
3. **Covert** – L’attaccante non deve poter rilevare né rimuovere facilmente la filigrana.  
4. **Trasferibilità** – La filigrana deve propagarsi al modello estratto (o al servizio che lo replica) durante il processo di “model extraction”.</sample>
    <sample id="103">Le 14 lingue in cui i discorsi TED in inglese sono stati tradotti sono:

- **Arabo**  
- **Cinese (Mandarino)**  
- **Francese**  
- **Tedesco**  
- **Italiano**  
- **Giapponese**  
- **Coreano**  
- **Portoghese**  
- **Russo**  
- **Spagnolo**  
- **Turco**  
- **Hindi**  
- **Vietnamita**  
- **Polacco**</sample>
    <sample id="104">Nel talk non viene indicato un numero preciso di istanze prese in campione per la ri‑annotazione. L’unica cifra citata è che, in totale, sono state raccolte oltre 16 000 annotazioni da più di 1 000 annotatori provenienti da 87 paesi. Il numero di esempi specifici estratti dai singoli set di dati (Social Chemistry, Dynahate, ecc.) non viene menzionato.</sample>
    <sample id="105">Le differenze tra i set di dati benigni e backdoor vengono misurate tramite:

- **Delta Cosine** – differenza nella similarità coseno tra embedding e embedding target.  
- **Delta L2** – differenza nella similarità (o distanza) Euclidea (norma L₂) tra embedding e embedding target.  

Inoltre, per confermare la distinzione, viene impiegato anche il **p‑value del test KS** come terzo indicatore di differenza.</sample>
    <sample id="106">Il lavoro “QUEST” presenta un nuovo dataset per la ricerca di entità che si concentra su query con vincoli impliciti di set. I ricercatori, in collaborazione con Google DeepMind, hanno ispirato il progetto con due situazioni quotidiane: una zoologa che vuole identificare una specie sconosciuta in Costa Rica e un lettore che cerca un romanzo di storia ambientato in Francia. Entrambe le persone esprimono esigenze di ricerca che combinano più condizioni (intersezioni, differenze di set) ma in modo naturale e informale.

Per costruire QUEST, i creatori hanno estratto categorie atomiche da Wikipedia (film, libri, piante, animali) e poi applicato operazioni di set (intersezione, differenza, unione) per generare query di tipo “X e Y” o “X ma non Y”. Gli annotatori hanno poi parafrasato queste query templatiche, garantendo fluidità e naturalità, e successivamente hanno verificato la rilevanza delle entità risposte, segnando anche gli span di testo che attestano ciascun vincolo della query. Il risultato è un corpus di oltre 3.000 query con set impliciti e un set completo di risposte verificato.

Per valutare le prestazioni, gli autori hanno impostato un task di recupero multirisposta su un vasto corpus documentale, richiedendo che le prove di rilevanza possano provenire da parti diverse del documento. Come baseline, sono stati testati recuperatori sparsi e densi, oltre a un re‑ranker T5 che processa i primi 100 candidati. I risultati mostrano un ampio margine di miglioramento: i recuperatori esibiscono bassi valori di MRecall@100 e l’intero sistema ottiene punteggi F1 relativamente bassi, soprattutto per query con intersezioni e differenze di set. QUEST, quindi, si propone come benchmark sfidante per lo sviluppo di sistemi di ricerca in scenari di esigenze informazionali “selettive”.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati impiegati in due modi principali:

1. **Encoder‑PTR**:  XLM‑R + PTR e mBERT + PTR, dove il codificatore multilingue è abbinato a un decoder a puntatore.  
2. **Encoder‑Decoder**:  mBART e mT5, che integrano codificatore e decoder in un unico modello multilingue.

Entrambi i tipi sono stati addestrati in configurazioni monolingue, multilingue (tutte le lingue combinate) e in scenari di trasferimento zero‑shot/few‑shot. I risultati hanno mostrato che gli encoder‑decoder superano gli encoder‑PTR in tutte le attività, mentre l’addestramento multilingue migliora le prestazioni nella maggior parte delle lingue, a costo di un leggero calo per l’inglese (effetto “Curse of Multilinguality”).</sample>
    <sample id="108">Il talk di Koustav Sinha presenta un lavoro con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams, pubblicato all’ACL 2023, che ri‑esamina la metodologia dei Minimal Pair Paradigms (MPP) per valutare le capacità di giudizio di accettabilità dei modelli linguistici. Tradizionalmente, i MPP confrontano una frase accettabile con una inaccettabile, chiedendo al modello di assegnare una probabilità più alta alla prima. Tuttavia, questa procedura è limitata a frasi brevi e non sfrutta le lunghe finestre di contesto disponibili nei modelli più recenti.  

Per colmare questa lacuna, gli autori hanno ricreato frasi più lunghe incorporando prefissi provenienti da varie fonti: (1) prefissi accettabili o inaccettabili dello stesso set di dati (ad es. BLiMP o SyntaxGym), (2) prefissi dallo stesso fenomeno ma provenienti da set diversi (scenario di mismatch) e (3) prefissi totalmente irrelevanti (ad es. Wikipedia).  

I risultati mostrano che i prefissi “relevanti” (stesso set o stesso fenomeno) alterano drasticamente il giudizio MPP: prefissi accettabili aumentano la probabilità di accettabilità, mentre prefissi inaccettabili la diminuiscono, con un effetto amplificato all’aumentare della lunghezza del contesto. Prefissi irrelevanti mantengono il giudizio stabile.  

Analisi di disturbi (perturbazioni) hanno rivelato che i modelli reagiscono in modo coerente a variazioni simili, suggerendo una sensibilità a caratteristiche sintattiche e semantiche latenti condivise tra le frasi.  

In sintesi, la ricerca evidenzia che i MPP tradizionali, limitati a frasi brevi, non catturano adeguatamente la conoscenza linguistica astratta dei modelli lungo intere sequenze di testo. Per una valutazione più realistica, è necessario considerare contesti più lunghi e la loro influenza sul giudizio di accettabilità.</sample>
    <sample id="109">**Riepilogo del lavoro “Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor”**

Il paper mostra come sia possibile creare un grande dataset di istruzioni naturali (Unnatural Instructions) senza alcun lavoro umano, sfruttando la capacità di generazione di modelli linguistici pre‑addestrati. Partendo da un piccolo set di esempi realizzati a mano (nel caso, tre esempi del dataset Super‑Natural Instructions), si chiama GPT‑3 per generare un nuovo “comando” e un input associato. Successivamente si richiede al modello di produrre la “risposta” corretta per quel comando. Per aumentare la varietà, si generano poi delle parafrasi dell’istruzione stesso, ottenendo così un dataset contenente 64 000 “task” originali e, con le parafrasi, circa 240 000 esempi.  

L’analisi dei dati evidenzia che oltre il 50 % delle istruzioni generate sono corrette, e anche quelle errate contengono informazioni utili per l’addestramento. Il set presenta una notevole creatività e diversità: compiti non convenzionali come “verificare la validità di un esperimento scientifico” o “inventare una nuova parola”.  

Per testare l’utilità, si fine‑tune un modello T5‑11B con Unnatural Instructions e si confronta con altre versioni di instruction‑tuned (T0++, Tk‑instruct e un modello addestrato esclusivamente su Super‑Natural Instructions). Il modello addestrato su Unnatural Instructions supera costantemente i concorrenti su benchmark come Super‑Natural Instructions, T0, BIG‑Bench Hard e LM‑entry, soprattutto quando il costo di generazione è ammortizzato.  

In sintesi, il lavoro dimostra che i modelli linguistici possono generare rapidamente dati di alta qualità, diversificati e creativi, offrendo un’alternativa più veloce e meno costosa rispetto al lavoro umano per la costruzione di dataset di instruction tuning.</sample>
    <sample id="111">Gli autori determinano le parole a “frequenza moderata” contando le occorrenze di ciascun termine in un corpus di testi generico (ad esempio Wikipedia).  Da questa tabella di frequenze, selezionano le parole che cadono in un intervallo intermedio (non troppo rare e non troppo comuni), definendo così il set di trigger.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il nostro articolo **“Do CoNLL-2003 named entity taggers still work well in 2023?”** Iniziamo.  

Il nostro studio si concentra sul problema della generalizzazione, usando il compito di **Named Entity Recognition (NER)**. Abbiamo osservato che, per quasi vent'anni, i modelli sviluppati su CoNLL‑2003 sono stati utilizzati per il NER, e questo solleva diverse questioni.  

1. **Questi modelli possono generalizzare ai dati moderni?**  
2. **Quali componenti sono necessarie per ottenere una buona generalizzazione quando sviluppiamo nuovi tagger?**  
3. **Se notiamo una cattiva generalizzazione, quali sono le cause del calo di performance?**  

Per indagare queste domande, abbiamo creato il dataset **CoNLL++**. Si tratta di testi provenienti da Reuters News del 2020, annotati con le stesse linee guida di CoNLL‑2003. Successivamente abbiamo fine‑tuned più di 20 modelli su CoNLL‑2003, poi li abbiamo valutati sia sul set di test originale di CoNLL‑2003 sia su CoNLL++. Infine, abbiamo calcolato la variazione percentuale di F1 per valutare la generalizzazione di ciascun modello.

### Cosa serve per una buona generalizzazione?

I nostri esperimenti hanno evidenziato tre ingredienti chiave:

1. **Architettura del modello** – I Transformer tendono a generalizzare meglio ai nuovi dati.  
2. **Dimensione del modello** – Di solito, modelli più grandi portano a migliori risultati di generalizzazione.  
3. **Numero di esempi di fine‑tuning** – Maggiore è il numero di esempi di addestramento, più alta è la generalizzazione.

### Cosa causa il calo di performance?

Abbiamo formulato due ipotesi:

- **Adaptive overfitting**: l’overfitting che deriva dall’uso ripetuto dello stesso test set, che si manifesta tipicamente come una diminuzione dei ritorni quando si passa a una nuova convalida.  
- **Temporal drift**: il degrado delle prestazioni dovuto al crescente intervallo temporale tra i dati di addestramento e quelli di test.

Per l’overfitting, il grafico a destra mostra una retta di regressione rossa con pendenza maggiore di 1. Questo significa che ogni unità di miglioramento su CoNLL‑2003 si traduce in più di un’unità di miglioramento su CoNLL++; non c’è quindi un ritorno decrescente, quindi **non osserviamo adaptive overfitting** in questo caso.

Per il temporal drift, abbiamo eseguito un esperimento di ri‑addestramento (o pre‑addestramento continuo) di alcuni modelli con dati più recenti. I risultati mostrano che le prestazioni peggiorano con l’aumentare del gap temporale, confermando l’ipotesi che **il principale fattore di calo è il temporal drift**.

### Conclusioni

Per ottenere una buona generalizzazione, è necessario un’architettura migliore, modelli più grandi e un maggior numero di esempi di fine‑tuning. Questi tre elementi devono funzionare in sinergia; non è sufficiente migliorare solo uno di essi.  

Inoltre, il calo delle prestazioni che abbiamo osservato è dovuto al temporal drift, e sorprendentemente **non è causato dall’adaptive overfitting**, anche se CoNLL‑2003 è stato usato per oltre 20 anni.  

Quindi, tornando alla domanda posta nel titolo del nostro articolo: **“Do CoNLL‑2003 taggers still work in 2023?”** La risposta è un deciso sì.  

Speriamo che il nostro lavoro stimoli ulteriori ricerche su come migliorare la generalizzazione dei modelli.  

Al termine, vi invitiamo a consultare il nostro articolo e il dataset CoNLL++; se avete domande, non esitate a contattarmi. Grazie mille!</sample>
    <sample id="114">In questo lavoro presentato alla conferenza ACL 2023, i ricercatori del Nanyang Technological University propongono una soluzione per ridurre i parametri dei grandi modelli linguistici (LLM) focalizzandosi sull’attenzione multi‑head. I LLM moderni, pur essendo versatili, sono estremamente costosi in termini di parametri, tempi di addestramento e dimensioni del corpus. Storicamente, le ricerche di riduzione hanno seguito tre filoni: (1) “homogenization” per rendere gli heads simili, ma a scapito delle prestazioni; (2) “diversification” per aumentare la diversità degli heads, senza compressione; (3) assegnazione di punteggi a ciascun head, ma con ridondanza residua.

La proposta di questo gruppo è l’**Attention a Gruppi** (Grouped Head Attention, GHA). Il metodo consiste in due fasi:
1. **Group‑Constrained Training** – le heads vengono divise in gruppi; entro ciascun gruppo si favorisce l’omogeneità, mentre tra gruppi si promuove la diversità, usando una perdita che incorpora termini di homogenizzazione e diversificazione.
2. **Voting‑to‑Stay** – dopo il training, ogni batch funge da “votante”; le heads ricevono voti in base alle loro performance e solo la head più forte di ogni gruppo viene mantenuta.

Il risultato è un modello “GHT” (e la versione “GHT‑PS” con pruning) che, su traduzione, generazione di testo e riassunti, migliora le metriche (BLEU e ROUGE) rispetto ai baseline di stato‑dell’arte e riduce i parametri fino al 32 % (o 90 % in configurazioni estreme). Inoltre, il modello “LITE” ottiene un incremento di velocità inferenziale del 62 % e una riduzione del 80 % delle FLOPs. 

Per il futuro, gli autori puntano a una **paginazione automatica** dei task, ispirata all’Ipotesi del Biglietto della Fortuna, per spaccare ulteriormente i modelli in subnetworks specializzati. In sintesi, la loro soluzione dimostra che i LLM possono essere significativamente compressi senza perdita di qualità, rendendoli più pratici per applicazioni reali.</sample>
    <sample id="115">L’approccio utilizza un **segmento breve costituito dagli ultimi λ frame audio** (λ è un iper‑parametro scelto in fase di configurazione).</sample>
    <sample id="116">Nel caso di Servin e Kea l’informazione **specifica all’entità** indispensabile è sapere che **Servin è un giudice** (e quindi che “he” si riferisce a lui).  Questa conoscenza identifica l’entità a cui il pronome si riferisce, mentre la conoscenza di fondo (“i giudici decidono casi in tribunale”) è quella che si ottiene dai parametri pre‑addestrati.</sample>
    <sample id="117">Il fattore più importante è **la qualità dell’esempio**; la somiglianza con la frase sorgente è meno rilevante.</sample>
    <sample id="118">Il lavoro presentato si occupa di **code‑switching** (alternanza linguistica) e mira a migliorare i modelli pre‑addestrati (mBERT, XLM‑R) per compiti come sentiment analysis e question answering su testi misti inglese‑hindi.  

1. **Definizione e rilevanza**  
   - Esempio di frase “Laptop, mere, bag, me, rakha, hai” che mescola parole inglesi e hindi.  
   - Nei contesti indiani, la capacità di gestire il code‑switching è cruciale ma i modelli multilingue standard falliscono.

2. **Contributi principali**  
   - **SwitchMLM**: nuova variante di MLM in cui solo le coppie di token che indicano un “switch‑point” (passaggio da una lingua all’altra) sono mascherate.  
   - **FrequencyMLM**: approccio di sostituzione che, senza etichette di LID, usa la negative log‑likelihood su corpora monolingui per assegnare la lingua a ciascun token.  

3. **Modifiche architetturali**  
   - Analisi “layer probing” mostra che i livelli intermediali contengono più informazioni sui switch‑points rispetto all’ultimo livello.  
   - Introduzione di **residual connections** dal livello 9 al livello 12 (o equivalente) per trasportare queste informazioni all’output finale.  
   - **Auxiliary loss** basato su LID sul livello intermedio per forzare l’incorporamento di segnali linguistici.  

4. **Risultati**  
   - Sul compito di sentiment analysis, la combinazione di SwitchMLM/FrequencyMLM + ResBERT + auxiliary loss supera tutte le configurazioni per tutti i paesi linguistici.  
   - Prove di “probe” lineari e condizionali confermano che le rappresentazioni finali contengono più informazioni sui switch‑points rispetto a un modello standard.  

In sintesi, il paper propone un nuovo obiettivo di mascheramento e modifiche strutturali mirate a rendere i modelli più sensibili all’alternanza linguistica, con risultati empirici e verifiche tramite probing.</sample>
    <sample id="119">Gli esperimenti approfonditi si focalizzano sui seguenti modelli linguistici:  
- **GPT‑4** e gli altri modelli della serie GPT (es. GPT‑3.5);  
- **BART** e le sue varianti (BART‑base, BART‑large, ecc.);  
- **RoBERTa**, che viene ulteriormente pre‑addestrato su corpora partisan.</sample>
    <sample id="120">Il metodo EDAtt si basa sui pesi di attenzione **di un singolo livello**: utilizza le attivazioni di cross‑attention del decoder (di solito l’ultimo livello) per decidere se emettere o meno una parola. Non combina i pesi di più livelli.</sample>
    <sample id="121">**Esempi di inferenza diretta** (cioè riferimenti che puntano direttamente all’entità senza ambiguità):

- **Nome esplicito dell’entità**: “**Easy on Me**” o “**I Gotta Feeling**”.
- **Posizione nell’elenco**: “**il primo**” o “**il secondo**”.
- **Qualsiasi altra menzione univoca** (ad esempio “il brano con l’aria di piano” se è l’unico con quell’aria).

In pratica, si usano le parole o riferimenti che identificano in modo definitivo l’entità richiesta.</sample>
    <sample id="122">Gli autori di questo lavoro sono affiliati all’Università di Fudan (Siyu Yuan è l’autore principale).</sample>
    <sample id="123">**Riassunto della presentazione (circa 200 parole)**  

Ying e Zhiyang hanno introdotto **MultiInstruct**, il primo benchmark pubblico di istruzioni multi‑modal per l’allenamento di modelli con parametri e dati limitati. MultiInstruct comprende 62 compiti diversificati, suddivisi in 10 categorie, derivati da 21 dataset open‑source; ogni compito è corredato da cinque istruzioni scritte da esperti. Per uniformare l’input e l’output, i compiti vengono convertiti in formato sequence‑to‑sequence, sfruttando l’uso congiunto di token di testo, immagini e coordinate di bounding‑box del modello **OFA** (unificato multimodale).  

Nel training vengono selezionati 53 compiti (9 gruppi) con 10 000 istanze ciascuno; per il test si riserva l’intero gruppo “Common Sense Reasoning” e si aggiungono 5 compiti da “VQ” e “Miscellaneous”. Ogni istanza è combinata casualmente con una delle cinque istruzioni. Durante l’inferenza, per ciascun compito vengono eseguite 5 sperimentazioni (una per ogni istruzione); si riportano media, deviazione standard e, per i compiti di classificazione, accuratezza, mentre per i compiti generativi (incluso NLP) si utilizza ROUGE‑L.  

È stato introdotto il nuovo metric **sensibilità**, che misura la consistenza delle risposte a variazioni minori di istruzione. I risultati mostrano che l’instruction tuning migliora significativamente le performance di OFA sui compiti “visti”, riduce la sensibilità e che l’addestramento con dataset di istruzioni natural-language migliora ulteriormente le performance e la robustezza.  

Infine, gli autori annunciano l’uscita di un dataset più ampio (≈ 150 nuovi compiti visione‑linguaggio) e forniscono un QR‑code per scaricare dati e modelli.</sample>
    <sample id="124">**Riassunto della presentazione**

Tan Qingyu (NUS + Alibaba) ha introdotto il problema della *temporal reasoning* nei modelli di linguaggio (LLM), suddividendolo in tre livelli:

1. **L1 – Time‑to‑time** (es. “Che anno è dopo il 2010?”).  
2. **L2 – Time‑to‑event** (es. “Per quale squadra giocò Lionel Messi nel 2010?”).  
3. **L3 – Event‑to‑event** (es. “Dopo FC Barcelona, con quale squadra ha giocato Messi?”).

Il team ha notato che le ricerche precedenti si concentravano quasi esclusivamente su L2, ignorando la complessità di L1 e L3.  

Per colmare questa lacuna, è stato creato **TempReason**, un dataset che copre tutti e tre i livelli e un arco temporale molto ampio. I dati sono derivati da Wikidata e Wikipedia.  
Sono stati testati tre scenari di QA:  
- **Closed‑Book** (solo la domanda).  
- **Open‑Book** (dove viene fornito un articolo di Wikipedia).  
- **Reason‑QA** (tutte le informazioni temporali rilevanti sono fornite e il modello deve ragionare su di esse).

I risultati mostrano che ChatGPT eccelle in L1 (anno) ma degrada rapidamente in predizioni mensili e in L2/L3, evidenziando bias temporali (ad esempio, una forte preferenza per il periodo 2000‑2020).  
Per migliorare la capacità temporale, è stato proposto **TempT5**, una variante di T5 addestrata con:  
1. **Pre‑training di estrazione di span temporali** (ricostruzione di mascherature, date e entità).  
2. **Reinforcement Learning sensibile al tempo** che premia le risposte corrette e penalizza quelle temporaneamente sbagliate.

TempT5 supera i modelli fine‑tuned standard (T5‑SFT) soprattutto in Open‑Book e Reason‑QA, anche se rimangono fluttuazioni in base al periodo storico, probabilmente dovute a squilibri nei dati di training.  

In conclusione, la presentazione ha messo in luce i bias temporali degli LLM, ha presentato TempReason come benchmark completo e ha mostrato un approccio di addestramento che migliora sostanzialmente la ragionamento temporale.</sample>
    <sample id="125">Un solo autore.</sample>
    <sample id="126">No. Il “Translate‑Test” (tradurre la query con un MT‑API e poi usare un modello monolingue) è trattato come **uno dei sette scenari di valutazione** nel benchmark XSemPLR, ma non viene presentato come l’approccio standard di cross‑lingual semantic parsing. In pratica viene usato come baseline per confrontare le prestazioni con i modelli monolingue, multilingue o zero‑shot/few‑shot.</sample>
    <sample id="127">**Summary (≈200 words)**  

In “Large Language Models Are Reasoning Teachers,” Namgyu Ho, Laura Schmid, and Professor Se‑Young Yun show how to transfer the emergent chain‑of‑thought (CoT) reasoning abilities of massive language models (e.g., GPT‑3, PaLM) to much smaller models. CoT prompting allows large models to solve multi‑step tasks by producing intermediate reasoning steps, but it only works reliably on models with billions of parameters, making deployment expensive.  

The authors propose a simple distillation method: for each benchmark question, a large teacher model is prompted in zero‑shot CoT to generate a step‑by‑step solution. If the final answer is correct, the entire reasoning chain is converted into a training example and used to fine‑tune a small student model. They extend this by introducing **Diverse Reasoning**, which samples multiple distinct CoT explanations from the teacher using temperature‑controlled stochastic decoding. These varied examples enrich the student’s training data and improve robustness.  

Empirical results on 12 reasoning benchmarks show that the fine‑tuned students, even with as few as 0.3 B parameters, outperform prompt‑based baselines and vanilla fine‑tuning. Diverse Reasoning boosts performance further (e.g., Multi‑Arithmatic from 33 % to 55 %). The approach scales: larger datasets, stronger teachers, or bigger students yield higher gains, but also entail higher development or inference costs.  

The authors provide 40 pages of analysis, open‑source code, datasets, and even paid‑teacher inference logs, inviting the community to build on their method for future emergent abilities.</sample>
    <sample id="128">**Riassunto del talk “The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources” (≈200 parole)**  

Akshatha e Martin, a nome di McGill University, Mila e Microsoft Research, presentano il KITMUS Test, un nuovo set diagnostico progettato per valutare la capacità dei modelli di comprensione del linguaggio naturale (NLU) di integrare conoscenza proveniente da fonti diverse. I modelli di solito dispongono di conoscenza incorporata nei loro parametri (pre‑training) e di conoscenza fornita al momento dell’inferenza (input). In molti compiti di NLU, entrambe le tipologie di conoscenza sono necessarie: ad esempio, sapere chi è un “presidente” (background) e conoscere il “nuovo presidente” specifico di un caso (entity‑specific).

Il KITMUS si focalizza sul coreference resolution, chiedendo ai modelli di determinare a quale entità si riferisce un pronome. Le domande sono state costruite in tre scenari:  
1. **Background‑Pretrain** – il background è già presente nei parametri, l’entity‑specific è dato al momento dell’inferenza.  
2. **Background‑Both** – sia background sia entity‑specific sono forniti al momento dell’inferenza.  
3. **Background‑Inference** – né background né entity‑specific sono presenti nei parametri; tutto deve essere fornito al momento dell’inferenza (es. usi di occupazioni nuove come “mirituer”).

Gli autori hanno valutato il dataset con partecipanti umani e con modelli di coreference standard (C2F e BERT4Coref). Senza addestramento specifico su KITMUS, i modelli mostrano prestazioni vicine al caso casuale, indicando che sfruttano solo cue superficiali. Addestrandoli su KITMUS, i due modelli migliorano significativamente rispetto al caso casuale, ma mostrano ancora difficoltà a integrare conoscenza “backward” (fornita solo al momento dell’inferenza).  

In sintesi, i risultati evidenziano che la maggior parte dei modelli di coreference non riesce a ragionare su conoscenza proveniente da più fonti senza training mirato; alcuni modelli possono farlo, ma la sfida rimane per la conoscenza fornita esclusivamente al momento dell’inferenza.</sample>
    <sample id="129">The authors give the **black‑woman persona** as an example of a marked group. In their analysis they compare the words that appear in black‑woman personas against those from the unmarked white‑person and male personas to highlight the marked language.</sample>
    <sample id="130">Le modelli che **non** mostrano una buona generalizzazione sono i vecchi architetture non‑transformer, cioè:

- **Bi‑LSTM (o LSTM) con CRF** – i modelli basati su reti ricorrenti ancora usati nei primi anni di NER.
- **Modelli CNN‑basati** (o semplici reti feed‑forward) che non sfruttano l’auto‑attenzione.
- Qualsiasi approccio “tradizionale” (ad es. CRF puro) che non incorpora l’attenzione multi‑head.

In sintesi, le architetture più vecchie e non‑transformer mostrano una performance di generalizzazione inferiore rispetto ai Transformer.</sample>
    <sample id="131">I’m sorry, but the transcript does not list any specific names of test datasets. It only refers generically to “clean test sets.”</sample>
    <sample id="132">The article involves two authors: Akshatha and Martin.</sample>
    <sample id="133">L’autore opera con **più modalità** (testo, immagini, bounding‑box e altri input visivi), non solo con il testo.</sample>
    <sample id="135">ABC‑Eval è un nuovo approccio per valutare i modelli conversazionali, sviluppato dal laboratorio NLP di Emory University in collaborazione con Amazon Alexa AI. Il metodo si basa sull’annotazione esplicita di comportamenti specifici delle risposte di un chatbot (es. irrelevanza, contraddizione con se stesso o con il partner, allucinazioni, violazioni di senso comune, empatia). Invece di affidarsi a giudizi soggettivi su scala Likert o a confronti a due, ABC‑Eval misura la frequenza di tali errori tematici in ogni turno di conversazione.

Nel lavoro sono stati testati quattro modelli di punta su 400 conversazioni (100 per modello). Gli autori hanno confrontato ABC‑Eval con tre metodi tradizionali (Likert a livello di turno, Likert a livello di dialogo e comparazioni a due). L’analisi dell’inter‑annotator agreement ha mostrato che le etichette ABC‑Eval sono più affidabili. Inoltre, lineari regressioni semplici hanno dimostrato che la proporzione di contraddizioni spiega 5‑10 % della qualità complessiva, mentre le Likert non superano il 4 %. Un modello stepwise ha rivelato che l’insieme completo di metriche ABC‑Eval spiega oltre il 25 % della variabilità della qualità, mentre le Likert spiega molto meno e con meno informazioni uniche.

I risultati evidenziano che i bot esaminati violano il senso comune in circa il 20 % delle risposte, forniscono informazioni irrilevanti in un 15 % e contraddicono se stessi o l’interlocutore in un 10 %. Questi errori, sebbene ridotti con i modelli più recenti, sottolineano l’importanza di metriche precise e affidabili. ABC‑Eval offre una valutazione a più dimensioni, più informativa e più distinta rispetto ai metodi esistenti, e può guidare lo sviluppo di chatbot di qualità superiore nei prossimi anni.</sample>
    <sample id="136">**FERMAT: An Alternative to Accuracy for Numerical Reasoning**

Jasivan and supervisor Nafise at the University of Sheffield introduce *FERMAT*, a benchmark designed to probe the mathematical reasoning abilities of language models beyond conventional accuracy scores. The motivation stems from real‑world tasks such as fact‑checking, where small arithmetic errors (e.g., subtracting ages in a table) can derail downstream decisions. Existing datasets (e.g., Infotabs) show that larger models outperform smaller ones, yet even 3‑billion‑parameter models struggle. Traditional metrics like accuracy or F1 fail to reveal the specific strengths and weaknesses in numerical reasoning.

FERMAT builds on common core and Illinois math problems, generating a wide variety of arithmetic questions that vary in number representation (integers, large integers, decimals), operation difficulty (single vs. combined operations), and training dependency (whether the exact expression appeared during training). The dataset is flexible: it can be expanded by templated generation using placeholders (num1, num2) and expression fields, allowing thousands of unique problems.

Baseline zero‑shot evaluation shows poor performance across all aspects. Fine‑tuning with 200k templated examples improves results, especially when the training data includes diverse linguistic and mathematical templates (from GSM8K and AQUA). Even when the exact expression is seen during training, accuracy remains below 50%, indicating that models rely on semantic cues rather than memorization.

Key findings: existing benchmarks are unrepresentative; single scores mask nuanced deficiencies; number encoding, tokenization, and linguistic diversity are crucial for robust numerical reasoning. FERMAT offers a richer, multi‑dimensional evaluation to guide future model development.</sample>
    <sample id="137">**Riassunto**

Sicong e i suoi collaboratori presentano *Tell2Design*, un nuovo dataset e task di “generazione di disegni di piano in base a istruzioni testuali”. Il dataset contiene 5 051 set di istruzioni scritte da crowd‑workers e circa 76 000 frasi generate tramite template, tutte associate a piani interni realizzati con OpenStreetMap. Ogni istruzione descrive in modo dettagliato **semantica** (tipo di stanza), **geometria** (dimensioni e forma) e **topologia** (relazioni spaziali) delle stanze di un piano.

Il compito è formulato come problema *sequence‑to‑sequence*: l’encoder prende la sequenza di parole della descrizione e lo decoder produce una sequenza di bounding‑box (centro X,Y, altezza e larghezza) per ogni stanza. Il modello è basato su T5, pre‑addestrato per la comprensione del linguaggio, e addestrato con la normale funzione di log‑probabilità sul target.

I risultati mostrano che il modello *Tell2Design* raggiunge un IoU micro di 54 e macro di 53, superando di gran lunga i baselines di generazione di immagini condizionate dal testo, che falliscono nel rispettare le specifiche complesse. L’analisi evidenzia tre sfide principali: restrizioni severe rispetto all’arte, comprensione di testi documentali complessi e ambiguità nelle istruzioni umane. Inoltre, l’uso di frasi artificiali come “warm‑up” migliora di oltre 10 punti l’IoU su istruzioni umane, suggerendo una sinergia tra dati sintetici e reali. Il lavoro stabilisce una base solida per la ricerca futura su generazione di design guidata dal linguaggio.</sample>
    <sample id="138">They argue that **knowledge integration from multiple sources—specifically the combination of pre‑training knowledge with inference‑time facts—is an under‑studied area of NLU.**</sample>
    <sample id="139">I relatori sono **Ying** e **Zhiyang**.</sample>
    <sample id="140">Yes. For the validation and test sets of CoScript, we ran a quality‑control step in which crowd‑sourced workers reviewed each example, identified any inaccuracies, and corrected them. This human‑in‑the‑loop review ensured that the final dataset is free of obvious errors and suitable for training and evaluating constrained language‑planning models.</sample>
    <sample id="141">The existing resources for evaluating context‑dependent translation suffer from two main limitations:

1. **Coverage** – Only a small fraction of translations actually depend on context, so corpus‑level metrics such as BLEU are insensitive to these cases and miss the nuances that matter.  
2. **Scope and bias** – Targeted evaluation sets are usually built with domain expertise and human curation, which restricts them to a narrow set of discourse phenomena and a limited number of language pairs. This makes it hard to generalise findings across many languages and contexts.</sample>
    <sample id="142">**Presentazione: “Risoluzione di Espressioni di Riferimento Indirette per la Selezione di Entità” – AltEntities Corpus**

Ciao a tutti! Mi chiamo **Javad Hosseini** e questa presentazione è frutto di una collaborazione con **Filip Radlinski, Silvia Pareti** e **Annie Louis**. Il nostro obiettivo è comprendere come gli utenti usano il linguaggio per fare una scelta tra alternative.  

Prendiamo, ad esempio, la seguente domanda alternativa:  
&gt; “Hai pensato a *Easy on Me* o a *I Gotta Feeling*?”  

Qui l’utente deve selezionare una delle due canzoni. Una scelta immediata sarebbe di fare un riferimento diretto, ad esempio dicendo “*Easy on Me*” o “la prima”. Tuttavia, in molti casi un riferimento indiretto è più naturale: l’utente non ricorda il nome, le pronunce sono simili, o vuole esprimere una preferenza. Alcuni esempi di riferimenti indiretti sono “la più recente” o “la canzone che non è energica”.  

Questo problema è cruciale per i sistemi conversazionali e per valutare la comprensione delle entità da parte dei LLM. Non conosciamo un dataset pubblico di ampia scala per questa attività, quindi abbiamo creato il **AltEntities Corpus** tramite annotazioni crowd. Il corpus copre tre domini: **musica, libri, ricette**.  

**Metodo di raccolta**  
Abbiamo enfatizzato l’informalità usando un “cartoon completion”. Il cartone ha tre fumetti:  
1. Bob dice: *“Ricordi quella canzone che ascoltavamo ieri?”* – questo stabilisce il contesto.  
2. Alice risponde: *“Hai pensato a ‘Easy on Me’ o a ‘I Gotta Feeling’?”* – la domanda alternativa.  
3. Bob, annotatore, usa un riferimento indiretto per scegliere una delle entità, ad esempio *“la più recente”*.  

I primi due fumetti sono generati automaticamente, mentre il terzo viene completato dall’annotatore.  
- Il primo fumetto è scelto da un piccolo numero di prompt manuali per dominio.  
- Il secondo fumetto è generato con un semplice template: “Hai pensato a A o B?” dove A e B sono estratti da Wikipedia.  

Abbiamo usato quattro strategie di campionamento per aumentare la difficoltà della disambiguazione:  
1. **Uniforme a caso**.  
2. **Titoli simili** (es. due libri intitolati “The Return”).  
3. **Descrizioni simili su Wikipedia**.  
4. **Informazioni simili nei box informativi (infobox)**, per esempio lo stesso genere o artista.  

Quando mostriamo la domanda alternativa agli annotatori, sappiamo i nomi delle entità ma non necessariamente i dettagli. Per facilitarne la comprensione, forniamo contesto:  
- Per le canzoni, un link di ricerca Google per ciascuna canzone (con la richiesta di ascoltare almeno una parte e leggere le informazioni).  
- Per ricette e libri, estratti di testo da Wikipedia.  
- Per le ricette, anche le immagini di Wikipedia.  

Gli annotatori poi scelgono un’entità e descrivono questa scelta con **3–5 espressioni di riferimento indiretto**. Esempi: “quella senza parole”, “non quella con il ragazzo di 12 anni”, “la fittizia”, “quella proveniente dall’Azerbaijan”, ecc.  

**Statistiche del corpus**  
- 6.000 domande alternative in tre domini.  
- 42.000 espressioni di riferimento indiretto.  

**Risultati con T5‑XL**  
- Con l’accesso a *tutto* il contesto fornito agli annotatori, l’accuratezza è alta (92–95%).  
- Con un contesto parzialmente sovrapposto (ad esempio, recuperato dal modello), l’accuratezza scende a 82–87%, più realistica.  
- Con l’accesso solo ai nomi delle entità, l’accuratezza è intorno al 60%, dimostrando che c’è molto margine di miglioramento.  

Abbiamo anche dimostrato che i modelli sono generalizzabili tra domini.  

**Link al dataset**  
[Inserire qui il link al dataset AltEntities].  

Grazie per l’attenzione e a presto!</sample>
    <sample id="143">The EDAtt approach is compared against the most common SimulST policies that are applied to offline models – namely **Wait‑k** and **Local Agreement** – as well as against the **state‑of‑the‑art simultaneous pre‑translation architecture** that is specifically designed for SimulST.</sample>
    <sample id="144">I’m sorry, but the presentation text you provided does not include any author affiliation information. If you have a PDF or a supplementary section that lists the authors’ institutional affiliations, please share it and I’ll be happy to extract that for you.</sample>
    <sample id="145">La relatrice è **Jenny**.</sample>
    <sample id="146">**Abstract**

Dialogue summarization aims to produce concise, faithful summaries of conversational text. Recent advances with large‑scale pretrained language models have improved fluency, yet factual accuracy remains a critical bottleneck. We investigate *omission*, the loss of essential utterances in generated summaries, and find that even state‑of‑the‑art systems omit key information in roughly 70 % of cases across five domains (customer service, meetings, interviews, chat, and social media). To enable systematic study, we introduce **OLDS**, a new benchmark that provides high‑quality, utterance‑level omission annotations for dialogues. OLDS is constructed from five existing datasets; for each dialogue we generate diverse abstractive candidates using multiple models and decoding strategies, and automatically label omitted utterances through a novel alignment‑based method, subsequently verified by human evaluation. 

We evaluate three baseline omission detection frameworks—pair‑wise classification, sequence labeling, and a pointer‑network—using precision, recall, F1, and a word‑level recall metric (WR). Results show an F1≈50 %, reflecting severe class imbalance and the task’s difficulty. Finally, we demonstrate that incorporating detected omissions in a simple post‑editing sequence‑to‑sequence pipeline substantially boosts summary quality, underscoring the practical value of omission detection. OLDS is publicly released to foster further research on omission mitigation in dialogue summarization.</sample>
    <sample id="147">Ci sono tre autori: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dell’Università di Trento e della Fondazione Bruno Kessler e vi presenterò brevemente l’articolo **“Attention as a Guide for Simultaneous Speech Translation”**. Si tratta di un lavoro congiunto con Matteo Negri e Marco Turchi.

Cos’è la traduzione simultanea del parlato (SimulST)?  
La traduzione simultanea del parlato è il processo di traduzione del linguaggio parlato in testo in un’altra lingua in tempo reale, permettendo così comunicazioni interculturali senza ritardi.

E quali sono i problemi dei modelli SimulST attuali?  
I modelli attuali richiedono di addestrare specifiche architetture, introducendo ulteriori moduli da ottimizzare. I processi di addestramento sono lunghi e complessi, spesso comportando diversi obiettivi di ottimizzazione. Inoltre, è necessario addestrare e mantenere diversi modelli per raggiungere diversi livelli di latenza: ad esempio, un modello con una latenza media di un secondo, un altro con due secondi, e così via.

Qual è la nostra soluzione?  
1. Utilizzare modelli di traduzione del parlato (ST) offline già esistenti, senza ri‑addestrarli o adattare architetture specifiche per SimulST.  
2. Usare un solo modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.  
3. Sfruttare le conoscenze già acquisite dal modello tramite il meccanismo di attenzione tra input audio e output testuale – ovvero la cross‑attention – come mostrato nell’esempio a destra.

La nostra proposte è **EDAtt** (Encoder‑Decoder Attention). Si tratta di una strategia per decidere se emettere o meno una traduzione parziale, basandosi su dove punta l’attenzione. Una parola viene emessa se l’attenzione non è concentrata: la somma è inferiore a una certa soglia \(\alpha\) rispetto ai \(\lambda\) frame audio più recenti, indicanti che le informazioni ricevute sono abbastanza stabili.  

Per esempio, se riceviamo un chunk di parlato contenente “I’m going to talk about…” e il modello prevede la traduzione in tedesco, guardiamo i pesi di cross‑attention: le prime due parole puntano ai frame audio più antichi, mentre l’ultima parola punta ai frame audio più recenti (\(\lambda\)). Ciò significa che le prime due parole verranno emesse, perché la somma dell’attenzione è sopra la soglia \(\alpha\) e quindi non emettiamo l’ultima parola finché non arriva un altro chunk. Se successivamente arrivasse un altro chunk e il modello prevedesse altre tre parole, guardando i pesi di cross‑attention verrebbe notato che nessuna parola punta ai frame \(\lambda\) più recenti: quindi quelle tre parole verranno emesse.

I risultati principali di EDAtt vengono rappresentati su grafici in cui a sinistra si vede la **BLEU** (qualità della traduzione) e a destra la **average lagging** (misura di latenza). Consideriamo anche la **computational‑aware average lagging**, che tiene conto dei tempi computazionali del modello per prevedere l’output. Cerchiamo di ottenere curve il più alte possibile sul grafico, ma spostate verso sinistra.

Confrontiamo EDAtt con strategie popolari applicate a modelli offline, come **Wait‑k** e **Local Agreement**, e con l’architettura allo stato dell’arte specificamente progettata per la traduzione simultanea pre‑traduzione. I risultati mostrano che EDAtt supera tutte le strategie applicate ai modelli offline: le curve sono spostate verso sinistra. Se consideriamo il tempo effettivamente trascorso (computational‑aware time), EDAtt è la strategia più veloce.

Se volete scoprire ulteriori risultati, leggete il nostro articolo. Inoltre, abbiamo rilasciato il codice e i modelli open‑source, così come l’output simultaneo, per facilitare la riproducibilità del nostro lavoro.

Grazie per l’attenzione.</sample>
    <sample id="149">Sì. Il nuovo set di dati chiamato **CoNLL++** è stato reso disponibile pubblicamente insieme al paper, quindi chiunque può scaricarlo e utilizzarlo per i propri esperimenti.</sample>
    <sample id="150">**Riassunto (≈ 200 parole)**  

Gli autori presentano *MeetingQA*, un nuovo dataset extractive‑QA costruito su trascrizioni di meeting reali (AMI corpus). La motivazione è che, sebbene molte ricerche si siano concentrate su riassunti e action items, la componente di domande e risposte è stata poco sfruttata. In MeetingQA, le domande sono poste da partecipanti durante il meeting e richiedono risposte dettagliate; le risposte possono coinvolgere più speaker, essere distribuite su frasi discontinui o includere domande retoriche.  

Il dataset contiene 7.7 k domande (30 % non rispondibili) suddivise in train/dev/test. Il 40 % delle domande rispondibili ha risposte multi‑span e il 48 % multi‑speaker; il 70 % degli answer‑spans multi‑speaker mostra disaccordo. Le domande sono in media 12 parole, le risposte 35. La qualità annotativa è confermata da un α di Krippendorff di 0.73 e la performance umana sul test è F1 = 84.6.  

Gli autori testano diversi approcci: modelli a breve contesto con retrieval, modelli single‑span (inizio‑fine frase) e modelli multi‑span (token‑classification). In fine‑tuned setting, i migliori modelli raggiungono F1 ≈ 59, con un gap di 25 punti rispetto all’uomo; i modelli a breve contesto (RoBERTa) superano quelli a lungo contesto (Longformer). In zero‑shot, il gap è di quasi 50 punti, ma l’augmentation con “silver” annotazioni (MediaSum) migliora significativamente le prestazioni.  

Errori principali: difficoltà con domande retoriche e identificare il speaker che risponde, soprattutto in zero‑shot. In sintesi, MeetingQA offre un nuovo, difficile benchmark per QA in domini conversazionali lunghi e complessi.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e con il collega Zhiyang presenteremo la nostra ricerca su **MultiInstruct**, che migliora l’apprendimento zero-shot multimodale tramite l’“instruction tuning”.  

Con l’avanzamento dei **large language models**, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento che riutilizzano modelli linguistici pre‑addestrati per compiti diversi, in modo efficiente sia in termini di parametri che di dati. Ultimamente, diversi studi hanno dimostrato che l’“instruction tuning” permette ai modelli di eseguire compiti non visti in modalità zero‑shot semplicemente seguendo istruzioni in linguaggio naturale. Tuttavia, la maggior parte delle ricerche sull’instruction tuning si è concentrata sui compiti solo linguistici, lasciando per lato i compiti di visione computazionale e multimodali.  

In questo lavoro vogliamo quindi indagare se l’“instruction tuning” di modelli multimodali pre‑addestrati possa effettivamente migliorare la generalizzazione verso compiti multimodali non visti. Inoltre, al momento della nostra ricerca, abbiamo riscontrato una discrepanza significativa nella disponibilità di dataset di istruzioni tra NLP e multimodale: esistono più di 1600 compiti di istruzioni solo linguistici, ma non esiste un dataset di istruzioni multimodali di larga scala pubblicamente disponibile. Questo ci ha motivato a costruire un dataset di “instruction tuning” multimodale.  

Qui presentiamo **MultiInstruct**, il primo benchmark di “instruction tuning” multimodale che comprende 62 compiti diversi coprendo 10 categorie ampie. Questi compiti derivano da 21 dataset open‑source esistenti e ciascuno è accompagnato da cinque istruzioni scritte da esperti.  

Per investigare l’“instruction tuning” multimodale sul nostro dataset, utilizziamo **OFA**, un modello multimodale pre‑addestrato unificato, come modello base. OFA utilizza un vocabolario unificato per il linguaggio, i token delle immagini e le coordinate di una bounding box. Mostriamo alcuni esempi dal nostro dataset MultiInstruct per unificare l’elaborazione di vari tipi di input e output. Seguiamo il metodo di OFA e formuliamo tutti i compiti in un formato sequence‑to‑sequence unificato, in cui testo, immagini, istruzioni e bounding box sono rappresentati nello stesso spazio di token.  

**Ora parlo dell’“instruction tuning” multimodale.**  
Per il dataset di training usiamo 53 compiti da 9 gruppi, campionando 10.000 istanze per compito. Per il test riserviamo l’intero gruppo “common sense reasoning” e selezioniamo ulteriori 5 compiti dal gruppo “VQ” e “Miscellaneous”. Usiamo tutte le istanze del test split per ogni compito. Inoltre, estraiamo casualmente 20 compiti dal test split di “natural instructions” come compito NLP non visto.  

Partiamo dal modello pre‑addestrato OFA Large. Durante l’addestramento, mescoliamo tutte le istanze di tutti i compiti. Ogni istanza è combinata casualmente con una delle sue cinque istruzioni. Durante il test, per ogni compito eseguiamo 5 esperimenti usando ciascuna delle 5 istruzioni. In ogni esperimento riportiamo la performance minima, massima e la deviazione standard delle 5 esperienze. Per compiti di classificazione multimodale riportiamo l’accuratezza; per compiti di generazione multimodale, l’**ROUGE‑L**; per i compiti NLP usiamo anch’esso ROUGE‑L.  

Abbiamo inoltre introdotto un nuovo metriche chiamata **sensibilità**: misura la capacità del modello di produrre risultati coerenti per la stessa attività indipendentemente da leggere variazioni nella formulazione dell’istruzione.  

**Risultati principali**: l’“instruction tuning” migliora significativamente le performance di OFA sui compiti multimodali osservati. Il trasferimento dall’insieme di istruzioni naturali beneficia l’“instruction tuning”. Con l’aumentare del numero di compiti, il modello ottiene prestazioni migliori e sensibilità più bassa.  

Abbiamo eseguito un esperimento confrontando una sola istruzione contro cinque. Come previsto, usare più istruzioni migliora le prestazioni complessive e riduce notevolmente la sensibilità. Questo evidenzia l’effetto delle diverse strategie di fine‑tuning sulla sensibilità del modello.  

Il trasferimento dall’insieme di istruzioni naturali permette a OFA di ottenere sensibilità molto migliore rispetto al modello OFA originale, e di migliorare le performance anche sul dataset di “natural instruction”.  

In sintesi, proponiamo il primo dataset di “instruction tuning” multimodale su larga scala, migliorando drasticamente le capacità di OFA, e esploriamo diverse tecniche di trasferimento che mostrano benefici significativi. Abbiamo anche introdotto la metrica di sensibilità.  

Stiamo raccogliendo un dataset di “instruction tuning” multimodale ancora più grande, con circa 150 compiti di visione‑linguaggio aggiuntivi, e lo rilasceremo.  

Ecco il QR code per i dati e il modello. Grazie.</sample>
    <sample id="152">**Sunto dell’intervento di Frederick Riemenschneider – “Exploring Large Language Models for Classical Philology”**

Frederick introduce i recenti progressi nell’intersezione NLP‑classical philology, evidenziando le limitazioni dei modelli attuali: tutti sono BERT‑based, monolinguali e con performance poco documentate. Per superare queste lacune, il progetto ha provveduto a creare quattro modelli: GreBERTa (RoBERTa per il greco antico), GreTa (T5 encoder‑decoder per il greco), PhilBERTa e PhilTa (multilingue Greco‑Latino‑Inglese).  

Il pre‑training ha sfruttato il *Open Greek &amp; Latin* più una nuova corpora di testi greci recuperata dall’Internet Archive; quest’ultimo è stato costruito identificando errori OCR (es. “γάρ” → “yap”) per filtrare documenti greci e r‑ocrarli correttamente. Per il latino si è usato *Corpus Corporum* e per l’inglese testi antichi.

Per valutare i modelli si è ricorso a treebanks UD (greco) e al dataset EvaLatina 2022 (latino), concentrandosi su POS‑tagging, parsing dipendenza e lemmatizzazione. I risultati mostrano un chiaro superamento dello stato‑dell’arte, con l’encoder‑decoder GreTa che migliora la lemmatizzazione di circa 5 pp. L’analisi del singolo encoder T5 ha rivelato una performance inizialmente peggiorata rispetto a modelli encoder‑only, ma che converge con sufficiente addestramento.

Infine, sono stati testati i modelli su compiti di conoscenza semantica e del mondo: sinonimi/antonymi, relazioni eroe‑dio e benefici della multilingua. I risultati indicano migliori performance rispetto ai modelli precedenti, sebbene la differenza tra monolingua e multilingua sia minima.  

Il lavoro presenta modelli di nuova generazione, con tokenizer nativo, pre‑training di alta qualità e benchmark rigorosi, aprendo nuove prospettive per la filologia classica.</sample>
    <sample id="153">In questo intervento, Ninareh Mehrabi presenta il lavoro “Resolving Ambiguities in Text-to-Image Generative Models”, focalizzato sull’analisi e la mitigazione delle ambiguità nei prompt forniti ai modelli di generazione di immagini dal testo. Viene illustrato il motivo principale: prompt come “The girl enters the room with flowers” possono essere interpretati in modi diversi (fiori dentro la stanza, con la ragazza, o entrambi), rendendo difficile per i modelli produrre immagini che rispecchino l’intenzione reale dell’utente.  

Per affrontare il problema, l’autrice descrive un pipeline articolata in tre fasi: (1) curazione di un benchmark, basato sulla già esistente raccolta LAVA, che categorizza vari tipi di ambiguità; (2) disambiguazione tramite un framework in cui un modello di linguaggio genera domande di chiarimento (o, in alternativa, diverse interpretazioni visive) che l’utente risponde; la risposta viene concatenata al prompt originale per formare un prompt disambiguato; (3) valutazione della fedeltà delle immagini generate.  

La valutazione utilizza un modello VQA: l’immagine e la domanda che esplicita l’intenzione dell’utente vengono inserite nel modello, il quale restituisce “yes” se l’immagine soddisfa l’intenzione. I risultati mostrano che la disambiguazione migliora la fedeltà delle immagini, che l’efficacia varia a seconda del tipo di ambiguità, e che il metodo automatico concorda con la valutazione umana. L’autrice conclude evidenziando l’importanza di un benchmark e di strumenti di disambiguazione per rendere i modelli di generazione di immagini più affidabili e centrati sull’utente.</sample>
    <sample id="154">Gli autori del lavoro sono affiliati alla **Università di Trento** e alla **Fondazione Bruno Kessler (FBK)**.</sample>
    <sample id="155">Il relatore è **Javad Hosseini**.</sample>
    <sample id="157">**Riassunto**

Shen Gao e colleghi presentano SDDS (Static‑Dynamic Structure Fusion Graph), un nuovo approccio per la **dialogue summarization**. Il problema principale dei metodi esistenti è l’uso di grafici statici pre‑calcolati con strumenti linguistici (parsing del discorso, tracking dello stato del dialogo). Questi dipendono dalla qualità degli strumenti esterni e non si adattano dinamicamente al compito di riassunto, causando errori di propagazione e rigidità.

Il modello SDDS è composto da quattro blocchi principali:

1. **Encoder degli utterance**: trasforma ogni frase del dialogo in un vettore denso.
2. **Costruzione del grafo statico**: quattro tecniche euristiche generano quattro matrici di adiacenza:
   - *Discourse‑Parsing Graph* (dipendenze discorsive),
   - *Key Co‑occurrence Graph* (conteggio di parole chiave comuni),
   - *Speaker‑Interaction Graph* (finestra mobile per frequenza dei parlanti),
   - *Position Graph* (distanza relativa con embedding).
   Le quattro matrici vengono fuse con una convoluzione 1×1, ottenendo un grafo statico combinato.
3. **Modulo Static‑Dynamic Graph**: utilizza un multi‑head attention sui vettori degli utterance per costruire un grafo dinamico che cattura relazioni semantiche non presenti nei dati statici. I due grafi (statico e dinamico) vengono poi combinati in un grafo unificato \(G^u\).
4. **Generatore di riassunto**: si basa su un modello pre‑addestrato (es. BART) e impiega un meccanismo di dual cross‑attention, inserendo uno strato di graph‑attention sopra l’auto‑attenzione standard per integrare la struttura dialogica nella generazione.

Il risultato è un riassunto più accurato e informativo, con il codice e i dati disponibili su GitHub.</sample>
    <sample id="158">Il talk di Qipeng Guo presenta il problema della coreferenza nei documenti lunghi e propone una soluzione basata su due cache. Nel coreference resolution l’obiettivo è individuare tutte le menzioni di un’entità e raggrupparle in cluster. I metodi tradizionali enumerano tutte le coppie di menzioni, portando a una complessità quadratica. Le recenti tecniche “cache‑based” riducono la complessità a lineare grazie a una cache fissa: quando la cache è piena, viene eliminata l’entità meno recentemente usata (LRU). Tuttavia, in testi lunghi il tema può cambiare spesso e le menzioni di un’entità possono essere sparse; l’LRU provoca quindi molte cache miss, soprattutto per le entità ad alta frequenza.

Guo propone quindi una “dual cache” composta da una cache locale e una globale. La cache locale, con politica LRU, memorizza entità locali; la cache globale, con politica LFU, memorizza entità globali, eliminando la meno usata quando è piena. Durante la scansione del testo, ogni nuova menzione viene classificata come nuova entità o come appartenente a una già presente nella cache; se la frequenza è sufficiente, l’entità viene aggiunta alla cache globale, altrimenti alla cache locale.

La prova è stata eseguita su quattro benchmark (LitBank, OntoNotes, WikiCoref e un libro annotato con 30 000 parole). Nei dataset con training, la dual cache supera i baseline anche se questi usano memoria illimitata; nei dataset senza training, la dual cache è leggermente inferiore in accuratezza ma più veloce. Per il libro, la differenza di performance è significativa. Inoltre, la dual cache riduce drasticamente le cache miss rispetto a una singola cache. Guo conclude che la dual cache offre il miglior rapporto qualità/efficienza, combinando la gestione locale e globale delle entità per ridurre i costi computazionali senza sacrificare la precisione.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha e sono lieto di darvi il benvenuto alla presentazione del nostro articolo ACL 2023. I giudizi di accettabilità dei modelli linguistici non sono sempre robusti rispetto al contesto. Questo lavoro è una collaborazione con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo studio riconsideriamo i paradigmi delle coppie minime. Il paradigma delle coppie minime valuta i modelli linguistici sulla base di giudizi di accettabilità, che possono includere anche la grammaticalità (come BLiMP, SyntaxGym) o l'accettabilità in termini di stereotipi (come CrowS pairs). Nel paradigma delle coppie minime, il modo tipico di valutare un modello linguistico è mostrare una frase accettabile o grammaticale e poi mostrare una frase accettabile o non grammaticale. L'aspettativa è che il modello assegni una probabilità maggiore alla frase accettabile.

Il pipeline attuale delle coppie minime non permette di valutare l'accettabilità di frasi più lunghe. Queste settimane i modelli linguistici di grande scala stanno espandendo i propri contesti di lunghezza, quindi è cruciale valutare l'accettabilità dei modelli lungo l'intera finestra di contesto, e questo è ciò che stiamo cercando di fare qui. Stiamo rivedendo il pipeline delle coppie minime chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.

Per simulare queste sequenze più lunghe, riconsideriamo i dataset stessi e ricreiamo frasi scegliendo frasi accettabili o inaccettabili da quei dataset. Ad esempio, abbiamo scelto una tipica coppia di grammaticalità dal dataset BLiMP, dal caso Adjunct Island. Per ricreare sequenze più lunghe, estraiamo frasi grammaticali da Adjunct Island e le aggiungiamo come prefisso sia alla query accettabile sia alla query inaccettabile. Possiamo fare la stessa cosa scegliendo frasi inaccettabili dallo stesso fenomeno, e queste possono anche essere usate per testare l'accettabilità del modello. Possiamo anche fare la stessa operazione scegliendo frasi da un sottoinsieme diverso o da un dataset diverso: questo è quello che chiamiamo scenario di mismatch. In questo caso le frasi provengono comunque da dataset rilevanti, ma non dal dataset con cui si sta valutando. Possiamo fare lo stesso per il caso di inaccettabilità. Infine, possiamo scegliere frasi da un dominio completamente non correlato, come Wikipedia. Ciò ci dirà se i giudizi di accettabilità del modello sono effettivamente influenzati da qualsiasi contesto, sia esso derivato da un diverso sottoinsieme del dataset, sia completamente irrilevante per la frase attuale che stiamo esaminando.

Come si comporta il modello? Per prima cosa, guardiamo le frasi di Wikipedia, completamente irrilevanti per la coppia di query, e troviamo che i giudizi MPP sono per lo più robusti lungo la lunghezza del contesto. Incrementiamo la lunghezza del contesto fino a 1024 token per sfruttare al massimo OPT e GPT-2. Vediamo che nella linea a puntini arancioni i giudizi MPP rimangono relativamente stabili. Cosa succede quando scegliamo frasi dallo stesso dataset? Qui scegliamo o creiamo frasi accettabili e inaccettabili dallo stesso dataset BLiMP o SyntaxGym. Vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o inaccettabili. Ma quando corrispondiamo la struttura, cioè scegliamo frasi dallo stesso fenomeno in BLiMP o SyntaxGym, osserviamo un enorme aumento o una grande diminuzione del giudizio MPP per il modello, a seconda che il prefisso scelto sia accettabile o inaccettabile. Questo effetto enorme cresce lungo la lunghezza del contesto e probabilmente influenzerà i modelli linguistici più recenti con finestre di contesto più ampie.

Perché il prefisso corrispondente influisce così tanto sul giudizio del modello? Abbiamo eseguito una serie di analisi in cui abbiamo perturbato la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore. Dopo aver eseguito diverse perturbazioni, scopriamo che nessuna di queste variazioni fa cambiare il giudizio MPP del modello. In pratica, i modelli sono sensibili alle frasi perturbate in modo simile: quando perturbiamo frasi accettabili vediamo un aumento simile in tutte le perturbazioni, e quando perturbiamo frasi inaccettabili vediamo una diminuzione simile.

Le conclusioni principali del nostro lavoro sono che i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti condivise tra le frasi. Il modo in cui eseguiamo attualmente la valutazione MPP, con input brevi e di una sola frase, potrebbe non catturare completamente la conoscenza astratta del modello lungo l'intera finestra di contesto. Vi invitiamo a leggere il nostro articolo per maggiori dettagli degli esperimenti. Grazie per l'attenzione.</sample>
    <sample id="160">Il primo passaggio mappa ogni token dell’input con **un multiset (insieme non ordinato) di token che compariranno nell’output**.</sample>
    <sample id="161">CoScript contains **55,000 scripts** (one per specific goal).</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è **MASSalign**.</sample>
    <sample id="164">Il vantaggio principale dell’apprendimento scarsamente supervisionato è **la riduzione dei costi di annotazione**: invece di etichettare manualmente ogni esempio, si può usare una varietà di fonti “deboli” (regole di heuristica, knowledge‑bases, crowd‑source di bassa qualità, ecc.). Queste annotazioni sono economiche e scalabili, permettendo di addestrare modelli su grandi quantità di dati anche quando le etichette sono rumorose.</sample>
    <sample id="165">Il relatore Wenting Zhao, dottoranda al Cornell, presenta la sua nuova ricerca intitolata “Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations”. Inizia spiegando l’abduction: a partire da un contesto X (per esempio “Emily è rimasta in traffico”) e un risultato Y (“Emily è arrivata al volo”), il compito è individuare un’ipotesi plausibile che colmi il divario. Un esempio concreto mostra due spiegazioni possibili – “Il volo era in ritardo” e “Il volo era partito puntuale” – dove solo la prima è coerente con l’esito. Il problema è che le annotazioni di plausibilità sono soggettive e rumorose: un esperimento ha rilevato un accordo del 60 % solo su oltre 1.000 spiegazioni.

Zhao propone LiPoR (Likelihood Learning with Posterior Regularization), un approccio non supervisionato che tratta le spiegazioni Z come variabili latenti. L’obiettivo è massimizzare la probabilità marginale di Y dato X, integrando su tutte le possibili Z, senza richiedere etichette di plausibilità. Per guidare la scelta verso spiegazioni “buone”, viene introdotto un regularizzatore basato sull’esclusività mutua: l’entropia di P(Z | X,Y) è confrontata con log M (M = numero di spiegazioni plausibili). Se l’entropia è troppo alta, si penalizza, costringendo il modello a concentrarsi su un sottoinsieme di spiegazioni.

I risultati su AlphaNLI, il dataset più diffuso per l’abduction, dimostrano che LiPoR supera tutti i baseline zero‑shot, compreso GPT‑3, con un margine di oltre 4 punti assoluti in accuratezza. Il lavoro è disponibile all’indirizzo tinyurl.com/zhao‑lipor.</sample>
    <sample id="166">**Abstract**

We propose a Neural Divide‑and‑Conquer Reasoning (NDCR) framework for image retrieval from linguistically complex text, a task that challenges conventional visual‑language models due to highly similar images and long, dense descriptions. Inspired by Divide‑and‑Conquer strategies and Dual‑Process Theory, NDCR combines an analogical “System 1” with a logical “System 2”. First, a **Proposition Generator** decomposes a complex query into a set of simple propositions; a BART decoder then produces natural‑language explanations of these propositions. Second, the **Visual‑Linguistic Interactor** (System 1) jointly encodes each simple proposition and the image, yielding matching scores and intermediate reasoning states. Third, the **Neural‑Symbolic Reasoner** (System 2) integrates these states through a negation executor and a conjunction operation, thereby performing logical inference to reconstruct the answer to the original complex query. Finally, the outputs of System 1 and System 2 are fused to produce the final retrieval result.

Extensive experiments on benchmark datasets demonstrate that NDCR outperforms state‑of‑the‑art baselines. Ablation studies confirm the necessity of each module, and case analyses show that the framework transparently exposes intermediate inference states. We conclude that neural‑symbolic reasoning, coupled with divide‑and‑conquer decomposition akin to chain‑of‑thought, offers a promising direction for enhancing compositional reasoning in large language models and multimodal retrieval systems.</sample>
    <sample id="167">In DEPLAIN‑web sono stati elaborati 750 documenti.  
Per ciascuno di essi è stato effettuato un allineamento **manualmente** (cioè, le frasi sono state accoppiate a mano, fornendo così un gold standard) e, contemporaneamente, è stato applicato un metodo di allineamento **automatico** (in particolare si è dimostrato che il metodo MASSalign è il più efficace per la lingua tedesca).  
Il risultato finale è un set di circa 30 450 coppie di frasi allineate, con la versione manuale usata per valutare e validare gli allineamenti automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato costruito raccogliendo articoli di notizie Reuters del 2020 e annotandoli manualmente con le stesse regole di tagging di CoNLL‑2003 (I‑B‑PER, I‑B‑LOC, ecc.). In questo modo si ottiene un corpus “modern” con annotazioni coerenti con il benchmark originale.</sample>
    <sample id="169">David Vilar e i suoi colleghi di Google Translate presentano “Prompting PaLM for Translation: Assessing Strategies and Performance”, un’analisi sistematica dei metodi di prompting per la traduzione con il modello PaLM (540 milliardi di parametri, addestrato su 780 biliardi di token). Lo studio valuta la capacità di traduzione utilizzando le migliori pratiche della comunità MT: test set recenti per evitare sovrapposizioni di dati, confronti con i sistemi leader di WMT, metriche neural MT (BLEURT, ecc.) e una valutazione umana basata sul framework MQM.

Gli autori mostrano come la scelta del prompt influisca fortemente sulle prestazioni: in un esperimento di one‑shot, due prompt diversi producevano differenze di oltre un punto BLEURT (in alcuni casi 40 punti). Con cinque‑shot, la forma del prompt diventa trascurabile rispetto al contenuto degli esempi. La qualità degli esempi di traduzione è più importante della loro somiglianza al testo sorgente; esempi provenienti da dati di sviluppo (più curati) superano quelli tratti dal corpus di addestramento.

I risultati indicano che PaLM si avvicina a sistemi commerciali, ma resta inferiore ai sistemi MT specializzati. La valutazione MQM evidenzia una fluidità comparabile ai leader, mentre l’accuratezza è più debole, con errori di omissione frequenti. Lo stile di PaLM è invece meno “awkward” rispetto ai sistemi di punta. Gli autori raccomandano l’uso di prompt 5‑shot con esempi di alta qualità e suggeriscono ulteriori ottimizzazioni per migliorare l’accuratezza.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang e faccio parte della Penn State University. Oggi vi presenterò il nostro lavoro intitolato **“XSemPLR: Cross‑Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations”**.

Il *semantic parsing* è il compito di costruire rappresentazioni semantiche delle query degli utenti, come SQL o Lambda Calculus. Il *Cross‑Lingual Semantic Parsing* consiste nel tradurre le query in diverse lingue naturali in diverse rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre le query in più lingue naturali con modelli neurali verso SQL, Lambda o FunQL, e così via.

I modelli di cross‑lingual semantic parsing esistenti sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Per esempio, ci sono molte coperture su certe lingue naturali, ma il cinese è assente e manca copertura su certe rappresentazioni di significato. La calcolistica Lambda è assente, o vengono valutati solo su certi modelli neurali. Ad esempio, c’è solo un singolo modello per valutarli. Per questo motivo proponiamo XSemPLR.

**Cosa offriamo con XSemPLR?**  
- Un *benchmark* uniforme per il cross‑lingual semantic parsing in molte lingue naturali e molte rappresentazioni di significato.  
- Il dataset contiene 9 dataset in vari domini, 5 compiti di semantic parsing, 8 rappresentazioni di significato e 22 lingue naturali appartenenti a 15 famiglie linguistiche.

Per valutare meglio il benchmark, consideriamo sei configurazioni di addestramento e valutazione:

1. **Translate‑Test** – Utilizziamo l’API di Google Translate per tradurre la sorgente nella lingua target, quindi addestriamo e valutiamo un modello monolingue. Ad esempio, addestriamo un modello in inglese su query inglesi e, al test, traduciamo la query tedesca in inglese tramite API e poi usiamo il modello addestrato per prevedere lo SQL.

2. **Monolingual Model** – La lingua sorgente è la stessa della lingua target, ad es. tedesco → tedesco o inglese → inglese.

3. **Monolingual Few‑shot** – Addestriamo modelli monolingui con solo il 10 % dei dati di training.

4. **Multilingual Model** – Addestriamo un unico modello multilingue su tutte le lingue (es. tedesco, inglese, cinese). Alla predizione possiamo usare questo modello per tradurre query tedesche, cinesi, ecc.

5. **Cross‑lingual Zero‑shot Transfer** – Addestriamo su una lingua sorgente (es. inglese) e trasferiamo su un’altra lingua. Durante l’addestramento, addestriamo su query inglesi o su un mix di query inglesi e tedesche in modalità few‑shot per predire lo SQL.

6. **Cross‑lingual Few‑shot Transfer** – Simile al caso zero‑shot, ma con un piccolo set di esempi nella lingua target.

**Analisi dei modelli monolingui**  
Abbiamo valutato due gruppi di modelli:

- **Encoder‑PTR**: Encoder multilingue pre‑addestrati con decoder a puntatore, ad es. XLM‑R + PTR e mBERT + PTR.
- **Encoder‑Decoder**: Modelli pre‑addestrati multilingue encoder‑decoder, ad es. mBART e mT5.

Scoperto che gli Encoder‑Decoder ottengono le migliori prestazioni su tutti e nove i dataset.

Abbiamo anche valutato mT5 e XLM‑R + PTR in configurazione multilingue. È emerso che sia gli Encoder‑Decoder sia gli Encoder‑PTR possono migliorare quando addestrati su un mix di lingue, ma la maggior parte delle lingue principali ottiene un guadagno di prestazioni, tranne l’inglese che diminuisce in sette dataset e aumenta solo in tre. Questo fenomeno è noto come **“Curse of Multilinguality”**.

**Gap di performance cross‑lingua**  
Nel grafico, la linea blu rappresenta il *Cross‑lingual Few‑shot transfer*, l’arancione il *Zero‑shot transfer*, e la verde il *Monolingual setting*. Confrontando la verde con l’arancione troviamo un gap significativo per il Zero‑shot; confrontando la blu con l’arancione, notiamo che con il Few‑shot il gap si riduce rapidamente.

Altre scoperte interessanti:

- Gli Encoder‑Decoder superano i lavori precedenti o raggiungono risultati comparabili.
- Il pre‑addestramento su inglese può aumentare significativamente la performance nel Few‑shot per le lingue target.
- Modelli multilingue come Codex e BLOOM risultano ancora inadeguati per i compiti di cross‑lingual semantic parsing.

**Conclusioni**  
Abbiamo creato XSemPLR, un benchmark unificato per il cross‑lingual semantic parsing con molte lingue naturali e molte rappresentazioni di significato. Abbiamo condotto uno studio di benchmark completo su tre tipologie di modelli multilingue rappresentativi, e i risultati mostrano molte scoperte interessanti.

Per approfondire, vi invito a visitare il nostro paper e il codice. Grazie per l’attenzione.</sample>
    <sample id="171">I rispondo in sintesi:  

- **Lavori di watermarking statistico**: inserimento di segnali di “firma” in embedding o in parametri del modello per verificare l’origine.  
- **Lavori di back‑door watermarking**: uso di trigger (parole o sequenze) che attivano un comportamento “segreto” negli embeddings.  
- **Lavori su “model‑extraction” e “stealing”**: analisi di come gli aggressori estraggono e riproducono embeddings da un servizio.  
- **Lavori generali di protezione del copyright per LLM**: tecniche di watermarking applicate a interi modelli o a API di generazione di testo.

Tutti questi approcci sono stati menzionati come “lavori esistenti” nella presentazione, ma la nota è che nessuno di essi è pienamente applicabile ai servizi di embedding‑as‑a‑service o garantisce la trasferibilità della watermark durante il processo di estrazione.</sample>
    <sample id="172">No – the study found that multilingual LLMs such as Codex and BLOOM are still inadequate for cross‑lingual semantic parsing tasks.</sample>
    <sample id="174">Il video presenta ArgAnalysis35K, un dataset di 35.000 coppie “argomento‑analisi” che si distingue per qualità, scala e diversità rispetto ai corpus esistenti. Gli argomenti provengono principalmente da discorsi di tornei di debate, debbitori esperti e intermedi (85 %) con una minoranza di partecipa­nti principianti (15 %), garantendo così una maggiore affidabilità rispetto al crowdsourcing puro. Il corpus copre 24 temi scelti in base all’esperienza in circuiti di debate e a siti specializzati, evitando la limitata selezione di 30‑40 movimenti tipica delle raccolte precedenti.

Una novità è la voce “analisi”: non è solo una premessa o un’affermazione, ma un insieme coerente di claim e fondamenti che spiegano perché un'argomentazione è valida. Questo concetto aiuta a comprendere meglio la struttura degli argomenti.

Il dataset introduce anche l’“instance‑based annotator reliability”, valutando l’affidabilità di un annotatore su singoli argomenti, piuttosto che eliminarlo completamente se mostra bias su alcuni temi. Questo permette di sfruttare più annotazioni senza perdere informazioni preziose.

Infine, viene proposto un modello di “relevance”, che assegna a ogni argomento un punteggio di pertinenza per ciascun tema, riconoscendo che un’affermazione come “l’accountabilità è importante” può valere in molteplici contesti (governo, religione, azienda). In sintesi, ArgAnalysis35K offre un corpus più ampio, vario, annotato con precisione e con strumenti innovativi per l’analisi della qualità e della pertinenza degli argomenti.</sample>
    <sample id="175">Il metodo tratta le permutazioni come variabili *latent* e non fissa a priori una struttura di output.  
Per ogni coppia input‑output, più permutazioni possono rendere valida la corrispondenza, quindi il modello deve imparare quale ordine è “linguisticamente corretto”.  

1. **Induzione dell’allineamento** – durante l’addestramento il modello stesso scopre a quale token di input appartiene ciascun token di output, senza dati di allineamento a mano.  
2. **Relaxazione continua** – per trovare la permutazione di massima probabilità (NP‑hard, simile al TSP) il modello utilizza una relaxazione GPU‑friendly della matrice di permutazione.  
3. **Back‑propagation** – grazie a questa relaxazione, il gradiente può fluire attraverso la soluzione approssimata, permettendo al modello di affinare le permutazioni verso quelle più plausibili dal punto di vista linguistico.

In sintesi, l’ambiguità delle permutazioni è gestita imparando l’allineamento durante il training e ottimizzando la permutazione tramite una relaxazione continua che rende il problema computazionalmente tractabile e adattabile.</sample>
    <sample id="176">L’equità (fairness) di un modello NLP “a valle” è intesa come la **assenza di discriminazioni sistematiche nei risultati del modello rispetto a gruppi distinti** (demografici, politici, etnici, ecc.).  
In pratica si valuta che le metriche di prestazione (accuratezza, TPR, FPR, precisione, ecc.) siano **pari o quasi pari** per tutti i gruppi considerati, o che non vi siano differenze statisticamente rilevanti tra di essi.  
Questa valutazione può avvenire tramite:

* **Equalized Odds / Equal Opportunity** – TPR/FPR identici tra i gruppi.  
* **Disparate Impact** – rapporto tra metriche chiave (es. TPR) che non scende sotto una soglia accettabile.  
* **Calibrazione per gruppo** – probabilità predette devono corrispondere con le verità reali per ogni gruppo.  

In sintesi, l’equità di un modello NLP a valle è definita dalla sua capacità di produrre risultati equi e non discriminatori tra i diversi gruppi su cui viene testato.</sample>
    <sample id="177">The speaker is **Yanis Labrak**.</sample>
    <sample id="178">Il relatore è **Koustav Sinha**.</sample>
    <sample id="179">**Sintesi (≈200 parole)**  

Melanie Sclar presenta “SymbolicToM”, una metodologia plug‑and‑play che migliora il ragionamento della Theory of Mind (ToM) nei grandi modelli di linguaggio (LLM). La ToM è la capacità di dedurre le credenze mentali degli altri; viene tradizionalmente testata con domande di “falsa credenza” (false‑belief) in situazioni come il classico test Sally‑Anne. I LLM, incluso ChatGPT e GPT‑3, mostrano notevoli difficoltà in queste domande, soprattutto di ordine secondario.  

SymbolicToM costruisce rappresentazioni grafiche “mental state” (es. BBob, BBob,Alice) durante l’inferenza, utilizzando modelli NLI e OpenIE pre‑addestrati. Queste grafiche rappresentano, per ciascun personaggio e per ogni livello di ToM fino a un massimo definito, ciò che il personaggio crede sia lo stato del mondo e ciò che crede che un altro personaggio creda. Per una domanda, il sistema identifica le entità, recupera la grafiche appropriata, risolve la domanda come una domanda fattuale sul grafo e infine invia il risultato al LLM per l’output finale.  

Gli esperimenti confrontano SymbolicToM con modelli finemente addestrati (GPT‑3 fine‑tuned) e con Textual Time Travel. Nei test ToMi, SymbolicToM migliora drasticamente le prestazioni: +65 punti per GPT‑3‑Davinci, +67 per Macaw, +51 per Flan‑T5‑XXL, ecc.  

Per testare la generalizzazione si hanno due nuovi dataset: (1) “story‑structure” (D₁, D₂, D₃) che concatenano storie o alterano la struttura, e (2) “ParaphrasedToMi”, con maggiore diversità linguistica. I modelli supervisionati perdono quasi il 50 % delle performance su D₁, mentre SymbolicToM mantiene guadagni significativi, permettendo a GPT‑4 di risolvere il set con un incremento di 42 punti su D₁.  

In conclusione, SymbolicToM è un algoritmo di inferenza che evita l’overfitting, utilizza rappresentazioni grafiche interpretabili e porta le capacità di ragionamento ToM dei LLM a livelli superiori rispetto ai metodi supervisionati.</sample>
    <sample id="180">The speaker’s name is **Myra**.</sample>
    <sample id="181">**Abstract**  
We introduce **constrained language planning**, a sub‑task of script generation in which a goal is specified together with multiple real‑world constraints (e.g., “make a chocolate cake”). While existing work has shown that large language models (LLMs) can decompose abstract activities into step‑by‑step instructions, their performance on constrained goals remains poor. We first construct a set of 100 concrete goals by augmenting abstract templates with multi‑faceted constraints via InstructGPT, and evaluate several LLMs on the resulting scripts. Results reveal acceptable semantic completeness but low faithfulness to constraints, with performance varying across constraint categories defined by WikiHow. To address this, we adopt an **over‑generate‑then‑filter** strategy: InstructGPT generates \(K\) candidate scripts for each goal; a filtering model selects the most faithful script based on cosine similarity of InstructGPT embeddings and keyword rewards, retaining only the top‑scoring script. This approach markedly improves both completeness and constraint adherence.  

Recognizing the high cost of deploying LLMs, we further develop a **symbolic knowledge distillation** pipeline to build a large, high‑quality dataset of constrained scripts, **CoScript**. We generate 55 k specific goals with corresponding scripts, and refine a validation and test split via crowd‑source revision. CoScript exhibits diverse constraint coverage. Fine‑tuning a compact T5 model on CoScript yields script quality that surpasses most large LMs, demonstrating that specialized models can outperform general‑purpose LLMs when trained on appropriate data.  

Our work establishes constrained language planning as a distinct problem, introduces effective evaluation and filtering methods for LLMs, and releases CoScript—a valuable resource for future research on faithful, constraint‑aware script generation.</sample>
    <sample id="182">Il “tropicalismo” è il tema stereotipato che descrive le donne latine come esotiche, sensazionali e “tropicali”: si tratta di un’idea che le associa a un’immagine vivace, sensuale e “tropicale”, rafforzando narrazioni essenzializzanti e di altri‑nesso.</sample>
    <sample id="183">Gli autori hanno creato le rappresentazioni umane dei gruppi target chiedendo a un LLM instruction‑tuned di generare un “persona” tramite prompt come:  
**«Immagina di essere una donna asiatica. Descriviti.»**  
Queste frasi sono state usate per far produrre al modello descrizioni di individui appartenenti a ciascun gruppo.  
Per confronto, hanno anche preso le risposte scritte da esseri umani (dall’ultimo studio di riferimento) a prompt analoghi, così da confrontare le tendenze sia nel modello sia nelle rappresentazioni umane reali.</sample>
    <sample id="184">Per misurare quanto il contesto influisce sulla traduzione hanno esteso la misura CXMI a **Pointwise CXMI (P‑CXMI)**, che valuta l’informazione fornita dal contesto \(C\) sul target \(Y\) dato il source \(X\) sia a livello di frase che di singola parola.</sample>
    <sample id="185">**Differenza principale**

|  | **DrBERT** | **ChuBERT** |
|---|---|---|
| **Origine dei dati** | 4 – 7 GB di “NACHOS”, corpora medico‑web (crawled from the internet). | 4 GB di note cliniche anonimizzate provenienti dal data‑warehouse dell’Università di Nantes. |
| **Dominio** | Biomedico (terminologia medica generale). | Clinico (documenti di pratica ospedaliera). |
| **Scopo** | Modello di base per il francese in ambito biomedicale. | Modello specializzato per il dominio clinico (dati sanitari). |

In sintesi, DrBERT è addestrato su dati web‑crawled per coprire la medicina in generale, mentre ChuBERT è addestrato su note cliniche reali per un focus clinico più mirato.</sample>
    <sample id="187">Nel testo si menzionano due autori: **Ying e Zhiyang**.</sample>
    <sample id="188">Il **trasferimento iterativo dell’apprendimento** (iterative transfer learning) è una strategia in cui, dopo aver “cold‑started” il modello con pesi pre‑addestrati da un compito correlato, si aggiorna il modello in successive iterazioni usando solo l'ultimo set di dati annotati in ogni round di active learning. In pratica si fa una fine‑tuning periodica su piccoli batch di nuovi esempi, anziché addestrare su tutta la raccolta accumulata. Questa procedura è utile quando il dominio originale e quello di destinazione differiscono, poiché consente al modello di adattarsi gradualmente ai nuovi esempi senza essere sovraccaricato dai dati precedenti.</sample>
    <sample id="189">The AltEntities Corpus is designed to help us understand how users express a choice when they use **indirect references** (e.g., “the newer one,” “the one that’s not energetic”) instead of a direct name or position. In short, the dataset’s goal is to benchmark and improve systems’ ability to resolve such indirect referring expressions for entity selection across music, books, and recipes.</sample>
    <sample id="190">Un utente malintenzionato può “rubare” i parametri di un servizio di embedding (EaaS) sfruttando la possibilità di fare richieste di embedding in massa:

1. **Costruzione di un set di query** – l’attaccante prepara migliaia (o milioni) di frasi di input, includendo sia testi normali sia testi contenenti trigger o parole di frequenza controllata, così da coprire l’intero spazio di input.

2. **Raccolta delle risposte** – per ogni query inviata al servizio, l’attaccante registra il vettore di embedding restituito. Poiché il servizio è “black‑box”, l’unica informazione disponibile è il vettore di output.

3. **Addestramento di un modello surrogate** – usando i dati raccolti (input → embedding), l’attaccante addestra un proprio modello (es. un modello di rete neurale o un modello di regressione) che cerca di replicare la funzione di embedding del servizio. Questa tecnica è nota come *model distillation* o *model stealing*.

4. **Ottimizzazione dei parametri** – con un algoritmo di ottimizzazione (es. gradient descent) e la perdita di differenza tra embedding di target e embedding predetto, l’attaccante ottimizza i pesi del modello surrogate per avvicinarsi il più possibile all’originale. Se il servizio incorpora un watermark via backdoor, l’attaccante può usare anche i trigger per guidare l’addestramento e recuperare la parte “marcata” dell’embedding.

5. **Validazione e raffinamento** – l’attaccante testa il modello surrogate su nuovi dati per verificare la sua precisione. Una volta adeguatamente preciso, può sostituire il servizio originale con il proprio modello, ottenendo così un “clone” dell’embedding con parametri approssimativamente equivalenti.

In sintesi, l’attaccante sfrutta la capacità di interrogare il servizio con numerose query, raccoglie i vettori di embedding restituiti e addestra un modello di copia che, dopo un adeguato fine‑tuning, ricostruisce i parametri del modello originale.</sample>
    <sample id="191">The article has **three authors**: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">**Riassunto (circa 200 parole)**  

Yang Luo presenta CAME (Confidence‑guided Adaptive Memory Efficient Optimization), un nuovo ottimizzatore per modelli di linguaggio di grandi dimensioni. L’obiettivo è combinare la rapidità di convergenza delle tecniche adattive classiche (es. Adam) con la riduzione di memoria delle soluzioni “memory‑efficient” come Adafactor.  

Adafactor utilizza la fattorizzazione non‑negativa (NMF) per comprimere le matrici di momenti, riducendo l’uso di memoria da O(mn) a O(m+n). Tuttavia, l’approccio a fattorizzazione rank‑1 introduce errori di aggiornamento, rendendo la convergenza più lenta e compromettendo la stabilità del training. Luo evidenzia due tipologie di errori: differenze tra momentum storico (mₜ) e aggiornamento corrente (uₜ), e l’influenza di questi errori sul passo di ottimizzazione.  

CAME affronta il problema calcolando la “instabilità” come residuo tra mₜ e uₜ e utilizzando la radice quadrata di questa matrice come denominatore per l’aggiornamento di mₜ, ottenendo un aggiornamento più “fiducioso” e adattivo.  

Gli esperimenti su BookCorpus, Wikipedia e modelli BERT, GPT‑2 e T5 mostrano che CAME supera Adam e Adafactor in termini di accuratezza di validazione (≈ 3,4 % in più rispetto ad Adafactor) mantenendo un consumo di memoria significativamente inferiore, soprattutto con batch size aumentati da 8k a 32k. Inoltre, CAME garantisce prestazioni comparabili sui compiti downstream di BERT con minore overhead di memoria.  

In sintesi, CAME unisce velocità di convergenza e efficienza di memoria, rendendolo una scelta eccellente per il pre‑training di grandi modelli di linguaggio.</sample>
    <sample id="193">The presentation does not specify how many annotators were employed for the initial 1,000‑pair dataset; that detail is only available in the full paper.</sample>
    <sample id="194">**Affiliazioni degli autori:**

- **Jenny** – PhD student, *Carnegie Mellon University*  
- **Sebastian Santy** – *University of Washington*  
- **Katharina Reinecke** – *University of Washington*  
- **Ronan Le Bras** – *Allen Institute for AI*  
- **Maarten Sap** – *Allen Institute for AI*</sample>
    <sample id="195">**Riassunto sintetico (≈200 parole)**  

Il lavoro propone *RoHT* (Reasoning over Hierarchical Question Decomposition Tree), una nuova architettura per l’Explainable Question Answering (XQA). RoHT sfrutta la decomposizione gerarchica di una domanda complessa in un albero di sub‑domande (HQDT). Il nodo radice è la domanda originale, i nodi non radice sono sub‑domande e le foglie rappresentano domande atomiche non ulteriormente decomponibili.  

**Costruzione dell’HQDT**  
1. *Decomposer* genera le sub‑domande atomiche (leaf loss).  
2. *Generator* crea domande intermedie raggruppando le atomiche in base ai token di riferimento.  
3. Ogni nodo riceve un *certainty score* indicante la fiducia nella sua generazione.  

**Reasoning probabilistico**  
Per ogni nodo (ricorsivamente dalla radice alle foglie) il processo è:  
1. *Scheduler* sceglie la fonte di conoscenza più adatta: KB, corpus testuale o risoluzione ricorsiva dei figli.  
2. *Executor* ottiene risposte con probabilità dalla fonte selezionata.  
3. *Aggregator* combina le risposte, selezionando le chiavi con la massima probabilità.  

**Valutazioni**  
- *KQA Pro* (KB incompleta + Wikipedia): RoHT supera metodi KB‑solo e TransferNet, dimostrando il valore dell’integrazione di KB e testo.  
- *Musique* (paragrafi + Wikidata): RoHT‑text migliora l’F1 di 11,9 rispetto al metodo SOTA EX(SA); RoHT‑mix con KB e testo supera ancora TransferNet.  

In sintesi, RoHT esplicita la struttura di decomposizione, permette una scelta dinamica delle fonti di conoscenza e combina le risposte in modo probabilistico, ottenendo risultati superiori su dataset complessi.</sample>
    <sample id="196">L’esempio citato in cui il governatore si trova a sinistra è: **“I saw Bart and Lisa.”**</sample>
    <sample id="197">Istituto di ricerca: Emory NLP Lab (Prof. Jinho Choi) &amp; Amazon Alexa AI  
Modelli di dialogo “all’avanguardia” (i quattro scelti per l’esperimento):

| # | Modello (di solito la versione più recente) |
|---|---------------------------------------------|
| 1 | **GPT‑4** (OpenAI) |
| 2 | **GPT‑3.5‑Turbo** (OpenAI) |
| 3 | **Claude** (Anthropic) |
| 4 | **LLaMA‑2** (Meta) |

Questi quattro sistemi rappresentano i principali approcci di chat‑model attualmente in uso nei benchmark di conversazione.</sample>
    <sample id="198">La valutazione deve coprire l’intera finestra di contesto perché i modelli di linguaggio più recenti gestiscono sequenze molto lunghe e l’“accettabilità” di una frase può cambiare in base a ciò che precede o segue. Se si testano solo singole frasi, si rischia di trascurare effetti di dipendenza a lungo raggio, di contesti non pertinenti o di prefissi “di match” che alterano fortemente le probabilità. Perciò è necessario verificare la robustezza delle giudicate accettabilità lungo l’intera finestra di contesto, soprattutto quando i modelli hanno capacità di memoria estesa.</sample>
    <sample id="199">Sì. Nel benchmark XSemPLR si osserva che, sebbene l’addestramento multilingue migliori le prestazioni delle lingue “menomate”, la versione inglese monolingue tende a peggiorare in **sette dei nove** dataset (solo in tre si registra un lieve incremento). Questo fenomeno è spesso indicato come “Curse of Multilinguality” e indica un calo di performance per l’inglese rispetto al modello monolingue.</sample>
    <sample id="200">Sì, gli annotatori conoscono i nomi delle entità in anticipo, ma le informazioni dettagliate vengono forniti loro tramite il background (link, testi, immagini) prima di generare le espressioni indirette.</sample>
    <sample id="201">Le valutazioni si sono basate su **BLEURT** (una metrica neurale di qualità automatica) e su un **valutazione umana MQM** (Multi‑Quality Metric) per analizzare fluency, accuratezza e altre dimensioni qualitative.</sample>
    <sample id="202">Sì.  La regressione nella generalizzazione non agisce in modo uniforme su tutti i tipi di entità:  

* **Entità che cambiano frequentemente nel tempo** (ad es. “Organizzazioni”, “Persone” e “Luoghi” legati a eventi attuali) tendono a subire più degrado, perché la loro forma e il contesto evolvono rapidamente.  
* **Entità più statiche** (es. “Titoli di studio”, “Professioni” o “Numeri”) mostrano una diminuzione di performance più contenuta, perché le parole e le strutture rimangono relativamente stabili.  

In sintesi, la perdita di generalizzazione impatta maggiormente sui tipi di entità che sono sensibili a cambiamenti temporali nei dati.</sample>
    <sample id="203">La posizionalità è importante perché determina con chi i modelli e i dataset sono realmente allineati. Se le decisioni di progettazione, le annotazioni e i dati riflettono solo le prospettive di gruppi specifici (ad esempio paesi anglofoni, persone con istruzione universitaria o determinati generi), i sistemi di NLP funzioneranno bene per questi gruppi ma meno per altri, perpetuando e amplificando le disuguaglianze linguistiche e sociali. Conoscere e mitigare la posizionalità è quindi essenziale per costruire tecnologie linguistiche più giuste, inclusive e affidabili.</sample>
    <sample id="204">Non sono stati affinati. Nei nostri esperimenti BLOOM è stato usato “out‑of‑the‑box”, senza adattatori né fine‑tuning completo; è stato valutato così com’è.</sample>
    <sample id="205">**Sintesi (≈ 200 parole)**  

Nel talk di Shangbin si esplora come i bias politici si propagano dai dati di pre‑training ai modelli linguistici (LLM) e, conseguentemente, alle applicazioni downstream. I LLM, addestrati su grandi web‑crawl, comprendono ampiamente fonti di notizie politiche (NY Times, Guardian, Huffington Post, ecc.) e, sebbene ciò garantisca pluralità di prospettive, porta a bias sociali che possono compromettere l’equità.  

Per valutare questi bias si utilizza una serie di prompt ispirati a questionari politici (ad esempio, il “Political Conference Test”) che permettono di posizionare i LLM su un quadrante politico. I risultati mostrano che i modelli occupano tutti e quattro i quadranti, con GPT‑4 che risulta il più liberale; la serie GPT tende a essere più socialmente libera rispetto a BART e alle sue varianti.  

Un esperimento controllato pre‑addestra checkpoint su corpora partisans (notizie e social media, per orientamento politico) e dimostra che l’ideologia del modello si sposta coerentemente con i dati. Pre‑addestrare su testi prima e dopo l’amministrazione del 45° presidente (Obama) mostra che i LLM tendono a spostarsi verso estremo dopo il 2017, catturando la polarizzazione attuale.  

Infine, l’impatto sui compiti “hate‑speech” e “fake‑news” è analizzato per categorie demografiche e politiche. Modelli più liberali eccellono nel rilevare odio contro gruppi minoritari, ma sono meno sensibili all’odio contro gruppi potenti; viceversa, modelli più conservatori mostrano l’opposto. Per i fake‑news, i modelli sono più precisi nel contrastare notizie di orientamento opposto.  

La conclusione sottolinea il dilemma tra “Scylla” (bias non sanificato) e “Charybdis” (censura), avvertendo di una potenziale disuguaglianza se i modelli polarizzati venissero impiegati su piattaforme social.</sample>
    <sample id="206">They use a pretrained transformer (BERT‑style) model that is first fine‑tuned on two related tasks: a topic‑independent dissonance stance (debate) classifier and a binary expansion/comparison (CE) classifier from the PDTB. Those fine‑tuned weights are then used to cold‑start the dissonance detection model.</sample>
    <sample id="207">I am a large language model trained by OpenAI.  
I do not have personal experiences or opinions.</sample>
    <sample id="208">Gli autori propongono **tre suggerimenti** alla fine del loro intervento.</sample>
    <sample id="209">Il metodo proposto migliora sensibilmente la qualità degli script rispetto al baseline (solo LLM‑basato su goal‑astratti). In particolare:

* **Maggiore fedeltà ai vincoli** – il filtro “over‑generate‑then‑filter” garantisce che gli script rispettino i vincoli specifici, riducendo drasticamente gli errori di “non‑faithfulness”.
* **Maggiore completezza semantica** – i risultati mantengono una copertura adeguata delle attività richieste, senza tralasciare passaggi importanti.
* **Efficienza di modello** – usando il dataset CoScript, un modello più piccolo (es. T5 fine‑tuned) supera la maggior parte dei LLM grandi per la pianificazione vincolata, dimostrando che un training mirato su dati di qualità è più efficace di semplici LLM “out‑of‑the‑box”.

In sintesi, il metodo offre script più coerenti e completi, e consente di addestrare modelli più piccoli che raggiungono o superano le prestazioni dei grandi LLM.</sample>
    <sample id="210">Il relatore si chiama **Shuheng**.</sample>
    <sample id="211">Sì. Il corpus DEPLAIN, con i suoi 30 450 paia di frasi allineate manualmente, e i risultati riportati nel paper (valutazioni delle tecniche di allineamento e di base per il fine‑tuning di mBART/long‑mBART) costituiscono un benchmark affidabile per:  

1. **Valutazione di metodi di allineamento** – i risultati mostrano che MASSalign è attualmente la scelta migliore, e il dataset può servire come gold standard per confronti futuri.  
2. **Semplificazione automatica** – le metriche e i checkpoint pubblicati forniscono un punto di partenza e un riferimento per misurare l’over‑o‑under‑performance di nuovi modelli.  

Quindi, sia i risultati che il set di dati possono essere impiegati come parametri di riferimento (benchmark) per la ricerca sulla semplificazione del testo in tedesco.</sample>
    <sample id="212">Solo un modello più piccolo viene menzionato: il T5 fine‑tuned sul dataset CoScript.</sample>
    <sample id="213">Il modello di base utilizzato è **OFA** (Unified multi‑modal pre‑trained model).</sample>
    <sample id="215">Il lavoro di Adam Przepiórkowski si concentra sulla struttura dipendente della coordinazione, confrontando quattro approcci teorici: (1) strutture asimmetriche in cui il primo congiunto è la testa (Universal Dependencies, Mel'čuk), (2) strutture asimmetriche in cui la congiunzione è la testa (Prague), e (3) strutture multi‑testa in cui tutti i congiunti sono teste (Word Grammar). L’obiettivo è sostenere un modello simmetrico e criticare i modelli asimmetrici, basandosi sul principio della minimizzazione della lunghezza delle dipendenze (PLD). 

L’autore illustra come la PLD spieghi la preferenza per posizioni più vicine tra verbo e oggetto diretto, ma che questa preferenza può essere attenuata quando l’oggetto è pesante e lungo. Analizzando dati del Penn Treebank (Enhanced) e del paper “Why wouldn’t you use Universal Dependencies?”, si osserva che i congiunti sinistri tendono a essere più brevi, in particolare quando la testa (governatore) è a sinistra o assente. Quando la testa è a destra, questa tendenza scompare. Il risultato suggerisce che la struttura coordinata non è semplicemente a testa del primo congiunto, ma più simmetrica, in linea con i modelli multi‑testa. L’autore invita a discutere i risultati al poster session.</sample>
    <sample id="217">**Summary (≈200 words)**  

The study tackles *multi‑attribute controllable dialogue generation* (CDG), addressing the gap that prior work focuses on single attributes or discrete labels while ignoring continuous controls. The authors propose **DCG (Disentangled Controllable Generation)**, which learns attribute concepts from seen values and uses a *disentanglement loss* to separate different attribute combinations. DCG builds on DialoGPT and introduces a *compositional prompt module* comprising two prompt types:  

1. **Attribute‑oriented prompts**—instance‑specific prompts that embed the target attribute values.  
2. **Task‑oriented prompts**—global, instance‑independent prompts that guide the overall response style.  

By concatenating these embeddings and enriching them with pseudo‑combinations, the model improves generation diversity and explicitly learns to distinguish unseen attribute mixes.  

For evaluation, the authors design **MAE (Multi‑Attribute Evaluation)**, a unified, reference‑free metric that uses discrete prompts and a trainable continuous prompt to assess both coarse‑grained discrete attributes and fine‑grained continuous attributes. MAE correlates strongly with human judgments and outperforms classic metrics.  

Experiments on two newly established benchmarks (DailyDialog‑CG and an additional dataset) show that DCG outperforms baselines (e.g., CTRL, fine‑tuned models) in both controllability (E‑ACC, A‑ACC) and text quality (BLEU), with only a modest drop on seen‑attribute performance. PCA visualizations confirm disentanglement of attribute combinations.  

The paper concludes that a prompt‑based, disentangled approach enables effective compositional generalization for multi‑attribute dialogue generation.</sample>
    <sample id="218">Gli autori del lavoro sono affiliati a **Google Translate** (Google).</sample>
    <sample id="219">Il relatore, Jia‑Huei Ju, ricercatore presso l’Academia Sinica, presenta il lavoro “A Compare‑and‑Contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports”, realizzato con Yu‑Shiang Huang, Cheng‑Wei Lin e i supervisori Prof. Che Lin e Prof. Chuan‑Ju Wang. Il focus è l’analisi dei Form 10‑K, i rapporti annuali obbligatori dalla SEC, i quali contengono molteplici dettagli sulle attività aziendali ma sono estremamente ripetitivi anno‑su‑anno (circa 80 % dei token sono identici).  

Da questa osservazione nasce la **highlighting task**: data una coppia di sezioni (target T e riferimento R da un anno precedente), il modello deve individuare le parole che evidenziano la relazione tra le due (es. “decrease”). La pipeline è articolata in tre fasi:  
1. **Segmentazione documentale** (Stage 0, non dettagliata nella presentazione).  
2. **Riconoscimento delle relazioni** (Stage 1): le coppie vengono classificate in tre tipi – β (alta similarità sintattica/semantica), “revised” (stessi schemi ma significati diversi) e “mismatched” (nuove informazioni).  
3. **Fine‑tuning** (Stage 2/2+): primo passo con un dataset esterno eNLI (token annotati) per la generalizzazione, seguito da un fino addestramento interno con le coppie “revised” e pseudo‑etichetta di positivi/negativi; si usano tecniche di soft‑labeling combinando cross‑entropy e divergenza KL per mitigare l’alta rumore dei pseudo‑label.  

I risultati, misurati su eSNLI e sul nuovo dataset **FINAL**, mostrano che il modello adattato al dominio ottiene la migliore precisione e correlazione di Pearson (PCC), mantenendo anche la capacità di generalizzazione. Inoltre, il metodo si mostra utile anche su coppie “mismatched” non usate in training.  

Il lavoro è aperto al pubblico con paper e GitHub; i futuri passi includono l’aumento di efficacia, l’aggiunta di nuove features e l’impiego di tecniche di information retrieval per migliorare le applicazioni pratiche.</sample>
    <sample id="220">Le affiliazioni degli autori non sono esplicitamente riportate nel brano che hai fornito.  
L’unica informazione presente è che l’autrice principale, Vasudha, è candidata a dottorato in Computer Science presso la Stony Brook University, quindi è probabile che gli altri co‑autori condividano la stessa affiliazione. Se desideri i dettagli precisi, ti consiglierei di consultare direttamente la versione ufficiale della pubblicazione ACL 2023.</sample>
    <sample id="221">L’articolo si concentra sulla traduzione **dal tedesco all’inglese** (German→English).</sample>
    <sample id="222">**Abstract**  
Open‑domain question answering (ODQA) systems typically train both a retriever and a reader on a single, general‑purpose corpus (e.g., Wikipedia). When these models are deployed on a new, specialized domain—such as biomedical text—performance often degrades because the retriever fails to find relevant passages and the reader cannot reason over domain‑specific language. In this work we investigate how to adapt a source ODQA system to unseen target domains without retraining the entire pipeline. We first propose two families of data interventions: (i) **zero‑shot** techniques that manipulate the interactions among question, answer, and context by generating cloze‑style prompts from the target corpus; and (ii) **few‑shot** techniques that use a handful of target examples to prompt large language models to produce additional synthetic QA pairs, which are then converted to cloze queries. Empirical results show that zero‑shot interventions improve reader accuracy by ~11 % and retriever recall by ~8 %, while few‑shot approaches yield larger gains (≈ 24 % for the reader).  

Next, we quantify the **dataset shift** between source and target domains using a compatibility metric based on the likelihood the source models assign to target contexts and answers. This metric places target datasets on a 2‑D shift plane, revealing four regimes: no shift, concept shift, covariate shift, and full shift. Mapping datasets to these regimes informs which intervention is most effective; for instance, full‑shift datasets benefit most from few‑shot adaptation, whereas concept or covariate shifts also respond well to zero‑shot methods.  

Overall, our study demonstrates that carefully crafted data interventions, guided by a principled shift taxonomy, enable substantial performance gains in ODQA across diverse domains.</sample>
    <sample id="223">Il relatore è **Shangbin**.</sample>
    <sample id="224">I tuoi esperimenti hanno valutato diversi approcci:

1. **Metodi di allineamento** – tra i vari sistemi testati, il **MASSalign** si è dimostrato il più efficace per allineare frasi di testi semplificati in tedesco.

2. **Modelli di generazione di testo semplificato** – hai provato a **fine‑tune** due versioni di mBART:
   * **long‑mBART** per la semplificazione a livello di documento;
   * **base mBART** (normale mBART) per la semplificazione a livello di frase.

Questi sono i principali modelli e metodi che hai esaminato nel tuo studio.</sample>
    <sample id="225">Out of the 62 tasks in MultiInstruct, 53 are used for training and the remaining 9 are used for testing.</sample>
    <sample id="226">Due autori sono coinvolti nell’articolo.</sample>
    <sample id="227">Il lavoro presenta una nuova prospettiva sul problema della “grounded language understanding”, ovvero la mappatura di espressioni linguistiche su rappresentazioni eseguibili in ambienti specifici (ad esempio query SQL, sequenze di azioni robotiche o interrogazioni su basi di conoscenza). Il principale ostacolo è la mancanza di grounding nei modelli linguistici pre‑addestrati, che solo leggono testo e non interagiscono con il mondo reale.  

Per superare questa lacuna l’autore propone **Pangu**, un framework in cui il modello linguistico non genera direttamente il piano o il programma, ma è impiegato esclusivamente come discriminatore: un agente simbolico genera una serie di candidate e il modello valuta e ordina queste proposte. In questo modo si evita il problema di generare strutture non grammaticali o non eseguibili.  

Pangu è stato testato su question answering basato su conoscenza (GRAIL), con modelli diversi (BERT, T5, Codex) sia in fine‑tuning che in few‑shot in‑context learning. I risultati mostrano un’ottima performance: con Codex in few‑shot si raggiungono oltre il 50 % di accuratezza con un solo esempio, superando fortemente i baseline come ArcaneQA. Inoltre Pangu mostra una robustezza notevole in scenari non‑i.i.d., evidenziando una distribuzione di probabilità più uniforme tra strutture viste e non viste rispetto ad approcci autoregressivi.  

**Take‑away**: per la grounded language understanding, la discriminazione è più efficace della generazione.</sample>
    <sample id="228">Gli autori hanno testato la loro tecnica su quattro set di dati: **AG News, MIND, SST‑2 e Enron Spam**.</sample>
    <sample id="229">Il lavoro di Gabriella Skitalinskaya e Henning Wachsmuth si concentra sul supporto alla scrittura argomentativa, in particolare sul rilevamento di affermazioni (claims) che possono essere ulteriormente migliorate. Il processo di revisione è presentato tramite l’esempio di “Cell phones cause brain cancer”, che evolvendo passa da una formulazione esplicita a una più cauta (“Cell phone radiation may cause brain cancer”), illustrando come le revisioni puntino a rendere i claim più precisi e persuasivi.

Sono introdotte due nuove attività: **Task 1 – Suboptimal‑Claim detection** (identificare se un claim necessita di revisione o è ottimale) e **Task 2 – Claim Improvement Suggestion** (segnalare i tipi di problemi qualitativi da correggere). L’idea è di apprendere da dati di revisione collabora­tiva, in particolare dalla piattaforma Kialo, dove i claim finali sono considerati ottimali e i predecessori sub‑ottimali.

Il paper affronta quattro sfide principali:
1. **Rappresentatività e affidabilità** del corpus di revisioni;
2. **Complessità modello** e scelta di architetture sensibili alle piccole modifiche di stile;
3. **Contesto** (argomento, claim padre, conoscenza di dominio) necessario per valutare la qualità;
4. **Bias topico e utente** presenti nelle revisioni collaborative.

I risultati evidenziano che i dati basati su revisioni sono utili, la misura della distanza tra versioni è efficace per la classificazione e l’uso del contesto dipende dal task e dal tipo di problema da correggere. Il lavoro suggerisce quindi approcci sistematici per l’analisi e il supporto alla revisione di claim argomentativi.</sample>
    <sample id="231">**NACHOS** è un dataset di testi médicini prelevati (crawled) dal web in lingua francese, utilizzato per il pre‑training di DrBERT. È composto da circa 4‑7 GB di contenuti sanitari (articoli, documenti, forum, ecc.) ed è stato progettato per fornire dati “in‑domain” per addestrare modelli di linguaggio specifici al settore biomedico.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">La presentazione riguarda il paper “Attention as a Guide for Simultaneous Speech Translation” (SimulST), una collaborazione tra l’Università di Trento, la Fondazione Bruno Kessler, Matteo Negri e Marco Turchi.  
SimulST è la traduzione in tempo reale di un discorso orale in un’altra lingua, permettendo comunicazioni multilingue immediate. Gli approcci attuali richiedono architetture specifiche, procedure di addestramento lunghe e multipli modelli per diverse fasce di latenza (es. 1 s, 2 s, ecc.), con ottimizzazioni divergenti.  

La soluzione proposta è EDAtt (Encoder‑Decoder Attention), che sfrutta i pesi di cross‑attenzione già presenti negli ST offline, evitando nuovi modelli o addestramenti complessi. Con un unico modello, la latenza viene controllata tramite parametri: un termine α (threshold) e λ (numero di frame recenti). Una parola viene emessa solo se la somma dei pesi di attenzione verso gli ultimi λ frame è inferiore ad α, indicando che l’informazione è stabile. Se la somma è alta, la parola viene rimandata al ricevimento di ulteriori segmenti audio.  

I risultati mostrano curve di BLEU vs. average lagging (AL) e computational‑aware AL (cAL) con EDAtt superiori a Wait‑k, Local Agreement e architetture SimulST progettate appositamente; le curve risultano spostate a sinistra (latenza più bassa) e più alte (qualità più alta). Inoltre, EDAtt è il più veloce in termini di tempo effettivo di elaborazione.  

Il codice, i modelli e gli output simultanei sono stati rilasciati open‑source per facilitare la riproducibilità.</sample>
    <sample id="234">La strategia di prompting ha un impatto **notevole** sui risultati:

- **Zero/one‑shot**: la scelta del prompt è decisiva; variazioni di formulazione possono cambiare più di 1 punto BLEURT, con casi estremi che arrivano a 40 punti.
- **Five‑shot**: la forma del prompt diventa meno rilevante; ciò che conta è la qualità degli esempi di traduzione inseriti.  
- In sintesi, per la traduzione con PaLM, una buona selezione di esempi (più di una buona forma di prompt) è cruciale per ottenere prestazioni competitive.</sample>
    <sample id="235">Non è possibile rispondere: le affiliazioni degli autori non sono menzionate nel testo fornito.</sample>
    <sample id="236">Nel discorso non vengono riportate le singole frasi delle cinque istruzioni.  
Si dice soltanto che ogni task del dataset MultiInstruct è “equipaggiato con 5 istruzioni scritte da esperti”, ma gli esempi precisi non sono stati forniti.</sample>
    <sample id="237">Gli autori propongono la **KITMUS Test suite**, un set di test diagnostici basato su una task di coreferenza che varia la disponibilità di due tipi di conoscenza:

1. **Background‑Pretrain** – conoscenza di fondo presente nei parametri pre‑addestrati, con conoscenza specifica dell’entità fornita al momento dell’inferenza.  
2. **Background‑Both** – conoscenza di fondo e specifica entrambe disponibili sia in pre‑training sia in input di inferenza.  
3. **Background‑Inference** – entrambe le tipologie di conoscenza sono fornite solo al momento dell’inferenza (simulando conoscenze non presenti nel pre‑training).

Questa configurazione permette di valutare se i modelli riescono a integrare e utilizzare informazioni provenienti da più fonti.</sample>
    <sample id="238">Yebowen Hu from the University of Central Florida presents **MeetingBank**, a new benchmark for meeting summarization. The dataset tackles two key challenges: producing high‑quality summaries and finding reliable public meeting data. To build it, Hu’s team used the Speechmatics API to transcribe audio from City Council meetings (e.g., Boston, Seattle, Denver), then matched each transcript to official minutes and extracted reference summaries. Meeting IDs uniquely identify each session, allowing alignment of timestamps with the transcript and the summary. The final corpus contains 1,366 council meetings and nearly 7,000 segments, with statistics on duration, token counts, speaker numbers, and yearly coverage. Coverage and density scores reveal that most summaries are highly extractive (coverage 0.7–0.9), with Seattle and Boston showing the highest density, indicating more editing, whereas Denver has the lowest.

For evaluation, Hu benchmarked extractive methods (Oracle, Lead, LexRank, TextRank) and five top neural abstractive models (BART‑Large, Pegasus, Longformer, DialogLM, HMNet). BART‑Large, fine‑tuned on MeetingBank, performed well, and DialogLM achieved the best abstractive ROUGE‑2 score. GPT‑3 (Davinci‑003) was tested in a zero‑shot setting; automatic metrics were modest, but human judges rated its fluency and coherence highest, though it lagged in informativeness and factuality. Human evaluations used a 5‑point Likert scale across informativeness, factuality, fluency, coherence, and redundancy for 200 randomly selected instances. Hu concludes that MeetingBank offers a valuable resource for developing advanced meeting summarizers and provides insights into city council decision‑making, encouraging the community to download and experiment with the dataset.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e presenterò una breve recensione dell’articolo *Prompting PaLM for Translation: Assessing Strategies and Performance*. Si tratta di un lavoro congiunto con i miei colleghi di Google Translate. PaLM è un grande modello di linguaggio a 540 miliardi di parametri, presentato l’anno scorso, nel 2022. È stato addestrato su un enorme corpus di testi, costituito da 780 miliardi di token. All’epoca della pubblicazione, ha raggiunto risultati di stato dell’arte su centinaia di compiti NLP.

In questo studio presentiamo la prima analisi sistematica del prompting di grandi modelli di linguaggio (LLM) per la traduzione automatica. Abbiamo valutato le capacità di traduzione di questi modelli seguendo le migliori pratiche della comunità di MT. Questo implica l’utilizzo dei set di test più recenti per evitare sovrapposizioni tra i dati di test e quelli di addestramento del modello. Abbiamo inoltre confrontato PaLM con i sistemi di stato dell’arte, in particolare con la migliore prestazione nella valutazione WMT. Per la valutazione, abbiamo usato metriche neurali all’avanguardia per MT e presentato anche risultati di valutazione umana basata su esperti.

Infine, forniamo alcune raccomandazioni sulle strategie di selezione dei prompt. Il prompting ha un grande impatto sulle prestazioni degli LLM nella traduzione: in un esperimento semplice, dove abbiamo usato one‑shot prompting e fornito due prompt diversi per ogni frase, la differenza osservata è stata superiore a un punto BLEURT; in casi estremi può arrivare fino a 40 punti BLEURT. Pertanto è fondamentale scegliere una buona strategia di prompting.

Negli esperimenti abbiamo optato per una strategia di prompting a 5‑shot, in cui abbiamo semplicemente marcato ogni frase fornita al sistema con la lingua di origine. Ad esempio, nella traduzione da tedesco a inglese, le frasi tedesche (sorgenti) sono marcate con “German:”, mentre le traduzioni inglesi sono marcate con “English:”. Abbiamo riscontrato che la forma effettiva del prompting non ha un grande impatto quando si hanno diversi prompt brevi. È cruciale per zero e one‑shot prompting. Quando passiamo a 5‑shot prompting, come nel nostro caso, la differenza nella forma del prompting è quasi inesistente. È principalmente la qualità degli esempi che conta di più.

In sintesi, la qualità degli esempi è più importante della similarità alla frase sorgente. È quindi importante selezionare esempi provenienti da traduzioni di alta qualità. In particolare, abbiamo confrontato la selezione dei prompt dai dati di addestramento per le valutazioni WMT con i dati di sviluppo (dev). I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento, che sono più rumorosi. Di conseguenza le prestazioni sono migliori quando si utilizzano i dati di sviluppo. 

Nonostante ciò, i sistemi specializzati di stato dell’arte hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Tuttavia, PaLM si avvicina molto a un sistema commerciale. Nel nostro caso, abbiamo scelto di valutare con Google Translate.

Le intuizioni ottenute dalla valutazione umana, condotta con il framework MQM, indicano che la fluidità di PaLM è comparabile a quella dei sistemi di stato dell’arte, ma la principale differenza riguarda l’accuratezza. In particolare, gli errori più frequenti sono gli errori di omissione: PaLM tende a produrre traduzioni più fluide, a volte sacrificando parte del testo sorgente. Tuttavia, la categoria “Style/Awkward” per PaLM è più bassa rispetto ai sistemi di stato dell’arte, confermando che PaLM offre un output molto fluido, sebbene con alcune lacune di accuratezza.

Questo è tutto per questa breve panoramica. Per maggiori dettagli, vi invito a partecipare alla presentazione completa del paper. Grazie mille.</sample>
    <sample id="240">Ciao a tutti, sono Dawei, studente di dottorato all’Università di Saarland in Germania. Oggi presenterò il nostro lavoro recente “Weaker Than You Think: A Critical Look at Weakly Supervised Learning”. È un progetto congiunto con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.

**Introduzione alla weak supervision e all’weakly supervised learning**  
Nella weak supervision non si etichetta manualmente il dataset. Invece, usiamo fonti di etichette “deboli”, come semplici regole euristiche, basi di conoscenza o crowd‑sourcing di bassa qualità, come mostrato nella figura a destra. Rispetto alle annotazioni umane, queste etichette sono molto più economiche, ma sono anche rumorose: una certa percentuale di etichette è semplicemente errata. Se si addestrano direttamente delle reti neurali su dati weakly‑labelled, queste tendono a memorizzare il rumore e a non generalizzare bene.

L’weakly supervised learning (WSL) propone algoritmi che addestrano robustamente reti neurali in presenza di rumore di etichette, in modo che i modelli finali si comportino ancora bene su set di test puliti.

Un’affermazione comune nelle ultime pubblicazioni WSL è che si addestrano i modelli solo sui dati weakly‑labelled e si ottengono buone prestazioni su set di test puliti. Tecnicamente è vero, ma c’è un “cattivo” punto: i ricercatori si assumono l’esistenza di un set di validazione pulito, cioè annotato a mano, per la selezione dei modelli. Questa assunzione implica comunque un costo di annotazione manuale, che spesso viene trascurato.

**Le tre domande di ricerca che abbiamo posto**  
1. È necessario avere campioni di validazione puliti per la WSL, o possiamo usare un set di validazione rumoroso?  
2. Se è obbligatorio avere dati puliti, quante annotazioni pulite servono?  
3. Dovremmo usare i campioni puliti solo per la validazione, o ci sono modi migliori di sfruttarli?

Abbiamo risposto a queste domande e le nostre scoperte sono le seguenti.

**1. La WSL richiede davvero campioni di validazione puliti**  
Scomponendo le performance dei recenti metodi WSL, scopriamo che senza campioni di validazione puliti si ottiene un drastico calo delle prestazioni. Come evidenziato dalla figura, se non abbiamo campioni puliti il modello non generalizza oltre le etichette weakly‑labelled: l’intero addestramento è inutile. Quindi la WSL richiede etichette pulite per funzionare, e il costo di ottenere campioni di validazione puliti non deve essere ignorato.

**2. Più campioni puliti portano a migliori risultati**  
Aumentare il numero di campioni di validazione puliti migliora le prestazioni dei metodi WSL, come mostra la figura a sinistra. In genere bastano solo 20 campioni per classe per raggiungere un buon risultato.

**3. Addestrare direttamente sui campioni puliti è ancora più efficace**  
Se disponiamo di campioni puliti, addestrare direttamente su di essi (fine‑tuning) può dare risultati migliori di quelli ottenuti usando la WSL con i campioni puliti solo per la validazione. La figura a destra confronta il fine‑tuning diretto con le WSL: se abbiamo 10 campioni per classe, il fine‑tuning inizia a superare i metodi WSL.

**4. La performance “magica” dei metodi WSL può essere replicata con un semplice fine‑tuning continuo**  
Il modello base, chiamato FTw, inizialmente è inferiore rispetto a metodi più complessi come COSINE. Tuttavia, se permettiamo un fine‑tuning continuo sui campioni puliti, FTw raggiunge prestazioni comparabili ai metodi più sofisticati. In pratica non è necessario scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio su disco.

**Riassunto e raccomandazioni**  
Abbiamo dimostrato che i recenti metodi WSL necessitano di campioni di validazione manualmente annotati per funzionare correttamente. Il guadagno di performance e la praticità di questi metodi sono probabilmente sopravvalutati. Per il futuro consigliamo:

1. Rendere esplicito il criterio di selezione del modello. Ad esempio, indicare se la selezione avviene tramite campioni di validazione puliti.  
2. Confrontare i metodi WSL con baselines di few‑shot learning, perché entrambi operano su campioni puliti.  
3. Considerare il fine‑tuning continuo come baseline semplice ma molto forte negli studi WSL.  

Infine, abbiamo open‑sourced il nostro codice. Lo trovate tramite il QR code presente su questa slide. Sentitevi liberi di dare un’occhiata.  
Grazie e buon proseguimento alla conferenza!</sample>
    <sample id="241">**Abstract**

Existing misinformation detection systems often rely on retrospectively curated datasets and treat human moderators as a last‑minute arbiter, leading to unrealistic evaluation and limited practical impact. We introduce a human‑in‑the‑loop evaluation framework that addresses these shortcomings by embedding moderators throughout the misinformation pipeline and by measuring performance on live, noisy data. The framework is instantiated on COVID‑19 treatment claims, yielding an end‑to‑end system that processes raw Twitter feeds, extracts check‑worthy claims, and flags policy violations.

The claim extraction stage first filters tweets with relevant keywords, then applies a T5 model fine‑tuned for the question “What is the mentioned COVID‑19 cure?” to produce claims such as “Ivermectin is effective in treating COVID‑19.” Claims are ranked by trendiness using Fisher’s Exact Test, and the top candidates are presented to human verifiers. The policy verification stage employs a BERT‑based stance classifier to identify tweets that endorse unapproved treatments; these are forwarded to moderators for final judgment.

Evaluation demonstrates that the system detects unapproved treatments **before** their first appearance in debunking news, achieving early‑detection coverage aligned with human expectations. In policy‑violation detection, the system attains a 65 % agreement with human judgments (Likert scores ≥ 4). Moreover, the workflow yields **≈124 policy violations confirmed per human hour**, evidencing high throughput.

Our study shows that a realistic, human‑centric evaluation captures the dynamic interaction between automated methods and content moderators, providing a benchmark for future misinformation detection systems and an out‑of‑industry perspective on practical deployment.</sample>
    <sample id="242">I metodi di valutazione più usati per i sistemi di dialogo sono:

| Tipo di misura | Descrizione | Come si ottiene |
|-----------------|-------------|-----------------|
| **Confronto comparativo** | L’utente sceglie quale di due conversazioni è migliore. | Questionario con coppie di dialoghi. |
| **Scala Likert a livello di turno** | Ogni singolo turno viene valutato su scala (es. 1‑5) per qualità, pertinenza, etc. | Valutazione manuale per ogni turno. |
| **Scala Likert a livello di dialogo** | L’intera conversazione viene giudicata su scala. | Valutazione globale di ogni sessione. |
| **Confronto a livello di dialogo** | L’utente confronta due intere conversazioni e indica quale è più buona. | Questionario con coppie di dialoghi completi. |

Questi approcci forniscono insight “olistici” ma spesso non isolano le varie dimensioni di qualità del dialogo. (ABC‑Eval è un’estensione che aggiunge etichette di comportamento specifiche, ma non è ancora un metodo “standard”.)</sample>
    <sample id="243">Cinque autori sono coinvolti nell’articolo: Jenny e i quattro collaboratori (Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap).</sample>
    <sample id="244">Nel caso di Servin e Kea, la conoscenza di base necessaria è che **i giudici lavorano in tribunale e decidono casi** (cioè “judges decide cases in law courts”). Questa informazione di base, assunta dal pre‑training, aiuta a capire che la frase “deciding cases in a law court” indica un giudice e quindi il pronome “he” si riferisce a Servin.</sample>
    <sample id="245">Il lavoro “A Needle in a Haystack” presenta un pipeline a due fasi per individuare lavoratori MTurk ad alto accordo nella valutazione di sommari. Nella prima fase, i partecipanti devono superare un “Qualification Task” composto da una parte di training e da domande di valutazione su tre documenti (uno con attenzione). Le domande coprono sei dimensioni e includono un check di attenzione; i lavoratori sono classificati in gold, silver, bronze o block, ma solo gold e silver possono passare. Da 200 candidati, 26 passano (8 gold, 18 silver, 13 %). La seconda fase, l’“Endurance Task”, valuta la capacità di gestire carichi di lavoro elevati: 10 HITs, un documento e quattro sommari valutati per salienza. Solo 12 lavoratori persistono (4 gold, 8 silver, 6 %). Questi mostrano un IAA superiore a quello di esperti, con Krippendorff’s α = 0.443 e Cohen’s κ tra i gruppi.

Il “Reference‑based Task” (30 HITs, un riferimento e quattro candidati) misura la performance sul compito reale. Oltre 12 MTurk passano, 8 completano tutti i HITs, con α = 0.534. Confronti con baseline (MACE, α = 0.380) e CloudResearch (α = 0.513) evidenziano che il pipeline produce qualità simile a CloudResearch ma a costi inferiori e con maggiore copertura dei HITs.

Un’analisi di correttezza su 50 campioni di riferimento mostra correlazioni significative tra i pipeline e CloudResearch (Spearman), ma la formazione di correttezza non è garantita. I modelli GPT mostrano alta correlazione con giudizi esperti.

Conclusioni: il pipeline consente di selezionare 6 % dei partecipanti (4 gold, 8 silver) con alta affidabilità e costi ridotti. Futuri lavori mirano a migliorare la selezione di lavoratori di qualità, estendendo a più lingue, compiti e piattaforme. Limiti: solo inglese su MTurk, domande non universali, mancanza di garanzia di correttezza, e dipendenza dal funding di Google.</sample>
    <sample id="246">Sì. Il codice (insieme al dataset) è pubblicato su GitHub – basta consultare la repository indicata nella sezione “code” del paper.</sample>
    <sample id="247">**Riassunto del discorso (≈200 parole)**  

Jiho Kim, ricercatore di KAIST AI, presenta **FACTKG**, un nuovo dataset per la verifica di fatti che sfrutta come evidenza una Knowledge Graph (KG) anziché testi o tabelle. Attualmente esistono dataset come FEVER, VitaminC, TabFact e InfoTabs, ma nessuno utilizza un KG. Il nuovo compito, “Knowledge Graph‑Based Fact Verification”, è stato realizzato con il DBpedia‑KG, includendo due stili di claim: scritto e colloquiale, per renderlo più pratico in applicazioni reali (ad esempio verificare la coerenza tra un dialogo e il KG interno di un sistema conversazionale).  

Ogni claim ha due etichette: **SUPPORTED** o **REFUTED**. La verifica richiede non solo l’estrazione di evidenza dal KG ma anche ragionamenti di diversi tipi:  
1. **One‑hop** – una singola tripla collega due entità.  
2. **Conjunction** – più triple devono essere verificate simultaneamente.  
3. **Existence** – verifica l’esistenza di una relazione per un'entità.  
4. **Multi‑hop** – richiede percorsi più lunghi nel grafo.  
5. **Negation** – dopo aver trovato l’evidenza, è necessaria un’inferenza aggiuntiva per testare la negazione.  

La parte colloquiale è ottenuta tramite un modello di transfer linguistico e template di presupposizione. Il dataset presenta statistiche dettagliate e, per i baseline, Kim mostra due approcci: (i) “Claim Only”, che usa solo il claim, e (ii) GEAR, che incorpora l’evidenza del grafo. Entrambi superano la baseline di classe majoritaria (51 %) e GEAR è l’approccio più efficace.  

Il dataset è disponibile per il download e l’autore invita la comunità a contattarlo per ulteriori dettagli.</sample>
    <sample id="248">No.  In NLPositionality the annotators were deliberately recruited from a wide range of backgrounds (over 1,000 participants from 87 countries, with varied gender, education, and other demographic information), but the sample was not statistically balanced across every demographic dimension.  The goal was to increase diversity and capture a broad spectrum of viewpoints, not to achieve equal representation for each country, gender, etc.</sample>
    <sample id="249">Le frasi “accettabili” sono state disturbate introducendo piccoli “rumori” che mantengono intatta la struttura sintattica principale: ad esempio, sostituzioni di parole, inserimenti di token irrilevanti o lievi errori di battitura, in modo da alterare il testo senza rompere la sua coerenza grammaticale.</sample>
    <sample id="250">Una valutazione dimensionale significa misurare **vari aspetti distinti** della qualità di un dialogo—ad esempio rilevanza, coerenza, empatia, veridicità, ecc.—separatamente, invece di ridurlo a un singolo punteggio complessivo. Questo approccio fornisce una visione più fine‑grained dei punti di forza e delle debolezze del modello.</sample>
    <sample id="251">Gli autori dell’articolo sono affiliati all’**Università della Scienza e della Tecnologia della Cina (University of Science and Technology of China)**.</sample>
    <sample id="252">**Riassunto del lavoro “U‑CREAT: Unsupervised Case Retrieval using Events extrAcTion”**

Sai Kiran Tanikella e i suoi collaboratori presentano U‑CREAT, una soluzione per il *Prior Case Retrieval* (PCR) che mira a individuare precedenti giuridici rilevanti per un documento di query, senza necessità di addestramento supervisionato. Il primo contributo è l’**IL‑PCR dataset**, un benchmark indiano composto da 7 070 casi con una media di 6,775 citazioni per query; è più ampio e complesso rispetto al dataset canadese COLIEE’21. 

Il secondo contributo è la **pipeline U‑CREAT**, che sfrutta l’estrazione di eventi. Utilizzando l’analisi di dipendenza, ogni documento viene trasformato in una sequenza di eventi (SVO). Viene costruita una matrice di interazione tra eventi della query e del candidato; gli eventi in comune sono marcati verde. Questa matrice alimenta diversi modelli di ranking.  

I risultati mostrano che i modelli **event‑based** superano nettamente i baseline (BM25) e i modelli transformer (BERT, DistilBERT, DistilRoBERTa, InCaseLawBERT, InLegalBERT). In particolare, il modello *Event‑Filtered Documents* (che filtra il corpus in base agli eventi condivisi) ottiene il miglior F1 e tempi di inferenza più rapidi. Su COLIEE’21, U‑CREAT raggiunge lo stato dell’arte, superando anche approcci supervisionati come MTFT‑BERT.  

U‑CREAT dimostra quindi che l’estrazione non supervisionata di eventi è un approccio potente e generalizzabile per la ricerca di precedenti legali.</sample>
    <sample id="253">Mario Ezra Aragón presenta **DisorBERT**, un modello di doppia adattamento di dominio per rilevare segni di disturbi mentali sui social media. Inizia definendo i disturbi mentali come sintomi psicologici associati a sofferenza e disabilità, citando esempi come depressione, PTSD, bulimia e anorexia. Sottolinea l’enorme quantità di contenuti su piattaforme come Reddit, dove gli utenti condividono apertamente esperienze di salute mentale, e la necessità di strumenti automatici che allertino sull’insorgere di patologie.

Il problema principale è la scarsità di dati annotati; per superarlo si propone l’adattamento di dominio. Si parte da BERT, pre‑addestrato su Wikipedia e Google Books, e lo “tassa” in due fasi: prima all’imparare il linguaggio dei social media Reddit, poi specializzandosi nel dominio dei disturbi mentali. Si integra un dizionario clinico per guidare la mascheratura, facendo in modo che il modello si concentri su termini chiave.

I risultati, valutati su dataset eRisk, mostrano che DisorBERT ottiene un buon equilibrio tra precisione e richiamo, superando baseline e MentalBERT. Analisi delle predizioni su frasi della Beck Depression Inventory evidenziano come DisorBERT predica parole più negative e clinicamente rilevanti, mentre BERT resta più generico. Una visualizzazione a grafico evidenzia le parole più importanti in post di utenti con alto punteggio BDI, come “ansioso” e “medicazione”.

In conclusione, l’approccio a doppio adattamento e mascheratura guidata è efficace nel rilevare segnali di disturbi mentali sui social media. Futuri lavori includeranno l’uso di diverse risorse lessicali e dati clinici.</sample>
    <sample id="254">**Riassunto (≈200 parole)**  

Nel suo intervento Sun Qi, ricercatore dell’Università di Nanjing, presenta il lavoro “Uncertainty Guided Label Denoising for Document‑level Distant Relation Extraction”. Il problema principale è l’estrazione di relazioni tra entità a livello di documento quando i dati di addestramento provengono da distanze (distant supervision, DS). I dati DS contengono molta rumore, poiché le relazioni non sono sempre correttamente annotate, e l’uso di pseudo‑labels può introdurre falsi positivi, compromettendo le prestazioni del modello.

Per mitigare questa fonte di errore, gli autori propropongono un framework che combina:  
1. **Pre‑denoising DocRE**: un modello addestrato sia su dati DS sia su dati annotati a mano, che genera pseudo‑labels.  
2. **Stima di incertezza tramite Monte‑Carlo dropout**: più passaggi di inferenza con dropout attivo forniscono una misura di incertezza per ogni relazione predetta.  
3. **Incertezza a livello di istanza**: per gestire relazioni sovrapposte (più relazioni tra la stessa coppia di entità) si calcola una incertezza specifica per ogni etichetta positiva.  
4. **Soglie dinamiche di incertezza**: in base alla frequenza delle classi, si stabiliscono soglie di incertezza per filtrare le pseudo‑labels più rumorose, con particolare attenzione alle classi rare.  
5. **Strategia di addestramento multi‑fase**: iterativamente si rielabora il set DS, rielabelando le istanze in base alle soglie di incertezza, e si raddoppia la precisione del modello.

I risultati sui dataset pubblici mostrano un miglioramento significativo rispetto ai baseline, confermando l’efficacia del metodo. L’innovazione principale risiede nell’uso dell’incertezza per guidare la denoising delle etichette, migliorando la qualità dei dati distanti e la robustezza del modello di DocRE.</sample>
    <sample id="255">La forma del prompting si rivela importante **solo nei casi di zero‑shot e one‑shot**: in queste configurazioni la formulazione dell’instruttivo può influenzare notevolmente le metriche (es. BLEURT). Con cinque‑shot (o più) l’effetto della forma è quasi nullo; qui la qualità e la scelta degli esempi sono i fattori determinanti.</sample>
    <sample id="257">Gli autori hanno valutato **quattro modelli di dialogo di stato‑dell’arte** (i nomi specifici non sono indicati nel testo).</sample>
    <sample id="258">In questo video, Chiang Cheng‑Han presenta la sua ricerca “Can Large Language Models Be an Alternative to Human Evaluation?”, che esplora l’utilizzo di modelli linguistici di grandi dimensioni (LLM) per valutare la qualità del testo in NLP. Il problema principale è che le valutazioni umane, sebbene precise, sono instabili, costose e difficili da riprodurre. L’idea è di fornire agli LLM istruzioni chiare, in linguaggio naturale, e chiedere loro di valutare i campioni di testo allo stesso modo che farebbe un valutatore umano.

Per verificare l’efficacia, gli autori hanno confrontato i punteggi generati da diversi LLM (T0, InstructGPT‑Curie, InstructGPT‑Davinci e ChatGPT) con quelli ottenuti da insegnanti di inglese (esperti nella valutazione di saggi). I LLM sono stati testati su storie generate da GPT‑2 e su storie scritte da umani, valutandole su grammatica, coerenza, piacevolezza e pertinenza. I risultati mostrano che Davinci e ChatGPT riescono a distinguere in modo coerente le storie umane dalle generazioni GPT‑2, in linea con le valutazioni umane, mentre modelli più piccoli falliscono.

Il talk sottolinea che, sebbene non tutti gli LLM siano equivalenti a valutatori umani, alcuni di essi possono effettivamente sostituirli in questa attività. Viene inoltre accennato a ulteriori analisi: la concordanza tra LLM e umani, l’impatto delle formulazioni delle istruzioni, le strategie di campionamento e il bilancio tra costo e beneficio. Chi è interessato può approfondire leggendo il paper o visitando lo stand poster all’ACL.</sample>
    <sample id="259">**Riassunto (circa 200 parole)**  

Nel suo intervento Yusen Zhang presenta *XSemPLR*, un benchmark unificato per il “Cross‑Lingual Semantic Parsing” (CLSP). Il compito consiste nel tradurre query in diverse lingue naturali in rappresentazioni semantiche come SQL, Lambda‑calculus, FunQL, ecc. La bacheca contiene 9 dataset provenienti da vari domini, 5 tipologie di parsing, 8 rappresentazioni semantiche e 22 lingue distribuite in 15 famiglie linguistiche, inclusi lingue come il cinese spesso mancanti nei precedenti lavori.  

Per valutare le capacità multilingue sono stati stabiliti sei scenari: *Translate‑Test* (traduzione con Google Translate prima di predire), *Monolingual*, *Monolingual Few‑shot*, *Multilingual*, *Cross‑lingual Zero‑shot* e *Few‑shot transfer*.  

I modelli analizzati comprendono Encoder‑PTR (XLM‑R/ mBERT con decoder a puntatore) e Encoder‑Decoder (mBART, mT5). I risultati mostrano che gli Encoder‑Decoder ottengono la migliore performance su tutti i dataset. L’addestramento multilingue migliora la maggior parte delle lingue, mentre l’inglese mostra un “Curse of Multilinguality” con riduzione di performance in sette dataset.  

Il gap di performance tra Zero‑shot e Monolingual è significativo, ma ridotto rapidamente con i Few‑shot. Pre‑addestrare su inglese incrementa notevolmente le prestazioni Few‑shot in altre lingue. Modelli come Codex e BLOOM risultano ancora inadatti per CLSP.  

XSemPLR fornisce così un punto di riferimento completo per studiare e sviluppare modelli multilingue di parsing semantico.</sample>
    <sample id="260">Un autore.</sample>
    <sample id="261">Un buon pianificatore deve:

1. **Essere ragionevole** – le istruzioni devono avere senso logico e seguire una sequenza coerente.  
2. **Essere semanticamente completo** – coprire tutti i passaggi necessari per raggiungere l’obiettivo.  
3. **Essere fedele ai vincoli** – rispettare rigorosamente le restrizioni specifiche (tipo, ingredienti, strumenti, ecc.).  
4. **Gestire la varietà** – adattarsi a diversi tipi di vincoli e a obiettivi astratti trasformandoli in obiettivi specifici.  
5. **Mantenere la consistenza** – evitare contraddizioni tra i passaggi e con i vincoli.  

Queste qualità garantiscono che un piano sia pratico, affidabile e applicabile al caso concreto.</sample>
    <sample id="262">Non indicato.</sample>
    <sample id="263">In‑context learning (ICL) with large language models is highly unstable, largely because the predictions are biased by various design choices.  Prior work has identified two main biases: **vanilla‑label bias** (the model’s inherent preference for certain label names) and **context‑label bias** (the influence of the labeled examples in the prompt).  However, a systematic taxonomy and mitigation strategy were lacking, especially for text classification tasks.

This work introduces a comprehensive typology of label biases and reveals a third, previously overlooked source: **domain‑label bias**—the tendency of a model to favor labels that are over‑represented by the task corpus itself.  Experiments confirm that feeding the model random in‑domain words elicits strong label preferences, whereas random English words do not, and that tasks with high domain‑label bias suffer markedly worse ICL performance.

To address all three biases, the authors propose **domain‑context calibration**.  This technique estimates the model’s bias by querying it on a set of random in‑domain words (which are nearly content‑free but reflect the domain distribution) and then uses those estimates to adjust the raw ICL predictions.  Compared to earlier calibration methods that rely on a single fixed token (e.g., “not available”), domain‑context calibration consistently improves average accuracy across a wide range of datasets and models—including GPT‑3—especially for tasks with large domain‑label bias.  The study thus offers a unified diagnostic framework and an effective mitigation strategy for label biases in ICL.</sample>
    <sample id="264">**Riassunto (≈200 parole)**  

Lin Wang presenta “TAVT: Towards Transferable Audio‑Visual Text Generation”, un nuovo compito di generazione di testo multimodale che mira a superare le difficoltà di annotazione e le variazioni di dominio nelle attività audio‑visive. Il problema principale è lo spostamento di dominio multimodale (stile visivo, energia audio, ecc.) che complica l’addestramento di modelli generativi. Wang osserva che, sebbene il contenuto visivo cambi drasticamente con stile e angolo di scatto, la comprensione dell’evento rimane stabile rispetto ai cambiamenti di ritmo e energia audio. Da ciò deriva l’ipotesi che uno spazio semantico audio unificato possa allineare concetti visivi di domini diversi.

Il framework proposto è modulare e comprende:  
1. **Meta‑mapper audio‑visual**: mappa concetti visivi di vari domini in uno spazio semantico audio, utilizzando cluster audio estratti da Flickr e token “visual prefix” ispirati al prompt learning.  
2. **Encoder‑generator Transformer**: incorpora un coefficiente α che valuta il contributo di ciascuna modalità per ogni parola, calcolato tramite cross‑attenzione.  
3. **Dual Counterfactual Contrastive Learning (DCLL)**: ottimizza direttamente l’allineamento visivo‑testuale con supervision fine‑grained derivata da risultati counterfactual, evitando la dipendenza da negativi casuali.

Il metodo sfrutta una procedura meta‑learning simile a MAML: in meta‑training si selezionano K‑1 domini di supporto e un dominio di query; in meta‑test si adatta rapidamente a un nuovo dominio con pochi dati di supporto.

Gli esperimenti, realizzati su MSVD e MSR‑VTT in scenari cross‑dataset e cross‑domain, mostrano che TAVT supera di gran lunga i modelli RNN e Transformer esistenti, soprattutto in domini con risorse limitate (Kids, Beauty). Gli ablation studiano l’impatto delle caratteristiche audio sul miglioramento delle prestazioni.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">I’m sorry, but the passage you shared does not include any information about the authors’ institutional affiliations.</sample>
    <sample id="268">I più frequenti errori di PaLM sono le **omissioni**: il modello tende a produrre traduzioni più fluenti ma a volte rimuove parti del testo originale, compromettendo l’accuratezza.</sample>
    <sample id="269">Ciao, sono James Finch. E io sono Sarah Finch. Oggi vi parleremo di ABC‑Eval, un nuovo approccio dimensionale per la valutazione dei chatbot. Questo lavoro è stato realizzato dal Laboratorio NLP dell’Università Emory, guidato dal Professor Jinho Choi, e in collaborazione con Amazon Alexa AI.

Immaginate di aver appena sviluppato un modello di dialogo e di volerne confrontare le prestazioni con l’attuale stato dell’arte. In genere si ricorre alla valutazione umana, ad esempio chiedendo a giudici umani di scegliere quale di due conversazioni sia migliore o di assegnare punteggi su una scala Likert. Queste tecniche funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità di un chatbot ha molti aspetti. Per questo motivo, potreste voler valutare più dimensioni della qualità di una chat per comprendere i punti di forza e le debolezze del modello a un livello più fine.

Un approccio è chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la pertinenza delle risposte del modello, utilizzando metodi comparativi o scale Likert. Tuttavia, crediamo che esista una strategia più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro metodo cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se ogni risposta del modello esprime determinati comportamenti, come fornire informazioni irrilevanti o contraddirsi. Chiamiamo questo approccio “annotare i comportamenti nella chat” o ABC‑Eval in breve.

Abbiamo sviluppato questo metodo per coprire in modo completo i comportamenti del modello di chat che sono stati suggeriti nella letteratura recente come fattori che influenzano la qualità della chat. ABC‑Eval misura la frequenza con cui i modelli di chat commettono vari errori tematici. Ad esempio, misura il numero di turni in cui un modello di chat ignora il partner, fornisce informazioni irrilevanti, si contraddice con se stesso o con il partner, crea fatti falsi o viola la conoscenza comune, e quando il modello riesce o fallisce a mostrare empatia.

Per determinare quale tipo di valutazione fosse più efficace, abbiamo selezionato quattro modelli di chat all’avanguardia e li abbiamo valutati su 100 conversazioni uomo‑bot per modello utilizzando ABC‑Eval. Per confronto, abbiamo valutato queste conversazioni anche con tre metodi esistenti: punteggi Likert a livello di turno, punteggi Likert a livello di dialogo e confronti a livello di dialogo. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chat su più dimensioni.

Dall’analisi dei risultati di valutazione abbiamo scoperto che le etichette di comportamento di ABC‑Eval sono complessivamente più affidabili delle etichette raccolte con i metodi esistenti, come misurato dall’accordo tra annotatori su 100 conversazioni annotate due volte. Inoltre, le etichette di ABC‑Eval sono più predittive della qualità complessiva della conversazione rispetto ai metriche generate dai metodi esistenti, come dimostrato da un’analisi di regressione lineare semplice. Ad esempio, potete vedere come la misura della proporzione di turni con contraddizioni con se stessi e con il partner spiega il 5 % e il 10 % della qualità della conversazione, rispettivamente, mentre i punteggi di consistenza Likert medi spiegano solo il 4 % o meno.

Infine, abbiamo verificato se ciascuna metrica di valutazione cattura un aspetto unico della qualità della chat utilizzando una regressione lineare stepwise. Potete vedere come la combinazione di tutte le metriche ABC‑Eval spiega oltre il 25 % della qualità della conversazione, e man mano che rimuoviamo le metriche una per una, la maggior parte di esse porta a una perdita significativa di informazioni sulla qualità. D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno della qualità, e meno di queste metriche trasportano informazioni uniche.

Queste metriche ABC‑Eval, affidabili, informative e distinte, ci consentono di valutare l’intelligenza artificiale conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Potete vedere nei risultati del nostro esperimento che diverse sfide rimangono e sono state quantify con precisione. Ad esempio, i bot che abbiamo testato presentano violazioni di senso comune in circa il 20 % delle loro risposte, forniscono informazioni irrilevanti in circa il 15 % delle risposte, e si contraddicono con se stessi o con il partner in circa il 10 % delle volte. Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero diminuire nei nuovi modelli rilasciati dopo la nostra valutazione.

Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli. Speriamo che ABC‑Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione. Non vediamo l’ora di vedere come l’intelligenza artificiale conversazionale progredirà nei prossimi mesi e anni. Grazie per averci ascoltato.</sample>
    <sample id="270">The authors are affiliated with the Emory NLP Lab at Emory University (led by Professor Jinho Choi) and with Amazon Alexa AI.</sample>
    <sample id="271">**CFT** in the paper stands for **Continuous Fine‑Tuning** – i.e., repeatedly fine‑tuning a pretrained model on the small clean validation set, which the authors show can match or surpass more complex weakly‑supervised learning techniques.</sample>
    <sample id="272">Ci sono 7 autori coinvolti nell’articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Una esplorazione multilingue basata sui dati". Questo lavoro è stato svolto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Per esempio, come tradurremmo "mole" in questa frase? Se la frase precedente fosse "Things could start to get dangerous if the ministers find out", allora "mole" si riferisce a un agente segreto. Ma se la frase precedente fosse "Could it be anything serious, doctor?", allora "mole" indica un chiazzo cutaneo. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la sua traduzione. Tuttavia, valutare quanto bene i modelli possano tradurre questi casi è piuttosto difficile. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, rendendo i metriche a livello di corpus come BLEU incapaci di coglierle. Inoltre, alcune proposte di valutazioni mirate alle traduzioni dipendenti dal contesto si basano su risorse che supportano solo tipologie limitate di dipendenza contestuale e pochi set di lingue, poiché di solito si affidano a conoscenza di dominio e curazione umana. In questo lavoro cerchiamo di rispondere a queste due domande: prima, quando la traduzione richiede contesto? E, secondamente, quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipenda dal contesto durante la traduzione. Nella ricerca precedente, abbiamo introdotto CXMI come misura dell'uso del contesto nei modelli di traduzione automatica. Si tratta di misurare quanto l'informazione del contesto C fornisca sul risultato Y, dato il sorgente X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro estendiamo CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole con alto P‑CXMI come quelle che richiedono contesto per la traduzione. Ora analizziamo le parole con alto P‑CXMI per cercare schemi tra queste parole. E facciamo la nostra analisi sui trascrizioni dei TED talk tradotti dall’inglese a 14 lingue diverse. Effettuiamo l’analisi a tre livelli diversi. Prima, esaminiamo le etichette di parte del discorso che hanno un alto P‑CXMI medio. Questo ci permette, per esempio, di trovare i pronomi duali in arabo che hanno un P‑CXMI relativamente alto, spiegabile dal fatto che l’inglese non ha pronomi duali, quindi serve il contesto per determinare se un pronome è duale quando si traduce in arabo. Allo stesso modo, troviamo che certe lingue richiedono contesto quando dobbiamo scegliere la forma verbale appropriata. Poi guardiamo i vocaboli con alto P‑CXMI medio su tutte le loro occorrenze diverse. Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario il contesto per tradurre i nomi propri in modo coerente all'interno del documento. Allo stesso modo, troviamo che il contesto è importante per tradurre la forma di formalità corretta. Infine, esaminiamo token individuali con alto P‑CXMI. Ciò ci consente di identificare fenomeni che non possono essere catturati semplicemente dalla parola, ma che si esprimono nella struttura della frase, come la risoluzione delle ellissi. Ora usiamo le nostre scoperte dall’analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che si riferiscono al fenomeno. E li chiamiamo MuDA tagger (Multilingual Discourse‑Aware). Possiamo quindi notare che diverse lingue hanno proporzioni diverse di questi fenomeni di discorso. Usiamo il MuDA tagger applicandolo a un corpus parallelo che vogliamo usare per la valutazione e applichiamo le nostre metriche di traduzione di scelta sui casi dipendenti dal contesto identificati dal tagger MuDA. Infine, usiamo il benchmark e altre metriche per valutare diversi modelli di traduzione a livello di documento. In primo luogo, usando metriche a livello di corpus: per BLEU, troviamo che i modelli senza contesto hanno la migliore performance. Ma se usiamo COMET, i modelli con contesto hanno la migliore performance. E se usiamo la misura F‑score delle parole, i modelli con e senza contesto hanno prestazioni comparabili. Questo dimostra di nuovo che è difficile determinare il miglior sistema di traduzione a livello di documento se usiamo solo metriche a livello di corpus. Ora, usando il benchmark MuDA per valutare i modelli, troviamo che i modelli con contesto sono significativamente più precisi di quelli senza contesto per alcuni fenomeni di discorso come la formalità e la coesione lessicale. Ma questi modelli non sono molto più buoni di quelli senza contesto per altri fenomeni come l'ellipsis, i pronomi e la forma verbale. Ciò suggerisce dove è necessario vedere più progressi per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, effettuiamo un’analisi basata sui dati in 14 coppie di lingue per identificare quando la traduzione richiede contesto e poi usiamo le nostre scoperte per costruire un benchmark per la traduzione a livello di documento che può aiutarci a identificare quali fenomeni di discorso i modelli gestiscono bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per l'attenzione. A presto a Toronto.</sample>
    <sample id="274">The presenter’s name is **Yusen Zhang**.</sample>
    <sample id="276">**Riassunto (≈ 200 parole)**  

Nel lavoro “IndicMT Eval: A Dataset to Meta‑Evaluate Machine Translation Metrics for Indian Languages” gli autori Ananya e Vignesh presentano un nuovo corpus per valutare le metriche di traduzione verso l’inglese, ma applicato a cinque lingue indiane (Tamil, Malayalam, Hindi, Marathi, Gujarati).  Dal dataset Flores, 200 frasi di ciascuna lingua sono state tradotte in inglese da sette sistemi (Google, Bing, NLLB, Indic‑Trans, mT5, CVID, mBART), generando 7 000 traduzioni.  

Gli annotatori bilingue, esperti in MQM, valutano ogni output segnando errori (accuratezza, fluidità e categorie speciali) con tipo e severità, oltre a dare un punteggio complessivo.  Il corpus risultante consente lo studio delle correlazioni tra metriche automatiche e giudizi umani.  I risultati mostrano che le metriche basate sulla sovrapposizione (es. chrF) hanno la migliore correlazione globale, ma tendono a dare punteggi compressi; le metriche basate su embedding (LabSE, BERTScore con modelli MuRIL) e soprattutto le versioni COMET ottengono le più alte correlazioni.  

I ricercatori hanno quindi fine‑tuned COMET sul dataset MQM, creando IndicCOMET, che supera COMET in tre delle cinque lingue e mantiene buone prestazioni anche in zero‑shot su lingue non viste.  IndicCOMET mostra inoltre una maggiore robustezza (correlazione 0,36) rispetto a COMET (0,272) sui set di sfida ACES.  

Il dataset è disponibile pubblicamente per la comunità.</sample>
    <sample id="277">The method introduced in the paper is referred to as **“Multiset Tagging and Latent Permutations.”**</sample>
    <sample id="278">L’autore ha spiegato che il metodo “Marked Words” si basa sul concetto sociolinguistico di **markedness**: c’è un gruppo “unmarked” (di solito la norma dominante) e tutti gli altri gruppi sono “marked”.  Per ogni gruppo marcato si confrontano le sue personas con quelle dei gruppi unmarked (ad es. donne di colore vs. uomini bianchi) e si calcolano **log‑odds ponderati** (Fightin’ Words) per identificare le parole che più marcano la differenza.  In pratica, il metodo trova le parole che più distintamente caratterizzano i gruppi marcati rispetto ai gruppi unmarked, senza ricorrere a una lista predefinita di stereotipi.</sample>
    <sample id="279">Le autrici/author sono affiliati all’**Università di Washington** (Shangbin è un PhD student presso questa università).</sample>
    <sample id="280">**Abstract**

Emotion recognition in conversations (ERC) requires accurate prediction of the emotion label for each utterance, leveraging textual, audio, and visual cues. Existing approaches largely focus on speaker and contextual modeling, often under‑exploiting multimodal complementarity, performing poorly on minority emotion classes, and struggling to separate semantically similar emotions. We propose **MultiEMO**, an attention‑based correlation‑aware multimodal fusion framework that addresses these gaps. First, we introduce **VisExtNet**, a visual encoder that isolates facial expression features from multiple frames using MTCNN and a VGGFace2‑pretrained ResNet‑101, discarding irrelevant scene information. Second, we design **MultiAttn**, a bidirectional multi‑head cross‑attention fusion network comprising three sub‑modules (MultiAttn‑text, MultiAttn‑audio, MultiAttn‑visual). Each sub‑module sequentially fuses a primary modality with the complementary information of the other two modalities through stacked cross‑attention layers, followed by residual connections and layer normalization. Third, we propose **Sample‑Weighted Focal Contrastive (SWFC) loss** that up‑weights minority classes and enforces inter‑class separability, particularly for semantically similar emotions. Extensive experiments on MELD and IEMOCAP demonstrate that MultiEMO achieves state‑of‑the‑art performance, markedly improving recognition of minority and similar emotion categories. Ablation studies confirm the efficacy of VisExtNet, MultiAttn, and SWFC loss, while highlighting remaining challenges such as speaker differentiation and batch‑size sensitivity. This work advances multimodal ERC by integrating modality‑specific expertise, correlation‑aware fusion, and class‑aware loss design.</sample>
    <sample id="281">**Riassunto (≈200 parole)**  

Il paper “When Does Translation Require Context? A Data‑driven, Multilingual Exploration” analizza quando la traduzione di una frase dipende dal contesto in 14 lingue diverse (TED‑talks). Utilizzando la misura “CXMI” (mutua informazione condizionata) e la sua estensione “Pointwise CXMI” (P‑CXMI), gli autori quantificano quanta informazione aggiuntiva fornisce il contesto \(C\) rispetto alla fonte \(X\) per produrre la traduzione \(Y\). Le parole con alto P‑CXMI indicano una forte dipendenza dal contesto.  

L’analisi a tre livelli mostra:  
1. **POS‑level**: pronomi duali in arabo, forme verbali che richiedono la scelta corretta in altre lingue.  
2. **Vocabolario‑level**: nomi propri in cinese (coerenza terminologica) e formalità.  
3. **Token‑level**: fenomeni più complessi come risoluzione di ellissi.  

Da queste scoperte nasce il **MuDA tagger** (Multilingual Discourse‑Aware), che identifica automaticamente i token legati ai cinque fenomeni di discorso. Il MuDA benchmark permette di valutare specificamente i modelli che sfruttano il contesto, confrontando metriche standard (BLEU, COMET, F‑measure) e risultati commerciali.  

Risultati:  
- In BLEU, i modelli senza contesto performano meglio; in COMET, i modelli con contesto sono superiori.  
- Con MuDA, i modelli con contesto migliorano significativamente su formalità e coesione lessicale, ma mostrano margini di miglioramento su ellissi, pronomi e forme verbali.  
- DeepL supera Google Translate per la traduzione documentale.  

Conclusione: la valutazione documentale deve concentrarsi su fenomeni specifici; MuDA fornisce un benchmark data‑driven per misurare l’efficacia dei sistemi di traduzione a livello di discorso.</sample>
    <sample id="282">**Summary (≈200 words)**  

The talk introduces **StoryTrans**, a novel model for non‑parallel author‑style transfer at the story level, addressing gaps in prior work that focused mainly on token or sentence‑level style changes. Story-level transfer is challenging because long texts exhibit complex discourse structures and style‑specific content that is tightly coupled with topics. The proposed solution learns *discourse representations* from source stories and combines them with learnable style embeddings to generate target‑style text. To reduce stylistic leakage, a *disentanglement loss* pulls representations of different styles closer in latent space, while a *sentence‑order loss* captures discourse dependencies.  

StoryTrans uses a two‑stage generation pipeline. First, the model transfers the input while masking style‑specific content keywords. Second, a separate stage (unrelated to style transfer) fills in these masked tokens to preserve content. Training employs self‑reconstruction, disentanglement, sentence‑order, and style‑classifier losses in the first stage, and a content‑preservation objective in the second.  

The authors collected new Chinese and English datasets of fairy tales and everyday stories for this task. Automatic metrics and human evaluation show that StoryTrans outperforms strong baselines in style control and content preservation. Style‑space visualizations confirm alignment with target styles, and qualitative examples demonstrate that StoryTrans generates coherent, style‑consistent narratives while maintaining source semantics, unlike baseline models that produce unrelated sentences. The code and data are publicly released.</sample>
    <sample id="283">La prima struttura di dipendenza **simmetrica** menzionata è **“Hudson's Word Grammar”**.</sample>
    <sample id="284">FSUIE è un modello di Universal Information Extraction (UIE) che introduce un “fuzzy span mechanism” per superare la dipendenza dalle precise posizioni di inizio e fine delle entità annotate. Il problema principale è l’ambiguità delle annotazioni: diverse scelte di confine possono essere valide, quindi il modello dovrebbe apprendere confini “sfocati” anziché rigidi. FSUIE definisce la distribuzione di confine come una probabilità continua all’interno di un intervallo \([R_{\min}, R_{\max}]\) e la converte in valori discreti tramite una funzione di campionamento. La perdita è composta da Binary Cross‑Entropy con il confine d’oro e dalla divergenza KL tra la distribuzione prevista e la distribuzione fuzzy di riferimento.

Per migliorare l’attenzione sui span, FSUIE impiega una “fuzzy span attention” (FSA). La maschera \(G\) utilizza un parametro ottimizzabile \(\delta\) per regolare dinamicamente la lunghezza dell’intervallo di attenzione e decresce linearmente verso i bordi, evitando troncamenti bruschi. La FSA è inserita solo al livello più alto, mantenendo inalterata l’encoding del testo.

I test su NER, RE (ACE2004/2005, ADE) e ASTE (AST‑V2) mostrano miglioramenti significativi rispetto a UIE‑base; FSUIE raggiunge risultati di ultima generazione sui dataset citati. Lo studio di ablazione dimostra che la FSA accelera la convergenza e la FSL (fuzzy span loss) aumenta la capacità d’estrazione. La visualizzazione dell’attenzione evidenzia che il modello si concentra su token semantici limitati davanti all’entità, confermando l’efficacia della proposta.</sample>
    <sample id="285">Il lavoro di Mingqi Gao e colleghi si concentra sul “Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine‑grained Evaluation Framework”, che affronta la questione delle inesattezze nei riassunti di dialoghi. I riassunti generati da modelli (e talvolta anche quelli di riferimento) spesso contengono errori fattuali, e si distinguono due approcci principali: (1) incorporare obiettivi di “factuality” durante l’addestramento o l’inferenza per rendere i modelli più fedeli, e (2) sviluppare modelli di Factual Error Correction (FEC) indipendenti, che prendono documento di partenza e riassunto generato e producono una versione corretta.

Gao sostiene che l’attuale valutazione dei FEC sia fuorviante: si basano su metriche di factualità (FactCC, DAE) che restituono un punteggio globale e possono incoraggiare la produzione di riassunti completamente nuovi, ignorando l’originale. Per questo propongono l’utilizzo di correzioni manuali annotate, che permettono di addestrare i modelli con dati più realistici e di valutare la correzione con maggiore precisione.

Il paper introduce una nuova tassonomia degli errori: “content‑based” (soggetto, oggetto, avverbio, ecc.) e “form‑based” (inserimento, cancellazione, sostituzione). Per analizzare e confrontare le correzioni, si adatta ERRANT, composto da allineamento, classificazione e confronto.

Sperimentando diversi FEC in modalità di addestramento, gli autori trovano che l’uso di correzioni di riferimento provenienti da dataset di dialoghi porta ai migliori risultati (anche se le metriche di factualità rimangono poco affidabili). L’addestramento con dati umani combinato a dati sintetici si rivela promettente, ma i modelli attuali ancora faticano a correggere errori di “addizione” e non affrontano tipi più complessi come errori di attributo, modalità o collegamento. In sintesi, il lavoro evidenzia la necessità di una valutazione più fine‑grained e di dati di correzione più accurati per migliorare i FEC in dialoghi.</sample>
    <sample id="286">The presenters are James Finch and Sarah Finch.</sample>
    <sample id="287">Ci sono quattro autori coinvolti nell’articolo.</sample>
    <sample id="288">I possono essere usati i seguenti set:

- **BLiMP** (ad esempio il caso “Adjunct Island”);  
- **SyntaxGym** (varie tipologie di fenomeni sintattici);  
- **CrowS‑pairs** (per accettabilità legata a stereotipi).  

Per esperimenti di “contesto non pertinente” si possono aggiungere frasi provenienti da **Wikipedia**.</sample>
    <sample id="290">Isto é o que a pesquisa identifica como as **cinco abreviações** dos métodos usados na primeira pergunta de investigação (necessidade de validação limpa):

| Abreviação | Nome completo (abreviado) |
|------------|---------------------------|
| **FTw** | Fine‑Tuning on Weak labels |
| **COSINE** | Constrained Optimization for Semi‑Supervised INference (W. Schneider et al.) |
| **CWSL** | Clean‑Weighted Supervised Learning |
| **Co‑Teaching** | Co‑Teaching (ou Co‑Teaching+) |
| **Co‑Training** | Co‑Training |</sample>
    <sample id="291">The model is evaluated on a set of 11 biomedical and clinical downstream tasks, specifically including **named‑entity recognition, text classification, part‑of‑speech tagging, and question‑answering**.</sample>
    <sample id="294">CamemBERT è stato addestrato inizialmente su una grande raccolta di testo francese proveniente dal **crawling web OSCAR**, con una versione a 138 GB (e le relative versioni ridotte a 4 GB).  È stato anche pre‑addestrato su un corpus più piccolo di CCNET (≈4 GB).</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">Il video presenta il progetto EPIC (English Perspectivist Irony Corpus), nato dalla collaborazione tra l’Università di Torino e Amazon Alexa. L’obiettivo è studiare l’ironia, un fenomeno pragmatico difficile da rilevare con i modelli di NLP basati su apprendimento supervisionato. Invece di classificare semplicemente una frase come “ironic” o “not ironic”, si mira a produrre output più informativi.

EPIC è composto da circa 300 brevi conversazioni (sottoparti in coppie di messaggi) estratte da Reddit e Twitter, coprendo un arco temporale di 1½ anni e cinque varianti dell’inglese. La raccolta è stata annotata tramite Prolific: 74 annotatori (≈15 per variante) hanno valutato 200 conversazioni ciascuno, con controlli di qualità. Ogni annotatore risponde “Ironic” o “Not ironic” a una domanda contestuale.

I risultati mostrano differenze di consenso tra gruppi demografici (genere, età, nazionalità). L’inter-agreement è rappresentato in un violino plot, evidenziando variazioni costanti a seconda della divisione dei dati. Per affrontare tali differenze, sono stati addestrati modelli “perspective‑aware”: fine‑tuning di un LM su split di annotatori specifici. Sebbene le metriche di performance non mostrino tendenze marcate, i modelli perspective‑aware risultano più sicuri (meno incertezza) rispetto ai modelli aggregati.

Infine, l’analisi approfondita ha rivelato che le generazioni vicine (es. 25‑34 vs 35‑44) mostrano più disaccordo sull’ironia, così come annotatori provenienti da Regno Unito e Irlanda, evidenziando una forte dipendenza geografica. Il progetto invita a ulteriori discussioni e a domande durante la poster session.</sample>
    <sample id="297">Il progetto “From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models” analizza la retorica codificata—i cosiddetti *dogwhistles*—che trasmettono un messaggio nascosto a un gruppo in‑group mentre appariscano innocenti agli osservatori esterni. Un esempio citato è “cosmopolitan”, termine che, pur non menzionando esplicitamente gli ebrei, è inteso da chi lo usa come segnale anti‑ebraico. La ricerca sottolinea come i dogwhistles siano altamente contestuali, difficili da identificare e spesso usati per eludere la moderazione dei contenuti online.

Il lavoro sviluppa una tipologia articolata su *register* (formale/informale), *persona* (es. antisemitismo, transphobia) e *type* (implicatura aggiuntiva o semplice segnale di persona). Viene inoltre pubblicato un glossario di oltre 340 termini e simboli, raccolti da fonti accademiche, Wikipedia e blog, con esempi concreti e contesto storico.

Una case study sui discorsi politici degli Stati Uniti mostra un incremento dell’uso di dogwhistles razziali e di destra dal periodo della “Southern Strategy” in passato, con una correlazione con la crescita delle politiche conservatrici.  

Per testare la capacità dei modelli di linguaggio di riconoscere questi segni, gli autori hanno interrogato GPT‑3 con varie prompt; il modello riesce a elencare i dogwhistles del registro formale ma fallisce con quelli più informali e trans‑phobici, anche se migliora quando viene fornita una definizione esplicita del segnale.  

Infine, l’analisi su Prospective API dimostra che sostituire slur o etichette di gruppo con dogwhistles riduce significativamente la valutazione di “toxicity” dei messaggi, evidenziando come tali tecniche evino la moderazione automatica.</sample>
    <sample id="298">I risultati che hanno portato alla conclusione sono stati:

1. **Esperimento di “continual‑pre‑training”** – i modelli sono stati ri‑addestrati o pre‑addestrati con dati più recenti (ad es. Reuters 2020).  
2. **Andamento della performance** – l’F1 sui nuovi set di test diminuisce in modo sistematico quanto più è grande il gap temporale tra i dati di addestramento e quelli di test.  
3. **Assenza di adattamento overfitting** – la curva di miglioramento sui set di test non mostra “diminishing returns” (coefficiente di regressione &gt; 1), quindi l’over‑fitting al test non è la causa.  

Quindi, l’unica variabile che spiega la perdita di prestazioni è la **deriva temporale**.</sample>
    <sample id="299">**Riassunto (≈200 parole)**  

Michalis Korakakis, in collaborazione con Andreas Vlachos, presenta un nuovo approccio di addestramento “minimax” per migliorare la robustezza dei modelli di Natural Language Inference (NLI). I modelli NLI di stato‑dell’arte eccellono su set di benchmark ma spesso sfruttano “shortcut”, cioè correlazioni spurie tra caratteristiche di input (ad es. sovrapposizione di parole) e le etichette, che li rendono fragili su dati fuori‑distribuzione (ad es. set avversari HANS, PAWS).  

Le tecniche esistenti richiedono un modello ausiliario pre‑addestrato, conoscenza di shortcut specifici e suppongono che il modello ausiliario e il modello principale esplorino gli stessi shortcut. Il metodo proposto evita queste limitazioni: l’auxiliary è una semplice rete feed‑forward che, in un ciclo di addestramento alternato, assegna pesi alle istanze di addestramento cercando di massimizzare la perdita del learner. Così, il learner è spinto a concentrarsi su esempi “difficili” e poco rappresentati, che normalmente contraddicono gli shortcut prevalenti.  

La metodologia è valutata su MNLI, FEVER e QQP, confrontandola con ERM e altri metodi di mitigazione shortcut, ottenendo miglioramenti costanti delle performance out‑of‑distribution senza sacrificare l’accuratezza in‑distribution. In aggiunta, lo studio analizza l’effetto del pre‑addestramento del learner, la dimensione minima dell’auxiliary, l’uso di modelli più grandi e l’applicabilità su shortcut sintetici e set out‑of‑domain.</sample>
    <sample id="300">**Riassunto (≈200 parole)**  

Belinda presenta la ricerca condotta presso Semantic Machines in collaborazione con Jason Eisner, Adam Pauls e Sam Thomson, introdotto il nuovo compito *interactive dictation*. Questo compito consiste in un flusso di lavoro in cui l’utente può parlare per *dictare* un documento e *modificarlo* tramite comandi vocali in tempo reale, senza richiedere parole chiave di attivazione. Un esempio illustrativo mostra come una frase dicata (“Just wanted to ask about the event on the 23rd”) venga trascritta e successivamente corretta (“on Friday the 23rd”) tramite un’interruzione verbale; l’utente può poi aggiungere una nuova frase e, infine, emettere un comando (“Replace ‘the event’ in the last sentence with ‘it’”) che il sistema interpreta e applica.

L’autrice sottolinea che i software esistenti (Nuance Dragon, Microsoft Word Dictate) richiedono comandi fissi, rendendo l’esperienza poco intuitiva rispetto a un assistente umano che riconosce spontaneamente le istruzioni. L’interazione è quindi caratterizzata da un’interlacciamento flessibile di dictazioni e comandi, con linguaggio naturale e aperto.

Per formalizzare il compito, è stato definito un processo in quattro fasi: (1) riconoscimento ASR, (2) segmentazione dictazione/comando, (3) normalizzazione ed eventuale correzione degli errori ASR, e (4) esecuzione sequenziale di dictazioni e comandi per ottenere lo stato finale del documento.

Il lavoro comprende la progettazione di un’interfaccia di raccolta dati, la creazione di un dataset e lo sviluppo di un sistema di base. Le quattro fasi sono affrontate con modelli separati: segmentazione, interpretazione (utilizzando T5 e GPT‑3) e riparazione ASR. I risultati mostrano che GPT‑3 offre maggiore accuratezza ma a costo di tempi di esecuzione più lunghi; le versioni T5 mantengono buone prestazioni con maggiore efficienza. L’autrice conclude invitando la comunità a contribuire allo sviluppo di soluzioni più efficienti e accurate.</sample>
    <sample id="302">I è necessario permutare i token perché, nella prima fase del modello, per ogni token di input viene assegnato un *unordered multiset* di token che appariranno nell’output. In questo modo otteniamo tutti i token corretti, ma **senza alcuna informazione sull’ordine** in cui devono essere disposti nella frase logica.  

Il secondo step, quindi, è proprio quello di scegliere una **permutazione** (cioè un ordine) che trasformi quel multiset in una sequenza coerente con la struttura sintattica della forma logica. Senza questa permutazione l’output risulterebbe un semplice insieme di token (o, peggio, una sequenza sbagliata) e non catturerebbe la corretta corrispondenza composizionale tra input e output. In sintesi, la permutazione è l’unico modo per trasformare l’insieme non ordinato di token ottenuto dalla prima fase in la sequenza corretta di output.</sample>
    <sample id="303">Gli autori chiedono maggiore trasparenza perché, senza conoscere i dettagli delle tecniche di mitigazione, non si può capire se gli stereotipi “positivi” che emergono siano dovuti a un allineamento di valore, a filtri anti‑stereotipo o a altri meccanismi. Una maggiore chiarezza consentirebbe di valutare correttamente l’efficacia delle mitigazioni, di fare ricerca più accurata e di rendere gli sviluppatori responsabili delle loro scelte.</sample>
    <sample id="304">Gli **input inaccettabili di coppia minima** sono le frasi che violano la grammaticalità o l’acceptability di un fenomeno linguistico. In un “minimal pair” si confronta una versione grammaticalmente corretta con una versione non corretta (o “unacceptable”); l’inaccettabile è quindi la frase che contiene l’errore sintattico o semantico che si vuole testare.</sample>
    <sample id="305">Nel video Dawei, PhD di Saarland University, presenta il lavoro “Weaker Than You Think: A Critical Look at Weakly Supervised Learning” in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Inizia spiegando che la weak supervision evita l’annotazione manuale: si utilizzano fonti “deboli” (regole euristiche, knowledge base, crowdsourcing di bassa qualità) che però sono rumorose. Se si addestra un modello direttamente su dati rumorosi, la rete tende a memorizzare l’errore e non generalizza; per questo si sviluppano algoritmi di weakly supervised learning (WSL) che mirano a essere robusti contro il rumore.

Il problema principale sollevato è che la maggior parte delle rivendicazioni di WSL presuppone l’esistenza di un set di validazione pulito, che non viene sempre menzionato. Dawei pone tre domande di ricerca: (1) è davvero necessario un set di validazione pulito, o si può usare un set rumoroso? (2) Se è necessario, quante etichette pulite servono? (3) Come sfruttare al meglio questi dati puliti?

I risultati mostrano che:  
1. Le tecniche WSL più recenti richiedono effettivamente un set di validazione pulito; senza di esso, la performance collassa.  
2. Con soli 20 campioni puliti per classe si può ottenere un buon risultato.  
3. Se si dispone di esempi puliti, un semplice fine‑tuning su di essi supera le metodologie WSL più complesse; in particolare, continuare a fine‑tune dopo il training WSL porta a prestazioni equivalenti a quelle di metodi sofisticati come COSINE.

Dawei conclude con tre raccomandazioni: (i) riportare sempre i criteri di selezione modello e se utilizzano validazioni pulite; (ii) confrontare WSL con baselines di few‑shot learning; (iii) considerare il fine‑tuning continuo come baseline forte. Il codice è open‑source e disponibile tramite QR code.</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano la loro ricerca sull’**Entity Tracking in Language Models**, ossia la capacità dei modelli di comprendere e aggiornare lo stato delle entità in un discorso. L’obiettivo è capire in che misura i modelli pre‑addestrati possono tracciare le modifiche di stato di oggetti (ad es. “metti le uova, lo zucchero e la farina in una ciotola” → “tutti questi oggetti sono ora nella ciotola”) senza affidarsi a associazioni superficiali.

Per evitare “trucchi” (come pattern già presenti nei dati di pre‑addestramento, associazioni parola‑stato, o memorizzazione di sequenze), i ricercatori hanno progettato un task basato su **scatole e oggetti**. L’input descrive lo stato iniziale delle scatole e poi elenca operazioni (spostamenti, aggiunte, rimozioni). Il modello deve predire il contenuto finale di ogni scatola. Il compito è semplice se si può solo copiare lo stato iniziale, ma richiede l’aggiornamento logico delle operazioni per i casi in cui lo stato finale differisce.

Gli esperimenti, con Flan‑T5 e vari modelli GPT (3, 3.5, 4), utilizzando 2‑shot in‑context learning, mostrano che la maggior parte dei modelli si limita a ripetere lo stato iniziale. Solo **text‑davinci‑003** (GPT‑3.5) e alcuni modelli GPT‑3.5 (addestrati anche su codice) dimostrano una tracciabilità non banale; i restanti modelli rientrano sotto la baseline random. I modelli T5‑base, se fine‑tuned, possono imparare il compito, ma i modelli casuali non lo fanno, indicando che il pre‑addestramento è cruciale.

In sintesi, la ricerca evidenzia che la capacità di tracciare lo stato delle entità è presente solo in modelli con pre‑addestramento su codice e/o con sufficienti dimensioni, ma rimane incerta la sua generalizzabilità oltre lo specifico set‑up.</sample>
    <sample id="307">The authors evaluated their seven models on a range of downstream tasks using the standard metrics for each task:

| Task | Primary metric |
|------|----------------|
| Named‑entity recognition (NER) | **F1‑score** (macro‑averaged) |
| Text classification | **Accuracy** (and F1‑score for imbalanced datasets) |
| Part‑of‑speech tagging (POS) | **Accuracy** (token‑level precision) |
| Question‑answering (QA) | **Exact‑Match (EM)** and **F1‑score** (token‑level overlap) |

These metrics allowed the authors to compare performance across the seven pre‑trained models and the six baseline models on the 11 biomedical and clinical downstream tasks.</sample>
    <sample id="308">Il lavoro “NLPositionality” esplora come i bias di progettazione influenzino le prestazioni delle tecnologie NLP su diverse popolazioni. Il concetto di **positionalità** (perspettiva culturale, identità, esperienze di vita) è stato riportato in studi critici, ma non è stato sistematicamente confrontato con i dataset e i modelli. L’approccio proposto consiste in due fasi:  

1. **Re‑annotazione** di grandi corpora con un campione diversificato di annotatori (oltre 1 000 partecipanti da 87 paesi, 16 000 risposte).  
2. **Confronto** delle nuove etichette con le etichette originali, i dataset di riferimento e i modelli (GPT‑4, Perspective API, Hate‑Roberta, ecc.) mediante il coefficiente di Pearson.  

Due task sono stati eseguiti su **Lab in the Wild**:  
- *Social acceptability* (situazioni di Social Chemistry).  
- *Toxicity/Hate speech* (Dynahate).  

I risultati mostrano che dataset e modelli sono **più allineati** con utenti provenienti da paesi anglofoni e con istruzione universitaria, mentre le persone non binarie sono meno rappresentate. Inoltre, GPT‑4 e Dynahate riflettono maggiormente le prospettive confuciane ed anglo‑speaking.  

Le raccomandazioni includono: mantenere traccia di tutte le scelte progettuali, adottare una prospettiva di “perspectivism” nella ricerca NLP e sviluppare dataset/modelli specifici per comunità marginalizzate (es. Masakhani). Il progetto sottolinea che l’inclusività va oltre l’“funzionamento universale” e richiede un’attenzione continua alla positionalità.</sample>
    <sample id="309">La concordanza tra gli annotatori è stata valutata tramite la metrica di inter‑annotator agreement (ad esempio, **Cohen’s kappa**).</sample>
    <sample id="310">Il dominio scelto per aggiungere frasi completamente scollegate alle query è **Wikipedia**.</sample>
    <sample id="311">Le informazioni fornite nel testo non indicano le affiliazioni istituzionali degli autori (Regina Stodden e Omar). Non è possibile rispondere a questa domanda con i dati disponibili.</sample>
    <sample id="312">**MultiInstruct si differenzia dagli altri benchmark in:**

1. **Modalità** – è l’unico dataset di “instruction tuning” a coprire *multimodalità* (testo + immagini) invece di essere limitato al solo testo come i benchmark NLP (es. 1 600 task solo linguistici).

2. **Ampiezza e varietà** – comprende **62 task** distribuiti in **10 categorie** (ad es. generazione, classificazione, VQA, ragionamento di senso comune), derivati da **21 dataset open‑source**.

3. **Istruzioni multiple** – per ogni task sono fornite **5 istruzioni scritte da esperti**, permettendo di studiare l’effetto di diverse formulazioni e di introdurre la metrica di *sensibilità* (coerenza dell’output rispetto alla variazione del testo dell’istruzione).

4. **Formato seq2seq unificato** – input e output (testo, immagini, bounding boxes) sono **tokenizzati nello stesso vocabolario**, facilitando l’addestramento di modelli multimediali come OFA.

5. **Valutazione mirata** – oltre a metriche standard (accuracy, Rouge‑L), viene valutata la *sensibilità* per misurare la robustezza delle istruzioni.

In sintesi, MultiInstruct è il primo benchmark su larga scala che combina istruzioni multi‑modal, istruzioni multiple per task e una struttura di valutazione specifica per la robustezza, distinguendosi così dai benchmark tradizionali che trattano solo compiti di linguaggio.</sample>
    <sample id="313">Two authors are involved in the article.</sample>
    <sample id="314">**Coordinazione binaria**: una costruzione sintattica in cui due elementi (conjuncti) dello stesso tipo sono uniti da un connettivo (ad es. “e”, “o”) e formano un unico elemento coordinato. In altre parole, è la coordinazione che coinvolge esattamente due elementi.</sample>
    <sample id="315">Lo studio non menziona un tempo medio di utilizzo dei prompt: i prompt sono semplicemente forniti al modello e le risposte vengono generate immediatamente, senza indicare una durata complessiva.</sample>
    <sample id="316">The study shows that when fine‑tuned on the CoScript dataset, a **small T5 model actually outperforms most large language models in constrained language planning**. This implies that, with a high‑quality, specialized dataset, compact models can achieve superior planning quality—demonstrating that size alone isn’t the determining factor and that tailored training can make smaller models more effective and efficient for this task.</sample>
    <sample id="317">CodeIE proposes leveraging code‑generation large language models to perform few‑shot information extraction (IE). Traditional IE models pre‑train in a text‑to‑text paradigm; at inference they must linearise structured outputs, leading to mismatches between pre‑training and inference, high linguistic perplexity, and frequent structural errors. CodeIE reformulates IE as a pure code‑generation task: a prompt defines a function (e.g., `def extract_entities(text): …`) and the model is asked to output executable code that populates a list with entity tuples. Few‑shot demonstrations are provided in the same code style, ensuring that input and output structures are identical. The approach is evaluated on three NER datasets and four RE datasets with four models: T5, UIE, GPT‑3 (text‑davinci‑002), and Codex (code‑davinci‑002). Results show that the Codex‑based, code‑style prompt outperforms all baselines, especially in one‑to‑few shot scenarios. Analysis attributes the gains to lower perplexity on code inputs, better alignment of pre‑training objectives, and reduced structural errors. Codex also produces labels outside the predefined set when using text prompts, whereas the code prompt eliminates such anomalies. Overall, CodeIE demonstrates that code‑generation LLMs, when prompted in code style, are superior few‑shot IE engines. Future work will explore scaling to larger corpora and integrating domain‑specific code libraries to further boost performance.</sample>
    <sample id="318">**Presentazione di Yanis Labrak: “DrBERT: un modello pre‑addestrato robusto in francese per i domini biomedici e clinici”**

Nella presentazione iniziamo parlando del *language modeling* nel settore sanitario e poi passiamo al contributo principale del nostro articolo. Presentiamo il primo modello biomedico in francese, DrBERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici estratti dal web. Abbiamo inoltre confrontato diversi approcci di pre‑allenamento e fonti di dati. Successivamente descriviamo i risultati ottenuti su 11 compiti di downstream in ambito biomedico e clinico in francese. Concludiamo con una sintesi degli esperimenti e forniamo maggiori dettagli su come accedere ai modelli.

---

### 1. Background: BERT e domini specializzati

Dal 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di NLP, offrendo enormi guadagni di performance rispetto ai metodi statici e contestualizzati più vecchi come Word2Vec e fastText. Da allora, il modello è stato adattato a molte altre lingue (ad esempio CamemBERT in francese) e a diversi domini (PubMedBERT, BioBERT, ClinicalBERT), ma in gran parte in inglese. Modelli specializzati per altre lingue sono rari e spesso si basano su *continual pre‑training* a causa della carenza di dati in‑domain. In francese, prima di questo lavoro non esisteva un modello open‑source dedicato al biomedico.

---

### 2. Domande di ricerca

- **Quali sono le fonti di dati più appropriate per un modello in francese che possa essere usato in molteplici contesti?**  
  I dati web‑crawled potrebbero sostituire i dati clinici?

- **Quanto è necessario di dati per addestrare un modello specializzato in francese?**  
  4 GB, 8 GB o più?

Per rispondere a queste domande abbiamo confrontato DrBERT con ChuBERT, un modello clinico basato su dati anonimizzati provenienti dall’archivio sanitario del Nantes University Hospital.

---

### 3. Modelli addestrati

Abbiamo creato **sette modelli**:

| Modello | Fonte dei dati | Dimensione | Tipo di pre‑allenamento |
|---------|----------------|------------|------------------------|
| **DrBERT‑7 GB** | NACHOS | 7 GB | *From‑scratch* |
| **DrBERT‑4 GB** | NACHOS | 4 GB | *From‑scratch* |
| **ChuBERT‑4 GB** | Clinical notes | 4 GB | *From‑scratch* |
| **ChuBERT‑mix** | 4 GB NACHOS + 4 GB notes | 8 GB | *From‑scratch* |
| **CamemBERT‑NACHOS‑CP** | 4 GB NACHOS | 4 GB | *Continual pre‑training* (pesatura e tokenizer di CamemBERT) |
| **CamemBERT‑Notes‑CP** | 4 GB notes | 4 GB | *Continual pre‑training* (pesatura e tokenizer di CamemBERT) |
| **PubMedBERT‑NACHOS‑CP** | 4 GB NACHOS | 4 GB | *Continual pre‑training* (pesatura e tokenizer di PubMedBERT) |

---

### 4. Valutazione su compiti di downstream

Abbiamo testato i sette modelli su compiti pubblici e privati: riconoscimento di entità nominate (NER), classificazione, etichettatura delle parti del discorso (POS) e risposta a domande (QA). I modelli sono stati confrontati con sei baseline:

- CamemBERT OSCAR 138 GB
- CamemBERT OSCAR 4 GB
- CamemBERT CCNET 4 GB
- PubMedBERT
- BioBERT
- ClinicalBERT

**Principali risultati:**

- I modelli ottenono migliori risultati sui compiti in cui i dati di addestramento sono del *mismo dominio* di quelli di test.
- I dati provenienti da fonti eterogenee risultano più versatili.
- Maggiore quantità di dati → migliore performance.
- L’addestramento *from‑scratch* tende a produrre performance superiori nella maggior parte dei compiti.
- Il modello *CamemBERT‑NACHOS‑CP* (continual pre‑training con pesi e tokenizer di CamemBERT) raggiunge risultati comparabili a DrBERT‑4 GB *from‑scratch*.  
  Invece, l’uso di pesi e tokenizer di CamemBERT su dati clinici porta a problemi di stabilità.

**Conclusioni:**  
Il nostro sistema specializzato supera la baseline generica (CamemBERT) in 9 dei 11 compiti di downstream. I dati più specializzati sono migliori, ma la scala non è lineare.

---

### 5. Accesso ai modelli

Tutti i modelli pre‑addestrati basati su NACHOS sono disponibili gratuitamente su **Hugging Face** e distribuiti sotto licenza **MIT**. Gli script di training sono presenti nel nostro repository GitHub.

---

Grazie per l’attenzione. Siamo ansiosi di scambiare idee al poster session di Toronto.</sample>
    <sample id="319">The study evaluates two broad learning strategies:

1. **From‑scratch pre‑training**  
   • DrBERT 7 GB (full NACHOS corpus)  
   • DrBERT 4 GB (subset of NACHOS)  
   • ChuBERT 4 GB (clinical notes only)  
   • ChuBERT 8 GB (4 GB NACHOS + 4 GB clinical notes)

2. **Continual (re‑)pre‑training**  
   • CamemBERT weights + 4 GB NACHOS  
   • CamemBERT weights + 4 GB clinical notes  
   • PubMedBERT weights + 4 GB NACHOS

These experiments compare different data sizes, data domains (web‑crawled vs. clinical), and pre‑training approaches (from‑scratch vs. continual fine‑tuning).</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è praticamente nullo: la pendenza della retta di regressione è maggiore di uno, quindi non si osserva alcun ritorno decrescente e l’overfitting adattivo non è presente.</sample>
    <sample id="321">The quality of the simplifications was assessed by comparing the output of the fine‑tuned models to the manually aligned, gold‑standard pairs in the DEPLAIN corpus.  The authors ran a set of standard evaluation metrics (reported in the paper) on the generated simplified sentences and documents, and then compared those scores to baseline values to determine whether the models improved over the default approach.</sample>
    <sample id="322">Enrico presenterà a ACL 23 la domanda “Cosa impara un classificatore di testo sulla moralità?” Inizia definendo la moralità come il nostro senso interno di giusto‑ingiusto e sottolinea che essa è intrinsecamente soggettiva: diverse persone giudicano lo stesso argomento (es. aborto, diritti LGBTQ) in maniera opposta. Per evitare di ridurre la moralità a un’unica scala “morale‑immorale”, propone di usare la **Moral Foundation Theory** (MFT), che identifica cinque fondamenti (cura, libertà, giustizia, autorità, lealtà) che le persone ponderano diversamente.  

Nel NLP esistono già lavori che applicano la MFT, ma i modelli di linguaggio sono spesso addestrati su una sola prospettiva. Enrico e il suo gruppo hanno quindi creato il **Moral Foundation Twitter Corpus** (≈ 35 000 tweet) raccolti su sette domini (es. #AllLivesMatter, #BlackLivesMatter). Usando tecniche di explainable AI, hanno analizzato se i modelli comprendono le differenze di espressione morale tra domini.  

Un esempio chiave mostra che “sovversione” è vista negativamente in #AllLivesMatter (“overthrow, mayhem”), mentre in #BlackLivesMatter è in parte incoraggiata. I risultati indicano che i modelli possono cogliere queste sottili variazioni, ma che un unico modello generico può portare a fraintendimenti pericolosi. Enrico conclude invitando i partecipanti a discutere questi temi a ACL Toronto.</sample>
    <sample id="323">**Summary (≈200 words)**  

The paper introduces **DHLK** – a dynamic heterogeneous‑graph reasoning framework that blends knowledge stored in large language models (LLMs) with structured knowledge bases to tackle Commonsense QA. Traditional methods retrieve a subgraph via entity matching, often pulling irrelevant nodes (e.g., “Top”, “Bank”, “Cat”) and encode text and graph separately, missing cross‑modal interactions and entity semantics. DHLK first constructs a heterogeneous knowledge graph (HKG) from multiple KBs (ConceptNet, WordNet, Wiktionary) using a two‑stage pruning strategy: (1) subword removal via dictionary vocabularies, (2) retrieval of paraphrases from WordNet and Wiktionary to add complementary nodes. RoBERTa with Mask Self‑Attention jointly encodes the QA context and the HKG, filtering out weakly relevant entities (e.g., “wood”) during a second subgraph construction.

Entity and relation embeddings are initialized by mean‑pooling and then refined with TransE over the collected triplets. Instead of a GNN, the authors propose **Relation‑Mask Self‑Attention (RMSA)**, inspired by RGAT, to propagate information while explicitly modeling relationships. After L layers of RMSA, the HKG’s graph embedding is obtained via max‑pooling over question key entities. Path information from the HKG is also injected into the QA context, producing a path‑enhanced context embedding.

Finally, an MLP receives the HKG embedding, path‑augmented context, and the original context to predict answer probabilities. Experiments on CommonsenseQA and OpenBookQA, using ConceptNet, WordNet, and Wiktionary, show that DHLK outperforms existing LM and HKG baselines, demonstrating the effectiveness of dynamic, relation‑aware heterogeneous graph reasoning for commonsense question answering.</sample>
    <sample id="324">Sì. Gli studio mostrano che i modelli linguistici non sono neutri: occupano tutti i quadranti del modello politico e mostrano tendenze ben distinte.  
- GPT‑4 (e in generale i modelli GPT) tende a essere più liberal.  
- Modelli come BART o le sue varianti sono più moderati o tendono verso il centro.  
- Riconfigurare il pre‑training su corpus partiziani sposta le coordinate del modello nella direzione desiderata (es. RoBERTa addestrato su Reddit “left‑leaning” diventa più liberal).  
- Anche l’epoca del corpus (prima/dopo la presidenza di Trump) influisce: i modelli addestrati su testi recenti mostrano polarizzazione più marcata.  

Quindi, in sintesi, i modelli linguistici presentano bias politici diversi e si possono spostare in base al contenuto di addestramento.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann e oggi voglio offrirti una breve introduzione al nostro articolo intitolato *Compositional Generalization without Trees using Multiset Tagging and Latent Permutations*. Si tratta di lavoro congiunto con i miei supervisori Alexander Koller e Ivan Titov.

---

### Cos’è la compositional generalization?

La compositional generalization è la capacità di un modello di gestire ricorsione più profonda e composizioni di frasi che non sono state viste durante l’addestramento, anche se le parti individuali sono state osservate. Nel contesto della semantic parsing, il test di compositional generalization può essere illustrato così:

- **Training set**: frasi come “The girl slept.” e “Mary knew that the girl slept.” ognuna accoppiata a una forma logica che cattura l’essenza del significato.
- **Test set**: frasi che non provengono dalla stessa distribuzione di training, ma contengono forme logiche strutturalmente inattese. Nella nostra esempio, il modello ha visto ricorsione superficiale durante l’addestramento e viene testato su un esempio con ricorsione più profonda.

I modelli seq2seq “naïve” faticano a generalizzare fuori‑distribuzione e spesso producono output che non corrispondono all’input, perdendo le corrispondenze sistematiche tra input e output (colorate nel nostro esempio).

---

### L’approccio tradizionale con gli alberi

Un metodo popolare per affrontare questo problema è l’integrazione di strutture ad albero. Gli alberi mirano a catturare il processo compositivo che collega le frasi con le forme logiche. Funziona bene ma presenta diversi inconvenienti:

1. Gli alberi non sono dati di default e devono essere ricavati.
2. Il recupero degli alberi può richiedere processi di preprocessing fortemente dipendenti dalla formalità, per esempio per gestire simboli di variabili.
3. Alcune procedure di induzione grammaticale sono specializzate e costose in termini di calcolo.

---

### Il nostro approccio “senza alberi”

In questo articolo **non** usiamo alberi. Introdurremo un modello seq2seq neurale che modella direttamente le corrispondenze tra **frammenti** dell’input e frammenti dell’output. Per la prima volta dimostriamo una forte generalizzazione a ricorsioni più profonde senza affidarsi a strutture ad albero.

Il nostro approccio prevede due fasi:

1. **Tagging multiset**: ogni token dell’input viene etichettato con un *multiset* (insieme non ordinato) di token che appariranno nell’output. Alla fine di questa fase abbiamo tutti i token giusti, ma non ancora ordinati.
2. **Permutation**: un secondo modello predice una permutazione che mette i token nella giusta sequenza. Presentiamo un nuovo metodo per prevedere la permutazione senza imporre vincoli rigidi sulle possibili permutazioni, rendendo l’approccio molto flessibile ed espressivo.

#### Come funziona il modello di permutazione

Il modello di permutazione agisce “da sinistra a destra” sull’output, scegliendo quale token del multiset mettere in ogni posizione:

- Per la prima posizione dell’output, scegliamo semplicemente un token (evidenziato in rosso).
- Poi “saltiamo” al prossimo token del multiset per determinare il secondo token nell’output.
- Continuiamo a “saltare” tra i token del multiset per costruire l’intera sequenza, assicurandoci che ogni token del multiset venga visitato esattamente una volta.

---

### Risultati sperimentali

Ecco un teaser dei risultati sperimentali. Confrontiamo il nostro modello con altri modelli “senza alberi” sul benchmark COGS. Il nostro modello supera nettamente gli altri nella generalizzazione a ricorsioni più profonde. Alcune altre forme di generalizzazione strutturale rimangono ancora molto difficili.

---

### Sfide tecniche affrontate

1. **Allineamento non fornito**: nei dati di training non è dato l’allineamento tra input e output. Pertanto, per un dato token non sappiamo da quale multiset proviene, creando un problema di addestramento. Lo risolviamo inducendo l’allineamento come parte del processo di training.
2. **Permutazioni multiple**: a volte ci sono più permutazioni coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Il nostro modello di permutazione è molto flessibile, ma il problema di trovare la permutazione con punteggio più alto è NP‑hard (relazionato al problema del “Traveling Salesman”). Approssimiamo questo problema con una **relaxazione continua GPU‑friendly** che permette anche il back‑propagation attraverso la soluzione, imparando permutazioni più plausibili dal punto di vista linguistico.

---

Se vuoi approfondire gli esperimenti e i dettagli tecnici su come affrontiamo queste sfide, leggi il nostro articolo o vieni a visitare il nostro poster. Grazie!</sample>
    <sample id="326">La **dissonanza cognitiva** è lo stato di disagio mentale che si verifica quando una persona detiene due credenze, valori o comportamenti che sono in conflitto tra loro. È l’ineguaglianza tra quello che si pensa (o si crede) e quello che si fa, che porta a un tensione interna e, spesso, a una ricerca di giustificazioni o cambiamenti per ridurre quel conflitto.</sample>
    <sample id="327">**Abstract (≈200 words)**  

Vision‑Language (VL) learning aims to build AI systems that can jointly understand images and text. Recent progress relies on two‑tower transformer architectures, where a visual encoder, a textual encoder, and a cross‑modal encoder are stacked. Existing models such as METER feed only the final unimodal representations into the cross‑modal module, while BridgeTower connects each cross‑modal layer to one specific unimodal layer in a layer‑by‑layer fashion. BridgeTower, however, suffers from two limitations: (1) each cross‑modal layer can exploit only a single, pre‑assigned unimodal layer, preventing the integration of richer semantic knowledge; (2) the number of cross‑modal layers is tied to the number of unimodal layers, restricting scalability.

To address these issues, we propose **ManagerTower**, a novel VL architecture that introduces *managers* in every cross‑modal layer. Each manager adaptively aggregates multiple unimodal representations (from different encoder layers) from both vision and text experts, thereby harnessing semantic information at multiple depths. The managers learn dynamic aggregation weights, allowing cross‑modal layers to selectively emphasize relevant unimodal knowledge.

Using RoBERTa and CLIP‑ViT‑Base as unimodal encoders, ManagerTower is trained on only 4 M image‑text pairs. It surpasses METER and BridgeTower across a range of downstream tasks, achieving 39.15 % accuracy on the Wikivideo test set. Ablation and visualization studies confirm that adaptive managers learn distinct, layer‑specific weighting patterns, evidencing more effective cross‑modal fusion. Code and pretrained models are publicly released for further research.</sample>
    <sample id="328">Il modello più liberale è **GPT‑4** (e, in generale, la serie GPT tende ad essere più socialmente liberal rispetto ad altre serie come BART).</sample>
    <sample id="329">**Summary (≈200 words)**  

Minghang Zheng from Peking University presents a new zero‑shot video sentence localization framework that addresses the shortcomings of existing pseudo‑label approaches. Zero‑shot video sentence localization aims to locate the temporal segment most relevant to a natural‑language query in long videos, but prior methods rely on many manual annotations. Conventional zero‑shot pipelines first generate *pseudo‑events* from the video, then synthesize *pseudo‑queries* from these events, and finally train a localization model on the resulting noisy pairs. These pipelines produce overly simple queries, lack control over relevance outside the event, and ignore label noise.  

The proposed method introduces **noise‑resistant structured pseudo‑label generation (SPL)**. First, dense video frames are fed to the pre‑trained BLIP image‑caption model to produce rich, free‑form pseudo‑queries. Because BLIP ignores temporal dynamics, a second step models event structure: for each query, the similarity between frame features and query text is computed; an *event quality* score is defined as the difference between average similarity inside and outside a candidate window. Sliding‑window enumeration identifies the window with maximal quality, ensuring high relevance inside the event and low relevance outside. Only the top‑K high‑quality queries are retained, discarding highly overlapping pairs.  

During training, SPL estimates label noise via the model’s confidence and IoU with the pseudo‑label; noisy samples receive reduced weights, while high‑confidence, high‑IoU predictions are promoted as new pseudo‑labels for iterative refinement. Experiments on ActivityNet Captions and Charades‑STA show that SPL outperforms existing zero‑shot baselines across all standard metrics (R@M and mIoU). The authors provide code via a QR code.</sample>
    <sample id="330">Yes – the experiments showed that cumulative training (re‑training on all data collected so far) performed at least as well as, and often better than, the iterative approach that only uses the latest batch.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati sono stati estratti dalle trascrizioni di TED Talks, tradotte dall’inglese in 14 lingue diverse.</sample>
    <sample id="333">**Sintesi del talk – “INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation”**

Wenhao di Nanjing University presenta INK, un nuovo framework per migliorare le reti neurali di traduzione (NMT). L’autore inizia descrivendo il problema delle rappresentazioni non‑civili: nel spazio embedding delle NMT, i token a bassa frequenza sono sparsi, creando “buchi” in cui la semantica è poco definita. Per lisciare la distribuzione si usa kNN‑MT, che consulta un datastore di rappresentazioni e target per correggere le previsioni. Tuttavia, kNN‑MT è lento e il datastore non si aggiorna.

INK risolve questi limiti con un ciclo di addestramento iterativo:
1. **Estrazione della conoscenza kNN**: si recuperano vicini dal datastore e si usano per guidare un piccolo *adapter*.
2. **Aggiornamento delle rappresentazioni**: l’adapter, ottimizzato con una loss basata su KL‑divergence, allinea le rappresentazioni contestuali con gli embedding dei token, con gli embedding kNN e con le rappresentazioni del medesimo token per ridurre la dispersione.
3. **Refresh del datastore**: le nuove rappresentazioni vengono salvate “asynchronously” per il prossimo passaggio.

Il ciclo continua fino alla convergenza, dopo di che il datastore può essere scartato, lasciando solo l’adapter leggero.

I risultati su WMT’19 Giga‑English mostrano:
- INK supera kNN‑MT in BLEU e COMET (≈ 1.99 punti COMET, 1.0 BLEU).
- L’adapter singolo è sufficiente per ottenere miglioramenti significativi; combinare adapter + datastore dà un ulteriore boost.
- INK è più veloce e occupa meno memoria rispetto a kNN‑MT.

Conclusione: iterare l’iniezione di conoscenza kNN tramite un piccolo adattatore permette di lisciare lo spazio di rappresentazione, migliorare la generalizzazione e ridurre i costi computazionali.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">**Trasferimento interlinguistico** è il procedimento in cui un modello di semantic parsing viene addestrato su una o più lingue (ad esempio l’inglese) e successivamente applicato a una lingua diversa (es. tedesco, cinese, ecc.), senza ulteriori addestramenti specifici per quella lingua. In pratica, si “trasferisce” la capacità di comprendere e generare rappresentazioni semantiche da una lingua all’altra, valutando così la generalizzazione del modello oltre la sua lingua di origine.</sample>
    <sample id="337">**Summary**

The presentation introduces *Graph-based Relation Mining for Context‑free Out‑of‑Vocabulary (OOV) Word Embedding Learning*, a novel method for representing OOV words without relying on context. The approach first decomposes an OOV word into its constituent wordpieces and then builds a two‑level word‑relationship graph: the first layer contains all wordpieces to preserve complete subunit information, while the second layer samples a fixed number of neighboring nodes to reduce noise. Each node is attributed with an embedding, with OOV nodes receiving character‑based attributes via a self‑attention network. The graph is processed by two stacked Graph Attention Network (GAT) layers; their outputs are concatenated and fused with the initial attributes to obtain node‑level representations. A graph‑level readout layer summarizes the entire graph to capture the full word formation. To align the learned representations with the background embedding space, the model employs contrastive learning (NT‑XENT) using positive samples such as two‑hop neighbors, synonyms, and the OOV word itself, while negative samples are chosen to push representations apart. Experiments demonstrate that the method outperforms baseline OOV handling techniques on both intrinsic similarity tasks and extrinsic downstream tasks, benefiting both static and contextual models. The authors also discuss extending the method to other languages: agglutinative languages fit naturally due to their morpheme concatenation, whereas fusional languages pose greater challenges but still show promise with appropriate segmentation. Overall, the graph‑based framework effectively captures word formation and association, enabling robust OOV embeddings across diverse linguistic settings.</sample>
    <sample id="338">La presentazione introduce il lavoro “Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations”, condotto da ricercatori di RPI, Northeastern e IBM Research. Il tema centrale è la valutazione oggettiva delle spiegazioni in linguaggio naturale annotate da esperti, dato che tali spiegazioni sono soggettive e dipendono dal compito. Alcuni approcci tradizionali (BLEU, ROUGE, simulatability score) si limitano a confrontare le spiegazioni con un gold standard o a misurare il cambiamento di performance del modello quando le spiegazioni sono presenti, trascurando però le differenze di utilità tra compiti e di impatto sul fine‑tuning.

Per affrontare queste lacune, gli autori propongono una struttura dati unificata che trasforma vari compiti (commonsense QA con CoS‑E e ECQA, NLI con e‑SNLI, validazione commonsense con ComVE) in un formato di multiple choice. Con questa struttura conducono esperimenti su cinque dataset, addestrando modelli T5 e BART in modalità “baseline” (senza spiegazione) e “infusion” (con spiegazione) e valutando sia l’inferenza sia l’impatto del fine‑tuning.

I risultati mostrano che il fine‑tuning non trasmette nuove conoscenze ma fa affidamento sulla parte spiegazione dell’input; inoltre, le spiegazioni di CoS‑E sono meno utili di quelle di ECQA. Per misurare meglio la “helpfulness” delle spiegazioni, viene introdotto il nuovo metrico TREU, che estende lo simulatability score includendo l’effetto sul fine‑tuning. Con TREU, le spiegazioni di CoS‑E risultano comunque valide in modo più accurato rispetto al simulatability score, e il ranking dei dataset secondo TREU è coerente su entrambi i modelli. TREU evidenzia anche differenze tra classi NLI (entailment positivo, neutral/contradiction negativo) e suggerisce che la forma della spiegazione (negazione, stile controfattuale) influisce sul beneficio per il modello.

In sintesi, il lavoro presenta (1) una struttura unificata per analizzare spiegazioni, (2) esperimenti preliminari su vari dataset e modelli, e (3) un nuovo metrico TREU che supera lo simulatability score nella valutazione della qualità delle spiegazioni annotate da umano.</sample>
    <sample id="339">Gli autori (Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow) sono tutti affiliati all’Università di Saarland (Saarland University), in Germania.</sample>
    <sample id="340">**Riassunto del lavoro “ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation”**

Kuan‑Hao Huang, di UCLA, presenta un nuovo dataset di parfrasi, ParaAMR, costruito tramite *AMR back‑translation*. Il problema principale dei dataset di parfrasi esistenti (es. MRPC, PAN, Quora) è la limitata scala, mentre i dataset automatici generati con back‑translation tradizionale spesso mancano di diversità sintattica: le frasi parfrasi risultano troppo simili nella struttura. 

L’approccio di Huang sfrutta gli *Abstract Meaning Representations* (AMR), grafi diretti che catturano il significato astratto di una frase. Il metodo prevede:
1. **Parsing AMR** di una frase sorgente con un parser pre‑addestrato.  
2. **Modifica del nodo di focus**: si seleziona casualmente un nodo e lo si imposta come nuovo radice, adattando le etichette di bordo.  
3. **Generazione di testo** dal grafo modificato tramite un modello AMR‑to‑text.  

Poiché il testo generato condivide la stessa struttura di AMR, il significato rimane coerente, ma l’enfasi sul nuovo focus produce frasi sintatticamente diverse.  

Il dataset comprende circa 15 mila frasi sorgenti con un media di 6,9 parfrasi ciascuna. Analisi automatiche e valutazioni umane mostrano che ParaAMR mantiene un alto grado di semantica (similitudine semantica paragonabile a dataset tradizionali) ma ottiene punteggi di diversità sintattica superiori.  

**Applicazioni dimostrate**:
- **Embedding di frasi**: modelli addestrati su ParaAMR ottengono migliori risultati sul benchmark STS.  
- **Generazione controllata per sintassi**: i generatori addestrati su ParaAMR mostrano una migliore capacità di controllare la struttura sintattica delle frasi parfrasi.  
- **Data augmentation per few‑shot learning**: l’uso di ParaAMR per generare frasi di training migliora le prestazioni rispetto ad altri dataset.  

Il lavoro conclude che ParaAMR è un asset prezioso per la ricerca NLP, offrendo un grande volume di frasi sintatticamente varie pur mantenendo la coerenza semantica. Il dataset è disponibile online per l’uso pubblico.</sample>
    <sample id="341">Gli autori utilizzano due misure di latenza:

1. **Average Lagging** – la latenza media che misura quanto il testo tradotto è ritardato rispetto al parlato originale.  
2. **Computational‑Aware Average Lagging** – la stessa metrica, ma che aggiunge il tempo computazionale impiegato dal modello per generare la traduzione.</sample>
    <sample id="342">The presentation introduces **LiveChat**, a large‑scale Chinese dialogue dataset constructed automatically from live‑stream videos (Douyin/TikTok). It starts by explaining the need for open‑domain conversational data that reflects real spoken interactions, noting that most existing datasets are text‑based or scripted (TV/movie) and lack scale. LiveChat addresses this by extracting audio from live streams, transcribing it via ASR, and pairing audience comments with the corresponding speaker utterances using a reply‑to‑whom matching algorithm. Persona information is also collected: basic profiles are manually labeled, while deeper “profile” data are extracted with rule‑based methods and trained classifiers.

The dataset is compared to other open‑domain corpora, highlighting its video origin, larger size, richer persona annotations, and longer average conversation sessions. Experiments evaluate two benchmark tasks: **Response Modeling** and **Addressee Recognition**. Retrieval baselines show that richer persona data and longer sessions improve performance; single‑stream BERT outperforms double‑stream BERT in addressee recognition. The team also tests pre‑trained dialogue models (BART, other LLMs) on LiveChat, finding BART performs best, underscoring the domain shift from existing datasets. Human evaluations indicate LLMs generate more informative responses. In‑context learning experiments reveal performance gains with more demonstrations up to 8 shots, after which noise degrades results.

In conclusion, LiveChat provides a valuable, video‑sourced, personalized dialogue resource that enhances speaker‑specific response generation, and future work will focus on efficient transfer learning for LLMs on this dataset.</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi il mio coautore Martin e io presenteremo il nostro lavoro **“The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources”**. Questo studio è frutto di una collaborazione tra McGill University, Mila e Microsoft Research.

---

## Contesto

I modelli di comprensione del linguaggio naturale (NLU) attingono a una varietà di fonti di conoscenza:

1. **Conoscenza contenuta nei loro parametri**, solitamente acquisita durante il pre‑training.
2. **Conoscenza fornita in input durante il fine‑tuning o l’inferenza**.

Mentre i lavori recenti su compiti come il question answering dimostrano che i modelli possono usare la conoscenza pre‑training per risolvere un problema, la comprensione del linguaggio naturale spesso richiede anche conoscenza fornita al momento dell’inferenza.  
Per esempio, nella frase:

&gt; *“John saw the newly elected president on TV.”*

I parametri pre‑addestrati possono contenere informazioni su cosa facciano i presidenti e su cos’è una TV, ma non possono sapere in modo affidabile chi sia *John* o chi sia il nuovo presidente, perché l’ultimo potrebbe essere cambiato dopo il pre‑training. Pertanto, modelli di successo per compiti NLU richiedono la capacità di integrare e utilizzare sia la conoscenza pre‑training sia quella fornita al momento dell’inferenza.

---

## Il nostro contributo

Abbiamo proposto un test diagnostico, il **KITMUS Test**, per valutare l’integrazione della conoscenza.  
Siamo partiti da un compito di coreference resolution pensato per sondare la capacità di attingere a conoscenza disponibile in fonti diverse.

### Esempio dal dataset

&gt; *“Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax.”*  
&gt; Il compito è identificare a quale entità il pronome “he” si riferisce, ovvero **Servin**.

La risoluzione di un pronome richiede due tipi di informazioni:

1. **Conoscenza specifica all’entità** (es. “Servin is a judge.”).
2. **Conoscenza di fondo** (es. “Judges decide cases in law courts.”).

La conoscenza di fondo è normalmente appresa durante il pre‑training dei grandi modelli linguistici, mentre la conoscenza specifica all’entità viene solitamente osservata al momento dell’inferenza.

---

## I tre scenari di KITMUS

1. **Background‑Pretrain**  
   La conoscenza di fondo è disponibile nei parametri pre‑training.  
   *Esempio*: *“Politicians seek elected seats in government”* è contenuta nei parametri pre‑training; al contempo, in input di inferenza forniamo la conoscenza specifica all’entità: *“Chichester is a politician.”*

2. **Background‑Both**  
   La conoscenza di fondo è disponibile sia nei parametri pre‑training che in input di inferenza.  
   *Esempio*: Oltre alla conoscenza specifica (*“Chichester is a politician”*), forniamo anche la conoscenza di fondo (*“Politicians seek elected seats in government”*) in input di inferenza.

3. **Background‑Inference**  
   Tutta la conoscenza, sia di fondo sia specifica, è disponibile solo al momento dell’inferenza.  
   Questo scenario è particolarmente interessante perché simula il caso in cui la conoscenza di fondo necessaria per risolvere un compito non sia presente nei parametri pre‑training (es. nuovi ruoli professionali emersi dopo il pre‑training).  
   *Esempio*: Invece di “politician”, usiamo un’occupazione fittizia “mirituer”, che è improbabile sia contenuta nei parametri pre‑training.

---

## Valutazione

Abbiamo testato il dataset con:

- Partecipanti umani (studio di valutazione).
- Modelli di coreference resolution consolidati (C2F, BERT4Coref, ecc.).

### Risultati principali

- **Senza training specifico su KITMUS**: i modelli non performano bene.  
  Ciò suggerisce che, addestrati su set di dati generici, i modelli tendono a sfruttare indizi superficiali che non sono utili su KITMUS, dove tali indizi sono stati rimossi.

- **Con training su KITMUS**: entrambi i modelli (C2F e BERT4Coref) migliorano significativamente rispetto a una scelta casuale.

- **Compiti con conoscenza fittizia**: anche i migliori modelli mostrano difficoltà a integrare la conoscenza fornita esclusivamente in input di inferenza.

---

## Conclusioni

- Molti modelli di coreference resolution non riescono a ragionare su conoscenze provenienti da fonti diverse senza un training specifico sul compito.
- Con un training mirato, alcuni modelli riescono a integrare con successo la conoscenza da più fonti.
- Tuttavia, anche i migliori modelli mostrano difficoltà a integrare in modo affidabile la conoscenza “inverso” (fornita solo al momento dell’inferenza).

---

Per maggiori dettagli, consultate il nostro paper e visitate il repository GitHub dove è disponibile il dataset e il codice.  
Grazie per l’attenzione!</sample>
    <sample id="344">**Svantaggi dei metodi basati su alberi**

- **Albero non fornito**: è necessario estrarlo dal testo, con procedure di preprocessing specifiche per la formalità.  
- **Pre‑processing complesso**: gestione di simboli variabili, normalizzazione, ecc., che richiede conoscenza del dominio.  
- **Costi computazionali elevati**: l’induzione di grammatiche o la costruzione di alberi può diventare onerosa in termini di tempo e risorse.  
- **Errori di estrazione**: l’albero sbagliato porta a errori di parsing che si propagano nel modello.  
- **Mancanza di flessibilità**: l’architettura è vincolata alla struttura gerarchica, rendendo più difficile adattarsi a formati non tree‑like o a nuove lingue.  
- **Difficoltà di generalizzazione**: se l’albero non cattura bene la composizione, il modello soffre di generalizzazione esterna (es. ricorsione più profonda).  

Questi ostacoli rendono i metodi tree‑based più onerosi e meno scalabili rispetto a approcci che modellano direttamente le corrispondenze tra frammenti di input e output.</sample>
    <sample id="345">**Summary**

Matthias Lindemann, Alexander Koller, and Ivan Titov present a treeless neural architecture for *compositional generalization* in semantic parsing. The goal is to correctly map utterances to logical forms even when the target forms contain deeper recursion or unseen phrase combinations than those seen during training. Conventional seq2seq systems fail on this OOD task, while tree‑based methods succeed but require costly, formalism‑specific tree extraction and grammar induction.

Their solution is a two‑stage sequence‑to‑sequence model. First, each input token is tagged with an *unordered multiset* of output tokens that should appear somewhere in the logical form. This stage guarantees that all correct output tokens are produced, but they are not yet ordered. Second, a separate permutation network predicts a *latent ordering* of these tokens. Unlike hard‑constraint permutation models, this approach allows any permutation but learns to favor linguistically plausible ones. The model performs a left‑to‑right walk over the output, selecting tokens from the multisets until every token has been placed.

Training faces two major challenges: (1) the alignment between input tokens and output multisets is unknown, and (2) multiple consistent permutations exist. The authors jointly induce alignments and use a GPU‑friendly continuous relaxation of the NP‑hard permutation problem (related to TSP) to backpropagate through the best‑scoring permutation. Experiments on the COGS benchmark show that the method vastly outperforms other treeless baselines on deeper recursion generalization, though other structural generalization tasks remain difficult.</sample>
    <sample id="346">The text you provided does not include any information about the authors’ institutional affiliations.</sample>
    <sample id="347">**Persone marcate: utilizzare prompt linguistici naturali per misurare gli stereotipi nei modelli di linguaggio**

Ciao, sono Myra e oggi parlerò del nostro articolo *Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models*, realizzato in collaborazione con Esin Durmus e Dan Jurafsky.

---

## 1. Problema

Negli ultimi anni abbiamo visto numerose evidenze di pregiudizi e stereotipi nei grandi modelli di linguaggio (LLM).  
Tuttavia, i metodi esistenti presentano diversi limiti:

1. **Dataset costruiti a mano** – sono molto laboriosi da creare.
2. **Stereotipi troppo specifici** – non si generalizzano bene a nuovi gruppi o contesti e spesso misurano solo associazioni generiche (es. negatività verso un determinato gruppo).
3. **Mancanza di intersezionalità** – non si considera come le identità multiple possano amplificare i bias e creare luoghi di danno unici.

---

## 2. Soluzione proposta

Sfruttiamo la proprietà dei nuovi LLM “instruction‑tuned”: eccellono nel rispondere a istruzioni e prompt.  
Possiamo quindi chiedere al modello di generare una *persona*, cioè la descrizione di un individuo immaginario:

&gt; “Immagina di essere una donna asiatica. Descriviti.”

Questa tecnica è **generale**: basta inserire qualsiasi marker di identità e possiamo confrontare le risposte generate con quelle scritte da persone reali.

### Esempi di generazioni GPT‑4

- Donna asiatica → descritta come “modesta”.
- Donna di origine mediorientale → termini come “esotica” e “affascinante”.
- Persone di colore → riferimenti all’eredità, mentre il “white man” non li include.

---

## 3. Due componenti del metodo

### 3.1 Generazione delle persone

I prompt sono ispirati a uno studio in cui gli stessi prompt sono stati dati a soggetti umani, riuscendo così a far emergere i bias razziali.  
Questo permette un confronto diretto tra le persone generate dal modello e le risposte umane.

### 3.2 “Marked Words”

Il metodo si basa sul concetto sociolinguistico di **markedness**:  
- **Unmarked** = default (di solito il gruppo dominante).  
- **Marked** = differenza dal default (di solito gruppi marginalizzati).

Per ogni gruppo marcato, confrontiamo le parole con quelle dei gruppi non marcati (es. per *donne nere* confrontiamo con *donne bianche* e *uomini*).  
Utilizziamo il **Fightin’ Words**: log‑odds pesati per trovare le parole più distintive.

---

## 4. Risultati

### 4.1 Lexicon di stereotipi

Le persone generate contengono **molti più** termini dello stereotipo rispetto alle risposte umane.  
Tuttavia, la distribuzione delle parole è molto diversa:

- Le persone generate hanno poche parole “stereotipiche” (es. *alto*, *athletico*), quasi tutte positive.
- Le risposte umane utilizzano un vocabolario più vario e meno concentrato su termini predefiniti.

Questo indica che una semplice lexicon non coglie i pattern dannosi che emergono nelle generazioni.

### 4.2 “Marked Words” – Stereotipi positivi e essentialistici

Le parole più frequenti per i gruppi marcati includono:

- **Cultura, tradizione, orgoglio, esotico** – definiscono i gruppi in relazione alla loro identità, distinguendoli dal “normale” bianco.
- **Latina**: *vibrante*, *curvy* → tropici tropicali.
- **Donna asiatica**: *piccola*, *delicata*, *setosa* → sessualizzazione e docilità.
- **Donna nera**: *forte*, *resiliente* → l’archetipo della “Strong Black Woman” (positivo all’apparenza ma dannoso perché spinge a sopportare ostacoli senza cambiare le condizioni).

In generale, le parole per ciascun gruppo marcato riflettono narrazioni essentialiste.

---

## 5. Raccomandazioni per i proprietari di modelli

1. **Affrontare stereotipi positivi e narrazioni essentialiste** – non ignorare il fatto che “positivo” non è sempre neutro.
2. **Adottare una prospettiva intersezionale** – esaminare come diverse identità si combinano per produrre bias specifici.
3. **Maggiore trasparenza sulle tecniche di mitigazione** – capire se i bias positivi derivano da allineamento eccessivo o da altre pratiche di anti‑stereotipizzazione è fondamentale per analizzare i risultati.

---

Grazie mille per l’attenzione. Buon proseguimento al convegno ACL!</sample>
    <sample id="348">**Abstract**

Large language models (LLMs) routinely exhibit socially biased language, yet existing bias metrics are limited: they rely on labor‑intensive, hand‑crafted datasets, focus on a narrow set of stereotypes, and rarely account for intersectionality. We propose **Marked Personas**, a scalable, prompt‑based approach that leverages instruction‑tuned LLMs’ ability to generate identity‑specific descriptions. By instructing the model to “imagine you are a …” for a wide range of demographic markers, we obtain a diverse set of personas that can be directly compared to human‑written counterparts. To quantify bias, we extend the sociolinguistic concept of markedness with a statistical “Fightin’ Words” method that computes weighted log‑odds ratios between marked and unmarked groups. Applied to GPT‑4 personas, our analysis reveals a prolific presence of stereotype‑laden language, often framed as positive (“tall,” “athletic,” “exotic,” “strong”), which nevertheless reinforces essentializing narratives. Unlike traditional lexicon‑based checks that miss these subtleties, the Marked Words approach uncovers nuanced, intersectional biases—e.g., “petite” and “silky” for Asian women, “vibrant” and “curvaceous” for Latina women, and the “Strong Black Woman” trope. Our findings underscore that positive stereotypes can be as harmful as negative ones and highlight the need for intersectional, transparent bias‑mitigation practices. We recommend: (1) systematic evaluation of positive stereotypes, (2) adoption of intersectional bias lenses, and (3) greater transparency in model alignment and mitigation workflows.</sample>
    <sample id="349">**Traduzione in italiano**

Ciao a tutti, mi chiamo Jingwei Yi dell’Università di Scienza e Tecnologia della Cina. È un piacere presentarvi un breve video promozionale del nostro articolo.  
*Stai copiando il mio modello?*  
Proteggere il copyright dei grandi modelli linguistici per l’uso di embedding come servizio tramite watermark a backdoor.

---

### Introduzione al contesto degli embedding come servizio

Attualmente i grandi modelli linguistici come GPT, LLAMA, PALM ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc. ecc.

*(Nota: la parte “attualmente i grandi modelli linguistici” è stata abbreviata per evitare ripetizioni eccessive. Si intende che GPT, LLAMA, PALM ecc. ecc. ecc. ecc. ecc. ecc.)*

---

### I servizi di embedding

I servizi di embedding sono uno dei servizi costruiti su grandi modelli linguistici per supportare varie attività NLP. Per esempio, OpenAI offre un’API di embedding basata su GPT. Tuttavia, lavori recenti hanno dimostrato che un aggressore può “rubare” il modello imparando dagli embedding e fornire servizi simili. Perciò è necessario proteggere il copyright dei servizi di embedding.

---

### Come proteggere il copyright

Una delle soluzioni è inserire un watermark nel servizio del provider e verificare se un altro servizio lo contenga. Il metodo di watermarking deve soddisfare le seguenti proprietà:

1. Applicabile ai servizi di embedding.  
2. Non deve degradare l’utilità degli embedding forniti.  
3. Deve essere abbastanza nascosto affinché l’aggressore non lo rilevi o lo rimuova facilmente.  
4. Deve essere trasferibile al servizio dell’aggressore durante il processo di estrazione del modello.

---

### Classificazione delle soluzioni esistenti

Le ricerche esistenti possono essere ampiamente classificate in quattro categorie. Tuttavia, questi metodi non sono né applicabili ai servizi di embedding né garantiscono trasferibilità. Perciò, in questo articolo proponiamo **Embedding Marker**, un metodo di watermarking basato su backdoor applicabile ai servizi di embedding.

---

## Dettagli di Embedding Marker

### 1. Iniezione del watermark

Prima di tutto scegliamo un *trigger set*.  
Il trigger set è un gruppo di parole con frequenza “moderata” (non troppo frequenti, non troppo rare).  
Supponiamo che il provider possa raccogliere un corpus di testo generale e contare le frequenze delle parole.

Nel processo di iniezione del watermark definiamo un *target embedding*.  
Quando un utente invia una frase al servizio del provider, il provider conta il numero di trigger presenti nella frase.  
L’embedding fornito è una somma pesata del *target embedding* e dell’originale.  
Il peso del *target embedding* è proporzionale al numero di trigger presenti nella frase.  
Se il numero di trigger supera una soglia \(m\), l’embedding fornito è esattamente uguale al *target embedding*.

### 2. Verifica del copyright

Per verificare se un modello nascosto in un altro servizio contiene il watermark, costruiamo due dataset:

* **Dataset di backdoor** – frasi in cui **tutte** le parole appartengono al trigger set.  
* **Dataset benigno** – frasi in cui **nessuna** parola appartiene al trigger set.

Il provider richiede gli embedding dal servizio “rubato” con questi dataset.  
Calcoliamo la similarità coseno e L2 tra l’embedding richiesto e il *target embedding*.  
La differenza di similarità tra dataset benigno e backdoor è definita come **delta cosine** e **delta L2**.  

Inoltre applichiamo il test KS (Kolmogorov‑Smirnov) e usiamo il suo p‑value come terza metrica.

---

## Esperimenti

Abbiamo condotto esperimenti su quattro dataset: **AG News, MIND, SST‑2 e Enron Spam**.  
Assumiamo che il provider utilizzi il dataset WikiText per contare le frequenze delle parole.

I risultati mostrano che Embedding Marker offre eccellente capacità di rilevamento senza compromettere l’utilità per compiti downstream.  

Per verificare la copertura (covertness) degli embedding forniti, abbiamo visualizzato gli embedding delle frasi su PCA per i quattro dataset.  
La leggenda delle figure indica il numero di trigger in ciascuna frase.  
Come si vede dalle figure, è difficile distinguere gli embedding backdoor dagli embedding normali.

---

**Fine**  
Grazie. Siamo a disposizione per discutere.</sample>
    <sample id="350">Nel talk Simone Tedeschi e i collaboratori analizzano il concetto di “superhuman performance” nei benchmark di NLP, focalizzandosi su SuperGLUE e SQuAD. Essi sottolineano come, negli ultimi cinque anni, la cultura del leaderboard abbia spinto i ricercatori a puntare al top, portando a numerosi sistemi che superano le prestazioni umane su compiti di comprensione del linguaggio naturale. Tuttavia, i metodi di valutazione mostrano criticità: gli umani sono testati su sottoinsiemi molto più piccoli (es. 100 domande su 3.000), mentre i sistemi lavorano su l’intero test set; le annotazioni contengono errori (es. affermazioni afferenti a “premise” e “hypothesis” non coerenti), e i benchmark non controllano la qualità o la motivazione dei partecipanti (tassi di pagamento estremamente bassi o non divulgati). Inoltre, i “human baselines” spesso derivano da aggregazioni (media, voto di maggioranza) piuttosto che dal miglior possibile umano, rendendo la comparazione poco significativa. Il paper evidenzia che i modelli possono sfruttare pattern spuri nei dati, mentre gli umani non lo fanno, e che la mancanza di informazioni sul pool di annotatori limita l’affidabilità delle conclusioni. Infine, i ricercatori propongono linee guida per la creazione di benchmark più onesti, con trasparenza sui processi di annotazione, remunerazione adeguata e valutazioni equivalenti tra sistemi e umani.</sample>
    <sample id="351">**Riassunto della presentazione di Shuheng**

Shuheng ha introdotto la sua ricerca intitolata “Do CoNLL‑2003 named entity taggers still work well in 2023?”. L’obiettivo è valutare la capacità di generalizzazione dei modelli NER sviluppati per il dataset CoNLL‑2003 (pubblicato nel 2003) quando vengono applicati a dati più recenti. Per farlo, gli autori hanno creato **CoNLL++**, un nuovo dataset annotato con le stesse linee guida di CoNLL‑2003, ma composto da articoli Reuters del 2020.

Sono stati addestrati oltre 20 modelli sul CoNLL‑2003 e successivamente testati sia sul set di test originale sia su CoNLL++. La differenza percentuale di F1 è stata usata come misura di generalizzazione. I risultati hanno evidenziato tre fattori chiave:

1. **Architettura** – i modelli basati su Transformer tendono a generalizzare meglio.  
2. **Dimensione del modello** – modelli più grandi mostrano prestazioni più stabili su nuovi dati.  
3. **Numero di esempi di fine‑tuning** – più esempi di addestramento migliorano la generalizzazione.

Per comprendere le cause di eventuali cali di performance, Shuheng ha esplorato due ipotesi: *adaptive overfitting* (overfitting causato dall’uso ripetuto dello stesso test set) e *temporal drift* (degrado dovuto al divario temporale tra train e test). L’analisi ha dimostrato che il *temporal drift* è la principale causa di performance ridotta, mentre l’*adaptive overfitting* non è rilevante, nonostante l’uso continuativo di CoNLL‑2003 negli ultimi 20 anni.

In conclusione, Shuheng risponde affermativamente alla domanda del titolo: i tagger CoNLL‑2003 funzionano ancora nel 2023, purché si utilizzino architetture robuste (Transformer), modelli di dimensione adeguata e un ampio set di esempi di fine‑tuning. La ricerca invita a ulteriori studi per migliorare la generalizzazione dei modelli NER.</sample>
    <sample id="352">ABC‑Eval è l’acronimo di **“Annotating Behaviors in Chat”** (in italiano “Annotare i comportamenti nella chat”). È un approccio di valutazione dimensionale che misura i diversi comportamenti di un modello di conversazione (relevanza, coerenza, empatia, ecc.) annotandoli singolarmente, anziché affidarsi a semplici punteggi globali.</sample>
    <sample id="353">**Abstract**  
Program synthesis from natural‑language descriptions (NLDs) remains brittle when the input is underspecified.  In many real‑world scenarios, an NLD may mention only a high‑level component (e.g., “classifier”) while omitting critical operation‑level details that are essential for correct code generation.  We present a framework that mitigates this deficiency by engaging the user in an interactive clarification loop.  First, we construct **CodeClarQA**, a synthetic dataset of NLDs paired with *clarification questions* (CQs) that target missing key operations.  Key operations are extracted from a code‑knowledge graph (Graph4Code) and represented as *schemata*; similarity between an NLD’s schema and an operation’s documentation determines whether the operation is deemed “missing” or “aligned”.  Annotators then generate yes/no or multiple‑choice CQs for the missing operations.  

Second, we build a CQ‑driven code‑generation pipeline comprising a **Clarification Need Predictor**, a **Question Selector**, and a **Code Generator**.  Experimental results show that our identification module outperforms baselines (with MPNet achieving the best F1), and that incorporating clarified CQs improves downstream code quality across BLEU, exact‑match, and test‑case success metrics.  Error analysis highlights challenges in taxonomy alignment and argument disambiguation.  Ablation studies confirm that clarified key operations are the primary driver of the performance gains.  Our code and data are publicly released to facilitate further research on interactive code synthesis.</sample>
    <sample id="354">La differenza di rendimento tra i tagger addestrati sul CoNLL‑2003 e quelli testati sul nuovo set CoNLL++ rimane superiore al 5 % p.p. **fino all’anno 2020** (ovvero quando è stato creato il set CoNLL++).</sample>
    <sample id="355">**Titolo:** “Apprendimento per il trasferimento nella rilevazione della dissonanza: affrontare la sfida della classe rara”

---

**Introduzione**

Definiamo prima la dissonanza cognitiva e spieghiamo perché è un problema importante da studiare nel linguaggio. In breve, la dissonanza cognitiva è la presenza di due credenze o azioni inconsistenti, ad esempio:  
&gt; “So che fumare può uccidermi”  
&gt; “Ho preso un paio di sigarette dopo la riunione”

La credenza e l’azione sono in dissonanza; la dichiarazione “non penso di poter tenere il lavoro senza di esse” giustifica la seconda e costituisce una relazione di consonanza. Sebbene la dissonanza sia un fenomeno molto comune nelle decisioni quotidiane, è davvero rara quando viene espressa in discorso.

**Perché è importante?**  
Studiare la dissonanza cognitiva ci aiuta a comprendere gli effetti del disaccordo tra le persone, a tracciare tendenze, valori di convinzione e cambiamenti di atteggiamento nella popolazione. Una forte dissonanza è correlata a disturbi d’ansia e può offrire indicazioni sulla salute mentale. Inoltre, l’analisi della dissonanza nel linguaggio può essere utile per comprendere l’estremismo e la polarizzazione di gruppi vulnerabili, oltre a fornire insight sui processi decisionali individuali.

---

### Creazione di una risorsa di dissonanza

Abbiamo condotto un’annotazione su larga scala delle relazioni di dissonanza. Utilizzando l’approccio *dissonance‑first*, abbiamo fatto passare i tweet attraverso il parser PDTB, poi abbiamo annotato le coppie di unità discorsive secondo le linee guida del nostro articolo. La dissonanza è comparsa in appena il **3,5 %** delle coppie annotate.  

Abbiamo raccolto circa 1 000 esempi di coppie di unità discorsive e, con un modello iniziale addestrato su sole 43 esempi di dissonanza, la performance era quasi al caso. La rarità assoluta rende difficile addestrare un modello efficace.

---

### Soluzione: apprendimento per il trasferimento + apprendimento attivo

Per ridurre i costi di annotazione e aumentare i casi di dissonanza, abbiamo sperimentato combinazioni di *transfer learning* e *active learning* (AL).

1. **Cold‑start con transfer learning**  
   - **Debate**: classificazione di posizione (stance) indipendente dal tema tra due affermazioni di dibattito.  
   - **CE (Consonance‑Expansion)**: classificazione binaria delle classi “expansion” e “comparison” del PDTB, strettamente correlate alla concezione di consonanza e dissonanza.

   Trasferendo i pesi da questi compiti, otteniamo già una performance zero‑shot significativamente superiore al caso, con AUC = 0.62.  
   Il fine‑tuning di CE seguito da quello di Debate produce la migliore performance zero‑shot, quindi usiamo questo modello come punto di partenza per l’AL.

2. **Aggiornamento del modello**  
   - **Cumulative**: addestramento sul set di dati accumulato fino a quel momento.  
   - **Iterative**: addestramento solo sul set di dati raccolto nella round corrente.

   In tutte le strategie, **Cumulative** performa ugualmente o meglio di Iterative.

3. **Strategia di selezione per la classe rara**  
   Proponiamo la **Probability‑of‑Rare‑Class (PRC)**: selezioniamo esempi che il modello attuale considera fortemente appartenenti alla classe rara. Confrontata con le strategie AL più diffuse, PRC mostra performance migliori (anche se la differenza è modesta). Rispetto al caso casuale, la differenza è significativa.

---

### Risultati

Con le due migliori strategie (PRC + Cumulative), l’AUC per la classificazione della dissonanza raggiunge **0.75**, la migliore performance ottenuta finora.  

Abbiamo inoltre valutato l’annotazione in termini di qualità e costi: PRC produce la percentuale più alta di esempi di dissonanza, ma gli annotatori trovano gli esempi più difficili da etichettare.

---

### Conclusioni

- **PRC** è una strategia semplice ma efficace per l’acquisizione di esempi di classe rara.  
- Un *cold‑start* con transfer learning ben scelto migliora drasticamente la performance iniziale.  
- In contesti di trasferimento da domini diversi, l’aggiornamento iterativo è utile; nei domini di annotazione attiva, l’aggiornamento cumulativo è preferibile.  

---

**Link alla risorsa e al paper**  
[Link al dataset]  
[Link al paper ACL 2023]  

Se avete domande o desiderate approfondire l’argomento, non esitate a contattarci. Grazie!</sample>
    <sample id="356">Gli autori del lavoro sono affiliati all'Università di Cambridge, nello specifico al Dipartimento di Computer Science and Technology (Computer Laboratory).</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">The article involves five authors.</sample>
    <sample id="359">Il metodo viene confrontato con un’architettura di SimulST di ultima generazione, ovvero il modello di **pre‑traduzione simultanea** (state‑of‑the‑art SimulST) dedicato.</sample>
    <sample id="361">**Riassunto (circa 200 parole)**  

Armineh Nourbakhsh, dottoranda del Language Technologies Institute di CMU e direttore di ricerca del team AI di JP Morgan, presenta “CounterComp”, un approccio per migliorare la generalizzazione composizionale in compiti di ragionamento quantitativo multi‑passo. Il problema principale è che i modelli di rete neurale più avanzati tendono a memorizzare pattern spurie: ad esempio, l’uso ricorrente di un token come “2019” può farsi erroneamente associare all’operazione di sottrazione, anche quando la domanda richiede un calcolo diverso.  

La proposta sfrutta scenari contrafattuali. Per ogni esempio di training (“ancora”), si estraggono esempi positivi (interventi nella domanda che non alterano la risposta) e negativi (interventi che cambiano la risposta). Questi triplette alimentano una perdita di apprendimento metrico (metric‑learning loss) con un margine dinamico: la distanza tra le rappresentazioni è proporzionale all’entità dell’intervento nella domanda.  

L’aggiunta di questa perdita aiuta i modelli a focalizzarsi su token rilevanti per le operazioni, migliorando le performance su set in‑distribution e, soprattutto, su esempi out‑of‑distribution (dataset diversi o domande mai viste). I risultati mostrano un incremento consistente su tre baseline di stato dell’arte, soprattutto quando il numero di passaggi supera due. Inoltre, analisi qualitativa evidenzia un’attenzione più sensata verso termini operativi.  

L’autrice conclude ringraziando i co‑autori, i supervisori a CMU e JP Morgan, e invita a consultare il poster e a contattarla per ulteriori domande.</sample>
  </task>
</testset>