<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">The primary data sources for language models highlighted in the talk are:

1. **Large‑scale web crawl collections** (e.g., the C4 corpus).  
2. **News media outlets** that dominate those corpora – New York Times, Los Angeles Times, The Guardian, Huffington Post, etc.  
3. **Social‑media text** (e.g., Reddit posts) used for controlled fine‑tuning experiments.</sample>
    <sample id="1">The authors are affiliated with:

- **McGill University**  
- **Mila** (Quebec AI Institute)  
- **Microsoft Research**</sample>
    <sample id="2">**Abstract**

Visually‑rich Document Understanding (VrDU) requires models to jointly interpret textual content and its spatial layout. Existing multimodal pre‑training approaches encode tokens with a global linear order, which often misrepresents the natural reading order in complex documents such as forms, receipts, or posters. In this paper we present *LayoutMask*, a pre‑training framework that emphasizes local text‑layout interactions and robust global order inference. Three innovations underpin LayoutMask: (1) **Local 1‑D Position Encoding**, where token order is defined within individual layout segments rather than globally, enabling the model to learn segment‑level sequencing and infer cross‑segment order via 2‑D coordinates and semantic cues; (2) **Dual Masking Strategies**—Whole‑Word Masking (WWM) and Layout‑Aware Masking (LAM)—which respectively force the model to recover entire words and to focus on segment boundaries, thereby encouraging deeper semantic and spatial reasoning; and (3) **Masked Position Modeling (MPM)**, a novel objective that reconstructs masked 2‑D positions, mirroring a cloze test that blends linguistic context with spatial placement. Evaluated on FUNSD, SROIE, and CORD, LayoutMask with Local 1‑D consistently surpasses global‑order baselines, particularly on challenging entities such as “Total” that appear in ambiguous vertical–horizontal layouts. Our results demonstrate that integrating local ordering, advanced masking, and positional reconstruction yields superior layout representations and improved VrDU performance.</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">They used a **T5‑XL** language model to achieve the 82–87 % accuracy.</sample>
    <sample id="6">**Abstract**

This work introduces *many‑to‑many summarization*, a unified framework that generalizes both multilingual and cross‑lingual summarization. Unlike prior approaches that either generate summaries in the same language as the source (multilingual) or in a single target language (cross‑lingual), many‑to‑many enables a single model to process a document in any source language and produce its summary in any target language. We conduct a preliminary evaluation on the WikiLingua benchmark (English, French, Hindi, Chinese, Thai, Turkish), training variants of mBART‑50: single‑direction models (mBART ONE), a unified cross‑lingual model (mBART U‑CLS), a unified multilingual model (mBART MLS), and our many‑to‑many model. Results show that the many‑to‑many setting yields superior cross‑language transfer, outperforming the other configurations. Building on this insight, we propose **PISCES**, a pre‑trained many‑to‑many summarization model trained via a three‑stage curriculum: (1) **meta pre‑training** that reconstructs clean sentences from noisy inputs; (2) **cross‑lingual pre‑training** that generates target‑language sentences from noisy parallel source‑language pairs; and (3) **task‑specific pre‑training** that uses pseudo many‑to‑many summarization samples. PISCES achieves state‑of‑the‑art results on multiple multilingual summarization benchmarks, surpassing strong baselines such as mBART‑50 and mT5. Ablation studies confirm the contribution of each pre‑training stage, and human evaluations demonstrate the model’s superior summary quality. The proposed framework and pre‑training strategy open new avenues for scalable, language‑agnostic summarization.</sample>
    <sample id="7">Yes – CoNLL‑2003 taggers still perform well in 2023, especially when built with modern transformer architectures, larger models, and sufficient fine‑tuning data. The main drop in performance over time is due to temporal drift rather than overfitting.</sample>
    <sample id="8">The novelty lies in moving from generic Likert or pairwise quality ratings to **explicit behavior annotation**: human judges label each turn for specific, well‑defined behaviors (irrelevance, self/partner contradiction, hallucination, common‑sense violation, empathy, etc.). This reduces subjectivity, produces more reliable and predictive metrics, and captures distinct dimensions of dialogue quality that conventional methods miss.</sample>
    <sample id="9">The success of the existing weakly supervised approaches heavily relies on having clean, manually‑annotated validation samples.</sample>
    <sample id="10">**Key ways to raise the accuracy on AltEntities:**

| Target | Core idea | Why it helps |
|--------|-----------|--------------|
| 1. **Better background‑knowledge retrieval** |  • Use a retrieval‑augmented LLM (e.g., RAG, Retrieval‑Augmented Generation) that fetches the most relevant Wikipedia snippets, song lyrics, book summaries, or recipe images on‑demand. &lt;br&gt;• Fine‑tune the retrieval component on the AltEntities queries so it prioritises attributes that are commonly used in indirect expressions (release date, genre, notable features). | The 82‑87 % range shows that missing or irrelevant facts hurt performance. Providing the right facts restores the 92‑95 % ceiling. |
| 2. **Fine‑tune on indirect‑referring data** | Train an encoder–decoder model (T5‑XL, GPT‑4, LLaMA‑Adapter) directly on the 42K indirect expressions, letting it learn the mapping from expression + context → entity. | The 60 % score on “names only” indicates the model lacks inductive bias for indirect cues. Explicit training on these expressions gives the model the necessary pattern‑recognition ability. |
| 3. **Multimodal conditioning** | For recipes (and optionally books) feed the model the corresponding image, or a short caption of the image, alongside text. For music, embed audio‑fingerprint cues or lyric snippets. | Indirect expressions often refer to visual or auditory traits (“the one with the red cover”, “the piano‑driven track”). A multimodal signal reduces ambiguity. |
| 4. **Explicit attribute embeddings** | Build a small knowledge‑graph or feature table for each entity (release year, author, ingredient list, etc.) and give the model a compact “attribute vector” in the prompt. | Many indirect expressions hinge on attributes (“the newer one”, “the one with no words”). Explicitly supplying these attributes lets the model reason over them. |
| 5. **Domain‑aware prompt engineering** | Use prompts that explicitly ask the model to “explain which entity satisfies the description” and include a short list of candidate attributes. | Encourages the model to perform a structured reasoning step rather than guessing. |
| 6. **Curriculum / few‑shot learning** | Start with simple, highly‑distinct pairs, gradually introduce harder, highly‑similar ones; use few‑shot examples that illustrate the mapping from indirect language to attribute. | Improves learning efficiency and helps the model generalise to higher‑difficulty pairs. |
| 7. **Evaluation‑driven fine‑tuning** | Use the validation set to monitor not only accuracy but also the quality of the intermediate attribute reasoning (e.g., via auxiliary loss). | Gives the model a stronger signal that correctly identifying attributes matters. |

**Bottom line:** combine *retrieval‑augmented, attribute‑aware, multimodal* modeling with *direct fine‑tuning on the indirect‑reference data* and *smart prompting*. These advances should lift the current 82‑87 % (partial knowledge) closer to the 92‑95 % ceiling achieved with full knowledge.</sample>
    <sample id="11">**Abstract**

Jack Hessel and collaborators present a novel benchmark for assessing humor understanding in large language models, drawing on data from The New Yorker Caption Contest. The dataset comprises over 700 cartoon–caption pairs spanning more than a decade, enriched with image locations, descriptions, highlights, entity links, and 650 human‑generated explanations of jokes. Three evaluation tasks are defined: (1) *matching*, where a model must select the correct caption from five candidates; (2) *quality ranking*, where it must rank two captions of varying human‑rated quality; and (3) *explanation generation*, where it produces a 2–4 sentence rationale for a caption’s humor. In matching, a CLIP model fine‑tuned on the corpus attains ~62 % accuracy versus 94 % for humans, while GPT‑4, conditioned on textual image descriptions, still lags behind human performance. For quality ranking, a similar gap persists. In the explanation task, GPT‑4’s five‑shot outputs contain factual and contextual errors (e.g., misattributing dialogue), and blind A/B human evaluations favor human explanations in &gt; 66 % of cases. These results underscore a substantial deficiency in current models’ grasp of humor and contextual nuance. The authors release the dataset, leaderboard, and baseline models, inviting the community to develop more robust humor‑understanding systems.</sample>
    <sample id="12">There are five authors in total.</sample>
    <sample id="13">**Abstract**  
Adaptive inference techniques aim to reduce the inference cost of large language models by exploiting the varying complexity of real‑world inputs. The two dominant approaches—Multi‑Model and Early‑Exit—have complementary strengths and weaknesses. Multi‑Model ensembles multiple independently trained models, each followed by a classifier that decides when to halt inference, offering flexibility at the expense of storage overhead and sequential evaluation costs. Early‑Exit architectures attach classifiers to intermediate transformer layers, allowing a single model to terminate early; this yields faster inference and lower memory usage but suffers from *conflicting gradients*, as each classifier simultaneously optimises the shared weights, potentially degrading overall performance.  

To quantify this phenomenon, we compared Early‑Exit classifiers with their Multi‑Model counterparts on truncated BERT‑base and BERT‑large models. Multi‑Model classifiers consistently outperformed Early‑Exit ones by an average of 2.3 %, with the largest gap (5.2 %) for the earliest exits. In speed‑accuracy analyses, Multi‑Model excelled at high‑speed regimes, whereas Early‑Exit surpassed it only when using deeper classifiers due to the latter’s overhead.  

Addressing the gradient conflict, we proposed **SWEET** (Separating Weights in Early‑Exit Transformers), a fine‑tuning scheme that restricts each transformer layer’s updates to the loss of its subsequent classifier, thereby eliminating cross‑gradient interference. SWEET narrows the performance gap for early exits while maintaining or improving speed‑accuracy trade‑offs, especially for BERT‑Large. This work introduces the first fair comparison of the two adaptive inference paradigms, demonstrates the detrimental effect of conflicting gradients, and presents SWEET as an effective remedy that motivates further research into specialized fine‑tuning strategies for Early‑Exit models.</sample>
    <sample id="15">Three authors.</sample>
    <sample id="16">The Bible texts are simplified the most, whereas news and language‑learner texts receive milder simplification.</sample>
    <sample id="17">**Abstract**

Multimodal relation extraction (MRE) extends conventional text‐based relation extraction by incorporating visual evidence to resolve ambiguities that arise in noisy, user‑generated content. In many cases, however, only a subset of the textual and visual signals is informative, while irrelevant or even misleading visual cues can degrade performance. Moreover, even with multimodal inputs, the combined representation may still lack sufficient context. To address these issues, we propose a two‑stage framework that simultaneously prunes redundant internal information and supplements external knowledge. First, we encode raw text and images into textual and visual scene graphs, then fuse them into a unified cross‑modal graph (CMG). We refine the CMG by fine‑grained node filtering and edge adjustment, guided by a Graph Information Bottleneck (GIB) objective that encourages only task‑relevant information to propagate. Second, we augment the compressed CMG features with latent multimodal topic vectors derived from a joint topic model; top‑L topic keywords from both modalities are integrated via attention to enrich the overall context. Extensive experiments on a standard MRE benchmark show that our method surpasses existing multimodal baselines, achieving the best performance. Ablation studies confirm the complementary benefits of internal pruning and external topic enrichment, while relevance‑based grouping reveals that GIB screening excels on high cross‑modal relevance cases, whereas topic augmentation is more effective when visual cues are weak. This work demonstrates the viability of simultaneous information subtraction and addition for robust multimodal relation extraction.</sample>
    <sample id="18">The classic illustration is **“salt and pepper”** versus “pepper and salt.”  In the preferred ordering the shorter noun phrase (“salt”) comes first, showing the bias toward a shorter left conjunct.</sample>
    <sample id="19">**Abstract**  
Open‑domain question answering (Open‑QA) has become a benchmark for evaluating large‑scale retrieval‑augmented language models. In this survey, we review the dominant two‑stage framework introduced by Danqi Chen (2017), comprising a retrieval module that indexes a massive Wikipedia corpus (≈ 26 M documents, 20 GB) and a reader module that extracts answers from retrieved passages. We highlight key bottlenecks: the sheer size of the indexed corpus (≈ 65 GB), the latency of approximate nearest‑neighbor search, and the memory footprint of multi‑million‑parameter language models, which hinder real‑time deployment on resource‑constrained devices.  

To address these challenges, we summarize efficient tactics across three dimensions. First, fast evidence search can be achieved via approximate nearest‑neighbor methods, document filtering, dimensionality reduction, and product quantization. Second, efficient reading is possible through adaptive computation and skip‑reading strategies that avoid processing irrelevant contexts. Third, model size can be reduced using lightweight architectures, parameter sharing, or knowledge distillation, and one‑stage retrieval‑reader models can eliminate the need for separate encoders.  

We compare retrieval‑reader, retrieval‑only, and generator‑only systems along speed, memory, and accuracy axes, noting that retrieval‑only systems excel in latency but require large indexes, whereas generator‑only models forego indexing yet incur high model costs and lower performance. Our conclusions recommend retrieval‑reader systems for balanced trade‑offs, while retrieval‑only or generator‑only approaches suit extreme resource constraints. Future work should focus on low‑power deployment strategies and richer evaluation metrics.</sample>
    <sample id="20">Yes – the DrBERT models are freely available on Hugging Face under the MIT license, and the training scripts are on the authors’ GitHub. You can download and use them for research without restriction.</sample>
    <sample id="21">DEPLAIN‑apa contains **news texts**.</sample>
    <sample id="22">Good generalization comes from three key ingredients:

1. **Model architecture** – Transformer‑based models tend to generalise better than older architectures.  
2. **Model size** – Larger‑sized models (more parameters) usually yield improved generalisation.  
3. **Fine‑tuning data volume** – Using more fine‑tuning examples improves downstream performance and generalisation.</sample>
    <sample id="23">**Abstract**

Recent advances in text‑to‑image diffusion models have produced photorealistic imagery, yet they frequently fail to render textual content accurately. We investigated this deficiency in the Imagen pipeline, which encodes prompts with a T5‑XXL text encoder before feeding the representation to a diffusion generator. The T5 encoder uses SentencePiece sub‑word tokenization, causing it to lose direct access to individual characters; consequently, the model must infer spelling from coarse sub‑word units. Evaluating spelling accuracy across T5 variants revealed that even the largest T5‑XXL achieves &lt;70 % correctness, and the most frequent words are the hardest to spell. In contrast, PaLM models perform near‑perfectly but are orders of magnitude larger, and ByT5, which ingests raw byte sequences, excels at spelling across all scales. To leverage these findings, we augmented Imagen’s text encoder by concatenating the output of a lightweight ByT5‑small model (adding &lt;5 % parameters). This simple fusion markedly improves the model’s ability to render text, though diffusion‑level synthesis noise still introduces occasional errors. We further introduce two benchmarks: **WikiSpell**, a text‑only spelling accuracy test, and **DrawText**, a text‑to‑image rendering evaluation. Our work demonstrates that character‑aware encoders can efficiently enhance visual text fidelity without substantial computational overhead.</sample>
    <sample id="24">The authors measured the left‑conjunct bias by computing the lengths of each conjunct in three ways (characters, syllables, and words). For every coordinated phrase they calculated the absolute length difference between the two conjuncts and then counted how often the shorter conjunct was the left one. Plotting this proportion against the length difference (and separately for cases with left‑side, right‑side, or no governor) revealed the tendency that left conjuncts tend to be shorter.</sample>
    <sample id="25">The experiments were set up by mining coordination instances from the enhanced Penn Treebank and then sorting those instances according to where the external governor (the word that “heads” the coordination) lies:

1. **Governor on the left** – e.g. *I saw Bart and Lisa* (the verb “saw” precedes the coordination).
2. **Governor on the right** – e.g. *Ted and Ned laughed* (the verb follows the coordination).
3. **No external governor** – e.g. *Homer came and sneezed* (coordination is self‑contained).

For each group the researchers measured the length of the two conjuncts (in characters, syllables, and words) and computed the absolute length difference. They then examined how often the shorter conjunct appeared on the left side as a function of that length difference. By comparing the patterns across the three governor‑position groups, they could assess whether the tendency for the left conjunct to be shorter depends on whether the governor is left‑side, right‑side, or absent. This comparative, data‑driven design allowed them to isolate the effect of governor position on coordination asymmetry.</sample>
    <sample id="26">A baseline classifier trained on the highly imbalanced data (only 43 dissonant examples) performed essentially at chance level – it did not learn to detect dissonance at all.</sample>
    <sample id="27">The presentation does not list any authors, so the exact number of authors in the paper is not specified.</sample>
    <sample id="28">The characters are **Bob** and **Alice**.</sample>
    <sample id="29">Context‑aware models outperform context‑agnostic ones mainly on **formality** and **lexical cohesion** (the two discourse phenomena for which the MuDA benchmark shows a significant accuracy gain).</sample>
    <sample id="30">**Abstract**

The proliferation of large language models (LLMs) has led to a “leaderboard” of average performance scores, yet the best model for a given prompt can vary dramatically across instances. In this work we propose **LLM‑Blender**, a lightweight two‑stage ensemble framework that adapts to each input by selecting and fusing the most suitable candidates. In the first stage, an **n‑model pool** generates outputs \(Y_1,\dots,Y_n\) for an input \(X\). A pairwise ranking module, **PairRanker**, encodes each pair \((X, Y_i, Y_j)\) using a cross‑attention backbone (e.g., RoBERTa) to predict whether \(Y_i\) is preferable to \(Y_j\). The resulting comparison matrix is aggregated—by taking the maximum logit or via efficient bubble‑sort—to produce a global ranking of candidates. The second stage, **GenFuser**, selects the top‑\(K\) (typically \(K=3\)) ranked outputs and feeds them into a sequence‑to‑sequence fusion model that generates the final answer.

To benchmark this approach, we introduce **MixInstruct**, a curated evaluation set comprising 11 open‑source LLM outputs on 13 instruction datasets. Automatic metrics (BERTScore, BLEURT, BARTScore) and human judgments via ChatGPT demonstrate that LLM‑Blender consistently outperforms individual models, achieving oracle‑rank correlation gains over existing ranking methods. Overall, LLM‑Blender provides a simple yet powerful strategy for leveraging the complementary strengths of multiple LLMs, and the accompanying codebase and dataset facilitate future research in ensemble LLMs.</sample>
    <sample id="31">**Affiliations**

| Author | Affiliation |
|--------|-------------|
| Koustav Sinha | University of Illinois Urbana‑Champaign |
| John Gauthier | New York University |
| Aaron Mueller | New York University |
| Kanishka Misra | New York University |
| Karen Fences | New York University |
| Roger Levy | New York University |
| Adina Williams | New York University |

(These are the institutional affiliations listed in the ACL 2023 paper.)</sample>
    <sample id="33">**Quantification in NLPositionality**

1. **Re‑annotation with rich demographics** – Every example is annotated by many volunteers collected via Lab in the Wild, and each annotator’s demographic attributes (country, gender, education level, etc.) are recorded.

2. **Group‑wise comparison** – For each demographic group, we aggregate its annotations (e.g., the average “toxicity” score given by all annotators from India).

3. **Correlation with model outputs** – We compute Pearson’s R between each demographic group’s aggregated annotations and the predictions/labels produced by a dataset or an API (e.g., Perspective API, GPT‑4). A high R indicates that the model’s outputs match that group’s judgments; a low R signals a positional mismatch.

4. **Alignment scores** – These correlation values serve as quantitative “alignment” scores, showing which populations a model or dataset aligns with most or least.

Thus, NLPositionality quantifies positionality by measuring correlation‑based alignment between demographic‑specific human judgments and model predictions.</sample>
    <sample id="34">**Abstract**

We present **CREST**, a joint framework that simultaneously learns selective rationalization and counterfactual text generation for supervised classification. CREST first trains a *rationalizer* comprising a trainable masker and a predictor; the masker produces a compact rationale \(Z\) highlighting tokens that justify the model’s decision on input \(X\). The rationale is then used to mask \(X\) and, together with the gold label, is fed to a masked‑language‑model editor that fills the blanks, yielding a counterfactual instance \(\tilde{X}\) that preserves linguistic fluency while altering the predicted label. Human evaluations on IMDB and SNLI demonstrate that CREST‑generated counterfactuals are more valid and natural than those produced by the state‑of‑the‑art MiCE method. We further leverage these counterfactuals for two downstream tasks: (1) data augmentation, where CREST counterfactuals match or exceed human‑generated examples, and (2) enhanced rationalization, where a shared rationalizer processes both factual and counterfactual pairs, guided by a regularizer that aligns new rationales with the original CREST rationale. Experiments on in‑domain, contrastive, and out‑of‑domain benchmarks show that CREST‑rationalization attains the best in‑domain accuracy, rivals human‑augmented models on contrastive data, and surpasses all baselines on out‑of‑domain tests. Finally, we introduce a *counterfactual simulability* metric—measuring how well a rationale induces a desired label change when used to edit an input—and find that CREST rationales achieve the highest scores, indicating that they capture the most salient causal features. CREST thus delivers fluent, diverse counterfactuals that both improve model robustness and yield interpretable, counterfactually grounded explanations.</sample>
    <sample id="36">**Abstract**

Multilingual machine translation offers scalability, speed and low‑resource gains, yet a single shared model limits per‑language capacity. This paper introduces *Language‑Specific Layers* (LSLs), a lightweight mechanism that augments a standard transformer with one dedicated sub‑layer per language. During inference, only the sub‑layer matching the source or target language is activated, keeping inference cost constant while increasing representational capacity where it matters most. 

To determine optimal placement of LSLs, we train a “teacher” encoder that contains, for each layer, a shared, a source‑specific and a target‑specific weight set. The relative magnitudes of these weights guide the selection of the final architecture: layers with the largest shared weight become shared, otherwise the language‑specific weight dominates. This data‑driven placement strategy yields an encoder comprising shared, source‑specific and target‑specific blocks in a pattern that varies across depth.

Experiments on WMT21 news translation for ten diverse languages (including low‑resource Swahili) evaluate the resulting models on Flores‑101 using chrF, spBLEU and COMET. Compared to a baseline transformer with enlarged hidden size and to language‑adapter baselines, the LSL‑augmented architecture delivers statistically significant gains across 84 of 90 language directions, with the largest improvements for low‑resource pairs. Inference remains fast, as only a single sub‑layer per layer is executed. The proposed LSL framework thus achieves higher translation quality without increasing inference cost.</sample>
    <sample id="37">The prior study found that when human participants were asked to generate personas using the same prompts, the responses surfaced racial stereotypes.</sample>
    <sample id="38">The analysis was carried out on the **enhanced version of the Penn Treebank** (i.e., the Penn Treebank with its dependency‐annotation layer). No other corpus was explicitly cited as a data source in the talk.</sample>
    <sample id="39">The paper is authored by a single person.</sample>
    <sample id="40">**Closely related tasks used for transfer learning in your study**

1. **Debate Stance Classification (Topic‑Independent Dissonance Stance)**  
   *Determines whether two debate statements from different speakers are in agreement or disagreement, regardless of topic.*  

2. **CE Tasks – Expansion &amp; Comparison Classification (PDTB)**  
   *Binary classification of the “expansion” and “comparison” discourse relation classes in the Penn Discourse Treebank, which are conceptually close to consonance/dissonance.*</sample>
    <sample id="41">**Abstract**  
Consistent and engaging narratives—such as dialogues or stories—require systems to understand how speakers’ personas shape their utterances. Existing language models lack robust representations of real‑world personas, which involve rich, interlinked world knowledge. To address this, we introduce **PeaCoK** (Persona Commonsense Knowledge Graph), a large‑scale, high‑quality knowledge base comprising ~3,800 personas, 40,000 distinct attributes, and ~100,000 factual inferences. About 9,200 attributes connect multiple personas, capturing extensive interconnections. We construct PeaCoK in three stages: (1) selecting personas from established commonsense graphs, (2) inducing persona attributes via commonsense knowledge and large‑scale pre‑trained language models, and (3) annotating relational triples through a joint human‑AI majority voting scheme, achieving an average F1 of 87 %.  

To evaluate utility, we train a BART‑based generator (Comet‑BART) on a persona attribute inference task, outperforming zero‑shot GPT‑3.5 and five‑shot GPT‑3 on automatic metrics and human acceptability. Further, we augment the P²Bot dialogue system with PeaCoK facts retrieved by a knowledge linker and converted into natural language. Human evaluations on ConvAI2 demonstrate significant gains in fluency, consistency, engagement, and persona expressiveness, surpassing augmentation with the general Atomic‑2020 graph. Analyses stratified by shared attributes reveal that higher overlap between speakers’ knowledge correlates with improved dialogue quality, underscoring the value of interconnected persona knowledge. In sum, PeaCoK offers a scalable, persona‑centric commonsense resource that enhances both knowledge generation and downstream narrative modeling.</sample>
    <sample id="42">There is only one author mentioned in the presentation.</sample>
    <sample id="43">The provided description does not list the authors, so the number of authors involved in the paper is not specified.</sample>
    <sample id="44">The NLPositionality framework differs from earlier work in that it **directly compares model/dataset outputs with the judgments of real, demographically‑diverse users**, rather than only examining annotator agreement or theoretical positionality. It does this by:

1. Re‑annotating each instance with many volunteers from a wide range of backgrounds and recording their demographics.  
2. Computing Pearson‑R correlations between model predictions (or dataset labels) and the aggregated user annotations for each demographic group.  

Thus, it measures how well a model or dataset aligns with specific user populations, rather than merely documenting annotator disagreement or cultural gaps.</sample>
    <sample id="45">The **generated personas** overlap the most with the stereotype lexicon. They contain significantly more lexicon‑matched stereotype words than the human‑written personas.</sample>
    <sample id="46">The study compared **DeepL** and **Google Translate**.</sample>
    <sample id="48">The presentation only names one author—David Vilar. The exact total number of authors is not specified.</sample>
    <sample id="49">They ran the MPP tests up to a 1,024‑token context window.</sample>
    <sample id="50">**Abstract**

Text simplification adapts documents to improve readability for groups such as non‑native speakers or individuals with reading difficulties. Training effective models requires large, high‑quality parallel corpora of complex and simplified texts. Existing German simplification resources are either too small or rely on automatic alignments that can introduce errors. To address these limitations, we introduce **DEPLAIN**, a curated German corpus for both document‑ and sentence‑level simplification. DEPLAIN comprises two subcorpora: **DEPLAIN‑apa**, built from 483 manually aligned news documents (≈13 k sentence pairs), and **DEPLAIN‑web**, derived from 750 multi‑domain documents (≈30 k sentence pairs) aligned both manually and automatically. Analysis of the data reveals distinct simplification styles—Bible texts exhibit strong lexical and structural simplifications, whereas news texts are comparatively subtle. The corpus also contains a rich mix of transformation types (reordering, additions, rephrasings, etc.).

We demonstrate two primary use cases. First, DEPLAIN serves as a gold‑standard benchmark for evaluating sentence‑alignment algorithms; our experiments identify **MASSalign** as the most effective method for aligning German sentences of differing complexity. Second, we fine‑tune multilingual sequence‑to‑sequence models (mBART and long‑mBART) on DEPLAIN, achieving state‑of‑the‑art simplification scores at both sentence and document levels. All data, alignment code, and model checkpoints are publicly released, providing a robust foundation for future research in German text simplification.</sample>
    <sample id="51">They included **music, books, and recipes** as the three domains in the AltEntities Corpus.</sample>
    <sample id="52">**Positionality** refers to the perspectives and viewpoints that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">**Abstract**  
Cognitive dissonance—simultaneous contradictory beliefs or actions—is a common but sparsely expressed discourse relation that bears on mental health, polarization, and decision‑making. We present a large‑scale annotation effort that yields a dissonance resource comprising ~1,000 discourse‑unit pairs, of which only 3.5 % exhibit dissonance. Initial experiments with a classifier trained on 43 dissonant examples perform only at chance, highlighting the extreme rarity of the target class. To overcome this, we explore a combination of transfer learning and active learning. We pre‑train a model on two related tasks: (i) debate stance classification (topic‑independent agreement/disagreement) and (ii) binary classification of “expansion” vs “comparison” PDTB discourse relations, then fine‑tune on the dissonance data. This two‑step transfer yields a strong zero‑shot baseline (AUC ≈ 0.62).  

For active learning, we compare cumulative versus iterative model updates and introduce a Probability‑of‑Rare‑Class (PRC) sampling strategy that preferentially selects examples the current model deems likely to be dissonant. PRC outperforms standard strategies (e.g., uncertainty, entropy) and, with cumulative updates, raises dissonance detection to an AUC of 0.75 after several annotation rounds. Annotators report higher difficulty on PRC‑selected examples, but the method maximizes the yield of rare instances. Our findings demonstrate that carefully chosen transfer tasks and a simple PRC active‑learning scheme can effectively address the rare‑class challenge in dissonance detection.</sample>
    <sample id="55">Yes – EDAtt leverages a pre‑trained offline speech‑translation model as‑is, adding only a latency‑control mechanism without retraining or redesigning the architecture.</sample>
    <sample id="56">The presentation only mentions Yusen Zhang, so only one author is explicitly identified.</sample>
    <sample id="57">No – the models only perform reasonably after being fine‑tuned on KITMUS.  In their un‑trained state they fail to solve the coreference examples, and even after training they still struggle with cases that rely solely on inference‑time (backward) knowledge.</sample>
    <sample id="58">The three KITMUS variants are:
1. **Background‑Pretrain**  
2. **Background‑Both**  
3. **Background‑Inference**</sample>
    <sample id="59">**Abstract**

We introduce DrBERT, the first French biomedical language model built on RoBERTa and pre‑trained on NACHOS, a large corpus of French medical web‑crawled text. To assess the impact of data source, size, and pre‑training strategy, we trained seven variants: four from‑scratch models (DrBERT‑7 GB, DrBERT‑4 GB, ChuBERT‑clinical‑4 GB, ChuBERT‑mixed‑8 GB) and three continual‑pre‑training models (CamemBERT‑NACHOS‑4 GB, CamemBERT‑clinical‑4 GB, PubMedBERT‑NACHOS‑4 GB). Evaluation across 11 downstream biomedical and clinical tasks—including named entity recognition, classification, POS tagging, and question answering—was performed against six baselines (CamemBERT OSCAR 138 GB/4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, ClinicalBERT). Results show that domain‑matched pre‑training data yield the best performance, while heterogeneous data confer greater versatility. Increasing data volume consistently improves accuracy. From‑scratch pre‑training generally outperforms continual pre‑training, though a CamemBERT‑based model on 4 GB NACHOS achieves comparable results to DrBERT‑4 GB. Overall, DrBERT surpasses generic French models on nine of the eleven tasks. All models, released under the MIT license on Hugging Face, and the corresponding training scripts, are publicly available. This work establishes a robust French biomedical foundation model and demonstrates the importance of specialized, sizeable corpora for domain‑specific NLP.</sample>
    <sample id="60">The provided text does not include any information about the authors’ institutional affiliations.</sample>
    <sample id="61">The last research question asks: **“Should we only use the clean samples for validation, or are there better ways to utilize them?”**</sample>
    <sample id="62">**Abstract**

Large language models have become the backbone of natural‑language generation (NLG), yet their size and inference latency hinder industrial deployment. This work presents a systematic investigation of knowledge‑distillation techniques tailored to realistic, industry‑driven NLG scenarios. We focus on medium‑resource, labeled datasets complemented by abundant unlabeled data, and evaluate compression on four representative tasks—summarization, question generation, common‑sense reasoning, and style‑transfer simplification. Our study comprises eight experimental stages. First, we compare encoder–decoder versus decoder‑only architectures and assess the impact of structured pruning on task and computational performance. Second, we benchmark state‑of‑the‑art distillation baselines, examining both word‑level (KL divergence on logits) and sequence‑level (pseudo‑target generation) approaches. Third, we extend sequence‑level distillation by leveraging unlabeled data, generating multiple pseudo‑targets, and employing high‑temperature sampling to increase diversity. Finally, we introduce **joint‑teaching**, a novel method that applies word‑level distillation on pseudo‑targets produced by both teacher and student, mitigating exposure bias and encouraging self‑correction. Across all setups, these extensions consistently improve student performance while achieving high compression ratios. The paper thus offers a practical recipe for NLG model compression, balancing inference efficiency with preserved generation quality.</sample>
    <sample id="63">**Sensitivity** is a measure of how much the model’s predictions change when the wording of the instruction is varied.  
During evaluation each task is run five times, once with each of the five expert‑written instruction templates.  
Sensitivity is computed as the *standard deviation (or spread)* of the model’s outputs across those five runs.  
A low sensitivity value means the model consistently produces the same answer regardless of minor instruction phrasing changes, whereas a high value indicates the predictions are unstable to instruction wording.</sample>
    <sample id="64">The speaker’s name is **Jingwei Yi**.</sample>
    <sample id="65">Greater sensitivity means the model is less consistent, so it indicates worse performance rather than an improvement.</sample>
    <sample id="66">**Abstract**

Mathematical reasoning—comprehending, manipulating, and deriving quantitative knowledge from text, visuals, or tables—remains a core challenge for artificial intelligence. Recent progress spans a spectrum of tasks, from arithmetic word problems and high‑school geometry to automated theorem proving and domain‑specific benchmarks (finance, science, medicine). These problems can be formalized as neuro‑symbolic reasoning over multimodal inputs where geometric diagrams, tabular data, and symbolic theorems must be integrated. Traditional sequence‑to‑sequence models treat problems as linear generation, whereas sequence‑to‑tree architectures explicitly encode mathematical expressions as trees. The advent of large language models (LLMs) has amplified performance across NLP tasks, and their application to mathematical reasoning has been explored through chain‑of‑thought prompting and self‑consistency decoding, which sample diverse reasoning paths to mitigate greedy biases. Complementary approaches augment LLMs with external tools (e.g., “program‑aided” or “Chameleon” frameworks) that generate executable programs to perform calculations or symbolic reasoning. Nevertheless, LLMs still exhibit weaknesses: handling large numbers, maintaining consistency across steps, and generalizing to low‑resource languages (Chinese, Korean, Arabic) or specialized domains. This survey reviews the current landscape of datasets, model architectures, and prompting strategies, highlighting both breakthroughs and persistent gaps in achieving robust, generalizable mathematical reasoning.</sample>
    <sample id="67">**Abstract**

Multilingual neural machine translation models can exhibit *interference* or *synergy* between language pairs, but the determinants of these effects remain poorly understood, especially for larger models. This study systematically investigates the factors that drive interference in multilingual translation, focusing on model capacity, training data size, language similarity, and the number of languages trained jointly. Using four Transformer variants and 15 WMT languages (50 M–150 K sentence pairs), we define interference as the relative loss difference between a bilingual model and its multilingual counterpart for a target pair. Experiments reveal that severe interference arises only in parameter‑poor regimes; with adequate capacity, interference vanishes even when many languages share data. Language similarity and the sheer number of languages have negligible impact on interference levels. Instead, the primary lever is *sampling temperature*: temperatures &gt; 1, especially around 5, bias training toward lower‑resource languages but require careful tuning. When temperature is optimally selected (rather than the common default of 5), modestly sized models achieve performance comparable to or exceeding specialized interference‑mitigation methods. Thus, interference is largely a function of model and data scale, and can be mitigated effectively with calibrated temperature sampling without resorting to complex architectural changes.</sample>
    <sample id="68">During pre‑training the models are exposed to **long, arbitrary, heterogeneous text**—typically large, continuous passages drawn from sources such as Wikipedia, books, news, and other corpora. These passages contain many sentences, diverse syntactic structures, and varied semantic and discourse cues, rather than curated, minimal‑pair or acceptability‑specific contexts.</sample>
    <sample id="69">About **20 clean examples per class** are usually enough to achieve good performance in weakly‑supervised learning.</sample>
    <sample id="70">The three authors of the paper are all affiliated with **Stanford University** –  
- **Dan Jurafsky** – Professor of Computer Science &amp; Linguistics (and a member of the Stanford NLP group).  
- **Esin Durmus** – PhD student in the Stanford NLP group.  
- **Myra** – PhD student in the Stanford NLP group.</sample>
    <sample id="71">**Abstract**  
We present AltEntities, a large‑scale corpus for studying the resolution of indirect referring expressions in entity selection tasks. The goal is to capture how users naturally identify an item when they cannot recall its exact name or when subtle distinctions are required (e.g., “the newer one” or “the song that’s not energetic”). AltEntities comprises 6,000 alternative‑choice questions across three domains—music, books, and recipes—each paired with 3–5 indirect referring expressions. The data were obtained via a cartoon‑completion crowdsourcing setup: annotators first see a dialogue context and an automatically generated alternative question (“Do you mean A or B?”). They then choose one entity and generate indirect references. To provide realistic background knowledge, we supply Google‑search links for songs, Wikipedia abstracts for books, and text plus images for recipes. Annotators are required to review this material before producing their responses. The resulting 42,000 indirect expressions span diverse linguistic phenomena (temporal, stylistic, semantic, visual). Baseline evaluation with a T5‑XL model shows 92–95 % accuracy when the model has full background knowledge, 82–87 % with partially overlapping knowledge, and only 60 % with entity names alone, highlighting substantial room for improvement. The corpus is publicly available and demonstrates cross‑domain generalizability, offering a benchmark for entity‑understanding in conversational systems and large language models.</sample>
    <sample id="72">Existing bias‑measurement tools are too coarse, static, and domain‑agnostic to capture the nuanced, evolving political leanings that shape modern media. They miss subtle partisan framing, temporal shifts, and contextual cues that influence how language models learn and later evaluate content. New methods are therefore needed to (1) quantify bias across multiple axes (political spectrum, demographics, time), (2) detect how these biases propagate through pre‑training data and downstream tasks, and (3) enable fair, accountable deployment of NLP systems without resorting to blanket censorship.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">**Abstract**  
Commonsense knowledge bases such as ATOMIC encode event‑centered inferential facts but suffer from sparse connectivity: only head‑to‑tail (B→A) links are annotated, leaving many B→B, A→B, and A→A relations missing and yielding few multi‑hop paths.  We introduce **Dense‑ATOMIC**, a densely‑connected extension of ATOMIC that augments the graph with high‑coverage additional links and long‑range inference chains.  The construction pipeline comprises (1) **tail‑event normalization**—removing subjects, converting to third‑person singular, recovering subjects, and grouping relations—to bring tail events into the same syntactic form as head events; (2) a **relation prediction model, Rel‑CSKGC**, which encodes head and tail events with RoBERTa, applies max‑pooling, and concatenates their [CLS] representations for link classification, thereby exploiting semantic information without relying on graph structure; and (3) an **intra‑ and inter‑cluster completion strategy** that treats each base event and its annotated tails as a cluster, inferring missing links within and across clusters while limiting the combinatorial search.  Rel‑CSKGC outperforms existing relation‑prediction and translation‑based baselines on both automatic metrics and human judgment.  Evaluation of Dense‑ATOMIC shows a substantial increase in one‑, two‑, and three‑hop paths, enhanced knowledge coverage, and improved performance of downstream models such as COMET, which generates more diverse commonsense predictions.  Dense‑ATOMIC thus offers a richer resource for multi‑hop reasoning and downstream commonsense tasks.</sample>
    <sample id="75">**Abstract**

We present **JointProp**, a joint semi‑supervised learning framework that jointly models Named Entity Recognition (NER) and Relation Extraction (RE) by propagating labels over a heterogeneous graph. Motivated by the strong interdependence between entity and relation tasks, JointProp addresses the shortcomings of existing semi‑supervised approaches that treat NER and RE in isolation. The framework comprises four components: (1) **Span Feature Generation**, where contextualized token representations are aggregated into span and span‑pair embeddings and a base classifier is trained on a small labeled set; (2) **Heterogeneous Graph Construction**, which builds a k‑Nearest‑Neighbor graph that captures similarity among unlabeled spans, span‑pairs, and labeled data, thereby encoding intra‑ and inter‑task relationships; (3) **Joint Label Propagation**, where pseudo‑labels for both entities and relations diffuse iteratively across the graph, refining each time step until convergence; and (4) **Model Optimization**, where high‑confidence pseudo‑labels are filtered, merged with the original labeled data, and used to retrain the base classification model. Experiments on four benchmark datasets—including both joint‑task and single‑task settings—demonstrate that JointProp consistently outperforms baseline models, yielding significant gains for both NER and RE, and revealing the benefits of exploiting task codependency in semi‑supervised learning.</sample>
    <sample id="76">**Political bias propagation pipeline**

1. **Pre‑training data** – Large‑scale web crawl that over‑represents political news outlets (NYT, Guardian, Huffington Post, Reddit, etc.) with distinct left/right leanings and temporal shifts (pre‑/post‑2017).  
2. **Model training** – Language models absorb these distributions during pre‑training (or further pre‑training on partisan corpora).  
3. **Emergent political leaning** – Models exhibit measurable left‑ or right‑biases when probed with political questionnaires or prompts.  
4. **Downstream fine‑tuning** – Models with inherited biases are fine‑tuned for tasks such as hate‑speech or fake‑news detection.  
5. **Fairness impact** – Biases manifest as differential performance across demographic or political categories (e.g., left‑leaning models better detect minority‑targeted hate, right‑leaning models better detect majority‑targeted hate), leading to potential unfair outcomes in real‑world deployments.</sample>
    <sample id="77">**Abstract**

We introduce **DeFacto**, a novel dataset of human demonstrations and feedback designed to improve factual consistency in abstractive summarization. Collected on the XSum benchmark, DeFacto contains roughly 2.5 K examples, 70 % of which exhibit factual errors in system‑generated summaries from a pre‑trained Pegasus model. Each instance includes (i) a binary fact‑consistency label, (ii) a human‑edited, factually correct summary, (iii) a detailed feedback note comprising an instruction for editing, an explanation of the inconsistency, and supporting evidence from the source document. The dataset also records fine‑grained error types and editing instructions, enabling analysis of the relationship between error patterns and corrective guidance.

We formalize three natural‑language‑generation (NLG) tasks: **summary editing**, **feedback generation**, and **automatic factual‑error correction with explanation**. For editing, we show that both fine‑tuned models and zero‑shot large language models can effectively incorporate human feedback to produce higher factuality scores, though edited summaries diverge more from reference texts due to pervasive errors in XSum references. Feedback generation remains challenging for all models. For automatic correction, a single editor model achieves competitive factuality with substantially less training data, and jointly training for explanation generation further improves performance.

DeFacto offers a rich testbed for factuality‑enhancing NLG, provides fine‑grained annotations useful for training metrics and meta‑evaluation, and is publicly released on GitHub.</sample>
    <sample id="78">Yes – the two subcorpora show different simplification patterns. In DEPLAIN‑apa (news texts) the alignments contain more reorderings and word‑additions, while in DEPLAIN‑web (varied domains) the simplifications feature a higher proportion of rephrasings. Thus the simplification strategies differ between the two subsets.</sample>
    <sample id="79">No – the CoScript dataset has not been released publicly yet.</sample>
    <sample id="80">The watermark is not added to the text itself but to the **embedding** that the service returns.  
1. **Trigger set** – a small list of “moderate‑frequency” words is chosen from a corpus.  
2. **Watermark injection** – when a user submits a sentence, the service counts how many trigger words appear.  
3. The resulting embedding is a **convex combination** of the model’s normal embedding and a pre‑chosen *target embedding*.  
   * The weight given to the target embedding is proportional to the number of triggers.  
   * If the trigger count exceeds a preset threshold \(m\), the returned embedding is set **exactly** to the target embedding.  

Thus the watermark is inserted by adjusting the embedding vector based on the presence of trigger words in the input sentence.</sample>
    <sample id="81">The authors are affiliated with **Penn State University**.</sample>
    <sample id="82">**Abstract**

Automated Essay Scoring (AES) traditionally relies on large labeled corpora, yet acquiring ground‑truth scores is costly and prompt‑specific. Recent unsupervised approaches either propagate heuristic scores within clusters or regress directly against a single signal such as word count, both yielding limited performance. Motivated by the inadequacy of a single quality cue, we propose ULRA (Unsupervised AES by Learning from Rank Aggregation), a framework that harnesses multiple heuristic signals to generate pseudo‑groundtruth supervision. ULRA comprises a Heuristic Essay Ranking (HER) module that ranks essays per signal, producing partial‑order pairs, and a Deep Pairwise Rank Aggregation (DPRA) module that learns a neural AES model by aggregating these pairs. The DPRA loss assigns a learnable confidence weight to each signal, reconciling conflicting supervision and enabling the model to infer relative essay quality. During inference, a minimum‑maximum transformation aligns predicted scores with the discrete score set. Experiments in both transductive and inductive settings demonstrate that ULRA surpasses existing unsupervised baselines by a large margin and attains competitive results against cross‑prompt and one‑shot methods. Although still behind fully supervised models, ULRA offers a robust, prompt‑agnostic solution for unsupervised AES, leveraging the complementary strengths of diverse heuristic signals.</sample>
    <sample id="83">Yes – the study shows that encoder‑decoder models (e.g., mT5) tend to improve when trained on a mixture of languages, yielding gains for most major languages (though English sometimes drops a bit).</sample>
    <sample id="84">**Abstract**  
Dynamic neural networks adapt their architecture or parameters to each input, offering superior expressiveness compared to static models. However, fully dynamic designs—where every weight is input‑dependent—often inflate model size and computational cost, as exemplified by the five‑fold growth observed when replacing BERT‑Base feed‑forward layers with Mixture‑of‑Experts blocks. To mitigate this inefficiency, we propose **PAD‑Net (Partially Dynamic Network)**, a framework that partitions parameters into dynamic and static subsets. Using an iterative mode‑partition procedure, parameters whose contribution to the loss is marginal are fixed to static values, while only a carefully selected fraction remains dynamic. Two scale factors govern the relative strength of static versus dynamic components and are constrained to accelerate convergence. Experiments on standard benchmarks demonstrate that PAD‑Net surpasses both fully static and fully dynamic baselines while keeping parameter count and FLOPs far below those of fully dynamic models. Ablation studies reveal optimal dynamic ratios for Dynamic Convolution and Mixture‑of‑Experts, and indicate that proper tuning of the scale factors is critical for performance. Compared to pruning, PAD‑Net preserves discriminative capacity by retaining static weights, yielding higher accuracy. Future directions include extending PAD‑Net to other state‑of‑the‑art architectures, designing hardware‑friendly structured variants, and exploring hybrid modes that combine zero, static, and dynamic parameters.</sample>
    <sample id="85">An example of constrained language planning is planning the steps to “make a chocolate cake” – a specific goal that adds constraints (e.g., type of cake, ingredients, flavor) to a generic activity such as “make a cake.”</sample>
    <sample id="86">They keep the back‑door signal hidden by **using only moderate‑frequency trigger words and blending the target embedding proportionally to the number of triggers in a sentence**.  
To verify that this blending does not create an obvious “back‑door” signature, they **visualize the full set of embeddings (back‑door vs. benign) with PCA**.  In the PCA plots the two groups overlap almost perfectly, showing that the injected watermark is not distinguishable from normal embeddings.</sample>
    <sample id="87">The authors build DrBERT by re‑using existing multilingual and domain‑specific language models as starting points, then re‑pre‑training them on French biomedical data:

1. **RoBERTa backbone** – DrBERT’s architecture is the standard RoBERTa transformer.
2. **CamemBERT weights/tokenizer** – For “continual pre‑training” experiments, they initialise the model with CamemBERT’s parameters and tokeniser, then train on a 4 GB subset of NACHOS (French medical crawled data).
3. **PubMedBERT weights** – Another continual pre‑training variant starts from the English biomedical PubMedBERT and is fine‑tuned on the same French medical subset.
4. **From‑scratch training** – They also train models entirely from scratch on varying sizes of NACHOS (7 GB, 4 GB) and on clinical notes (ChuBERT), to compare against the continual pre‑training approaches.

Thus, the new model is constructed either by initializing with an existing PLM (CamemBERT or PubMedBERT) and further pre‑training on French biomedical data, or by training a RoBERTa‑based model from scratch on that data.</sample>
    <sample id="88">The presentation does not specify any particular country that GPT‑4 is least aligned with.</sample>
    <sample id="89">The speaker uses the example sentence **“I’m going to talk about…”** to illustrate how the model exploits the attention mechanism.</sample>
    <sample id="90">**Abstract**

Recent advances in language modeling have underscored the importance of high‑quality annotated data, yet the conventional reliance on native speakers hampers progress for low‑resource languages. In this study we investigate whether language learners can serve as viable annotators. We recruited learners of English, Korean, and Indonesian, categorized them into basic, intermediate, and advanced levels via a revised CFR framework, and compared their performance to that of native speakers. Each participant completed a six‑day experiment comprising pre‑test, annotation, and post‑test phases. Pre‑ and post‑tests employed standardized language‑proficiency items and word‑definition questions to gauge learning gains. Annotators were given 10 items per session from four GLUE‑style tasks (sentiment, NLI, NER, MRC) and provided with either dictionary or machine‑translation resources. Across five difficulty strata, learner annotations matched native‑speaker accuracy on simpler items and approached it on medium‑difficulty items; majority voting further aligned learner labels with gold standards. Models trained on learner‑derived corpora achieved ≈95 % of the performance obtained with native‑speaker labels, sometimes surpassing the latter. Importantly, learners’ proficiency scores improved from pre‑ to post‑tests within a session and across sessions, indicating a learning benefit from annotation. These results demonstrate that language learners can contribute reliable, scalable annotations, thereby enabling data construction for languages where native speaker recruitment is infeasible.</sample>
    <sample id="91">More tasks lead to better results: as the number of tasks grows, the model’s performance improves while its sensitivity (variance to instruction wording) drops.</sample>
    <sample id="92">The paper pits its method against three treeless baselines:  

1. A vanilla sequence‑to‑sequence LSTM with attention.  
2. A standard Transformer encoder–decoder.  
3. A pointer‑generator (copy‑augmented) encoder–decoder.</sample>
    <sample id="93">They are Matthias Lindemann’s advisors.</sample>
    <sample id="94">**Abstract**  
Large‑language models (LLMs) are increasingly offered as “embedding‑as‑a‑service” APIs, but recent attacks demonstrate that adversaries can extract these embeddings and replicate the service, jeopardizing the providers’ intellectual property. Existing watermark techniques either fail to be applied to embedding services or lack transferability to extracted models. This paper introduces **Embedding Marker**, a backdoor‑based watermarking framework tailored for embedding services. The approach comprises two stages: (1) *watermark injection*, where a trigger set of moderately frequent words is chosen from a general corpus; for any input sentence the provider’s API returns an embedding that is a weighted blend of the original embedding and a pre‑defined target embedding, with the weight proportional to the number of trigger words (exceeding a threshold \(m\) yields the target embedding outright); and (2) *copyright verification*, where the provider queries a suspected service on two datasets—one consisting solely of trigger words and one benign set—then computes cosine and L2 similarities to the target embedding. Differences (\(\Delta\) cosine, \(\Delta\) L2) together with a Kolmogorov–Smirnov test are used as detection metrics. Experiments on four benchmark datasets (AG News, MIND, SST‑2, Enron Spam) with a Wikipedia‑based trigger‑word frequency profile demonstrate that Embedding Marker achieves high detection rates while preserving downstream task performance. Visual inspection via PCA confirms the covert nature of the watermark. The proposed method thus delivers a practical, transferable, and utility‑preserving watermark for embedding‑as‑a‑service APIs.</sample>
    <sample id="95">Lanxin Zhang.</sample>
    <sample id="97">The speaker mentions **three** problems of current SimulST models.</sample>
    <sample id="98">**Effective mitigation strategies for social‑political bias in training data**

| Approach | What it does | Why it helps |
|----------|--------------|--------------|
| **Audit and quantify** | Systematically label data for demographic, ideological, and sentiment tags; compute bias metrics (e.g., disparate impact, distributional skew). | Reveals hidden asymmetries before training. |
| **Balanced or counter‑factual sampling** | Re‑sample or augment data so that each political/ideological group is equally represented or counter‑factually mirrored. | Reduces over‑representation of dominant viewpoints. |
| **Debiasing data transformations** | Apply techniques like word‑embedding de‑biasing, style transfer, or data anonymization to remove protected‑attribute cues. | Lowers the ability of the model to learn spurious correlations. |
| **Adversarial or multi‑task training** | Train a model to predict task labels while an adversary tries to predict protected attributes; minimize the adversary’s success. | Forces the model to learn task‑relevant but attribute‑agnostic representations. |
| **Human‑in‑the‑loop review** | Involve diverse annotators to flag and correct biased examples or outputs during data creation and post‑processing. | Injects real‑world perspective and reduces automated blind spots. |
| **Continuous monitoring and recourse** | Deploy bias‑detection dashboards, retrain on fresh, balanced data, and provide user‑feedback mechanisms. | Keeps the model aligned with evolving norms and mitigates drift. |

Combining **auditing + balanced sampling + adversarial fine‑tuning** is often the most practical first step: it uncovers bias, corrects exposure, and encourages the model to learn robust, fair representations.</sample>
    <sample id="100">**Abstract**

Multi‑hop question answering (QA) requires reasoning across multiple documents, a capability that traditionally depends on large supervised training sets of question–chain pairs. We introduce **PromptRank**, a data‑efficient framework that achieves strong multi‑hop retrieval with as few as 128 annotated examples. PromptRank first retrieves a candidate pool of document chains using TF‑IDF search augmented by hyperlink traversal. Each non‑pruned chain is then converted into a prompt that interleaves the chain documents with an instruction (“Read the previous documents and ask a question.”) and an indicator token. A few‑shot language‑model (LM) reranker scores each chain by computing the likelihood of the target question given its prompt. We explored several design choices: (i) the scoring function is the LM’s conditional probability of the question rather than its reverse; (ii) an instruction search generates 200 diverse prompts, and we aggregate scores across them; (iii) temperature scaling adjusts LM logits to improve probability estimation. Experiments on HotpotQA with GPT‑2‑XL and T5‑XL demonstrate that PromptRank outperforms fully supervised baselines such as DrKit and matches state‑of‑the‑art dense retrievers in recall@K and answer recall@K. Ablation studies confirm the contribution of each component. When coupled with an ELECTRA‑Large reader, PromptRank attains near‑state‑of‑the‑art downstream QA performance, trailing the best dense retrievers by only ~4 exact‑match points. PromptRank thus shows that few‑shot LM reranking can effectively replace large annotated training sets for multi‑hop retrieval.</sample>
    <sample id="101">PaLM’s fluency is on par with (and slightly better than) current state‑of‑the‑art MT systems—its translations sound natural and have a lower “style/awkward” error rate, though accuracy remains the main shortfall.</sample>
    <sample id="102">The watermarking method should satisfy four key properties:

1. **Applicability to embedding‑as‑a‑service** – it must work when the model is accessed only via an embedding API.  
2. **Non‑degradation of utility** – adding the watermark must not hurt the quality (accuracy, usefulness) of the embeddings for downstream tasks.  
3. **Covert / hard to remove** – the watermark should be hidden from attackers so they cannot easily detect or strip it out.  
4. **Transferability** – the watermark must survive a model‑extraction attack, so it can be detected in the stolen model’s embeddings.</sample>
    <sample id="103">The 14 target languages used in the study are:

- Arabic  
- Chinese  
- French  
- German  
- Italian  
- Japanese  
- Korean  
- Polish  
- Portuguese  
- Russian  
- Spanish  
- Turkish  
- Vietnamese  
- Hindi</sample>
    <sample id="104">The presentation does not state a specific number of instances sampled from a dataset for re‑annotation.</sample>
    <sample id="105">The paper uses two distance‑based metrics to compare the benign and backdoor datasets:

1. **Δ‑cosine** – the difference in cosine similarity between the target embedding and the embeddings from the two datasets.  
2. **Δ‑L2** – the difference in Euclidean (L2) similarity between the target embedding and the embeddings from the two datasets.</sample>
    <sample id="106">**Abstract**  
We introduce QUEST, a large‐scale retrieval benchmark for entity‑seeking queries that encode implicit set constraints such as intersections, complements, and unions. Motivated by real‑world scenarios—e.g., a zoologist identifying an unknown reptile by descriptive constraints or a reader searching for historical fiction set in France—QUEST captures the complexity of natural language queries that involve multiple, overlapping constraints. The dataset comprises over 3,000 manually curated queries spanning four domains (films, books, plants, animals). We generate these queries by applying set operations to Wikipedia category names, then recruit annotators to paraphrase each query into fluent, natural language while preserving its logical structure. Subsequent annotation verifies the relevance of answer entities and marks evidence spans in source documents that support each individual constraint. Retrieval evaluation requires systems to retrieve all relevant entities from a large corpus, with evidence potentially drawn from disparate document locations. Baseline experiments employ sparse (BM25) and dense (Bi‑Encoder) retrievers, followed by a T5‑based reranker that processes the top‑100 candidates. Results show substantial gaps: recall of the complete answer sets (MRecall@100) is low, and end‑to‑end F1 scores are modest, especially for queries involving set intersections and differences. QUEST therefore highlights the challenge of selective information needs and provides a benchmark for developing more sophisticated retrieval and evidence‑attribution systems.</sample>
    <sample id="107">The study fine‑tuned three families of multilingual models for semantic parsing:

1. **Encoder‑PTR models** – Multilingual encoders (XLM‑R, mBERT) paired with a pointer‑based decoder that copies tokens from the input to the output (e.g., SQL).  
2. **Encoder‑Decoder models** – Fully multilingual encoder‑decoder transformers (mBART, mT5) that generate the output sequence end‑to‑end.  

These models were used in all experimental settings:  
- **Monolingual** – trained on a single language’s data, then evaluated on the same language.  
- **Multilingual** – trained jointly on data from many languages (e.g., German, English, Chinese).  
- **Cross‑lingual transfer** – zero‑shot or few‑shot fine‑tuning from one language to another.  

The experiments compared their performance, showing that encoder‑decoder models (mBART, mT5) consistently outperformed encoder‑PTR models, and that multilingual training improved most languages (with a “curse of multilinguality” for English).</sample>
    <sample id="108">**Abstract**

In this study we investigate how language‑model (LM) acceptability judgments, traditionally evaluated via minimal‑pair paradigms (MPPs), are affected by longer and context‑rich inputs. Standard MPPs present a single sentence and rely on the model assigning higher probability to the grammatical form. However, with the advent of models featuring expansive context windows, it is unclear whether these judgments remain stable when the target sentence is embedded in a longer context. To explore this, we augment minimal pairs from BLiMP, SyntaxGym, and CrowS‑pairs by prefixing each pair with longer sentences extracted from the same dataset (matched prefix), a different subset of the same dataset (mismatch), or an unrelated domain such as Wikipedia. Results show that when the prefix is drawn from the same linguistic phenomenon, MPP scores shift dramatically—either increasing for acceptable prefixes or decreasing for unacceptable ones—while arbitrary Wikipedia contexts leave judgments largely unchanged. Perturbation analyses confirm that these effects are not due to superficial lexical changes but reflect sensitivity to shared syntactic and semantic features across the context. Our findings indicate that LMs encode latent structural knowledge that can be triggered by context, and that conventional short‑sentence MPP evaluations may underestimate a model’s abstract linguistic competence over extended contexts. The full paper details our experimental setup, analyses, and implications for LM evaluation.</sample>
    <sample id="109">**Abstract**  
Instruction‑tuning enables language models to solve novel tasks in a zero‑shot setting, yet existing datasets are limited to pre‑existing benchmarks.  In this work we present **Unnatural Instructions**, a large‑scale dataset of natural language instructions, inputs, and outputs that is generated entirely without human annotation.  Beginning with a small manually curated seed from the Super‑Natural Instructions corpus, we prompt a GPT‑3 variant to produce a new instruction–input pair.  The model is then asked to generate the corresponding output, and subsequently to produce paraphrases of the instruction, yielding 64 k unique examples (≈240 k when paraphrases are included).  Analysis shows that more than half of the examples are correct, and even incorrect samples often contain useful signals for instruction tuning.  The dataset covers a wide spectrum of tasks, including creative and domain‑specific problems (e.g., assessing experimental design, inventing new words), far beyond conventional NLP benchmarks.  Fine‑tuning an 11‑billion‑parameter T5 on Unnatural Instructions outperforms strong baselines (T0++, Tk‑instruct) on Super‑Natural Instructions, T0, BIG‑Bench Hard, and LMentry, and remains superior when the generation cost is amortized.  Unnatural Instructions demonstrates that large language models can autonomously produce diverse, high‑quality instruction data at scale, offering a rapid, low‑cost alternative to costly crowd‑source annotation.</sample>
    <sample id="111">The authors build a trigger set by first collecting a general text corpus, counting how often each word appears in that corpus, and then selecting the words that fall into a **moderate‑frequency interval** (i.e., neither very rare nor very common).</sample>
    <sample id="114">**Abstract**

Large language models dominate natural‑language processing but suffer from excessive parameter counts, long training times, and token‑intensive corpora. In particular, multi‑head self‑attention (MHSA) often contains redundant heads that can be pruned without degrading performance. We propose **Grouped‑Head Transformer (GHT)**, a two‑stage framework that compresses MHSA by first enforcing structured head relationships and then selectively retaining a single representative head per group.  

In the **group‑constrained training** phase, heads are partitioned into groups and optimized with a hybrid loss: a *homogenization* term that encourages intra‑group similarity and a *diversification* term that promotes inter‑group separation. This yields compact head clusters while preserving expressivity.  

The subsequent **Voting‑to‑Stay** algorithm aggregates head importance scores over training batches, assigning votes to each head. Heads that receive insufficient votes are pruned, leaving a single head per group.  

Evaluations on machine translation, language modeling, and abstractive summarization demonstrate that GHT and its pruned variant GHT‑PS exceed state‑of‑the‑art baselines by 3.8–7 % in BLEU and ROUGE, while compressing 16.9–32.1 % of parameters. In the most aggressive setting, a compressed model (LITE) achieves 90 % parameter reduction, 62 % faster inference, and 80 % fewer FLOPs without sacrificing accuracy.  

Future work will explore task‑specific automatic pruning guided by the Lottery Ticket Hypothesis to further tailor large models to real‑world applications.</sample>
    <sample id="115">The approach uses a **20‑frame speech segment** (≈ 0.8 s at 40 ms per frame).</sample>
    <sample id="116">The entity‑specific fact needed is that **Servin is a judge** (i.e., the role of Servin).</sample>
    <sample id="117">Example quality is the more important factor.</sample>
    <sample id="118">**Abstract**  
Code‑switching, the interleaving of multiple languages within a single utterance, is pervasive in multilingual societies such as India. Existing multilingual pre‑trained models (mBERT, XLM‑R) underperform on code‑switched downstream tasks (e.g., sentiment analysis, question answering) because they are trained on monolingual data and do not explicitly model language transitions. In this work we introduce *SwitchMLM*, a masked‑language‑model objective that selectively masks only tokens at language‑switch boundaries (switch‑points). Since gold language‑identification (LID) tags are often unavailable, we also propose *FrequencyMLM*, a surrogate LID method that assigns language tags based on negative log‑likelihood comparisons across monolingual corpora.  

To better propagate switch‑point information through the network, we augment BERT with residual connections from intermediate layers (identified via probing to encode substantial switch‑point knowledge) to the final layer, and we add an auxiliary LID‑based loss at the intermediate layer. These architectural changes, termed *ResBERT*, encourage the model to encode language‑identity cues explicitly.  

Experiments on Hindi–English sentiment analysis and question answering demonstrate that the combined Switch/FrequencyMLM objective with ResBERT and auxiliary loss yields the best performance across all language pairs. Probing classifiers confirm that our methods increase the amount of switch‑point information in both intermediate and final representations. Overall, our approach delivers a principled, data‑efficient solution for pre‑training code‑switched language models.</sample>
    <sample id="119">The extended experiments focus on **GPT‑style models (e.g., GPT‑4 and the broader GPT series), BART‑style models (the BART series and its variants), and RoBERTa** (including checkpoints further pretrained on partisan corpora).</sample>
    <sample id="120">The EDAtt strategy uses the cross‑attention scores from a single, specific layer – typically the final encoder‑decoder cross‑attention layer – rather than combining scores from multiple layers.</sample>
    <sample id="121">Direct inference examples are the explicit, unambiguous ways the user picks an entity—e.g., naming it outright (“Easy on Me”) or by its position in the list (“the first one”).</sample>
    <sample id="122">The authors are affiliated with **Fudan University**.</sample>
    <sample id="123">**Abstract**  
Recent advances in large language models (LLMs) have highlighted instruction tuning as a powerful paradigm for zero‑shot generalisation. While most studies focus on language‑only tasks, the impact of instruction tuning on multimodal learning remains underexplored. In this work, we introduce **MultiInstruct**, the first large‑scale multimodal instruction‑tuning benchmark, comprising 62 diverse tasks across ten broad categories. These tasks are derived from 21 open‑source datasets, each accompanied by five expert‑written instruction templates, and are represented in a unified sequence‑to‑sequence format using the OFA pre‑trained model’s shared token space.

We conduct extensive experiments with the OFA‑Large model, training on 53 tasks (10 k instances per task) and evaluating on unseen common‑sense reasoning, visual‑question‑answering, and miscellaneous tasks, as well as 20 natural‑language tasks. For classification, accuracy is reported; for generation, ROUGE‑L is used; and we introduce a novel **sensitivity** metric measuring consistency across instruction variants. Results demonstrate that instruction tuning markedly improves OFA’s performance on seen multimodal tasks, and that transfer learning from natural‑language instruction datasets yields further gains in both performance and reduced sensitivity. Moreover, training with multiple instruction templates enhances robustness. We also outline an ongoing effort to expand the dataset by 150 additional vision‑language tasks. The dataset, code, and models will be publicly released via a QR code.</sample>
    <sample id="124">**Abstract**

Temporal reasoning is a core capability of large language models (LLMs) yet remains inadequately benchmarked. We decompose temporal reasoning into three levels: (L1) time‑to‑time (e.g., “What is the year after 2010?”), (L2) time‑to‑event (e.g., “Which team did Lionel Messi play for in 2010?”), and (L3) event‑to‑event (e.g., “Which team did Messi join after FC Barcelona?”). Prior work has focused mainly on L2; we introduce **TempReason**, a comprehensive benchmark covering all three levels with extensive historical coverage. L1 questions are expanded from year to month prediction, while L2 and L3 pairs are automatically constructed from Wikidata and Wikipedia.

We evaluate closed‑book, open‑book, and a novel “Reasoning‑QA” setting where all relevant temporal facts are supplied to the model. Baseline experiments with T5‑L, FLAN‑T5‑L, and ChatGPT reveal a strong bias toward the 2000‑2020 period and significant degradation on month‑level L1 tasks. To mitigate this, we propose a two‑stage training paradigm: (1) **Temporal‑Span Extraction Pre‑training** that reconstructs masked temporal and entity spans, and (2) **Time‑Sensitive Reinforcement Learning** that rewards correct temporal predictions and penalizes temporally inconsistent outputs. The resulting model, **TempT5**, outperforms both fine‑tuned T5 and instruction‑tuned LLMs across all settings, especially on L2/L3 reasoning. However, residual performance fluctuations across time periods highlight remaining data‑imbalance biases. Future work will focus on reducing these temporal biases to further enhance LLM reasoning.</sample>
    <sample id="125">Only one author is mentioned in the presentation – Yanis Labrak.</sample>
    <sample id="126">Yes – the “Translate‑Test” setting is a baseline that first translates the query with a machine‑translation system (e.g., Google Translate) and then applies a monolingual semantic‑parsing model to the translated text.</sample>
    <sample id="127">**Abstract**

Chain‑of‑thought (CoT) prompting enables large language models (LLMs) to solve multi‑step reasoning tasks, yet the technique is limited to models with billions of parameters due to their computational cost. We propose a simple yet effective distillation framework that transfers the CoT reasoning ability of a large teacher LLM to a small student model (&lt; 1 B parameters). For each benchmark question, a zero‑shot CoT prompt is issued to the teacher; if the final answer is correct, the entire step‑by‑step reasoning is reformatted into a training pair. To enrich the student’s exposure, we introduce **Diverse Reasoning**, generating multiple distinct solution traces per question by sampling the teacher at high temperature. The student is fine‑tuned to produce a CoT response followed by the answer. Across 12 reasoning benchmarks (including arithmetic, data‑interpretation, and symbolic tasks), the distilled student attains up to 55 % accuracy on Multi‑Arith and markedly outperforms prompt‑based baselines and vanilla fine‑tuning, even with a 0.3 B‑parameter model. Scaling experiments reveal that performance benefits from larger teacher models, more training data, and increased student capacity, highlighting clear trade‑offs between development‑time costs (teacher inference, dataset creation) and inference‑time efficiency. Our open‑source code, data, and trained checkpoints demonstrate that emergent reasoning abilities can be transferred to practical, resource‑constrained LLMs, offering a scalable path toward deployable reasoning systems.</sample>
    <sample id="128">**Abstract**

We introduce *KITMUS*, a diagnostic test suite that probes the ability of natural language understanding models to integrate knowledge from multiple sources—namely, pretrained parameters and inference‑time context. Coreference resolution is employed as the probing task because resolving a pronoun often requires both entity‑specific information (e.g., “Servin is a judge”) and background knowledge (e.g., “judges decide cases in law courts”). KITMUS systematically varies the availability of these two knowledge types across three settings: (1) *Background‑Pretrain*, where background facts are assumed to be encoded in the model’s parameters; (2) *Background‑Both*, where background facts are provided at both pretraining and inference time; and (3) *Background‑Inference*, where background knowledge is unavailable at pretraining and must be supplied only at inference time (e.g., novel occupations). We constructed examples that isolate surface cues, ensuring that performance depends on genuine knowledge integration. Human participants and state‑of‑the‑art coreference models (C2F, BERT4Coref) were evaluated on these settings. Without KITMUS training, models performed near chance, highlighting reliance on surface heuristics. Task‑specific training on KITMUS markedly improved performance, yet even the best models struggled with inference‑time–only background facts. These results demonstrate that current coreference systems lack robust multi‑source knowledge reasoning, and that targeted training can partially alleviate this limitation. The KITMUS dataset and code are publicly available for further research.</sample>
    <sample id="129">They gave **black women** as an example of a marked group.</sample>
    <sample id="130">The paper found that **non‑transformer architectures** (e.g., traditional RNN/LSTM‑based taggers, smaller models, or other older architectures) do **not generalize well** to the newer CoNLL++ data. In contrast, transformer‑based models and larger model sizes showed better generalization.</sample>
    <sample id="131">The paper evaluates its methods on two standard image‑classification benchmarks: **CIFAR‑10** and **CIFAR‑100**. These are the clean test sets used to report the final performance numbers.</sample>
    <sample id="132">Two authors – Akshatha and Martin.</sample>
    <sample id="133">The authors work with multiple modalities (both text and vision) rather than text alone.</sample>
    <sample id="135">**Abstract**  
This work introduces ABC‑Eval, a dimensional evaluation framework for conversational AI that explicitly annotates discrete behavioral phenomena in dialogue responses. Developed by the Emory NLP Lab and Amazon Alexa AI, ABC‑Eval targets behaviors such as irrelevance, self‑contradiction, partner contradiction, hallucination, commonsense violation, and empathy. The authors evaluated four state‑of‑the‑art chat models on 100 human‑bot conversations per model, comparing ABC‑Eval to three conventional methods: turn‑level Likert ratings, dialogue‑level Likert ratings, and pairwise dialogue comparisons across eight standard quality aspects. Inter‑annotator agreement on ABC‑Eval labels was substantially higher than on existing metrics. Linear regression analyses showed that proportions of contradictory or self‑contradictory turns explain 5‑10 % of overall conversation quality, surpassing the predictive power of Likert consistency scores (≤ 4 %). A stepwise regression revealed that the full set of ABC‑Eval metrics accounts for over 25 % of variance in perceived quality, whereas all Likert metrics together explain far less, and many fail to capture unique information. ABC‑Eval quantified error rates in contemporary models: 20 % commonsense violations, 15 % irrelevant replies, and 10 % contradictions. These findings demonstrate that behavior‑labeling yields reliable, informative, and distinct metrics, enabling finer‑grained assessment of chat systems and guiding future improvements.</sample>
    <sample id="136">**Abstract**

Numerical reasoning is critical for many downstream NLP applications such as fact‑checking and information‑retrieval, yet existing benchmarks largely rely on coarse metrics (accuracy, F1) that mask a model’s strengths and weaknesses in mathematical reasoning. In this work we introduce **FERMAT** (Flexible Evaluation set for Arithmetic and Mathematical Tasks), a diagnostic suite derived from CommonCore and Illinois math problems, designed to probe a model’s ability to understand numbers, perform arithmetic operations, and generalize across diverse training regimes. FERMAT expands each prompt by systematically varying numeric representations (small integers, large integers, decimals) and operations (single operations, nested operations), enabling a nuanced analysis of model performance beyond overall accuracy.

We first evaluate a range of instruction‑tuned language models on FERMAT in a zero‑shot setting, revealing that models with ≥10 B parameters outperform smaller ones, yet all struggle with 3‑B models. We then fine‑tune models on 200 k teacher‑generated templates that replace concrete numbers with placeholders, allowing large‑scale data augmentation. This fine‑tuning yields substantial gains across all numeric regimes. Further experiments assess “training dependency” by testing whether models recall exact arithmetic expressions seen during training; results show significant improvement but still sub‑50 % accuracy, indicating limited memorization. Finally, we demonstrate that incorporating diverse template styles from GSM8K and AQUA enhances performance more than merely scaling data volume.

FERMAT thus offers a richer, more informative alternative to single‑score benchmarks, highlighting the importance of linguistic and mathematical diversity, as well as number encoding and tokenization strategies, for advancing numerical reasoning in language models.</sample>
    <sample id="137">**Abstract**  
Designing interior layouts often requires close collaboration between non‑experts and architects. We introduce **Tell2Design**, the first large‑scale benchmark for *language‑guided floor plan generation*, where a model must translate detailed natural‑language specifications into a 2‑D floor plan. Each sample consists of a set of human or template‑generated instructions that describe room semantics, geometry, and topological relations, and a target sequence of room bounding boxes (type, center coordinates, height, width). The dataset contains 5,051 crowd‑sourced instruction sets and 76,000 template‑derived ones, each averaging &gt;200 words. Three challenges arise: (1) tight design constraints; (2) extracting a coherent global layout from long, fuzzy documents; and (3) handling ambiguous or incomplete instructions. We cast the task as sequence‑to‑sequence learning with a transformer encoder‑decoder initialized from T5, treating the instruction text as the input and the bounding‑box sequence as the output. Trained on both synthetic and human data, the model achieves a Micro‑IoU of 54 % and Macro‑IoU of 53 % on the test split, outperforming text‑conditional image generation baselines by a large margin. Warm‑up with synthetic instructions improves performance by &gt;10 % IoU, indicating complementary value across instruction types. Our work establishes a foundation for future research in language‑guided design generation.</sample>
    <sample id="138">The authors argue that **integrating knowledge from multiple sources—specifically combining pre‑training‑time knowledge with inference‑time (context‑provided) knowledge—is an understudied area in natural language understanding.**</sample>
    <sample id="139">The speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes. For the validation and test sets, the authors had crowd‑sourced workers review and correct any incorrect samples, ensuring quality in CoScript.</sample>
    <sample id="141">Existing resources for evaluating context‑dependent translation are narrow: they cover only a handful of discourse phenomena (e.g., pronouns, lexical cohesion, formality) and are hand‑curated, so they cannot generalise to other types of context dependence. Moreover, they support only a limited set of languages, making cross‑lingual or large‑scale evaluation difficult.</sample>
    <sample id="143">The paper compares its EDAtt strategy against the following existing SimulST policies:  

1. **Wait‑k**  
2. **Local Agreement**  
3. The **state‑of‑the‑art simultaneous pre‑translation architecture** specifically designed for SimulST.</sample>
    <sample id="144">The authors are affiliated with the University of Nantes and the Nantes University Hospital (CHU de Nantes).</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">**Abstract**

Dialogue summarization aims to produce concise, faithful summaries of conversational data, yet current large‑scale pretrained models frequently omit crucial information, resulting in incomplete and potentially misleading outputs. To quantify this “omission” problem, we analyze summaries generated by six state‑of‑the‑art models across five domains and find that roughly 70 % contain omissions, with omitted content appearing randomly throughout dialogues. Motivated by this finding, we introduce **OLDS (Omission Labels for Dialogue Summaries)**, a large, high‑quality dataset constructed from five existing dialogue‑summarization benchmarks. For each dialogue, we generate diverse candidate summaries using multiple abstractive models and decoding strategies, then automatically label utterances that are missing in the candidate but present in the gold reference. Human evaluation confirms the reliability of these labels.

We evaluate three baseline omission‑detection frameworks—pair‑wise classification, sequence labeling, and a pointer‑network—using precision, recall, F1, and a word‑level recall metric (WR). Results show F1 scores near 50 %, highlighting the task’s difficulty and the severe class imbalance in the data. Finally, we demonstrate that incorporating detected omissions as a post‑editing step significantly improves summary quality, underscoring the practical value of omission detection for enhancing dialogue summarization. The OLDS dataset and code are publicly released for future research.</sample>
    <sample id="147">Three.</sample>
    <sample id="149">The text does not state that the CoNLL++ dataset is released or publicly available.</sample>
    <sample id="150">**Abstract**  
Meeting transcripts constitute a vast, under‑exploited NLP domain, characterized by long, domain‑specific, information‑dense documents. Existing research has largely focused on summarization and action‑item extraction, neglecting the intrinsic question‑answering (QA) component inherent to meetings. To address this gap, we introduce **MeetingQA**, an extractive QA dataset derived from the AMI meeting corpus (≈100 h of multi‑party transcripts). We collect naturally occurring questions posed by participants, filter out trivial cases, and annotate answer spans with sentence‑level labels, achieving a Krippendorff’s α of 0.73. The dataset contains 7.7 k questions (30 % unanswerable) split into train, dev, and test sets, with 40 % multi‑span and 48 % multi‑speaker answers. Questions average 12 words, answers 35 words, and a large portion are yes/no or opinion‑seeking, with 20 % rhetorical and 70 % involving speaker disagreement. Human performance reaches an F1 of 84.6. We evaluate a range of methods: short‑context retrieval, single‑span and multi‑span models, and silver data augmentation using automatically labeled MediaSum interview questions. Fine‑tuned models lag human performance by ~25 F1 points; short‑context RoBERTa slightly outperforms long‑context Longformer. Zero‑shot results show a ~50 F1 gap, mitigated by silver data and instruction‑tuned FLAN‑T5. Error analysis reveals difficulty with rhetorical questions and speaker attribution. MeetingQA remains a challenging benchmark for both fine‑tuned and zero‑shot QA systems.</sample>
    <sample id="152">**Abstract**  
We present a suite of large language models tailored to classical philology, addressing gaps in existing resources for Ancient Greek and Latin. Building on the success of monolingual BERT variants, we pre‑train two encoder‑only and two encoder‑decoder models: GreBERTa and GreTa for Greek, and their multilingual counterparts PhilBERTa and PhilTa for Greek, Latin, and English. To overcome the scarcity of high‑quality Greek corpora, we construct a novel pre‑training dataset by mining the Internet Archive: Greek texts are identified via OCR‑erroneous stop‑words and re‑processed with Greek‑enabled OCR, yielding a clean corpus. Latin data are sourced from Corpus Corporum, while English texts are thematically linked to antiquity. Models are evaluated on Universal Dependencies (Greek) and EvaLatina 2022 (Latin) across POS tagging, dependency parsing, and lemmatization. GreTa’s encoder alone initially underperforms but converges to native encoder performance after additional training; its encoder‑decoder architecture yields a 5‑point gain in Greek lemmatization, surpassing prior state‑of‑the‑art. Multilingual pre‑training does not significantly outperform monolingual models on semantic or world‑knowledge benchmarks, suggesting limited cross‑linguistic transfer for these tasks. Our contributions include: (1) high‑quality Greek pre‑training data, (2) new monolingual and multilingual models with native tokenizers, (3) comprehensive benchmarking against existing systems, and (4) analysis of T5 encoder behaviour in classical language settings. These models provide a robust foundation for future NLP research in classical philology.</sample>
    <sample id="153">**Abstract**  
Text‑to‑image generative models often misinterpret ambiguous user prompts, leading to images that do not reflect the intended scene. In this work, we investigate the nature of prompt ambiguities and propose systematic methods for both resolving them and evaluating the fidelity of generated images. We first curate a benchmark dataset derived from the LAVA corpus, categorizing prompts by five distinct ambiguity types (e.g., prepositional attachment, quantifier scope, referent ambiguity). A language‑model‑based disambiguation pipeline then augments each prompt with external signals: (1) *clarifying questions* generated via in‑context learning, answered by the user; or (2) *candidate visual interpretations* produced by the model, to which the user selects the intended scenario. The resulting disambiguated prompt is obtained by concatenating the user’s response to the original text.  

To assess faithfulness, we generate images for both the ambiguous and disambiguated prompts using a state‑of‑the‑art text‑to‑image model. A visual question‑answering (VQA) system receives the image and a question derived from the user’s intended meaning; a “Yes” answer indicates that the image satisfies the intention. Our experiments reveal that disambiguation improves faithfulness across most ambiguity types, albeit with varying efficacy, and that the automated VQA‑based evaluation correlates strongly with human judgments. These findings demonstrate that systematic prompt refinement can mitigate ambiguity in text‑to‑image generation and provide a reliable, scalable evaluation framework.</sample>
    <sample id="154">The paper’s authors are affiliated with:

- **University of Trento** (Trento, Italy)  
- **Fondazione Bruno Kessler (FBK)** (Trento, Italy)</sample>
    <sample id="155">Javad Hosseini.</sample>
    <sample id="157">**Abstract**

Dialogue summarization seeks to distill salient information from multi‑party, semi‑structured conversations into concise, coherent summaries. Existing approaches typically rely on external linguistic tools—discourse parsers or dialogue state trackers—to pre‑compute a static graph that encodes utterance relations. These methods suffer from error propagation and a lack of task‑specific adaptivity. In this work, we propose **SDDS (Static‑Dynamic Structure Fusion Graph)**, a unified framework that jointly learns static dialogue structure and dynamic semantic relations for end‑to‑end summary generation.

SDDS first encodes each utterance with a transformer‑based utterance encoder. Four heuristic procedures construct a static graph: (1) **Discourse Parsing Graph** via dependency‑based discourse trees; (2) **Key‑Co‑occurrence Graph** capturing lexical overlap; (3) **Speaker Interaction Graph** derived from sliding‑window speaker frequency; and (4) **Position Graph** using relative utterance distances. The adjacency matrices of these graphs are treated as separate channels and fused with a 1×1 convolutional layer.

A **Dynamic Graph Module** then learns utterance relations directly from deep representations using multi‑head attention, bypassing external tools. The static and dynamic relations are combined into a unified graph, which is integrated into a pre‑trained language model generator through a dual cross‑attention mechanism. Experiments on benchmark dialogue summarization datasets demonstrate that SDDS outperforms prior state‑of‑the‑art baselines, highlighting the benefits of jointly modeling static linguistic cues and task‑adaptive dynamic semantics. The code and data are publicly released on GitHub.</sample>
    <sample id="158">**Abstract**

Coreference resolution aims to identify and cluster all mentions of entities within a text. Conventional neural models enumerate all mention pairs, incurring quadratic time and memory costs. Recent cache‑based approaches mitigate this by maintaining a fixed‑size cache of candidate entities, but use a single eviction policy (typically Least‑Recently‑Used, LRU). In long documents, topical shifts cause entity mentions to be widely dispersed; LRU then suffers frequent cache misses, especially for high‑frequency entities that appear globally.  

We introduce **Dual Cache**, a two‑tier caching scheme that separates local and global entity management. A *local cache* stores recent, locally‑bound entities and employs LRU eviction. A *global cache* tracks long‑range, frequently mentioned entities using a Least‑Frequently‑Used (LFU) policy. As the document is processed left‑to‑right, each new mention is first classified as either a new entity or one already in the cache. Its updated frequency determines whether it is inserted into the global cache (if highly frequent) or into the local cache otherwise. When a cache is full, the appropriate eviction policy is triggered.

We evaluated Dual Cache on four public coreference benchmarks (LitBank, OntoNotes, WikiCoref, and a 30‑k‑word book). Across all datasets, Dual Cache outperformed single‑cache baselines—even those with unbounded memory—while achieving higher speed and a superior performance‑to‑cost ratio. In the book‑level scenario, the performance gap widened markedly, and cache miss rates dropped significantly. Dual Cache therefore offers an efficient, cost‑effective solution for neural coreference resolution in long documents.</sample>
    <sample id="160">It maps each input token to an **unordered multiset of output‑side tokens** that will be needed in the final logical form.</sample>
    <sample id="161">CoScript contains **55,000 scripts** (one script per generated specific goal).</sample>
    <sample id="163">The best alignment method for DEPLAIN is **MASSalign**.</sample>
    <sample id="164">**Benefit of weakly supervised learning**  
It lets you train models on large amounts of data using inexpensive, noisy “weak” signals (heuristics, knowledge bases, crowdsourcing) instead of costly manual annotations, thereby scaling learning while keeping labeling effort low.</sample>
    <sample id="165">**Abstract**  
Abductive commonsense reasoning seeks a plausible explanation that bridges a given context \(X\) and an observed outcome \(Y\). Traditional approaches rely on supervised annotation of explanations, yet crowd workers disagree on the plausibility of over 60 % of explanations in large datasets, motivating a fully unsupervised method. We propose **LiPoR** (Likelihood Learning with Posterior Regularization), which treats the set of candidate explanations \(Z\) as a latent variable. The core objective maximizes the marginal likelihood \(\log p(Y\mid X)=\log\sum_{z}p(Y\mid X,z)p(z\mid X)\), thereby avoiding explicit plausibility labels. To guide the model toward a realistic subset of explanations, LiPoR incorporates a regularizer \(\Omega\) that exploits the inherent mutual exclusivity of explanations: if \(Z\) contains many competing hypotheses, their posterior entropy will be high. \(\Omega\) is defined as \(\max\{H(p(z\mid X,Y)),\log M\}\), where \(M\) is the presumed number of plausible explanations. When the entropy exceeds \(\log M\), the regularizer penalizes the posterior, encouraging a sparser, more mutually exclusive explanation set. Experiments on the AlphaNLI dataset demonstrate that LiPoR surpasses all zero‑shot baselines—including a strong GPT‑3 model—and outperforms the prior state‑of‑the‑art unsupervised approach by over 4 % absolute accuracy. Our results confirm that mutual‑exclusivity constraints can effectively substitute costly supervision in abductive commonsense reasoning.</sample>
    <sample id="166">**Abstract**  
Retrieving images from linguistically complex text—long, detailed descriptions that refer to highly similar visual content—poses a significant challenge for existing visual‑language models, whose analogical reasoning capabilities deteriorate under such demands. We propose the **Neural Divide‑and‑Conquer Reasoning (NDCR)** framework, inspired by the divide‑and‑conquer strategy and the dual‑process theory of human cognition. NDCR decomposes a complex proposition into a set of simpler propositions via a **Proposition Generator** that leverages a BART decoder to produce interpretable sentences. A **Visual‑Linguistic Interactor** (System 1) performs analogical inference, producing matching scores and intermediate reasoning states for each proposition. Complementing this, a **Neural‑Symbolic Reasoner** (System 2) applies logical operations: a **Negation Executor** derives negated reasoning states, and a **Conjunction Operation** aggregates positive and negated states to infer the final answer. The final image retrieval decision is obtained by fusing the outputs of both systems. Extensive experiments on benchmark datasets demonstrate that NDCR surpasses state‑of‑the‑art baselines, and ablation studies confirm the necessity of each module. Notably, NDCR can expose intermediate inference states, offering interpretability and facilitating further analysis. Our work illustrates how neural and symbolic reasoning, orchestrated through a divide‑and‑conquer approach, can effectively tackle complex visual‑text retrieval tasks.</sample>
    <sample id="167">All 750 documents in DEPLAIN‑web were aligned twice: each document received a manual alignment and, separately, an automatic alignment. In other words, the full set of 750 documents was used for both manual and automatic alignment.</sample>
    <sample id="168">The CoNLL++ dataset was built by taking Reuters news articles from 2020 and annotating the text with the same entity‑labeling scheme (BIO tags, entity types) used in the original CoNLL‑2003 shared task.</sample>
    <sample id="169">**Abstract**

We present a systematic investigation of large‑language‑model prompting for machine translation, focusing on PaLM, a 540 billion‑parameter model trained on 780 billion tokens. Using WMT benchmarks and modern neural evaluation metrics (BLEURT, BLEU, etc.) we compare PaLM’s translation quality against leading commercial and research systems while ensuring no overlap between test data and PaLM’s training corpus. Our experiments demonstrate that the choice of prompt has a substantial effect on performance: one‑shot prompting can change BLEURT scores by more than one point, and in extreme cases by up to 40 points. We therefore adopt a 5‑shot strategy in which each source sentence is prefixed with a language tag (e.g., “German: …” → “English: …”). In this regime, the specific wording of the prompt is largely irrelevant; the quality of the demonstration examples dominates. Selecting high‑quality, curated examples (e.g., from WMT dev sets) yields higher performance than using raw training data. Despite narrowing the gap, PaLM still lags behind specialized state‑of‑the‑art MT systems, although it approaches commercial baselines in fluency. Human evaluation with the MQM framework reveals that PaLM’s translations are fluent but suffer from accuracy issues, notably omission errors, whereas style‑related errors are less frequent than in competing systems. These findings highlight the importance of prompt design and example selection for LLM‑based translation and provide actionable guidance for practitioners.</sample>
    <sample id="171">The paper does not enumerate specific prior works. It only notes that “existing works can be broadly classified into four categories,” but these categories are not detailed and the methods in each are deemed either unsuitable for embedding‑as‑a‑service or lacking transferability.</sample>
    <sample id="172">No. Our experiments showed that large multilingual LLMs like Codex and BLOOM still fall short on cross‑lingual semantic parsing, so they are not sufficient for CLSP.</sample>
    <sample id="174">**Abstract**

ArgAnalysis35K is the largest publicly available dataset for automatic argument quality assessment, comprising 35,000 argument–analysis pairs with human‑rated quality scores on a 0–1 scale. Unlike prior corpora that rely on limited crowdsourced data, ArgAnalysis35K sources 85 % of its arguments from high‑quality speeches, expert and intermediate debaters, and only 15 % from novices, thereby ensuring superior linguistic quality. The dataset covers 24 thematic domains derived from parliamentary debate practice and online resources, and for each theme it aggregates multiple motions, yielding a richer and more diverse set of argumentative contexts than the 30–40‑motion datasets that dominate the field.

A novel contribution is the *analysis* field, which captures the internal structure of an argument—claims, premises, or a combination—providing a deeper explanatory layer beyond the surface claim. Another key innovation is *instance‑based annotator reliability*, which models annotator bias at the level of individual arguments rather than discarding entire workers, enabling more nuanced aggregation of quality judgments. Finally, ArgAnalysis35K introduces a *relevance model* that assigns a 0–1 score to each argument–theme pair, acknowledging that arguments can be reused across multiple motions (e.g., “accountability” applies to governments, corporations, and schools).

Together, these features—scale, quality, thematic diversity, structural analysis, fine‑grained reliability, and relevance scoring—make ArgAnalysis35K a comprehensive resource for advancing research on argument quality analysis and related NLP tasks.</sample>
    <sample id="175">The model learns a *soft* permutation rather than trying to enumerate all possible orderings.  
During training it jointly induces the hidden alignment between input tokens and the multiset tokens, then uses a GPU‑friendly continuous relaxation of the permutation matrix. This relaxation lets the model score all permutations, back‑propagate the best‑scoring (most linguistically plausible) one, and gradually learn to pick the correct ordering even when several permutations are consistent with the data.</sample>
    <sample id="176">**Fairness of a downstream NLP model is defined as the absence of systematic, harmful differences in the model’s predictions or performance across socially relevant groups (e.g., political leanings, demographics).**  

In practice, this means that the model should:

1. **Deliver comparable accuracy, precision, recall, etc., for each group** – so no group (e.g., left‑leaning or right‑leaning, minority or majority) is consistently under‑ or over‑predicted.  
2. **Avoid biased decision thresholds** that disproportionately flag or ignore content for certain groups (e.g., hate‑speech detection that is more sensitive to minority‑targeted speech for left‑leaning models but not for right‑leaning ones).  
3. **Maintain consistent predictive behavior** regardless of the group identity or political stance of the text or its author, ensuring that downstream applications do not marginalize or mislabel any group.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">**Abstract**  
Theory of Mind (ToM) reasoning—predicting the mental states of others—is a critical benchmark for language models, yet large LLMs often fail on classic false‑belief tasks such as the Sally‑Anne test. This study introduces **SymbolicToM**, a plug‑and‑play inference‑time framework that augments any off‑the‑shelf LLM with explicit symbolic belief graphs. For a given narrative, SymbolicToM constructs multi‑level belief graphs (e.g., \(B_{\text{Bob}}\), \(B_{\text{Bob,Alice}}\)) that encode first‑ and second‑order mental states by leveraging pretrained NLI and OpenIE systems. During inference, a query is parsed, the relevant belief graph retrieved, and the question transformed into a factual query over the graph; the resulting facts are then fed to the base LLM to produce the final answer.

Extensive experiments on the ToMi benchmark demonstrate that SymbolicToM yields substantial accuracy gains across a spectrum of models—from GPT‑3 to Flan‑T5‑XXL—improving second‑order false‑belief performance by up to 67 accuracy points. To assess generalization, we constructed three out‑of‑domain datasets (D₁–D₃) that stress story‑structure and linguistic diversity, and a paraphrased version of ToMi. While supervised baselines degrade sharply (≈50 % accuracy), SymbolicToM consistently enhances performance, enabling GPT‑4 to solve the challenging datasets with &gt;40 % accuracy improvement.

In summary, SymbolicToM provides a lightweight, interpretable augmentation that markedly boosts ToM reasoning in LLMs without fine‑tuning, achieving state‑of‑the‑art results on both in‑domain and out‑of‑domain benchmarks.</sample>
    <sample id="180">The speaker is **Myra**.</sample>
    <sample id="181">**Abstract**  
Human agents routinely plan actions by following goal‑oriented scripts that obey domain‑specific constraints (e.g., “make a chocolate cake”). While large language models (LLMs) can decompose abstract goals such as *make a cake* into stepwise scripts, their ability to respect fine‑grained constraints remains largely unexplored. We formalize the *constrained language planning* problem and evaluate a suite of LLMs on 100 manually constructed specific goals derived from 10 abstract activities via InstructGPT. Results show satisfactory semantic completeness but poor faithfulness to constraints, with performance varying markedly across constraint categories defined in WikiHow. To address this, we adopt an *over‑generate‑then‑filter* strategy: InstructGPT first produces \(K\) candidate scripts, which are then scored by cosine similarity of InstructGPT embeddings and keyword matching for the target constraint. The highest‑scoring script is retained, yielding substantial gains in both completeness and faithfulness.  

Recognizing the cost of deploying LLMs, we create *CoScript*, a dataset of 55 k specific goals paired with high‑quality scripts generated by the above pipeline. Crowd workers refine a validation and test split, ensuring correctness. CoScript exhibits diverse constraint distribution and enables training of smaller, specialized models. Fine‑tuned T5 models trained on CoScript outperform most large LLMs on constrained planning metrics, demonstrating that appropriately distilled data can unlock superior performance in lightweight systems. Our work establishes a benchmark for constrained script generation, provides a high‑quality dataset, and illustrates effective distillation techniques for future research.</sample>
    <sample id="182">**Tropicalism** in the paper refers to the stereotype trope that portrays Latina women as “tropical”—i.e., exotic, vibrant, curvaceous, and otherwise depicted through a romanticized, exoticized lens that essentializes their identity.</sample>
    <sample id="183">The authors asked human participants to write a self‑portrait in response to the same “Imagine you are a [group]… describe yourself” prompt that they used for the LLMs, collecting those human‑written responses as the human portrayals.</sample>
    <sample id="184">They measured context usage with the **CXMI** metric, extended to a pointwise form (P‑CXMI) to evaluate context dependence at the sentence‑ or word‑level.</sample>
    <sample id="185">**DrBERT** is a RoBERTa‑based French biomedical model that was pre‑trained from scratch on the *NACHOS* corpus—web‑crawled medical text in French.  

**ChuBERT** is a clinical model that draws its training data from anonymized clinical notes from the Nantes University Hospital data warehouse.  ChuBERT also comes in a mixed version that combines 4 GB of NACHOS with 4 GB of clinical notes.  

In short: DrBERT = web‑crawled medical text; ChuBERT = hospital clinical notes (with a hybrid variant).</sample>
    <sample id="187">There are **two authors** involved in the paper: Ying and Zhiyang.</sample>
    <sample id="188">**Iterative transfer learning** is a sequential fine‑tuning strategy where a pre‑trained model is first adapted to one related task, then the resulting model is further fine‑tuned on another related task, and finally on the target task. In the paper, the authors first fine‑tune on the CE (consonance‑expansion) task, then on the debate stance task, and then on the dissonance detection data. Each step builds on the knowledge learned in the previous step, gradually transferring useful representations to the rare‑class dissonance domain.</sample>
    <sample id="189">The dataset aims to help models understand and resolve **indirect referring expressions** that users employ when selecting an entity in a conversation—i.e., to benchmark and improve entity‑selection in conversational systems.</sample>
    <sample id="190">An attacker can recover the underlying model (or a close surrogate) by treating the EaaS as a black‑box oracle:

1. **Query the service repeatedly** – send a large, diverse set of input sentences (including specially crafted trigger‑based sentences) to the embedding API.  
2. **Collect the returned embeddings** – the attacker stores the embedding vectors for each input.  
3. **Re‑train a substitute model** – using the input–embedding pairs, the attacker trains a new neural network (or performs distillation) that learns to map text to the same embeddings, effectively approximating the internal parameters of the original model.  
4. **Optionally exploit backdoors** – if the provider has injected a watermark, the attacker can use trigger‑only sentences to force the embeddings toward a target vector, giving direct clues about the model’s weight structure or enabling more efficient extraction.

Thus, by exhaustive querying and training a surrogate, the attacker can extract or closely approximate the model’s parameters through the embedding‑as‑a‑service interface.</sample>
    <sample id="191">Three authors: Sara Papi, Matteo Negri, and Marco Turchi.</sample>
    <sample id="192">**Abstract**

Large‑scale language model training relies heavily on adaptive gradient optimizers such as Adam, yet these methods incur substantial auxiliary memory because they must store first‑ and second‑moment estimates for every parameter. Memory‑efficient alternatives like Adafactor reduce this overhead by factorizing the moment matrices via non‑negative matrix factorization (NMF), but the resulting rank‑1 approximation introduces erroneous updates that slow convergence and degrade training stability. This work introduces CAME (Confidence‑guided Adaptive Memory‑Efficient Optimizer), which mitigates these errors by exploiting the residual between the predicted (momentum) and actual updates. Using the residual as a confidence measure, CAME adaptively scales the momentum term, thereby reducing the impact of approximation noise while preserving the fast convergence characteristic of Adam‑style methods. Extensive experiments on BookCorpus, English Wikipedia, and three pivotal models—BERT, GPT‑2, and T5—demonstrate that CAME achieves up to 3.4 % higher validation accuracy than Adafactor and outperforms Adam on large‑batch pre‑training, all while cutting memory usage by a factor of three relative to Adam/LAMB and surpassing SM3. Downstream evaluations confirm that BERT‑based models trained with CAME match baseline performance with reduced memory footprints. CAME thus provides a practical, scalable solution for efficient, high‑performance language model training.</sample>
    <sample id="193">Two annotators were used to create the initial dataset.</sample>
    <sample id="194">**Authors and their affiliations**

| Author | Affiliation |
|--------|-------------|
| Jenny | Carnegie Mellon University (PhD student) |
| Sebastian Santy | University of Washington |
| Ronan Le Bras | University of Washington |
| Katharina Reinecke | Allen Institute for AI |
| Maarten Sap | Allen Institute for AI |

These are the institutions explicitly mentioned in the presentation.</sample>
    <sample id="195">**Abstract**  
Explainable Question Answering (XQA) seeks not only to provide an answer but also to justify its selection. Existing approaches either translate questions into formal queries for knowledge bases (KB) or decompose them into natural‑language sub‑steps, yet both suffer from limited recall (KB incompleteness) or high ambiguity (free‑text corpora). We propose **RoHT**—*Reasoning over Hierarchical Question Decomposition Tree*—to bridge this gap by flexibly integrating heterogeneous sources during decomposition. RoHT first constructs a **Hierarchical Question Decomposition Tree (HQDT)**: the root is the original query; internal nodes are intermediate sub‑questions; leaves are atomic questions that cannot be further split. A question decomposer generates leaf questions, a generator groups them into higher‑level sub‑questions, and a certainty scorer estimates the reliability of each node.  

In the second stage, RoHT performs probabilistic reasoning over the HQDT. For each node, a scheduler selects an appropriate knowledge source (KB, text corpus, or recursive child evaluation). Executors retrieve candidate answers with associated probabilities, and an aggregator fuses these results to produce the final answer ranked by joint confidence.  

We evaluate RoHT on two complex QA benchmarks: **KQA Pro** (KB‑centric, incomplete triples + Wikipedia) and **Musique** (text‑centric with supplementary Wikidata). RoHT outperforms state‑of‑the‑art KB methods on KQA Pro, achieving substantial gains when Wikipedia is added, and surpasses the current best text model on Musique, with further improvement when KB knowledge is incorporated. These results demonstrate that explicit hierarchical decomposition with probabilistic multi‑source reasoning substantially enhances explainable, accurate answers to complex questions.</sample>
    <sample id="196">The example with the governor on the left is **“I saw Bart and Lisa.”**</sample>
    <sample id="197">The talk mentions that the authors evaluated **four state‑of‑the‑art chat models**, but it does not list their names. In general, typical state‑of‑the‑art dialogue systems that are often benchmarked include large‑scale transformer models such as **OpenAI’s GPT‑4**, **Meta’s Llama 2**, **Anthropic’s Claude**, and **Google’s Gemini** (or similar large‑language‑model‑based chat agents).</sample>
    <sample id="198">We need to test acceptability judgments over the full context window because modern language models can process much longer sequences than the single‑sentence inputs used in traditional minimal‑pair tests. Context can significantly alter a model’s probability distribution, so a model may judge a sentence acceptable when isolated but not when preceded by a longer, syntactically or semantically related prefix. Evaluating only short inputs therefore fails to capture how well the model’s abstract knowledge persists across the entire context it can actually see.</sample>
    <sample id="199">Yes. In the multilingual experiments, the English model’s accuracy dropped on most datasets—only improving in three out of nine—so multilingual training generally caused a performance decline for English.</sample>
    <sample id="200">No – annotators only know the entity names. They receive background information (search results, wiki text, images) to learn about the entities before creating indirect references.</sample>
    <sample id="201">They evaluated the translations with BLEURT (a state‑of‑the‑art neural MT metric) alongside other neural MT metrics used in the WMT evaluation.</sample>
    <sample id="202">The study did not report differential effects across entity types – the drop in performance was measured on overall F1 and no evidence was presented that specific NER categories (e.g., PER, LOC, ORG, MISC) were hit harder than others.</sample>
    <sample id="203">Positionality matters in NLP because it determines who a model or dataset serves well and who it marginalises. When the perspectives of annotators, researchers, or data sources are skewed toward certain demographics, the resulting models exhibit systematic performance gaps—e.g., better toxicity detection for English‑speaking users but poorer sensitivity to Indian contexts—leading to unfair, biased, and less inclusive systems. Understanding positionality is essential for building equitable, trustworthy NLP technology.</sample>
    <sample id="204">The paper reports that the multilingual LLMs (e.g., BLOOM, Codex, mT5, XLM‑R) were **fully fine‑tuned** on the cross‑lingual semantic parsing data, not fine‑tuned via adapters.</sample>
    <sample id="205">**Abstract**  
Large‑scale language models are pretrained on heterogeneous web‑crawl corpora that contain extensive coverage of political news outlets such as *The New York Times*, *Los Angeles Times*, *The Guardian*, *Huffington Post*, etc. While this affords exposure to diverse viewpoints, it also embeds socially biased political positions that may propagate to downstream applications. We investigate this bias‑propagation pipeline by addressing two questions: (1) How can the political leaning of a language model be quantified, and what role does its pretraining data play? (2) How do models with distinct leanings perform on socially sensitive downstream tasks, potentially inducing fairness violations?  

We adopt the Political Conference Test—an established political‑science questionnaire—to probe models under multiple prompt formats, revealing that contemporary models occupy all four quadrants of the political spectrum. GPT‑4 emerges as the most liberal, whereas BART‑based models are comparatively conservative. A controlled re‑pretraining experiment on six partisan corpora (news and social media, split by left/right leanings) demonstrates that ideological coordinates shift predictably with the training data, e.g., RoBERTa fine‑tuned on a left‑leaning Reddit corpus exhibits a substantial liberal shift. Temporal analysis further shows that models pretrained on post‑2017 data lean farther from the center, mirroring societal polarization.  

Evaluating these models on hate‑speech and fake‑news detection tasks reveals systematic fairness gaps: left‑leaning models excel at detecting hate speech against minority groups but underperform against powerful groups, while right‑leaning models exhibit the converse pattern; similar biases appear in fake‑news classification. Qualitative analyses corroborate that political leanings influence predictions across social categories.  

These findings highlight a “Scylla‑Charybdis” dilemma: sanitizing political content risks censorship, yet unfiltered data propagates bias, leading to unfair downstream outcomes. Our work underscores the urgency of addressing political biases in pretraining data to ensure equitable NLP applications.</sample>
    <sample id="206">They cold‑start the active‑learning loop with a **pre‑trained transformer that is first fine‑tuned on the CE task (binary classification of the expansion–comparison classes in the PDTB) and then fine‑tuned on the debate stance task**.  
In other words, their transfer‑learning model is a standard pre‑trained language model (e.g., BERT/RoBERTa) that is sequentially adapted to CE and then to debate before being used for dissonance detection.</sample>
    <sample id="207">The authors evaluated PaLM on the **latest WMT benchmark test sets** – i.e., the WMT 22 (and, where available, the newer WMT 23) test collections for the language pairs studied, together with the corresponding curated dev sets extracted from those same corpora. These WMT test sets were chosen specifically to avoid any overlap with PaLM’s training data.</sample>
    <sample id="208">Three.</sample>
    <sample id="209">The excerpt you shared does not contain a specific numerical value for the improvement over the strongest baseline.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Yes – the DEPLAIN corpus and the evaluation results presented are intended as a benchmark. The paper provides manually aligned sentence pairs (the gold standard) for testing alignment methods and shows fine‑tuned model scores that can serve as baseline performance metrics for future automatic text‑simplification research.</sample>
    <sample id="212">They experiment with a single smaller model – a T5 model fine‑tuned on CoScript.</sample>
    <sample id="213">The base model used is **OFA (the large variant of the unified multi‑modal pre‑trained model)**.</sample>
    <sample id="215">**Abstract**

Coordination is a fundamental syntactic phenomenon that has been modeled in a variety of dependency frameworks.  The Universal Dependencies (UD) and Mel’čuk’s meaning‑text theory treat the first conjunct as the head of the coordinate structure, whereas the Prague approach assumes a conjunctive head that governs all conjuncts.  Hudson’s Word Grammar adopts a multi‑headed scheme, with each conjunct acting as a head.  This paper presents a novel argument for symmetric coordination structures—where no single conjunct is privileged—by leveraging the principle of Dependency Length Minimisation (DLM).  Using illustrative English sentences, we show that reordering constituents can reduce total dependency length when a heavy object is placed after an adjunct, thereby explaining why seemingly ungrammatical orders (e.g., *Marge read yesterday the book*) can be acceptable.  We then analyse coordination patterns in the enhanced Penn Treebank, measuring conjunct lengths in words, syllables, and characters.  The results reveal a strong tendency for the left conjunct to be shorter when the governor is on the left or absent, a pattern that disappears when the governor is on the right.  These empirical findings challenge asymmetric head‑based models and support a symmetric treatment of coordination, consistent with DLM expectations.  The full argument and methodology are available in the accompanying paper.</sample>
    <sample id="217">**Abstract**

Controllable dialogue systems that simultaneously manipulate multiple attributes (e.g., emotion, persona, intent) are critical for practical applications, yet existing methods either focus on single attributes or rely on discrete labels, limiting their ability to generate responses for unseen attribute combinations. In this work, we investigate compositional generalization for multi‑attribute controllable dialogue generation and find that state‑of‑the‑art models lack this capability. We propose **DCG (Disentangled Controllable Generation)**, a prompt‑based framework built on DialoGPT that learns attribute concepts from seen values and employs a disentanglement loss to separate different attribute‑combination representations. Two complementary prompt types—attribute‑oriented and task‑oriented—are concatenated to guide the model, and pseudo‑combination prompts are introduced to enhance diversity. To evaluate controllability without costly labeling, we introduce **MAE (Multi‑Attribute Evaluation)**, a unified reference‑free metric that uses discrete and continuous prompts to probe coarse‑ and fine‑grained attributes. We establish two new benchmarks and demonstrate that DCG outperforms strong baselines (CTRL, fine‑tuned DialoGPT, etc.) on both controllability (E‑ACC, A‑ACC) and text equality (BLEU), with only a marginal drop on unseen attribute combinations. MAE correlates strongly with human judgments and remains robust across PLMs, confirming its effectiveness. Overall, our method enables efficient generalization from seen to unseen attribute combinations, advancing multi‑attribute controllable dialogue generation.</sample>
    <sample id="218">The authors are all from **Google AI/Google Research**—specifically, the Google Translate team.</sample>
    <sample id="219">**Abstract**

This work introduces a multi‑stage pipeline for automatically uncovering salient financial signals in the SEC Form 10‑K annual reports. The reports exhibit high lexical overlap (≈80 %) across consecutive years, motivating a comparative “highlighting” task that identifies the most informative words when contrasting a target report with its preceding‑year reference. We formalize the task as a token‑level importance prediction: given a pair of report segments (target T, reference R), the model outputs importance scores for the words in T.  

The pipeline consists of: (0) document segmentation; (1) relation recognition that classifies segment pairs into three types—highly similar, syntactically similar but semantically divergent, and mismatched; (2) out‑of‑domain fine‑tuning on eSNLI (a natural‑language‑inference dataset with token annotations); and (3) in‑domain fine‑tuning using pseudo‑labels derived from revised pairs, combined with cross‑entropy and KL‑divergence objectives to mitigate label noise.  

We release the *FINAL* dataset, comprising annotated 10‑K pairs, and evaluate on both eSNLI and FINAL. Metrics include precision (at fixed recall) and Pearson correlation (PCC). Our domain‑adaptive highlighting model outperforms baselines on FINAL while retaining generalization on eSNLI. Ablation studies show benefits on mismatched pairs, an unseen test scenario.  

Future directions include incorporating additional features from information‑retrieval techniques and exploring more sophisticated fine‑tuning strategies to further enhance effectiveness. The full paper and code are available on GitHub.</sample>
    <sample id="220">**Affiliation(s) of the paper’s authors**

All authors of *“Transfer Learning for Dissonance Detection: Addressing the Rare‑Class Challenge”* are affiliated with **Stony Brook University**.</sample>
    <sample id="221">The paper focused on **German → English** translation (i.e., the German‑to‑English language pair).</sample>
    <sample id="222">**Abstract**  
Open‑domain question answering (ODQA) systems typically rely on a retriever–reader pipeline trained on a general‑purpose corpus such as Wikipedia. When deployed in a new domain (e.g., biomedical or legal), the retriever may retrieve irrelevant passages and the reader may fail to extract correct answers, even if the target data are sparse. This work investigates systematic data‑driven interventions to enable out‑of‑domain generalization while preserving performance on the source domain. We propose two families of interventions: (1) **Zero‑shot** techniques that manipulate the interaction among question, answer, and context by generating controlled cloze‑style prompts without target examples; and (2) **Few‑shot** techniques that synthesize additional domain‑specific examples through large language models, converting generated facts into cloze questions. We evaluate these interventions on seven target datasets spanning six domains, measuring the impact on both retriever and reader accuracy.

To understand the nature of domain shift, we adapt a data‑shift taxonomy to ODQA, distinguishing **no shift**, **concept shift**, **covariate shift**, and **full shift**. We introduce a compatibility metric based on likelihoods assigned by the source retriever and reader to fixed question–answer–context triples, allowing us to map target datasets onto a 2‑D shift space. Our experiments reveal that few‑shot interventions consistently improve reader performance (up to 24 %) across all shifts, while zero‑shot techniques are particularly effective for concept and covariate shifts. These results demonstrate that tailored data interventions, guided by a principled shift analysis, can substantially enhance ODQA models’ cross‑domain adaptability.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">The experiments examined:

1. **Alignment method** – the MASSalign algorithm (the best‑performing method for aligning German simplification pairs).  
2. **Text‑simplification models** – two mBART variants:  
   * **long‑mBART** for document‑level simplification, and  
   * the standard **mBART base** for sentence‑level simplification.</sample>
    <sample id="225">Out of the 62 tasks in MultiInstruct, **53 tasks are used for training** and the remaining **9 tasks are reserved for testing**.</sample>
    <sample id="226">Two authors are involved in the paper.</sample>
    <sample id="227">**Abstract**

Grounded language understanding—mapping natural‑language utterances to executable representations in a specific environment—remains a bottleneck for contemporary language models (LMs). Existing approaches typically rely on autoregressive decoding, which often yields grammatically invalid or non‑executable plans, particularly in complex domains such as knowledge‑base query generation or robotic instruction execution. We propose **Pangu**, a framework that reframes this problem as a discrimination task. A symbolic agent proposes a set of candidate plans, and a pre‑trained LM is used only to score and rank these candidates. By decoupling plan generation from language modeling, Pangu alleviates the need for the LM to enforce syntactic or semantic validity, enabling it to focus on discriminative reasoning.

We instantiate Pangu on knowledge‑based question answering, using diverse LMs (BERT, T5, Codex) under both fine‑tuning and in‑context learning regimes. Across all settings, Pangu outperforms the autoregressive baseline ArcaneQA, achieving superior sample efficiency (e.g., &gt;50 % accuracy on GRAIL with a single in‑context demo) and robust generalization to non‑i.i.d. test distributions. Analysis of probability distributions reveals that Pangu maintains similar confidence on seen and unseen query structures, whereas autoregressive models overfit to training patterns. These findings suggest that discrimination, rather than generation, is a more effective strategy for leveraging LMs in grounded language understanding. Future work will extend Pangu to other grounding domains such as robotics and semantic search.</sample>
    <sample id="228">The authors ran experiments on four datasets: **AG News, MIND, SST‑2, and Enron Spam**.</sample>
    <sample id="229">**Abstract**

This paper investigates automatic support for argumentative writing by detecting and improving suboptimal claims. We formalize two tasks: (1) Suboptimal‑Claim Detection, which classifies a claim as either requiring revision or already optimal; and (2) Claim Improvement Suggestion, which identifies the specific quality issues that should be addressed. Leveraging revision histories from collaborative debate platforms (e.g., Kialo), we treat the final version of a claim as an optimally phrased exemplar, while all preceding versions are considered suboptimal. We address four core challenges inherent to revision‑based corpora: (i) *Representativity and Reliability*—ensuring that final versions truly reflect optimal quality rather than community oversight; (ii) *Model Complexity and Architecture*—choosing models sensitive to fine‑grained linguistic edits and evaluating the impact of pre‑training and fine‑tuning; (iii) *Contextual Dependence*—determining when and how surrounding discourse, parent claims, or domain knowledge influence quality judgments; and (iv) *Topical and User Bias*—mitigating noise introduced by controversial topics and moderator preferences. Experiments across multiple architectures demonstrate that modelling the distance between claim pairs improves suboptimal claim detection, and that contextual cues benefit improvement suggestion only for specific quality dimensions. Our findings confirm that revision‑based data is a viable resource for automatic argumentative quality assessment and provide guidance for future systems aimed at assisting writers in refining their claims.</sample>
    <sample id="231">NACHOS is a French biomedical text corpus built by crawling medical and health‑related websites. It contains raw, publicly available medical documents (≈7 GB of text) and is used as the in‑domain training data for DrBERT.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">**Abstract**  
Simultaneous speech translation (SimulST) converts spoken input into another language in real time, yet prevailing systems rely on specialised architectures and require separate models for each latency target, resulting in cumbersome training pipelines. We present **EDAtt** (Encoder‑Decoder Attention), a lightweight strategy that repurposes existing offline speech‑to‑text models for SimulST without retraining or architectural changes. EDAtt monitors cross‑attention weights between the acoustic encoder and the decoder; a target token is emitted only when the summed attention over the most recent λ acoustic frames drops below a threshold α, signalling that sufficient context has been gathered. This adaptive emission mechanism yields a single model capable of operating across a spectrum of latency regimes. Experiments on the English‑to‑German WMT’20 dataset demonstrate that EDAtt surpasses conventional SimulST strategies (Wait‑k, Local Agreement) applied to offline models and outperforms state‑of‑the‑art architectures specifically engineered for simultaneous translation, achieving higher BLEU scores at lower average lagging and computational‑aware lagging. All code, pretrained models, and simultaneous decoding outputs are released publicly to facilitate reproducibility. Our results show that attention dynamics can effectively guide real‑time translation without the need for bespoke architectures.</sample>
    <sample id="234">Prompting strategy has a **large impact** – switching prompts can change BLEURT scores by **over 1 point on average and up to ~40 points in extreme cases**. Thus, selecting a good prompt is crucial.</sample>
    <sample id="235">The authors come from four research groups:

- **Kayo Yin** – University of Toronto  
- **Patrick Fernandes** – University of Texas at Austin  
- **Emmy Liu** – University of Washington  
- **André F. T. Martins** – University of Washington  
- **Graham Neubig** – Carnegie Mellon University  

These institutions provided the institutional affiliations for the paper “When Does Translation Require Context? A Data‑driven, Multilingual Exploration.”</sample>
    <sample id="236">The presentation does not list the exact wording of the five expert‑written instructions for each task. They are mentioned as part of the dataset design, but the specific instruction templates are not provided in the transcript.</sample>
    <sample id="237">They introduce the **KITMUS test suite**—a coreference‐resolution task that systematically varies whether background facts are available only in the model’s pre‑trained parameters, only in the inference‑time context, or in both. This benchmark is designed to probe a model’s ability to integrate and use knowledge drawn from multiple sources.</sample>
    <sample id="238">**Abstract**

MeetingBank is a new benchmark dataset for meeting summarization, comprising 1,366 City Council meetings from seven U.S. cities and nearly 7,000 individual instances. Audio recordings were transcribed using Speechmatics and aligned with meeting minutes to extract summary segments and timestamps, resulting in a paired transcript‑summary corpus. Dataset statistics show an average meeting length of several hours, 3–5 speakers per meeting, and summary coverage scores between 0.7–0.9, indicating that most summaries are largely extractive. Density analyses reveal higher abstraction in Seattle and Boston compared to Denver. The test split was evaluated with both extractive (Oracle, LEAD, LexRank, TextRank) and abstractive models (BART‑Large, Pegasus, Longformer, DialogLM, HMNet). Fine‑tuned BART‑Large and DialogLM achieved the highest ROUGE‑2 scores, while Oracle extractive summarization achieved the best ROUGE‑2, suggesting the source contains most of the reference content. GPT‑3 (Davinci‑003) zero‑shot prompting performed poorly on automatic metrics but achieved superior fluency and coherence in a human evaluation of 200 instances, though it lagged in informativeness and factuality. Additional metrics (BERTScore, MoverScore, QA‑based) were explored to better reflect human judgment. MeetingBank provides a rich, publicly available resource for developing and evaluating advanced meeting summarization systems and offers insights into City Council decision processes.</sample>
    <sample id="241">**Abstract**  
Accurate and timely detection of misinformation on social media remains a challenge because most existing systems are evaluated on retrospectively constructed datasets and treat humans only as a final adjudicator.  We propose a human‑in‑the‑loop evaluation framework that addresses these shortcomings by (1) using live Twitter data, (2) integrating human feedback throughout the pipeline, and (3) measuring real‑world utility such as early detection and human workload.  Our instantiated system targets COVID‑19 treatment claims.  In the first stage, keyword filtering isolates relevant tweets, after which a T5 model is fine‑tuned to answer “What is the mentioned COVID‑19 cure?” and extract candidate claims.  Claims are ranked by trendiness via Fisher’s Exact Test and presented to human moderators for verification.  The second stage applies a BERT‑based stance classifier to flag tweets that support the verified claims; these flagged posts are then subjected to a Likert‑scale policy violation assessment.  We operationalize early detection as identifying an unapproved treatment before its first appearance in a debunking news article.  Evaluation on live data shows the system achieves 65 % accuracy in policy‑violation detection and can confirm 124.2 violations per human hour.  This framework demonstrates a realistic, end‑to‑end human‑centric approach to misinformation detection and provides a reproducible benchmark for future work.</sample>
    <sample id="242">Common human‑based evaluation methods for dialogue systems include:

* **Pairwise comparison** – judges choose which of two chats is better.  
* **Turn‑level Likert ratings** – judges rate each turn on a scale (e.g., 1‑5).  
* **Dialogue‑level Likert ratings** – judges rate the overall conversation on a scale.  

These methods are the standard practice for measuring multiple aspects of chat quality.</sample>
    <sample id="243">5 authors.</sample>
    <sample id="244">The background knowledge needed is that **judges decide cases in law courts**.</sample>
    <sample id="245">**Abstract**

We propose a two‑step pipeline for recruiting high‑agreement annotators on Amazon Mechanical Turk (MTurk) for summarization tasks. The first stage, a *Qualification Task*, assesses workers’ ability to evaluate multiple dimensions of summary quality. After pre‑task filtering (location, HIT count, approval rate), 200 participants completed a training set and a qualification set comprising three documents with an attention check and six‑dimensional summary evaluations. Workers were classified into gold, silver, bronze, or blocked; only gold and silver qualified, yielding 26 workers (8 gold, 18 silver; 13 % of the cohort). The second stage, an *Endurance Task*, tests sustained performance on 10 HITs of single‑document, four‑summary saliency judgments. Twelve workers (4 gold, 8 silver; 6 % of the cohort) passed, achieving inter‑annotator agreement (IAA) comparable to experts (Cohen’s κ ≈ 0.44, Krippendorff’s α = 0.443). A subsequent *Reference‑Based Task* (30 HITs, 1 reference, 4 candidate summaries) further evaluated information‑coverage judgments. Pipeline workers produced 8/12 fully completed HITs, with Cohen’s κ ≈ 0.53 and Krippendorff’s α = 0.534. Baseline MTurk workers using MACE achieved α = 0.380 with limited coverage, while CloudResearch workers reached α = 0.513 at lower acceptance rates. A Spearman correlation heat‑map on 50 random samples revealed strong agreement between Pipeline and CloudResearch workers, though Pipeline did not guarantee correctness training; GPT‑based judgments aligned closely with expert scores. Overall, pre‑task filtering and the two‑stage pipeline yield high‑agreement, cost‑effective annotations comparable to premium platforms, mitigating waste from low‑quality work. Limitations include English‑only summation, task‑specific design, and lack of correctness training guarantees. Future work will extend to other languages, tasks, and platforms. Funding was provided by Google.</sample>
    <sample id="246">Yes – the code (along with the KITMUS data set) is released on GitHub.</sample>
    <sample id="247">**Abstract**  
We introduce **FactKG**, the first large‑scale dataset for fact verification that uses a knowledge graph (KG) as the sole evidence source. Existing benchmarks (FEVER, VitaminC, TabFact, InfoTabs) rely on Wikipedia text or tabular data, but lack an explicit graph‑based reasoning framework. FactKG is built on DBpedia and contains 10,000 natural‑language claims in both written and colloquial styles, each labeled as *SUPPORTED* or *REFUTED*. Claims are designed to require one of five reasoning patterns: (1) **one‑hop** (a single triple), (2) **conjunction** (multiple one‑hop facts), (3) **existence** (presence of a specific relation for an entity), (4) **multi‑hop** (inference across several graph edges), and (5) **negation** (requiring a negated inference). The dataset also includes synthetic colloquial claims generated via a style‑transfer model and presupposition templates to reflect real‑world dialogue. Baselines include a *Claim‑Only* model and a graph‑aware *GEAR* model that retrieves evidence triples and performs reasoning. All baselines surpass the 51 % majority‑class baseline, with GEAR achieving the highest accuracy, demonstrating the feasibility of KG‑based fact verification and establishing a new benchmark for reasoning over structured knowledge. The dataset and code are publicly available.</sample>
    <sample id="248">No – while NLPositionality recruited a large, geographically and culturally diverse pool (≈ 1,000 annotators from 87 countries), the sample was not evenly balanced across every demographic axis (country, gender, education, etc.). The results show that many models and datasets align best with English‑speaking, college‑educated users, indicating that the annotator distribution was skewed rather than perfectly balanced.</sample>
    <sample id="249">Sentences in the acceptable domain were perturbed by adding “noise” to the token sequence while keeping the underlying syntactic structure intact – e.g., slightly jumbled or altered words that still preserve the grammatical frame. The perturbations were designed to modify surface form only, not to change the core acceptability of the sentence.</sample>
    <sample id="250">A dimensional evaluation breaks overall quality into separate, measurable facets—each dimension (e.g., relevance, consistency, hallucination, empathy, etc.) is assessed independently rather than using a single aggregate score.</sample>
    <sample id="251">The paper’s author(s) are affiliated with the **University of Science and Technology of China**.</sample>
    <sample id="252">**Abstract**

Prior Case Retrieval (PCR) is essential for legal professionals to locate precedent cases that are both relevant to a query document and cited within it. We introduce **IL-PCR**, an Indian Legal Prior Case Retrieval dataset comprising 7,070 cases with an average of 6.8 citations per query, featuring longer documents, richer vocabulary, and a larger candidate pool than existing benchmarks such as COLIEE’21. Alongside the dataset, we present **U‑CREAT**, an unsupervised, event‑centric pipeline that achieves state‑of‑the‑art retrieval performance across Indian and Canadian legal corpora without domain‑specific tuning. The pipeline first extracts events from each document by parsing dependency graphs to form subject‑verb‑object triplets, then constructs an interaction matrix of shared events between query and candidate documents. Three retrieval strategies are evaluated: (i) count‑based (BM25), (ii) transformer‑based (BERT, DistilBERT, DistilRoBERTa, legal‑domain BERTs), and (iii) event‑based models. Event‑based approaches—especially the **Event‑Filtered Documents** variant that retains only sentences yielding matching events—significantly surpass baselines, achieving higher F1 scores and lower inference times. In comparison to the supervised MTFT‑BERT model on COLIEE’21, U‑CREAT sets a new performance benchmark. The IL‑PCR dataset and U‑CREAT pipeline thus provide robust tools for scalable, cross‑jurisdictional prior case retrieval.</sample>
    <sample id="253">**Abstract**  
Mental‑health disorders such as depression, PTSD and eating disorders are frequently discussed on social media, yet identifying early signs automatically remains challenging due to limited labeled data. We present **DisorBERT**, a double‑domain adaptation framework that first adapts a pre‑trained BERT model to the linguistic style of Reddit and then further fine‑tunes it on a mental‑health domain corpus. Guided masking, informed by a mental‑health lexicon, biases the masked‑language‑model training toward clinically relevant terms. We evaluate DisorBERT on the eRisk datasets, demonstrating a superior precision‑recall balance compared to baseline models, including MentalBERT. Ablation studies on the Beck Depression Inventory (BDI) reveal that DisorBERT predicts more psychologically oriented words (e.g., “focus”, “breath”, “sleep”) than vanilla BERT, indicating better attention to disorder‑specific language. Attention visualizations on high‑scoring depression users highlight salient tokens such as “anxious” and “medication”, corroborating the model’s focus on relevant symptoms. Overall, double domain adaptation combined with guided masking effectively captures mental‑disorder signals in social‑media text. Future work will explore additional lexical resources and integration of clinical data to further enhance detection performance.</sample>
    <sample id="254">**Abstract**  
Document‑level relation extraction (DocRE) aims to discover relations among entities within a document. While distant supervision (DS) supplies abundant training data, it is heavily contaminated with false positives that degrade model performance. We propose an uncertainty‑guided label denoising framework to improve the quality of DS annotations and thus enhance DocRE. First, a pre‑denoising DocRE model is trained on a mix of DS and small human‑annotated corpora to generate pseudo labels. To assess the reliability of these predictions, we adopt Monte‑Carlo dropout to obtain stochastic forward passes and compute an *instance‑level* uncertainty score for each predicted relation, explicitly handling overlapping relations that a single entity pair may possess. We observe that uncertainty distributions differ across relation classes and that frequent classes exhibit lower average uncertainty than long‑tail classes. Accordingly, we introduce *dynamic class‑wise thresholds*: only pseudo labels whose uncertainty falls below the class‑specific threshold are retained, while others are discarded or relabeled. Finally, a multi‑phase training strategy iteratively relabels DS data using the refined pseudo labels, progressively improving model quality. Experiments on two public DocRE benchmarks demonstrate that our method surpasses state‑of‑the‑art baselines, achieving significant gains in relation extraction accuracy. The key contributions are (1) a holistic uncertainty‑guided denoising pipeline, (2) an instance‑level uncertainty estimator for overlapping relations, (3) dynamic, class‑aware filtering to address long‑tail noise, and (4) a multi‑phase training regime that fully exploits DS data.</sample>
    <sample id="255">The form of the prompt matters only in low‑shot settings.  
- **Zero‑shot and one‑shot prompting**: the wording or structure of the prompt has a noticeable impact on performance.  
- **Higher‑shot prompting (e.g., five‑shot)**: the specific form of the prompt is largely irrelevant; the examples themselves drive the result.</sample>
    <sample id="257">The authors evaluated **four state‑of‑the‑art chat models** (the specific model names are not provided in the excerpt).</sample>
    <sample id="258">**Abstract**

This paper investigates whether large language models (LLMs) can replace human judgments in evaluating natural‑language text. We propose a framework in which LLMs receive explicit natural‑language instructions to rate samples on predefined quality attributes—grammar, coherence, likability, and relevance—and output numeric scores. To validate the approach, we evaluate stories generated by GPT‑2 and by human authors. Human evaluation is conducted by qualified English teachers who use the same instructions and scoring rubric, providing a ground‑truth benchmark.

Four LLMs are tested: T0, InstructGPT (Curie and Davinci), and ChatGPT. Results show that human raters consistently prefer human‑written stories over GPT‑2 outputs. Among the LLMs, only Davinci and ChatGPT exhibit a comparable preference pattern, while the smaller models fail to discriminate meaningfully. We further analyze inter‑annotator agreement between LLMs and humans, the impact of instruction phrasing, and sampling strategies on evaluation outcomes. Cost‑benefit considerations highlight that LLM‑based evaluation offers rapid, reproducible scoring with minimal human effort, albeit with variability across model sizes.

The study demonstrates that sufficiently powerful LLMs can approximate human judgments for certain tasks, offering a scalable alternative to labor‑intensive human evaluation. Future work will extend the methodology to other NLP tasks and explore systematic instruction design to enhance reliability.</sample>
    <sample id="259">**Abstract**  
Cross‑lingual semantic parsing—i.e., converting user utterances in multiple natural languages into formal meaning representations such as SQL, Lambda calculus, or FunQL—remains under‑explored for many languages and formalisms.  We introduce **XSemPLR**, a unified benchmark that addresses this gap by aggregating 9 datasets across diverse domains, covering 5 semantic‑parsing tasks, 8 formal representations, and 22 languages from 15 language families (including Chinese).  To evaluate model capabilities, we design six training–evaluation regimes: Translate‑Test, Monolingual, Monolingual Few‑shot, Multilingual, Cross‑lingual Zero‑shot, and Cross‑lingual Few‑shot.  We benchmark three representative multilingual architectures: encoder‑pointer (XLM‑R + PTR, mBERT + PTR) and encoder‑decoder (mBART, mT5).  Our results show that encoder‑decoder models consistently outperform encoder‑pointer baselines across all datasets.  Multilingual pretraining yields gains for most languages but induces a “curse of multilinguality” that hurts English performance on several tasks.  Zero‑shot cross‑lingual transfer suffers a large performance gap, which shrinks markedly when few target‑language examples are provided.  Finally, we observe that English‑only pretraining boosts few‑shot performance, yet large multilingual models such as Codex and BLOOM remain inadequate for cross‑lingual semantic parsing.  XSemPLR provides a comprehensive testbed for future research in multilingual semantic understanding.</sample>
    <sample id="260">The paper is authored by a single researcher – Jingwei Yi.</sample>
    <sample id="261">A good planner should produce scripts that are **reasonable**—i.e., logically coherent and complete steps for the task—and **faithful** to all imposed constraints, ensuring the plan adheres to the specific requirements of the goal.</sample>
    <sample id="262">The paper has only one author, Siyu Yuan.</sample>
    <sample id="263">**Abstract**

In‑context learning (ICL) enables large language models to perform downstream tasks by conditioning on a handful of labeled examples. However, ICL is notoriously unstable: predictions vary with the choice, order, and content of the examples. Existing studies attribute this instability to “label bias,” yet a systematic taxonomy and mitigation strategy are lacking. In this work, we propose a comprehensive typology of label biases affecting ICL: (1) **vanilla‑label bias**—the model’s inherent preference for certain label names; (2) **context‑label bias**—the influence of the labeled examples on the model’s predictions; and (3) a previously unreported **domain‑label bias**, capturing the effect of the task corpus on label selection. We empirically demonstrate that random in‑domain words can drastically skew predictions, whereas random English words do not, underscoring the presence of domain‑label bias.

To address all three bias types, we introduce **domain‑context calibration (DCC)**, a lightweight post‑processing technique that estimates the model’s bias using a set of random in‑domain words as content‑free prompts, then adjusts the logits of the original ICL output. Extensive experiments across diverse text‑classification datasets and multiple GPT‑3 variants show that DCC substantially improves average ICL accuracy, with the greatest gains on tasks exhibiting large domain‑label bias. Ablation studies confirm that replacing a single predefined content‑free token with multiple random words, especially from the target domain, yields further performance boosts. Our findings highlight the importance of systematic bias analysis in ICL and provide a practical, broadly applicable calibration method.</sample>
    <sample id="264">**Abstract**

Audio–visual text generation has attracted limited attention due to the high cost of multimodal annotations and severe performance degradation when transferring across domains. In this work, we introduce **Transferable Audio‑Visual Text Generation (TAVT)**, a task that aims to generate coherent textual descriptions from audio–visual inputs while coping with multimodal domain shifts such as visual style, shooting angle, and audio energy. We observe that visual semantics vary more across domains than audio semantics; hence we propose to align visual concepts into a unified audio‑semantic space. To this end, we construct an **audio‑visual meta‑mapper** that clusters a large pool of Flickr audio clips (via k‑means) and learns learnable visual‑prefix tokens to map visual features to a probability distribution over audio clusters, thereby tightening visual‑audio alignment. The second component is a transformer‑based encoder–generator that introduces a dynamic weighting factor α_t to modulate the contribution of each modality per generated token. Finally, we formulate **Dual Counterfactual Contrastive Learning (DCLL)**, which constructs fine‑grained supervision from counterfactual samples to directly optimise visual‑textual alignment without dependency on negative sampling. The entire framework is trained under a MAML‑style meta‑learning regime, allowing rapid adaptation to new audio‑visual domains with only a few labeled examples.

Experiments on two newly constructed benchmarks—cross‑dataset and cross‑domain splits of MSVD and MSR‑VTT—demonstrate that TAVT consistently outperforms state‑of‑the‑art RNN‑ and transformer‑based baselines across all metrics. Ablation studies confirm the importance of audio features and the effectiveness of the counterfactual contrastive objective. TAVT thus establishes a strong baseline for transferable audio‑visual text generation.</sample>
    <sample id="265">The speaker’s name is **Vasudha**.</sample>
    <sample id="266">The supplied excerpt does not include any information about the institutional affiliations of the paper’s authors.</sample>
    <sample id="268">The most common errors observed in PaLM translations are **omission errors**—the model tends to drop or omit parts of the source sentence.</sample>
    <sample id="270">The authors are affiliated with Emory University (the Emory NLP Lab, led by Professor Jinho Choi) and Amazon Alexa AI.</sample>
    <sample id="271">CFT stands for **Continual Fine‑Tuning**.</sample>
    <sample id="272">There are **seven** authors in total.</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">**Abstract**  
We introduce *IndicMT‑Eval*, a benchmark for meta‑evaluating machine‑translation (MT) metrics on Indian languages. While most existing studies focus on English‑target MT, we target translations *into* English from five typologically diverse Indian languages (Tamil, Malayalam, Hindi, Marathi, Gujarati). Using the Flores corpus, we randomly select 200 source sentences per language and generate 1,400 candidate translations per language via seven commercial and open‑source MT systems, yielding 7,000 system outputs. Human experts annotate each translation with fine‑grained error types and severity (in the MQM framework) and assign an overall quality score. Our analyses reveal that overlap‑based metrics (e.g., chrF) correlate poorly, whereas embedding‑based metrics (e.g., LabSE, BERTScore with MuRIL) and especially COMET variants achieve the highest Pearson/Kendall‑tau correlations with human judgments. We observe that metrics often exhibit a skewed score distribution, limiting interpretability. Further, accuracy‑error subsets elicit higher metric correlations than fluency subsets. Leveraging our MQM annotations, we fine‑tune COMET to create IndicCOMET, which outperforms vanilla COMET on most languages and retains strong zero‑shot performance on unseen languages. IndicCOMET also demonstrates superior robustness on the ACES Translation Accuracy Challenge Sets. The dataset, annotations, and fine‑tuned models are publicly released to foster further research in MT evaluation for Indian languages.</sample>
    <sample id="277">Multiset Tagging and Latent Permutations.</sample>
    <sample id="278">The “marked words” method identifies the vocabulary that linguistically marks a minority group by comparing its persona to those of the unmarked (dominant) groups.  It uses weighted log‑odds ratios (the Fightin’ Words technique) to compute which words are significantly more frequent in the marked group’s texts than in the unmarked ones, thereby revealing the words that “mark” that identity.</sample>
    <sample id="279">All authors are affiliated with the **University of Washington**.</sample>
    <sample id="280">**Abstract**  
Emotion regulation in conversations (ERC) aims to predict the emotion of each utterance in a dialogue by leveraging its textual, audio, and visual modalities. Existing ERC methods largely rely on textual cues or simple concatenation, leaving the complementary nature of multimodal signals underexplored, and suffering from poor performance on minority and semantically similar emotion classes. We propose **MultiEMO**, an attention‑based correlation‑aware multimodal fusion framework that addresses these gaps. First, we introduce **VisExtNet**, a visual extractor that isolates facial expressions from irrelevant scene context using MTCNN and a pre‑trained ResNet‑101 on VGGFace2. Second, we design **MultiAttn**, a bidirectional multi‑head cross‑attention network that sequentially fuses text with audio and then with visual modalities, capturing inter‑modal dependencies while preserving modality‑specific information. Third, we propose **Sample‑Weighted Focal Contrastive (SWFC) Loss**, which emphasizes hard‑to‑classify minority samples and increases inter‑class separability for semantically similar emotions. Extensive experiments on MELD and IEMOCAP demonstrate that MultiEMO surpasses state‑of‑the‑art methods, especially on minority classes and difficult emotion pairs. Limitations remain: VisExtNet cannot separate speakers from background persons, SWFC loss requires large batch sizes, and minority class performance still lags majority classes. Nonetheless, MultiEMO offers a comprehensive solution for robust ERC across modalities.</sample>
    <sample id="281">**Abstract**

Context is crucial for accurate machine translation (MT), yet it is difficult to evaluate how well MT systems handle context-dependent phenomena. We present a data‑driven, multilingual study that first quantifies the dependence of individual words on surrounding context using point‑wise Context‑X‑Mutual Information (P‑CXMI). Applying this measure to English‑to‑14‑language TED‑talk transcripts, we identify patterns among high‑P‑CXMI tokens, part‑of‑speech tags, and vocabulary items. These patterns reveal five discourse phenomena that often require context: formality, lexical cohesion, pronoun resolution, ellipsis, and verb‑form selection. 

To operationalize our findings, we develop the Multilingual Discourse‑Aware (MuDA) tagger, which automatically flags tokens belonging to each phenomenon in any parallel corpus. Using MuDA‑tagged data, we construct a benchmark for document‑level MT evaluation. We benchmark numerous models—including context‑agnostic, context‑aware, and commercial systems—across BLEU, COMET, and token‑level F‑measure. Corpus‑level metrics show that BLEU prefers context‑agnostic models, while COMET rewards context‑aware systems; token‑level F‑measure yields comparable results. MuDA‑based evaluation, however, demonstrates that context‑aware models significantly outperform their counterparts on formality and lexical cohesion, but only marginally on ellipsis, pronouns, and verb‑form. Moreover, DeepL consistently outperforms Google Translate in document‑level translation quality.

Our work offers a systematic, multilingual framework for identifying when translation requires context and provides a benchmark that exposes the strengths and weaknesses of current MT systems on discourse‑sensitive translation.</sample>
    <sample id="282">**Abstract**  
We present **StoryTrans**, a novel framework for non‑parallel story‑level author‑style transfer that explicitly models discourse structure and preserves narrative content. Existing style‑transfer work largely operates at the token or sentence level, leaving the rich discourse‑level stylistic cues of long texts underexplored. StoryTrans first encodes source stories into discourse‑aware representations and combines them with learnable style embeddings to generate target‑style continuations. To disentangle style from content, we introduce a multi‑objective training regime: a self‑reconstruction loss, a style‑disentanglement loss on sentence embeddings, a sentence‑order loss to capture discourse dependencies, and a style‑classifier loss to enforce style signals. Content preservation is further enhanced by a two‑stage generation pipeline: (1) a style‑transfer step that masks style‑specific content keywords, and (2) a content‑recovery step that explicitly inserts the masked keywords. We train the two stages separately, ensuring that the latter stage focuses on faithfully restoring content rather than style.  

To evaluate our approach, we constructed large Chinese and English story corpora spanning fairytales and everyday narratives paired with well‑defined author styles. Automatic metrics (BLEU, style accuracy) and human judgments confirm that StoryTrans outperforms strong baselines on both style control and content fidelity. Visualizations in style‑feature space demonstrate tight alignment of generated stories with target styles, while case studies illustrate the model’s ability to enrich plots and preserve core semantic content. The corresponding datasets and code are publicly released.</sample>
    <sample id="283">The first symmetrical dependency structure mentioned is the **Prague approach** (Prague dependency treebank).</sample>
    <sample id="284">**Abstract**  
Universal Information Extraction (UIE) models rely on precise span boundaries, yet annotated spans are often ambiguous and transformers are biased toward global features that ignore the limited length of target spans. We propose **FSUIE**—a framework that introduces *fuzzy span learning* and *adaptive fuzzy span attention* to address these shortcomings. The fuzzy span loss models the start and end positions as continuous distributions within a range \([R_{\min}, R_{\max}]\); a sampling function converts this distribution into discrete candidates, and the loss combines binary cross‑entropy with the gold span and KL‑divergence between the predicted fuzzy distribution and a supplementary prior. To guide the attention mechanism, we design a mask function \(G\) that (i) incorporates an optimizable parameter \(\delta\) to flexibly adjust the full attention span, and (ii) applies a linear decay at the span boundaries, thereby reducing abrupt truncation. The fuzzy span attention layer is inserted only at the top transformer level, preserving the encoder’s representational power while shaping the decision process.

We evaluate FSUIE on three core IE tasks: Named Entity Recognition, Relationship Extraction, and Aspect Sentiment Triplet Extraction. Across datasets (ACE2004/2005, ADE, AST‑V2), FSUIE achieves new state‑of‑the‑art results, with pronounced gains on small‑scale data. Ablation studies confirm that the fuzzy span attention (FSA) accelerates convergence, while the fuzzy span loss (FSL) enhances extraction capability; jointly they yield the largest benefit. Visualization of attention confirms that the module focuses on semantically relevant preceding tokens within a controlled span. Overall, FSUIE demonstrates a unified, efficient, and generalizable approach to IE.</sample>
    <sample id="285">**Abstract**  
Factual inaccuracies remain a pervasive challenge in dialogue‑summarization, yet prior studies on factual error correction (FEC) have largely focused on extractive summarization and rely on coarse factuality metrics such as FactCC and DAE. These metrics provide only an overall score, are unreliable, and blur the distinction between true error‑correction and de‑fact‑generation. We argue that an FEC model should explicitly amend the original summary using the minimal set of substitutions, insertions, and deletions, thereby preserving fluency and non‑redundancy. To enable this, we introduce manually annotated reference corrections for dialogue summaries, furnishing richer training data and a more granular evaluation. We propose a new taxonomy that distinguishes **content‑based** errors (modeled via part‑of‑speech and dependency relations) from **form‑based** errors (captured as add, delete, or substitute operations). Building on the ERRANT framework, we perform alignment, classification, and comparison to evaluate corrections. Experiments across multiple FEC training regimes reveal that models trained solely on synthetic data underperform those supplemented with human‑corrections. While incorporating reference summaries yields the best factuality scores, current FEC approaches still fail to correct addition‑type errors and cannot address attribute, modality, or link errors. Our work highlights the necessity of fine‑grained, human‑guided evaluation and training for robust factual error correction in dialogue summarization.</sample>
    <sample id="286">James Finch.</sample>
    <sample id="287">Four authors are involved in the paper.</sample>
    <sample id="288">Datasets used for testing syntactic phenomena include:

- **BLiMP** – a collection of minimal‑pair acceptability judgments for a wide range of grammatical features.  
- **SyntaxGym** – a suite of grammaticality tests covering many syntactic constructions.  
- **CrowS‑Pairs** – minimal‑pair datasets that also capture acceptability in the presence of social stereotypes.</sample>
    <sample id="290">The five weak‑supervision methods examined in the first research question are abbreviated as follows:

- **FTw** – Fine‑tune with weak labels  
- **COSINE** – the COSINE‑based method  
- **CoT** – Co‑Training  
- **ST** – Self‑Training  
- **WLS** – Weak‑Label‑Selection (generic WSL baseline)</sample>
    <sample id="291">The models are evaluated on a suite of 11 biomedical/clinical downstream tasks, including:

* **Named‑entity recognition (NER)**
* **Text classification** (e.g., label/diagnosis classification)
* **Part‑of‑speech (POS) tagging**
* **Question answering (QA)**

These four task families cover the 11 benchmark datasets used in the study.</sample>
    <sample id="294">CamemBERT was originally pre‑trained on a large French corpus – the OSCAR dataset (≈138 GB of French text) together with a smaller CCNet French subset (~4 GB).</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">**Abstract**

This work reports on the creation and analysis of the English Perspectivist Irony Corpus (EPIC), a collaborative effort between the University of Turin and Amazon Alexa. EPIC contains 300 short conversational exchanges (pairs of utterances) drawn from Reddit and Twitter over a 1½‑year period, covering five varieties of English. Using the crowdsourcing platform Prolific, 74 annotators were recruited (≈15 per English variety). Each annotator labeled 200 conversations with a binary “ironic” vs. “not ironic” decision, with attention‑check items to ensure quality. On average, each conversation received five independent labels, enabling the study of inter‑annotator agreement (IAA). The resulting violin plots reveal systematic differences in IAA across demographic axes—gender, age group, and nationality—indicating that the conventional assumption of a single ground truth is problematic for irony detection.

To address this, we introduced *perspective‑aware* models: fine‑tuned pre‑trained language models trained on splits of the data corresponding to specific annotator groups. While overall predictive performance did not differ markedly from baseline gold‑standard models, perspective‑aware models exhibited higher confidence (lower entropy) in their predictions. Further exploratory analysis uncovered that inter‑generational disagreement peaks among adjacent age cohorts, and that the largest variation in annotations arises between UK and Irish annotators. These findings highlight the need for models that explicitly account for annotator perspective in detecting latent pragmatic phenomena such as irony. The dataset and preliminary models are publicly available, and we welcome further discussion at the poster session.</sample>
    <sample id="297">**Abstract**  
This paper investigates the phenomenon of dogwhistles—coded linguistic signals that convey covert meanings to an in‑group while remaining innocuous to an out‑group. We first construct a comprehensive typology that captures register (formal vs. informal), type (additive vs. covert implicature), and persona (e.g., anti‑Semitic, transphobic, racist). Leveraging this framework, we assemble a glossary of over 340 English dogwhistles, each annotated with contextual background, persona, register, type, and illustrative real‑world examples drawn from academic literature, Wikipedia, blogs, and historical corpora. A large‑scale case study of the U.S. Congressional Record demonstrates a marked rise in the frequency of racial and other dogwhistles since the Civil Rights era, paralleling the Republican Southern Strategy and showing a strong temporal association with conservative politics.  

We evaluate the ability of contemporary language models to surface and recognize dogwhistles. Prompting GPT‑3 yields a high recall for formal‑register terms but significantly lower performance on informal, social‑media‑style, and transphobic dogwhistles; performance improves markedly when explicit prompts include definitions or secret cues. Finally, we test the capacity of automated toxicity detection systems (Prospective API) to flag dogwhistle‑laden text. Replacing overt slurs with dogwhistles consistently reduces toxicity scores, evidencing an effective evasion strategy for content moderation. Together, these results provide a systematic resource for the study of coded rhetoric and highlight the challenges it poses for natural language processing and online safety.</sample>
    <sample id="298">The conclusion that temporal drift drives the loss came from two key observations:

1. **No adaptive overfitting** – The regression line for improvements on CoNLL‑2003 vs. CoNLL++ had a slope greater than one, meaning gains on the old test set translated to *larger* gains on the new test set, so there was no diminishing‑returns pattern that would signal over‑fitting.

2. **Performance degrades with time** – When models were retrained or continued pre‑training on progressively more recent data, their F1 scores fell as the temporal gap between the training data and the 2020 Reuters test set increased. This temporal‑gap effect confirmed that the main cause of the drop is temporal drift.</sample>
    <sample id="299">**Abstract**  
Natural Language Inference (NLI) models now achieve state‑of‑the‑art scores, yet many of these successes stem from exploiting dataset shortcuts—spurious correlations such as high word‑overlap that bias predictions. Existing shortcut mitigation strategies rely on an auxiliary model explicitly trained to use shortcuts and then re‑weight training samples; this approach requires domain‑specific knowledge, presumes the learner will follow the same shortcut patterns, and incurs extra computational cost.  

We propose a minimax training framework that removes these assumptions. A lightweight feed‑forward auxiliary network learns weights for each training instance, maximizing the learner’s loss by targeting examples where the learner performs poorly (“hard” instances). The learner, in turn, minimizes the NLI loss under these weights. By iteratively alternating the two optimizations, the learner is encouraged to focus on under‑represented hard examples that counteract shortcut exploitation, while the auxiliary is trained only through gradients from the learner’s loss.  

The method is evaluated on MNLI, FEVER, and QQP with their respective out‑of‑distribution adversarial test sets (HANS Symmetric, PAWS). Compared to empirical risk minimization and the best existing shortcut‑mitigation baselines, our approach consistently improves OOD accuracy while preserving in‑distribution performance. Additional experiments demonstrate transfer to larger models, robustness to synthetic shortcuts, and effectiveness on out‑of‑domain data. We also analyze the impact of learner pre‑training, the minimal size required for the auxiliary, and qualitatively inspect the learned weight distribution. The proposed minimax training offers a general, efficient means to enhance NLI robustness without prior shortcut knowledge.</sample>
    <sample id="300">**Abstract**

We introduce *interactive dictation*, a novel task that enables users to simultaneously dictate and edit a document using natural speech. Unlike traditional dictation systems that separate speaking from editing, interactive dictation permits seamless interleaving of dictation and vocal edit commands without fixed trigger phrases. We formalize the task as a four‑step pipeline: automatic speech recognition (ASR), utterance segmentation into dictation and command segments, command extraction and normalization, and incremental execution of the resulting actions to produce the final document state. To support research on this task, we designed a custom annotation interface and collected a large, high‑quality dataset of speech–document trajectories in which annotators alternated between dictating text and issuing natural‑language edit commands. The dataset includes ASR transcripts, segmented utterances, normalized edit programs, and ground‑truth document states.

We also present a baseline system that trains separate models for each pipeline component. For segmentation we use a fast, accurate sequence labeling model; for interpretation we experiment with both T5 and GPT‑3 architectures, exploring two output styles: explicit edit programs versus direct state prediction. Evaluation on exact‑match of the final document state shows that GPT‑3 yields higher accuracy but incurs substantial latency, whereas T5 offers a favorable speed–accuracy trade‑off. The results highlight the feasibility of the task while revealing significant room for improvement, and we release the code and data to encourage further work in this emerging research area.</sample>
    <sample id="302">Because the first step only determines *which* output tokens are needed (an unordered multiset), it does not provide their correct sequence. The second step is required to order those tokens into a syntactically correct logical form, i.e., to find a permutation that places the multiset items in the right positions.</sample>
    <sample id="303">Because without clear disclosure of how bias mitigation is applied, it’s impossible to tell whether the positive‑sounding stereotypes the models produce are a side‑effect of alignment, an unintended outcome of mitigation techniques, or something else entirely. Greater transparency lets researchers and users understand, evaluate, and improve the methods that shape these harmful patterns.</sample>
    <sample id="304">Minimal‑pair *unacceptable* inputs are the ungrammatical or otherwise “bad” sentences that are paired with a grammatical counterpart in the evaluation setup. They form the “negative” side of a minimal‑pair test, used to see whether a model assigns a lower probability or lower acceptability score to the erroneous sentence compared to the correct one.</sample>
    <sample id="305">**Abstract**  
Weakly supervised learning (WSL) aims to train deep models using inexpensive, noisy labels derived from heuristics, knowledge bases, or low‑quality crowdsourcing. While recent literature reports high test‑set performance achieved solely on weakly labeled data, these claims implicitly rely on an additional clean validation set for model selection—a requirement that is often overlooked. In this study we systematically investigate three core questions: (1) is clean validation data essential for WSL? (2) how many clean samples are sufficient? (3) how should the clean data be used? We evaluate state‑of‑the‑art WSL methods on a variety of benchmarks and demonstrate that, in the absence of clean validation samples, the trained models fail to generalize beyond the weak labels, resulting in a substantial performance drop. Our experiments further show that a modest number of clean examples—approximately 20 per class—suffices to achieve high test‑set accuracy, and that directly fine‑tuning a pretrained model on these clean samples can outperform or match sophisticated WSL techniques. Consequently, the purported gains of complex WSL algorithms are largely attributable to downstream fine‑tuning rather than to their core training procedures. Based on these findings, we recommend that future WSL work explicitly report validation strategies, benchmark against few‑shot learning baselines, and consider continuous fine‑tuning as a strong, low‑cost baseline. All code and data are publicly released.</sample>
    <sample id="306">**Abstract**  
Understanding discourse requires an agent to track entities and their evolving states. In this study we investigate whether large pre‑trained language models possess this capability. We design a controlled evaluation task that isolates entity tracking from superficial cue exploitation. Each instance begins with a description of the contents of several boxes. A sequence of state‑changing operations (e.g., moving or adding objects) follows, and the model must predict the final contents of each box. To prevent shortcuts, we (i) balance entity‑state distributions so that pre‑training statistics cannot reveal the outcome, (ii) ensure that individual words or phrases do not uniquely determine the state, and (iii) block memorization of operation patterns through careful prompt construction. We evaluate Flan‑T5, GPT‑3, and GPT‑3.5 variants using 2‑shot in‑context learning. Accuracy is plotted against the number of operations affecting a box. Models generally copy the initial state, yielding high accuracy when the target state matches the starting state. Only text‑davinci‑003 displays non‑trivial tracking for altered states, surpassing a random baseline. A deeper probe into the GPT series reveals that GPT‑3.5 models, pretrained on extensive code corpora, exhibit the strongest tracking, whereas models lacking code exposure do not. Fine‑tuned T5‑base can learn the task, but randomly initialized models cannot, underscoring the importance of pre‑training. These findings suggest that code‑centric pre‑training fosters emergent entity‑tracking abilities, though generalization beyond the synthetic setup remains to be explored.</sample>
    <sample id="307">The paper reports the standard evaluation metrics for each downstream task:

- **Named‑entity recognition (NER)** – micro‑averaged precision, recall and the F1‑score (the metric most often used in biomedical NER benchmarks).  
- **Text classification** – overall accuracy (and, when datasets are imbalanced, the macro‑averaged F1‑score).  
- **Part‑of‑speech (POS) tagging** – token‑level accuracy (sometimes complemented by a micro‑averaged F1).  
- **Question answering (QA)** – Exact‑Match (EM) accuracy and the F1‑score over the predicted answer spans.  

These four metrics (precision/recall/F1 for NER, accuracy for classification, accuracy for POS, EM/F1 for QA) were used to compare the seven pre‑trained models against the six baselines.</sample>
    <sample id="308">**Abstract**

This study investigates the positionality of NLP datasets and models—how their performance systematically varies across demographic groups—using a newly developed framework, NLPositionality. We re‑annotated two benchmark corpora, *Social Chemistry* (social acceptability) and *Dynahate* (hate‑speech detection), with a diverse set of 1,000+ annotators from 87 countries via the Lab in the Wild platform. Each instance received multiple annotations, enabling a demographic‑rich comparison between human judgments and model predictions. We evaluated GPT‑4, Perspective API, Rewire API, and Hate‑Roberta against the re‑annotated data, measuring alignment with Pearson’s R across demographic slices. Results reveal pronounced positionality: models are most aligned with English‑speaking, college‑educated, and Confucian‑cultural populations, while non‑binary individuals are under‑represented in model judgments. The findings underscore that current NLP systems privilege certain identities, leaving others systematically misrepresented. We recommend (1) meticulous documentation of design choices to surface bias sources, (2) adopting a perspectivist research lens that foregrounds diverse viewpoints, and (3) developing community‑specific datasets and models—as exemplified by the Masakhani initiative—to counteract systemic exclusion. This work highlights the need for explicit positionality analysis to guide the creation of more inclusive NLP technologies.</sample>
    <sample id="309">The inter‑annotator agreement was measured using **Cohen’s κ (kappa) statistic**.</sample>
    <sample id="310">They used **Wikipedia** as the domain for adding completely unrelated sentences.</sample>
    <sample id="311">The provided text does not mention the authors’ institutional affiliations.</sample>
    <sample id="312">MultiInstruct is the first large‑scale, multi‑modal instruction‑tuning benchmark. Unlike most existing datasets that focus on language tasks or single‑modal vision, it covers 62 diverse tasks (10 categories) drawn from 21 open‑source datasets, each paired with five expert‑written instruction templates. It unifies text, images, and bounding‑box tokens into a single sequence‑to‑sequence format, enabling instruction‑based fine‑tuning of a unified model (OFA). Additionally, it introduces a sensitivity metric to measure robustness to instruction wording. This combination of multi‑modal coverage, instruction diversity, and unified tokenization sets MultiInstruct apart from other benchmarks.</sample>
    <sample id="313">Three authors are involved: James Finch, Sarah Finch, and Professor Jinho Choi.</sample>
    <sample id="314">Binary coordination is a syntactic construction that links exactly two conjuncts—typically nouns, verbs, clauses, or phrases—using a coordinating element (such as “and,” “or,” “but,” etc.). It forms a coordination structure with two branches (the two conjuncts) that are syntactically linked by the conjunction, yielding a binary tree representation.</sample>
    <sample id="315">The prompts were very brief – just a handful of words. On average they were around 5–8 words long.</sample>
    <sample id="316">The T5 model shows that, once it is fine‑tuned on the high‑quality, constraint‑aware CoScript data, it can generate scripts that are as good as—often better than—those produced by many large language models. This implies that with a suitable, domain‑specific dataset, smaller, cheaper models can surpass larger ones in constrained language planning, offering a more efficient and scalable solution.</sample>
    <sample id="317">**Abstract**  
We present *CodeIE*, a novel framework that reframes few‑shot information extraction (IE) as a structured code‑generation task. Traditional IE models such as T5 or GPT‑3 treat extraction as a text‑to‑text problem: during pre‑training the model is exposed to plain text, while inference requires linearizing structured outputs, leading to a mismatch that hampers performance. CodeIE circumvents this by employing large code‑generation models (e.g., Codex) and prompt engineering that explicitly defines extraction functions in a code‑style format. For Named Entity Recognition (NER) we prompt the model to write a function that iteratively extracts entity–text pairs and appends them to a list; for Relation Extraction (RE) we design analogous prompts. In a series of experiments on three NER datasets and four RE datasets, we compare CodeIE against T5, UIE, text‑davinci‑002 (GPT‑3) and code‑davinci‑002 (Codex). Across one‑to‑few shot settings, CodeIE with code‑style prompts consistently outperforms text‑style baselines, yielding higher F1 scores and recall. Analysis reveals that code‑format inputs yield lower perplexity for code‑pre‑trained models, and Codex generates far fewer structural errors than GPT‑3. Moreover, Codex avoids producing spurious labels not present in the predefined schema. These findings suggest that aligning inference format with pre‑training objectives via code prompts enhances few‑shot IE, and motivate the use of code‑generation models for structured NLP tasks. Our code and data are publicly released.</sample>
    <sample id="319">The study compares two main pre‑training strategies:

1. **From‑scratch pre‑training** – training a RoBERTa‑based model on French biomedical text (NACHOS) of varying sizes (4 GB, 7 GB, or mixed with clinical notes).

2. **Continual (continued) pre‑training** – starting from existing weights (CamemBERT or PubMedBERT) and further pre‑training on 4 GB of French biomedical or clinical data.</sample>
    <sample id="320">There is essentially no over‑fitting effect from re‑using the CoNLL‑2003 test set. In the plot the red best‑fit line has a slope &gt; 1, meaning every gain on CoNLL‑2003 yields an even larger gain on the new CoNLL++ data – i.e., there is no diminishing return. In short, the factor of over‑fitting due to test reuse is negligible (effectively zero).</sample>
    <sample id="321">The simplifications were judged automatically against the manually‑aligned plain‑language references.  The authors computed standard text‑simplification metrics (e.g., BLEU, SARI, and readability scores such as Flesch‑Kincaid/Grade‑Level) to compare the model outputs to the gold‑standard simplified sentences.</sample>
    <sample id="322">**Abstract**  
Human morality is inherently pluralistic and context‑dependent, yet many natural language processing (NLP) systems treat moral judgment as a single, continuous scale.  This work investigates what a text classifier actually learns about morality by applying explainable‑AI techniques to models trained on moral language.  Using the Moral Foundation Twitter Corpus—35 000 tweets spanning seven topical domains (e.g., #AllLivesMatter, #BlackLivesMatter)—we first encode the five moral foundations (care, fairness, loyalty, authority, purity) and then probe domain‑specific linguistic patterns.  Experiments reveal that models capture subtle domain differences: for instance, the moral foundation of subversion is associated with words such as *overthrow* and *mayhem* in #AllLivesMatter, whereas in #BlackLivesMatter the same foundation is linked to terms that encourage rebellion.  These results demonstrate that classifiers can distinguish nuanced moral rhetorics across contexts, but also that a single, monolithic model risks misinterpreting morality when applied cross‑domain.  Our findings highlight the necessity of domain‑aware moral modeling and provide an interpretability framework for evaluating how language models represent moral foundations.  This study contributes to safer deployment of NLP systems in socially sensitive applications.</sample>
    <sample id="323">**Abstract**

Commonsense question answering (QA) demands retrieval and reasoning over external knowledge, yet current approaches that combine language models (LMs) with knowledge bases (KBs) suffer from noisy subgraph construction, isolated modality encoding, and neglect of semantic relations among entities. We propose **DHLK** (Dynamic Heterogeneous‑Graph Reasoning with Language Models and Knowledge Representation Learning) to address these issues. First, we build a heterogeneous knowledge graph (HKG) from ConceptNet, WordNet, and Wiktionary, applying a two‑stage pruning strategy: (1) dictionary‑based removal of sub‑word fragments from entity phrases, and (2) dynamic elimination of weakly relevant nodes via RoBERTa’s attention weights. Paraphrases of key entities are retrieved from WordNet and Wiktionary and added as nodes, enriching the subgraph. Entities and relations are initialized by mean‑pooling and further optimized with TransE. We then encode the QA context and the HKG jointly with RoBERTa and a novel Relation‑Mask Self‑Attention (RMSA) layer, which incorporates relation types into self‑attention and iteratively updates node embeddings over multiple layers. The final graph representation is obtained by max‑pooling over question key entities, while path‑enhanced context embeddings are derived by integrating HKG paths. These representations are fed into an MLP to predict the answer. Experiments on CommonsenseQA and OpenBookQA, using external KBs, demonstrate that DHLK outperforms existing LM‑based and HKG‑based baselines, achieving state‑of‑the‑art results on both datasets.</sample>
    <sample id="324">Yes.  
Experiments show that different language models (e.g., GPT‑4 vs. BART, RoBERTa) exhibit distinct political leanings—some are more liberal, others more conservative—and that fine‑tuning on partisan corpora shifts those biases predictably.</sample>
    <sample id="326">Cognitive dissonance is the psychological discomfort that arises when a person holds two or more conflicting beliefs, attitudes, or behaviors—e.g., believing that smoking is harmful while actually smoking. It reflects an inconsistency between cognition (belief) and action (behavior).</sample>
    <sample id="327">**Abstract**  
Vision‑Language (VL) learning aims to build AI systems capable of jointly understanding images and text, with Visual Question Answering (VQA) serving as a canonical downstream task. Recent large‑scale self‑supervised models have adopted a two‑tower architecture comprising a textual encoder, a visual encoder, and a cross‑modal encoder. Existing designs such as METER feed only the final unimodal representation to the cross‑modal module, whereas BridgeTower connects each cross‑modal layer to a single, pre‑assigned unimodal layer, thereby limiting the exploitation of multi‑level semantic knowledge and scaling to additional unimodal layers.  

In this work we introduce **ManagerTower**, a new VL architecture that augments each cross‑modal layer with a *manager* module. Each manager aggregates the representations (insights) from multiple pre‑trained unimodal experts at different depths, allowing the model to flexibly weigh low‑, mid‑, and high‑level visual or textual features. This design decouples the number of cross‑modal layers from the number of unimodal experts, enhancing scalability and expressiveness.  

Using RoBERTa and CLIP‑ViT‑Base as unimodal encoders, ManagerTower is pre‑trained on only 4 M image‑text pairs. It substantially outperforms METER and BridgeTower across a suite of VL benchmarks, achieving a 39.15 % accuracy on the WikiVideo test set while matching the pre‑training and fine‑tuning settings of its baselines. Visualization of manager weights over VQA‑v2 reveals adaptive, layer‑wise aggregation patterns that differ between visual and textual modalities, confirming the model’s ability to selectively harness multi‑level semantic knowledge. Code and pretrained checkpoints are publicly released.</sample>
    <sample id="328">**GPT‑4** is the most liberal language model according to your findings.</sample>
    <sample id="329">**Abstract**

Zero‑shot video sentence localization seeks to identify the temporal segment of a long video that best matches a natural‑language query, without relying on costly manual annotations. Existing zero‑shot approaches generate pseudo‑events and pseudo‑queries but suffer from overly simplistic queries, weak negative alignment, and unfiltered label noise. We propose a noise‑resistant Structured Pseudo‑Label (SPL) framework that addresses these shortcomings in three stages. First, densely sampled video frames are fed to a pre‑trained image‑caption model (BLIP), producing rich, free‑form pseudo‑queries that capture complex semantics. Second, we compute frame‑query similarity and evaluate “event quality” as the difference between mean similarity inside a candidate window and outside it, using a sliding‑window search to select the most discriminative pseudo‑event for each query. We retain only the top‑\(K\) pseudo‑queries and eliminate high‑overlap query–event pairs. Third, the generated pseudo‑labels train a localization model with two noise‑mitigation strategies: (i) sample re‑weighting based on the model’s confidence and IoU with the pseudo‑label, and (ii) iterative label refinement where high‑confidence predictions replace low‑confidence ones. Experiments on ActivityNet Captions and Charades‑STA demonstrate that SPL outperforms prior zero‑shot methods across all standard metrics (R@M, mIoU), achieving the best reported performance on both datasets. This work establishes a robust, annotation‑free pipeline for video sentence localization.</sample>
    <sample id="330">Yes – in the experiments cumulative training was found to match or outperform iterative updating for all active‑learning settings.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">The MuDA benchmark is built from transcripts of TED talks (English sources translated into 14 target languages).</sample>
    <sample id="333">**Abstract**  
Neural machine translation (NMT) models often learn a highly non‑smooth representation space, leading to sparse low‑frequency token embeddings and poorly defined semantic “holes” that degrade generalization. Existing nearest‑neighbor NMT (kNN‑MT) mitigates this by retrieving nearest representations during decoding, but incurs expensive datastore look‑ups and fixes the representation distribution once the datastore is built. We propose **INK** (Injecting kNN Knowledge) to iteratively refine the NMT representation space via a lightweight adapter while obviating the need for a datastore at inference. In each training cycle, kNN knowledge extracted from a temporary datastore guides the adapter to adjust contextualized representations; the updated representations are then asynchronously used to refresh the datastore. We align contextualized embeddings with (i) token embeddings, (ii) kNN token embeddings, and (iii) embeddings of the same target token using KL‑divergence, yielding a joint objective that smooths the space. Experiments on the WMT’19 German‑English news benchmark, using the winning baseline model, demonstrate that INK surpasses state‑of‑the‑art kNN‑MT, achieving an average gain of 1.99 COMET and 1.0 BLEU. Moreover, INK attains these improvements with reduced memory footprint and faster inference, and joint use of the adapter and datastore further boosts performance, indicating that the adapter alone does not fully capture the benefits of kNN knowledge. Overall, INK offers a scalable, efficient framework for enhancing NMT generalization through iterative representation refinement.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Cross‑lingual transfer is the process of training a semantic‑parsing model on data in one language (or a combination of languages) and then applying that model to a different target language. It can be done in a zero‑shot setting—using no target‑language examples—or in a few‑shot setting—using only a small amount of target‑language data—so the model learns to interpret queries in the new language despite never having seen many of its examples during training.</sample>
    <sample id="337">**Abstract**

Out‑of‑vocabulary (OOV) words pose a persistent challenge for embedding‑based downstream models. We propose a graph‑based framework that infers OOV embeddings by exploiting both word formation and lexical association. An OOV token is first decomposed into word‑pieces, which together with related words form a two‑level *Word Relationship Graph* (WRG). The first layer retains all word‑pieces to preserve fine‑grained sub‑word information, while the second layer samples a fixed number of neighboring nodes to reduce noise. Node attributes for unseen OOV nodes are generated by a self‑attention network over character embeddings, producing an initial representation. Two stacked Graph Attention Network (GAT) layers, each concatenated with its input, yield robust node‑level embeddings; a read‑out module aggregates these into a graph‑level representation. To align the learned embeddings with the background vector space, we employ contrastive learning with the NT‑XENT objective, using positive pairs drawn from two‑hop neighbors, synonyms, and the OOV word itself, while contrasting against unrelated samples. Extensive experiments on intrinsic (similarity, analogy) and extrinsic (text classification, NER) benchmarks demonstrate that our approach outperforms baseline OOV handling methods for both static and contextual models. Preliminary analyses suggest that the framework generalizes to agglutinative languages where morpheme segmentation is straightforward, while fusional languages present additional challenges. Overall, the WRG‑based architecture effectively captures morphological and contextual cues, enabling accurate OOV embedding generation without requiring large annotated corpora.</sample>
    <sample id="338">**Abstract**

Human‑generated natural language explanations are routinely used to train and evaluate large language models, yet methods for objectively assessing their quality remain scarce. This work investigates the utility of such explanations across diverse downstream tasks and proposes an objective evaluation framework. Five benchmark datasets—CoS‑E, ECQA (commonsense QA), e‑SNLI (NLI), ComVE (commonsense validation), and an additional task—are mapped to a unified multiple‑choice format that supports both baseline (no explanation) and infusion (explanation as auxiliary input) settings for sequence‑to‑sequence models. Extensive fine‑tuning experiments on T5 and BART reveal that explanations generally benefit model performance, but their effectiveness varies with task and explanation style; fine‑tuning with infusion encourages models to rely on the provided explanations, while the benefit to baseline models is task‑dependent. Motivated by these observations, we introduce **TREU (Task‑Relevance Evaluation of Explanations)**, an extension of the simulatability score that additionally measures the impact of explanations during fine‑tuning by comparing two models trained under baseline versus infusion conditions. TREU consistently ranks dataset‑specific explanation quality for both models, outperforming the conventional simulatability metric, especially on e‑SNLI and ComVE where task semantics (e.g., negation, counterfactuals) influence explanatory utility. Our results demonstrate that TREU provides a more faithful assessment of human explanations and underscores the importance of quality checks in collaborative annotation workflows.</sample>
    <sample id="339">All five authors are affiliated with **Saarland University, Germany**.</sample>
    <sample id="340">**Abstract**

Paraphrase generation is a cornerstone task for robust NLP systems, yet existing corpora either lack scale or syntactic diversity. We introduce **ParaAMR**, a large‑scale paraphrase dataset (≈15 M source sentences, 6.9 paraphrases each) built via *AMR back‑translation*. Each source sentence is parsed into an Abstract Meaning Representation (AMR) graph. By randomly selecting a non‑root node as a new focus, we re‑label the corresponding edges to reflect the changed emphasis, then generate surface text from the modified graph with a state‑of‑the‑art AMR‑to‑text model. This procedure preserves semantic fidelity while inducing substantial syntactic variation, because the generated sentences prioritize the new focus at the sentence onset. Automatic and human evaluations show that ParaAMR achieves semantic similarity comparable to conventional back‑translation datasets while markedly improving syntactic diversity scores. We demonstrate the dataset’s utility in three downstream tasks: (1) learning sentence embeddings, where ParaAMR‑based training yields superior performance on the STS benchmark; (2) syntactic‑control paraphrase generation, achieving better style transfer with fewer data; and (3) data augmentation for few‑shot learning, where paraphrases from ParaAMR enhance model generalization. ParaAMR is publicly released, offering an easily accessible resource for training paraphrase models that require both scale and structural variety.</sample>
    <sample id="341">The authors evaluate latency using two metrics:

1. **Average Lagging (AL)** – the standard measure of how far behind the translation lags the source speech.  
2. **Computational‑Aware Average Lagging** – a variant of AL that also incorporates the actual computational time spent predicting each token, giving a more realistic runtime latency.</sample>
    <sample id="342">**Abstract**

Open‑domain dialogue systems have traditionally relied on large text‑based corpora, leaving a gap for video‑sourced conversational data that better reflects natural speech. This paper introduces **LiveChat**, a Chinese large‑scale personalized dialogue dataset automatically constructed from live‑streaming videos on Douyin (TikTok). LiveChat is assembled in three stages: 1) extraction of audio from raw streams and transcription into utterances via ASR; 2) construction of multi‑party dialogues by matching audience comments to the corresponding speaker using a reply‑to‑whom algorithm; and 3) extraction of persona information through manual labeling of basic profiles and rule‑based or classifier‑based inference of additional traits. Compared to existing corpora, LiveChat is video‑sourced, contains longer average sessions, and provides rich persona annotations for each speaker. To demonstrate its utility, we evaluate retrieval‑based baselines on **Response Modeling** and **Addressee Recognition**. Results show that persona features and longer contexts improve performance, with rule‑based persona extraction outperforming pure classification. We further assess pretrained dialogue models (BART, GPT‑style LLMs) on LiveChat, revealing that domain mismatch hampers performance but large‑scale in‑context learning with carefully selected demonstrations yields gains up to eight examples, after which performance degrades due to noise. LiveChat thus offers a novel resource for personalized, multi‑party dialogue research and highlights challenges in transfer learning for video‑derived conversational data.</sample>
    <sample id="344">Tree‑based approaches suffer from several practical limitations:  

1. **Tree extraction is required** – trees are not supplied, so they must be induced or derived from the logical form.  
2. **Formalism‑specific preprocessing** – obtaining trees often involves heavy, domain‑specific parsing (e.g., handling variable symbols).  
3. **Computational cost** – tree induction or parsing can be expensive in time and resources.  
4. **Limited flexibility** – the need to fit a particular formalism may restrict the model’s applicability to new languages or logical forms.</sample>
    <sample id="345">**Abstract**  
Compositional generalization—the ability to handle unseen combinations of familiar phrases—remains a challenge for neural semantic parsers. Typical approaches impose explicit tree structures to capture the compositional process, yet such trees are rarely available and require costly, formalism‑specific preprocessing.  We propose a treeless seq2seq architecture that directly learns the correspondence between input fragments and output tokens.  The model operates in two stages: (1) **Multiset Tagging** – each input token is annotated with an unordered multiset of output tokens that will appear in the final logical form.  This step guarantees that the correct set of tokens is produced, but not their order. (2) **Latent Permutation** – a second network predicts a permutation that orders the multiset into a syntactically valid logical form.  The permutation is inferred without hard constraints, allowing great flexibility.  Training is complicated by the absence of explicit input‑output alignments and the existence of multiple valid permutations; we address this by jointly inducing alignments and employing a GPU‑friendly continuous relaxation of the permutation problem, enabling end‑to‑end back‑propagation.  Experiments on the COGS benchmark demonstrate that our method outperforms existing treeless models, achieving superior generalization to deeper recursion while remaining computationally efficient.  Remaining structural generalization tasks remain difficult, underscoring the need for further research.</sample>
    <sample id="346">The presentation does not provide any information about the authors’ institutional affiliations, so that detail is not available from the text you supplied.</sample>
    <sample id="348">**Abstract**

Recent studies have catalogued social bias in large language models (LLMs), yet most metrics rely on labor‑intensive, hand‑crafted datasets, target only a narrow set of stereotypes, and ignore intersectionality.  We propose a scalable, prompt‑driven approach that exploits instruction‑tuned LLMs’ ability to generate persona descriptions (“Imagine you are an Asian woman. Describe yourself.”).  By comparing model outputs with human‑written responses, we uncover that LLMs produce far richer stereotypical content, although traditional lexicons miss many harmful patterns.  To systematically identify group‑specific markers we introduce **Marked Words**, grounded in sociolinguistic markedness: unmarked (dominant) versus marked (marginalized) groups are compared using weighted log‑odds ratios (Fightin’ Words).  Across white men, black women, Asian women, and Latina women, the method reveals essentializing tropes—“exotic”, “culture”, “proud” for women of color, “petite”, “delicate” for Asian women, “strong”, “resilient” for black women—highlighting how seemingly positive descriptors perpetuate harm.  Our findings underscore the need to audit positive stereotypes, adopt an intersectional lens, and demand transparency in bias‑mitigation strategies.  This work offers a generalizable framework for measuring nuanced stereotypes in LLMs.</sample>
    <sample id="350">**Abstract**

Leaderboard‑based evaluation has become the default standard for assessing natural language understanding (NLU) systems, and many recent models now claim to surpass human performance on popular benchmarks. In this paper we scrutinise the validity of such “superhuman” claims by analysing two central datasets—SuperGLUE and SQuAD. We reveal that the reported human baselines are often computed on arbitrarily small, non‑representative subsets of the test data, whereas systems are evaluated on the full test set. Moreover, we identify pervasive annotation errors (e.g., entailment mistakes) and spurious patterns that models exploit but humans cannot. Human scores are further confounded by low or unspecified remuneration, variable annotator backgrounds, and opaque hiring procedures, all of which undermine the comparability between best‑in‑class systems and the best possible human performance. We argue that the use of such flawed human baselines renders claims of superhuman performance scientifically unsound. Our analysis highlights the brittleness of current NLU models and calls for more rigorous, transparent, and motivated human evaluation protocols. We conclude with concrete recommendations for constructing future benchmarks that enable fair, meaningful comparisons between artificial and human intelligence.</sample>
    <sample id="351">**Abstract**  
The CoNLL‑2003 dataset has been the de‑facto benchmark for Named Entity Recognition (NER) for almost two decades, yet it remains unclear whether models trained on this legacy data still generalize to contemporary text. We address this question by introducing **CoNLL++**, a new benchmark consisting of Reuters news articles from 2020 that are re‑annotated with the exact CoNLL‑2003 guidelines. We fine‑tune 20 state‑of‑the‑art NER models on the original CoNLL‑2003 training split and evaluate them on both the legacy test set and CoNLL++. The relative change in F1 score between the two test sets serves as a proxy for generalization.

Our experiments reveal three principal factors that influence a model’s ability to transfer to recent data: (1) architecture—transformer‑based models consistently outperform non‑transformer baselines; (2) model size—larger models exhibit stronger generalization; and (3) fine‑tuning data volume—more annotated examples improve downstream performance. To explain observed performance drops, we test two hypotheses. Adaptive overfitting, arising from repeated exposure to the same test set, is ruled out, as evidenced by an increasing‑return trend when moving from CoNLL‑2003 to CoNLL++. In contrast, temporal drift—degradation caused by widening temporal gaps between training and test data—accounts for the majority of the drop. We confirm this by re‑pre‑training selected models on newer corpora and observing a monotonic decline in performance with increasing recency gaps.

In summary, despite a temporal shift, contemporary CoNLL‑2003 taggers still perform competitively on 2023 data when appropriately scaled and fine‑tuned. Our findings underscore the importance of architectural choices, model capacity, and data quantity for robust generalization, and highlight temporal drift as the primary challenge for legacy NER systems. The CoNLL++ dataset, code, and experimental results are publicly released to encourage further research in this direction.</sample>
    <sample id="352">ABC‑Eval stands for **“Annotating Behaviors in Chat.”**</sample>
    <sample id="353">**Abstract**  
Code generation from natural‑language descriptions (NLDs) remains brittle when the input is underspecified, a common scenario in real‑world programming tasks. We argue that interactive clarification can mitigate this issue, but first we must identify where specifications are missing and determine the level of granularity needed. To that end, we introduce **CodeClarQA**, a synthetic dataset that annotates key operation‑level specifications within code and generates corresponding clarification questions (yes‑no or multiple‑choice). Key operations are extracted from a code knowledge graph (Graph4Code) and represented by a schema of essential elements; similarity thresholds between an NLD’s schema and operation documentation flag missing operations. Human annotators validate missing specifications on held‑out sets.  

We evaluate several models for missing‑operation detection, finding MPNet to perform best. Error analysis highlights taxonomy and argument‑value challenges. Building on this, we design a clarification‑driven generation pipeline comprising a Clarification Need Predictor, a Question Selector, and a Code Generator. Experiments confirm that incorporating clarifications improves code quality over baselines, though the full pipeline still trails a model trained only on NLD–code pairs, reflecting the difficulty of the clarification‑ranking task. Analysis shows that clarified key operations largely explain the performance gains, with oracle clarifications yielding near‑ground‑truth code. This work demonstrates the feasibility and benefits of interactive, question‑based code synthesis and provides resources for further research.</sample>
    <sample id="354">The study only reports a delta for the 2020 Reuters data (CoNLL++).  In that year the drop relative to CoNLL‑2003 exceeds 5 pp; no other years were examined.</sample>
    <sample id="356">**Affiliations**

- **Matthias Lindemann** – University of Freiburg, Germany  
- **Alexander Koller** – University of Freiburg, Germany  
- **Ivan Titov** – University of Cambridge, United Kingdom</sample>
    <sample id="357">The speaker is **Siyu Yuan**.</sample>
    <sample id="358">Five authors are involved in the paper.</sample>
    <sample id="359">It is compared against the **state‑of‑the‑art simultaneous pre‑translation architecture**—the dedicated SimulST model specifically designed for real‑time speech translation.</sample>
    <sample id="361">**Abstract**  
Multi‑step quantitative reasoning over structured data—such as financial tables—requires models to execute a sequence of arithmetic operations. Current neural architectures achieve modest accuracy on such tasks, particularly when the reasoning chain exceeds two steps, because they tend to memorize spurious correlations between frequent tokens and specific operations. We introduce *CounterComp*, a self‑supervised approach that mitigates this problem by exploiting counterfactual question variants. For each training example (the anchor), we mine a *positive* counterfactual (a question whose textual perturbation leaves the answer unchanged) and a *negative* counterfactual (a question whose perturbation changes the answer). These triplets are used to train an auxiliary metric‑learning objective with a dynamic margin that scales with the degree of question modification. Adding this loss to three strong baselines consistently improves performance on in‑distribution test sets and, more importantly, on out‑of‑distribution scenarios—both cross‑dataset transfer and unseen in‑dataset examples—thereby advancing compositional generalization. Qualitative analysis shows that models trained with CounterComp attend to semantically meaningful tokens that correspond to the underlying operations. These results demonstrate that counterfactual mining and metric learning can be a low‑cost, effective strategy to enhance compositional reasoning in quantitative question answering.</sample>
  </task>
</testset>