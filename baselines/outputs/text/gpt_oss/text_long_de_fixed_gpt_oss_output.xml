<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für die meisten großen Sprachmodelle sind:

| Quelle | Typ | Beispiele |
|--------|-----|-----------|
| **Web‑Crawls** | Gesamtheitlicher Internet‑Scraping (z. B. Common Crawl) | C4‑Corpus, Wikipedia‑Dump, diverse Webseiten |
| **Nachrichten‑Websites** | Nachrichtenartikel, Presseberichte | New York Times, Los Angeles Times, The Guardian, Huffington Post, usw. |
| **Soziale Medien** | Nutzer‑generierte Inhalte | Reddit‑Korpora (verschiedene Subreddits), Twitter‑Threads, etc. |
| **Weitere öffentliche Korpora** | Textsammlungen | OpenWebText, BooksCorpus, Project Gutenberg, etc. |

Kurz gesagt: große, heterogene Web‑Crawls (insbesondere Nachrichten‑ und Social‑Media‑Daten) bilden die Hauptbasis für das Training moderner Sprachmodelle.</sample>
    <sample id="1">Die Autoren sind von der **McGill University** (zusammen mit Mila und Microsoft Research).</sample>
    <sample id="2">**Abstract (≈170 Wörter)**  

In diesem Beitrag stellen wir *LayoutMask*, ein neues vortrainiertes Modell für die visuell‑reiche Dokumentverarbeitung (VrDU), vor. Im Gegensatz zu bisherigen Ansätzen, die globale 1‑D‑Positionskennungen (0, 1, 2 …) nutzen, setzt LayoutMask auf **lokale 1‑D‑Positionen** innerhalb einzelner Textsegmente. Dadurch werden Kreuzsegment‑Ordnungssignale durch die Kombination von 1‑D‑, 2‑D‑Layout‑ und semantischen Informationen erfasst, was eine effektivere Text‑Layout‑Interaktion ermöglicht.  

Zur Förderung dieser Interaktion erweitern wir die klassische Masked Language Modeling‑(MLM)‑Methode um:  
1) **Whole‑Word‑Masking (WWM)** – Maskierung auf Wortebene, um Kontextabhängigkeit zu erhöhen.  
2) **Layout‑Aware Masking (LAM)** – höhere Maskierungswahrscheinlichkeit für die ersten und letzten Wörter eines Segments, sodass das Modell über Segment‑Grenzen hinweg lernen muss.  

Außerdem führen wir die neue Aufgabe **Masked Position Modeling (MPM)** ein, bei der zufällig maskierte 2‑D‑Positionen rekonstruiert werden, wodurch die semantische und räumliche Kohärenz zusammengeführt wird.  

Experimente auf FUNSD, SROIE und CORD zeigen, dass lokale 1‑D‑Positionen zusammen mit WWM/LAM/MPM die Leistung bei der Erkennung von Entitäten, besonders bei komplexen Layout‑Strukturen (z. B. „Total“), signifikant steigern.</sample>
    <sample id="3">**Regina Stodden**  
„Hallo! Willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Identifikation von deutschen Texten auf Dokumentenebene sowie auf Satzebene. Mein Name ist Regina Stodden und ich werde Sie durch den ersten Teil der Präsentation führen.  

Zuerst definieren wir Textvereinfachung. Textvereinfachung ist ein Prozess, bei dem ein Text so angepasst wird, dass er für eine bestimmte Zielgruppe – etwa Menschen mit Leseschwierigkeiten oder Nicht‑Muttersprachler – besser verständlich wird. Um ein Modell für die Textvereinfachung zu trainieren, benötigen wir parallele Textpaare, zum Beispiel von Dokumenten oder Sätzen. Im folgenden Beispiel sehen Sie ein parallel ausgerichtetes Satzpaar: ein komplexer deutscher Satz und seine Übersetzung in einfache Sprache. Um den Satz zu vereinfachen, können verschiedene Techniken angewendet werden – wie Sie im Beispiel sehen: lexikalische Substitution, Klausel‑Deletion, Neuordnung oder die Einfügung von Wörtern.  

Wir stellen unseren neuen Korpus DEPLAIN vor, weil in den letzten Jahren Probleme mit bestehenden Korpora aufgetreten sind. Beispielsweise sind diese Korpora zu klein, um ein Textvereinfachungsmodell zu trainieren, und die drei in den letzten Jahren vorgeschlagenen Korpora sind alle automatisch ausgerichtet, was fehleranfällige Ausrichtungen zur Folge haben kann. Deshalb präsentieren wir DEPLAIN, der in zwei Teilkorpora aufgeteilt ist: DEPLAIN‑apa und DEPLAIN‑web.  

DEPLAIN‑apa basiert auf Nachrichtentexten. In DEPLAIN‑apa haben wir 483 Dokumente vollständig manuell ausgerichtet, was ungefähr 13 000 parallele Satzpaare ergibt.  

DEPLAIN‑web umfasst verschiedene Domänen und wir haben ebenfalls alle 750 Dokumente manuell und mit automatischen Ausrichtungsverfahren ausgerichtet. Insgesamt entstehen 30 450 Satzpaare.  

Wir haben die Satzpaare noch einmal genauer analysiert, zum Beispiel hinsichtlich der Art der Vereinfachung. Auf der Grafik sehen Sie, dass die Bibeltexte deutlich stärker vereinfacht sind als zum Beispiel Nachrichten‑ oder Sprachlerntexte. Auf allen Ebenen – etwa lexikalische Vereinfachung, strukturelle Vereinfachung oder der Gesamt­vereinfachungsgrad – ist das zu beobachten.  

Weiterhin zeigt sich, dass der DEPLAIN‑Korpus eine hohe Vielfalt verschiedener Vereinfachungs­transformationen aufweist. In DEPLAIN‑apa finden wir zum Beispiel deutlich mehr Neuordnungen und Wortergänzungen als in DEPLAIN‑web, während im Web‑Korpus mehr Umformulierungen vorkommen.  

---

**Omar**  
„Nun möchte ich die Anwendungsfälle für unseren Datensatz DEPLAIN erläutern.  

Erstens können wir automatische Ausrichtungsverfahren evaluieren. In den letzten Jahren wurden zahlreiche Ausrichtungsverfahren entwickelt, insbesondere im Bereich der maschinellen Übersetzung, wo es darum geht, Sätze in zwei parallelen Dokumenten, die in unterschiedlichen Sprachen verfasst sind, auszurichten. Unser Anwendungsfall unterscheidet sich: wir wollen Sätze in zwei parallelen Dokumenten ausrichten, die dieselbe Sprache und denselben Inhalt haben, aber auf unterschiedlichem Komplexitäts­niveau liegen. Mit DEPLAIN, das manuell ausgerichtete Sätze enthält, können wir diese Sätze als Gold‑Standard‑Ausrichtungen nutzen, um verschiedene vorgeschlagene Verfahren zu bewerten. Wir haben einige dieser Verfahren angepasst, die Anpassungen sowie den Code zur Ausführung unserer Experimente im Papier veröffentlicht. Abschließend haben wir festgestellt, dass das automatische Ausrichtungsverfahren MASSalign die beste Wahl für die deutsche Textvereinfachung ist. Den Code, um dieses Verfahren selbst auf Ihren Dokumenten anzuwenden, finden Sie ebenfalls im Papier.  

Zweitens haben wir einen Anwendungsfall der automatischen Textvereinfachung gezeigt, bei dem wir Sprachmodelle feinabgestimmt haben, um aus komplexem Eingangstext vereinfachte Texte zu erzeugen. Wir haben zwei Modelle feinabgestimmt: Long‑mBART für dokumentenweite Vereinfachungen und das Basis‑mBART für satzweite Vereinfachungen. Alle Check‑points stehen zur Verfügung, und Sie können sich die detaillierten Ergebnisse sowie die Evaluationsmetriken in unserem Papier anschauen. Wir konnten zeigen, dass diese einfache Feinabstimmung bessere Ergebnisse als die Baselines liefert, und wir schlagen diese Resultate als Ausgangsbewertung für zukünftige Arbeiten zur automatischen Textvereinfachung vor.  

Vielen Dank für Ihre Aufmerksamkeit. Wir freuen uns darauf, Sie alle auf der Konferenz zu treffen.“</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">Das Modell ist die **T5 XL**.</sample>
    <sample id="6">**Abstract**

In this work we introduce *many‑to‑many summarization*, a unified framework that simultaneously handles multilingual (same source and target language) and cross‑lingual (different source and target languages) summarization. The goal is to train a single model capable of summarizing a document in any source language into a summary in any target language. We conduct a preliminary study on the WikiLingua benchmark (English, French, Hindi, Chinese, Thai, Turkish) and compare four variants of mBART‑50: (1) *mBART ONE* – one‑direction models, (2) *mBART U‑CLS* – a single model trained on all cross‑lingual pairs, (3) *mBART MLS* – a unified multilingual model trained on monolingual pairs, and (4) *mBART Many‑to‑Many* – our proposed setting. Results show that the many‑to‑many model yields better cross‑lingual knowledge transfer than the other configurations.

To further exploit this setting, we propose **PISCES**, a pre‑trained many‑to‑many summarization model built through a three‑stage pre‑training pipeline: (i) *meta pre‑training* reconstructs original sentences from noisy inputs, (ii) *cross‑lingual pre‑training* generates target‑language sentences from noisy parallel pairs, and (iii) *task‑specific pre‑training* uses pseudo summarization data. PISCES outperforms strong baselines such as mBART‑50 and mT5, and ablation as well as human evaluations confirm the contribution of each pre‑training stage.</sample>
    <sample id="7">Ja – CoNLL‑2003‑Tagger funktionieren immer noch in 2023.  
Die Studie zeigt, dass moderne Transformer‑Modelle, größere Größen und mehr Feintuning‑Beispiele die Generalisierung verbessern. Der Leistungsrückgang, den manche Modelle zeigen, ist vor allem auf **zeitlichen Drift** zurückzuführen, nicht auf adaptives Overfitting. In Summe: CoNLL‑2003‑Tagger sind nach wie vor brauchbar, solange sie mit geeigneten Architekturen, genügend Daten und ggf. Aktualisierung der Trainingsdaten gepflegt werden.</sample>
    <sample id="8">Die neue Methode, ABC‑Eval, annotiert jede Modellantwort nicht mehr mit einer allgemeinen „Güte‑Bewertung“, sondern markiert explizit, ob bestimmte **Verhaltensweisen** vorliegen (z. B. Irrelevanz, Selbst‑/Partner‑Widerspruch, Halluzination, Empathie‑Fehlen). Durch diese verhaltens‑spezifische, turn‑by‑turn‑Annotation wird die Subjektivität reduziert und die Fehlerhäufigkeiten präzise gemessen.</sample>
    <sample id="9">Der Erfolg der aktuellen WSL‑Methoden hängt primär davon ab, dass man ein **sauberes Validierungs‑/Test‑Set** hat – ohne mindestens ~20 sauber annotierte Beispiele pro Klasse verlieren sie die Generalisierung und können nicht mehr über die schwachen Labels hinausbleiben.  Mit einer kleinen Menge sauberer Daten (≈10‑20 pro Klasse) kann man zudem schlichtes Fine‑Tuning auf diesen Beispielen durchführen, was die komplexeren WSL‑Ansätze gleichwertig oder sogar übertreffen kann.</sample>
    <sample id="10">Um die Genauigkeit weiter zu steigern, kann man:

1. **Bessere Hintergrund‑Abfrage** – Automatisierte Retrieval‑Module nutzen umfangreichere, aktuellere Quellen (z. B. Wikipedia‑API, Musik‑Datenbanken, Rezept‑APIs), um die relevanten Fakten schneller und präziser zu holen.  
2. **Retrieval‑Augmented Generation (RAG)** – Kombiniere ein Retrieval‑System mit einem generativen Modell, sodass das Modell die richtigen Fakten dynamisch einbeziehen kann.  
3. **Domain‑spezifische Fine‑Tuning‑Datasets** – Erweitere das Training um mehr indirekte Referenzen aus jedem Domain‑Szenario (Musik, Bücher, Rezepte) und füge multimodale Hinweise (Bilder, Audio‑Snippets) hinzu.  
4. **Mehr Kontext‑Encoding** – Lass das Modell den vorherigen Dialog (erste und zweite Sprechblase) als Kontext nutzen, etwa durch Prompt‑Engineering oder ein Memory‑Module.  
5. **Erweiterte Annotation** – Füge zusätzliche Meta‑Infos (z. B. Entitätseigenschaften, Nutzer‑Intention) ein, damit Modelle lernen, welche Attribute für die Auswahl relevant sind.  
6. **Evaluation mit realen Nutzern** – Teste das System in echten Chat‑Szenarien, um die Robustheit gegen unvollständige Infos und natürliche Sprachvariationen zu prüfen.  

Durch diese Schritte wird das Modell nicht nur auf die exakten Daten trainiert, sondern lernt, fehlende Fakten aktiv zu ergänzen und kontextuell passende indirekte Referenzen zu erkennen.</sample>
    <sample id="11">**Abstract**  
In dieser Arbeit wird ein neues Benchmark zur Messung des Humorerkennungsvermögens großer Sprachmodelle vorgestellt, basierend auf dem New Yorker Caption Contest. Für über 700 Cartoons wurden Bildbeschreibungen, Hervorhebungen und Entitätsverweise sowie 650 zweier bis vier Satz‑Erklärungen zu den besten Captions annotiert. Das Benchmark umfasst drei Aufgaben: (1) **Matching** – Auswahl des richtigen Captions unter fünf Optionen, (2) **Qualitäts‑Ranking** – Bewertung zweier Captions, die beide zum selben Bild passen, und (3) **Erklärungsgenerierung** – Erklären, warum ein Cartoon‑Caption witzig ist.  

Bei Matching erreichte ein auf CLIP basierendes Modell 62 % Genauigkeit gegenüber 94 % bei Menschen, während GPT‑4 (mit Bildbeschreibung) ebenfalls deutlich unterhalb menschlicher Leistung blieb. Für die Erklärungsgenerierung zeigte GPT‑4 häufig faktische Fehler und wurde in Blindstudien in mehr als zwei Dritteln der Fälle von menschlichen Erklärungen übertroffen. Das veröffentlichte Datensatz‑ und Leaderboard‑Portal soll weitere Forschungen zur humoristischen Text‑ und Bildverarbeitung anregen.</sample>
    <sample id="12">Fünf Autoren – Dawei plus Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">**Abstract (≈ 180 Wörter)**  

Adaptive Inference reduziert die Ausführungszeit großer Sprachmodelle, indem einfachere Daten mit kleineren Modellen bearbeitet werden. Die zwei verbreitetsten Varianten sind (1) Multi‑Model, bei dem mehrere Modelle nacheinander ausgeführt werden, bis ein Klassifikator entscheidet zu stoppen, und (2) Early‑Exit, bei dem mehrere Klassifikatoren an Zwischenschichten eines einzigen Modells angebracht sind. Multi‑Model ist vielseitig, aber speicherintensiv und weist einen Overhead auf, wenn späterere Modelle verwendet werden. Early‑Exit ist speichereffizient und ohne Overhead, kann jedoch durch „conflicting gradients“ leiden, da verschiedene Klassifikatoren gleichzeitig die gemeinsamen Modellgewichte aktualisieren.  

Durch einen fairen Vergleich mit abgeschnittenen BERT‑Modellen zeigte sich, dass Multi‑Model die Early‑Exit‑Klassifikatoren um durchschnittlich 2,3 % (BERT‑Base) und 5,2 % (BERT‑Large) übertrifft, insbesondere bei frühen Klassifikatoren.  

Zur Lösung des Problems wurde SWEET (Separating Weights in Early‑Exit Transformers) eingeführt: jede Transformer‑Schicht wird nur von dem nachfolgenden Klassifikator aktualisiert, wodurch die Konflikte eliminiert werden. SWEET schließt die Leistungslücke zu Multi‑Model und liefert bei hohen Geschwindigkeiten die beste Speed/Accuracy‑Kurve, insbesondere bei BERT‑Large.  

Die Arbeit demonstriert die Existenz von Konfliktgradienten, bietet einen ersten fairen Vergleich der beiden Ansätze und liefert SWEET als Ausgangspunkt für weitere Optimierungen.</sample>
    <sample id="14">**Titel:** Die Abhängigkeitsstruktur der Koordination

**Einleitung**  
Hallo, mein Name ist Adam Przepiórkowski. In diesem Vortrag geht es um die Abhängigkeitsstruktur von Koordination. Wie Sie vielleicht wissen, gibt es unterschiedliche theoretische und korpusbasierte Annahmen darüber, wie Koordinationskonstruktionen strukturiert sind.  

**Beispiel 1 – Asymmetrische Ansätze**  
Im Universal Dependencies‑Korpus (UD) sowie in Igor Mel'čuks *Meaning‑Text‑Theory* ist die Koordination so strukturiert, dass das erste Konjunkt (das erste Element der Koordination) die Spitze (den „Head“) der gesamten Koordination bildet.  
Beispiel: *Lisa, Bart und Maggie* – hier ist *Lisa* der Kopf der Koordination.  
Diese beiden Theorien sind also asymmetrisch, weil sie ein bestimmtes Element (das erste) hervorheben.

**Beispiel 2 – Konjunkt‑geführter Ansatz**  
Im Prag‑Ansatz, wie er in den Prag‑Abhängigkeitsbaumkorpora verwendet wird, wird die Koordination von der Konjunktion selbst geleitet. Das bedeutet, dass die Konjunktion die Spitze der Koordination ist und von ihr Abhängigkeiten zu allen Konjunkten gerichtet werden.  

**Beispiel 3 – Mehr‑geführter Ansatz**  
In Hudsons *Word‑Grammar* wird ein mehr‑geführter Ansatz verwendet: alle Konjunkten sind zugleich Köpfe der Koordination. Somit entstehen Abhängigkeiten vom Governer zu jedem Konjunkt einzeln, z. B. *loves* zu *Lisa, Bart und Maggie*.

**Ziel der Arbeit**  
Das Ziel dieser Arbeit ist es, ein neues Argument für symmetrische Koordinationsstrukturen (wie Beispiel 1) und gegen asymmetrische Strukturen (wie Beispiel 2) zu liefern. Das Argument stützt sich auf das Prinzip der Minimierung der Abhängigkeitslänge (*dependency length minimization* – DLM).

---

## Prinzip der Minimierung der Abhängigkeitslänge (DLM)

In Englisch bevorzugen direkte Objekte typischerweise eine Nähe zum Verb, während Adverbiale (Adjuncts) weiter entfernt sein können.  
- *Marge read it yesterday.* – hier ist das direkte Objekt („it“) nahe am Verb, was gut klingt.  
- *Marge read yesterday it.* – hier liegt zwischen Verb und Objekt ein Adverbial („yesterday“), was die Präferenz verletzt und das Satzklang schlechter macht.

**Erleichterung bei langen, schweren Objekten**  
Wenn das direkte Objekt sehr lang oder schwer ist, kann es sinnvoll sein, es nach dem Adverbial zu platzieren, ohne dass das DLM verletzt wird.  
- *Marge read this absolutely fascinating book about bees yesterday.* – hier ist das lange Objekt („this absolutely fascinating book about bees“) nahe am Verb.  
- *Marge read yesterday this absolutely fascinating book about bees.* – auch hier ist das Objekt nahe, weil das Gesamtschema die Abhängigkeitslänge minimiert.  

**Begründung mit Abhängigkeitslängen**  
In den beiden Sätzen betrachten wir nur die beiden wesentlichen Abhängigkeiten:  
1. Verb → Adverbial (Länge = 7 Wörter)  
2. Verb → Objekt (Länge = 4 Wörter)  
Summe = 11.  
Wenn wir die beiden Konstituenten tauschen, wird die Summe der Längen 6 (Verb → Adverbial = 2, Verb → Objekt = 4). 6 &lt; 11, also ist die zweite Anordnung kürzer und damit bevorzugt.  

---

## Empirische Untersuchung

Wir haben verschiedene Statistiken zur Koordination aus der Enhanced Version des Penn Treebank extrahiert (vgl. „Why wouldn’t you use Universal Dependencies?“). Diese Statistiken bestätigen die langjährigen Beobachtungen, dass linke Konjunktoren tendenziell kürzer sind:

- „salt and pepper“ statt „pepper and salt“ (gemessen in Silben).  
- Die Tendenz nimmt zu, je größer der Unterschied in der Länge der beiden Konjunktoren ist.  

**Neuer Befund**  
Der neue Befund lautet: Diese Tendenz tritt nur auf, wenn der Governer (das übergeordnete Element) links steht oder nicht vorhanden ist.

- **Governer links**  
  Beispiel: *I saw Bart and Lisa.*  
  Der Governer (*saw*) steht links.  
  Hier bevorzugt der linke Konjunkt (Bart) die kürzere Länge, besonders wenn der Unterschied groß ist.  

- **Kein Governer**  
  Beispiel: *Homer came and sneezed.*  
  Koordination von zwei Verben ohne externen Governer.  
  Gleiches Muster: der linke Konjunkt ist kürzer.  

- **Governer rechts**  
  Beispiel: *Ted and Ned laughed.*  
  Der Governer (*laughed*) steht rechts.  
  In diesem Fall verschwindet die Tendenz, dass der linke Konjunkt kürzer ist.  

Wir haben die Längen in Zeichen, Silben und Wörtern gemessen; die Ergebnisse sind in der rechten Spalte (Wörter) am deutlichsten. Wenn der Governer links steht, wächst die Tendenz, dass der linke Konjunkt kürzer ist, stetig mit zunehmendem absoluten Unterschied in der Wortlänge. Bei rechts stehenden Governern verschwindet diese Tendenz.

---

## Schlussfolgerung

Diese Beobachtung liefert ein Argument gegen asymmetrische Koordinationsstrukturen (erste Konjunkt als Kopf oder Konjunktion als Kopf) und spricht für symmetrische Strukturen, bei denen kein einzelnes Element die Koordination dominiert.  

Weitere Details und die vollständige Argumentation finden Sie in der zugehörigen Arbeit. Wir freuen uns auf Fragen und eine anregende Diskussion beim Poster‑Session.  

Vielen Dank.</sample>
    <sample id="15">Drei Autoren sind an der Arbeit beteiligt – Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Analyse zeigt, dass die Bibeltexte am stärksten vereinfacht werden – sie weisen einen deutlich höheren Simplifizierungsgrad als beispielsweise Nachrichten- oder Lerntext‑Domänen auf.</sample>
    <sample id="17">**Abstract**

Wir stellen einen multimodalen Ansatz zur Relationsextraktion (RE) vor, der die Stärken von Text‑ und Bilddaten kombiniert und gleichzeitig die Probleme „intern‑Information‑Übernutzung“ und „extern‑Information‑Unterausnutzung“ adressiert. Zunächst werden Text‑ und Bildbeschreibungen in jeweiligen Szenengraphen kodiert, die zu einem gemeinsamen Cross‑Modal Graph (CMG) fusioniert werden. Durch ein Graph‑Information‑Bottleneck‑Prinzip filtern wir feinkörnig irrelevante Knoten und passen die Kanten an, um Rauschen zu eliminieren. Anschließend ergänzen wir die komprimierten CMG‑Features mit latenten multimodalen Topic‑Features, die aus den Top‑L‑Text‑ und Bild‑Keywords abgeleitet und über ein Attention‑Modul integriert werden. Experimente auf dem MRE‑Dataset zeigen, dass unser Modell die führenden multimodalen Baselines übertrifft. Ablationsanalysen bestätigen, dass sowohl das interne Screening als auch die externe Topic‑Enrichment entscheidend sind; bei hohen Text‑Bild‑Relevanzen dominiert das Screening, bei niedrigen Relevanzen die Topic‑Enrichment. Diese Kombination von Informationssubtraktion und -addition liefert signifikante Verbesserungen in der multimodalen Relationsextraktion.</sample>
    <sample id="18">Das Beispiel lautet: **„salt and pepper“** (kurze linke Konjunktion) gegenüber **„pepper and salt“**.</sample>
    <sample id="19">**Abstract**

Open‑Domain Question Answering (ODQA) typically follows a two‑stage pipeline: a retrieval module first selects evidence passages from a Wikipedia corpus, and a reader module then extracts or generates the final answer. Despite its effectiveness, ODQA faces severe resource constraints: the corpus (≈26 M documents) occupies ≈20 GB, the inverted index can reach 65 GB, and inference speed is bottlenecked by index search. Moreover, state‑of‑the‑art models contain millions of parameters, making real‑time deployment on constrained devices impractical.  

This survey surveys recent efficiency tactics. For retrieval, approximate nearest‑neighbour search, document filtering, dimensionality reduction, and product quantization shrink index size and speed up search. For reading, adaptive computation and skip‑reading skip irrelevant context. Model‑level optimisations include lightweight backbones, parameter sharing, and one‑stage architectures that combine retrieval and reading into a single model.  

Empirical comparison shows retrieval‑reader systems offer the best trade‑off among speed, memory, and accuracy; retrieval‑only systems are fastest but still need large indices; generator‑only systems avoid indexing but demand huge models and achieve lower performance.  

We conclude with guidelines for resource‑constrained settings and outline future work on low‑power deployment and richer evaluation metrics.</sample>
    <sample id="20">Ja – die Modelle sind frei verfügbar (Hugging Face, MIT‑Lizenz) und können ohne Einschränkung für Forschungszwecke genutzt, angepasst und weiterentwickelt werden.</sample>
    <sample id="21">DEplain‑apa besteht aus Nachrichten‑Texten (nachrichtenbasierte Dokumente).</sample>
    <sample id="22">Eine gute Generalisierung wird laut der Studie durch drei Hauptfaktoren gefördert:

1. **Modellaarchitektur** – Transformer‑basierte Modelle generalisieren am besten.  
2. **Modellgröße** – Größere Modelle führen zu besserer Generalisierung.  
3. **Anzahl der Fine‑Tuning‑Beispiele** – Mehr Trainingsdaten (Fine‑Tuning‑Samples) verbessern die Leistung auf neuen Daten.</sample>
    <sample id="23">**Abstract**

Text‑Image‑Modelle wie *Imagen* erzeugen hochwertige Bilder, scheitern jedoch häufig an korrekter Textdarstellung. Der Grund liegt in der Subword‑Tokenisierung des T5‑Encoders: beim Rendern eines Wortes muss das Modell die Sub‑Tokens in einzelne Buchstaben auflösen. In Experimente zeigte sich, dass T5‑Base und T5‑Large weniger als 20 % korrekte Rechtschreibung liefern, während T5‑XXL knapp 70 % erreicht. Im Gegensatz dazu bewältigen PaLM‑Modelle die Aufgabe nahezu fehlerfrei, sind jedoch deutlich größer. Der Byte‑level‑Encoder *ByT5* besitzt dagegen direkten Zugriff auf die Zeichen und erzielt auf allen Größenordnungen hohe Spelling‑Genauigkeit, unabhängig von der Wortfrequenz.  

Um die Bild‑Renderer zu verbessern, wurde die *Imagen*‑Architektur um die Textrepräsentation des ByT5‑Small‑Modells erweitert. Diese leichte Ergänzung (≈ +5 % Parameter) erhöht die Textqualität signifikant, ohne die Diffusions‑Schritte zu verändern.  

Für die Bewertung wurden die Benchmarks **WikiSpell** (nur Text) und **DrawText** (Text‑zu‑Bild) eingeführt. Die Studie zeigt, dass die Kombination eines byte‑sensitiven Encoders mit einem subword‑basierten Modell eine effiziente Lösung für die Text‑Rendering‑Problematik in multimodalen Modellen darstellt.</sample>
    <sample id="24">Die Tendenz wurde durch eine quantitativen Analyse des erweiterten Penn‑Treebank‑Datensatzes ermittelt. Für jede Koordination wurden die Längen der beiden Konjunkt­teile (in Zeichen, Silben oder Wörtern) gemessen. Anschließend wurde die Differenz der Längen berechnet und die Häufigkeit ermittelt, mit der der linke Konjunkt­teil der kürzere ist – als Funktion der Länge­differenz. Diese Proportion zeigte, dass der linke Konjunkt bei größeren Längen­unterschieden vermehrt kürzer ist.</sample>
    <sample id="25">Die Untersuchung bestand aus einer korpus‑basierten Analyse der Enhanced‑Penn‑Treebank. Dabei wurden alle Koordinations‑Strukturen extrahiert und in drei Gruppen klassifiziert: (1) Koordinationen mit einem linken Governator, (2) Koordinationen ohne Governator (z. B. koordinierte Verben), und (3) Koordinationen mit einem rechten Governator. Für jede Koordination wurden die Längen der beiden Konjunktionen in Zeichen, Silben und Wörtern gemessen. Anschließend wurden die prozentualen Häufigkeiten ermittelt, bei denen die linke Konjunktion kürzer war, in Abhängigkeit vom absoluten Längenunterschied. Durch die Gegenüberstellung der drei Gruppen konnte die Effekte der Position des Governators auf die Längenverteilung der Konjunktionen untersucht werden.</sample>
    <sample id="26">Ein Basisklassifikator, der auf stark unausgewogenen Daten (z. B. 43 Dissonanz‑Beispielen) trainiert wird, liefert kaum besser als Zufall – die Leistung liegt nahe bei 50 % bzw. einem AUC von knapp 0,5.</sample>
    <sample id="27">Die Präsentation nennt keinen konkreten Autor*innen‑Verteiler – es wird lediglich der Vortragende, Shangbin, erwähnt.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ‑Modelle zeigen deutlich bessere Ergebnisse bei **Formality (Formalität)** und **lexikalischer Kohäsion**.</sample>
    <sample id="30">**Abstract**

Wir stellen *LLM‑Blender* vor, einen einfachen, aber leistungsstarken Ensemble‑Ansatz für große Sprachmodelle (LLMs). Anstatt ein einziges Modell für alle Eingaben zu wählen, führen wir zunächst \(n\) unterschiedliche LLMs aus und erhalten \(n\) Kandidatenausgaben. Ein Pairwise‑Ranking‑Modul namens **PairRanker** bewertet jedes Paar von Kandidaten im Kontext der Eingabe, indem es die Eingabe und die beiden Antworten zusammen in einen Cross‑Attention‑Encoder (z. B. RoBERTa) einspeist. Die daraus resultierende Vergleichsmatrix wird anschließend aggregiert (idealerweise mittels Max‑Logits, alternativ Bubble‑Sort), um eine Rangfolge der Kandidaten zu bestimmen. Die Top‑\(K\) (typischerweise drei) werden als Eingabe an ein sequenz‑zu‑sequenz‑Modell **GenFuser** übergeben, das diese Antworten fusioniert und die finale Ausgabe erzeugt.  

Zur Evaluierung haben wir den neuen Datensatz **MixInstruct** erstellt, der Anweisungs‑Daten mit Kandidaten aus 11 Open‑Source‑LLMs kombiniert. Wir messen Qualität mit BERTScore, BLUERT, BARTScore und einem ChatGPT‑Jury. Unsere Experimente zeigen, dass PairRanker die „Oracle‑Ranking“-Korrelation deutlich übertrifft und dass LLM‑Blender die Leistung von Top‑Modellen (z. B. Vicuna, Open‑Assistant) in 68 % bzw. 76 % der Beispiele übersteigt. Die vorgestellte Pipeline ist modular, leicht reproduzierbar und bietet einen neuen Standard für Ensemble‑Lernverfahren in der LLM‑Forschung.</sample>
    <sample id="31">Die meisten Autoren stammen vom **University of Illinois Urbana‑Champaign** (UIUC).  
Einzige Ausnahme ist **Roger Levy**, der zu **Columbia University** gehört.</sample>
    <sample id="33">Das Framework misst die Positionalität, indem es die Modell‑ und Datensatz‑labels mit den von vielen, demografisch vielfältigen Annotatoren erstellten Bewertungen vergleicht. Für jede demografische Gruppe wird der Pearson‑Korrelation‑Score zwischen den Modellvorhersagen (oder Dataset‑Labels) und den annotierten Bewertungen berechnet. Dieser Score gibt an, wie stark ein Modell bzw. ein Datensatz mit einer bestimmten Population übereinstimmt – je höher die Korrelation, desto stärker ist die Positionalität zu dieser Gruppe.</sample>
    <sample id="34">**Abstract**

CREST (Joint Framework for Rationalization and Counterfactual Text Generation) kombiniert selektive Rationalisierung und kontrafaktische Textgenerierung, um erklärbare und kontrollierbare Textänderungen zu erzeugen. Der Rahmen besteht aus einem Masker, der für ein Eingabe‑Text‑Beispiel \(X\) eine rationale Auswahl \(Z\) generiert, einem Predictor, der mit \(Z\) klassifiziert, und einem Editor (Maskiertes Sprachmodell), das auf \(Z\) basierende kontrafaktische Varianten \(\tilde{X}\) erzeugt, indem die relevanten Tokens maskiert und der Gold‑Label‑Token vorangestellt wird. CREST liefert kontrafaktische Beispiele, die in menschlichen Bewertungen hinsichtlich Validität und Natürlichkeit dem manuellen Vorgehen überlegen sind und CREST gegenüber MiCE schneiden. Durch die gleichzeitige Verarbeitung von originalen und kontrafaktischen Flows in einem gemeinsamen Rationalisierer wird die Plausibilität der Rationalen verbessert und ein neuer Regularisierungsterm fördert die Übereinstimmung mit den ursprünglichen Rationalen. Experimente auf IMDB und SNLI zeigen, dass CREST‑Rationalisierung die Leistung auf In‑Domain‑, Kontrast‑ und Out‑of‑Domain‑Datensätzen übertrifft, insbesondere bei Out‑of‑Domain. Abschließend wird ein neuer Messwert, „counterfactual simulability“, eingeführt, der die Fähigkeit einer Rationalisierung bewertet, Klassifikationsentscheidungen bei kontrafaktischen Modifikationen zu ändern. CREST liefert damit valide, fließende und vielfältige kontrafaktische Beispiele, die die Qualität erklärbarer Modelle signifikant steigern. (198 Wörter)</sample>
    <sample id="36">**Abstract**

Multilinguale maschinelle Übersetzung bietet Skalierbarkeit, schnellere direkte Übersetzungen und bessere Leistungen für Sprachen mit wenig Daten, jedoch reduziert sie die Kapazität pro Sprache. Wir stellen *Language‑Specific Layers* (LSL) vor, bei denen pro Sprache ein eigener Transformer‑Layer im Encoder existiert. Während des Trainings werden sowohl geteilte als auch sprachspezifische Gewichte verwendet; zur Bestimmung der optimalen LSL‑Platzierung trainieren wir ein großes Modell und wählen anschließend die Komponente mit dem größten Gewicht pro Ebene. Das resultierende Netzwerk besteht aus einer Mischung aus geteilten und sprachspezifischen Schichten (z. B. zwei geteilte, zwei portugiesische, mehrere geteilte, drei swahili‑spezifische und eine abschließende geteilte Schicht), sodass die Inferenzkosten unverändert bleiben. Auf WMT 21 News‑Translation mit 10 Sprachen (inkl. Swahili) und Evaluation auf Flores‑101 (chrF, spBLEU, COMET) übertrifft die LSL‑Architektur sowohl Sprachadapter als auch das größte baseline‑Modell, wobei die Verbesserungen insbesondere bei Low‑Resource‑Sprachen signifikant sind (84/90 Übersetzungsrichtungen). Unsere Methode kombiniert höhere Kapazität mit konstanten Laufzeitkosten und bietet einen skalierbaren Ansatz für hochqualitative, mehrsprachige Übersetzungsmodelle.</sample>
    <sample id="37">The earlier study found that when human participants were given the same “imagine you are …” persona prompts, they too produced responses that surfaced racial stereotypes.</sample>
    <sample id="38">In der Studie wurden hauptsächlich syntaktische Ressourcen aus der Linguistik verwendet:

* **Enhanced Penn Treebank** – die erweiterte Version, aus der die Koordinations‑Statistiken extrahiert wurden.  
* **Universal Dependencies** – die von der Universal‑Dependencies‑Community gepflegten Daten (z. B. als Referenz für die Analyse der Koordinations‑Strukturen).</sample>
    <sample id="39">Der Vortrag ist von Adam Przepiórkowski selbst gehalten – es handelt sich um eine Ein‑Autor‑Arbeit.</sample>
    <sample id="40">Engaged‑tasks that are most closely related to cognitive dissonance in our study are:

1. **Debate Stance Classification** – a topic‑independent task that labels whether two debate statements agree or disagree (used as a source for transfer learning).  
2. **Expansion &amp; Comparison Classification (CE)** – the binary classification of the two PDTB discourse classes that are conceptually linked to consonance and dissonance.</sample>
    <sample id="41">**Abstract**  
Wir stellen PeaCoK vor – einen Persona‑Commonsense‑Knowledge‑Graph, der das Welt‑Level‑Verständnis von Personen und deren Eigenschaften strukturiert. PeaCoK umfasst ca. 3 800 Personas, 40 000 Attribute‑Typen und 100 000 persönliche Inferenz‑Fakten, von denen 9 200 Attribute mehreren Personas zugeordnet sind. Die Relationen werden in drei Dimensionen beschrieben: vier Haupt‑Relationen (z. B. „Interessen“, „Beruf“, „Bildung“, „Verhalten“) sowie Interaktivität und Einzigartigkeit.  

Der Aufbau erfolgt in drei Schritten: (1) Auswahl von Personas aus bestehenden Commonsense‑Graphen, (2) Induktion von Attributen mittels Commonsense‑Graphen und vortrainierter Sprachmodelle (z. B. InstructGPT‑3) sowie (3) Crowdsourcing‑Annotationen mit einem Joint Human‑AI‑Majority‑Voting, das 87 % F1‑Genauigkeit erreicht.  

Wir demonstrieren die Nützlichkeit von PeaCoK:  
* Ein BART‑basierter Generator, trainiert auf PeaCoK, übertrifft Zero‑/Five‑Shot GPT‑3/3.5 bei der Vorhersage von Attributen.  
* In der Persona‑Chat‑Dialo­g‑Erzeugung (ConvAI2) steigert die Einbindung von PeaCoK‑Fakten die Flüssigkeit, Konsistenz, Engagement und Persona‑Ausdruck, besonders wenn die Sprecher gemeinsame Attribute teilen.  

PeaCoK liefert damit eine hochwertige, erweiterbare Persona‑Wissensbasis, die leichte Modelle mit persona‑sensiblen Fähigkeiten ausstattet und narrative Systeme verbessert.</sample>
    <sample id="42">Ein Autor.</sample>
    <sample id="43">Ein Autor.</sample>
    <sample id="44">Das NLPositionality‑Framework unterscheidet sich von bisherigen Ansätzen dadurch, dass es nicht nur die Inter‑Annotator‑Agreement‑Diskrepanzen untersucht, sondern **direkt die Entscheidungen der Endnutzer mit den Labels, Vorhersagen und dem Verhalten von Modellen bzw. Datensätzen vergleicht**.  

- Es re‑annotiert Beispiele mit einer großen, diversifizierten Stichprobe von Annotatoren und sammelt umfangreiche demografische Daten.  
- Anstatt nur die Übereinstimmung zwischen Annotatoren zu messen, wird die Übereinstimmung zwischen **konsentierten Nutzerlabels und Modell‑/Datensatzoutputs** mittels Pearson‑R‑Korrelation ermittelt.  
- Damit wird die „Positionalität“ von Modellen/Datasets in Bezug auf reale Nutzergruppen sichtbar – ein Schritt, der in der bisherigen Literatur nicht üblich war.</sample>
    <sample id="45">Die generierten Personas.</sample>
    <sample id="46">Die verglichenen kommerziellen Systeme waren **DeepL** und **Google Translate**.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit **„From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models“**. Sprachmodelle werden auf großskaligen Web‑Crawl‑Daten trainiert. Politische Nachrichtenmedien sind in diesen Trainingsdaten gut vertreten. Laut einer Umfrage des C4‑Korpus sind beispielsweise die New York Times, Los Angeles Times, The Guardian, Huffington Post usw. stark in der Trainingsbasis von Sprachmodellen vertreten. Das hat ein zweischneidiges Potenzial für Sprachmodell‑Anwendungen. Einerseits können sie aus vielfältigen Perspektiven lernen, was die Demokratie und die Pluralität von Ideen zelebriert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und können zu Fairness‑Problemen in downstream‑Aufgaben führen. 

Um dies zu untersuchen, schlagen wir vor, den Propagations‑Pipeline von politischen Vorurteilen zu analysieren – von den Trainingsdaten über die Sprachmodelle bis hin zu downstream‑Aufgaben. Konkret stellen wir uns folgende Fragen:  
1. **Wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle spielen die Trainingsdaten dabei?**  
2. **Wie wirken sich Sprachmodelle mit unterschiedlichen politischen Neigungen auf downstream‑Aufgaben aus und können daraus Fairness‑Probleme entstehen?**

Wir beginnen damit, Sprachmodelle mit unterschiedlichen Prompt‑Formaten zu testen, indem wir politische Fragebögen wie den „Political Conference Test“ einsetzen. Dies ermöglicht eine automatische Bewertung, die in der Politikwissenschaft etabliert ist. Erste Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen aufweisen und alle vier Quadranten des politischen Spektrums besetzen. GPT‑4 erweist sich beispielsweise als das liberalste Modell; generell sind die GPT‑Reihen sozial liberaler als die BART‑Reihen und deren Varianten.

Zweitens untersuchen wir, inwieweit die politischen Vorurteile von Sprachmodellen tatsächlich aus den Trainingsdaten stammen. Hierfür führen wir kontrollierte Experimente durch: Wir trainieren zusätzliche Sprachmodell‑Checkpoints auf sechs partizipierten Korpora, die nach Nachrichtenart (News vs. Social Media) und politischer Neigung (links vs. rechts) differenziert sind. Durch ein weiteres Training auf einem linksliberalen Reddit‑Korpus verschiebt sich die Ideologie von RoBERTa signifikant in Richtung Liberalismus. Wir untersuchen zudem, ob Sprachmodelle die aktuelle Polarisierung übernehmen können. Dafür teilen wir die Trainingskorpora in Daten vor und nach dem 45. Präsidenten der Vereinigten Staaten (d. h. vor und nach 2017) auf und trainieren getrennt darauf. Die Ergebnisse zeigen, dass Modelle nach 2017 deutlich weiter vom Zentrum entfernt sind – ein Hinweis darauf, dass Sprachmodelle die gesellschaftliche Polarisierung aufnehmen.

Schließlich evaluieren wir Sprachmodelle mit unterschiedlichen politischen Neigungen bei Hate‑Speech‑Detection und Fake‑News‑Detection, zwei NLP‑Anwendungen, die große gesellschaftliche Auswirkungen haben können. Wenn wir die Leistung nach Kategorien (z. B. nach demografischen Merkmalen oder der politischen Neigung der Nachrichtenquellen) aufschlüsseln, entdecken wir ein Muster: Linksliberale Modelle sind besser darin, Hate‑Speech gegen sozial marginalisierte Gruppen zu erkennen, aber schlechter bei Hate‑Speech gegen mächtige Gesellschaftsgruppen. Rechts‑liberalere Modelle zeigen das Gegenteil: Sie erkennen Hate‑Speech gegen Weiße und Männer besser, aber schlechter gegen Schwarze, LGBTQ+ und andere Minderheiten. Ähnliche Trends zeigen sich bei Fake‑News‑Detection: Linksliberale Modelle sind besser darin, Fehlinformationen aus ihrer gegensätzlichen politischen Neigung zu erkennen und umgekehrt. Wir präsentieren qualitative Beispiele, die zeigen, wie Modelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen für Hate‑Speech‑ und Misinformation‑Beispiele liefern. Diese Ergebnisse unterstreichen ein dringendes Fairness‑Problem, das durch die politischen Neigungen von Sprachmodellen entsteht. Beispielsweise würde die Feinabstimmung eines rechts‑liberalen Modells für Hate‑Speech‑Erkennung auf einer großen Social‑Media‑Plattform bedeuten, dass Menschen mit gegensätzlichen politischen Ansichten marginalisiert werden und Hate‑Speech gegen Minderheiten unkontrolliert weiter verbreitet wird.

**Diskussion**  
Wir wollen die besondere Dilemma der politischen Voreingenommenheit von Sprachmodellen hervorheben – ein Zwiespalt zwischen „Scylla“ und „Charybdis“. Ohne politische Meinungen in den Trainingsdaten zu säubern, wird die Voreingenommenheit von Trainingsdaten über Sprachmodelle zu downstream‑Aufgaben übertragen und führt zu Fairness‑Problemen. Versucht man hingegen, die Daten zu säubern, besteht die Gefahr von Zensur oder Ausschluss. Es ist extrem schwierig zu bestimmen, was tatsächlich neutral ist und welche Daten erhalten bleiben sollten. Kurz gesagt ist es ein modernes „Elektrisches Trolley‑Problem“.

Danke für eure Aufmerksamkeit.</sample>
    <sample id="48">Ein Autor – David Vilar.</sample>
    <sample id="49">Bis zu 1 024 Token Kontextlänge wurden für die MPP‑Auswertungen getestet.</sample>
    <sample id="50">**Abstract (Deutsch, ≤200 Wörter)**  

DEPLAIN ist ein neues, deutschsprachiges Parallelkorpus für Text‑Simplifizierung auf Dokument‑ und Satz‑Ebene. Es umfasst zwei Subkorpora: DEPLAIN‑apa (483 hand‑alignierte Nachrichtenartikel mit ca. 13 000 Satz‑Paaren) und DEPLAIN‑web (750 Dokumente aus verschiedenen Domänen, 30 450 Satz‑Paaren, teilweise automatisch, teilweise manuell aligniert). Die manuellen Annotationen ermöglichen eine präzise Analyse von Simplifizierungsstrategien (lexikalische Substitution, Klausel‑Deletion, Umordnung, Ergänzungen). Vergleichsstudien zeigen, dass Bibeltexte stärker vereinfacht werden als Nachrichten‑ oder Lerntextsätze; DEPLAIN‑apa weist mehr Umordnungen und Wortergänzungen auf, während DEPLAIN‑web häufiger Rephrasings enthält.  

Zwei Hauptanwendungsfälle werden vorgestellt. Erstens dient DEPLAIN als Gold‑Standard für die Bewertung von automatischen Alignments. Nach Anpassungen verschiedener Verfahren wird MASSalign als am besten geeignete Methode für deutsche Simplifizierungs‑Alignments identifiziert. Zweitens werden zwei Modelle – long‑mBART für Dokument‑Simplifizierung und mBART‑Base für Satz‑Simplifizierung – fein‑getuned; die Resultate übertreffen Baselines und bilden einen Bench‑mark für zukünftige Arbeiten. Der Beitrag liefert neben den Korpora auch Code und Check‑points für Alignment‑Methoden und Modell‑Fine‑Tuning.</sample>
    <sample id="51">Die Datensätze umfassen die Domains **Musik, Bücher und Rezepte**.</sample>
    <sample id="52">Positionalität beschreibt die Perspektiven und Standpunkte, die Individuen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen – und die dadurch die Entscheidungen und Ergebnisse von Forschung, Datensätzen und Modellen beeinflussen.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">**Abstract (≤ 200 Wörter)**  

Wir stellen eine neue Ressource und Methode zur Erkennung kognitiver Dissonanz in Texten vor. Kognitive Dissonanz beschreibt das Auftreten widersprüchlicher Überzeugungen oder Handlungen, z. B. „Ich weiß, dass Zigaretten mich töten können, ich rauche trotzdem“. Obwohl Dissonanz alltäglich ist, taucht sie nur selten in diskursiven Relationen auf – in einer großen Annotationsstudie wurde sie in lediglich 3,5 % der Paare gefunden. Eine erste Klassifikationsprobe mit 43 Dissonanz‑Beispielen erreichte kaum mehr als Zufall.  

Um dem Problem der absolute Seltenheit zu begegnen, kombinieren wir Transfer‑Learning (TL) und Active‑Learning (AL). Wir übertragen Gewichte von zwei verwandten Aufgaben: (1) Debattier‑Stance‑Klassifikation (Debate) und (2) die Expansion/Comparison‑Klassen des PDTB (CE). Das gefilterte Modell liefert bereits im Zero‑Shot‑Modus ein AUC von 0,62.  

Für AL evaluieren wir kumulative vs. iterative Modellupdates und verschiedene Auswahlstrategien. Unsere neue Probability‑of‑Rare‑Class (PRC) Methode, die Beispiele mit hoher Wahrscheinlich­keit für Dissonanz auswählt, übertrifft etablierte AL‑Strategien (nur geringfügig) und verbessert das AUC nach mehreren Runden auf 0,75. PRC liefert die höchste Dissonanz‑Quote, erfordert jedoch mehr annotatorische Anstrengung. Insgesamt zeigt sich, dass TL‑basierter Kaltstart, kumulative Updates und PRC eine effektive, kostengünstige Lösung für seltene Klassen darstellen.</sample>
    <sample id="55">Ja – EDAtt wurde genau dafür entwickelt, bestehende Offline‑ST‑Modelle zu nutzen. Es braucht kein neu‑trainiertes Modell oder spezielle Architekturen; die vorhandenen Cross‑Attention‑Gewichte werden einfach als Leitfaden für die Emission von Übersetzungspartikeln verwendet.</sample>
    <sample id="56">Based on the information provided, only **one author**—Yusen Zhang—is mentioned.</sample>
    <sample id="57">Nein – die Coreference‑Modelle zeigen in der KITMUS‑Testsuite ohne gezieltes Training kaum Leistung. Erst nachdem sie speziell auf KITMUS trainiert wurden, erreichen C2F und BERT4Coref deutlich bessere Ergebnisse als Zufall, aber auch sie haben weiterhin Schwierigkeiten, rein inference‑basierte („backward“) Fakten zuverlässig zu nutzen.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:  

1. **Background‑Pretrain** – Hintergrundwissen ist im Pre‑train‑Modell vorhanden.  
2. **Background‑Both** – Hintergrundwissen ist sowohl im Pre‑train‑Modell als auch im inference‑time‑Kontext vorhanden.  
3. **Background‑Inference** – Hintergrundwissen ist ausschließlich im inference‑time‑Kontext verfügbar.</sample>
    <sample id="59">**Abstract**  
In der vorliegenden Arbeit stellen wir DrBERT vor, das erste frei verfügbare französische Biomedizin‑Modell, das auf RoBERTa basiert und mit NACHOS – einer über das Web gecrawlten Sammlung medizinischer Texte – von Grund auf trainiert wurde. Wir vergleichen DrBERT mit dem klinisch orientierten ChuBERT, das aus anonymisierten Aufzeichnungen des Nantes University Hospital stammt, und untersuchen, wie sich Datengröße und -quelle auf die Modellleistung auswirken. Vier „from‑scratch“‑Modelle (DrBERT 7 GB, DrBERT 4 GB, ChuBERT 4 GB und ein gemischter ChuBERT 4 GB + NACHOS 4 GB) sowie drei Modelle mit fortlaufendem Pre‑Training (CamemBERT‑Gewichte + NACHOS 4 GB, CamemBERT‑Gewichte + klinische Notizen 4 GB, PubMedBERT + NACHOS 4 GB) wurden entwickelt. Die Modelle wurden auf 11 französischsprachigen biomedizinischen Down‑stream‑Aufgaben (NER, Klassifikation, POS‑Tagging, QA) evaluiert und mit CamemBERT (OSCAR 138 GB / 4 GB, CCNET 4 GB), PubMedBERT, BioBERT und ClinicalBERT verglichen. Ergebnisse zeigen, dass Modelle am besten bei Aufgaben mit Daten gleicher Herkunft performen, heterogene Daten jedoch mehr Vielseitigkeit bieten, und mehr Trainingsdaten die Leistung steigern. From‑scratch‑Pre‑Training liefert in der Regel die höchste Leistung; jedoch kann ein CamemBERT‑Transfer mit 4 GB NACHOS vergleichbare Resultate erzielen. DrBERT übertrifft CamemBERT in neun von elf Aufgaben. Alle Modelle, Trainingsskripte und Daten liegen unter MIT-Lizenz auf Hugging Face bzw. GitHub.</sample>
    <sample id="60">Die im Text genannten Autoren (Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis) werden nicht mit einer Universität in Verbindung gebracht; die zugehörigen Institutionen sind im bereitgestellten Material nicht angegeben.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: **Wie können die sauber annotierten Stichproben am besten genutzt werden – nur zur Validierung oder gibt es effizientere Wege, sie in die Modellbildung einzubeziehen (z. B. durch Fine‑Tuning)?**</sample>
    <sample id="62">**Abstract (max. 200 Wörter)**  

Diese Arbeit liefert eine systematische Untersuchung von Knowledge‑Distillation‑Methoden für natürliche Sprachgenerierung (NLG) in industriellen, realen Szenarien. Wir konzentrieren uns auf vier typische NLG‑Aufgaben – Text‑Zusammenfassung, Fragen‑Generierung, Common‑Sense‑Reasoning sowie Vereinfachung und Stil‑Transfer – mit mittleren Mengen annotierter Daten (1 : 4‑Verhältnis zu unannotierten Daten) und moderaten Teacher‑Modellen. Der Distillation‑Prozess besteht in acht aufeinanderfolgenden Stufen: (1) Architekturanalyse (Encoder‑Decoder vs. Decoder‑Only), (2) Pruning‑Einfluss, (3) Auswahl von Wissen, (4) Vergleich von State‑of‑the‑Art‑Baselines, (5–7) Erweiterungen der Nutzung von Pseudo‑Zielen und (8) die vorgeschlagene Joint‑Teaching‑Strategie.  

Joint‑Teaching kombiniert Wort‑Level‑KL‑Distillation mit Pseudo‑Zielen, die sowohl vom Teacher als auch vom Student generiert werden, um Exposure‑Bias, grounded learning und Selbstkorrektur zu adressieren. Wir zeigen, dass: (a) unannotierte Daten wesentlich sind, (b) mehrere, diversifizierte Pseudo‑Ziele (z. B. durch Sampling mit hoher Temperatur) die Leistung des Students verbessern, und (c) Joint‑Teaching die besten Ergebnisse liefert, indem es die Lernkurve des Students verfeinert.  

Die Studie liefert damit ein praktikables Rezept für effiziente, hochkomprimierte NLG‑Modelle in Industrie‑Settings.</sample>
    <sample id="63">**Sensitivitätsmetrik – Kurz erklärt**

Die Sensitivitätsmetrik misst, wie stark die Ausgabe eines Modells an die Formulierung der Instruktion hängt.  
- Für jede Aufgabe werden fünf unterschiedliche, aber semantisch identische Instruktion‑Templates verwendet.  
- Das Modell wird jeweils mit einer dieser Instruktionen getestet und die Resultate (z. B. Accuracy, ROUGE‑L) werden aufgezeichnet.  
- Die Sensitivität ist dann die Streuung dieser fünf Ergebnisse – üblicherweise als Standardabweichung, aber auch als Differenz zwischen Max‑ und Min‑Werten.  
- **Geringe Sensitivität** bedeutet, dass das Modell unabhängig von kleinen Wortschaltvariationen konsistente Antworten liefert; hohe Sensitivität weist auf eine starke Abhängigkeit von der genauen Formulierung hin.</sample>
    <sample id="64">Der Referent*in heißt **Jingwei Yi**.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell stärker von kleinen Variationen in der Formulierung der Anweisung beeinflusst wird – also weniger konsistent ist. Deshalb gilt: **Geringere Sensitivität → bessere Modellleistung / höhere Robustheit**.</sample>
    <sample id="66">**Abstract**  
Mathematische Argumentation ist ein zentrales Element menschlicher Intelligenz, das sich von einfachen arithmetischen Problemen bis zu komplexen Theorembeweisen erstreckt. Diese Arbeit bietet einen Überblick über die aktuellen Forschungsfelder im Bereich des mathematischen Reasonings. Wir stellen die wesentlichen Aufgaben vor – von Text‑basierten Wortproblemen über multimodale Szenarien (Diagramme, Tabellen) bis hin zu geometrischen Aufgabenstellungen, die neuro‑symbolische Modelle erfordern, und dem automatisierten Theorem‑Proving. Für jede Aufgabe werden die wichtigsten Datensätze und Benchmarks (Numeric Commonsense, High‑Level Problem Solving, Finanz‑, Wissenschafts‑ und Medizin‑Daten) sowie die aufkommenden Low‑Resource‑Sprachen (Chinesisch, Koreanisch, Arabisch) vorgestellt. Methodologisch fokussieren wir die Entwicklung von Sequenz‑zu‑Sequenz- und Sequenz‑zu‑Baum‑Modellen, die mathematische Ausdrücke strukturieren, sowie die Einbindung von Large Language Models (LLMs). Durch Chain‑of‑Thought‑Prompting, Self‑Consistency und tool‑aided LLM‑Ansätze (z. B. Chameleon) lassen sich die Leistungsgrenzen von LLMs bei präziser Mathematik überwinden. Abschließend diskutieren wir die bestehenden Schwächen bezüglich Generalisierung, Umgang mit großen Zahlen und Konsistenz, und geben Richtungen für zukünftige Forschung an. (≈170 Wörter)</sample>
    <sample id="67">**Abstract (≈170 Wörter)**  

In diesem Beitrag wird untersucht, wann und warum Interferenzen zwischen Sprachpaaren in multilingualen Transformer‑Übersetzungsmodellen auftreten. Durch Vergleich der Verlustwerte eines zweisprachigen Modells (S → T) mit denen desselben Sprachpaares in einem multilingualen Modell wird Interferenz als relativer Leistungsunterschied definiert. Die Experimente umfassen vier Transformer‑Architekturen und 15 WMT‑Sprachen, deren Datenmengen von 150 K bis über 50 M Sätzen reichen.  

Hauptfaktoren für Interferenz sind die Modell‑ und Daten­größe: bei kleinen Modellen (wenig Parameter) und/oder geringer Datenmenge zeigt sich schwerwiegende Interferenz, die mit zunehmender Skalierung verschwindet. Sprachähnlichkeit (z. B. Romance‑vs‑Slavisch) hat nur einen schwachen Einfluss, ebenso die Gesamtzahl der unterstützten Sprachen.  

Ein zentrales Ergebnis ist, dass die Wahl der Sampling‑Temperatur entscheidend ist. Eine Temperatur T &gt; 1 erhöht die Auswahl von Low‑Resource‑Beispielen; jedoch führt eine zu hohe Temperatur zu schlechteren Ergebnissen. Durch Abstimmung von T (typischerweise 1–5) lässt sich Interferenz ohne zusätzliche Spezialalgorithmen erheblich reduzieren.  

Zusammenfassend zeigen die Skalierungs­gesetze, dass modest skalierte Modelle mit gezielt kalibrierter Temperatur bereits die meisten Interferenzprobleme lösen.</sample>
    <sample id="68">Die Modelle werden auf sehr großen, heterogenen Textkorpora vortrainiert (z. B. Wikipedia, Common Crawl, Web‑Scraped‑Corpus). Dort durchlaufen sie eine breite Palette syntaktischer und semantischer Strukturen – von einfachen Sätzen bis zu langen, komplexen Konversationen – und lernen damit einen allgemein­sprachlichen Kontext, der sämtliche grammatische und pragmatische Phänomene umfasst.</sample>
    <sample id="69">In der Regel reichen etwa **20 saubere Validierungsbeispiele pro Klasse** für eine gute Leistung im Weakly Supervised Learning.</sample>
    <sample id="70">Die Autoren (Myra, Esin Durmus und Dan Jurafsky) sind am **Stanford University** tätig.</sample>
    <sample id="71">**Abstract**

Wir stellen den AltEntities Corpus vor, ein neues, öffentlich zugängliches Datenset zur Untersuchung indirekter Referenzen bei der Auswahl von Entitäten in Konversationen. Ziel ist es, die Sprache von Nutzern zu verstehen, wenn sie zwischen mehreren Optionen wählen, z. B. zwischen Songs, Büchern oder Rezepten. Das Corpus umfasst 6 000 alternative Fragen in drei Domänen (Musik, Bücher, Rezepte) und 42 000 indirekte Referenzen, die von Crowd‑Annotatoren generiert wurden. Für jede Frage wird ein Kontext (z. B. „Erinnerst du dich an das Lied, das wir gestern gehört haben?“) und eine Alternative‑Frage („Meinst du „Easy on Me“ oder „I Gotta Feeling“?“) automatisch erzeugt. Annotatoren erhalten Hintergrundinformationen zu den beiden Entitäten (z. B. Google‑Suchergebnisse, Wikipedia‑Texte, Bilder) und formulieren anschließend 3–5 indirekte Ausdrücke (z. B. „der neue“, „das nicht energetische“). Unser Samplingschema variiert die Ähnlichkeit der Entitäten, um unterschiedliche Schwierigkeitsgrade zu erzeugen. Basierend auf T5‑XL‑Modellen zeigt die Evaluation, dass bei Zugang zu identischem Hintergrundwissen eine Genauigkeit von 92–95 % erreicht wird, bei teilweise überlappendem Wissen 82–87 %, während ausschließlich mit Namen die Genauigkeit auf ca. 60 % sinkt. Das Corpus bietet damit eine wertvolle Grundlage für die Entwicklung und Bewertung von Modellen zur indirekten Entitätsauflösung in natürlichen Sprachdialogen.</sample>
    <sample id="72">Neue Messmethoden für Medienverzerrungen sind nötig, weil aktuelle Ansätze die komplexen, politisch geprägten Verzerrungen, die in großen Web‑Crawls und damit in den vortrainierten Modellen verankert sind, nicht zuverlässig erkennen oder quantifizieren können. Ohne präzise, politikwissenschaftlich fundierte Messungen bleiben die politisch‑ladenen Bias‑Pfaden von Daten über Modelle bis hin zu downstream‑Aufgaben verborgen, was zu unfairen Ergebnissen (z. B. in Hate‑Speech‑ oder Fake‑News‑Detection) führt. Daher sind neue Methoden erforderlich, um Bias frühzeitig zu identifizieren, die Auswirkungen auf verschiedene Bevölkerungsgruppen sichtbar zu machen und gezielt Gegenmaßnahmen zu entwickeln.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">**Abstract**

Commonsense Wissen ist für menschen‑ähnliche Interaktion entscheidend. Das Knowledge‑Base ATOMIC liefert hochqualitative, aber stark eingeschränkte, ein‑Hop‑Verknüpfungen, da viele mögliche Relationen (B→A, B→B, A→B, A→A) fehlen. Wir stellen Dense‑ATOMIC vor – eine dichte, event‑zentrierte Wissensbasis, die diese Lücken schließt und 2‑ und 3‑Hop‑Pfad‑Dichte signifikant erhöht. Der Aufbau erfolgt in drei Schritten: (1) Normalisierung der Ziel‑Events (Subjekt‑Entfernung, Singular‑Konjugation, Subjekt‑Wiederherstellung, Relation‑Gruppierung), sodass Head‑ und Tail‑Events vergleichbar sind. (2) Training des Rel‑CSKGC‑Modells, das mit RoBERTa Head‑ und Tail‑Events kodiert und die Relation über Start‑Token‑Repräsentationen vorhersagt. Dieses Modell nutzt keine Graph‑Struktur, wodurch die Sparsität von ATOMIC umgangen wird, und erfasst zugleich semantische Informationen. (3) Intra‑ und Inter‑Cluster‑Vervollständigung: Cluster bestehen aus einem Basis‑Event und seinen annotierten Tail‑Events; fehlende Verbindungen innerhalb bzw. zwischen Clustern werden inferiert. Evaluation zeigt, dass Rel‑CSKGC relationale Vorhersagen gegenüber klassischen Methoden und Übersetzungs‑Modellen übertrifft. Dense‑ATOMIC bietet höhere Wissensabdeckung, mehr 1‑, 2‑ und 3‑Hop‑Pfade und verbessert die Generierung von COMET. Die Ergebnisse demonstrieren das Potenzial dichten Commonsense‑Graphs für fortgeschrittene Mehr‑Hop‑Raisonnierung.</sample>
    <sample id="75">**Abstract**

Zheng Yandan, Hao Anran und Luu Anh Tuan stellen „Jointprop“, ein halbsupervisiertes Lernframework für die gleichzeitige Erkennung von Named Entities (NER) und Beziehungen (RE). Das Verfahren nutzt die gegenseitige Abhängigkeit beider Aufgaben, um fehlende Labels in unlabeled Daten zu ergänzen. Zunächst werden span‑ und span‑pair‑Repräsentationen mit einem vortrainierten Sprachmodell erzeugt. Darauf aufbauend wird ein heterogener Graph aufgebaut, der K‑Nearest‑Neighbor‑Nachbarschaften sowohl zwischen unlabeled als auch zwischen labeled Daten abbildet. Durch iterative Label‑Propagation werden Pseudo‑Labels für Entitäten und Relationen erzeugt; jeder Schritt verbessert die Abstimmung via Softmax‑Scores und einer Confidence‑Prüfung. Anschließend werden nur hochqualitative Pseudo‑Labels mit den Original‑Labels kombiniert und das Klassifikationsmodell erneut trainiert. Experimente an vier Joint‑ und Single‑Task‑Datasets zeigen, dass Jointprop die Leistung von NER und RE deutlich steigert—insbesondere bei begrenzten gelabelten Daten—und die Codependenz beider Aufgaben effektiv nutzt. Das Verfahren liefert somit einen robusten Ansatz für semi‑supervised Joint‑NER/RE ohne zusätzliche Annotierungsressourcen.</sample>
    <sample id="76">**Pipeline zur Verbreitung politischer Vorurteile**

1. **Pretraining‑Daten**  
   - Web‑Crawls, Nachrichten‑Magazine (NYT, LA Times, Guardian, Huffington Post, …)  
   - Soziale‑Medien‑Korpora (Reddit, Twitter)  
   - Politisch geprägte Subsets (linke/rechte, vor/nach 45. Präsident)

2. **Sprachmodell‑Training**  
   - Modelle wie GPT‑4, RoBERTa, BART werden auf diesen Daten trainiert  
   - Durch das Training übernehmen die Modelle die politischen Tendenzen der Daten

3. **Messung der politischen Neigung**  
   - Prompt‑Tests (z. B. Political Conference Test) → Bewertung auf einem politischen Campus  
   - Beobachtung: Modelle liegen in allen vier Quadranten; GPT‑4 ist am liberalsten

4. **Einfluss auf Downstream‑Aufgaben**  
   - Hate‑Speech‑Detection, Fake‑News‑Detection  
   - Leistungsunterschiede je nach politischer Neigung des Modells (z. B. linke Modelle erkennen besser Hass gegen Minderheiten, rechte Modelle besser gegen dominante Gruppen)

5. **Fairness‑Probleme**  
   - Modelle mit ausgeprägter politischer Neigung können bei Einsatz in Systemen Benachteiligungen erzeugen (z. B. marginalisierte Gruppen übersehen).  

**Kurzfassung:** Pretraining‑Daten → Sprachmodell‑Training → Politische Neigung → Downstream‑Aufgaben → Fairness‑Risiken.</sample>
    <sample id="77">**Abstract**

Wir stellen DeFacto vor, einen neu erstellten Datensatz, der menschliche Demonstrationen und Feedback zur Verbesserung der faktischen Konsistenz von abstrakten Zusammenfassungen enthält. DeFacto basiert auf dem XSum‑Korpus, wobei Ausgangssummaries vom vortrainierten Pegasus-Modell generiert wurden. Annotatoren bewerten die Summaries hinsichtlich ihrer faktischen Konsistenz, liefern korrigierte, konsistente Versionen, sowie strukturierte Rückmeldungen (Anweisungen, Erklärungen, Belege). Die Sammlung umfasst ca. 2 500 Beispiele, von denen 70 % Fehler aufweisen. Im Vergleich zu den Originalsummaries weisen die annotierten Varianten höhere automatische Faktizitätsscores auf, jedoch geringere Übereinstimmung mit den Referenzsummaries, da Letztere häufig Fehler enthalten. Wir definieren drei neue NLG‑Aufgaben: (1) Summary Editing – Modelle editieren mithilfe menschlichen Feedbacks, (2) Feedback Generation – Kritiker erzeugen erklärendes Feedback, (3) Automatic Factual Error Correction – Modelle korrigieren Fehler und liefern Erklärungen. Fine‑tuned Modelle und Zero‑Shot‑Large‑Language‑Models schneiden bei Editing gut ab; Feedback‑Generation bleibt herausfordernd. Das Editor‑Modell erreicht vergleichbare Leistung wie Baselines mit deutlich weniger Daten, wobei die Generierung von Erklärungen die Performance steigert. DeFacto unterstützt außerdem die Entwicklung von Faktizitätsmetriken und Meta‑Evaluation. Der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="78">Ja – die Art der Vereinfachung unterscheidet sich zwischen DEPLAIN‑apa und DEPLAIN‑web.  
In **DEPLAIN‑apa** (Nachrichten) kommen vor allem **Umstellungen** und **Wortergänzungen** häufiger vor, während in **DEPLAIN‑web** (verschiedene Domains) mehr **Reformulierungen** genutzt werden. Zudem zeigen die Analysen, dass die Bibeltexte stärker vereinfacht sind als Nachrichten oder Lernmaterialien, was auf einen höheren Simplifizierungsgrad hinweist.</sample>
    <sample id="79">No – the CoScript dataset is not publicly released yet. It is only available to researchers through request or collaboration (the paper does not indicate a public download link).</sample>
    <sample id="80">Das Wasserzeichen wird in den Embedding‑Output eingebettet, nicht im Text selbst:  
1. **Trigger‑Set wählen** – Wörter mit mittlerer Häufigkeit.  
2. **Ziel‑Embedding festlegen** – ein spezielles Embedding, das das Wasserzeichen repräsentiert.  
3. **Beim Anfrage‑Verfahren**:  
   * Zähle, wie viele Trigger‑Wörter im Eingabesatz enthalten sind.  
   * Setze das zurückgegebene Embedding als lineare Kombination aus dem ursprünglichen Embedding und dem Ziel‑Embedding.  
   * Das Gewicht des Ziel‑Embeddings ist proportional zur Trigger‑Anzahl; wenn die Anzahl die Schwelle \(m\) überschreitet, wird das Ergebnis exakt das Ziel‑Embedding sein.

So wird das Wasserzeichen durch die gewichtete Mischung eines speziell definierten Ziel‑Embeddings in die vom Service erzeugten Embeddings eingebettet.</sample>
    <sample id="81">Die Autoren sind an der **Penn State University** angestellt.</sample>
    <sample id="82">**Abstract (max. 200 Wörter)**

In diesem Video wird die Arbeit *„Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring“* vorgestellt. Das Ziel ist automatisierte Aufsatzbewertung (AES) ohne menschliche Labels. Bestehende unsupervised‑Methoden nutzen entweder ein einzelnes heuristisches Signal (z. B. Anzahl einzigartiger Begriffe) oder einen direkten Regressionsansatz (z. B. Wortanzahl), was zu unkontrollierten Clustern bzw. schwacher Leistung führt. Die Autoren schlagen daher das ULRA‑Framework vor, das mehrere heuristische Qualitätsmetriken als pseudo‑Ground‑Truth nutzt und daraus Partial‑Order‑Paare erzeugt. Das Herzstück ist der Deep Pairwise Rank Aggregation (DPRA)‑Modul, das mithilfe eines lernbaren Gewichts die unterschiedlichen Signale integriert und Konflikte auflöst. Für die Vorhersage wird ein Min‑Max‑Transformationsverfahren angewendet, um die Ausgabe in das vorgegebene Bewertungsschema zu überführen. Experimente in transduktiven und induktiven Settings zeigen, dass ULRA die bisherigen unsupervised‑Baselines deutlich übertrifft und mit Cross‑Prompt/One‑Shot‑Methoden konkurrenzfähig ist, obwohl es unter realen Supervised‑Methoden noch zurückbleibt. Die Arbeit demonstriert somit die Eignung von Rank‑Aggregation‑Lösungen für das unsupervised AES‑Problem.</sample>
    <sample id="83">Ja – laut der Studie können Encoder‑Decoder‑Modelle wie mT5 durch gemeinsames Training an mehreren Sprachen (eine Sprach‑Mischung) deutlich gesteigert werden.</sample>
    <sample id="84">Wir stellen PAD‑Net vor, ein Framework für partielle dynamische Netzwerke, das das Problem übermäßiger Parameter in vollständig dynamischen Modellen adressiert. Während klassische dynamische Architekturen (Mixture of Experts, Dynamic Convolution) die Modellgröße stark erhöhen, partitioniert PAD‑Net die Parameter in statische und dynamische Komponenten. Durch iterative Modus‑Partition identifiziert das Verfahren redundante dynamische Gewichte, die als statisch behandelt werden, ohne die Verlustfunktion erheblich zu beeinflussen. Zwei Skalierungsfaktoren steuern die Intensität beider Modi und beschleunigen das Training. Experimente auf BERT‑Base zeigen, dass PAD‑Net die Leistung von reinen statischen und dynamischen Netzen übertrifft, während es gleichzeitig die Modellgröße und die Rechenkomplexität reduziert. Ablationsstudien bestimmen optimale Dynamik­quoten für Dynamic Convolution und Mixture of Experts sowie die kritische Bedeutung der Skalierungsfaktoren. Im Vergleich zu herkömmlichem Pruning bietet PAD‑Net höhere Genauigkeit, da es statische Parameter beibehält. Darüber hinaus erzeugt das Verfahren diskriminiertere Ausgaben, was die Leistung verbessert. Zukünftige Arbeiten umfassen die Anwendung auf weitere Architekturen, hardware‑freundliche Strukturen und die Integration zusätzlicher Modi wie Null‑Elemente.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist: **„Einen Schokoladenkuchen backen“** statt des allgemeinen Ziels **„einen Kuchen backen“**, wobei zusätzliche Einschränkungen wie die Art des Kekses (Schokolade) berücksichtigt werden.</sample>
    <sample id="86">Sie sichern die Opazität, indem sie die Wasserzeichen‑Backdoor subtil einbauen:  

* **Trigger-Auswahl** – Ein Satz Wörter mit mittlerer Häufigkeit wird als Trigger‑Set definiert, sodass sie im normalen Text kaum auffallen.  
* **Gewichtete Einfügung** – Die Embedding‑Ausgabe wird als gewichtete Summe aus dem Original‑Embedding und einem Ziel‑Embedding generiert; das Ziel‑Embedding wird nur bei einer hohen Trigger‑Zahl (≥ m) stark gewichtet und bei wenigen Triggern kaum sichtbar.  
* **Verdeckte Wirkung** – Durch diese graduelle Gewichtung bleiben die Embeddings für typische Downstream‑Aufgaben nahezu unverändert, während die Wasserzeichen‑Spur nur bei gezieltem Trigger‑Input erkennbar wird.  
* **Beweis der Verdecktheit** – Die Autoren visualisieren die Embeddings mit PCA und zeigen, dass sich Backdoor‑Embeddings kaum von normalen unterscheiden, und prüfen die Ähnlichkeitsmessungen (Cosine, L2, KS‑Test), die nahe beieinander liegen.

Damit bleibt die Wasserzeichen‑Spur für Angreifer verborgen, während die Dienstqualität erhalten bleibt.</sample>
    <sample id="87">Die Autoren bauen DrBERT nicht von Grund auf neu, sondern nutzen bestehende PLMs als Ausgangsbasis:  

* **CamemBERT** (französische Basis‑Lernarchitektur) liefert Tokenizer und Gewichte, die dann in einer *continual‑pre‑training*‑Phase auf das medizinische Korpus NACHOS (4 GB bzw. 7 GB) angewendet werden.  
* **PubMedBERT** (englisches, biomedizinisches Modell) wird ebenfalls als Ausgangspunkt für ein weiteres kontinuierliches Pre‑training auf NACHOS benutzt.  

Parallel dazu trainieren sie vom NUL (RoBERTa‑Architektur) ganz neue Modelle ausschließlich auf NACHOS, um die Wirkung von „from‑scratch“‑Pre‑training zu prüfen.  

Kurz: Sie übernehmen Gewichte/Tokenizer von CamemBERT/PubMedBERT und führen anschließend ein domain‑spezifisches Pre‑training auf französischen medizinischen Texten durch, während sie gleichzeitig eigene „from‑scratch“-Modelle auf denselben Daten trainieren.</sample>
    <sample id="88">GPT‑4 ist am wenigsten auf ein Land ausgerichtet, das weder englischsprachig noch konfuzianisch ist – zum Beispiel ein nicht‑englischsprachiges, nicht‑konfuzianisches Land wie Brasilien oder Deutschland.</sample>
    <sample id="89">Der beispielhafte Satz lautet: **„I’m going to talk about…​“.**</sample>
    <sample id="90">**Abstract (max. 200 Wörter)**  

Wir untersuchen, ob Sprachlernende anstelle von Muttersprachlern zur Datenerfassung in NLP eingesetzt werden können. Für Englisch, Koreanisch und Indonesisch haben wir vier typische GLUE‑Aufgaben (Sentiment, NLI, NER, MRC) ausgewählt und 120 Testproben in fünf Schwierigkeitsstufen unterteilt. Lernende wurden in Grund-, Mittel- und Fortgeschrittenen eingeteilt (CFR‑Kriterien) und in zwei Gruppen mit unterschiedlichen Ressourcenzugängen (Wörterbücher, MT‑Tools) aufgeteilt. In sechs Sessions von je drei Tagen absolvierte jedes Teilnehmende einen Pre‑Test, annotierte 10 Aufgaben und erfuhr einen Post‑Test. Zusätzlich verglichen wir ihre Leistung mit der von Muttersprachlern.

Ergebnisse zeigen: Für einfache bis mittlere Aufgaben liefern Lernende nahezu genauso präzise Labels wie Muttersprachler; bei Mehrfachaggregation durch Mehrheitsabstimmung erreichen die Lernenden die gleiche Genauigkeit. Modelle, die mit Lernenden‑Labels trainiert wurden, erreichten 95 % der Ground‑Truth‑Performance und übertrafen gelegentlich Modelle mit Muttersprachler‑Labels. Darüber hinaus verbesserten sich die Sprachkompetenz und das Vokabular der Lernenden zwischen Pre‑ und Post‑Tests, was einen Lerngewinn durch Annotation impliziert.

Diese Arbeit demonstriert, dass Sprachlernende als annotierende Ressourcen für Low‑Resource-Sprachen geeignet sind und die Notwendigkeit, ausschließlich Muttersprachler zu rekrutieren, in Frage stellen. Sie eröffnet neue Möglichkeiten für die schnelle Erweiterung von NLP‑Benchmarks ohne geografische oder technologische Beschränkungen.</sample>
    <sample id="91">Mehr Aufgaben führen zu besserer Leistung des Modells und gleichzeitig zu einer geringeren Sensitivität.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei baumlosen (treeless) Baselines:

1. **Ein klassisches Seq2Seq‑Modell mit Attention**  
2. **Ein Transformer‑basierter Seq2Seq‑Ansatz**  
3. **Ein Pointer‑Generator (CopyNet) Modell**</sample>
    <sample id="93">Die beiden Co‑Autoren, Alexander Koller und Ivan Titov, sind die Betreuer bzw. Mentoren des ersten Autors, Matthias Lindemann, und haben gemeinsam das Papier verfasst.</sample>
    <sample id="94">**Abstract (≤ 200 Wörter)**  

Die zunehmende Nutzung von großen Sprachmodellen (z. B. GPT, LLaMA, PaLM) als „Embedding‑as‑a‑Service“ birgt das Risiko, dass Angreifer die Modelle durch Abfrage ihrer Einbettungen extrahieren und ähnliche Dienste anbieten. Zur Wahrung des Urheberrechts wird ein Back‑door‑basierter Wasserzeichenmechanismus namens **Embedding Marker** vorgeschlagen. Zunächst wählt der Anbieter einen Trigger‑Set aus Wörtern mittlerer Häufigkeit. Bei jeder Anfrage wird die Anzahl der Trigger in der Eingabe gezählt; das zurückgegebene Embedding ist ein gewichteter Mittelwert aus dem ursprünglichen Embedding und einem vorgegebenen Ziel‑Embedding, dessen Gewicht proportional zur Trigger‑Zahl ist. Überschreitet die Trigger‑Zahl einen Schwellenwert m, wird das Embedding exakt dem Ziel‑Embedding zugeordnet.  

Zur Urheberrechtsüberprüfung werden zwei Datensätze angefragt: ein Back‑door‑Set (alle Wörter im Trigger‑Set) und ein benign‑Set (keine Trigger‑Wörter). Die Kosinus‑ und L₂‑Ähnlichkeiten zum Ziel‑Embedding werden berechnet und die Differenzen (Δcos, ΔL₂) sowie ein Kolmogorov‑Smirnov‑Test dienen als Kennzahlen. Experimente auf AG‑News, MIND, SST‑2 und Enron‑Spam zeigen, dass der Embedding Marker eine hohe Erkennungsrate erzielt, die Modell‑Utility kaum beeinträchtigt und die Wasserzeichen durch PCA‑Visualisierung kaum auffindbar sind.</sample>
    <sample id="95">Der erste Autor des PaLM‑Papiers ist **Ching‑Hao Chen**.</sample>
    <sample id="96">Hallo zusammen. Ich bin Jenny, erste‑Jahr PhD‑Studentin an der Carnegie Mellon University, und heute werde ich euch unsere Arbeit „NLPositionality – Charakterisierung von Design‑Biases in Datensätzen und Modellen“ vorstellen. Diese Forschung wurde in Zusammenarbeit mit Kolleginnen und Kollegen aus der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap.

Stellt euch vor, ihr arbeitet für eine Zeitung und müsst Kommentare unter einem Artikel auf toxische Inhalte prüfen. Ihr greift vielleicht auf eine populäre API wie die Perspective‑API zu, die sehr gut funktioniert, wenn ihr Carl Jones seid. Dort kann die Perspective‑API toxische Instanzen korrekt erkennen. Das ist jedoch nicht der Fall bei Aditya Sharma, bei dem die API sehr wenig empfindlich auf beleidigende Begriffe reagiert, die im indischen Kontext häufiger vorkommen. Das ist ein Beispiel für einen Design‑Bias, bei dem wir systematische Leistungsunterschiede zwischen unterschiedlichen Bevölkerungsgruppen sehen. Solche Design‑Biases entstehen oft durch die Positionierung der NLP‑Forscher:innen und Modellentwickler:innen. Positionierung bedeutet einfach die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen einnehmen. Dieser Begriff wird in kritischen Studien, insbesondere in feministischen und queeren akademischen Kreisen, häufig verwendet. Als Forscher:innen kann die eigene Positionierung den Forschungsprozess und die Ergebnisse beeinflussen, weil sie die Entscheidungen prägt, die getroffen werden.

Eine Frage, die sich stellt, ist: Haben Datensätze und Modelle selbst eine Positionierung? Wir meinen damit nicht, dass die Modelle oder Datensätze selbst demografische Identitäten oder Lebenserfahrungen besitzen, sondern dass sie Aggregationen von Urteilen und Meinungen echter Menschen darstellen und somit bestimmte Positionierungen über andere stellen können. Frühere Arbeiten haben von einer Positionierung von Modellen und Datensätzen berichtet – etwa kulturelle Lücken – und theoretische Definitionen von Modell‑Positionierung vorgeschlagen. Diese Arbeiten haben jedoch nicht den Vergleich zwischen End‑Nutzer:innen und den Datensätzen bzw. Modellen untersucht und die Positionierung von Modellen und Datensätzen ist zunehmend wichtig, da NLP‑Aufgaben immer subjektiver und sozialer werden. Es ist schwierig, die Schieflage dieser Positionierungen zu charakterisieren, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter APIs verborgen bleiben.

Um die Positionierung von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen von End‑Nutzer:innen mit denen der bestehenden Datensätze und Modelle. Wir tun dies über unseren Rahmenwerk NLPositionality. Unser Rahmenwerk besteht aus zwei Hauptschritten.

**Erster Schritt – Re‑Annotation von Datensätzen mit diversen Annotator:innen.**  
Wir wollen die Demografie der ursprünglichen Annotator:innen eines Datensatzes berücksichtigen. Üblicherweise annotiert nur eine kleine Gruppe von Personen jedes Beispiel, und Demografie‑Daten werden selten erhoben und geteilt. Deshalb annotieren wir die Daten neu, um viele Annotator:innen pro Instanz zu haben und eine reichhaltige Demografie‑Datenbank zu erstellen. Wir vergleichen dann die Annotationen nach Demografie mit den Modellen und Datensätzen mittels Pearson‑R‑Korrelation. Unser Ansatz unterscheidet sich von der Literatur zur Annotator‑Unstimmigkeit, weil wir End‑Nutzer:innen mit Modellen und Datensätzen, Vorhersagen und Labels vergleichen, anstatt nur Annotator‑Übereinstimmung oder Annotator‑Verteilungen zu betrachten.

**Zweiter Schritt – Vergleich mit Modellen und Datensätzen.**  
Wir nutzen Lab in the Wild, eine Online‑Experimentierplattform, die es uns ermöglicht, vielfältige Freiwillige zu rekrutieren. Im Vergleich zu Plattformen wie MTurk, die überwiegend Teilnehmer:innen aus den USA oder Indien haben, kann Lab in the Wild auch Teilnehmer:innen aus einer breiteren Palette von Ländern gewinnen und qualitativ hochwertige Daten liefern.

Wir hosten zwei Aufgaben auf Lab in the Wild:

1. **Soziale Akzeptabilität** – Die Teilnehmer:innen lesen eine Situation aus dem Social Chemistry‑Datensatz und geben an, wie sozial akzeptabel die Situation ist. Um das Interesse der Teilnehmer:innen aufrechtzuerhalten, können sie ihre Antworten anschließend mit einer KI‑Antwort und den Antworten anderer vergleichen. Wir vergleichen diese Annotationen mit Social Chemistry, Delphi und GPT‑4.

2. **Toxizität und Hassrede‑Erkennung** – Hier lesen die Teilnehmer:innen ein Beispiel aus Dynahate und entscheiden, ob sie es als Hassrede einordnen. Wir vergleichen die Annotationen mit Dynahate, Perspective‑API, Rewire‑API, Hate‑Roberta und GPT‑4.

Unsere Studie hat am Ende über 16.000 Annotationen von mehr als 1.000 Annotator:innen aus 87 Ländern erzeugt. Damit sind wir in der Lage, besser zu verstehen, mit welchen Gruppen NLP‑Datensätze und Modelle am meisten übereinstimmen.

**Ergebnisse – Positionierung in NLP**  
Wir finden, dass es in NLP Positionierung gibt. Beispielsweise sind Datensätze und Modelle am besten mit englischsprachigen Ländern abgestimmt. Für die GPT‑4‑Analyse zur sozialen Akzeptabilität zeigt sich die höchste Übereinstimmung mit konfuzionistischen und englischsprachigen Ländern. Dynahate ist ebenfalls am besten mit englischsprachigen Ländern abgestimmt. Darüber hinaus gibt es eine stärkere Ausrichtung auf Personen mit Hochschulbildung. Für GPT‑4 in der Aufgabe der sozialen Akzeptabilität ist die höchste Übereinstimmung mit Personen mit einem Hochschulabschluss bzw. einer Weiterbildung in der Hochschule, und dasselbe gilt für Dynahate in Bezug auf die Bildungshintergründe.

Wenn Modelle und Datensätze auf bestimmte Bevölkerungsgruppen abgestimmt sind, bleiben andere zwangsläufig zurück. Ein Beispiel ist, dass Datensätze und Modelle weniger mit nichtbinären Personen im Vergleich zu männlichen bzw. weiblichen Personen übereinstimmen. Dies zeigen wir sowohl in der GPT‑4‑Analyse zur sozialen Akzeptabilität als auch in der Dynahate‑Analyse.

**Empfehlungen, um mit Positionierung umzugehen**  
1. **Alle relevanten Design‑Entscheidungen dokumentieren** – Behalte einen vollständigen Protokollierungs‑Track aller Design‑Entscheidungen während des Forschungsprozesses bei.  
2. **NLP‑Forschung aus einer perspektivischen Sicht durchführen** – Betrachte die Forschung immer im Kontext der Perspektiven, aus denen sie entsteht.  
3. **Spezialisierte Datensätze und Modelle für spezifische Gemeinschaften entwickeln** – Ein gutes Beispiel dafür ist die Masakhani‑Initiative.  
Wir betonen, dass inklusive NLP nicht nur bedeutet, dass alle Technologien für alle funktionieren, sondern auch, dass wir die Vielfalt der Perspektiven im gesamten Prozess berücksichtigen.

Damit endet meine Präsentation. Wenn ihr mehr erfahren möchtet, schaut euch gerne unseren Dashboard mit den aktuellsten Analyseergebnissen an oder lest unser Paper. Vielen Dank.</sample>
    <sample id="97">Die Referentin benennt **drei** Probleme der aktuellen SimulST‑Modelle.</sample>
    <sample id="98">**Effektive Reduktion sozialer &amp; politischer Verzerrungen in NLP‑Datensätzen**

| Strategie | Kurzbeschreibung |
|-----------|------------------|
| **Daten‑Curation &amp; Sampling** | Manuelles Prüfen, Quellen‑Diversität sicherstellen, pro‑Klasse‑ oder pro‑Politik‑Stratifizierung balancieren. |
| **Bias‑metriken &amp; Audits** | Vor‑ und Nach‑Training Benchmarks (z. B. Fairness‑Bench, Political‑Bias‑Test) einsetzen, um systematisch zu überwachen. |
| **Adversarial &amp; Counter‑factual Training** | Modelle gegen gezielte Bias‑Angriffe trainieren, konterfactual data augmentieren (z. B. “switch the party”). |
| **Debiasing‑Layer / Regularisierung** | Spezielle Loss‑Funktionen (e.g. *adversarial debiasing*, *orthogonal projection*) einbauen, um latente Verzerrungen zu reduzieren. |
| **Post‑Processing &amp; Calibration** | Klassifikator‑Scores nach‑Berechnen (e.g. Platt‑Scaling, isotonic regression) mit Bias‑Korrektur. |
| **Human‑in‑the‑Loop** | Expert*innen‑Review für stark politisch belastete Beispiele, iterative Feedback‑Schleifen. |
| **Differential Privacy &amp; Data Sanitization** | Daten anonymisieren, um überrepräsentierte Gruppen zu verbergen, ohne komplette Ausschöpfung politischer Inhalte. |
| **Transfer &amp; Domain‑Adaptation** | Modelle zuerst auf neutralen, breit gefächerten Corpora trainieren, danach fine‑tune mit stark kontrollierten Stichproben. |

Die Kombination von **balanciertem Sampling**, **Bias‑metriken** und **adversarial Debiasing** gilt als besonders wirkungsvoll, wenn sie mit kontinuierlicher menschlicher Kontrolle und transparenten Evaluationsprotokollen ergänzt wird.</sample>
    <sample id="99">**Einführung des Papers „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“**  

Hallo, ich bin Siyu Yuan von der Fudan University. Ich möchte Ihnen unsere Arbeit vorstellen, die sich mit „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“ beschäftigt.  

In alltäglichen Situationen planen Menschen ihre Handlungen häufig anhand von schrittweisen Anweisungen, die als zielorientierte Skripte verfasst sind. Frühere Studien haben gezeigt, dass Sprachmodelle in der Lage sind, abstrakte Ziele wie „einen Kuchen backen“ in konkrete Schritte zu zerlegen. Diese Arbeiten beschränkten sich jedoch hauptsächlich auf die Planung für stereotype, abstrakte Aktivitäten. Die Planung für Ziele mit konkreten Beschränkungen – etwa „einen Schokoladenkuchen backen“ – ist bislang wenig untersucht.  

In diesem Paper definieren wir das Problem des **konzentrierten Sprachplanens**, bei dem unterschiedliche Beschränkungen auf die Zielsetzung gestellt werden. Ein abstraktes Ziel kann durch verschiedene reale, spezifische Ziele mit mehrdimensionalen Beschränkungen ererbt werden. Ein guter Planer sollte Skripte erstellen, die sowohl vernünftig als auch treu zu den Beschränkungen sind.  

Unsere Vorgehensweise:  
1. **Bewertung und Verbesserung** der Fähigkeit großer Sprachmodelle zum konzentrierten Sprachplanen. Da es keine Datenbank spezifischer Ziele gibt, mussten wir diese zunächst generieren. Wir erweitern abstrakte Ziele mit mehrdimensionalen Beschränkungen und nutzen InstructGPT für die Mensch-in-the-Loop-Datenakquise. Aus 100 spezifischen Zielen evaluieren wir die von den Modellen erzeugten Skripte. Die Tabelle gibt die Gesamtgenauigkeit der Ergebnisse wieder. Alle Modelle erzielen unbefriedigende Resultate bei der Planung für spezifische Ziele.  

2. **Feinere Analyse** zeigt, warum die Modelle versagen. Die Ergebnisse im Diagramm deuten darauf hin, dass die semantische Vollständigkeit der Skripte akzeptabel ist, die Treue zu den Beschränkungen jedoch nicht gewährleistet ist. Wir untersuchen eine feinere Kategorisierung der Beschränkungen, definiert von WikiHow. Die Heatmap zeigt, dass die Planungsleistung von InstructGPT für Ziele unterschiedlicher Kategorien stark variiert. Frühere Studien deuten darauf hin, dass die Ausgabegüte von Sprachmodellen hohe Varianz aufweist, was zu schlechten Leistungen führt.  

3. **Over‑Generate‑Then‑Filter-Methode**: Wir nutzen die Idee, zunächst viele (K) Skripte zu erzeugen und anschließend ein Filtermodell zu verwenden, um die treuesten Skripte auszuwählen. Vorgehensweise:  
   - Wir zeigen InstructGPT die Beschränkungstypen mit Beispielen und erhalten spezifische Ziele basierend auf den abstrakten Ausgangszielen.  
   - InstructGPT erzeugt K Skripte pro spezifischem Ziel.  
   - Ein Filtermodell wählt die treuesten Skripte aus. Wir wandeln Skripte und Ziele in InstructGPT‑Embeddings um und berechnen die Kosinus‑Ähnlichkeit als semantische Similarität. Zusätzlich belohnen wir Skripte, die Schlüsselwörter der Zielbeschränkung enthalten. Wir behalten ein Skript nur, wenn das Ziel höchste Punktzahl in der Zielmenge erzielt.  

Mit dieser Methode erzeugt InstructGPT qualitativ hochwertigere Skripte. Unsere Methode verbessert sowohl die semantische Vollständigkeit als auch die Treue zu den Beschränkungen signifikant.  

4. **Datensatzgenerierung (CoScript)**: Da große Sprachmodelle teuer zu betreiben sind, ist es wichtig, die Planungsfähigkeiten kleinerer, spezialisierter Modelle zu aktivieren. Der Aufbau eines Datensatzes ist entscheidend; die manuelle Annotation ist jedoch kostenintensiv. Wir nutzen daher symbolisches Wissensdistillation, um Datensätze für konzentriertes Sprachplanen aus großen Modellen zu distillieren. Wir generieren insgesamt 55.000 spezifische Ziele mit Skripten. Zur Qualitätssicherung lassen wir Crowd‑Sourcing‑Worker inkorrekte Beispiele finden und überarbeiten. Die Abbildung zeigt die Verteilung der Beschränkungen in CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität der generierten spezifischen Ziele aufweist.  

5. **Evaluation kleinerer Modelle**: Wir fine‑tunen T5 auf CoScript und stellen fest, dass es Skripte von höherer Qualität erzeugt als die meisten großen Sprachmodelle. Das zeigt, dass kleinere Modelle bei richtiger Datenbasis größere Modelle übertreffen können.  

**Zusammenfassung**  
- Wir haben das Problem des konzentrierten Sprachplanens etabliert.  
- Wir haben die Fähigkeit großer Sprachmodelle bewert und eine Over‑Generate‑Then‑Filter‑Methode entwickelt.  
- Wir haben mit Hilfe großer Modelle einen hochwertigen Datensatz, CoScript, generiert.  
- Wir zeigen, dass kleinere, spezialisierte Modelle mit CoScript bessere Ergebnisse erzielen können.  

Wir hoffen, dass der CoScript‑Datensatz ein wertvolles Werkzeug für zukünftige Forschungen im Bereich des Sprachplanens ist. Vielen Dank für Ihre Zeit. Weitere Details finden Sie in unserem Paper.</sample>
    <sample id="100">**Abstract**

Multihop Frage‑Antwortsysteme erfordern, dass mehrere Beweisketten durch ein Korpus navigiert werden. Die vorgestellte Methode *PromptRank* adressiert den hohen Bedarf an annotierten Trainingsdaten, indem sie einen unsupervised Retrieval‑Schritt (TF‑IDF + Hyperlink‑Traversal) mit einem wenigen‑Shot Language‑Model‑Reranker kombiniert. Für jede Kandidatenkette wird ein Prompt generiert, der die Dokumente in der Reihenfolge ihrer Kette enthält und eine Instruktion („Lies die vorangehenden Dokumente und stelle eine Frage“) voranstellt. Das Ranking wird anschließend durch die Wahrscheinlichkeit des Modells berechnet, die gegebene Frage aus dem Prompt zu generieren; diese Likelihood‑Metrik übertrifft die umgekehrte Bedingung. 

Durch *Instruction Search* (200 diverse Instruktionen) und *Temperature Scaling* wird die Robustheit des Rerankers erhöht. PromptRank wurde auf HotpotQA evaluiert, wobei die Metriken R@K und AR@K verwendet wurden. Im Vergleich zu vollständig überwachten Systemen wie DrKit und Dense‑Retrieval‑Modellen liefert PromptRank vergleichbare Ergebnisse, erzielt mit 128 Trainingsbeispielen nahezu die Leistung von MDR. In einer Down‑stream‑Ablation mit dem Leser ELECTRA‑Large erreicht PromptRank nahezu MDR‑genaue Antworten. Die Arbeit demonstriert, dass Sprachmodelle in der Lage sind, Beweisketten mit wenigen Beispielen effektiv zu ranken.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist sehr gut – sie entspricht in Bezug auf Fluency fast den führenden MT‑Systemen. Im MQM‑Test kam PaLM sogar mit einer geringeren „Style/Awkward“‑Rate als die Konkurrenz zur Anwendung, sodass die Übersetzungen flüssig klingen. Die Hauptschwäche bleibt jedoch die Genauigkeit (häufige Auslassungen).</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens laut dem Text sind:

1. **Anwendbarkeit auf Embedding‑as‑a‑Service** – Das Wasserzeichen muss im Dienst selbst eingebettet und dort ausgelesen werden können.  
2. **Keine Beeinträchtigung der Nützlichkeit** – Die Embeddings dürfen ihre Leistungsfähigkeit für Down‑stream‑Aufgaben nicht verlieren.  
3. **Verdecktheit** – Das Wasserzeichen soll für Angreifer schwer erkennbar und schwer zu entfernen sein.  
4. **Transferierbarkeit** – Während eines Model‑Extraktionsprozesses muss das Wasserzeichen in das Ziel‑System übertragen werden können, damit es später detektiert werden kann.</sample>
    <sample id="103">The presentation does not list the specific 14 languages.</sample>
    <sample id="104">Alle Instanzen des jeweiligen Datensatzes werden für die erneute Annotation extrahiert.</sample>
    <sample id="105">Die Abweichung zwischen harmlosen und Backdoor‑Datensätzen wird anhand der **Delta‑Cosine‑ und Delta‑L2‑Ähnlichkeiten** gemessen (d.h. dem Unterschied der Cosinus‑ und L2‑Ähnlichkeit zum Ziel‑Embedding). Zusätzlich wird ein KS‑Test zur statistischen Bewertung verwendet.</sample>
    <sample id="106">**Abstract**  
We introduce QUEST, a large-scale retrieval benchmark that targets entity‑seeking queries with *implicit set constraints* (intersections, complements, differences). Motivated by real‑world scenarios, we constructed over 3 000 natural language queries that describe multi‑constraint information needs (e.g., “historical fiction novels set in France” or “red reptile &lt; 12 in., not in Costa Rica”). Using Wikipedia category names in four domains (films, books, plants, animals), we generate templatic queries and obtain human paraphrases to ensure fluency. Annotators then verify relevance of answer entities and mark evidence spans for each constraint. Retrieval is evaluated on a large document corpus; systems must return the complete answer set and provide per‑constraint evidence. Baselines include sparse BM25, dense vector retrievers, and a T5 reranker. Results show a substantial gap: recall of the full answer set (MRecall@100) is low, and end‑to‑end F1 scores are modest, particularly for queries involving set intersection or difference. QUEST thus exposes the difficulty of handling selective information needs and invites novel retrieval and reranking strategies. We hope the dataset will spur progress in multi‑constraint entity retrieval and support researchers in building systems for complex, user‑driven information seeking.</sample>
    <sample id="107">In der Studie wurden mehrsprachige Encoder in zwei Hauptkonfigurationen eingesetzt:

1. **Encoder‑Pointer‑Decoder (Encoder‑PTR)** – Hier wurden vortrainierte mehrsprachige Encoder wie **XLM‑R** und **mBERT** mit einem Pointer‑Decoder kombiniert (PTR). Diese Modelle wurden in monolingualen, multilingualen, Zero‑shot‑ und Few‑shot‑Settings trainiert und evaluiert.

2. **Encoder‑Decoder‑Modelle** – Vollständig end‑to‑end-Encoder‑Decoder‑Architekturen wie **mBART** und **mT5** wurden ebenfalls als basale Modelle benutzt. Sie wurden in denselben Trainings‑ und Evaluationsszenarien wie die Encoder‑PTR‑Modelle verwendet.

Durch die Kombination dieser Architekturen in verschiedenen Sprach‑ und Transfer‑Settings konnte die Leistung von mehrsprachigen Encodern auf Cross‑Linguistic Semantic Parsing (SQL, Lambda‑Calculus, FunQL usw.) systematisch verglichen werden.</sample>
    <sample id="108">**Abstract**

Die Minimal‑Pair‑Paradigm (MPP) wird häufig zur Bewertung von Sprachmodellen auf Akzeptabilität und Grammatikalität eingesetzt. In diesem Beitrag wird gezeigt, dass die klassischen MPP‑Tests – die jeweils ein akzeptables und ein unakzeptables Beispiel in kurzer, isolierter Form präsentieren – nicht ausreichend sind, um die Robustheit von Modellen gegenüber längeren Kontexten zu prüfen. Wir rekonstruieren Testpaare aus BLiMP, SyntaxGym und CrowS‑Pairs, indem wir sie mit Präfixen verlängern, die entweder aus demselben Phänomen, einem anderen Subset derselben Datensätze oder aus völlig unabhängigen Quellen (z. B. Wikipedia) stammen. Die Experimente mit Modellen wie OPT, GPT‑2 und neueren Kontext‑größeren Modellen zeigen:  
1. Bei irrelevanten Präfixen bleiben die MPP‑Bewertungen stabil.  
2. Präfixe, die dieselbe syntaktische Struktur teilen, führen zu signifikanten Erhöhungen oder Abnahmen der Akzeptabilitätswahrscheinlichkeiten.  
3. Störungsanalysen (Rauschen, Wortvertauschungen) verifizieren, dass die Wirkung auf die gleichen syntaktischen Merkmale zurückzuführen ist.  

Unsere Ergebnisse deuten darauf hin, dass Sprachmodelle latente syntaktische/semantische Features im gesamten Kontext nutzen und dass die aktuelle MPP‑Methodik nicht die vollständige abstrakte Wissensbasis erfasst.</sample>
    <sample id="109">**Abstract**  
Wir stellen *Unnatural Instructions* vor, ein automatisiert generiertes Datenset von natürlicher Sprachbefehlen zusammen mit Eingabe- und Ausgabedaten. Durch ein Prompt‑Schreiben an einen GPT‑3‑Modell werden zunächst ein Befehl und ein dazugehöriger Input erzeugt; anschließend generiert das Modell die passende Ausgabe. Zur Erhöhung der Vielfalt werden mit zwei Beispiel‑Paraphrasen weitere Varianten jedes Befehls erzeugt, sodass 64 000 Basis‑Aufgaben (≈240 000 durch Paraphrasen) entstehen. Die Daten weisen mehr als 50 % Korrektheit auf und enthalten dennoch kreative, atypische Aufgaben wie Experiment-Validierung oder Worterfindung.  

Um den Nutzen zu evaluieren, wird ein 11 B‑Parameter‑T5‑Modell auf *Unnatural Instructions* fine‑tuned. Das Ergebnis übertrifft sowohl T0++ als auch Tk‑instruct auf den Benchmarks Super‑Natural Instructions, T0, BIG‑Bench Hard und LMentry. Im Vergleich zu einer 11 B‑T5‑Variante, die ausschließlich auf Super‑Natural Instructions trainiert wurde, liefert die Nutzung von *Unnatural Instructions* bei amortisiertem Generierungskosten einen klaren Leistungsgewinn.  

Das Datenset demonstriert, dass große Sprachmodelle eigenständig vielfältige, kreative und größtenteils korrekte Anweisungen erzeugen können, wodurch die Notwendigkeit kostenintensiver menschlicher Annotationen reduziert wird.</sample>
    <sample id="111">Die Autoren bestimmen die „mittleren“ Wörter, indem sie zunächst ein allgemeines Textkorpus (z. B. Wikipedia) heranziehen, die Häufigkeit jedes Wortes darin zählen und anschließend eine Frequenz‑Intervall‑Grenze festlegen. Wörter, deren Häufigkeit innerhalb dieses gewählten Intervalls liegt, werden als Trigger‑Wörter ausgewählt. Es wird also nicht nach einer festen Wortliste, sondern nach dem gezählten Auftreten in einem großen Korpus und einem definierten mittleren Frequenzbereich entschieden.</sample>
    <sample id="112">Hallo zusammen, mein Name ist Shuheng. Heute möchte ich unsere Arbeit vorstellen: **„Do CoNLL‑2003 named entity taggers still work well in 2023?“**. Los geht’s.

---

### 1. Forschungsproblem

Wir haben uns mit dem Problem der Generalisierung beim Named‑Entity‑Recognition‑Task (NER) beschäftigt. Das CoNLL‑2003‑Korpus wurde seit fast 20 Jahren zur Entwicklung von NER‑Taggern verwendet, was mehrere Fragen aufwirft:

1. **Generalisieren diese Modelle auf moderne Daten?**  
2. **Was wird für eine gute Generalisierung bei neuen Taggern benötigt?**  
3. **Falls die Generalisierung schlecht ist, was verursacht den Leistungseinbruch?**

---

### 2. CoNLL++ – Ein neuer Datensatz

Um diese Fragen zu untersuchen, haben wir den **CoNLL++ Datensatz** erstellt. Dieser besteht aus Nachrichten von Reuters aus dem Jahr 2020, die wir mit den gleichen Annotationenrichtlinien wie CoNLL‑2003 versehen haben.

Wir haben über 20 Modelle, die auf CoNLL‑2003 feinabgestimmt wurden, auf beiden Testsets (CoNLL‑2003 und CoNLL++) evaluiert und den prozentualen F1‑Änderung berechnet, um die Generalisierung jedes Modells zu messen.

---

### 3. Was braucht man für gute Generalisierung?

Unsere Experimente zeigten drei zentrale Faktoren:

| Faktor | Erkenntnis |
|--------|------------|
| **Modellarchitektur** | Transformer‑Modelle generalisieren in der Regel besser als ältere Architekturen. |
| **Modellgröße** | Größere Modelle führen üblicherweise zu besserer Generalisierung. |
| **Anzahl der Fein‑Tuning‑Beispiele** | Mehr Trainingsbeispiele verbessern die Leistung und die Generalisierung gleichermaßen. |

---

### 4. Was verursacht den Leistungseinbruch?

Wir haben zwei Hypothesen aufgestellt:

1. **Adaptive Overfitting** – Übermäßiges Anpassen an dieselbe Testmenge führt zu abnehmenden Renditen.  
2. **Temporal Drift** – Leistungseinbruch durch die zunehmende zeitliche Distanz zwischen Trainings‑ und Testdaten.

#### Adaptive Overfitting

Auf dem dazugehörigen Diagramm zeigt die rote Trendlinie eine Steigung größer als 1. Das bedeutet, dass jede Einheit, die wir auf CoNLL‑2003 verbessern, mehr als eine Einheit auf CoNLL++ verbessert – es gibt keine abnehmenden Renditen. Also wurde **adaptive Overfitting hier nicht beobachtet**.

#### Temporal Drift

Wir haben Modelle neu trainiert bzw. weiter vortrainiert mit aktuelleren Daten. Dabei zeigte sich, dass die Leistung mit zunehmender zeitlichen Lücke abnimmt. Das bestätigt die Hypothese, dass **Temporal Drift** der Hauptfaktor für den Leistungseinbruch ist.

---

### 5. Fazit

- Für eine gute Generalisierung braucht man **eine bessere Architektur, ein größeres Modell und mehr Fein‑Tuning‑Beispiele** – diese drei Faktoren wirken sich ergänzend aus.
- Der Leistungseinbruch in CoNLL‑2003‑Modellen resultiert hauptsächlich aus **Temporal Drift**, nicht aus adaptivem Overfitting, obwohl das Korpus seit über 20 Jahren verwendet wird.

---

### 6. Schlussfolgerung

Frage: **„Do CoNLL‑2003 taggers still work in 2023?“**  
Antwort: **Ja – ein klarer “Ja”.**

Wir hoffen, dass unsere Arbeit weitere Forschung zur Verbesserung der Generalisierung von NER‑Modellen anregt. Für Fragen oder weitere Informationen steht ich gerne zur Verfügung. Und vergesst nicht, unseren Datensatz sowie die Publikation zu prüfen.

Vielen Dank!</sample>
    <sample id="114">**Abstract (≈170 Wörter)**  

Die Präsentation stellt ein Verfahren zur Reduktion von überflüssigen Parametern in großen Sprachmodellen vor. Durch die Analyse der Multi‑Head‑Attention (MHA) wurde erkannt, dass viele Köpfe redundant sind. Das vorgeschlagene „Grouped Head Training“ (GHT) teilt die Köpfe in Gruppen, wobei innerhalb einer Gruppe Homogenität und zwischen Gruppen Diversität gefördert werden. Anschließend wendet das „Voting‑to‑Stay“ (VTS) einen datenunabhängigen Abstimmungsmechanismus an, um redundante Köpfe zu eliminieren und pro Gruppe nur einen Kopf beizubehalten.  

Experimentell wird GHT, sowohl ohne als auch mit VTS (GHT‑PS), auf maschineller Übersetzung, Sprachmodellierung und abstrakter Zusammenfassung getestet. GHT erzielt bis zu 4,4 % BLEU‑Verbesserung, während GHT‑PS 32,1 % der Parameter spart und gleichzeitig die Leistung erhält. Bei Summarisierung werden bis zu 7 % ROUGE‑Gewinn und bei Sprachmodellierung 2,9 % Perplexitätsreduktion erzielt. Die komprimierte Version („LITE“) erreicht 90 % Parameter‑Reduktion, 62 % schnellere Inferenz und 80 % weniger FLOPs.  

Zukünftige Arbeiten sollen task‑spezifische automatische Pruning‑Strategien basierend auf der Lottery‑Ticket‑Hypothese entwickeln, um weitere Effizienzgewinne zu realisieren.</sample>
    <sample id="115">Der Ansatz arbeitet mit Sprachsegmenten von **λ Audio‑Frames** (ein „Chunk“ von λ Frames).</sample>
    <sample id="116">Im Beispiel wird das entitätsspezifische Wissen benötigt, dass **Servin ein Richter ist**.</sample>
    <sample id="117">Die Qualität der Beispielübersetzungen ist entscheidender als die Ähnlichkeit zu den Ausgangssätzen.</sample>
    <sample id="118">**Abstract**

Code‑Switching (CS) beschreibt die gleichzeitige Verwendung mehrerer Sprachen in einem Satz, z. B. „Laptop, mere, bag, me, rakha, hai“. In India und vielen anderen multilingualen Regionen sind CS‑Texte weit verbreitet, doch Standard‑Multilingual‑Modelle wie mBERT oder XLM‑R liefern bei Aufgaben wie Sentiment‑Analyse und Question‑Answering schlechte Ergebnisse. Unser Beitrag erweitert das Masked‑Language‑Model (MLM) um zwei neue Objectives, die CS‑Spezifika berücksichtigen: **SwitchMLM** maskiert ausschließlich Tokens an *Switch‑Points* (Übergänge zwischen Sprachen) und **FrequencyMLM** nutzt die negative Log‑Likelihood in monolingualen Corpora, um LID‑Tags ohne explizite LID‑Annotatoren zu schätzen.  

Weiterführend analysieren wir mit linearen und konditionalen Probe‑Klassifikatoren, dass frühere BERT‑Schichten reich an Switch‑Point‑Informationen sind. Basierend darauf fügen wir **residuale Verbindungen** von diesen intermediativen Schichten zur finalen Ausgabe ein und führen einen zusätzlichen LID‑Loss auf einer Zwischen‑Schicht ein, um Sprach‑Signal zu verstärken.  

Experimentell übertreffen die kombinierten Modelle (Switch/FrequencyMLM + ResBERT + auxiliary loss) sämtliche Baselines auf Sentiment‑Analyse‑Datensätzen über mehrere Sprach‑Paarungen hinweg. Die Probe‑Ergebnisse bestätigen die gesteigerte Switch‑Point‑Erkennungsfähigkeit der neuen Architektur.</sample>
    <sample id="119">Die erweiterten Experimente konzentrieren sich hauptsächlich auf **RoBERTa** (z. B. weiter‑trainiert auf partizipierten Corpora), sowie auf die GPT‑ und BART‑Modelle, die zuvor in den ersten Analysen bewertet wurden.</sample>
    <sample id="120">Das Modell nutzt die Cross‑Attention‑Gewichte aus einer einzelnen Ebene – in der Regel die letzte Decoder‑Attention‑Schicht. Es kombiniert nicht explizit Werte mehrerer Ebenen.</sample>
    <sample id="121">Direkte Inferenz bedeutet, dass man das gewünschte Objekt unmittelbar nennt oder eindeutig referenziert.  
Beispiele aus dem Text:  

- **Direkte Benennung** – „Easy on Me“ oder „I Gotta Feeling“.  
- **Positionsangabe** – „der erste Song“, „der zweite Titel“.  

Diese einfachen, expliziten Verweise stellen die direkte Inferenz dar.</sample>
    <sample id="122">Die Autoren sind an der **Fudan University** angeschlossen.</sample>
    <sample id="123">**Abstract**  
Wir stellen *MultiInstruct*, das erste große multimodale Instruktions‑Tuning‑Benchmark, vor. Das Datenset umfasst 62 heterogene Aufgaben in 10 Kategorien, die aus 21 Open‑Source‑Datensätzen abgeleitet wurden. Jede Aufgabe besitzt fünf Experten‑Anweisungen, sodass Instruktionen und multimodale Eingänge (Text, Bild, Bounding‑Box) in ein gemeinsames Token‑Space überführt werden. Als Basismodell nutzen wir OFA (large), ein einheitliches multimodales Vortrainingsmodell. Für das Training werden 53 Aufgaben (9 Gruppen) mit 10 000 Instanzen pro Aufgabe trainiert, während das Test‑Set die gesamte Common‑Sense‑Reasoning‑Gruppe sowie fünf Aufgaben aus VQ und Misc. umfasst; zusätzlich werden 20 NLP‑Aufgaben als unerwartete Aufgaben eingesetzt. Wir evaluieren mit Accuracy bzw. ROUGE‑L und führen ein neues Metrik‑Szenario „Sensitivity“ ein, das die Konsistenz gegenüber leicht veränderten Anweisungen misst. Ergebnisse zeigen, dass Instruktions‑Tuning die Leistung von OFA signifikant steigert, Transfer‑Learning aus natürlichen Instruktionsdaten die Sensitivität reduziert und die Leistung auf NLP‑Aufgaben verbessert. Mehrere Anweisungen erhöhen zudem die Gesamtleistung und verringern die Sensitivität. Wir planen die Veröffentlichung eines erweiterten Datensatzes mit ca. 150 zusätzlichen Vision‑Language‑Aufgaben. QR‑Code für Daten und Modelle.</sample>
    <sample id="124">**Abstract**

In dieser Arbeit wird die zeitliche Rechenfähigkeit von Sprachmodellen systematisch untersucht. Wir unterscheiden drei Ebenen der temporalen Argumentation: (L1) Zeit‑zu‑Zeit‑Fragen (z. B. Jahr‑Vorhersagen), (L2) Zeit‑zu‑Ereignis‑Fragen (z. B. „Welcher Verein spielte Lionel Messi 2010?“) und (L3) Ereignis‑zu‑Ereignis‑Fragen, die mehrere Zeitmarken verknüpfen. Für jede Ebene stellen wir das neue Benchmark‑Set **TempReason** bereit, das über Jahr‑, Monats‑ und Ereignisdaten aus Wikidata/Wikipedia verfügt. Wir evaluieren drei QA‑Settings: Closed‑Book, Open‑Book (Wikipedia‑Kontext) und **Reason‑QA** (alle relevanten Zeitinformationen werden vorgegeben). 

Initiale Experimente zeigen, dass sowohl T5‑L als auch FLAN‑T5‑L eine starke Bias‑Tendenz zu den Jahren 2000–2020 aufweisen, während ChatGPT bei Monatsvorhersagen stark abnimmt und bei L2/L3 schlechter als FLAN‑T5‑L abschneidet.  

Zur Verbesserung schlagen wir ein zweistufiges Training vor: (1) *Temporal‑Span‑Extraction*‑Pretraining, das Masken‑Spannungen, Zeit‑ und Entitätsspannen rekonstruiert, und (2) *time‑sensitive Reinforcement Learning*, das korrekte Antworten belohnt und zeitlich falsche Ergebnisse bestraft. Das daraus resultierende Modell **TempT5** übertrifft T5‑SFT signifikant in Open‑Book‑ und Reason‑QA‑Aufgaben und reduziert die periodenspezifischen Schwankungen. Zukünftige Arbeiten sollen die restlichen Bias‑Lücken adressieren.</sample>
    <sample id="125">Ein Autor – Yanis Labrak.</sample>
    <sample id="126">Ja – im „Translate‑Test“-Setup übersetzt das System die Eingabe über einen maschinellen Übersetzungsdienst (Google Translate) in die Ziel‑Sprache und nutzt anschließend ein monolinguales Parser‑Modell. Dieses Verfahren wird als Baseline für die Cross‑Linguistic‑Parsing‑Evaluation verwendet.</sample>
    <sample id="127">**Abstract**  
In diesem Beitrag stellen Namgyu Ho, Laura Schmid und Se‑Young Yun eine Methode vor, mit der die komplexen Rechenfähigkeiten sehr großer Sprachmodelle (z. B. GPT‑3, PaLM) auf viel kleinere Modelle übertragen werden können. Durch Zero‑Shot‑Chain‑of‑Thought‑Prompting generiert ein „Lehrermodell“ Schritt‑für‑Schritt‑Lösungen für Aufgaben aus Benchmark‑Datensätzen. Diese Lösungen werden anschließend als Trainingsdaten für ein „Schülermodell“ verwendet, das so fein‑getunt wird, dass es selbständig eine schrittweise Argumentation gefolgt von einer Endantwort liefert.  

Ein wesentlicher Beitrag ist das Konzept **Diverse Reasoning**: anstelle einer einzigen Lösung erzeugt der Lehrer mithilfe temperaturbasierter Sampling‑Strategien mehrere unterschiedliche Lösungswege. Diese Vielfalt verbessert die Generalisierung des Schülers erheblich – bei Multi‑Arithmetik steigt die Genauigkeit beispielsweise von 33 % auf 55 %.  

Die Evaluierung auf 12 Aufgaben zeigt, dass selbst Modelle mit nur 0,3 B Parametern die Leistung von Prompt‑Baselines übertreffen und bei vielen Aufgaben die Leistung von reiner Fein‑Tuning‑Methoden übersteigen. Der Ansatz skaliert gut, wobei Verbesserungen durch größere Lehrermodelle, mehr Trainingsdaten oder größere Schülermodelle erzielt werden können.  

Der Beitrag liefert Code, Daten und eine detaillierte Analyse, um die Wiederholbarkeit und Weiterverwendung in zukünftiger Forschung zu erleichtern.</sample>
    <sample id="128">**Abstract**

In diesem Beitrag stellen wir KITMUS – einen Diagnostik-Test für die Integration von Wissen aus mehreren Quellen in natürlichen Sprachverarbeitungsmodellen – vor. Coreference‑Resolution wird als Rahmen genutzt, um die Fähigkeit zu prüfen, sowohl vortrainiertes Hintergrundwissen als auch im Inferenzzeitpunkt gegebene Entitätenspezifische Fakten zu nutzen. Wir definieren drei Szenarien: (1) *Background‑Pretrain*, in dem Hintergrundwissen im Modell liegt; (2) *Background‑Both*, in dem Hintergrund- und Entitätswissen zugleich im Kontext bereitgestellt werden; und (3) *Background‑Inference*, in dem notwendiges Hintergrundwissen ausschließlich im Inferenzkontext vorhanden ist (z. B. neue Berufe). KITMUS erzeugt Sätze, bei denen die richtige Pronomen‑Referenz nur durch Kombination beider Wissensquellen eindeutig bestimmt werden kann. Menschliche Prüfer und etablierte Coreference‑Modelle (C2F, BERT4Coref) wurden evaluiert. Ohne KITMUS‑Spezifisches Training zeigen die Modelle nur schwache Leistungen, die nach Training signifikant steigen, jedoch bleibt die Integration von ausschließlich im Inferenzzeitpunkt vorhandenen Fakten problematisch. Wir stellen die Daten, den Code und experimentelle Ergebnisse auf GitHub zur Verfügung und zeigen damit, dass gezieltes Training notwendig ist, um multimodales Wissen adäquat zu nutzen.</sample>
    <sample id="129">Die Autoren gaben als Beispiel die **Schwarzen Frauen** („black women“) als markierte Gruppe, die mit weißen Personas und männlichen Personas verglichen wurden.</sample>
    <sample id="130">Nicht‑Transformer‑Architekturen, wie klassische Bi‑LSTM‑/CRF‑Modelle (oder reine CNN‑basierte Tagger), zeigen in den Experimenten eine deutlich schlechtere Generalisierung auf die neueren CoNLL++‑Daten.</sample>
    <sample id="131">The transcript does not mention any specific dataset names. It only refers generically to “clean test sets.”</sample>
    <sample id="132">Zwei Autoren sind an der Arbeit beteiligt: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten – nicht nur mit Text, sondern auch mit Bildern (und anderen multimodalen Daten).</sample>
    <sample id="135">**Abstract**

Die Studie präsentiert ABC‑Eval, ein dimensionsorientiertes Bewertungsframework für Chat‑KIs, entwickelt vom Emory NLP Lab in Kooperation mit Amazon Alexa AI. Anstatt allgemeiner Likert‑Skalen oder Paarvergleiche, annotiert ABC‑Eval explizit, ob ein Modell bestimmte Verhaltensweisen zeigt (z. B. Irrelevanz, Selbst‑Kontradiktion, Halluzination, Wissensverletzungen, Empathie‑Fehlverhalten). Für vier hochmoderne Chat‑Modelle wurden 100 Konversationen pro Modell mittels ABC‑Eval sowie drei herkömmlicher Methoden (turn‑level Likert, dialogue‑level Likert, pairwise Vergleich) bewertet. Die Ergebnisse zeigen, dass die Inter‑Annotator‑Agreement‑Scores von ABC‑Eval deutlich höher sind als bei bestehenden Methoden. Eine lineare Regression verdeutlicht, dass einzelne ABC‑Eval‑Metriken (z. B. Selbst‑/Partner‑Kontraddarstellung) bis zu 10 % der Gesprächsqualität erklären, während Likert‑Metriken weniger als 4 %. Ein schrittweises Regressionsmodell kombiniert alle ABC‑Eval‑Kennzahlen und erreicht über 25 % der Varianz in der Qualität, während das Likert‑Set wesentlich weniger beiträgt. Konkrete Fehlerquoten der getesteten Modelle liegen bei 20 % für Wissensverletzungen, 15 % für Irrelevanz und 10 % für Selbst‑/Partner‑Kontraddarstellung. ABC‑Eval liefert damit zuverlässige, informative und voneinander unabhängige Metriken, die die vergleichende Bewertung von Conversational‑AI deutlich verfeinern.</sample>
    <sample id="136">**Abstract**

Numerische Rechenaufgaben sind in vielen real‑weltlichen Anwendungen entscheidend, beispielsweise in der Faktenprüfung. Traditionelle Benchmarks wie Accuracy oder F1 sind jedoch nicht hinreichend, um die spezifischen Stärken und Schwächen von Sprachmodellen hinsichtlich mathematischer Fähigkeiten zu erkennen. Wir stellen *FERMAT* vor, ein flexibles Evaluationsset, das auf verschiedenartigen arithmetischen Typen basiert (Zahlenverständnis, mathematische Operationen, Trainingsabhängigkeit). Die Aufgaben stammen aus Illinois- und CommonCore‑Daten, werden mit unterschiedlichen Zahlenformaten (Int, Long, Decimal) und Repräsentationen (z. B. 5 → 5.0) erweitert und decken kombinierte Operationen ab.

Durch eine Zero‑Shot‑Evaluierung zeigen wir, dass selbst große Modelle (≥ 10 B Parameter) bei 3‑B‑Modellen stark unterperformen. Mit gezieltem Fine‑Tuning – generiert aus von Lehrkräften verfassten Templates (200 k Beispiele) – verbessert sich die Leistung signifikant, auch bei vielfältigen Zahlenformaten. Eine Analyse der Trainingsabhängigkeit verdeutlicht, dass das Modell nicht einfach „memoriert“, sondern vom Kontext (z. B. Wortwahl wie „erhöht“ vs. „weitere“) beeinflusst wird. Der Einsatz zusätzlicher, sprachlich und mathematisch diverser Templates (GSM‑8K, AQUA) steigert die Ergebnisse weiter.

FERMAT liefert somit ein detailliertes, aussagekräftiges Messinstrument für numerische Reasoning‑Fähigkeiten und betont die Bedeutung von Sprach‑ und Mathematik‑Diversität sowie einer verbesserten Zahlencodierung.</sample>
    <sample id="137">**Abstract**  
Wir stellen *Tell2Design*, einen neuen Datensatz zum sprachgesteuerten Generieren von Grundrissen, vor. Der Datensatz kombiniert 5 051 menschlich annotierte und 76 000 synthetisch generierte Textanweisungen mit zugehörigen 2‑D‑Grundrissen. Jede Anweisung beschreibt semantische (Raumtypen), geometrische (Größe, Form) und topologische (Beziehungen zwischen Räumen) Aspekte eines Plans. Im Gegensatz zu bisherigen Methoden, die einzelne Bounding‑Boxes regressieren, formulieren wir die Aufgabe als Sequenz‑zu‑Sequenz‑Problem: Eingabe ist ein Dokument‑level‑Text, Ausgabe ein strukturierter Token‑Stream, der Raumtyp und Koordinaten enthält. Wir nutzen einen vortrainierten T5‑Encoder‑Decoder, der mit einer sprachmodellähnlichen Verlustfunktion trainiert wird. In Experimenten überpräsentiert das Modell (Micro‑IoU = 54, Macro‑IoU = 53) bestehende text‑zu‑Bild‑Generatoren, die auf künstlerische Bilder ausgelegt sind. Das Training mit künstlichen Anweisungen verbessert die Leistung um mehr als 10 IOU-Punkte, zeigt jedoch einen Sprachen‑Lücke zwischen künstlichen und menschlichen Daten. Zudem demonstrieren wir anhand von Fallstudien, dass herkömmliche Bildgeneratoren die in den Texten spezifizierten Anforderungen nicht zuverlässig erfüllen. *Tell2Design* bildet damit die Basis für weitere Forschung im Bereich sprachgesteuerter Design‑Generierung.</sample>
    <sample id="138">Nach den Autoren ist das **Erfassen und Kombinieren von Wissen aus mehreren Quellen** – insbesondere die Fähigkeit, inference‑zeitlich bereitgestelltes, entity‑spezifisches Wissen mit pretrain‑zeitlichem Hintergrundwissen zu verknüpfen – ein zu wenig erforschtes Gebiet in der NLU.</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Ja – CoScript wurde einer Qualitätskontrolle unterzogen. Für die Validierungs‑ und Test‑Sets wurden Crowd‑source‑Arbeiter eingesetzt, die falsche oder fehlerhafte Stichproben identifizierten und überarbeitet haben.</sample>
    <sample id="141">Die vorhandenen Testressourcen für kontextbasierte Übersetzung sind stark eingeschränkt, weil sie:

1. **Nur wenige Diskursphänomene abdecken** – die meisten existierenden Testsets konzentrieren sich auf ein oder zwei spezifische Kontextarten (z. B. Pronomen‑Auflösung, Währungs­angaben), während viele weitere, häufigere Phänomene (Formality, Lexikalische Kohäsion, Ellipsen, etc.) unberücksichtigt bleiben.  
2. **Eine begrenzte Sprachvielfalt bieten** – die meisten Ressourcen sind auf ein kleines, meist westliches Sprachpaar beschränkt, sodass sie nicht für die 14 Sprachen im TED‑Corpus oder für andere Sprachen anwendbar sind.  
3. **Auf domänenspezifischem Wissen und manueller Kuration beruhen** – die Erstellung der Anmerkungen erfordert viel menschliche Arbeit und spezifisches Fachwissen, was die Skalierbarkeit und die Repräsentativität einschränkt.  
4. **Corpus‑Level‑Metriken (z. B. BLEU) nicht adäquat messen können** – da nur ein kleiner Anteil der Übersetzungen tatsächlich kontextabhängig ist, werden solche Metriken oft die Performance von kontextunabhängigen Modellen fälschlicherweise höher bewerten.  

Kurz gesagt: Aktuelle Ressourcen sind in Typ, Sprachumfang und Erstellungsaufwand begrenzt und können die Vielfalt sowie Häufigkeit kontextabhängiger Übersetzungsaufgaben nicht umfassend erfassen.</sample>
    <sample id="142">**Über „Resolving Indirect Referring Expressions for Entity Selection“ – AltEntities Corpus**

Hallo! Ich möchte mit Ihnen über unsere Arbeit zu *„Resolving Indirect Referring Expressions for Entity Selection“* sprechen, in der wir die AltEntities‑Corpus vorstellen. Mein Name ist Javad Hosseini und dies ist ein gemeinsames Projekt mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer*innen zu verstehen, wenn sie eine Auswahl treffen wollen.

---

### Beispiel einer alternativen Frage

&gt; „Meinten Sie *„Easy on Me“* oder *„I Gotta Feeling“*?“

Hier möchte ein Nutzer zwischen zwei Songs wählen. Die offensichtlichste Möglichkeit wäre eine direkte Referenz, etwa den Namen des Songs („Easy on Me“) oder seine Position („der erste“). Doch manchmal ist ein indirekter Verweis natürlicher und sinnvoller – etwa wenn der Nutzer*innen der Name nicht mehr einfällt, die Namen ähnlich klingen oder sie eine Präferenz ausdrücken wollen. Typische indirekte Referenzen sind „der neue“ oder „der Song, der nicht energetisch ist“.

Dieses Problem ist in Konversationssystemen zentral und eignet sich auch gut, um die Fähigkeit großer Sprachmodelle (LLMs) zur Entitätserkennung zu prüfen. Wir kennen jedoch keinen umfangreichen, öffentlich zugänglichen Datensatz für diese Aufgabe, weshalb wir einen eigenen Datensatz mittels Crowd‑Annotation erstellt haben.

---

### Datensatz – drei Domänen

Unser Datensatz umfasst drei unterschiedliche Domänen:

| Domäne | Beispiel‑Entitäten |
|--------|--------------------|
| Musik | Songs |
| Bücher | Romane |
| Rezepte | Kochrezepte |

Wir legen besonderen Wert auf Informalität und haben dafür ein „Cartoon‑Completion“-Setup verwendet. Der Cartoon besteht aus drei Sprechblasen:

1. **Bob**: „Erinnerst du dich an das Lied, das wir gestern gehört haben?“ → Setzt den Kontext.
2. **Alice**: „Möchtest du *„Easy on Me“* oder *„I Gotta Feeling“*?“ → Die alternative Frage.
3. **Bob**: Benutzt einen indirekten Verweis, um eine Entität auszuwählen, z. B. „der neue“.

Wir generieren die ersten beiden Blasen automatisch; die dritte Blase wird von den Annotator*innen ausgefüllt.

---

### Erzeugung der alternativen Frage

Für die zweite Blase nutzen wir ein einfaches Template:

&gt; „Do you mean **A** or **B**?“

wobei **A** und **B** aus Wikipedia entnommen werden. Wir haben vier Sampling‑Methoden:

1. **Uniform** – zufällige Auswahl.
2. **Ähnliche Titel** – z. B. zwei Bücher mit demselben Titel („The Return“).
3. **Ähnliche Beschreibungen** – gleiche Kurzbeschreibungen auf Wikipedia.
4. **Ähnliche Infobox‑Attribute** – gleiche Genres, Künstler, etc. (bei Songs).

Je höher die Methode in der Liste, desto ähnlicher sind die Entitäten, was die Unterscheidung erschwert.

---

### Hintergrundwissen

Wenn wir die alternative Frage den Annotator*innen zeigen, kennen diese die Namen der Entitäten, aber nicht unbedingt deren Details. Daher stellen wir zusätzlich Hintergrundinformationen bereit:

- **Musik**: Zeigen wir einen Google‑Suchlink zu jedem Song und fordern die Annotator*innen auf, mindestens ein Stück zu hören und die Song‑Informationen zu lesen.
- **Rezepte &amp; Bücher**: Zeigen wir Textauszüge aus Wikipedia. Für Rezepte zeigen wir zusätzlich Bilder von Wikipedia, damit die Annotator*innen die Gerichte visuell erkennen können.

Danach werden die Annotator*innen gebeten, sich für eine der beiden Entitäten zu entscheiden und diese mithilfe von drei bis fünf indirekten Referenzen zu beschreiben, z. B. „der mit dem Klavierklang“, „nicht der mit dem 12‑jährigen Jungen“, „der fiktive“, „der aus Aserbaidschan kommt“, usw.

---

### Statistiken

| Domäne | Alternative Fragen | Indirekte Referenzen |
|--------|--------------------|----------------------|
| Musik | 2 000 | 14 000 |
| Bücher | 2 000 | 14 000 |
| Rezepte | 2 000 | 14 000 |
| **Gesamt** | **6 000** | **42 000** |

---

### Modellauswertung (T5‑XL)

| Hintergrundwissen | Genauigkeit |
|--------------------|-------------|
| Genaues, identisches Wissen (wie die Annotator*innen) | 92 – 95 % |
| Teilweise überlappendes Wissen (z. B. Retrieval‑Support) | 82 – 87 % |
| Nur Entitätsnamen | ~60 % |

Das zeigt, dass ein realistischer Ansatz, bei dem das Modell Hintergrundwissen über Retrieval‑Mechanismen erhält, deutlich bessere Ergebnisse liefert als reine Namens‑Lookup‑Modelle. Zudem haben wir gezeigt, dass die Modelle domänenübergreifend generalisieren können.

---

### Zugriff

Der AltEntities‑Corpus ist öffentlich verfügbar unter:  
[Link zum Datensatz](https://...).

Vielen Dank für Ihre Aufmerksamkeit!</sample>
    <sample id="143">The approach is compared with the two most common SimulST strategies that are also applied to offline models—**Wait‑k** and **Local Agreement**—as well as with a state‑of‑the‑art architecture that is specifically designed for simultaneous pre‑translation.</sample>
    <sample id="144">Die Autoren sind an der **Universität Nantes (Université de Nantes) in Frankreich** tätig, wie aus den Datenquellen des „Nantes University Hospital“ ersichtlich ist.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">**Abstract**

Dialogsummarisation ist ein Subtask der Textzusammenfassung, bei dem aus einem Gespräch die wichtigsten Fakten kompakt wiedergegeben werden. Trotz großer Fortschritte durch große vortrainierte Sprachmodelle bleiben generierte Zusammenfassungen häufig unvollständig – ein Phänomen, das wir als *Omission* bezeichnen. In unserer Analyse erreichen selbst die besten Modelle einen Omission‑Rate von ca. 70 % über fünf Domänen und sechs Modelle. Die fehlenden Informationen liegen dabei gleichmäßig im Gespräch verteilt, was die Unstrukturierung der Dialoge und die Schwierigkeit der Schlüssel‑Informationsidentifikation verdeutlicht.

Um das Problem systematisch zu untersuchen, stellen wir die **OLDS‑Dataset** vor. Dieses besteht aus fünf bestehenden Benchmarks, zu denen wir über verschiedene Modelle und Decodierungsstrategien 10 abstrakte Kandidaten pro Dialog generieren und automatisch Omission‑Labels liefern. Eine menschliche Qualitätskontrolle bestätigt die Zuverlässigkeit der Labels.

Wir evaluieren drei Baseline‑Ansätze – Paired Classification, Sequence Labeling und Pointer Network – mit Precision, Recall, F1 und einem Wort‑Level‑Recall (WR). Das Beste F1 erreicht nur ~50 %, was die Härte der Aufgabe unterstreicht.  

Schließlich demonstrieren wir, dass die erkannten Omissionen als zusätzliche Input‑Informationen bei einer Post‑Editing‑Methode die Qualität der Zusammenfassung signifikant steigern. Diese Ergebnisse zeigen, dass Omission‑Detection ein wertvoller Schritt zur Qualitätsverbesserung in der Dialogsummarisation ist.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trento und der Fondazione Bruno Kessler und möchte Ihnen kurz das Paper „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist.

**Was ist simultane Sprachübersetzung?**  
Simultane Sprachübersetzung, kurz SimulST, bezeichnet den Prozess, bei dem gesprochene Sprache in Echtzeit in einen Text einer anderen Sprache übersetzt wird, um die grenzüberschreitende Kommunikation zu ermöglichen.

**Welche Probleme haben die aktuellen SimulST‑Modelle?**  
- Spezifische Architekturen werden meist separat trainiert, was zusätzliche Module zur Optimierung mit sich bringt.  
- Die Trainingsverfahren sind lang und komplex, oft mit unterschiedlichen Optimierungszielen.  
- Für unterschiedliche Latenzregime werden mehrere Modelle trainiert und gepflegt – z. B. ein Modell mit einer durchschnittlichen Latenz von einer Sekunde, ein anderes mit zwei Sekunden usw.

**Was ist unsere Lösung?**  
1. Wir nutzen bereits vorhandene Offline‑ST‑Modelle, ohne sie neu zu trainieren oder spezifische Architekturen für SimulST zu entwickeln.  
2. Für jedes Latenzregime setzen wir ein einziges Modell ein und steuern die Latenz über spezifische Parameter.  
3. Wir nutzen das bereits durch die Aufmerksamkeit („attention“) zwischen Audioeingang und Textausgang erworbene Wissen – das sogenannte Cross‑Attention‑Mechanismus (siehe Beispiel rechts).

Unsere Methode, EDAtt (Encoder–Decoder Attention), entscheidet, ob ein Teil der Übersetzung ausgegeben wird oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt.  
Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht konzentriert ist, d. h. wenn die Summe der Cross‑Attention‑Gewichte unter einem bestimmten Schwellenwert α für die letzten λ Sprachframes liegt – das bedeutet, die empfangenen Informationen sind ausreichend stabil.

Beispiel:  
- Wir erhalten einen Sprachchunk mit „I’m going to talk about…“.  
- Unser Modell erzeugt die Übersetzung ins Deutsche und wir betrachten die Cross‑Attention‑Gewichte.  
- Die ersten beiden Wörter zeigen auf die frühesten erhaltenen Sprachframes, während das letzte Wort auf die letzten λ Sprachframes zeigt.  
- Da die Summe der Cross‑Attention für die ersten beiden Wörter unter α liegt, werden diese ausgegeben; das letzte Wort wird nicht ausgegeben, weil die Summe über α liegt, und wir warten auf einen weiteren Sprachchunk.  
- Sobald ein weiterer Chunk eintrifft und das Modell drei weitere Wörter erzeugt, zeigen deren Cross‑Attention‑Gewichte nicht mehr auf die letzten λ Frames – also werden diese drei Wörter ausgegeben.

**Ergebnisse von EDAtt**  
Wir visualisieren die SimulST‑Ergebnisse in Diagrammen, in denen BLEU (Übersetzungsqualität) und Average Lagging (Latenz) aufgetragen sind. Zusätzlich berücksichtigen wir das computational‑aware Average Lagging, das die Rechenzeit des Modells zur Vorhersage einbezieht.  
Wir streben an, dass die Kurven so hoch wie möglich sind und sich nach links verschieben.  
Wir vergleichen EDAtt mit populären Strategien, die auch auf Offline‑Modellen angewendet werden, nämlich Wait‑k und Local Agreement. Außerdem vergleichen wir uns mit der aktuellen, speziell für simultane Pre‑Translation entwickelten Architektur.

**Fazit**  
Die Ergebnisse für die simultane Sprachübersetzung ins Deutsche zeigen, dass EDAtt alle Strategien, die auf Offline‑Modellen basieren, übertrifft – die Kurven sind nach links verschoben.  
Wenn man die tatsächlich verstrichene Zeit oder die computational‑aware Zeit betrachtet, ist EDAtt die schnellste Strategie.  

Weitere Ergebnisse finden Sie in unserem Paper. Wir haben den Code, die Modelle und die simultane Ausgabe Open‑Source freigegeben, um die Reproduzierbarkeit unserer Arbeit zu erleichtern.

Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja – der CoNLL++‑Datensatz wird im Rahmen der Veröffentlichung geteilt und steht der Community zur Verfügung.</sample>
    <sample id="150">**Abstract**

MeetingQA ist ein neues, quelloffenes Dataset für extraktive Frage‑Antwort‑Aufgaben in Meeting‑Transkripten, das 7 770 Fragen aus 100 Stunden des AMI‑Korpus enthält. 30 % der Fragen sind unbeantwortbar; von den beantwortbaren Fragen sind 40 % mehrteilige Antwortspannen und 48 % mehrsprachige Antworten. Die meisten Fragen sind Ja/Nein‑ oder Meinungs­fragestellungen, 20 % sind rhetorisch formuliert und 70 % der mehrsprachigen Antworten enthalten Unein‑ oder Meinungsverschiedenheiten. Fragen und Antworten haben durchschnittlich 12 bzw. 35 Wörter. Menschliche Leistung liegt bei F1 = 84,6 %. 

Zur Modellierung wurden kurze‑Kontext‑Modelle (z. B. RoBERTa) mit Kontext‑Retrieval, ein‑Spann‑Modelle (Start‑End‑Position) sowie Mehr‑Spann‑Modelle (Token‑Classification) entwickelt. Fine‑tuned Modelle erzielen ein F1‑Gap von 25 Punkten zu den menschlichen Benchmarks, wobei kurze‑Kontext‑Modelle leicht überlegen sind. Im Zero‑Shot‑Modus liegt das Gap bei 50 Punkten; die Verwendung von silbernen Annotationen aus MediaSum verbessert die Performance signifikant. Fehleranalysen zeigen, dass Modelle rhetorische Fragen und Sprecherzugehörigkeit besonders schwerer erkennen. Insgesamt bleibt MeetingQA eine anspruchsvolle Herausforderung für aktuelle QA‑Modelle in beiden Settings.</sample>
    <sample id="151">Hallo zusammen, mein Name ist Ying und meine Kollegin Zhiyang und ich werden unser Forschungsprojekt „MultiInstruct – Verbesserung der multimodalen Zero‑Shot‑Lernen durch Instruction‑Tuning“ vorstellen. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen erforscht, bei denen vortrainierte Sprachmodelle für verschiedene Down‑stream‑Aufgaben mit wenigen Parametern und Daten effizient wiederverwendet werden können. Kürzlich haben zahlreiche Studien gezeigt, dass Instruction‑Tuning große Sprachmodelle in die Lage versetzt, unbekannte Aufgaben in einem Zero‑Shot‑Modus zu lösen, indem sie natürliche Anweisungen befolgen. Allerdings konzentrierten sich die meisten bisherigen Arbeiten auf die Verbesserung der Zero‑Shot‑Leistung bei sprachbasierten Aufgaben, während Computer‑Vision‑ und multimodale Aufgaben weitgehend außen vor blieben. Daher wollen wir in dieser Arbeit untersuchen, ob das Instruction‑Tuning von multimodalen, vortrainierten Modellen die Generalisierung auf unbekannte multimodale Aufgaben tatsächlich verbessern kann.

Darüber hinaus haben wir während unserer Forschung festgestellt, dass es einen erheblichen Unterschied in der Verfügbarkeit von Instruction‑Datensätzen zwischen NLP und multimodalen Aufgaben gibt. Es existieren mehr als 1600 sprachbasierte Instruction‑Aufgaben, jedoch gibt es keinen groß angelegten, öffentlich zugänglichen multimodalen Instruction‑Task‑Datensatz. Dies motiviert uns, einen solchen Datensatz zu erstellen. Hier stellen wir MultiInstruct vor – den ersten multimodalen Instruction‑Tuning‑Benchmark‑Datensatz, der 62 vielfältige multimodale Aufgaben aus 10 breiten Kategorien umfasst. Diese Aufgaben stammen aus 21 bestehenden Open‑Source‑Datensätzen, und jede Aufgabe ist mit fünf von Experten verfassten Anweisungen ausgestattet.

Zur Untersuchung des multimodalen Instruction‑Tuning haben wir OFA, ein einheitliches multimodales vortrainiertes Modell, als Basismodell gewählt. OFA verwendet ein einheitliches Vokabular für Sprache, Bild‑Token und die Koordinaten eines Bounding Boxes. Wir zeigen einige Beispielinstanzen aus unserem MultiInstruct‑Datensatz, um die Verarbeitung verschiedener Eingabe‑ und Ausgabe‑Datentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequence‑to‑Sequence‑Format, in dem Eingabetext, Bilder, Anweisungen und Bounding‑Boxen im selben Token‑Raum dargestellt werden.

Nun spreche ich über multimodales Instruction‑Tuning. Für den Trainingsdatensatz nutzen wir 53 Aufgaben aus 9 Gruppen und ziehen pro Aufgabe 10 000 Instanzen. Für die Tests reservieren wir die gesamte Gruppe „Common Sense Reasoning“ und wählen zusätzlich 5 Aufgaben aus den Gruppen „VQ“ und „Miscellaneous“. Wir verwenden alle Instanzen im Test‑Split für jede Aufgabe. Zusätzlich ziehen wir zufällig 20 Aufgaben aus dem Test‑Split der „Natural Instructions“ als unerwartete Aufgabe für NLP. Wir nutzen das vortrainierte OFA‑Large‑Modell als Basismodell. Während des Trainings mischen wir alle Instanzen aller Aufgaben. Jede Instanz wird zufällig mit einer der fünf Anweisungsvorlagen kombiniert. Für jeden Test‑Task führen wir insgesamt 5 Experimente durch, indem wir das Modell mit jeweils einer der fünf Anweisungen evaluieren. In jedem Experiment berichten wir die minimalen und maximalen Leistungen sowie die Standardabweichung über die 5 Experimente. Für multimodale Klassifikationsaufgaben berichten wir die Genauigkeit. Für multimodale Generierungsaufgaben und für NLP‑Aufgaben berichten wir ROUGE‑L. Zusätzlich führen wir eine weitere Metrik namens „Sensitivity“ ein, die die Fähigkeit des Modells misst, bei leicht unterschiedlichen Formulierungen der Anweisung konsistente Ausgaben zu erzeugen.

Hier sind unsere wichtigsten Ergebnisse. Es zeigt sich, dass Instruction‑Tuning die Leistung von OFA bei gesehenen multimodalen Aufgaben signifikant verbessert. Außerdem profitieren Transfer‑Learning‑Ansätze aus natürlichen Instruction‑Datensätzen. Wir sehen, dass mit zunehmender Anzahl an Aufgaben das Modell bessere Leistungen erzielt und gleichzeitig die Sensitivität sinkt. Wir haben zusätzlich ein Experiment mit einer vs. fünf Anweisungen durchgeführt. Es wird deutlich, dass mehr Anweisungen die Gesamtleistung verbessern und die Sensitivität stark reduzieren. Das verdeutlicht die Wirkung verschiedener Fein‑Tuning‑Strategien auf die Modell‑Sensitivität. Durch Transfer‑Learning aus natürlichen Instruction‑Datensätzen erreicht das Modell wesentlich bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell. Ebenso hilft Transfer‑Learning aus natürlichen Instruction‑Datensätzen OFA, bei der Natural‑Instruction‑Menge deutlich bessere Leistungen zu erzielen. Zusammenfassend stellen wir den ersten groß angelegten multimodalen Instruction‑Tuning‑Datensatz vor, der die Kurzzeitfähigkeit von OFA erheblich verbessert, und untersuchen verschiedene Transfer‑Learning‑Techniken und deren Nutzen. Wir haben zudem eine neue Metrik namens Sensitivität eingeführt. Wir sammeln derzeit einen noch größeren multimodalen Instruction‑Tuning‑Datensatz mit rund 150 zusätzlichen Vision‑Language‑Aufgaben und werden ihn veröffentlichen. Hier ist ein QR‑Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">**Abstract**

Wir stellen neue, von Grund auf trainierte Sprachmodelle für die klassische Philologie vor, die sowohl monolingual (Ancient Greek) als auch multilingual (Greek, Latin, Englisch) arbeiten. Für die monolingualen Modelle wurden ein RoBERTa‑Basiertes (GreBERTa) und ein encoder‑decoder‑Basiertes T5‑Modell (GreTa) entwickelt. Die multilingualen Varianten PhilBERTa und PhilTa kombinieren die gleichen Architekturen mit Daten aus dem Internet Archive, dem Corpus Corporum und englischen antiquity‑Texten. Der Pre‑Training‑Korpus für Griechisch wurde durch gezielte OCR‑Erkennung von Stop‑Wörtern und anschließende erneute OCR‑Läufe auf den Internet‑Archive‑Büchern erstellt, wodurch ein qualitativ hochwertiger Datensatz entstand.

Die Modelle wurden anhand von POS‑Tagging, Abhängigkeitsparsing und Lemmatisierung auf den Universal‑Dependencies‑Treebanks (Griechisch) und EvaLatina‑2022 (Latein) benchmarked. Alle Modelle übertreffen den bisherigen State‑of‑the‑Art. Besonders das encoder‑decoder‑Modell erzielt bei Lemmatisierung einen Plus‑5‑Punkt‑Zuwachs. Untersuchungen zur Funktionsweise des T5‑Encoders zeigen, dass dieser nach wenigen Epochen schlechter als ein zufällig initialisiertes Modell abschneidet, sich aber mit weiterem Training annähert. Semantische und Welt‑Kenntnistests (Synonyme, Antonyme, Held‑Gott‑Relationen) demonstrieren signifikante Verbesserungen; die Mehrsprachigkeit trägt jedoch nicht wesentlich zur zusätzlichen Leistung bei. Insgesamt bieten die vorgestellten Modelle ein leistungsfähiges Werkzeug für die Verarbeitung und Analyse antiker Texte.</sample>
    <sample id="153">**Abstract**

In diesem Beitrag adressieren wir Ambiguitäten in text‑zu‑Bild‑Modellen, die zu fehlgeleiteten Bildausgaben führen. Wir stellen ein Benchmark‑Set vor, das auf der LAVA‑Korpus basiert und verschiedene Ambiguitätstypen (z. B. Mehrdeutigkeit von Adjektiven, Präpositionen, Verben) abbildet. Zur Auflösung generiert ein Sprachmodell mittels In‑Context‑Learning zunächst klärende Fragen; die Nutzerantwort wird an den ursprünglichen Prompt angehängt, um einen disambiguierten Prompt zu formen. Alternativ erzeugt das Modell mögliche visuelle Interpretationen, mit denen der Nutzer interagiert, um seine Absicht zu verdeutlichen. Anschließend generieren wir mit einem Text‑zu‑Bild‑Modell Bilder zu beiden Prompts. Für die Bewertung nutzen wir ein VQA‑System: die Bild‑/Prompt‑Kombination zusammen mit einer Frage, die die Nutzerabsicht ausdrückt, wird analysiert; ein „Ja“-Antwort signalisiert, dass die Intention erfüllt ist. Unsere Experimente zeigen, dass die Disambiguation die Bildtreue signifikant verbessert, dass die Wirksamkeit je nach Ambiguitätstyp variiert und dass die automatische Bewertung mit menschlicher Einschätzung übereinstimmt. Damit bieten wir ein vollständiges Framework, um Ambiguitäten zu erkennen, zu beheben und deren Auswirkung auf die Bildqualität systematisch zu evaluieren.</sample>
    <sample id="154">Die Autoren sind an der **University of Trento** sowie der **Fondazione Bruno Kessler** tätig.</sample>
    <sample id="155">Der Referent ist die Entität, auf die sich die indirekte Ausdrucksweise bezieht – in Ihrem Beispiel wäre das eines der beiden Songs, also z. B. „Easy on Me“ oder „I Gotta Feeling“.</sample>
    <sample id="157">**Abstract**  
Dialogue summarization seeks to distill key information from multi‑speaker conversations into concise summaries. Existing approaches rely on pre‑computed static graphs derived from external linguistic tools (e.g., discourse parsing, dialogue state tracking), which are error‑prone and inflexible for downstream summarization. We propose SDDS (Static‑Dynamic Structure Fusion Graph), a novel framework that integrates static dialogue structure with a learnable dynamic graph. An utterance encoder first generates deep representations of each utterance. Four heuristic methods construct static graphs: (1) discourse parsing, (2) keyword co‑occurrence, (3) speaker interaction via sliding‑window frequency, and (4) utterance position graph. Adjacent matrices from these channels are fused by a 1×1 convolution. A dynamic graph is then built solely from utterance embeddings using multi‑head self‑attention, capturing semantic relations without external tools. Static and dynamic adjacency matrices are combined into a unified graph, which is fed into a graph‑attention layer atop the transformer’s self‑attention, forming a dual cross‑attention mechanism that guides the pre‑trained language model to generate the summary. This architecture allows the model to adaptively learn dialogue structure while mitigating propagation of external‑tool errors. Code and data are publicly released on GitHub. The approach demonstrates superior flexibility and robustness in dialogue summarization tasks.</sample>
    <sample id="158">**Abstract**

Die vorgestellte Arbeit „Dual Cache for Long Document Neural Coreference Resolution“ adressiert die Herausforderung, Coreferenz‑Entitäten in langen Texten effizient zu verfolgen. Traditionelle Methoden prüfen alle möglichen Mentions‑Paare, was quadratische Kosten verursacht. Cache‑basierte Ansätze reduzieren die Komplexität auf linear, nutzen jedoch einen einzelnen Cache mit LRU‑Ereignis. In langen Dokumenten führt das häufige Themenwechseln zu hohem Cache‑Miss‑Raten, insbesondere bei hochfrequenten Entitäten. Daher wird ein Dual‑Cache‑System eingeführt, das einen lokalen Cache (LRU) für kurzlebige, einen globalen Cache (LFU) für häufig auftretende Entitäten nutzt. Beim Scannen des Textes entscheidet das Modell, ob ein Merkmal neu oder bereits im Cache ist, aktualisiert die Häufigkeit und entscheidet anhand eines Schwellenwerts, welchem Cache die Entität zugeordnet wird. Evaluation an LitBank, OntoNotes und WikiCoref zeigt, dass Dual‑Cache die Leistung gegenüber Einzel‑Cache‑Methoden übertrifft, auch bei begrenztem Speicher. Bei einer 30.000‑Wort‑Buchprobe reduziert der Dual‑Cache den Miss‑Raten drastisch und liefert die beste Performance‑zu‑Kosten‑Verhältnis.</sample>
    <sample id="159">Hallo zusammen. Ich bin Koustav Sinha und freue mich, euch zu unserem Vortrag über unser ACL‑2023‑Paper zu begrüßen.  
**Sprachmodelle geben nicht immer robuste Akzeptabilitätsurteile in Bezug auf den Kontext.** Dieses Paper ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.

Wir haben uns das Minimal‑Pair‑Paradigma (MPP) erneut angeschaut. Dieses Paradigma bewertet Sprachmodelle anhand von Akzeptabilitätsurteilen – zum Beispiel Grammatik (BLiMP, SyntaxGym) oder auch Akzeptabilität in Bezug auf Stereotype (CrowS‑Pairs). Im klassischen MPP‑Ansatz wird dem Modell ein akzeptabler bzw. ein unakzeptabler Satz gezeigt; man hofft, dass das Modell dem akzeptablen Satz eine höhere Wahrscheinlichkeit zuordnet.

Die aktuelle MPP‑Pipeline erlaubt es nicht, die Akzeptabilität bei längeren Sätzen zu testen. Heutzutage haben große Sprachmodelle immer größere Kontexts­fenster, daher ist es entscheidend, die Akzeptabilität entlang des gesamten Kontexts zu bewerten – und genau das versuchen wir hier zu tun. Wir haben den MPP‑Ansatz neu gestaltet, indem wir das Modell auffordern, die Akzeptabilität über längere Sequenzen hinweg zu bewerten.

**Wie wir das machen:**

Um längere Sequenzen zu simulieren, haben wir die Datensätze selbst überarbeitet und Sätze neu erstellt, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen ausgewählt haben.  
Beispielsweise haben wir ein typisches Grammatik‑Minimal‑Pair aus BLiMP zur Adjunct‑Island‑Falle genommen. Wir extrahieren grammatikalisch korrekte Sätze aus Adjunct Island und fügen sie als Präfix sowohl zum akzeptablen als auch zum unakzeptablen Query‑Satz hinzu. Dasselbe kann mit unakzeptablen Sätzen aus demselben Datensatz gemacht werden. Außerdem wählen wir Sätze aus einem anderen Subset oder sogar aus einem völlig anderen Datensatz – das nennen wir den **Mismatch‑Szenario**. In diesem Fall stammen die Sätze zwar noch aus relevanten Datensätzen, aber nicht aus demjenigen, mit dem wir eigentlich bewerten. Für den Unakzeptabilitäts‑Fall können wir sogar Sätze aus einem komplett unzusammenhängenden Bereich wie Wikipedia wählen. Dadurch können wir prüfen, ob die Akzeptabilitätsurteile des Modells tatsächlich von jeglichem Kontext beeinflusst werden – egal, ob der Kontext aus demselben Subset stammt oder völlig irrelevant ist.

**Was passiert mit den Modellen?**

Zunächst betrachten wir Wikipedia‑Sätze, die völlig irrelevant zum aktuellen Minimal‑Pair sind. Wir sehen, dass die MPP‑Urteile für beliebige Kontexthöhen weitgehend robust sind. Wir erhöhen die Kontextlänge bis zu 1 024 Tokens, um die Obergrenze von OPT und GPT‑2 zu erreichen, und die MPP‑Urteile bleiben relativ stabil (orange gepunktete Linie).

Was passiert, wenn wir Sätze aus demselben Datensatz wählen? Wir erzeugen Sätze aus akzeptabler und unakzeptabler Domäne im selben BLiMP‑ oder SyntaxGym‑Datensatz. Dann stellen wir fest, dass die MPP‑Urteile entweder deutlich steigen oder fallen, wenn wir akzeptable bzw. unakzeptable Präfixe hinzufügen. Besonders stark ist der Effekt, wenn wir die Präfixe aus genau dem gleichen Phänomen des Datensatzes wählen – in diesem Fall steigt oder sinkt das MPP‑Urteil massiv, abhängig davon, ob das Präfix akzeptabel oder unakzeptabel ist. Dieser Effekt nimmt mit der Kontextlänge zu und dürfte auch bei neueren Modellen mit größeren Kontextfenstern relevant sein.

**Warum beeinflusst das passende Präfix die Urteile so stark?**

Wir haben eine Reihe von Analysen durchgeführt, bei denen wir die Eingabesätze perturbiert haben – wir haben die relevanten Strukturen beibehalten, aber Rauschen hinzugefügt. Nach mehreren Perturbationen stellen wir fest, dass keine dieser Störungen das Urteil des Modells wesentlich verändert. Die Modelle reagieren ähnlich auf die Perturbationen: Wenn wir akzeptable Sätze perturbieren, steigt das MPP‑Urteil in ähnlicher Weise; bei unakzeptablen Sätzen sinkt es. Das zeigt, dass die Modelle sehr empfindlich auf latente syntaktische und semantische Merkmale reagieren, die zwischen den Sätzen geteilt werden. Das aktuelle MPP‑Evaluation, das auf kurzen, einzelnen Sätzen basiert, erfasst diese abstrakten Kenntnisse des Modells im gesamten Kontextfenster nicht vollständig.

**Fazit:**  
Sprachmodelle sind empfindlich gegenüber latenten syntaktischen und semantischen Merkmalen, die über den Kontext hinweg geteilt werden. Die bisherige MPP‑Bewertung mit kurzen, einzelnen Sätzen erfasst möglicherweise nicht die vollständige abstrakte Wissensbasis eines Modells entlang des gesamten Kontextfensters. Weitere Details findet ihr in unserem Papier.  
Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Im ersten Schritt werden die Input‑Token jedem ein **unordered multiset (eine ungeordnete Mengen‑Liste) von Ausgabetoken** zugeordnet.</sample>
    <sample id="161">55 000 Skripte.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist **MASSalign**.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen liegt darin, dass es die Kosten und den Aufwand für die Datenerfassung drastisch senkt: Anstatt jedes Beispiel von Menschen annotieren zu lassen, nutzt man kostengünstige, leicht zugängliche Quellen wie Heuristiken, Wissensbasen oder Low‑Quality‑Crowdsource‑Labels. Dadurch kann man mit viel mehr Daten trainieren, ohne signifikante zusätzliche Annotationskosten zu verursachen.</sample>
    <sample id="165">**Abstrakt (≈170 Wörter)**  

Dieses Papier untersucht das Abductive Commonsense Reasoning, bei dem aus einem Kontext \(X\) und einem Ergebnis \(Y\) plausibelere Erklärungen \(Z\) ermittelt werden sollen. Klassische Ansätze erfordern annotierte Plausibilitäten, die jedoch laut einer Studie von 60 % Diskrepanzen aufweisen. Wir stellen LiPoR (Likelihood Learning with Posterior Regularization) vor, ein vollständig unüberwachtes Verfahren, das die Erklärungen als latente Variablen behandelt. Das Hauptziel maximiert die Randwahrscheinlichkeit \(p(Y|X)\) über alle möglichen Erklärungen, wodurch keine explizite Plausibilitätskennzeichnung nötig ist.  

Um dennoch plausiblen Erklärungen Vorrang einzuräumen, nutzt LiPoR die Eigenschaft, dass Erklärungen sich gegenseitig ausschließen. Der Regularisierer \(\Omega\) minimiert die Entropie von \(p(Z|X,Y)\), solange diese größer als \(\log M\) (Anzahl plausibler Erklärungen) ist, und begrenzt somit die Anzahl der mit Masse versehenen Erklärungen.  

Anhand des AlphaNLI‑Datensatzes zeigt LiPoR eine Verbesserung von über 4 % gegenüber allen Zero‑Shot‑Modellen, einschließlich eines starken GPT‑3‑Baselines, und übertrifft damit die bisher beste unüberwachte Methode.</sample>
    <sample id="166">**Abstract (≈180 Wörter)**  

Wir stellen NDCR – ein Neural Divide‑and‑Conquer Reasoning Framework – vor, das die Herausforderung der Bild‑zur‑Text‑Retrieval‑Aufgabe mit sprachlich komplexen Beschreibungen adressiert. Die Methode kombiniert Dual‑Process‑Theory (Analogie‑System 1 und logisches System 2) mit einer Divide‑and‑Conquer‑Strategie. Zunächst wird ein *Proposition Generator* die lange, komplexe Textproposition in mehrere einfache, BART‑dekodierte Propositionen zerlegt. Ein *Visual‑Linguistic Interactor* (System 1) liefert für jede Proposition ein Bild‑Matching‑Score sowie einen vorläufigen Beweiszustand. Anschließend übernimmt ein *Neural‑Symbolic Reasoner* (System 2) die logische Integration: ein Negation‑Executor wandelt positive Propositionen in deren Negationen um, während ein Conjunction‑Operation die Resultate aller Propositionen zu einer endgültigen Bild‑Entscheidung zusammenführt. Die beiden Systemausgaben werden kombiniert, um das finale Retrieval-Ergebnis zu erzeugen.  

Experimente auf einem speziell konzipierten Datensatz zeigen, dass NDCR bestehende Baselines deutlich übertrifft, und Ablationsstudien bestätigen die Wirksamkeit jedes Moduls. Fallstudien illustrieren die Interoperabilität der Zwischenschritte, indem sie Beweiszustände und Zwischenergebnisse visualisieren. Wir diskutieren die Einbettung neural‑symbolischer Berechnungen in große Sprachmodelle, die Parallelen zum Self‑Asking Chain‑of‑Thought sowie die Vorteile einer Dual‑Process‑Integration für komplexe Reasoning‑Aufgaben.</sample>
    <sample id="167">In DEPLAIN‑web wurden genau 750 Dokumente verwendet.  
Jedes dieser 750 Dokumente wurde **manuell** abgeglichen und zusätzlich **automatisch** (z. B. mit MASSalign) ausgerichtet.  
Damit gibt es für jedes Dokument sowohl eine manuelle als auch eine automatische Satz‑Übereinstimmung.</sample>
    <sample id="168">Der CoNLL++‑Datensatz wurde aus Reuters‑Nachrichten aus dem Jahr 2020 zusammengestellt und dann nach denselben Annotation‑Regeln wie beim CoNLL‑2003‑Korpus (I‑B‑E‑S‑O‑Tags für Entitäten) manuell annotiert.</sample>
    <sample id="169">**Abstract**

In dieser Studie wird die Leistung des 540‑Billionen‑Parameter‑Modells PaLM (2022) als Prompt‑basierte Machine‑Translation (MT)‑Lösung systematisch untersucht. Unter Verwendung von WMT‑Testsets, die keine Übersetzungen aus dem Trainingskorpus enthalten, werden diverse Prompt‑Strategien evaluiert, wobei sowohl automatisierte Metriken (BLEURT, BLEU, COMET) als auch MQM‑basierte menschliche Bewertungen zum Einsatz kommen. Ein Vergleich zwischen Zero‑, One‑ und Five‑Shot‑Prompting zeigt, dass die Prompt‑Form bei Five‑Shot nahezu irrelevant ist, während bei Zero/One‑Shot die Form einen signifikanten Einfluss hat. Ein einfaches Experiment mit zwei unterschiedlichen One‑Shot‑Prompts verzeichnete Unterschiede von &gt; 1 BLEURT‑Punkt, bis zu 40 in Extremfällen. Hierbei erweist sich die Qualität der Beispielsätze als entscheidender Faktor als deren Ähnlichkeit zum Ausgangstext; Beispiele aus einem hochwertigen Dev‑Korpus führen zu besserer Performance als solche aus dem Training. PaLM erreicht mit Five‑Shot‑Prompting nahezu die Qualität eines kommerziellen Systems, bleibt jedoch in der Genauigkeit hinter spezialisierten MT‑Modellen. MQM‑Analyse zeigt vergleichbare Fluency‑Scores, aber höhere Omission‑Fehler; der „Style/Awkward“-Score ist dagegen niedriger als bei State‑of‑the‑Art‑Systemen. Die Ergebnisse liefern konkrete Empfehlungen zur Prompt‑Auswahl und unterstreichen die Bedeutung hochwertiger Beispiele für die Optimierung von LLM‑basierten Übersetzungen.</sample>
    <sample id="170">Hallo zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute möchte ich unsere Arbeit „XSemPLR: Cross‑Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“ vorstellen.

**Semantische Analyse** ist die Aufgabe, semantische Repräsentationen von Benutzerabfragen zu erzeugen – zum Beispiel SQL‑Statements, Lambda‑Kalkül, FunQL usw. **Cross‑Lingual Semantic Parsing** bedeutet, Abfragen in mehreren natürlichen Sprachen in verschiedene semantische Repräsentationen zu übersetzen. Wie in der Abbildung gezeigt, müssen wir die Abfrage in verschiedenen Ausgangssprachen mit neuronalen Modellen in SQL, Lambda‑Kalkül, FunQL oder andere Formate übersetzen.

Bisher wurden aktuelle Cross‑Lingual‑Semantic‑Parsing‑Modelle einzeln entwickelt und auf Datensätzen mit begrenzten Aufgaben und Anwendungen evaluiert. Es gibt beispielsweise viele Arbeiten für bestimmte Sprachen, aber Chinesisch fehlt oft und die Abdeckung bestimmter semantischer Repräsentationen ist unvollständig. Das Lambda‑Kalkül wird häufig nicht berücksichtigt oder die Modelle werden nur auf einer begrenzten Gruppe neuronaler Architekturen getestet – meist mit nur einem einzigen Modell. 

Aus diesem Grund haben wir XSemPLR entwickelt. Wir stellen ein einheitliches Datenset zur Verfügung, das Cross‑Lingual‑Semantic‑Parsing in mehreren natürlichen Sprachen und semantischen Repräsentationen abdeckt. Es beinhaltet:

- 9 Datensätze aus verschiedenen Domänen  
- 5 semantische Analyseaufgaben  
- 8 semantische Repräsentationen  
- 22 natürliche Sprachen aus 15 Sprachfamilien  

Um die Benchmark möglichst umfassend zu evaluieren, berücksichtigen wir sechs Trainings‑ und Evaluierungssettings:

1. **Translate‑Test** – Wir nutzen die Google‑Translate‑API, um die Quellsprache in die Zielsprache zu übersetzen, anschließend trainieren und evaluieren wir ein monolinguales Modell. Beispiel: Wir trainieren ein englisches Modell mit englischen Abfragen und übersetzen während der Inferenz eine deutsche Abfrage auf Englisch, bevor wir das Modell zur SQL‑Vorhersage einsetzen.
2. **Monolinguales Modell** – Die Quell‑ und Zielsprachen sind identisch (z. B. Deutsch zu Deutsch, Englisch zu Englisch).
3. **Monolinguales Few‑Shot** – Hier trainieren wir monolinguale Modelle mit nur 10 % der Trainingsdaten.
4. **Multilinguales Modell** – Wir trainieren ein einzelnes multilinguales Modell, das alle Sprachen gleichzeitig nutzt (z. B. Deutsch, Englisch, Chinesisch). Während der Inferenz kann dieses Modell dann Abfragen in jeder dieser Sprachen verarbeiten.
5. **Cross‑Lingual Zero‑Shot Transfer** – Wir trainieren auf einer Ausgangssprache und übertragen die Fähigkeit auf eine andere Sprache ohne zusätzliche Trainingsdaten.
6. **Cross‑Lingual Few‑Shot Transfer** – Wir trainieren auf einer Ausgangssprache und ergänzen mit wenigen (z. B. 10 %) Beispielen aus der Ziel‑Sprachgruppe, um den Transfer zu verbessern.

**Ergebnisse und Analyse**

Wir haben zwei Hauptmodelkategorien untersucht:

- **Encoder‑PTR**: Multilingual vortrainierte Encoder mit pointer‑basierten Decodern (z. B. XLM‑R + PTR, mBERT + PTR).
- **Encoder‑Decoder**: Multilingual vortrainierte Encoder‑Decoder‑Modelle (z. B. mBART, mT5).

**Hauptbefunde**

- Encoder‑Decoder‑Modelle erzielen die beste Leistung auf allen neun Datensätzen.
- In der multilingualen Einstellung profitieren Encoder‑Decoder und Encoder‑PTR von der Mischung mehrerer Sprachen. Die meisten Hauptsprachen erfahren eine Leistungssteigerung; bei Englisch sinkt die Performance in sieben Datensätzen, verbessert sich aber in drei.
- Dieses Phänomen wird oft als „Curse of Multilinguality“ bezeichnet.
- Der Performance‑Gap zwischen Cross‑Lingual Zero‑Shot und Monolingualen Modellen ist signifikant, während der Gap bei Cross‑Lingual Few‑Shot sich deutlich verkürzt.
- Encoder‑Decoder übertrifft frühere Arbeiten oder erreicht vergleichbare Ergebnisse.
- Eine Sprachvortrainierung auf Englisch kann die Leistung bei Few‑Shot‑Tasks in Zielsprache deutlich steigern.
- Multilinguale Modelle wie Codex oder BLOOM erweisen sich noch immer als unzureichend für Cross‑Lingual‑Semantic‑Parsing‑Aufgaben.

**Fazit**

Wir haben XSemPLR als einheitlichen Benchmark für Cross‑Lingual‑Semantic‑Parsing mit mehreren natürlichen Sprachen und semantischen Repräsentationen geschaffen. Wir führen eine umfassende Benchmark‑Studie über drei repräsentative Arten multilingueller Sprachmodelle durch und zeigen dabei zahlreiche interessante Erkenntnisse. 

Besucht gerne unser Paper und den Code. Vielen Dank fürs Zuhören!</sample>
    <sample id="171">**Briefly, prior research has explored:**

1. **Watermarking for LLMs** – methods that embed hidden signals into model outputs or parameters to detect unauthorized use.  
2. **Back‑door based watermarks** – inserting trigger inputs that produce distinctive outputs, enabling later verification.  
3. **Model‑extraction detection** – techniques that test whether a stolen model reproduces the watermark behavior.  
4. **Transferability studies** – investigations into how well a watermark survives model fine‑tuning or re‑implementation.  

These works collectively address copyright protection, but, as noted, most either do not target embedding‑as‑a‑service APIs or lack strong transferability when a model is extracted.</sample>
    <sample id="172">Nein – laut den Ergebnissen in der Studie sind mehrsprachige LLMs wie Codex oder BLOOM für Cross‑Lingual Semantic Parsing (CLSP) noch nicht ausreichend. Sie zeigen im Vergleich zu spezialisierten Encoder‑Decoder‑Modellen (z. B. mT5, mBART) deutlich schlechtere Leistungen.</sample>
    <sample id="174">**Abstract**

Das Video stellt die *ArgAnalysis35K*-Datenbank vor, die 35 000 Argument‑Analyse‑Paare umfasst und damit die bislang größte Sammlung im Bereich Argumentqualität darstellt. Im Gegensatz zu bestehenden Datensätzen, die meist über Crowdsourcing erhoben, stammen 85 % der Argumente aus hochqualitativen Quellen wie Debattierturnieren und erfahrenen Debattierern, während die übrigen 15 % von Anfängern stammen. Die Sammlung greift 24 vielfältige Themen auf, die auf Debattiererfahrungen, Online‑Motivationslisten und Expertenrat basieren, und umfasst zahlreiche Motive pro Thema, um die Motionsvielfalt zu erhöhen. Ein zentrales Merkmal ist die Einführung des Begriffs **Analyse**, der sowohl Behauptungen als auch Prämissen zusammenfasst und damit die Erklärung eines Arguments vertieft. Darüber hinaus wird ein **instance‑basiertes Annotator‑Reliability‑Modell** eingesetzt, das annotator‑spezifische Biases auf einzelne Argumente beschränkt, anstatt annotatoren global auszuschließen. Schließlich bietet ein **Relevanz‑Modell** Score‑Werte (0–1) für die Passgenauigkeit eines Arguments zu jedem Thema. Diese Eigenschaften erhöhen die Argumentqualität, Diversität und die Zuverlässigkeit der Bewertungen.</sample>
    <sample id="175">Die Methode betrachtet mehrere mögliche Permutationen als latente Größen. Während des Trainings wird die Zuordnung der Multisets zu den Ausgabetoken gleichzeitig gelernt (Alignment‑Induktion). Anschließend wird die NP‑harte Permutationsauswahl durch eine GPU‑freundliche, kontinuierliche Relaxierung approximiert, die gleichzeitig durch das Modell zurückpropagiert werden kann. So kann das Modell die sprachlich plausibelste Permutation auswählen, ohne harte Einschränkungen aufzuerlegen.</sample>
    <sample id="176">Fairness eines nachgeschalteten NLP‑Modells wird hier als **gleiche bzw. ausgewogene Leistung über verschiedene Gruppen** definiert.  
Das bedeutet, dass das Modell bei Aufgaben wie Hass‑Sprach‑ oder Fake‑News‑Erkennung **unterschiedlich performt** – nicht aufgrund der politischen Ausrichtung der Quelle, der sozialen Gruppe des Zielpublikums oder anderer demografischer Merkmale.  

Kurz gesagt: ein fairer Nachschalt‑NLP‑Modell soll keine systematischen Leistungsunterschiede zwischen
– politischen Leanings  
– demografischen Kategorien (z. B. Minderheiten vs. Mehrheitsgruppen)  
– anderen relevanten Gruppen aufweisen.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Der Referent heißt **Koustav Sinha**.</sample>
    <sample id="179">**Abstract**  
Large Language Models (LLMs) perform poorly on Theory‑of‑Mind (ToM) tasks that require reasoning about characters’ mental states, especially on false‑belief questions. In diesem Beitrag wird SymbolicToM vorgestellt, ein inference‑time Ansatz, der LLM‑Antworten durch explizite graphische Repräsentationen von Glaubenszuständen verbessert. Für jede Kombination von Charakteren \(p_{1}\dots p_{m}\) werden bis zu einer vorgegebenen ToM‑Ebene Graphen berechnet (z. B. BBob, BBob,Alice). Die Graphen entstehen aus NLI‑ und OpenIE‑Modellen und ermöglichen die Rekursion über Fragen, sodass faktische Fragen direkt auf dem Graphen beantwortet werden können. 

Die Methode wird auf mehreren LLMs (GPT‑3, Macaw, Flan‑T5‑XXL, GPT‑4) evaluiert und übertrifft fein‑tuned Baselines im In‑Domain‑Dataset ToMi. Für zwei neu konzipierte Out‑of‑Domain‑Datensätze (D₁‑D₃) und das linguistisch diversifizierte ParaphrasedToMi zeigen supervised Modelle starke Leistungsabfälle, während SymbolicToM signifikante Accuracy‑Gewinne erzielt (bis zu 67 Punkte bei GPT‑3‑Davinci, 42 Punkte bei GPT‑4 auf D₁).  

Zusammenfassend liefert SymbolicToM eine plug‑and‑play, interpretable Lösung, die die ToM‑Fähigkeiten von LLMs ohne Überanpassung deutlich steigert.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">**Abstract**

In diesem Beitrag stellen wir das Problem des *constrained language planning* vor, bei dem ein abstraktes Ziel wie „Einen Kuchen backen“ mit konkreten, mehrdimensionalen Restriktionen (z. B. „Schokolade verwenden“, „Glutenfrei“ oder „In 30 Minuten fertig“) versehen wird. Wir evaluieren die Fähigkeit großer Sprachmodelle (InstructGPT, GPT‑4) zur Erstellung von Schritt‑für‑Schritt‑Skripten für solche spezifischen Ziele. Die Ergebnisse zeigen, dass Modelle zwar semantisch vollständig sind, die Einhaltung der Constraints jedoch stark variieren und häufig unzureichend ist. 

Zur Verbesserung führen wir eine *over‑generate‑then‑filter*‑Strategie ein: zunächst werden K Skripte generiert; anschließend filtert ein Modell anhand von InstructGPT‑Einbettungen, semantischer Ähnlichkeit und Schlüsselwort‑Belohnungen das Faithful‑Skript aus. Diese Methode steigert sowohl die semantische Vollständigkeit als auch die Faithfulness signifikant.  

Da das Training großer Modelle teuer ist, distillieren wir mithilfe dieser Technik einen umfangreichen Datensatz namens **CoScript** (55 000 spezifische Ziele + Skripte). Crowd‑source‑Überprüfungen stellen die Qualität sicher. Auf CoScript finetunierte T5‑Modelle übertreffen in vielen Fällen große Modelle, was die Wirksamkeit kleiner, spezialisierter Modelle unterstreicht.  

Wir stellen CoScript als Ressource für zukünftige Arbeiten zum constrained language planning vor.</sample>
    <sample id="182">**Tropikalismus** (im Sinne des Papers) bezeichnet die stereotype Darstellung von Menschen – insbesondere von lateinamerikanischen oder „lateinischen“ Frauen – als „tropisch“, also exotisch, sinnlich und lebhaft. In den generierten Personas werden solche Begriffe („vibrant“, „curvaceous“, etc.) verwendet, um die Identität der Gruppe auf diese Weise zu charakterisieren und dadurch eine essentialisierende, exotisierende Narrative zu erzeugen.</sample>
    <sample id="183">Die Autoren ließen Menschen dieselben Persona‑Prompts wie für die LLM‑Generierung beantworten – also z. B. „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich.“ – und sammelten die dadurch entstandenen menschlichen Beschreibungen.</sample>
    <sample id="184">Sie nutzten die **CXMI‑Messung** (Context‑dependent Cross‑Entropy Mutual Information) – erweitert auf die **Pointwise‑CXMI (P‑CXMI)** – um die Kontext­nutzung bei der Übersetzung zu quantifizieren.</sample>
    <sample id="185">**DrBERT vs. ChuBERT**  
- **Domain / Data source**  
  - **DrBERT**: Built from scratch on *NACHOS*, a large set of French medical texts crawled from the web.  
  - **ChuBERT**: Built from scratch on anonymized *clinical notes* from the Nantes University Hospital data warehouse.  

In short, DrBERT is a general biomedical model trained on web‑crawled data, whereas ChuBERT is a clinical model trained on real hospital‑note data.</sample>
    <sample id="187">Zwei Autoren (Ying und Zhiyang).</sample>
    <sample id="188">**Iteratives Transferlernen**  
Beim iterativen Transferlernen wird ein Modell, das bereits von einer verwandten Aufgabe (z. B. Debatte‑Stance‑Classification oder Expansion/Comparison‑Klassen) vortrainiert wurde, in **mehreren Runden** (Runden des Active‑Learning) nach und nach mit neuen, annotierten Beispielen fein‑justiert.  

* Für jede Runde wird ausschließlich das aktuell gesammelte Annotation‑Set verwendet, um das Modell zu aktualisieren, anstatt alle bisher gesammelten Daten erneut zu trainieren (d. h. keine „kumulative“ Wiederholung).  
* Dadurch kann das Modell schrittweise an die spezifische Domäne (hier: dissonante Diskurse) angepasst werden, während die ursprünglichen Transfergewichte erhalten bleiben.  

Kurz gesagt: iterative Transferlernen bedeutet, ein vortrainiertes Modell in wiederholten, schrittweisen Fine‑Tuning‑Runden mit jeweils neuen Daten zu aktualisieren.</sample>
    <sample id="189">Das Ziel des AltEntities‑Korpus ist, indirekte Bezüge zu Entitäten in Auswahl‑Dialogen zu erfassen und damit ein Benchmark für Modelle zu schaffen, die verstehen sollen, welche Entität ein Nutzer anhand von indirekten Hinweisen (z. B. „der neue“, „der nicht energetisch“) meint.</sample>
    <sample id="190">Ein Angreifer kann die Modellparameter eines Embedding‑as‑a‑Service (EaaS) durch systematisches Abfragen des Dienstes extrahieren. Dabei sendet er eine große Menge gezielter Texte – insbesondere solche mit Trigger‑Wörtern –, sammelt die zurückgegebenen Embeddings und nutzt diese Beobachtungen, um das zugrunde liegende Modell zu rekonstruieren (z. B. durch lineare Regression, Gleichungssystem‑Lösung oder Gradient‑Approximation). Durch die gezielte Verwendung von Backdoor‑Triggern kann er zudem spezifische Signale auslösen, die die Parameter noch genauer identifizierbar machen.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">**Abstract (≈180 Wörter)**  

In der vorliegenden Arbeit wird der Optimierer *CAME* (Confidence‑guided Adaptive Memory‑Efficient Optimization) vorgestellt, der die beiden Kernziele der modernen Sprachmodell‑Schulung – schnelle Konvergenz und geringer Speicherbedarf – gleichzeitig erfüllt. Traditionelle adaptive Verfahren wie Adam benötigen für die Speicherung der ersten und zweiten Moment‑Schätzungen dreifach so viel Speicher wie das Modell selbst, während speichereffiziente Varianten wie Adafactor zwar den Speicher drastisch reduzieren, jedoch die Leistung beeinträchtigen. CAME nutzt eine rank‑1 NMF‑Approximation, um die Speicherkomplexität von O(m n) auf O(m + n) zu senken, und adressiert dabei die durch die Approximation entstehenden „fehlerhaften“ Updates. Für jedes Update wird die Diskrepanz zwischen dem vorhergesagten und dem tatsächlich erzeugten Update als Instabilitätsmaß berechnet; dieses Maß dient als adaptiver Nenner bei der Aktualisierung der Momentum‑Schätzung. Experimente auf BookCorpus, Wikipedia und den großen Modellen BERT, GPT‑2 sowie T5 zeigen, dass CAME gegenüber Adafactor eine Verbesserung der Validierungs‑Genauigkeit um ca. 3,4 % erzielt und bei sehr großen Modellen sogar Adam übertrifft, während die Speicherauslastung bei Batch‑Größen von 8 K bis 32 K deutlich reduziert wird. CAME demonstriert damit eine effiziente, speichereffiziente und dennoch leistungsstarke Optimierungsstrategie für große Sprachmodelle.</sample>
    <sample id="193">Die Präsentation nennt keine explizite Zahl der Annotatoren – die genaue Anzahl wird nicht angegeben.</sample>
    <sample id="194">Die an der Arbeit beteiligten Forscher kommen von drei Institutionen:  

- **Carnegie Mellon University** (Jenny, die PhD‑Studentin)  
- **University of Washington** (Sebastian Santy, Ronan Le Bras)  
- **Allen Institute for AI** (Katharina Reinecke, Maarten Sap)</sample>
    <sample id="195">**Abstract**

Explainable Question Answering (XQA) aims to provide not only an answer but also a rationale. Existing neuro‑symbolic methods translate queries into formal queries (e.g., SPARQL) and are limited by the incompleteness of knowledge bases (KBs), whereas decomposition‑based approaches rely on free‑text corpora and struggle with natural language diversity. We propose **RoHT** (Reasoning over Hierarchical Question Decomposition Tree), a two‑stage framework that first induces a *Hierarchical Question Decomposition Tree* (HQDT) for a complex question. The tree’s root is the original query; internal nodes are generated by grouping leaf (atomic) sub‑questions, while a certainty score guides the granularity. In the second stage, RoHT performs probabilistic reasoning over the HQDT: a scheduler selects appropriate knowledge sources (KB, text corpus, or recursive child solving), executors retrieve answer candidates with probabilities, and an aggregator combines them to output the most probable key answers. Experiments on KQA Pro (KB + Wikipedia) and Musique (paragraphs + Wikidata) show that RoHT surpasses state‑of‑the‑art KB‑only and mixed methods, achieving significant gains (e.g., +11.9 F1 on Musique). RoHT demonstrates that explicit hierarchical decomposition and multi‑source integration markedly improve complex QA and its explainability.</sample>
    <sample id="196">**Beispiel mit dem Begrenzer (Governor) auf der linken Seite:**  
„I saw Bart and Lisa.“</sample>
    <sample id="197">Der aktuelle Stand der Technik bei Dialogsystemen besteht aus hochentwickelten Sprach‑Modellen (LLMs), die flüssige, kontextbezogene Gespräche erzeugen können.  In Experimenten mit vier führenden Modellen zeigte sich, dass sie dennoch signifikante Fehler machen – etwa 20 % der Antworten verletzen das commonsense‑Wissen, rund 15 % liefern irrelevante Informationen und ca. 10 % enthalten Selbst‑ oder Partner‑Konzepte.  Trotz dieser Schwächen sind die Modelle die leistungsfähigste verfügbare Technologie für konversationelle KI.</sample>
    <sample id="198">We must test acceptability over the whole context window because modern LLMs can process far longer inputs, and their judgments can be swayed by the preceding context. Evaluating only single‑sentence minimal pairs ignores how acceptability behaves when the model has to consider many preceding tokens, so we need to ensure robustness across the entire (potentially thousands‑token) context.</sample>
    <sample id="199">Ja – bei der meisten Mehrsprachigkeit sank die Leistung des englischen Modells in sieben von neun Datensätzen, wobei nur in drei Datensätzen ein Gewinn zu verzeichnen war (typischer „Curse of Multilinguality“).</sample>
    <sample id="200">Nein. Die Annotatoren sehen zwar die beiden Entitäten (A und B) und erhalten dazu Hintergrundinformationen, aber sie wissen im Voraus nicht, welche der beiden Entitäten letztlich ausgewählt wird – sie bestimmen selbst anhand der indirekten Beschreibungen, welche der beiden gemeint ist.</sample>
    <sample id="201">Die Bewertung stützt sich in erster Linie auf die **BLEURT‑Metrik** (ein fortschrittliches, lernbasiertes Qualitätsmaß). Zusätzlich werden die üblichen, in der WMT‑Community eingesetzten, **neuronalen MT‑Metriken** verwendet – typischerweise BLEU und COMET – um die Übersetzungsqualität umfassend zu erfassen.</sample>
    <sample id="202">Nein – die Studie untersuchte die Generalisierung ausschließlich anhand des Gesamt‑F1‑Scores und hat die Ergebnisse nicht nach einzelnen NER‑Typen (PER, LOC, ORG, MISC) aufgeschlüsselt. Daher lässt sich aus dem vorliegenden Bericht nicht ableiten, ob die Regression bei bestimmten Typen stärker ausgeprägt ist.</sample>
    <sample id="203">Positionalität ist wichtig, weil sie die Perspektiven und Werte der Personen widerspiegelt, die Daten sammeln, annotieren und Modelle entwickeln. Da NLP‑Modelle auf diesen Daten basieren, übernehmen sie unvermeidlich die Vorurteile und Sichtweisen dieser Gruppen. Das führt zu systematischen Leistungsunterschieden zwischen unterschiedlichen Bevölkerungsgruppen (z. B. sprachliche oder kulturelle Diskrepanzen) und kann marginalisierte Nutzer benachteiligen. Durch die Analyse und das Bewusstsein für Positionalität können Entwickler Bias erkennen, transparentere Entscheidungen treffen und inklusivere, fairere Systeme schaffen.</sample>
    <sample id="204">Nein – in der Studie wurden die mehrsprachigen LLMs wie BLOOM nicht über Adapter oder eine komplette Fein‑Tuning‑Pipeline angepasst. Sie wurden als vortrainierte Modelle eingesetzt, ohne zusätzliche Anpassungsschritte.</sample>
    <sample id="205">**Abstract**

Language models (LMs) trained on large‑scale web crawls inevitably absorb the political biases present in their pre‑training corpora. We investigate the propagation of these biases from data to model to downstream tasks. First, we evaluate LM political leanings using prompt‑based political questionnaires (e.g., the Political Conference Test), revealing that LMs occupy all four quadrants of the political spectrum; GPT‑4 is the most liberal, while BART variants are comparatively conservative. Second, we conduct controlled pre‑training experiments on six partisan corpora (news and social media, left vs. right). Fine‑tuning LMs on left‑leaning Reddit shifts their ideological coordinates toward liberalism, demonstrating that training data directly shapes bias. Third, we compare models pre‑trained on pre‑ and post‑2017 corpora, finding that post‑2017 models exhibit increased extremism, mirroring societal polarization. Finally, we assess fairness in hate‑speech and fake‑news detection. Left‑leaning LMs outperform on detecting hate against minority groups but underperform on hate targeting dominant groups; right‑leaning LMs show the opposite trend. Similar asymmetries appear in fake‑news detection. These findings underscore a critical fairness gap: political bias in LMs can lead to unequal content moderation, potentially marginalizing minority voices. Our work calls for careful data curation and bias mitigation strategies to balance democratic pluralism with equitable NLP deployment.</sample>
    <sample id="206">Sie nutzen ein vortrainiertes Transformer‑Modell (z. B. BERT/RoBERTa). Das Modell wird zunächst auf die **CE‑Aufgabe** (Erweiterungs‑/Vergleichsklassifikation) und anschließend auf die **Debate‑Aufgabe** (unabhängige Diskussions‑Stance‑Klasse) fein‑tuned. Dieses sequenziell fine‑tuned Modell dient dann als Basis für das Transfer‑Learning in der Dissonanz‑Detektion.</sample>
    <sample id="207">Die PaLM‑Bewertung basierte auf den neuesten WMT‑Benchmark‑Testsets – also den aktuellen WMT‑Evaluation‑Daten (z. B. WMT 2021/22), die speziell dafür ausgewählt wurden, keine Überschneidungen mit dem Trainingsdatensatz zu haben.</sample>
    <sample id="208">Die Autoren schlagen **drei** Empfehlungen vor.</sample>
    <sample id="209">Der Text gibt keinen konkreten Zahlenwert an – es wird lediglich beschrieben, dass die vorgeschlagene Methode die Qualität der Skripte gegenüber der stärksten Baseline verbessert, aber kein konkreter prozentualer Gewinn wird genannt.</sample>
    <sample id="210">Der Referent heißt **Shuheng**.</sample>
    <sample id="211">Ja – die DEPLAIN‑Korpus‑Ergebnisse und die veröffentlichten Metriken werden ausdrücklich als Basis‑Benchmark für zukünftige Arbeiten im Bereich deutscher Textvereinfachung vorgeschlagen.</sample>
    <sample id="212">Ein einzelnes kleineres Modell – T5 – wurde in der Arbeit experimentiert.</sample>
    <sample id="213">OFA – das vortrainierte, große Multi‑Modal‑Modell OFA.</sample>
    <sample id="215">**Abstract (≈180 Wörter)**  

Die Vorlesung präsentiert ein neues Argument für symmetrische Koordinationsstrukturen im Vergleich zu asymmetrischen Modellen (z. B. Universal Dependencies, Melʹčuk, Pragau). Ausgangspunkt ist die Prinzip der Minimierung der Abhängigkeitslänge (Dependency Length Minimization, DLM). Beispiele zeigen, dass ein Objekt, das nach einem Adverbial verschoben wird, dennoch akzeptabel ist, wenn die Gesamtlänge der wichtigsten Abhängigkeiten (Verb‑Adjunkt + Verb‑Objekt) reduziert wird.  

Um die Hypothese empirisch zu prüfen, wurden Koordinationsphänomene im enhanced Penn Treebank analysiert. Es wurde beobachtet, dass linke Konjunktionen tendenziell kürzer sind als rechte, und diese Tendenz mit zunehmendem Längenunterschied stärker wird – jedoch nur, wenn der Governer links liegt oder fehlt. Bei einem rechten Governer verschwindet dieser Effekt.  

Diese Befunde stehen im Widerspruch zu asymmetrischen Modellen, die den ersten Konjunkt als Kopf bestimmen, und unterstützen stattdessen symmetrische Varianten, bei denen alle Konjunktionen gleichwertig behandelt werden und die DLM-Regelungen berücksichtigt werden. Das Ergebnis legt nahe, dass Koordination in natürlichen Sprachen eher durch strukturelle Symmetrie als durch feste Kopf‑Regeln geprägt ist.</sample>
    <sample id="217">**Abstract**

In diesem Beitrag adressieren wir die fehlende Forschung zu mehrdimensionalen, kontrollierbaren Dialoggenerierungssystemen. Während frühere Ansätze sich meist auf einzelne Attribute beschränken, stellen wir die „Seen‑to‑Unseen“-Problemstellung vor, bei der Modelle neuartige Kombinationen von Attributewerten erzeugen müssen. Wir entwickeln **DCG (Disentangled Controllable Generation)**, ein auf DialoGPT basierendes Prompt‑System, das zwei Arten von Prompts kombiniert: ein attributorientiertes Prompt, das instanzspezifische Steuerungen aus vortrainierten Sprachmodellen extrahiert, und ein task‑orientiertes Prompt, das globale, instanzunabhängige Features nutzt. Durch die Einführung von Pseudo‑Kombinationen und einer Disentanglement‑Verlustfunktion lernen die Modelle, unterschiedliche Attributkombinationen voneinander zu trennen und neue Kombinationen zu generalisieren. 

Zur Bewertung schlagen wir **MAE (Multi‑Attribute Evaluation)** vor, ein referenz‑freies, einheitliches Framework, das sowohl diskrete als auch kontinuierliche Attribute erfasst und sich robust gegenüber Vorlagenbias zeigt. Wir etablieren zwei neue Benchmarks (DailyDialog‑CG und ein weiteres Set) und demonstrieren, dass DCG die Kompositionalität über alle Attribute hinweg signifikant übertrifft, während die Kontrollierbarkeit (E‑ACC, A‑ACC) und Textqualität (BLEU) nahezu unverändert bleiben. MAE korreliert stark mit menschlichen Urteilen und übertrifft klassische Metriken für beide Attributtypen. Unsere Ergebnisse zeigen, dass Prompt‑Disentanglement die Generalisierung von gesehenen zu ungewohnten Attributkombinationen effektiv ermöglicht.</sample>
    <sample id="218">Die Autoren sind nicht an einer Universität angestellt – sie arbeiten bei Google (insbesondere im Google‑Translate‑Team).</sample>
    <sample id="219">**Abstract (≤200 Wörter)**  

Wir stellen einen mehrstufigen Pipeline-Ansatz zur Analyse von SEC‑Form 10‑K‑Berichten vor, bei dem ein Highlighting‑Task die wesentlichen Wörter zwischen einem Zielbericht (T) und seinem Referenzbericht (R) identifiziert. Der Ansatz nutzt die hohe Textähnlichkeit von Jahresberichten (≈ 80 % gleiche Tokens) und betrachtet die Relation zwischen T und R in drei Typen: *β* (hohe syntaktische/semantische Ähnlichkeit), *revised* (gleiche Syntax, unterschiedliche Bedeutung) und *mismatched* (neue Informationen).  

Stage 0 umfasst die Dokumentsegmentierung (nicht im Vortrag detailliert). In Stage 1 klassifizieren wir Paare in die drei Typen. Stage 2+‑Aufbau beinhaltet zunächst ein out‑of‑domain‑Fine‑Tuning mit eSNLI (ein NLI‑Datensatz mit token‑basierten Rationalen) und anschließend ein in‑domain‑Fine‑Tuning auf *revised*‑Paare, wobei pseudo‑positive Labels aus den Revisionen und zufällige negative Labels verwendet werden. Durch Soft‑Labeling (Cross‑Entropy + KL‑Divergenz) reduzieren wir die Fehleranfälligkeit der pseudo‑Labels.  

Wir evaluieren auf eSNLI und dem selbst erstellten FINAL‑Datensatz. Die Metriken Precision und Pearson‑Correlation (PCC) zeigen, dass das domänenspezifische Highlighting die beste Leistung liefert und gleichzeitig Generalisierungsfähigkeit bewahrt.  

Zukünftige Arbeiten umfassen die Integration zusätzlicher Features und Information‑Retrieval‑Methoden zur weiteren Leistungssteigerung.</sample>
    <sample id="220">Die Autoren sind am Stony Brook University (USA) tätig.</sample>
    <sample id="221">Die Studie konzentrierte sich auf die deutschen‑zu‑englischen (de→en) und englischen‑zu‑deutschen (en→de) Übersetzungspaare.</sample>
    <sample id="222">**Abstract (max. 200 Wörter)**  

In diesem Paper wird die Übertragbarkeit von Open‑Domain‑Question‑Answering (ODQA) Modellen aus einer allgemein‑sprachlichen Quelle (Wikipedia) auf spezialisierte Domänen untersucht. Wir stellen drei Hauptbeiträge vor:  

1. **Dateninterventionen** – Wir evaluieren zwei Interventionstypen: *zero‑shot* (keine Zielbeispiele) und *few‑shot* (einige Beispiele). Letztere werden mithilfe großer Sprachmodelle generiert, indem aus Passage‑Wissen Fakten extrahiert und zu Cloze‑Fragen umgeformt werden. Diese Interventionen führen zu einer durchschnittlichen Leistungssteigerung von 8 % bei Retrievers und 11 % bei Readers.  

2. **Shift‑Taxonomie** – Durch Messung der Kompatibilität (Likelihood‑Score) von Retriever‑ und Reader‑Modellen auf Ziel‑Triples klassifizieren wir Shift‑Typen: *No Shift*, *Concept*, *Covariate* und *Full Shift*. Ziel‑Datensätze werden in ein 2‑D‑Raster eingeordnet, etwa CliCR und NewsQA liegen im Full‑Shift-Bereich, während SearchQA nahe No Shift liegt.  

3. **Effektive Interventionen** – Für alle Shifts zeigen *few‑shot* Anpassungen signifikante Verbesserungen. *Zero‑shot* hilft besonders bei Concept‑ und Covariate‑Shift, während bei No Shift kaum Änderungen auftreten.  

Die Experimente erreichen bis zu 24 % Verbesserungen beim Reader‑Modell und demonstrieren, dass die Wahl der Intervention stark vom erkannten Shift‑Typ abhängt.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Die Experimente haben sich auf drei Modelle konzentriert:

| Art des Experiments | Modell / Methode | Zweck |
|---------------------|------------------|-------|
| **Alignment** | **MASSalign** | Beste automatische Zeilen‑Alignment‑Methode für gleichsprachige, komplexe und vereinfachte Dokumente |
| **Text‑Simplifizierung (Dokument‑Ebene)** | **Long‑mBART** | Fine‑tuned für die Vereinfachung ganzer Dokumente |
| **Text‑Simplifizierung (Satz‑Ebene)** | **Base‑mBART** | Fine‑tuned für die Vereinfachung einzelner Sätze |

So die untersuchten Modelle sind MASSalign, Long‑mBART und Base‑mBART.</sample>
    <sample id="225">**Training**: 53 tasks (from 9 groups)  
**Testing**: 5 tasks (selected from the VQ and Miscellaneous groups) plus all tasks that belong to the Common‑Sense‑Reasoning group (the exact number of those is not specified in the talk).</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="227">**Abstract**

Wir stellen Pangu vor, ein generelles Framework für das Grounded Language Understanding (GLU), das das klassische Generation‑Ansatzproblem umgeht. Anstelle eines autoregressiven Modells, das eigenständig einen Plan oder ein Programm erzeugt, agiert ein symbolischer Agent als Planer: er generiert eine Menge potenzieller, umsetzbarer Kandidaten (z. B. SQL‑Abfragen, Roboter‑Aktionsequenzen). Ein Sprachmodell (BERT, T5, Codex) wird ausschließlich zur Bewertung und Rangfolge dieser Kandidaten eingesetzt. Dadurch entfällt die Notwendigkeit, Grammatik‑ und Validitätsprüfungen selbst zu lernen, und die Gefahr von unvollständigen oder inkorrekten Ausgaben verringert sich erheblich. Wir evaluieren Pangu im Bereich des Knowledge‑Based Question Answering (GRAIL‑Dataset) in Fine‑Tuning‑ und In‑Context‑Learning‑Szenarien. Die Ergebnisse zeigen, dass Pangu sowohl bei kleinen Datensätzen als auch bei großen Modellen (Codex) die Genauigkeit über dem Baseline‑Modell ArcaneQA steigert und eine hohe Stichproben‑Effizienz erreicht (z. B. &gt; 50 % der GRAIL‑Accuracy mit nur einem Demo‑Beispiel). Darüber hinaus demonstriert Pangu eine robuste Verteilung der Bewertungswahrscheinlichkeiten über bekannte und unbekannte Strukturen, was seine Widerstandsfähigkeit gegenüber Non‑IID‑Verteilungen erklärt. Abschließend betonen wir, dass die diskriminative Strategie für GLU überlegen ist, weil sie die Stärken von Sprachmodellen nutzt und gleichzeitig die Schwächen der generativen Herangehensweise eliminiert.</sample>
    <sample id="228">Die Autoren haben Experimente auf den folgenden Datensätzen durchgeführt: **AG News, MIND, SST‑2 und Enron Spam**.</sample>
    <sample id="229">Wir stellen zwei neue Aufgaben zur Unterstützung der Argumentationsschreibung vor: (1) die Erkennung von suboptimalen Behauptungen (Suboptimal‑Claim Detection) und (2) die Klassifizierung von Qualitätsproblemen, die bei der Überarbeitung adressiert werden sollten (Claim Improvement Suggestion). Für beide Aufgaben nutzen wir revisionsbasierte Daten aus der Debattenplattform Kialo, bei der finale Versionen als optimal gelten und Vorgänger als suboptimal betrachtet werden. Wir analysieren vier zentrale Herausforderungen: (i) Repräsentativität und Zuverlässigkeit der Datensätze, (ii) Modellkomplexität und Architektur, (iii) Kontextabhängigkeit von Qualitätsdimensionen und (iv) thematische sowie benutzerspezifische Verzerrungen. Durch Experimente zeigt sich, dass die Distanz zwischen Varianten einer Behauptung die Erkennung suboptimaler Aussagen verbessert und dass der Einsatz von Kontextinformation je nach Aufgabe und Qualitätsproblem unterschiedlich wirkt. Unsere Ergebnisse demonstrieren, dass revisionsbasierte Daten ein effektives Mittel zur automatischen Qualitätsbewertung von Argumenten darstellen.</sample>
    <sample id="231">NACHOS ist ein französisches medizinisches Korpus, das aus im Internet gecrawlten Gesundheitstexten besteht und als Trainingsdaten für die Vor‑Training‑Phase von DrBERT verwendet wird.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">**Abstract**

Simultaneous Speech Translation (SimulST) delivers real‑time translations of spoken language, yet current systems rely on architecture‑specific designs, multi‑stage training, and distinct models for each latency setting. We propose **EDAtt** (Encoder–Decoder Attention), a lightweight strategy that repurposes pre‑trained offline Speech‑Translation (ST) models without architectural changes. EDAtt decides whether to emit a partial translation by examining cross‑attention weights between audio frames and output tokens. A token is emitted only when the cumulative attention over the last λ speech frames falls below a threshold α, indicating sufficient stability of the input. This single‑model approach controls latency through λ and α, enabling continuous adjustment across regimes. Experiments on English→German demonstrate that EDAtt simultaneously improves BLEU scores and reduces average lagging, outperforming conventional Wait‑k and Local Agreement policies applied to offline models and surpassing specialized SimulST architectures. Additionally, we evaluate computational‑aware average lagging, showing that EDAtt incurs lower runtime overhead. All models, code, and generated simultaneous outputs are released open‑source to support reproducibility and further research.</sample>
    <sample id="234">Die Prompt‑Strategie hat einen großen Einfluss auf die Übersetzungsleistung.  
In wenigen‑Shot‑Setups (Zero/One‑Shot) kann ein anderer Prompt die BLEURT‑Bewertung um mehr als 1 Punkt – in extremen Fällen sogar um 40 Punkte – verändern. Bei 5‑Shot‑Prompting spielt die konkrete Form des Prompts kaum mehr Rolle; hier bestimmen vielmehr die Beispiele selbst die Qualität. Generell gilt: hochwertige, gut übersetzte Beispiele aus sauberem Dev‑Datensatz führen zu besseren Ergebnissen als zufällig ausgewählte Trainingsbeispiele.</sample>
    <sample id="235">Alle genannten Autoren sind am **University of Washington** (UW) angestellt.</sample>
    <sample id="236">In der Präsentation werden die fünf Expert*innen‑Anweisungen nicht explizit aufgeführt. Es wird lediglich erwähnt, dass jedes der 62 Multi‑Modal‑Aufgaben‑Set fünf von Experten verfasste Instruktionen enthält, die in den Trainings‑ und Evaluationsphasen zufällig ausgewählt werden. Die konkreten Formulierungen der fünf Anweisungen stehen daher im bereitgestellten Material nicht vor und können hier nicht wiedergegeben werden. Wenn Sie Zugriff auf die komplette MultiInstruct‑Datei haben, finden Sie die fünf Anweisungen pro Aufgabe dort.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mithilfe eines diagnostischen Test‑Sets namens **KITMUS** zu prüfen.  
KITMUS besteht aus Coreference‑Aufgaben, bei denen gezielt die Verfügbarkeit von  
- **Prätraining‑Kenntnissen** (Hintergrundwissen)  
- **Inference‑Zeit‑Kenntnissen** (entitätsspezifisches Wissen)  

variiert wird. Durch die drei Settings (Background‑Pretrain, Background‑Both, Background‑Inference) wird getestet, wie gut Modelle Wissen aus unterschiedlichen Quellen zusammenführen können.</sample>
    <sample id="238">**Abstract**

MeetingBank ist ein neuer Benchmark-Datensatz für die Zusammenfassung von städtischen Ratssitzungen. Er umfasst 1 366 Sitzungen aus 15 US-Städten, die mittels Speechmatics API transkribiert, mit Metadaten (MeetingID, ItemID) angereichert und in Zeitabschnitte segmentiert wurden. Für jeden Abschnitt wurde ein von Experten verfasster Textzusammenfassung erstellt. Der Datensatz liefert Transkripte, Zusammenfassungen, Zeitstempel und Verweis‑URLs. Statistische Analysen zeigen durchschnittliche Sitzungsdauer von 1 h 15 min, durchschnittlich 7 Sprecher pro Sitzung und 1 200 Token pro Transkript, 60 Token pro Zusammenfassung. Die Abstraktionsgrade werden über Coverage (70‑90 %) und Density (variiert je Stadt) gemessen, wobei Seattle und Boston hohe Densitäten und Denver niedrige aufweisen. Für die Bewertung wurden extraktive Modelle (Oracle, LEAD, LexRank, TextRank) und abstraktive Modelle (BART‑Large, Pegasus, Longformer, DialogLM, HMNet) eingesetzt. DialogLM erreichte die höchste ROUGE‑2 unter abstraktiven Systemen, während GPT‑3‑Davinci‑003 in der menschlichen Evaluierung Spitzenwerte bei Fluency und Coherence erzielte, jedoch bei Faktizität und Informationsgehalt zurückblieb. MeetingBank bietet damit eine umfangreiche Ressource für die Entwicklung und Bewertung von Meeting‑Summarization‑Algorithmen.</sample>
    <sample id="239">Hallo zusammen, mein Name ist David Vilar und ich werde Ihnen eine kurze Übersicht über die Arbeit **„Prompting PaLM for Translation: Assessing Strategies and Performance“** geben. Diese Studie ist ein gemeinsames Projekt mit meinen Kollegen aus Google Translate.

PaLM ist ein 540‑Billionen‑Parameter‑Large‑Language‑Model, das im Jahr 2022 vorgestellt wurde. Es wurde mit einer riesigen Textsammlung von 780 Billionen Tokens trainiert und erreichte zu seiner Veröffentlichung den Stand‑der‑Art in Hunderten von NLP‑Aufgaben.

In dieser Arbeit führen wir die erste systematische Untersuchung des Prompt‑Trainings von LLMs für maschinelle Übersetzung durch. Wir bewerten die Übersetzungsfähigkeit solcher Modelle nach den besten Praktiken der MT‑Community – also mithilfe der aktuellsten Testsets, um Überschneidungen mit den Trainingsdaten des Modells zu vermeiden. Zudem vergleichen wir die Ergebnisse mit den besten bestehenden Systemen, also dem WMT‑Best‑of‑the‑Year. Wir verwenden dazu hochmoderne neuronale MT‑Metriken und ergänzen diese um Experten‑basierte, menschliche Evaluierungen. Schließlich geben wir Empfehlungen für die Auswahl geeigneter Prompt‑Strategien.

Ein einfacher Test zeigt, wie stark die Prompt‑Auswahl die Leistung der LLMs für Übersetzungen beeinflusst: Wir haben One‑Shot‑Prompting mit zwei unterschiedlichen Prompt‑Formulierungen für dieselbe Satzsequenz durchgeführt. Bei 516 von 1 000 Sätzen war die Differenz mehr als ein BLEURT‑Punkt – in extremen Fällen bis zu 40 BLEURT‑Punkte. Das verdeutlicht, wie wichtig eine gute Prompt‑Strategie ist.

In unseren Experimenten haben wir uns für ein 5‑Shot‑Prompting entschieden, bei dem jeder zu übersetzende Satz mit der jeweiligen Sprache gekennzeichnet wird. Beispiel: Beim Übersetzen von Deutsch nach Englisch wird der deutsche Ausgangssatz mit „German:“ und die englische Übersetzung mit „English:“ gekennzeichnet. Wir fanden heraus, dass die konkrete Formulierung des Prompts bei mehrfachem (5‑Shot) Prompting kaum Einfluss hat – im Gegensatz zu Zero‑ und One‑Shot‑Setups, bei denen die Formulierung entscheidend ist. Der entscheidende Faktor sind also die Beispiele selbst – nicht deren Ähnlichkeit zum Ausgangssatz. Hochwertige Übersetzungen als Beispiele sind daher vorteilhaft. Wir haben die Prompt‑Auswahl sowohl aus den Trainingsdaten als auch aus den Entwicklungsdaten (dev) von WMT verglichen. Die dev‑Daten sind in der Regel besser kuratiert und qualitativ hochwertiger als die Trainingsdaten, die etwas noisiger sind. Das resultierte in einer besseren Leistung, wenn die Beispiele aus den dev‑Daten stammen.

Trotzdem behalten spezialisierte, state‑of‑the‑art‑Systeme einen erheblichen Vorteil gegenüber PaLM. PaLM kommt jedoch nahe an ein kommerzielles System heran – in unserer Auswertung haben wir Google Translate als Referenz gewählt. Die Erkenntnisse aus einer MQM‑basierten menschlichen Evaluation zeigen, dass die Flüssigkeit von PaLM mit den besten Systemen vergleichbar ist, während die Genauigkeit noch hinterherhinkt. Die häufigsten Fehler sind Auslassungen: PaLM produziert oft flüssigere Übersetzungen, aber manchmal lässt es Teile des Ausgangstextes weg. Im Gegensatz dazu ist die Kategorie „Style/Awkward“ für PaLM niedriger als bei den state‑of‑the‑art‑Systemen, was ein zusätzliches Signal dafür ist, dass PaLM wirklich flüssige, aber bei der Genauigkeit noch verbesserungswürdige Übersetzungen liefert.

Das war ein kurzer Überblick. Für weitere Details lade ich Sie herzlich zur vollständigen Präsentation des Papers ein. Vielen Dank!</sample>
    <sample id="240">Hallo, ich bin Dawei, Doktorand an der Universität des Saarlandes in Deutschland. In diesem Vortrag möchte ich unsere jüngsten Arbeiten mit dem Titel „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ vorstellen. Das ist ein gemeinsames Projekt mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.  

Zunächst möchte ich Ihnen kurz die Grundlagen der schwachen Überwachung und des schwachen überwachten Lernens (Weakly Supervised Learning, WSL) erläutern. Bei schwacher Überwachung werden die Daten nicht durch manuelle Annotationen beschriftet. Stattdessen generieren wir Labels mithilfe schwacher Beschriftungsquellen – etwa einfachen Heuristikregeln, Wissensbasen oder minderwertigen Crowdsourcing‑Daten, wie es in der Abbildung rechts gezeigt ist. Im Vergleich zu menschlichen Anmerkungen sind diese schwachen Labels zwar deutlich günstiger, aber sie sind auch verrauscht, das heißt, ein Teil der Labels ist fehlerhaft.  

Wenn wir neuronale Netzwerke direkt mit schwach beschrifteten Daten trainieren, neigen die Modelle dazu, den Rauschen zu lernen und nicht zu generalisieren. Im schwachen überwachten Lernen werden deshalb Lernalgorithmen vorgeschlagen, die robust gegen solche Label‑Noise sind, damit die trainierten Modelle trotzdem gut auf sauberen Testdaten abschneiden.  

In vielen aktuellen Arbeiten im Bereich WSL wird häufig behauptet, man trainiere Modelle ausschließlich mit schwach beschrifteten Daten und erziele dabei hohe Leistungen auf sauber annotierten Testsets. Technisch ist diese Aussage zwar korrekt, doch gibt es einen entscheidenden Fehlschlag: Man nimmt an, dass zusätzlich ein sauber annotiertes Validationsset zur Modell­auswahl zur Verfügung steht. Diese Annahme wird oft übersehen und führt zu zusätzlichen manuellen Annotationskosten.  

Diese Beobachtung führt zu drei Forschungsfragen:  
1. Ist ein sauberes Validationsset zwingend erforderlich für WSL, oder können wir eventuell auch ein verrauschtes Validationsset verwenden?  
2. Falls sauber annotierte Daten notwendig sind, wie viele davon benötigen wir dann?  
3. Müssen die sauberen Samples ausschließlich für die Validierung genutzt werden, oder gibt es bessere Wege, sie einzusetzen?  

Wir haben diese Fragen in unserer Arbeit adressiert und die folgenden Erkenntnisse gewonnen:  

*Erstens* – Wir haben festgestellt, dass aktuelle WSL‑Methoden tatsächlich saubere Validationssamples benötigen, um ordnungsgemäß zu funktionieren. Andernfalls fällt die Leistung dramatisch ein. Wie in der Abbildung gezeigt, wenn keine sauberen Validationssamples vorhanden sind, generalisiert das Modell nicht über die ursprünglichen schwachen Labels hinaus, was das Training nutzlos macht. Das bedeutet, dass WSL‑Ansätze tatsächlich sauber beschriftete Daten brauchen und die Kosten für deren Beschaffung nicht unterschätzt werden dürfen.  

*Zweitens* – Die Anzahl der sauberen Validationssamples hat einen großen Einfluss auf die endgültige Performance. Wie in der linken Abbildung veranschaulicht, reichen häufig nur etwa 20 Samples pro Klasse, um die Leistung signifikant zu steigern. Doch das ist nicht das Ende der Geschichte: Sobald wir saubere Samples haben, kann das direkte Training (Fine‑Tuning) auf diesen Daten sogar bessere Ergebnisse liefern. In der rechten Abbildung vergleichen wir die Performance von Fine‑Tuning-Ansätzen, die direkt auf den sauberen Daten trainieren, mit WSL-Methoden, die die sauberen Daten nur zur Validierung nutzen. Sobald wir zum Beispiel 10 Samples pro Klasse haben, übertrifft das direkte Fine‑Tuning bereits die WSL‑Ansätze.  

*Drittens* – Der von früheren WSL‑Methoden behauptete Performance‑Boost lässt sich leicht erreichen, wenn man dem Modellen die Möglichkeit gibt, nach dem initialen Training weiter auf den sauberen Validationssamples zu fine‑tunen. In der Grafik sehen Sie, dass das Grundmodell (FTw) zunächst hinter komplexeren WSL‑Methoden wie COSINE zurückbleibt. Sobald jedoch ein weiteres Fine‑Tuning auf den sauberen Samples erfolgt, erreicht FTw die gleiche Leistung wie die anderen Verfahren. Praktisch bedeutet das: Es gibt keinen Grund, auf aufwändigere WSL‑Methoden zu setzen, die mehr Rechenzeit und Speicherplatz benötigen.  

Zusammenfassend zeigt unsere Arbeit, dass aktuelle WSL‑Ansätze sauber annotierte Samples für Modell­auswahl benötigen und deren Leistungssteigerung sowie Praktikabilität stark überschätzt werden. Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten:  

1. **Bericht der Modell­auswahlkriterien** – Geben Sie an, ob die Modell­auswahl mittels sauber annotierter Validationssamples erfolgt.  
2. **Vergleich mit Few‑Shot‑Learning‑Baselines** – Beide Ansätze nutzen sauber annotierte Daten; ein fairer Vergleich ist unerlässlich.  
3. **Kontinuierliches Fine‑Tuning** – Dieses einfache, aber starke Baseline‑Verfahren sollte in zukünftigen WSL‑Studien berücksichtigt werden.  

Außerdem haben wir unseren Code öffentlich zugänglich gemacht. Sie finden ihn über den QR‑Code auf dieser Folie. Bitte schauen Sie ihn sich an.  

Vielen Dank und viel Spaß bei der Konferenz!</sample>
    <sample id="241">**Abstract (max. 200 Wörter)**  

In diesem Paper stellen wir ein human‑in‑the‑loop‑Framework vor, das die beiden häufigen Schwächen bestehender Misinformation‑Erkennungssysteme – unrealistische Evaluation und fehlende menschliche Einbindung – adressiert. Unsere End‑to‑End‑Pipeline arbeitet mit Live‑Tweets zu COVID‑19‑Behandlungen. Zunächst filtert ein Keyword‑Modul relevante Beiträge, danach extrahiert ein T5‑basierter Frage‑Antwort‑Modell Aussagen wie „Ivermectin ist wirksam gegen COVID‑19“. Die entdeckten Claims werden nach Trendigkeit (Fisher‑Exact‑Test) gewichtet und anschließend von Moderatoren verifiziert. In der zweiten Phase klassifiziert ein BERT‑Stance‑Classifier die Haltung des Autors gegenüber unautorisierten Behandlungen; unterstützende Tweets werden als potenzielle Policy‑Verstöße markiert.  

Die Evaluation konzentriert sich auf **frühe Erkennung** – Claims werden identifiziert, bevor sie in einem Debunking‑Artikel auftauchen – sowie auf die Genauigkeit der Policy‑Verstoß‑Erkennung. Wir erreichen 65 % Genauigkeit bei der Violation‑Detektion und können pro moderiertem Stundenanteil durchschnittlich 124,2 Verstöße bestätigen. Diese Ergebnisse demonstrieren, dass unser systematisch evaluiertes, Mensch‑zentriertes Verfahren sowohl frühzeitig als auch effizient auf aufkommende Fehlinformationen reagiert und damit praktische Relevanz für Content‑Moderation bietet.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind:

1. **Menschliche Vergleiche** – Paare von Konversationen werden von menschlichen Prüfern verglichen („Besser?“).  
2. **Likert‑Skalen** – Bewertungen auf:
   - **Stufenniveau** (Turn‑by‑Turn-Likert)  
   - **Gesprächsniveau** (Gesamtlikert).  

Diese Verfahren liefern holistische Qualitätswerte, werden aber oft durch mehrdimensionale Ansätze ergänzt.</sample>
    <sample id="243">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="244">Das benötigte Hintergrundwissen lautet: **„Richter entscheiden Fälle in Gerichten.“**</sample>
    <sample id="245">Wir stellen einen zweistufigen Rekrutierungs‑Pipeline vor, um hoch‑Konsens‑Amazon‑Mechanical‑Turk‑Arbeiter (MTurk) für Text‑Zusammenfassung zu identifizieren. Zunächst werden vorläufige Qualifikationen (Ort, Anzahl der HITs, HIT‑Approval‑Rate) gesetzt. In der Qualifikations‑Stufe bewerten 200 Bewerber anhand von drei Trainings‑ und drei Qualifikations‑Dokumenten mit Attention‑Check; die Bewertung umfasst sechs Dimensionen. Vierzig‑fünfzig Prozent werden in Gold‑ (8) und Silver‑ (18) Kategorien eingeteilt—nur Gold und Silver bestehen die Prüfung. Anschließend folgt die Ausdauer‑Stufe mit 10 HITs, einer einzigen Text‑Datei und vier Zusammenfassungen zur Salienz‑Bewertung; hier bleiben 12 Arbeiter (4 Gold, 8 Silver) übrig (6 % der ursprünglichen Gruppe). Inter‑Annotator‑Agreement (IAA) erreicht mit Krippendorff’s Alpha 0.443 und Cohen’s Kappa eine hohe Übereinstimmung. In einer Referenz‑basierten Aufgabe (30 HITs, 4 Kandidaten pro Referenz) erzielen Pipeline‑Arbeiter einen Alpha von 0.534; nur 8 von 12 beenden alle HITs. Im Vergleich schlagen statische Filter (MACE) Alpha 0.380 und CloudResearch‑Arbeiter Alpha 0.513, jedoch mit geringerer Akzeptanz. Eine Heat‑Map‑Analyse zeigt signifikante Spearman‑Korrelationen zwischen Pipeline‑ und CloudResearch‑Arbeitern; GPT‑Modelle korrelieren stark mit Experten. Fazit: Vor‑Filterung reduziert Ressourcenverschwendung, liefert kostengünstige, qualitativ äquivalente Annotationen. Limitationen: Englisch‑Summarisation, keine Garantie für Korrektheit, nicht universell einsetzbare Fragen. Zukünftige Arbeiten: Erweiterung auf andere Sprachen, Aufgaben und Plattformen.</sample>
    <sample id="246">Ja – der Code (zusammen mit dem Datensatz) ist öffentlich auf GitHub verfügbar.</sample>
    <sample id="247">**Abstract**

Das Paper stellt FactKG vor, einen Datensatz für die Aufgabe der Faktenverifikation mittels Wissensgraphen. Im Gegensatz zu bestehenden Korpora wie FEVER, VitaminC, TabFact oder InfoTabs, die Wikipedia‑Texte bzw. Tabellen als Evidenz nutzen, liefert FactKG Evidenz aus DBpedia. Jede Behauptung existiert sowohl in einer formalen (geschriebenen) als auch in einer umgangssprachlichen Variante und erhält die Klasse SUPPORTED oder REFUTED. Die Verifizierungsaufgabe umfasst die Extraktion relevanter Tripel und die Bewertung der Behauptung unter Verwendung von fünf Logik‑Typen: One‑Hop, Konjunktion, Existenz, Multi‑Hop und Negation. Beispielsweise erfordert eine Multi‑Hop‑Behauptung einen Pfad von AIDAStella zu Papenburg. Um umgangssprachliche Varianten zu erzeugen, werden ein Stil‑Transfer‑Modell und Prä‑sposition‑Vorlagen eingesetzt. Der Datensatz enthält 10 000 Behauptungen, aufgeteilt in Trainings-, Validierungs- und Testmengen. Baseline‑Methoden umfassen reine Text‑Modelle (Claim‑Only) sowie ein graph‑basiertes GEAR‑Modell, das Evidenz aus dem KG nutzt. Beide Baselines übertreffen die 51 %-Majority‑Baseline, wobei GEAR die höchste Genauigkeit erzielt. FactKG bietet eine neue Herausforderung für KI‑Systeme, die Konsistenzprüfungen zwischen natürlicher Sprache und internen Wissensgraphen durchführen müssen.</sample>
    <sample id="248">Nein – die Annotatoren wurden gezielt **vielfältig**, aber nicht gleichmäßig über jede demografische Gruppe verteilt.  
Im Rahmen von NLPositionality sammelten die Forscher über 16 000 Anmerkungen von mehr als 1 000 Teilnehmenden aus 87 Ländern. Dennoch zeigt die Analyse, dass die Daten stark von Englisch‑sprachigen und bildungsstarken Populationen dominiert werden, während andere Gruppen – etwa Nicht‑binäre Personen oder Menschen aus weniger vertretenen Ländern – weniger repräsentiert sind. Daher ist die Stichprobe zwar breit, aber nicht vollständig ausgeglichen.</sample>
    <sample id="249">Durch strukturbewahrende Störungen: die akzeptablen Sätze wurden mit zufälligem „Rauschen“ manipuliert – etwa durch Token‑Ersetzungen, Einfügungen oder Umstellungen, die die grammatische Struktur erhalten, sodass die Sätze inhaltlich durcheinandergebracht, aber syntaktisch gleichbleibend blieben.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, die Qualität eines Chat‑Modells nicht nur als Gesamtsumme, sondern anhand mehrerer spezifischer Aspekte (z. B. Relevanz, Konsistenz, Halluzination, Empathie) zu messen. Dadurch lassen sich Stärken und Schwächen des Modells feingranularer und objektiver erfassen.</sample>
    <sample id="251">Die Autoren sind an der **University of Science and Technology of China** (USTC) angestellt.</sample>
    <sample id="252">**Abstract – U‑CREAT: Unsupervised Case Retrieval using Events extrAcTion**  
In der Rechtsinformatik ist das Auffinden relevanter Präzedenzfälle (Prior Case Retrieval, PCR) entscheidend, jedoch zunehmend herausfordernd, da die Fallzahl steigt. Wir stellen zwei Kernbeiträge vor. Erstens das **IL‑PCR‑Dataset**, ein deutsch‑indisches Benchmark‑Korpus mit 7 070 indischen Gerichtsfällen, der im Mittel 6,8 Zitationen pro Anfrage enthält. Im Vergleich zu COLIEE’21 bietet IL‑PCR größere Dokumentlängen, einen umfangreicheren Wortschatz und mehr Zitationen, was es zu einer anspruchsvollen Testumgebung für PCR‑Algorithmen macht. Zweitens die **U‑CREAT‑Pipeline**, ein vollständig unüberwachter Ansatz, der Falltext in Ereignisse (Subjekt‑Verb‑Objekt‑Triplets) umwandelt. Durch Abhängigkeitsparsen werden Ereignisse extrahiert und ein Interaktionsmatrix zwischen Anfragedokument und Kandidaten erstellt. Basierend auf dieser Matrix werden Kandidaten nach Relevanz sortiert. Im Vergleich zu klassischen BM25‑ und Transformer‑Modellen (BERT, DistilBERT, InCaseLawBERT) erzielen eventbasierte Modelle, insbesondere das Event‑Filtered‑Docs‑Modell, signifikante F1‑Verbesserungen und geringere Laufzeiten. Auf dem COLIEE’21‑Datensatz übertrifft U‑CREAT sogar fortgeschrittene überwachte Ansätze und etabliert sich damit als führende Methode für Prior Case Retrieval.</sample>
    <sample id="253">**Abstract**

DisorBERT ist ein zweifach domänenadaptierter Modellansatz zur automatischen Erkennung von Anzeichen psychischer Erkrankungen in Social‑Media‑Posts. Ausgangspunkt ist das vortrainierte BERT‑Modell, das zunächst an die sprachlich spezifische Terminologie von Reddit angepasst wird und anschließend auf den mental‑health‑Domain‑Korpus spezialisiert. Zur Verstärkung des Fokus auf relevante Schlüsselwörter wird ein lexikon‑geführtes Masking‑Schema eingesetzt, das das Modell bei der Vorhersage von Masken‑Tokens auf psychologisch relevante Begriffe lenkt. Durch diese Kombination aus Domain‑Adaptation und Guided Masking erzielt DisorBERT eine ausgeglichene Präzision‑Recall‑Perfomance auf den eRisk‑Datensätzen und übertrifft die Leistung von MentalBERT, einem Modell, das mit umfangreichen Daten trainiert wurde. Analyse von BERT‑ und DisorBERT‑Masken‑Vorhersagen auf Beck‑Depressionsinventar‑Sätzen zeigt, dass DisorBERT stärker auf negative und psychologisch orientierte Wörter zurückgreift. Zusätzlich visualisieren wir mit einem interaktiven Attention‑Graphen die wichtigsten Textsequenzen, z. B. „anxious“, „medication“, die für Depression relevant sind. Zukünftige Arbeiten planen den Einsatz weiterer Lexika und klinischer Daten zur weiteren Verbesserung.</sample>
    <sample id="254">**Abstract**

Dokumenten‑basiertes, distanzgesteuertes Relation‑Extraction (DocRE) ist durch stark verrauschte, automatisch gelabelte Daten (DS‑Daten) geprägt, die häufig falsche bzw. fehlende Relation‑Labels erzeugen. Wir stellen ein neues Framework vor, das die Qualität dieser DS‑Labels mithilfe von Unsicherheits­schätzungen verbessert. Zunächst wird ein Vor‑Denoising‑DocRE‑Modell mit DS‑ und menschlich annotierten Daten trainiert, um Pseudo‑Labels zu generieren. Um die Vertrauenswürdigkeit der Vorhersagen zu bestimmen, verwenden wir Monte‑Carlo‑Dropout, passen die Unsicherheits­schätzung jedoch an, um überlappende Relationen (z. B. mehrere Beziehungen zwischen demselben Entitätenpaar) adressieren zu können. Für jede positive Pseudo‑Relation wird ein instanz‑basiertes Unsicherheits­score berechnet. Anschließend werden dynamische, klassen‑spezifische Unsicherheits­schwellen definiert, die es ermöglichen, hochunsichere Labels zu filtern. Schließlich iterieren wir mit einem mehrphasigen Trainings‑ und Re‑Label‑Verfahren, wodurch DS‑Daten kontinuierlich verbessert werden. Auf zwei öffentlichen DocRE‑Datasets erzielt unser Ansatz die bisher beste Performance und demonstriert, dass Unsicherheits‑gesteuerte Label‑Denoising signifikante Verbesserungen in der Relationen‑Extraktion ermöglicht.</sample>
    <sample id="255">Die Form des Prompts ist vor allem bei **Zero‑Shot und One‑Shot** – insbesondere bei sehr kurzen Prompts – relevant. Bei **Five‑Shot** Prompting hat die genaue Form kaum Einfluss; hier bestimmen die Beispiele selbst die Leistung.</sample>
    <sample id="257">Die Autoren haben vier aktuelle, State‑of‑the‑Art‑Chat‑Modelle evaluiert – die konkreten Modellnamen werden im Text jedoch nicht genannt.</sample>
    <sample id="258">**Abstract**  
In diesem Beitrag wird untersucht, ob große Sprachmodelle (LLM) eine praktikable Alternative zu menschlichen Bewertungen in der NLP-Forschung darstellen können. Traditionell werden Qualitätssignale für generierte Texte über Laborstudien mit menschlichen Gutachtern ermittelt, ein Prozess jedoch fehleranfällig, teuer und schwer reproduzierbar. Wir zeigen, dass LLMs, die auf Anweisungen in natürlicher Sprache reagieren können, ähnliche Bewertungen liefern. Dazu instruieren wir vier Modelle (T0, InstructGPT Curie, InstructGPT Davinci und ChatGPT) dazu, über Grammatik, Kohärenz, Beliebtheit und Relevanz von von GPT‑2 generierten sowie menschlich verfassten Kurzgeschichten zu bewerten. Die Ergebnisse werden mit den Bewertungen von qualifizierten Englischlehrern verglichen, die als Experten gelten. Davinci und ChatGPT zeigen eine klare Präferenz für menschlich geschriebene Texte, ähnlich wie die menschlichen Gutachter, während kleinere Modelle keine signifikante Unterscheidung treffen. Wir analysieren zudem die Einflussfaktoren von Hinweisformulierungen, Stichprobenmethoden und Kosten/Nutzen gegenüber menschlicher Evaluation. Die Studie legt nahe, dass LLM‑basierte Bewertungssysteme für bestimmte Aufgaben eine zuverlässige, skalierbare Alternative darstellen.</sample>
    <sample id="259">**Abstract (≤ 200 Wörter)**  

XSemPLR stellt einen einheitlichen Benchmark für die cross‑linguale semantische Analyse (CL‑SA) bereit, bei dem Nutzeranfragen in 22 natürlichen Sprachen und 15 Sprachfamilien in 8 verschiedenen Bedeutungsdarstellungen (SQL, Lambda‑Kalkül, FunQL etc.) übersetzt werden. Der Datensatz umfasst neun Domänen und fünf Aufgaben (z. B. Frage‑Antwort, Abfragegenerierung, Intent‑Erkennung). Wir evaluieren sechs Trainings‑/Evaluationssettings: Translate‑Test (Google‑Translate + monolinguales Modell), Monolingual, Monolingual‑Few‑Shot, Multilingual, Cross‑Linguistic Zero‑Shot und Few‑Shot Transfer.  

Für die Modellarchitektur vergleichen wir Encoder‑PTR‑Modelle (XLM‑R + PTR, mBERT + PTR) mit Encoder‑Decoder‑Modellen (mBART, mT5). Encoder‑Decoder‑Modelle erzielen die höchste Genauigkeit auf allen Datensätzen. Multilinguale Trainingsverbesserungen zeigen ein „Curse‑of‑Multilinguality“‑Phänomen: die meisten Sprachen profitieren, während Englisch in sieben Fällen zurückgeht.  

Die Cross‑Linguistic Transfer‑Analyse zeigt, dass Zero‑Shot‑Transfer erhebliche Leistungsrückgänge aufweist, die mit nur wenigen wenigen‑Shot‑Beispielen drastisch reduziert werden. Zudem verbessert ein auf Englisch vortrainiertes Modell die Few‑Shot‑Leistung in Zielsprachen stark. Moderne multimodale Modelle wie Codex oder BLOOM liefern bislang keine brauchbaren Ergebnisse für CL‑SA.  

XSemPLR bietet somit ein umfassendes Testfeld für zukünftige Arbeiten im Bereich der mehrsprachigen semantischen Analyse.</sample>
    <sample id="260">Ein Autor.</sample>
    <sample id="261">Ein guter Planer muss:

1. **Begründet und sinnvoll** – die erzeugten Skripte sollten logisch strukturiert und praktisch umsetzbar sein.  
2. **Konfidenz‑treu** – der Plan muss die vorgegebenen Einschränkungen (z. B. Zutaten, Zeit, Ausrüstung) genau einhalten.  
3. **Semantisch vollständig** – alle notwendigen Schritte zur Erreichung des Ziels sind enthalten.  
4. **Konsistent** – keine widersprüchlichen Anweisungen oder unnötigen Schritte.  
5. **Robust** – bei unterschiedlichen Kategorien von Einschränkungen zuverlässig funktionieren.</sample>
    <sample id="262">Nur ein Autor – Siyu Yuan.</sample>
    <sample id="263">**Abstract**  
In‑context learning (ICL) for text classification is highly sensitive to the choice and order of prompt examples, which introduces systematic label biases. We formalize ICL as a function of a labeled context, the target text, and the set of label names, and identify three bias components: (i) *vanilla‑label bias*—the model’s intrinsic preference for certain label names; (ii) *context‑label bias*—effects induced by the specific prompt examples; and (iii) a newly discovered *domain‑label bias*—the influence of the task corpus itself. Experiments with random in‑domain words reveal that such words can heavily skew predictions, whereas random English words do not, confirming the existence of domain‑label bias. To mitigate all bias types, we propose **domain‑context calibration**: for each label, we estimate its bias by feeding the model random in‑domain tokens and then adjust the raw ICL logits accordingly. Across multiple large language models and diverse datasets, this calibration consistently boosts accuracy, especially on tasks with high domain‑label bias, and outperforms prior methods that rely on a single fixed token. Our work provides a systematic taxonomy of label biases and a practical calibration technique that enhances the reliability of ICL for text classification.</sample>
    <sample id="264">**Abstract**

We introduce **TAVT** (Transferable Audio‑Visual Text Generation), a novel multimodal generation task that addresses severe domain shifts in audio‑visual text synthesis. Existing methods fail when visual style, shooting angle, or audio rhythm change, yet the underlying event semantics remain stable. TAVT overcomes this by mapping visual concepts from diverse domains into a unified audio semantic space. The framework comprises three components: (1) an **audio‑visual meta‑mapper** that clusters a large collection of Flickr audio clips via k‑means and learns visual‑prefix tokens to reconstruct audio distributions conditioned on visual queries, thereby aligning cross‑domain visual features with audio semantics; (2) a **transformer‑based encoder‑generator** that dynamically weights audio and visual contributions (α‑coefficients) for each generated token; and (3) **Dual Counterfactual Contrastive Learning (DCLL)**, which constructs fine‑grained supervision from counterfactual pairs to directly optimise visual‑text alignment without relying on random negatives. 

Training follows a MAML‑style meta‑learning protocol: K–1 domains form a support set for fast adaptation, while the remaining domain serves as a query set. We evaluate on cross‑dataset and cross‑domain splits of MSVD and MSR‑VTT, demonstrating that TAVT consistently outperforms state‑of‑the‑art RNN and transformer baselines across all metrics, especially in low‑resource settings. Ablation studies confirm the critical role of audio features in enhancing transferability.</sample>
    <sample id="265">Der Referent bzw. die Referentin heißt **Vasudha**.</sample>
    <sample id="266">Die Autoren sind an der **Universität Warschau** tätig.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind **Verluste von Inhalt – Omission‑Fehler**. Dabei lässt das Modell häufig Teile des Ausgangstextes weg, was die Genauigkeit beeinträchtigt, obwohl die Flüssigkeit gut bleibt.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir euch alles über ABC‑Eval erzählen, einen neuen dimensionsbasierten Ansatz zur Bewertung von Conversational AI.

Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University in Zusammenarbeit mit Amazon Alexa AI durchgeführt.

Angenommen, ihr habt gerade ein Dialogmodell entwickelt und wollt sehen, wie es im Vergleich zum derzeitigen Stand‑der‑Kunst abschneidet. Die übliche Praxis ist, menschliche Bewertungen einzusetzen, zum Beispiel indem menschliche Gutachter entscheiden, welches von zwei Gesprächen besser ist, oder indem sie Gespräche anhand einer Likert‑Skala bewerten. Diese Ansätze funktionieren gut, um ganzheitliche Bewertungen der Gesamtdialogqualität zu liefern, aber die Dialogqualität hat viele Facetten. Daher möchtet ihr vielleicht mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.

Eine Möglichkeit wäre, menschliche Gutachter bitten, mehrere Dimensionen der Dialogqualität zu bewerten, etwa die Relevanz der Modellantworten, mithilfe bestehender Vergleichs‑ oder Likert‑Skalensysteme. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionsbasierte Dialogbewertung gibt.

Unser Ansatz versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem explizit annotiert wird, ob bzw. nicht jede Modellantwort bestimmte Verhaltensweisen ausdrückt – etwa irrelevante Informationen zu liefern, sich selbst zu widersprechen oder sich mit dem Partner zu widersprechen. Wir nennen diesen Ansatz das Annotieren von Verhaltensweisen im Chat, kurz ABC‑Eval.

Wir entwickelten diese Methode, um sämtliche Verhaltensweisen zu erfassen, die in der jüngsten Literatur als Einflussfaktoren auf die Chatqualität identifiziert wurden. ABC‑Eval kann die Häufigkeit messen, mit der Chatmodelle thematische Fehler begehen. Zum Beispiel misst ABC‑Eval die Anzahl der Züge, in denen ein Chatmodell seinen Partner ignoriert oder etwas Unrelevantes sagt, sich selbst oder den Partner widerspricht, falsche Fakten halluciniert oder das Allgemeinwissen verletzt, und wann das Modell Empathie zeigt oder nicht.

Um herauszufinden, welche Art von Evaluation am effektivsten ist, wählten wir vier aktuelle Chatmodelle und bewerteten sie mit ABC‑Eval anhand von 100 menschlich-bot‑Konversationen pro Modell. Zur Gegenüberstellung bewerteten wir diese Konversationen außerdem mit drei bestehenden Methoden: Likert‑Bewertungen auf Zügsebene, Likert‑Bewertungen auf Dialogebene und dialogweite Paarvergleiche. Für jede der bestehenden Methoden sammelten wir Bewertungen zu acht der am häufigsten gemessenen Aspekte von Dialogen, da dies die Standardpraxis ist, um Chatmodelle entlang mehrerer Dimensionen zu bewerten.

Aus unserer Analyse der Bewertungsergebnisse stellten wir fest, dass ABC‑Eval‑Verhaltenslabels insgesamt zuverlässiger sind als Labels, die durch die bestehenden Methoden gesammelt wurden, gemessen an der Inter‑Annotator‑Agreement bei 100 doppelt annotierten Konversationen. Darüber hinaus sind die ABC‑Eval‑Labels besser vorhersagend für die Gesamtqualität eines Gesprächs im Vergleich zu den Kennzahlen aus den bestehenden Methoden, wie durch diese einfache lineare Regressionsanalyse gezeigt wurde. So lässt sich zum Beispiel aus der Messung des Anteils an Zügen mit Selbst‑ und Partner‑Widersprüchen 5 % bzw. 10 % der Gesprächsqualität erklären, während die durchschnittlichen Likert‑Konsistenzwerte nur 4 % oder weniger erklären.

Schließlich haben wir überprüft, ob jede Bewertungskategorie einen einzigartigen Aspekt der Chatqualität erfasst, indem wir eine schrittweise lineare Regression durchgeführt haben. Dabei können wir sehen, dass die Kombination aller ABC‑Eval‑Kennzahlen mehr als 25 % der Gesprächsqualität erklärt, und wenn man die Kennzahlen nacheinander entfernt, verliert man in den meisten Fällen einen beträchtlichen Anteil an Information über die Qualität. Die Kombination aller Zügsebene‑Likert‑Kennzahlen erklärt hingegen deutlich weniger der Qualität, und weniger dieser Kennzahlen tragen einzigartige Informationen bei.

Diese zuverlässigen, informativen und eindeutigen ABC‑Eval‑Kennzahlen ermöglichen es uns, Conversational AI mit einer höheren Auflösung zu bewerten als vorherige Methoden. In den Ergebnissen unseres Experiments haben wir noch immer einige Herausforderungen eindeutig quantifiziert. Zum Beispiel weisen die getesteten Bots in rund 20 % ihrer Antworten Verstöße gegen das Allgemeinwissen auf. Sie liefern in etwa 15 % ihrer Antworten irrelevante Informationen und widersprechen sich selbst oder ihrem Partner in rund 10 % der Fälle. Durch den raschen Fortschritt in diesem Feld könnten diese Fehlerquoten bei neueren Modellen seit der Durchführung unserer Bewertung durchaus sinken. Das ist jedoch ein zusätzlicher Grund, zuverlässige und präzise Bewertungsmetriken zu verfolgen, um Modelle vergleichen zu können.

Wir hoffen, dass ABC‑Eval von anderen in der Forschung genutzt werden kann, um einen bedeutsamen Schritt in diese Richtung zu machen. Und wir freuen uns darauf, zu sehen, wie Conversational AI in den kommenden Monaten und Jahren weiter fortschreitet.

Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren gehören zur Emory University.</sample>
    <sample id="271">CFT steht für **Continuous Fine‑Tuning** (kontinuierliches Feintuning).</sample>
    <sample id="272">Sieben Autoren (Koustav Sinha plus John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams).</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit vorstellen, die den Titel trägt „When Does Translation Require Context? A Data‑driven, Multilingual Exploration“. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Nehmen wir zum Beispiel das Wort „mole“ in diesem Satz: Wenn der vorherige Satz lauten würde: „Things could start to get dangerous if the ministers find out“, dann bezieht sich „mole“ auf einen Spion. Wenn der vorherige Satz jedoch lautet: „Could it be anything serious, doctor?“, dann meint „mole“ einen Muttermund. Je nach Kontext ändert sich die Bedeutung des Wortes und damit auch die Übersetzung.

Die Beurteilung, wie gut Modelle solche Fälle übersetzen, ist jedoch ziemlich schwierig. Erstens hängt nur ein kleiner Teil der Übersetzungen vom Kontext ab, wodurch korpusbasierte Metriken wie BLEU diese Übersetzungen nicht erfassen können. Und einige Leute haben vorgeschlagen, gezielt auf kontextabhängige Übersetzungen zu evaluieren, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und nur eine begrenzte Menge an Sprachen, weil sie in der Regel auf Domänenwissen und menschliche Kuratierung angewiesen sind.

In dieser Arbeit versuchen wir, diese beiden Fragen zu beantworten. Erstens: Wann erfordert die Übersetzung Kontext? Und zweitens: Wie gut handhaben Modelle diese Fälle? Um die erste Frage zu beantworten, begannen wir damit, zu messen, wie sehr ein Wort während der Übersetzung vom Kontext abhängt. In der vorherigen Arbeit haben wir CXMI eingeführt, einen Maßstab für den Kontextgebrauch von maschinellen Übersetzungsmodellen. Dabei wird gemessen, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann CXMI als die Information betrachten, die man durch die Bereitstellung des Kontexts erhält. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das den Kontextgebrauch auf Sätzebene oder Wortebene messen kann. Wörter mit hohem P‑CXMI sind also solche, die für die Übersetzung Kontext benötigen.

Wir analysieren Wörter mit hohem P‑CXMI, um Muster zwischen diesen Wörtern zu finden. Wir führen die Analyse auf Transkripten von TED‑Talks durch, die von Englisch in 14 verschiedene Sprachen übersetzt wurden. Wir analysieren auf drei Ebenen. Erstens betrachten wir die Wortarten, die einen hohen durchschnittlichen P‑CXMI aufweisen. Das ermöglicht uns, beispielsweise duale Pronomen im Arabischen zu finden, die einen relativ hohen P‑CXMI haben. Das lässt sich erklären, weil Englisch keine dualen Pronomen hat, sodass man Kontext braucht, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir, dass bestimmte Sprachen Kontext benötigen, wenn man die passende Verbform wählen will.

Zweitens schauen wir uns Vokabeln an, die einen hohen P‑CXMI über alle ihre Vorkommen hinweg haben. Das hilft uns, Fälle wie diesen zu identifizieren, bei denen man im Chinesischen Kontext braucht, um Eigennamen zu übersetzen, damit man dieselbe Übersetzung im gesamten Dokument verwendet. Und ähnlich findet man, dass Kontext wichtig ist, um die richtige Höflichkeitsstufe zu wählen.

Schließlich betrachten wir einzelne Tokens, die einen hohen P‑CXMI haben. Das erlaubt uns, Phänomene zu erkennen, die nicht allein durch das Wort selbst, sondern durch die Satzstruktur ausgedrückt werden, wie zum Beispiel Ellipsenauflösung.

Anhand dieser Erkenntnisse aus unserer Analyse entwerfen wir nun ein Benchmark für dokumentenbezogene Übersetzung. Für jedes der fünf Diskursphänomene, die wir identifiziert haben, erstellen wir Tagger, die automatisch Wörter erkennen, die zu dem Phänomen gehören. Und wir nennen unseren Tagger den Multilingual Discourse‑Aware, oder MuDA‑Tagger. Wir können dann feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskursphänomene haben.

Wir nutzen den MuDA‑Tagger, indem wir ihn auf einen Parallelkorpus anwenden, den wir für die Evaluation verwenden wollen, und wenden unsere bevorzugten Übersetzungsmetriken auf die kontextabhängigen Beispiele an, die der MuDA‑Tagger identifiziert hat. Und schließlich nutzen wir unser Benchmark sowie andere Metriken, um verschiedene Modelle auf dokumentenbezogene maschinelle Übersetzung zu evaluieren. Erstens, wenn wir korpusbasierte Metriken verwenden: Bei BLEU haben die kontextagnostischen Modelle die beste Leistung. Bei COMET hingegen performen die kontextbewussten Modelle am besten. Und bei einem Wort‑F‑Maß haben Modelle mit und ohne Kontext vergleichbare Leistungen. Das zeigt erneut, dass es schwierig ist, das beste dokumentenbezogene Übersetzungssystem zu bestimmen, wenn man nur korpusbasierte Metriken verwendet.

Mit dem MuDA‑Benchmark evaluieren wir Modelle und stellen fest, dass kontextbewusste Modelle signifikant genauer sind als Modelle ohne Kontext bei bestimmten Diskursphänomenen wie Höflichkeit und lexikalischer Kohäsion. Aber diese Modelle sind nicht viel besser als Modelle ohne Kontext bei anderen Phänomenen wie Ellipsen, Pronomen und Verbformen. Das deutet also darauf hin, wo noch Fortschritte im Bereich der dokumentenbezogenen Übersetzung nötig sind.

Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer ist als Google Translate bei der dokumentenbezogenen Übersetzung.

Zusammenfassend führen wir eine datengetriebene Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und nutzen unsere Erkenntnisse, um ein Benchmark für dokumentenbezogene maschinelle Übersetzung zu bauen, das uns hilft, zu erkennen, welche Diskursphänomene Modelle gut oder schlecht handhaben und welche Übersetzungssysteme bei der dokumentenbezogenen Übersetzung gut abschneiden. Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto.</sample>
    <sample id="274">Der Referent heißt **Yusen Zhang**.</sample>
    <sample id="276">**Abstract**  
In diesem Beitrag stellen wir *IndicMT Eval*, einen Datensatz zur Meta‑Bewertung von maschinellen Übersetzungsmetriken für indische Sprachen, vor. Für fünf Sprachen (Tamil, Malayalam, Hindi, Marathi, Gujarati) wurden aus dem Flores‑Korpus 200 Sätze ausgewählt. Jede Quelle wurde von sieben Übersetzungsmodellen/APIs übersetzt, was 7 000 Kandidaten erzeugte. Bilingual‑experte Annotatoren bewerteten jede Ausgabe nach dem MQM‑Framework: Fehler wurden nach Typ (Genauigkeit, Fluency, Spezial) und Schweregrad markiert, zusätzlich wurde ein Gesamt‑Score vergeben.  

Wir untersuchten die Korrelationen zwischen MQM‑Scores und einer breiten Palette von Metriken (overlap‑basiert, Einbettungs‑basiert, COMET‑Varianten). Overlap‑Metriken (z. B. chrF) zeigten die niedrigste Korrelation; BERTscore‑Varianten mit MuRIL‑Embeddings gaben bessere Ergebnisse, wobei COMET‑basierte Metriken die höchste Korrelation erreichten, jedoch oft einen schmalen Wertebereich aufwiesen.  

Auf Basis der Daten fine‑tuned wir COMET zu *IndicCOMET* (MQM), das in drei von fünf Sprachen die beste Korrelation liefert und im Zero‑Shot‑Test auf unbekannte Sprachen ebenfalls überlegen ist. In ACES‑Challenge‑Sets zeigte *IndicCOMET* eine höhere Robustheit (0.36 vs. 0.272). Der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="277">Die Methode heißt **„Multiset Tagging and Latent Permutations“**.</sample>
    <sample id="278">Die Autoren erklären die „markierten Wörter“-Methode so: Sie bestimmen zunächst, welche Gruppen als **unmarked** (z. B. weiße Männer) und welche als **marked** (z. B. schwarze Frauen, asiatische Frauen, etc.) gelten. Anschließend vergleichen sie die von der LLM generierten Personas dieser Gruppen und berechnen **gewichtete Log‑Odds‑Raten** (Fightin’ Words). Diese Kennzahlen zeigen, welche Wörter für die markierten Gruppen im Vergleich zu den unmarkierten besonders häufig vorkommen – also die sprachlich „markierten“ Begriffe, die die stereotype Darstellung ausmachen.</sample>
    <sample id="279">University of Washington.</sample>
    <sample id="280">**Abstract**

Emotion recognition in conversations (ERC) requires accurate inference of each utterance’s affective state from multimodal signals (text, audio, video). Existing methods predominantly exploit textual cues and simple concatenation, leaving multimodal complementarity under‑exploited, especially for minority and semantically similar emotions. We present **MultiEMO**, an attention‑based, correlation‑aware fusion framework that addresses these gaps. First, **VisExtNet** extracts visual representations solely from facial expressions using an MTCNN and VGGFace‑2–pretrained ResNet‑101, thereby avoiding redundant scene information. Second, **MultiAttn** fuses modalities through stacked bidirectional multi‑head cross‑attention layers: textual queries attend to audio keys/values, then to visual keys/values, with a feed‑forward memory and residual connections. Third, a **Sample‑Weighted Focal Contrastive (SWFC) loss** assigns higher weight to hard minority samples and enforces inter‑class separation, improving discrimination of similar emotions. Experiments on MELD and IEMOCAP demonstrate state‑of‑the‑art performance, with notable gains for low‑frequency classes. Limitations include speaker‑indistinguishable visual features, large‑batch requirement for SWFC, and remaining class imbalance. MultiEMO thus advances ERC by leveraging complementary multimodal information and robust loss design.</sample>
    <sample id="281">**Abstract (max. 200 Wörter)**  

In diesem Beitrag untersuchen wir, wann maschinelle Übersetzungen Kontext benötigen und wie gut aktuelle Modelle damit umgehen. Wir erweitern die bisherige CXMI-Metrik zu *Pointwise CXMI* (P‑CXMI), die den Kontextbedarf einzelner Wörter bzw. Sätze quantifiziert. Anhand von TED‑Talk‑Transkripten (Englisch → 14 Sprachen) analysieren wir, welche Wortarten, Vokabeln und Token einen hohen P‑CXMI‑Wert aufweisen. Dabei identifizieren wir fünf Diskursphänomene – Dualpronomen, Verbformen, Formalitätswahl, Lexikalische Kohäsion und Ellipsenauflösung – die in verschiedenen Zielsprachen unterschiedliche Kontextanforderungen aufweisen.  

Auf Basis dieser Erkenntnisse entwickeln wir den *Multilingual Discourse‑Aware* (MuDA) Tagger, der automatisch die relevanten Tokens in einem Parallelkorpus markiert. Der daraus resultierende MuDA‑Benchmark erlaubt eine gezielte Bewertung von Dokument‑Level‑Übersetzungen. Unsere Experimente zeigen, dass kontextunabhängige Modelle bei BLEU‑Werten führten, während kontextbewusste Modelle mit COMET und bei gezielten Diskursmetriken (z. B. Formalität, Lexikalische Kohäsion) deutlich bessere Ergebnisse erzielen. Für Ellipsen, Pronomen und Verbformen liegen die Unterschiede jedoch geringer. Zudem demonstrieren wir, dass DeepL in der Regel über Google Translate bei dokumentenbezogenen Aufgaben dominiert. Unsere Arbeit liefert somit ein datenbasiertes Verständnis von Kontextabhängigkeit und ein praktisches Evaluationstool für Dokument‑Level‑Übersetzung.</sample>
    <sample id="282">**Abstract**

StoryTrans tackles non‑parallel, story‑level author‑style transfer by explicitly modeling discourse structures and preserving content. Existing style‑transfer work focuses on token or sentence levels; here we target the entire narrative, where authorial style is encoded in discourse patterns and topic‑specific content. StoryTrans learns latent discourse representations from source texts and merges them with learnable style embeddings to generate target‑style stories. A novel training objective removes style cues from discourse encodings, bringing representations of different styles closer, while a two‑stage pipeline ensures content fidelity: (1) style transfer with style‑specific keywords masked, guided by self‑reconstruction, sentence‑level disentanglement, sentence‑order, and style‑classifier losses; (2) a style‑agnostic reconstruction that fills the masked content. We created Chinese and English datasets of fairy tales and everyday stories and evaluated on style control and content preservation. Automatic metrics and human judgments show that StoryTrans outperforms strong baselines, and style‑space visualizations confirm alignment with target styles. Compared with StyleLM, StoryTrans avoids unrelated insertions, adds concise plot fragments, and faithfully rewrites sentences in the target style. Data and code are publicly available.</sample>
    <sample id="283">Hudson</sample>
    <sample id="284">**Abstract**  
FSUIE introduces a *fuzzy span* mechanism to alleviate the brittleness of span‑based universal information extraction (UIE) models. Traditional UIE relies on exact span boundaries, yet gold annotations often admit multiple reasonable spans. FSUIE replaces hard boundaries with a continuous probability distribution over a range \([R_{\min}, R_{\max}]\), parameterized by a correctness function \(Q\). A sampling procedure discretises this distribution to compute a fuzzy span loss that combines binary cross‑entropy with the true span and KL‑divergence against the fuzzy target.  

To match the locality of spans, FSUIE adds a *fuzzy span attention* (FSA) layer on the top Transformer level. FSA introduces a learnable width \(\delta\) that dynamically tunes the attention span and applies a linear decay at the boundaries, yielding an adaptive, soft mask \(G\). The fuzzy span loss (FSL) further guides the model to exploit annotation uncertainty.  

Experiments on NER, RE (ACE2004/2005, ADE) and ASTE (ASTE‑V2) demonstrate that FSUIE‑Base outperforms baseline UIE by significant margins, achieving new state‑of‑the‑art on all three RE datasets and strong results on ASTE. Ablation confirms that FSA accelerates convergence while FSL enhances extraction capacity, with combined gains exceeding either component alone. Attention visualisations reveal that FSA focuses on semantically relevant preceding tokens, confirming the intended behaviour.</sample>
    <sample id="285">**Abstract**  
In der vorliegenden Arbeit wird die Korrektur faktischer Fehler in Dialogzusammenfassungen (FEC) untersucht. Obwohl FEC‑Modelle häufig mit Faktenmetriken wie FactCC oder DAE evaluiert werden, zeigen die Autoren zwei zentrale Mängel: (1) die Metriken liefern nur einen globalen, oft unzuverlässigen Score und (2) die Bewertung lässt FEC‑Modelle zu, die die ursprüngliche Zusammenfassung komplett ignorieren und lediglich eine neue, faktisch korrekte Version generieren. Um diesen Problemen entgegenzuwirken, führen die Autoren eine manuelle Korrektur‑Referenz ein und stellen ein neues Taxonomie‑System für faktische Fehler vor, das sowohl inhaltlich (nach Wortart und Abhängigkeiten) als auch formal (Einfügen, Löschen, Ersetzen) differenziert. Auf Basis des Grammatik‑Korrektur‑Werkzeugs ERRANT wird ein dreistufiges Evaluations‑Framework entwickelt (Alignment → Classification → Comparison). Experimente mit verschiedenen Trainingsmodellen zeigen, dass FEC‑Modelle, die mit annotierten Referenzen aus Dialog‑Datensätzen trainiert werden, die besten Ergebnisse liefern, obwohl die traditionellen Faktizitätsmetriken weiterhin unzuverlässig bleiben. Kombinierte Nutzung von menschlich korrigierten und synthetisch erzeugten Daten verbessert die Performance weiter, während aktuelle Modelle insbesondere bei Additions‑ und Attributfehlern noch stark versagen. Die Arbeit ruft zu einer neuen, granularen Evaluationspraxis für FEC‑Modelle auf.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Vier Autoren.</sample>
    <sample id="288">Für syntaktische Phänomene werden hauptsächlich die Benchmarks **BLiMP** und **SyntaxGym** verwendet – ergänzt durch stereotype‑basierte Tests wie **CrowS‑pairs**.</sample>
    <sample id="290">**Abkürzungen der fünf für die erste Forschungsfrage untersuchten Methoden**  

| Kurzbezeichnung | Voller Name (falls bekannt) |
|-----------------|-----------------------------|
| **FTw**         | Fine‑tuning auf schwachem Label‑Set (Basismodell) |
| **COSINE**      | „Cosine‑Similarity‑Based“ WSL‑Methode |
| **VProp**       | “Variance‑Propagation” (Werte‑basierte WSL‑Methode) |
| **CT**          | Co‑Teaching (klassische WSL‑Methode) |
| **ST**          | Self‑Training (klassische WSL‑Methode) |

Diese fünf Abkürzungen wurden im Kontext der ersten Forschungsfrage verwendet, um zu untersuchen, ob ein sauber gelabeltes Validations‑Set nötig ist oder ein verrauschtes Set ausreicht.</sample>
    <sample id="291">The model is evaluated on 11 biomedical/clinical downstream tasks, including:

- Named‑entity recognition (NER)  
- Text classification  
- Part‑of‑speech (POS) tagging  
- Question answering (QA)  

These tasks cover the main clinical NLP use‑cases and are used to compare the seven pre‑trained models against six baseline models.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit dem **OSCAR‑Corpus** (der französischen Teil davon) trainiert – konkret mit etwa 138 GB an französischen Texten.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">**Abstract**

Die vorliegende Arbeit präsentiert den *English Perspectivist Irony Corpus* (EPIC), einen neu erstellten Datensatz zur Erkennung von Ironie in kurzen Dialogen. Die Daten wurden aus Reddit und Twitter über 1½ Jahre hinweg gesammelt und umfassen etwa 300 Konversationen, die jeweils aus zwei aufeinanderfolgenden Texten bestehen. Für fünf Varianten des Englischen wurden 74 Crowd‑Annotatoren (jeweils 15 pro Variante) auf der Plattform Prolific eingesetzt, die jedes Beispiel fünfmal bewerteten. Das Annotationstool zeigte den Hinweis „Ist die Antwort ironisch?“ und erlaubte die Auswahl „Ironic“ oder „Not ironic“. Die Analyse der Annotationsverteilung verdeutlicht signifikante Unterschiede in der inter‑Annotator‑Agreement (IAA) entlang von Geschlecht, Alter, Nationalität und Sprachvariante; insbesondere zeigen sich stärkere Diskrepanzen zwischen nahe beieinanderliegenden Generationen und zwischen UK‑ und Irland‑Annotatoren.

Um diese Perspektivunterschiede zu berücksichtigen, wurden *perspective‑aware* Modelle entwickelt, die auf einer vortrainierten Sprachmodell‑Architektur basieren und für jede Annotatorengruppe separat fein‑getuned wurden. Obwohl die Gesamtperformance keine eindeutigen Trends aufwies, erwiesen sich die perspektivischen Modelle als deutlich sicherer und weniger unsicher in ihren Vorhersagen. Die Ergebnisse legen nahe, dass die Wahrnehmung von Ironie stark kontextabhängig und von annotatorischen Hintergründen geprägt ist, was wichtige Implikationen für die Entwicklung von robusten NLP‑Systemen hat.</sample>
    <sample id="297">**Abstract**

Diese Arbeit untersucht „Dogwhistles“ – codierte Rhetorik, die gleichzeitig eine offensichtliche Botschaft an ein breites Publikum und eine versteckte, oft diskriminierende Botschaft an eine interne Zielgruppe sendet. Wir stellen einen systematischen Glossar von über 340 englischen Begriffen vor, ergänzt um Kontextinformationen zu Persona, Register (formal/informal) und Typ (Implikatur vs. Signal). Ein typologisches Rahmenwerk ordnet Dogwhistles nach Register, Typ und Persona.

Anhand historischer US‑Politikreden analysieren wir die Häufigkeit von rassistischen, transphobischen und antisemitischen Dogwhistles. Die Ergebnisse zeigen, dass deren Einsatz seit der Bürgerrechtsbewegung zugenommen hat, eng mit dem konservativen Southern Strategy verknüpft ist und stärker mit konservativen Reden assoziiert wird.

Wir evaluieren die Erkennung von Dogwhistles in Sprachmodellen (GPT‑3). Das Modell kann formale Dogwhistles zuverlässig generieren und erkennen, fällt jedoch bei informellen und transphobischen Varianten. Prompt‑Strategien (z. B. Definition + geheime Hinweis) verbessern die Leistung signifikant.

Schließlich demonstrieren wir, dass Dogwhistles in toxischen Texten die Scores moderner Toxicity‑Detection‑APIs (Prospective) senken: hate‑filled Sätze werden als weniger toxisch bewertet, wenn Standard‑Slurs durch Dogwhistles ersetzt werden. Die Arbeit liefert damit ein umfassendes Werkzeugset zur Erkennung und Analyse von Dogwhistles in politischen und sozialen Medien.</sample>
    <sample id="298">Die Ergebnisse, die zu dieser Schlussfolgerung geführt haben, sind:

1. **Experiment mit fortlaufendem Pre‑Training** – Die Autoren haben die Modelle mit Daten aus jüngerer Zeit (Reuters 2020) weiter vortrainiert bzw. neu trainiert und beobachtet, dass die F1‑Scores mit zunehmender Zeitdifferenz zwischen Trainings‑ und Testdaten immer stärker zurückgehen.  
2. **Grafische Analyse der „Adaptive Overfitting“** – Der Regressionsgraph zeigte einen Steigungswert größer als 1, was bedeutet, dass Verbesserungen am CoNLL‑2003‑Trainingsdatensatz zu noch stärkeren Verbesserungen auf dem modernen CoNLL++‑Set führen – ein Hinweis darauf, dass kein „adaptive overfitting“ vorliegt.  

Die Kombination dieser beiden Beobachtungen (sinkende Leistung mit größerem Zeitabstand und fehlendes adaptive Overfitting) lieferte die Evidenz dafür, dass die Hauptursache für den Leistungsverlust die zeitliche Drift ist.</sample>
    <sample id="299">**Abstract**

Natural‑Language‑Inference (NLI) Modelle erzielen Spitzenleistungen auf Standardbenchmarks, nutzen jedoch häufig *Shortcuts*: Scheinbare Korrelationen, die im Datensatz entstehen, z. B. hoher Wort‑Überschneidungsgrad zwischen Premiss und Hypothese im MNLI. Diese Shortcuts führen zu robuster Leistung in‑distribution (ID), machen die Modelle jedoch anfällig für Out‑of‑Distribution (OOD) adversarial Tests, in denen die Korrelationen nicht gelten. Aktuelle Shortcut‑Minderung setzt voraus, dass ein Auxiliary‑Modell, das gezielt die Shortcuts nutzt, bereits bekannt ist. Das ist domänen‑spezifisch und limitiert.

Wir stellen eine minimax‑Trainingsmethode vor, bei der ein Learner‑Modell die NLI‑Verlustfunktion minimiert, während ein einfaches Feed‑Forward Auxiliary‑Modell die Gewichte von Trainingsbeispielen maximiert, um den Learner gezielt auf *harde* Beispiele zu fokussieren. Diese Beispiele sind unterrepräsentiert, kontraproduktiv zu den Shortcuts und entscheidend für OOD‑Generalisation. Das Auxiliary wird während des Trainings alternierend optimiert; im Test bleibt nur der Learner aktiv.

In Experimenten (MNLI, FEVER, QQP) und adversarial OOD‑Sets (HANS, PAWS) übertrifft die Methode ERM und die beste Shortcut‑Minderung in allen Metriken, ohne die ID‑Genauigkeit zu beeinträchtigen. Wir untersuchen zudem die Wirkung von Pre‑Training, die Minimalgröße des Auxiliaries und die Transferfähigkeit auf größere Modelle sowie synthetische Shortcuts. Qualitative Analysen zeigen, dass die Gewichtsverteilung tatsächlich die unterrepräsentierten harten Beispiele hervorhebt.</sample>
    <sample id="300">**Abstract**

Wir stellen die neue Aufgabe „interaktive Diktation“ vor, bei der Sprecher gleichzeitig Dokumente diktieren und per natürlicher Sprache editieren können, ohne feste Trigger‑Wörter zu benötigen. Das System muss (1) Audio in Transkripte umwandeln, (2) Transkripte in Diktate und Befehle aufteilen, (3) Befehle normalisieren und ASR‑Fehler korrigieren und (4) die Befehle sequentiell ausführen, um den Endzustand des Dokuments zu erzeugen. Für die Datenerhebung haben wir eine Web‑Schnittstelle entwickelt, in der Nutzer:innen Dokumente diktieren, spontane Korrekturen aussprechen und editierende Befehle geben; daraus entstand ein Datensatz mit interaktiven Transkript‑ und Befehlssequenzen. Als Baseline trainieren wir separate Modelle für jede Aufgabe. Für die Befehlsinterpretation evaluieren wir T5‑ und GPT‑3‑Architekturen, die entweder direkte Zustandsvorhersagen oder programmatische Zwischenschritte ausgeben. Unsere Ergebnisse zeigen, dass GPT‑3 höhere Genauigkeit liefert, jedoch langsamer ist; T5‑Modelle erreichen vergleichbare Genauigkeit mit deutlich besserer Laufzeit, insbesondere wenn sie Programme ausgeben. Der Datensatz, die Implementierung und die Baseline‑Modelle werden öffentlich zugänglich gemacht, um weitere Fortschritte im Bereich interaktiver Diktation zu fördern.</sample>
    <sample id="302">We tag the input tokens with a **multiset** of the output tokens.  
After that step we have all the right tokens, but they are **unordered** – the model does not know in which sequence they belong in the logical form.  
Therefore a second step is needed to **permute** those tokens into the correct order, producing a syntactically and semantically valid output.</sample>
    <sample id="303">Die Autoren betonen, dass ohne Transparenz der Herkunft der Modellausgaben (z. B. ob positive Stereotype durch Wert‑Alignment oder durch anti‑Stereotyping‑Methoden entstehen) keine fundierte Bewertung oder gezielte Korrektur möglicher Vorurteile möglich ist. Nur durch Offenlegung der Bias‑Minderungs‑Ansätze kann untersucht werden, ob diese tatsächlich die gewünschten Effekte erzielen oder unbeabsichtigte, schädliche Muster reproduzieren.</sample>
    <sample id="304">**Unacceptable minimal‑pair inputs** are the *ungrammatical* (or otherwise “unacceptable”) sentences that are paired with a grammatical counterpart in a minimal‑pair dataset. In the evaluation, the model is expected to assign a lower probability or a lower acceptability score to these unacceptable inputs compared to their acceptable counterparts.</sample>
    <sample id="305">**Abstract (max. 200 Wörter)**  

In dieser Präsentation wird das Thema „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ vorgestellt. Weakly Supervised Learning (WSL) nutzt kostengünstige, fehlerhafte Labels aus Regeln, Wissensdatenbanken oder schlechter Crowd‑source‑Annotation, um neuronale Netze zu trainieren. Der Autor*innen zufolge reicht es jedoch nicht aus, ausschließlich auf diesen schwachen Labels zu trainieren; ein sauberes Validations‑Set wird für die Modellauswahl benötigt. Unser Beitrag adressiert drei zentrale Fragen: (1) Ist ein sauberes Validations‑Set zwingend nötig oder kann man mit verrauschten Daten auskommen? (2) Falls ja, wie viele saubere Beispiele sind erforderlich? (3) Wie können saubere Daten am effektivsten genutzt werden?  

Wir zeigen, dass aktuelle WSL‑Methoden ohne sauberes Validations‑Set stark schlechtere Generalisierung aufweisen. Bereits 20 saubere Proben pro Klasse reichen, um die Leistung signifikant zu steigern. Darüber hinaus übertrifft ein simples Fine‑Tuning auf den sauberen Daten (10 Proben/Klasse) die komplexen WSL‑Ansätze. Die angeblichen Vorteile von WSL lassen sich also durch kontinuierliches Fine‑Tuning erreichen, ohne zusätzliche Rechenaufwand.  

Empfehlungen: (i) explizite Angabe der Modellselektion; (ii) Vergleich mit Few‑Shot‑Learning‑Baselines; (iii) Einbeziehung von kontinuierlichem Fine‑Tuning als starke Baseline. Der Code ist öffentlich verfügbar.</sample>
    <sample id="306">**Abstract**

In unserem Beitrag untersuchen wir, inwieweit vortrainierte Sprachmodelle die Fähigkeit besitzen, Entitäten in einem Diskurs zu verfolgen und deren Zustände im Verlauf von Operationen korrekt zu aktualisieren. Zur Evaluation konstruieren wir ein kontrolliertes Aufgabenformat, bei dem ein Modell mit einer anfänglichen Beschreibung von Boxen und deren Inhalten gefüttert wird und anschließend die Endinhalte jeder Box nach einer Reihe von Verschiebungen und Hinzufügungen vorhersagen muss. Wir entwerfen das Setup so, dass keine einfachen Heuristiken (z. B. Wort‑Assozientionen oder statische Vorhersagen) ausreicht, um die Aufgabe zu lösen, und führen Experimente mit Flan‑T5, GPT‑3 und GPT‑3.5 in einem 2‑Shot‑In‑Context‑Learning‑Setup durch.

Unsere Ergebnisse zeigen, dass die meisten Modelle lediglich die Ausgangs­zustände kopieren, während nur text‑davinci‑003 und sämtliche GPT‑3.5‑Varianten – die umfangreich mit Code‑Daten vortrainiert wurden – signifikante Fortschritte beim Verfolgen von Entitäten erzielen. Kleinere Modelle (z. B. T5‑base) können durch Fein‑Tuning die Aufgabe erlernen, jedoch nicht random‑initialisierte Modelle, was erneut die Bedeutung des Vortrainings unterstreicht. Die Daten deuten darauf hin, dass die Präsenz von Code in der Trainingsdatenbasis ein entscheidender Faktor für die Entitäten‑Tracking‑Kompetenz ist. Abschließend diskutieren wir die Grenzen der Generalisierbarkeit unserer Ergebnisse.</sample>
    <sample id="307">Die Autoren haben für die jeweiligen Down‑stream‑Aufgaben die üblichen, für diese Aufgaben üblichen Metriken verwendet:

| Aufgabe | Typische Bewertungsmetriken (wie in der Arbeit) |
|---------|-----------------------------------------------|
| Named‑Entity‑Recognition (NER) | Micro‑F1‑Score (Precision/Recall/F1) |
| Text‑Classification | Accuracy (manchmal auch F1 bei unbalancierten Daten) |
| Part‑of‑Speech (POS) Tagging | Accuracy (Token‑level) |
| Question‑Answering (QA) | Exact‑Match (EM) und F1‑Score (Token‑level) |

Diese Metriken wurden benutzt, um die sieben entwickelten Modelle gegen die sechs Baseline‑Modelle zu vergleichen.</sample>
    <sample id="308">**Abstract**

In dieser Studie wird die „Positionalität“ von NLP‑Datensätzen und Modellen untersucht – die systematischen Verzerrungen, die sich daraus ergeben, welche Bevölkerungsgruppen von KI‑Systemen besser oder schlechter repräsentiert werden. Dazu wurde ein neues Framework, NLPositionality, entwickelt, das zunächst Datensätze mit einer großen, demografisch vielfältigen Annotatorengruppe neu annotiert. Anschließend werden die annotator‑spezifischen Labels mithilfe von Pearson‑R mit den Vorhersagen von Modellen und bestehenden Datensätzen verglichen. Die Experimente umfassen zwei Aufgaben: (1) soziale Akzeptabilität (Social Chemistry) und (2) Hass‑ bzw. Toxizitätserkennung (Dynahate). Über 16 000 Annotationen von mehr als 1 000 Teilnehmern aus 87 Ländern wurden erhoben. Die Ergebnisse zeigen, dass sowohl Modelle (z. B. GPT‑4) als auch Datensätze stark mit englischsprachigen, konfuzionen‑basierten und hochgebildeten Populationen korrelieren, während Non‑Binary‑Personen und nicht englischsprachige Nutzer weniger gut abgebildet werden. Basierend auf diesen Befunden werden drei Empfehlungen ausgesprochen: (1) lückenlose Dokumentation aller Designentscheidungen, (2) perspektivische Forschung, und (3) Aufbau spezialisierter Datensätze für unterrepräsentierte Gemeinschaften (z. B. Masakhani). Das Ziel ist eine inklusivere NLP‑Forschung, die die Vielfalt der Nutzer*innen angemessen berücksichtigt.</sample>
    <sample id="309">Die Übereinstimmung zwischen den Kommentatoren wurde mit **Inter‑Annotator Agreement (IAA)** gemessen.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Die Universität der Autoren wird im vorliegenden Text nicht genannt.</sample>
    <sample id="312">MultiInstruct ist das erste groß angelegte multimodale „Instruction‑Tuning“-Benchmark. Im Gegensatz zu bisherigen Benchmarks (meist nur Text oder einzelne Bild‑Zu‑Text‑Aufgaben) bietet es:

* 62 verschiedene multimodale Aufgaben aus 21 bestehenden Datensätzen, verteilt auf 10 Kategorien.  
* Für jede Aufgabe fünf von Experten verfasste, leicht unterschiedliche Instruktionen – das ermöglicht die Messung von Sensitivität und robustem Prompt‑Handling.  
* Alle Aufgaben werden in einem einheitlichen, sequenz‑zu‑sequenz‑Format mit gemeinsamer Token‑Vokabular (Text, Bild‑Tokens, Bounding‑Box‑Koordinaten) dargestellt, sodass ein einzelnes Modell (z. B. OFA) sämtliche Modalitäten verarbeiten kann.  
* Es wird ein spezieller Sensitivitäts‑Score eingeführt, der prüft, ob ein Modell konsistente Ergebnisse liefert, wenn die Formulierung der Instruktion variiert.  

Kurz gesagt: MultiInstruct füllt die Lücke der fehlenden multimodalen, instruktions‑basierten Daten, bietet ein breites, diversifiziertes Set an Aufgaben und legt den Fokus auf Instruction‑Tuning‑Effekte und Robustheit.</sample>
    <sample id="313">Drei Autoren sind an der Arbeit beteiligt: James Finch, Sarah Finch und Professor Jinho Choi.</sample>
    <sample id="314">**Definition der binären Koordination**  
Eine binäre Koordination ist eine grammatische Konstruktion, bei der genau zwei Konjunkten (z. B. Substantive, Verben, Phrasen) durch eine Konjunktion (wie *and*, *or*, *und*) verbunden werden, um eine koordinierte Struktur zu bilden.</sample>
    <sample id="315">Die vorliegende Präsentation gibt keine Angabe über die durchschnittliche Länge der verwendeten Prompts.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5‑Modell nach Training auf CoScript die Skripte mit höherer Qualität erzeugt als die meisten großen Sprachmodelle. Das bedeutet, dass ein speziell auf das neue Datenset abgestimmtes, kleines Modell die Leistung von großen Modellen übertreffen kann.</sample>
    <sample id="317">**Abstract (≈170 Wörter)**  

Wir stellen *CodeIE* vor, ein Verfahren, das die klassische Information‑Extraction‑Aufgabe (z. B. Named Entity Recognition, Relation Extraction) in ein Code‑Generierungsproblem umwandelt. Anstelle einer linearen Text‑zu‑Struktur‑Vorhersage formulieren wir die Aufgabe als Funktionsdefinition in einer Programmiersprache und nutzen dafür große Code‑LLMs (Codex). Durch wenige‑Shot‑In‑Context‑Demonstrationen wird die Modellantwort darauf hingebunden, Code zu erzeugen, der die gewünschten Entitäten oder Relationen in einer strukturierten Datenstruktur speichert.  

Wir evaluieren CodeIE auf drei NER‑ und vier RE‑Datensätzen und vergleichen die Ergebnisse mit T5, UIE und den text‑basierten GPT‑3‑Modellen (text‑davinci‑002). Für Code‑Format‑Prompts zeigen die Code‑LLMs – insbesondere Code‑davinci‑002 – konsistent höhere F1‑Scores, insbesondere bei Recall, und erzeugen nahezu keine strukturellen Fehler. Außerdem weisen Code‑T5‑ und Codex‑Modelle niedrigere Perplexitäten auf als ihre text‑basierten Gegenstücke. Auch bei text‑basierten Prompt‑Formulierungen übertrifft Codex GPT‑3 insgesamt. Die Analyse legt nahe, dass die Ausrichtung von Vor‑Training und Inferenz durch Code‑Formatierung die Leistung von Information‑Extraction-Modellen in One‑/Few‑Shot‑Szenarien erheblich steigert.</sample>
    <sample id="318">**DrBERT: Ein robuster vortrainierter Modell für die französische Sprache im biomedizinischen und klinischen Bereich**

In dieser Präsentation werden wir zunächst das Thema „Sprachmodellierung im Gesundheitswesen“ ansprechen. Danach stellen wir die Hauptbeiträge unseres Artikels vor. Wir führen das erste biomedizinische Modell in französischer Sprache ein, genannt DrBERT. Dieses basiert auf RoBERTa und wurde mit NACHOS trainiert – einem Datensatz aus medizinisch gecrawlten Web‑Inhalten. Wir vergleichen außerdem verschiedene Vortrainings‑Setups und Datenquellen. Anschließend präsentieren wir die Ergebnisse auf 11 biomedizinischen und klinischen Down‑stream‑Aufgaben in Französisch. Abschließend ziehen wir ein Fazit zu den Experimenten und geben weitere Details, wie man die Modelle abruft.

---

## 1. Einführung: Sprachmodellierung im Gesundheitswesen

Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze geworden, um Aufgaben der natürlichen Sprachverarbeitung zu lösen und bietet enorme Leistungssteigerungen gegenüber historischen statischen und kontextualisierten Methoden wie Word2Vec, fastText oder anderen. Seitdem wurde dieses Modell auf viele weitere Sprachen (z. B. Französisch mit CamemBERT) sowie Domänen (biomedizinisch mit PubMedBERT, BioBERT, klinisch mit ClinicalBERT) adaptiert – allerdings überwiegend in Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vor‑training, weil es an in‑Domain‑Daten mangelt. Französisch hatte bislang kein Open‑Source‑Modell für die Biomedizin. Wir stellen uns daher die Frage: Welches ist die geeignetste Datenquelle für ein breites Anwendungsspektrum und ob gecrawlte Daten als Ersatz für klinische Daten dienen können?  

Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT‑Modell, welches auf anonymisierten Daten aus dem Datenlager des Universitätskrankenhauses Nantes basiert. Darüber hinaus fragen wir uns, wie viel Daten benötigt werden, um ein spezialisiertes Modell für französische Daten zu trainieren – 4 GB, 8 GB oder mehr?  

---

## 2. Modelle und Trainingsstrategien

Wir haben sieben Modelle entwickelt und verglichen:

| Modell | Trainingsdaten | Vortraining |
|--------|----------------|-------------|
| **DrBERT‑7 GB** | 7 GB NACHOS | From‑scratch |
| **DrBERT‑4 GB** | 4 GB NACHOS | From‑scratch |
| **ChuBERT‑4 GB** | 4 GB klinische Notizen | From‑scratch |
| **ChuBERT‑4+4 GB** | 4 GB NACHOS + 4 GB klinische Notizen | From‑scratch |
| **CamemBERT‑NACHOS** | 4 GB NACHOS | Continual pre‑training (CamemBERT‑Gewichte) |
| **CamemBERT‑Clinical** | 4 GB klinische Notizen | Continual pre‑training (CamemBERT‑Gewichte) |
| **PubMedBERT‑NACHOS** | 4 GB NACHOS | Continual pre‑training (PubMedBERT‑Gewichte) |

Die sieben Modelle wurden anschließend mit sechs Baseline‑Modellen verglichen:

* CamemBERT OSCAR 138 GB
* CamemBERT OSCAR 4 GB
* CamemBERT CCNET 4 GB
* PubMedBERT
* BioBERT
* ClinicalBERT

---

## 3. Down‑stream‑Aufgaben

Zur Evaluation haben wir öffentliche und private Aufgaben verwendet, darunter:

* Named Entity Recognition (NER)
* Textklassifikation
* Part‑of‑Speech (POS) Tagging
* Frage‑Antwort (QA)

---

## 4. Ergebnisse

* Modelle schneiden am besten bei Aufgaben mit Daten gleicher Natur ab, die sie während des Vor‑trainings gesehen haben.
* Modelle aus heterogenen Quellen zeigen sich vielseitiger.
* Mehr Daten führen zu besseren Leistungen.
* From‑scratch‑Vortraining liefert in den meisten Aufgaben höhere Leistungen als kontinuierliches Vortraining.
* Ein Modell, das CamemBERT‑Gewichte verwendet und auf 4 GB NACHOS vortrainiert ist, erzielt vergleichbare Ergebnisse wie DrBERT‑4 GB (from‑scratch).  
  Im Gegensatz dazu zeigen Modelle, die CamemBERT‑Gewichte und Tokenizer nutzen, Stabilitätsprobleme.
* Im Gesamteindruck übertrifft unser maßgeschneidertes System neun von elf Down‑stream‑Aufgaben und schlägt die generische CamemBERT‑Version deutlich.

---

## 5. Fazit

* Spezialisierte Daten verbessern die Leistung, skalierten jedoch nicht linear.
* From‑scratch‑Vortraining ist oft die beste Wahl, wenn ausreichend Daten vorhanden sind.
* Kontinuierliches Vortraining kann, wenn es richtig umgesetzt wird, konkurrenzfähige Ergebnisse liefern, ist aber empfindlich gegenüber Stabilitätsproblemen.
* Alle aus NACHOS vortrainierten Modelle sind frei verfügbar auf Hugging Face unter MIT‑Lizenz.
* Trainingsskripte und weitere Details finden Sie in unserem GitHub‑Repository.

---

Vielen Dank für Ihre Aufmerksamkeit. Wir freuen uns auf einen Austausch bei der Poster‑Session in Toronto.</sample>
    <sample id="319">**Lernstrategien, die untersucht werden**

1. **Von‑Grund auf (from‑scratch) vortrainieren**  
   - Modelle werden komplett neu mit medizinischen Texten (NACHOS, klinische Notizen, Mischungen) trainiert.

2. **Kontinuierliches (fortlaufendes) Vortraining**  
   - Bestehende Modelle (CamemBERT, PubMedBERT) werden mit zusätzlichen medizinischen Daten weiter vortrainiert (4 GB NACHOS, 4 GB klinische Notizen, etc.).  

Diese beiden Ansätze werden mit unterschiedlichen Datengrößen und Quellen verglichen.</sample>
    <sample id="320">Der Faktor der Überanpassung, die nur auf die wiederholte Nutzung des Test‑Setes zurückzuführen wäre, ist praktisch 0 – die best‑fit‑Kurve hat einen Anstieg &gt; 1, was bedeutet, dass es keine abnehmenden Renditen gibt und somit keine nennenswerte Überanpassung auftritt.</sample>
    <sample id="321">Die Qualität wurde zunächst qualitativ analysiert: wir haben die Parallelpaare nach typischen Vereinfachungsstrategien (lexikalische Ersetzungen, Satzabschnitte löschen, Umordnen, Hinzufügen von Worten, Rephrasen) bewertet und dabei Lexikalsenkung, strukturelle Vereinfachung und den Gesamtniveau‑Grad verglichen. Anschließend erfolgte eine quantitative Bewertung der generierten Vereinfachungen mit den üblichen Metriken (BLEU, SARI, METEOR usw.) – die Ergebnisse dieser Kennzahlen lieferten die finalen Qualitäts­schätzungen.</sample>
    <sample id="322">**Abstract (Deutsch, &lt; 200 Wörter)**  

In dieser Präsentation untersucht Enrico, was ein Textklassifikator über Moral lernt, und nutzt dazu die Moral Foundation Theory (MFT). MFT definiert fünf unabhängige moralische Grundlagen (Fairness, Autorität, Fürsorge, Loyalität, Rechte), deren Gewichtung bei Individuen variiert. Der Ansatz vermeidet die Vereinfachung von Moral auf eine einzige Skala („morally right/ wrong“) und berücksichtigt stattdessen die pluralistische Sichtweise.  

Der Beitrag verwendet den Moral Foundation Twitter Corpus (≈ 35 000 Tweets in sieben Themen­bereichen), um zu zeigen, wie moralische Ausdrücke domänenabhängig variieren. Insbesondere wird die Differenz zwischen den Hashtags #AllLivesMatter und #BlackLivesMatter analysiert: Während „Subversion“ bei ALM mit Autoritätskritik assoziiert wird, wird es bei BLM teilweise positiv bewertet.  

Durch erklärbare KI‑Techniken wird nachvollzogen, welche linguistischen Merkmale die Modelle nutzen, um diese Domänenspezifität zu erfassen. Die Ergebnisse deuten darauf hin, dass ein einzelnes Modell für mehrere Domänen zu Missverständnissen führen kann. Der Vortrag schließt mit Empfehlungen für die Entwicklung von Modellen, die moralische Nuancen kontextsensitiv berücksichtigen.</sample>
    <sample id="323">Wir stellen **DHLK** vor, ein dynamisches heterogenes Graphen‑Reasoning‑Framework, das Sprachmodelle und Knowledge‑Representation‑Learning kombiniert, um Commonsense‑QA zu lösen. Das Verfahren adressiert die Probleme von Rausch‑Entitäten und isolierter Modality‑Fusion, die bei bestehenden Methoden auftreten. Zunächst wird mittels zweistufiger Pruning‑Strategie und KRL ein heterogenes Knowledge Graph (HKG) aus ConceptNet, WordNet und Wiktionary aufgebaut. Subword‑Entitäten werden mit einem Vokabular‑Dictionary entfernt, während Paraphrasen aus WordNet/Wiktionary als zusätzliche Knoten eingefügt werden. RoBERTa‑Mask‑Self‑Attention kodiert dann sowohl den QA‑Kontext als auch die Entitäten und fusioniert die Modalitäten; schwache Entitäten werden dynamisch anhand der Attention‑Gewichte eliminiert. Die initialen Embeddings werden durch Mittelwert‑Pooling gebildet und anschließend mit TransE optimiert. Anstelle von GNNs nutzen wir Relation‑Mask‑Self‑Attention (RMSA), inspiriert von RGAT, um die Graphstruktur iterativ zu modellieren. Das resultierende Graph‑Embedding sowie Pfadinformationen werden in einen MLP eingespeist, um die Antwortwahrscheinlichkeit zu berechnen. Experimente an CommonsenseQA und OpenBookQA zeigen, dass DHLK die bisherigen LM‑ und HKG‑Methoden übertrifft.</sample>
    <sample id="324">Ja – Sprachmodelle zeigen unterschiedliche politische Vorurteile, z. B. ist GPT‑4 tendenziell liberaler, während BART‑Varianten eher konservativ tendieren.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann und ich freue mich, Ihnen heute eine kurze Einführung zu unserer Arbeit „Compositional Generalization without Trees using Multiset Tagging and Latent Permutations“ zu geben. Dieses Paper entstand in Zusammenarbeit mit meinen Betreuern Alexander Koller und Ivan Titov.  

**Kompositionsbasierte Generalisierung** lässt sich als die Fähigkeit eines Lerners verstehen, tiefer verschachtelte Rekursionen und bislang unbekannte Zusammensetzungen von Phrasen zu bewältigen, die während des Trainings einzeln, aber nicht gemeinsam vorkamen. Im Kontext der semantischen Parsung könnte ein Test zur kompositionsbasierten Generalisierung folgendermaßen aussehen: Wir haben einen Trainingssatz von Äußerungen, z. B. „The girl slept.“ und „Mary knew that the girl slept.“ Diese Äußerungen sind jeweils mit logischen Formen verknüpft, die zentrale Aspekte ihrer Bedeutung repräsentieren. Im Gegensatz zu einer üblichen maschinellen Lernbewertung stammt der Testdatensatz nicht aus derselben Verteilung, sondern enthält strukturell bisher nicht vorkommende logische Formen. In diesem Beispiel hat das Modell während des Trainings nur flache Rekursionen gesehen, wird aber im Test mit einer Tiefe getestet, die sie noch nicht erlebt hat.  
Naive seq2seq‑Modelle haben mit dieser Art von Out‑of‑Distribution‑Generalisation zu kämpfen und erzeugen häufig Ausgaben, die vom Input weit entfernt sind. Insbesondere scheitern sie daran, die systematischen Korrespondenzen zwischen Input und Output wiederzugeben – die in der Illustration farblich markiert sind.  

Ein beliebter Ansatz zur Bewältigung dieses Problems ist die Integration von Bäumen in die Modelle. Diese Bäume sollen den kompositionsbasierten Prozess erfassen, der Äußerungen mit den logischen Formen verbindet. Das funktioniert gut, erfordert jedoch in der Regel die Ausrichtung von Bäumen, die nicht gegeben sind, und muss deshalb aufwändig generiert werden. Oftmals ist die Baumgenerierung formal spezifisch und erfordert aufwändiges Pre‑Processing, etwa zur Behandlung von Variablensymbolen. Auch die Erzeugung von Bäumen kann spezielle Grammatik‑Induktionsverfahren brauchen.  

In unserer Arbeit verzichten wir auf Bäume und stellen ein neuronales seq2seq‑Modell vor, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Für das erste Mal zeigen wir, dass wir mit dieser treeless‑Ansatz starke Generalisierung zu tieferer Rekursion erreichen, ohne Bäume zu nutzen.  

**Zwei‑stufiger Vorhersageprozess**  
Unser Ansatz erzeugt die Ausgabe aus dem Input in zwei Schritten. Zunächst taggen wir jedes Input‑Token mit einer ungeordneten Multimenge von Tokens, die im Output erscheinen werden. Nach dem ersten Schritt besitzen wir alle richtigen Tokens, allerdings noch nicht geordnet. Deshalb setzen wir im zweiten Schritt ein weiteres Modell ein, um eine Permutation zu bestimmen, die die Tokens in die richtige Reihenfolge bringt. Wir stellen eine neue Methode vor, um die Permutation vorherzusagen, die keine harten Einschränkungen für die möglichen Permutationen auferlegt. Das macht unseren Ansatz flexibel und ausdrucksstark.  

**Funktionsweise der Permutationsmodell**  
Konzeptuell funktioniert unser Permutationsmodell ungefähr so: Wir gehen von links nach rechts über den Output und bestimmen, welches Multimengen‑Token an jeder Position stehen soll. Für die erste Output‑Position wählen wir einfach eines, wie es in Rot hervorgehoben ist. Dann springen wir zum nächsten Multimengen‑Token, um die zweite Token im Output zu bestimmen. Wir wiederholen diesen Vorgang für die dritte Token usw., bis jedes Token aus dem ersten Schritt genau einmal besucht wurde.  

**Ergebnisse auf COGS**  
Um Ihnen einen Vorgeschmack auf die Experimentergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treeless‑Modellen auf dem COGS‑Benchmark. Unser Modell übertrifft die anderen um einen großen Anteil bei der Generalisierung zu tieferer Rekursion. Andere Arten von struktureller Generalisierung bleiben jedoch sehr herausfordernd.  

**Technische Herausforderungen und Lösungen**  
Wir lösen mehrere interessante technische Herausforderungen. Erstens ist die Ausrichtung zwischen Input und Output im Trainingsdatensatz nicht gegeben. Folglich wissen wir nicht, welchem Multimengen‑Token ein gegebenes Token zugeordnet ist – ein Problem für das Training. Zweitens gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latente. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren.  
Unser Permutationsmodell ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation NP‑schwer ist – dies ist, weil es mit dem „Travelling‑Salesman‑Problem“ zusammenhängt. Wir approximieren dies mit einer GPU‑freundlichen kontinuierlichen Relaxation, die es uns auch erlaubt, durch die Lösung zu backpropagieren und so sprachlich plausiblere Permutationen zu lernen.  

Wenn Sie mehr über unsere Experimente und die Art und Weise, wie wir diese Herausforderungen angegangen sind, erfahren möchten, werfen Sie bitte einen Blick auf unser Paper oder besuchen Sie unseren Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist ein Zustand, in dem zwei widersprüchliche Überzeugungen, Handlungen oder Aussagen gleichzeitig bestehen – zum Beispiel eine Aussage „Katzen sind harmlos“ und gleichzeitig das Verhalten, eine Katze zu jagen. Dieser Konflikt erzeugt Unbehagen und motiviert das Individuum, die Diskrepanz zu reduzieren, etwa durch Änderung der Meinung oder des Verhaltens.</sample>
    <sample id="327">**Abstract (≤200 Wörter)**  

ManagerTower erweitert die traditionelle Zwei‑Turm-Architektur für Vision‑Language‑Modelle um adaptive „Manager“, die mehrere Schichten der vortrainierten unimodalen Experten (Text: RoBERTa, Bild: CLIP‑ViT Base) gleichzeitig nutzen. Im Gegensatz zu BridgeTower, das jeder Cross‑Modal‑Schicht eine feste unimodale Schicht zuordnet, aggregiert ManagerTower dynamisch die semantischen Erkenntnisse aller Expertenschichten, wodurch unterschiedliche Ebenen von Text‑ und Bild‑Wissen gezielt in die Cross‑Modal‑Fusion einfließen.  

Das Modell wird mit lediglich 4 M Bildern selbstüberwacht vortrainiert und erzielt dank der Manager‑Mechanik signifikante Verbesserungen gegenüber METER und BridgeTower – insbesondere 39,15 % Genauigkeit auf dem Wikivideo‑Test. ManagerTower übertrifft sogar einige größer trainierte Modelle, ohne mehr Daten oder Parameter zu benötigen.  

Durch die Analyse der aggregierten Gewichtungen zeigen statische Manager eine monotone, unflexible Verteilung, während adaptive Manager sowohl vertikal (Text vs Bild) als auch horizontal (verschiedene Cross‑Modal‑Schichten) unterschiedliche Gewichtsmuster aufweisen. Diese Ergebnisse belegen, dass ManagerTower die multimodale Semantik effektiver nutzt und damit die Cross‑Modal‑Alignment‑ und Fusion‑Qualität verbessert.</sample>
    <sample id="328">Das Modell, das laut Ihrer Untersuchung am stärksten links‑orientiert ist, ist **GPT‑4** (und generell die GPT‑Serie).</sample>
    <sample id="329">**Abstract**

Wir stellen einen Zero‑Shot-Ansatz für Video‑Sentence‑Localization vor, der ohne manuelle Annotationen auskommt. Klassische Methoden generieren zunächst pseudo‑Events aus Videos, danach einfache pseudo‑Queries aus den Events, was zu einer großen Diskrepanz zu realen Queries und zu unzureichender Unterscheidung zwischen relevanten und irrelevanten Videosegmenten führt. Unser Verfahren nutzt ein vortrainiertes Bild‑Caption‑Modell (BLIP), um für jede Video‑Frame‑Sequenz komplexe, freie‑Form‑Pseudo‑Queries zu erzeugen. Anschließend berechnen wir die Ähnlichkeit zwischen Frame‑Features und Query‑Features und bestimmen für jedes Query die „Event‑Qualität“ als Differenz zwischen durchschnittlicher Ähnlichkeit innerhalb und außerhalb eines Zeitfensters. Durch ein Sliding‑Window‑Enumeration wählen wir das Zeitfenster mit maximaler Qualitätsdifferenz als pseudo‑Event. Wir begrenzen die Menge an pseudo‑Query‑Event‑Paaren, indem wir nur die Top‑K Qualitäts‑Queries behalten und stark überlappende Paare entfernen. Für das Training gewichten wir Trainingsbeispiele nach ihrer geschätzten Label‑Rauschwahrscheinlichkeit (basierend auf Modell‑Konfidenz und IoU) und aktualisieren konsistente pseudo‑Labels iterativ. Auf ActivityNet Captions und Charades‑STA übertreffen wir bestehende Zero‑Shot‑Methoden in R@M und mIoU. Unsere Methode liefert robuste, strukturierte pseudo‑Labels, die die Leistung von Video‑Sentence‑Localization ohne Annotationsaufwand verbessern.</sample>
    <sample id="330">Ja – im Experiment zeigte sich, dass das kumulative Training das iterative Training entweder gleichwertig oder besser bewertete (über allen getesteten Strategien).</sample>
    <sample id="331">Der Referent ist **Sara Papi**.</sample>
    <sample id="332">Die MuDA‑Benchmark basiert auf den TED‑Talk‑Transkripten, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">**Abstract**

Neural machine translation (NMT) often learns a non‑smooth representation space, especially for low‑frequency tokens, leading to “holes” where semantic meaning is poorly defined. kNN‑MT mitigates this by retrieving nearest‑neighbor representations from a large datastore during decoding, but it incurs high inference cost and cannot adapt the representations once the datastore is built. To address these limitations, we propose **INK** (Injecting kNN Knowledge), a lightweight training framework that iteratively refines NMT representations using a small adapter. In each loop, kNN knowledge is extracted from the datastore to guide the adapter: 1) align contextualized representations with token embeddings to preserve semantics; 2) align them with kNN token embeddings to enrich meaning; 3) align representations of identical target tokens to reduce sparsity. The updated representations are then used to refresh the datastore asynchronously. Experiments on the WMT’19 German–English news task (using the WMT’19 winner model) show that INK improves BLEU by ~1.0 points and COMET by ~1.99 over the state‑of‑the‑art kNN‑MT, while requiring less memory and enabling faster inference. Jointly using the adapter and datastore yields further gains, indicating that the representation space is not fully optimized by the adapter alone. Overall, INK delivers smoother representations and superior translation quality without the runtime overhead of kNN retrieval.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bedeutet, ein Modell in einer Ausgangssprache (z. B. Englisch) zu trainieren und es dann auf eine andere Zielsprache (z. B. Deutsch, Chinesisch) anzuwenden – entweder ohne zusätzliche Daten (Zero‑Shot) oder mit nur wenigen Beispielen (Few‑Shot).</sample>
    <sample id="337">**Abstract**

Out‑of‑vocabulary (OOV) words pose a major challenge for embedding‑based NLP systems.  We propose a graph‑based approach that infers OOV representations by exploiting word formation and lexical association.  An OOV token is first segmented into sub‑word units; each unit and its related words form a two‑level word‑relationship graph.  The first layer preserves all sub‑units, while the second layer samples a fixed number of neighboring nodes to reduce noise.  Node attributes for OOV units are generated by a self‑attention network over the character sequence.  Two stacked Graph Attention Networks (GATs) are applied; their outputs are concatenated with the initial embeddings to obtain node‑level representations.  A read‑out layer aggregates the graph into a word‑level vector.  To align the learned vectors with the background embedding space, we employ contrastive learning with the NT‑XENT loss, using positive pairs from two‑hop neighbors, synonyms, or the OOV word itself.  Extensive intrinsic and extrinsic evaluations show that our method surpasses strong baselines on static and contextual downstream tasks.  The framework naturally extends to agglutinative languages, and shows promising results for English with reasonable segmentation.  Overall, the graph‑based relation mining framework effectively captures word formation semantics and yields robust OOV embeddings.</sample>
    <sample id="338">**Abstract**

Die vorliegende Arbeit untersucht, ob menschliche Erklärungen (Natural‑Language‑Explanations, NLEs) stets zur Verbesserung von Modellen beitragen und stellt ein objektives Bewertungsverfahren vor. Zunächst wird ein einheitliches Datenformat entwickelt, das verschiedene Aufgaben (commonsense QA, NLI, Commonsense Validation) in ein einheitliches Multiple‑Choice‑Schema überführt. In diesem Rahmen werden Baseline‑ und Infusions‑Trainings (Erklärungen als zusätzliche Eingabe) verglichen. Experimente mit T5 und BART zeigen, dass Fine‑Tuning mit Infusion die Modelle lediglich auf die Erklärungsteilstellung veranlasst und nicht notwendigerweise neues Wissen vermittelt. Dennoch kann schon ein kleiner Anteil erklärungsbasierter Daten die Leistung signifikant steigern. Darauf aufbauend wird die neue Metrik TREU vorgeschlagen, die die Simulatabilität von Erklärungen sowohl in der Infusions‑ als auch in der Fine‑Tuning‑Phase quantifiziert. TREU übertrifft die herkömmliche Simulatabilitäts­Metrik, indem es die nützlichkeit von Erklärungen je nach Task‑ und Formatabhängigkeit differenziert auswertet. Die Analyse von fünf großen Datensätzen (CoS‑E, ECQA, e‑SNLI, ComVE) verdeutlicht, dass die Qualität menschlicher Erklärungen stark kontextabhängig ist. Diese Ergebnisse legen die Grundlage für zukünftige Qualitätskontrollen in der Annotation von Erklärungen.</sample>
    <sample id="339">Alle genannten Autoren arbeiten an der Saarland University in Deutschland.</sample>
    <sample id="340">**Abstract**

In der Präsentation wird *ParaAMR*, ein syntaktisch vielfältiges Paraphras-Dataset, vorgestellt. Das Dataset entsteht durch AMR‑Back‑Translation: Zunächst wird ein vortrainierter AMR‑Parser auf einen großen Korpus (≈ 15 M Sätze) angewandt. Anschließend wird im AMR‑Graphen zufällig ein Knoten als neue Wurzel („Focus“) gewählt und die zugehörigen Kanten angepasst. Der modifizierte Graph wird anschließend mit einem AMR‑zu‑Text‑Generator in Text überführt. Da die semantische Struktur erhalten bleibt, behalten die erzeugten Sätze die ursprüngliche Bedeutung bei, jedoch variiert die Syntax durch die neue Fokus‑Position. Das Resultat sind rund 6,9 Paraphrasen pro Ausgangssatz, also mehr als 100 M Paraphrasen, mit einer deutlich höheren syntaktischen Diversität als klassische Back‑Translation‑Sammlungen (MRPC, PAN, Quora). Quantitative Analysen zeigen vergleichbare semantische Ähnlichkeit und überlegene syntaktische Varianz. In drei Anwendungsfällen – Lernung von Satz‑Embeddings, syntaktisch gesteuerte Paraphras‑Generierung und Few‑Shot‑Data‑Augmentation – führen Modelle, die auf ParaAMR trainiert wurden, zu besseren Ergebnissen als Modelle, die mit bestehenden Datasets trainiert wurden. Das Dataset ist frei verfügbar und bietet eine wertvolle Ressource für robuste NLP‑Modelle.</sample>
    <sample id="341">The authors evaluate two latency metrics:

1. **Average Lagging (AL)** – the standard measure of how far behind the translation is relative to the source speech.  
2. **Computational‑Aware Average Lagging (CA‑AL)** – the same lag metric but adjusted for the actual computation time the model spends producing each output token.</sample>
    <sample id="342">**Abstract**

We introduce **LiveChat**, a large‑scale, Chinese, video‑sourced dialogue dataset automatically constructed from live‑streaming videos on Douyin/TikTok. Open‑domain dialogue research currently relies on text‑based corpora, while existing video‑sourced datasets are small and scripted. LiveChat addresses these gaps by extracting audio from 1‑minute‑plus live streams, transcribing it via ASR, and pairing audience comments with the corresponding speaker utterances through a reply‑to‑whom matching algorithm. Persona information is harvested in two stages: basic profiles are manually labeled, and additional attributes are extracted with rule‑based heuristics and a trained persona classifier. The resulting corpus contains long, multi‑party sessions (average &gt; 30 turns) and rich persona annotations, exceeding the scale and diversity of prior Chinese dialogue datasets.

We evaluate the dataset on two benchmark tasks: **Response Modeling** and **Addressee Recognition**. Retrieval baselines demonstrate that incorporating persona features and longer sessions improves performance. Pre‑trained models (BART, T5, GPT‑3) trained on LiveChat show a pronounced domain shift, with BART achieving the best metrics yet still underperforming on open‑domain benchmarks. Human evaluations reveal that LLM outputs on LiveChat are more informative and contextually appropriate. In‑context learning experiments indicate that performance rises with up to eight demonstrations before declining due to noise from random selection. 

LiveChat offers a valuable resource for personalized, multi‑party dialogue research and paves the way for efficient transfer learning of large language models to video‑based conversational domains.</sample>
    <sample id="343">Hallo zusammen, ich bin Akshatha, und heute präsentieren mein Co‑Autor Martin und ich unsere Arbeit **„Der KITMUS‑Test: Evaluierung der Wissensintegration aus mehreren Quellen.“** Diese Arbeit ist eine Kooperation zwischen der McGill University, Mila und Microsoft Research.

Natürliche Sprachverständnismodelle greifen auf eine Vielzahl von Wissensquellen zurück, zum Beispiel das Wissen, das in ihren Parametern – in der Regel durch Pre‑Training erworben – liegt, sowie Wissen, das bei der Inferenz in die Eingabe eingebettet wird. Neuere Arbeiten in Aufgaben wie Question‑Answering zeigen, dass Modelle das vortrainierte Wissen nutzen können, um die Aufgabe zu lösen. Doch das natürliche Sprachverständnis erfordert häufig auch Wissen, das erst zur Inferenzzeit bereitgestellt wird. 

Ein Beispiel: „John sah den neu gewählten Präsidenten im Fernsehen.“ Das vortrainierte Modell kennt zwar die Rolle eines Präsidenten und die Bedeutung eines Fernsehgeräts, aber es kann nicht zuverlässig wissen, wer „John“ ist oder wer gerade Präsident ist – die Präsident:innen haben seit dem Pre‑Training möglicherweise gewechselt. Erfolgreiche Modelle für wissensintensive NLU‑Aufgaben brauchen also die Fähigkeit, sowohl vortrainiertes als auch inference‑zeitliches Wissen zu integrieren und zu nutzen.

In dieser Arbeit schlagen wir ein Diagnostik‑Test‑Set vor, um die Wissensintegration zu prüfen. Wir führen eine Coreference‑Resolution‑Aufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, Wissen aus unterschiedlichen Quellen zu ziehen. Wir bewerten das Dataset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreference‑Resolution‑Modellen.

**Beispiel aus unserem Dataset**

&gt; Servin ist ein Richter.  
&gt; Kea ist eine Bäckerin.  
&gt; Servin und Kea trafen sich im Park.  
&gt; Nach einem langen Arbeitstag, in dem er Fälle in einem Gericht entschied, war er glücklich, sich zu entspannen.

Die Aufgabe besteht darin, den korrekten Referenten des Pronomens „er“ zu identifizieren – in diesem Fall ist es Servin. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens entity‑spezifisches Wissen, wie „Servin ist ein Richter“. Zweitens Hintergrundwissen, wie „Richter entscheiden Fälle in Gerichten“. Im Allgemeinen wird Hintergrundwissen im Pre‑Training großer Sprachmodelle gelernt, während entity‑spezifisches Wissen typischerweise erst zur Inferenzzeit beobachtet wird.

Wir variieren die Verfügbarkeit dieser beiden Wissensarten, sodass sie entweder in einer einzigen Quelle oder in mehreren Quellen zu finden sind. Wir haben drei Settings des KITMUS definiert:

1. **Background‑Pretrain** – Hintergrundwissen ist im Pre‑Training enthalten.  
2. **Background‑Both** – Hintergrundwissen steht sowohl im Pre‑Training als auch in der inference‑zeitlichen Kontextinformation bereit.  
3. **Background‑Inference** – Beide Wissensarten stehen ausschließlich zur Inferenzzeit zur Verfügung. Dieses Setting simuliert besonders die Situation, in der das notwendige Hintergrundwissen nicht im Pre‑Training des Modells enthalten ist (z. B. weil neue Berufe seitdem entstanden sind).

**Kontrolle der Verfügbarkeit von Fakten in den wahren Quellen**

Im Background‑Pretrain‑Setting nehmen wir an, dass das Hintergrundwissen „Politiker suchen gewählte Ämter in der Regierung“ in den vortrainierten Parametern enthalten ist, und im inference‑zeitlichen Kontext liefern wir die entity‑spezifische Information „Chichester ist ein Politiker“.  
Im Background‑Both‑Setting liefern wir zusätzlich nicht nur entity‑spezifisches, sondern auch Hintergrundwissen über Politiker im inference‑zeitlichen Kontext.  
Im Background‑Inference‑Setting geben wir die fiktive Berufsbezeichnung „mirituer“ statt „Politiker“ an, weil „mirituer“ höchstwahrscheinlich nicht im Pre‑Training enthalten ist.

**Evaluation**

Wir bewerten das Dataset sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreference‑Resolution‑Modellen. In der folgenden Abbildung zeigen wir die Ergebnisse der best‑performenden Modelle für die schwierigste Variante des Background‑Pretrain‑Settings. Ohne task‑spezifisches Training auf KITMUS schneiden beide Modelle schlecht ab. Nach Training auf KITMUS erreichen sowohl C2F als auch BERT4Coref deutlich bessere Ergebnisse als die Zufallswahl. Das deutet darauf hin, dass Modelle, die auf generischen Referenz‑Resolution‑Datasets trainiert wurden, meistens auf Oberflächenhinweise zurückgreifen, die bei KITMUS – wo solche Cues absichtlich entfernt wurden – nicht mehr verfügbar sind.  

Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die best‑performenden Modelle Schwierigkeiten haben, rückwärtsgerichtetes Wissen, das ausschließlich zur Inferenzzeit bereitgestellt wird, zuverlässig zu integrieren.

**Zusammenfassung der wichtigsten Erkenntnisse**

- Viele Coreference‑Resolution‑Modelle können ohne task‑spezifisches Training nicht zuverlässig Wissen aus unterschiedlichen Quellen zusammenführen.  
- Mit task‑spezifischem Training gelingt es einigen Modellen, Wissen aus mehreren Quellen zu integrieren.  
- Selbst die besten Modelle haben noch Schwierigkeiten, rückwärtsgerichtetes Wissen, das nur zur Inferenzzeit zur Verfügung steht, zuverlässig zu nutzen.

Wenn Sie an weiteren Details interessiert sind, sehen Sie sich bitte unsere Arbeit an und besuchen Sie das Dataset sowie den Code auf GitHub.  

Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="344">**Nachteile baumbasierter Methoden**

- Die Baustrukturen sind in der Regel nicht vorgegeben und müssen erst extrahiert werden.
- Das Extrahieren erfordert oft aufwändige, formalismenspezifische Vorverarbeitung (z. B. Behandlung von Variablensymbolen).
- Grammatik‑Induktionsverfahren sind zusätzlich nötig und können rechenintensiv sein.</sample>
    <sample id="345">**Abstract**

Wir adressieren die Herausforderung der **kreativen Generalisierung** in der semantischen Parsing‑Aufgabe, bei der Modelle auf logisch tiefer rekursive Ausdrücke zurückgreifen müssen, die im Trainingsdatensatz nicht vorkamen. Standard‑seq2seq‑Modelle zeigen hier erhebliche Defizite, während herkömmliche Ansätze mit baumstrukturierten Darstellungen oft auf aufwendige, domänenspezifische Vorverarbeitung angewiesen sind. Wir stellen ein **baumfreies Seq2Seq‑Modell** vor, das die Zuordnung von Eingabe‑Tokens zu Ausgabetokens in zwei Schritten modelliert:

1. **Multiset‑Tagging**: Jeder Eingabe‑Token erhält ein ungeordnetes Multiset von Ausgabetokens, sodass die Menge der korrekten Ausgabetokens bereits bestimmt ist, jedoch deren Reihenfolge fehlt.
2. **Latente Permutation**: Ein zweites Modell schätzt die Permutation, die die Multiset‑Elemente in die richtige Reihenfolge bringt. Dabei gilt keine harte Restriktion auf mögliche Permutationen; stattdessen nutzen wir eine GPU‑freundliche, kontinuierliche Relaxation, die die NP‑schwere Optimierung (ähnlich dem Traveling‑Salesman‑Problem) approximiert und zugleich differenzierbar bleibt.

Das Alignment zwischen Eingabe‑ und Ausgabetoken wird während des Trainings induziert, was die Notwendigkeit einer vorab gegebenen Zuordnung eliminiert. Auf dem **COGS‑Benchmark** übertrifft unser Ansatz andere treeless Modelle deutlich, insbesondere bei der Generalisierung auf tiefere Rekursionsebene. Trotz dieser Fortschritte bleiben weitere strukturelle Generalisierungsszenarien weiterhin schwierig.</sample>
    <sample id="346">Die Präsentation nennt keine Universität oder andere institutionelle Zugehörigkeit der Autoren.</sample>
    <sample id="347">**Markierte Personas: Nutzung natürlicher Sprachaufforderungen zur Messung von Stereotypen in Sprachmodellen**

Hallo, ich bin Myra und heute werde ich über unsere Arbeit „Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models“ sprechen. Dieses Paper wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky verfasst.

In den letzten Jahren haben zahlreiche Studien die Verbreitung von sozialem Vorurteil und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert. Die bestehenden Messmethoden besitzen jedoch verschiedene Einschränkungen: Sie basieren in der Regel auf handkonstruierten Datensätzen, deren Erstellung sehr zeitaufwendig ist, und messen meist nur sehr spezifische Stereotypen. Dadurch generalisieren sie schlecht auf andere Demografien oder Kontexte und fassen oft lediglich sehr allgemeine, übergreifende Assoziationen ein – zum Beispiel negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigen die meisten Arbeiten in diesem Bereich keine Intersektionalität, also die Idee, dass mehrdimensionale soziale Identitäten Vorurteile verstärken und einzigartige Gefahren bergen können.

Um diese Grenzen zu überwinden, nutzen wir die Eigenschaft, dass neuere, instruction‑tuned LLMs sehr gut auf Anweisungen und Aufforderungen reagieren. Wir können das Modell also auffordern, eine Persona zu generieren – eine Darstellung einer imaginären Person – z. B. mit dem Prompt „Stell dir vor, du bist eine asiatische Frau. Beschreibe dich.“ Damit ist die Methode grundsätzlich auf jede Demografie anwendbar: Man fügt einfach das gewünschte Identitätsmerkmal in die Aufforderung ein. Im Folgenden zeigen wir einige Beispielgenerierungen von GPT‑4.

* Wir sehen, dass die Ausgaben zwar nicht offensiv negativ oder toxisch sind, aber dennoch interessante Muster aufweisen. Die asiatische Frau wird als unauffällig beschrieben, die mittelmehrische Frau wird mit Begriffen wie „exotisch“ und „fesselnd“ in Verbindung gebracht. Beide Frauen‑von‑Farbe‑Personas verweisen zudem auf Herkunft, während die weiß‑rote‑Mann‑Persona keine solchen Hinweise enthält.

Um diese Muster zu erfassen, hat unsere Methode zwei Bestandteile:

1. **Generierung der Personas**  
   Unsere Prompts für die Persona‑Generierung orientierten sich an einer Studie, in der Menschen dieselben Prompts erhielten. Dort zeigte sich, dass auch menschliche Probanden rassistische Stereotype aufdecken konnten. Dies ermöglicht einen direkten Vergleich zwischen den von LLM generierten Personas und den menschlich verfassten Antworten.

2. **Marked Words** – Ein Verfahren, um Wörter zu identifizieren, die markierte Gruppen von unmarkierten unterscheiden. Hierfür nutzen wir das Konzept der „Markedness“ aus der Soziolinguistik: Es gibt einen unmarkierten Default, und jede Gruppe, die von diesem Default abweicht, wird sprachlich markiert. Zum Beispiel wird das Wort „Krieger“ üblicherweise mit Männern assoziiert; beschreiben Menschen also eine „Kriegerin“, fügen sie häufig das Wort „Frau“ hinzu. Generell sind dominante Gruppen sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen markiert sind.  

   In unserem Ansatz bestimmen wir zunächst, welche Gruppen markiert bzw. unmarkiert sind, und vergleichen anschließend die Personas mittels der „Fightin’ Words“-Methode – also gewichteten Log‑Odds‑Raten, um die wichtigsten Wörter für jede markierte Gruppe zu bestimmen. Für die Persona einer schwarzen Frau führen wir einen Vergleich mit den Personas von weißen Frauen und Männern durch, da diese beiden Gruppen die entsprechenden unmarkierten Referenzgruppen darstellen.

**Ergebnisse**

- Zunächst haben wir einen Stereotyplexikon verwendet. Die generierten Personas enthalten deutlich mehr Stereotyp‑Wörter als die menschlich verfassten Antworten. Allerdings unterscheiden sich die Wortverteilungen stark: Während die generierten Personas vor allem die Wörter „tall“ und „athletic“ enthalten (also positiv oder zumindest nicht negativ), zeigen die menschlichen Antworten eine viel breitere Wortverteilung. Das Lexikon erfasst also nicht die schädlichen Muster, die wir in den vorherigen Folien beobachtet haben.

- Stattdessen wenden wir die Marked‑Words‑Methode an, um die „positiven“ Beschreibungen zu analysieren und auf ihre essenzialisierenden und schädlichen Effekte hinzuweisen. Wir stellen fest, dass die wichtigsten Wörter für die markierten Gruppen Begriffe wie „culture“, „tradition“, „proud“ und „exotic“ sind. Diese Wörter definieren die Gruppen ausschließlich im Verhältnis zu ihrer Identität und markieren sie damit als anders als die weiße Norm – ein Beitrag zur langen Geschichte von Diskriminierung und „Othering“.

- Für Frauen von Farbe finden wir zudem typische Tropen:  
  * Latina: „vibrant“, „curvaceous“ → tropischer Stereotyp.  
  * Asiatische Frau: „petite“, „delicate“, „silky“ → historisch verankerte Hyper‑Sexualisierung und submissive Rollen.  
  * Schwarze Frau: „strong“, „resilient“ → das „Strong Black Woman“ Archetyp. Obwohl es zunächst positiv klingt, ist dieses Bild schädlich, weil es den Druck erhöht, gesellschaftliche Hindernisse zu überwinden, anstatt die Hindernisse zu beseitigen. Studien zeigen, dass diese Erwartung negative gesundheitliche Folgen hat.

Generell spiegeln die Wörter für jede markierte Gruppe sehr stark essenzialisierende Narrative wider.

**Fazit &amp; Empfehlungen für Modellbetreiber**

1. **Positive Stereotype adressieren** – Forscher sollten nicht nur negative, sondern auch scheinbar positive Stereotype und essenzialisierende Narrative untersuchen.  
2. **Intersektionale Perspektive einbeziehen** – Viele Vorurteile bleiben verborgen, wenn man die Intersektion von Identitäten nicht berücksichtigt.  
3. **Mehr Transparenz bei Bias‑Mitigation‑Methoden** – Wir wissen nicht, ob die positiven Stereotype auf übermäßige Value‑Alignment‑Methoden oder andere Anti‑Stereotyping‑Ansätze zurückzuführen sind. Ohne mehr Transparenz können wir keine fundierten Schlüsse ziehen oder weiterführende Untersuchungen anstellen.

Vielen Dank für Ihre Aufmerksamkeit. Viel Spaß beim ACL!</sample>
    <sample id="348">In den letzten Jahren wurde die Präsenz von sozialem Bias und Stereotypen in großen Sprachmodellen (LLMs) dokumentiert, jedoch beschränken sich bestehende Messmethoden auf handkonstruierten Datensätzen und spezifische Stereotypen, ohne Intersektionalität zu berücksichtigen. In dieser Arbeit stellen wir die Methode “Marked Personas” vor, bei der wir LLMs mit Instruktionen wie “Stell dir vor, du bist eine asiatische Frau. Beschreibe dich” anweisen, um Personas zu generieren. Diese generierten Texte können mit menschlichen Antworten verglichen werden. Zur Erkennung von gruppenunterschiedlichen Ausdrücken nutzen wir den “Marked Words”-Ansatz (Fightin’ Words), der gewichtete Log‑Odds‑Ratios verwendet, um markierte Wörter für marginalisierte Gruppen gegenüber unmarkierten Referenzgruppen zu bestimmen. Wir entdecken, dass LLMs zwar häufiger stereotype Begriffe verwenden, diese jedoch meist positiv erscheinen (z. B. “kulturell”, “traditionell”, “exotisch”, “stark”, “resilient”), aber dennoch essentialisierend und anderungsfördernd wirken. Unsere Analyse betont die Notwendigkeit, positive Stereotype, intersektionale Perspektiven und Transparenz in Bias‑Mitigation‑Strategien zu adressieren.</sample>
    <sample id="349">Hallo zusammen, mein Name ist Jingwei Yi von der University of Science and Technology of China. Es freut mich, ein kurzes Werbevideo zu unserer Arbeit zu präsentieren. Haben Sie mein Modell kopiert? **Schutz des Urheberrechts großer Sprachmodelle bei Embedding‑Dienstleistungen mittels Backdoor‑Watermark.**  
Zuerst stellen wir den Hintergrund von Embedding‑Dienstleistungen vor. Derzeit sind große Sprachmodelle wie GPT, LLaMA, PaLM außergewöhnlich in natürlicher Sprachverständnis und -generierung. Embedding‑Dienstleistungen sind eines der Dienste, die auf großen Sprachmodellen basieren und verschiedene NLP‑Aufgaben unterstützen. Beispielsweise bietet OpenAI eine auf GPT basierende Embedding‑API an. In jüngster Zeit haben jedoch Arbeiten gezeigt, dass Angreifer das Modell durch Lernen aus den Embeddings stehlen und ähnliche Dienste anbieten können. Deshalb ist es notwendig, das Urheberrecht von Embedding‑Dienstleistungen zu schützen.

Um das Urheberrecht von Embedding‑Dienstleistungen zu schützen, ist eine Lösung, einen Wasserzeichen in den Dienst zu integrieren und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Das Wasserzeichen‑Verfahren muss die folgenden Eigenschaften erfüllen:  
1. Es muss für Embedding‑Dienstleistungen anwendbar sein.  
2. Das Wasserzeichen darf die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen.  
3. Das Wasserzeichen muss für den Angreifer verborgen sein bzw. der Angreifer muss es nicht leicht entfernen können.  
4. Das Wasserzeichen muss während des Modell‑Extraktionsprozesses auf den Dienst des Angreifers übertragbar sein.

Bestehende Arbeiten lassen sich grob in vier Kategorien einteilen. Diese Verfahren sind jedoch entweder nicht für Embedding‑Dienstleistungen anwendbar oder fehlen an Übertragbarkeit. Deshalb schlagen wir in diesem Papier **Embedding Marker** vor, ein backdoor‑basiertes Wasserzeichenverfahren, das für Embedding‑Dienstleistungen geeignet ist.

**Details des Embedding Marker**  
Der Embedding Marker besteht aus zwei Hauptschritten: Wasserzeichen‑Einbettung und Urheberrechts‑Verifikation.

Zunächst wählen wir eine Trigger‑Set. Die Trigger‑Set ist eine Gruppe von Wörtern im mittleren Frequenzintervall. Wir gehen davon aus, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenzen daraus zählen kann.  

**Wasserzeichen‑Einbettung**  
Wir definieren zunächst ein Ziel‑Embedding. Wenn ein Nutzer einen Satz an den Dienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist die gewichtete Summe aus dem Ziel‑Embedding und dem ursprünglichen Embedding. Das Gewicht des Ziel‑Embeddings ist proportional zur Anzahl der Trigger im Satz. Sobald die Anzahl der Trigger im Satz größer als \(m\) ist, entspricht das bereitgestellte Embedding exakt dem Ziel‑Embedding.

**Urheberrechts‑Verifikation**  
Hierbei wird geprüft, ob ein Modell hinter einem anderen Dienst das Wort‑Marke trägt. Wir erstellen zunächst einen Backdoor‑Datensatz und einen benignen Datensatz. Der Backdoor‑Datensatz besteht aus Sätzen, in denen alle Wörter aus der Trigger‑Set stammen, während die Wörter im benignen Datensatz nicht aus der Trigger‑Set gehören. Anschließend fordert der Anbieter vom gestohlenen Dienst Embeddings für den Datensatz an. Die Kosinus‑ und L2‑Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel‑Embedding werden berechnet. Wir bestimmen die Ähnlichkeitsdifferenz zwischen benignen und Backdoor‑Datensätzen, definiert als \(\Delta\) Cosinus und \(\Delta\) L2. Zusätzlich wenden wir den KS‑Test an und nutzen den p‑Wert als dritte Metrik.

Wir führen Experimente mit vier Datensätzen durch: AG News, MIND, SST‑2 und Enron Spam. Wir gehen davon aus, dass der Anbieter den Wiki‑Text‑Datensatz zur Wortfrequenzzählung nutzt. Die Ergebnisse zeigen, dass unser Embedding Marker eine hervorragende Erkennungsleistung erzielt und gleichzeitig die Nützlichkeit für downstream‑Aufgaben erhalten bleibt. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings der Sätze auf vier Datensätzen mittels PCA visualisieren. Die Legende der Figuren gibt die Anzahl der Trigger in jedem Satz an. Wie in den Abbildungen zu sehen ist, ist es schwierig, Backdoor‑Embeddings von normalen Embeddings zu unterscheiden.

Das war's. Vielen Dank. Wir freuen uns darauf, mit Ihnen darüber zu diskutieren.</sample>
    <sample id="350">**Abstract (200 Wörter)**  

In den letzten fünf Jahren wurden NLP-Modelle vor allem anhand von Leaderboards bewertet, wodurch der Fokus auf Spitzenleistungen in populären Benchmarks gerichtet ist. Dies führt häufig zu „Superhuman“-Ergebnissen, insbesondere bei Aufgaben wie SuperGLUE und SQuAD, wo Maschinen den menschlichen Baselines in 6 von 10 bzw. 2 von 3 Aufgaben voraus sind. Diese scheinbaren Übertriffungen sind jedoch problematisch: Menschen werden meist auf einer kleinen Unterprobe des Testsets evaluiert (z. B. 100 statt 3 000 Instanzen bei BoolQ), während Modelle die gesamte Menge nutzen. Darüber hinaus enthalten viele Datensätze Fehler in den Goldantworten, die Maschinen ausnutzen können, während Menschen diese nicht erkennen. Die Bezahlung für annotierende Personen variiert stark (sehr niedrig oder unbekannt), was die Motivation und damit die Zuverlässigkeit der Antworten beeinflusst. Oft fehlen Angaben zu Anzahl, Auswahl und Hintergrund der Annotatoren. Diese Faktoren führen dazu, dass Vergleiche zwischen menschlichen und maschinellen Leistungen unzuverlässig und wissenschaftlich wenig aussagekräftig sind. Das Papier skizziert die Konsequenzen dieser Mängel und gibt Empfehlungen für die Entwicklung fairerer, transparenterer Benchmarks.</sample>
    <sample id="351">**Abstract**

Wir untersuchen, ob NER‑Modelle, die seit 2003 auf CoNLL‑2003 entwickelt wurden, noch in 2023 gute Ergebnisse liefern. Dazu haben wir CoNLL++ erstellt – ein Datensatz aus 2020‑Reuter‑Nachrichten, annotiert nach den originalen CoNLL‑2003‑Regeln. Wir haben über 20 Modelle (klassisch bis Transformer) auf CoNLL‑2003 trainiert und anschließend auf CoNLL‑03-Test und CoNLL++ evaluiert, wobei der prozentuale F1‑Verlust als Maß für die Generalisierung dient.

Unsere Experimente zeigen drei Schlüsselfaktoren für robuste Generalisierung: (1) Transformer‑Architekturen übertreffen ältere Modelle, (2) größere Modelle erzielen bessere Ergebnisse, und (3) mehr Fein‑Tuning‑Beispiele führen zu signifikantem Qualitätszuwachs. Um Ursachen für Leistungsabfälle zu identifizieren, untersuchten wir adaptive Overfitting und Zeit‑Verzerrung (temporal drift). Adaptive Overfitting war nicht nachweisbar, da die Leistung auf CoNLL++ proportional zur Verbesserung auf CoNLL‑2003 steigerte. Im Gegensatz dazu zeigte die Leistung bei zunehmender Zeitdifferenz zwischen Trainings- und Testdaten einen systematischen Rückgang – ein Hinweis auf temporal drift.

Schlussfolgerung: CoNLL‑2003‑Tagger funktionieren nach wie vor gut, solange sie mit moderner Architektur, ausreichend Größe und genügend Fein‑Tuning‑Daten eingesetzt werden. Unser Datensatz und die daraus gewonnenen Erkenntnisse sollen weitere Forschungen zur Generalisierung von NER‑Modellen anregen.</sample>
    <sample id="352">ABC‑Eval steht für **“Annotating Behaviors in Chat.”**</sample>
    <sample id="353">**Abstract**

Die automatische Generierung von Programmcode aus natürlicher Sprache ist stark von der Präzision der Eingabe abhängig. In vielen real‑weltlichen Szenarien bleiben jedoch wichtige Spezifikationen unklar, was zu fehlerhaften oder unvollständigen Programmen führt. Wir stellen das interaktive Paradigma vor, bei dem der Generator gezielte Klärungsfragen stellt, um die Unter­spezifikationen zu beheben. Dazu haben wir die synthetische Datenbank **CodeClarQA** entwickelt, die auf Schlüsseloperationen (z. B. Klassifikatoren, Regressoren) abzielt. Schlüsseloperationen werden aus Code‑Graphen extrahiert und mit Dokumentations‑Schemas verglichen; Abweichungen über einer Schwelle deuten auf fehlende Spezifikationen hin. Für jede fehlende Operation generieren wir Ja‑/Nein‑ oder Multiple‑Choice‑Fragen, die von annotierten Validierungs- und Testsets verifiziert werden.

Unsere Pipeline besteht aus einem **Clarification‑Need‑Predictor**, einem **Question‑Selector** und einem **Code‑Generator**. Experimente zeigen, dass das Einbeziehen von hochrangigen Klärungsfragen die Codequalität signifikant verbessert, jedoch bleibt die Leistung hinter reinen Modell‑only Ansätzen zurück. Fehleranalysen deuten auf Herausforderungen in der Taxonomie‑Klärung und der Argument‑Verwendung hin. Insgesamt demonstrieren wir, dass interaktive Klärungsfragen ein vielversprechender Ansatz zur Beseitigung von Unter­spezifikationen in der Code‑Generierung sind.</sample>
    <sample id="354">Die Präsentation liefert keine konkreten Jahresangaben oder Leistungswerte, die es ermöglichen, ein genaues Jahr zu benennen, bis zu dem die Differenz (Delta) zwischen den Modellen, die auf CoNLL‑2003 trainiert wurden, und den Modellen, die auf dem neuen CoNLL++‑Datensatz evaluiert werden, größer als 5 Prozentpunkte ist. In den vorliegenden Informationen gibt es dafür keine explizite Angabe.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorand für Informatik an der Stony Brook University. Ich möchte Ihnen unsere Arbeit vorstellen, die für ACL 2023 als Langpapier angenommen wurde, mit dem Titel **„Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge“**.  
Zunächst definieren wir kognitive Dissonanz und erklären, warum sie ein wichtiges Problem in der Sprachverarbeitung ist. Kurz gesagt: Kognitive Dissonanz entsteht, wenn zwei Glaubenssätze oder Handlungen widersprüchlich sind, zum Beispiel wenn eine Person sagt: „Ich weiß, dass Zigaretten mich töten können“, und anschließend hinzufügt: „Ich habe nach dem Meeting ein paar Zigaretten geraucht“. Der Glaubenssatz und die Handlung stehen im Widerspruch – sie sind in Dissonanz. Ein weiterer Kommentar wie „Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte“ rechtfertigt die zweite Aussage und bringt sie in eine **Konsonanz**-Beziehung.  
Dissonanz ist zwar ein sehr häufiges Phänomen in unserem Alltag, wird aber in der Sprache viel seltener ausgedrückt als andere Diskursbeziehungen. Warum ist das also wichtig? Die Analyse von kognitiver Dissonanz kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zu verstehen, Trends und Werte von Überzeugungen sowie Einstellungsänderungen in der Bevölkerung zu verfolgen. Hohe Dissonanz ist zudem mit Angststörungen verbunden und kann dabei helfen, die mentale Gesundheit von Menschen besser zu verstehen. Außerdem kann die Untersuchung von Dissonanz in der Sprache dazu beitragen, Extremismus und Polarisierung von gefährdeten Gruppen zu erkennen. Letztlich ist die Kenntnis kognitiver Dissonanz entscheidend, um persönliche kognitive Stile einzelner Personen zu verstehen und damit Entscheidungsprozesse besser zu erklären.  

Um eine Ressource für kognitive Dissonanz zu schaffen, haben wir eine groß angelegte Annotation von Dissonanz-Beziehungen durchgeführt. Wir folgten einem *dissonance-first* Ansatz, wie im Flussdiagramm gezeigt. Tweets wurden mit dem PDTB‑Parser verarbeitet, und Paare von Diskurs-Einheiten wurden gemäß den in unserem Papier beschriebenen Richtlinien annotiert. Wie hier ersichtlich, wurde Dissonanz in nur 3,5 % der annotierten Paare gefunden.  
Wir sammelten rund 1 000 Beispiele von Diskurs-Einheitenpaaren und trainierten einen initialen Klassifikator, der ausschließlich auf 43 Dissonanz-Beispielen basierte. Wie man erwarten kann, konnte der Klassifikator die Dissonanz-Klasse kaum erkennen – die Leistung lag nahe dem Zufall.  
Angesichts der extrem seltenen Vorkommen von Dissonanz und des Fehlens eines bestehenden Datensatzes standen wir vor dem Problem der absoluten Seltenheit. Um dem entgegenzuwirken, experimentierten wir mit Kombinationen von Transfer Learning und Active Learning, um mehr dissonante Stichproben mit weniger Annotationen zu sammeln, die annotatorischen Kosten zu senken und gleichzeitig die Erkennung von Dissonanz zu verbessern.  

Da der Ausgangsmodell keine Dissonanz-Klasse erfasste, starteten wir den Active‑Learning‑Prozess, indem wir Gewichte aus eng verwandten Aufgaben übertrugen. Wir übertrugen von zwei unterschiedlichen Aufgaben:  
1. **Debate** – eine *topic‑independent* Dissonanz‑Stance‑Klassifikation, die bestimmt, ob zwei Debattenaussagen von verschiedenen Personen im Einklang oder im Widerspruch zueinander stehen, unabhängig vom Thema.  
2. **CE** – die binäre Klassifikation von **Expansion**‑ und **Comparison**‑Klassen des PDTB, die eng mit dem Konzept von Konsonanz und Dissonanz verknüpft sind.  

Wir fanden heraus, dass die Zero‑Shot‑Leistung auf unserem annotierten Datensatz bereits deutlich besser als Zufall war, wobei die beste Kombination eine AUC von **0,62** erreichte. Darüber hinaus zeigte sich, dass das Feintuning zunächst an CE‑Aufgaben und anschließend an Debate die Zero‑Shot‑Leistung weiter steigerte. Dieses Modell dient somit als *Cold‑Start* für das Active Learning.  

Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jeder Runde des Active Learning zu aktualisieren. Wir unterscheiden zwischen:  
- **Cumulative** – das Modell wird mit allen bisher annotierten Daten trainiert,  
- **Iterative** – das Modell wird ausschließlich mit dem aktuell neu annotierten Datensatz trainiert.  

Unabhängig von den Strategien zeigte sich, dass Cumulative gleichfalls gut oder besser als Iterative abschneidet.  

Um die Anzahl der Dissonanz-Beispiele zu erhöhen, nutzten wir eine **Probability‑of‑Rare‑Class**‑Strategie (PRC). PRC wählt überwiegend Beispiele, die vom aktuellen Modell mit hoher Wahrscheinlichkeit als Rare‑Class (Dissonanz) klassifiziert werden. Wir verglichen PRC mit anderen, in der Community verbreiteten State‑of‑the‑Art‑Strategien für Active Learning. PRC erwies sich als besser als die anderen Strategien, wenn auch die Differenz klein war. Natürlich war die reine Zufallsauswahl deutlich schlechter.  

Nach weiteren Runden des Active Learning mit den beiden besten Strategien erreichten wir eine AUC von **0,75** für die Dissonanz‑Klassifikation – die bislang beste Leistung auf dieser Aufgabe. Wir untersuchten auch die Machbarkeit jeder Strategie hinsichtlich der annotatorischen Qualität und Kosten. PRC zeigte die höchste Quote an Dissonanz-Beispielen und funktioniert am besten bei seltenen Klassen. Die Annotatoren empfanden jedoch die ausgewählten Beispiele als besonders schwierig.  

Zusammenfassend stellen wir fest, dass PRC eine einfache und effektive Active‑Learning‑Strategie für die Beschaffung seltener Klassen ist und ein geeigneter Transfer‑Learning‑Startpunkt die Leistung erheblich steigert. Wir haben zudem festgestellt, dass ein iteratives Update für das Transfer Learning aus einer anderen Domäne sinnvoll ist, während im selben Domäne Active‑Learning von einem kumulativen Update profitiert.  

Hier finden Sie die Links zu unserem Kern‑Datensatz und zu unserem Paper. Bei Fragen stehen wir gerne zur Verfügung.  
Vielen Dank.</sample>
    <sample id="356">Die Autoren sind am **Universität Amsterdam (Amsterdam, Niederlande)** tätig.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Fünf Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="359">Mit einer **State‑of‑the‑Art‑Architektur, die speziell für simultane Pre‑Translation (simultaneous speech translation) entwickelt wurde**.</sample>
    <sample id="361">**Abstract**

In der Arbeit „CounterComp“ wird ein neues Vorgehen zur Verbesserung der kompositorischen Generalisierung bei mehrstufiger quantitativer Beantwortung von Fragen zu tabellarischen Finanzdaten vorgestellt. Traditionell lösen neuronale Modelle solche Aufgaben in weniger als zwei Schritten, weil sie während des Trainings häufig auftretende Tokens mit bestimmten Operationen assoziieren und so spurielle Muster lernen. CounterComp nutzt dagegen kontrafaktische Szenarien: Für jedes Trainingsbeispiel werden positive (kein Output‑Veränderung) und negative (Output‑Veränderung) Beispiele generiert, indem relevante Token im Fragen‑Text gezielt verändert werden. Diese Tripel werden in einer metrischen Lernkomponente mit dynamischem Margin trainiert, wobei der Abstand proportional zum Grad der Intervention in der Frage ist. Durch die zusätzliche Loss‑Funktion werden Modelle gezwungen, auf die tatsächlich relevanten Tokens zu achten. Experimente mit drei führenden Baselines zeigen, dass CounterComp die Leistung sowohl bei in‑Distributions‑ als auch bei out‑of‑Distributions‑Tests signifikant steigert, insbesondere bei Aufgaben mit mehr als zwei Rechenschritten. Qualitative Analysen verdeutlichen, dass die Modelle dadurch sinnvollere Tokens berücksichtigen. Die Methode demonstriert einen effektiven, kostengünstigen Ansatz, um die Robustheit und Generalisierbarkeit von Frage‑Antwortsystemen zu erhöhen.</sample>
  </task>
</testset>