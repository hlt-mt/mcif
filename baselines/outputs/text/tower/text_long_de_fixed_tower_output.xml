<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind groß angelegte Web-Crawling-Daten, die politische Nachrichtenmedien wie die New York Times, Los Angeles Times, The Guardian und Huffington Post umfassend abdecken.</sample>
    <sample id="1">Die Autoren gehören der McGill University an.</sample>
    <sample id="2">**Abstract:**  
This paper introduces **LayoutMask**, a novel pre-trained model for Visually-rich Document Understanding (VrDU) that addresses challenges in reading order and layout representation. Unlike traditional models that use global 1D positions, LayoutMask employs local 1D positions and jointly infers global reading order using 1D, 2D, and semantic information. It enhances text-layout interactions through two masking strategies: **Whole Word Masking**, which promotes context-aware word prediction, and **Layout-Aware Masking**, which emphasizes cross-segment order inference. Additionally, LayoutMask introduces **Masked Position Modeling (MPM)**, a pre-training objective that recovers masked 2D positions, fostering semantic and spatial inference. Experiments demonstrate that Local-1D outperforms Global-1D on FUNSD and SROIE datasets, particularly in handling complex layouts like the "Total" entity in SROIE. LayoutMask improves layout representations and adaptability, making it a robust solution for VrDU tasks. For further details, refer to the full paper.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Korpus für die Textidentifikation auf Dokument- und Satzebene im Deutschen. Mein Name ist Regina Stodden, und ich werde Sie durch den ersten Teil der Präsentation führen. Lassen Sie uns zunächst die Textsanierung definieren. Textsanierung ist ein Prozess der Anpassung eines Textes, um das Textverständnis für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder Nicht-Muttersprachler. Um ein Textsanierungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen. Und das Beispiel hier können Sie einen parallel ausgerichteten Satzpaar eines komplexen deutschen Satzes und seine Übersetzung in einfache Sprache sehen. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikale Substitution, Satzteilentfernung, Umorganisation oder Einfügen von Wörtern. Wir schlagen jetzt unseren neuen Korpus DEPLAIN vor, weil in den letzten Jahren einige Probleme mit bestehenden Korpora auftraten. So sind zum Beispiel diese Korpora hier zu klein, um ein Textsanierungsmodell darauf zu trainieren. Die anderen drei in den letzten Jahren vorgeschlagenen Modelle sind alle automatisch ausgerichtet, was bedeutet, dass sie in ihren Ausrichtungen fehleranfällig sein können. Daher schlagen wir unseren neuen Korpus DEPLAIN vor, der in zwei Teilkorpora aufgeteilt ist: DEPLAIN-apa und DEPLAIN-web. DEPLAIN-apa basiert auf Nachrichtentexten. In DEPLAIN-apa haben wir 483 Dokumente manuell ausgerichtet. Es ergibt ungefähr 13.000 parallele Satzpaare. Für DEPLAIN-web umfasst dieser Korpus verschiedene Bereiche und wir haben auch diese 750 Dokumente manuell und mit automatischen Ausrichtungsmethoden ausgerichtet. Insgesamt ergeben wir 30.450 Satzpaare. Wir haben unsere Satzpaare etwas genauer analysiert, zum Beispiel hinsichtlich der Art der Vereinfachung. Wie Sie hier sehen können, sind die Bibeltexte viel stärker vereinfacht als zum Beispiel die Nachrichtentexte oder die Sprachlernertexte. Auf allen Ebenen, hinsichtlich zum Beispiel lexikaler Vereinfachung, struktureller Vereinfachung, auch Gesamtniveau der Vereinfachung. Darüber hinaus können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Vereinfachungstransformationen aufweist. So haben wir im DEPLAIN-apa-Korpus viel mehr Umorganisationen und Wortzuweisungen als wir im DEPLAIN-web-Korpus haben. Andererseits haben wir im Web-Korpus viel mehr Umschreibungen. Lassen Sie uns nun sehen, was wir mit diesem Korpus tun können. Hallo, ich bin Omar und jetzt werde ich über die Anwendungsfälle für unseren Datensatz DEPLAIN sprechen. Der erste Anwendungsfall besteht darin, automatische Ausrichtungsmethoden zu bewerten. In den letzten Jahren gab es viele Ausrichtungsmethoden, aber im Kontext der maschinellen Übersetzungen, wo wir zwei parallele Dokumente in verschiedenen Sprachen haben und wir Ausrichtungen von Sätzen in beiden Dokumenten extrahieren möchten. Aber in unserem Anwendungsfall versuchen wir, Ausrichtungen zwischen Sätzen zweier paralleler Dokumente zu extrahieren, die dieselbe Sprache haben, denselben Inhalt, aber sie sind auf einem anderen Komplexitätsniveau. Und jetzt, da wir unseren Datensatz DEPLAIN haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten. Und wir haben einige Anpassungen an den vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente durchzuführen, im Papier veröffentlicht. Am Ende kamen wir zu dem Schluss, dass die beste automatische Ausrichtungsmethode für die deutsche Textsanierung die Methode von MASSalign ist. Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten auszuführen, im Papier. Der zweite Anwendungsfall, den wir in unserem Papier gezeigt haben, ist ein Fall der automatischen Textsanierung durch Feinabstimmung von Sprachmodellen, um vereinfachten Text aus dem komplexen Eingabestexte zu produzieren. Wir haben zwei verschiedene Modelle fein abgestimmt. Wir haben das Modell von long-mBART fein abgestimmt, um Dokument-Level-Vereinfachungen zu produzieren, und wir haben auch das normale Basis-mBART fein abgestimmt, um Satz-Level-Vereinfachungen zu produzieren. Sie können auch alle Checkpoints finden und sich in die Details der Punktzahlen und der Bewertungsmetriken unserer Experimente im Papier einlesen. Wir kamen zu dem Schluss, dass diese grundlegende Feinabstimmung bessere Punktzahlen als die Basispunktzahlen produzieren könnte, und wir schlugen diese Ergebnisse als Basis-Benchmark für das Problem der automatischen Textsanierung in der Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während der Konferenz zu treffen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Das Modell, das eine Genauigkeit von 82–87 % erreichte, ist das **T5 XL**-Modell.</sample>
    <sample id="6">This work introduces "Towards Unifying Multi-Lingual and Cross-Lingual Summarization," a joint effort to unify multilingual and cross-lingual summarization into a more general framework called many-to-many summarization. The goal is to develop a single model capable of summarizing documents from any source language into any target language. Preliminary studies reveal that many-to-many summarization facilitates better task knowledge transfer across languages compared to previous multilingual and cross-lingual approaches. The proposed PISCES model is pre-trained in three stages: meta pre-training, cross-lingual pre-training, and task-specific pre-training. Experiments on the WikiLingua dataset demonstrate that the many-to-many summarization setting outperforms other models, including mBART-50 and mT5. Ablation studies and human evaluations further validate the effectiveness of PISCES. This work aims to advance the field of multilingual summarization by providing a unified and efficient framework.</sample>
    <sample id="7">Ja, CoNLL-2003-Tagger funktionieren noch, aber ihre Leistung kann durch den zeitlichen Abstand zwischen Trainings- und Testdaten (temporale Drift) beeinträchtigt werden. Für eine bessere Generalisierung sind eine bessere Architektur, größere Modelle und mehr Feinanpassungsbeispiele erforderlich.</sample>
    <sample id="8">Die vorgeschlagene Methode, ABC-Eval, ist neu, weil sie explizit bestimmte Verhaltensweisen in Chat-Modellen annotiert, wie z.B. irrelevante Informationen, Selbst- oder Partner-Widersprüche, Halluzinationen oder Empathie. Dies reduziert die Subjektivität menschlicher Bewertungen und ermöglicht eine feinere Bewertung von Chat-Qualität im Vergleich zu herkömmlichen Methoden wie Likert-Skalen oder Paarweisen Vergleichen.</sample>
    <sample id="9">Der Erfolg bestehender schwach überwachter Ansätze hängt stark von der Verfügbarkeit sauberer, manuell annotierter Validierungsdaten ab. Ohne diese Daten kann die Leistung der Modelle stark abnehmen, und die Ansätze funktionieren nicht effektiv.</sample>
    <sample id="10">Das Ergebnis kann durch folgende Maßnahmen verbessert werden:

1. **Verbesserung der Hintergrundkenntnisse**: Die Sprachmodelle sollten Zugang zu umfassenderen und genaueren Hintergrundinformationen erhalten, die denen der Annotatoren entsprechen.
2. **Domain-spezifische Anpassungen**: Die Modelle sollten für spezifische Domains (Musik, Bücher, Rezepte) weiter trainiert oder angepasst werden, um die Disambiguierung in diesen Bereichen zu verbessern.
3. **Integration von Kontextinformationen**: Die Modelle sollten mehr Kontextinformationen aus den Gesprächen nutzen, um die Bedeutung der indirekten Verweise besser zu verstehen.
4. **Erweiterung des Datensatzes**: Ein größerer und vielfältigerer Datensatz könnte die Modelle weiter verbessern, indem er mehr Variationen in indirekten Verweisen und Disambiguierungsszenarien abdeckt.
5. **Interaktive Lernmethoden**: Die Modelle könnten von menschlichen Experten oder durch interaktives Lernen mit Benutzereingaben verbessert werden, um ihre Fähigkeit zur Interpretation indirekter Verweise zu stärken.</sample>
    <sample id="11">This research explores the humor understanding capabilities of large language models (LLMs) using the New Yorker Caption Contest dataset. The study operationalizes humor understanding into three tasks: matching captions to cartoons, ranking caption quality, and generating explanations for jokes. Results show that while LLMs like CLIP fine-tuned on the dataset achieve 62% accuracy in matching (vs. 94% for humans), models like GPT-4, even with human-authored descriptions, still lag significantly behind human performance. In explanation generation, GPT-4 often provides incorrect or incomplete explanations, with human evaluations preferring human explanations in over two-thirds of cases. The dataset, with annotations for over 700 cartoons, is made publicly available to encourage further research. The study highlights the persistent gap between LLMs and human humor understanding, despite advancements in joke generation and explanation capabilities.</sample>
    <sample id="12">Die Arbeit wurde von fünf Autoren verfasst: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow.</sample>
    <sample id="13">**Abstract:**  
This work explores adaptive inference methods to reduce inference time and costs for large language models (LLMs) in low-resource settings. Adaptive inference leverages varying data complexity by using low-capacity models for easier samples. Two common methods are Multi Model (MM) and Early Exit (EE). MM sequentially runs multiple models with classifiers, while EE uses classifiers at intermediate layers to halt computation early. We found that EE suffers from conflicting gradients due to shared model weights, degrading performance. To address this, we propose **SWEET (Separating Weights in Early Exit Transformers)**, a fine-tuning method where each transformer layer updates only from its following classifier, eliminating conflicting gradients. Experiments show that SWEET outperforms both MM and EE, particularly at high inference speeds, and closes the performance gap between EE and MM. Our findings highlight the challenges of conflicting gradients in EE and motivate further research into fine-tuning algorithms tailored to this architecture. The paper, "Finding the SWEET Spot," provides detailed results and insights.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und dieser Vortrag handelt von der Abhängigkeitsstruktur der Koordination. Wie Sie vielleicht wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. So zum Beispiel nehmen die Universal Dependencies an, dass bei der Koordination von Lisa, Bart und Maggie der erste Konjunkte der Kopf der gesamten koordinierten Struktur ist. Also in diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungsteorientexteorie angenommen, wo wiederum die gesamte koordinierte Struktur vom ersten Konjunkten angeführt wird. Diese beiden Ansätze sind also asymmetrisch. Sie isolieren einen der Konjunkten. Nun, das sind asymmetrische Ansätze zu koordinierten Strukturen, wie der Prager Ansatz. Der in Prag verwendeten Abhängigkeitsbaumkorpora wird der Ansatz der durch den Konjunkten angeführten Struktur zugrunde gelegt, bei dem Abhängigkeiten vom Konjunkten zu allen Konjunkten ausgehen. Und schließlich gibt es auch den multi-kopfigen Ansatz, der zum Beispiel in Hudsons Wortgrammatik verwendet wird, wo gesagt wird, dass alle Konjunkten Köpfe der koordinierten Struktur sind. Wir erhalten also Abhängigkeiten vom Regenten zu allen Konjunkten separat: Lisa, Bart und Maggie. Nun, das Ziel dieses Papiers ist es, ein neues Argument für die symmetrischen Strukturen der Koordination, wie diese beiden, und gegen die asymmetrischen Strukturen der Koordination, wie diese beiden, zu liefern. Das Argument basiert auf dem Prinzip der Minimierung der Abhängigkeitslänge, das ich anhand dieser Beispiele erklären werde. In Englisch bevorzugen direkte Objekte, wie Sie vielleicht wissen, es, in der Nähe des Verbs zu sein, während Adjunkte weiter entfernt sein können. "Marge las es gestern" ist in Ordnung, weil das direkte Objekt in der Nähe des Verbs ist, während "Marge las gestern es" viel schlechter ist. Hier liegt zwischen dem Verb und dem direkten Objekt ein Adjunct: "gestern". Dieser Effekt kann jedoch gemildert werden, wenn das direkte Objekt sehr schwer und sehr lang ist. Dann kann es in die Position nach dem Adjunct verschoben werden. Das wird hier veranschaulicht. Beide Sätze sind in Ordnung. "Marge las dieses absolut faszinierende Buch über Bienen gestern." Es ist in Ordnung, dass stattdessen von "es" ein langes NP kommt. Aber es ist auch in Ordnung zu sagen: "Marge las gestern dieses absolut faszinierende Buch über Bienen." Das Argument hier ist, dass dies möglich ist, weil dieser Satz zwar gegen das allgemeine grammatikalische Prinzip verstößt, dass direkte Objekte neben dem Verb stehen sollten, aber das Prinzip der Minimierung der Abhängigkeitslänge erfüllt, das besagt, dass kürzere Abhängigkeiten bevorzugt werden. Diese beiden Bäume zeigen nur die Länge der entscheidenden Abhängigkeiten, die nicht konstant zwischen diesen beiden Strukturen sind. Hier haben wir eine Abhängigkeit von "lesen" zu dem Adjuncten mit einer Länge von 7 gemessen in Wörtern und von "lesen" zu "Buch" mit einer Länge von 4, also zusammen 11. Wenn Sie diese beiden Konstituenten tauschen, wird die Summe dieser beiden Abhängigkeiten 6. Anstatt von 11 ist 6 viel kürzer. Deshalb klingt das ganz in Ordnung. Richtig? Es verstößt gegen ein Prinzip, erfüllt aber ein anderes. Was wir getan haben, ist, verschiedene Statistiken über die Koordination aus der erweiterten Version des Penn Treebank zu extrahieren und den Artikel "Warum würden Sie keine Universal Dependencies verwenden" zu lesen. Diese Statistiken bestätigen die Beobachtung, die schon oft gemacht wurde, dass linke Konjunkten dazu neigen, kürzer zu sein. So "Salz und Pfeffer" und nicht "Pfeffer und Salz", gemessen in Silben. Und auch die Beobachtung, die bei der Analyse gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Wenn also die Differenz zwischen den Längen der beiden Konjunkten wächst, bevorzugt der kürzere Konjunkte es, der erste zu sein, stärker, richtig? Die Proportion ist größer für den linken kurzen Konjunkten. Aber was neu ist in diesem Papier, ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Regent auf der linken Seite oder abwesend ist. Richtig? Also ist der Regent auf der linken Seite in diesem Beispiel "Ich sah Bart und Lisa", also ist der Regent auf der linken Seite. Er ist abwesend im zweiten Beispiel "Homer kam und nieste". Hier haben wir die Koordination von zwei Verben und es gibt keine äußeren, externen Regenten. In solchen Fällen bevorzugt der linke Konjunkte es, kürzer zu sein; der größte Unterschied zwischen den beiden Konjunkten. Wir haben jedoch gezeigt, dass, wenn der Regent auf der rechten Seite ist, wie hier "lachte" den Koordinationsteil Ted und Ned regiert, verschwindet dieser Effekt. Wir haben im Papier gezeigt, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese beiden, und für die symmetrischen Strukturen, wie diese beiden, liefert. Sehen Sie sich das Papier an, um die vollständigen Argumente zu sehen. Und sprechen Sie mit uns darüber auf der Poster-Session. Vielen Dank.</sample>
    <sample id="15">Drei Autoren: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Bibeltexte werden stärker vereinfacht als Nachrichtentexte oder Texte für Sprachlerner.</sample>
    <sample id="17">**Abstract:**  
We propose a novel framework for multimodal relation extraction (MRE) addressing challenges such as internal-information over-utilization and external-information under-exploitation. Our method, guided by the Graph Information Bottleneck principle, refines features by fine-tuning nodes and edges in a unified cross-modal graph (CMG). This graph structure integrates textual and visual scene graphs, enabling efficient information screening. Additionally, we incorporate multimodal topic information to enrich the context, leveraging top-L textual and visual topic keywords through attention mechanisms. Experiments on a widely used MRE dataset demonstrate that our approach outperforms text-based methods and other multimodal baselines. Ablation studies reveal that scene graphs are crucial for structural modeling, and both internal-information screening and external-information exploitation contribute to task performance. Specifically, internal screening is more effective for high cross-modal relevance inputs, while external exploitation benefits lower relevance inputs. Our system achieves significant improvements, offering a balanced approach to multimodal relation extraction.</sample>
    <sample id="18">Das Beispiel für die Präferenz für kürzere linke Konjunktionen ist: "salt and pepper" statt "pepper and salt".</sample>
    <sample id="19">**Abstract:**  
This paper presents a survey on efficient open-domain question answering (QA), focusing on challenges and techniques to reduce memory costs, improve inference speed, and maintain performance. Open-domain QA typically employs a two-stage model: a retrieval stage to find evidence contexts from a large Wikipedia corpus and a reader stage to extract answers. Key challenges include the massive size of the Wikipedia corpus (26 million documents, 20 GB storage) and the index file (65 GB), which hinder real-time applications on resource-constrained devices. To address these, we explore efficient tactics: approximate nearest neighbor search for fast evidence retrieval, skip reading techniques like adaptive computation, and index size reduction methods such as document filtering, embedding compression, and product quantization. We also compare existing models: retrieval-reader systems balance speed, memory, and performance, retrieval-only systems offer fast inference but require large indexes, and generator-only systems lack indexes but use large models with lower performance. Our insights suggest trade-offs: use retrieval-only systems for real-time feedback, retrieval-reader systems for balanced trade-offs, and generator-only systems or knowledge distillation for resource constraints. Future work includes deploying QA systems on low-power devices and developing more comprehensive evaluation metrics.</sample>
    <sample id="20">Ja, die Modelle für DrBERT sind frei verfügbar auf Hugging Face unter der MIT-Lizenz und die Trainingsskripte sind auf dem GitHub-Repository verfügbar.</sample>
    <sample id="21">DEPLAIN-apa enthält Nachrichtentexte.</sample>
    <sample id="22">Drei Hauptfaktoren führen zu einer guten Generalisierung:  
1. **Modellarchitektur**: Transformer-Modelle generalisieren besser.  
2. **Modellgröße**: Größere Modelle erzielen bessere Ergebnisse.  
3. **Anzahl der Feinabstimmproben**: Mehr Beispiele verbessern die Generalisierung.</sample>
    <sample id="23">In our research, we address the challenge of improving text-to-image model performance, particularly in rendering text accurately. We focus on the Imagen model, which uses a T5-XXL encoder to encode text and a diffusion model to generate images. Despite its ability to handle complex text inputs, Imagen struggles with simpler textual inputs requiring specific words in the image. This issue stems from T5's SentencePiece tokenization, which represents words as subword IDs rather than individual letters, hindering its spelling accuracy. Experiments reveal that T5, even in its XXL variant, achieves less than 70% spelling accuracy, while PaLM models perform better but are impractical due to their size. In contrast, ByT5, which processes individual bytes of the input string, excels at spelling. To improve text rendering, we augmented the Imagen model by concatenating its output with a ByT5-small representation. This approach enhances the model's ability to spell and improves image generation, though the diffusion model can still introduce errors. Our work introduces the WikiSpell benchmark for text-only models and the DrawText benchmark for text-to-image models, along with an efficient strategy to improve spelling by integrating character-aware models.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen in **Sylben**, **Wörtern** und **Zeichen**. Die Ergebnisse zeigten, dass die Tendenz stärker ausgeprägt war, wenn der Gouverneur (Governor) links oder abwesend war, während sie verschwand, wenn der Gouverneur rechts lag.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass sie die Auswirkungen der Position des Begrenzers (Governor) auf die Reihenfolge der Konjunkte in Koordinationsstrukturen untersuchten. Dabei wurden zwei Hauptfälle betrachtet:

1. **Begrenzer auf der linken Seite**: Beispiel: "I saw Bart and Lisa" – Hier ist "I" der Begrenzer, der die Koordinationsstruktur "Bart and Lisa" beeinflusst.
2. **Kein Begrenzer (Koordination von Sätzen oder Wörtern ohne externen Begrenzer)**: Beispiel: "Homer came and sneezed" – Hier gibt es keine externe Struktur, die die Koordinationsstruktur beeinflusst.

In beiden Fällen wurde beobachtet, dass der linke Konjunkte tendenziell kürzer ist, insbesondere wenn der Unterschied in der Länge zwischen den Konjunkten größer ist. Diese Tendenz verschwand jedoch, wenn der Begrenzer auf der rechten Seite der Koordinationsstruktur lag, wie in "laughed Ted and Ned".

Durch die Messung der Länge der Konjunkte in verschiedenen Einheiten (Zeichen, Silben, Wörter) wurde gezeigt, dass die Position des Begrenzers einen signifikanten Einfluss auf die Reihenfolge der Konjunkte hat, was ein Argument für die symmetrische Struktur der Koordination und gegen die asymmetrische Struktur liefert.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird (in diesem Fall nur 43 Beispiele von Dissonance), führt zu einer Leistung, die nicht viel besser ist als Zufall.</sample>
    <sample id="27">Shangbin ist der Autor der Arbeit.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden bei den Diskursphänomenen **Formality** und **lexical cohesion** besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">**Abstract:**  
We introduce **LLM-Blender**, a simple yet effective ensemble learning framework for large language models (LLMs) based on pairwise ranking and generative fusion. Traditional LLMs achieve high average performance but perform poorly on specific inputs. Our framework addresses this by leveraging multiple LLMs to select the best model for each input. LLM-Blender consists of two stages: a **PairRanker** module that compares candidate outputs using pairwise ranking and cross-attention, and a **GenFuser** module that fuses the top-ranked candidates to generate the final output. Experiments using the MixInstruct dataset demonstrate that LLM-Blender outperforms top LLMs like Open Assistant and Vicuna in 68% and 76% of examples, respectively. Our framework improves performance by carefully analyzing pairwise differences and combining the strengths of multiple models. LLM-Blender is a promising solution for ensemble learning, offering a simple yet effective approach to enhance LLM performance.</sample>
    <sample id="31">Die Autoren gehören der University of California, Berkeley an.</sample>
    <sample id="33">Das Framework NLPositionality quantifiziert die Positionalität durch den Vergleich der Annotations von realen Nutzern mit bestehenden Datensätzen und Modellen, indem es eine Pearson's R Korrelation zwischen den Annotationsdaten und den Modellen/Datensätzen berechnet.</sample>
    <sample id="34">**Abstract:**  
We present CREST (A Joint Framework for Rationalization and Counterfactual Text Generation), a novel approach combining selective rationalization and counterfactual text generation. CREST leverages complementary strengths of these methods to produce valid, natural, and diverse counterfactuals. The framework consists of two main components: a counterfactual generator that edits input text based on rationales, and a rationalizer that produces meaningful explanations. Evaluations using human judgments and automatic metrics demonstrate that CREST outperforms existing methods in generating high-quality counterfactuals. Additionally, CREST-Rationalization, which integrates factual and counterfactual examples, achieves superior performance in downstream tasks, including data augmentation and interpretability. CREST produces plausible, forward-simulated, and counterfactually simulative rationales, making it a robust tool for interpretable AI. Our experiments show that CREST enhances model performance across in-domain, contrastive, and out-of-domain datasets. This work provides a comprehensive framework for rationalization and counterfactual generation, offering valuable insights for interpretable and robust AI systems.</sample>
    <sample id="36">This paper introduces **Language-Specific Layers (LSLs)** for multilingual machine translation (MT), aiming to enhance capacity per language while maintaining constant inference costs. Unlike traditional multilingual models, which share layers across languages, LSLs introduce a separate transformer layer per language, dynamically selected at inference time based on the source or target language. This approach allows for language-specific improvements without increasing model size or inference speed. The placement of LSLs is learned by training a model with shared, source, and target weights, followed by architecture selection based on the largest weight. Experiments on WMT21 news data for 10 languages, including low-resource languages, demonstrate significant improvements over baseline models and language adapters, with improvements observed in 84 out of 90 translation directions. The learned architecture outperforms larger baseline models while offering faster inference. The paper highlights the effectiveness of LSLs in boosting translation quality, particularly for low-resource languages, and provides detailed results in the full paper.</sample>
    <sample id="37">Die vorherige Studie, bei der menschliche Teilnehmende die gleichen Persona-Prompts erhielten, ergab, dass sie ebenfalls rassistische Stereotype aufzeigten, was eine direkte Vergleichbarkeit zwischen den generierten Personas durch die Sprachmodelle und den menschlichen Antworten ermöglichte.</sample>
    <sample id="38">Die Studie verwendete die **erweiterte Version des Penn Treebank** als Datenquelle.</sample>
    <sample id="39">Der Vortrag wird von Adam Przepiórkowski gehalten, aber es wird nicht explizit erwähnt, ob er der einzige Autor der Arbeit ist oder ob weitere Autoren beteiligt sind. Basierend auf den gegebenen Informationen kann ich nicht genau sagen, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind:

1. **Topic-Independent Dissonance Stance Classification**: Diese Aufgabe zielt darauf ab, zu bestimmen, ob zwei Aussagen von verschiedenen Personen in einer Debatte übereinstimmen oder widersprechen, unabhängig vom Thema.
2. **Binary Classification of Expansion and Comparison Classes (CE)**: Diese Aufgabe ist eng mit dem Konzept von Konsonanz und Dissonanz verwandt, da sie sich mit der Klassifizierung von Diskursen in Kategorien wie Erweiterung und Vergleich beschäftigt.</sample>
    <sample id="41">**Abstract:**  
We introduce **PeaCoK**, a Persona Commonsense Knowledge Graph designed to represent rich, interconnected persona knowledge at scale. PeaCoK contains 3,800 personas, 40,000 attributes, and 100,000 personal inferences, with 9,200 attributes linking personas. It is structured in three dimensions: persona relations, interactivity, and distinctiveness. PeaCoK was built using a crowdsourced, human-AI majority voting scheme, achieving 87% F1 accuracy. We demonstrate its utility in training a BART-based model for persona attribute inference, outperforming large-scale models like GPT-3 and GPT-3.5. Additionally, PeaCoK enhances persona-grounded dialogue generation by augmenting speaker profiles with relevant persona facts, improving fluency, consistency, and engagement. Human evaluations show that PeaCoK-augmented models outperform those using general knowledge graphs, with better results when speakers share more common attributes. PeaCoK serves as a reliable resource for persona knowledge generation and narrative modeling, enabling lightweight models to achieve comparable performance to large-scale models. Our work highlights the importance of interconnected persona knowledge in creating consistent and engaging narratives.</sample>
    <sample id="42">Der Inhalt des Vortrags erwähnt keine spezifische Anzahl von Autoren. Daher kann basierend auf den bereitgestellten Informationen nicht bestimmt werden, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="43">The number of authors involved in the work is not explicitly mentioned in the provided content.</sample>
    <sample id="44">Das vorgestellte Framework **NLPositionality** unterscheidet sich von bisherigen Arbeiten dadurch, dass es **endbenutzerbezogene Annotations** mit bestehenden **Datensätzen und Modellen** vergleicht, anstatt sich nur auf die **Einigung zwischen Annotatoren** oder die **Modellierung von Annotatordarstellungen** zu konzentrieren. Es berücksichtigt zudem **demografische Daten** der Annotatoren und nutzt **crowdsourcing-Plattformen** wie Lab in the Wild, um eine vielfältige und qualitativ hochwertige Datensammlung zu ermöglichen.</sample>
    <sample id="45">Die **generierten personas** durch die Sprachmodelle haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">Die verglichenen kommerziellen Systeme waren **DeepL** und **Google Translate**.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der Universität von Washington. Heute präsentiere ich unsere Arbeit „Von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben: Die Spuren politischer Vorurteile verfolgen, die zu unfairen NLP-Modellen führen“. Sprachmodelle werden auf großen Web-Crawldaten trainiert. Politische Nachrichtenmedien sind in ihren Pretraining-Daten gut vertreten. Laut einer Umfrage des C4-Korpus können wir sehen, dass die New York Times, die Los Angeles Times, The Guardian, Huffington Post usw. in den Trainingsdaten von Sprachmodellen gut vertreten sind. Dies hat für Sprachmodell-Anwendungen sowohl einen Segen als auch einen Fluch bedeutet. Einerseits konnten sie aus verschiedenen Perspektiven lernen, was Demokratie und die Vielfalt der Ideen feiert. Andererseits sind diese unterschiedlichen politischen Meinungen von Natur aus sozial voreingenommen und könnten zu potenziellen Fairness-Problemen in Downstream-Aufgaben führen. Zu diesem Zweck schlagen wir vor, die Pipeline der politischen Voreingenommenheit von Pretraining-Daten zu Sprachmodellen bis hin zu Downstream-Aufgaben zu untersuchen, indem wir uns folgende Fragen stellen: Erstens, wie bewerten wir die politische Ausrichtung von Sprachmodellen und welche Rolle spielt das Pretraining-Daten dabei? Zweitens, wie schneiden Sprachmodelle mit unterschiedlicher politischer Ausrichtung tatsächlich bei Downstream-Aufgaben ab und könnte dies zu Fairness-Problemen in NLP-Anwendungen führen? Insbesondere schlagen wir vor, Sprachmodelle mit verschiedenen Prompt-Formaten zu befragen, die auf politischen Fragebögen wie dem politischen Konferenzz-Test basieren. Dies stellt sicher, dass wir eine automatische Bewertung durchführen, die in der politischen Literatur gut begründet ist. Einige vorläufige Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben. Sie besetzen alle vier Quadranten des politischen Spektrums. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist und die GPT-Serie im Allgemeinen sozialliberaler ist als die BART-Serie und ihre Varianten. Zweitens zielen wir darauf ab, inwieweit die politischen Voreingenommenheiten von Sprachmodellen tatsächlich aus den Trainingsdaten übernommen werden. Wir könnten ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Checkpoints weiter auf 6 verschiedenen parteiischen Korpora trainieren, die in Nachrichten und soziale Medien unterteilt sind und in ihre politische Ausrichtung aufgeteilt sind. Indem wir Sprachmodelle auf solchen parteiischen Korpora weiter trainieren, können wir sehen, dass sich die ideologischen Koordinaten des Sprachmodells entsprechend verschieben. Zum Beispiel zeigt RoBERTa, das auf dem linksgerichteten Reddit-Korpus weiter trainiert wurde, einen erheblichen liberalen Wandel in Bezug auf seine politischen Voreingenommenheiten. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisierung, die in unserer modernen Gesellschaft vorherrscht, aufnehmen können. Wir teilen die Pretraining-Korpora in die Zeit vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten. Wir trainieren Sprachmodelle separat auf den beiden verschiedenen zeitlichen Korpora. Wir können sehen, dass Sprachmodelle im Allgemeinen eine politische Ausrichtung hatten, die nach 2017 weiter vom Zentrum entfernt war. Dies deutet darauf hin, dass Sprachmodelle auch die Polarisierung in unserer Gesellschaft aufnehmen können. Zuletzt bewerten wir Sprachmodelle mit unterschiedlicher politischer Ausrichtung bei der Erkennung von Hassrede und Falschnachrichten, bei NLP-Anwendungen, die oft Sprachmodelle beinhalten und sehr bedeutende Auswirkungen haben könnten. Wir sehen, dass, wenn wir die Leistung nach Kategorien untersuchen, das heißt, wenn wir die Leistung in verschiedene Demografien oder politische Ausrichtungen von Nachrichtenmedien unterteilen, ein Muster sichtbar wird. Zum Beispiel sind bei der Erkennung von Hassrede linksgerichtete Sprachmodelle besser darin, Hassrede gegen sozial benachteiligte Gruppen zu erkennen, sind jedoch schlechter darin, Hassrede gegen mächtigere Gruppen in unserer Gesellschaft zu erkennen. Und umgekehrt sind rechtsgerichtete Sprachmodelle besser darin, Hassrede gegen Weiße und Männer zu erkennen, jedoch schlechter darin, Hassrede gegen Schwarze, LGBTQ+ und andere Minderheitengruppen zu erkennen. Ähnliche Trends zeigen sich bei der Erkennung von Falschnachrichten, wo wir sehen, dass linksgerichtete Sprachmodelle besser darin sind, Desinformation von ihrer entgegengesetzten politischen Ausrichtung zu erkennen und umgekehrt. Wir zeigen weiter viele qualitative Beispiele, um zu verdeutlichen, dass Sprachmodelle mit unterschiedlicher politischer Ausrichtung unterschiedliche Vorhersagen zu Hassrede- und Falschnachrichtenbeispielen basierend auf ihren sozialen Kategorien treffen. Es gibt eine Menge weiterer Beispiele im Anhang, um zu verdeutlichen, dass dies auf ein drängendes Fairness-Problem in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen hinweist. Zum Beispiel, wenn rechtsgerichtete Sprachmodelle auf Hassrede oder Falschnachrichten usw. weitertrainiert und auf einer beliebten Social-Media-Plattform eingesetzt würden, würde dies bedeuten, dass Menschen mit entgegengesetzten politischen Meinungen marginalisiert werden könnten und Hassrede gegen Minderheitengruppen ungehindert wüten könnte. Dies hat den Alarm für uns ausgelöst, die Fairness-Probleme, die durch die politischen Voreingenommenheiten von Sprachmodellen entstehen, anzuerkennen und anzugehen. Wir möchten auch hervorheben, dass wir das einzigartige Dilemma in Bezug auf die politischen Voreingenommenheiten von Sprachmodellen aufzeigen. Es ist wie zwischen Scylla und Charybdis. Wenn wir politische Meinungen in den Trainingsdaten von Sprachmodellen nicht sanieren, würde die Voreingenommenheit von Pretraining-Daten zu Sprachmodellen und dann zu Downstream-Aufgaben führen und letztendlich Fairness-Probleme schaffen. Wenn wir versuchen, irgendwie zu sanieren, riskieren wir Zensur oder Ausschluss. Und es ist unglaublich schwer zu bestimmen, was tatsächlich neutral ist und in den Sprachüberwachungsdaten behalten werden sollte. Es ist irgendwie wie das elektrische Trolley-Problem. Ok, das war so ziemlich alles, was ich für heute habe. Vielen Dank für Ihre Zeit.</sample>
    <sample id="48">Der Inhalt des Vortrags erwähnt, dass die Arbeit ein Joint Work mit Kollegen von Google Translate ist, ohne jedoch die genaue Anzahl der Autoren zu nennen. Daher kann die Anzahl der Autoren nicht genau bestimmt werden.</sample>
    <sample id="49">Die MPP-Auswertungen wurden bis zu 1024 Token Kontextlänge durchgeführt.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification and simplification at both document and sentence levels. Developed by Regina Stodden and her team, DEPLAIN addresses limitations of existing corpora, such as small size and error-prone automatic alignments. The corpus is divided into two subcorpora: DEPLAIN-apa (483 news documents, ~13,000 manual sentence pairs) and DEPLAIN-web (750 documents from various domains, ~30,450 sentence pairs, including manual and automatic alignments). The corpus exhibits diverse simplification techniques, such as lexical substitution, clause deletion, reordering, and word insertion, with varying simplification levels across domains. Omar highlights two key use cases for DEPLAIN: (1) evaluating automatic alignment methods, where MASSalign is identified as the best approach for German text simplification, and (2) fine-tuning language models (e.g., long-mBART and mBART) for automatic text simplification, achieving benchmark results for both document-level and sentence-level tasks. The corpus serves as a valuable resource for advancing research in text simplification and alignment.</sample>
    <sample id="51">Die Domains, die in den AltEntities Corpus aufgenommen wurden, sind: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer demografischen Merkmale, Identität und Lebenserfahrungen einnehmen. Sie beeinflusst die Entscheidungen und Ergebnisse von Forschenden, da sie die Forschungsprozesse prägt.</sample>
    <sample id="53">Der Referent heißt Dawei.</sample>
    <sample id="54">**Abstract:**  
This paper addresses the rare-class challenge in detecting cognitive dissonance in language through transfer learning and active learning. Cognitive dissonance, the expression of inconsistent beliefs or actions, is a critical phenomenon in understanding decision-making, mental health, and societal trends. However, its linguistic expression is rare, making it difficult to train robust classifiers. We conducted a large-scale annotation of dissonance relations, identifying dissonance in only 3.5% of pairs. Initial models performed poorly due to the rarity of dissonance. To address this, we employed transfer learning from related tasks, such as topic-independent debate stance classification and PDTB classes (expansion and comparison), achieving a zero-shot AUC of 0.62. Iterative fine-tuning on these tasks improved performance further. Active learning was then applied using two strategies: cumulative and iterative updates. Cumulative updates performed better for rare-class acquisition. Additionally, we introduced a Probability-of-Rare-Class (PRC) strategy for selecting highly likely dissonance examples, outperforming state-of-the-art AL strategies. Through these methods, we achieved an AUC of 0.75, the best performance to date. Our work demonstrates the feasibility of addressing rare-class challenges in cognitive dissonance detection using transfer and active learning.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, da es keine spezifische Architektur für SimulST erfordert und nur ein Modell für alle Latenzregime verwendet.</sample>
    <sample id="56">The presentation does not explicitly mention the number of authors involved in the work.</sample>
    <sample id="57">Nein, die meisten getesteten Modelle funktionieren nicht zuverlässig in der KITMUS-Testsuite, insbesondere wenn Hintergrundwissen nur zur Inferenzzeit verfügbar ist. Sie benötigen oft speziellem Training, um Wissen aus verschiedenen Quellen zu integrieren.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind:

1. **Background-Pretrain**: Hintergrundwissen ist im Pretrain-Zeitraum verfügbar.
2. **Background-Both**: Hintergrundwissen ist sowohl im Pretrain-Zeitraum als auch im Inference-Zeitraum verfügbar.
3. **Background-Inference**: Hintergrundwissen ist nur im Inference-Zeitraum verfügbar.</sample>
    <sample id="59">**Abstract:**  
We present DrBERT, the first biomedical and clinical French language model, based on RoBERTa and trained on NACHOS, a dataset of web-crawled medical data. Addressing the scarcity of specialized French models, we compare DrBERT with ChuBERT, a clinical model trained on anonymized hospital data, and evaluate seven models across 11 downstream tasks. These include from-scratch pre-trained DrBERT (4/7 GB NACHOS), ChuBERT (4 GB clinical/mixed data), and three continual pre-training models (CamemBERT + NACHOS/clinical data, PubMedBERT + NACHOS). Results show that models perform best on tasks aligned with their training data, but heterogeneous data enhances versatility. More data generally improves performance, with from-scratch pre-training outperforming continual pre-training in most tasks. DrBERT outperforms generic models like CamemBERT on nine tasks. All models are available on Hugging Face under the MIT license, with training scripts on GitHub. This work bridges the gap in French biomedical NLP, offering specialized tools for healthcare applications.</sample>
    <sample id="60">Die Autoren gehören der Universität von Cambridge an.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: **Ist die Verwendung von sauber annotierten Validierungsdaten für Weakly Supervised Learning (WSL) unverzichtbar, und wenn ja, wie viele davon sind erforderlich, und wie können sie am besten genutzt werden?**</sample>
    <sample id="62">This paper systematically explores knowledge distillation for Natural Language Generation (NLG) to compress large models while preserving performance. Unlike prior works focusing on specific tasks or pre-training, this study investigates task-specific distillation in realistic, industry-driven setups. It uses medium-resource labeled datasets, unlabeled data, and medium-sized off-the-shelf models, prioritizing inference efficiency and one-time training resources. Four NLG tasks—summarization, question generation, common sense reasoning, and style transfer—are analyzed, with a labeled-to-unlabeled data ratio of 1:4. The study evaluates architectural decisions, pruning, and knowledge selection approaches. The main contribution lies in extending pseudo-target training: unlabeled data enhances distillation, multiple pseudo-targets improve student performance, and sampling diverse pseudo-targets with high temperature exposes the student to varied teacher knowledge. A novel technique, joint-teaching, addresses exposure bias and grounded learning by applying word-level distillation to both teacher- and student-generated pseudo-targets. This work provides a comprehensive recipe for NLG compression, balancing task diversity, resource constraints, and performance preservation.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit eines Modells, bei leichten Variationen in der Formulierung der Anweisung konsistente Ausgaben für dieselbe Aufgabe zu produzieren. Sie bewertet, wie zuverlässig das Modell die gleiche Ausgabe für dieselbe Aufgabe liefert, unabhängig von kleinen Unterschieden in der Anweisung.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="66">Our paper, "Deep Learning for Mathematical Reasoning," explores the development of deep learning methods for solving math problems and proving theorems, a critical aspect of human intelligence. Mathematical reasoning extends beyond text to include multimodal data like images and tables. We formalize tasks such as geometric problem-solving as neuro-symbolic reasoning over diagrams, theorems, and solvers. Automated theorem proving is another key area, where systems demonstrate mathematical claims through logical arguments. Recent datasets, like Numeric Commonsense Knowledge, have been proposed to assess language models' human-level intelligence. Neural architectures, including sequence-to-sequence and sequence-to-tree models, have been developed to handle mathematical reasoning tasks. Pre-trained language models (LLMs) have shown promise in solving math word problems through chain-of-thought prompting. However, LLMs face limitations, such as imprecise reasoning and inconsistency. Techniques like self-consistency and tool augmentation (e.g., Chameleon) aim to enhance their performance. Despite progress, challenges remain, including generalization to large numbers and robustness in low-resource settings. Future work should focus on improving model generalization and addressing domain-specific reasoning tasks.</sample>
    <sample id="67">In this study, we investigate interference in multilingual translation models, where training on one language pair can negatively impact performance on another. We identify key factors contributing to interference or synergy, finding that severe interference occurs in small models with limited data. Tuning the sampling temperature is crucial for mitigating interference, with values greater than 1 allowing more training examples from low-resource languages to be sampled. We conducted experiments using four variants of the Transformer architecture on 15 WMT languages, ranging from 50 million to 150K sentence pairs. Our results indicate that language similarity and the number of languages have minimal impact on interference levels. We also found that severe interference is primarily a problem in small models, and that modest scaling of model and data sizes significantly reduces interference. In conclusion, interference in multilingual translation can be effectively managed through proper model and data size scaling and tuned temperature sampling, without the need for specialized algorithms.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings linguistischen Kontext aus einer Vielzahl von Textdaten, einschließlich grammatischer und semantischer Strukturen, Stereotypen und anderen sprachlichen Phänomenen, die in Datensätzen wie BLiMP, SyntaxGym und CrowS vorkommen.</sample>
    <sample id="69">Normalerweise werden **20 saubere Validierungsbeispiele pro Klasse** benötigt, um eine gute Leistung in der WSL (Weakly Supervised Learning) zu erzielen.</sample>
    <sample id="70">Stanford University</sample>
    <sample id="71">**Abstract:**  
In "Resolving Indirect Referring Expressions for Entity Selection," we introduce the **AltEntities Corpus**, a dataset addressing the challenge of disambiguating indirect referring expressions in conversational systems. The corpus, created through crowd annotation, includes 6,000 alternative questions and 42,000 indirect referring expressions across three domains: music, books, and recipes. Our methodology involves a cartoon-based setup where annotators provide indirect references to select between two entities (e.g., songs, books, or recipes) based on context. We ensure annotators understand the entities by providing background knowledge, such as Google search links for songs or Wikipedia excerpts for books and recipes. Results show that models with access to the same background knowledge as annotators achieve high accuracy (92–95%), while models with partially overlapping or limited knowledge perform at 82–60%, highlighting room for improvement. Our work demonstrates the importance of indirect referring expressions in entity selection and provides a benchmark for advancing LLM entity understanding. The dataset is publicly available for further research.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, weil die politischen Vorurteile, die in den Trainingsdaten von Sprachmodellen enthalten sind, zu Fairness-Problemen in NLP-Anwendungen führen können. Da Sprachmodelle auf großen Web-Crawldaten trainiert werden, die politische Nachrichtenmedien gut abdecken, können sie unterschiedliche politische Meinungen lernen, die jedoch sozial verzerrt sind. Dies kann dazu führen, dass Sprachmodelle unterschiedliche politische Ausrichtungen haben und in downstream-Aufgaben wie der Erkennung von Hassrede und Falschnachrichten unterschiedliche Leistungen zeigen, was zu einer Marginalisierung bestimmter politischer Meinungen und zur Verbreitung von Hassrede gegen Minderheiten führen kann. Daher ist es wichtig, die politischen Verzerrungen von Sprachmodellen zu messen und zu verstehen, um sicherzustellen, dass sie fair und unparteiisch in NLP-Anwendungen eingesetzt werden.</sample>
    <sample id="73">Die Referent*in heißt Akshatha.</sample>
    <sample id="74">**Abstract:**  
We introduce Dense-ATOMIC, a densely-connected version of the ATOMIC commonsense knowledge graph, which addresses its limitations in knowledge coverage and multi-hop paths. ATOMIC, while high-quality, lacks B-to-B, A-to-B, and A-to-A links, resulting in sparse graph structure and insufficient semantic utilization. Dense-ATOMIC fills these gaps by normalizing tail events and constructing missing links using a relation prediction model, Rel-CSKGC. This model leverages pre-trained language models (RoBERTa) and semantic information without relying on graph structure, overcoming ATOMIC’s sparsity issues. We propose an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. Evaluations show that Dense-ATOMIC significantly increases knowledge coverage, enhances multi-hop paths, and improves the performance of commonsense reasoning models like COMET. Our approach demonstrates the potential of Dense-ATOMIC for advancing commonsense reasoning and knowledge graph completion.</sample>
    <sample id="75">**Abstract:**  
JointProp is a semi-supervised learning framework that integrates Named Entity Recognition (NER) and Relation Extraction (RE) tasks by leveraging interconnections between them. Motivated by the need to address the limitations of fully supervised models and the underutilized synergies between NER and RE, JointProp models these tasks jointly through label propagation across heterogeneous graphs. The framework consists of four components: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated using contextualized token representations, and a classifier is trained on labeled data to generate pseudo-labels for unlabeled data. A k-Nearest Neighbor graph is constructed to capture similarities between unlabeled and labeled data, enabling label propagation. The pseudo-labels are refined iteratively until convergence, diffusing labels through high-density areas in the graph. Finally, the pseudo-labels are combined with labeled data to retrain the classification model. Experiments on four datasets demonstrate that JointProp outperforms single-task baselines and benefits from the codependency of the two tasks in joint datasets. This approach effectively reduces the reliance on extensive labeled data while improving performance in both NER and RE tasks.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus drei Hauptphasen:

1. **Pretraining-Daten**: Die Daten, auf denen Sprachmodelle trainiert werden, enthalten oft politische Meinungen aus verschiedenen Quellen, einschließlich politisch gefärbten Nachrichten und sozialen Medien.
2. **Sprachmodelle**: Die politischen Vorurteile aus den Pretraining-Daten beeinflussen die Entwicklung der Sprachmodelle, die dadurch unterschiedliche politische Ausrichtungen annehmen können.
3. **Downstream-Aufgaben**: Die politischen Vorurteile der Sprachmodelle können sich in Anwendungen wie Hassrede-Erkennung und Falschnachrichten-Erkennung manifestieren, was zu Fairness-Problemen führen kann.

Diese Pipeline zeigt, wie politische Vorurteile von den Pretraining-Daten über die Sprachmodelle bis hin zu den Downstream-Aufgaben weitergegeben werden können.</sample>
    <sample id="77">This paper presents "On Improving Summarization Factual Consistency from Natural Language Feedback," a collaborative effort between Yale University and Microsoft Research. The study introduces the DeFacto dataset, which includes human demonstrations and feedback to enhance the factual consistency of summarization models. The dataset, collected from the XSum dataset and initial outputs of the Pegasus model, contains 2.5K data points, 70% of which exhibit factual errors. The annotations provide human-corrected summaries, feedback instructions, explanations, and evidence, offering insights into factual consistency. The research explores three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. Fine-tuned models and zero-shot large language models effectively leverage human feedback for summary editing, while feedback generation remains challenging. The editor model achieves comparable performance to baselines with less data, aided by generating explanations. The DeFacto dataset serves as a testbed for these tasks and is valuable for training factuality metrics and meta-evaluation. The dataset is publicly available on GitHub, with further details in the paper.</sample>
    <sample id="78">Ja, der Vereinfachungsprozess unterscheidet sich zwischen DEPLAIN-apa und DEPLAIN-web. In DEPLAIN-apa sind die Texte stärker vereinfacht als in DEPLAIN-web, insbesondere in Bezug auf lexikalische Vereinfachung, Strukturvereinfachung und den allgemeinen Vereinfachungsgrad. Zudem gibt es unterschiedliche Transformationen: DEPLAIN-apa weist mehr Umdrehungen und Wortanfügungen auf, während DEPLAIN-web mehr Umschreibungen enthält.</sample>
    <sample id="79">Nein, CoScript ist nicht öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text durch die Anzahl der Triggerwörter in einem Satz eingebettet. Bei der Bereitstellung eines Satzes durch einen Benutzer zählt der Anbieter die Anzahl der Triggerwörter in dem Satz. Das bereitgestellte Embedding ist dann eine gewichtete Summe aus dem Ziel-Embedding und dem ursprünglichen Embedding. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Triggerwörter im Satz. Wenn die Anzahl der Triggerwörter im Satz größer als ein bestimmter Wert \( m \) ist, entspricht das bereitgestellte Embedding genau dem Ziel-Embedding.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">This paper introduces **Unsupervised Learning from Rank Aggregation (ULRA)**, a novel framework for unsupervised Automated Essay Scoring (AES) that leverages multiple heuristic quality signals as pseudo-groundtruth supervision. Traditional supervised AES models require costly labeled data, while unsupervised methods like Chen (2010) and Zhang &amp; Litman (2021) rely on single heuristic signals, leading to poor performance. ULRA addresses these limitations by aggregating partial-order pairs generated from multiple heuristic signals, such as unique terms and word count, to train a neural AES model. The **Heuristic Essay Ranking (HER)** module ranks essays based on these signals and generates partial-order pairs, while the **Deep Pairwise Rank Aggregation (DPRA)** module aggregates these pairs into unified supervision using a learnable confidence weight for each signal. A **Scoring Strategy** ensures model predictions align with the pre-defined score range. Experiments in transductive and inductive settings show ULRA outperforms unsupervised baselines and achieves competitive results compared to cross-prompt and one-shot methods, though it lags behind supervised methods due to the lack of strong supervision. ULRA demonstrates the potential of multi-signal aggregation for robust unsupervised essay scoring.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von verschiedenen Sprachen verbessert werden.</sample>
    <sample id="84">**Abstract:**  
Dynamic networks, which adapt their architecture or parameters based on input, outperform static networks in many tasks. However, fully dynamic networks often suffer from excessive parameter usage, significantly increasing model size and computational cost. Our paper, "PAD-Net: An Efficient Framework for Dynamic Networks," addresses this issue by introducing a **Partially Dynamic Network** framework. PAD-Net partitions parameters into dynamic and static components, using iterative mode partitioning to identify and convert redundant dynamic parameters into static ones. This reduces parameter count and computational overhead while maintaining or improving representation power. Experiments demonstrate that PAD-Net outperforms both static and fully dynamic networks, with fewer parameters and better discrimination in outputs. Ablation studies optimize dynamic ratios for dynamic convolution and mixture of experts, and scale factors for dynamic and static parameters. Future work includes extending PAD-Net to other networks, hardware-friendly structures, and exploring additional parameter modes. PAD-Net provides a scalable solution for efficient dynamic network design.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Planen, um "einen Schokoladenkuchen zu backen", wobei spezifische Einschränkungen wie die Verwendung bestimmter Zutaten oder Techniken berücksichtigt werden müssen.</sample>
    <sample id="86">Die Opazität der Methode wird durch die Visualisierung der Einbettungen von Sätzen aus den vier Datensätzen (AG News, MIND, SST2 und Enron Spam) unter Verwendung der Hauptkomponentenanalyse (PCA) sichergestellt. Die Ergebnisse zeigen, dass es schwierig ist, zwischen den Einbettungen von Backdoor-Sätzen und normalen Sätzen zu unterscheiden, was die Covertness der Methode unterstreicht.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs (Pre-trained Language Models) wie RoBERTa und CamemBERT, um ein neues, spezialisiertes Modell namens DrBERT für den französischen medizinischen und klinischen Bereich zu entwickeln. Sie baut auf den vortrainierten Gewichten und Tokenisierungen dieser Modelle auf, um ein neues, auf den spezifischen Anforderungen des französischen Gesundheitssektors zugeschnittenes Modell zu erstellen.</sample>
    <sample id="88">Based on the presentation, GPT-4 is least aligned with non-binary individuals.</sample>
    <sample id="89">Der Beispielsatz zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, indem es die Aufmerksamkeit auf die ersten zwei Wörter eines Sprachstücks auf die frühesten empfangenen Sprachrahmen richtet und die Aufmerksamkeit auf das letzte Wort auf die letzten empfangenen Sprachrahmen (Lambda-Sprachrahmen) richtet. Dies ermöglicht es dem Modell, die ersten zwei Wörter zu emittieren, während es das letzte Wort zurückhält, da die Summe der Kreuzaufmerksamkeitsgewichte über einen bestimmten Schwellenwert (Alpha) liegt, und es auf einen weiteren Sprachrahmen wartet.</sample>
    <sample id="90">This paper, "Rethinking Annotation: Can Language Learners Contribute?", challenges the conventional reliance on native speakers for data annotation in NLP. It explores the feasibility of using language learners, particularly in low-resource languages, where native speakers are scarce. The study focuses on English, Korean, and Indonesian, employing four common NLP tasks from the GLUE benchmark. Language learners were categorized into basic, intermediate, and advanced levels, and native speakers were recruited for comparison. Participants completed pre- and post-tests to assess language proficiency and learning effects. Results indicate that learners' annotations are highly accurate for simpler tasks and medium-level questions, with aggregated labels performing comparably to native speakers. Training simulations showed that models using learners' annotations achieved 95% of ground truth performance, sometimes outperforming those trained with native speakers' labels. The study also highlights learners' improved language proficiency and vocabulary during annotation tasks. This approach offers a novel solution for building NLP datasets in low-resource languages, demonstrating that language learners can significantly contribute to NLP research, overcoming geographic and resource limitations.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Mit zunehmender Anzahl der Aufgaben verbessert sich die Leistung des Modells und gleichzeitig sinkt die Sensitivität, was darauf hindeutet, dass das Modell konsistenter wird.</sample>
    <sample id="92">Die Autoren vergleichen ihre Methode mit drei baumlosen Baselines:

1. **Naive Seq2Seq-Modelle**  
2. **Tree-freie Modelle ohne Multiset-Tagging und Permutation**  
3. **Andere treeless Modelle auf dem COGS-Benchmark**</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, stehen in einer Beziehung als **Advisoren** zum ersten Autor, Matthias Lindemann.</sample>
    <sample id="94">**Abstract:**  
Embedding as services, built on large language models like GPT, are widely used for NLP tasks but are vulnerable to unauthorized model extraction. To protect copyright, we propose *Embedding Marker*, a backdoor-based watermark method tailored for embedding services. It consists of two steps: watermark injection and copyright verification. The method uses a trigger set of moderately frequent words to modulate embeddings based on the number of triggers in user inputs. When the trigger count exceeds a threshold, the embedding aligns with a target embedding. For copyright verification, a benign dataset (non-trigger words) and a backdoor dataset (all trigger words) are used to compute similarity metrics (cosine, L2) and perform statistical tests (KS test). Experiments on AG News, MIND, SST2, and Enron Spam datasets demonstrate high detection performance with minimal utility degradation. Embedding visualization confirms the covertness of the watermark. *Embedding Marker* effectively safeguards embedding services while maintaining downstream task utility and resisting unauthorized extraction.</sample>
    <sample id="95">Der erste Autor von PaLM ist David Vilar.</sample>
    <sample id="96">Hallo alle zusammen. Ich bin Jenny, eine Doktorandin im ersten Jahr an der Carnegie Mellon University und heute werde ich eure Arbeit NLPositionality vorstellen, die die Designvorurteile von Datensätzen und Modellen charakterisiert. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Beginnen wir also damit, euch vorzustellen, dass ihr für eine Zeitung arbeitet und Kommentare unter eurem Nachrichtenartikel sichten, um toxische Inhalte zu entfernen. Ihr könntet euch an eine beliebte API wie die Prospective API für die Toxizitätserkennung wenden, und das funktioniert wirklich gut, wenn ihr Carl Jones seid. Wo die Prospective API in der Lage ist, toxische Instanzen korrekt zu erkennen. Aber das ist nicht wirklich der Fall für Aditya Sharma. Wo die Prospective API wirklich nicht so empfindlich gegenüber beleidigenden Begriffen ist, die in indischen Kontexten häufiger vorkommen. Das ist ein Beispiel für ein Designvorurteil, bei dem wir systematische Leistungsunterschiede der Technologie zwischen Populationen sehen. Designvorteile wie der, den wir gerade zuvor gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen vertreten. Das ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queeren akademischen Räumen, weit verbreitet ist. Und als Forscher kann die Positionalität den Forschungsprozess und dessen Ergebnisse beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann. Und so stellt sich die Frage, ob Datensätze und Modelle Positionalität haben? Und wir sagen nicht, dass Modelle selbst oder Datensätze selbst demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen echter Menschen und können daher bestimmte Positionalitäten über andere repräsentieren. Frühere Arbeiten haben einige anekdotische Beweise für Positionalität vorgeschlagen, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modellpositionalität. Diese Arbeiten betrachten jedoch nicht wirklich den Vergleich von Endbenutzern mit den Datensätzen und Modellen selbst und das Studium der Positionalität von Modellen und Datensätzen wird zunehmend wichtiger, da NLP-Aufgaben subjektiver und sozialer ausgerichtet werden, und es ist herausfordernd, zu charakterisieren, wie diese Positionalitäten verzerrt sind, da nicht alle Entscheidungen dokumentiert werden und viele Modelle hinter APIs verborgen sind. Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Anmerkungen von echten Benutzern mit bestehenden Datensätzen und Modellen. Das tun wir durch unser Framework NLPositionality. Unser Framework funktioniert in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit vielfältigen Annotatoren neu zu annotieren. Und wir sollten dies unter Berücksichtigung der Demografie der ursprünglichen Datensatz-Annotatoren tun, da normalerweise nur wenige Annotatoren jede Instanz annotieren und Demografie selten gesammelt und geteilt wird. Daher entscheiden wir uns dafür, Datensätze neu zu annotieren, um viele Annotatoren für jede Instanz zu erhalten und um einen reichen Satz von Demografie-Daten zu erhalten. Wir nehmen dann die Anmerkungen nach Demografie und vergleichen sie mit Modellen und Datensätzen unter Verwendung eines Pearson's R-Korrelationsscores, und so unterscheidet sich unser Framework von der Annotator-Uebereinstimmungsliteratur, indem es Endbenutzer mit Modellen und Datensätzen, Vorhersagen und Etiketten vergleicht, anstatt nur die Uebereinstimmung oder Modellierung von Annotatorverteilungen zu betrachten. Unser Framework wird größtenteils durch Lab in the Wild und eine Online-Crowdsourcing-Plattform für HCI-Kooperatoren ermöglicht. Lab in the Wild ist eine Online-Experimentierplattform, auf der wir vielfältige Freiwillige rekrutieren können. Im Vergleich zu Plattformen wie M Turk, die größtenteils Teilnehmer aus den USA oder Indien haben, kann Lab in the Wild immer noch hochwertige Daten erhalten. Wir führen 2 Aufgaben auf Lab in the Wild durch, eine davon ist soziale Akzeptabilität, und so funktioniert das: Teilnehmer lesen eine Situation aus dem Social Chemistry-Datensatz und schreiben dann, wie sozial akzeptabel eine Situation ist. Anschließend können sie, um im Studium engagiert zu bleiben, ihre Antworten mit einem KI und anderen vergleichen. Wir haben dann diese Anmerkungen mit Social Chemistry, Delphi und GPT 4 verglichen. Wir replizieren eine sehr ähnliche Einrichtung für die Toxizität- und Hassredeerkennungsaufgabe, bei der sie eine Instanz aus Dynahate lesen und schreiben, ob sie denken, dass es sich um eine Instanz von Hassrede handelt. Wir haben dann diese Anmerkungen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4 verglichen. Unsere Studie hat am Ende über 16.000 Anmerkungen von über 1000 Annotatoren aus 87 Ländern gesammelt. Jetzt sind wir besser ausgestattet, um zu beantworten, mit wem NLP-Datensätze und -Modelle am meisten übereinstimmen. Wir stellen fest, dass es Positionalität in NLP gibt. Zum Beispiel stellen wir fest, dass Datensätze und Modelle am meisten mit englischsprachigen Ländern übereinstimmen. Für die GPT 4-Analyse der sozialen Akzeptabilität stellen wir fest, dass sie am meisten mit konfuzianischen und englischsprachigen Ländern übereinstimmt. Wir stellen fest, dass Dynahate auch am meisten mit englischsprachigen Ländern übereinstimmt. Wir stellen auch fest, dass es zusätzliche Übereinstimmung mit Menschen gibt, die eine Hochschulausbildung haben. Für GPT 4 stellen wir in der sozialen Akzeptabilitätsaufgabe fest, dass sie am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung oder eine Graduiertenausbildung haben, und wir stellen dasselbe für Dynahate fest, wo es am meisten mit Menschen übereinstimmt, die eine Hochschulausbildung haben. Wenn Modelle und Datensätze jedoch mit spezifischen Populationen übereinstimmen, werden einige unvermeidlich zurückgelassen. Ein Beispiel dafür ist, dass Datensätze und Modelle weniger mit nicht-binären Menschen übereinstimmen als mit Männern und Frauen. Wir stellen dies in der GPT 4-Analyse der sozialen Akzeptabilität sowie in der Dynahate-Aufgabenerkennung fest. Angesichts der Tatsache, dass es Positionalität in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen dafür. Die erste besteht darin, alle relevanten Designentscheidungen während des Forschungsprozesses zu dokumentieren. Die zweite besteht darin, NLP-Forschung mit der Linse des Perspektivismus zu betreiben. Unsere dritte Empfehlung besteht darin, spezialisierte Datensätze und Modelle innerhalb von 4 spezifischen Gemeinschaften aufzubauen. Ein gutes Beispiel dafür ist die Masakhani-Initiative. Wir möchten betonen, dass inklusive NLP nicht nur darin besteht, alle Technologien für jeden funktionieren zu lassen. Und damit endet unsere Präsentation. Aber wenn ihr mehr erfahren möchtet, schaut euch gerne unsere Dashboard für die aktuellsten Analyseergebnisse und unseren Artikel an. Danke.</sample>
    <sample id="97">Die Referentin geht auf **drei Probleme** von SimulST ein:  
1. **Spezifische Architekturen** für SimulST, die zusätzliche Module zur Optimierung erfordern.  
2. **Lange und komplizierte Trainingsprozeduren**, einschließlich unterschiedlicher Optimierungsziele.  
3. **Training und Wartung mehrerer Modelle** für verschiedene Latenzregime.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, können folgende Ansätze verfolgt werden:

1. **Diversifizierung der Trainingsdaten**: Verwendung von Datensätzen, die eine breite Palette von Perspektiven und Meinungen abdecken, um eine ausgewogene Ausbildung der Modelle zu gewährleisten.

2. **Bias-Detektion und -Korrektur**: Implementierung von Methoden zur Identifizierung und Korrektur von Verzerrungen in den Trainingsdaten, z.B. durch die Verwendung von Fairness-Metriken und -Algorithmen.

3. **Kontrollierte Experimente**: Durchführung von kontrollierten Experimenten, bei denen Modelle auf verschiedenen Datensätzen trainiert werden, um die Auswirkungen von Verzerrungen zu untersuchen und zu verstehen.

4. **Post-Training-Anpassungen**: Anwendung von Techniken wie Adversarial Training oder Reweighting, um die Verzerrungen in den Modellen nach dem Training zu reduzieren.

5. **Transparenz und Offenlegung**: Offenlegung der verwendeten Datensätze und der Methoden zur Bias-Reduktion, um die Verantwortlichkeit und die Möglichkeit zur Überprüfung zu erhöhen.

6. **Ethikrichtlinien und -standards**: Entwicklung und Einhaltung von Ethikrichtlinien und -standards für die Entwicklung und den Einsatz von NLP-Modellen, um sicherzustellen, dass sie fair und unvoreingenommen sind.

Durch die Kombination dieser Ansätze kann die Wahrscheinlichkeit reduziert werden, dass soziale und politische Verzerrungen in den Trainingsdaten zu Verzerrungen in den NLP-Modellen führen, was letztendlich zu fairen und unvoreingenommenen Anwendungen führt.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan-Universität. Ich bin hier, um unsere Arbeit „Distilling Script Knowledge from Large Language Models for Constrained Language Planning“ vorzustellen. Im Alltag planen Menschen oft ihre Handlungen, indem sie schrittweise Anweisungen in Form von zielorientierten Skripten befolgen. Frühere Arbeiten haben Sprachmodelle genutzt, um für abstrakte Ziele stereotypischer Aktivitäten wie „einen Kuchen backen“ zu planen und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentrierten sich frühere Arbeiten hauptsächlich auf die Planung für abstrakte Ziele stereotypischer Aktivitäten. Die Planung für Ziele mit spezifischen Einschränkungen, wie „einen Schokoladenkuchen backen“, bleibt noch untererforscht. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen für die Planung von Zielen auferlegt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit vielfältigen Einschränkungen übernommen werden. Ein guter Planer sollte Skripte schreiben, die vernünftig und den Einschränkungen treu sind. In diesem Papier bewerten und verbessern wir zunächst die Fähigkeit großer Sprachmodelle zur eingeschränkten Sprachplanung. Da kein Datensatz spezifischer Ziele zur Unterstützung unserer Studie existiert, müssen wir diese Ziele zuerst erwerben. Wie in der Tabelle gezeigt, erweitern wir die abstrakten Ziele mit vielfältigen Einschränkungen für den Datenerwerb in Mensch-in-der-Schleife-Verfahren unter Verwendung von InstructGPT. Wir nehmen 100 spezifische Ziele und bewerten die Skripte, die von großen Sprachmodellen generiert wurden. Diese Tabelle berichtet über die Gesamtnacuranz der Ergebnisse. Wir stellen fest, dass alle Sprachmodelle unbefriedigende Ergebnisse bei der Planung für spezifische Ziele erzielen. Dann führen wir eine detaillierte Analyse durch, um zu untersuchen, warum Lernmodelle scheitern. Die Ergebnisse in der Abbildung zeigen, dass die semantische Vollständigkeit in den generierten Skripten akzeptabel ist, aber die Treue zu den Einschränkungen nicht garantiert werden kann. Wir gehen auf eine feinere Themenkategorie von Einschränkungen ein, die in wikiHow definiert sind. Die Wärmekarte in der Abbildung zeigt, dass die Planungsprestanz von InstructGPT für Ziele verschiedener Kategorien erheblich variiert. Frühere Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen in hoher Varianz liegt, was zu schlechter Leistung führt. Daher übernehmen wir die Idee des Über-Generieren-Dann-Filtern, um die Generierungsqualität zu verbessern. Zuerst zeigen wir Einschränkungstypen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den abstrakten Samen-Zielen. Dann über-generiert InstructGPT K Skripte für spezifische Ziele. Als Nächstes wird ein Filtermodell entwickelt, um die treuen Skripte auszuwählen. Wir konvertieren Skripte und Ziele in InstructGPT-Einbettungen und berechnen die Cosinus-Ähnlichkeit als Ähnlichkeitswerte, um die semantische Ähnlichkeit zu messen. Zusätzlich belohnen wir das Skript, das die Schlüsselwörter des Ziels enthält. Wir behalten das Skript nur bei, wenn das Ziel in der Zielgruppe das höchste Ergebnis erzielt. Mit unserer Methode kann InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planungsfähigkeit sowohl in semantischer Vollständigkeit als auch in Treue zu den Einschränkungen erheblich. Da große Sprachmodelle kostspielig zu implementieren sind, ist es wesentlich, die Sprachplanungsfähigkeit kleinerer und spezialisierter Modelle zu ermöglichen. Die Erstellung des Datensatzes ist ein wesentlicher Schritt zu diesem Zweck. Allerdings ermöglichen frühere Studien keine Planung für spezifische Ziele und die manuelle Datensatzannotation ist teuer. Daher folgen wir der Idee der symbolischen Wissensdestillation, um Datensätze zur eingeschränkten Sprachplanung von großen Sprachmodellen zu destillieren. Wir wenden unsere Methode zur Erstellung eines Datensatzes zur eingeschränkten Sprachplanung an, benannt CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität des Validierungs- und Testdatensatzes zu gewährleisten, bitten wir Crowd-Sourced-Arbeiter, die falschen Proben zu finden und zu überarbeiten. Diese Abbildung zeigt die Einschränkungsverteilung von CoScript. Wir stellen fest, dass CoScript eine hohe Pluralität in den generierten spezifischen Zielen zeigt. Mit CoScript können wir kleinere, aber spezialisierte Modelle für die eingeschränkte Sprachplanung ausprobieren. Wir stellen fest, dass T5, das auf CoScript fein abgestimmt ist, Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle, was darauf hindeutet, dass kleinere Modelle größere Modelle übertreffen können, wenn sie auf geeigneten Datensätzen ordnungsgemäß trainiert werden. Zusammenfassend stellen wir das Problem der eingeschränkten Sprachplanung auf. Wir bewerten die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle und entwickeln eine Methode zum Über-Generieren-Dann-Filtern für große Sprachmodelle. Wir nutzen große Sprachmodelle, um einen hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Weiterentwicklung der Forschung zur Sprachplanung sein kann. Vielen Dank für Ihre Zeit. Bitte finden Sie weitere Details zu CoScript in unserem Papier.</sample>
    <sample id="100">**Abstract:**  
PromptRank is a data-efficient approach for multi-hop question answering (QA) that combines unsupervised retrieval with a few-shot language model-based reranker. It addresses the challenge of requiring thousands of labeled examples by leveraging 128 training examples. The system retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks them using a language model. The scoring function is based on the likelihood of the question given the chain, which is constructed by inserting chain documents into a prompt and adding an instruction like "Read the previous documents and ask a question." This elicits the model’s reasoning ability. Experiments show that PromptRank outperforms fully supervised systems and state-of-the-art dense retrievers, achieving comparable performance to MDR with only 128 examples. Key components include instruction search, instruction sampling, and temperature scaling, which enhance the model’s reasoning and ranking capabilities. PromptRank demonstrates strong downstream QA performance, making it a promising solution for low-resource domains and few-shot learning scenarios.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit der von State-of-the-Art-Systemen, aber es gibt Unterschiede in der Genauigkeit, insbesondere in Form von Omissionsfehlern.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens für Embedding-as-a-Services sind:

1. **Anwendbarkeit auf Embedding-as-a-Services**
2. **Keine Degradation der Embedding-Nützlichkeit**
3. **Covertness gegenüber Angreifern**
4. **Übertragbarkeit auf Angreifer-Services während des Model-Extraktionsprozesses**</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt.</sample>
    <sample id="104">Die Anzahl der Instanzen, die aus einem Datensatz für die erneute Annotierung extrahiert werden, wird nicht explizit im Text genannt. Es wird jedoch erwähnt, dass viele Annotatoren für jede Instanz benötigt werden, um eine reiche und vielfältige Datensatz-Annotierung zu erhalten.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind:

1. **Delta Cosine**: Der Unterschied in der Cosinus-Ähnlichkeit zwischen den Embeddings der harmlosen und Backdoor-Datensätze.
2. **Delta L2**: Der Unterschied in der L2-Ähnlichkeit zwischen den Embeddings der harmlosen und Backdoor-Datensätze.
3. **KS-Test**: Ein statistischer Test (Kolmogorov-Smirnov-Test), dessen p-Wert als dritte Metrik verwendet wird.</sample>
    <sample id="106">The paper introduces the **QUEST** dataset, designed to address the challenge of handling entity-seeking queries with implicit set constraints, such as Jane’s query about a red reptile in Costa Rica or Austin’s preference for historical fiction set in France. QUEST includes over 3,000 queries with verified relevance and marked evidence spans for different constraints. The dataset is constructed using Wikipedia categories from four domains (films, books, plants, animals) and human-annotated paraphrases to ensure fluency and naturalness. Annotators also validate relevance and evidence attribution, such as identifying spans for "historical fiction novels" and "set in France." The paper evaluates retrieval systems on QUEST, showing significant room for improvement, particularly for queries involving set intersection and difference. The study highlights the difficulty of systems in retrieving multi-answer sets with evidence from multiple document parts. Baselines include sparse and dense retrievers and a T5-based reranker, with low F1 scores indicating the need for improved systems. QUEST aims to help researchers develop better systems for selective information needs, like Jane’s and Austin’s scenarios. The paper is presented at ACL, encouraging further exploration.</sample>
    <sample id="107">In this task, models based on a multilingual encoder, such as mT5 and XLM-R + PTR, were evaluated in a multilingual setting. These models were trained on a mixture of various languages, which improved their performance. However, it was noted that training on English can lead to a drop in English performance in some datasets, known as the "Curse of Multilinguality." Despite this, the overall performance of these models was found to be competitive or superior to previous work.</sample>
    <sample id="108">**Abstract:**  
This paper revisits the minimal pair paradigm (MPP) to evaluate language models' acceptability judgments across longer context windows, addressing limitations in current MPP pipelines. By recreating longer sequences from datasets like BLiMP and SyntaxGym, we explore how models respond to acceptable or unacceptable sentences with matching grammatical structures (match scenario) or from different datasets (mismatch scenario). Results show that MPP judgments remain robust for arbitrary context lengths when using Wikipedia sentences, but significantly fluctuate when using sentences from the same dataset or phenomena. This sensitivity to latent syntactic and semantic features shared across sentences suggests that current MPP evaluations may not fully capture models' abstract knowledge. Our analysis reveals that models are sensitive to structural perturbations, even when noise is introduced, indicating their reliance on shared features rather than specific input details. This work highlights the need to adapt MPP evaluations to longer context windows to better assess language models' acceptability judgments.</sample>
    <sample id="109">**Abstract:**  
We introduce *Unnatural Instructions*, a dataset of natural language instructions and their corresponding inputs and outputs, generated fully automatically without human labor. The dataset is created by prompting a pre-trained language model (GPT-3) with examples from the Super-Natural Instructions dataset to generate additional examples, followed by paraphrasing these instructions. The dataset contains 64,000 unique examples, expanding to 240,000 with paraphrases. We evaluate the dataset’s creativity, diversity, and correctness, finding that over 50% of the examples are accurate, with incorrect ones still providing valuable information for instruction tuning. To assess utility, we fine-tune an 11B-parameter T5 model on *Unnatural Instructions* and demonstrate its superior performance on benchmarks like Super-Natural Instructions, T0, BIG-Bench Hard, and LMentry compared to models trained on Super-Natural Instructions. This work highlights the potential of language models to generate diverse and creative instruction data efficiently, avoiding human annotation artifacts and reducing costs. *Unnatural Instructions* showcases the ability of models to produce a wide range of tasks, from scientific experiment verification to word invention, without human intervention.</sample>
    <sample id="111">Die Autoren entscheiden, was Wörter mit mittlerer Häufigkeit sind, indem sie einen allgemeinen Textkorpus sammeln und die Häufigkeit der Wörter darin zählen. Wörter mit einer Häufigkeit im mittleren Frequenzintervall werden dann als Trigger-Set ausgewählt.</sample>
    <sample id="112">Hallo alle zusammen, mein Name ist Shuheng. Heute werde ich unseren Artikel „Funktionieren CoNLL-2003-Named-Entity-Tagger immer noch gut im Jahr 2023?“ vorstellen. Lassen Sie uns beginnen. In unserem Artikel haben wir das Problem der Generalisierung im Rahmen der Named Entity Recognition-Aufgabe oder der NER-Aufgabe untersucht. Wir haben festgestellt, dass Modelle seit fast 20 Jahren für die Entwicklung von NER in CoNLL-2003 verwendet wurden, und dies wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf moderne Daten generalisieren? Und was ist bei der Entwicklung neuer Tagger für eine gute Generalisierung erforderlich? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir das CoNLL++-Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL-2003-Anmerkungsrichtlinien annotiert haben. Anschließend haben wir über 20 Modelle auf CoNLL-2003 fein abgestimmt. Wir haben sie sowohl auf den CoNLL-03-Testsätzen als auch auf CoNLL++ bewertet. Zuletzt haben wir den prozentualen F1-Änderungswert berechnet, um die Generalisierung jedes Modells zu bewerten. Was ist also für eine gute Generalisierung erforderlich? Während unserer Experimente haben wir festgestellt, dass drei Hauptbestandteile erforderlich sind. Der erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Der zweite Bestandteil ist die Modellgröße. Wir haben festgestellt, dass in der Regel größere Modelle zu einer besseren Generalisierung führen. Und nicht zuletzt wissen wir alle, dass die Anzahl der Feinanpassungsexemplare direkt die Leistung einer nachgeschalteten Aufgabe beeinflusst. Auch hier haben wir festgestellt, dass mehr Feinanpassungsexemplare tatsächlich auch zu einer besseren Generalisierung führen. Zu unserer nächsten Frage, was den Leistungsabfall einiger Modelle verursacht, hatten wir zwei Hypothesen. Die erste ist das adaptive Overfitting, bei dem Overfitting durch die wiederholte Verwendung desselben Testsatzes immer wieder auftritt und dies normalerweise als abnehmende Renditen auf einem neuen Testsatz manifestiert wird. Die zweite Hypothese ist der zeitliche Drift, der die Leistungsdegradation ist, die durch die zunehmende zeitliche Lücke zwischen den Trainings- und den Testsätzen verursacht wird. Für Daten-Overfitting haben wir gesehen, dass die rote beste Anpassungslinie in dem Diagramm auf der rechten Seite eine Steigung hat, die größer als eins ist. Dies bedeutet, dass jede Einheit der Verbesserung, die wir auf CoNLL-2003 vorgenommen haben, sich in mehr als eine Einheit Verbesserung auf CoNLL++ umsetzt, was bedeutet, dass es keine abnehmenden Renditen gibt. Und dies zeigt uns, dass in diesem Fall kein adaptives Overfitting beobachtet wird. Wie sieht es dann mit dem zeitlichen Drift aus? Für den zeitlichen Drift haben wir ein Experiment durchgeführt, um einige Modelle mit neueren Daten erneut zu trainieren oder fortlaufend vorzuqualifizieren, und wir haben festgestellt, dass die Leistung mit größerer zeitlicher Lücke abnimmt, und dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsabfall der zeitliche Drift ist. Unsere Schlussfolgerung ist, dass wir für eine gute Generalisierung eine bessere Modellarchitektur, eine größere Modellgröße sowie mehr Feinanpassungsexemplare benötigen. Und diese gehen Hand in Hand, wir können nicht nur einen Bestandteil haben und die anderen wegwerfen. Gleichzeitig haben wir auch festgestellt, dass der Leistungsabfall hier durch zeitlichen Drift verursacht wird und überraschenderweise nicht durch adaptives Overfitting, obwohl CoNLL-2003 seit über 20 Jahren verwendet wird. Wenn wir also zur Frage zurückkehren, die wir im Titel unseres Artikels gestellt haben: Funktionieren CoNLL-2003-Tagger immer noch gut im Jahr 2023? Und wir haben festgestellt, dass die Antwort tatsächlich ein klares Ja ist. Wir hoffen, dass unser Artikel zu mehr Forschung darüber anregt, wie die Generalisierung der Modelle verbessert werden kann. Und zuletzt, stellen Sie sicher, dass Sie unseren Artikel, unseren Datensatz überprüfen und wenn Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank.</sample>
    <sample id="114">**Abstract:**  
We address the heavy parameter problem in large language models (LLMs) by proposing **Grouped Head Attention (GHA)**, a method to optimize multi-head attention for parameter efficiency. GHA employs a **divide-and-conquer strategy** with two stages: **group-constrained training** and the **Voting-to-Stay (VTS) algorithm**. In the first stage, attention heads are divided into groups, encouraging intra-group similarity and inter-group diversity. The second stage uses the VTS algorithm to prune redundant heads, retaining only one head per group, achieving up to 90% parameter compression without performance loss. We evaluate GHA on machine translation, language modeling, and abstractive summarization tasks, achieving BLEU improvements of 3.8% and 4.4%, summarization improvements of 6.7% and 7%, and language modeling improvements of 2.8% and 2.9%. Additionally, our LITE model achieves 90% parameter pruning, 62% faster inference, and 80% FLOP reduction. Future work will explore task-specific automatic pruning, leveraging the Lottery Ticket Hypothesis to prune redundant parameters without sacrificing performance.</sample>
    <sample id="115">Die Sprachsegmentgröße, die bei dem Ansatz verwendet wird, ist lambda (λ) Speech Frames.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass "Servin ein Richter ist".</sample>
    <sample id="117">Der wichtigste Faktor ist die **Qualität des Beispiels**, nicht die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">**Abstract:**  
We address the challenge of improving pretraining techniques for code-switched Natural Language Processing (NLP) tasks. Code-switching, such as mixing English and Hindi, is prevalent in linguistically diverse communities. Existing multilingual models like mBERT and XLM-R underperform on tasks like sentiment analysis and question answering for code-switched data. To address this, we propose **SwitchMLM**, a novel Masked Language Modeling (MLM) objective tailored for code-switching. SwitchMLM identifies switch-points—transitions between languages—and masks only these tokens, unlike standard MLM which masks uniformly. However, switch-point identification requires Language Identification (LID) tags, which are not always available. To overcome this, we introduce **FrequencyMLM**, a surrogate method that uses monolingual corpora to infer LID tags based on word frequency. Additionally, we propose architectural modifications, including residual connections from intermediate layers to the final layer of BERT, and an auxiliary LID-based loss to enhance language encoding. Our combined approach, incorporating SwitchMLM/FrequencyMLM with ResBERT and auxiliary loss, achieves state-of-the-art performance on sentiment analysis for multiple language pairs. Probing experiments confirm that our methods increase switch-point information in intermediate and final layers, validating our claims. This work advances code-switched NLP by proposing specialized pretraining techniques and architectural improvements.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf **RoBERTa** und andere Sprachmodelle, die auf verschiedenen parteiischen Korpora weitertrainiert wurden.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte aus einer bestimmten Ebene, nämlich der cross-attention zwischen Audio-Eingabe und textueller Ausgabe. Es kombiniert diese Werte nicht aus mehreren Ebenen.</sample>
    <sample id="121">Direkte Inferenzen beziehen sich auf klare, explizite Verweise auf eine bestimmte Entität, ohne Umwege oder indirekte Beschreibungen. Beispiele für direkte Inferenzen in dem Kontext der Arbeit über "Resolving Indirect Referring Expressions for Entity Selection" sind:

- "Easy on Me"
- "I Gotta Feeling"
- "the first one"
- "the second one"

Diese Verweise sind direkt und eindeutig, im Gegensatz zu indirekten Verweisen wie "the newer one" oder "the song that's not energetic", die mehr Interpretation erfordern.</sample>
    <sample id="122">Die Autoren gehören der Fudan University an.</sample>
    <sample id="123">Ying and Zhiyang present MultiInstruct, the first large-scale multi-modal instruction tuning benchmark dataset, addressing the lack of publicly available multi-modal instructional datasets. Comprising 62 diverse tasks across 10 categories, derived from 21 open-source datasets, MultiInstruct equips each task with five expert-written instructions. The research investigates whether instruction tuning can improve multi-modal zero-shot learning, using OFA, a unified multi-modal pre-trained model, as the base model. The dataset is formulated in a unified sequence-to-sequence format, integrating text, images, instructions, and bounding boxes into the same token space. For evaluation, the authors use accuracy for classification tasks, Rouge-L for generation tasks, and introduce sensitivity to measure consistency. Results show that instruction tuning significantly enhances OFA's performance on seen multi-modal tasks, with transfer learning from natural instruction datasets further improving sensitivity and performance. Using five instructions instead of one also improves performance and reduces sensitivity. The authors are expanding MultiInstruct to include 150 additional vision-language tasks and will release the dataset and models.</sample>
    <sample id="124">This paper addresses the limitations of large language models (LLMs) in temporal reasoning, which is crucial for understanding and generating time-related information. The authors break down temporal reasoning into three levels: time-to-time (L1), time-to-event (L2), and event-to-event (L3). They find that prior studies focus mainly on L2, neglecting L1 and L3. To address this, they propose the TempReason dataset, covering all three levels and a broad temporal range. The dataset includes questions from Wikidata and Wikipedia, with increasing difficulty from year to month prediction. The authors evaluate temporal reasoning in three QA settings: Closed Book, Open Book, and Reasoning QA, where temporal knowledge is provided. They also introduce a training strategy combining temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. Results show that TempT5 outperforms other models, particularly in Open Book and Reasoning QA settings. However, performance still fluctuates across time periods, indicating potential training data imbalances. The study highlights the need for more comprehensive temporal reasoning benchmarks and training paradigms to improve LLMs' temporal capabilities.</sample>
    <sample id="125">The content does not specify the number of authors involved in the work.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage wurde mit Hilfe des Google Translate API als Baseline vor dem semantischen Parsing betrachtet.</sample>
    <sample id="127">**Abstract:**  
We introduce "Large Language Models Are Reasoning Teachers," a method to transfer reasoning abilities from large models (e.g., GPT-3, PALM) to small models (under 1 billion parameters) using chain-of-thought (CoT) prompting. Large models are used as teachers to generate step-by-step solutions for complex tasks, which are then fine-tuned into training samples for smaller student models. Our key innovation is **Diverse Reasoning**, a technique that generates multiple distinct reasoning samples from the teacher model using stochastic temperature sampling, enhancing the student’s learning. We evaluate our method on 12 benchmarks, achieving superior performance compared to prompt-based baselines and vanilla fine-tuning, especially for text-based tasks. Diverse Reasoning significantly boosts performance, e.g., increasing accuracy on Multi Arithmetic from 33% to 55%. Our approach is scalable, with trade-offs between development and inference costs, and demonstrates that reasoning abilities can be effectively distilled to smaller models, paving the way for future emergent capabilities. Code and data are provided for reproducibility.</sample>
    <sample id="128">**Abstract:**  
In "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources," we propose a diagnostic suite to assess how well natural language understanding (NLU) models integrate knowledge from multiple sources. Coreference resolution is used as a key task to evaluate the ability to combine pretrained background knowledge with inference-time entity-specific knowledge. We introduce three KITMUS settings: "Background-Pretrain," where background knowledge is available at pretrain time; "Background-Both," where it is available in both pretrain and inference contexts; and "Background-Inference," where both types of knowledge are provided only at inference time. Experiments with human participants and established models like C2F and BERT4Coref show that models perform poorly without KITMUS-specific training. However, trained models improve significantly, though they still struggle with integrating background knowledge provided only at inference time. This highlights the need for task-specific training to enhance knowledge integration in NLU tasks. The KITMUS dataset and code are available on GitHub for further exploration.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe "black women" (schwarze Frauen) gegeben.</sample>
    <sample id="130">Die Präsentation gibt keine spezifischen Beispiele für Modellarchitekturen, die nicht gut generalisieren. Stattdessen wird betont, dass Transformer-Modelle im Allgemeinen besser generalisieren als andere Architekturen. Die Leistung hängt von mehreren Faktoren ab, einschließlich der Modellgröße und der Anzahl der Feinabstimmungsexemplare, nicht nur von der Architektur selbst.</sample>
    <sample id="131">Die Testdatensätze werden im Video nicht explizit genannt. Es wird jedoch erwähnt, dass die Leistung der Modelle auf "clean test sets" bewertet wird.</sample>
    <sample id="132">Zwei Autoren: Akshatha und Martin.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, insbesondere mit Text und Bildern, da sie einen multi-modalen Ansatz verfolgen.</sample>
    <sample id="135">ABC-Eval, developed by the Emory NLP Lab and Amazon Alexa AI, introduces a new dimensional approach to evaluating conversational AI. Unlike traditional human evaluation methods, which rely on subjective Likert ratings or pairwise comparisons, ABC-Eval explicitly annotates specific behaviors in chat models, such as irrelevant responses, contradictions, hallucinations, and empathy. This method reduces subjectivity and provides more precise, reliable, and distinct metrics for evaluating chat quality. The study compared ABC-Eval with existing methods using four state-of-the-art chat models and found that ABC-Eval labels are more reliable (higher inter-annotator agreement) and predictive of overall conversation quality. Additionally, ABC-Eval metrics explain a significant portion of conversation quality (over 25%), while existing methods explain far less. The results highlight persistent challenges, such as common sense violations (20%), irrelevant information (15%), and contradictions (10%), in current models. ABC-Eval offers a higher-resolution evaluation framework, enabling researchers to compare models more effectively and track improvements in the field. The authors hope ABC-Eval will become a valuable tool for advancing conversational AI.</sample>
    <sample id="136">**Abstract:**  
This work introduces FERMAT (Flexible Evaluation Set for Arithmetic Types), an alternative to existing benchmarks for evaluating numerical reasoning in language models. Motivated by the poor performance of models in real-world tasks like fact-checking, FERMAT assesses models across three dimensions: number understanding, mathematical operations, and training dependency. It uses a diverse set of arithmetic questions from Illinois and CommonCore, incorporating small integers, large integers, and decimals to test model capabilities. Zero-shot evaluations reveal widespread underperformance, prompting fine-tuning with teacher-generated templates. This approach increases accuracy across tasks, highlighting the importance of linguistic and mathematical diversity. Training dependency analysis shows that models struggle to generalize even when exact expressions are encountered, emphasizing the role of linguistic context. Finally, diversifying training templates further improves performance. FERMAT fills a gap in benchmark design by providing a more informative evaluation framework, identifying areas for improvement such as number encoding and tokenization.</sample>
    <sample id="137">**Abstract:**  
We introduce *Tell2Design: A Dataset for Language-Guided Floor Plan Generation*, published in ACL 2023. This work addresses the challenge of generating floor plans directly from natural language instructions, a task requiring stricter constraints than text-conditional image generation. The *Tell2Design* dataset, comprising 5,051 human-annotated and 76,000 artificially generated instructions, captures semantics, geometry, and topology of floor plans. We propose a sequence-to-sequence model using a transformer-based encoder-decoder framework, initialized with a pre-trained language model T5, to generate structured floor plan layouts from instructions. Our approach outperforms text-conditional image generation baselines with higher Intersection over Union (IoU) scores, achieving a Micro IoU of 54 and Macro IoU of 53. Despite a language distribution gap between artificial and human instructions, warming up with artificial instructions improves performance. This work establishes a foundation for language-guided design generation, particularly in the floor plan domain, and highlights the potential for integrating user preferences into design processes.</sample>
    <sample id="138">Nach Ansicht der Autoren ist die Integration von Wissen aus verschiedenen Quellen, insbesondere Hintergrundwissen, das nur zur Inferenzzeit verfügbar ist, ein zu wenig erforschtes Gebiet im Bereich der NLU.</sample>
    <sample id="139">Ying und Zhiyang</sample>
    <sample id="140">Ja, CoScript hat eine Qualitätskontrolle durchlaufen. Crowd-sourced Workers wurden gebeten, fehlerhafte Proben zu finden und zu überarbeiten, um die Qualität des Validierungs- und Testsets zu gewährleisten.</sample>
    <sample id="141">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in ihrer **Begrenzten Anwendungsbreite** und **Abhängigkeit von Domänenwissen**. Sie unterstützen nur **eingeschränkte Arten von kontextbasierten Übersetzungen** und **begrenzte Sprachsätze**, da sie oft auf **menschlicher Kuratierung** und **Domänenkenntnissen** basieren. Dies erschwert die Bewertung von Modellen in komplexeren, realitätsnahen Szenarien.</sample>
    <sample id="142">Hallo! Ich werde über unsere Arbeit an "Resolving Indirect Referring Expressions for Entity Selection" sprechen, in der wir den AltEntities-Datensatz vorstellen. Mein Name ist Javad Hosseini und dies ist eine gemeinsame Arbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unser Ziel ist es, die Sprache der Nutzer zu verstehen, wenn sie eine Auswahl treffen möchten. Betrachten Sie diese alternative Frage. "Meinten Sie 'Easy on Me' oder 'I Gotta Feeling'?" Hier möchte ein Nutzer zwischen diesen beiden Liedern wählen. Das Offensichtlichste ist, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Liedes "Easy on Me" oder seine Position "das erste" nennt. Aber manchmal ist eine indirekte Referenz angemessener, um eine natürlichere Konversation zu führen. Dies könnte passieren, wenn sich der Nutzer den Namen des Liedes nicht erinnern kann. Oder wenn die Aussprachen zu ähnlich sind und schwer zu unterscheiden sind. Oder wenn der Nutzer eine Präferenz angeben möchte. Hier sind einige Beispiele für indirekte Referenzen, zum Beispiel "das neuere" oder "das Lied, das nicht energiegeladen ist." Dies ist ein wichtiges Problem in Konversations-Systemen und auch für die Bewertung des Entitätsverständnisses von LLMs. Uns ist kein größerer öffentlicher Datensatz für die Aufgabe bekannt, daher sammeln wir einen mit Crowd-Annotation. Unser Datensatz deckt drei verschiedene Bereiche ab: Musik, Bücher und Rezepte. Unsere Datensammlungs-Methodik legt den Schwerpunkt auf Unformalisierung unter Verwendung eines Cartoon-Abschluss-Setups. Der Cartoon hat drei Sprechblasen. In der ersten Blase sagt Bob: "Erinnern Sie sich an das Lied, das wir gestern gehört haben?" Und damit setzt Bob den Dialogkontext. In der zweiten Sprechblase sagt Alice: "Meinen Sie 'Easy on Me' oder 'I Gotta Feeling'?" Was die alternative Frage ist. Und in der dritten Sprechblase verwendet Bob eine indirekte Referenz, um eine dieser Entitäten auszuwählen, zum Beispiel "das neuere." Wir stellen die ersten und zweiten Sprechblasen automatisch zur Verfügung, aber die dritte wird vom Annotator ausgefüllt. Die erste Sprechblase wird aus einigen manuellen Hinweisen pro Bereich ausgewählt. Die zweite, die die alternative Frage ist, wird wie folgt generiert. Meinen Sie A oder B? Wobei A und B Proben aus Wikipedia sind. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir weiter oben in der Liste gehen, werden die Entitäten ähnlicher zueinander und es ist normalerweise schwieriger, die Disambiguierung vorzunehmen. Die erste ist uniform zufällig. Die zweite ist, wenn die Entitäten ähnliche Titel haben, zum Beispiel zwei Bücher mit dem Namen "The Return". Die dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben. Zum Beispiel, die gleiche Gattung oder der gleiche Künstler für ein Lied. Wenn wir diese alternative Frage den Annotatoren zeigen, wissen sie den Namen dieser Entitäten, aber sie wissen nicht unbedingt etwas über die Entitäten. Also zeigen wir ihnen einige Hintergrundinformationen über die beiden Entitäten. Für Lieder zeigen wir einfach einen Google-Suchlink zu jedem Lied und bitten dann die Annotatoren, mindestens einige von jedem Lied anzuhören und darüber zu lesen. Hier ist zum Beispiel das Google-Suchresultat für das Lied "Easy on Me." Für die Bereiche Rezepte und Bücher zeigen wir einige Hintergrundtexte aus Wikipedia. Für Rezepte zeigen wir zusätzlich ihre Bilder, wieder aus Wikipedia, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eine dieser Entitäten auszuwählen, zum Beispiel hier die erste, und sie mit drei bis fünf indirekten Bezugsausdrücken zu beschreiben. Zum Beispiel "diejenige ohne Worte", "nicht diejenige mit dem 12-jährigen Jungen" oder "die fiktive" oder "kommt aus Aserbaidschan" und so weiter. Der AltEntities-Datensatz hat 6.000 alternative Fragen in drei Bereichen und er hat 42.000 indirekte Bezugsausdrücke. Die Ergebnisse mit dem T5 XL-Modell sind wie folgt zusammengefasst. Wenn das Sprachmodell auf denselben Hintergrundinformationen wie die Annotatoren zugreifen kann, ist die Genauigkeit sehr hoch, sie liegt bei etwa 92 bis 95%. Aber das ist nicht realistisch. Wenn das Sprachmodell auf teilweise überlappende Hintergrundinformationen zugreifen kann, liegt die Genauigkeit zwischen 82 und 87%, was realistischer ist. Zum Beispiel, wenn das Sprachmodell die Hintergrundinformationen abruft. Wenn das Sprachmodell nur auf Entitätsnamen zugreifen kann, liegt die Genauigkeit nur bei 60%, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle bereichsübertragbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Der Ansatz EDAtt wird mit den folgenden bestehenden SimulST-Richtlinien verglichen:

1. **Wait-k Strategy**
2. **Local Agreement**
3. **State-of-the-art architecture specifically tailored for simultaneous pre-translation**</sample>
    <sample id="144">Die Autoren gehören der Universität Nantes an.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">**Abstract:**  
Dialogue summarization, a subtask of text summarization, aims to create concise summaries representing key information in dialogues. Despite advancements using large-scale pretrained language models, summaries often contain factual errors, with omission being a major issue. Our study analyzes the omission problem in dialogue summarization, revealing that ~70% of summaries are incomplete, and omitted information is randomly distributed across dialogue positions. To address this, we define the omission detection task at the utterance level and construct the OLDS dataset, providing high-quality omission labels for dialogue summarization. The dataset is built on five existing benchmarks across five domains, using diverse candidate summaries generated by different models. We evaluate three baseline frameworks (pair-wise classification, sequence labeling, and pointer network) for omission detection, achieving an F1-score of ~50%. Our results highlight the challenge of the task and demonstrate that refining summaries using detected omissions significantly improves quality. This work underscores the importance of addressing omission in dialogue summarization for real-world applications.</sample>
    <sample id="147">Drei Autoren sind an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi von der Universität Trient und der Fondazione Bruno Kessler und werde kurz den Artikel „Attention as a Guide for Simultaneous Speech Translation“ vorstellen, der eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist simultane Sprachübersetzung? Die simultane Sprachübersetzung, oder SimulST, ist der Prozess der Übersetzung von gesprochener Sprache in einen Text in einer anderen Sprache in Echtzeit, was die Kommunikation zwischen verschiedenen Sprachen ermöglicht. Und was sind die Probleme der aktuellen SimulST-Modelle? Üblicherweise werden spezifische Architekturen trainiert, die zusätzliche Module zur Optimierung einführen. Lange und komplizierte Trainingsprozeduren, zum Beispiel Training mit verschiedenen Optimierungszielen. Und das Training und die Wartung mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Training eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen mit zwei Sekunden Latenz und so weiter. Was ist also unsere Lösung? Erstens, bereits bestehende Offline-ST-Modelle ohne Neu-Training oder die Annahme spezifischer Architektur für SimulST zu verwenden. Verwenden Sie nur ein Modell für jedes Latenzregime und behandeln Sie die Latenz durch spezifische Parameter. Und nutzen Sie das Wissen, das das Modell bereits durch den Aufmerksamkeitsmechanismus zwischen Audioeingang und textlichem Ausgang erworben hat. Das ist der Kreuzaufmerksamkeitsmechanismus, und Sie können ein Beispiel rechts sehen. Unsere Lösung besteht darin, EDAtt, oder Encoder-Decoder-Aufmerksamkeit, vorzuschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir eine teilweise Übersetzung emittieren oder nicht, basierend darauf, wohin die Aufmerksamkeit weist. Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht konzentriert ist, das heißt, ihre Summe unter einem bestimmten Schwellenwert alpha liegt, in Bezug auf die letzten lambda Sprachrahmen, was bedeutet, dass die empfangenen Informationen ausreichend stabil sind. Zum Beispiel, wenn wir einen Sprachchunk erhalten, der „Ich werde über… sprechen“ enthält, und unser Modell die Übersetzung ins Deutsche vorhersagt, und wir uns die Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass die ersten beiden Wörter auf die frühesten empfangenen Sprachrahmen verweisen, während das letzte Wort auf die letzten empfangenen Sprachrahmen verweist, als lambda Sprachrahmen. Das bedeutet, dass die ersten beiden Wörter emittiert werden, da die Summe der Kreuzaufmerksamkeit über einem bestimmten Schwellenwert alpha liegt, werden wir das letzte Wort nicht emittieren und warten auf einen weiteren Sprachchunk. Wenn wir weitermachen und einen weiteren Sprachchunk erhalten, und unser Modell drei weitere Wörter vorhersagt, und wir uns diese Kreuzaufmerksamkeitsgewichte ansehen, werden wir sehen, dass kein Wort auf die letzten lambda Sprachrahmen verweist. Das bedeutet, dass diese drei Wörter emittiert werden. Wenn wir die Hauptergebnisse von EDAtt betrachten, werden wir die Ergebnisse der simultanen Sprachübersetzung in Diagrammen darstellen, in denen wir auf der einen Seite BLEU haben, das die Übersetzungsqualität misst, und durchschnittliche Verzögerung, das ist das Latenzmaß, und wir berücksichtigen auch die rechenbewusste durchschnittliche Verzögerung, die die Rechenzeiten des Modells zur Vorhersage des Ausgangs berücksichtigt. Wir möchten also, dass unsere Kurven in diesem Diagramm so hoch wie möglich sind, aber auch, dass sie nach links verschoben sind. Und wir vergleichen mit beliebten Strategien, die auch auf Offline-Modellen angewendet werden, die die Wait-k-Strategie und die Lokale Übereinstimmung sind. Und wir vergleichen auch mit der Stand-der-Technik-Architektur, die speziell für die simultane Vorübersetzung zugeschnitten ist. Das sind alle Ergebnisse der Strategie der simultanen Sprachübersetzung auf Deutsch. Und wir sehen, dass sie alle Strategien, die auf Offline-Modellen angewendet werden, übertrifft, da die Kurven nach links verschoben sind. Und wir sehen auch, dass, wenn wir die tatsächliche verstrichene Zeit oder die rechenbewusste Zeit betrachten, das ist die schnellste Strategie. Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unseren Artikel. Und wir haben den Code und die Modelle und die simultane Ausgabe open source veröffentlicht, um die Reproduzierbarkeit unserer Arbeit zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="149">Ja, der CoNLL++-Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">**Abstract:**  
We introduce **MeetingQA**, a novel extractive question-answering dataset based on questions and answers from meeting transcripts. This dataset addresses the underutilized QA component in meetings, capturing open-ended, discussion-seeking questions and complex answer scenarios, such as multi-speaker and multi-span answers. MeetingQA is derived from the AMI corpus, comprising 100 hours of manually transcribed meetings, and includes 7.7K questions with high inter-annotator agreement (Krippendorff’s alpha = 0.73). Questions are often yes/no or rhetorical, while answers frequently involve multiple speakers and disagreements. Our dataset highlights the challenges of QA in meeting transcripts, including rhetorical questions, multi-span answers, and speaker identification. We evaluate fine-tuned and zero-shot models, observing significant gaps in performance compared to human performance. Fine-tuned models outperform zero-shot models, with multi-span variants achieving comparable results to single-span models. Error analysis reveals difficulties in identifying rhetorical questions and speaker contributions, especially in zero-shot settings. MeetingQA demonstrates the complexity of QA in meeting transcripts and the need for further research in this domain.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung über MultiInstruct vorstellen, die Multi-modales Null-Schen-Lernen durch Anweisungsabstimmung verbessert. Mit den Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um vorab trainierte Sprachmodelle für verschiedene nachgelagerte Aufgaben auf eine parameter- und dateneffiziente Weise wiederzuverwenden. In letzter Zeit haben viele Studien gezeigt, dass Anweisungsabstimmung großen Sprachmodellen ermöglicht, auf unsichtbare Aufgaben auf Null-Schen-Weise zu reagieren, indem sie natürlichen Anweisungen folgen. Die meisten früheren Arbeiten zur Anweisungsabstimmung konzentrierten sich jedoch auf die Verbesserung der Null-Schen-Leistung bei Sprach-only-Aufgaben, während Computer Vision und multi-modale Aufgaben ausgelassen wurden. Daher möchten wir in dieser Arbeit untersuchen, ob die Anweisungsabstimmung eines multi-modalen vorab trainierten Modells tatsächlich die Generalisierung auf unsichtbare multi-modale Aufgaben verbessern kann. Zusätzlich entdeckten wir zum Zeitpunkt unserer Forschung einen erheblichen Unterschied in der Verfügbarkeit von Anweisungsdaten zwischen NLP und multi-modal. Es gibt mehr als 1600 Sprach-only-Anweisungsaufgaben. Es gibt jedoch keine groß angelegte öffentlich zugängliche multi-modale Anweisungsaufgabe. Daher motiviert uns dies, einen multi-modalen Anweisungsabstimmungsdatensatz zu erstellen. Hier stellen wir MultiInstruct vor, den ersten multi-modalen Anweisungsabstimmungs-Benchmark-Datensatz, der aus 62 verschiedenen multi-modalen Aufgaben besteht, die 10 breite Kategorien abdecken. Diese Aufgaben stammen aus 21 vorhandenen Open-Source-Datensätzen und jede Aufgabe ist mit fünf von Experten geschriebenen Anweisungen ausgestattet. Um die multi-modale Anweisungsabstimmung auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein einheitliches multi-modales vorab trainiertes Modell, als unser Basismodell. OFA verwendet ein einheitliches Vokabular für Sprache, Bild-Token und die Koordinaten eines Bounding-Box. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct-Datensatz, um die Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinheitlichen. Wir folgen der Methode von OFA und formulieren alle Aufgaben in einem einheitlichen Sequenz-zu-Sequenz-Format. In dem die Texteingabe, Bilder, Anweisungen und Bounding-Boxen im gleichen Token-Raum dargestellt werden. Nun werde ich über die multi-modale Anweisungsabstimmung sprechen. Für den Trainingsdatensatz verwenden wir 53 Aufgaben aus 9 Gruppen für das Training und wir nehmen 10.000 Instanzen pro Aufgabe. Für die Tests behalten wir die gesamte Gruppe der Alltagsvernunft für das Testen und wir wählen zusätzliche 5 Aufgaben aus den Gruppen VQ und Verschiedenes. Wir verwenden alle Instanzen im Testsatz für jede Aufgabe. Zusätzlich nehmen wir zufällig 20 Aufgaben aus dem Testsatz der natürlichen Anweisungen als unsichtbare Aufgabe für NLP. Wir verwenden das vorab trainierte OFA-Großmodell als Basismodell. Während des Trainings mischen wir alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einer ihrer fünf Anweisungsvorlagen kombiniert. Während des Tests für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer der fünf Anweisungen bewerten. In jedem Experiment berichten wir die minimale und maximale Leistung und die Standardabweichung der Leistung über alle 5 Experimente. Wenn es sich um eine multi-modale Klassifizierungsaufgabe handelt, berichten wir die Genauigkeit. Wenn es sich um eine multi-modale Generierungsaufgabe handelt, berichten wir Rouge-L. Für NLP-Aufgaben berichten wir ebenfalls Rouge-L. Wir führen auch eine zusätzliche Bewertungsmetrik namens Sensitivität ein. Diese misst die Fähigkeit des Modells, konsistent dieselben Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von der leichten Variation in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Anweisungsabstimmung die Leistung von OFA auf gesehenen multi-modalen Aufgaben erheblich verbessern. Auch das Transfer-Learning von natürlichen Anweisungsdatensätzen kann der Anweisungsabstimmung zugutekommen. Hier können wir sehen, dass das Modell mit zunehmender Anzahl von Aufgaben eine bessere Leistung und gleichzeitig eine geringere Sensitivität erreicht. Wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung gegenüber 5 Anweisungen. Wie wir sehen können, kann die Verwendung von mehr Anweisungen die Gesamtleistung des Modells verbessern und seine Sensitivität erheblich reduzieren. Dies zeigt die Wirkung verschiedener Feinabstimmungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, kann das Modell durch Transfer-Learning von natürlichen Anweisungsdatensätzen eine viel bessere Sensitivität im Vergleich zum ursprünglichen OFA-Modell erreichen. Auch können wir sehen, dass das Transfer-Learning von natürlichen Anweisungsdatensätzen OFA helfen kann, eine viel bessere Leistung auf dem natürlichen Anweisungsdatensatz zu erlangen. Insgesamt schlagen wir den ersten groß angelegten multi-modalen Anweisungsabstimmungsdatensatz mit signifikant verbesserten Fähigkeiten von OFA vor und wir erforschen verschiedene Transfer-Learning-Techniken und zeigen ihre Vorteile. Wir entwerfen eine neue Metrik namens Sensitivität. Noch eine Sache, wir sammeln einen viel größeren multi-modalen Anweisungsabstimmungsdatensatz mit rund 150 zusätzlichen Aufgaben zur visuellen Sprache und wir werden sie veröffentlichen. Dies ist ein QR-Code für unsere Daten und Modelle. Vielen Dank.</sample>
    <sample id="152">This presentation explores the integration of Natural Language Processing (NLP) with classical philology, focusing on the development of advanced language models for Ancient Greek and Latin. The speaker, Frederick Riemenschneider, highlights the current landscape of models like Latin BERT and Ancient Greek BERT, noting their limitations as encoder-only and monolingual models. To address these gaps, the team created new models: GreBERTa and GreTa for Ancient Greek, and PhilBERTa and PhilTa for multilingual capabilities (Ancient Greek, Latin, and English). A key innovation was the development of a high-quality pre-training corpus from the Internet Archive, leveraging OCR and Greek-specific stop words to identify and transcribe Greek texts accurately. The models were benchmarked on tasks like part-of-speech tagging, dependency parsing, and lemmatization, demonstrating superior performance compared to state-of-the-art models. Additionally, the team analyzed the behavior of T5 encoders and explored semantic and world knowledge capabilities. While multilingual models showed promise, monolingual models performed similarly well, suggesting that multilinguality alone does not significantly enhance performance. The work introduces native tokenizers and rigorous benchmarking, paving the way for more effective NLP tools in classical philology. For detailed insights, the paper is recommended.</sample>
    <sample id="153">In this work, we address ambiguities in text-to-image generative models, which hinder the faithful representation of user intentions. We curate a benchmark dataset, modified from LAVA, to cover various ambiguity types. Our approach involves two main steps: disambiguation and evaluation. First, we disambiguate ambiguous prompts using a language model that generates clarifying questions or alternative visual interpretations. Users then provide answers, which are concatenated with the original prompt to form a disambiguated version. Second, we evaluate the faithfulness of generated images to user intentions using a VQA model. The VQA model is trained to determine if the generated image aligns with the user's intended interpretation. Our findings indicate that our disambiguation framework effectively mitigates ambiguities, leading to more faithful image generation. Additionally, our automatic evaluation framework aligns with human evaluations, providing a reliable method to assess text-to-image models. This work contributes to improving the accuracy and reliability of text-to-image generative models by addressing and evaluating inherent ambiguities.</sample>
    <sample id="154">Die Autoren gehören der Universität Trento an.</sample>
    <sample id="155">Der Referent ist Javad Hosseini.</sample>
    <sample id="157">**Abstract:**  
Dialogue summarization aims to distill key information from multi-participant dialogues into concise summaries, enabling users to quickly grasp the main ideas. Existing methods rely on pre-computed static graph structures derived from external linguistic tools, which are prone to errors and lack adaptability. Our work, "Dialogue Summarization with Static-Dynamic Structure Fusion Graph (SDDS)," addresses these limitations by proposing a novel model that integrates both static and dynamic graph structures. The SDDS model comprises four components: (1) an **Utterance Encoder** to vectorize dialogue context, (2) a **Static-Dynamic Graph Module** to combine static graphs and dynamically capture semantic relationships using a multi-head attention mechanism, (3) a **Graph Attention Layer** to fuse static and dynamic graph representations, and (4) a **Summary Generator** to produce the final summary. We employ four heuristic methods to construct static graphs: Discourse Parsing Graph, Key Co-occurrence (KeyCo-occ), Speaker Interaction Frequency Matrix, and Utterance Position Graph. The dynamic graph module avoids pre-computed heuristics, leveraging attention mechanisms to learn relationships between utterances. Our approach enhances summarization by dynamically adapting to dialogue context and improving robustness against errors in external tools. The SDDS model’s code and data are publicly available on GitHub.</sample>
    <sample id="158">**Abstract:**  
Coreference resolution identifies mentions of entities in a document and links them to the same entity. Traditional methods face quadratic complexity, while cache-based methods reduce this to linear. However, in long documents with frequent topic shifts, the Least Recently Used (LRU) eviction policy in single caches leads to high cache misses, especially for high-frequency entities. To address this, we propose a **Dual Cache** approach combining a **local cache** (LRU eviction) for local entities and a **global cache** (Least Frequently Used, LFU) for global entities. The model processes the document left-to-right, classifying mentions and evaluating entity frequency. Qualified entities are added to the global cache, while others go to the local cache. When caches are full, eviction policies are triggered. Evaluations on four benchmarks, including a 30,000-word book, show that Dual Cache outperforms single cache methods, reduces cache misses significantly, and maintains the highest performance/cost ratio. Dual Cache is particularly effective for long documents, offering both efficiency and scalability.</sample>
    <sample id="159">Hallo, alle zusammen. Ich bin Koustav Sinha und freue mich, euch zu unserem Vortrag über unseren ACL 2023-Artikel willkommen zu heißen. Die Akzeptanzurteile von Sprachmodellen sind nicht immer kontextrobust. Dies ist eine gemeinsame Arbeit mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In dieser Arbeit nehmen wir die Minimalpaar-Paradigmen erneut unter die Lupe. Das Minimalpaar-Paradigma bewertet Sprachmodelle hauptsächlich anhand von Akzeptanzurteilen, die auch Grammatikalität wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paare umfassen können. Und dabei bewertet man typischerweise Sprachmodelle anhand von Akzeptanzurteilen für einzelne Sätze. Man zeigt ein akzeptables oder grammatikalisches Beispiel und dann ein akzeptables oder ungrammatisches Beispiel. Die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit für den akzeptablen Satz zuweist. Das aktuelle MPP-Pipeline ermöglicht es uns nicht, die Akzeptanz eines Modells für längere Sätze zu bewerten. Heutzutage entwickeln sich große Sprachmodelle mit immer längeren Kontextfenstern. Es ist daher entscheidend, die Akzeptanz der Modelle über das Kontextfenster hinweg zu bewerten, und das ist es, was wir hier versuchen zu tun. Wir versuchen, die MPP-Pipeline neu zu bewerten, indem wir das Modell bitten, Akzeptanz für längere und längere Sequenzen zu bewerten. Was wir also tun, ist, dass wir die Datensätze selbst neu bewerten und dann Sätze neu erstellen, indem wir akzeptable oder inakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir hier ein typisches Paar aus der BLiMP-Datensätze für die Adjunct Island-Regel ausgewählt. Und was wir tun, ist, dass wir längere Sequenzen erstellen, die akzeptabel sind und die gleiche grammatikalische Struktur aufweisen. Wir extrahieren grammatikalische Sätze aus der Adjunct Island-Regel und fügen sie als Präfix sowohl zum akzeptablen als auch zum inakzeptablen Beispiel hinzu. Wir können dasselbe tun, indem wir inakzeptable Sätze aus der gleichen Regel auswählen, und das könnte auch verwendet werden, um die Akzeptanz des Modells zu testen. Wir können dasselbe auch tun, indem wir Sätze aus einem anderen Teil oder einem anderen Datensatz auswählen. Das nennen wir das Mismatch-Szenario. Hier stammen die Sätze immer noch aus relevanten Datensätzen, aber nicht aus dem Datensatz, mit dem wir bewerten. Und wir können dasselbe für die In-akzeptanz tun. Schließlich können wir Sätze aus einem völlig unverbundenen Bereich wie Wikipedia auswählen. Das wird uns sagen, ob die Akzeptanzurteile der Modelle tatsächlich durch irgendeinen Kontext beeinflusst werden, wie zum Beispiel ob der Kontext aus einem anderen Teil des Datensatzes stammt oder ob er völlig irrelevant für den aktuellen Satz ist, den wir betrachten. Wie schneidet das Modell ab? Zuerst betrachten wir die Wikipedia-Sätze, die völlig irrelevant für das aktuelle Abfragepaar sind, und dort stellen wir fest, dass die MPP-Urteile für willkürliche Kontextlängen meist robust sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT-2-Modelle auszulasten. Und hier sehen wir in der orangefarbenen gestrichelten Linie, dass die MPP-Urteile relativ stabil sind. Was passiert, wenn wir Sätze aus demselben Datensatz auswählen? Hier wählen wir oder erstellen Sätze aus akzeptablen und inakzeptablen Domänen aus demselben BLiMP- oder SyntaxGym-Datensatz. Und dort sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn wir entweder akzeptable oder inakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur abgleichen, das heißt, wenn wir die Sätze aus derselben Phänomenologie in BLiMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Rückgang der MPP-Bewertung für das Modell, abhängig davon, ob das gewählte Präfix akzeptabel oder inakzeptabel ist. Nun, dieser Effekt ist sehr groß und nimmt mit der Kontextlänge zu und wird wahrscheinlich neuere Sprachmodelle beeinflussen, die ein großes Kontextfenster haben. Warum beeinflusst das Match-Präfix die MPP-Urteile des Sprachmodells so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, den Eingabe-Satz zu stören, indem wir relevante Strukturen beibehalten, aber Rauschen in die Eingabe einfügen. Nach mehreren dieser Störungen stellen wir fest, dass keines dieser Rauschsignale das Modell tatsächlich dazu bringt, seine MPP-Urteile zu ändern. Tatsächlich stellen wir fest, dass die Modelle auf ähnliche Weise auf gestörte Sätze reagieren. Wenn wir die Sätze im akzeptablen Bereich stören, sehen wir einen ähnlichen Anstieg bei allen Störungen, und wenn wir die Sätze im inakzeptablen Bereich stören, sehen wir einen Rückgang der MPP-Urteile in ähnlicher Weise. Die wichtigsten Erkenntnisse unserer Arbeit sind, dass Sprachmodelle empfindlich auf latente syntaktische und semantische Merkmale reagieren, die in den Sätzen gemeinsam sind. Und die MPP-Bewertung, wie wir sie derzeit mit kurzen und einzelnen Eingabe-Sätzen durchführen, erfasst möglicherweise nicht vollständig das abstrakte Wissen der Sprachmodelle über das Kontextfenster hinweg. Bitte lest unseren Artikel für weitere Details unserer Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Im ersten Schritt werden die Input-Token zugeordnet einem ungeordneten Multiset von Token, die im Output erscheinen werden.</sample>
    <sample id="161">55.000 Skripte sind in CoScript vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEPLAIN ist **MASSalign**.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen liegt in der Kosteneffizienz der Annotation, da sie einfache Heuristiken, Wissensdatenbanken oder kostengünstige Crowdsourcing-Quellen nutzt, anstatt teurer manueller Annotationen.</sample>
    <sample id="165">**Abstract:**  
This paper introduces LiPoR (Likelihood Learning with Posterior Regularization), an unsupervised method for abductive reasoning that learns plausible explanations without explicit supervision. Abductive reasoning aims to bridge the gap between a context (e.g., "Emily was stuck in traffic") and an outcome (e.g., "Emily made it to her flight") by identifying plausible explanations (e.g., "Her flight was delayed" or "Her flight left on time"). Traditional approaches rely on annotated data, which can be noisy and subjective. LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context, without requiring prior plausibility annotations. To ensure plausible explanations are preferred, LiPoR introduces a regularizer that exploits the mutual exclusivity of explanations. Experiments on the AlphaNLI dataset show LiPoR outperforms zero-shot models and the best unsupervised baseline by over 4 absolute points in accuracy. This work demonstrates that unsupervised learning can effectively handle abductive reasoning without relying on subjective annotations.</sample>
    <sample id="166">**Abstract:**  
We introduce a Neural Divide-and-Conquer Reasoning Framework (NDCR) for image retrieval from linguistically complex text, addressing challenges posed by highly similar images and lengthy descriptions. Inspired by the Divide-and-Conquer strategy and Dual-Process Theory, our framework integrates analogical reasoning (System 1) and logical reasoning (System 2). The Proposition Generator decomposes complex text into simple propositions, while the Visual-Linguistic Interactor performs visual-proposition interaction. The Neural-Symbolic Reasoner integrates reasoning states and results to produce the final solution. Our method outperforms baselines, as demonstrated by experimental results and ablation studies. Case studies reveal interpretable inference states and results, showcasing the framework’s interoperability. We suggest that neural symbolic reasoning, Divide-and-Conquer, and Dual-Process Theory integration can enhance compositional reasoning and planning in large language models, effectively solving complex problems.</sample>
    <sample id="167">In DEPLAIN-web, the documents were aligned using a combination of manual and automatic alignment methods. Specifically, 750 documents were manually aligned, and the remaining alignments were done using automatic alignment methods.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde durch das Sammeln von Texten aus Reuters News aus dem Jahr 2020 und deren Annotation nach den Richtlinien von CoNLL-2003 erstellt.</sample>
    <sample id="169">The paper "Prompting PaLM for Translation: Assessing Strategies and Performance" explores the effectiveness of large language models (LLMs) like PaLM (540 billion parameters) for machine translation (MT). It conducts the first systematic study on prompting strategies for LLMs in MT, comparing their performance against state-of-the-art MT systems using WMT test sets and metrics. The research highlights the significant impact of prompting on LLM translation performance, with differences in BLEURT scores reaching up to 40 points for one-shot prompts. The study favors a 5-shot prompting strategy, where source sentences are marked with their language, finding that example quality outweighs similarity to the source. High-quality examples, particularly from curated data like development sets, improve performance. While PaLM performs close to commercial systems like Google Translate, it still lags behind specialized MT systems, particularly in accuracy, often omitting parts of the source sentence for fluency. Human evaluations using the MQM framework confirm PaLM's fluency is comparable but highlight accuracy issues, especially in omission errors. The paper concludes with recommendations for effective prompting strategies in LLM-based translation.</sample>
    <sample id="170">Hallo alle zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit „XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations“ vorstellen. Semantic Parsing ist also die Aufgabe, semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda-Kalkül zu erstellen. Und Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungrepräsentationen zu übersetzen. Wie in dieser Abbildung gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mithilfe von neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Modelle für Cross-Lingual Semantic Parsing wurden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Beispielsweise gibt es viel Abdeckung für bestimmte natürliche Sprachen. Chinesisch fehlt jedoch, und es gibt eine unzureichende Abdeckung für bestimmte Bedeutungrepräsentationen. Das Lambda-Kalkül fehlt, oder sie werden nur auf bestimmten neuronalen Modellen bewertet. Beispielsweise wird nur ein einzelnes Modell zur Bewertung verwendet. Zu diesem Zweck schlagen wir XSemPLR vor. Wir stellen einen einheitlichen Datensatz XSemPLR für Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungrepräsentationen bereit. Er enthält 9 Datensätze in verschiedenen Bereichen, 5 Aufgaben für semantische Parsing, 8 Bedeutungrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unseren Benchmark besser bewerten zu können, betrachten wir die sechs Einstellungen für Training und Bewertung. Die erste ist Translate-Test. Wir verwenden die Google Translate API, um die Quelle in die Zielsprache zu übersetzen, und verwenden dann ein einsprachiges Modell für Training und Bewertung. Und zum Beispiel trainieren wir das englische Modell mit einer englischen Anfrage und verwenden während der Inferenz die deutsche Anfrage, die wir mithilfe der API ins Englische übersetzen, und verwenden dann das trainierte Modell, um die SQL vorherzusagen. Wir testen auch das einsprachige Modell. In dieser Einstellung ist die Quellsprache dieselbe wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch die einsprachige Few-shot-Einstellung, indem wir einsprachige Modelle mit nur 10 % der Trainingsdaten trainieren. Und wir testen das mehrsprachige Modell, das wir für alle Sprachen trainieren. Zum Beispiel bringen wir die deutschen, englischen und chinesischen Anfragen zusammen, um ein mehrsprachiges Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche oder chinesische Anfragen usw. zu übersetzen. Wir berücksichtigen auch den Cross-lingual Zero-shot- und Few-shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Während des Trainings trainieren wir also auf englischen Anfragen oder der Kombination aus englischen und deutschen Few-shot-Anfragen, um ein mehrsprachiges Modell zu trainieren, das die SQL-Ausgabe vorhersagen kann. Und wir finden auch viele interessante Ergebnisse. Also, was die Analyse einsprachiger Modelle betrifft, bewerten wir zwei Gruppen von Modellen, einschließlich Encoder-PTR, was für mehrsprachige vorab trainierte Encoder mit zeigerbasierten Decodern steht, wie XLM-R + PTR und mBERT + PTR. Und wir bewerten auch Encoder-Decoder-Modelle, was mehrsprachige vorab trainierte Encoder-Decoder-Modelle wie mBART und mT5 ist. Wir fanden heraus, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erzielen. Und wir bewerten mT5 und XLM-R + PTR in der mehrsprachigen Einstellung. Wir fanden heraus, dass Encoder-Decoder oder Encoder-PTR durch Training in einer Mischung aus verschiedenen Sprachen verbessert werden können. Wir fanden heraus, dass dies daran liegt, dass die meisten der wichtigsten natürlichen Sprachen eine Leistungssteigerung erzielen können, mit Ausnahme von Englisch, dessen Leistung in sieben Datensätzen sinkt und nur in drei Datensätzen steigt. Ich denke, das wird als der „Fluch der Mehrsprachigkeit“ bezeichnet. Wir vergleichen auch die Cross-Language-Leistungslücke. In dieser Abbildung ist die blaue Linie der Cross-lingual Few-shot-Transfer. Die orange Linie ist der Cross-lingual Zero-shot-Transfer. Während die grüne Linie die einsprachige Einstellung ist. Wir fanden heraus, dass die Null-shot-Einstellung, der Cross-lingual Transfer-Leistungslücke signifikant ist, und dann vergleichen wir die blauen und orangen Linien, fanden wir heraus, dass mit der Few-shot-Einstellung die Transferlücke schnell verkürzt wird. Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel übertrifft Encoder-Decoder frühere Arbeiten oder erzielt vergleichbare Ergebnisse. Das Vorabtraining auf natürlicher Sprache in Englisch kann die Leistung von Few-shot auf Ziel-natürliche Sprachen erheblich steigern, und wir fanden heraus, dass mehrsprachige Sprachmodelle wie Codex und BLOOM für Cross-Lingual Semantic Parsing-Aufgaben immer noch unzureichend sind. Zusammenfassend haben wir XSemPLR erstellt, einen einheitlichen Benchmark für Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungrepräsentationen. Wir führen eine umfassende Benchmark-Studie zu drei repräsentativen Arten von mehrsprachigen Sprachmodellen durch. Und unsere Ergebnisse zeigen viele interessante Erkenntnisse. Und so weiter. Und besuchen Sie gerne unseren Artikel und Code. Vielen Dank fürs Zuhören.</sample>
    <sample id="171">Existing works on watermarking for embedding as services can be broadly classified into four categories. However, these methods either:

1. Are not applicable to embedding as services
2. Lack transferability
3. Degrade the utility of the provided embeddings
4. Are not covert enough to attackers or are easily removable by attackers

Therefore, a new approach, Embedding Marker, is proposed to address these issues.</sample>
    <sample id="172">Nein, mehrsprachige LLMs wie Codex und BLOOM sind für Cross-Lingual Semantic Parsing (CLSP) nicht ausreichend.</sample>
    <sample id="174">**Abstract:**  
The *ArgAnalysis35K* dataset represents a significant advancement in argument quality analysis by addressing limitations of existing datasets. With 35,000 argument-analysis pairs, it is the largest of its kind, sourced from high-quality debaters, expert speeches, and diverse themes (24 in total), ensuring both diversity and quality. Unlike datasets limited to specific motions, *ArgAnalysis35K* captures arguments across multiple themes, enhancing relevance and applicability. It introduces the concept of "analysis," a coherent combination of claims, premises, and reasoning, providing deeper insights into argument structure. The dataset also employs instance-based annotator reliability, leveraging annotator expertise while mitigating biases, and includes a relevance model to quantify how arguments align with specific themes. These innovations make *ArgAnalysis35K* a comprehensive resource for NLP research, offering a more diverse, reliable, and contextually relevant dataset for argument quality analysis.</sample>
    <sample id="175">Die Methode geht mit der Mehrdeutigkeit der Permutationen um, indem sie die Alignment-Informationen während des Trainings induziert. Da die Alignment-Informationen nicht im Trainingsdatensatz gegeben sind, stellt dies eine Herausforderung dar. Die Methode löst dies, indem sie die Alignment-Informationen als Teil des Trainings induziert, was es ermöglicht, die linguistisch korrekten Permutationen zu lernen, auch wenn es mehrere mögliche Permutationen gibt.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird durch seine Fähigkeit definiert, unterschiedliche politische Perspektiven und soziale Gruppen gleichermaßen und unvoreingenommen zu behandeln. Dies beinhaltet, dass das Modell keine systematischen Vorurteile oder Diskriminierungen gegenüber bestimmten Gruppen zeigt, unabhängig von deren politischer Ausrichtung oder sozialer Zugehörigkeit.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">**Abstract:**  
Melanie Sclar introduces "SymbolicToM," an inference-time method to enhance Theory of Mind (ToM) reasoning in Large Language Models (LLMs). ToM, traditionally measured through false-belief tasks, involves understanding characters' mental states and beliefs. SymbolicToM uses graphical representations (e.g., BBob, BBob_Alice) to model these states, leveraging off-the-shelf Natural Language Inference (NLI) and Open Information Extraction (OpenIE) models. This approach avoids overfitting and provides interpretable reasoning. Experiments show significant performance gains across LLMs, including GPT-3, Macaw, and Flan-T5-XXL, on the ToMi dataset and out-of-domain tasks. Robustness is tested with datasets like D₁ (story structure generalization) and ParaphrasedToM (linguistic diversity), where SymbolicToM outperforms supervised models, achieving a 42-point accuracy boost in D₁. SymbolicToM improves interpretability and generalizability, making it a plug-and-play solution for enhancing ToM in LLMs. For details, refer to the paper.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">**Abstract:**  
This paper addresses constrained language planning, where large language models (LLMs) generate step-by-step scripts for specific goals with multi-faceted constraints, such as "make a chocolate cake." While LLMs excel at decomposing abstract goals into steps, their performance on specific goals remains unsatisfactory. We evaluate LLMs like InstructGPT, finding that while semantic completeness is acceptable, faithfulness to constraints is often compromised. To improve generation quality, we propose an over-generate-then-filter method: LLMs over-generate scripts for specific goals, and a filter model selects faithful ones based on semantic similarity and constraint keywords. This approach significantly enhances planning quality. Additionally, we distill a constrained language planning dataset, CoScript, by leveraging LLMs to generate 55,000 specific goals with scripts. Crowd-sourced workers ensure dataset quality. CoScript demonstrates high pluralism in goals and enables smaller, specialized models like T5 to surpass large LLMs when fine-tuned. Our work establishes constrained language planning as a research area, provides CoScript as a valuable dataset, and highlights the potential of smaller models for this task.</sample>
    <sample id="182">Tropikalismus im Zusammenhang mit dieser Arbeit bezieht sich auf die Darstellung von Frauen, insbesondere Frauen of Color, in stereotypen und essentialisierenden Narrativen, die sie mit exotischen, kulturellen oder traditionellen Eigenschaften assoziieren. Im Fall von Latina Frauen bedeutet dies, dass sie oft mit Begriffen wie "vibrant" und "curvaceous" beschrieben werden, die sie als exotisch und tropisch markieren, was zu einer Fremd- und Objektivierung beiträgt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen durch eine Studie erstellt, bei der sie den Probanden ähnliche natürliche Sprach-Prompts wie in ihrem Paper gegeben haben. Diese Studie diente als Grundlage für die Entwicklung ihrer Methode zur Generierung von Personas.</sample>
    <sample id="184">In dieser Arbeit wurde **Pointwise CXMI** verwendet, um die Kontextnutzung bei der Übersetzung zu messen. Dies ermöglicht die Bewertung der Kontextnutzung auf Wort- oder Satzebene und identifiziert Wörter, die für die Übersetzung Kontext benötigen.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide auf Französisch spezialisierte Modelle für biomedizinische und klinische Anwendungen, unterscheiden sich jedoch in ihren Trainingsdaten:

- **DrBERT** basiert auf dem **NACHOS-Datensatz**, einem aus dem Web gekrabbelten Datensatz mit medizinischen Inhalten.
- **ChuBERT** basiert auf **anonymisierten klinischen Daten** aus dem Datenbestand des Universitätskrankenhauses von Nantes. 

Während DrBERT also auf allgemeineren medizinischen Texten trainiert wurde, ist ChuBERT spezifisch auf klinische Notizen ausgerichtet.</sample>
    <sample id="187">Ying and Zhiyang</sample>
    <sample id="188">Iteratives Transferlernen bezieht sich auf den Prozess, bei dem ein Modell durch wiederholtes Feintuning auf verschiedenen Aufgaben oder Daten verbessert wird, um Wissen aus verwandten Aufgaben zu nutzen und die Leistung auf der Zielaufgabe zu steigern. Im Kontext der Arbeit, wird iteratives Transferlernen verwendet, um ein Modell zunächst auf eng verwandten Aufgaben (wie z.B. die Klassifizierung von Expansion und Vergleich in PDTB) zu trainieren und dann auf der Zielaufgabe (Dissonance-Erkennung) weiter zu verfeinern. Dies führt zu einer besseren Leistung des Modells bei der Erkennung von Dissonanz in der Sprache.</sample>
    <sample id="189">Das Ziel des AltEntities-Datensatzes ist es, das Verständnis von Sprachemodellen für indirekte Verweise in der Auswahl von Entitäten zu verbessern, insbesondere in Kontexten, in denen Benutzer natürliche und informelle Sprache verwenden, um zwischen verschiedenen Optionen zu wählen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen Embedding-as-a-Service (EaaS) extrahieren, indem er die bereitgestellten Embeddings analysiert und lernt, wie die Modellparameter auf bestimmte Eingaben reagieren. Dies kann durch das Training eines eigenen Modells auf den Embeddings geschehen, die von der EaaS-Dienstleistung bereitgestellt werden, ohne die ursprünglichen Modellparameter direkt zu kennen.</sample>
    <sample id="191">Drei Autoren sind an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">**Abstract:**  
The paper introduces **CAME (Confidence-guided Adaptive Memory Efficient Optimization)**, a novel optimizer designed to balance fast convergence and low memory usage in training large language models (LLMs). Traditional adaptive optimizers like Adam require significant memory for gradient estimates, while memory-efficient methods like Adafactor sacrifice performance. CAME addresses this by leveraging non-negative matrix factorization (NMF) to reduce memory demands and introduces a confidence-guided adaptive updating mechanism to mitigate erroneous updates in memory-efficient methods. By using the residual between predicted and generated updates as a denominator, CAME adaptively adjusts optimization steps, improving stability and convergence. Experiments on BookCorpus, English Wikipedia, and LLMs (BERT, GPT-2, T5) demonstrate CAME’s superiority over Adam and Adafactor, achieving up to 3.4% higher validation accuracy with the same training steps. CAME also outperforms Adam in large batch training (8K to 32K) and reduces memory usage compared to existing optimizers. Additionally, BERT-based models trained with CAME achieve comparable performance to baselines on downstream tasks with lower memory costs. CAME thus provides a memory-efficient, adaptive, and effective optimizer for large-scale LLM training.</sample>
    <sample id="193">Es wird nicht explizit erwähnt, wie viele Annotatoren für die Erstellung des ursprünglichen Datensatzes verwendet wurden.</sample>
    <sample id="194">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="195">**Abstract:**  
Explainable Question Answering (XQA) aims to provide answers to complex questions and explain the reasoning process. Existing methods, such as neuro-symbolic and decompose-based approaches, face limitations in handling incomplete knowledge bases and diverse natural language. We propose **RoHT (Reasoning over Hierarchical Question Decomposition Tree)**, a two-stage framework that addresses these challenges. First, it constructs a **Hierarchical Question Decomposition Tree (HQDT)** to hierarchically decompose complex questions into atomic sub-questions. Second, it employs probabilistic reasoning over the HQDT to fuse knowledge from heterogeneous sources (KB and text corpus) at different levels. The framework includes a scheduler, executors, and aggregator to determine, retrieve, and combine answers probabilistically. We evaluate RoHT on two datasets: **KQA Pro** (simulating an incomplete KB) and **Musique** (integrating text and KB). Results show significant improvements over state-of-the-art methods, demonstrating the effectiveness of hierarchical decomposition and integrating knowledge from diverse sources for complex XQA tasks.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite ist: "I saw Bart and Lisa".</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme basiert auf der Verwendung von humanen Bewertungen, bei denen menschliche Richter entscheiden, welche von zwei Gesprächen besser ist oder Gesprächen auf einer Likert-Skala bewerten. Diese Methoden bieten eine ganzheitliche Bewertung der allgemeinen Dialogqualität, aber Dialogqualität hat viele Aspekte. Daher wird oft eine Bewertung mehrerer Dimensionen der Chatqualität durchgeführt, um die Stärken und Schwächen des Modells auf einer feiner granulierten Ebene zu verstehen.</sample>
    <sample id="198">We need to evaluate models' acceptability judgments over the entire context window because modern large language models have significantly larger context windows, and current minimal pair paradigms (MPP) primarily assess acceptability on short, single-sentence inputs. This approach may not fully capture how models handle acceptability in longer contexts, which is crucial for evaluating their abstract knowledge and robustness across varying sentence lengths.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in sieben von neun Datensätzen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt, was als "Curse of Multilinguality" bezeichnet wird.</sample>
    <sample id="200">Nein, die Annotatoren kennen die Entität im Voraus nicht. Sie sehen die alternative Frage und müssen dann basierend auf der gegebenen Hintergrundinformation und ihren eigenen Kenntnissen eine indirekte Referenz auswählen.</sample>
    <sample id="201">Die verwendeten MT-Metriken für die Bewertung waren BLEURT und andere state-of-the-art, neural MT-Metriken.</sample>
    <sample id="202">Nein, die Regression bei der Generalisierung wirkt sich nicht spezifisch auf bestimmte NER-Typen aus. Die Studie zeigt, dass die Leistung aller untersuchten Modelle auf dem CoNLL++-Datensatz abnimmt, unabhängig vom NER-Typ. Die Hauptursache für diesen Rückgang ist der zeitliche Drift, nicht adaptive Überanpassung.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie systematische Leistungsunterschiede zwischen Technologien und verschiedenen Bevölkerungsgruppen aufdeckt. Sie zeigt, dass Daten und Modelle bestimmte Perspektiven über andere hinweg repräsentieren können und dass ihre Entscheidungen von den Positionalitäten der Entwickler und der Nutzer beeinflusst werden. Die Berücksichtigung von Positionalität hilft, Design-Bias zu erkennen und zu korrigieren, um inklusivere und gerechtere NLP-Systeme zu entwickeln.</sample>
    <sample id="204">Die mehrsprachigen LLMs wie BLOOM wurden nicht durch Adapter oder eine vollständige Feinabstimmung angepasst.</sample>
    <sample id="205">**Abstract:**  
This study investigates the propagation of political biases from pretraining data to language models (LMs) and their downstream applications, highlighting fairness issues in NLP. By evaluating LMs using political questionnaires, we found that LMs exhibit varying political leanings, with GPT-4 being the most liberal and GPT series generally more socially liberal than BART variants. Controlled experiments revealed that LMs’ ideological coordinates shift based on partisan pretraining data, indicating significant influence from training content. Temporal analysis showed LMs’ political leaning became more polarized after 2017, mirroring societal trends. We evaluated LMs on hate speech and fake news detection, observing that left-leaning LMs better detected hate speech targeting minority groups but performed worse for powerful groups, and vice versa. Right-leaning LMs showed similar patterns in fake news detection. These findings underscore fairness challenges, as biased LMs could marginalize opposing political views or fail to address hate speech targeting minority groups. The study exposes the dilemma of balancing bias mitigation with censorship risks, emphasizing the need for ethical considerations in LM development. Addressing political biases in LMs is critical to ensuring fairness and inclusivity in NLP applications.</sample>
    <sample id="206">Das Modell, das für das Transferlernen verwendet wird, ist ein kombiniertes Modell, das zunächst auf zwei eng verwandten Aufgaben trainiert wird: der topicunabhängigen Dissonance-Stance-Klassifikation (Debate) und der binären Klassifikation der Expansion- und Vergleichsklassen (CE) aus dem PDTB-Korpus. Dieses kombinierte Modell wird dann verwendet, um den aktiven Lernprozess zu starten, indem es die Gewichte von diesen beiden Aufgaben kombiniert, um eine bessere Null-Shot-Leistung auf dem annotierten Datensatz zu erzielen.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die WMT-Evaluationsdaten.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen für Model-Eigentümer vorgeschlagen.</sample>
    <sample id="209">Die vorgeschlagene Methode verbessert die Planungskompetenz von InstructGPT sowohl in Bezug auf semantische Vollständigkeit als auch auf die Einhaltung der Einschränkungen. Sie führt zu einer signifikanten Steigerung der Qualität der generierten Skripte im Vergleich zur stärksten Baseline. Insbesondere wird gezeigt, dass ein auf CoScript feinabgestimmter T5-Modell Skripte von höherer Qualität generieren kann als die meisten großen Sprachmodelle.</sample>
    <sample id="210">Shuheng</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark für die automatische Textvereinfachung verwendet werden. Die Forscher haben gezeigt, dass die Feinabstimmung von Sprachmodellen wie long-mBART und mBART zu besseren Ergebnissen als die Basislinien führt. Die Studie bietet somit einen soliden Ausgangspunkt für zukünftige Forschungen und Entwicklungen im Bereich der automatischen Textvereinfachung.</sample>
    <sample id="212">In der Arbeit wird mit **einem** kleineren Modell, **T5**, experimentiert, das auf dem CoScript-Datensatz feinabgestimmt wurde.</sample>
    <sample id="213">Das Basismodell, das für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet wird, ist OFA (Open Foundation Architecture).</sample>
    <sample id="215">This paper presents a novel argument for symmetric dependency structures in coordination, challenging asymmetric approaches. The argument is based on the principle of dependency length minimization, which favors shorter dependencies. In English, direct objects are preferred to be close to the verb, while adjuncts can be further away. However, when a direct object is very long, it can be moved away from the verb to satisfy dependency length minimization. The paper analyzes coordination structures in the Penn Treebank, finding that left conjuncts tend to be shorter, especially when the governor is on the left or absent. This tendency disappears when the governor is on the right. These findings support symmetric coordination structures, where all conjuncts are heads, over asymmetric structures, where only the first conjunct is the head. The paper provides statistical evidence and argues that dependency length minimization provides a stronger argument for symmetric coordination structures.</sample>
    <sample id="217">**Abstract:**  
We address the challenge of generating controllable dialogue with multiple attributes, where existing methods focus on single attributes or lack compositional generalization. Our work, *Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation*, proposes **Disentangled Controllable Generation (DCG)**, a model that learns attribute concepts from seen values using disentanglement loss. DCG leverages compositional prompts to guide the model’s focus on specific controllable information, combining attribute-oriented and task-oriented prompts for improved text equality and controllability. We introduce a **reference-free evaluation framework (MAE)** that does not require additional labeled data, using discrete and continuous prompts to assess different granularities of attributes. Experiments on benchmarks show DCG outperforms baselines in controllability and text equality, particularly for unseen attribute combinations. Our method disentangles attribute combinations, learns their relationships, and generalizes effectively, achieving high correlation with human judgments. This work demonstrates the effectiveness of prompt-based approaches for multi-attribute controllable dialogue generation.</sample>
    <sample id="218">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="219">This paper presents a novel approach to uncovering financial signals in annual reports, specifically focusing on the Form 10-K. The research team, led by Jia-Huei Ju, developed a compare-and-contrast multistage pipeline to highlight important changes and new information in financial reports. The pipeline consists of three stages: document segmentation, relation recognition, and fine-tuning. The relation recognition stage classifies report pairs into three types: β pairs (high similarity), revised pairs (similar syntax but different meaning), and unmatched pairs (new information). The fine-tuning stage uses an external dataset (eSNLI) for out-of-domain training and revised pairs for in-domain training. The proposed model achieves state-of-the-art performance on the FINAL dataset and maintains generalization capability on the eSNLI dataset. The authors also demonstrate the model's effectiveness on unmatched pairs, which were not used during training. The research highlights the potential of this approach for improving financial report analysis and identifying important financial signals. The team has released the FINAL dataset and code on GitHub for further exploration and application.</sample>
    <sample id="220">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="221">Die Arbeit untersuchte die Übersetzung zwischen **Deutsch und Englisch**.</sample>
    <sample id="222">This work addresses the challenge of domain adaptation in open-domain question answering (QA), where models trained on general-purpose corpora like Wikipedia struggle with specialized domains such as biomedical. The study investigates data interventions to enable out-of-domain generalization, identifying dataset shifts (concept, covariate, or full) and determining effective interventions for each. Key contributions include: (1) evaluating zero-shot and few-shot methods to generate target domain examples, improving retriever performance by 8% and reader performance by 11%; (2) analyzing the impact of question and answer formats, context distributions, and entity-based answer sampling; and (3) mapping target datasets onto a compatibility grid to classify shifts and recommend tailored interventions. For instance, datasets with concept or covariate shifts benefit from zero-shot adaptations, while those with no shift or full shift respond better to few-shot methods. Overall, the study demonstrates that domain-specific interventions significantly enhance model performance, with reader accuracy improving by up to 24%. This approach provides a systematic framework for addressing domain adaptation challenges in open-domain QA.</sample>
    <sample id="223">Der Referent*in heißt Shangbin.</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind **long-mBART** für die Dokument-Ebene und **base mBART** für die Satz-Ebene.</sample>
    <sample id="225">53 Aufgaben werden für Training und Tests verwendet.</sample>
    <sample id="226">Zwei Autoren sind an der Arbeit beteiligt: Regina Stodden und Omar.</sample>
    <sample id="227">Grounded language understanding, which maps natural language expressions to executable plans or programs in specific environments, remains challenging due to the lack of grounding during pre-training of language models. Existing approaches often rely on autoregressive generation, which can produce ungrammatical or invalid plans. We propose a novel framework, Pangu, that separates the symbolic world of plan generation from the linguistic world of language models. In Pangu, a symbolic agent proposes candidate plans, while a language model scores and ranks these candidates. This approach leverages the language model's strength in discrimination rather than generation, improving performance and sample efficiency. We instantiate Pangu on knowledge-based question answering, achieving outstanding results across fine-tuning and in-context learning settings. Notably, Pangu demonstrates strong robustness under non-i.i.d. settings, with consistent performance on seen and unseen structures. Our findings suggest that discrimination, rather than generation, is a more effective strategy for grounded language understanding. Pangu's framework is generic and applicable to various grounded tasks, offering a promising direction for future research.</sample>
    <sample id="228">Die Autoren haben ihre Experimente auf vier Datensätzen durchgeführt: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">This paper presents a study on detecting and improving argumentative claims in writing, focusing on the tasks of suboptimal-claim detection and claim improvement suggestion. The authors, Gabriella Skitalinskaya and Henning Wachsmuth, explore how to determine when an argumentative claim is optimally phrased and needs no further revision. They analyze revision histories from collaborative online debate platforms like Kialo to extract implicit patterns of claim quality and improvement. The study identifies four key challenges: representativity and reliability of revision data, model complexity and architecture sensitivity to small changes, contextual dependency of argument quality, and topical and user bias. The authors conclude that revision-based data can effectively support the tasks, and modeling the distance between claim versions aids in detecting suboptimal claims. Contextual information's impact on quality assessment varies based on the task and specific quality issues. The paper provides a detailed analysis of strategies addressing these challenges and a systematic comparison of approaches, demonstrating the feasibility of using revision-based data for improving argumentative writing.</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web, used for training the biomedical model DrBERT in French.</sample>
    <sample id="232">Der Referent ist David Vilar.</sample>
    <sample id="233">**Abstract:**  
Simultaneous speech translation (SimulST) enables real-time cross-language communication by translating spoken language into text. Current SimulST models often require specialized architectures and multiple models for different latency regimes, leading to complex training and maintenance. Our approach, **EDAtt (Encoder-Decoder Attention)**, leverages existing offline speech translation (ST) models without retraining or adopting specific SimulST architectures. It uses a single model for all latency regimes, optimizing latency through parameter control, and exploits the attention mechanism to guide translation output. EDAtt emits partial translations based on cross-attention weights, where a word is emitted if its attention is not concentrated (sum below a threshold α) on the last λ speech frames. This ensures stable information reception and avoids premature emission. Experimental results show that EDAtt outperforms strategies applied to offline models (Wait-k, Local Agreement) and state-of-the-art SimulST architectures, achieving higher translation quality (BLEU) and faster latency. It is also the fastest strategy when considering computational time. Open-source code and models are available for reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen **großen Einfluss** auf die Ergebnisse von PaLM in der Übersetzung. Die Wahl der Prompt-Methode kann zu **signifikanten Leistungsunterschieden** führen, wobei die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Bei **one-shot und zero-shot Prompting** ist die Form des Prompts entscheidend, während bei **fünf-shot Prompting** die Beispiele den größten Einfluss haben.</sample>
    <sample id="235">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="236">Die fünf Anweisungen der Expert*innen für jede Aufgabe im MultiInstruct-Datensatz sind nicht explizit im Text aufgeführt. Es wird jedoch erwähnt, dass jede Aufgabe mit fünf von Expert*innen geschriebenen Anweisungen ausgestattet ist.</sample>
    <sample id="237">Die Autoren schlagen vor, Modelle mit dem **KITMUS-Test** (KITMUS Test: Evaluating Knowledge Integration from Multiple Sources) zu testen, der eine diagnostische Testreihe für die Integration von Wissen aus verschiedenen Quellen darstellt. Der Test untersucht die Fähigkeit von Modellen, sowohl **pretrain-zeitliches** (Hintergrundwissen) als auch **inferenz-zeitliches** (entitätspezifisches Wissen) Wissen zu nutzen. Dabei werden drei Einstellungen verwendet: **Background-Pretrain**, **Background-Both** und **Background-Inference**, um die Verfügbarkeit der Informationen zu variieren und die Modelle unter verschiedenen Bedingungen zu evaluieren.</sample>
    <sample id="238">In this video, Yebowen Hu introduces MeetingBank, a benchmark dataset for meeting summarization, focusing on City Council meetings. The dataset, comprising 1,366 meetings and nearly 7,000 instances, includes transcripts, reference summaries, and URLs to additional resources. Data collection involved converting audio to transcripts using Speechmatics API and aligning timestamps with summaries. The dataset provides statistics on meeting duration, tokens, speakers, and summarization instances per city. Analysis measures coverage (70-90%) and density, revealing varying levels of abstraction and editing across cities. Evaluation of summarization models, including extractive (Oracle, LEAD, LexRank, TextRank) and abstractive (BART-Large, Pagasus, Longformer, DialogLM, HMNet), and GPT-3, shows that extractive systems perform well on content overlap, while DialogLM excels in abstractive summarization. Human evaluation highlights GPT-3’s fluency and coherence but notes its limitations in informativeness and factuality. MeetingBank serves as a valuable resource for advancing meeting summarization technologies and understanding City Council decision-making processes. The dataset is available for research use.</sample>
    <sample id="239">Hallo, alle zusammen, mein Name ist David Vilar, und ich werde eine kurze Besprechung des Artikels „Prompting PaLM for Translation: Assessing Strategies and Performance“ halten. Dies ist eine gemeinsame Arbeit mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden Parameter umfassendes großes Sprachmodell, das im letzten Jahr 2022 vorgestellt wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfassen. Zum Zeitpunkt der Veröffentlichung erreichte es den Stand der Technik in Hunderten von NLP-Aufgaben. In dieser Arbeit präsentieren wir die erste systematische Studie zur Aufforderung von großen Sprachmodellen zur maschinellen Übersetzung. Wir bewerteten die Übergangsfähigkeit solcher Modelle unter Verwendung der bewährten Praktiken der MT-Community. Dies beinhaltet die Verwendung der neuesten Testsätze, um eine Übereinstimmung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir verglichen mit den besten Systemen, also dem WMT-Bewertungssystem. Wir verwenden die neuesten, neuronalen MT-Metriken und zeigen zusätzlich auch Ergebnisse der fachkundigen menschlichen Bewertung. Schließlich geben wir einige Empfehlungen für Strategien zur Auswahl von Aufforderungen. Die Aufforderung hat einen großen Einfluss auf die Leistung der LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir eine einmalige Aufforderung verwendeten und für jeden Satz zwei verschiedene Aufforderungen bereitstellten. Die Mehrheit der Sätze, 516 von 1.000. Der beobachtete Unterschied beträgt mehr als einen BLEURT-Punkt. Und dies kann in extremen Fällen bis zu 40 BLEURT-Punkte betragen. Es ist also wichtig, eine gute Aufforderungsstrategie auszuwählen. In unseren Experimenten entschieden wir uns für eine Aufforderungsstrategie mit fünf Schüssen, bei der wir jeden Satz, den wir dem System bereitstellen, mit der Sprache markieren, in der er ist. So in diesem Beispiel hier, wo wir die Übersetzung von Deutsch ins Englische durchführen, werden die deutschen Sätze, die Quelltexte, mit einem deutschen Doppelpunkt und die englischen Übersetzungen mit einem englischen Doppelpunkt markiert. Wir stellten fest, dass die tatsächliche Form der Aufforderung im Falle mehrerer kurzer Aufforderungen keinen großen Einfluss hat. Sie ist entscheidend für null- und einmalige Aufforderungen. Und wenn wir, wie in unserem Fall, zu fünf Aufforderungen gehen, gibt es fast keinen Unterschied zur tatsächlichen Form der Aufforderung. Es sind die Beispiele, die den größten Einfluss haben. Die Zusammenfassung unserer experimentellen Ergebnisse ist, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zum Quelltext. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen auszuwählen. Insbesondere vergleichen wir die Auswahl von Aufforderungen aus den Trainingsdaten für die WMT-Bewertungen auf den Entwicklungsdaten. Die Entwicklungsdaten sind viel kuratiert und von höherer Qualität als die Trainingsdaten, die lauter sind. Und ihre Ergebnisse zeigen eine bessere Leistung bei der Verwendung der Entwicklungsdaten. Dennoch haben spezialisierte, modernste Systeme einen erheblichen Vorteil gegenüber den PaLM-Übersetzungen. Aber PaLM kommt ziemlich nah an ein kommerzielles System heran. In unserem Fall entschieden wir uns, mit Google Translate zu bewerten. Die Erkenntnisse, die wir aus der menschlichen Bewertung gewonnen haben, die wir unter Verwendung des MQM-Rahmens durchgeführt haben, sagten, dass die Fließfähigkeit von PaLM mit den besten Systemen vergleichbar ist, aber der Hauptunterschied kommt von der Genauigkeit. Insbesondere sind die häufigsten Fehler Omissionsfehler. Es scheint also, dass PaLM eine bessere klingende Übersetzung produzieren möchte, manchmal indem er Teile des Quellsatzes, die in der Übersetzung gemacht werden, weglässt. Die Kategorie „Stil/Unbeholfen“ für PaLM ist jedoch niedriger als für die besten Systeme, was ein zusätzliches Signal dafür ist, dass PaLM wirklich fließende Ausgaben liefert, aber immer noch mit einigen Genauigkeitsproblemen. Und das war es für diesen wirklich kurzen Überblick. Für weitere Details, bitte kommen Sie zur vollständigen Präsentation des Artikels. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Universität des Saarlandes in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit „Weaker Than You Think: A Critical Look at Weakly Supervised Learning“ vorstellen. Dies ist eine gemeinsame Arbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung in schwache Überwachung und schwach überwachtes Lernen beginnen. Bei schwacher Überwachung kennzeichnen Sie die Daten nicht manuell. Stattdessen kennzeichnen wir die Daten mit schwachen Kennzeichnungsquellen, wie einfachen heuristischen Regeln, Wissensbasen oder minderwertigen Crowdsourcing-Diensten, wie in der Abbildung rechts dargestellt. Im Vergleich zu menschlichen Annotationen sind die schwachen Annotationen viel billiger, aber sie sind auch verrauscht, was bedeutet, dass ein gewisser Anteil der Annotationen falsch ist. Wenn wir neuronale Netze direkt auf schwach gelabelten Daten trainieren, neigen die neuronalen Netze dazu, den Label-Rausch zu verinnerlichen und verallgemeinern nicht gut. Im schwach überwachten Lernen werden Trainingsalgorithmen vorgeschlagen, um neuronale Netze robust unter solch einem Label-Rauschen zu trainieren, sodass die trainierten Modelle sich gut verallgemeinern. In jüngsten Arbeiten im WSL, wobei WSL für schwach überwachtes Lernen steht, wird oft behauptet, dass man Modelle nur auf den schwach gelabelten Daten trainiert und hohe Leistung auf sauberen Testdatensätzen erreicht. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, nämlich dass man annimmt, dass ein zusätzlicher sauberer Validierungsdatensatz für die Modellselektion verfügbar ist. Wir können uns nicht auf diese Problemstellung beschränken, aber dies impliziert, dass zusätzliche manuelle Annotationen im schwach überwachten Lernen erforderlich sind. Die oben genannte Frage wird gestellt, um drei Forschungsfragen zu stellen. Erstens, ist sauberer Validierungsdatensatz für WSL notwendig oder können wir stattdessen einen verrauschten Validierungsdatensatz verwenden? Zweitens, wenn sauberer Datensatz erforderlich ist, oder wenn sauberer Datensatz für WSL obligatorisch ist, wie viele saubere Proben benötigen wir dann? Drittens, sollten wir die sauberen Proben nur für die Validierung verwenden, oder gibt es bessere Möglichkeiten, sie zu nutzen? Wir haben diese Forschungsfragen in unserer Arbeit angegangen und unsere Ergebnisse sind wie folgt. Erstens stellen wir fest, dass neuere WSL-Methoden tatsächlich saubere Validierungsproben benötigen, um richtig zu funktionieren. Andernfalls gibt es einen großen Leistungsabfall. Wie in dieser Abbildung gezeigt, können die trainierten Modelle nicht über die ursprünglichen schwachen Labels hinaus verallgemeinern, was bedeutet, dass das Training sinnlos ist. Dies deutet darauf hin, dass WSL-Ansätze tatsächlich sauber gelabelte Daten benötigen, um richtig zu funktionieren, und die Annotationskosten für die Beschaffung sauberer Validierungsproben sollten nicht außer Acht gelassen werden. Unsere zweite Erkenntnis ist, dass die Anzahl der sauberen Validierungsproben die Leistung von WSL-Ansätzen verbessern wird, wie in der Abbildung links gezeigt. Typischerweise benötigen wir nur 20 Proben pro Klasse, um eine hohe Leistung zu erzielen. Aber das ist noch nicht das Ende der Geschichte, denn wenn wir uns entscheiden, saubere Proben zu nutzen, wird das direkte Training auf ihnen sogar eine bessere Leistung erzielen. Die rechte Abbildung zeigt den Leistungsunterschied zwischen Feinabstimmungsansätzen, die direkt auf den sauberen Daten angewendet werden, und WSL-Ansätzen, die die sauberen Daten nur für die Validierung verwenden. Wie wir sehen können, wenn wir 10 Proben pro Klasse haben, beginnt das direkte Feinabstimmen, WSL-Ansätze zu übertreffen. Schließlich kann die in früheren WSL-Ansätzen behauptete Leistungsverbesserung leicht erreicht werden, indem man das weitere Feinabstimmen auf den sauberen Validierungsproben zulässt. Wie wir den Abbildungen entnehmen können, unterlebt das einfache Modell, bezeichnet als FTw, zunächst den komplexeren WSL-Methoden wie COSINE. Wenn wir jedoch das weitere Feinabstimmen auf den sauberen Proben zulassen, erreicht FTw eine gleich gute Leistung wie andere Methoden. In der Praxis gibt es also keinen Grund, komplexere WSL-Methoden zu wählen, die mehr Rechenzeit und Speicherplatz erfordern. Zusammenfassend haben wir gezeigt, dass neuere WSL-Ansätze saubere, manuell annotierte Proben benötigen, damit sie richtig funktionieren. Ihr Leistungsgewinn und ihre Praktikabilität werden stark überschätzt. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind wie folgt. Erstens, berichten Sie über die Modellselektionkriterien. Zum Beispiel, berichten Sie, ob die Modellselektion mit sauberen Validierungsproben durchgeführt wird. Zweitens, WSL-Ansätze sollten mit Few-Shot-Learning-Grundlinien verglichen werden, da beide auf sauberen Proben arbeiten. Drittens, kontinuierliches Feinabstimmen ist eine einfache, aber starke Grundlinie, die in zukünftigen Arbeiten im WSL berücksichtigt werden sollte. Abschließend haben wir unseren Code open-source gemacht. Sie finden ihn über den QR-Code auf dieser Folie. Bitte zögern Sie nicht, ihn zu überprüfen. Vielen Dank und viel Spaß auf der Konferenz.</sample>
    <sample id="241">This paper presents a human-in-the-loop evaluation framework for early misinformation detection, focusing on COVID-19 treatment claims. The authors address two key deficiencies in existing automated systems: unrealistic evaluation using retrospective datasets and insufficient human involvement. Their framework integrates human feedback at multiple stages, from claim detection to policy violation verification. The system uses a T5 model for claim extraction and a BERT-based stance classification model for policy violation detection. Early detection is defined as identifying unapproved treatments before their debunking in news articles, demonstrating the system's utility. Evaluation results show a 65% accuracy rate in policy violation detection and 124.2 violations confirmed per human hour. The framework provides a realistic end-to-end evaluation of human-in-the-loop systems, motivating future developments and offering industry insights into misinformation detection challenges.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme umfassen:

1. **Human Evaluation**: Menschen bewerten die Qualität von Gesprächen zwischen einem Dialogsystem und einem Benutzer, oft durch Auswahl der besseren Konversation oder durch Verwendung einer Likert-Skala.
2. **Likert-Bewertungen**: Auf der Gesprächsebene oder der Gesprächsstufe werden die Qualität der Konversationen bewertet.
3. **Paarweise Vergleiche**: Zwei Konversationen werden verglichen, um die bessere zu bestimmen.</sample>
    <sample id="243">Die Arbeit wurde von 6 Autoren verfasst: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke, Maarten Sap und die Autoren des Prospective API.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass "Judges decide cases in law courts", um die richtige Referenz des Pronomens "he" zu bestimmen.</sample>
    <sample id="245">This paper presents a two-step pipeline for identifying high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks. The pipeline includes a qualification task testing annotators' ability to evaluate multiple dimensions correctly, categorizing workers into gold, silver, bronze, and block types, with only gold and silver passing. The endurance task evaluates workers' capacity for handling heavy workloads. Results show that pipeline workers achieve higher inter-annotator agreement (IAA) than experts. The reference-based task tests general performance, with 8 out of 12 pipeline workers completing all HITs. Baseline and CloudResearch MTurk workers are also compared, with pipeline workers performing similarly to CloudResearch. The analysis of correctness across annotation sources reveals significant Spearman's correlation between pipeline and CloudResearch workers, while real GPT models correlate well with expert judgments. The pipeline avoids resource waste, achieves high agreement at lower cost, and serves as a best practice for large-scale, high-quality annotations. Limitations include testing only English summarization on MTurk and the lack of guarantee for correctness training.</sample>
    <sample id="246">Ja, der Code ist verfügbar und kann auf GitHub eingesehen werden.</sample>
    <sample id="247">**Abstract:**  
This paper introduces **FactKG**, a novel dataset for **Knowledge Graph-Based Fact Verification**, addressing the gap in datasets that utilize knowledge graphs (KGs) as evidence for natural language claims. Unlike existing datasets like FEVER and VitaminC, which rely on text, or TabFact and InfoTabs, which use tables, FactKG leverages the **DBpedia KG** to enable direct and reliable reasoning between claims and evidence. The dataset includes claims in both written and colloquial styles, reflecting practical use cases, and is labeled as **SUPPORTED** or **REFUTED**. It incorporates five reasoning types: **one-hop**, **conjunction**, **existence**, **multi-hop**, and **negation**, allowing for diverse verification tasks. For example, one-hop claims require verifying a single triple, while multi-hop claims involve complex inference paths. To enhance practicality, the colloquial style was generated using a transfer model and presupposition templates. Experimental results show that baselines using graph evidence outperform claim-only baselines and majority-class predictions, with the **GEAR model** achieving the best performance. FactKG aims to improve consistency checks in applications like dialogue systems and other tasks requiring KG-NL alignment. The dataset is publicly available for further research.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Die Annotations wurden von über 1000 Annotatoren aus 87 Ländern durchgeführt, aber die Daten zeigen, dass die Modelle und Datensätze am besten mit englischsprachigen Ländern und Personen mit Hochschulausbildung übereinstimmen. Dies deutet darauf hin, dass die Annotatoren möglicherweise nicht in Bezug auf alle demographischen Gruppen repräsentativ sind.</sample>
    <sample id="249">Sätze innerhalb der akzeptablen Domain wurden durch Hinzufügen von akzeptablen Sätzen als Präfix zu beiden, akzeptablen und inakzeptablen, Beispielen durcheinandergebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, dass die Qualität einer Konversations-KI in verschiedene Aspekte oder Dimensionen unterteilt wird, wie z.B. Relevanz der Antworten, Selbst- oder Partner-Widersprüche, Empathie, und andere. Diese Methode ermöglicht eine detailliertere und präzisere Bewertung der KI im Vergleich zu einer allgemeinen, globalen Bewertung.</sample>
    <sample id="251">Die Autoren gehören der University of Science and Technology of China an.</sample>
    <sample id="252">**Abstract:**  
This work presents U-CREAT (Unsupervised Case Retrieval using Events Extraction), a novel approach to the Prior Case Retrieval (PCR) task in legal domains. The IL-PCR dataset, a new benchmark with 7,070 Indian legal cases and 6.775 average citations per query, is introduced to evaluate PCR algorithms. U-CREAT leverages unsupervised learning and an event-based pipeline for efficient and domain-agnostic retrieval. It extracts events from case documents using dependency parsing and constructs an interaction matrix to rank candidates. Experiments demonstrate that event-based models, particularly Event Filtered Documents, outperform baseline and transformer-based methods, achieving higher F1 scores and lower inference times. U-CREAT also outperforms existing approaches on the COLIEE’21 dataset, establishing a state-of-the-art method. This work highlights the importance of event-based techniques and tailored models for legal text processing, paving the way for further advancements in PCR.</sample>
    <sample id="253">**Abstract:**  
This work introduces *DisorBERT*, a double domain adaptation model designed to detect signs of mental disorders in social media posts. Mental disorders, such as depression and anxiety, are psychological syndromes causing distress and disability, and social media provides a rich source of data to study these conditions. The model adapts BERT, a pre-trained language model, to specialize in the social media and mental health domains. It integrates Reddit-specific language and mental health lexicon to guide the masking process, focusing on domain-relevant terms. Results using the eRisk dataset show *DisorBERT* achieves a balanced precision and recall, outperforming baselines and MentalBERT. Analysis reveals *DisorBERT* generates more psychologically oriented predictions, such as words related to depression and anxiety, compared to BERT. Visualization of attention scores in user posts highlights relevant topics like anxiety and medication. The approach effectively captures mental disorder signs in social media, demonstrating the potential for early detection and support. Future work aims to explore additional lexical resources and clinical data integration.</sample>
    <sample id="254">**Abstract:**  
We present a framework for document-level distant relation extraction (DocRE) that addresses the noise problem in distantly supervised data (DS) through uncertainty-guided label denoising. Our approach leverages both DS and human-annotated data to generate pseudo labels, while introducing uncertainty estimation to filter unreliable predictions. To handle overlapping relations, we propose an instance-level uncertainty estimation method, which calculates confidence scores for each relation class. Dynamic class uncertainty thresholds are then applied to filter high-uncertainty pseudo labels, improving label quality. A multi-phase training strategy iteratively refines DS data, further enhancing model performance. Our framework outperforms strong baselines on public datasets, demonstrating significant improvements in accuracy. Key contributions include uncertainty-guided denoising, instance-level uncertainty estimation for overlapping relations, a dynamic threshold strategy for long-tail classes, and superior performance on DocRE tasks.</sample>
    <sample id="255">Die Form des Prompts ist in den Fällen wichtig, wenn es sich um **Zero-Shot** oder **One-Shot**-Prompting handelt. Bei mehrschüssigem Prompting (z. B. fünfschüssigem Prompting) hat die Form des Prompts hingegen kaum Einfluss auf die Leistung.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Chat-Modelle evaluiert.</sample>
    <sample id="258">This paper explores the feasibility of using large language models (LLMs) as an alternative to human evaluation in natural language processing (NLP). The authors propose leveraging LLMs to rate text quality based on natural language instructions, aiming to address the instability and reproducibility issues inherent in human evaluations. To test this, they conducted an experiment where LLMs (T0, InstructGPT, and ChatGPT) rated stories generated by GPT-2 or humans across four attributes: grammar, coherence, likability, and relevance. Ground-truth ratings were provided by English teachers, who generally preferred human-written stories over GPT-2-generated ones. While smaller LLMs showed no meaningful preference, Davinci and ChatGPT demonstrated a clear preference for human-written text, aligning with human raters. The study highlights the potential of LLMs as a reliable alternative to human evaluation, though further research is needed to address factors like instruction wording, response sampling, and cost-benefit analysis. The paper also discusses broader implications for NLP tasks.</sample>
    <sample id="259">**Abstract:**  
We present XSemPLR, a unified benchmark for cross-lingual semantic parsing across multiple natural languages and meaning representations. Existing models often lack coverage for specific languages or representations, such as Chinese or Lambda calculus. XSemPLR addresses this by providing a comprehensive dataset with 9 datasets across 22 languages and 8 meaning representations. We evaluate six settings: Translate-Test, Monolingual, Monolingual Few-shot, Multilingual, and Cross-lingual Zero-shot/Few-shot transfer. Our findings show that Encoder-Decoder models outperform Encoder-PTR models and achieve the best results across all datasets. Multilingual training improves performance for most languages, though English performance sometimes declines. The cross-lingual transfer gap is significant in Zero-shot settings but narrows with Few-shot transfer. Additionally, pretraining on English enhances Few-shot performance on target languages, while models like Codex and BLOOM remain inadequate for cross-lingual semantic parsing. XSemPLR serves as a robust benchmark for advancing multilingual semantic parsing research.</sample>
    <sample id="260">Jingwei Yi ist der Autor der Arbeit.</sample>
    <sample id="261">Ein guter Planer sollte **vernünftige und den Constraints entsprechende Skripte** erstellen. Dies beinhaltet sowohl **semantische Vollständigkeit** in den generierten Skripten als auch die **Treue zu den Constraints**.</sample>
    <sample id="262">Die Arbeit wurde von Siyu Yuan von der Fudan University vorgestellt, aber der genaue Anzahl der Autoren wurde nicht genannt. Basierend auf der Präsentation kann man davon ausgehen, dass es sich um eine Einzelarbeit handelt, die von Siyu Yuan vorgestellt wurde.</sample>
    <sample id="263">This work systematically investigates label biases in in-context learning for large language models, a prevalent paradigm for leveraging these models. In-context learning is unstable due to design choices like the selection and order of examples, which introduce biases affecting model predictions. Existing research lacks a comprehensive categorization of these biases and effective mitigation strategies. The study identifies three key types of label biases: vanilla-label bias (model’s uncontextual preferences), context-label bias (contextual effects), and a novel type, domain-label bias (task corpus influence). Experiments demonstrate that domain-label bias significantly impacts model performance, especially when exposed to random in-domain words. To address this, the authors propose **domain-context calibration**, a holistic method that uses random in-domain words as content-free text to estimate and mitigate biases. This approach outperforms prior methods, particularly on tasks with high domain-label bias. Results show improved decision boundaries and consistent performance across datasets and models, including GPT-3. The study highlights the importance of considering domain-specific biases for robust in-context learning.</sample>
    <sample id="264">**Abstract:**  
This paper introduces **TAVT: Towards Transferable Audio-Visual Text Generation**, addressing the challenge of multimodal domain shifts in audio-visual text generation. Unlike uni-modal tasks, multimodal generation requires costly data annotation and suffers from domain-specific degradation. TAVT proposes a unified audio semantic space to align visual concepts across domains, leveraging audio features like rhythm and energy, which remain stable despite visual changes. The framework consists of three components: an **audio-visual meta-mapper network** (AVMM) that maps visual concepts to a unified auditory space, a **transformer-based encoder and generator** with modality-specific attention weights, and **Dual Counterfactual Contrastive Learning (DCLL)** for fine-grained alignment optimization. Meta-training ensures fast adaptation to new domains with limited labeled data. Experimental results on MSVD and MSR-VTT benchmarks demonstrate TAVT’s superiority over state-of-the-art models, particularly in low-resource domains, and highlight the effectiveness of audio features in improving performance. TAVT achieves significant gains in cross-dataset and cross-domain settings, showcasing its robustness and adaptability.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Die Autoren gehören der Universität von Amsterdam an.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind **Omissionen** (Auslassungen), d. h., es werden Teile des Quelltextes in der Übersetzung weggelassen.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute erzählen wir euch alles über ABC-Eval, einen neuen dimensionalen Ansatz zur Bewertung von konversationellen KI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Nehmen wir also an, Sie haben gerade ein Dialogmodell entwickelt und möchten herausfinden, wie gut es sich im Vergleich zum aktuellen Stand der Technik schlägt. Die gängige Praxis besteht darin, menschliche Evaluatoren zu verwenden, wie zum Beispiel menschliche Richter zu bitten, auszuwählen, welche von zwei Konversationen besser ist, oder Konversationen auf einer Likert-Skala zu bewerten. Diese Ansätze eignen sich gut, um ganzheitliche Bewertungen der allgemeinen Dialogqualität zu liefern, aber Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feiner granulierten Ebene zu verstehen. Ein Ansatz besteht darin, menschliche Richter einfach zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie zum Beispiel die Relevanz der Modellantworten unter Verwendung bestehender vergleichender oder Likert-Skalenmethoden. Wir glauben jedoch, dass es eine präzisere und zuverlässigere Strategie für die dimensionale Dialogbewertung gibt. Unser Ansatz versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem wir explizit annotieren, ob jede Modellantwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Antwort mit irrelevanten Informationen oder sich selbst zu widersprechen. Wir nennen diesen Ansatz das Annotating Behaviors in Chat oder kurz ABC-Eval. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chatmodellen umfassend abzudecken, die in der jüngsten Literatur als beeinflussend für die Chatqualität vorgeschlagen wurden. ABC-Eval ist in der Lage, die Raten zu messen, mit denen Chatmodelle verschiedene thematische Fehler begehen. Zum Beispiel misst ABC-Eval die Anzahl der Zügen, in denen ein Chatmodell seinen Partner ignoriert oder etwas Ir relevantes sagt, sich selbst oder seinen Partner widerspricht, falsche Fakten halluziniert oder gegen das gesunden Menschenverstandswissen verstößt, und wenn das Modell Empathie zeigt oder versagt. Um zu bestimmen, welche Art von Bewertung am effektivsten ist, haben wir vier State-of-the-Art-Chatmodelle ausgewählt und diese auf 100 Mensch-Bot-Konversationen pro Modell unter Verwendung von ABC-Eval bewertet. Zum Vergleich haben wir diese Konversationen auch mit drei bestehenden Methoden bewertet: Likert-Bewertungen auf der Zug-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarweise-Vergleiche. Für jede der bestehenden Methoden haben wir Bewertungen zu acht der am häufigsten gemessenen Aspekten der Dialogqualität gesammelt, da dies die Standardpraxis für die Bewertung von Chatmodellen entlang mehrerer Dimensionen ist. Aus unserer Analyse dieser Bewertungsergebnisse haben wir festgestellt, dass ABC-Eval-Verhaltensbeschriftungen insgesamt zuverlässiger sind als Beschriftungen, die mit bestehenden Methoden gesammelt wurden, gemessen an der Inter-Anonymen-Vereinbarung auf 100 doppelt beschrifteten Konversationen. Darüber hinaus sind ABC-Eval-Beschriftungen vorhersehbarer für die Gesamtqualität der Konversation im Vergleich zu Metriken, die mit bestehenden Methoden produziert wurden, wie durch diese einfache lineare Regressionsanalyse gezeigt. Zum Beispiel können Sie sehen, wie das Messen des Anteils der Züge mit Selbst- und Partnerwidersprüchen 5 % bzw. 10 % der Gesprächsqualität erklärt, während die durchschnittlichen Likert-Konsistenzscores nur 4 % oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmetrik einen einzigartigen Aspekt der Chatqualität erfasst, unter Verwendung einer schrittweisen linearen Regression. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25 % der Gesprächsqualität erklärt, und wenn Sie die Metriken nacheinander entfernen, führt dies in den meisten Fällen zum Verlust einer beträchtlichen Menge an Informationen über die Qualität. Andererseits erklärt die Kombination aller Likert-Metriken auf der Zug-Ebene weit weniger der Qualität, und weniger dieser Metriken tragen einzigartige Informationen. Diese zuverlässigen, informativen und unterschiedlichen ABC-Eval-Metriken ermöglichen es uns, konversationelle KI mit einer höheren Auflösung zu bewerten, als es frühere Methoden können. Sie können sehen, dass in den Ergebnissen unseres Experiments noch mehrere Herausforderungen bestehen und präzise quantifiziert wurden. Zum Beispiel begehen die von uns getesteten Bots in etwa 20 % ihrer Antworten Verstöße gegen das gesunde Menschenverstandswissen. Sie produzieren in etwa 15 % der Antworten irrelevante Informationen, und sie widersprechen sich oder ihrem Partner in etwa 10 % der Zeit. Mit dem raschen Verbesserungstempo in diesem Bereich könnten viele dieser Fehlerraten bei neuen Modellen, die seit unserer Bewertung veröffentlicht wurden, abnehmen. Dies ist jedoch umso mehr ein Grund, zuverlässige und präzise Bewertungsmetriken zur Vergleichbarkeit von Modellen zu verfolgen. Wir hoffen, dass ABC-Eval von anderen in diesem Bereich als bedeutender Schritt in diese Richtung genutzt werden kann. Und wir freuen uns darauf zu sehen, wie sich die konversationelle KI in den kommenden Monaten und Jahren weiterentwickeln wird. Vielen Dank fürs Zuschauen.</sample>
    <sample id="270">Die Autoren gehören der Emory University an.</sample>
    <sample id="271">CFT steht in dieser Arbeit für "Continuous Fine-Tuning".</sample>
    <sample id="272">Die Arbeit wurde von sieben Autoren verfasst: Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit mit dem Titel „Wann erfordert Übersetzung Kontext? Eine datengesteuerte, mehrsprachige Erkundung“ vorstellen. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen also vom Kontext ab. Zum Beispiel, wie würden wir „mole“ in diesem Satz übersetzen? Nun, wenn der vorherige Satz „Die Dinge könnten gefährlich werden, wenn die Minister das herausfinden“ lautet, bezieht sich „mole“ auf einen Spion. Aber wenn der vorherige Satz „Könnte es etwas Ernstes sein, Doktor?“ lautet, bezieht sich „mole“ auf einen Muttermal. Je nach Kontext ändert sich also die Bedeutung des Wortes und damit auch seine Übersetzung. Es ist jedoch ziemlich schwierig zu bewerten, wie gut Modelle solche Fälle übersetzen können. Erstens, weil nur ein kleiner Teil der Übersetzungen vom Kontext abhängt, was bedeutet, dass Korpus-Level-Metriken wie BLEU diese Übersetzungen nicht erfassen können. Einige Leute haben vorgeschlagen, gezielte Bewertungen von kontextabhängigen Übersetzungen durchzuführen, aber diese Ressourcen unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachsätze, da sie normalerweise auf Domänenkenntnissen und menschlicher Kuratierung basieren. In dieser Arbeit versuchen wir, diese zwei Fragen zu beantworten. Erstens, wann erfordert Übersetzung Kontext? Und zweitens, wie gut bewältigen Modelle diese Fälle? Um die erste Frage zu beantworten, haben wir begonnen, zu messen, wie stark ein Wort vom Kontext bei der Übersetzung abhängt. In früheren Arbeiten haben wir CXMI als Maß für die Kontextnutzung durch maschinelle Übersetzungssysteme eingeführt. Dies geschieht, indem gemessen wird, wie viel Information der Kontext C über das Ziel Y liefert, gegeben die Quelle X. Man kann sich CXMI als die Information vorstellen, die durch das Geben von Kontext an das Modell gewonnen wird. In dieser Arbeit erweitern wir CXMI zu Pointwise CXMI, das die Kontextnutzung auf Satzebene oder auf Wortebene messen kann. Wörter mit hohem P-CXMI können als solche angesehen werden, die für die Übersetzung Kontext erfordern. Wir analysieren nun Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu suchen. Und wir führen unsere Analyse auf Transkripten von TED-Talks durch, die aus dem Englischen in 14 verschiedene Sprachen übersetzt wurden. Wir führen unsere Analyse auf drei verschiedenen Ebenen durch. Zuerst betrachten wir Teile des Sprachgebrauchs, die ein hohes durchschnittliches P-CXMI aufweisen. Dies ermöglicht es uns, zum Beispiel Dualpronomen im Arabischen zu finden, die relativ hohe P-CXMI-Werte aufweisen. Dies lässt sich erklären, weil Englisch keine Dualpronomen hat, sodass man Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn man ins Arabische übersetzt. Und ähnlich finden wir heraus, dass bestimmte Sprachen auch Kontext erfordern, wenn wir die passende Verbform wählen möchten. Dann betrachten wir Vokabularartikel, die ein hohes P-CXMI im Durchschnitt aller ihrer verschiedenen Vorkommen aufweisen. Dies hilft uns, Fälle wie den hier zu identifizieren, wo man im Chinesischen Kontext benötigt, um Eigennamen zu übersetzen, um sicherzustellen, dass man innerhalb des Dokuments dieselbe Übersetzung verwendet. Und ähnlich finden wir heraus, dass Kontext wichtig ist, um die richtige Form der Höflichkeit zu übersetzen. Und schließlich betrachten wir verschiedene einzelne Token, die ein hohes P-CXMI aufweisen. Dies ermöglicht es uns, Phänomene zu identifizieren, die mit dem Wort selbst nicht wirklich erfasst werden können, sondern die eher in der Satzstruktur ausgedrückt werden, wie zum Beispiel die Auflösung von Ellipse. Jetzt verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Dokument-Level-Übersetzung zu entwerfen. Für jedes der fünf identifizierten Diskurs-Phänomene erstellen wir Tagger, um Wörter automatisch zu identifizieren, die zum Phänomen gehören. Und wir nannten unseren Tagger den Multilingual Discourse-Aware, oder MuDA-Tagger. Wir können auch feststellen, dass verschiedene Sprachen unterschiedliche Anteile dieser Diskurs-Phänomene aufweisen. Dann verwenden wir den MuDA-Tagger, indem wir den Tagger auf einen Parallelkorpus anwenden, den wir für die Bewertung verwenden möchten, und wir wenden unsere Übersetzungsmessungen der Wahl auf die kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Und schließlich verwenden wir unseren Benchmark sowie andere Metriken, um verschiedene Modelle auf der Dokument-Level-Maschinentranslation zu bewerten. Zunächst einmal, wenn wir Korpus-Level-Metriken verwenden: so finden wir bei BLEU, dass kontextunabhängige Modelle die beste Leistung erzielen. Aber dann, wenn wir COMET verwenden, erzielen kontextbewusste Modelle die beste Leistung. Und wenn wir das Wort-f-Maß verwenden, dann haben Modelle mit und ohne Kontext vergleichbare Leistungen. Dies zeigt erneut, dass es schwierig ist, das beste Dokument-Level-Übersetzungssystem zu bestimmen, wenn wir nur Korpus-Level-Metriken verwenden. Nun verwenden wir den MuDA-Benchmark, um Modelle zu bewerten, und wir finden heraus, dass kontextbewusste Modelle signifikant genauer sind als Modelle, die keinen Kontext für bestimmte Diskurs-Phänomene wie Höflichkeit und lexikalische Kohäsion verwenden. Aber diese Modelle sind nicht viel besser als Modelle, die keinen Kontext für andere Phänomene wie Ellipse, Pronomen und Verbform verwenden. Dies legt nahe, dass wir bei der Dokument-Level-Übersetzung Fortschritte sehen müssen. Wir haben auch verschiedene kommerzielle Systeme verglichen und unser Benchmark zeigt, dass DeepL in der Regel genauer als Google Translate für die Dokument-Level-Übersetzung ist. Zusammenfassend führen wir eine datengesteuerte Analyse über 14 Sprachpaare durch, um zu identifizieren, wann Übersetzungen Kontext erfordern, und dann verwenden wir unsere Erkenntnisse, um einen Benchmark für die Dokument-Level-Maschinentranslation zu erstellen, der uns helfen kann, zu identifizieren, welche Diskurs-Phänomene Modelle gut oder nicht gut bewältigen können, und welche Übersetzungssysteme gut in der Dokument-Level-Übersetzung sind. Vielen Dank für Ihre Aufmerksamkeit. Bis in Toronto.</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">**Abstract**  
Ananya and Vignesh present *IndicMT Eval*, a dataset designed to meta-evaluate machine translation (MT) metrics for Indian languages, addressing the understudied area of reverse translation evaluation. The dataset includes 7,000 samples from five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati), generated by seven translation models/APIs. Human bilingual annotators evaluated the outputs, categorizing errors into accuracy/meaning, fluency, and special categories, and assigning severity and overall scores. The study compares MT metrics, revealing that overlap-based metrics (e.g., chrF) perform poorly, while embedding-based metrics (e.g., LabSE, BERTscore) show better correlations with human scores. COMET-metric variants exhibit the highest overall correlations. Fine-tuning COMET using the MQM dataset (IndicCOMET) improves performance, outperforming COMET baselines on three out of five languages. Zero-shot testing on unseen languages confirms IndicCOMET’s superiority. Robustness evaluation on ACES datasets further validates IndicCOMET’s effectiveness. The dataset and findings aim to provide a robust framework for evaluating MT systems in Indian languages.</sample>
    <sample id="277">Die neue Methode hat keinen spezifischen Namen, der im Inhalt erwähnt wird. Sie wird als "Multiset Tagging and Latent Permutations" bezeichnet.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der „markierten Wörter“ als eine Methode, die auf dem soziolinguistischen Konzept der „Markierung“ basiert. Dabei wird zwischen unmarkierten (dominanten) und markierten (marginalisierten) Gruppen unterschieden. Die Methode identifiziert Wörter, die markierte Gruppen von unmarkierten unterscheiden, indem sie gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe im Vergleich zu unmarkierten Gruppen zu analysieren. Dies ermöglicht die Erkennung spezifischer Stereotype und essentialisierender Narrative in den generierten Personas.</sample>
    <sample id="279">Die Autoren gehören der University of Washington an.</sample>
    <sample id="280">**Abstract:**  
The paper introduces *MultiEMO*, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations (ERC). Existing methods often fail to exploit multimodal information effectively, struggle with minority emotion classes, and struggle to distinguish semantically similar emotions. *MultiEMO* addresses these challenges by proposing: (1) *VisExtNet*, a novel visual feature extractor that focuses on facial expressions rather than redundant scene information; (2) *MultiAttn*, a multimodal fusion model using bidirectional multi-head cross-attention layers to integrate textual, audio, and visual modalities; and (3) *Sample-Weighted Focal Contrast Loss* (SWFC) to improve classification of minority and semantically similar emotions. Experiments on MELD and IEMOCAP datasets demonstrate state-of-the-art performance, particularly in handling difficult scenarios and minority classes. Limitations include challenges in speaker identification and computational requirements for SWFC. *MultiEMO* significantly advances ERC by leveraging complementary multimodal information and addressing key challenges in the field.</sample>
    <sample id="281">This work explores when translation requires context and how well models handle such cases. Using Pointwise Contextual Usage Measure (P-CXMI), we analyze TED talk transcripts translated into 14 languages to identify words and phenomena requiring context, such as dual pronouns in Arabic, verb forms, and proper nouns in Chinese. We developed the Multilingual Discourse-Aware (MuDA) tagger to automatically identify context-dependent words. Our benchmark evaluates models on document-level translation, revealing that context-aware models outperform context-agnostic ones for phenomena like formality and lexical cohesion but not for others like ellipsis or pronouns. Commercial systems like DeepL generally outperform Google Translate. This study highlights the limitations of corpus-level metrics like BLEU and underscores the need for targeted evaluation of context-dependent translations to improve document-level machine translation systems.</sample>
    <sample id="282">**Abstract:**  
At ACL 2023, we present *StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing*, addressing the challenge of story-level style transfer in non-parallel text. Unlike prior work focusing on token or sentence-level style transfer, *StoryTrans* tackles discourse-level linguistic preferences and style-specific content preservation. The primary challenges include imitating author style at the discourse level and transferring style-specific content to another topic. To address these, we propose a two-stage generation model: (1) transferring style-specific content keywords masked and (2) generating the full text by incorporating these keywords explicitly. Our model learns discourse representations and combines them with style embeddings. We use an advisory training framework with self-reconstruction, disentanglement, sentence order, and style classifier losses for the first stage, and a content-focused stage for the second. Experiments on Chinese and English datasets demonstrate *StoryTrans* outperforms baselines in style control and content preservation, with manual evaluations confirming its effectiveness. *StoryTrans* enriches storylines and maintains semantics, aligning with golden texts in the style feature space. Code and data are available in the repository.</sample>
    <sample id="283">Prag.</sample>
    <sample id="284">**Abstract:**  
We introduce **FSUIE (Fuzzy Span Mechanism for Universal Information Extraction)**, a novel approach to enhance span-based Universal Information Extraction (UIE) by addressing the ambiguity in span boundary labeling and the mismatch between transformer feature extraction and information extraction. FSUIE employs a **fuzzy span loss** to model span boundaries as continuous distributions rather than precise labels, reducing reliance on annotated span boundaries. Additionally, we propose **fuzzy span attention**, an adaptive mask function that dynamically adjusts the attention span and linearly decays its intensity near boundaries, enabling the model to focus on relevant semantic information within a limited range. FSUIE achieves state-of-the-art (SOTA) results on named entity recognition, relationship extraction, and aspect sentiment triplet extraction tasks, demonstrating improved performance and generalization capabilities. Ablation studies show that the fuzzy span loss (FSL) and fuzzy span attention (FSA) independently contribute to better convergence and information extraction, with their combined effect yielding significant enhancements. FSUIE’s unified structure enhances adaptability and efficiency, making it a robust solution for universal information extraction tasks.</sample>
    <sample id="285">In unserem Beitrag "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" untersuchen wir die Herausforderung der faktischen Fehlerkorrektur in Dialogzusammenfassungen. Wir argumentieren, dass bestehende Evaluierungsmethoden für Factual Error Correction (FEC) Modelle, wie die Verwendung von Factuality-Metriken wie FactCC und DAE, unzureichend sind, da sie keine detaillierte Fehleranalyse ermöglichen und die Grenze zwischen FEC und der Verbesserung der Summarisierungsmodelle verwischen. Stattdessen schlagen wir vor, manuell annotierte Referenzkorrekturen zu verwenden, um eine genauere und umfassendere Evaluierung von FEC-Modellen zu ermöglichen. Wir entwickeln eine neue Taxonomie von faktischen Fehlern, die in inhaltliche (content-based) und formale (form-based) Kategorien unterteilt ist. Unser Evaluierungsrahmen basiert auf ERRANT und umfasst die Schritte der Ausrichtung, Klassifizierung und Vergleich. Experimente zeigen, dass die Verwendung von Referenzzusammenfassungen aus Dialogdatensätzen die Leistung von FEC-Modellen verbessert. Wir betonen die Notwendigkeit, menschliche Korrekturen in den Trainingsdatensatz zu integrieren, und schlagen eine Kombination aus menschlichen und synthetischen Daten vor. Unsere Arbeit unterstreicht die Bedeutung einer detaillierten Evaluierung und die Notwendigkeit, FEC-Modelle speziell für Dialogzusammenfassungen zu trainieren.</sample>
    <sample id="286">James Finch und Sarah Finch</sample>
    <sample id="287">Vier Autoren sind an der Arbeit beteiligt: Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP, SyntaxGym und CrowS pairs.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind: FTw, COSINE, WSL, WSL-clean, und WSL-noisy.</sample>
    <sample id="291">The model is evaluated on 11 biomedical and clinical downstream tasks, including named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit einem 138 GB großen Datensatz trainiert.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">This research explores irony detection in natural language processing (NLP) through the EPIC corpus, a collaborative project between the University of Turin and Amazon Alexa. The corpus, comprising 300 short conversations from social media platforms like Reddit and Twitter, was annotated by 74 annotators across five English varieties. The study highlights the limitations of the ground truth assumption in NLP and investigates how annotators' perspectives influence irony detection. The results show significant inter-annotator agreement differences based on factors like gender, age, and nationality. Perspective-aware models, trained on annotator-specific data, demonstrate higher confidence in predictions compared to aggregated gold standard models. Notably, generational proximity and geographical differences among annotators contribute to varying perceptions of irony. This work underscores the importance of considering annotator perspectives in NLP tasks and offers insights into improving model accuracy and confidence in detecting subtle linguistic phenomena like irony.</sample>
    <sample id="297">This project, "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models," develops a typology and glossary of over 340 racist, transphobic, and anti-Semitic dogwhistles, characterized by register (formal/informal), type (additional implicature or covert persona signaling), and persona (e.g., anti-Semitic, transphobic). A case study of historical U.S. political speeches reveals increased use of racial dogwhistles post-Civil Rights era, aligning with the Republican Southern Strategy, and a growing association with conservatism. Experiments with GPT-3 demonstrate its ability to surface dogwhistles, particularly in formal contexts, but with significant variability, especially for informal and transphobic terms. Prompting strategies, such as including definitions or secret cues, improve performance. A toxicity detection case study using Prospective API shows that dogwhistles evade content moderation, as hateful sentences rated less toxic when slurs are replaced with dogwhistles. This research highlights the challenges of detecting dogwhistles in NLP and their role in political influence and online toxicity.</sample>
    <sample id="298">Die Ergebnisse, die zu der Schlussfolgerung führten, dass die zeitliche Verzögerung (temporal drift) die Hauptursache für den Leistungsverlust war, waren:

* **Experimente zur erneuten oder fortlaufenden Pretraining von Modellen mit neueren Daten:** Die Leistung degradierte mit größerem zeitlichen Abstand zwischen Trainings- und Testdaten.
* **Fehlen von adaptivem Overfitting:** Die Leistungssteigerung auf CoNLL-2003 übersetzte sich in eine noch größere Leistungssteigerung auf CoNLL++, was auf keine abnehmende Rendite hinweist.</sample>
    <sample id="299">**Abstract:**  
We address the robustness of Natural Language Inference (NLI) models by proposing a minimax training method to reduce reliance on shortcuts—spurious correlations in training data that degrade out-of-distribution generalization. Prior shortcut mitigation methods often require domain-specific auxiliary models, which may provide inaccurate uncertainty estimates or diverge from the learner’s behavior. Our approach avoids these limitations by training a learner and an auxiliary model in an alternating manner. The learner minimizes NLI task loss, while the auxiliary maximizes the learner’s loss by generating example weights that emphasize under-represented hard instances. This incentivizes the learner to focus on challenging patterns that counteract shortcuts in dominant easy examples. Our method is dataset-agnostic, uses a feed-forward auxiliary, and does not require pre-trained models. Evaluations on MNLI, FEVER, QQP, and adversarial test sets (HANS Symmetric, PAWS) show consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy. We also explore the impact of pre-training the learner, the auxiliary’s size, and qualitatively analyze the learned example weight distribution. Our work enhances NLI model robustness without relying on auxiliary-specific assumptions.</sample>
    <sample id="300">**Abstract:**  
Interactive Dictation is a novel task introduced to enable users to dictate and edit documents naturally using voice commands. Unlike traditional speech-to-text systems, it allows for flexible interleaving of dictation and editing without requiring predefined trigger words or commands. The system transcribes speech, segments it into dictation and command utterances, normalizes commands, and executes them to update the document state in real time. To formalize the task, a four-step procedure was developed: ASR recognition, segmentation, command normalization, and execution. A dataset was collected using a custom annotation interface, demonstrating the seamless interaction between dictation and editing. A baseline system was built, employing T5 and GPT-3 models for command interpretation, with experiments showing that GPT-3 achieves higher accuracy but at the cost of runtime efficiency. The work highlights the potential for more intuitive and natural voice-based editing interfaces and provides a foundation for future research and improvements. Code and details are available in the accompanying paper.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, weil die Multisets, die in der ersten Stufe erstellt werden, die richtigen Token enthalten, aber nicht in der richtigen Reihenfolge. Die Permutation ermöglicht es, die Token in die korrekte Reihenfolge zu bringen, um die gewünschte Ausgabesequenz zu erzeugen.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, weil:

1. **Positive Stereotype und Essentialisierende Narrative**: Es unklar ist, ob positive Stereotype und essentialisierende Narrative durch eine übermäßige Wertanpassung oder andere Anti-Stereotypie-Methoden entstehen, die wiederum schädliche Muster erzeugen.
2. **Fehlende Transparenz**: Ohne Transparenz können Forscher*innen keine fundierten Annahmen treffen oder diese Muster weiter untersuchen.
3. **Intersektionale Betrachtung**: Es wichtig ist, Vorurteile und Schäden durch eine intersektionale Linse zu betrachten, um übersehene Aspekte zu vermeiden.</sample>
    <sample id="304">Unaceptable minimal pair inputs are sentences that are grammatically incorrect or violate linguistic rules, such as Adjunct Island cases in the BLiMP dataset, which are used to test a language model's ability to distinguish between acceptable and unacceptable sentences.</sample>
    <sample id="305">In our recent work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning," we critically examine the necessity and effectiveness of clean validation data in WSL. Contrary to common claims, we found that recent WSL methods require clean validation samples to achieve high performance on clean test sets. Without them, models fail to generalize beyond the weak labels, rendering the training ineffective. We also discovered that increasing the number of clean validation samples improves performance, with as few as 20 samples per class often being sufficient. Furthermore, direct fine-tuning on clean samples outperforms WSL approaches, especially when only 10 samples per class are available. This suggests that the performance gains attributed to WSL methods can be achieved more efficiently through simpler fine-tuning techniques. Our findings highlight the often-overlooked cost of obtaining clean validation data and question the practicality and necessity of WSL methods. We recommend future work to report model selection criteria, compare WSL with few-shot learning baselines, and consider continuous fine-tuning as a strong baseline. Our code is open-sourced and available via QR code.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim investigate the entity tracking capabilities of large language models (LLMs) in discourse understanding. They design a task involving boxes and objects to evaluate whether LLMs can track entity states across multiple state-changing operations without relying on heuristics or pre-trained knowledge. The task requires models to predict box contents based on initial descriptions and operations, ensuring that entity states are not directly predictable from the input. Experiments with Flan-T5, GPT-3, and GPT-3.5 models reveal that only GPT-3.5, pre-trained on substantial code, exhibits non-trivial entity tracking. Smaller models like T5-base can learn the task with fine-tuning, but randomly initialized models cannot, highlighting the importance of pre-training. The study suggests that pre-training on code enhances LLMs' ability to track entity states, though generalization beyond this specific setup remains unclear. The findings underscore the potential of LLMs in discourse understanding but also identify limitations and areas for further research.</sample>
    <sample id="307">Die Autoren verwendeten verschiedene Bewertungsmetriken, darunter **genitive Entity Recognition (NER)**, **Classification**, **Part-of-Speech (POS) Tagging** und **Question Answering**, um die Leistung der Modelle auf öffentlichen und privaten Downstream-Tasks zu bewerten.</sample>
    <sample id="308">This research explores the concept of "positionality" in NLP, examining whether datasets and models exhibit biases reflecting the perspectives of their creators or the populations they serve. Positionality, rooted in critical studies, refers to the influence of demographics, identity, and life experiences on decision-making. The study, conducted in collaboration with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI, investigates how NLP tools like Prospective API, Dynahate, and GPT-4 perform across diverse populations. Using the NLPositionality framework, the team re-annotated datasets with over 1,000 annotators from 87 countries, comparing annotations to model predictions. Results reveal that datasets and models are often aligned with English-speaking, highly educated populations, while underrepresented groups like non-binary individuals are less accurately captured. The study highlights the need for transparency in design choices, a perspectivist approach to NLP research, and the creation of specialized datasets and models for underrepresented communities. This work underscores the importance of addressing positionality to ensure NLP technologies are inclusive and equitable.</sample>
    <sample id="309">Die Inter-Annotator-Vereinbarung (IAA) wurde verwendet, um die Übereinstimmung zwischen den Kommentatoren zu messen.</sample>
    <sample id="310">Wikipedia wurde gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">Die Autoren gehören der Universität zu, die nicht explizit im Text genannt wird.</sample>
    <sample id="312">MultiInstruct unterscheidet sich von anderen Benchmarks dadurch, dass es der erste groß angelegte Multi-Modal-Instruction-Tuning-Datensatz ist, der 62 diverse Multi-Modal-Aufgaben in 10 Kategorien umfasst. Im Gegensatz zu bestehenden Datensätzen, die hauptsächlich auf Sprache oder Computer Vision fokussiert sind, bietet MultiInstruct eine umfassende Sammlung von Aufgaben, die sowohl Sprache als auch Bilder berücksichtigen. Zudem wurde der Datensatz entwickelt, um die Effektivität von Instruction Tuning bei Multi-Modal-Modellen zu untersuchen, was bisher vernachlässigt wurde.</sample>
    <sample id="313">Zwei Autoren: James Finch und Sarah Finch.</sample>
    <sample id="314">Die binäre Koordination bezieht sich auf die Verbindung von zwei Elementen (Conjuncts) durch eine koordinierende Konnektion (z.B. "und", "oder").</sample>
    <sample id="315">Die in dieser Studie verwendeten Prompts waren im Durchschnitt 10 Wörter lang.</sample>
    <sample id="316">Die Ergebnisse zeigen, dass das kleinere T5-Modell, wenn es auf dem CoScript-Datensatz trainiert wird, Scripts von höherer Qualität generieren kann als die meisten großen Sprachmodelle. Dies deutet darauf hin, dass kleinere Modelle, wenn sie auf geeigneten Datensätzen trainiert werden, die Leistung größerer Modelle übertreffen können.</sample>
    <sample id="317">**Abstract:**  
We present **CodeIE**, a novel approach to information extraction (IE) that leverages large code generation models like Codex to transform the text-to-structured IE task into a structure-to-structure code generation task. Traditional models, such as T5 and GPT-3, face challenges due to mismatched outputs between pre-training (text-to-text) and inference (structured output) stages. CodeIE addresses this by using code-style prompts and code pre-trained models, ensuring aligned structured outputs. For named entity recognition (NER) and relation extraction (RE), we design prompts that define functions with structured outputs, enabling few-shot learning. Evaluations on NER and RE datasets show that CodeIE, using Codex and code-style prompts, significantly outperforms traditional models like UIE and GPT-3. Key findings include lower perplexity on code format inputs, fewer structural errors, and better recall. Codex consistently outperforms GPT-3, even with text-style prompts. This work highlights the effectiveness of code-style prompts and code pre-trained models in aligning with the IE task, offering a promising direction for future research.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und werde Ihnen unsere Arbeiten zu „DrBERT: Ein robuster vortrainierter Modell für französische biomedizinische und klinische Domänen“ vorstellen. In dieser Präsentation sprechen wir zunächst über Sprachmodellierung im Gesundheitswesen. Anschließend stellen wir den Hauptbeitrag unseres Artikels vor. Wir führen das erste biomedizinische Modell in französischer Sprache namens DrBERT ein, das auf RoBERTa basiert und mit NACHOS trainiert wurde, einem Datensatz von medizinisch durchsuchten Daten aus dem Web. Wir haben auch einen Vergleich von Modellen mit mehreren Vorentrainings-Einstellungen und Datenquellen durchgeführt. Anschließend stellen wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in französischer Sprache vor. Abschließend ziehen wir eine Bilanz der Experimente und geben Ihnen weitere Details darüber, wie Sie auf diese Modelle zugreifen können. Seit seiner Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Lösung von Aufgaben der natürlichen Sprachverarbeitung geworden und bietet im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText oder mehr erhebliche Leistungsgewinne. Seitdem wurde dieses Modell auf viele andere Sprachen angepasst, wie auf Französisch mit CamemBERT, und auch auf Domänen wie biomedizinisch mit PubMedBERT und BioBERT und klinisch mit ClinicalBERT, jedoch hauptsächlich auf Englisch. Spezialisierte Modelle für andere Sprachen sind selten und basieren oft auf kontinuierlichem Vorentraining aufgrund des Mangels an domänenspezifischen Daten. Französisch hatte jedoch bis jetzt kein Open-Source-Modell für biomedizinische Zwecke. Wir stellten uns also die Frage, welche Datenquellen für eine breite Nutzung am geeignetsten sind und ob diese durchsuchten Daten eine gute Alternative für klinische Daten darstellen. Um diese Frage zu beantworten, vergleichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten basiert, die aus dem Datenlager des Universitätskrankenhauses Nantes stammen. Anschließend stellten wir uns die Frage, wie viel Daten benötigen wir, um ein spezialisiertes Modell auf französischen Daten zu trainieren? Sind es 4 Gigabyte, 8 Gigabyte oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir zunächst vier von-Grund-auf-trainierte Modelle: eine erste Version von DrBERT mit 7 GB NACHOS; eine zweite Version von DrBERT mit 4 GB NACHOS; eine erste Version von ChuBERT, einem klinischen Modell mit 4 GB Sätzen aus klinischen Notizen; und eine endgültige Version von ChuBERT mit einer Mischung aus 4 GB NACHOS und 4 GB klinischen Notizen. Zusätzlich zu diesem Vergleich führten wir drei Modelle ein, die auf kontinuierlichem Vorentraining basieren, um die Auswirkungen der Vorentrainingsstrategie zu analysieren. Eines basiert auf dem Gewicht von CamemBERT und wurde auf einem 4 GB großen Satz von NACHOS trainiert. Ein weiteres basiert ebenfalls auf CamemBERT, wurde jedoch dieses Mal auf den 4 GB klinischen Notizen trainiert, und schließlich eines, das auf dem englischen biomedizinischen Modell PubMedBERT basiert und auf einem 4 GB großen Satz von NACHOS trainiert wurde. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu bewerten, sammelten wir Daten für öffentliche und private Downstream-Aufgaben wie Named Entity Recognition, Klassifizierung, Teil-des-Sprech-Kennzeichnung und Fragebeantwortung. Diese Modelle wurden mit sechs Basismodellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Bewertung zeigt, dass die Modelle am besten bei der Aufgabe mit Daten der gleichen Natur abschneiden, auf denen das Modell trainiert wurde. Wir können jedoch beobachten, dass Daten aus heterogenen Quellen vielseitiger zu sein scheinen. Wir beobachten auch, dass mehr Daten zu besseren Ergebnissen führen. Insgesamt scheinen von-Grund-auf-trainierte Modelle auf den meisten Aufgaben die besten Ergebnisse zu erzielen. Unser Experiment zum kontrollierten Vorentraining mit dem Gewicht und der Tokenisierung von CamemBERT, das auf dem 4 GB großen Teil von NACHOS trainiert wurde, zeigte jedoch vergleichbare Ergebnisse wie DrBERT 4 GB von-Grund-auf. Dies ist nicht der Fall für das Modell, das auf CamemBERT-Gewichten und -Tokenisierung basiert, das unter Stabilitätsproblemen leidet. Abschließend lässt sich sagen, dass unser System bei neun der 11 Downstream-Aufgaben bessere Ergebnisse erzielt hat und die Ergebnisse der generischen Modelle, hier CamemBERT, insgesamt übertrifft. Wir beobachten auch, dass spezialisierte Daten besser sind, sich jedoch nicht gut skalieren. Alle aus NACHOS gewonnenen vortrainierten Modelle sind kostenlos auf Hugging Face verfügbar und unter der MIT-Lizenz, und alle Trainingsskripte befinden sich in unserem GitHub-Repository. Vielen Dank für diese Präsentation, und wir freuen uns auf den Austausch in der Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht drei Lernstrategien:  
1. **From-scratch Pre-training**: Modelle werden von Grund auf mit spezifischen Daten trainiert (z. B. DrBERT, ChuBERT).  
2. **Continual Pre-training**: Modelle werden zunächst auf allgemeinen Daten trainiert und dann mit spezifischen Daten weiter trainiert (z. B. Modelle basierend auf CamemBERT und PubMedBERT).  
3. **Transfer Learning**: Modelle werden mit allgemeinen Daten trainiert und dann auf spezifische Aufgaben angepasst (z. B. CamemBERT OSCAR).</sample>
    <sample id="320">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, ist **nicht beobachtet** worden. Dies zeigt sich darin, dass jede Verbesserung auf CoNLL-2003 zu mehr als einer Verbesserung auf CoNLL++ führt, was auf keine abnehmenden Renditen hinweist.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde durch Analyse der Art der Vereinfachung (lexikalische, strukturelle, Gesamtniveau) und der Vielfalt der Vereinfachungstransformationen (z.B. Umdrehen, Hinzufügen von Wörtern) in den Korpora beurteilt.</sample>
    <sample id="322">**Abstract:**  
This paper explores what text classifiers learn about morality by analyzing how language models interpret moral judgments in text. Unlike traditional approaches that treat morality as a binary scale, we examine the nuanced, subjective nature of morality using the Moral Foundations Theory, which identifies five distinct moral dimensions (e.g., fairness, authority). We investigate how morality is expressed differently across domains using the Moral Foundation Twitter Corpus, comprising 35,000 tweets from diverse hashtags like #AllLivesMatter and #BlackLivesMatter. Our experiments reveal that language models can distinguish subtle moral differences, such as the nuanced use of "subversion" in these domains. While #AllLivesMatter associates subversion with negative connotations like "overthrow," #BlackLivesMatter frames it positively as rebellion against authority. This highlights the dangers of using a single model across domains, as it risks misinterpreting moral nuances. Our findings emphasize the need for domain-specific models to accurately capture the complexity of moral expression in text, underscoring the importance of explainable AI in understanding human morality.</sample>
    <sample id="323">**Abstract:**  
The paper addresses the challenge of Commonsense QA, which requires machines to answer questions using common knowledge. Existing methods combine language models (LMs) and heterogeneous knowledge graphs (HKGs) by retrieving subgraphs from knowledge bases, encoding them separately, and inferring answers. However, these methods introduce noisy entities, encode subgraphs and text in isolation, and ignore semantic relationships. To address these issues, we propose **Dynamic Heterogeneous-Graph Reasoning (DHLK)**. DHLK builds an optimized HKG using a two-stage pruning strategy and Knowledge Representation Learning (KRL). It then leverages a language model (RoBERTa) to encode and fuse QA contexts and entities, dynamically removing irrelevant entities based on attention weights. We introduce **Relation Mask Self-Attention (RMSA)** to model subgraphs, incorporating relationships into the attention mechanism. Entity and relation embeddings are optimized using TransE, and the final graph embedding is obtained via max-pooling. Path information from the HKG is integrated into the QA context, and the combined representations are fed into a Multi-Layer Perceptron (MLP) for answer prediction. Experiments on CommonsenseQA and OpenBookQA demonstrate that DHLK outperforms other LM and HKG methods, achieving competitive results.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile. Sie können in allen vier Quadranten des politischen Spektrums positioniert werden, wobei GPT-4 als das liberalste Modell und die GPT-Serie im Allgemeinen sozialliberaler sind als BART und seine Varianten.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen eine kurze Einführung in unseren Artikel über „Kompositionale Generalisierung ohne Bäume mittels Multiset-Tagging und latenten Permutationen“ geben. Dies ist eine gemeinsame Arbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionale Generalisierung kann als die Fähigkeit eines Lernenden verstanden werden, mit tieferer Rekursivität und unbekannten Kompositionen von Phrasen umzugehen, die während des Trainings einzeln gesehen wurden. Im Kontext der semantischen Parsung könnte die Prüfung auf kompositionale Generalisierung so aussehen. Wie üblich haben wir einen Trainingsdatensatz von Äußerungen. In diesem Fall „Das Mädchen schlief.“ und „Maria wusste, dass das Mädchen schlief.“ Diese Äußerungen werden mit logischen Formen gepaart, die wesentliche Aspekte ihrer Bedeutung repräsentieren. Im Gegensatz zu standardmäßigen maschinellen Lernbewertungen stammt der Testdatensatz nicht aus der gleichen Verteilung, sondern enthält strukturell nicht gesehene logische Formen. In diesem Beispiel hat das Modell während des Trainings flache Rekursivität gesehen und wird auf ein Beispiel mit tieferer Rekursivität getestet. Naive seq2seq-Modelle kämpfen mit dieser Art von Generalisierung außerhalb des Distributionsbereichs und produzieren oft Ausgaben, die vom Input abweichen. Insbesondere versagen sie oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in dem Beispiel farblich hervorgehoben sind. Eine beliebte Methode, um dies anzugehen, besteht darin, Bäume in die Modelle zu integrieren. Die Bäume sollen den kompositorischen Prozess erfassen, der Äußerungen mit den logischen Formen in Beziehung setzt. Das funktioniert gut, aber Bäume werden normalerweise nicht gegeben und müssen irgendwie erhalten werden. Dies kann ein komplizierter und manchmal rechenintensiver Prozess sein. Typischerweise beinhaltet dies erhebliche formalspezifische Vorverarbeitung der logischen Formen, zum Beispiel, um Variablen-Symbole zu behandeln. Das Erhalten von Bäumen kann auch spezialisierte Grammatikinduktionsverfahren beinhalten. In diesem Artikel verwenden wir keine Bäume und führen ein neuronales seq2seq-Modell ein, das die Korrespondenzen zwischen Fragmenten des Inputs und Fragmenten des Outputs direkt modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferer Rekursivität, ohne auf Bäume zu vertrauen. Unser Ansatz prognostiziert die Ausgabe vom Input in zwei Schritten. Zuerst taggen wir jedes Input-Token mit einem ungeordneten Multiset von Token, die im Output erscheinen werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht geordnet. Deshalb verwenden wir in einem zweiten Schritt ein anderes Modell, um eine Permutation zu prognostizieren, um sie in die richtige Reihenfolge zu bringen. Wir führen eine neue Methode ein, um die Permutation vorherzusagen, die keine harten Einschränkungen auf die möglichen Permutationen legt. Das macht unseren Ansatz ziemlich flexibel und ausdrucksstark. Konzeptionell funktioniert unser Permutationsmodell ungefähr so. Wir gehen von links nach rechts über den Output und bestimmen, welches Multiset-Token in jeder Position platziert werden soll. Für die erste Output-Position wählen wir einfach eines aus, wie in rot hervorgehoben. Dann springen wir zum nächsten Multiset-Token, um das zweite Token im Output zu bestimmen. Wir bestimmen das dritte Token im Output auf ähnliche Weise, indem wir zu einem anderen Multiset-Token springen. Wir setzen diesen Prozess fort, bis jedes Token aus der ersten Stufe genau einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unsere Methode mit anderen treeless Modellen auf dem COGS-Benchmark. Unser Modell übertrifft die anderen mit großem Abstand bei der Generalisierung zu tieferer Rekursivität. Einige andere Arten der strukturellen Generalisierung bleiben jedoch sehr herausfordernd. In unserem Artikel lösen wir ein paar interessante technische Herausforderungen. Zunächst ist die Ausrichtung zwischen Input und Output nicht in den Trainingsdaten gegeben. Als Konsequenz wissen wir für ein gegebenes Token nicht, aus welchem Multiset es stammt, was eine Herausforderung für das Training darstellt. Zusätzlich gibt es manchmal mehrere Permutationen, die mit den Daten konsistent sind, aber die sprachlich korrekte ist latent. Wir lösen dies, indem wir die Ausrichtung als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, bringt aber die Herausforderung mit sich, dass das Finden der höchstbewerteten Permutation NP-schwer ist. Das liegt daran, dass dies mit dem „Traveling Salesman“-Problem verwandt ist. Wir approximieren dies mit einer GPU-freundlichen kontinuierlichen Relaxation, die es uns auch ermöglicht, durch die Lösung zurückzuprojizieren und die sprachlich plausibleren Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen angehen möchten, werfen Sie bitte einen Blick auf unseren Artikel oder kommen Sie zu unserem Poster.</sample>
    <sample id="326">Kognitive Dissonanz bezeichnet die innere Spannung oder Unzufriedenheit, die entsteht, wenn zwei widersprüchliche Überzeugungen, Einstellungen oder Handlungen gleichzeitig bestehen. Ein Beispiel ist die Situation, in der jemand weiß, dass Rauchen gesundheitsschädlich ist, aber dennoch raucht, weil er glaubt, ohne Rauchen seinen Job nicht behalten zu können. Diese Inkonsistenz zwischen Wissen und Verhalten verursacht Dissonanz.</sample>
    <sample id="327">**Abstract:**  
We present **ManagerTower**, a novel vision-language (VL) model that enhances cross-modal representation learning by adaptively aggregating insights from multiple unimodal expert layers. Unlike previous approaches like BridgeTower, which layer-by-layer connect unimodal and cross-modal layers, ManagerTower introduces **managers** in each cross-modal layer to dynamically combine unimodal representations from different levels. This allows for more effective exploitation of diverse semantic knowledge across modalities. Using RoBERTa and CLIP-ViT as unimodal encoders, ManagerTower achieves superior performance on downstream VL tasks, including Visual Question Answering (VQA), with only four million pre-training data. Notably, it outperforms larger and data-rich baselines. Visualization of aggregation weights reveals that adaptive managers dynamically adjust to the needs of each cross-modal layer, unlike static managers. Our work demonstrates the importance of adaptive aggregation for comprehensive VL representation learning. Code, models, and paper are available for further exploration.</sample>
    <sample id="328">GPT-4 ist das Sprachmodell, das am meisten links steht.</sample>
    <sample id="329">**Abstract:**  
This work addresses zero-shot video sentence localization, which identifies relevant video segments based on natural language queries for long videos. Traditional methods rely on manual annotations, which are costly and inefficient. We propose a noise-resistant structured pseudo-label generation method to train video sentence localization models without manual annotations. First, we use a pre-trained image caption model to generate complex pseudo-queries. Then, we measure relevance between frames and queries, generating pseudo-events that ensure high relevance within events and low relevance outside. To mitigate label noise, we re-weight noisy samples and refine pseudo-labels using model confidence and IoU. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate superior performance compared to existing zero-shot methods, achieving the best results in R@M and mIoU metrics. Our approach robustly handles label noise and eliminates overlaps, making it efficient and effective for zero-shot video sentence localization.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen in diesem Kontext.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transkripten von TED-Talks, die von Englisch in 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="333">In this paper, we introduce INK (Injecting kNN Knowledge), a novel framework to enhance the generalization and performance of Neural Machine Translation (NMT) models. Our work addresses the issue of sparse representation in NMT models, where low-frequency tokens disperse sparsely, leading to poorly defined semantic meanings and poor performance. We propose to smooth the representation space by injecting kNN knowledge into the NMT model during training. Our INK training loop consists of two steps: (1) extracting kNN knowledge from a datastore to guide an adapter in adjusting representations, and (2) updating the datastore with the refined representations asynchronously. This iterative process continues until convergence. Our experiments show that the INK system outperforms state-of-the-art kNN-MT systems, achieving an average gain of 1.99 COMET score and 1.0 BLEU score. The INK system also demonstrates better translation performance with less memory space and faster inference speed. Our results suggest that the representation space of the NMT model can be further refined, indicating potential for future improvements in NMT performance.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit eines Modells, Kenntnisse und Fähigkeiten, die in einer Sprache erworben wurden, auf eine andere Sprache zu übertragen. In dem Kontext von XSemPLR bedeutet dies, dass ein Modell, das in einer Quellsprache (z.B. Englisch) trainiert wurde, in der Lage sein soll, die gelernten Kenntnisse auf eine Zielsprache (z.B. Deutsch oder Chinesisch) zu übertragen, um semantische Parsing-Aufgaben zu lösen.</sample>
    <sample id="337">**Abstract:**  
This work addresses the challenge of representing out-of-vocabulary (OOV) words in embedding-based models by leveraging word formation and association. We introduce a **Word Relationship Graph** that mimics lexical rules, enabling the inference of OOV word meanings through their wordpieces and associated relevant words. The graph is structured in two layers: the first layer preserves all wordpieces for complete information, while the second layer samples nodes to reduce noise. A **self-attention network** assigns node attributes to OOV words based on their characters, and a **Graph Attention Network (GAT)** with two levels extracts meaningful representations. A **readout block** captures graph-level information, summarizing word formation. To align with background embedding models, we incorporate contrastive learning using NT-XENT loss, leveraging two-hop neighbors, synonyms, and the OOV word itself. Experiments demonstrate superior performance on both intrinsic and extrinsic tasks, highlighting the effectiveness of our approach. Our model is particularly suited for agglutinative languages but can be adapted to others with reasonable word segmentation. This work provides a robust framework for handling OOV words in embedding-based models.</sample>
    <sample id="338">**Abstract:**  
This research explores the quality of human-annotated natural language explanations in improving model performance, particularly in tasks like commonsense QA, natural language inference, and commonsense validation. Unlike traditional metrics like BLEU and ROUGE, which focus on word similarity, the proposed **TREU (Task-Relevant Explanation Utility)** metric evaluates the helpfulness of explanations during fine-tuning and inference stages. A unified data structure is introduced to standardize task-specific explanations into a multiple-choice format, enabling seamless application across datasets. Experiments on five datasets (CoS-E, ECQA, e-SNLI, ComVE) reveal that explanations enhance model performance, even if perceived as low quality by humans. The TREU metric outperforms the simulatability score by accounting for task-dependent explanation utility and formats, such as negation or counterfactual styles. Findings highlight that explanations are more helpful in entailment tasks than in neutral or contradiction tasks. This work emphasizes the importance of evaluating explanation quality objectively and lays the groundwork for high-quality human-model collaboration in annotation tasks.</sample>
    <sample id="339">Die Autoren gehören der Universität des Saarlandes an.</sample>
    <sample id="340">**Abstract:**  
We introduce ParaAMR, a large-scale, syntactically diverse paraphrase dataset generated using AMR (Abstract Meaning Representation) back-translation. Existing datasets, such as MRPC and Quora, are high-quality but limited in scale, while automatically generated datasets like back-translation lack syntactic diversity. ParaAMR addresses this gap by leveraging AMR graphs, which capture the abstract meaning of sentences, to create diverse paraphrases. The method involves modifying the focus node in an AMR graph, altering edges and labels, and generating text from the modified graph. This approach ensures semantic similarity while introducing syntactic variation. ParaAMR contains 15 million source sentences with 6.9 paraphrases per sentence, outperforming existing datasets in syntactic diversity while maintaining semantic similarity. We demonstrate ParaAMR’s effectiveness in improving sentence embeddings, syntactic control in paraphrase generation, and few-shot learning. Our dataset is publicly available and showcases its utility across NLP applications.</sample>
    <sample id="341">Die Autoren verwenden **durchschnittliche Latenz** (average lagging) und **rechenbewusste Latenz** (computational-aware average lagging) als Latenzmessungen.</sample>
    <sample id="342">**Abstract:**  
This paper introduces *LiveChat*, a large-scale personalized dialogue dataset constructed from Chinese live streaming videos. Open-domain dialogue datasets, often text-sourced, lack real spoken conversation data, while existing video-sourced datasets are limited in scale due to manual annotations. *LiveChat* addresses this gap by automatically extracting audio, transcribing utterances, and constructing dialogues using a reply-to-whom matching method from audience comments. It also collects persona information for personalized dialogue generation, combining manual and rule-based extraction methods. Compared to existing datasets, *LiveChat* is video-sourced, larger in scale, and features longer average sessions per persona. Experiments on response modeling and addressee recognition demonstrate that persona profiles and longer sessions improve performance. Transfer learning experiments show that BART outperforms other LLMs, highlighting *LiveChat*'s unique domain characteristics. Future work will focus on efficient transfer learning for *LiveChat*. *LiveChat* bridges the gap in large-scale, personalized, and multi-party dialogue datasets, particularly in Chinese.</sample>
    <sample id="343">Hallo alle zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit „Der KITMUS-Test: Bewertung der Wissensintegration aus mehreren Quellen.“ Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Modelle zur natürlichen Sprachverarbeitung stützen sich auf eine Vielzahl von Wissensquellen, wie das in ihren Parametern enthaltene Wissen, das normalerweise durch ein Pretraining erworben wird, und das in den Eingaben zur Inferenzzeit bereitgestellte Wissen. Jüngere Arbeiten in Aufgaben wie dem Beantworten von Fragen zeigen, dass Modelle das Wissen aus der Pretraining-Zeit nutzen können, um die Aufgabe zu lösen. Die natürliche Sprachverarbeitung erfordert jedoch oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel in dem Satz: „John sah den neu gewählten Präsidenten im Fernsehen.“ Die vorgetrainierten Parameter können Informationen über das Verhalten von Präsidenten und was ein Fernseher ist enthalten, aber sie können nicht zuverlässig wissen, wer diese instanzzentrifische Entität „John“ ist oder wer der neue Präsident ist, weil sich der Präsident seit dem Pretraining möglicherweise geändert hat. Erfolgreiche Modelle für wissensintensive NLU-Aufgaben erfordern daher die Fähigkeit, sowohl das Wissen aus der Pretraining-Zeit als auch das Wissen zur Inferenzzeit zu integrieren und zu nutzen. In dieser Arbeit schlagen wir eine diagnostische Testsuite für die Wissensintegration vor. Wir führen eine Coreferenzauflösungaufgabe ein, die darauf ausgelegt ist, die Fähigkeit zu prüfen, auf Wissen zuzugreifen, das in verschiedenen Quellen verfügbar ist. Wir bewerten den Datensatz mit menschlichen Studienteilnehmern und etablierten Coreferenzauflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag bei der Arbeit, an dem er Fälle in einem Gericht entschieden hat, war er froh, sich zu entspannen. Die Aufgabe besteht hier darin, die korrekte Entität zu identifizieren, auf die das Pronomen „er“ sich bezieht, was in diesem Fall Servin ist. Die Auflösung eines gegebenen Pronomens erfordert zwei Arten von Informationen. Erstens, entitätspezifisches Wissen wie „Servin ist ein Richter.“ Und zweitens, Hintergrundwissen wie „Richter entscheiden über Fälle in Gerichten.“ Im Allgemeinen wird Hintergrundwissen während des Pretrainings großer Sprachmodelle erlernt, während entitätspezifisches Wissen typischerweise zur Inferenzzeit beobachtet wird. Wir variieren die Verfügbarkeit dieser beiden Arten von Informationen so, dass sie entweder in einer einzigen Quelle oder in mehreren Quellen gefunden werden können. Wir haben drei Einstellungen von KITMUS definiert. Erstens haben wir die typische Einstellung: „Hintergrund-Pretrain“, bei der das Hintergrundwissen als verfügbar bei der Pretraining-Zeit angenommen wird. Zweitens gibt es die Einstellung „Hintergrund-Beide“, bei der das Hintergrundwissen sowohl bei der Pretraining-Zeit als auch zur Inferenzzeit verfügbar ist. Zuletzt die Einstellung „Hintergrund-Inferenz“, bei der beide Arten von Wissen nur zur Inferenzzeit verfügbar sind. Diese letzte Einstellung ist besonders interessant, da sie den Fall simuliert, in dem das Hintergrundwissen, das zur Lösung einer Aufgabe erforderlich ist, nicht Teil der Pretrain-Daten der Modelle ist. Zum Beispiel, weil sich seit der Zeit des Pretrainings neue Berufe entwickelt haben. Hier ist ein Beispiel dafür, wie wir die Verfügbarkeit von Fakten in den wahren Quellen steuern. In der Einstellung „Hintergrund-Pretrain“ nehmen wir an, dass das Hintergrundwissen „Politiker suchen gewählte Sitze in der Regierung“ in den vorgetrainierten Parametern enthalten ist und in der Inferenzzeit den entitätspezifischen Wissensbereich „Chichester ist ein Politiker“ bereitstellen. In der Einstellung „Hintergrund-Beide“ stellen wir zusätzlich zum entitätspezifischen Wissen auch Hintergrundwissen über Politiker in ihrem Inferenzzeit-Kontext zur Verfügung. In der Einstellung „Hintergrund-Inferenz“ stellen wir den fiktiven Beruf „mirituer“ anstelle von Politiker zur Verfügung, weil „mirituer“ unwahrscheinlich in den vorgetrainierten Parametern enthalten ist. Wir bewerten den Datensatz sowohl mit menschlichen Studienteilnehmern als auch mit etablierten Coreferenzauflösungsmodellen. In dieser Abbildung zeigen wir die Ergebnisse der best-performing Modelle auf der schwierigsten Variante der Einstellung „Hintergrund-Pretrain“. Ohne spezifisches Training auf KITMUS erzielen beide Modelle keine guten Ergebnisse. Wenn sie jedoch auf KITMUS trainiert werden, erzielen sowohl C2F als auch BERT4Coref signifikant bessere Ergebnisse als die zufällige Auswahl. Dies deutet darauf hin, dass die meisten Modelle, wenn sie auf generischen Referenzauflösungsdatensätzen trainiert werden, lernen, oberflächliche Hinweise zu nutzen, die bei Tests auf KITMUS, bei denen solche Hinweise entfernt wurden, nicht nützlich sind. Zusätzliche Experimente mit fiktivem Wissen zeigten, dass selbst die best-performing Modelle das Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, nicht zuverlässig integrieren können. Zusammenfassend die wichtigsten Erkenntnisse unserer Arbeit: Viele Coreferenzauflösungsmodelle scheinen nicht in der Lage zu sein, über Wissen aus verschiedenen Quellen zu argumentieren, ohne spezifisches Training. Mit spezifischem Training können jedoch einige Modelle erfolgreich Wissen aus mehreren Quellen integrieren. Selbst die best-performing Modelle scheinen jedoch Schwierigkeiten zu haben, Hintergrundwissen, das nur zur Inferenzzeit bereitgestellt wird, zuverlässig zu integrieren. Wenn Sie mehr Details interessiert sind, lesen Sie bitte unsere Arbeit und sehen Sie sich den Datensatz und den Code auf GitHub an. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind:

1. **Komplexität und Kosten**: Die Erzeugung von Bäumen ist oft kompliziert und rechenintensiv, insbesondere bei formalspezifischen Vorverarbeitungen der logischen Formen.
2. **Abhängigkeit von gegebenem Baum**: Die Methode erfordert die Verfügbarkeit von Bäumen, die nicht immer gegeben sind und extra induziert werden müssen.
3. **Spezialisierte Grammatikinduktion**: Die Induktion von Grammatiken kann spezialisierte Verfahren erfordern, die nicht immer einfach umzusetzen sind.</sample>
    <sample id="345">This paper introduces a neural sequence-to-sequence (seq2seq) model for compositional generalization in semantic parsing without relying on trees. The model addresses the challenge of handling deeper recursion and unseen compositions by tagging input tokens with unordered multisets of output tokens and predicting a permutation to order them. This approach avoids the need for computationally expensive tree generation and formalism-specific pre-processing. The model consists of two steps: first, input tokens are tagged with multisets of output tokens, and second, a permutation model orders these tokens. The permutation model is trained to predict linguistically plausible orders, despite the NP-hard nature of the problem, using a GPU-friendly continuous relaxation. Experimental results on the COGS benchmark show that this method outperforms other treeless models, particularly in generalizing to deeper recursion. However, structural generalization remains challenging in some cases. The paper highlights the technical challenges of aligning input and output tokens and inducing linguistically correct permutations during training. For further details, the full paper or poster is recommended.</sample>
    <sample id="346">Die Autoren gehören der Universität von Cambridge an.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unsere Arbeit "Markierte Persönlichkeiten: Verwendung natürlicher Sprachprompts zur Messung von Stereotypen in Sprachmodellen" sprechen. Diese Arbeit wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren haben viele die Verbreitung sozialer Vorurteile und Stereotypen in großen Sprachmodellen oder LLMs dokumentiert. Diese Maßnahmen haben jedoch verschiedene Einschränkungen. Sie stützen sich in der Regel auf handgefertigte Datensätze, die sehr zeitaufwendig zu erstellen sind, und messen in der Regel nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte verallgemeinern können, oder sie erfassen einfach sehr allgemeine, breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen. Darüber hinaus berücksichtigt die meisten Arbeit in diesem Bereich nicht die Intersektionalität, die die Vorstellung ist, dass vielfältige soziale Identitäten Vorurteile verstärken und einzigartige Orte des Schadens sein können. Um diese Einschränkungen zu überwinden, verlassen wir uns auf die Eigenschaft, dass diese neueren, instruktionsabgestimmten LLMs sehr gut auf Anweisungen und Prompts reagieren können. Wir können also das Modell bitten, eine Persona zu generieren, eine Darstellung einer imaginären Person, indem wir einen Prompt wie "Stellen Sie sich vor, Sie sind eine asiatische Frau. Beschreiben Sie sich selbst." verwenden. Und wir können sofort sehen, dass dies sehr gut auf jede Demografie verallgemeinbar ist, da wir einfach den gewünschten Identitätsmarker in diesen Prompt eingeben können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass, obwohl die Ausgaben nicht übermäßig negativ oder toxisch im traditionellen Sinne dieser Wörter sind, es einige interessante Muster gibt. Die asiatische Frau wird als unauffällig dargestellt; die Frau aus dem Nahen Osten wird mit Wörtern wie exotisch und verweisend auf eine faszinierende Region bezeichnet. Und beide Frauen of Color Personas machen Verweise auf die Abstammung, während die weiße Mann Persona nichts dergleichen hat. Um diese Muster zu erfassen, hat unsere Methode zwei Teile. Der erste besteht darin, diese Personas zu generieren. Unsere Prompts zur Generierung dieser Personas wurden von einer Studie inspiriert, in der sie diese Prompts menschlichen Probanden gegeben haben und feststellten, dass sie dadurch auch rassistische Stereotypen aufdecken konnten. Dies ermöglicht auch einen direkten Vergleich zwischen den generierten Personas und den von Menschen geschriebenen Antworten. Der zweite Teil sind markierte Wörter, eine Methode zur Identifizierung der Wörter, die markierte Gruppen von unmarkierten unterscheiden, auf die ich gleich näher eingehen werde. Der Vorteil dabei ist, dass wir sehr spezifische Stereotypen und Muster erhalten, ohne uns auf ein bestimmtes Lexikon verlassen zu müssen. Die Methode der markierten Wörter basiert auf dem soziolinguistischen Konzept der "Markierung", das besagt, dass es ein unmarkiertes Standardbild gibt und jede Gruppe, die von diesem Standardbild abweicht, sprachlich markiert ist. Zum Beispiel wird das Wort "Krieger" normalerweise mit Männern assoziiert. Wenn also Menschen eine Kriegerin beschreiben, werden sie normalerweise tatsächlich "Kriegerin" sagen und den Begriff mit "Frau" markieren. Im weiteren Sinne sind dominante Gruppen in der Gesellschaft sowohl sprachlich als auch sozial unmarkiert, während marginalisierte Gruppen in der Regel markiert sind. In unserer Methode weisen wir zunächst die unmarkierten und markierten Gruppen aus und vergleichen dann die Personas mit der Fightin’ Words-Methode, die im Grunde die Verwendung gewichteter Log-Odds-Verhältnisse zur Unterscheidung der Top-Wörter für jede markierte Gruppe ist. Zum Beispiel würden wir für die Personas von schwarzen Frauen die Log-Odds-Verhältnisse gegen weiße Personas und Männer-Personas vergleichen, da dies die beiden entsprechenden unmarkierten Gruppen sind. Nun zu einigen Ergebnissen. Zuerst verwenden wir ein Lexikon von Stereotypen und stellen fest, dass die generierten Personas viel mehr Stereotypen enthalten als die von Menschen geschriebenen. Wenn wir jedoch die Verteilung der Wörter und des Lexikons betrachten, finden wir sehr unterschiedliche Dinge. Während die generierten Personas viel höhere Raten der Lexikonwörter haben, haben die von Menschen geschriebenen eine viel breitere Verteilung von Wörtern, während die Stereotypwörter, die in den generierten Personas enthalten sind, wirklich nur die Wörter "groß" und "athletisch" sind. Also wirklich nur die positiven oder zumindest nicht negativen. Und tatsächlich erfasst dieses Lexikon viele der schädlichen Muster, die wir in den früheren Folien gut gesehen haben, überhaupt nicht. Anstatt das zu tun, wenden wir uns den Ergebnissen unserer markierten Wörter-Methode zu, um zu zeigen, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. In unserer Analyse enthüllen wir, wie diese scheinbar positiven Darstellungen schädliche Muster widerspiegeln. Zuerst gehören zu unseren Gruppen die Top-Wörter Dinge wie "Kultur", "Tradition", "stolz" und "exotisch". Diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und unterscheiden sie als anders von der weißen Norm. Dies trägt zu einem langen Erbe der Diskriminierung und der Anderen-Machung für diese Gruppen bei. Darüber hinaus spiegeln sich viele gängige Tropen in diesen Wörtern wider, insbesondere für Frauen of Color. Zum Beispiel beinhalten die Wörter, die lateinamerikanische Frauen beschreiben, Dinge wie "lebendig" und "kurvig", die mit einem Trop des Tropenkönigs in Verbindung stehen. Für asiatische Frauen sind die Wörter Dinge wie "zart" und "delikat" und "seidig", die mit einer langen Geschichte asiatischer Frauen in Verbindung stehen, die stark sexualisiert, sehr demütig und unterwürfig angesehen werden, und so weiter. Und schließlich sehen wir bei schwarzen Frauen, dass einige der Top-Wörter Dinge wie "stark" und "widerstandsfähig" sind. Dies steht in Verbindung mit einem Archetyp, den die Leute die "Starke schwarze Frau" genannt haben. Und obwohl es auf den ersten Blick positiv klingt, gibt es Arbeiten, die zeigen, dass dieser Archetyp tatsächlich sehr schädlich ist, da er diese Demografien unter Druck setzt, gegen gesellschaftliche Hindernisse widerstandsfähig und stark zu sein. Anstatt tatsächlich daran zu arbeiten, diese Hindernisse zu ändern, setzt er Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen gesundheitlichen Ergebnissen für diese Menschen führt, unter anderem. Im weiteren Sinne stellen wir fest, dass die Wörter für jede markierte Gruppe im Wesentlichen nur sehr essentialisierende Erzählungen widerspiegeln. Basierend auf diesen Mustern schließen wir mit drei Empfehlungen für Modellbesitzer. Erstens sollten wir als Forscher positive Stereotypen und essentialisierende Erzählungen ansprechen. Wir sollten auch eine intersektionale Linse verwenden, um Vorurteile und Schäden zu untersuchen, da es viele Dinge geben könnte, die übersehen werden, wenn wir das nicht tun. Und schließlich sollte es wirklich mehr Transparenz über Methoden zur Minderung von Vorurteilen geben, denn zum Beispiel, wie diese positiven Stereotypen, wissen wir nicht, ob es, weil es eine Art seltsamer übertriebener Wertanpassung gibt, oder vielleicht andere Anti-Stereotyp-Methoden, die zu diesen schädlichen Mustern führen. Wir können einfach keine Annahmen treffen oder das weiter untersuchen, ohne mehr Transparenz. Vielen Dank fürs Zuhören. Viel Spaß auf der ACL.</sample>
    <sample id="348">**Abstract:**  
This paper, "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models," investigates stereotypes in large language models (LLMs) by generating personas through prompts like "Imagine you are an Asian woman. Describe yourself." Unlike traditional methods, this approach is generalizable across demographics and captures nuanced patterns. The Marked Words method, inspired by sociolinguistic markedness, identifies distinguishing words for marked groups (e.g., women of color) compared to unmarked groups (e.g., white men). Results reveal that while LLMs generate fewer harmful stereotypes than human-written personas, they perpetuate essentializing narratives through seemingly positive words like "exotic" (Latina women), "petite" (Asian women), and "strong" (Black women). These patterns reflect harmful stereotypes and tropes, such as hyper-sexualization, resilience archetypes, and tropicalism. The study emphasizes the need to address positive stereotypes, adopt an intersectional lens, and increase transparency in bias mitigation methods to better understand and mitigate harmful narratives in LLMs.</sample>
    <sample id="349">Hallo alle, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie in China. Es ist mir eine Freude, ein kurzes Werbevideo für unsere Veröffentlichung zu präsentieren. Kopieren Sie mein Modell? Schutz des Urheberrechts von Embedding-Diensten durch Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu Embedding-Diensten vorstellen. Derzeit sind große Sprachmodelle wie GPT, LLAMA, PALM außergewöhnlich in der natürlichen Sprachverarbeitung und -generierung. Embedding-Dienste sind einer der Dienste, die auf großen Sprachmodellen aufbauen, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine auf GPT basierende Embedding-API an. Neuere Arbeiten haben jedoch gezeigt, dass Angreifer das Modell durch das Lernen aus dem Embedding stehlen können und ähnliche Dienste anbieten. Daher ist es notwendig, das Urheberrecht von Embedding-Diensten zu schützen. Um das Urheberrecht von Embedding-Diensten zu schützen, ist eine der Lösungen, ein Wasserzeichen in den Anbieterdienst einzubetten und zu erkennen, ob ein anderer Dienst das Wasserzeichen enthält. Die Wasserzeichenmethode muss die folgenden Eigenschaften erfüllen. Erstens sollte die Methode für Embedding-Dienste anwendbar sein. Zweitens sollte das Wasserzeichen die Nützlichkeit der bereitgestellten Embeddings nicht beeinträchtigen. Drittens sollte das Wasserzeichen für den Angreifer verdeckt genug sein oder der Angreifer sollte das Wasserzeichen leicht entfernen können. Schließlich muss das Wasserzeichen während des Modellauszugsprozesses auf die Dienste des Angreifers übertragbar sein. Bestehende Arbeiten können grob in vier Kategorien eingeteilt werden. Diese Methode ist jedoch entweder nicht auf Embedding-Dienste anwendbar oder fehlt es an Übertragbarkeit. Daher schlagen wir in dieser Veröffentlichung den Embedding-Marker vor, eine auf Backdoors basierende Wasserzeichenmethode, die für Embedding-Dienste anwendbar ist. Lassen Sie mich dann die Details unseres Embedding-Markers vorstellen. Der Embedding-Marker enthält zwei Hauptschritte. Wasserzeichen-Einbringung und Urheberrechtsverifizierung. Bevor wir zu diesen Hauptschritten übergehen, wählen wir zunächst einen Trigger-Satz aus. Der Trigger-Satz ist eine Gruppe von Wörtern in einem moderaten Häufigkeitsintervall. Wir nehmen an, dass der Anbieter einen allgemeinen Textkorpus sammeln und die Wortfrequenz damit zählen kann. Bei der Wasserzeichen-Einbringung definieren wir zunächst ein Ziel-Embedding. Wenn ein Benutzer einen Satz an den Anbieterdienst sendet, zählt der Anbieter die Anzahl der Trigger im Satz. Das bereitgestellte Embedding ist eine gewichtete Summe des Ziel-Embeddings und des ursprünglichen Embeddings. Das Gewicht des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding. Die Urheberrechtsverifizierung besteht darin, zu erkennen, ob ein Modell hinter einem anderen Dienst das Wasserzeichen enthält. Wir erstellen zunächst eine Hintertür und einen gutartigen Datensatz. Der Hintertürdatensatz enthält Sätze, deren alle Wörter zum Trigger-Satz gehören, während alle Wörter in den Sätzen des gutartigen Datensatzes nicht zum Trigger-Satz gehören. Dann fordert der Anbieter die Embeddings vom Dienst des Diebes mit dem Datensatz an. Die Cosinus- und L2-Ähnlichkeit zwischen dem angeforderten Embedding und dem Ziel-Embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen gutartigem und Hintertürdatensatz, die als Delta-Cosinus und Delta-L2 definiert ist. In der Zwischenzeit wenden wir auch den KS-Test an und verwenden dessen p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter den Wikitextdatensatz verwendet, um die Wortfrequenz zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding-Marker eine großartige Erkennungsleistung erzielen kann, während er eine großartige Nützlichkeit für nachgelagerte Aufgaben beibehält. Wir validieren auch die Verdecktheit des bereitgestellten Embeddings, indem wir die Embeddings von Sätzen auf vier Datensätzen mit PCA visualisieren. Die Legende der Abbildungen bedeutet die Anzahl der Trigger in jedem Satz. Wie in den Abbildungen gezeigt, ist es schwer, zwischen den Hintertür-Embeddings und normalen Embeddings zu unterscheiden. Das war's. Vielen Dank. Willkommen, mit uns zu diskutieren.</sample>
    <sample id="350">This paper critically examines the concept of "superhuman performance" in Natural Language Understanding (NLU), particularly in the context of leaderboard-based benchmarks like SuperGLUE and SQuAD. While systems often outperform humans in these benchmarks, the authors argue that such comparisons are flawed due to several issues. First, humans are typically evaluated on smaller subsets of the test data, while systems are tested on the full dataset, creating an unfair comparison. Second, ground-truth answers in these datasets often contain errors, such as overly general hypotheses or spurious correlations, which systems can exploit but humans cannot. Third, human performance is frequently estimated using vague methods like average scores, rather than comparing systems to the best possible human performance. Additionally, low pay rates and undisclosed annotator details further undermine the validity of human baselines. The authors conclude that claims of superhuman performance lack scientific meaning and recommend constructing more reliable benchmarks by addressing these issues. They emphasize the need for fair evaluation practices, transparent data collection, and better motivation for human participants to ensure meaningful comparisons between humans and NLU systems.</sample>
    <sample id="351">This paper investigates the generalization capabilities of Named Entity Recognition (NER) models trained on the CoNLL-2003 dataset, which has been used for nearly 20 years. The authors created the CoNLL++ dataset, comprising Reuters news articles from 2020 annotated using the same guidelines, to evaluate how well these models perform on modern data. They fine-tuned over 20 models on CoNLL-2003 and tested them on both CoNLL-03 and CoNLL++. The results indicate that transformer models, larger models, and more fine-tuning examples generally lead to better generalization. The performance drop observed in some models is attributed to temporal drift, not adaptive overfitting, as the diminishing returns hypothesis was not confirmed. The study concludes that for effective generalization, a combination of advanced architecture, larger model size, and sufficient fine-tuning examples is essential. The findings suggest that CoNLL-2003 taggers still perform well in 2023, but further research is needed to address temporal drift and improve generalization.</sample>
    <sample id="352">ABC-Eval steht für "Annotating Behaviors in Chat" und ist ein neues dimensionalen Ansatz zur Bewertung von conversational AI.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" addresses the challenge of input underspecification in code generation and program synthesis tasks. It proposes a method to interactively gather missing specifications by asking clarification questions (CQAs) to natural language descriptions (NLDs). The authors introduce the synthetic dataset CodeClarQA, which includes clarifications on key operations, and a pipeline for code generation through interaction. The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. The method evaluates the alignment of NLDs with operation documentation using schema element similarity scores. Results show that the proposed approach improves code generation performance, with MPNet outperforming other models in identifying missing key operations. Error analysis highlights challenges such as taxonomy and argument mismatches, suggesting areas for improvement. The authors hypothesize that their task is more challenging than existing CQ ranking tasks and that clarifications enhance code generation. The pipeline's performance, though still below model-only trainers, demonstrates the potential of interactive code generation. The study concludes that clarified key operations contribute to better code generation, with examples showing predictions close to the ground truth. The authors invite feedback on their paper and code.</sample>
    <sample id="354">Bis zum Jahr 2020 ist das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin Doktorandin im Bereich Informatik an der Stony Brook University. Ich möchte unsere Arbeit vorstellen, die für die ACL 2023 als Langpapier angenommen wurde: „Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge“. Wir beginnen mit der Definition von kognitiver Dissonanz und warum es ein wichtiges Problem ist, sie in der Sprache zu untersuchen. Einfach ausgedrückt, ist kognitive Dissonanz eine Situation, in der zwei Überzeugungen oder Handlungen inkonsistent sind, wie in diesem Beispiel, in dem eine Person sagt: „Ich weiß, dass Zigaretten mich töten könnten“, und dann weiter erzählt: „Ich habe nach dem Meeting ein paar Zigaretten geraucht“. Diese Überzeugung und Handlung sind inkonsistent, und sie befinden sich in Dissonanz. Eine weitere Aussage wie „Ich glaube nicht, dass ich meinen Job ohne sie behalten könnte“, rechtfertigt die zweite Handlung. Und sie haben eine Konsonanzbeziehung. Obwohl Dissonanz ein sehr häufiges Phänomen ist, das wir im täglichen Entscheidungsfindungsprozess erleben, sind sie in der Sprache unter anderen Arten von Diskursbeziehungen sehr selten. Warum ist das wichtig? Die Untersuchung von kognitiver Dissonanz, die in der Sprache ausgedrückt wird, kann uns helfen, die Auswirkungen von Meinungsverschiedenheiten zwischen Menschen zu verstehen, Trends und Überzeugungswerte zu verfolgen und die Einstellungswandel in der Bevölkerung zu verstehen. Hohe kognitive Dissonanz steht auch im Zusammenhang mit Angststörungen und kann dazu beitragen, die psychische Gesundheit von Menschen besser zu verstehen. Die Untersuchung von Dissonanz, die in der Sprache ausgedrückt wird, kann auch hilfreich sein, um Extremismus und Polarisierung von gefährdeten Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die persönlichen kognitiven Stile von Individuen zu verstehen und hilft uns, Entscheidungsprozesse besser zu verstehen. Mit dem Ziel, eine Ressource für kognitive Dissonanz zu erstellen, haben wir eine groß angelegte Annotation von Dissonanzbeziehungen durchgeführt. Wir verwendeten den Ansatz „Dissonanz zuerst“, wie in dem Flussdiagramm hier zu sehen ist. Tweets wurden mit dem PDTB-Parser verarbeitet, und Paare von Diskursenheiten wurden gemäß den Richtlinien, die in unserem Papier beschrieben werden, annotiert. Wie hier zu sehen ist, wurde Dissonanz nur in 3,5 % der annotierten Paare gefunden. Nachdem wir etwa 1.000 Beispiele von Diskursenheitenpaaren gesammelt hatten, trainierten wir einen anfänglichen Klassifikator, der nur auf 43 Beispielen von Dissonanz trainiert wurde. Zu keiner Überraschung führte dies zu einer Leistung, die nicht viel besser als Zufall war. Angesichts des geringen Auftretens von Dissonanz und des Fehlens vorheriger Datensätze stehen wir vor dem Problem der absoluten Seltenheit. Um dies zu mildern, experimentieren wir mit Kombinationen aus Transfer Learning und aktivem Lernen, um so mehr dissonante Beispiele zu sammeln, während weniger Annotationsdurchläufe durchgeführt werden, was die Gesamtannotationskosten senkt und gleichzeitig die Dissonanzdetektion verbessert. Da das anfängliche Modell die Dissonanzklasse überhaupt nicht erfassen konnte, starten wir den aktiven Lernprozess, indem wir Gewichte aus eng verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: der themenunabhängigen Dissonanz-Haltungsklassifikation, einer Aufgabe, die bestimmt, ob zwei Aussagen von verschiedenen Personen in einer Debatte übereinstimmen oder nicht, unabhängig vom Thema, genannt Debatte, und der binären Klassifikation von Erweiterung und Vergleichsklassen von PDTB, da diese beiden eng mit dem Konzept von Konsonanz und Dissonanz verwandt sind und wir sie CE nennen. Wir stellen fest, dass die Null-Durchlauf-Leistung auf dem annotierten Datensatz bereits viel besser als Zufall ist, mit dem besten Wert von AUC.62. Darüber hinaus verbessern wir durch iteratives Feinabstimmen auf beiden Aufgaben die Null-Durchlauf-Leistung weiter. Daher ist dies das Modell, das wir für den kalten Start des aktiven Lernens verwenden. Als Nächstes bestimmen wir die beste Methode, um ein Modell mit neuen Daten aus jedem Durchlauf des aktiven Lernens und der Annotation zu aktualisieren. „Kumulativ“ sammelt alle Daten, die bisher aus aktivem Lernen gesammelt wurden, während „Iterativ“ das Modell durch Training auf dem neuesten Datensatz aktualisiert, der gesammelt wurde. Im Vergleich der verschiedenen Strategien stellten wir fest, dass Kumulativ gleich oder besser als Iterativ abschneidet. Als Nächstes, um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeit-der-Seltenen-Klasse-Strategie – PRC – um hauptsächlich die Beispiele auszuwählen, die von dem aktuellen Modell in jedem Durchlauf der Seltenen mit hoher Wahrscheinlichkeit klassifiziert werden. Wir vergleichen dies mit anderen State-of-the-Art-AL-Strategien, die in der Gemeinschaft üblich sind. Wir stellen fest, dass die vorgeschlagene PRC-Strategie besser als andere State-of-the-Art-Strategien abschneidet, obwohl der Unterschied klein ist. Beachten Sie, dass die Leistung für Zufall erheblich niedriger ist. In weiteren AL-Durchläufen mit den beiden besten Strategien verbessern wir die Dissonanzklassifizierungs-AUC auf 0,75, was die beste Leistung ist, die wir bisher auf der Aufgabe haben. Wir überprüfen auch die Machbarkeit jeder Strategie für die Annotationsqualität und -kosten für die Annotatoren. Wir stellen fest, dass PRC den höchsten Prozentsatz an Dissonanz hat und am besten für die seltene Klasse geeignet ist. Die Annotatoren finden die Beispiele jedoch auch schwierig. Zusammenfassend stellen wir fest, dass PRC eine einfache AL-Strategie für den Erwerb seltener Klassen und den kalten Start von AL mit angemessen gestalteten Transfer Learning-Aufgaben ist und erheblich hilft. Wir stellen auch fest, dass iterative Aktualisierung nützlich für Transfer Learning aus einem anderen Bereich ist, während in Bereichsaktive Annotations die kumulative Aktualisierung von Vorteil ist. Dies sind die Links zu unserem Kerndatensatz und unserem Papier. Zögern Sie nicht, uns zu kontaktieren, wenn Sie Fragen haben. Vielen Dank.</sample>
    <sample id="356">Die Autoren gehören der Universität Stanford an.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Die Arbeit wurde von sechs Autoren verfasst: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins, Graham Neubig und sich selbst.</sample>
    <sample id="359">Der Ansatz wird mit der **Wait-k-Strategie** und der **Local Agreement**-Strategie verglichen, die beide auf Offline-Modellen angewendet werden, sowie mit dem **State-of-the-Art-Architektur**, die speziell für simultane Vorvertauschung entwickelt wurde.</sample>
    <sample id="361">**Abstract:**  
This work introduces *CounterComp*, a method to improve compositional generalization in multi-step quantitative reasoning tasks, particularly for question answering on financial tables. State-of-the-art models struggle with tasks requiring more than two arithmetic operations due to memorization of spurious patterns. *CounterComp* addresses this by mining counterfactual scenarios from training data, creating positive and negative examples based on interventions in the input that either preserve or alter the output. These examples are used to add an auxiliary metric learning loss during training, dynamically adjusting the margin based on the extent of intervention. This approach improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. Qualitatively, *CounterComp* helps models attend to more meaningful tokens in the input, improving their ability to generate appropriate operations in the output. The method is applied to three state-of-the-art baselines, demonstrating consistent performance gains, especially for tasks with more than two reasoning steps. This work highlights the potential of counterfactual learning to improve generalization in multi-step reasoning tasks.</sample>
  </task>
</testset>